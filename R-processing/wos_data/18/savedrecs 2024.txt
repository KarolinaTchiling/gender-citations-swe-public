FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Coupeau, P
   Fasquel, JB
   Dinomais, M
AF Coupeau, Patty
   Fasquel, Jean-Baptiste
   Dinomais, Mickael
TI On the use of GNN-based structural information to improve CNN-based
   semantic image segmentation
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Image segmentation; Structural information; Node classification; Graph
   neural network; Graph coarsening
AB Convolutional neural networks (CNNs) are widely used for semantic image segmentation across various fields (medicine, robotics), capturing local pixel dependencies for good results. Nevertheless, CNNs struggle to grasp global contextual representations, sometimes leading to structural inconsistencies. Recent approaches aim to broaden their scope using attention mechanisms or deep models, resulting in heavy-weight architectures. To boost CNN performance in semantic segmentation, we propose using a graph neural network (GNN) as a post -processing step. The GNN conducts node classification on appropriately coarsened graphs encoding class probabilities and structural information related to regions segmented by the CNN. The proposal, applicable to any CNN producing a segmentation map, is evaluated on several CNN architectures, using two public datasets (FASSEG and IBSR), with four graph convolution operators. Results reveal performance improvements, enhancing on average the Hausdorff distance by 24.3% on FASSEG and by 74.0% on IBSR. Furthermore, our approach demonstrates resilience to small training datasets.
C1 [Coupeau, Patty; Fasquel, Jean-Baptiste; Dinomais, Mickael] Univ Angers, LARIS, SFR MATHSTIC, F-49000 Angers, France.
   [Dinomais, Mickael] Univ Angers, Dept Med Phys & Readaptat, Ctr Hosp, F-49000 Angers, France.
C3 Universite d'Angers; Universite d'Angers; Centre Hospitalier
   Universitaire d'Angers
RP Coupeau, P (corresponding author), Univ Angers, LARIS, SFR MATHSTIC, F-49000 Angers, France.
EM patty.coupeau@etud.univ-angers.fr
CR Abdullah AA, 2022, IEEE ACCESS, V10, P36538, DOI 10.1109/ACCESS.2022.3163384
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Taghanaki SA, 2021, ARTIF INTELL REV, V54, P137, DOI 10.1007/s10462-020-09854-1
   Bacciu D, 2020, NEURAL NETWORKS, V129, P203, DOI 10.1016/j.neunet.2020.06.006
   Beauchemin M., 1998, CAN J REMOTE SENS, V24, P3, DOI DOI 10.1080/07038992.1998.10874685
   Bloch I, 2015, FUZZY SET SYST, V281, P280, DOI 10.1016/j.fss.2015.06.017
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen YP, 2019, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2019.00052
   Chopin J., 2020, INT CONF IMAG PROC, P1, DOI [10.1109/IPTA50016.2020.9286611, DOI 10.1109/ipta50016.2020.9286611]
   Chopin J, 2023, COMPUT VIS IMAGE UND, V235, DOI 10.1016/j.cviu.2023.103744
   Cicek Ozgun, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P424, DOI 10.1007/978-3-319-46723-8_49
   Coupeau P, 2022, INT CONF IMAG PROC, DOI 10.1109/IPTA54936.2022.9784143
   Coupeau P, 2022, COMPUT METH PROG BIO, V214, DOI 10.1016/j.cmpb.2021.106563
   Delaye A, 2011, PROC INT CONF DOC, P1220, DOI 10.1109/ICDAR.2011.246
   Diao Q, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14020305
   Diehl F, 2019, Arxiv, DOI arXiv:1905.10990
   Ding J, 2022, PROC CVPR IEEE, P11573, DOI 10.1109/CVPR52688.2022.01129
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fasquel JB, 2019, IEEE T PATTERN ANAL, V41, P1043, DOI 10.1109/TPAMI.2018.2827939
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gao HY, 2022, IEEE T PATTERN ANAL, V44, P4948, DOI 10.1109/TPAMI.2021.3081010
   Garcia-Garcia A, 2018, APPL SOFT COMPUT, V70, P41, DOI 10.1016/j.asoc.2018.05.018
   Hatamizadeh A, 2022, IEEE WINT CONF APPL, P1748, DOI 10.1109/WACV51458.2022.00181
   Jin Y, 2021, PATTERN RECOGN LETT, V148, P29, DOI 10.1016/j.patrec.2021.04.024
   Khan K, 2015, IEEE IMAGE PROC, P827, DOI 10.1109/ICIP.2015.7350915
   Kunze L, 2014, IEEE INT C INT ROBOT, P2910, DOI 10.1109/IROS.2014.6942963
   Kushibar K, 2018, MED IMAGE ANAL, V48, P177, DOI 10.1016/j.media.2018.06.006
   Laura CO, 2022, METHODS, V202, P3, DOI 10.1016/j.ymeth.2021.06.008
   Lee B, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0236493
   Li X., 2020, P IEEE CVF C COMP VI, P8950
   Li YM, 2020, Arxiv, DOI arXiv:2009.05266
   Liu QH, 2021, Arxiv, DOI [arXiv:2009.01599, 10.48550/ARXIV.2009.01599]
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Maciel J, 2003, IEEE T PATTERN ANAL, V25, P187, DOI 10.1109/TPAMI.2003.1177151
   Mohamed Abduallah, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14412, DOI 10.1109/CVPR42600.2020.01443
   Morris C, 2019, AAAI CONF ARTIF INTE, P4602
   Nassar Ahmed Samy, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P488, DOI 10.1007/978-3-030-58571-6_29
   Ouyang S, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13010119
   Renton G, 2021, PATTERN RECOGN LETT, V152, P391, DOI 10.1016/j.patrec.2021.09.020
   Romero A., 2018, INT C LEARN REPR, P2920, DOI DOI 10.48550/ARXIV.1710.10903
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shin SY, 2019, MED IMAGE ANAL, V58, DOI 10.1016/j.media.2019.101556
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Simonovsky M, 2017, PROC CVPR IEEE, P29, DOI 10.1109/CVPR.2017.11
   Tan MX, 2019, PR MACH LEARN RES, V97
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang K, 2022, BIOMED SIGNAL PROCES, V75, DOI 10.1016/j.bspc.2022.103621
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Welling M., 2016, ICLR, P1, DOI DOI 10.48550/ARXIV.1609.02907
   Xie EZ, 2021, ADV NEUR IN, V34
   Xu RT, 2022, ENG APPL ARTIF INTEL, V110, DOI 10.1016/j.engappai.2022.104739
   Ying R, 2018, ADV NEUR IN, V31
   Zanfir A, 2018, PROC CVPR IEEE, P2684, DOI 10.1109/CVPR.2018.00284
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang J.-W., 2022, ADV NEURAL INFORM PR, P6575
   Zhang J, 2021, MED IMAGE ANAL, V73, DOI 10.1016/j.media.2021.102183
   Zhang ZW, 2022, IEEE T KNOWL DATA EN, V34, P249, DOI 10.1109/TKDE.2020.2981333
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   ZIJDENBOS AP, 1994, IEEE T MED IMAGING, V13, P716, DOI 10.1109/42.363096
NR 60
TC 0
Z9 0
U1 3
U2 3
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104167
DI 10.1016/j.jvcir.2024.104167
EA MAY 2024
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TQ3Q6
UT WOS:001242690200001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zhai, HX
   Chen, ZX
   Liu, CY
   Bai, HB
   Wu, QMJ
AF Zhai, Hanxiao
   Chen, Zhenxue
   Liu, Chengyun
   Bai, Huibin
   Wu, Q. M. Jonathan
TI Category-based depth incorporation for salient object ranking
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Salient object ranking; Category awareness; Depth incorporation; Human
   psychology
ID IMAGE
AB Salient object ranking is a visual task that aims to mimic the human visual system's ability to prioritize various salient objects in complex scenes. It integrates various computer vision tasks, including object detection and instance segmentation, and draws insights from psychology and biology. Despite recent advancements in salient object ranking research, there is still a need for a more in-depth exploration of certain underlying factors that influence the ranking. Previous studies have primarily focused on factors such as the scale and position of objects, as well as interactions between objects and their context. However, they often overlooked the crucial interaction between objects and observers. Unlike other visual tasks, salient object ranking is highly subjective and influenced by the observer. Therefore, establishing a meaningful connection between the observed object and the observer becomes crucial. Our model addresses this gap by placing emphasis on the distance between objects and observers. It investigates the implicit relationship between object categories and distance through the Category-Aware Attention (CAA) module. This innovative approach incorporates depth into salient object ranking, resulting in an improvement in ranking performance.
C1 [Zhai, Hanxiao; Chen, Zhenxue; Liu, Chengyun; Bai, Huibin] Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
   [Wu, Q. M. Jonathan] Univ Windsor, Dept Elect & Comp Engn, Windsor, ON N9B 3P4, Canada.
C3 Shandong University; University of Windsor
RP Chen, ZX (corresponding author), Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
EM chenzhenxue@sdu.edu.cn; liuchengyun@sdu.edu.cn
RI Zhang, Youmin/AAT-7095-2020
OI Zhang, Youmin/0000-0002-9731-5943; Chen, Zhenxue/0000-0001-9637-5170
FU Key R&D Project of Shandong Province [2022CXGC010503]; The 2022 Industry
   Leading Talents Support Program "Hai You Project
FX This work was supported in part by the Key R&D Project of Shandong
   Province (2022CXGC010503) , in part by 2022 Industry Leading Talents
   Support Program "Hai You Project'' (Research and industrialization of
   management and control system for information technology service
   industry based on AI and edge computing) . Hanxiao Zhai and Zhenxue Chen
   contributed equally to this work and should be considered as the
   co-first authors.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Chen JS, 2016, PROC CVPR IEEE, P507, DOI 10.1109/CVPR.2016.61
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Fang H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16311, DOI 10.1109/ICCV48922.2021.01602
   Fang YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6890, DOI 10.1109/ICCV48922.2021.00683
   Goyal P, 2018, Arxiv, DOI arXiv:1706.02677
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He SF, 2015, INT J COMPUT VISION, V115, P330, DOI 10.1007/s11263-015-0822-0
   Heuer F, 2021, IEEE INT CONF COMP V, P997, DOI 10.1109/ICCVW54120.2021.00116
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657
   Islam MA, 2018, PROC CVPR IEEE, P7142, DOI 10.1109/CVPR.2018.00746
   Kim J, 2016, INT C PATT RECOG, P609, DOI 10.1109/ICPR.2016.7899701
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee Y., 2020, CVPR, P13906
   Lee YW, 2019, IEEE COMPUT SOC CONF, P752, DOI 10.1109/CVPRW.2019.00103
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu NA, 2022, IEEE T PATTERN ANAL, V44, P8321, DOI 10.1109/TPAMI.2021.3107872
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma GX, 2020, IEEE T VIS COMPUT GR, V26, P3535, DOI 10.1109/TVCG.2020.3023636
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Quan R, 2018, IEEE T MULTIMEDIA, V20, P1101, DOI 10.1109/TMM.2017.2763780
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ruder S, 2017, Arxiv, DOI [arXiv:1706.05098, DOI 10.48550/ARXIV.1706.05098]
   Schauerte B, 2014, IEEE INT C INT ROBOT, P995, DOI 10.1109/IROS.2014.6942680
   Siris Avishek, 2020, P IEEE CVF C COMP VI, P12133
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Tian X, 2022, PROC CVPR IEEE, P5872, DOI 10.1109/CVPR52688.2022.00579
   Uno K., 2020, Non-periodic gait planning based on salient region detection for a planetary cave exploration robot
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang SR, 2020, AAAI CONF ARTIF INTE, V34, P12208
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang S, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1383, DOI 10.1145/3343031.3350990
   Yao LL, 2023, IEEE INT CONF ROBOT, P3353, DOI 10.1109/ICRA48891.2023.10161487
   Yu BY, 2022, IEEE T INF FOREN SEC, V17, P413, DOI 10.1109/TIFS.2021.3135748
   Zhang DW, 2017, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2017.436
   Zhang JM, 2016, PROC CVPR IEEE, P5733, DOI 10.1109/CVPR.2016.618
   Zhang L, 2019, PROC CVPR IEEE, P6017, DOI 10.1109/CVPR.2019.00618
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang P, 2017, NEUROCOMPUTING, V257, P115, DOI 10.1016/j.neucom.2016.10.073
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhou ZK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9846, DOI 10.1109/ICCV48922.2021.00972
   Zhu T, 2022, IEEE T MULTIMEDIA, V24, P3036, DOI 10.1109/TMM.2021.3092202
NR 56
TC 0
Z9 0
U1 3
U2 3
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104165
DI 10.1016/j.jvcir.2024.104165
EA MAY 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TE7A0
UT WOS:001239638800001
DA 2024-08-05
ER

PT J
AU Wang, YH
   Wang, YF
   Cui, TY
   Fang, ZJ
AF Wang, Yihan
   Wang, Yongfang
   Cui, Tengyao
   Fang, Zhijun
TI Occupancy map-based low complexity motion prediction for video-based
   point cloud compression
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Dynamic point cloud compression; Occupancy map; Low complexity; Motion
   prediction
ID VVC
AB This paper proposes an occupancy map -based low complexity motion prediction method for video -based point cloud compression (V-PCC). Firstly, we propose to utilize the occupancy map, direction gradient, and regional dispersion to divide the attribute maps into static, complex, and common blocks. Then, we propose an early termination method for static blocks, an adaptive motion search range method for complex blocks, and an early inter prediction mode decision algorithm for affine motion regions in common blocks. Experiment results show that, in comparison to the test model category2 (TMC2) v15.0, called the anchor method, the average bitrate savings of Y, U, and V components of the proposed method achieve 24.27%, 32.64%, and 31.23% on 8i voxelized full bodies version 2 (8iVFBv2) dataset, respectively. Further, the time savings is 41.97% for attribute maps. Similarly, the proposed method also achieves consistent performance on Microsoft voxelized upper bodies (MVUB) dataset.
C1 [Wang, Yihan; Wang, Yongfang; Cui, Tengyao] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Sch Commun & Informat Engn, Shanghai, Peoples R China.
   [Fang, Zhijun] Donghua Univ, Sch Comp Sci & Technol, Shanghai, Peoples R China.
   [Wang, Yihan; Wang, Yongfang; Cui, Tengyao] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Sch Commun & Informat Engn, 99 Shangda Rd, Shanghai 200444, Peoples R China.
   [Fang, Zhijun] Donghua Univ, Sch Comp Sci & Technol, 2999 Renmin North Rd, Shanghai 201620, Peoples R China.
C3 Shanghai University; Donghua University; Shanghai University; Donghua
   University
RP Wang, YF (corresponding author), Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Sch Commun & Informat Engn, Shanghai, Peoples R China.
EM wangyihan@shu.edu.cn; yfw@shu.edu.cn; tyaocui@shu.edu.cn;
   zjfang@dhu.edu.cn
FU National Natural Science Foundation of China [61671283, U2033218]
FX This work was supported by National Natural Science Foundation of China
   under Grant No. 61671283, U2033218.
CR Anis A, 2016, INT CONF ACOUST SPEE, P6360, DOI 10.1109/ICASSP.2016.7472901
   [Anonymous], 2017, ISO/IEC JTC1/SC29 WG11 Doc N17248
   Bjontegaard G., 2001, document ITU-T SG16 Q6, VCEG-M33
   Chien WD, 2014, IEEE IMAGE PROC, P3696, DOI 10.1109/ICIP.2014.7025750
   Costa A, 2019, IEEE INT WORKSH MULT, DOI 10.1109/mmsp.2019.8901690
   Cui J, 2020, IEEE DATA COMPR CONF, P103, DOI 10.1109/DCC47342.2020.00018
   de Queiroz RL, 2017, IEEE T IMAGE PROCESS, V26, P3886, DOI 10.1109/TIP.2017.2707807
   dEon Eugene, 2017, ISO/IEC JTC1/SC29 Joint WG11/WG1 (MPEG/JPEG) input document WG11M40059/WG1M74006, V7, P11
   github, Point Cloud Compression Category 2 Reference Software, TMC2-15.0
   Graziosi D, 2020, APSIPA TRANS SIGNAL, V9, DOI 10.1017/ATSIP.2020.12
   He JL, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3382506
   He LY, 2017, ASIA-PAC CONF COMMUN, P345
   Hong H, 2022, IEEE DATA COMPR CONF, P369, DOI 10.1109/DCC52660.2022.00045
   HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011
   Huang XP, 2017, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-017-0226-x
   Hussain AJ, 2014, ADV INTELL SYST, V264, P359, DOI 10.1007/978-3-319-04960-1_32
   Kammerl J, 2012, IEEE INT CONF ROBOT, P778, DOI 10.1109/ICRA.2012.6224647
   Kim J, 2020, IEEE ACCESS, V8, P83538, DOI 10.1109/ACCESS.2020.2991478
   Lee TK, 2017, IEEE INT CON MULTI, P1249, DOI 10.1109/ICME.2017.8019346
   Li L, 2020, IEEE T IMAGE PROCESS, V29, P289, DOI 10.1109/TIP.2019.2931621
   Li L, 2019, IEEE DATA COMPR CONF, P498, DOI 10.1109/DCC.2019.00058
   Li L, 2018, IEEE T CIRC SYST VID, V28, P1934, DOI 10.1109/TCSVT.2017.2699919
   Liu JQ, 2019, IEEE INT CON MULTI, P904, DOI 10.1109/ICME.2019.00160
   Loop C., 2016, document ISO/IEC JTC1/SC29 Joint WG11/WG1 (MPEG/JPEG), V2016
   Mekuria R, 2017, IEEE T CIRC SYST VID, V27, P828, DOI 10.1109/TCSVT.2016.2543039
   Mercat A, 2021, IEEE ACCESS, V9, P67813, DOI 10.1109/ACCESS.2021.3077116
   Pan ZQ, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159170
   Ren WZ, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12071143
   Schwarz S., 2017, Document ISO/IEC JTC1/SC29/WG11 m41779
   Schwarz S, 2018, PICT COD SYMP, P61, DOI 10.1109/PCS.2018.8456265
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Schwarz Sebastian, 2018, Document ISO/IEC JTC1/SC29/WG11 w17766
   Shang XW, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103683
   Shi HY, 2022, IEEE OPEN J SIGNAL P, V3, P155, DOI 10.1109/OJSP.2022.3160392
   Singla Ashutosh, 2023, IXR '23: Proceedings of the 2nd International Workshop on Interactive eXtended Reality, P21, DOI 10.1145/3607546.3616803
   Souto AL, 2020, Arxiv, DOI arXiv:2008.08438
   Thanou D, 2016, IEEE T IMAGE PROCESS, V25, P1765, DOI 10.1109/TIP.2016.2529506
   Thanou D, 2015, IEEE IMAGE PROC, P3235, DOI 10.1109/ICIP.2015.7351401
   Xiong J, 2022, IEEE T CIRC SYST VID, V32, P813, DOI 10.1109/TCSVT.2021.3063501
   Zhou W, 2018, COMPUT GRAPH-UK, V77, P50, DOI 10.1016/j.cag.2018.09.012
   Zupancic I, 2021, PICT COD SYMP, P231, DOI 10.1109/PCS50896.2021.9477405
NR 41
TC 0
Z9 0
U1 3
U2 3
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104110
DI 10.1016/j.jvcir.2024.104110
EA MAR 2024
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NS8X4
UT WOS:001202545900001
DA 2024-08-05
ER

PT J
AU Hu, XY
   Wang, XM
   Yang, DM
   Ren, W
   Wang, JL
   Liu, B
AF Hu, Xiaoyu
   Wang, Xingmei
   Yang, Dongmei
   Ren, Wei
   Wang, Jinli
   Liu, Bo
TI FFLDGA-Net: Image retrieval method based on Feature Fusion Learnable
   Descriptor Graph Attention Network
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Image retrieval; Graph attention network; Hard negative mining; Fusion
AB Image retrieval aims to retrieve and return the image in the database that is most similar to the query image. However, the performance of image retrieval models is often hindered by the limited dimensionality of images, which lacks depth information about objects. To address this issue, we propose a novel image retrieval model called FFLDGA-Net (Feature Fusion -based Learnable Descriptor Graph Attention Network). This model aims to overcome the absence of depth information in images by fusing feature information from both image data and point cloud data. First, we introduce the LDGA-Net, which effectively improved the model's ability to mine hard samples and negative samples. Then, we combine the multi -scale route convolution module with a one-dimensional path aggregation network to fuse point clouds and image features at multiple scales, and establish a high -dimensional relationship between features and low -dimensional features. To mitigate training noise, we incorporate a soft label strategy tailored to the dataset's characteristics. Our experimental results on two benchmark datasets demonstrate the significant improvements achieved by FFLDGA-Net in image retrieval performance.
C1 [Hu, Xiaoyu; Wang, Xingmei; Yang, Dongmei; Ren, Wei; Wang, Jinli] Harbin Engn Univ, Coll Comp Sci & Technol, Harbin, Peoples R China.
   [Wang, Xingmei] Modeling & Emulat Egovt Natl Engn Lab, Harbin, Peoples R China.
   [Liu, Bo] Key Lab Av Syst Integrated Technol, Shanghai, Peoples R China.
C3 Harbin Engineering University
RP Yang, DM (corresponding author), Harbin Engn Univ, Coll Comp Sci & Technol, Harbin, Peoples R China.
EM xiaoyuhu@hrbeu.edu.cn; wangxingmei@hrbeu.edu.cn;
   yangdongmei@hrbeu.edu.cn; rwkuaile@hrbeu.edu.cn; 894921255@hrbeu.edu.cn;
   liubogoogle@126.com
OI Hu, Xiaoyu/0000-0003-4715-0682
FU Key Laboratory of Avionics System Integrated Technology, Fundamental
   Research Funds for the Central Universities in China [3072022JC0601];
   Ministry of Industry and Information Technology High-tech Ship Project
   [331]
FX This work is supported by a grant from Key Laboratory of Avionics System
   Integrated Technology, Fundamental Research Funds for the Central
   Universities in China, Grant No. 3072022JC0601, and the Ministry of
   Industry and Information Technology High-tech Ship Project [2019] No.
   331.
CR Aiswarya G., 2017, 2017 International Conference on Communication and Signal Processing (ICCSP). Proceedings, P0817, DOI 10.1109/ICCSP.2017.8286478
   Bella MIT, 2019, COMPUT ELECTR ENG, V75, P46, DOI 10.1016/j.compeleceng.2019.01.022
   Brown Andrew, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P677, DOI 10.1007/978-3-030-58545-7_39
   Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164
   Cakir F, 2019, PROC CVPR IEEE, P1861, DOI 10.1109/CVPR.2019.00196
   Chen W, 2023, IEEE T PATTERN ANAL, V45, P7270, DOI 10.1109/TPAMI.2022.3218591
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Chrysos GG, 2022, IEEE T PATTERN ANAL, V44, P4021, DOI 10.1109/TPAMI.2021.3058891
   Czúni L, 2003, LECT NOTES COMPUT SC, V2849, P76
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Danapur N, 2020, MULTIMED TOOLS APPL, V79, P24463, DOI 10.1007/s11042-020-09109-9
   Garg S, 2022, INT J ROBOT RES, V41, P573, DOI 10.1177/0278364919839761
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15
   Gu Y., 2018, arXiv
   Hammouche R, 2022, EXPERT SYST APPL, V197, DOI 10.1016/j.eswa.2022.116743
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hegde V., 2016, arXiv
   Hui L, 2022, IEEE T IMAGE PROCESS, V31, P1258, DOI 10.1109/TIP.2021.3136714
   Khwildi R, 2020, VISUAL COMPUT, V36, P1111, DOI 10.1007/s00371-019-01719-1
   Lan RS, 2018, MULTIMED TOOLS APPL, V77, P10853, DOI 10.1007/s11042-017-5341-2
   Lei XY, 2019, IEEE ACCESS, V7, P124087, DOI 10.1109/ACCESS.2019.2927169
   Li B, 2021, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR46437.2021.00376
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu ZJ, 2019, ADV NEUR IN, V32
   Ma X, 2022, Arxiv, DOI arXiv:2202.07123
   Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Qi CR, 2017, ADV NEUR IN, V30
   Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566
   Ramzi E., 2021, ANN C NEUR INF PROC, P23569
   Revaud J, 2019, IEEE I CONF COMP VIS, P5106, DOI 10.1109/ICCV.2019.00521
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shin S, 2022, LECT NOTES COMPUT SC, V13672, P631, DOI 10.1007/978-3-031-19775-8_37
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Tsoumakas G., 2007, International Journal of Data Warehousing and Mining, V3, P1, DOI [10.4018/jdwm.2007070101, DOI 10.4018/JDWM.2007070101]
   Uy MA, 2018, PROC CVPR IEEE, P4470, DOI 10.1109/CVPR.2018.00470
   Veličkovic P, 2018, Arxiv, DOI arXiv:1710.10903
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang X, 2022, Vis. Comput., P1
   Wang X, 2020, PROC CVPR IEEE, P6387, DOI 10.1109/CVPR42600.2020.00642
   Wei JX, 2022, IEEE INT C INT ROBOT, P9879, DOI 10.1109/IROS47612.2022.9981296
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Yu FS, 2016, Arxiv, DOI arXiv:1511.07122
   Yu J, 2020, IEEE T NEUR NET LEAR, V31, P661, DOI 10.1109/TNNLS.2019.2908982
   Zhang L, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2490823
NR 49
TC 0
Z9 0
U1 4
U2 4
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104109
DI 10.1016/j.jvcir.2024.104109
EA MAR 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NY1U8
UT WOS:001203930600001
DA 2024-08-05
ER

PT J
AU Song, HX
   Wang, ZC
   Zhang, XP
AF Song, Haoxian
   Wang, Zichi
   Zhang, Xinpeng
TI Identification of the Original Images
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Original images; Deep network models; Variant images
ID EFFICIENT
AB In social networks, people usually communicate online by posting and passing images, people will find that there are many similar or even visually identical images. When the images determine whether the published information is true or false, original images became crucial, and at the same time, deep learning excelled in image identification. Combining the original image identification and deep learning methods, this paper proposes the original image identification method based on the deep network models, aiming to use the deep network models to identify the original images between the original images and a bunch of variant images of the original images, and obtain satisfactory results. In this way, through the trained network models, the original images can be identified and can help the viewers to determine the correctness of the information. Experimental results verify the feasibility, effectiveness and security of the proposed method.
C1 [Song, Haoxian; Wang, Zichi; Zhang, Xinpeng] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
C3 Shanghai University
RP Zhang, XP (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
EM songhaoxian@shu.edu.cn; wangzichi@shu.edu.cn; xzhang@shu.edu.cn
FU Natural Science Foundation of China [U22B2047, 62002214]; Chenguang
   Program of Shanghai Education Development Foun-dation; Shanghai
   Municipal Education Commission, China [22CGA46]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants U22B2047 and 62002214, and supported in part by the
   Chenguang Program of Shanghai Education Development Foun-dation and
   Shanghai Municipal Education Commission, China under Grant 22CGA46.
CR Amerini I, 2011, IEEE T INF FOREN SEC, V6, P1099, DOI 10.1109/TIFS.2011.2129512
   [Anonymous], 2003, P DIG FOR RES WORKSH
   Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Bappy JH, 2019, IEEE T IMAGE PROCESS, V28, P3286, DOI 10.1109/TIP.2019.2895466
   Bappy JH, 2017, IEEE I CONF COMP VIS, P4980, DOI 10.1109/ICCV.2017.532
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Camacho IC, 2021, J IMAGING, V7, DOI 10.3390/jimaging7040069
   Chen XR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14165, DOI 10.1109/ICCV48922.2021.01392
   Chowdhary K., 2020, Fundamentals of artificial intelligence, V37, P603
   Deng XD, 2021, NEUROCOMPUTING, V457, P74, DOI 10.1016/j.neucom.2021.06.041
   Fridrich J., 2003, DIG FOR RES WORKSH, V3, P652
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hirschberg J, 2015, SCIENCE, V349, P261, DOI 10.1126/science.aaa8685
   Huang N, 2018, IEEE TRUST, P1702, DOI 10.1109/TrustCom/BigDataSE.2018.00255
   Huh M, 2018, LECT NOTES COMPUT SC, V11215, P106, DOI 10.1007/978-3-030-01252-6_7
   Jing Dong, 2013, 2013 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP), P422, DOI 10.1109/ChinaSIP.2013.6625374
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolov T, 2011, INT CONF ACOUST SPEE, P5528
   Ng A., 2018, Sequence Models
   Pan XY, 2010, IEEE T INF FOREN SEC, V5, P857, DOI 10.1109/TIFS.2010.2078506
   Piva A., 2013, An Overview on Image Forensics, V2013
   Popescu AC, 2004, LECT NOTES COMPUT SC, V3200, P128
   Popescu AC, 2005, IEEE T SIGNAL PROCES, V53, P758, DOI 10.1109/TSP.2004.839932
   Rao Y, 2016, IEEE INT WORKS INFOR
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Verdoliva L, 2020, IEEE J-STSP, V14, P910, DOI 10.1109/JSTSP.2020.3002101
   Vinyals O, 2016, Arxiv, DOI [arXiv:1511.06391, DOI 10.48550/ARXIV.1511.06391]
   Warbhe AD, 2016, PROCEDIA COMPUT SCI, V78, P464, DOI 10.1016/j.procs.2016.02.089
   Wen BH, 2016, IEEE IMAGE PROC, P161, DOI 10.1109/ICIP.2016.7532339
   Wu Y, 2019, PROC CVPR IEEE, P9535, DOI 10.1109/CVPR.2019.00977
   Yang ZL, 2019, IEEE T INF FOREN SEC, V14, P1280, DOI 10.1109/TIFS.2018.2871746
   Yao GL, 2021, NEUROCOMPUTING, V456, P168, DOI 10.1016/j.neucom.2021.05.086
   Zhou P, 2020, AAAI CONF ARTIF INTE, V34, P13058
   Zhou P, 2018, PROC CVPR IEEE, P1053, DOI 10.1109/CVPR.2018.00116
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 40
TC 0
Z9 0
U1 4
U2 4
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104017
DI 10.1016/j.jvcir.2023.104017
EA JAN 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JE4O4
UT WOS:001171476600001
DA 2024-08-05
ER

PT J
AU Zhang, BH
   Wu, DY
   Lu, XQ
   Li, YX
   Gu, Y
   Li, JJ
   Wang, JY
AF Zhang, Baohua
   Wu, Dongyang
   Lu, Xiaoqi
   Li, Yongxiang
   Gu, Yu
   Li, Jianjun
   Wang, Jingyu
TI A domain generalized person re-identification algorithm based on<i>
   meta</i>-bond domain alignment
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Person re -identification; Domain generalization; Meta -learning; Domain
   alignment
ID REFINEMENT; LABEL
AB Domain Generalization (DG) model is an important tool to improve the robustness of person re-identification algorithm, but the domain gap makes it difficult to transfer knowledge cross-domain effectively. To solve the above problems, this paper proposes a generalization model based on a Meta-Bond Domain Alignment (M-BDA) model. To learn a generalizable model, a meta-learning strategy is introduced to simulate the training-testing process of the domain generalization. Then a bond-domain module is constructed in the training to align the source domain, which can reduce the domain gap between the source domain and the target domain, and facilitate the knowledge transfer. Finally, the bond-domain loss is counted on the feature space of the bonddomain to prevent the generated bond-domain from overfitting to the source domain. The experimental results show that the proposed algorithm achieves better performance in DukeMTMC-ReID, Market1501 and MSMT17 alternating as the source and the target domain tasks. In the generalization experiments of Market1501 -> DukeMTMC-ReID, mAP and Rank-1 increased by 7.7 % and 3.6 %, respectively, and in the generalization experiments of DukeMTMC-ReID -> Market-1501, mAP and Rank-1 are increased by 9.3 % and 3.5 %, respectively, which are significantly better than the newer representative algorithms.
C1 [Zhang, Baohua; Wu, Dongyang; Gu, Yu; Li, Jianjun; Wang, Jingyu] Inner Mongolia Univ Sci & Technol, Sch Informat Engn, Baotou 014010, Inner Mongolia, Peoples R China.
   [Lu, Xiaoqi] Mongolia Ind Univ, Sch Informat Engn, Hohhot 010051, Inner Mongolia, Peoples R China.
   [Li, Yongxiang; Li, Jianjun] Inner Mongolia Agr Univ, Coll Energy & Transportat Engn, Hohhot 010051, Inner Mongolia, Peoples R China.
C3 Inner Mongolia University of Science & Technology; Inner Mongolia
   Agricultural University
RP Zhang, BH (corresponding author), Inner Mongolia Univ Sci & Technol, Sch Informat Engn, Baotou 014010, Inner Mongolia, Peoples R China.
EM zbh_wj2004@imust.cn
RI lu, xiaoqi/G-3472-2013; https://www.bcm.edu/, Yongxiang/HKV-6138-2023
FU National Natural Science Foundation of China [62262048, 61962046,
   62001255, 62066036, 61841204]; Inner Mongolia Science and Technology
   Plan Project [2020GG0315, 2021GG0082]; Inner Mongolia Natural Science
   Foundation [2022MS06017, 2019MS06003, 2018MS06018]; Central Government
   Guides Local Science and Technology Development Fund Project of China
   [2021ZY0004]; Inner Mongolia College Science and Technology Research
   Project [NJZY145]; Chunhui Program of the Ministry Education [1383]
FX The authors thank the anonymous reviewers and editors for the very
   constructive comments. This work was supported by the National Natural
   Science Foundation of China (62262048, 61962046, 62001255, 62066036,
   61841204) ; Inner Mongolia Science and Technology Plan Project
   (2020GG0315, 2021GG0082) ; Inner Mongolia Natural Science Foundation
   (2022MS06017, 2019MS06003, 2018MS06018) ; The Central Government Guides
   Local Science and Technology Development Fund Project of China (grant
   number: 2021ZY0004) ; Inner Mongolia College Science and Technology
   Research Project (grant numbers: NJZY145) ; Chunhui Program of the
   Ministry Education (1383) .
CR Balaji Y, 2018, ADV NEUR IN, V31
   Bu G.G., 2023, Computer Systems and Applications, V32, P202
   Bui MH, 2021, ADV NEUR IN, V34
   Chattopadhyay Prithvijit, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P301, DOI 10.1007/978-3-030-58545-7_18
   Chen H, 2022, arXiv
   Chen KY, 2022, NEUROCOMPUTING, V467, P418, DOI 10.1016/j.neucom.2021.09.046
   Choi S, 2021, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR46437.2021.00343
   Chong YW, 2021, APPL INTELL, V51, P5219, DOI 10.1007/s10489-020-02107-2
   Dai YX, 2021, IEEE T IMAGE PROCESS, V30, P7815, DOI 10.1109/TIP.2021.3104169
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Fang Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P526, DOI 10.1007/978-3-030-58621-8_31
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ge YX, 2020, Arxiv, DOI [arXiv:2001.01526, 10.48550/arXiv.2001.01526]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He T, 2022, AAAI CONF ARTIF INTE, P879
   Hou HP, 2022, Arxiv, DOI arXiv:2201.03803
   Hu Rong, 2023, P AAAI C ART INT, P6012
   Jia ZQ, 2023, VISUAL COMPUT, V39, P1205, DOI 10.1007/s00371-022-02398-1
   Jiao B., 2022, arXiv
   Jin X, 2020, Arxiv, DOI arXiv:2006.12009
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Kumar D, 2020, IEEE WINT CONF APPL, P2634, DOI 10.1109/WACV45572.2020.9093606
   Lan X, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108514
   Li YY, 2021, J IMAGING, V7, DOI 10.3390/jimaging7040062
   [梁文琦 Liang Wenqi], 2022, [自动化学报, Acta Automatica Sinica], V48, P103
   Lin S, 2021, IEEE T IMAGE PROCESS, V30, P1596, DOI 10.1109/TIP.2020.3046864
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Liu WM, 2022, PROCEEDINGS OF THE ACM WEB CONFERENCE 2022 (WWW'22), P1181, DOI 10.1145/3485447.3512166
   Qi L, 2023, PATTERN RECOGN, V140, DOI 10.1016/j.patcog.2023.109546
   Qi L, 2023, Arxiv, DOI arXiv:2111.15077
   Qi L, 2019, IEEE I CONF COMP VIS, P8079, DOI 10.1109/ICCV.2019.00817
   Shao ZF, 2021, J VIS COMMUN IMAGE R, V80, DOI 10.1016/j.jvcir.2021.103302
   Shengcai Liao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P456, DOI 10.1007/978-3-030-58621-8_27
   Shu Y, 2021, PROC CVPR IEEE, P9619, DOI 10.1109/CVPR46437.2021.00950
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Tang CR, 2022, J VIS COMMUN IMAGE R, V89, DOI 10.1016/j.jvcir.2022.103674
   Trung N. N., 2022, P 29 INT C COMP LING, P4741
   Wang GQ, 2020, PROC CVPR IEEE, P6677, DOI 10.1109/CVPR42600.2020.00671
   Wang M., 2023, IEEE Trans. Network Sci. Eng.
   Wang WH, 2022, IEEE T IMAGE PROCESS, V31, P1532, DOI 10.1109/TIP.2022.3140614
   Wei GQ, 2021, PROC CVPR IEEE, P16638, DOI 10.1109/CVPR46437.2021.01637
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu D, 2019, NEUROCOMPUTING, V337, P354, DOI 10.1016/j.neucom.2019.01.079
   Xiang SC, 2023, MACH LEARN, V112, P1923, DOI 10.1007/s10994-022-06184-x
   Xu Q, 2022, AAAI CONF ARTIF INTE, P2884
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522
   Yang K, 2023, IEEE T MULTIMEDIA, V25, P7545, DOI 10.1109/TMM.2022.3223213
   Yang Zou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P87, DOI 10.1007/978-3-030-58536-5_6
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Zhang H, 2021, IEEE T IMAGE PROCESS, V30, P5287, DOI 10.1109/TIP.2021.3082298
   Zhang MY, 2021, AAAI CONF ARTIF INTE, V35, P3360
   Zhang TY, 2021, PROC CVPR IEEE, P11501, DOI 10.1109/CVPR46437.2021.01134
   Zhang XX, 2022, PROC CVPR IEEE, P4900, DOI 10.1109/CVPR52688.2022.00486
   Zhang Z, 2022, IEEE T CIRC SYST VID, V32, P1160, DOI 10.1109/TCSVT.2021.3074745
   Zhao Y., 2023, IEEE Transactions on Pattern Analysis and Machine Intelligence
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
   Zhou KY, 2021, Arxiv, DOI arXiv:2104.02008
   Zhou KY, 2023, IEEE T PATTERN ANAL, V45, P4396, DOI 10.1109/TPAMI.2022.3195549
   Zhou SR, 2021, NEURAL COMPUT APPL, V33, P4001, DOI 10.1007/s00521-020-05566-3
   Zhu J., 2023, IEEE Trans. Geosci. Remote Sens.
NR 66
TC 0
Z9 0
U1 4
U2 4
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104054
DI 10.1016/j.jvcir.2024.104054
EA JAN 2024
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IA3H7
UT WOS:001163555300001
DA 2024-08-05
ER

PT J
AU Jia, HZ
   Zhou, HB
   Qin, HZ
   Wang, TH
AF Jia, Huizhen
   Zhou, Huaibo
   Qin, Hongzheng
   Wang, Tonghan
TI Dual-stream mutually adaptive quality assessment for authentic
   distortion image
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Image quality assessment; No reference; Authentic distortions;
   Unsupervised learning
AB To address semantic misperception caused by distortion and accurately simulate human perceptual processes, this study proposed a method that utilised dual -stream mutually adaptive feature mapping. To extract complementary features, a dual -stream unsupervised method was used during the training phase. One stream was responsible for the extraction of low-level and global features, whereas the other was dedicated to extracting high-level semantic and positional features. Following the freezing of feature extraction network weights, we proposed a structure that used standard deviation labels to predict the quality distribution. The experimental results obtained from 10 published image quality databases demonstrated the superiority of the proposed algorithm. The algorithm outperformed the majority of mainstream methods when evaluated on authentic distortion databases and exhibited competitive performance on synthetic distortion databases.
C1 [Wang, Tonghan] East China Univ Technol, Sch Informat Engn, Nanchang, Peoples R China.
   East China Univ Technol, Jiangxi Engn Lab Radioact Geosci & Big Data Techno, Nanchang, Peoples R China.
C3 East China University of Technology; East China University of Technology
RP Wang, TH (corresponding author), East China Univ Technol, Sch Informat Engn, Nanchang, Peoples R China.
EM thwang@ecut.edu.cn
FU National Natural Science Foundation of China [62266001, 62261001]
FX This work was supported in part by the National Natural Science
   Foundation of China under grants 62266001 and 62261001.
CR Agnolucci Lorenzo, 2024, P IEEE CVF WINT C AP, P189
   Caron Mathilde, 2020, Advances in Neural Information Processing Systems, V33, P9912, DOI DOI 10.5555/3495724.3496555
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen Y, 2020, IEEE ACCESS, V8, P85760, DOI 10.1109/ACCESS.2020.2992746
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   [方玉明 Fang Yuming], 2021, [中国图象图形学报, Journal of Image and Graphics], V26, P265
   Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Jean-Bastien Grill, 2020, NeurIPS
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kim J, 2017, IEEE J-STSP, V11, P206, DOI 10.1109/JSTSP.2016.2639328
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lin HH, 2019, INT WORK QUAL MULTIM
   Lin KY, 2018, PROC CVPR IEEE, P732, DOI 10.1109/CVPR.2018.00083
   Liu JZ, 2023, IEEE T MULTIMEDIA, V25, P5358, DOI 10.1109/TMM.2022.3190700
   Liu TJ, 2018, IEEE T IMAGE PROCESS, V27, P1138, DOI 10.1109/TIP.2017.2771422
   Liu Z, 2022, PROC CVPR IEEE, P11999, DOI 10.1109/CVPR52688.2022.01170
   Madhusudana PC, 2022, IEEE T IMAGE PROCESS, V31, P4149, DOI 10.1109/TIP.2022.3181496
   Messai O, 2022, IEEE IMAGE PROC, P2721, DOI 10.1109/ICIP46576.2022.9897616
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Ponomarenko Nikolay, 2013, 2013 4th European Workshop on Visual Information Processing (EUVIP), P106
   Razavi A, 2019, ADV NEUR IN, V32
   Rohaly AM, 2000, P SOC PHOTO-OPT INS, V4067, P742, DOI 10.1117/12.386632
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Siahaan E, 2018, SIGNAL PROCESS-IMAGE, V60, P237, DOI 10.1016/j.image.2017.10.009
   Su SL, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.109047
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Sun SM, 2023, IEEE T MULTIMEDIA, V25, P2912, DOI 10.1109/TMM.2022.3152942
   Varga D, 2018, IEEE INT CON MULTI
   Wang H, 2021, DISPLAYS, V69, DOI 10.1016/j.displa.2021.102058
   Wang JH, 2015, IEEE T IMAGE PROCESS, V24, P3400, DOI 10.1109/TIP.2015.2446942
   Wang Jiheng, 2014, 2014 IEEE INT C MULT, P1
   Wang P., 2022, PMLR, P23318
   Wei XK, 2022, INT J INTELL SYST, V37, P8730, DOI 10.1002/int.22965
   Ying ZQ, 2020, PROC CVPR IEEE, P3572, DOI 10.1109/CVPR42600.2020.00363
   Yinpeng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11027, DOI 10.1109/CVPR42600.2020.01104
   Zeng H, 2018, IEEE IMAGE PROC, P609, DOI 10.1109/ICIP.2018.8451285
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhou W, 2020, INFORM SCIENCES, V528, P205, DOI 10.1016/j.ins.2020.04.030
   Zhou W, 2019, IEEE T IMAGE PROCESS, V28, P3946, DOI 10.1109/TIP.2019.2902831
NR 41
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD JUN
PY 2024
VL 102
AR 104216
DI 10.1016/j.jvcir.2024.104216
EA JUN 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XO5N5
UT WOS:001262639300001
DA 2024-08-05
ER

PT J
AU Lin, X
   Chen, JL
   Ai, SJ
   Liu, J
   Li, BC
   Li, QY
   Ma, R
AF Lin, Xin
   Chen, Junli
   Ai, Shaojie
   Liu, Jing
   Li, Bochao
   Li, Qingying
   Ma, Rui
TI MSTG: Multi-Scale Transformer with Gradient for joint spatio-temporal
   enhancement
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Video Bit-Depth Enhancement; Video super-resolution; Deep learning;
   Transformer
AB Despite the widespread availability of HDR (High Dynamic Range) display devices, the majority of video sources are still stored in SDR (Standard Dynamic Range) format, leading to an urgent need for UHD (Ultra High Definition) video reconstruction. A simple solution is to divide the task into two sub-tasks: video superresolution (SR) and video Bit-Depth Enhancement (BDE). While some joint enhancement tasks led by SR have been explored in multiple dimensions, the quantitative dimension has often been overlooked. In this paper, we address the first joint task of BDE and SR and propose a succinct network called MSTG. We conduct a systematic analysis to explain how this effect can be achieved and overcome the core challenge arising from distortions exacerbated by the differing optimization directions of BDE and SR. This challenge is attributed to indistinguishable contours and detailed textures. To tackle this, we employ a dual-branch module that leverages both gradient and image information to discern between BDE and SR. Furthermore, we propose a novel model named Cross-scale Transformer (CTF) embedded with Cross-scale Attention (CSA). This model facilitates information aggregation while adaptively utilizing structural similarity features. It jointly extracts, aligns, and fuses frame features at multiple scales, forming the MST module. Experiments demonstrate that the proposed algorithm outperforms the cascaded video enhancement methods by large margins on both synthetic and realistic datasets.
C1 [Lin, Xin; Chen, Junli; Ai, Shaojie; Li, Bochao] Shanghai Inst Satellite Engn, Shanghai, Peoples R China.
   [Liu, Jing; Li, Qingying; Ma, Rui] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
C3 Tianjin University
RP Chen, JL (corresponding author), Shanghai Inst Satellite Engn, Shanghai, Peoples R China.
FU Shanghai Rising Star Project [23QA1408800]
FX This work is supported by Shanghai Rising Star Project under Grant
   23QA1408800.
CR Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI DOI 10.48550/ARXIV.2005.14165
   Chan K.C., 2022, P IEEE CVF C COMP VI, P5972
   Chan KCK, 2021, PROC CVPR IEEE, P4945, DOI 10.1109/CVPR46437.2021.00491
   Chang SN, 2023, Arxiv, DOI [arXiv:2303.08685, 10.48550/ARXIV.2303.086858]
   Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041
   Choi Y.J., 2021, Group-based bi-directional recurrent wavelet neural networks for video super-resolution
   Dai ZG, 2021, PROC CVPR IEEE, P1601, DOI 10.1109/CVPR46437.2021.00165
   Devlin J., 2019, NAACL-HLT
   Fuoli D, 2019, IEEE INT CONF COMP V, P3476, DOI 10.1109/ICCVW.2019.00431
   Haris M, 2020, PROC CVPR IEEE, P2856, DOI 10.1109/CVPR42600.2020.00293
   Isobe T, 2022, PROC CVPR IEEE, P17390, DOI 10.1109/CVPR52688.2022.01689
   Jing YC, 2023, PROC CVPR IEEE, P24345, DOI 10.1109/CVPR52729.2023.02332
   Jing YC, 2021, PROC CVPR IEEE, P15704, DOI 10.1109/CVPR46437.2021.01545
   Khan Z, 2022, INT C PATT RECOG, P1959, DOI 10.1109/ICPR56361.2022.9956659
   Kim SY, 2019, IEEE I CONF COMP VIS, P3116, DOI 10.1109/ICCV.2019.00321
   Kingma D. P., 2014, arXiv
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Liu J., 2017, INT FORUM DIGITAL TV, P255
   Liu J., 2021, IEEE Trans. Multimed.
   Liu J, 2019, IEEE T MULTIMEDIA, V21, P2397, DOI 10.1109/TMM.2019.2897909
   Liu J, 2019, IEEE T IMAGE PROCESS, V28, P4926, DOI 10.1109/TIP.2019.2912294
   Liu J, 2018, IEEE T IMAGE PROCESS, V27, P4860, DOI 10.1109/TIP.2018.2803306
   Liu S., 2022, NeurIPS, V35, P1100
   Liu SH, 2023, PROC CVPR IEEE, P3759, DOI 10.1109/CVPR52729.2023.00366
   Liu Z, 2022, PROC CVPR IEEE, P3192, DOI 10.1109/CVPR52688.2022.00320
   Liu Z, 2022, PROC CVPR IEEE, P11999, DOI 10.1109/CVPR52688.2022.01170
   Mikolov T, 2011, INT CONF ACOUST SPEE, P5528
   Mittal G, 2012, 2012 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Pan Z., 2021, Less is more: Pay less attention in vision transformers
   Punnappurath A, 2022, IEEE T PATTERN ANAL, V44, P9718, DOI 10.1109/TPAMI.2021.3125692
   Ranade P, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9534192
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Sajjadi MSM, 2018, PROC CVPR IEEE, P6626, DOI 10.1109/CVPR.2018.00693
   Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756
   Sun W, 2020, NEUROCOMPUTING, V406, P24, DOI 10.1016/j.neucom.2020.03.068
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Ulichney R.A., Bit-Depth Increase by bit replication
   Vaswani A., 2017, Advances in neural information processing systems, P5998
   Wang Xintao, 2019, CVPR WORKSH
   Yang X., 2022, Adv. Neural Inf. Process. Syst., V35, P25739
   Yang XY, 2023, PROC CVPR IEEE, P22552, DOI 10.1109/CVPR52729.2023.02160
   Yang XY, 2022, LECT NOTES COMPUT SC, V13694, P73, DOI 10.1007/978-3-031-19830-4_5
   Yi P, 2019, IEEE I CONF COMP VIS, P3106, DOI 10.1109/ICCV.2019.00320
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zhang YZ, 2021, PROC INT C TOOLS ART, P753, DOI 10.1109/ICTAI52525.2021.00119
   Zhu F., 2020, Revisiting temporal modeling for video super-resolution
   Zhu L, 2023, PROC CVPR IEEE, P10323, DOI 10.1109/CVPR52729.2023.00995
   Zhu XB, 2019, AAAI CONF ARTIF INTE, P5981, DOI 10.1609/aaai.v33i01.33015981
NR 50
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD JUN
PY 2024
VL 102
AR 104209
DI 10.1016/j.jvcir.2024.104209
EA JUN 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YD5E2
UT WOS:001266553600001
DA 2024-08-05
ER

PT J
AU Qiao, SH
   Chen, R
AF Qiao, Sihai
   Chen, Rong
TI Progressive feature fusion for SNR-aware low-light image enhancement
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Low light enhancement; SNR; Progressive feature fusion; High frequency
   information
AB Restoring image quality in low-light environments is an intriguing topic. While deep learning models have made significant strides in low-light enhancement, most models do not take into account the inherent characteristics of objects themselves. In this paper, we use the characteristics of the image itself to construct a Signal-to-Noise Ratio (SNR) map that guides the signal space variation to dynamically stretch the pixel values. Specifically, we propose a novel signal-to-noise ratio image-guided enhancement framework that uses the feature information of the original image to guide spatial variations in the image. It involves step-wise guidance for image feature fusion, gradually emphasizing high-frequency feature information within the image. Meanwhile, we introduced a texture optimization module that utilizes the feature information extracted by the feature fusion module to address the issues of overexposure and detail loss. We performed qualitative and quantitative evaluations on synthetic and real low-light image datasets to demonstrate the performance of our method. The experimental results show that our model outperforms other state-of-the-art methods (SOTA) in robust low-light enhancement, especially in processing images captured in complex scenes.
C1 [Qiao, Sihai; Chen, Rong] Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian 116000, Peoples R China.
C3 Dalian Maritime University
RP Chen, R (corresponding author), Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian 116000, Peoples R China.
EM rchen@dlmu.edu.cn
OI Sihai, Qiao/0009-0009-4812-0865
FU National Natural Science Founda-tion of China [61672122, 61902050,
   61906027]; Fundamental Research Funds for the Central Universities
   [3132019355]
FX This work is supported by the National Natural Science Founda-tion of
   China (NO. 61672122, NO. 61902050, NO. 61906027) , and the Fundamental
   Research Funds for the Central Universities (NO. 3132019355) .
CR Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Chaitanya BSNV, 2021, J VIS COMMUN IMAGE R, V74, DOI 10.1016/j.jvcir.2020.103014
   Chen HC, 2018, AAAI CONF ARTIF INTE, P2127
   Chen X, 2023, PROC CVPR IEEE, P5896, DOI 10.1109/CVPR52729.2023.00571
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dhara SK, 2022, IEEE T CIRC SYST VID, V32, P3438, DOI 10.1109/TCSVT.2021.3113559
   Florea L, 2016, IEEE COMPUT SOC CONF, P936, DOI 10.1109/CVPRW.2016.121
   Fu HY, 2023, PROC CVPR IEEE, P18125, DOI 10.1109/CVPR52729.2023.01738
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Guo BY, 2020, J VIS COMMUN IMAGE R, V71, DOI 10.1016/j.jvcir.2020.102851
   Guo XJ, 2023, INT J COMPUT VISION, V131, P48, DOI 10.1007/s11263-022-01667-9
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hai J, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103712
   Huang SC, 2013, IEEE T IMAGE PROCESS, V22, P1032, DOI 10.1109/TIP.2012.2226047
   Ibrahim H, 2007, IEEE T CONSUM ELECTR, V53, P1752, DOI 10.1109/TCE.2007.4429280
   Lee C, 2012, IEEE IMAGE PROC, P965, DOI 10.1109/ICIP.2012.6467022
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P4225, DOI 10.1109/TPAMI.2021.3063604
   Li JQ, 2021, IEEE T MULTIMEDIA, V23, P3153, DOI 10.1109/TMM.2020.3021243
   Li JY, 2023, PROC CVPR IEEE, P9914, DOI 10.1109/CVPR52729.2023.00956
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Liang JX, 2022, IEEE T CIRC SYST VID, V32, P7332, DOI 10.1109/TCSVT.2022.3181781
   Liang JX, 2022, IEEE T MULTIMEDIA, V24, P1609, DOI 10.1109/TMM.2021.3068840
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lv FF, 2021, INT J COMPUT VISION, V129, P2175, DOI 10.1007/s11263-021-01466-8
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Ma L, 2023, IEEE T MULTIMEDIA, V25, P3573, DOI 10.1109/TMM.2022.3162493
   Ma L, 2022, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR52688.2022.00555
   Mehra A, 2021, J VIS COMMUN IMAGE R, V77, DOI 10.1016/j.jvcir.2021.103137
   Pang TY, 2021, PROC CVPR IEEE, P2043, DOI 10.1109/CVPR46437.2021.00208
   Park S, 2017, IEEE T CONSUM ELECTR, V63, P178, DOI 10.1109/TCE.2017.014847
   Paszke A, 2019, ADV NEUR IN, V32
   Tian CW, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109050
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Wei YC, 2018, PROC CVPR IEEE, P7268, DOI 10.1109/CVPR.2018.00759
   Xu XG, 2022, PROC CVPR IEEE, P17693, DOI 10.1109/CVPR52688.2022.01719
   Ying R, 2018, ADV NEUR IN, V31
   Ying ZQ, 2017, Arxiv, DOI arXiv:1711.00591
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Zhang DH, 2023, EXPERT SYST APPL, V231, DOI 10.1016/j.eswa.2023.120842
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zhang ZJ, 2018, 2018 IEEE/ACM 26TH INTERNATIONAL SYMPOSIUM ON QUALITY OF SERVICE (IWQOS), DOI 10.1109/IWQoS.2018.8624183
   Zhao HT, 2021, J VIS COMMUN IMAGE R, V74, DOI 10.1016/j.jvcir.2020.102921
   Zhou JC, 2023, IEEE J OCEANIC ENG, V48, P474, DOI 10.1109/JOE.2022.3223733
   Zhou JC, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3293912
   Zhou JC, 2023, ENG APPL ARTIF INTEL, V121, DOI 10.1016/j.engappai.2023.105946
   Zhu MF, 2020, AAAI CONF ARTIF INTE, V34, P13106
NR 50
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104148
DI 10.1016/j.jvcir.2024.104148
EA APR 2024
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RW3I3
UT WOS:001230653900001
DA 2024-08-05
ER

PT J
AU Chang, BC
   Li, JJ
   Ren, L
   Chen, Z
AF Chang, Baocai
   Li, Jinjiang
   Ren, Lu
   Chen, Zheng
TI Dual branch Transformer-CNN parametric filtering network for underwater
   image enhancement
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Underwater image enhancement; Color correction; Filter; Attention
   mechanism
ID WATER
AB Due to the absorption and scattering of light on the water surface, underwater images often face challenges such as low contrast, color deviation, insufficient exposure, and blurred details, exacerbating the difficulty of underwater tasks. In recent years, underwater image enhancement has become increasingly crucial in marine applications. Among existing underwater enhancement methods, the focus has often been on pixel-level learning, which may lead to image noise and an inability to finely adjust the image. In this paper, we propose a dual-branch Transformer-CNN Parameter Filtering network for underwater image enhancement, referred to as DTCPF. Specifically, to better aggregate window information, we introduce an overlapping window selfattention module to enhance interaction between windows. Additionally, we employ an improved Transformer encoder and decoder, utilizing long-distance attention and reversible neural networks to extract low-frequency and high-frequency information from the image. Moreover, we introduce a regression parameter filtering group for regression prediction, using the predicted parameters to enhance the image and obtain a reliable underwater enhancement model. Our approach undergoes qualitative and quantitative analyses on four real underwater datasets, demonstrating outstanding performance.
C1 [Chang, Baocai] Shandong Technol & Business Univ, Sch Informat & Elect Engn, Yantai 264005, Peoples R China.
   [Li, Jinjiang; Chen, Zheng] Shandong Technol & Business Univ, Sch Comp Sci & Technol, Yantai 264005, Peoples R China.
   [Ren, Lu] Inst Network Technol INT, Yantai 264005, Peoples R China.
C3 Shandong Technology & Business University; Shandong Technology &
   Business University
RP Chen, Z (corresponding author), Shandong Technol & Business Univ, Sch Comp Sci & Technol, Yantai 264005, Peoples R China.
EM chenzheng@sdtbu.edu.cn
OI Chang, Baocai/0009-0003-5101-5309
FU National Natural Science Foun-dation of China [61772319, 62002200,
   62202268, 62272281]; Shan-dong Natural Science Foundation of China
   [ZR2021MF107, ZR2022MA 076]; Yantai science and technology innovation
   development plan, China [2022JCYJ031]
FX <B>Acknowledgments</B> This research was supported by the National
   Natural Science Foun-dation of China (61772319, 62002200, 62202268,
   62272281) , Shan-dong Natural Science Foundation of China (ZR2021MF107,
   ZR2022MA 076) , Yantai science and technology innovation development
   plan, China (2022JCYJ031) .
CR Akkaynak D, 2019, PROC CVPR IEEE, P1682, DOI 10.1109/CVPR.2019.00178
   Bhalla K, 2022, J VIS COMMUN IMAGE R, V84, DOI 10.1016/j.jvcir.2022.103485
   Chiang JY, 2012, IEEE T IMAGE PROCESS, V21, P1756, DOI 10.1109/TIP.2011.2179666
   Cong RM, 2022, IEEE T CONSUM ELECTR, V68, P376, DOI 10.1109/TCE.2022.3205376
   Cong RM, 2023, IEEE T CYBERNETICS, V53, P1920, DOI 10.1109/TCYB.2022.3169431
   Dinh L, 2017, Arxiv, DOI [arXiv:1605.08803, DOI 10.48550/ARXIV.1605.08803]
   Drews PLJ, 2016, IEEE COMPUT GRAPH, V36, P24, DOI 10.1109/MCG.2016.26
   Fu ZQ, 2022, LECT NOTES COMPUT SC, V13678, P465, DOI 10.1007/978-3-031-19797-0_27
   Galdran A, 2015, J VIS COMMUN IMAGE R, V26, P132, DOI 10.1016/j.jvcir.2014.11.006
   Gao SB, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2919947
   Ghani ASA, 2015, APPL SOFT COMPUT, V37, P332, DOI 10.1016/j.asoc.2015.08.033
   Ghani ASA, 2015, APPL SOFT COMPUT, V27, P219, DOI 10.1016/j.asoc.2014.11.020
   Guo YC, 2020, IEEE J OCEANIC ENG, V45, P862, DOI 10.1109/JOE.2019.2911447
   Hambarde P, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3120130
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hou GJ, 2024, IEEE T CIRC SYST VID, V34, P799, DOI 10.1109/TCSVT.2023.3290363
   Hou GJ, 2020, J VIS COMMUN IMAGE R, V66, DOI 10.1016/j.jvcir.2019.102732
   Iqbal K, 2010, IEEE INT C SYSTEMS M
   Islam MJ, 2020, IEEE ROBOT AUTOM LET, V5, P3227, DOI 10.1109/LRA.2020.2974710
   Islam MJ, 2020, Arxiv, DOI arXiv:2002.01155
   Jiang QP, 2022, IEEE T CIRC SYST VID, V32, P5959, DOI 10.1109/TCSVT.2022.3164918
   Kang YZ, 2023, IEEE T CIRC SYST VID, V33, P988, DOI 10.1109/TCSVT.2022.3208100
   Kumawat HC, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3188050
   Lei XY, 2022, APPL OPTICS, V61, P5304, DOI 10.1364/AO.456368
   Li CY, 2021, IEEE T IMAGE PROCESS, V30, P4985, DOI 10.1109/TIP.2021.3076367
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107038
   Li CY, 2018, IEEE SIGNAL PROC LET, V25, P323, DOI 10.1109/LSP.2018.2792050
   Li H., 2019, arXiv
   Li J, 2018, IEEE ROBOT AUTOM LET, V3, P387, DOI 10.1109/LRA.2017.2730363
   Li XJ, 2022, ENG APPL ARTIF INTEL, V111, DOI 10.1016/j.engappai.2022.104759
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Liu RS, 2020, IEEE T CIRC SYST VID, V30, P4861, DOI 10.1109/TCSVT.2019.2963772
   Liu SL, 2023, IEEE SENS J, V23, P13797, DOI 10.1109/JSEN.2023.3269085
   Liu SB, 2022, IEEE ROBOT AUTOM LET, V7, P5326, DOI 10.1109/LRA.2022.3156176
   Lu HM, 2016, J VIS COMMUN IMAGE R, V38, P504, DOI 10.1016/j.jvcir.2016.03.029
   Lu JY, 2019, OPT LASER TECHNOL, V110, P105, DOI 10.1016/j.optlastec.2018.05.048
   Maaz M., 2022, EUR C COMP VIS, P3, DOI DOI 10.1007/978-3-031-25082-8_1
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Moran Sean, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12823, DOI 10.1109/CVPR42600.2020.01284
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Peng LT, 2023, IEEE T IMAGE PROCESS, V32, P3066, DOI 10.1109/TIP.2023.3276332
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Qi Q, 2022, Arxiv, DOI arXiv:2201.02832
   Razakarivony S, 2016, J VIS COMMUN IMAGE R, V34, P187, DOI 10.1016/j.jvcir.2015.11.002
   Shen Z., 2022, UDAformer: Underwater image enhancement based on dual attention transformer
   Tu B, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3176286
   Uplavikar Pritish M., 2019, CVPR WORKSH, P1
   Wang D, 2022, 2022 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA 2022), P4592, DOI 10.1109/ICRA46639.2022.9812457
   Wang Y, 2017, IEEE IMAGE PROC, P1382, DOI 10.1109/ICIP.2017.8296508
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu ZH, 2020, Arxiv, DOI arXiv:2004.11886
   Xie J, 2022, IEEE T CIRC SYST VID, V32, P3514, DOI 10.1109/TCSVT.2021.3115791
   Yang M, 2020, SIGNAL PROCESS-IMAGE, V81, DOI 10.1016/j.image.2019.115723
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Yin M, 2023, IEEE SENS J, V23, P7728, DOI 10.1109/JSEN.2023.3251326
   Yuan JY, 2021, IEEE T GEOSCI REMOTE, V59, P8117, DOI 10.1109/TGRS.2020.3033407
   Yue GH, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3264047
   Yue GH, 2022, IEEE J BIOMED HEALTH, V26, P4090, DOI 10.1109/JBHI.2022.3173948
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang WD, 2022, IEEE T IMAGE PROCESS, V31, P3997, DOI 10.1109/TIP.2022.3177129
   Zhang WD, 2022, IEEE J OCEANIC ENG, V47, P718, DOI 10.1109/JOE.2022.3140563
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zhou JC, 2023, INT J COMPUT VISION, DOI 10.1007/s11263-023-01853-3
   Zhou JC, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3293912
   Zhou JC, 2021, OPT EXPRESS, V29, P28228, DOI 10.1364/OE.432900
   Zhuang PX, 2022, IEEE T IMAGE PROCESS, V31, P5442, DOI 10.1109/TIP.2022.3196546
NR 67
TC 0
Z9 0
U1 7
U2 7
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104131
DI 10.1016/j.jvcir.2024.104131
EA APR 2024
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RA8P7
UT WOS:001225046500001
DA 2024-08-05
ER

PT J
AU Zhai, J
   Cheng, ZN
   Zhang, WK
   Zhu, DJ
   Yang, WK
AF Zhai, Jiang
   Cheng, Zinan
   Zhang, Wenkang
   Zhu, Dejun
   Yang, Wankou
TI Efficient object tracking on edge devices with MobileTrack
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Visual tracking; Fast and efficient; Siamese network
AB Object tracking has made significant progress in recent years. However, the state-of-the-art trackers are becoming increasingly heavy and expensive, making their deployment challenging in resource -constrained applications. In this study, we introduce MobileTrack, a visual object tracker that strikes a perfect balance between tracking accuracy and inference speed. Utilizing a novel coordinated perception -aware fusion module and a lightweight prediction head, our proposed methodology outperforms most Siamese trackers on various academic benchmarks in terms of both accuracy and efficiency. When deployed on resource -constrained embedded devices such as NVIDIA Jetson TX2, MobileTrack ensures real-time performance at a speed exceeding 33FPS, while LightTrack only operates at 18FPS. Therefore, MobileTrack holds significant potential to unlock a wide range of practical applications across various industries. MobileTrack is released at here.
C1 [Zhai, Jiang; Zhang, Wenkang; Yang, Wankou] Southeast Univ, Sch Automat, Nanjing 210096, Peoples R China.
   [Cheng, Zinan] Jiangsu Automat Res Inst, Lianyungang 222061, Peoples R China.
   [Zhu, Dejun] Tsinghua Univ, Dept Hydraul Engn, State Key Lab Hydrosci & Engn, Beijing 100084, Peoples R China.
C3 Southeast University - China; Tsinghua University
RP Yang, WK (corresponding author), Southeast Univ, Sch Automat, Nanjing 210096, Peoples R China.; Zhu, DJ (corresponding author), Tsinghua Univ, Dept Hydraul Engn, State Key Lab Hydrosci & Engn, Beijing 100084, Peoples R China.
EM zhudejun@tsinghua.edu.cn; wkyang@seu.edu.cn
RI Zhai, Jiang/KFR-0746-2024
OI Zhai, Jiang/0009-0005-8472-3226
FU National Natural Science Founda-tion of China [62276061]
FX This work was supported by the National Natural Science Founda-tion of
   China under no. 62276061.
CR Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Blatter P, 2023, IEEE WINT CONF APPL, P1571, DOI 10.1109/WACV56688.2023.00162
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Borsuk V, 2022, LECT NOTES COMPUT SC, V13682, P644, DOI 10.1007/978-3-031-20047-2_37
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Ersanilli E., 2020, arXiv, DOI 10.31234/osf.io/g7rzq
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2017.128
   Guo DY, 2021, PROC CVPR IEEE, P9538, DOI 10.1109/CVPR46437.2021.00942
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Han K., 2019, ARXIV
   Han S, 2016, Arxiv, DOI [arXiv:1510.00149, 10.48550/arXiv.1510.00149]
   HARRIS FJ, 1978, P IEEE, V66, P51, DOI 10.1109/PROC.1978.10837
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Howard A. G., 2017, ARXIV, DOI DOI 10.48550/ARXIV.1704.04861
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Iandola S., 2017, arXiv
   Kristan M., 2020, P COMP VIS ECCV 20 5, P547, DOI 10.1007/978-3-030-68238-5_39
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li B., 2019, arXiv
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Mehta S., 2021, ARXIV
   Müller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Sandler M, 2018, ARXIV
   Shen JB, 2022, Arxiv, DOI arXiv:1907.10586
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang ZQ, 2019, IEEE I CONF COMP VIS, P3977, DOI 10.1109/ICCV.2019.00408
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xiong M, 2018, P EUR C COMP VIS ECC
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Yan B, 2021, PROC CVPR IEEE, P5285, DOI 10.1109/CVPR46437.2021.00525
   Yan B, 2021, PROC CVPR IEEE, P15175, DOI 10.1109/CVPR46437.2021.01493
   Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028
   Ye BT, 2022, LECT NOTES COMPUT SC, V13682, P341, DOI 10.1007/978-3-031-20047-2_20
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
NR 47
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104126
DI 10.1016/j.jvcir.2024.104126
EA MAR 2024
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QF2U9
UT WOS:001219406600001
DA 2024-08-05
ER

PT J
AU Rajevenceltha, J
   Gaidhane, VH
AF Rajevenceltha, J.
   Gaidhane, Vilas H.
TI A no-reference image quality assessment model based on neighborhood
   component analysis and Gaussian process
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE No-reference; Perceptual features; Structural information; Neighborhood
   component analysis; Gaussian process; Regression
ID NATURAL SCENE STATISTICS
AB In this paper, a human visual system (HVS) based no-reference image quality assessment model (NR-IQA) is proposed. This is an objective NR-IQA model that uses perceptual features such as structure and orientation to compute the loss of naturalness in the image. An adaptive feature extraction process is modeled to capture the underlying significant features of the images along with the noise. Moreover, the optimal subset of features is derived using the neighborhood component analysis and further used as inputs to the Gaussian process for regression. The various experimentations are carried out on LIVE, TID2008, and TID2013 databases to test the effectiveness of the proposed approach. It is observed that the presented model exhibits a competitive performance in comparison with the existing IQA approaches. Moreover, the computation complexity and run-time show the effectuality of the proposed approach. Further, the experiments show that the predicted score matches with human perceptions. Thus, the proposed model is more accurate, less complex, independent of distortions, and well-suited for real-time applications.
C1 [Rajevenceltha, J.] Birla Inst Technol & Sci, Dept Elect & Elect Engn, Dubai Campus, Pilani 345055, U Arab Emirates.
   [Gaidhane, Vilas H.] Birla Inst Technol & Sci, APPCAIR, Dubai Campus, Pilani 345055, U Arab Emirates.
   [Gaidhane, Vilas H.] Birla Inst Technol & Sci, Dept Elect & Elect Engn, Dubai Campus, Dubai 345055, U Arab Emirates.
   [Gaidhane, Vilas H.] Birla Inst Technol & Sci, APPCAIR, Dubai Campus, Dubai 345055, U Arab Emirates.
RP Gaidhane, VH (corresponding author), Birla Inst Technol & Sci, Dept Elect & Elect Engn, Dubai Campus, Dubai 345055, U Arab Emirates.; Gaidhane, VH (corresponding author), Birla Inst Technol & Sci, APPCAIR, Dubai Campus, Dubai 345055, U Arab Emirates.
EM vilasgd612@gmail.com
CR Abdalmajeed S, 2014, ELECTRON LETT, V50, P595, DOI 10.1049/el.2013.3585
   Bahrami K, 2014, IEEE SIGNAL PROC LET, V21, P751, DOI 10.1109/LSP.2014.2314487
   Balas B, 2019, PERCEPTION, V48, P58, DOI 10.1177/0301006618812581
   Bianco S, 2018, SIGNAL IMAGE VIDEO P, V12, P355, DOI 10.1007/s11760-017-1166-8
   Chow LS, 2016, BIOMED SIGNAL PROCES, V27, P145, DOI 10.1016/j.bspc.2016.02.006
   Deshpande AM, 2021, J INDIAN SOC REMOTE, V49, P2903, DOI 10.1007/s12524-021-01429-z
   Gaidhane V.H., 2022, IETE Tech Rev, V11, P1
   Gaidhane VH, 2015, SIGNAL IMAGE VIDEO P, V9, P203, DOI 10.1007/s11760-015-0775-3
   Gao F, 2017, NEUROCOMPUTING, V257, P104, DOI 10.1016/j.neucom.2017.01.054
   Goldberger J., 2004, Advances in Neural Information Processing Systems, V17
   Golestaneh SA, 2022, IEEE WINT CONF APPL, P3989, DOI 10.1109/WACV51458.2022.00404
   Han DM, 2018, EXPERT SYST APPL, V95, P43, DOI 10.1016/j.eswa.2017.11.028
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Khelifi L, 2020, IEEE ACCESS, V8, P126385, DOI 10.1109/ACCESS.2020.3008036
   Kuo TY, 2016, J VIS COMMUN IMAGE R, V40, P76, DOI 10.1016/j.jvcir.2016.06.010
   Li CF, 2011, IEEE T NEURAL NETWOR, V22, P793, DOI 10.1109/TNN.2011.2120620
   Li QH, 2017, NEUROCOMPUTING, V236, P93, DOI 10.1016/j.neucom.2016.09.105
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu LX, 2016, SIGNAL PROCESS-IMAGE, V40, P1, DOI 10.1016/j.image.2015.10.005
   Liu YT, 2018, IEEE T MULTIMEDIA, V20, P379, DOI 10.1109/TMM.2017.2729020
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Malpica W., 2009, 4 INT WORKSHOP VIDEO
   Mantiuk RK, 2012, COMPUT GRAPH FORUM, V31, P2478, DOI 10.1111/j.1467-8659.2012.03188.x
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Rajevenceltha J, 2022, ENG SCI TECHNOL, V30, DOI 10.1016/j.jestch.2021.07.002
   Rajevenceltha J, 2021, SIGNAL IMAGE VIDEO P, V15, P547, DOI 10.1007/s11760-020-01775-4
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Schulz E, 2018, J MATH PSYCHOL, V85, P1, DOI 10.1016/j.jmp.2018.03.001
   Sheikh Hamid R, 2005, LIVE IMAGE QUALITY A, DOI DOI 10.1109/CVPR.2015.7298594
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P1918, DOI 10.1109/TIP.2005.854492
   Srivastava D, 2020, NEURAL COMPUT APPL, V32, P10819, DOI 10.1007/s00521-018-3611-1
   Srivastava D, 2019, MULTIMED TOOLS APPL, V78, P14129, DOI 10.1007/s11042-018-6793-8
   Testolina M., 2021, SPIE Appl. Digital Image Process XLIV, V302-315
   Wang Z, 2002, IEEE IMAGE PROC, P477
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Yang SD, 2022, IEEE COMPUT SOC CONF, P1190, DOI 10.1109/CVPRW56347.2022.00126
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Ye P, 2012, IEEE T IMAGE PROCESS, V21, P3129, DOI 10.1109/TIP.2012.2190086
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
NR 44
TC 0
Z9 0
U1 6
U2 6
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104041
DI 10.1016/j.jvcir.2023.104041
EA JAN 2024
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC3C6
UT WOS:001164075100001
DA 2024-08-05
ER

PT J
AU Chen, BY
   Su, JN
   Chen, GY
   Gan, M
AF Chen, Bing-Yuan
   Su, Jian-Nan
   Chen, Guang-Yong
   Gan, Min
TI FISTA acceleration inspired network design for underwater image
   enhancement
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Underwater image enhancement; FISTA algorithm; Proximal gradient; Deep
   learning
ID ALGORITHM
AB Underwater image enhancement, especially in color restoration and detail reconstruction, remains a significant challenge. Current models focus on improving accuracy and learning efficiency through neural network design, often neglecting traditional optimization algorithms' benefits. We propose FAIN-UIE, a novel approach for color and fine-texture recovery in underwater imagery. It leverages insights from the Fast Iterative Shrink-Threshold Algorithm (FISTA) to approximate image degradation, enhancing network fitting speed. FAIN-UIE integrates the residual degradation module (RDM) and momentum calculation module (MC) for gradient descent and momentum simulation, addressing feature fusion losses with the Feature Merge Block (FMB). By integrating multi-scale information and inter-stage pathways, our method effectively maps multi-stage image features, advancing color and fine-texture restoration. Experimental results validate its robust performance, positioning FAIN-UIE as a competitive solution for practical underwater imaging applications.
C1 [Chen, Bing-Yuan; Su, Jian-Nan; Chen, Guang-Yong; Gan, Min] Fuzhou Univ, Coll Comp & Data Sci, Coll Software, Fuzhou 350000, Fujian, Peoples R China.
C3 Fuzhou University
RP Chen, GY (corresponding author), Fuzhou Univ, Coll Comp & Data Sci, Coll Software, Fuzhou 350000, Fujian, Peoples R China.
EM gychen@fzu.edu.cn
FU National Natural Sci-ence Foundation of China [62173091]; National
   Natural Science Foundation of Fujian Province [2023J01268]; Special
   Funds for Promoting High-quality Development of Marine and Fishery
   Industries in Fujian Provice [FJHYF-ZH-2023-02]
FX This work was supported in part by the National Natural Sci-ence
   Foundation of China under Grant 62173091; in part by the National
   Natural Science Foundation of Fujian Province under Grant 2023J01268; in
   part by the Special Funds for Promoting High-quality Development of
   Marine and Fishery Industries in Fujian Provice under Grant
   FJHYF-ZH-2023-02.
CR Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252
   Ancuti C, 2012, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2012.6247661
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Berman D, 2021, IEEE T PATTERN ANAL, V43, P2822, DOI 10.1109/TPAMI.2020.2977624
   Cai XW, 2024, IEEE J OCEANIC ENG, V49, P226, DOI 10.1109/JOE.2023.3245760
   Chen Guang-Yong, 2024, IEEE Transactions on Circuits and Systems for Video Technology, V34, P4762, DOI 10.1109/TCSVT.2023.3331883
   Chen L, 2021, IEEE T CIRC SYST VID, V31, P3078, DOI 10.1109/TCSVT.2020.3035108
   Dong LL, 2022, SIGNAL PROCESS-IMAGE, V104, DOI 10.1016/j.image.2022.116684
   Drews PLJ, 2016, IEEE COMPUT GRAPH, V36, P24, DOI 10.1109/MCG.2016.26
   Fan GD, 2022, IEEE T CIRC SYST VID, V32, P7403, DOI 10.1109/TCSVT.2022.3186880
   Fan GD, 2024, IEEE T NEUR NET LEAR, V35, P1598, DOI 10.1109/TNNLS.2022.3184164
   Fu XY, 2020, SIGNAL PROCESS-IMAGE, V86, DOI 10.1016/j.image.2020.115892
   Fu XY, 2020, IEEE T NEUR NET LEAR, V31, P1794, DOI 10.1109/TNNLS.2019.2926481
   Fu XY, 2014, IEEE IMAGE PROC, P4572, DOI 10.1109/ICIP.2014.7025927
   Fu ZQ, 2022, LECT NOTES COMPUT SC, V13678, P465, DOI 10.1007/978-3-031-19797-0_27
   Fu ZQ, 2022, INT CONF ACOUST SPEE, P2764, DOI 10.1109/ICASSP43922.2022.9747758
   Galdran A, 2015, J VIS COMMUN IMAGE R, V26, P132, DOI 10.1016/j.jvcir.2014.11.006
   Ge XR, 2024, IEEE T DEPEND SECURE, V21, P1286, DOI 10.1109/TDSC.2023.3276360
   Guo YC, 2020, IEEE J OCEANIC ENG, V45, P862, DOI 10.1109/JOE.2019.2911447
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hitam Muhammad Suzuri, 2013, 2013 INT C COMP APPL, P1
   Hou GJ, 2024, IEEE T CIRC SYST VID, V34, P799, DOI 10.1109/TCSVT.2023.3290363
   Hou GJ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3578584
   Huang SR, 2023, PROC CVPR IEEE, P18145, DOI 10.1109/CVPR52729.2023.01740
   Islam MJ, 2020, IEEE ROBOT AUTOM LET, V5, P3227, DOI 10.1109/LRA.2020.2974710
   Jiang JX, 2023, Arxiv, DOI arXiv:2305.08824
   Jiang NF, 2022, IEEE T MULTIMEDIA, V24, DOI 10.1109/TMM.2021.3115442
   Jiang QP, 2024, IEEE T MULTIMEDIA, V26, P4884, DOI 10.1109/TMM.2023.3327613
   Jiang ZY, 2022, IEEE T CIRC SYST VID, V32, P6584, DOI 10.1109/TCSVT.2022.3174817
   Li CY, 2016, IEEE T IMAGE PROCESS, V26, P5664, DOI 10.1109/TIP.2016.2612882
   Li CY, 2023, Arxiv, DOI arXiv:2302.11831
   Li CY, 2021, IEEE T IMAGE PROCESS, V30, P4985, DOI 10.1109/TIP.2021.3076367
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P4225, DOI 10.1109/TPAMI.2021.3063604
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107038
   Li H., 2019, arXiv
   Li JR, 2019, OPT LASER TECHNOL, V110, P129, DOI 10.1016/j.optlastec.2018.05.034
   Li J, 2018, IEEE ROBOT AUTOM LET, V3, P387, DOI 10.1109/LRA.2017.2730363
   Liu RS, 2022, IEEE T IMAGE PROCESS, V31, P4922, DOI 10.1109/TIP.2022.3190209
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Liu RS, 2020, IEEE T CIRC SYST VID, V30, P4861, DOI 10.1109/TCSVT.2019.2963772
   Ma L, 2023, INT J COMPUT VISION, DOI 10.1007/s11263-023-01900-z
   Ma L, 2022, IEEE T NEUR NET LEAR, V33, P5666, DOI 10.1109/TNNLS.2021.3071245
   Monga V, 2021, IEEE SIGNAL PROC MAG, V38, P18, DOI 10.1109/MSP.2020.3016905
   Panetta K, 2022, IEEE J OCEANIC ENG, V47, P59, DOI 10.1109/JOE.2021.3086907
   Peng LT, 2023, IEEE T IMAGE PROCESS, V32, P3066, DOI 10.1109/TIP.2023.3276332
   Peng YT, 2018, IEEE T IMAGE PROCESS, V27, P2856, DOI 10.1109/TIP.2018.2813092
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Sanila KH, 2019, OCEAN ELECTR, P106, DOI [10.1109/sympol48207.2019.9005301, 10.1109/SYMPOL48207.2019.9005301]
   Song W, 2020, IEEE T BROADCAST, V66, P153, DOI 10.1109/TBC.2019.2960942
   Tan CS, 2005, OPT LASER ENG, V43, P995, DOI 10.1016/j.optlaseng.2004.10.005
   Tao Y, 2021, OPT EXPRESS, V29, P32412, DOI 10.1364/OE.432756
   Wang N, 2021, Arxiv, DOI arXiv:1912.10269
   Wang Y, 2017, IEEE IMAGE PROC, P1382, DOI 10.1109/ICIP.2017.8296508
   Wang YD, 2021, SIGNAL PROCESS-IMAGE, V96, DOI 10.1016/j.image.2021.116250
   Xu CP, 2024, Arxiv, DOI arXiv:2404.08965
   Xu CP, 2023, IEEE T MULTIMEDIA, V25, P4199, DOI 10.1109/TMM.2022.3172547
   Xu CP, 2023, IEEE T MULTIMEDIA, V25, P4052, DOI 10.1109/TMM.2022.3171085
   Xue XW, 2021, IEEE SIGNAL PROC LET, V28, P818, DOI 10.1109/LSP.2021.3072563
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Yuan JY, 2021, IEEE T GEOSCI REMOTE, V59, P8117, DOI 10.1109/TGRS.2020.3033407
   Zhang WC, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3248106
   Zhang WD, 2022, IEEE T IMAGE PROCESS, V31, P3997, DOI 10.1109/TIP.2022.3177129
   Zhang WD, 2024, IEEE T CIRC SYST VID, V34, P2469, DOI 10.1109/TCSVT.2023.3299314
   Zhang WD, 2022, COMPUT ELECTRON AGR, V192, DOI 10.1016/j.compag.2021.106585
   Zhang WD, 2021, COMPUT ELECTR ENG, V91, DOI 10.1016/j.compeleceng.2021.106981
   Zhao YZ, 2022, OPT LASER ENG, V148, DOI 10.1016/j.optlaseng.2021.106777
   Zheng YN, 2024, IEEE T GEOSCI REMOTE, V62, P18, DOI 10.1109/TGRS.2024.3374389
   Zheng YN, 2022, IEEE T IMAGE PROCESS, V31, P5456, DOI 10.1109/TIP.2022.3196815
   Zhou JC, 2024, INT J COMPUT VISION, DOI 10.1007/s11263-024-01987-y
   Zhou JC, 2023, IEEE J OCEANIC ENG, V48, P1322, DOI 10.1109/JOE.2023.3275615
   Zhou JC, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3293912
   Zhou JC, 2023, ENG APPL ARTIF INTEL, V121, DOI 10.1016/j.engappai.2023.105946
   Zhuang PX, 2022, IEEE T IMAGE PROCESS, V31, P5442, DOI 10.1109/TIP.2022.3196546
NR 74
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD AUG
PY 2024
VL 103
AR 104224
DI 10.1016/j.jvcir.2024.104224
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZK5J6
UT WOS:001275202800001
DA 2024-08-05
ER

PT J
AU Baisa, NL
AF Baisa, Nathanael L.
TI Local-aware global attention network for person re-identification based
   on body and hand images
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Person re-identification; Deep representation learning; Attention
   mechanisms; Global features; Part-level features
AB Learning representative, robust and discriminative information from images is essential for effective person re-identification (Re-Id). In this paper, we propose a compound approach for end-to-end discriminative deep feature learning for person Re-Id based on both body and hand images. We carefully design the Local-Aware Global Attention Network (LAGA-Net), a multi-branch deep network architecture consisting of one branch for spatial attention, one branch for channel attention, one branch for global feature representations and another branch for local feature representations. The attention branches focus on the relevant features of the image while suppressing the irrelevant backgrounds. The global and local branches intends to capture global context and fine-grained information, respectively. A set of ablation study shows that each component contributes to the increased performance of the LAGA-Net. Extensive evaluations on four popular body-based person Re-Id benchmarks and two publicly available hand datasets demonstrate that our proposed method consistently outperforms existing state-of-the-art methods.
C1 [Baisa, Nathanael L.] De Montfort Univ, Sch Comp Sci & Informat, Leicester LE1 9BH, England.
C3 De Montfort University
RP Baisa, NL (corresponding author), De Montfort Univ, Sch Comp Sci & Informat, Leicester LE1 9BH, England.
EM nathanael.baisa@dmu.ac.uk
CR Afifi M, 2019, MULTIMED TOOLS APPL, V78, P20835, DOI 10.1007/s11042-019-7424-8
   Attia A, 2021, SIGNAL IMAGE VIDEO P, V15, P851, DOI 10.1007/s11760-020-01806-0
   Baisa NL, 2022, INT CONF IMAG PROC, DOI 10.1109/IPTA54936.2022.9784133
   Baisa NL, 2022, INT C PATT RECOG, P727, DOI 10.1109/ICPR56361.2022.9956555
   Baisa NL, 2021, J VIS COMMUN IMAGE R, V80, DOI 10.1016/j.jvcir.2021.103279
   Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Chen XD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11793, DOI 10.1109/ICCV48922.2021.01160
   Dantcheva A, 2016, IEEE T INF FOREN SEC, V11, P441, DOI 10.1109/TIFS.2015.2480381
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jain Anil K., 2022, IEEE Transactions on Biometrics, Behavior, and Identity Science, V4, P303, DOI 10.1109/TBIOM.2021.3115465
   Khan S., 2021, arXiv
   Kumar A, 2016, IEEE T INF FOREN SEC, V11, P2338, DOI 10.1109/TIFS.2016.2574309
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Ristani E, 2018, PROC CVPR IEEE, P6036, DOI 10.1109/CVPR.2018.00632
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Shaw P., 2018, Self-attention with relative position representations, P464
   Shen ZR, 2020, Arxiv, DOI arXiv:2010.03019
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Xuan SY, 2021, PROC CVPR IEEE, P11921, DOI 10.1109/CVPR46437.2021.01175
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yuan Y., 2020, ICVISP 2020, DOI [10.1145/3448823.3448838, DOI 10.1145/3448823.3448838]
   Zhang X, 2021, PROC CVPR IEEE, P3435, DOI 10.1109/CVPR46437.2021.00344
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
NR 39
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD AUG
PY 2024
VL 103
AR 104207
DI 10.1016/j.jvcir.2024.104207
EA JUL 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU5V8
UT WOS:001264212900001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wang, JY
   Xiang, XZ
   Ding, S
   El Saddik, A
AF Wang, Jiye
   Xiang, Xuezhi
   Ding, Shuai
   El Saddik, Abdulmotaleb
TI 3D hand pose estimation and reconstruction based on multi-feature fusion
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Hand pose estimation; Shape reconstruction; Multi-feature fusion;
   Multi-scale; Multi-root loss
AB 3D hand pose estimation and shape reconstruction is to recover the hand joint points and hand mesh vertices coordinates from the image. However, existing methods usually only use the high-level semantic features extracted by the backbone network to represent the hand mesh vertex features, which leads to a single representation of the hand vertices features and cannot fully utilize the feature information extracted by the network. In this paper, we propose a method for real-time 3D reconstruction of hands from a single RGB image, which enriches the 3D semantic information of the mesh vertices through multi -feature fusion. Firstly, we regress the 2D features of mesh vertices through Integral Pose Regression (IPR) and regard them as prior information to 3D features. Then we design a Multi -Scale Sampling(MSS) module to extract multi -scale information. Finally we fuse 2D prior features, multi -scale features, and high-level semantic features extracted by backbone to represent 3D initial feature. Additionally, we propose a Multi-Root(MR) loss function to address the imbalance problem caused by a single root joint. The experimental results indicate that our network achieves competitive performance on the FreiHAND and HO -3D public datasets, achieving fast inference speed with fewer parameters.
C1 [Wang, Jiye; Xiang, Xuezhi; Ding, Shuai] Harbin Engn Univ, Sch Informat & Commun Engn, Harbin 150001, Peoples R China.
   [Xiang, Xuezhi] Key Lab Adv Marine Commun & Informat Technol, Harbin 150001, Peoples R China.
   [El Saddik, Abdulmotaleb] Univ Ottawa, Sch Elect Engn & Comp Sci, Ottawa, ON K1N 6N5, Canada.
C3 Harbin Engineering University; University of Ottawa
RP Xiang, XZ (corresponding author), Harbin Engn Univ, Sch Informat & Commun Engn, Harbin 150001, Peoples R China.
EM xiangxuezhi@hrbeu.edu.cn
FU National Natural Science Foundation of China [62271160]; Natural Science
   Foundation of Heilongjiang Province of China [LH2021F011]; Fundamental
   Research Funds for the Central Universities of China [3072022TS0801];
   Chinese Association for Artificial Intelligence (CAAI) -Huawei Mind
   Spore Open Fund; Key Laboratory of Advanced Marine Communication and
   Information Technology Open Fund [AMCIT2103-03]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62271160, in part by the Natural Science
   Foundation of Heilongjiang Province of China under Grant LH2021F011, in
   part by the Fundamental Research Funds for the Central Universities of
   China under Grant 3072022TS0801, in part by the Chinese Association for
   Artificial Intelligence (CAAI) -Huawei Mind Spore Open Fund, and in part
   by the Key Laboratory of Advanced Marine Communication and Information
   Technology Open Fund under Grant AMCIT2103-03.
CR Baek S, 2019, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2019.00116
   Boukhayma A, 2019, PROC CVPR IEEE, P10835, DOI 10.1109/CVPR.2019.01110
   Calli B, 2015, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR), P510, DOI 10.1109/ICAR.2015.7251504
   Chen P, 2021, P IEEE CVF INT C COM, P12929
   Chen X., 2021, P IEEECVF C COMPUTER, P13274
   Chen XY, 2022, PROC CVPR IEEE, P20512, DOI 10.1109/CVPR52688.2022.01989
   Chen YJ, 2021, PROC CVPR IEEE, P10446, DOI 10.1109/CVPR46437.2021.01031
   Cheng J, 2022, AAAI CONF ARTIF INTE, P419
   Cho JY, 2022, LECT NOTES COMPUT SC, V13661, P342, DOI 10.1007/978-3-031-19769-7_20
   Choi Hongsuk, 2020, COMPUTER VISION ECCV
   Ge LH, 2019, PROC CVPR IEEE, P10825, DOI 10.1109/CVPR.2019.01109
   Gyeongsik Moon, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P752, DOI 10.1007/978-3-030-58571-6_44
   Hampali S, 2020, PROC CVPR IEEE, P3193, DOI 10.1109/CVPR42600.2020.00326
   Hasson Y, 2020, PROC CVPR IEEE, P568, DOI 10.1109/CVPR42600.2020.00065
   Hasson Y, 2019, PROC CVPR IEEE, P11799, DOI 10.1109/CVPR.2019.01208
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Iqbal U, 2018, LECT NOTES COMPUT SC, V11215, P125, DOI 10.1007/978-3-030-01252-6_8
   Kanis J, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23125509
   Kulon D, 2020, PROC CVPR IEEE, P4989, DOI 10.1109/CVPR42600.2020.00504
   Lim I, 2019, LECT NOTES COMPUT SC, V11131, P349, DOI 10.1007/978-3-030-11015-4_26
   Lin K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12919, DOI 10.1109/ICCV48922.2021.01270
   Lin K, 2021, PROC CVPR IEEE, P1954, DOI 10.1109/CVPR46437.2021.00199
   Misra D, 2021, IEEE WINT CONF APPL, P3138, DOI 10.1109/WACV48630.2021.00318
   Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055
   Ren JW, 2023, Arxiv, DOI arXiv:2204.08154
   Rezaei M, 2023, EXPERT SYST APPL, V223, DOI 10.1016/j.eswa.2023.119922
   Romero Javier, 2022, arXiv
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Spurr A., 2021, arXiv
   Spurr A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11210, DOI 10.1109/ICCV48922.2021.01104
   Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33
   Tang Xiao, 2021, ICCV, P11698
   Vaswani A, 2017, ADV NEUR IN, V30
   Yang L., 2020, arXiv
   Yu X, 2021, INT CONF 3D VISION, P505, DOI 10.1109/3DV53792.2021.00060
   Zhang JL, 2022, PROC CVPR IEEE, P13222, DOI 10.1109/CVPR52688.2022.01288
   Zheng XZ, 2021, INT SYM MIX AUGMENT, P99, DOI 10.1109/ISMAR52148.2021.00024
   Zhou YX, 2020, PROC CVPR IEEE, P5345, DOI 10.1109/CVPR42600.2020.00539
   Zimmermann C, 2019, IEEE I CONF COMP VIS, P813, DOI 10.1109/ICCV.2019.00090
NR 39
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104160
DI 10.1016/j.jvcir.2024.104160
EA APR 2024
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SM4W3
UT WOS:001234866800001
DA 2024-08-05
ER

PT J
AU Xu, Z
   Fu, ZY
AF Xu, Zhi
   Fu, Zhenyong
TI Using Mixture of Experts to accelerate dataset distillation
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Dataset distillation; Mixture of experts; Accelerate
AB Recently, large datasets have become increasingly necessary for most deep learning tasks, however, large datasets may bring some problems, such as disk storage and huge computational expense. Dataset distillation is an emerging field that aims to synthesize a small dataset from the original dataset, then a random model trained on the distillation dataset can achieve comparable performances to the same architecture model trained on the original dataset. Matching Training Trajectories (MTT) achieves a leading performance in this field, but it needs to pre-train 200 expert models before the formal distillation process, which is called buffer process. In this paper, we propose a new method to reduce the consumed time of buffer process. Concretely, we use Mixture of Experts (MoE) to train several expert models parallelly in buffer process. The experiments show our method can achieve a speedup of up to approximately 4 similar to 8x in buffer process with getting comparable distillation performances.
C1 [Xu, Zhi; Fu, Zhenyong] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
C3 Nanjing University of Science & Technology
RP Fu, ZY (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
EM z.fu@njust.edu.cn
FU National Natural Science Founda-tion of China [62276132, 61876085];
   China Post-doctoral Science Foundation [2017M621748, 2019T120430]
FX <B>Funding</B> This work was supported by the National Natural Science
   Founda-tion of China (Grant No. 62276132 and 61876085) and the China
   Post-doctoral Science Foundation (Grant No. 2017M621748 and 2019T120430)
   .
CR Andrychowicz M, 2016, ADV NEUR IN, V29
   Cazenavette G, 2022, PROC CVPR IEEE, P10708, DOI 10.1109/CVPR52688.2022.01045
   Cui J., 2022, Scaling up dataset distillation to imagenet-1k with constant memory
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng ZW, 2022, Arxiv, DOI arXiv:2206.02916
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Du J., 2022, Minimizing the accumulated trajectory error to improve dataset distillation
   Eigen D, 2014, Arxiv, DOI [arXiv:1312.4314, DOI 10.48550/ARXIV.1312.4314]
   Fedus W, 2022, J MACH LEARN RES, V23
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79
   Kim J.-H., 2022, Dataset condensation via efficient synthetic-data parameterization
   Krizhevsky A., 2009, Learning multiple layers of features from tiny images
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Le Y., 2015, CS 231N, V7, P3
   Lee Saehyung, 2022, P MACHINE LEARNING R
   Lepikhin D, 2020, Arxiv, DOI [arXiv:2006.16668, DOI 10.48550/ARXIV.2006.16668]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu S., 2024, Adv. Neural Inf. Process. Syst., V36
   Liu S., 2022, NeurIPS, V35, P1100
   Nguyen Timothy, 2020, ARXIV
   Phillips J. M., 2016, ABS160100617 CORR
   Riba E, 2020, IEEE WINT CONF APPL, P3663, DOI 10.1109/WACV45572.2020.9093363
   Shao S, 2019, IEEE I CONF COMP VIS, P8429, DOI 10.1109/ICCV.2019.00852
   Shazeer Noam, 2017, arXiv, DOI DOI 10.48550/ARXIV.1701.06538
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang K, 2022, PROC CVPR IEEE, P12186, DOI 10.1109/CVPR52688.2022.01188
   Wang TZ, 2020, Arxiv, DOI arXiv:1811.10959
   Yang X., 2022, Deep model reassembly
   Yang XY, 2022, LECT NOTES COMPUT SC, V13694, P73, DOI 10.1007/978-3-031-19830-4_5
   Zhang G, 2023, Arxiv, DOI [arXiv:2304.07987, DOI 10.48550/ARXIV.2304.07987]
   Zhang L., 2022, Accelerating dataset distillation via model augmentation
   Zhao B., 2020, arXiv
   Zhao B., 2021, INT C MACHINE LEARNI, P12674
   Zhao B, 2023, IEEE WINT CONF APPL, P6503, DOI 10.1109/WACV56688.2023.00645
   Zhou Y., 2022, Advances in Neural Information Processing Systems, V35, P9813
NR 37
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104137
DI 10.1016/j.jvcir.2024.104137
EA APR 2024
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RO2E9
UT WOS:001228531300001
DA 2024-08-05
ER

PT J
AU Qi, XY
   Song, TY
   Dong, HB
   Jin, JY
   Jin, GY
   Li, PP
AF Qi, Xuanyu
   Song, Tianyu
   Dong, Haobo
   Jin, Jiyu
   Jin, Guiyue
   Li, Pengpeng
TI Dual-branch collaborative transformer for effective
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Single image deraining; Transformer; Multi-dimensional features; Feature
   fusion
ID RAIN STREAKS REMOVAL; IMAGE
AB Recently, Transformer -based architecture has been introduced into single image deraining tasks due to its advantage in modeling non -local information. However, existing approaches typically utilize self -attention along single spatial or channel dimension. They neglect feature fusion in different dimensions to explore contextual information fully, which limits the effective receptive field of the network and makes it challenging to learn image degradation relationships. To fully explore potential correlations between different dimensions of degraded images, we develop a D ual -branch C ollaborative Trans former , called DCformer. To be specific, we employ parallel multi -head self -attention (PMSA) as the core block to extract long-range contextual relationships across spatial and channel dimensions. Additionally, a local perception block (LPB) is introduced to provide the ability of local information acquisition in the network, which is complementary to the global modeling ability of the parallel hybrid self -attention mechanism. Finally, we design a feature interaction block (FIB) to further enhance the interaction of features at different resolutions. Extensive experiments on benchmark datasets demonstrate the effectiveness of our proposed method.
C1 [Qi, Xuanyu; Song, Tianyu; Dong, Haobo; Jin, Jiyu; Jin, Guiyue; Li, Pengpeng] Dalian Polytech Univ, Sch Informat Sci & Engn, Dalian 116034, Peoples R China.
C3 Dalian Polytechnic University
RP Jin, JY (corresponding author), Dalian Polytech Univ, Sch Informat Sci & Engn, Dalian 116034, Peoples R China.
EM 13993448762@163.com; songtienyu@163.com; dhb2638@163.com;
   jiyu.jin@dlpu.edu.cn; jiyu.jin@dlpu.edu.cn; pengpengli@njust.edu.cn
CR Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen LY, 2022, LECT NOTES COMPUT SC, V13667, P17, DOI 10.1007/978-3-031-20071-7_2
   Chen X, 2021, IEEE COMPUT SOC CONF, P872, DOI 10.1109/CVPRW53098.2021.00097
   Das S, 2022, IEEE COMPUT SOC CONF, P81, DOI 10.1109/CVPRW56347.2022.00018
   Fu XY, 2020, IEEE T NEUR NET LEAR, V31, P1794, DOI 10.1109/TNNLS.2019.2926481
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Guo CL, 2022, PROC CVPR IEEE, P5802, DOI 10.1109/CVPR52688.2022.00572
   He CM, 2023, IEEE I CONF COMP VIS, P12577, DOI 10.1109/ICCV51070.2023.01159
   Hu RZ, 2023, IEEE T CYBERNETICS, V53, P3651, DOI 10.1109/TCYB.2021.3128023
   Hu RZ, 2021, FRONT COMPUT NEUROSC, V15, DOI 10.3389/fncom.2021.746549
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Jiang N, 2023, IEEE T MULTIMEDIA, V25, P2226, DOI 10.1109/TMM.2022.3144890
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Kim JH, 2013, IEEE IMAGE PROC, P914, DOI 10.1109/ICIP.2013.6738189
   Kui Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8343, DOI 10.1109/CVPR42600.2020.00837
   Kulkarni A., 2022, EUR C COMP VIS SPRIN, P344
   Li JJ, 2022, IEEE T IND INFORM, V18, P163, DOI 10.1109/TII.2021.3085669
   Li P., 2021, INT C COMM NETW CHIN, P405
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Liang YC, 2022, IEEE COMPUT SOC CONF, P588, DOI 10.1109/CVPRW56347.2022.00074
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388
   Mehri A, 2021, IEEE WINT CONF APPL, P2703, DOI 10.1109/WACV48630.2021.00275
   Qin GY, 2023, Arxiv, DOI [arXiv:2304.04952, 10.1609/aaai.v37i2.25302, DOI 10.1609/AAAI.V37I2.25302]
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Seif G, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1468, DOI 10.1109/ICASSP.2018.8461664
   Song TY, 2023, IEEE INT CON MULTI, P1889, DOI 10.1109/ICME55011.2023.00324
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang H, 2024, IEEE T NEUR NET LEAR, V35, P8668, DOI 10.1109/TNNLS.2022.3231453
   Wang H, 2020, INT J MACH LEARN CYB, V11, P853, DOI 10.1007/s13042-020-01061-2
   Wang Q, 2023, IEEE INT CON MULTI, P2747, DOI 10.1109/ICME55011.2023.00467
   Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255
   Wang XL, 2017, PROC CVPR IEEE, P3039, DOI 10.1109/CVPR.2017.324
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Wei Y., 2022, IEEE Trans. Pattern Anal. Mach. Intell.
   Xie ZF, 2023, IEEE T NEUR NET LEAR, V34, P4499, DOI 10.1109/TNNLS.2021.3116209
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang H, 2017, IEEE WINT CONF APPL, P1259, DOI 10.1109/WACV.2017.145
   Zhu L, 2023, PROC CVPR IEEE, P10323, DOI 10.1109/CVPR52729.2023.00995
   Zhu L, 2017, IEEE I CONF COMP VIS, P2545, DOI 10.1109/ICCV.2017.276
NR 48
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104136
DI 10.1016/j.jvcir.2024.104136
EA APR 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RC7S9
UT WOS:001225543900001
DA 2024-08-05
ER

PT J
AU Zhan, YR
   Xiong, SH
   He, XH
   Tang, BW
   Chen, HG
AF Zhan, Yanrui
   Xiong, Shuhua
   He, Xiaohai
   Tang, Bowen
   Chen, Honggang
TI A channel-wise contextual module for learned intra video compression
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Deep learning; Intra video compression; Contextual module; Feature
   information
ID IMAGE COMPRESSION
AB In the multimedia era, exploding image and video data highlight the importance of video compression for storage and transmission. The All-Intra structure is a coding mode in HEVC and VVC, in which each frame is encoded using intra coding, and in this paper learned All-Intra coding is explored on the basis of the research of the learned image compression. A channel-wise contextual module based on channel segmentation is introduced to fully exploit non-local information. Then, two distinct attention mechanisms are designed for different feature layers to enhance the effectiveness of the transform network. Additionally, a post -processing module is employed to enhance the quality of decoded frames. Experimental results on the Kodak and Tecnick datasets demonstrate that the proposed method performs better than the majority of the recent learning-based methods and traditional image codecs (BPG, JPEG2000 and JPEG), and also perform better than traditional video codecs in terms of PSNR.
C1 [Zhan, Yanrui; Xiong, Shuhua; He, Xiaohai; Tang, Bowen; Chen, Honggang] Sichuan Univ, Coll Elect & Informat Engn, 24 South Sect,1 Yihuan Rd, Chengdu 610065, Peoples R China.
C3 Sichuan University
RP He, XH (corresponding author), Sichuan Univ, Coll Elect & Informat Engn, 24 South Sect,1 Yihuan Rd, Chengdu 610065, Peoples R China.
EM cherry_530@stu.scu.edu.cn; xiongsh@scu.edu.cn;
   cherry_530@stu.scu.edu.cn; tangbww@foxmail.com; honggang_chen@scu.edu.cn
FU National Natural Science Foun-dation of China [62271336, 62211530110];
   TCL Science and Technol-ogy Innovation Fund; Sichuan Province
   Interna-tional Science and Technology Innovation Cooperation Fund
   [24GJHZ0381]
FX <B>Acknowledgments</B> This work was supported by the National Natural
   Science Foun-dation of China (Grant No. 62271336 and Grant No.
   62211530110) . This work was supported in part by the TCL Science and
   Technol-ogy Innovation Fund, and in part by the Sichuan Province
   Interna-tional Science and Technology Innovation Cooperation Fund (Grant
   No. 24GJHZ0381) .
CR Agustsson Eirikur, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8500, DOI 10.1109/CVPR42600.2020.00853
   Ball‚ J, 2018, Arxiv, DOI arXiv:1802.01436
   Bellard F., 2015, BPG image format, V1, P1
   Chen T, 2021, IEEE T IMAGE PROCESS, V30, P3179, DOI 10.1109/TIP.2021.3058615
   Choi Y, 2019, IEEE I CONF COMP VIS, P3146, DOI 10.1109/ICCV.2019.00324
   Cui Z, 2022, Arxiv, DOI arXiv:2003.02012
   Djelouah A, 2019, IEEE I CONF COMP VIS, P6430, DOI 10.1109/ICCV.2019.00652
   Fu HS, 2023, SIGNAL PROCESS, V202, DOI 10.1016/j.sigpro.2022.108778
   Hu YY, 2022, IEEE T PATTERN ANAL, V44, P4194, DOI 10.1109/TPAMI.2021.3065339
   Hu ZH, 2021, PROC CVPR IEEE, P1502, DOI 10.1109/CVPR46437.2021.00155
   Jeny AA, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103737
   Kim JH, 2022, PROC CVPR IEEE, P5982, DOI 10.1109/CVPR52688.2022.00590
   Kingma D. P., 2014, arXiv
   Koyuncu AB, 2022, LECT NOTES COMPUT SC, V13679, P447, DOI 10.1007/978-3-031-19800-7_26
   Lee JY, 2019, Arxiv, DOI arXiv:1809.10452
   Li DW, 2022, J VIS COMMUN IMAGE R, V87, DOI 10.1016/j.jvcir.2022.103573
   Li DY, 2022, NEUROCOMPUTING, V489, P40, DOI 10.1016/j.neucom.2022.03.019
   Li M, 2023, IEEE T NEUR NET LEAR, V34, P1132, DOI 10.1109/TNNLS.2021.3104974
   Lin JP, 2020, PROC CVPR IEEE, P3543, DOI 10.1109/CVPR42600.2020.00360
   Lin RQ, 2023, NEUROCOMPUTING, V548, DOI 10.1016/j.neucom.2023.126396
   Liu Dong, 2022, IEEE Trans. Multimed
   Lu G, 2019, PROC CVPR IEEE, P10998, DOI 10.1109/CVPR.2019.01126
   Minnen D, 2020, IEEE IMAGE PROC, P3339, DOI [10.1109/icip40778.2020.9190935, 10.1109/ICIP40778.2020.9190935]
   Minnen D, 2018, ADV NEUR IN, V31
   Mishra D, 2022, NEUROCOMPUTING, V507, P397, DOI 10.1016/j.neucom.2022.08.009
   Ohm Jens-Rainer, 2018, PICTURE CODING S
   Rabbani M, 2002, SIGNAL PROCESS-IMAGE, V17, P3, DOI 10.1016/S0923-5965(01)00024-8
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu CY, 2018, LECT NOTES COMPUT SC, V11212, P425, DOI 10.1007/978-3-030-01237-3_26
   Xie F, 2023, J VIS COMMUN IMAGE R, V95, DOI 10.1016/j.jvcir.2023.103889
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Zhang YH, 2022, J VIS COMMUN IMAGE R, V87, DOI 10.1016/j.jvcir.2022.103555
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhang ZF, 2019, PROC CVPR IEEE, P7974, DOI 10.1109/CVPR.2019.00817
   Zhengxue Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7936, DOI 10.1109/CVPR42600.2020.00796
   Zhihao Hu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P193, DOI 10.1007/978-3-030-58536-5_12
   Zhou J, 2019, Arxiv, DOI arXiv:1910.07844
NR 40
TC 0
Z9 0
U1 5
U2 5
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104070
DI 10.1016/j.jvcir.2024.104070
EA FEB 2024
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JX3A4
UT WOS:001176408000001
DA 2024-08-05
ER

PT J
AU Chen, QQ
   Xing, YX
   Song, LL
AF Chen, Qiuqiu
   Xing, Yuanxiu
   Song, Linlin
TI Neighbor2Global: Self-supervised image denoising for Poisson-Gaussian
   noise
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Image denoising; Self-supervised training; GAT; Image fusion
AB Sub-sampled pairs generated from noisy images solve the lack of noisy-clean pairs in denoising model training. However, real sub-sampled image pairs with signal-dependent Poisson-Gaussian noise and the approximation of neighboring pixels lead to denoising performance degradation. For this reason, a novel self-supervised Neighbor2Global is proposed to train an efficient denoising model for real-world images denoising. Firstly, the problems faced by model training are analyzed theoretically, and a GAT-based image generation strategy is introduced to make the sub-sampled pair from a single noisy image approximately independent. Secondly, a complementary training strategy is presented to train the denoising network on sub-sampled GAT image pairs, GAT images and PD-transformed GAT images, with improved reconstruction loss and optimized regulation for better performance. Extensive experiments have been conducted on both synthetic and real-world datasets to validate the superior performance of the proposed method in balancing detailed texture and global information, as well as weakening over-smoothing.
C1 [Xing, Yuanxiu] Wuhan Univ Sci & Technol, Coll Sci, Wuhan 430065, Hubei, Peoples R China.
   Wuhan Univ Sci & Technol, Hubei Prov Key Lab Syst Sci Met Proc, Wuhan 430081, Hubei, Peoples R China.
C3 Wuhan University of Science & Technology; Wuhan University of Science &
   Technology
RP Xing, YX (corresponding author), Wuhan Univ Sci & Technol, Coll Sci, Wuhan 430065, Hubei, Peoples R China.
EM xingyuanxiu@wust.edu.cn
CR Abdelhamed A, 2018, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2018.00182
   Abdelhamed Abdelrahman, About us, P6
   ANSCOMBE FJ, 1948, BIOMETRIKA, V35, P246, DOI 10.1093/biomet/35.3-4.246
   Batson J, 2019, PR MACH LEARN RES, V97
   Byun J, 2021, PROC CVPR IEEE, P5764, DOI 10.1109/CVPR46437.2021.00571
   Byun J, 2020, IEEE SIGNAL PROC LET, V27, P1105, DOI 10.1109/LSP.2020.3002652
   Chen GY, 2015, IEEE I CONF COMP VIS, P477, DOI 10.1109/ICCV.2015.62
   Chen JW, 2018, PROC CVPR IEEE, P3155, DOI 10.1109/CVPR.2018.00333
   Cheng S, 2021, PROC CVPR IEEE, P4894, DOI 10.1109/CVPR46437.2021.00486
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Foi A, 2008, IEEE T IMAGE PROCESS, V17, P1737, DOI 10.1109/TIP.2008.2001399
   Franzen R., "Kodak lossless true color image suite
   Hasinoff S.W., 2014, Photon, poisson noise, DOI DOI 10.1007/978-0-387-31439-6_482
   Huang T, 2021, PROC CVPR IEEE, P14776, DOI 10.1109/CVPR46437.2021.01454
   Huang YQ, 2019, OPT EXPRESS, V27, P12289, DOI 10.1364/OE.27.012289
   Kingma D., 2015, P INT C LEARN REPR S, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Krull A, 2019, PROC CVPR IEEE, P2124, DOI 10.1109/CVPR.2019.00223
   Laine S, 2019, ADV NEUR IN, V32
   Lee W, 2022, PROC CVPR IEEE, P17704, DOI 10.1109/CVPR52688.2022.01720
   Lehtinen J, 2018, PR MACH LEARN RES, V80
   Liu YK, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15020364
   Mäkitalo M, 2013, IEEE T IMAGE PROCESS, V22, P91, DOI 10.1109/TIP.2012.2202675
   Mao JG, 2023, INT J COMPUT VISION, V131, P1909, DOI 10.1007/s11263-023-01790-1
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mei HY, 2023, INT J COMPUT VISION, V131, P3019, DOI 10.1007/s11263-023-01838-2
   Moran Nick, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12061, DOI 10.1109/CVPR42600.2020.01208
   Nagpal A., 2022, L1 L2 REGULARIZATION
   Pan HD, 2023, IEEE J-STARS, V16, P3045, DOI 10.1109/JSTARS.2023.3257051
   Pang TY, 2021, PROC CVPR IEEE, P2043, DOI 10.1109/CVPR46437.2021.00208
   Paszke A, 2019, ADV NEUR IN, V32
   Quan YH, 2020, PROC CVPR IEEE, P1887, DOI 10.1109/CVPR42600.2020.00196
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Wang ZJ, 2023, Arxiv, DOI arXiv:2303.05183
   Wang ZJ, 2022, PROC CVPR IEEE, P2017, DOI 10.1109/CVPR52688.2022.00207
   Xiaohe Wu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P352, DOI 10.1007/978-3-030-58548-8_21
   Xu J, 2020, IEEE T IMAGE PROCESS, V29, P9316, DOI 10.1109/TIP.2020.3026622
   Zeyde R., 2012, INT C CURV SURF, P711
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhou YQ, 2020, AAAI CONF ARTIF INTE, V34, P13074
   Zou YH, 2023, IEEE I CONF COMP VIS, P13219, DOI 10.1109/ICCV51070.2023.01220
NR 41
TC 1
Z9 1
U1 10
U2 10
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104049
DI 10.1016/j.jvcir.2024.104049
EA JAN 2024
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD4J4
UT WOS:001157539600001
DA 2024-08-05
ER

PT J
AU Ma, YH
AF Ma, Yihan
TI Design and optimization of an aerobics movement recognition system based
   on high-dimensional biotechnological data using neural networks
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE High-dimensional biotechnological data; Aerobics movement recognition;
   Convolutional neural networks; Long Short-Term Memory networks; Deep
   learning
AB This study presents the design and optimization of an aerobics movement recognition system utilizing highdimensional biotechnological data. Deep learning techniques are employed to achieve accurate classification and recognition of movement actions. Biosensing technology and wearable devices are used to collect real-time, multidimensional physiological signal data from key anatomical regions of athletes. The system is constructed using convolutional neural networks (CNNs) and Long Short-Term Memory. Model performance is optimized through parameter selection and strategies such as Xavier initialization, the cross-entropy loss function, and the Adam optimizer. The results indicate that Model C achieves an accuracy of 0.987, significantly outperforming the standalone CNNs (accuracy 0.975) and the recurrent neural network models (accuracy 0.965). Furthermore, it demonstrates notable efficiency in practical applications, with considerably reduced execution times of 10 s for data processing, 25 s for feature extraction, and 20 s for classification. This aerobics recognition system excels in performance and efficiency, supporting precise movement classification.
C1 [Ma, Yihan] Shanghai Polytech Univ, Sports Dept, Shang Hai 201209, Peoples R China.
RP Ma, YH (corresponding author), Shanghai Polytech Univ, Sports Dept, Shang Hai 201209, Peoples R China.
EM yhma@sspu.edu.cn
CR Abayomi-Alli OO, 2021, EXPERT SYST, V38, DOI 10.1111/exsy.12746
   Alvarez-Bueno C, 2020, J SPORT SCI, V38, P582, DOI 10.1080/02640414.2020.1720496
   Birara M, 2022, MULTIMED TOOLS APPL, V81, P24377, DOI 10.1007/s11042-022-12399-w
   Cao TT, 2023, WIREL NETW, V29, P1611, DOI 10.1007/s11276-022-03123-5
   Chen DL, 2021, SUSTAIN CITIES SOC, V66, DOI 10.1016/j.scs.2020.102655
   Chen WY, 2023, PREV MED, V174, DOI 10.1016/j.ypmed.2023.107642
   Chien HY, 2021, ADV MECH ENG, V13, DOI 10.1177/16878140211026082
   Degardin B, 2023, IMAGE VISION COMPUT, V137, DOI 10.1016/j.imavis.2023.104750
   Hacker S, 2020, J CLIN MED, V9, DOI 10.3390/jcm9051380
   Hisham Basma, 2021, International Journal of Information Technology, V13, P1221, DOI 10.1007/s41870-020-00518-5
   Hussain A, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22072489
   Jain R, 2022, VISUAL COMPUT, V38, P1957, DOI 10.1007/s00371-021-02259-3
   Kang MS, 2021, IEEE ACCESS, V9, P76270, DOI 10.1109/ACCESS.2021.3083273
   Kumar R, 2023, WIRELESS PERS COMMUN, V130, P1141, DOI 10.1007/s11277-023-10324-4
   Li LP, 2022, REV BRAS MED ESPORTE, V28, P792, DOI 10.1590/1517-8692202228062022_0089
   Liu YZ, 2022, IEEE T IMAGE PROCESS, V31, P4104, DOI 10.1109/TIP.2022.3180585
   MacKay-Lyons M, 2020, PHYS THER, V100, P149, DOI 10.1093/ptj/pzz153
   Mahmoud R, 2022, NEURAL COMPUT APPL, V34, P13713, DOI 10.1007/s00521-022-07165-w
   Seals DR, 2019, J PHYSIOL-LONDON, V597, P4901, DOI 10.1113/JP277764
   Singh T, 2021, NEURAL COMPUT APPL, V33, P469, DOI 10.1007/s00521-020-05018-y
   Siyal S, 2021, RATION SOC, V33, P401, DOI 10.1177/10434631211033660
   Sun LS, 2021, J SENSORS, V2021, DOI 10.1155/2021/2793474
   Wang S., 2023, Int. J. Inf. Commun. Technol., V22, P281
   Yan CG, 2021, IEEE T PATTERN ANAL, V43, P1445, DOI 10.1109/TPAMI.2020.2975798
   [颜成钢 Yan Chenggang], 2022, [信号处理, Journal of Signal Processing], V38, P1111
   Yan CG, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472810
   Yan CG, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3468872
   Yan CG, 2022, IEEE T CIRC SYST VID, V32, P43, DOI 10.1109/TCSVT.2021.3067449
   Yan CG, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3404374
   Yan G, 2022, MOBILE NETW APPL, V27, P1252, DOI 10.1007/s11036-022-01939-1
   Yoo M, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22072513
   Yue S, 2021, MICROPROCESS MICROSY, V82, DOI 10.1016/j.micpro.2021.103925
   Zhang ZH, 2022, NAT NANOTECHNOL, V17, P27, DOI 10.1038/s41565-021-01003-1
   Zhao H., 2021, Sci. Program, V2021, P1
   Zhou YH, 2021, WIREL COMMUN MOB COM, V2021, DOI 10.1155/2021/9208891
   Zhu DD, 2021, SCI PROGRAMMING-NETH, V2021, DOI 10.1155/2021/5526971
   Zuo N., 2023, Int. J. Inf. Commun. Technol., V22, P362
NR 37
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD AUG
PY 2024
VL 103
AR 104227
DI 10.1016/j.jvcir.2024.104227
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZK4W7
UT WOS:001275189900001
DA 2024-08-05
ER

PT J
AU Chlubna, T
   Milet, T
   Zemcik, P
AF Chlubna, Tomas
   Milet, Tomas
   Zemcik, Pavel
TI Efficient random-access GPU video decoding for light-field rendering
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Light field; Plane sweep; Image-based; Rendering; GPU; Focusing
ID IMAGE COMPRESSION
AB Compression method for GPU streaming of discrete light fields is proposed in this paper. Views on the scene are encoded with video codec to enable streaming in real time. Instead of using a classic scheme, all frames are encoded according to one reference frame. Any frame is decoded directly, in a random-access manner that is suitable for light -field rendering methods, where only few frames are necessary on the GPU. The proposed scheme reaches the best decoding quality/time ratio in comparison to other schemes, where all preceding frames need to be decompressed, and all -key -frame video that supports random access, but is extremely large. The proposed method solves the space -requirements and streaming -bandwidth issues using the GPU accelerated decoding, and enables incorporating light -field assets in real-time 3D simulations. Compared to existing methods, the proposal is easy to implement, does not depend on specific video format or extension and is efficient on consumer GPUs.
C1 [Chlubna, Tomas; Milet, Tomas; Zemcik, Pavel] Brno Univ Technol, Fac Informat Technol, Bozetechova 1-2, Brno 61200, Czech Republic.
   [Zemcik, Pavel] Lappeenranta Lahti Univ Technol, Sch Engn Sci, Yliopistonkatu 34, Lappeenranta 53850, Finland.
C3 Brno University of Technology; Lappeenranta-Lahti University of
   Technology LUT
RP Chlubna, T (corresponding author), Brno Univ Technol, Fac Informat Technol, Bozetechova 1-2, Brno 61200, Czech Republic.
EM ichlubna@fit.vutbr.cz
RI Chlubna, Tomáš/Y-7496-2018
OI Chlubna, Tomáš/0000-0003-3126-0545
FU KDT JU project AIDOaRt [101007350]
FX <B>Acknowledgments</B> This work was supported by the KDT JU project
   AIDOaRt, grant agreement No 101007350. The authors would like to thank
   the anony-mous reviewers who helped to improve the quality of the
   manuscript.
CR Adelson E. H., 1991, PLENOPTIC FUNCTION E, V2
   Ahmad W, 2019, IEEE ACCESS, V7, P143002, DOI 10.1109/ACCESS.2019.2944765
   Alain M, 2019, Arxiv, DOI arXiv:1910.04699
   Avramelos V, 2020, MULTIMED TOOLS APPL, V79, P12847, DOI 10.1007/s11042-019-08605-x
   Bakier N, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102880
   Barina D., 2019, COMPUTER SCI RES N 1, V2901, P55, DOI [10.24132/CSRN.2019.2901.1.7, DOI 10.24132/CSRN.2019.2901.1.7]
   Barina D, 2022, MULTIMED TOOLS APPL, V81, P2517, DOI 10.1007/s11042-021-11645-x
   Bhaskaran V., 1997, Image and video compression standards: algorithms and architectures
   Brites C, 2021, IEEE T CIRC SYST VID, V31, P339, DOI 10.1109/TCSVT.2020.2976784
   Chai JX, 2000, COMP GRAPH, P307, DOI 10.1145/344779.344932
   Chen Y, 2022, Arxiv, DOI arXiv:2208.06464
   Chitalu FM, 2017, 14TH EUROPEAN CONFERENCE ON VISUAL MEDIA PRODUCTION (CVMP), DOI 10.1145/3150165.3150173
   Chlubna T, 2021, COMPUT VIS MEDIA, V7, P319, DOI 10.1007/s41095-021-0205-0
   Chlubna T, 2023, J SIGNAL PROCESS SYS, V95, P703, DOI 10.1007/s11265-023-01874-8
   Choi M., 2020, CVPR
   El Mesloul Nasri S.A., 2018, Recent Trends in Computer Applications, P99
   Erat O, 2019, IEEE T VIS COMPUT GR, V25, P3063, DOI 10.1109/TVCG.2019.2932237
   Frayne S., 2018, US Patent App, Patent No. [10/012,841, 10012841]
   Fu K, 2022, COMPUT VIS MEDIA, V8, P509, DOI 10.1007/s41095-021-0256-2
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Han HX, 2018, LECT NOTES COMPUT SC, V11165, P274, DOI 10.1007/978-3-030-00767-6_26
   Hedayati E, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9534210
   Hou JH, 2019, IEEE T CIRC SYST VID, V29, P517, DOI 10.1109/TCSVT.2018.2802943
   Huszák A, 2017, MULTIMED TOOLS APPL, V76, P373, DOI 10.1007/s11042-015-3048-9
   Jia CM, 2019, IEEE J EM SEL TOP C, V9, P177, DOI 10.1109/JETCAS.2018.2886642
   Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938
   Jiang XR, 2022, IEEE T IMAGE PROCESS, V31, P6922, DOI 10.1109/TIP.2022.3217374
   Khoury J, 2019, INT CONF COMPUT NETW, P588, DOI [10.1109/iccnc.2019.8685526, 10.1109/ICCNC.2019.8685526]
   Kovacs PT, 2017, J IMAGING SCI TECHN, V61, DOI 10.2352/J.ImagingSci.Technol.2017.61.1.010403
   Lee S, 2022, IEEE T CIRC SYST VID, V32, P3575, DOI 10.1109/TCSVT.2021.3121010
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Li JH, 2023, PROC CVPR IEEE, P22616, DOI 10.1109/CVPR52729.2023.02166
   Li Y, 2016, IEEE T CIRC SYST VID, V26, P1308, DOI 10.1109/TCSVT.2015.2450333
   Liu DY, 2023, IEEE T MULTIMEDIA, V25, P4400, DOI 10.1109/TMM.2022.3175023
   Magnor M, 2000, IEEE T CIRC SYST VID, V10, P338, DOI 10.1109/76.836278
   Mehajabin N, 2019, IEEE IMAGE PROC, P3567, DOI [10.1109/icip.2019.8803668, 10.1109/ICIP.2019.8803668]
   Miandji E, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3269980
   Monteiro RJS, 2020, IEEE ACCESS, V8, P115728, DOI 10.1109/ACCESS.2020.3004625
   Palau R., 2021, Journal of Integrated Circuits and Systems, V16, P1
   Rao KS, 2022, PATTERN RECOGN IMAGE, V32, P33, DOI 10.1134/S1054661822010072
   Ravishankar J., 2023, P IEEE CVF C COMP VI, P3446
   Rerabek M., 2016, 8 INT C QUALITY MULT
   Santos JM, 2018, J VIS COMMUN IMAGE R, V54, P21, DOI 10.1016/j.jvcir.2018.03.003
   Schwarz H., 2014, Block Structures and Parallelism Features in HEVC, P49, DOI [10.1007/978-3-319-06895-43, DOI 10.1007/978-3-319-06895-43]
   Singh M, 2021, PICT COD SYMP, P211, DOI 10.1109/PCS50896.2021.9477448
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Trottnow J, 2019, SA'19: SIGGRAPH ASIA 2019 TECHNICAL BRIEFS, P71, DOI 10.1145/3355088.3365158
   Vaish V., 2008, Comput. Graphics Lab., Stanford Univ., V6
   van Dongen J., 2008, CGI 2008 C P
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang G, 2016, IEEE T IMAGE PROCESS, V25, P5104, DOI 10.1109/TIP.2016.2603602
   Xue Z., 2016, Technical Report
   Yang R, 2020, PROC CVPR IEEE, P6627, DOI 10.1109/CVPR42600.2020.00666
NR 53
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD JUN
PY 2024
VL 102
AR 104201
DI 10.1016/j.jvcir.2024.104201
EA JUN 2024
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WZ8T1
UT WOS:001258796500001
DA 2024-08-05
ER

PT J
AU Litifu, A
   Xiao, JS
   Yan, YC
   Jiang, H
AF Litifu, Ayixiamu
   Xiao, Jinsheng
   Yan, Yuchen
   Jiang, Hao
TI Offline writer identification approach using moment features and
   high-order correlation functions
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Feature fusion; Semantic Codebook; Multiple classifiers; Factor
   analysis; Factor analysis Writer identification
ID BASIC IMAGE FEATURES; SEGMENTATION; DESCRIPTORS; EXTRACTION; CODEBOOK
AB For text -independent writer identification(WI) tasks, this paper proposes a novel method based on semantic codebooks and two effective feature descriptors. This method requires constructing semantic codebooks consisting of various subimages and their labels. Moment features and high -order correlation functions are used to describe handwriting style, and the standard Euclidean distance function is used for distance measurement. Then, two-way analysis of variance(TW-ANOVA) is used to filter out character factors that affect identification accuracy. Finally, a combined measure obtained from multiple classifiers based on a fuzzy integral rule is used to determine the candidate writers. To verify the effectiveness of the system, experiments are performed using the Firemaker, IAM, CRUG-CN and Uyghur2016 databases. The evaluation results demonstrate better performance than most existing methods.
C1 [Litifu, Ayixiamu] Xinjiang Normal Univ, Sch Phys & Elect Engn, Xinjiang Key Lab Luminescence Minerals & Opt Funct, 102 Xinyi Rd, Urumqi 830054, Peoples R China.
   [Xiao, Jinsheng; Yan, Yuchen; Jiang, Hao] Wuhan Univ, Sch Elect Informat, 299 Bayi Rd, Wuhan 430072, Peoples R China.
C3 Xinjiang Normal University; Wuhan University
RP Xiao, JS (corresponding author), Wuhan Univ, Sch Elect Informat, 299 Bayi Rd, Wuhan 430072, Peoples R China.
EM Ayixia@whu.edu.cn; xiaojs@whu.edu.cn; yyc@whu.edu.cn; jh@whu.edu.cn
FU Natural Science Foundation of Xinjiang Uyghur Autonomous Region, China
   [2020D01A76]; Wuhan University; Open Project Program Foundation of the
   Key Laboratory of Opto-Electronics Information Processing, Chinese
   Academy of Sciences [OEIP-O-202009]
FX The authors would like to thank following grants: (1) Natural Science
   Foundation of Xinjiang Uyghur Autonomous Region, China (Grant No.
   2020D01A76) . (2) The numerical calculations in this article have been
   done on the super computing system in the Supercomputing Center of Wuhan
   University. (3) The Open Project Program Foundation of the Key
   Laboratory of Opto-Electronics Information Processing, Chinese Academy
   of Sciences (Grant No. OEIP-O-202009) .
CR Abdi MN, 2015, PATTERN RECOGN, V48, P1890, DOI 10.1016/j.patcog.2014.10.027
   Aubin V, 2018, PATTERN RECOGN, V79, P414, DOI 10.1016/j.patcog.2018.02.024
   Benouini R, 2019, PATTERN RECOGN LETT, V123, P39, DOI 10.1016/j.patrec.2019.03.001
   Bertolini D, 2013, EXPERT SYST APPL, V40, P2069, DOI 10.1016/j.eswa.2012.10.016
   Bi N, 2019, PATTERN RECOGN LETT, V121, P123, DOI 10.1016/j.patrec.2018.05.005
   Brink AA, 2012, PATTERN RECOGN, V45, P162, DOI 10.1016/j.patcog.2011.07.005
   Bulacu M, 2003, PROC INT CONF DOC, P937
   Bulacu M, 2007, IEEE T PATTERN ANAL, V29, P701, DOI 10.1109/TPAMI.2007.1009
   Chahi A, 2020, APPL SOFT COMPUT, V92, DOI 10.1016/j.asoc.2020.106277
   Chahi A, 2020, ENG APPL ARTIF INTEL, V89, DOI 10.1016/j.engappai.2019.103459
   Chahi A, 2019, EXPERT SYST APPL, V123, P357, DOI 10.1016/j.eswa.2019.01.045
   Chen SM, 2019, INFORM SCIENCES, V482, P156, DOI 10.1016/j.ins.2019.01.024
   Cheng PR, 2022, NEUROCOMPUTING, V501, P705, DOI 10.1016/j.neucom.2022.06.057
   Christlein V, 2017, PROC INT CONF DOC, P991, DOI 10.1109/ICDAR.2017.165
   Christlein V, 2017, PATTERN RECOGN, V63, P258, DOI 10.1016/j.patcog.2016.10.005
   Christlein V, 2015, LECT NOTES COMPUT SC, V9358, P540, DOI 10.1007/978-3-319-24947-6_45
   Djeddi C, 2013, PATTERN RECOGN LETT, V34, P1196, DOI 10.1016/j.patrec.2013.03.020
   Durou A, 2019, INFORM PROCESS MANAG, V56, P354, DOI 10.1016/j.ipm.2017.09.005
   Dyla MHM, 2019, OPTIK, V188, P52, DOI 10.1016/j.ijleo.2019.04.128
   Fiel S, 2013, PROC INT CONF DOC, P545, DOI 10.1109/ICDAR.2013.114
   Gattal A, 2018, EXPERT SYST APPL, V99, P155, DOI 10.1016/j.eswa.2018.01.038
   Ghiasi G, 2013, IMAGE VISION COMPUT, V31, P379, DOI 10.1016/j.imavis.2013.03.002
   Hadjadji B, 2018, PATTERN RECOGN, V82, P147, DOI 10.1016/j.patcog.2018.05.001
   Hannad Y, 2016, EXPERT SYST APPL, V47, P14, DOI 10.1016/j.eswa.2015.11.002
   He S, 2021, PATTERN RECOGN, V117, DOI 10.1016/j.patcog.2021.107975
   He S, 2020, IEEE T INF FOREN SEC, V15, P3013, DOI 10.1109/TIFS.2020.2981236
   He S, 2019, PATTERN RECOGN, V88, P64, DOI 10.1016/j.patcog.2018.11.003
   He S, 2017, PATTERN RECOGN, V63, P321, DOI 10.1016/j.patcog.2016.09.017
   He S, 2017, PATTERN RECOGN, V63, P451, DOI 10.1016/j.patcog.2016.09.044
   He S, 2015, PATTERN RECOGN, V48, P4036, DOI 10.1016/j.patcog.2015.05.022
   Hough P.V.C., 1962, United States Patent, Patent No. [3069654, 3069654A]
   Nguyen HT, 2019, PATTERN RECOGN LETT, V121, P104, DOI 10.1016/j.patrec.2018.07.022
   Javidi M, 2020, ENG APPL ARTIF INTEL, V95, DOI 10.1016/j.engappai.2020.103912
   Khalifa E, 2015, PATTERN RECOGN LETT, V59, P18, DOI 10.1016/j.patrec.2015.03.004
   Khan FA, 2019, IEEE T INF FOREN SEC, V14, P289, DOI 10.1109/TIFS.2018.2850011
   Khan FA, 2017, EXPERT SYST APPL, V71, P404, DOI 10.1016/j.eswa.2016.11.012
   Kumar P, 2020, COMPUT ELECTR ENG, V85, DOI 10.1016/j.compeleceng.2020.106707
   Lai SX, 2020, IEEE T INF FOREN SEC, V15, P3553, DOI 10.1109/TIFS.2020.2991880
   Li XM, 2023, PATTERN RECOGN LETT, V171, P45, DOI 10.1016/j.patrec.2023.04.015
   Litifu A, 2019, 5 AS C PATT REC ACPR, V1180, P47
   Litifu A, 2021, APPL INTELL, V51, P8865, DOI 10.1007/s10489-021-02307-4
   Marti U.-V., 2002, International Journal on Document Analysis and Recognition, V5, P39, DOI 10.1007/s100320200071
   Newell AJ, 2014, PATTERN RECOGN, V47, P2255, DOI 10.1016/j.patcog.2013.11.029
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pervouchine V, 2007, PATTERN RECOGN, V40, P1004, DOI 10.1016/j.patcog.2006.08.008
   Rehman A, 2019, IEEE ACCESS, V7, P17149, DOI 10.1109/ACCESS.2018.2890810
   Roy PP, 2016, PATTERN RECOGN, V60, P1057, DOI 10.1016/j.patcog.2016.04.012
   Schomaker L, 2004, IEEE T PATTERN ANAL, V26, P787, DOI 10.1109/TPAMI.2004.18
   Siddiqi I, 2010, PATTERN RECOGN, V43, P3853, DOI 10.1016/j.patcog.2010.05.019
   Songxuan Lai, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1137, DOI 10.1109/ICDAR.2019.00184
   Tan GJ, 2017, FORENSIC SCI INT, V279, P41, DOI 10.1016/j.forsciint.2017.07.034
   Tan J, 2012, NEUROCOMPUTING, V89, P213, DOI 10.1016/j.neucom.2012.02.026
   Wang HY, 2023, PATTERN RECOGN LETT, V165, P146, DOI 10.1016/j.patrec.2022.12.014
   Wu XQ, 2014, IEEE T INF FOREN SEC, V9, P526, DOI 10.1109/TIFS.2014.2301274
   Xiao JS, 2023, PROC CVPR IEEE, P14613, DOI 10.1109/CVPR52729.2023.01404
   Xiao JS, 2023, EXPERT SYST APPL, V211, DOI 10.1016/j.eswa.2022.118665
   Xie ZC, 2019, NEUROCOMPUTING, V350, P271, DOI 10.1016/j.neucom.2019.04.001
   Xiong YJ, 2015, PROC INT CONF DOC, P91, DOI 10.1109/ICDAR.2015.7333732
   Yang LQ, 2022, PROCEDIA COMPUT SCI, V199, P1458, DOI 10.1016/j.procs.2022.01.185
NR 59
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD JUN
PY 2024
VL 102
AR 104183
DI 10.1016/j.jvcir.2024.104183September
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XF0B6
UT WOS:001260140200001
DA 2024-08-05
ER

PT J
AU Chen, HW
   Wang, XJ
   Shao, F
AF Chen, Hangwei
   Wang, Xuejin
   Shao, Feng
TI Blind cartoon image quality assessment based on local structure and
   chromatic statistics
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE No-reference image quality assessment; Cartoon image; Decorative line
   detection; Multiscale chromatic statistics
ID SCREEN CONTENT IMAGES
AB With the help of computer -assisted systems, cartoon synthesis has become convenient and efficient by reusing existing cartoon materials. However, quality evaluation of the obtained cartoon image still relies on laborintensive subjective judgment. This accordingly raises an urgent demand for effective quality evaluation methods to automatically select a cartoon image of high quality from a set of candidates with different parameter settings. In this paper, a new blind image quality assessment metric is developed for evaluating the perceptual quality of cartoon images by considering structure and chromatic distortions. The extracted gradient -based local structure features and multiscale chromatic statistical features are integrated into representation for an overall perceptual quality prediction. Experimental results on two benchmark cartoon image datasets, i.e., NBU-CIQAD and HFUT-CID, indicate that the proposed metric outperforms both state-of-the-art blind quality evaluation methods designed for natural images or synthetic images.
C1 [Chen, Hangwei; Shao, Feng] Ningbo Univ, Fac Elect Engn & Comp Sci, Ningbo 315000, Peoples R China.
   [Wang, Xuejin] Fujian Univ Technol, Sch Comp Sci & Math, Fuzhou 350100, Peoples R China.
C3 Ningbo University; Fujian University of Technology
RP Shao, F (corresponding author), Ningbo Univ, Fac Elect Engn & Comp Sci, Ningbo 315000, Peoples R China.
EM 1010075746@qq.com; 1020468620@qq.com; shaofeng@nbu.edu.cn
OI Chen, Hangwei/0000-0002-3756-2029
FU Natural Science Foundation of China [62071261]; Ningbo Natural Science
   Foundation of China [2022 J067]
FX This work was supported by the Natural Science Foundation of China
   (grant 62071261) , and Ningbo Natural Science Foundation of China (grant
   2022 J067) .
CR CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen HW, 2024, IEEE T INSTRUM MEAS, V73, DOI 10.1109/TIM.2024.3365174
   Chen HW, 2023, IEEE T CIRC SYST VID, V33, P3055, DOI 10.1109/TCSVT.2022.3231041
   Chen HW, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3286542
   Chen HW, 2023, IEEE T MULTIMEDIA, V25, P140, DOI 10.1109/TMM.2021.3121875
   Chen Y, 2023, IEEE T NEUR NET LEAR, V34, P6650, DOI 10.1109/TNNLS.2021.3127720
   Chen Y, 2020, IEEE T CIRC SYST VID, V30, P3282, DOI 10.1109/TCSVT.2019.2931589
   Cui YL, 2024, IEEE T MULTIMEDIA, V26, P5092, DOI 10.1109/TMM.2023.3330096
   Cui YL, 2022, IEEE T EM TOP COMP I, V6, P1222, DOI 10.1109/TETCI.2022.3165935
   Cui YL, 2021, DIGIT SIGNAL PROCESS, V117, DOI 10.1016/j.dsp.2021.103138
   Cui YH, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3149331
   Deng CW, 2020, IEEE T CYBERNETICS, V50, P1146, DOI 10.1109/TCYB.2018.2889376
   Fang YM, 2020, IEEE T CIRC SYST VID, V30, P4050, DOI 10.1109/TCSVT.2019.2951747
   Fang YM, 2018, IEEE T IMAGE PROCESS, V27, P1600, DOI 10.1109/TIP.2017.2781307
   Fang ZW, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3306527
   Gevers T, 1999, PATTERN RECOGN, V32, P453, DOI 10.1016/S0031-3203(98)00036-3
   Gu K, 2017, IEEE T IMAGE PROCESS, V26, P4005, DOI 10.1109/TIP.2017.2711279
   Gu K, 2018, IEEE T NEUR NET LEAR, V29, P1301, DOI 10.1109/TNNLS.2017.2649101
   Gu K, 2017, IEEE T CYBERNETICS, V47, P4559, DOI 10.1109/TCYB.2016.2575544
   Gu K, 2016, NEUROCOMPUTING, V196, P140, DOI 10.1016/j.neucom.2015.11.101
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Gu K, 2014, IEEE T BROADCAST, V60, P555, DOI 10.1109/TBC.2014.2344471
   Jiang QP, 2022, IEEE T CIRC SYST VID, V32, P5959, DOI 10.1109/TCSVT.2022.3164918
   Jiang QP, 2020, IEEE T INSTRUM MEAS, V69, P9784, DOI 10.1109/TIM.2020.3005111
   Jiang QP, 2020, IEEE T INSTRUM MEAS, V69, P7398, DOI 10.1109/TIM.2020.2984928
   Jiang QP, 2019, IEEE T CIRC SYST VID, V29, P323, DOI 10.1109/TCSVT.2017.2783938
   Koh CC, 2006, PROC SPIE, V6057, DOI 10.1117/12.642569
   Lee D, 2016, IEEE T IMAGE PROCESS, V25, P3875, DOI 10.1109/TIP.2016.2579308
   Lee SH, 2023, J VIS COMMUN IMAGE R, V94, DOI 10.1016/j.jvcir.2023.103850
   Li CY, 2022, IEEE INT WORKSH MULT, DOI 10.1109/MMSP55362.2022.9949270
   Li LD, 2017, IEEE T MULTIMEDIA, V19, P1030, DOI 10.1109/TMM.2016.2640762
   Liu LX, 2016, SIGNAL PROCESS-IMAGE, V40, P1, DOI 10.1016/j.image.2015.10.005
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Romeny B. M. H., 2008, written in mathematica, V27
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Shao F, 2018, IEEE T SYST MAN CY-S, V48, P1521, DOI 10.1109/TSMC.2017.2676180
   Shao F, 2013, IEEE T IMAGE PROCESS, V22, P1940, DOI 10.1109/TIP.2013.2240003
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   van de Weijer J, 2006, IEEE T PATTERN ANAL, V28, P150, DOI 10.1109/TPAMI.2006.3
   van de Weijer J, 2006, LECT NOTES COMPUT SC, V3952, P334
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu QB, 2016, IEEE T CIRC SYST VID, V26, P425, DOI 10.1109/TCSVT.2015.2412773
   Wu W, 2024, J VIS COMMUN IMAGE R, V98, DOI 10.1016/j.jvcir.2023.104030
   Yang JC, 2021, IEEE T IMAGE PROCESS, V30, P6801, DOI 10.1109/TIP.2021.3098245
   Yang Y, 2010, IEEE T CIRC SYST VID, V20, P1745, DOI 10.1109/TCSVT.2010.2087452
   Yu J, 2012, IEEE T SYST MAN CY B, V42, P1413, DOI 10.1109/TSMCB.2012.2192108
   Yue GH, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Zhai GT, 2008, SIGNAL PROCESS-IMAGE, V23, P417, DOI 10.1016/j.image.2008.04.007
   Zhang JQ, 2022, J VIS COMMUN IMAGE R, V88, DOI 10.1016/j.jvcir.2022.103617
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
NR 61
TC 0
Z9 0
U1 4
U2 4
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104152
DI 10.1016/j.jvcir.2024.104152
EA APR 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QB4K6
UT WOS:001218405400001
DA 2024-08-05
ER

PT J
AU Chen, Q
   Zheng, BL
   Yan, CG
   Zhu, ZJ
   Wang, TY
   Slabaugh, G
   Yuan, SX
AF Chen, Quan
   Zheng, Bolun
   Yan, Chenggang
   Zhu, Zunjie
   Wang, Tingyu
   Slabaugh, Gregory
   Yuan, Shanxin
TI GoLDFormer: A global-local deformable window transformer for efficient
   image restoration
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Image restoration; Transformer; Adaptive filter; Window-based attention
ID NETWORK
AB Thanks to the powerful modeling capabilities of multi -head self attention (MSA), transformers have shown significant performance gains in vision tasks. However, as transformers require heavy computation, more efficient designs are required. In this paper, we present an efficient transformer architecture named GoLDFormer for image restoration. GoLDFormer extends the capability of window -based self -attention through two core designs. First, We propose a globally -enhanced window -based transformer block (G-WTB), which applies transposed attention to a compressed window representation rather than the spatial features, thus establishing connections between all windows with less computational complexity. Second, since the interactions between image content and window attention weights can be interpreted as spatially varying convolution, we introduce an adaptive filter structure into transformer models and propose a deformable filtering block (DFB) to enable cross -window connections. By adjusting the shape of the generated filters in the DFB, we can balance the computational costs and the degree of adjacent window interaction. Extensive experiments on several image restoration tasks demonstrate that GoLDFormer achieves competitive results against recent methods but with optimal computational costs.
C1 [Chen, Quan; Zheng, Bolun; Yan, Chenggang] Hangzhou Dianzi Univ, Hangzhou 310018, Peoples R China.
   [Zhu, Zunjie; Wang, Tingyu] Hangzhou Dianzi Univ, Lishui Inst, Lishui 323010, Peoples R China.
   [Slabaugh, Gregory; Yuan, Shanxin] Queen Mary Univ London, London E1 2AD, England.
C3 Hangzhou Dianzi University; Hangzhou Dianzi University; University of
   London; Queen Mary University London
RP Zheng, BL (corresponding author), Hangzhou Dianzi Univ, Hangzhou 310018, Peoples R China.
EM chenquan@hdu.edu.cn; blzheng@hdu.edu.cn; cgyan@hdu.edu.cn;
   zunjiezhu@hdu.edu.cn; wongtyu@hdu.edu.cn; g.slabaugh@qmul.ac.uk;
   shanxin.yuan@qmul.ac.uk
OI Zheng, Bolun/0000-0001-8788-1725; Slabaugh, Greg/0000-0003-4060-5226
FU National Key R&D Program of China [2021YFA0715202]; National Nature
   Science Founda-tion of China [62371175, 62001146, U21B2024]; Key R&D
   Program of Zhejiang [2023C01044]; Fundamental Research Funds for the
   Provincial Universities of Zhejiang [GK239909299001-013,
   GK229909299001-009]
FX <B>Acknowledgments</B> This work was supported by National Key R&D
   Program of China under Grant 2021YFA0715202, the National Nature Science
   Founda-tion of China No. 62371175, 62001146, U21B2024, the Key R&D
   Program of Zhejiang under Grant No. 2023C01044. This work is also
   supported by the Fundamental Research Funds for the Provincial
   Universities of Zhejiang under Grants No. GK239909299001-013 and
   GK229909299001-009.
CR Abdelhamed A, 2018, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2018.00182
   Abuolaim Abdullah, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P111, DOI 10.1007/978-3-030-58607-2_7
   Abuolaim A., 2022, P IEEE WINT C APPL C, P1231
   Abuolaim A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2269, DOI 10.1109/ICCV48922.2021.00229
   Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Anwar S, 2019, IEEE I CONF COMP VIS, P3155, DOI 10.1109/ICCV.2019.00325
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI DOI 10.48550/ARXIV.2005.14165
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chang M., 2020, COMPUTER VISION ECCV, P171
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen Q, 2023, NEURAL COMPUT APPL, DOI 10.1007/s00521-023-08852-y
   Cheng S, 2021, PROC CVPR IEEE, P4894, DOI 10.1109/CVPR46437.2021.00486
   Cordonnier JB, 2020, Arxiv, DOI [arXiv:1911.03584, DOI 10.48550/ARXIV.1911.03584]
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   De Brabandere B, 2016, ADV NEUR IN, V29
   Ding XH, 2022, PROC CVPR IEEE, P11953, DOI 10.1109/CVPR52688.2022.01166
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Dutta S, 2021, IEEE COMPUT SOC CONF, P2398, DOI 10.1109/CVPRW53098.2021.00272
   Fedus W., 2021, SWITCH TRANSFORMERS
   Frants V, 2023, IEEE T CYBERNETICS, V53, P5448, DOI 10.1109/TCYB.2023.3238640
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu Y, 2022, IEEE T PATTERN ANAL, V44, P3404, DOI 10.1109/TPAMI.2021.3059911
   Grigoryan A.M., 2014, Appl. Math. Sci.: An Int. J. (MathSJ), V1, P23
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu XW, 2019, PROC CVPR IEEE, P8014, DOI 10.1109/CVPR.2019.00821
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Ignatov Andrey, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P213, DOI 10.1007/978-3-030-67070-2_13
   Ignatov A, 2020, IEEE COMPUT SOC CONF, P1676, DOI 10.1109/CVPRW50498.2020.00217
   Ju MY, 2021, IEEE T IMAGE PROCESS, V30, P2180, DOI 10.1109/TIP.2021.3050643
   Kim Y, 2020, PROC CVPR IEEE, P3479, DOI 10.1109/CVPR42600.2020.00354
   Kui Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8343, DOI 10.1109/CVPR42600.2020.00837
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Lee H, 2022, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR52688.2022.00218
   Lee J, 2021, PROC CVPR IEEE, P2034, DOI 10.1109/CVPR46437.2021.00207
   Lee J, 2019, PROC CVPR IEEE, P12214, DOI 10.1109/CVPR.2019.01250
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li G, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1056, DOI 10.1145/3240508.3240636
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu PJ, 2018, IEEE COMPUT SOC CONF, P886, DOI 10.1109/CVPRW.2018.00121
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loshchilov I, 2017, Sgdr: Stochastic gradient descent with warm restarts, DOI [10.48550/arXiv.1608.03983, DOI 10.48550/ARXIV.1608.03983]
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mildenhall B, 2018, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2018.00265
   Mosseri I, 2013, IEEE INT CONF COMPUT
   Mou C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4308, DOI 10.1109/ICCV48922.2021.00429
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37
   Pan XR, 2022, PROC CVPR IEEE, P805, DOI 10.1109/CVPR52688.2022.00089
   Plötz T, 2017, PROC CVPR IEEE, P2750, DOI 10.1109/CVPR.2017.294
   Purohit K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2289, DOI 10.1109/ICCV48922.2021.00231
   Purohit K, 2019, IEEE INT CONF COMP V, P3417, DOI 10.1109/ICCVW.2019.00424
   Qian R, 2018, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2018.00263
   Ren C, 2021, PROC CVPR IEEE, P8592, DOI 10.1109/CVPR46437.2021.00849
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sharif SMA, 2021, IEEE COMPUT SOC CONF, P233, DOI 10.1109/CVPRW53098.2021.00032
   Son H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2622, DOI 10.1109/ICCV48922.2021.00264
   Son S., 2021, IEEE Trans. Pattern Anal. Mach. Intell.
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang TY, 2022, IEEE T CIRC SYST VID, V32, P867, DOI 10.1109/TCSVT.2021.3061265
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Wei KX, 2022, IEEE T PATTERN ANAL, V44, P8520, DOI 10.1109/TPAMI.2021.3103114
   Wei W, 2019, PROC CVPR IEEE, P3872, DOI 10.1109/CVPR.2019.00400
   Wen BH, 2020, IEEE T IMAGE PROCESS, V29, P5310, DOI 10.1109/TIP.2020.2980753
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xia Zhihao, 2020, P IEEE CVF C COMP VI, P11844
   Xiao J, 2023, IEEE T PATTERN ANAL, V45, P12978, DOI 10.1109/TPAMI.2022.3183612
   Xu J, 2015, IEEE I CONF COMP VIS, P244, DOI 10.1109/ICCV.2015.36
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yasarla R, 2019, PROC CVPR IEEE, P8397, DOI 10.1109/CVPR.2019.00860
   Yuan W., 2024, Signal Process, V215
   Yuan W, 2023, DIGIT SIGNAL PROCESS, V137, DOI 10.1016/j.dsp.2023.104029
   Yue ZS, 2019, ADV NEUR IN, V32
   Zamir Syed Waqas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P492, DOI 10.1007/978-3-030-58595-2_30
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zamir SW, 2020, PROC CVPR IEEE, P2693, DOI 10.1109/CVPR42600.2020.00277
   Zha ZY, 2021, IEEE T IMAGE PROCESS, V30, P5819, DOI 10.1109/TIP.2021.3086049
   Zha ZY, 2022, IEEE T CYBERNETICS, V52, P12440, DOI 10.1109/TCYB.2021.3084931
   Zha ZY, 2020, IEEE T IMAGE PROCESS, V29, P8960, DOI 10.1109/TIP.2020.3021291
   Zha ZY, 2020, IEEE T IMAGE PROCESS, V29, P8561, DOI 10.1109/TIP.2020.3015545
   Zha ZY, 2020, IEEE T IMAGE PROCESS, V29, P3254, DOI 10.1109/TIP.2019.2958309
   Zha ZY, 2018, NEUROCOMPUTING, V275, P2294, DOI 10.1016/j.neucom.2017.11.004
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhang HG, 2019, PROC CVPR IEEE, P5971, DOI 10.1109/CVPR.2019.00613
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang KH, 2024, IEEE T CIRC SYST VID, V34, P3755, DOI 10.1109/TCSVT.2023.3319330
   Zhang KH, 2022, INT J COMPUT VISION, V130, P1754, DOI 10.1007/s11263-022-01620-w
   Zhang KH, 2021, IEEE T IMAGE PROCESS, V30, P7608, DOI 10.1109/TIP.2021.3108019
   Zhang KH, 2020, PROC CVPR IEEE, P2734, DOI 10.1109/CVPR42600.2020.00281
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang T, 2022, INT J COMPUT VISION, V130, P2885, DOI 10.1007/s11263-022-01660-2
   Zhang X., 2022, arXiv, DOI DOI 10.48550/ARXIV.2203.06697
   Zhang YL, 2021, IEEE T PATTERN ANAL, V43, P2480, DOI 10.1109/TPAMI.2020.2968521
   Zhang YL, 2019, Arxiv, DOI arXiv:1903.10082
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao HR, 2022, IEEE T CIRC SYST VID, V32, P4138, DOI 10.1109/TCSVT.2021.3123621
   Zhao SY, 2021, IEEE T IMAGE PROCESS, V30, P3391, DOI 10.1109/TIP.2021.3060873
   Zheng BL, 2022, IEEE T COMPUT IMAG, V8, P346, DOI 10.1109/TCI.2022.3171417
   Zheng BL, 2022, IEEE T PATTERN ANAL, V44, P7705, DOI 10.1109/TPAMI.2021.3115139
   Zheng BL, 2020, IEEE T CIRC SYST VID, V30, P3982, DOI 10.1109/TCSVT.2019.2931045
   Zheng BL, 2020, PROC CVPR IEEE, P3633, DOI 10.1109/CVPR42600.2020.00369
   Zhou SC, 2019, IEEE I CONF COMP VIS, P2482, DOI 10.1109/ICCV.2019.00257
   Zongsheng Yue, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P41, DOI 10.1007/978-3-030-58607-2_3
NR 116
TC 0
Z9 0
U1 2
U2 2
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104117
DI 10.1016/j.jvcir.2024.104117
EA MAR 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PX4H3
UT WOS:001217361500001
DA 2024-08-05
ER

PT J
AU Liu, JY
   Huang, XX
   Shu, XY
   Dong, XD
AF Liu, Jianyi
   Huang, Xingxing
   Shu, Xinyu
   Dong, Xudong
TI Multiple correlation filters with gaussian constraint for fast online
   tracking
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Correlation filters; Gaussian distribution; Bayesian optimization
AB The correlation filter based online tracking methods usually can achieve high real-time performance due to the leverage of the well-known FFT. However, they are also apt to generate the "corrupted training samples"in scenarios with complex background, which will trigger the model drift and deteriorate the tracking accuracy rapidly. The existing methods usually consider this problem from certain aspect and none of them has mined the potential of combining multiple formulas. In this paper, we propose the Output Constraint TransferDiscriminative Scale Space Tracking (OCT-DSST) algorithm, which has taken full consideration of multiple channel feature, multiple filters, kernel trick, memory with incremental learning, and the self -supervision mechanism. We re-formulate the online tracking process by combining all formulas above in a unified framework. The so obtained adaptive learning rate can better exploit the feedback information coming from the intermediate tracking results, and effectively mitigate the corrupted sample problem. The experimental results on the OTB-50/100 and the VOT2016 datasets reveal that the proposed method is comparative to most state -of -the -arts algorithms, and can increase the accuracy by 2% and the success rate by 1.7%, compared to the traditional DSST method.
C1 [Liu, Jianyi; Shu, Xinyu; Dong, Xudong] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Peoples R China.
   [Huang, Xingxing] Jiaotong Univ, Sch Software Engn, Xian 710049, Peoples R China.
   [Huang, Xingxing] Tencent, Network Platform Dept, Shenzhen 518057, Peoples R China.
C3 Xi'an Jiaotong University; Tencent
RP Huang, XX (corresponding author), Jiaotong Univ, Sch Software Engn, Xian 710049, Peoples R China.; Huang, XX (corresponding author), Tencent, Network Platform Dept, Shenzhen 518057, Peoples R China.
EM 13627912045@163.com; jyliu@xjtu.edu.cn
FU National Key Research and Develop-ment Program of China [2021ZD0110700]
FX <B>Acknowledgments</B> This work is supported by the National Key
   Research and Develop-ment Program of China (No. 2021ZD0110700) .
CR Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bhat G, 2018, LECT NOTES COMPUT SC, V11206, P493, DOI 10.1007/978-3-030-01216-8_30
   Bishop C.M., 2006, Machine Learning, P430, DOI DOI 10.1117/1.2819119
   Bishop N.M.N. C. M., 2007, Electron. Imaging, V16
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Camplani M., 2015, 2015 BRIT MACHINE VI
   Chen S., 1999, A Gaussian prior for smoothing maximum entropy models
   Danelljan M., 2014, BRIT MACH VIS C NOTT, DOI DOI 10.5244/C.28.65
   Danelljan M., 2016, CoRR abs/1608.05571
   Danelljan M., 2016, 2016 IEEE C COMPUTER
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2019, PATTERN RECOGN LETT, V124, P74, DOI 10.1016/j.patrec.2018.03.009
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Galoogahi HK, 2015, PROC CVPR IEEE, P4630, DOI 10.1109/CVPR.2015.7299094
   Gladh S, 2016, INT C PATT RECOG, P1243, DOI 10.1109/ICPR.2016.7899807
   HELMBOLD DP, 1994, MACH LEARN, V14, P27, DOI 10.1007/BF00993161
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Huang ZY, 2019, IEEE I CONF COMP VIS, P2891, DOI 10.1109/ICCV.2019.00298
   Jongwon Choi, 2022, 2022 13th International Conference on Information and Communication Technology Convergence (ICTC), P621, DOI 10.1109/ICTC55196.2022.9952592
   Li F, 2020, IEEE T IMAGE PROCESS, V29, P7045, DOI 10.1109/TIP.2020.2997521
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Ma SG, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3178482
   Mei X., 2010, IEEE INT C COMPUTER, P750
   Sun YX, 2019, PROC CVPR IEEE, P5776, DOI 10.1109/CVPR.2019.00593
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Wang MM, 2017, PROC CVPR IEEE, P4800, DOI 10.1109/CVPR.2017.510
   Wang Z., 2022, 2022 IEEE INT C DEPE, P1
   Xu TY, 2019, IEEE I CONF COMP VIS, P7949, DOI 10.1109/ICCV.2019.00804
   Yan C.G., 2021, ACM Trans. Multimed. Comput. Commun. Appl.
   Yan CG, 2021, IEEE T PATTERN ANAL, V43, P1445, DOI 10.1109/TPAMI.2020.2975798
   Yan CG, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3468872
   Yan CG, 2022, IEEE T CIRC SYST VID, V32, P43, DOI 10.1109/TCSVT.2021.3067449
   Yan CG, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3404374
   Yuan WT, 2021, PROC CVPR IEEE, P13139, DOI 10.1109/CVPR46437.2021.01294
   Zhang BC, 2017, IEEE T SYST MAN CY-S, V47, P693, DOI 10.1109/TSMC.2016.2629509
   Zhu G., 2016, 30 AAAI C ARTIFICIAL, P12
   Zuo WM, 2019, IEEE T PATTERN ANAL, V41, P1158, DOI 10.1109/TPAMI.2018.2829180
NR 42
TC 0
Z9 0
U1 3
U2 3
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104089
DI 10.1016/j.jvcir.2024.104089
EA FEB 2024
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NS8V3
UT WOS:001202543800001
DA 2024-08-05
ER

PT J
AU Li, FC
   Ye, MM
   Shao, F
AF Li, Fucui
   Ye, Mengmeng
   Shao, Feng
TI Blind quality assessment of light field image based on view and focus
   stacks
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Light field; Blind quality assessment; Spatiotemporal Gabor filters;
   Sub-aperture images
ID PERCEPTUAL QUALITY; GRADIENT
AB Light field imaging can capture rich information of real scenes, but various distortions will inevitably be introduced in the process of light field image processing. Therefore, it is very important to effectively evaluate the quality of light field images. Due to the lack of commercial light field displays, this paper proposes a blind quality assessment method of light field image based on view and focus stack, based on the visualization technology used in the subjective evaluation of light field images and the human brain's perception process of visual information in the study of visual physiology. First, spatiotemporal Gabor filtering is performed on distorted sub-aperture images (SAIs) to extract the spatial and angular information. On the other hand, the SAIs are refocused to obtain the refocused images with different focusing depths, and then the focus stack is filtered by spatiotemporal Gabor filtering to extract the angular information. In addition, the focus stack is detected to form in-focus area and out-of-focus area, and the features related to spatial structure, depth and semantic information of the refocused images are extracted to evaluate the spatial quality. Finally, support vector regression (SVR) is performed to predict the objective scores by establishing the relationship between the features and subjective scores. Experimental results show that our method can well predict the human eye's perception quality of light field images.
C1 [Li, Fucui] Zhejiang Inst Mech & Elect Engn, Sch Modern Informat Technol, Hangzhou, Peoples R China.
   [Ye, Mengmeng; Shao, Feng] Ningbo Univ, Fac Informat Sci & Engn, Ningbo, Peoples R China.
C3 Zhejiang Institute of Mechanical & Electrical Engineering; Ningbo
   University
RP Shao, F (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo, Peoples R China.
EM shaofeng@nbu.edu.cn
CR Adelson E. H., 1991, PLENOPTIC FUNCTION E, V2
   ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284
   Adhikarla VK, 2017, PROC CVPR IEEE, P3720, DOI 10.1109/CVPR.2017.396
   Bahrami K, 2016, IEEE T MULTIMEDIA, V18, P1568, DOI 10.1109/TMM.2016.2573139
   Battisti F, 2018, EUR SIGNAL PR CONF, P2155, DOI 10.23919/EUSIPCO.2018.8553558
   Chai XL, 2024, IEEE T MULTIMEDIA, V26, P607, DOI 10.1109/TMM.2023.3268370
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen J, 2018, IEEE T IMAGE PROCESS, V27, P4889, DOI 10.1109/TIP.2018.2839524
   Cui YL, 2024, IEEE T MULTIMEDIA, V26, P5092, DOI 10.1109/TMM.2023.3330096
   Dansereau D., 2016, Light field toolbox v0.4
   Dendi SVR, 2020, IEEE T IMAGE PROCESS, V29, P5612, DOI 10.1109/TIP.2020.2984879
   Fang YM, 2018, 2018 IEEE FOURTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM)
   Gershun A, 1939, J. Math. Phys, V18, P51, DOI [10.1002/sapm193918151, DOI 10.1002/SAPM193918151]
   Hahne C, 2018, IEEE T IND ELECTRON, V65, P9757, DOI 10.1109/TIE.2018.2818644
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Ihrke I, 2016, IEEE SIGNAL PROC MAG, V33, P59, DOI 10.1109/MSP.2016.2582220
   Kara PA, 2018, IEEE T BROADCAST, V64, P407, DOI 10.1109/TBC.2018.2834736
   Lasmar NE, 2009, IEEE IMAGE PROC, P2281, DOI 10.1109/ICIP.2009.5414404
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Levoy M, 2006, COMPUTER, V39, P46, DOI 10.1109/MC.2006.270
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Liyanage N, 2020, SIGNAL PROCESS, V167, DOI 10.1016/j.sigpro.2019.107294
   Ma J, 2024, IEEE T CIRC SYST VID, V34, P1696, DOI 10.1109/TCSVT.2023.3297016
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3790, DOI 10.1109/TIP.2020.2966081
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Paudyal P, 2021, IEEE T BROADCAST, V67, P395, DOI 10.1109/TBC.2020.3034445
   Paudyal P, 2019, IEEE T BROADCAST, V65, P152, DOI 10.1109/TBC.2019.2892092
   Paudyal P, 2017, IEEE IMAGE PROC, P196, DOI 10.1109/ICIP.2017.8296270
   Paudyal P, 2017, IEEE T BROADCAST, V63, P507, DOI 10.1109/TBC.2017.2704430
   Petkov N, 2007, BIOL CYBERN, V97, P423, DOI 10.1007/s00422-007-0182-0
   Shan L, 2019, IEEE ACCESS, V7, P127217, DOI 10.1109/ACCESS.2019.2940093
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shi LK, 2020, IEEE T CIRC SYST VID, V30, P4114, DOI 10.1109/TCSVT.2019.2955011
   Shi LK, 2019, IEEE IMAGE PROC, P3781, DOI [10.1109/icip.2019.8803559, 10.1109/ICIP.2019.8803559]
   Shi LK, 2018, IEEE IMAGE PROC, P41, DOI 10.1109/ICIP.2018.8451077
   Soh LK, 1999, IEEE T GEOSCI REMOTE, V37, P780, DOI 10.1109/36.752194
   Tian Y, 2018, J VIS COMMUN IMAGE R, V57, P212, DOI 10.1016/j.jvcir.2018.11.005
   Viola I, 2018, INT WORK QUAL MULTIM, P189
   Wang YL, 2018, IEEE T IMAGE PROCESS, V27, P4274, DOI 10.1109/TIP.2018.2834819
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu GC, 2017, IEEE J-STSP, V11, P926, DOI 10.1109/JSTSP.2017.2747126
   Xiang JJ, 2021, IEEE T CIRC SYST VID, V31, P2575, DOI 10.1109/TCSVT.2020.3030049
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Zhan YB, 2018, IEEE T MULTIMEDIA, V20, P1796, DOI 10.1109/TMM.2017.2780770
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang ZY, 2023, IEEE T IMAGE PROCESS, V32, P6426, DOI 10.1109/TIP.2023.3329663
   Zhou W, 2020, IEEE T IMAGE PROCESS, V29, P4070, DOI 10.1109/TIP.2020.2969777
NR 52
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104074
DI 10.1016/j.jvcir.2024.104074
EA FEB 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KE9O7
UT WOS:001178401100001
DA 2024-08-05
ER

PT J
AU Pan, XY
   Yuan, MR
   Zhang, TX
   Wang, H
AF Pan, Xiaoying
   Yuan, Minrui
   Zhang, Tianxin
   Wang, Hao
TI Deformable attention object tracking network based on cross-correlation
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Siamese; Deformable attention module; Dual cross-correlation;
   Lightweight object tracking network
ID SIAMESE NETWORKS
AB With the increasing popularity and widespread application of embedded devices, lightweight target tracking networks have emerged as a crucial research direction in the field of computer vision. In this paper, we propose a deformable attention object tracking network based on cross -correlation (DAM-LOTNet), which utilizes the lightweight MobileNetV3 network for efficient feature extraction. Aiming at the problem that lightweight networks cannot better capture target features and morphological changes, deformable attention modules are proposed to generate attention maps along the channel and spatial dimensions. Furthermore, a dual crosscorrelation operation is proposed, aiming at fusing channel information and pixel -level information. Thus, the model obtains a more comprehensive feature representation and a more accurate similarity calculation capability while maintaining a lightweight framework. Finally, a feature enhancement network constructed by depthwise separable convolution was added to the classification and regression branches. The designed DAM-LOTNet has 0.552M parameters and 0.121G FLOPs. On the UAV123 dataset, DAM-LOTNet achieves state-of-the-art performance compared to other lightweight object tracking algorithms. Extensive experiments on visual tracking benchmarks (including VOT2018, VOT2019, OTB100, etc.) have demonstrated that DAMLOTNet has extremely low model FLOPs and parameters compared to common deep learning -based trackers, and is essentially on par with these trackers in terms of performance.
C1 [Pan, Xiaoying; Yuan, Minrui; Zhang, Tianxin] Xian Univ Posts & Telecommun, Sch Comp Sci & Technol, Xian 710121, Shaanxi, Peoples R China.
   [Pan, Xiaoying; Yuan, Minrui; Zhang, Tianxin] Xian Univ Posts & Telecommun, Shaanxi Key Lab Network Data Anal & Intelligent Pr, Xian 710121, Shaanxi, Peoples R China.
   [Wang, Hao] Northwestern Polytech Univ, Sch Software, Xian, Peoples R China.
   [Wang, Hao] Natl Engn Lab Air Earth Sea Integrat Big Data Appl, Xian, Peoples R China.
C3 Xi'an University of Posts & Telecommunications; Xi'an University of
   Posts & Telecommunications; Northwestern Polytechnical University
RP Pan, XY (corresponding author), Xian Univ Posts & Telecommun, Sch Comp Sci & Technol, Xian 710121, Shaanxi, Peoples R China.
EM panxiaoying@xupt.edu.cn
OI pan, xiaoying/0000-0002-8899-7540
CR Arar Moab, 2022, P IEEECVF C COMPUTER, P10841
   Bender G, 2018, PR MACH LEARN RES, V80
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bingyan Liao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P429, DOI 10.1007/978-3-030-58542-6_26
   Cai Han, 2020, 8 INT C LEARNING REP
   Cao ZA, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15437, DOI 10.1109/ICCV48922.2021.01517
   Chen BY, 2022, LECT NOTES COMPUT SC, V13682, P375, DOI 10.1007/978-3-031-20047-2_22
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Cheng L, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22041585
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2017.128
   Guo DY, 2021, PROC CVPR IEEE, P9538, DOI 10.1109/CVPR46437.2021.00942
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Kristanl M, 2019, IEEE INT CONF COMP V, P2206, DOI 10.1109/ICCVW.2019.00276
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Real E, 2017, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR.2017.789
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O., 2015, INT J COMPUT VISION, P211
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang GT, 2019, PROC CVPR IEEE, P3638, DOI 10.1109/CVPR.2019.00376
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Yan B, 2021, PROC CVPR IEEE, P5285, DOI 10.1109/CVPR46437.2021.00525
   Yan B, 2021, PROC CVPR IEEE, P15175, DOI 10.1109/CVPR46437.2021.01493
   Zhang D., 2022, P IEEE INT C MACH LE, P26161
   Zhang H, 2022, IEEE COMPUT SOC CONF, P2735, DOI 10.1109/CVPRW56347.2022.00309
   Zhang Wenqi, 2023, 2023 IEEE International Conference on Robotics and Automation (ICRA), P3275, DOI 10.1109/ICRA48891.2023.10160685
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
   Zhu L, 2023, PROC CVPR IEEE, P10323, DOI 10.1109/CVPR52729.2023.00995
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
   Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7
   Zichao Guo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P544, DOI 10.1007/978-3-030-58517-4_32
NR 60
TC 0
Z9 0
U1 13
U2 13
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104039
DI 10.1016/j.jvcir.2023.104039
EA JAN 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GO3W3
UT WOS:001153582100001
DA 2024-08-05
ER

PT J
AU Sriram, KV
   Havaldar, RH
AF Sriram, K. V.
   Havaldar, R. H.
TI Chronological Gazelle Optimization with Deep Learning-Based pixel
   prediction for video steganography in H.264 video for defence
   applications
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Deep Convolutional Neural Network; Pixel prediction; Haar Wavelet
   Transform; Chronological concept; Gazelle Optimization Algorithm
AB With the recent progress in Information technology and the internet, there has been an increase in violations of information security and privacy, particularly in the defence domain. In this work, a secure video steganography method using deep learning-based pixel prediction in H.264 video is presented. The pixels in the keyframe on which the secret image is embedded are predicted using a Deep Convolutional Neural Network (DCNN) optimized by the Chronological Gazelle optimization algorithm (CGOA). Later, embedding is carried out using Haar Wavelet Transform (HWT). The security of the proposed steganography technique has been analysed by performing steganalysis using a Convolutional Neural Network (CNN). The efficiency of the approach is examined by considering evaluation measures, like structural Similarity Index measure (SSIM), Normalized Correlation (NC), Peak Signal to Noise Ratio (PSNR), Bit Error Rate (BER), and Mean Squared Error (MSE), and has attained values of 0.979, 0.974, 49.624, 4.655, and 0.790, revealing imperceptibility and robustness.
C1 [Sriram, K. V.] Angadi Inst Technol & Management, Dept Elect & Commun Engn, Belagavi, Karnataka, India.
   [Havaldar, R. H.] KLE Dr MS Sheshgiri Coll Engn & Technol, Dept Biomed Engn, Belagavi, Karnataka, India.
C3 Angadi Institute of Technology & Management
RP Sriram, KV (corresponding author), Angadi Inst Technol & Management, Dept Elect & Commun Engn, Belagavi, Karnataka, India.
EM shreeramkv@gmail.com
CR Agushaka JO, 2023, NEURAL COMPUT APPL, V35, P4099, DOI 10.1007/s00521-022-07854-6
   Chakraborti T, 2017, Comput. vis. Pattern Recognition
   Chen YK, 2006, J VIS COMMUN IMAGE R, V17, P509, DOI 10.1016/j.jvcir.2005.05.004
   Dalal M, 2022, INF SECUR J, V31, P196, DOI 10.1080/19393555.2021.1896055
   Dar MN, 2015, INT CONF IT CONVERGE
   Dasgupta K., 2012, INT J SECURITY PRIVA, V1
   Douglas M, 2018, MULTIMED TOOLS APPL, V77, P17333, DOI 10.1007/s11042-017-5308-3
   Gurunathan K, 2020, MULTIMED TOOLS APPL, V79, P3893, DOI 10.1007/s11042-019-7471-1
   homepages inf ed ac uk, CAVIAR Test Case Scenarios dataset is from
   Hu WW, 2019, IEEE ACCESS, V7, P121303, DOI 10.1109/ACCESS.2019.2937390
   Hussain M, 2018, SIGNAL PROCESS-IMAGE, V65, P46, DOI 10.1016/j.image.2018.03.012
   Kaur Ramandeep, 2016, International Journal of Image, Graphics and Signal Processing, V8, P31, DOI 10.5815/ijigsp.2016.09.05
   Khan MK, 2018, AUST J FORENSIC SCI, V50, P525, DOI 10.1080/00450618.2017.1296186
   Kunhoth J, 2023, MULTIMED TOOLS APPL, V82, P41943, DOI 10.1007/s11042-023-14844-w
   Kwon SK, 2006, J VIS COMMUN IMAGE R, V17, P186, DOI 10.1016/j.jvcir.2005.05.010
   Liu YX, 2018, IEEE ACCESS, V6, P53984, DOI 10.1109/ACCESS.2018.2869148
   Mstafa RJ, 2020, IEEE ACCESS, V8, P161825, DOI 10.1109/ACCESS.2020.3021356
   Niu K, 2019, IEEE ACCESS, V7, P61523, DOI 10.1109/ACCESS.2019.2902464
   Pibre L, 2018, Arxiv, DOI arXiv:1511.04855
   Pingan Fan Hong Zhang Xianfeng Zhao, 2022, EURASIP Journal on Information Security
   Rabie T, 2019, IEEE ACCESS, V7, P21948, DOI 10.1109/ACCESS.2019.2898838
   Sadek MM, 2015, MULTIMED TOOLS APPL, V74, P7063, DOI 10.1007/s11042-014-1952-z
   Shang YF, 2008, COMPUT MED IMAG GRAP, V32, P109, DOI 10.1016/j.compmedimag.2007.10.004
   Sugave S, 2020, COMPUT J, V63, P817, DOI 10.1093/comjnl/bxz135
   Suresh M, 2022, J KING SAUD UNIV-COM, V34, P3489, DOI 10.1016/j.jksuci.2020.08.007
   Suresh M, 2021, MULTIMED TOOLS APPL, V80, P13253, DOI 10.1007/s11042-020-10395-6
   Suresh M, 2020, MULTIMED TOOLS APPL, V79, P27023, DOI 10.1007/s11042-020-09330-6
   Wang J, 2019, IEEE ACCESS, V7, P119393, DOI 10.1109/ACCESS.2019.2936614
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yang J, 2018, MULTIMED TOOLS APPL, V77, P11979, DOI 10.1007/s11042-017-4844-1
   Yang YY, 2019, MULTIMED TOOLS APPL, V78, P8423, DOI 10.1007/s11042-018-6859-7
   Zhao HG, 2021, IEEE ACCESS, V9, P55506, DOI 10.1109/ACCESS.2021.3059654
NR 32
TC 1
Z9 1
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104024
DI 10.1016/j.jvcir.2023.104024
EA JAN 2024
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GO3B6
UT WOS:001153561400001
DA 2024-08-05
ER

PT J
AU Wang, HY
   Song, HY
   Li, PH
AF Wang, Haoyu
   Song, Haiyu
   Li, Peihong
TI Multi-task network with inter-task consistency learning for face parsing
   and facial expression recognition at real-time speed
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Multi-task network; Face parsing; Real-time inference; Facial expression
   recognition
ID TRANSFORMER
AB In recent years, face parsing and facial expression recognition have attracted increasing interest. Even though there are relevant results about face parsing and face representation, these approaches seek accuracy at the expense of speed. In this paper, we design a novel multi-task learning network for face parsing and facial expression recognition (MPENet). Specifically, MPENet consists of shared encoders and three downstream branches. In the edge perceiving branch, we use category edge and binary edge to extract face boundary information and improve localization of face boundaries. In the segmentation branch, we use graph learning to fuse edge and semantic information of the image, analyze the relations between different feature regions, and capture more contextual relationships. Finally, we design a consistent learning loss function, forcing different branches to learn the same predictions. We have carried out experiments on face datasets, and found that it shows high precision and fast inference speed. Specifically, MPENet achieves F1 scores of 85.9 on the CelebAMask-HQ dataset and 92.9 on the Lapa dataset, with an inference speed of 92.9 FPS. Moreover, MPENet precisely delineates the semantic boundaries of facial regions and, through consistent multi-task learning, effectively facilitates synergy among various tasks.
C1 [Wang, Haoyu; Li, Peihong] Univ Sci & Technol China, Hefei, Peoples R China.
   [Song, Haiyu] Zhejiang Univ Finance & Econ, Coll Informat Management & Artificial Intelligence, Hangzhou, Peoples R China.
   [Song, Haiyu] Zhejiang Univ Technol, Binjiang Inst Artificial Intelligence, Hangzhou, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Zhejiang University of Finance & Economics; Zhejiang
   University of Technology
RP Song, HY (corresponding author), Zhejiang Univ Finance & Econ, Coll Informat Management & Artificial Intelligence, Hangzhou, Peoples R China.; Song, HY (corresponding author), Zhejiang Univ Technol, Binjiang Inst Artificial Intelligence, Hangzhou, Peoples R China.
EM haiyusong@gmail.com
FU National Natural Science Foundation of China [62073284]
FX This work is supported by the National Natural Science Foundation of
   China under Grant 62073284.
CR Chaurasia A, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Chen L.-C., 2018, P EUR C COMP VIS ECC, P801, DOI DOI 10.1007/978-3-030-01234-2_49
   Ge HL, 2022, COMPUT METH PROG BIO, V215, DOI 10.1016/j.cmpb.2022.106621
   Gusi Te, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P258, DOI 10.1007/978-3-030-58610-2_16
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KL, 2021, IEEE T MED IMAGING, V40, P2118, DOI 10.1109/TMI.2021.3072956
   Huang PL, 2023, IEEE T NEUR NET LEAR, V34, P1439, DOI 10.1109/TNNLS.2021.3105386
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Lee J, 2021, IEEE COMPUT SOC CONF, P1501, DOI 10.1109/CVPRW53098.2021.00166
   Li HC, 2019, PROC CVPR IEEE, P9514, DOI 10.1109/CVPR.2019.00975
   Lian CF, 2022, IEEE T NEUR NET LEAR, V33, P4056, DOI 10.1109/TNNLS.2021.3055772
   Lin JP, 2019, PROC CVPR IEEE, P5637, DOI 10.1109/CVPR.2019.00580
   Lin YM, 2021, IMAGE VISION COMPUT, V112, DOI 10.1016/j.imavis.2021.104190
   Liu C, 2023, INFORM SCIENCES, V619, P781, DOI 10.1016/j.ins.2022.11.068
   Liu SF, 2017, Arxiv, DOI arXiv:1708.01936
   Liu YL, 2020, AAAI CONF ARTIF INTE, V34, P11637
   Liu ZL, 2023, J VIS COMMUN IMAGE R, V93, DOI 10.1016/j.jvcir.2023.103806
   Luo L, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10093135
   Mao JW, 2023, Arxiv, DOI [arXiv:2301.12149, DOI 10.48550/ARXIV.2301.12149]
   Nan F, 2022, KNOWL-BASED SYST, V236, DOI 10.1016/j.knosys.2021.107678
   Paszke A, 2016, Arxiv, DOI arXiv:1606.02147
   Serengil S. I., 2020, 2020 INN INT SYST AP, P23, DOI [DOI 10.1109/ASYU50717.2020.9259802, 10.1109/ASYU50717.2020.9259802]
   Shakeel MS, 2022, J VIS COMMUN IMAGE R, V88, DOI 10.1016/j.jvcir.2022.103628
   Sima Y, 2021, APPL SOFT COMPUT, V113, DOI 10.1016/j.asoc.2021.108029
   Te GS, 2021, IEEE T IMAGE PROCESS, V30, P8236, DOI 10.1109/TIP.2021.3113780
   Umirzakova S, 2022, KNOWL-BASED SYST, V250, DOI 10.1016/j.knosys.2022.109036
   Wang GJ, 2022, KNOWL-BASED SYST, V250, DOI 10.1016/j.knosys.2022.109114
   Wei Z, 2019, IEEE T IMAGE PROCESS, V28, P4659, DOI 10.1109/TIP.2019.2909652
   Wu XM, 2023, APPL SOFT COMPUT, V145, DOI 10.1016/j.asoc.2023.110530
   Xue FL, 2022, IEEE COMPUT SOC CONF, P2411, DOI 10.1109/CVPRW56347.2022.00269
   Yang B, 2022, PATTERN RECOGN LETT, V164, P173, DOI 10.1016/j.patrec.2022.11.004
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu WM, 2022, PATTERN RECOGN, V123, DOI 10.1016/j.patcog.2021.108401
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng QP, 2022, PROC CVPR IEEE, P4146, DOI 10.1109/CVPR52688.2022.00412
   Zhou L, 2017, Arxiv, DOI arXiv:1708.03736
   Zhu PF, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3241211
   Zhu S., 2023, J. Vis. Commun. Image Represent.
NR 38
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD AUG
PY 2024
VL 103
AR 104213
DI 10.1016/j.jvcir.2024.104213
EA JUL 2024
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YC7J8
UT WOS:001266351100001
DA 2024-08-05
ER

PT J
AU Fan, XD
   Peng, C
   Jiang, XL
   Han, Y
   Hou, LM
AF Fan, Xiaodong
   Peng, Chang
   Jiang, Xiaoli
   Han, Ying
   Hou, Limin
TI Stacked deformable convolution network with weighted non-local attention
   and branch residual connection for image quality assessment
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Image quality assessment; Deep learning; Deformable convolution;
   Self-attention
ID SIMILARITY; INDEX; DEVIATION; EFFICIENT
AB Convolutional neural networks are data-driven Image Quality Assessment (IQA) models based on the convolutional operation in a rectangular window. Deformable convolutions with learnable receptive fields can efficiently extract structural features of irregular objects in an image. By superimposing deformable convolutions, a plug-and-play module is designed to obtain information for irregular geometric shapes. To selectively fuse shallow and deep features, we propose a weighted non-local attention (WNLA) module with the input and output of self-attention in a weighted manner. This paper proposes a dual branch residual full-reference IQA network that combines weighted non-local attention and stacked deformable convolution. The proposed network was trained on PIPAL dataset and tested on LIVE and TID2013. The cross-dataset evaluation shows that the network has a competitive generalization ability. Ablation experiments indicate that the proposed modules can effectively improve the performance of the network. Comparative experiments reveal that our network is superior to existing excellent networks. The codes for training, test and visualization are available at: https://github.com/Pengchang-haha/SDCN.git.
C1 [Fan, Xiaodong; Han, Ying; Hou, Limin] Liaoning Tech Univ, Fac Elect & Control Engn, Huludao 125105, Liaoning, Peoples R China.
   [Peng, Chang; Jiang, Xiaoli] Bohai Univ, Coll Math, Jinzhou 121013, Liaoning, Peoples R China.
C3 Liaoning Technical University; Bohai University
RP Fan, XD (corresponding author), Liaoning Tech Univ, Fac Elect & Control Engn, Huludao 125105, Liaoning, Peoples R China.
EM bhdxfxd@163.com
FU National Natural Science Foundation of China [52177047, 62203197];
   Foundation of Liaoning Province Education Administration, China
   [LJKZ1026, LJKZ1030]
FX <B>Acknowledgments</B> This work was supported by National Natural
   Science Foundation of China [grant numbers 52177047, 62203197] ;
   Foundation of Liaoning Province Education Administration, China [grant
   numbers LJKZ1026, LJKZ1030] .
CR Ayyoubzadeh SM, 2021, IEEE COMPUT SOC CONF, P388, DOI 10.1109/CVPRW53098.2021.00049
   Bae SH, 2016, IEEE T IMAGE PROCESS, V25, P2392, DOI 10.1109/TIP.2016.2545863
   Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Cao MD, 2023, IEEE T CIRC SYST VID, V33, P160, DOI 10.1109/TCSVT.2022.3201045
   Chang HW, 2013, IEEE T IMAGE PROCESS, V22, P4007, DOI 10.1109/TIP.2013.2266579
   Cheon M, 2021, IEEE COMPUT SOC CONF, P433, DOI 10.1109/CVPRW53098.2021.00054
   Cong H, 2022, IEEE COMPUT SOC CONF, P1200, DOI 10.1109/CVPRW56347.2022.00127
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Damera-Venkata N, 2000, IEEE T IMAGE PROCESS, V9, P636, DOI 10.1109/83.841940
   Ding KY, 2021, INT J COMPUT VISION, V129, P1258, DOI 10.1007/s11263-020-01419-7
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Feng ZP, 2023, ELECTRON LETT, V59, DOI 10.1049/ell2.12698
   Gu J., 2020, P EUR C COMP VIS, P663
   Gu JJ, 2021, IEEE COMPUT SOC CONF, P677, DOI 10.1109/CVPRW53098.2021.00077
   Gu K, 2016, IEEE T BROADCAST, V62, P446, DOI 10.1109/TBC.2015.2511624
   Hammou D, 2021, IEEE COMPUT SOC CONF, P541, DOI 10.1109/CVPRW53098.2021.00066
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Keshari A, 2022, Arxiv, DOI arXiv:2204.09779
   Kingma D. P., 2014, arXiv
   Lao SS, 2022, IEEE COMPUT SOC CONF, P1139, DOI 10.1109/CVPRW56347.2022.00123
   Laparra V., 2016, ELECT IMAGING, V2016, P1, DOI DOI 10.2352/ISSN.2470-1173.2016.16.HVEI-103
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li K, 2024, APPL SOFT COMPUT, V150, DOI 10.1016/j.asoc.2023.111033
   Lin HH, 2019, INT WORK QUAL MULTIM
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Liu JZ, 2023, IEEE T MULTIMEDIA, V25, P5358, DOI 10.1109/TMM.2022.3190700
   Ma C, 2017, COMPUT VIS IMAGE UND, V158, P1, DOI 10.1016/j.cviu.2016.12.009
   Madhusudana PC, 2022, IEEE T IMAGE PROCESS, V31, P4149, DOI 10.1109/TIP.2022.3181496
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Nafchi HZ, 2016, IEEE ACCESS, V4, P5579, DOI 10.1109/ACCESS.2016.2604042
   Pei SC, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440172
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Prashnani E, 2018, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2018.00194
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shi SW, 2021, IEEE COMPUT SOC CONF, P324, DOI 10.1109/CVPRW53098.2021.00042
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Sun SM, 2023, IEEE T MULTIMEDIA, V25, P2912, DOI 10.1109/TMM.2022.3152942
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yang SD, 2022, Arxiv, DOI arXiv:2204.08958
   Yao W., 2018, Infrared Laser Eng., V47, P39
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2012, IEEE IMAGE PROC, P1473, DOI 10.1109/ICIP.2012.6467149
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang L, 2010, IEEE IMAGE PROC, P321, DOI 10.1109/ICIP.2010.5649275
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang WX, 2023, PROC CVPR IEEE, P14071, DOI 10.1109/CVPR52729.2023.01352
   Zhang WX, 2021, IEEE T IMAGE PROCESS, V30, P3474, DOI 10.1109/TIP.2021.3061932
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 56
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD AUG
PY 2024
VL 103
AR 104214
DI 10.1016/j.jvcir.2024.104214
EA JUL 2024
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YB1T9
UT WOS:001265942100001
DA 2024-08-05
ER

PT J
AU Bai, S
   Liang, C
   Wang, Z
   Pan, WC
AF Bai, Shuang
   Liang, Chen
   Wang, Zhen
   Pan, Wenchao
TI Information entropy induced graph convolutional network for semantic
   segmentation
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Semantic segmentation; Graph convolutional network; Information entropy;
   Contextual information
AB Semantic segmentation is a challenging task in computer vision. Contextual information has been proven to be helpful for this task. But few previous works have ever tied to explicitly utilizing contextual information from easy regions in images to facilitate the recognition of hard regions for semantic segmentation. In this paper, we propose a method to identify easy regions and hard regions in images for the semantic segmentation task and propagate contextual information from easy regions to hard regions to facilitate the recognition of hard regions. Specifically, to identify easy and hard regions in images for the semantic segmentation task, we introduce information entropy to measure the easiness of the prediction of the semantic labels of the image pixels. Then, we establish a graph by treating easy regions and hard regions as nodes in the graph. After that, we use graph convolution to propagate contextual information from easy regions to hard regions to reduce the uncertainty of the hard regions for semantic segmentation prediction. The proposed method is named Information Entropy Induced Graph Convolution Network (IEGNet). It can be used in popular semantic segmentation models in a plug-and-play way. Our methmod yields compelling results over several datasets (eg., PASCAL VOC 2012, Cityscapes, ADE20K). Obtained experiment results demonstrated that incorporating IEGNet into semantic segmentation models can improve the performance of semantic segmentation effectively.
C1 [Bai, Shuang; Liang, Chen; Wang, Zhen; Pan, Wenchao] Beijing Jiaotong Univ, Sch Elect & Informat Engn, 3 Shang Yuan Cun, Beijing, Peoples R China.
RP Bai, S (corresponding author), Beijing Jiaotong Univ, Sch Elect & Informat Engn, 3 Shang Yuan Cun, Beijing, Peoples R China.
EM shuangb@bjtu.edu.cn
FU Fundamental Research Funds for the Central Universities [2022JBMC003];
   China Railway Beijing Group Co., Ltd.'s Science and Technology Research
   and Development Program [2024BH02, 2024BG01]; National Radar Signal
   Processing Laboratory [JKW202208]
FX This work was supported by the Fundamental Research Funds for the
   Central Universities 2022JBMC003, China Railway Beijing Group Co.,
   Ltd.'s Science and Technology Research and Development Program 2024BH02,
   2024BG01 and National Radar Signal Processing Laboratory under Grant
   JKW202208.
CR Bao H., 2021, INT C LEARN REPR
   Chai YN, 2021, PROC CVPR IEEE, P15995, DOI 10.1109/CVPR46437.2021.01574
   Chen L.-C., 2018, P EUR C COMP VIS ECC, P801, DOI DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2016, Arxiv, DOI [arXiv:1412.7062, DOI 10.48550/ARXIV.1412.7062, 10.48550/ARXIV.1412.7062]
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Z., 2022, 11 INT C LEARN REPR
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan H, 2017, IEEE COMPUT SOC CONF, P2217, DOI 10.1109/CVPRW.2017.275
   Fang LY, 2024, IEEE T NEUR NET LEAR, V35, P4138, DOI 10.1109/TNNLS.2022.3201820
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Fu SM, 2023, J VIS COMMUN IMAGE R, V96, DOI 10.1016/j.jvcir.2023.103924
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Garcia V., 2018, INT C LEARN REPR, P1
   Glorot X., 2011, P 14 INT C ART INT S, V15, P315, DOI DOI 10.1002/ECS2.1832
   Gu JX, 2019, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2019.00207
   Guo M-H, 2022, P ADV NEUR INF PROC, V35, P1140
   Han GX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3243, DOI 10.1109/ICCV48922.2021.00325
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   He JJ, 2019, PROC CVPR IEEE, P7511, DOI 10.1109/CVPR.2019.00770
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu H., 2020, EUR C COMP VIS, P1
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang L, 2019, Arxiv, DOI arXiv:1907.12273
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Ke TW, 2018, LECT NOTES COMPUT SC, V11205, P605, DOI 10.1007/978-3-030-01246-5_36
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Kirillov A, 2023, IEEE I CONF COMP VIS, P3992, DOI 10.1109/ICCV51070.2023.00371
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee CW, 2018, PROC CVPR IEEE, P1576, DOI 10.1109/CVPR.2018.00170
   Li GL, 2022, IEEE SIGNAL PROC LET, V29, P46, DOI 10.1109/LSP.2021.3124186
   Li HY, 2020, IEEE WCNC, DOI 10.1109/wcnc45663.2020.9120639
   Li X., 2020, P IEEE CVF C COMP VI, P8950
   Li X, 2019, IEEE I CONF COMP VIS, P9166, DOI 10.1109/ICCV.2019.00926
   Liang J.C., 2023, P 40 INT C MACH LEAR, P20787
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Lu Y, 2019, LECT NOTES COMPUT SC, V11554, P97, DOI 10.1007/978-3-030-22796-8_11
   Pan HH, 2023, IEEE T INTELL TRANSP, V24, P3448, DOI 10.1109/TITS.2022.3228042
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shenaj D, 2022, IMAGE VISION COMPUT, V121, DOI 10.1016/j.imavis.2022.104426
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan F, 2018, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2018.00141
   Wan Q, 2023, Arxiv, DOI arXiv:2312.14733
   Wan WT, 2019, IEEE I CONF COMP VIS, P3404, DOI 10.1109/ICCV.2019.00350
   Wang WG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7283, DOI 10.1109/ICCV48922.2021.00721
   Wu LS, 2023, IEEE T PATTERN ANAL, V45, P8827, DOI 10.1109/TPAMI.2022.3233584
   Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26
   Xu JC, 2023, PROC CVPR IEEE, P19529, DOI 10.1109/CVPR52729.2023.01871
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yang Y, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107798
   Yao L, 2019, AAAI CONF ARTIF INTE, P7370
   Yao XD, 2021, IEEE J-STARS, V14, P8407, DOI 10.1109/JSTARS.2021.3105421
   Yu FS, 2016, Arxiv, DOI arXiv:1511.07122
   Yu T, 2022, Arxiv, DOI arXiv:2201.13027
   Yuan YH, 2021, INT J COMPUT VISION, V129, P2375, DOI 10.1007/s11263-021-01465-9
   Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P173, DOI 10.1007/978-3-030-58539-6_11
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang L., 2019, P BRIT MACH VIS C 20
   Zhang W., 2020, Adv. Neural Inf. Process. Syst., V33, P11778
   Zhang Y, 2021, PATTERN RECOGN, V115, DOI 10.1016/j.patcog.2021.107940
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
   Zhou Q, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108290
   Zhou TF, 2023, IEEE T PATTERN ANAL, V45, P8296, DOI 10.1109/TPAMI.2023.3239194
   Zhou TF, 2022, PROC CVPR IEEE, P2572, DOI 10.1109/CVPR52688.2022.00261
NR 74
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD AUG
PY 2024
VL 103
AR 104217
DI 10.1016/j.jvcir.2024.104217
EA JUL 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YJ1F2
UT WOS:001268024100001
DA 2024-08-05
ER

PT J
AU David, L
   Pedrini, H
   Dias, Z
AF David, Lucas
   Pedrini, Helio
   Dias, Zanoni
TI P-NOC: Adversarial training of CAM generating networks for robust weakly
   supervised semantic segmentation priors
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Semantic Segmentation; Saliency Detection; Weak Supervision; Adversarial
   Erasing; Adversarial training; CAM
AB Weakly Supervised Semantic Segmentation (WSSS) techniques explore individual regularization strategies to refine Class Activation Maps (CAMs). In this work, we first analyze complementary WSSS techniques in the literature, their segmentation properties, and the conditions in which they are most effective. Based on these findings, we devise two new techniques: P-NOC and C 2 AM-H. In the first, we promote the conjoint training of two adversarial CAM generating networks: the generator, which progressively learns to erase regions containing class -specific features, and a discriminator, which is refined to gradually shift its attention to new class discriminant features. In the latter, we employ the high quality pseudo -segmentation priors produced by PNOC to guide the learning to saliency information in a weakly supervised fashion. Finally, we employ both pseudo -segmentation priors and pseudo -saliency proposals in the random walk procedure, resulting in higher quality pseudo -semantic segmentation masks, and competitive results with the state of the art.
C1 [David, Lucas; Pedrini, Helio; Dias, Zanoni] Univ Estadual Campinas, Inst Comp, Av Albert Einstein 1251,Cidade Univ, BR-13083852 Campinas, Brazil.
C3 Universidade Estadual de Campinas
RP David, L (corresponding author), Univ Estadual Campinas, Inst Comp, Av Albert Einstein 1251,Cidade Univ, BR-13083852 Campinas, Brazil.
EM lucas.david@ic.unicamp.br; helio@ic.unicamp.br; zanoni@ic.unicamp.br
OI Oliveira David, Lucas/0000-0002-8793-7300
FU CNPq, Brazil [140929/2021-5, 302530/2022-3, 304836/2022-2]
FX The authors would like to thank CNPq, Brazil (grants 140929/2021-5,
   302530/2022-3, and 304836/2022-2) and LNCC/MCTI for providing HPC
   resources of the SDumont supercomputer.
CR Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231
   Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523
   Bhanu B., 2012, The Springer International Series in Engineering and Computer Science
   Chaudhry A, 2017, Arxiv, DOI arXiv:1707.05821
   Chen L.-C., 2018, P EUR C COMP VIS ECC, P801, DOI DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Q, 2022, PROC CVPR IEEE, P4278, DOI 10.1109/CVPR52688.2022.00425
   Chen YT, 2020, J AMB INTEL HUM COMP, DOI 10.1007/s12652-020-02066-z
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fan JS, 2020, PROC CVPR IEEE, P4282, DOI 10.1109/CVPR42600.2020.00434
   Gelana Fraol, 2019, Smart Innovations in Communication and Computational Sciences. Proceedings of ICSICCS-2018. Advances in Intelligent Systems and Computing (AISC 851), P25, DOI 10.1007/978-981-13-2414-7_3
   Hernández-López JJ, 2012, PROC TECH, V3, P196, DOI 10.1016/j.protcy.2012.03.021
   Jiang PT, 2022, PROC CVPR IEEE, P16865, DOI 10.1109/CVPR52688.2022.01638
   Jo S, 2021, IEEE IMAGE PROC, P639, DOI 10.1109/ICIP42928.2021.9506058
   Krahenbuhl P., 2011, PROC NEURAL INF PRO, V24
   Kweon H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6974, DOI 10.1109/ICCV48922.2021.00691
   LaLonde R, 2021, MED IMAGE ANAL, V68, DOI 10.1016/j.media.2020.101889
   Lee J., 2021, P IEEE CVF C COMP VI, P4071
   Lee J., 2021, P INT C NEUR INF PRO, V34, P27408
   Lee M, 2022, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR52688.2022.00429
   Lee S, 2021, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR46437.2021.00545
   Li KP, 2018, PROC CVPR IEEE, P9215, DOI 10.1109/CVPR.2018.00960
   Li Y, 2022, AAAI CONF ARTIF INTE, P1447
   Li Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6944, DOI 10.1109/ICCV48922.2021.00688
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu S, 2022, PROC CVPR IEEE, P2596, DOI 10.1109/CVPR52688.2022.00263
   Liu XF, 2020, AAAI CONF ARTIF INTE, V34, P11629
   Mo YJ, 2022, NEUROCOMPUTING, V493, P626, DOI 10.1016/j.neucom.2022.01.005
   Müller R, 2019, ADV NEUR IN, V32
   Oh SJ, 2017, PROC CVPR IEEE, P5038, DOI 10.1109/CVPR.2017.535
   Ouassit Y, 2022, INT J ONLINE BIOMED, V18, P83, DOI 10.3991/ijoe.v18i10.31531
   Qin J, 2022, AAAI CONF ARTIF INTE, P2117
   Ridnik T, 2021, Arxiv, DOI [arXiv:2104.10972, 10.48550/arXiv.2104.10972, DOI 10.48550/ARXIV.2104.10972]
   Rossetti S, 2022, LECT NOTES COMPUT SC, V13690, P446, DOI 10.1007/978-3-031-20056-4_26
   Ru LX, 2023, PROC CVPR IEEE, P3093, DOI 10.1109/CVPR52729.2023.00302
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Sun FD, 2019, PATTERN RECOGN LETT, V120, P62, DOI 10.1016/j.patrec.2019.01.009
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Vilone G, 2020, Arxiv, DOI [arXiv:2006.00093, 10.48550/arXiv.2006.00093]
   Wang YC, 2022, PROC CVPR IEEE, P4238, DOI 10.1109/CVPR52688.2022.00421
   Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687
   Xie FY, 2020, COMPUT METH PROG BIO, V186, DOI 10.1016/j.cmpb.2019.105241
   Xie JH, 2022, PROC CVPR IEEE, P979, DOI 10.1109/CVPR52688.2022.00106
   Xu L, 2022, PROC CVPR IEEE, P4300, DOI 10.1109/CVPR52688.2022.00427
   Yi S, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108504
   Yude Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12272, DOI 10.1109/CVPR42600.2020.01229
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhan ZQ, 2020, IEEE ACCESS, V8, P21036, DOI 10.1109/ACCESS.2020.2969812
   Zhang D., 2020, PROC ADV NEURAL IN, V33, P655, DOI DOI 10.5555/3495724.3495780
   Zhang M, 2020, ARTIF INTELL REV, V53, P4259, DOI 10.1007/s10462-019-09792-7
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 53
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD JUN
PY 2024
VL 102
AR 104187
DI 10.1016/j.jvcir.2024.104187
EA JUN 2024
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WC9T6
UT WOS:001252795500001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Lv, GL
   Ding, YH
   Chen, XY
   Zheng, YJ
AF Lv, Guilin
   Ding, Yanhui
   Chen, Xinyuan
   Zheng, Yuanjie
TI MP2PMatch: A Mask-guided Part-to-Part Matching network based on
   transformer for occluded person re-identification
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Occluded person re-identification; Human parsing; Data augmentation;
   Transformer
AB Occluded person re-identification (ReID) remains challenging due to the misaligned body parts. Existing works, mainly utilizing extra clues, excel in predicting holistic person images but falter when confronting substantial occlusion. This paper proposes a transformer-based Mask-guided Part-to-Part Matching (MP2PMatch) network for fine-grained matching. Firstly, the Consistency Occlusion Augmentation (COA) processes holistic person images and corresponding body part masks to construct occluded "image-mask"pairs. Next, we introduce learnable part tokens to capture semantic features of various body parts, performing "pull close"and "push apart"operations based on identity labels and part visibility, ensuring the one-to-one correspondence between part features and body parts. Additionally, the proposed Body Region Attention (BRA) utilizes the overall attention on body regions to guide the network to overcome interference from both occlusion and background. Extensive experiments demonstrate that MP2PMatch achieves exceptional occluded ReID performance.
C1 [Lv, Guilin; Ding, Yanhui; Chen, Xinyuan; Zheng, Yuanjie] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Shandong, Peoples R China.
   [Zheng, Yuanjie] Shanghai Artificial Intelligence Lab, Shanghai 200433, Peoples R China.
C3 Shandong Normal University
RP Ding, YH (corresponding author), Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Shandong, Peoples R China.
EM dingyanhui@sdnu.edu.cn; yjzheng@sdnu.edu.cn
OI Chen, Xinyuan/0009-0002-6573-6791
FU National Nature Science Foundation of China [62272283]; Major Basic
   Research Project of Shandong Natural Science Foundation, China
   [ZR2019ZD04]; New Twentieth Items of Universities in Jinan, China
   [2021GXRC049]
FX <B>Acknowledgments</B> This work is supported in part by the National
   Nature Science Foundation of China (No. 62272283) ; Major Basic Research
   Project of Shandong Natural Science Foundation, China (No. ZR2019ZD04) ;
   New Twentieth Items of Universities in Jinan, China (No. 2021GXRC049) .
CR Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen PX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11813, DOI 10.1109/ICCV48922.2021.01162
   Delussu R, 2023, EXPERT SYST APPL, V226, DOI 10.1016/j.eswa.2023.120216
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Dou SG, 2023, IEEE T IMAGE PROCESS, V32, P458, DOI 10.1109/TIP.2022.3229639
   Gao LS, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3771, DOI 10.1145/3394171.3413833
   Ge YX, 2018, ADV NEUR IN, V31
   Gong C., 2021, arXiv, DOI 10.48550/arXiv.2104.12753
   He LX, 2018, PROC CVPR IEEE, P7073, DOI 10.1109/CVPR.2018.00739
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Hou RB, 2022, IEEE T PATTERN ANAL, V44, P4894, DOI 10.1109/TPAMI.2021.3079910
   Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954
   Huang HJ, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102789
   Huang HJ, 2018, PROC CVPR IEEE, P5098, DOI 10.1109/CVPR.2018.00535
   Huang MY, 2023, IEEE T IMAGE PROCESS, V32, P1568, DOI 10.1109/TIP.2023.3247159
   Jia MX, 2021, AAAI CONF ARTIF INTE, V35, P1673
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Kuan Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P346, DOI 10.1007/978-3-030-58580-8_21
   Li PK, 2022, IEEE T PATTERN ANAL, V44, P3260, DOI 10.1109/TPAMI.2020.3048039
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu YH, 2023, IMAGE VISION COMPUT, V131, DOI 10.1016/j.imavis.2023.104633
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Ming ZQ, 2022, IMAGE VISION COMPUT, V119, DOI 10.1016/j.imavis.2022.104394
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shang Gao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11741, DOI 10.1109/CVPR42600.2020.01176
   Shi Han, 2022, arXiv, DOI [DOI 10.48550/ARXIV.2202.08625, 10.48550/arXiv.2202.08625]
   Somers V, 2023, IEEE WINT CONF APPL, P1613, DOI 10.1109/WACV56688.2023.00166
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun H, 2019, IEEE I CONF COMP VIS, P6736, DOI 10.1109/ICCV.2019.00684
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Sun YF, 2019, PROC CVPR IEEE, P393, DOI 10.1109/CVPR.2019.00048
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tan Lei, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P531, DOI 10.1145/3503161.3547764
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang PF, 2023, IEEE T MULTIMEDIA, V25, P3154, DOI 10.1109/TMM.2022.3156282
   Wang T, 2022, AAAI CONF ARTIF INTE, P2540
   Wang ZK, 2022, PROC CVPR IEEE, P4744, DOI 10.1109/CVPR52688.2022.00471
   Yan G, 2023, IEEE T CIRC SYST VID, V33, P4217, DOI 10.1109/TCSVT.2023.3241764
   Yang WJ, 2019, PROC CVPR IEEE, P1389, DOI 10.1109/CVPR.2019.00148
   Zhang GQ, 2022, J VIS COMMUN IMAGE R, V87, DOI 10.1016/j.jvcir.2022.103581
   Zhang X, 2018, Arxiv, DOI arXiv:1711.08184
   Zhang ZW, 2024, NEUROCOMPUTING, V578, DOI 10.1016/j.neucom.2024.127442
   Zhao CR, 2023, IEEE T IMAGE PROCESS, V32, P4223, DOI 10.1109/TIP.2023.3290525
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zheng KC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4537, DOI 10.1145/3474085.3475610
   Zheng L, 2016, Arxiv, DOI arXiv:1610.02984
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhuo JX, 2018, IEEE INT CON MULTI
NR 53
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104128
DI 10.1016/j.jvcir.2024.104128
EA MAR 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QS8Y2
UT WOS:001222963000001
DA 2024-08-05
ER

PT J
AU Wang, CX
   Wang, J
   Xu, WT
AF Wang, Chuanxu
   Wang, Jing
   Xu, Wenting
TI Double branch synergies with modal reinforcement for weakly supervised
   temporal action detection
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Multi-branch synergies; Temporal action localization; Modal
   reinforcement; Weakly supervising learning
ID LOCALIZATION
AB Weakly supervised Temporal Action localization (WTAL) aims to locate the action instances and identify their corresponding labels. Most current methods rely on a Multi-Instance Learning (MIL) framework to predict start and end boundaries of each action in a video. However, they have shortcomings of incomplete positioning and context confusion. Therefore, we propose an algorithm of Double Branch Synergies with Modal Reinforcement (DBSMR), which utilizes long-short temporal attention to model contextual relationships and refines segmental features to encourage more distinguishable segment classification. In terms of blur boundaries between actions and camouflage background in complex scenes and easily resulting in wrong positioning, we construct a sparse graph focusing on the effective representation of context motion by optical flow modal learning, further enhancing the representation of the active region to be examined, and suppressing the interference from the background. Finally, with the idea of "All roads lead to Rome", we design motion-guided loss constraints to balance the long-short temporal module and graph reinforcement module, by which the two branches can converge to almost the same detection goal, thus to achieve an agreement of near ground truth localization result. The algorithm achieves mAP of 69.1% and 42.0% detection performances on the THUMOS14 and ActivityNet1.2 datasets respectively. We also verify its effectiveness by comparing it with the state-of-the-art methods.
C1 [Wang, Chuanxu; Wang, Jing; Xu, Wenting] Qingdao Univ Sci & Technol, Sch Informat Sci & Technol, Qingdao 266001, Peoples R China.
C3 Qingdao University of Science & Technology
RP Wang, J (corresponding author), Qingdao Univ Sci & Technol, Sch Informat Sci & Technol, Qingdao 266001, Peoples R China.
EM wj0323i@163.com
FU National Natural Science Foundation of China [61672305]
FX <B>Acknowledgments</B> This work described in this paper was partially
   supported by the National Natural Science Foundation of China (Grant no.
   61672305) .
CR Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Cao M, 2022, IEEE T IMAGE PROCESS, V31, P5203, DOI 10.1109/TIP.2022.3193752
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Fan Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P420, DOI 10.1007/978-3-030-58548-8_25
   Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924
   Hu WM, 2011, IEEE T SYST MAN CY C, V41, P797, DOI 10.1109/TSMCC.2011.2109710
   Huang LJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7982, DOI 10.1109/ICCV48922.2021.00790
   Idrees H, 2017, COMPUT VIS IMAGE UND, V155, P1, DOI 10.1016/j.cviu.2016.10.018
   Islam A, 2021, AAAI CONF ARTIF INTE, V35, P1637
   Lee P, 2021, AAAI CONF ARTIF INTE, V35, P1854
   Lee P, 2020, AAAI CONF ARTIF INTE, V34, P11320
   Lee YJ, 2012, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2012.6247820
   Liu DC, 2019, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2019.00139
   Maron O, 1998, ADV NEUR IN, V10, P570
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Paul S, 2018, LECT NOTES COMPUT SC, V11208, P588, DOI 10.1007/978-3-030-01225-0_35
   Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706
   Qin X, 2022, NEUROCOMPUTING, V510, P48, DOI 10.1016/j.neucom.2022.08.040
   Rashid M, 2020, IEEE WINT CONF APPL, P604, DOI [10.1109/WACV45572.2020.9093404, 10.1109/wacv45572.2020.9093404]
   Shou Z, 2018, LECT NOTES COMPUT SC, V11220, P162, DOI 10.1007/978-3-030-01270-0_10
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh KK, 2017, IEEE I CONF COMP VIS, P3544, DOI 10.1109/ICCV.2017.381
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wang L, 2019, PROC CVPR IEEE, P10288, DOI 10.1109/CVPR.2019.01054
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Xu H, 2019, PROC CVPR IEEE, P9290, DOI 10.1109/CVPR.2019.00952
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yuan Y., 2019, P 7 INT C LEARN REPR
   Yuanhao Zhai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P37, DOI 10.1007/978-3-030-58539-6_3
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhang C, 2021, PROC CVPR IEEE, P16005, DOI 10.1109/CVPR46437.2021.01575
   Zhang CW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P738, DOI 10.1145/3343031.3351044
   Zhang M, 2022, IEEE ACCESS, V10, P96378, DOI 10.1109/ACCESS.2022.3205594
   Zhang XY, 2021, COMPUT VIS IMAGE UND, V210, DOI 10.1016/j.cviu.2021.103256
   Zhang YR, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.109027
   Zhang Z, 2012, IEEE T PATTERN ANAL, V34, P436, DOI 10.1109/TPAMI.2011.157
   Zhong JX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P35, DOI 10.1145/3240508.3240511
   Zhu LC, 2021, IEEE T IMAGE PROCESS, V30, P4253, DOI 10.1109/TIP.2021.3070733
NR 38
TC 0
Z9 0
U1 2
U2 2
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104090
DI 10.1016/j.jvcir.2024.104090
EA FEB 2024
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LK0O1
UT WOS:001186577200001
DA 2024-08-05
ER

PT J
AU Demir, Y
   Kaplan, NH
   Kucuk, S
   Severoglu, N
AF Demir, Y.
   Kaplan, N. H.
   Kucuk, S.
   Severoglu, N.
TI Pixel-wise low-light image enhancement based on metropolis theorem
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Low-light enhancement; Metropolis theorem; Multi-scale decomposition;
   Gamma correction
ID QUALITY ASSESSMENT; NETWORK
AB Images taken in low-light conditions frequently encounter visibility problems, such as severe noise, reduced brightness, and low contrast. This paper introduces an approach to enhance low-light images using the Metropolis Theorem (MT). The method begins by applying a global gamma correction to the input image, followed by transforming the globally corrected image into the HSV (Hue, Saturation, Value - V) domain. To achieve multi-scale decomposition, an application of the MT is proposed, resulting in approximation and detail sub-images of the V component. Subsequently, local gamma correction is employed on both the final approximation and detail images to enhance local contrast. The refined approximation and detail images are then combined to reconstruct the refined V component. The reconstructed image is obtained by weighting each band of the image with the refined V component. Experimental results demonstrate that the proposed method outperforms state-of-the-art methods, providing improved visual quality and more natural colors.
C1 [Demir, Y.; Kaplan, N. H.; Kucuk, S.; Severoglu, N.] Erzurum Tech Univ, Elect & Elect Engn Dept, TR-25050 Erzurum, Turkiye.
C3 Erzurum Technical University
RP Kaplan, NH (corresponding author), Erzurum Tech Univ, Elect & Elect Engn Dept, TR-25050 Erzurum, Turkiye.
EM huseyin.kaplan@erzurum.edu.tr
CR Aakerberg A., 2021, 35 C NEUR INF PROC S, V35, P1
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Celik T, 2011, IEEE T IMAGE PROCESS, V20, P3431, DOI 10.1109/TIP.2011.2157513
   Demir Y, 2023, DIGIT SIGNAL PROCESS, V138, DOI 10.1016/j.dsp.2023.104054
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Huang SC, 2013, IEEE T IMAGE PROCESS, V22, P1032, DOI 10.1109/TIP.2012.2226047
   Jiang H, 2023, MULTIMED TOOLS APPL, DOI 10.1007/s11042-023-16914-5
   Jiang H, 2020, J ELECTRON IMAGING, V29, DOI 10.1117/1.JEI.29.4.043016
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Kaplan NH, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103720
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lee C, 2012, IEEE IMAGE PROC, P965, DOI 10.1109/ICIP.2012.6467022
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P9396, DOI 10.1109/TPAMI.2021.3126387
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P4225, DOI 10.1109/TPAMI.2021.3063604
   Li CY, 2018, PATTERN RECOGN LETT, V104, P15, DOI 10.1016/j.patrec.2018.01.010
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Li ZW, 2023, NEUROCOMPUTING, V518, P332, DOI 10.1016/j.neucom.2022.10.083
   Lim S, 2021, IEEE T MULTIMEDIA, V23, P4272, DOI 10.1109/TMM.2020.3039361
   Liu JY, 2021, INT J COMPUT VISION, V129, P1153, DOI 10.1007/s11263-020-01418-8
   Liu W, 2020, IEEE T CIRC SYST VID, V30, P23, DOI 10.1109/TCSVT.2018.2890202
   Liu XY, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3289626
   Liu YF, 2017, IEEE T CIRC SYST VID, V27, P1171, DOI 10.1109/TCSVT.2016.2527338
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Ma L, 2022, IEEE T NEUR NET LEAR, V33, P5666, DOI 10.1109/TNNLS.2021.3071245
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Rahman S, 2016, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-016-0138-1
   Rasheed MT, 2023, SIGNAL PROCESS, V204, DOI 10.1016/j.sigpro.2022.108821
   Ren XT, 2020, IEEE T IMAGE PROCESS, V29, P5862, DOI 10.1109/TIP.2020.2984098
   Singh K, 2014, PATTERN RECOGN LETT, V36, P10, DOI 10.1016/j.patrec.2013.08.024
   Tao X, 2017, IEEE I CONF COMP VIS, P222, DOI 10.1109/ICCV.2017.33
   Wang LW, 2020, IEEE T IMAGE PROCESS, V29, P7984, DOI 10.1109/TIP.2020.3008396
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang W., 2023, Image Commun., V118
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P3461, DOI 10.1109/TIP.2021.3062184
   Yang X., 2022, Adv. Neural Inf. Process. Syst., V35, P25739
   Yang XY, 2022, LECT NOTES COMPUT SC, V13694, P73, DOI 10.1007/978-3-031-19830-4_5
   Yu WY, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21030845
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhao ZJ, 2022, IEEE T CIRC SYST VID, V32, P1076, DOI 10.1109/TCSVT.2021.3073371
NR 48
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD JUN
PY 2024
VL 102
AR 104211
DI 10.1016/j.jvcir.2024.104211
EA JUN 2024
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XG6Q5
UT WOS:001260573400001
DA 2024-08-05
ER

PT J
AU Islam, Z
   Ben Hamza, A
AF Islam, Zaedul
   Ben Hamza, A.
TI Multi-hop graph transformer network for 3D human pose estimation
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE 3D human pose estimation; Graph convolutional network; Transformer;
   Multi-hop; Dilated convolution
AB Accurate 3D human pose estimation is a challenging task due to occlusion and depth ambiguity. In this paper, we introduce a multi -hop graph transformer network designed for 2D -to -3D human pose estimation in videos by leveraging the strengths of multi-head self-attention and multi -hop graph convolutional networks with disentangled neighborhoods to capture spatio-temporal dependencies and handle long-range interactions. The proposed network architecture consists of a graph attention block composed of stacked layers of multi-head self-attention and graph convolution with learnable adjacency matrix, and a multi -hop graph convolutional block comprised of multi -hop convolutional and dilated convolutional layers. The combination of multi-head self-attention and multi -hop graph convolutional layers enables the model to capture both local and global dependencies, while the integration of dilated convolutional layers enhances the model's ability to handle spatial details required for accurate localization of the human body joints. Extensive experiments demonstrate the effectiveness and generalization ability of our model, achieving competitive performance on benchmark datasets.
C1 [Islam, Zaedul; Ben Hamza, A.] Concordia Univ, Concordia Inst Informat Syst Engn, Montreal, PQ, Canada.
C3 Concordia University - Canada
RP Ben Hamza, A (corresponding author), Concordia Univ, Concordia Inst Informat Syst Engn, Montreal, PQ, Canada.
EM hamza@ciise.concordia.ca
FU Discovery Grants program of Natural Sciences and Engineering Research
   Council of Canada
FX This work was supported in part by the Discovery Grants program of
   Natural Sciences and Engineering Research Council of Canada.
CR Ailing Zeng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P507, DOI 10.1007/978-3-030-58568-6_30
   Azizi N, 2022, LECT NOTES COMPUT SC, V13661, P160, DOI 10.1007/978-3-031-19769-7_10
   Cai YJ, 2019, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2019.00236
   Chen TL, 2022, IEEE T CIRC SYST VID, V32, P198, DOI 10.1109/TCSVT.2021.3057267
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Ching-Hang Chen, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P5759, DOI 10.1109/CVPR.2017.610
   Ding R., 2023, P IEEE INT C AC SPEE
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Gu YW, 2019, IEEE INT CONF COMP V, P2619, DOI 10.1109/ICCVW.2019.00320
   Habibie I, 2019, PROC CVPR IEEE, P10897, DOI 10.1109/CVPR.2019.01116
   Hassanin M., 2022, arXiv
   Hossain MRI, 2018, LECT NOTES COMPUT SC, V11214, P69, DOI 10.1007/978-3-030-01249-6_5
   Ingwersen C.K., 2023, P IEEE C COMP VIS PA
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Islam Z, 2023, J VIS COMMUN IMAGE R, V95, DOI 10.1016/j.jvcir.2023.103908
   Lee J.Y., 2022, P BRIT MACH VIS C
   Lee K, 2018, LECT NOTES COMPUT SC, V11211, P123, DOI 10.1007/978-3-030-01234-2_8
   Li C., 2020, P BRIT MACH VIS C
   Li C, 2019, PROC CVPR IEEE, P9879, DOI 10.1109/CVPR.2019.01012
   Lin J., 2019, P BRIT MACH VIS C
   Liu JF, 2021, IEEE INT CONF ROBOT, P3374, DOI 10.1109/ICRA48506.2021.9561605
   Liu RX, 2020, PROC CVPR IEEE, P5063, DOI 10.1109/CVPR42600.2020.00511
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Luvizon DC, 2021, IEEE T PATTERN ANAL, V43, P2752, DOI 10.1109/TPAMI.2020.2976014
   Mao W, 2019, IEEE I CONF COMP VIS, P9488, DOI 10.1109/ICCV.2019.00958
   Martinez Julieta, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2659, DOI 10.1109/ICCV.2017.288
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   Park S, 2016, LECT NOTES COMPUT SC, V9915, P156, DOI 10.1007/978-3-319-49409-8_15
   Pavlakos G, 2018, PROC CVPR IEEE, P7307, DOI 10.1109/CVPR.2018.00763
   Pavlakos G, 2017, PROC CVPR IEEE, P1263, DOI 10.1109/CVPR.2017.139
   Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794
   Quan J., 2021, P BRIT MACH VIS C
   Song LC, 2021, J VIS COMMUN IMAGE R, V76, DOI 10.1016/j.jvcir.2021.103055
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33
   Sun X, 2017, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2017.284
   Tekin B, 2017, IEEE I CONF COMP VIS, P3961, DOI 10.1109/ICCV.2017.425
   Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603
   Vaswani A, 2017, ADV NEUR IN, V30
   Xu TH, 2021, PROC CVPR IEEE, P16100, DOI 10.1109/CVPR46437.2021.01584
   Yang W, 2018, PROC CVPR IEEE, P5255, DOI 10.1109/CVPR.2018.00551
   Yu F., 2016, ICLR, P1
   Zanfir A., 2023, P C ROB LEARN
   Zeng AL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11416, DOI 10.1109/ICCV48922.2021.01124
   Zerui Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P715, DOI 10.1007/978-3-030-58580-8_42
   Zhan Y, 2022, PROC CVPR IEEE, P13106, DOI 10.1109/CVPR52688.2022.01277
   Zhang Z., 2022, P BRIT MACH VIS C
   Zhao L, 2019, PROC CVPR IEEE, P3420, DOI 10.1109/CVPR.2019.00354
   Zhao QT, 2023, PROC CVPR IEEE, P8877, DOI 10.1109/CVPR52729.2023.00857
   Zhao WX, 2022, PROC CVPR IEEE, P20406, DOI 10.1109/CVPR52688.2022.01979
   Zheng C, 2024, ACM COMPUT SURV, V56, DOI 10.1145/3603618
   Zheng C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11636, DOI 10.1109/ICCV48922.2021.01145
   Zhou XY, 2016, LECT NOTES COMPUT SC, V9915, P186, DOI 10.1007/978-3-319-49409-8_17
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
   Zou Z., 2020, P BRIT MACH VIS C
   Zou ZM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11457, DOI 10.1109/ICCV48922.2021.01128
NR 56
TC 0
Z9 0
U1 2
U2 2
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104174
DI 10.1016/j.jvcir.2024.104174
EA MAY 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TS2N3
UT WOS:001243183900001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Qi, K
   Xu, WH
   Chen, WB
   Tao, X
   Chen, PJ
AF Qi, Ke
   Xu, Wenhao
   Chen, Wenbin
   Tao, Xi
   Chen, Peijia
TI Multiple object tracking with segmentation and interactive multiple
   model
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Computer vision; Multiple object tracking; Pattern recognition; Motion
   estimation
AB Multiple object tracking (MOT) is a sophisticated computer vision task that aims to detect and track the trajectories of all objects within a given scene. MOT necessitates the establishment of unique identifiers for each object in the scene and currently, the majority of MOT works adopt tracking -by -detection, using reidentification techniques to associate objects based on appearance or motion features. However, traditional MOT methods may yield suboptimal results when appearance features are unreliable or geometric features are confounded by irregular motions. Consequently, this paper proposes a cascaded matching tracker called IMMSegTrack, which replaces detection boxes with segmentation contours for IoU matching and generates convincing prediction outcomes by blending multiple adaptive Kalman filter models. With the guide of interactive multiple model and pixel -level matching, better performance could be achieved through welldesigned cascaded association. Our experimental innovations mainly focus on the prediction and matching aspects within the MOT framework, while tests on DanceTrack and SoccerNet datasets have been conducted using the original models of YOLOv8 to obtain desired results. The relevant codes for the experiments are available at https://github.com/xwh129/IMMSegTrack.git. In addition, it is considerable to retrain model weights and expand the experiments to more datasets for better performance.
C1 [Qi, Ke; Xu, Wenhao; Chen, Wenbin; Tao, Xi; Chen, Peijia] Guangzhou Univ, Sch Comp Sci & Cyber Engn, Guangzhou 510006, Peoples R China.
C3 Guangzhou University
RP Xu, WH (corresponding author), Guangzhou Univ, Sch Comp Sci & Cyber Engn, Guangzhou 510006, Peoples R China.
EM 2112206128@e.gzhu.edu.cn
OI Xu, Wenhao/0009-0007-5826-3486
CR Aharon N, 2022, Arxiv, DOI [arXiv:2206.14651, DOI 10.48550/ARXIV.2206.14651]
   Bai HX, 2021, PROC CVPR IEEE, P6715, DOI 10.1109/CVPR46437.2021.00665
   Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Bochinski E, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Brasó G, 2020, PROC CVPR IEEE, P6246, DOI 10.1109/CVPR42600.2020.00628
   Cao JK, 2023, PROC CVPR IEEE, P9686, DOI 10.1109/CVPR52729.2023.00934
   Chen L, 2018, IEEE INT CON MULTI, DOI 10.1097/PEC.0000000000001553
   Chu P, 2019, IEEE I CONF COMP VIS, P6171, DOI 10.1109/ICCV.2019.00627
   Cioppa A, 2022, IEEE COMPUT SOC CONF, P3490, DOI 10.1109/CVPRW56347.2022.00393
   Dendorfer P., 2020, arXiv
   Du Y., 2023, IEEE Trans. Multimed.
   Du YH, 2021, IEEE INT CONF COMP V, P2809, DOI 10.1109/ICCVW54120.2021.00315
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gustafsson F, 2002, IEEE T SIGNAL PROCES, V50, P425, DOI 10.1109/78.978396
   Hornakova A, 2020, PR MACH LEARN RES, V119
   Jinlong Peng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P145, DOI 10.1007/978-3-030-58548-8_9
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Lauer J, 2022, NAT METHODS, V19, P496, DOI 10.1038/s41592-022-01443-0
   Li S, 2022, PROC CVPR IEEE, P8845, DOI 10.1109/CVPR52688.2022.00865
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Luiten J, 2021, INT J COMPUT VISION, V129, P548, DOI 10.1007/s11263-020-01375-2
   Mazor E, 1998, IEEE T AERO ELEC SYS, V34, P103, DOI 10.1109/7.640267
   Meinhardt T, 2022, PROC CVPR IEEE, P8834, DOI 10.1109/CVPR52688.2022.00864
   Milan A, 2016, Arxiv, DOI arXiv:1603.00831
   Ming Wu, 2010, Proceedings of the 2010 International Conference on Machine Vision and Human-Machine Interface (MVHI 2010), P64, DOI 10.1109/MVHI.2010.88
   Redmon J., 2018, CoRR
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sun PZ, 2022, PROC CVPR IEEE, P20961, DOI 10.1109/CVPR52688.2022.02032
   Sun Peize, 2020, ABS201215460 CORR
   Voigtlaender P, 2019, PROC CVPR IEEE, P7934, DOI 10.1109/CVPR.2019.00813
   Wang Q, 2021, PROC CVPR IEEE, P3875, DOI 10.1109/CVPR46437.2021.00387
   Welch G., 1995, An introduction to the kalman filter
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Wu JL, 2021, PROC CVPR IEEE, P12347, DOI 10.1109/CVPR46437.2021.01217
   Xu JR, 2019, IEEE I CONF COMP VIS, P3987, DOI 10.1109/ICCV.2019.00409
   Yang F, 2023, IEEE WINT CONF APPL, P4788, DOI 10.1109/WACV56688.2023.00478
   Yang JY, 2023, Arxiv, DOI [arXiv:2304.11968, DOI 10.48550/ARXIV.2304.11968]
   Yu FW, 2016, LECT NOTES COMPUT SC, V9914, P36, DOI 10.1007/978-3-319-48881-3_3
   Zeng FG, 2022, LECT NOTES COMPUT SC, V13687, P659, DOI 10.1007/978-3-031-19812-0_38
   Zhang YF, 2022, LECT NOTES COMPUT SC, V13682, P1, DOI 10.1007/978-3-031-20047-2_1
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
   Zhichao Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14656, DOI 10.1109/CVPR42600.2020.01468
   Zhongdao Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P107, DOI 10.1007/978-3-030-58621-8_7
   Zhu X., 2020, ARXIV
NR 50
TC 0
Z9 0
U1 7
U2 7
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104064
DI 10.1016/j.jvcir.2024.104064
EA FEB 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LK6Q7
UT WOS:001186737800001
DA 2024-08-05
ER

PT J
AU Song, TY
   Li, PP
   Fan, SM
   Jin, JY
   Jin, GY
   Fan, L
AF Song, Tianyu
   Li, Pengpeng
   Fan, Shumin
   Jin, Jiyu
   Jin, Guiyue
   Fan, Lei
TI Exploring a context-gated network for effective image deraining
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Single image deraining; Image restoration; Context enhancement; Gated
   aggregation; Multi-scale features
ID REMOVAL; RAIN
AB The existing deraining methods have obtained noteworthy improvements, but it is a challenging problem to extend the methods for complicated rain conditions where rain streaks exhibit different distribution densities, sizes, shapes, etc. The main challenges are the ability to fully explore and utilize the multi-scale context information of rain streaks that maintain both global structure completeness and local detail accurateness. To this end, this paper proposes an Exploring Context-Gated Network, known as ECGNet. To adequately explore the richer context information, the proposed method consists of two key elements: context-enhanced feature block (CEFB) and multi-scale-gated aggregation block (MGAB). Specifically, the various scale features can be captured by CEFB with the multi-scale operation, to better remove the rain streaks and effectively restore the local detail textures. Subsequently, the captured features from different spaces are sent to MGAB, to aggregate and transmit these different scale features from the encoder to the decoder and reduce the transmission loss of information. Massive experiments on the commonly used benchmarks have demonstrated that the proposed method obtains more appealing performances against other competitive methods.
C1 [Song, Tianyu; Li, Pengpeng; Fan, Shumin; Jin, Jiyu; Jin, Guiyue; Fan, Lei] Dalian Polytech Univ, Sch Informat Sci & Engn, Dalian 116034, Peoples R China.
C3 Dalian Polytechnic University
RP Jin, JY (corresponding author), Dalian Polytech Univ, Sch Informat Sci & Engn, Dalian 116034, Peoples R China.
EM songtienyu@163.com; 1310951990@qq.com; shuminfan@163.com;
   jiyu.jin@dlpu.edu.cn; guiyue.jin@dlpu.edu.cn; fanlei@dlpu.edu.cn
OI Song, Tianyu/0000-0002-5546-2363
FU Scientific Research Project of the Education Department of Liaoning
   Province, China [LJKZ0518, LJKZ0519]
FX <BOLD>Acknowledgments</BOLD> This work was supported in part by the
   Scientific Research Project of the Education Department of Liaoning
   Province, China (LJKZ0518, LJKZ0519) .
CR Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chen LY, 2022, LECT NOTES COMPUT SC, V13667, P17, DOI 10.1007/978-3-031-20071-7_2
   Chen X, 2021, IEEE COMPUT SOC CONF, P872, DOI 10.1109/CVPRW53098.2021.00097
   Cho SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4621, DOI 10.1109/ICCV48922.2021.00460
   Ding XH, 2016, MULTIMED TOOLS APPL, V75, P2697, DOI 10.1007/s11042-015-2657-7
   Dollár P, 2009, PROC CVPR IEEE, P304, DOI 10.1109/CVPRW.2009.5206631
   Fan ZW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1751, DOI 10.1145/3240508.3240694
   Fu XY, 2023, IEEE T PATTERN ANAL, V45, P9534, DOI 10.1109/TPAMI.2023.3241756
   Fu XY, 2020, IEEE T NEUR NET LEAR, V31, P1794, DOI 10.1109/TNNLS.2019.2926481
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Guo Meng-Hao, 2022, Advances in neural information processing systems
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Hou QB, 2020, PROC CVPR IEEE, P4002, DOI 10.1109/CVPR42600.2020.00406
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang HB, 2021, PROC CVPR IEEE, P7728, DOI 10.1109/CVPR46437.2021.00764
   Jiang Kui, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P827, DOI 10.1145/3503161.3547760
   Jiang K, 2023, IEEE T NEUR NET LEAR, V34, P3594, DOI 10.1109/TNNLS.2021.3112235
   Jiang K, 2021, IEEE T IMAGE PROCESS, V30, P7404, DOI 10.1109/TIP.2021.3102504
   Jiang K, 2021, IEEE T CIRC SYST VID, V31, P3981, DOI 10.1109/TCSVT.2020.3044887
   Jin X, 2019, IEEE IMAGE PROC, P2761, DOI [10.1109/icip.2019.8803238, 10.1109/ICIP.2019.8803238]
   Kui Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8343, DOI 10.1109/CVPR42600.2020.00837
   Kulkarni A., 2022, EUR C COMP VIS SPRIN, P344
   Li G, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1056, DOI 10.1145/3240508.3240636
   Li PP, 2022, IEEE COMPUT SOC CONF, P4275, DOI 10.1109/CVPRW56347.2022.00473
   Li R., 2017, IEEE C COMPUT VIS PA
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li Y., 2023, IEEECVF C COMPUTER V, P4198
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Liu Huajun, 2021, arXiv
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388
   Mou C, 2022, PROC CVPR IEEE, P17378, DOI 10.1109/CVPR52688.2022.01688
   Qian R, 2018, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2018.00263
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sen Deng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14548, DOI 10.1109/CVPR42600.2020.01457
   Song TY, 2023, IEEE INT CON MULTI, P1889, DOI 10.1109/ICME55011.2023.00324
   Song TY, 2023, IEEE GEOSCI REMOTE S, V20, DOI 10.1109/LGRS.2023.3319832
   Tang F, 2022, PROC CVPR IEEE, P8731, DOI 10.1109/CVPR52688.2022.00854
   Wang C, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1643, DOI 10.1145/3394171.3413820
   Wang H, 2019, Arxiv, DOI arXiv:1909.08326
   Wang H, 2020, PROC CVPR IEEE, P3100, DOI 10.1109/CVPR42600.2020.00317
   Wang Q, 2022, IEEE SIGNAL PROC LET, V29, P159, DOI 10.1109/LSP.2021.3129667
   Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Wei W, 2019, PROC CVPR IEEE, P3872, DOI 10.1109/CVPR.2019.00400
   Wu YH, 2023, IEEE T PATTERN ANAL, V45, P12760, DOI 10.1109/TPAMI.2022.3202765
   Xiao J, 2023, IEEE T PATTERN ANAL, V45, P12978, DOI 10.1109/TPAMI.2022.3183612
   Yang WH, 2020, AAAI CONF ARTIF INTE, V34, P12629
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yang Y., 2020, INT C MULTIMEDIA EXP, P1
   Yasarla R, 2020, PROC CVPR IEEE, P2723, DOI 10.1109/CVPR42600.2020.00280
   Ye YT, 2022, PROC CVPR IEEE, P5811, DOI 10.1109/CVPR52688.2022.00573
   Yi QS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4218, DOI 10.1109/ICCV48922.2021.00420
   Yin HT, 2022, IEEE SIGNAL PROC LET, V29, P747, DOI 10.1109/LSP.2022.3154981
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhong YQ, 2022, PROC CVPR IEEE, P15324, DOI 10.1109/CVPR52688.2022.01491
   Zhou JC, 2023, INT J COMPUT VISION, DOI 10.1007/s11263-023-01853-3
   Zhou JC, 2023, IEEE J OCEANIC ENG, V48, P1322, DOI 10.1109/JOE.2023.3275615
   Zhou JC, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3293912
   Zhou JC, 2023, ENG APPL ARTIF INTEL, V121, DOI 10.1016/j.engappai.2023.105946
   Zhou JC, 2023, ENG APPL ARTIF INTEL, V121, DOI 10.1016/j.engappai.2023.105952
   Zhou JC, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2022.3170702
NR 69
TC 1
Z9 1
U1 2
U2 2
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104060
DI 10.1016/j.jvcir.2024.104060
EA JAN 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JC6K1
UT WOS:001170994200001
DA 2024-08-05
ER

PT J
AU Yang, S
   Wang, ZB
   Wang, GA
   Ke, YZ
   Qin, F
   Guo, J
   Chen, LM
AF Yang, Shuai
   Wang, Zibei
   Wang, Guangao
   Ke, Yongzhen
   Qin, Fan
   Guo, Jing
   Chen, Liming
TI A self-supervised image aesthetic assessment combining masked image
   modeling and contrastive learning
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Image aesthetic assessment; Self-supervised learning; Masked image
   modeling
AB Learning more abundant image features helps improve the image aesthetic assessment task performance. Masked Image Modeling (MIM) is implemented based on the Vision Transformer (ViT), which learns pixel-level features while reconstructing images. Contrastive learning pulls in the same image features while pushing away different image features in the feature space to learn high-level semantic features. Since contrastive learning and MIM capture different levels of image features, combining these two methods could learn more rich feature representations and thus promote the performance of aesthetic assessment. Therefore, we propose a pretext task combining contrastive learning and MIM with learning richer image features. In this approach, the original image is randomly masked and reconstructed on the online network. The reconstructed and original images composition the positive example to calculate the contrastive loss on the target network. In the experiment on the AVA dataset, our method obtained better performance than the baseline.
C1 [Yang, Shuai; Wang, Zibei; Wang, Guangao; Ke, Yongzhen; Guo, Jing] Tiangong Univ, Sch Comp Sci & Technol, Tianjin, Peoples R China.
   [Yang, Shuai; Ke, Yongzhen; Guo, Jing] Tianjin Key Lab Autonomous Intelligence Technol &, Tianjin, Peoples R China.
   [Qin, Fan] Nankai Univ, Business Sch, Tianjin, Peoples R China.
   [Ke, Yongzhen] Tiangong Univ, Natl Demonstrat Ctr Expt Engn Training Educ, Tianjin, Peoples R China.
   [Chen, Liming] Fitow Tianjin Detect Technol Co LTD, Tianjin, Peoples R China.
C3 Tiangong University; Nankai University; Tiangong University
RP Ke, YZ (corresponding author), Tiangong Univ, Sch Comp Sci & Technol, Tianjin, Peoples R China.
EM yangshuai@tiangong.edu.cn; 15032797455@163.com; 763915020@qq.com;
   keyongzhen@tiangong.edu.cn; Qinfan@nankai.edu.cn;
   guojing@tiangong.edu.cn; liming.chen@fitow.com
OI ke, yongzhen/0000-0002-2792-8728
CR Bao H., 2021, arXiv preprint arXiv:2106.08254, DOI DOI 10.48550/ARXIV.2106.08254
   Chen M, 2020, PR MACH LEARN RES, V119
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen XK, 2022, Arxiv, DOI arXiv:2202.03026
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Ching JH, 2020, IEEE IMAGE PROC, P2246, DOI [10.1109/ICIP40778.2020.9191130, 10.1109/icip40778.2020.9191130]
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Grill J-B, 2020, PROC 34 INT C NEURAL
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   He S.Q., 2023, P 31 ACM INT C MULT
   Hessel M, 2018, AAAI CONF ARTIF INTE, P3215
   Jing YC, 2021, PROC CVPR IEEE, P15704, DOI 10.1109/CVPR46437.2021.01545
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   Pfister J, 2021, IEEE COMPUT SOC CONF, P816, DOI 10.1109/CVPRW53098.2021.00091
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Sheng K., 2020, P AAAI C ART INT
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   van Hasselt H, 2018, Arxiv, DOI arXiv:1812.02648
   Xie ZD, 2022, PROC CVPR IEEE, P9643, DOI 10.1109/CVPR52688.2022.00943
   Zhou JH, 2022, Arxiv, DOI arXiv:2111.07832
NR 23
TC 0
Z9 0
U1 2
U2 2
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104184
DI 10.1016/j.jvcir.2024.104184
EA MAY 2024
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UH3G1
UT WOS:001247120100001
DA 2024-08-05
ER

PT J
AU Wang, SS
   Xu, DW
   He, SH
AF Wang, Shanshan
   Xu, Dawen
   He, Songhan
TI Adaptive HEVC video steganograhpy based on PU partition modes
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Adaptive steganography; HEVC; PU partition mode; Syndrome-trellis code
ID CNN
AB High Efficiency Video Coding (HEVC) -based steganography has gained attention as a prominent research focus. Especially, block structure based HEVC video steganography has received increasing attention due to commendable performance. However, current block structure- based steganography algorithms confront with challenges such as reduced coding efficiency and limited capacity. To avoid these problems, an adaptive video steganography algorithm based on Prediction Unit (PU) partition mode in I-frames is proposed. This is done through the analysis of the block division process and the visual distortion resulting from the modification of the PU partition mode in HEVC. The PU block structure is utilized as steganographic covers, and the Rate Distortion Optimization (RDO) technique is introduced to establish an adaptive distortion function for Syndrome-trellis code (STC). Further comparison is performed between the proposed method and the state-of-the-art steganography algorithms, confirming its advantages in embedding capacity, compression efficiency, visual quality, and resistance to video steganalysis.
C1 [Wang, Shanshan] Anhui Univ Sci & Technol, Sch Math & Big Data, Huainan 232001, Peoples R China.
   [Xu, Dawen] Ningbo Univ Technol, Sch Cyber Sci & Engn, Ningbo 315211, Peoples R China.
   [He, Songhan] Ningbo Univ, Coll Informat Sci & Engn, Ningbo 315211, Peoples R China.
C3 Anhui University of Science & Technology; Ningbo University of
   Technology; Ningbo University
RP Xu, DW (corresponding author), Ningbo Univ Technol, Sch Cyber Sci & Engn, Ningbo 315211, Peoples R China.
EM dawenxu@126.com
RI he, songhan/KVY-0154-2024
OI xu, dawen/0000-0002-9619-8407
FU National Natural Science Foundation of China [62071267]; Ningbo Natural
   Science Foundation [2023J022]
FX This work is supported by the National Natural Science Foundation of
   China (62071267) , Ningbo Natural Science Foundation (2023J022) .
CR Chang PC, 2014, J VIS COMMUN IMAGE R, V25, P239, DOI 10.1016/j.jvcir.2013.10.007
   Chen YL, 2021, IEEE T DEPEND SECURE, V18, P1320, DOI 10.1109/TDSC.2019.2932983
   Dai HJ, 2024, IEEE T CIRC SYST VID, V34, P2663, DOI 10.1109/TCSVT.2023.3309861
   Dong Y, 2023, IEEE T DEPEND SECURE, V20, P769, DOI 10.1109/TDSC.2022.3144139
   Dong Y, 2023, IEEE T MULTIMEDIA, V25, P2698, DOI 10.1109/TMM.2022.3150180
   Dong Y, 2017, LECT NOTES COMPUT SC, V10431, P149, DOI 10.1007/978-3-319-64185-0_12
   Filler T, 2011, IEEE T INF FOREN SEC, V6, P920, DOI 10.1109/TIFS.2011.2134094
   He SH, 2024, J VIS COMMUN IMAGE R, V98, DOI 10.1016/j.jvcir.2023.103995
   He SH, 2024, IEEE T MULTIMEDIA, V26, P687, DOI 10.1109/TMM.2023.3269663
   He SH, 2022, J VIS COMMUN IMAGE R, V87, DOI 10.1016/j.jvcir.2022.103549
   Li ZH, 2023, IEEE T DEPEND SECURE, V20, P606, DOI 10.1109/TDSC.2022.3140899
   Li ZH, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11081015
   Lin Y., 2022, J. Electron. Imag., V31
   Liu JD, 2022, IEEE T MULTIMEDIA, V24, P2084, DOI 10.1109/TMM.2021.3075858
   Liu Y., 2022, J. Electron. Imaging, V31
   Liu YX, 2019, MULTIMED TOOLS APPL, V78, P6459, DOI 10.1007/s11042-018-6320-y
   Meng L., IEEE Trans. Multimedia, DOI 10.1109./TMM.2023.3344357
   [盛琪 Sheng Qi], 2017, [光电子·激光, Journal of Optoelectronics·Laser], V28, P433
   Tew Y, 2014, IEEE IMAGE PROC, P5502, DOI 10.1109/ICIP.2014.7026113
   [王家骥 Wang Jiaji], 2014, [光电子·激光, Journal of Optoelectronics·Laser], V25, P1578
   Wang J, 2019, IEEE ACCESS, V7, P119393, DOI 10.1109/ACCESS.2019.2936614
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang J, 2018, MULTIMED TOOLS APPL, V77, P11979, DOI 10.1007/s11042-017-4844-1
   Yang L, 2024, IEEE T MULTIMEDIA, V26, P4255, DOI 10.1109/TMM.2023.3321496
   Yang L, 2023, J INF SECUR APPL, V73, DOI 10.1016/j.jisa.2023.103442
   Yang YY, 2019, MULTIMED TOOLS APPL, V78, P8423, DOI 10.1007/s11042-018-6859-7
   Zhai LM, 2020, IEEE T INF FOREN SEC, V15, P1762, DOI 10.1109/TIFS.2019.2949428
   Zhang ZZ, 2021, EURASIP J IMAGE VIDE, V2021, DOI 10.1186/s13640-021-00547-5
   Zhao Y., 2015, INT WORKSH DIG FOR W, P25
NR 29
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104176
DI 10.1016/j.jvcir.2024.104176
EA MAY 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SN8U1
UT WOS:001235230300001
DA 2024-08-05
ER

PT J
AU Qin, KS
   Liu, D
   Wang, F
   Zhou, JC
   Yang, JX
   Zhang, WS
AF Qin, Ken Sinkou
   Liu, Di
   Wang, Fei
   Zhou, Jingchun
   Yang, Jiaxuan
   Zhang, Weishi
TI Improved YOLOv7 model for underwater sonar image object detection
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Underwater object detection; Sonar image; Criss-Cross Attention; Model
   pruning; YOLOv7 model
AB Underwater object detection technology for sonar images is widely employed and in high demand for civil and military purposes. However, due to the inhomogeneity of the seawater medium, which causes attenuation and distortion of the acoustic signal, achieving ideal performance for sonar object detection approaches is challenging. Furthermore, the process of acoustic wave transmission is further complicated by floating objects and particles, resulting in increased multipath effects. This study proposes an object detection method named YOLOv7C, which is based on deep learning to address these challenges. An attention mechanism is incorporated into the backbone network of the model to improve its ability to handle complex backgrounds in sonar images and effectively extract features. In addition, to facilitate high-order interaction between key features, the Neck part of the network integrates Multi-GnBlock blocks. Model redundancy pruning is used to substantially reduce the model size while maintaining high detection accuracy, thereby enhancing real-time performance. The proposed YOLOv7C achieves a 1.9% increase in the mean average precision, reduces the model memory by 47.50% after pruning, and enhances the detection speed by nearly 2.5 times compared with the YOLOv7C. These findings indicate that the success rate of multi-object detection is significantly enhanced by the attention mechanism and the new module. Additionally, the model can be controlled within a reasonable size by employing appropriate pruning methods.
C1 [Qin, Ken Sinkou; Liu, Di; Wang, Fei; Zhou, Jingchun; Zhang, Weishi] Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian 116026, Peoples R China.
   [Yang, Jiaxuan] Dalian Maritime Univ, Nav Coll, Dalian, Peoples R China.
C3 Dalian Maritime University; Dalian Maritime University
RP Zhang, WS (corresponding author), Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian 116026, Peoples R China.
EM qinkenjaist@icloud.com; Dino@dlmu.edu.cn; teesiv@dlmu.edu.cn
RI Zhou, Jingchun/AAF-6817-2019
OI Zhou, Jingchun/0000-0002-4111-6240; Qin, Ken/0000-0002-6047-9899
FU National Natural Science Foundation of China [62103072, 62301105];
   Fundamental Research Funds for the Central Universities, China
   [017210314]; Dalian Excellent Youth Talent Fund Project [2022RY23]
FX <STRONG> </STRONG>This work was funded by the National Natural Science
   Foundation of China, grant number 62103072 and 62301105, the Fundamental
   Research Funds for the Central Universities, China, grant number
   017210314 and Dalian Excellent Youth Talent Fund Project, grant number
   2022RY23.
CR Anguelov D., 2015, SSD: Single shot MultiBox detector
   Bochkovskiy A., 2020, ARXIV, DOI [10.48550/ARXIV.2004.10934, DOI 10.48550/ARXIV.2004.10934]
   Chen Q, 2021, PROC CVPR IEEE, P13034, DOI 10.1109/CVPR46437.2021.01284
   Chen RY, 2022, OCEANS-IEEE, DOI 10.1109/OCEANS47191.2022.9976986
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fang GF, 2023, Arxiv, DOI arXiv:2301.12900
   Galusha A, 2019, PROC SPIE, V11012, DOI 10.1117/12.2519521
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang ZL, 2023, IEEE T PATTERN ANAL, V45, P6896, DOI 10.1109/TPAMI.2020.3007032
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li WX, 2015, CHIN CONTR CONF, P3870, DOI 10.1109/ChiCC.2015.7260236
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298
   Meng FQ, 2013, PROCEEDINGS 2013 INTERNATIONAL CONFERENCE ON MECHATRONIC SCIENCES, ELECTRIC ENGINEERING AND COMPUTER (MEC), P1187, DOI 10.1109/MEC.2013.6885245
   Rao YM, 2022, Arxiv, DOI arXiv:2207.14284
   Redmon J., 2018, CoRR
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shaw P., 2018, ARXIV
   Vaswani A, 2023, Arxiv, DOI arXiv:1706.03762
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang F., 2022, J. Sens.
   Wang F, 2022, J ELECTRON INF TECHN, V44, P3419, DOI 10.11999/JEIT220260
   Xie KB, 2022, SCI DATA, V9, DOI 10.1038/s41597-022-01854-w
   Zhao WL, 2013, IEEE T IMAGE PROCESS, V22, P980, DOI 10.1109/TIP.2012.2226043
   Zhou XM, 2016, 2016 2ND INTERNATIONAL CONFERENCE ON INDUSTRIAL INFORMATICS - COMPUTING TECHNOLOGY, INTELLIGENT TECHNOLOGY, INDUSTRIAL INFORMATION INTEGRATION (ICIICII), P73, DOI [10.1109/ICIICII.2016.0029, 10.1109/ICIICII.2016.82]
   Zhu L, 2014, 2014 TENTH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY (CIS), P171, DOI 10.1109/CIS.2014.67
NR 30
TC 1
Z9 1
U1 5
U2 5
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104124
DI 10.1016/j.jvcir.2024.104124
EA MAR 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PZ8K0
UT WOS:001217988500001
DA 2024-08-05
ER

PT J
AU Shehin, AU
   Sankar, D
AF Shehin, A. U.
   Sankar, Deepa
TI Copy Move Forgery detection and localisation robust to rotation using
   block based Discrete Cosine Transform and eigenvalues
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Digital image forensics; Copy Move Forgery detection; Anti-forensics;
   Discrete Cosine Transform; Post-processing; Eigenvalues
ID DIGITAL IMAGES; SIFT; SURF
AB The contemporary era faces a widespread issue with digital image forgery, posing a significant challenge due to its ease and the broad reach enabled by high-speed internet. This manipulation of images carries substantial socio-political implications globally. Hence, robust digital image forensic methods are critical for detecting such forgeries. This article presents an innovative algorithm specifically designed to detect and locate copy -move duplication within digital images. By utilising the Discrete Cosine Transform and eigenvalues as distinguishing features, the algorithm precisely identifies and pinpoints replicated image regions from overlapping pixel blocks. Uniquely, another cumulative DCT feature enhances the algorithm's ability to discern duplicated regions, even when subjected to post -processing rotation attacks. Experiments using various datasets demonstrate that this method outperforms avant-garde techniques in detecting and localising forgeries, showcasing promising results. This approach contributes significantly to the field of digital image forensics, providing a valuable tool for identifying and localising manipulated content.
C1 [Shehin, A. U.; Sankar, Deepa] Cochin Univ Sci & Technol, Sch Engn, Div Elect Engn, Kochi 682022, Kerala, India.
C3 Cochin University Science & Technology
RP Shehin, AU (corresponding author), Cochin Univ Sci & Technol, Sch Engn, Div Elect Engn, Kochi 682022, Kerala, India.
EM shehinau@cusat.ac.in; deepasankar@cusat.ac.in
CR Abd Warif NB, 2017, J VIS COMMUN IMAGE R, V46, P219, DOI 10.1016/j.jvcir.2017.04.004
   Ahmed S, 2023, MULTIMED TOOLS APPL, V82, P43945, DOI 10.1007/s11042-023-14835-x
   Al-Hammadi MM, 2016, IEEE INT SYM MULTIM, P341, DOI [10.1109/ISM.2016.91, 10.1109/ISM.2016.0075]
   Al-Qershi OM, 2018, MULTIMED TOOLS APPL, V77, P31807, DOI 10.1007/s11042-018-6201-4
   Alberry Hesham A., 2018, Future Computing and Informatics Journal, V3, P159, DOI 10.1016/j.fcij.2018.03.001
   Alkawaz MH, 2018, NEURAL COMPUT APPL, V30, P183, DOI 10.1007/s00521-016-2663-3
   Amerini I, 2011, IEEE T INF FOREN SEC, V6, P1099, DOI 10.1109/TIFS.2011.2129512
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Baziyad M, 2021, MULTIMED TOOLS APPL, V80, P8611, DOI 10.1007/s11042-020-10008-2
   Bute S., 2014, Soc. Media Politics: Case Stud. Political Power Soc. Media, P355
   Cao G, 2010, IEEE INT CON MULTI, P89, DOI 10.1109/ICME.2010.5583869
   Chen CC, 2019, MULTIMED TOOLS APPL, V78, P18293, DOI 10.1007/s11042-019-7165-8
   Christlein V, 2012, IEEE T INF FOREN SEC, V7, P1841, DOI 10.1109/TIFS.2012.2218597
   Cozzolino D, 2015, IEEE T INF FOREN SEC, V10, P2284, DOI 10.1109/TIFS.2015.2455334
   Crisan D, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app112311482
   de Araújo SA, 2011, INTEGR COMPUT-AID E, V18, P75, DOI 10.3233/ICA-2011-0358
   Ding F, 2019, MULTIMED TOOLS APPL, V78, P8225, DOI 10.1007/s11042-018-6807-6
   Dong CB, 2023, IEEE T PATTERN ANAL, V45, P3539, DOI 10.1109/TPAMI.2022.3180556
   Dougherty E. R., 2020, Digital image processing methods
   Elaskily MA, 2020, MULTIMED TOOLS APPL, V79, P19167, DOI 10.1007/s11042-020-08751-7
   Fadl SM, 2017, NEUROCOMPUTING, V265, P57, DOI 10.1016/j.neucom.2016.11.091
   Ferreira WD, 2020, COMPUT ELECTR ENG, V85, DOI 10.1016/j.compeleceng.2020.106685
   Fridrich J., 2003, DIG FOR RES WORKSH, V3, P652
   Gani G, 2021, MULTIMED TOOLS APPL, V80, P32219, DOI 10.1007/s11042-021-11174-7
   Gani G, 2020, J INF SECUR APPL, V54, DOI 10.1016/j.jisa.2020.102510
   Guo XX, 2005, Fifth International Conference on Computer and Information Technology - Proceedings, P619
   Gupta A., 2013, Int. J. Sci. Res. Publ, V3, P1
   Gupta S, 2022, ARTIF INTELL REV, V55, P1629, DOI 10.1007/s10462-021-10046-8
   Hilal A, 2017, 2017 SENSORS NETWORKS SMART AND EMERGING TECHNOLOGIES (SENSET)
   Huang YP, 2011, FORENSIC SCI INT, V206, P178, DOI 10.1016/j.forsciint.2010.08.001
   Jie Hu, 2011, 2011 Second International Conference on Networking and Distributed Computing, P23, DOI 10.1109/ICNDC.2011.12
   KHOTANZAD A, 1990, IEEE T PATTERN ANAL, V12, P489, DOI 10.1109/34.55109
   Kumar S, 2023, MULTIMED TOOLS APPL, V82, P1431, DOI 10.1007/s11042-022-12391-4
   Li J, 2015, IEEE T INF FOREN SEC, V10, P507, DOI 10.1109/TIFS.2014.2381872
   Li QW, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-19325-y
   Li YM, 2019, IEEE T INF FOREN SEC, V14, P1307, DOI 10.1109/TIFS.2018.2876837
   Li YA, 2013, FORENSIC SCI INT, V224, P59, DOI 10.1016/j.forsciint.2012.10.031
   Liu YQ, 2018, MULTIMED TOOLS APPL, V77, P18269, DOI 10.1007/s11042-017-5374-6
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mahfoudi G, 2019, EUR SIGNAL PR CONF
   Mahmood T, 2018, J VIS COMMUN IMAGE R, V53, P202, DOI 10.1016/j.jvcir.2018.03.015
   Mahmood T, 2017, FORENSIC SCI INT, V279, P8, DOI 10.1016/j.forsciint.2017.07.037
   Meena K. B., 2021, ADV COMP DAT SCI 5 I, P364
   Meena KB, 2020, J INF SECUR APPL, V52, DOI 10.1016/j.jisa.2020.102481
   Mehta V., 2019, FUT TRENDS NETW COMP, V2, P532
   Niyishaka P, 2020, MULTIMED TOOLS APPL, V79, P26045, DOI 10.1007/s11042-020-09225-6
   Paul KH, 2020, ADV INTELL SYST COMP, V944, P234, DOI 10.1007/978-3-030-17798-0_20
   Piva A., 2013, Int. Sch. Res. Notices, V2013
   Popescu A.C., 2004, Comput. Sci. Tech. Rep, P1
   Prakash CS, 2019, MULTIMED TOOLS APPL, V78, P23535, DOI 10.1007/s11042-019-7629-x
   Qureshi MA, 2019, IET IMAGE PROCESS, V13, P1811, DOI 10.1049/iet-ipr.2018.6587
   Rosin PL, 2010, COMPUT VIS IMAGE UND, V114, P790, DOI 10.1016/j.cviu.2010.02.005
   Ryu SJ, 2010, LECT NOTES COMPUT SC, V6387, P51, DOI 10.1007/978-3-642-16435-4_5
   Sadu C, 2022, TENCON 2022 2022 IEE, P1, DOI [10.1109/TENCON55691.2022.9977490, DOI 10.1109/TENCON55691.2022.9977490]
   Samir S, 2020, INFORMATION, V11, DOI 10.3390/info11050275
   Sánchez J, 2018, IMAGE PROCESS ON LIN, V8, P305, DOI 10.5201/ipol.2018.229
   Shehin AU, 2022, INT CONF SYST SIGNAL, DOI 10.1109/IWSSIP55020.2022.9854436
   Shehin A.U., 2023, 2023 9 INT C SMART C, P162, DOI [10.1109/ICSCC59169.2023.10335059, DOI 10.1109/ICSCC59169.2023.10335059]
   Shen CH, 2019, NEW MEDIA SOC, V21, P438, DOI 10.1177/1461444818799526
   Silva E, 2015, J VIS COMMUN IMAGE R, V29, P16, DOI 10.1016/j.jvcir.2015.01.016
   Tahaoglu G, 2022, MULTIMED TOOLS APPL, V81, P22867, DOI 10.1007/s11042-021-11503-w
   Teerakanok S, 2019, IEEE ACCESS, V7, P40550, DOI 10.1109/ACCESS.2019.2907316
   Thakur R, 2020, FORENSIC SCI INT, V312, DOI 10.1016/j.forsciint.2020.110311
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Tralic Dijana, 2013, Proceedings of the 2013 55th International Symposium. ELMAR-2013, P49
   Tralic D, 2014, INT CONF SYST SIGNAL, P167
   Uliyan DM, 2016, SYMMETRY-BASEL, V8, DOI 10.3390/sym8070062
   Ustubioglu B, 2016, AEU-INT J ELECTRON C, V70, P1076, DOI 10.1016/j.aeue.2016.05.005
   Ustubioglu B, 2015, 2015 9TH INTERNATIONAL CONFERENCE ON ELECTRICAL AND ELECTRONICS ENGINEERING (ELECO), P185, DOI 10.1109/ELECO.2015.7394438
   Wen BH, 2016, IEEE IMAGE PROC, P161, DOI 10.1109/ICIP.2016.7532339
   Woods R. E., 2008, Digital Image Processing
   Wu Y, 2018, LECT NOTES COMPUT SC, V11210, P170, DOI 10.1007/978-3-030-01231-1_11
   Wu Y, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1480, DOI 10.1145/3123266.3123411
   Xuefeng Hu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P312, DOI 10.1007/978-3-030-58589-1_19
   Yang F, 2017, ENG APPL ARTIF INTEL, V59, P73, DOI 10.1016/j.engappai.2016.12.022
   Yang HY, 2019, MULTIMED TOOLS APPL, V78, P34585, DOI 10.1007/s11042-019-08169-w
   Yang JX, 2021, DIGIT SIGNAL PROCESS, V113, DOI 10.1016/j.dsp.2021.103032
   Yang PP, 2020, J IMAGING, V6, DOI 10.3390/jimaging6030009
   Zhang B, 2008, IEEE T IMAGE PROCESS, V17, P664, DOI 10.1109/TIP.2008.919949
   Zhu Y, 2020, IEEE T IND INFORM, V16, P6714, DOI 10.1109/TII.2020.2982705
NR 81
TC 2
Z9 2
U1 2
U2 2
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104075
DI 10.1016/j.jvcir.2024.104075
EA FEB 2024
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KY4U3
UT WOS:001183523200001
DA 2024-08-05
ER

PT J
AU Cao, G
   Wang, YQ
   Zhu, HC
   Lou, ZJ
   Yu, LF
AF Cao, Gang
   Wang, Yuqi
   Zhu, Haochen
   Lou, Zijie
   Yu, Lifang
TI Transferable adversarial attack on image tampering localization
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Anti-forensics; Adversarial example; Optimization-based adversarial
   attack; Fast gradient sign method; Image tampering localization
ID EXAMPLES
AB It is significant to evaluate the security of existing digital image tampering localization algorithms in real-world applications. In this paper, we propose an adversarial attack scheme to reveal the reliability of such deep learning-based tampering localizers, which would be fooled and fail to predict altered regions correctly. Specifically, two practical adversarial example methods are presented in a unified attack framework. In the optimization-based adversarial attack, the victim image forgery is treated as the parameter to be optimized via Adam optimizer. In the gradient-based adversarial attack, the invisible perturbation yielded by Fast Gradient Sign Method (FGSM) is added to the tampered image along gradient ascent direction. The black-box attack is achieved by relying on the transferability of such adversarial examples to different localizers. Extensive experiments verify that our attacks can sharply reduce the tampering localization accuracy while preserving high visual quality for attacked images. Source code is available at https://github.com/multimediaFor/AttackITL.
C1 [Cao, Gang; Wang, Yuqi; Zhu, Haochen; Lou, Zijie] Commun Univ China, Sch Comp & Cyber Sci, Beijing 100024, Peoples R China.
   [Cao, Gang; Wang, Yuqi; Zhu, Haochen; Lou, Zijie] Commun Univ China, State Key Lab Media Convergence & Commun, Beijing 100024, Peoples R China.
   [Cao, Gang] Changsha Med Univ, Sch Informat Engn, Changsha 410219, Peoples R China.
   [Yu, Lifang] Beijing Inst Graph Commun, Beijing Key Lab Signal & Informat Proc High End Pr, Beijing 100026, Peoples R China.
C3 Communication University of China; Communication University of China;
   Changsha Medical University
RP Cao, G (corresponding author), Commun Univ China, Sch Comp & Cyber Sci, Beijing 100024, Peoples R China.
EM gangcao@cuc.edu.cn
FU Fundamental Research Funds for the Central Universities [62071434,
   62172005]; Fundamental Research Funds for the Central Universities
   [CUC24GT01]; CUC Public Computing Cloud
FX This work was supported in part by National Natural Science Foundation
   of China (62071434, 62172005) , Fundamental Research Funds for the
   Central Universities (CUC24GT01) , CUC Public Computing Cloud.
CR Avcibas I, 2002, J ELECTRON IMAGING, V11, P206, DOI 10.1117/1.1455011
   Barni M, 2019, INT CONF ACOUST SPEE, P8286, DOI [10.1109/ICASSP.2019.8683772, 10.1109/icassp.2019.8683772]
   Cao G, 2015, LECT NOTES COMPUT SC, V9314, P97, DOI 10.1007/978-3-319-24075-6_10
   Cao G, 2014, SCI CHINA INFORM SCI, V57, DOI 10.1007/s11432-013-4928-0
   Cao G, 2010, MM&SEC 2010: 2010 ACM SIGMM MULTIMEDIA AND SECURITY WORKSHOP, PROCEEDINGS, P25
   Chen XR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14165, DOI 10.1109/ICCV48922.2021.01392
   Chen ZP, 2017, IEEE INT WORKS INFOR
   Cozzolino D, 2020, IEEE T INF FOREN SEC, V15, P144, DOI 10.1109/TIFS.2019.2916364
   de Carvalho TJ, 2013, IEEE T INF FOREN SEC, V8, P1182, DOI 10.1109/TIFS.2013.2265677
   Fang S., 2022, arXiv
   GOODFELLOW I, 2014, ADV NEURAL INFORM PR, V27
   Güera D, 2017, IEEE COMPUT SOC CONF, P1840, DOI 10.1109/CVPRW.2017.230
   Guillaro F, 2023, PROC CVPR IEEE, P20606, DOI 10.1109/CVPR52729.2023.01974
   Huh M, 2018, LECT NOTES COMPUT SC, V11215, P106, DOI 10.1007/978-3-030-01252-6_7
   Jing Dong, 2013, 2013 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP), P422, DOI 10.1109/ChinaSIP.2013.6625374
   Kingma D. P., 2014, arXiv
   Kwon MJ, 2022, INT J COMPUT VISION, V130, P1875, DOI 10.1007/s11263-022-01617-5
   Li HD, 2018, IEEE T CIRC SYST VID, V28, P31, DOI 10.1109/TCSVT.2016.2599849
   Liu XH, 2022, IEEE T CIRC SYST VID, V32, P7505, DOI 10.1109/TCSVT.2022.3189545
   Liu YP, 2017, Arxiv, DOI arXiv:1611.02770
   Madry A, 2019, Arxiv, DOI arXiv:1706.06083
   Marra F, 2018, SIGNAL PROCESS-IMAGE, V65, P240, DOI 10.1016/j.image.2018.04.007
   Mayer O, 2020, IEEE J-STSP, V14, P1049, DOI 10.1109/JSTSP.2020.3001516
   Mo XB, 2023, PROCEEDINGS OF THE 2023 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, CCS 2023, P3552, DOI 10.1145/3576915.3624390
   Ng T.T., 2004, ADVENT Technical Report
   Novozámsky A, 2020, IEEE WINT CONF APPL, P71, DOI [10.1109/WACVW50321.2020.9096940, 10.1109/wacvw50321.2020.9096940]
   Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36
   Szegedy C, 2014, Arxiv, DOI arXiv:1312.6199
   Tondi B, 2018, ELECTRON LETT, V54, P1220, DOI 10.1049/el.2018.6469
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wen BH, 2016, IEEE IMAGE PROC, P161, DOI 10.1109/ICIP.2016.7532339
   Wu HW, 2022, IEEE T INF FOREN SEC, V17, P443, DOI 10.1109/TIFS.2022.3144878
   Wu Y, 2019, PROC CVPR IEEE, P9535, DOI 10.1109/CVPR.2019.00977
   Xie H, 2022, IEEE T CIRC SYST VID, V32, P1701, DOI 10.1109/TCSVT.2021.3068294
   Zheng LL, 2019, J VIS COMMUN IMAGE R, V58, P380, DOI 10.1016/j.jvcir.2018.12.022
NR 35
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD JUN
PY 2024
VL 102
AR 104210
DI 10.1016/j.jvcir.2024.104210
EA JUN 2024
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XJ6V8
UT WOS:001261365000001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wang, C
   Wu, ZW
   Ke, W
   Xiong, Z
AF Wang, Cui
   Wu, Zewei
   Ke, Wei
   Xiong, Zhang
TI A simple transformer-based baseline for crowd tracking with Sequential
   Feature Aggregation and Hybrid Group Training
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Crowd tracking; Transformer-based tracking; Temporal enhanced
   representation; Hybrid Group Training
ID MULTIPLE-OBJECT TRACKING; GRAPH
AB Tracking pedestrians in crowded scenes is a challenging task. Existing transformer -based tracking methods integrate detection and tracking into a unified model, which simplifies the tracking process. However, these methods also introduce complicated attention mechanisms that increase the model complexity and cost. To address this issue, we propose SOTTrack, a simple online transformer -based method for crowd tracking. Our method enhances feature learning and training strategies without sacrificing simplicity and efficiency. Specifically, we introduce the Sequential Feature Aggregation (SFA) module and the Hybrid Group Training (HGT) approach. The SFA module fuses features from sequential images to improve the temporal consistency of visual features within short time intervals. The HGT approach assigns different queries to multiple guided tasks, such as label assignment and de -noising, which are only used during training and do not incur any inference cost. We evaluate our method on the MOT17 and MOT20 datasets and demonstrate its competitive performance.
C1 [Wang, Cui; Wu, Zewei; Ke, Wei] Macao Polytech Univ, Fac Appl Sci, Macau 999078, Macao, Peoples R China.
   [Xiong, Zhang] Beihang Univ, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
C3 Macao Polytechnic University; Beihang University
RP Wu, ZW (corresponding author), Macao Polytech Univ, Fac Appl Sci, Macau 999078, Macao, Peoples R China.
EM zewei.wu@mpu.edu.mo
OI cui, wang/0000-0001-9601-7824
FU National Key Research and Development Program of China [2022YFB3306500];
   National Natural Science Foundation of China [62372023]; Science and
   Technology Development Fund, Macau SAR [0122/2023/AMJ]; Open Fund of the
   State Key Laboratory of Software Development Environment
   [SKLSDE-2023ZX-11]; HAWKEYE Group
FX This work is supported by the following funds: the National Key Research
   and Development Program of China under Grant 2022YFB3306500, the
   National Natural Science Foundation of China under Grant 62372023, the
   Science and Technology Development Fund, Macau SAR, under Grant
   0122/2023/AMJ, and the Open Fund of the State Key Laboratory of Software
   Development Environment under Grant SKLSDE-2023ZX-11. Thanks for the
   support from HAWKEYE Group.
CR Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Brasó G, 2020, PROC CVPR IEEE, P6246, DOI 10.1109/CVPR42600.2020.00628
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen Z, 2018, PR MACH LEARN RES, V80
   Chu P, 2023, IEEE WINT CONF APPL, P4859, DOI 10.1109/WACV56688.2023.00485
   Cioppa A, 2022, IEEE COMPUT SOC CONF, P3490, DOI 10.1109/CVPRW56347.2022.00393
   Dendorfer P., 2020, arXiv
   Doering A, 2022, PROC CVPR IEEE, P20931, DOI 10.1109/CVPR52688.2022.02029
   Dollár P, 2009, PROC CVPR IEEE, P304, DOI 10.1109/CVPRW.2009.5206631
   Dong L, 2022, IEEE T CONSUM ELECTR, V68, P307, DOI 10.1109/TCE.2022.3190384
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fabbri M, 2018, LECT NOTES COMPUT SC, V11208, P450, DOI 10.1007/978-3-030-01225-0_27
   Jia D, 2022, Arxiv, DOI arXiv:2207.13080
   Jinlong Peng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P145, DOI 10.1007/978-3-030-58548-8_9
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Kim C, 2015, IEEE I CONF COMP VIS, P4696, DOI 10.1109/ICCV.2015.533
   Kokkinos I, 2017, PROC CVPR IEEE, P5454, DOI 10.1109/CVPR.2017.579
   Li JC, 2020, IEEE T INTELL TRANSP, V21, P3634, DOI 10.1109/TITS.2019.2930310
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luiten J, 2021, INT J COMPUT VISION, V129, P548, DOI 10.1007/s11263-020-01375-2
   Lv K, 2021, IEEE T MULTIMEDIA, V23, P4198, DOI 10.1109/TMM.2020.3038311
   Lv K, 2020, IEEE T IMAGE PROCESS, V29, P5163, DOI 10.1109/TIP.2020.2980130
   Meinhardt T, 2022, PROC CVPR IEEE, P8834, DOI 10.1109/CVPR52688.2022.00864
   Milan A, 2016, Arxiv, DOI arXiv:1603.00831
   Pang B, 2020, PROC CVPR IEEE, P6307, DOI 10.1109/CVPR42600.2020.00634
   Pang JM, 2021, PROC CVPR IEEE, P164, DOI 10.1109/CVPR46437.2021.00023
   Paszke A, 2019, ADV NEUR IN, V32
   Qin ZY, 2023, IEEE T IMAGE PROCESS, V32, P6543, DOI 10.1109/TIP.2023.3328485
   Qin ZY, 2023, IEEE-CAA J AUTOMATIC, V10, P1192, DOI 10.1109/JAS.2023.123456
   Qin ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1884, DOI 10.1145/3474085.3475342
   Sener O, 2018, ADV NEUR IN, V31
   Shao S., 2018, arXiv
   Sheng H, 2021, IEEE INTERNET THINGS, V8, P2193, DOI 10.1109/JIOT.2020.3035415
   Sheng H, 2020, IEEE T CIRC SYST VID, V30, P2971, DOI 10.1109/TCSVT.2020.2988649
   Sun Peize, 2020, ABS201215460 CORR
   Sun SJ, 2021, IEEE T PATTERN ANAL, V43, P104, DOI 10.1109/TPAMI.2019.2929520
   Tokmakov P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10840, DOI 10.1109/ICCV48922.2021.01068
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang S, 2024, IEEE T IND INFORM, V20, P369, DOI [10.1109/TII.2023.3261890, 10.1080/09553002.2023.2267667]
   Wang S, 2022, IEEE T IMAGE PROCESS, V31, P5257, DOI 10.1109/TIP.2022.3192706
   Wang YX, 2021, IEEE INT CONF ROBOT, P13708, DOI 10.1109/ICRA48506.2021.9561110
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Wu JL, 2021, PROC CVPR IEEE, P12347, DOI 10.1109/CVPR46437.2021.01217
   Wu YB, 2023, IEEE INTERNET THINGS, V10, P4735, DOI 10.1109/JIOT.2022.3219627
   Wu Y, 2023, LECT NOTES COMPUT SC, V13847, P485, DOI 10.1007/978-3-031-26293-7_29
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Xingyi Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P474, DOI 10.1007/978-3-030-58548-8_28
   Xu Y, 2021, ARXIV
   Xu YN, 2021, INT EL DEVICES MEET, DOI 10.1109/IEDM19574.2021.9720659
   Zeng FG, 2022, LECT NOTES COMPUT SC, V13687, P659, DOI 10.1007/978-3-031-19812-0_38
   Zhang H, 2022, Arxiv, DOI [arXiv:2203.03605, DOI 10.48550/ARXIV.2203.03605, 10.48550/arXiv.2203.03605]
   Zhang SS, 2017, PROC CVPR IEEE, P4457, DOI 10.1109/CVPR.2017.474
   Zhang Y, 2020, IEEE INTERNET THINGS, V7, P7892, DOI 10.1109/JIOT.2020.2996609
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
   Zheng L, 2017, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2017.357
   Zhu J, 2018, LECT NOTES COMPUT SC, V11209, P379, DOI 10.1007/978-3-030-01228-1_23
   Zhu X., 2020, ARXIV
NR 58
TC 1
Z9 1
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104144
DI 10.1016/j.jvcir.2024.104144
EA APR 2024
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RQ4Z4
UT WOS:001229125500001
DA 2024-08-05
ER

PT J
AU Zhang, ZY
   Zhao, XF
   Cao, Y
AF Zhang, Zeyu
   Zhao, Xianfeng
   Cao, Yun
TI Unveiling tampering traces: Enhancing image reconstruction errors for
   visualization
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Digital image forensics; Reconstruction Error; Mixed generator;
   Tampering localization; Latent features
ID FORGERIES
AB Contemporary tampering detection models in digital image forensics often rely on established techniques such as SRM and ELA for revealing evidence of manipulation. Despite their widespread use, these methods often yield unsatisfactory visualizations. In order to enhance the visibility of tampering artifacts, we propose an innovative image representation called Tamper Reconstruction Error (TRE). TRE measures the error between an input image and its reconstructed counterpart using a pre -trained mixed generator. We observed that utilizing a model proficient in computational visual tasks to extract reconstruction errors did not clearly reveal tampering traces in manually manipulated images. To emphasize the more pronounced discrepancies in the reconstruction of tampered images, the image representation TRE is fed into two dedicated extractors designed to capture manipulation features in both the frequency and spatial domains. Throughout the learning process, these extractors adaptively express the essential forgery traces back into spatial domain. Furthermore, to validate the importance of the extracted errors in tampering localization, we introduced a localization annotator. This annotator integrates reconstruction errors at different stages during the encoding and decoding of latent features. Experimental results demonstrate that the integration of extracted features significantly improves the performance of tampering localization, outperforming other state-of-the-art localization frameworks.
C1 [Zhao, Xianfeng] Chinese Acad Sci, Inst Informat Engn, Beijing 100085, Peoples R China.
   Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100085, Peoples R China.
C3 Chinese Academy of Sciences; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS
RP Zhao, XF (corresponding author), Chinese Acad Sci, Inst Informat Engn, Beijing 100085, Peoples R China.
EM zhaoxianfeng@iie.ac.cn
FU National Key Technology Research and Development Program [2022QY0101];
   NSFC [62272456]
FX This work was supported by National Key Technology Research and
   Development Program under 2022QY0101, and NSFC under 62272456.
CR Abd Warif NB, 2015, INT CONF SYST ENG, P23, DOI 10.1109/ICSEngT.2015.7412439
   Bayar B, 2018, IEEE T INF FOREN SEC, V13, P2691, DOI 10.1109/TIFS.2018.2825953
   Bi XL, 2019, IEEE COMPUT SOC CONF, P30, DOI 10.1109/CVPRW.2019.00010
   Chen BJ, 2021, IEEE T MULTIMEDIA, V23, P3506, DOI 10.1109/TMM.2020.3026868
   Chi L., 2020, ADV NEURAL INF PROCE, V33, P4479, DOI DOI 10.5555/3495724.3496100
   Cozzolino D, 2020, IEEE T INF FOREN SEC, V15, P144, DOI 10.1109/TIFS.2019.2916364
   Cun XD, 2019, LECT NOTES COMPUT SC, V11130, P252, DOI 10.1007/978-3-030-11012-3_22
   de Carvalho TJ, 2013, IEEE T INF FOREN SEC, V8, P1182, DOI 10.1109/TIFS.2013.2265677
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dong CB, 2023, IEEE T PATTERN ANAL, V45, P3539, DOI 10.1109/TPAMI.2022.3180556
   Guan HY, 2019, IEEE WINT CONF APPL, P63, DOI 10.1109/WACVW.2019.00018
   Gunawan Teddy Surya, 2017, Indonesian Journal of Electrical Engineering and Computer Science, V7, P131
   Huh M, 2018, LECT NOTES COMPUT SC, V11215, P106, DOI 10.1007/978-3-030-01252-6_7
   Islam A, 2020, PROC CVPR IEEE, P4675, DOI 10.1109/CVPR42600.2020.00473
   Jing Dong, 2013, 2013 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP), P422, DOI 10.1109/ChinaSIP.2013.6625374
   Kniaz VV, 2019, ADV NEUR IN, V32, P215
   Korus P, 2017, IEEE T INF FOREN SEC, V12, P809, DOI 10.1109/TIFS.2016.2636089
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lu M, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9050858
   Lyu SW, 2014, INT J COMPUT VISION, V110, P202, DOI 10.1007/s11263-013-0688-y
   Mahfoudi G, 2019, EUR SIGNAL PR CONF
   Matern F, 2020, IEEE T INF FOREN SEC, V15, P1303, DOI 10.1109/TIFS.2019.2935913
   Mayer O, 2020, IEEE T INF FOREN SEC, V15, P1331, DOI 10.1109/TIFS.2019.2924552
   Novozámsky A, 2020, IEEE WINT CONF APPL, P71, DOI [10.1109/WACVW50321.2020.9096940, 10.1109/wacvw50321.2020.9096940]
   Pomari T, 2018, IEEE IMAGE PROC, P3788, DOI 10.1109/ICIP.2018.8451227
   Pun CM, 2016, J VIS COMMUN IMAGE R, V38, P195, DOI 10.1016/j.jvcir.2016.03.005
   Rao Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15014, DOI 10.1109/ICCV48922.2021.01476
   Rao Y, 2016, IEEE INT WORKS INFOR
   Roy AG, 2019, IEEE T MED IMAGING, V38, P540, DOI 10.1109/TMI.2018.2867261
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Verdoliva L, 2020, IEEE J-STSP, V14, P910, DOI 10.1109/JSTSP.2020.3002101
   Wang XY, 2021, IETE TECH REV, V38, P149, DOI 10.1080/02564602.2020.1782274
   Wang ZD, 2023, Arxiv, DOI arXiv:2303.09295
   Wen BH, 2016, IEEE IMAGE PROC, P161, DOI 10.1109/ICIP.2016.7532339
   Wu HW, 2022, IEEE T INF FOREN SEC, V17, P443, DOI 10.1109/TIFS.2022.3144878
   Wu Haiwei, 2023, arXiv
   Wu Y, 2019, PROC CVPR IEEE, P9535, DOI 10.1109/CVPR.2019.00977
   Xuefeng Hu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P312, DOI 10.1007/978-3-030-58589-1_19
   Yang C, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102825
   Yarlagadda Sri Kalyan, 2018, arXiv
   Zhou P, 2018, PROC CVPR IEEE, P1053, DOI 10.1109/CVPR.2018.00116
   Zhu Hanlin, 2023, ARXIV
   Zhu HC, 2024, J VIS COMMUN IMAGE R, V98, DOI 10.1016/j.jvcir.2023.103981
   Zhuang PY, 2021, IEEE T INF FOREN SEC, V16, P2986, DOI 10.1109/TIFS.2021.3070444
NR 44
TC 0
Z9 0
U1 2
U2 2
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104125
DI 10.1016/j.jvcir.2024.104125
EA MAR 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QR9Y8
UT WOS:001222728100001
DA 2024-08-05
ER

PT J
AU Wei, DB
   Xie, HJ
   Zhang, ZX
   Yan, TT
AF Wei, Debin
   Xie, Hongji
   Zhang, Zengxi
   Yan, Tiantian
TI Learning a Holistic-Specific color transformer with Couple Contrastive
   constraints for underwater image enhancement and beyond
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Underwater image enhancement; Underwater object detection; Contrastive
   learning
ID QUALITY ASSESSMENT; ENSEMBLE; MODEL
AB Underwater images suffer from different types of degradation due to medium characteristics and interfere with underwater tasks. While deep learning methods based on the Convolutional Neural Network (CNN) excel at detection tasks, they have inherent limitations when it comes to handling long-range dependencies. The enhanced images generated by these methods often have problems such as color cast, artificial traces and insufficient contrast. To address these limitations, we present a novel Holistic-Specific attention (HSA) mechanism based on the Vision Transformer (ViT). This mechanism allows us to capture global information in finer detail and perform initial enhancements on underwater images. Notably, even when combined with ViT, CNNs do not always approach the ideal state of image enhancement, as reference images themselves may involve human intervention. To tackle this, we design a loss function that incorporates contrastive learning, using the source image as a negative example. This approach guides the enhancement results to be closer to the ideal enhancement state while keeping away from the degraded state, not just closer to the reference. Additionally, we introduce patch-based contrastive learning to address the shortcomings of imagebased methods in fine-detail correction. Our extensive qualitative and quantitative experiments demonstrate that the proposed method outperforms state-of-the-art techniques.
C1 [Wei, Debin; Xie, Hongji] Dalian Univ, Commun & Network Lab, Dalian 116622, Peoples R China.
   [Zhang, Zengxi] Dalian Univ Technol, Dalian 116024, Peoples R China.
   [Yan, Tiantian] Dalian Univ, Natl & Local Joint Engn Lab Comp Aided Design, Dalian 116622, Peoples R China.
   [Yan, Tiantian] Dalian Univ, Sch Software Engn, Dalian 116622, Peoples R China.
C3 Dalian University; Dalian University of Technology; Dalian University;
   Dalian University
RP Yan, TT (corresponding author), Dalian Univ, Natl & Local Joint Engn Lab Comp Aided Design, Dalian 116622, Peoples R China.
EM weidebin@163.com; lshy030407@163.com; cyouzoukyuu@gmail.com;
   yan_tiantian01@163.com
FU Dalian Youth Science and Technology Star Program [2023RQ014];
   Interdisciplinary Project of Dalian University [DLUXK-2023-QN-016]; The
   111 Project [D23006]
FX This work is supported by the Dalian Youth Science and Technology Star
   Program under Grant 2023RQ014 and the Interdisciplinary Project of
   Dalian University (nos. DLUXK-2023-QN-016) and partially supported by
   111 Project (nos. D23006) .
CR Akkaynak D, 2019, PROC CVPR IEEE, P1682, DOI 10.1109/CVPR.2019.00178
   Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen XL, 2021, Arxiv, DOI arXiv:2101.00991
   Chen Y, 2023, OPT LASER ENG, V170, DOI 10.1016/j.optlaseng.2023.107745
   Chiang JY, 2012, IEEE T IMAGE PROCESS, V21, P1756, DOI 10.1109/TIP.2011.2179666
   Dai Z, 2021, ADV NEUR IN, V34
   Deng YY, 2022, PROC CVPR IEEE, P11316, DOI 10.1109/CVPR52688.2022.01104
   Gao SB, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2919947
   Ghani ASA, 2015, APPL SOFT COMPUT, V27, P219, DOI 10.1016/j.asoc.2014.11.020
   Han JL, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14174297
   Han K, 2021, ADV NEUR IN
   Huang SR, 2023, PROC CVPR IEEE, P18145, DOI 10.1109/CVPR52729.2023.01740
   Jiang ZY, 2023, Arxiv, DOI arXiv:2305.18092
   Jiang ZY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3783, DOI 10.1145/3503161.3547966
   Jiang ZY, 2023, VISUAL COMPUT, V39, P5563, DOI 10.1007/s00371-022-02681-1
   Jiang ZY, 2022, IEEE T CIRC SYST VID, V32, P6584, DOI 10.1109/TCSVT.2022.3174817
   Kang Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P355, DOI 10.1007/978-3-030-58595-2_22
   Kitaev Nikita, 2020, arXiv
   Li CY, 2021, IEEE T IMAGE PROCESS, V30, P4985, DOI 10.1109/TIP.2021.3076367
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li GH, 2023, Arxiv, DOI arXiv:2309.00872
   Lin RJ, 2022, VISUAL COMPUT, V38, P4419, DOI 10.1007/s00371-021-02305-0
   Liu J., 2022, arXiv
   Liu J.-W., 2023, ICCV
   Liu JY, 2023, INFORM FUSION, V95, P237, DOI 10.1016/j.inffus.2023.02.027
   Liu JY, 2022, PROC CVPR IEEE, P5792, DOI 10.1109/CVPR52688.2022.00571
   Liu JY, 2022, IEEE T CIRC SYST VID, V32, P5026, DOI 10.1109/TCSVT.2022.3144455
   Liu JY, 2022, IEEE T CIRC SYST VID, V32, P105, DOI 10.1109/TCSVT.2021.3056725
   Liu RS, 2023, IEEE T PATTERN ANAL, V45, P5953, DOI 10.1109/TPAMI.2022.3212995
   Liu RS, 2022, IEEE T IMAGE PROCESS, V31, P4922, DOI 10.1109/TIP.2022.3190209
   Liu RS, 2020, IEEE T CIRC SYST VID, V30, P4861, DOI 10.1109/TCSVT.2019.2963772
   Liu RS, 2021, IEEE T CIRC SYST VID, V31, P1366, DOI 10.1109/TCSVT.2020.3004854
   Liu RS, 2021, IEEE T IMAGE PROCESS, V30, P1261, DOI 10.1109/TIP.2020.3043125
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu SQ, 2023, J VIS COMMUN IMAGE R, V96, DOI 10.1016/j.jvcir.2023.103926
   Ma L, 2023, Arxiv, DOI arXiv:2306.01343
   Mu Pan, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P2286, DOI 10.1145/3503161.3548087
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Park T., 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Parmar N, 2018, PR MACH LEARN RES, V80
   Peng LT, 2023, IEEE T IMAGE PROCESS, V32, P3066, DOI 10.1109/TIP.2023.3276332
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Prytula Slavko, 2020, UNDERWATER OBJECT DE
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shang XK, 2024, INFORM FUSION, V102, DOI 10.1016/j.inffus.2023.102073
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tian Y., 2020, Advances in neural information processing systems, V33, P6827, DOI DOI 10.5555/3495724.3496297
   Ummar M, 2023, ENG APPL ARTIF INTEL, V126, DOI 10.1016/j.engappai.2023.107069
   Wang Y, 2018, COMPUT ELECTR ENG, V70, P904, DOI 10.1016/j.compeleceng.2017.12.006
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang DH, 2023, EXPERT SYST APPL, V231, DOI 10.1016/j.eswa.2023.120842
   Zhang ZengXi, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P7314, DOI 10.1145/3581783.3611727
   Zhang ZX, 2023, Arxiv, DOI arXiv:2309.01102
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhou JC, 2023, IEEE J OCEANIC ENG, V48, P474, DOI 10.1109/JOE.2022.3223733
   Zhou JC, 2023, INT J COMPUT VISION, DOI 10.1007/s11263-023-01853-3
   Zhou JC, 2023, IEEE J OCEANIC ENG, V48, P1322, DOI 10.1109/JOE.2023.3275615
   Zhou JC, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3293912
   Zhou JC, 2023, ENG APPL ARTIF INTEL, V121, DOI 10.1016/j.engappai.2023.105946
   Zhou JC, 2023, ENG APPL ARTIF INTEL, V121, DOI 10.1016/j.engappai.2023.105952
   Zhu PL, 2024, IEEE J OCEANIC ENG, V49, P48, DOI 10.1109/JOE.2023.3317903
   Zhu PL, 2023, ENG APPL ARTIF INTEL, V126, DOI 10.1016/j.engappai.2023.106866
NR 68
TC 0
Z9 0
U1 9
U2 9
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104059
DI 10.1016/j.jvcir.2024.104059
EA JAN 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IR7F6
UT WOS:001168116000001
DA 2024-08-05
ER

PT J
AU Sheng, QH
   Pan, R
   Chen, JY
   Lai, CC
   Liu, YY
   Huang, XF
   Yin, HB
   Yan, CG
AF Sheng, Qinghua
   Pan, Rui
   Chen, Junyu
   Lai, Changcai
   Liu, Yuanyuan
   Huang, Xiaofeng
   Yin, Haibing
   Yan, Chenggang
TI Efficient 2D transform hardware architecture for the versatile video
   coding standard
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Versatile video coding (VVC); Transform coding; Transpose memory;
   Hardware architecture; Pipeline
ID FPGA-BASED ARCHITECTURE
AB As the latest generation of video coding standards, versatile video coding (VVC) introduces several new coding tools in transform coding to concentrate the energy of residual blocks. In this work, we propose a regular multiplier (RM) based hardware architecture for 2D transform that can process different transform sizes and types using a unified architecture. This architecture consists of a 1D row transform, a transpose memory, and a 1D column transform, allowing for processing 16 coefficients per cycle. For the 1D row/column transform, the unified calculation structure consists of 512 regular multipliers supporting various transform sizes and types. For the transpose memory, we design an SRAM -based diagonal storage approach, along with a FIFO for storing the block information. Compared with the state-of-the-art MCM-based design, the proposed RM-based design shows a 30.9% reduction in area while can operate at a higher frequency.
C1 [Sheng, Qinghua; Pan, Rui; Lai, Changcai; Liu, Yuanyuan] Hangzhou Dianzi Univ, Sch Elect & Informat, Hangzhou 310018, Peoples R China.
   [Huang, Xiaofeng; Yin, Haibing; Yan, Chenggang] Hangzhou Dianzi Univ, Sch Commun Engn, Hangzhou 310015, Peoples R China.
   [Chen, Junyu] Zhejiang Univ, Polytech Inst, Hangzhou 310015, Peoples R China.
   [Huang, Xiaofeng; Yin, Haibing] Peking Univ, Adv Inst Informat Technol, Hangzhou 310015, Peoples R China.
C3 Hangzhou Dianzi University; Hangzhou Dianzi University; Zhejiang
   University; Peking University
RP Yin, HB (corresponding author), Hangzhou Dianzi Univ, Sch Commun Engn, Hangzhou 310015, Peoples R China.; Yin, HB (corresponding author), Peking Univ, Adv Inst Informat Technol, Hangzhou 310015, Peoples R China.
EM xfhuang@hdu.edu.cn; yhb@hdu.edu.cn
OI huang, xiaofeng/0000-0002-8479-6960
FU National Key R&D Program of China [2023YFB4502804]; NSFC [62031009];
   ZJNSF [LDT23F01014F01]; National Natural Science Foundation of China
   [61901150, 61931008, 61972123]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2023YFB4502804, in part by the NSFC under Grant 62031009,
   the ZJNSF under Grant LDT23F01014F01, and in part by the National
   Natural Science Foundation of China under Grant 61901150, 61931008, and
   61972123.
CR Bossen F, 2021, IEEE T CIRC SYST VID, V31, P3765, DOI 10.1109/TCSVT.2021.3072204
   Bross B, 2021, P IEEE, V109, P1463, DOI 10.1109/JPROC.2020.3043399
   Budagavi M, 2013, IEEE J-STSP, V7, P1029, DOI 10.1109/JSTSP.2013.2270429
   CHEN WH, 1977, IEEE T COMMUN, V25, P1004, DOI 10.1109/TCOM.1977.1093941
   DEMPSTER AG, 1995, IEEE T CIRCUITS-II, V42, P569, DOI 10.1109/82.466647
   Fan YB, 2020, IEEE T CIRC SYST VID, V30, P3289, DOI 10.1109/TCSVT.2019.2934752
   Farhat I, 2020, INT CONF ACOUST SPEE, P1663, DOI [10.1109/icassp40776.2020.9054281, 10.1109/ICASSP40776.2020.9054281]
   Farhat I, 2021, IEEE T CONSUM ELECTR, V67, P329, DOI 10.1109/TCE.2021.3126549
   Garrido MJ, 2020, IEEE ACCESS, V8, P81887, DOI 10.1109/ACCESS.2020.2991299
   Garrido MJ, 2019, IEEE T CONSUM ELECTR, V65, P274, DOI 10.1109/TCE.2019.2913327
   Garrido MJ, 2018, IEEE T CONSUM ELECTR, V64, P53, DOI 10.1109/TCE.2018.2812459
   Hao Z., 2021, 2021 IEEE 14 INT C A, P1
   Hao ZJ, 2022, IEEE INT SYMP CIRC S, P2012, DOI 10.1109/ISCAS48785.2022.9937709
   Hao ZJ, 2023, IEEE T VLSI SYST, V31, P658, DOI 10.1109/TVLSI.2023.3245291
   Huang X., 2024, IEEE Trans. Circuits Syst. Video Technol., P1
   Huang XF, 2023, IEEE T BROADCAST, V69, P422, DOI 10.1109/TBC.2023.3247953
   Meher PK, 2014, IEEE T CIRC SYST VID, V24, P168, DOI 10.1109/TCSVT.2013.2276862
   Mert AC, 2017, IEEE I C CONS ELECT, P31, DOI 10.1109/ICCE-Berlin.2017.8210582
   Mert AC, 2017, IEEE T CONSUM ELECTR, V63, P117, DOI 10.1109/TCE.2017.014862
   Shabani A, 2021, IEEE T CIRCUITS-I, V68, P1259, DOI 10.1109/TCSI.2020.3044248
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Voronenko Y, 2007, ACM T ALGORITHMS, V3, DOI 10.1145/1240233.1240234
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xu M, 2023, IEEE T BROADCAST, V69, P832, DOI 10.1109/TBC.2023.3262170
   Zhao D, 2019, IEEE INT SOC CONF, P176, DOI 10.1109/SOCC46988.2019.1570548652
   Zhao X, 2021, IEEE T CIRC SYST VID, V31, P3878, DOI 10.1109/TCSVT.2021.3087706
   Zheng MK, 2020, IEEE T CIRC SYST VID, V30, P810, DOI 10.1109/TCSVT.2019.2896294
NR 27
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD JUN
PY 2024
VL 102
AR 104202
DI 10.1016/j.jvcir.2024.104202
EA JUN 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WW4B9
UT WOS:001257884500001
DA 2024-08-05
ER

PT J
AU Fu, ZX
   Huang, HY
   Yu, B
   Li, XP
AF Fu, Zhengxin
   Huang, Hangying
   Yu, Bin
   Li, Xiaopeng
TI Size-invariant two-in-one image secret sharing scheme based on gray
   mixing model
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Size -invariant Visual Cryptography Scheme; TiOISSS; Multi -level
   grayscale image; Grayscale probabilistic matrix
ID VISUAL CRYPTOGRAPHY; RECOVERY
AB TiOISSS (Two-in-One Image Secret Sharing Scheme) can reconstruct the secret through computation and realize secret preview recovery when the power system is destroyed or the network is paralyzed. However, the visual quality of previewing images obtained by the existing TiOISSS is not satisfactory and usually affected by pixel expansion. To resolve the two problems, this paper introduces multiple gray values in the share image generation stage and establishes an optimization model to obtain the grayscale probabilistic matrix needed in the encoding stage. By using the probability matrix, a secret sharing algorithm is designed to obtain size-invariant shares with multiple gray values. Then, the secret information of another image can be shared and embedded into the multilevel gray shares through Lagrange polynomials over finite fields. In the decryption stage, the electronic shares can be directly superimposed based on a gray mixing model, or they can be printed to transparent films by a multi-level gray printer and then superimposed to obtain the preview image. If computational devices are available, a high-resolution gray image can be obtained by Lagrange interpolation over finite fields. Experimental results indicate that the proposed scheme is effective for generating shares without pixel expansion and improving the preview image quality.
C1 [Fu, Zhengxin; Huang, Hangying; Yu, Bin; Li, Xiaopeng] Zhengzhou Informat Sci & Technol Inst, Zhengzhou, Peoples R China.
C3 PLA Information Engineering University
RP Fu, ZX (corresponding author), Zhengzhou Informat Sci & Technol Inst, Zhengzhou, Peoples R China.
EM fzx2515@163.com
FU National Natural Science Foundation of China [61602513]
FX This work was supported by National Natural Science Foundation of China
   (Grant No. 61602513) . The authors would like to thank all the reviewers
   who participated in the review for their valuable suggestions and
   MJEditor ( www.mjeditor.com ) for its linguistic assistance during the
   preparation of this manuscript.
CR ASMUTH C, 1983, IEEE T INFORM THEORY, V29, P208, DOI 10.1109/TIT.1983.1056651
   Bakshi A, 2019, J INF SECUR APPL, V46, P281, DOI 10.1016/j.jisa.2019.03.004
   Chanu OB, 2019, INT J MULTIMED INF R, V8, P195, DOI 10.1007/s13735-018-0161-3
   Chih-Ching Thien, 2002, Computers & Graphics, V26, P765, DOI 10.1016/S0097-8493(02)00131-0
   Cimato S, 2007, THEOR COMPUT SCI, V374, P261, DOI 10.1016/j.tcs.2007.01.006
   Cimato S, 2006, COMPUT J, V49, P97, DOI 10.1093/comjnl/bxh152
   D'Arco P, 2014, LECT NOTES COMPUT SC, V8317, P18, DOI 10.1007/978-3-319-04268-8_2
   Jin D, 2005, J ELECTRON IMAGING, V14, DOI 10.1117/1.1993625
   Ju G, 2021, THEOR COMPUT SCI, V863, P19, DOI 10.1016/j.tcs.2021.02.010
   Kukreja S, 2020, MULTIMED TOOLS APPL, V79, P26155, DOI 10.1007/s11042-020-09130-y
   Li P, 2018, J REAL-TIME IMAGE PR, V14, P41, DOI 10.1007/s11554-016-0621-z
   Li P, 2012, J VIS COMMUN IMAGE R, V23, P441, DOI 10.1016/j.jvcir.2012.01.003
   Lidl R., 1996, Finite Fields, DOI [10.1017/cbo9780511525926.005, DOI 10.1017/CBO9780511525926.005]
   Lin SJ, 2007, PATTERN RECOGN, V40, P3652, DOI 10.1016/j.patcog.2007.04.001
   Liu F, 2012, J VIS COMMUN IMAGE R, V23, P331, DOI 10.1016/j.jvcir.2011.11.003
   Liu X, 2018, MULTIMED TOOLS APPL, V77, P16461, DOI 10.1007/s11042-017-5215-7
   Motta GHMB, 2020, J DIGIT IMAGING, V33, P88, DOI 10.1007/s10278-019-00243-x
   Naor M., 1995, Advances in Cryptology - EUROCRYPT '94. Workshop on the Theory and Application of Cryptographic Techniques. Proceedings, P1, DOI 10.1007/BFb0053419
   Okawa Y, 2020, IEEE INT SYMP INFO, P852, DOI [10.1109/isit44484.2020.9174121, 10.1109/ISIT44484.2020.9174121]
   Pougkakiotis S, 2021, COMPUT OPTIM APPL, V78, P307, DOI 10.1007/s10589-020-00240-9
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   Shyu SJ, 2008, 2008 IEEE ASIA-PACIFIC SERVICES COMPUTING CONFERENCE, VOLS 1-3, PROCEEDINGS, P1332, DOI 10.1109/APSCC.2008.223
   Sridhar S, 2021, J VIS COMMUN IMAGE R, V74, DOI 10.1016/j.jvcir.2020.102996
   Sridhar S, 2018, MULTIMED TOOLS APPL, V77, P28601, DOI 10.1007/s11042-018-6019-0
   Ustubioglu A, 2019, MULTIMED TOOLS APPL, V78, P22269, DOI 10.1007/s11042-019-7529-0
   Wu XT, 2013, J VIS COMMUN IMAGE R, V24, P48, DOI 10.1016/j.jvcir.2012.11.001
   Xiong LZ, 2021, SIGNAL PROCESS, V185, DOI 10.1016/j.sigpro.2021.108064
   Yan XH, 2021, INFORM SCIENCES, V562, P475, DOI 10.1016/j.ins.2021.03.029
   Yan XH, 2020, IEEE T INF FOREN SEC, V15, P3848, DOI 10.1109/TIFS.2020.3001735
   Yan XH, 2018, DIGIT SIGNAL PROCESS, V82, P80, DOI 10.1016/j.dsp.2018.07.015
   Yan XH, 2018, LECT NOTES COMPUT SC, V10749, P174, DOI 10.1007/978-3-319-75786-5_15
   Yan XH, 2015, MULTIMED TOOLS APPL, V74, P3231, DOI 10.1007/s11042-013-1784-2
   Yan XH, 2015, J VIS COMMUN IMAGE R, V26, P94, DOI 10.1016/j.jvcir.2014.11.003
   Yang CN, 2021, IEEE T CIRC SYST VID, V31, P2465, DOI 10.1109/TCSVT.2020.3017126
   Yang CN, 2010, IMAGE VISION COMPUT, V28, P1600, DOI 10.1016/j.imavis.2010.04.003
   Yang CN, 2004, PATTERN RECOGN LETT, V25, P481, DOI 10.1016/j.patrec.2003.12.011
   Zhou Q, 2020, INFORM SCIENCES, V512, P641, DOI 10.1016/j.ins.2019.10.007
NR 37
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104134
DI 10.1016/j.jvcir.2024.104134
EA APR 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SA7Z7
UT WOS:001231817900001
DA 2024-08-05
ER

PT J
AU Aslam, N
   Kolekar, MH
AF Aslam, Nazia
   Kolekar, Maheshkumar H.
TI TransGANomaly: Transformer based Generative Adversarial Network for
   Video Anomaly Detection
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Anomaly detection; Video vision transformer; Adversarial training;
   Generative adversarial network
ID EVENT DETECTION; LOCALIZATION
AB Video anomaly detection aims to identify a set of abnormal events in videos. Deep reconstruction and prediction -based models have been employed to detect anomalies. Deep reconstruction models sometimes recreate the abnormal events along with the normal ones. However, the prediction -based approaches have demonstrated encouraging results. This paper presents a video vision transformer (ViViT) based generative adversarial network (GAN), TransGANomaly, a novel approach for detecting anomalies. The proposed framework is a video frame predictor and trained only on normal video data adversarially. The generator of the GAN is a ViViT network that receives 3D input tokens from the video snippets. The generator aims to predict the future frame based on past sequences. After that, the predicted and original frames are given to the model's discriminator for binary classification. Extensive experiments have been performed on UCSD Pedestrian, CUHK Avenue, and ShanghiaTech datasets to validate the efficacy of the proposed method.
C1 [Aslam, Nazia; Kolekar, Maheshkumar H.] Indian Inst Technol Patna, Dept Elect Engn, Video Surveillance Lab, Bihta 801106, India.
C3 Indian Institute of Technology (IIT) - Patna
RP Aslam, N (corresponding author), Indian Inst Technol Patna, Dept Elect Engn, Video Surveillance Lab, Bihta 801106, India.
EM n.aslam921@gmail.com; mahesh@iitp.ac.in
RI Aslam, Nazia/AGZ-4243-2022
OI Aslam, Nazia/0000-0002-8381-9702
CR Adam A, 2008, IEEE T PATTERN ANAL, V30, P555, DOI 10.1109/TPAMI.2007.70825
   Aggarwal CC, 2014, CH CRC DATA MIN KNOW, P1
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Aslam N, 2024, VISUAL COMPUT, V40, P1729, DOI 10.1007/s00371-023-02882-2
   Aslam N, 2022, J VIS COMMUN IMAGE R, V87, DOI 10.1016/j.jvcir.2022.103598
   Aslam N, 2022, MULTIMED TOOLS APPL, V81, P42457, DOI 10.1007/s11042-022-13496-6
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI DOI 10.48550/ARXIV.2005.14165
   Chen DY, 2020, IMAGE VISION COMPUT, V98, DOI 10.1016/j.imavis.2020.103915
   Chen J., 2021, TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation
   Chong YS, 2017, LECT NOTES COMPUT SC, V10262, P189, DOI 10.1007/978-3-319-59081-3_23
   Cosar S, 2017, IEEE T CIRC SYST VID, V27, P683, DOI 10.1109/TCSVT.2016.2589859
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fan YX, 2020, COMPUT VIS IMAGE UND, V195, DOI 10.1016/j.cviu.2020.102920
   Feng XY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5546, DOI 10.1145/3474085.3475693
   Gong D, 2019, IEEE I CONF COMP VIS, P1705, DOI 10.1109/ICCV.2019.00179
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86
   Hong CQ, 2019, IEEE T IND INFORM, V15, P3952, DOI 10.1109/TII.2018.2884211
   Hong CQ, 2015, IEEE T IMAGE PROCESS, V24, P5659, DOI 10.1109/TIP.2015.2487860
   Hospedales T, 2009, IEEE I CONF COMP VIS, P1165, DOI 10.1109/ICCV.2009.5459342
   Huang C, 2023, IEEE T NEUR NET LEAR, V34, P9389, DOI 10.1109/TNNLS.2022.3159538
   Huang C, 2022, IEEE T IND INFORM, V18, P5171, DOI 10.1109/TII.2021.3122801
   Hyunjong Park, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14360, DOI 10.1109/CVPR42600.2020.01438
   Ionescu RT, 2017, IEEE I CONF COMP VIS, P2914, DOI 10.1109/ICCV.2017.315
   Jiang YF, 2021, ADV NEUR IN
   Kim J, 2009, PROC CVPR IEEE, P2913
   Kingma D. P., 2014, arXiv
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   Li P, 2023, Arxiv, DOI arXiv:2311.11289
   Li P, 2023, Arxiv, DOI arXiv:2306.10346
   Li P, 2023, INFORM PROCESS MANAG, V60, DOI 10.1016/j.ipm.2022.103204
   Li WX, 2014, IEEE T PATTERN ANAL, V36, P18, DOI 10.1109/TPAMI.2013.111
   Liu W, 2018, PROC CVPR IEEE, P6536, DOI 10.1109/CVPR.2018.00684
   Liu ZA, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13568, DOI 10.1109/ICCV48922.2021.01333
   Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338
   Luo WX, 2017, IEEE I CONF COMP VIS, P341, DOI 10.1109/ICCV.2017.45
   Luo WX, 2017, IEEE INT CON MULTI, P439, DOI 10.1109/ICME.2017.8019325
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Mo X, 2014, IEEE T CIRC SYST VID, V24, P631, DOI 10.1109/TCSVT.2013.2280061
   Nawaratne R, 2020, IEEE T IND INFORM, V16, P393, DOI 10.1109/TII.2019.2938527
   Ravanbakhsh M, 2017, IEEE IMAGE PROC, P1577, DOI 10.1109/ICIP.2017.8296547
   Medel JR, 2016, Arxiv, DOI arXiv:1612.00390
   Saligrama V, 2012, PROC CVPR IEEE, P2112, DOI 10.1109/CVPR.2012.6247917
   Song H, 2020, IEEE T MULTIMEDIA, V22, P2138, DOI 10.1109/TMM.2019.2950530
   Sultani W, 2018, PROC CVPR IEEE, P6479, DOI 10.1109/CVPR.2018.00678
   Tang Y, 2020, PATTERN RECOGN LETT, V129, P123, DOI 10.1016/j.patrec.2019.11.024
   Nguyen TN, 2019, IEEE I CONF COMP VIS, P1273, DOI 10.1109/ICCV.2019.00136
   Vaswani A, 2017, ADV NEUR IN, V30
   von Neumann J., 1947, THEORY GAMES EC BEHA
   Wang SQ, 2018, NEUROCOMPUTING, V277, P161, DOI 10.1016/j.neucom.2016.08.156
   Wang T, 2018, OPTIK, V152, P50, DOI 10.1016/j.ijleo.2017.07.064
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang X, 2018, INT CONF SIGN PROCES, P474, DOI 10.1109/ICSP.2018.8652354
   Wu SD, 2010, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2010.5539882
   Xu XH, 2020, SOFTWARE PRACT EXPER, V50, P476, DOI 10.1002/spe.2701
   Yan SY, 2020, IEEE T COGN DEV SYST, V12, P30, DOI 10.1109/TCDS.2018.2883368
   Yang ZW, 2021, IEEE ACCESS, V9, P107842, DOI 10.1109/ACCESS.2021.3100678
   Ye MC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1805, DOI 10.1145/3343031.3350899
   Yu G, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P583, DOI 10.1145/3394171.3413973
   Yu J, 2022, IEEE T PATTERN ANAL, V44, P563, DOI 10.1109/TPAMI.2019.2932058
   Yu J, 2015, IEEE T CYBERNETICS, V45, P767, DOI 10.1109/TCYB.2014.2336697
   Yuan HC, 2021, IEEE ACCESS, V9, P123977, DOI 10.1109/ACCESS.2021.3109102
   Zaheer MZ, 2020, IEEE SIGNAL PROC LET, V27, P1705, DOI 10.1109/LSP.2020.3025688
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang JG, 2019, IEEE IMAGE PROC, P4030, DOI [10.1109/ICIP.2019.8803657, 10.1109/icip.2019.8803657]
   Zhang Y, 2021, IEEE T CIRC SYST VID, V31, P3694, DOI 10.1109/TCSVT.2020.3039798
   Zhao J, 2018, AQUACULTURE, V493, P165, DOI 10.1016/j.aquaculture.2018.04.064
   Zhao YR, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1933, DOI 10.1145/3123266.3123451
   Zhong YQ, 2023, IEEE I CONF COMP VIS, P4250, DOI 10.1109/ICCV51070.2023.00394
   Zhou JT, 2020, IEEE T CIRC SYST VID, V30, P4639, DOI 10.1109/TCSVT.2019.2962229
   Zhou JT, 2019, IEEE T INF FOREN SEC, V14, P2537, DOI 10.1109/TIFS.2019.2900907
   Zhou SF, 2016, SIGNAL PROCESS-IMAGE, V47, P358, DOI 10.1016/j.image.2016.06.007
   Zhu YY, 2013, IEEE J-STSP, V7, P91, DOI 10.1109/JSTSP.2012.2234722
NR 75
TC 0
Z9 0
U1 9
U2 9
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104108
DI 10.1016/j.jvcir.2024.104108
EA MAR 2024
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NS4Y9
UT WOS:001202442500001
DA 2024-08-05
ER

PT J
AU Xiao, F
   Li, X
   Li, W
   Shi, JJ
   Zhang, NR
   Gao, XP
AF Xiao, Fen
   Li, Xiang
   Li, Wei
   Shi, Junjie
   Zhang, Ningru
   Gao, Xieping
TI Integrating category-related key regions with a dual-stream network for
   remote sensing scene classification
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Remote sensing scene classification; Swin transformer; Convolutional
   neural network; Dual-stream; Category-related key region localization
ID CONVOLUTIONAL NEURAL-NETWORK
AB Remote sensing image scene classification has made great progress with deep learning. Due to complex backgrounds and the large number of objects with inhomogeneous sizes, remote sensing image scene classification still remains challenging. In fact, only a few parts of regions are expected to be representative of the scene. In this paper, we propose a dual -stream framework for remote sensing classification with the help of category -related regions. Swin Transformer -based global feature extractor is used to produce coarse classification results. A category -related key region localization module is developed to extract the representative region from the whole image. The ResNet50 is employed as a local feature extractor to mine critical features from prominent regions. Finally, the classification results is obtained with the shared weighted classify head on global -local features. Extensive experiments show that our method provides a promising approach for key region locations and performs best on three public remote sensing datasets.
C1 [Xiao, Fen; Li, Xiang; Li, Wei; Shi, Junjie; Zhang, Ningru] Xiangtan Univ, MOE Key Lab Intelligent Comp & Informat Proc, Xiangtan 411105, Peoples R China.
   [Gao, Xieping] Hunan Normal Univ, Coll Informat Sci & Engn, Changsha 410006, Peoples R China.
C3 Xiangtan University; Hunan Normal University
RP Gao, XP (corresponding author), Hunan Normal Univ, Coll Informat Sci & Engn, Changsha 410006, Peoples R China.
EM xiaof@xtu.edu.cn; 202221633033@smail.xtu.edu.en;
   202021002425@smail.xtu.edu.cn; 202121632835@smail.xtu.edu.cn;
   202121632898@smail.xtu.edu.cn; xpgao@hunnu.edu.cn
FU National Science and Technol-ogy Major Project, China [2020YFA0713504];
   National Natural Science Foundation of China [62376238]; Scientific
   Research Foundation of Education Department of Hunan Province of China
   [21A0109]
FX <B>Acknowledgments</B> This research was supported by the National
   Science and Technol-ogy Major Project, China (Grant No. 2020YFA0713504)
   , the National Natural Science Foundation of China (No. 62376238) , and
   the Scientific Research Foundation of Education Department of Hunan
   Province of China (Grant No. 21A0109) .
CR Bazi Y, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13030516
   Chen JP, 2022, NEUROCOMPUTING, V501, P359, DOI 10.1016/j.neucom.2022.06.041
   Chen SB, 2022, IEEE T IMAGE PROCESS, V31, P99, DOI 10.1109/TIP.2021.3127851
   Chen XM, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2022.3150801
   Cheng G, 2017, P IEEE, V105, P1865, DOI 10.1109/JPROC.2017.2675998
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng PF, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3109061
   Gamba P, 2013, P IEEE, V101, P570, DOI 10.1109/JPROC.2012.2189089
   Gao BB, 2021, IEEE T IMAGE PROCESS, V30, P5920, DOI 10.1109/TIP.2021.3088605
   Gao Y, 2021, EUR J REMOTE SENS, V54, P141, DOI 10.1080/22797254.2020.1868273
   Ghazouani F, 2019, IEEE T GEOSCI REMOTE, V57, P8775, DOI 10.1109/TGRS.2019.2922908
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu F, 2015, REMOTE SENS-BASEL, V7, P14680, DOI 10.3390/rs71114680
   Kingma D. P., 2014, arXiv
   Liang JL, 2020, IEEE J-STARS, V13, P4325, DOI 10.1109/JSTARS.2020.3011333
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Longbotham N, 2012, IEEE T GEOSCI REMOTE, V50, P1155, DOI 10.1109/TGRS.2011.2165548
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu XQ, 2019, IEEE T GEOSCI REMOTE, V57, P7894, DOI 10.1109/TGRS.2019.2917161
   Lv PY, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3157671
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Wang J.-L, 2023, IEEE Trans. Geosci. Remote Sens
   Wang Q, 2022, IEEE T NEUR NET LEAR, V33, P1414, DOI 10.1109/TNNLS.2020.3042276
   Wang X, 2022, IEEE J-STARS, V15, P422, DOI 10.1109/JSTARS.2021.3135566
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xia GS, 2017, IEEE T GEOSCI REMOTE, V55, P3965, DOI 10.1109/TGRS.2017.2685945
   Xu KJ, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3152566
   Xu KJ, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3075712
   Yang Y., 2010, P 18 SIGSPATIAL INT, P270
   Zhang H, 2023, IEEE T MED IMAGING, V42, P444, DOI 10.1109/TMI.2022.3219260
   Zhang JR, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13204143
   Zhang L., 2022, IEEE Trans. Multimed.
   Zhang Y., 2023, Neural Comput. Appl., P1
   Zhao L., 2023, IEEE Geosci. Remote Sens. Lett.
   Zhao M., 2023, IEEE Trans. Geosci. Remote Sens.
   Zhao WZ, 2017, IEEE J-STARS, V10, P3386, DOI 10.1109/JSTARS.2017.2680324
   Zhao Y.-P., 2023, IEEE Trans Geosci Remote Sens, V61, P1, DOI DOI 10.1109/TGRS.2023.3336471
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhengzhou Li, 2022, IEEE Geoscience and Remote Sensing Letters, V19, DOI 10.1109/LGRS.2020.3017542
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu QQ, 2016, IEEE GEOSCI REMOTE S, V13, P747, DOI 10.1109/LGRS.2015.2513443
NR 43
TC 1
Z9 1
U1 4
U2 4
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104098
DI 10.1016/j.jvcir.2024.104098
EA MAR 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NS5Y9
UT WOS:001202468800001
DA 2024-08-05
ER

PT J
AU Li, XW
   Wang, ZH
   Xu, BW
AF Li, Xiwen
   Wang, Zhihua
   Xu, Binwei
TI Blind image quality assessment with semi-supervised learning
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Blind image quality assessment; gMAD competition; Learning to rank;
   Semi-supervised learning
AB Blind image quality assessment (BIQA) aims to automatically predict the perceptual quality of an image without requiring access to its pristine reference counterpart. BIQA models are typically developed through supervised learning, optimizing and testing them by comparing their predictions to human ratings, usually expressed as mean opinion scores (MOS), which can be labor-intensive to collect. The performance of these BIQA models is significantly reliant on the amount of labeled training data. When there is a shortage of human -rated data, these BIQA models may perform inadequately. In this study, we investigate the potential of incorporating unlabeled data to mitigate this issue and enhance the performance of BIQA models. To achieve this, we propose a deep ensemble -based BIQA model (referred to as the "target model") with two heads: one for quality estimation and the other for pseudo -label generation. Initially, we train it on a small set of humanrated images where the supervisory signals are binary labels indicating the pairwise ranking of perceptual quality for image pairs. Then, the head responsible for pseudo -label generation assigns pseudo -binary labels to unlabeled pairs. Subsequently, we re-train the target model using a combination of labeled and pseudolabeled datasets. This process can be iterated, allowing for the progressive improvement of the target model's performance. We conduct comprehensive case studies to illustrate the advantages of utilizing unlabeled data for BIQA, particularly in terms of model generalization and identifying cases of model failure.
C1 [Li, Xiwen; Xu, Binwei] Ningbo Univ, Sch Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Wang, Zhihua] Shenzhen MSU BIT Univ, Dept Engn, Shenzhen 518000, Peoples R China.
C3 Ningbo University; Shenzhen MSU-BIT University
RP Xu, BW (corresponding author), Ningbo Univ, Sch Informat Sci & Engn, Ningbo 315211, Peoples R China.
EM xubinwei@nbu.edu.cn
RI Xu, Binwei/GPS-9140-2022
FU Natural Science Foundation of China [62301323]
FX <B>Acknowledgments</B> This work is supported by the Natural Science
   Foundation of China (62301323) .
CR Arazo E, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207304
   Berthelot D., 2019, P INT C LEARN REPR I
   Berthelot D, 2019, ADV NEUR IN, V32
   Bianco S, 2018, SIGNAL IMAGE VIDEO P, V12, P355, DOI 10.1007/s11760-017-1166-8
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Boyd S., 2004, Convex optimization, DOI [DOI 10.1017/CBO9780511804441, 10.1017/CBO9780511804441]
   Chapelle O., 2009, SEMISUPERVISED LEARN, V20, P542, DOI 10.1109/TNN.2009.2015974
   Chen HH, 2018, IEEE T NEUR NET LEAR, V29, P5366, DOI 10.1109/TNNLS.2017.2784814
   Ciancio A, 2011, IEEE T IMAGE PROCESS, V20, P64, DOI 10.1109/TIP.2010.2053549
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Pham H, 2021, PROC CVPR IEEE, P11552, DOI 10.1109/CVPR46437.2021.01139
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Iscen A, 2019, PROC CVPR IEEE, P5065, DOI 10.1109/CVPR.2019.00521
   Kingma D.P., 2014, Proc. of ICLR
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lee D.H., 2013, WORKSH CHALL REPR LE, P07
   Lee SF, 2015, Arxiv, DOI [arXiv:1511.06314, 10.48550/arXiv.1511.06314, DOI 10.48550/ARXIV.1511.06314]
   Lin HH, 2019, INT WORK QUAL MULTIM
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Ma KD, 2019, IEEE IMAGE PROC, P2344, DOI [10.1109/icip.2019.8803390, 10.1109/ICIP.2019.8803390]
   Ma KD, 2020, IEEE T PATTERN ANAL, V42, P851, DOI 10.1109/TPAMI.2018.2889948
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   Ma KD, 2016, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2016.184
   Ma Y, 2012, ENSEMBLE MACHINE LEARNING: METHODS AND APPLICATIONS, P1, DOI 10.1007/978-1-4419-9326-7
   Ming-Feng Tsai, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P383, DOI 10.1145/1277741.1277808
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Pham H., 2019, Semi -supervised learning by coaching
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Qizhe Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10684, DOI 10.1109/CVPR42600.2020.01070
   Rizve M. N., 2020, INT C LEARN REPR
   Sajjadi M, 2016, ADV NEUR IN, V29
   Samuli L., 2017, INT C LEARN REPR ICL, V4, P6
   SCUDDER HJ, 1965, IEEE T INFORM THEORY, V11, P363, DOI 10.1109/tit.1965.1053799
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sohn K., 2020, NEURIPS
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Tarvainen A., 2017, P 31 ANN C NEUR INF, P1195, DOI DOI 10.1137/0330046
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   van Engelen JE, 2020, MACH LEARN, V109, P373, DOI 10.1007/s10994-019-05855-6
   VQEG, 2000, Final Report From the Video Quality Experts Groupon the Validation of Objective Models of Video Quality Assessment
   Wang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1041, DOI 10.1145/3123266.3123359
   Wang XM, 2021, PR MACH LEARN RES, V139, P7748
   Wang Z., 2021, arXiv
   Wang ZH, 2022, IEEE T PATTERN ANAL, V44, P4577, DOI 10.1109/TPAMI.2021.3071759
   Webb A, 2021, LECT NOTES ARTIF INT, V12459, P109, DOI 10.1007/978-3-030-67664-3_7
   Wu JJ, 2020, IEEE T IMAGE PROCESS, V29, P7414, DOI 10.1109/TIP.2020.3002478
   Xie Q., 2020, Advances in Neural Information Processing Systems, V33, P6256
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Yalniz I. Z., 2019, ARXIV
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Ying ZQ, 2020, PROC CVPR IEEE, P3572, DOI 10.1109/CVPR42600.2020.00363
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang WX, 2022, Arxiv, DOI arXiv:2102.09717
   Zhang WX, 2021, IEEE T IMAGE PROCESS, V30, P3474, DOI 10.1109/TIP.2021.3061932
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhu H., 2020, P IEEECVF C COMPUTER, P14143
NR 65
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104100
DI 10.1016/j.jvcir.2024.104100
EA MAR 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NX3O8
UT WOS:001203716000001
DA 2024-08-05
ER

PT J
AU Ji, KK
   Zhang, QL
   Zhu, SH
AF Ji, Kangkang
   Zhang, Qingliang
   Zhu, Songhao
TI Subdomain alignment based open-set domain adaptation image
   classification
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Open-set domain adaptation; Transfer learning; Image classification; MMD
ID NETWORK
AB Domain adaptation has achieved great success in using labeled source domain samples to identify unlabeled target domain samples. Here, we aim to solve the open-set domain adaptation, which is different from the closedset domain adaptation in that it contains categories in target domain that do not appear in source domain. To solve this problem, this paper proposes open-set domain adaptation model based on subdomain alignment, which uses variable weights for discriminative training of unknown samples in target domain. Aiming at the distribution differences between domains, the model aligns the distributions of the category subspaces of source and target domains, enhancing the distribution similarity within the subspaces of the same category. Through experiments on different domain adaptation datasets, the results show that the model proposed in this paper effectively improves the accuracy of open-set domain adaptation classification.
C1 [Ji, Kangkang; Zhang, Qingliang; Zhu, Songhao] Nanjing Univ Posts & Telecommun, Coll Automat, Nanjing 210023, Peoples R China.
C3 Nanjing University of Posts & Telecommunications
RP Zhu, SH (corresponding author), Nanjing Univ Posts & Telecommun, Coll Automat, Nanjing 210023, Peoples R China.
EM zhush@njupt.edu.cn
CR Bendale A, 2016, PROC CVPR IEEE, P1563, DOI 10.1109/CVPR.2016.173
   Busto PP, 2017, IEEE I CONF COMP VIS, P754, DOI 10.1109/ICCV.2017.88
   Fu JH, 2019, IEEE IMAGE PROC, P2506, DOI [10.1109/icip.2019.8803287, 10.1109/ICIP.2019.8803287]
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gao Y, 2020, NEUROCOMPUTING, V410, P174, DOI 10.1016/j.neucom.2020.05.032
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jain LP, 2014, LECT NOTES COMPUT SC, V8691, P393, DOI 10.1007/978-3-319-10578-9_26
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lian Q, 2019, Arxiv, DOI arXiv:1905.01068
   Liu H, 2019, PROC CVPR IEEE, P2922, DOI 10.1109/CVPR.2019.00304
   Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Long MS, 2014, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2014.183
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Peng XC, 2017, Arxiv, DOI arXiv:1710.06924
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Saito K, 2018, LECT NOTES COMPUT SC, V11209, P156, DOI 10.1007/978-3-030-01228-1_10
   Shermin T, 2021, IEEE T MULTIMEDIA, V23, P2732, DOI 10.1109/TMM.2020.3016126
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Xue Y, 2023, IEEE T EVOLUT COMPUT, V27, P778, DOI 10.1109/TEVC.2023.3252612
   Xue Y, 2023, IEEE T IND INFORM, V19, P6804, DOI 10.1109/TII.2022.3184700
   Xue Y, 2021, IEEE COMPUT INTELL M, V16, P67, DOI 10.1109/MCI.2021.3084435
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13864, DOI 10.1109/CVPR42600.2020.01388
   Yosinski J, 2014, ADV NEUR IN, V27
   Zhao SC, 2022, IEEE T NEUR NET LEAR, V33, P473, DOI 10.1109/TNNLS.2020.3028503
   Zhu YC, 2021, IEEE T NEUR NET LEAR, V32, P1713, DOI 10.1109/TNNLS.2020.2988928
NR 32
TC 0
Z9 0
U1 34
U2 34
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104047
DI 10.1016/j.jvcir.2024.104047
EA JAN 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HE1L7
UT WOS:001157725000001
DA 2024-08-05
ER

PT J
AU Huang, BL
   Qi, XL
   Chen, B
AF Huang, Bailiang
   Qi, Xiaolong
   Chen, Bin
TI Cross-modal feature learning and alignment network for text-image person
   re-identification
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Machine vision; Deep learning; Cross modal; Person re -identification
ID ADAPTATION
AB In the search for missing persons or criminal cases, the eyewitness's verbal description of the target person is crucial when the initial image of the target person is not available. The main task is to search for the target in person images using the verbal descriptions. Existing methods often address the modal differences between text and image dimensional information using auxiliary information such as the fine-grained or body structure to align features. However, due to scale differences between data and the subjectivity present in descriptive text, these methods suffer from the problem of feature alignment failure. To address this issue, we propose the construction of a cross-modal feature learning and alignment (CFLAA) network. In this network, a feature alignment block (FAB) dynamically learns the feature correspondence between text and images in a multi-scale manner. Subsequently, to enhance the global dependency and global feature representation of text information, an enhanced text global awareness (ETGA) module is designed. A body correlation network (BC-Net) is introduced to improve the global feature representation of images. Finally, detailed comparative experiments are conducted on three datasets to demonstrate that the CFLAA network outperforms current state-of-the-art networks.
C1 [Huang, Bailiang; Chen, Bin] Changchun Tech Univ Automobile, Changchun 130000, Jilin, Peoples R China.
   [Qi, Xiaolong] Changchun Elect Power Design Co Ltd, Changchun 130000, Jilin, Peoples R China.
RP Huang, BL (corresponding author), Changchun Tech Univ Automobile, Changchun 130000, Jilin, Peoples R China.
EM ww5171351@126.com
FU Changchun Technical University of Automobile [XJKY202413]; Jilin
   Provincial Department of Science and Technology [YDZJ202102CXJD062];
   Jilin Province Machine Vision Intelligent Manufacturing and Inspection
   Technology Innovation Center [20180623039TC]
FX <BOLD>Funding</BOLD> This work was supported by Changchun Technical
   University of Automobile [grant numbers XJKY202413] ; Jilin Provincial
   Department of Science and Technology [grant numbers YDZJ202102CXJD062] ;
   and Jilin Province Machine Vision Intelligent Manufacturing and
   Inspection Technology Innovation Center [grant numbers 20180623039TC] .
CR Aggarwal S, 2020, IEEE WINT CONF APPL, P2606, DOI [10.1109/wacv45572.2020.9093640, 10.1109/WACV45572.2020.9093640]
   Bailiang H., 2023, J. Comput., V34, P187
   Chen TL, 2018, IEEE WINT CONF APPL, P1879, DOI 10.1109/WACV.2018.00208
   Chen WB, 2022, MULTIMED TOOLS APPL, V81, P4649, DOI 10.1007/s11042-020-10494-4
   Chen YC, 2021, IEEE T IMAGE PROCESS, V30, P4057, DOI 10.1109/TIP.2021.3068825
   Chen YH, 2022, NEUROCOMPUTING, V494, P171, DOI 10.1016/j.neucom.2022.04.081
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Ding ZF, 2021, Arxiv, DOI arXiv:2107.12666
   Farooq A, 2022, AAAI CONF ARTIF INTE, P4477
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Han X, 2021, Arxiv, DOI arXiv:2110.10807
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jiang D, 2023, PROC CVPR IEEE, P2787, DOI 10.1109/CVPR52729.2023.00273
   Jing Y, 2020, AAAI CONF ARTIF INTE, V34, P11189
   Li HF, 2022, IEEE T CIRC SYST VID, V32, P2814, DOI 10.1109/TCSVT.2021.3099943
   Li HF, 2021, IEEE T INF FOREN SEC, V16, P1480, DOI 10.1109/TIFS.2020.3036800
   Li H, 2022, IEEE T CIRC SYST VID, V32, P1624, DOI 10.1109/TCSVT.2021.3073718
   Li S, 2017, IEEE I CONF COMP VIS, P1908, DOI 10.1109/ICCV.2017.209
   Li S, 2017, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2017.551
   Sarafianos N, 2019, IEEE I CONF COMP VIS, P5813, DOI 10.1109/ICCV.2019.00591
   Shao ZY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5566, DOI 10.1145/3503161.3548028
   Shu X., 2022, P ECCV, P624
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wang YF, 2024, APPL INTELL, V54, P398, DOI 10.1007/s10489-023-05131-0
   Wang YM, 2024, VISUAL COMPUT, V40, P2363, DOI 10.1007/s00371-023-02923-w
   Wang YM, 2022, IEEE T INF FOREN SEC, V17, P3321, DOI 10.1109/TIFS.2022.3207893
   Wang ZJ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5314, DOI 10.1145/3503161.3548057
   Wu YS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1604, DOI 10.1109/ICCV48922.2021.00165
   Yan S, 2023, IEEE Trans. Image Process.
   Yan SL, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3310118
   Zhang Y, 2018, LECT NOTES COMPUT SC, V11205, P707, DOI 10.1007/978-3-030-01246-5_42
   Zhe Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P402, DOI 10.1007/978-3-030-58610-2_24
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zhu AC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P209, DOI 10.1145/3474085.3475369
NR 35
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD AUG
PY 2024
VL 103
AR 104219
DI 10.1016/j.jvcir.2024.104219
EA JUL 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YD1E5
UT WOS:001266449800001
DA 2024-08-05
ER

PT J
AU Shi, ZH
   Wan, WG
AF Shi, Zhihua
   Wan, Weiguo
TI Transformer-Based adversarial network for semi-supervised face sketch
   synthesis
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Face sketch synthesis; Vision Transformer; Detail extractor; Adaptive
   window attention; Adversarial network
ID ATTENTION
AB Face sketch synthesis is a technique utilized to convert real face images into artistic sketches, which holds vast potential within criminal investigation and entertainment. The existing methods usually train the generation models on the paired face photo -sketch datasets, which are challenging to acquire. Moreover, their results usually produce blurring, artifacts, and structural distortion, leading to inferior visual effects. To solve the above issues, we propose a semi -supervised Transformer -based adversarial network for face sketch synthesis, which can be trained on unpaired datasets. In the network, the Transformer encoder structure is modified with the adaptive window attention (AWA) to better extract local and global facial features while minimizing computational complexity. A Transformer -based feature fusion module is used to fuse the extracted features. In addition, a detail extractor module is designed by Laplacian operators to effectively preserve the detail information of the face photo images to the face sketch images. In the detail extractor module, we introduce a mask operation to remove the textures that do not exist in the original face photo images. Experimental results on the CUHK, AR, XM2VTS, and CUFSF datasets showcase the excellent subjective and objective performance of the proposed face sketch synthesis method compared to current state-of-the-art methods.
C1 [Shi, Zhihua; Wan, Weiguo] Jiangxi Univ Finance & Econ, Sch Software & Internet Things Engn, Nanchang 330032, Peoples R China.
C3 Jiangxi University of Finance & Economics
RP Wan, WG (corresponding author), Jiangxi Univ Finance & Econ, Sch Software & Internet Things Engn, Nanchang 330032, Peoples R China.
EM 2202203121@stu.jxufe.edu.cn; wanweiguo@jxufe.edu.cn
FU National Natural Science Foundation of China [62261025, 62262023];
   Natural Science Foundation of Jiangxi Province, China [20232BAB212015];
   Xizang Autonomous Region Science and Technology Plan, China
   [XZ202303ZY0005G]
FX <B>Acknowledgments</B> This study has been supported in part by the
   National Natural Science Foundation of China (62261025, 62262023) , by
   the Natural Science Foundation of Jiangxi Province, China
   (20232BAB212015) , and by the Xizang Autonomous Region Science and
   Technology Plan, China (XZ202303ZY0005G) .
CR Bertasius G, 2021, PR MACH LEARN RES, V139
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Cheng Kun, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P6959, DOI 10.1145/3581783.3611834
   Deb D., 2020, IEEEIAPR INT JOINT, P1, DOI DOI 10.1109/ijcb48548.2020.9304898
   Deng YY, 2022, PROC CVPR IEEE, P11316, DOI 10.1109/CVPR52688.2022.01104
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Duan SC, 2021, IEEE T INF FOREN SEC, V16, P1218, DOI 10.1109/TIFS.2020.3031386
   Gao F., 2023, P IEEE CVF INT C COM, P7237
   Geitgey A., 2017, Welcome to Face Recognition's documentation!
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Hu XQ, 2022, PROC CVPR IEEE, P18270, DOI 10.1109/CVPR52688.2022.01775
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Ji F, 2022, INT C PATT RECOG, P733, DOI 10.1109/ICPR56361.2022.9956661
   Jiao LC, 2018, PATTERN RECOGN, V76, P125, DOI 10.1016/j.patcog.2017.10.025
   Kim B, 2023, Arxiv, DOI [arXiv:2305.15086, DOI 10.48550/ARXIV.2305.15086]
   Kingma D. P., 2014, arXiv
   Lee K, 2024, Arxiv, DOI [arXiv:2107.04589, DOI 10.48550/ARXIV.2107.04589]
   Li SH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1716, DOI 10.1145/3123266.3123425
   Liang CC, 2024, IEEE T NEUR NET LEAR, V35, P9352, DOI 10.1109/TNNLS.2022.3233025
   Liu QS, 2005, PROC CVPR IEEE, P1005
   Liu SH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6629, DOI 10.1109/ICCV48922.2021.00658
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Messer K., 1999, P 2 INT C AUD VID BA, V964, P965, DOI DOI 10.1177/1550147716668071
   Milborrow S., 2010, Pattern Recognit. Assoc. S. Afr., V201
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Park T., 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Peng CL, 2023, KNOWL-BASED SYST, V259, DOI 10.1016/j.knosys.2022.110026
   Peng CL, 2020, IEEE T IMAGE PROCESS, V29, P8519, DOI 10.1109/TIP.2020.3016502
   Peng CL, 2017, IEEE T CIRC SYST VID, V27, P288, DOI 10.1109/TCSVT.2015.2502861
   Radford A., 2019, OpenAI blog, V1, P9
   Ren DP, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22186725
   Sheng B, 2019, IEEE T VIS COMPUT GR, V25, P3216, DOI 10.1109/TVCG.2018.2866090
   Song YB, 2014, LECT NOTES COMPUT SC, V8694, P800, DOI 10.1007/978-3-319-10599-4_51
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan WG, 2021, NEUROCOMPUTING, V438, P107, DOI 10.1016/j.neucom.2021.01.050
   Wang NN, 2018, PATTERN RECOGN, V76, P215, DOI 10.1016/j.patcog.2017.11.008
   Wang NN, 2017, IEEE T IMAGE PROCESS, V26, P1264, DOI 10.1109/TIP.2017.2651375
   Wang WX, 2023, Arxiv, DOI arXiv:2303.06908
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wen LF, 2023, PROC CVPR IEEE, P18300, DOI 10.1109/CVPR52729.2023.01755
   Wu ZJ, 2022, LECT NOTES COMPUT SC, V13676, P189, DOI 10.1007/978-3-031-19787-1_11
   Xiong R., 2020, INT C MACHINE LEARNI, P10524, DOI 10.48550/arXiv.2002.04745
   Xu B, 2015, Arxiv, DOI arXiv:1505.00853
   Yu J, 2021, IEEE T CYBERNETICS, V51, P4350, DOI 10.1109/TCYB.2020.2972944
   Yu WB, 2023, IEEE T IMAGE PROCESS, V32, P483, DOI 10.1109/TIP.2022.3229614
   Yuqian Zhang, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P64, DOI 10.1007/978-3-319-46604-0_5
   Zhang CY, 2023, PROCEEDINGS OF THE 2023 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2023, P105, DOI 10.1145/3591106.3592257
   Zhang CY, 2022, Arxiv, DOI arXiv:2210.12381
   Zhang DY, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2623485
   Zhang QM, 2022, LECT NOTES COMPUT SC, V13685, P466, DOI 10.1007/978-3-031-19806-9_27
   Zhang QM, 2023, INT J COMPUT VISION, V131, P1141, DOI 10.1007/s11263-022-01739-w
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang SC, 2019, IEEE T NEUR NET LEAR, V30, P1419, DOI 10.1109/TNNLS.2018.2869574
   Zhang W, 2011, PROC CVPR IEEE, P513, DOI 10.1109/CVPR.2011.5995324
   Zhang YX, 2019, IEEE IJCNN, DOI 10.1109/ijcnn.2019.8852211
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou H, 2012, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2012.6247788
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu M., 2021, PROC 13 INT JOINT C, P1352, DOI DOI 10.24963/IJCAI.2021/187
   Zhu MR, 2023, IEEE T CIRC SYST VID, V33, P5200, DOI 10.1109/TCSVT.2023.3253773
   Zhu MR, 2021, INT J COMPUT VISION, V129, P1820, DOI 10.1007/s11263-021-01442-2
   Zhu MR, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1048
NR 65
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD JUN
PY 2024
VL 102
AR 104204
DI 10.1016/j.jvcir.2024.104204
EA JUN 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XD7V1
UT WOS:001259820300001
DA 2024-08-05
ER

PT J
AU Han, LB
   Ren, YZ
   Tao, S
   Zhang, XF
   Gao, WL
AF Han, Libo
   Ren, Yanzhao
   Tao, Sha
   Zhang, Xinfeng
   Gao, Wanlin
TI Reversible data hiding with automatic contrast enhancement for color
   images
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Reversible data hiding; Color image; Automatic contrast enhancement;
   Histogram shifting
AB Automatic contrast enhancement (ACE) is a technique that can automatically enhance the image contrast. Reversible data hiding (RDH) with ACE (ACERDH) can achieve ACE while hiding data. However, some methods with good performance for color images suffer from insufficient enhancement. Therefore, an ACERDH method based on the R, G, B, and V channels enhancement is proposed. First, histogram shifting with contrast control is proposed to enhance the R, G, and B channels. It can prevent contrast degradation and histogram shifting from stopping prematurely. Then, the V channel is enhanced. Since some RDH methods with non -ACE that can well enhance the V channel have a low automation level, histogram shifting with brightness control that can realize ACE very well is proposed. It can effectively avoid over -enhancement by controlling the brightness. Experimental results verify that the proposed method improves the image quality and embedding capability better than some state-of-the-art methods.
C1 [Han, Libo; Tao, Sha; Gao, Wanlin] China Agr Univ, Coll Informat & Elect Engn, Beijing 100083, Peoples R China.
   [Ren, Yanzhao] Beijing Technol & Business Univ, Sch Comp Sci & Engn, Beijing 100048, Peoples R China.
   [Zhang, Xinfeng] Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
   [Han, Libo; Tao, Sha; Gao, Wanlin] China Agr Univ, Key Lab Agr Informatizat Standardizat, Minist Agr & Rural Affairs, Beijing 100083, Peoples R China.
C3 China Agricultural University; Beijing Technology & Business University;
   Beijing University of Technology; China Agricultural University;
   Ministry of Agriculture & Rural Affairs
RP Gao, WL (corresponding author), China Agr Univ, Coll Informat & Elect Engn, Beijing 100083, Peoples R China.
EM gaowlin@cau.edu.cn
CR Chen HS, 2020, COMPUT J, V63, P1584, DOI 10.1093/comjnl/bxaa072
   Chen HS, 2016, SIGNAL PROCESS-IMAGE, V46, P1, DOI 10.1016/j.image.2016.04.006
   Coltuc D, 2024, IEEE T INF FOREN SEC, V19, P529, DOI 10.1109/TIFS.2023.3326662
   Franzen R., 2013, Kodak lossless true color image suite
   Gao GY, 2022, J INF SECUR APPL, V68, DOI 10.1016/j.jisa.2022.103223
   Gao GY, 2021, SIGNAL PROCESS, V178, DOI 10.1016/j.sigpro.2020.107817
   Gao GY, 2015, IEEE SIGNAL PROC LET, V22, P2078, DOI 10.1109/LSP.2015.2459055
   Gao M.-Z., 2013, Adv. Intell. Syst. Appl., V2, P331, DOI [10.1007/978-3-642-35473-133, DOI 10.1007/978-3-642-35473-133]
   Gu K, 2018, IEEE T NEUR NET LEAR, V29, P1301, DOI 10.1109/TNNLS.2017.2649101
   Gu K, 2016, IEEE T CYBERNETICS, V46, P284, DOI 10.1109/TCYB.2015.2401732
   Guan ZH, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102950
   He WG, 2021, IEEE T IMAGE PROCESS, V30, P5045, DOI 10.1109/TIP.2021.3078088
   Hotta M, 2022, DENT MATER J, V41, P11, DOI 10.4012/dmj.2021-032
   Jafar IF, 2016, J VIS COMMUN IMAGE R, V39, P239, DOI 10.1016/j.jvcir.2016.06.002
   Kim S, 2019, IEEE T CIRC SYST VID, V29, P2271, DOI 10.1109/TCSVT.2018.2869935
   Kim S, 2019, IEEE T CIRC SYST VID, V29, P3236, DOI 10.1109/TCSVT.2018.2878932
   Kim S, 2015, IEEE INT WORKS INFOR
   Kurokawa R, 2022, IEICE T INF SYST, VE105D, P54, DOI 10.1587/transinf.2021MUP0001
   Lissner I, 2012, IEEE T IMAGE PROCESS, V21, P1153, DOI 10.1109/TIP.2011.2163522
   Lyu WL, 2023, J VIS COMMUN IMAGE R, V92, DOI 10.1016/j.jvcir.2023.103798
   Mansouri S, 2021, J VIS COMMUN IMAGE R, V81, DOI 10.1016/j.jvcir.2021.103359
   Nunez-Ramirez D, 2022, J KING SAUD UNIV-COM, V34, P5468, DOI 10.1016/j.jksuci.2021.05.007
   Qi WF, 2023, SIGNAL PROCESS, V207, DOI 10.1016/j.sigpro.2023.108956
   Reza ZN, 2016, INT CONF ELECTR ENG
   Sharma G, 2005, COLOR RES APPL, V30, P21, DOI 10.1002/col.20070
   Shi M, 2022, J INF SECUR APPL, V70, DOI 10.1016/j.jisa.2022.103324
   Sugimoto Y, 2022, J IMAGING, V8, DOI [10.3390/jimaging8020027, 10.3390/jirnaging8020027]
   Wang R., 2013, IEEE INT C MULT EXP, P1
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weng SW, 2020, CMC-COMPUT MATER CON, V62, P157, DOI 10.32604/cmc.2020.05681
   Wu HT, 2022, IEEE T CIRC SYST VID, V32, P7605, DOI 10.1109/TCSVT.2022.3180007
   Wu HT, 2021, IEEE SIGNAL PROC LET, V28, P160, DOI 10.1109/LSP.2020.3048840
   Wu HT, 2020, IET IMAGE PROCESS, V14, P327, DOI 10.1049/iet-ipr.2019.0423
   Wu HT, 2019, IEEE ACCESS, V7, P83332, DOI 10.1109/ACCESS.2019.2921407
   Wu HT, 2018, SIGNAL PROCESS-IMAGE, V62, P64, DOI 10.1016/j.image.2017.12.006
   Wu HT, 2015, IEEE SIGNAL PROC LET, V22, P81, DOI 10.1109/LSP.2014.2346989
   Yang Y, 2022, IEEE T CIRC SYST VID, V32, P1860, DOI 10.1109/TCSVT.2021.3084676
   Yang Y, 2020, IEEE SIGNAL PROC LET, V27, P256, DOI 10.1109/LSP.2020.2965826
   Zhang L, 2011, J ELECTRON IMAGING, V20, DOI 10.1117/1.3600632
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang TC, 2022, IEEE T CIRC SYST VID, V32, P5041, DOI 10.1109/TCSVT.2022.3146159
   Zhou JF, 2018, COMPUT ELECTRON AGR, V151, P319, DOI 10.1016/j.compag.2018.06.016
   Zhu SH, 2011, 2011 12TH INTERNATIONAL CONFERENCE ON ELECTRONIC PACKAGING TECHNOLOGY AND HIGH DENSITY PACKAGING (ICEPT-HDP), P664
NR 43
TC 0
Z9 0
U1 6
U2 6
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104181
DI 10.1016/j.jvcir.2024.104181
EA MAY 2024
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TV4B7
UT WOS:001244010800001
DA 2024-08-05
ER

PT J
AU Hu, WT
   Zhou, D
   Zhu, ZX
   Qiao, T
   Yao, Y
   Hassaballah, M
AF Hu, Weitong
   Zhou, Di
   Zhu, Zhenxin
   Qiao, Tong
   Yao, Ye
   Hassaballah, Mahmoud
TI Privacy-preserving face recognition method based on extensible feature
   extraction *
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Privacy preserving; Face recognition; Cloud computing; Extensible
   feature extraction
AB Face recognition (FR) technique has become a pervasive and ubiquitous part of daily lives, from unlocking our smartphones with a glance to being scanned by surveillance cameras in various outdoor locations. When people's face photos are uploaded to the cloud for face recognition processing, they often have legitimate concerns about the privacy and security of their biometric data. A number of privacy -preserving face recognition (PPFR) frameworks have been proposed to address these issues by enabling the cloud to perform face recognition without revealing the identity or features of the face photos. However, these frameworks suffer from several limitations. They rely on computationally intensive operations that increase the cost and time of face recognition, leading to less applications in the real -world scenario. Many current frameworks support only one face recognition method and cannot be extended to different models. To overcome these challenges, in this paper, we propose a PPFR framework with high recognition accuracy based on extensible feature extraction for different application scenarios. In particular, features are extracted by a selective model, such as MobileFaceNet, ResNet-18 or ResNet-50, and encrypted by a randomness -based encryption algorithm in both face owner and user. Cloud service provider (SP) performs face recognition by comparing the Euclidean distances between features received from the above two entities. Extensive experiments verify that the proposed framework has significant advantages in terms of accuracy and efficiency.
C1 [Hu, Weitong; Zhu, Zhenxin; Qiao, Tong; Yao, Ye] Hangzhou Dianzi Univ, Sch Cyberspace, Hangzhou, Peoples R China.
   [Zhou, Di] Zhejiang Uniview Technol Co Ltd, Hangzhou, Peoples R China.
   [Hassaballah, Mahmoud] Prince Sattam Bin Abdulaziz Univ, Coll Comp Engn & Sci, Dept Comp Sci, Alkharj, Saudi Arabia.
C3 Hangzhou Dianzi University; Prince Sattam Bin Abdulaziz University
RP Zhou, D (corresponding author), Zhejiang Uniview Technol Co Ltd, Hangzhou, Peoples R China.
EM zhoudi@uniview.com
RI Qiao, Tong/AAA-9982-2022; Hassaballah, Mahmoud/A-5197-2018
OI Qiao, Tong/0000-0003-4912-2132; Hassaballah, Mahmoud/0000-0001-5655-8511
FU Zhejiang Provincial Natural Science Foundation of China [LQ21F020013,
   LZ23F020006]
FX This work was supported by Zhejiang Provincial Natural Science
   Foundation of China (No. LQ21F020013, No. LZ23F020006) .
CR Abaza A, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2431211.2431221
   Barni M, 2015, IEEE SIGNAL PROC MAG, V32, P66, DOI 10.1109/MSP.2015.2438131
   Bowyer KW, 2004, IEEE TECHNOL SOC MAG, V23, P9, DOI 10.1109/MTAS.2004.1273467
   Chen S, 2018, LECT NOTES COMPUT SC, V10996, P428, DOI 10.1007/978-3-319-97909-0_46
   Erkin Z, 2009, LECT NOTES COMPUT SC, V5672, P235, DOI 10.1007/978-3-642-03168-7_14
   Dos Santos CFG, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3490235
   Guo SW, 2019, SIGNAL PROCESS, V164, P320, DOI 10.1016/j.sigpro.2019.06.024
   Hammouche R, 2022, EXPERT SYST APPL, V197, DOI 10.1016/j.eswa.2022.116743
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu GQ, 2023, IEEE T CLOUD COMPUT, V11, P111, DOI 10.1109/TCC.2021.3081564
   Huang G.B., 2008, WORKSH FAC REAL LIF
   Jain AK, 2008, EURASIP J ADV SIG PR, DOI 10.1155/2008/579416
   Korshunov P, 2013, 2013 10TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2013), P208, DOI 10.1109/AVSS.2013.6636641
   Li MH, 2022, ENG APPL ARTIF INTEL, V110, DOI 10.1016/j.engappai.2022.104669
   Maiorana E, 2022, PATTERN RECOGN LETT, V156, P29, DOI 10.1016/j.patrec.2022.03.002
   Moschoglou S, 2017, IEEE COMPUT SOC CONF, P1997, DOI 10.1109/CVPRW.2017.250
   Newton EM, 2005, IEEE T KNOWL DATA EN, V17, P232, DOI 10.1109/TKDE.2005.32
   Qiao T., 2022, IEEE Trans. Multimed., P1
   Qin Z, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P497, DOI 10.1145/2647868.2654941
   Rahulamathavan Y, 2015, IEEE ICC, P7102, DOI 10.1109/ICC.2015.7249459
   Rajasekar V, 2023, INT J ARTIF INTELL T, V32, DOI 10.1142/S0218213023400171
   Sadeghi AR, 2010, LECT NOTES COMPUT SC, V5984, P229
   Sethi M, 2023, SOFT COMPUT, V27, P6307, DOI 10.1007/s00500-023-07907-5
   Shan ZH, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3158363
   Teoh ABJ, 2006, IEEE T PATTERN ANAL, V28, P1892, DOI 10.1109/TPAMI.2006.250
   Wang M., 2020, P IEEE C COMP VIS PA
   Wang M, 2022, IEEE T PATTERN ANAL, V44, P8433, DOI 10.1109/TPAMI.2021.3103191
   Wang YJ, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/260148
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xia ZH, 2018, IEEE T CLOUD COMPUT, V6, P276, DOI 10.1109/TCC.2015.2491933
   Xia ZM, 2022, INFORM SCIENCES, V607, P654, DOI 10.1016/j.ins.2022.06.003
   Xiang C, 2016, SOFT COMPUT, V20, P3735, DOI 10.1007/s00500-015-1759-5
   Xiang T, 2016, IEEE T INF FOREN SEC, V11, P951, DOI 10.1109/TIFS.2016.2515503
   Zhang L, 2017, IEEE T PARALL DISTR, V28, P3258, DOI 10.1109/TPDS.2017.2712148
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
NR 35
TC 1
Z9 1
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
DI 10.1016/j.jvcir.2024.104140
EA APR 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RG8S3
UT WOS:001226611100001
DA 2024-08-05
ER

PT J
AU Li, JH
   Hou, XS
   Wang, HK
   Bi, SH
   Qian, XM
AF Li, Junhui
   Hou, Xingsong
   Wang, Huake
   Bi, Shuhao
   Qian, Xueming
TI AMP-BCS: AMP-based image block compressed sensing with permutation of
   sparsified DCT coefficients
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Block compressed sensing; Approximate message passing; Coefficient
   permutation; Discrete cosine transform; Convolutional neural network
ID FRAMEWORK
AB Block compressive sensing (BCS), an emerging approach for signal acquisition and reconstruction, combines high-speed sampling and compression, making it widely applicable in various imaging tasks. However, image BCS generally face the issues: challenges in accurate sampling rate allocation (SRA) and block artifact removal, and poor reconstruction algorithms. In this paper, we propose an approximate message passing (AMP) -based BCS (AMP-BCS) method. Specifically, within the sampling module, a sparsified DCT coefficientbased permutation strategy is proposed to achieve uniform energy distribution among blocks, effectively addressing the issue of SRA. Within the reconstruction module, by reweighting shallow and deep multi -scale features using several attention mechanisms, the multi -scale deep attention network (MDANet) is proposed to improve the denoising capabilities of the AMP reconstruction. Through independent sampling and joint iterative denoising, block artifacts are substantially removed. Extensive experiments demonstrate that the AMPBCS method significantly outperforms current state-of-the-art BCS algorithms in both visual perception and objective metrics.
C1 [Li, Junhui; Hou, Xingsong; Wang, Huake; Bi, Shuhao; Qian, Xueming] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University
RP Hou, XS (corresponding author), Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Xian 710049, Peoples R China.
EM houxs@mail.xjtu.edu.cn
OI Li, Junhui/0000-0001-9143-1321
FU National Natural Science Foundation of China [62272376, 61872286]; Key
   Research and Development Program of Shaanxi Province [2020ZDLGY04-05,
   S2021-YF-YBSF-0094]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62272376 and Grant 61872286 and in part
   by the Key Research and Development Program of Shaanxi Province under
   Grant 2020ZDLGY04-05 and Grant S2021-YF-YBSF-0094.
CR Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bello I, 2019, IEEE I CONF COMP VIS, P3285, DOI 10.1109/ICCV.2019.00338
   Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002
   Chen B, 2022, IEEE T IMAGE PROCESS, V31, P5412, DOI 10.1109/TIP.2022.3195319
   Chen J, 2020, J VIS COMMUN IMAGE R, V66, DOI 10.1016/j.jvcir.2019.102734
   Chen TG, 2016, OPT LASER TECHNOL, V84, P118, DOI 10.1016/j.optlastec.2016.05.012
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   Chen Z, 2021, IEEE T IMAGE PROCESS, V30, P7112, DOI 10.1109/TIP.2021.3088611
   Chen Z, 2020, IEEE T CIRC SYST VID, V30, P1109, DOI 10.1109/TCSVT.2019.2898908
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Deng LJ, 2022, IEEE GEOSC REM SEN M, V10, P279, DOI 10.1109/MGRS.2022.3187652
   Donoho DL, 2009, P NATL ACAD SCI USA, V106, P18914, DOI 10.1073/pnas.0909892106
   Fang H, 2014, IEEE T SIGNAL PROCES, V62, P196, DOI 10.1109/TSP.2013.2284762
   Feng L, 2016, NEUROCOMPUTING, V216, P45, DOI 10.1016/j.neucom.2016.07.012
   Fowler JE, 2011, EUR SIGNAL PR CONF, P564
   Gan HP, 2023, IEEE T COMPUT IMAG, V9, P133, DOI 10.1109/TCI.2023.3244396
   Gan L, 2007, PROCEEDINGS OF THE 2007 15TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, P403
   Gao ZF, 2023, IEEE T MED IMAGING, V42, P1859, DOI 10.1109/TMI.2023.3240862
   Gou Y., 2022, Advances in Neural Information Processing Systems, P14099
   He LH, 2009, IEEE T SIGNAL PROCES, V57, P3488, DOI 10.1109/TSP.2009.2022003
   Huang Y, 2021, KNOWL-BASED SYST, V231, DOI 10.1016/j.knosys.2021.107384
   Hui C., 2022, IEEE INT C MULTIMEDI, P1
   Jiang QR, 2020, IEEE T MULTIMEDIA, V22, P594, DOI 10.1109/TMM.2019.2931400
   Johnson D. H., 2006, Scholarpedia, V1, P2088, DOI [DOI 10.4249/SCHOLARPEDIA.2046, DOI 10.4249/SCHOLARPEDIA.2088]
   Kingma D. P., 2014, arXiv
   Kulkarni K, 2016, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2016.55
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Liu PJ, 2018, IEEE COMPUT SOC CONF, P886, DOI 10.1109/CVPRW.2018.00121
   Liu SC, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3179288
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Metzler C., 2017, ADV NEURAL INFORM PR, P1772, DOI DOI 10.5555/3294771
   Metzler CA, 2016, IEEE T INFORM THEORY, V62, P5117, DOI 10.1109/TIT.2016.2556683
   Monika R, 2022, OCEANS-IEEE, DOI 10.1109/OCEANSChennai45887.2022.9775497
   Monika R, 2022, IEEE SENS J, V22, P776, DOI 10.1109/JSEN.2021.3130947
   Mou C, 2022, PROC CVPR IEEE, P17378, DOI 10.1109/CVPR52688.2022.01688
   Mun S, 2009, IEEE IMAGE PROC, P3021, DOI 10.1109/ICIP.2009.5414429
   Ren C, 2021, PROC CVPR IEEE, P8592, DOI 10.1109/CVPR46437.2021.00849
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shang C, 2022, J VIS COMMUN IMAGE R, V85, DOI 10.1016/j.jvcir.2022.103524
   Sun B, 2023, IEEE T COGN DEV SYST, V15, P591, DOI 10.1109/TCDS.2022.3167042
   Tian CW, 2020, NEURAL NETWORKS, V124, P117, DOI 10.1016/j.neunet.2019.12.024
   Timofte R., 2017, P IEEE C COMPUTER VI, P114
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Wang HK, 2023, IEEE T IMAGE PROCESS, V32, P2761, DOI 10.1109/TIP.2023.3274967
   Wang L, 2021, J VIS COMMUN IMAGE R, V80, DOI 10.1016/j.jvcir.2021.103300
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yao C, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11030418
   Yuan QQ, 2018, IEEE J-STARS, V11, P978, DOI 10.1109/JSTARS.2018.2794888
   Zhai GT, 2011, IEEE IMAGE PROC, P1857, DOI 10.1109/ICIP.2011.6115828
   Zhang B, 2019, J VIS COMMUN IMAGE R, V60, P69, DOI 10.1016/j.jvcir.2019.02.023
   Zhang J, 2020, IEEE J-STSP, V14, P765, DOI 10.1109/JSTSP.2020.2977507
   Zhang J, 2018, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR.2018.00196
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang JG, 2017, MULTIMED TOOLS APPL, V76, P4227, DOI 10.1007/s11042-016-3496-x
   Zhang K, 2022, IEEE T PATTERN ANAL, V44, P6360, DOI 10.1109/TPAMI.2021.3088914
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang KY, 2023, IEEE T MULTIMEDIA, V25, P5676, DOI 10.1109/TMM.2022.3198323
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang ZH, 2021, IEEE T IMAGE PROCESS, V30, P1487, DOI 10.1109/TIP.2020.3044472
   Zhou K, 2022, J VIS COMMUN IMAGE R, V88, DOI 10.1016/j.jvcir.2022.103596
   Zhou SW, 2021, IEEE T MULTIMEDIA, V23, P2627, DOI 10.1109/TMM.2020.3014561
   Zhu SY, 2015, J VIS COMMUN IMAGE R, V30, P94, DOI 10.1016/j.jvcir.2015.03.006
NR 66
TC 0
Z9 0
U1 4
U2 4
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104092
DI 10.1016/j.jvcir.2024.104092
EA FEB 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MM9M4
UT WOS:001194155900001
DA 2024-08-05
ER

PT J
AU Khandouzi, A
   Ezoji, M
AF Khandouzi, Ali
   Ezoji, Mehdi
TI Coarse-to-fine underwater image enhancement with lightweight CNN and
   attention-based refinement
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Underwater image enhancement; Deep learning; Image processing;
   Convolutional neural network; Attention module; Modified histogram
   equalization
ID QUALITY ASSESSMENT
AB This paper presents a deep learning-based underwater image enhancement method supported by a classical image processing technique. The proposed method includes an end-to-end three-module structure. The first module is a lightweight two-branch network that retrieves lost colors and to some extent overall appearance through the global and local image enhancement. The second module is the modified histogram equalization to improve the global intensity, contrast and local colors of the image by controlling over-intensity and artificial colors that may result from histogram equalization. The last part is the attention module, utilized to help the proposed framework have a synergistic combination of the previous modules. The attention module is designed to combine the advantages of the previous modules and evade their drawbacks. Experiments to objectively and subjectively evaluate the performance of the proposed model show that the proposed model is superior to existing underwater image enhancement methods.
C1 [Khandouzi, Ali; Ezoji, Mehdi] Babol Noshirvani Univ Technol, Fac Elect & Comp Engn, Babol, Iran.
C3 Babol Noshirvani University of Technology
RP Ezoji, M (corresponding author), Babol Noshirvani Univ Technol, Fac Elect & Comp Engn, Babol, Iran.
EM m.ezoji@nit.ac.ir
FU Babol Noshirvani University of Technology [BNUT/389079/1401-5]
FX The authors acknowledge the funding support of Babol Noshirvani
   University of Technology through Grant program No. BNUT/389079/1401-5
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252
   Ancuti C, 2012, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2012.6247661
   Fabbri C, 2018, IEEE INT CONF ROBOT, P7159
   Fu XY, 2020, SIGNAL PROCESS-IMAGE, V86, DOI 10.1016/j.image.2020.115892
   Fu Z., 2022, EUROPEAN C COMPUTER
   Fu ZQ, 2022, INT CONF ACOUST SPEE, P2764, DOI 10.1109/ICASSP43922.2022.9747758
   Galdran A, 2015, J VIS COMMUN IMAGE R, V26, P132, DOI 10.1016/j.jvcir.2014.11.006
   Gao SB, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2919947
   Gonzalez R, 2003, Digital image processing using MATLAB, VSecond
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Huang YF, 2022, SIGNAL PROCESS-IMAGE, V107, DOI 10.1016/j.image.2022.116797
   JAFFE JS, 1990, IEEE J OCEANIC ENG, V15, P101, DOI 10.1109/48.50695
   Jian MW, 2021, SIGNAL PROCESS-IMAGE, V91, DOI 10.1016/j.image.2020.116088
   Jiang Duan, 2004, Proceedings. Third International Conference on Image and Graphics, P55
   Jiang Q, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108324
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Khandouzi A., 2022, 2022 INT C MACHINE V, P1, DOI [10.1109/MVIP53647.2022.9738773, DOI 10.1109/MVIP53647.2022.9738773]
   Kingma D. P., 2014, arXiv
   Li CY, 2016, IEEE T IMAGE PROCESS, V26, P5664, DOI 10.1109/TIP.2016.2612882
   Li CY, 2021, IEEE T IMAGE PROCESS, V30, P4985, DOI 10.1109/TIP.2021.3076367
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li F, 2023, SIGNAL PROCESS-IMAGE, V110, DOI 10.1016/j.image.2022.116890
   Liu K, 2021, OPT EXPRESS, V29, P10321, DOI 10.1364/OE.413164
   Liu RS, 2020, IEEE T CIRC SYST VID, V30, P4861, DOI 10.1109/TCSVT.2019.2963772
   McGlamery B. L., 1979, Proceedings of the Society of Photo-Optical Instrumentation Engineers, V208, P221
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Muniraj M, 2023, SIGNAL PROCESS-IMAGE, V113, DOI 10.1016/j.image.2023.116939
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Peng YT, 2018, IEEE T IMAGE PROCESS, V27, P2856, DOI 10.1109/TIP.2018.2813092
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Rad MS, 2019, IEEE I CONF COMP VIS, P2710, DOI 10.1109/ICCV.2019.00280
   Schettini R, 2010, EURASIP J ADV SIG PR, DOI 10.1155/2010/746052
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wang YD, 2021, SIGNAL PROCESS-IMAGE, V96, DOI 10.1016/j.image.2021.116250
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Xue XW, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.109041
   Yang HH, 2021, IEEE INT CONF ROBOT, P685, DOI 10.1109/ICRA48506.2021.9561263
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Zhang C, 2018, LECT NOTES ELECTR EN, V451, P81, DOI 10.1007/978-981-10-5768-7_8
   Zhang WD, 2021, SIGNAL PROCESS-IMAGE, V90, DOI 10.1016/j.image.2020.116030
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zhuang PX, 2021, ENG APPL ARTIF INTEL, V101, DOI 10.1016/j.engappai.2021.104171
NR 44
TC 1
Z9 1
U1 3
U2 3
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104068
DI 10.1016/j.jvcir.2024.104068
EA JAN 2024
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JH5T2
UT WOS:001172293400001
DA 2024-08-05
ER

PT J
AU Jia, ZX
   Hou, SJ
   Li, P
AF Jia, Zhixiang
   Hou, Sujuan
   Li, Peng
TI Context-based modeling for accurate logo detection in complex
   environments
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Logo detection; Feature enhancement; Contextual information; Object
   detection
AB Logo detection involves the tasks of locating and classifying logo objects in images and videos, and has been widely applied in the real world. However, most existing approaches rely on general object detection strategies that do not fully utilize the unique characteristics of logos. This can lead to sub -optimal performance in complex environments, especially when logos are small or have varying sizes and shapes. We observe that logos belonging to the same category often share similar context information, such as background dependency of the logo. This motivates us to incorporate contextual information to improve logo detection. Our proposed method, Context -based Modeling Enhancement Network (CME-Net), aims to enhance the distinctive region feature of logos using contextual information. We achieve this by modeling both the logo and its background region to extract their contextual information. This contextual information serves as a guide for enhancing the saliency of the distinctive regions within the logo image. To further improve the accuracy of detection, we have implemented a scale feature balance strategy. This strategy solves the problem of losing scale information caused by enhancement, ensuring that all scales are appropriately considered. Additionally, noise generated during the enhancement process is also effectively suppressed. By effectively leveraging contextual information, our method successfully tackles the challenge of accurately locating logo objects. Our extensive experiments on four public benchmark datasets demonstrate that CME-Net improves the accuracy of logo detection in complex environments.
C1 [Jia, Zhixiang; Hou, Sujuan] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250014, Shandong, Peoples R China.
   [Li, Peng] Shandong Normal Univ, Sch Math & Stat, Jinan 250014, Shandong, Peoples R China.
C3 Shandong Normal University; Shandong Normal University
RP Hou, SJ (corresponding author), Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250014, Shandong, Peoples R China.
EM sujuanhou@sdnu.edu.cn
FU National Natural Science Foundation of China [62072289, 62372278]
FX <B>Acknowledgments</B> This work is supported by the National Natural
   Science Foundation of China (62072289, 62372278) .
CR Bao Y, 2016, 8TH INTERNATIONAL CONFERENCE ON INTERNET MULTIMEDIA COMPUTING AND SERVICE (ICIMCS2016), P319, DOI 10.1145/3007669.3007728
   Bianco S, 2017, NEUROCOMPUTING, V245, P23, DOI 10.1016/j.neucom.2017.03.051
   Hoi SCH, 2015, Arxiv, DOI arXiv:1511.02462
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Chen G, 2022, IEEE T CIRC SYST VID, V32, P6981, DOI 10.1109/TCSVT.2022.3178173
   Chen H, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4789, DOI 10.1145/3474085.3479227
   ChengdeWan Zhicheng Zhao, 2013, Visual Communications and Image Processing, P1
   Constantinopoulos C, 2011, IEEE INT CON MULTI
   Ding HH, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.109018
   Feng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3490, DOI 10.1109/ICCV48922.2021.00349
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hou Q, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4670, DOI 10.1145/3474085.3475289
   Hou SJ, 2023, IEEE INT CON MULTI, P1493, DOI 10.1109/ICME55011.2023.00258
   Jiaqi Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P403, DOI 10.1007/978-3-030-58548-8_24
   Li CY, 2022, Arxiv, DOI [arXiv:2209.02976, DOI 10.48550/ARXIV.2209.02976]
   Li GY, 2023, IEEE T IMAGE PROCESS, V32, P5257, DOI 10.1109/TIP.2023.3314285
   Li GY, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3145483
   Li GY, 2023, IEEE T CYBERNETICS, V53, P526, DOI 10.1109/TCYB.2022.3162945
   Li LL, 2022, PROC CVPR IEEE, P1236, DOI 10.1109/CVPR52688.2022.00131
   Li S, 2022, PROC CVPR IEEE, P9377, DOI 10.1109/CVPR52688.2022.00917
   Li X., 2020, ADV NEURAL INF PROCE, V33, P21002
   Li XZ, 2023, ENTROPY-SWITZ, V25, DOI 10.3390/e25010174
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu HM, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3274926
   Liu KH, 2021, IEEE SYS MAN CYBERN, P3013, DOI 10.1109/SMC52423.2021.9658730
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Z., 2023, IEEE Trans. Circuits Syst, VII
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu L, 2023, NEUROCOMPUTING, V536, P40, DOI 10.1016/j.neucom.2023.03.035
   Mahaur B, 2023, EXPERT SYST APPL, V234, DOI 10.1016/j.eswa.2023.121036
   Meng Y, 2021, DISPLAYS, V70, DOI 10.1016/j.displa.2021.102090
   mmfewshot Contributors, 2021, OpenMMLab few shot learning toolbox and benchmark
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Qi Y., 2023, IEEE Trans. Image Process
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Romberg S., 2011, P 1 ACM INT C MULT R, P1
   Snell J, 2017, ADV NEUR IN, V30
   Su H., 2018, BRIT MACHINE VISION
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Trappey AJC, 2022, ADV ENG INFORM, V52, DOI 10.1016/j.aei.2022.101567
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang J, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3466780
   Wang M, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2023.103752
   Wang Q, 2021, J VIS COMMUN IMAGE R, V79, DOI 10.1016/j.jvcir.2021.103260
   Wang Yang, 2022, 2022 IEEE 6th Information Technology and Mechatronics Engineering Conference (ITOEC), P656, DOI 10.1109/ITOEC53115.2022.9734458
   Wu YH, 2023, IEEE T PATTERN ANAL, V45, P12760, DOI 10.1109/TPAMI.2022.3202765
   Xiao JS, 2023, EXPERT SYST APPL, V211, DOI 10.1016/j.eswa.2022.118665
   Ye ZY, 2023, J VIS COMMUN IMAGE R, V95, DOI 10.1016/j.jvcir.2023.103903
   Yue Wu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10183, DOI 10.1109/CVPR42600.2020.01020
   Zhang BS, 2023, MATHEMATICS-BASEL, V11, DOI 10.3390/math11020481
   Zhang H., 2020, P EUR C COMP VIS, P260, DOI DOI 10.1007/978-3-030-58555-6_16
   Zhang HY, 2023, PATTERN RECOGN, V143, DOI 10.1016/j.patcog.2023.109801
   Zhang S., 2020, P IEEE CVF C COMP VI, P9759
   Zhang XW, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109098
   Zhao ZP, 2023, PATTERN RECOGN, V140, DOI 10.1016/j.patcog.2023.109579
   Zhou TF, 2022, PROC CVPR IEEE, P2572, DOI 10.1109/CVPR52688.2022.00261
NR 60
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104061
DI 10.1016/j.jvcir.2024.104061
EA JAN 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JB4N7
UT WOS:001170682000001
DA 2024-08-05
ER

PT J
AU Fu, HY
   Yang, YJ
   Mishra, VK
   Kuo, CCJ
AF Fu, Hongyu
   Yang, Yijing
   Mishra, Vinod K.
   Kuo, C. -C. Jay
TI Subspace learning machine (SLM): Methodology and performance evaluation
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Machine learning; Subspace learning; Classification; Regression
ID MULTILAYER FEEDFORWARD NETWORKS; NEURAL-NETWORKS; APPROXIMATION;
   PERCEPTRON
AB Inspired by the feedforward multilayer perceptron (FF-MLP), decision tree (DT) and extreme learning machine (ELM), a new classification model, called the subspace learning machine (SLM), is proposed in this work. SLM first identifies a discriminant subspace, S degrees, by examining the discriminant power of each input feature. Then, it uses probabilistic projections of features in S degrees to yield 1D subspaces and finds the optimal partition for each of them. This is equivalent to partitioning S degrees with hyperplanes. A criterion is developed to choose the best q partitions that yield 2q partitioned subspaces among them. We assign S degrees to the root node of a decision tree and the intersections of 2q subspaces to its child nodes of depth one. The partitioning process is recursively applied at each child node to build an SLM tree. When the samples at a child node are sufficiently pure, the partitioning process stops and each leaf node makes a prediction. The idea can be generalized to regression, leading to the subspace learning regressor (SLR). Furthermore, ensembles of SLM/SLR trees can yield a stronger predictor. Extensive experiments are conducted for performance benchmarking among SLM/SLR trees, ensembles and classical classifiers/regressors.
C1 [Fu, Hongyu; Yang, Yijing; Kuo, C. -C. Jay] Univ Southern Calif, Los Angeles, CA 90007 USA.
   [Mishra, Vinod K.] Army Res Lab, Adelphi, MD USA.
C3 University of Southern California; United States Department of Defense;
   US Army Research, Development & Engineering Command (RDECOM); US Army
   Research Laboratory (ARL); United States Army
RP Fu, HY (corresponding author), Univ Southern Calif, Los Angeles, CA 90007 USA.
EM hongyufu@usc.edu; yijingya@usc.edu; vinod.k.mishra.civ@army.mil;
   jckuo@usc.edu
RI Yang, Yijing/HQB-2970-2023; Kuo, C.-C. Jay/A-7110-2011
OI Yang, Yijing/0000-0003-4197-2570; Kuo, C.-C. Jay/0000-0001-9474-5035
FU US Army Research Laboratory (ARL) [W911NF2020157]; University of
   Southern California's Center for Advanced Research Computing (CARC)
FX <B>Acknowledgments</B> This material is based on research sponsored by
   US Army Research Laboratory (ARL) under contract number W911NF2020157.
   The U.S. Government is authorized to reproduce and distribute reprints
   for Gov-ernmental purposes notwithstanding any copyright notation
   thereon. The views and conclusions contained herein are those of the
   authors and should not be interpreted as necessarily representing the
   official policies or endorsements, either expressed or implied, of US
   Army Research Laboratory (ARL) or the U.S. Government. Computation for
   the work was supported by the University of Southern California's Center
   for Advanced Research Computing (CARC) .
CR Ahad A, 2002, ISCON 2002: IEEE STUDENTS CONFERENCE ON EMERGING TECHNOLOGIES, PROCEEDINGS, P103
   Amit Y, 1997, NEURAL COMPUT, V9, P1545, DOI 10.1162/neco.1997.9.7.1545
   [Anonymous], 2013, International Journal of Computing Algorithm
   [Anonymous], 1989, INT 1989 JOINT C NEU, V1, P613, DOI 10.1109/IJCNN.1989.118640
   Asuncion Arthur, 2007, UCI machine learning repository
   Blewitt ME, 2008, NAT GENET, V40, P663, DOI 10.1038/ng.142
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Chen T. He, 2015, R PACKAGE VERSION, V2, P1, DOI DOI 10.1145/2939672.2939785
   Chen TQ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2939672.2939785
   Chen YR, 2020, IEEE IMAGE PROC, P3294, DOI 10.1109/ICIP40778.2020.9191012
   Chen YR, 2020, J VIS COMMUN IMAGE R, V70, DOI 10.1016/j.jvcir.2019.102749
   Chen YR, 2018, PICT COD SYMP, P174, DOI 10.1109/PCS.2018.8456277
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Dietterich TG, 2000, MACH LEARN, V40, P139, DOI 10.1023/A:1007607513941
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Frean M, 1990, NEURAL COMPUT, V2, P198
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963
   Gallant S I, 1990, IEEE Trans Neural Netw, V1, P179, DOI 10.1109/72.80230
   GLOVER F, 1986, COMPUT OPER RES, V13, P533, DOI 10.1016/0305-0548(86)90048-1
   Göpfert JP, 2022, NEUROCOMPUTING, V470, P344, DOI 10.1016/j.neucom.2021.05.105
   Ho TK, 1998, IEEE T PATTERN ANAL, V20, P832, DOI 10.1109/34.709601
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Huang GB, 2008, NEUROCOMPUTING, V71, P3460, DOI 10.1016/j.neucom.2007.10.008
   Huang GB, 2007, NEUROCOMPUTING, V70, P3056, DOI 10.1016/j.neucom.2007.02.009
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Huang GB, 2006, IEEE T NEURAL NETWOR, V17, P879, DOI 10.1109/TNN.2006.875977
   Jihoon Yang, 1999, Intelligent Data Analysis, V3, P55, DOI 10.1016/S1088-467X(99)00005-0
   KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuo CCJ, 2018, J VIS COMMUN IMAGE R, V50, P237, DOI 10.1016/j.jvcir.2017.11.023
   Kuo CCJ, 2016, J VIS COMMUN IMAGE R, V41, P406, DOI 10.1016/j.jvcir.2016.11.003
   Kuo CCJ, 2019, J VIS COMMUN IMAGE R, V60, P346, DOI 10.1016/j.jvcir.2019.03.010
   Kwok TY, 1997, IEEE T NEURAL NETWOR, V8, P1131, DOI 10.1109/72.623214
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   LESHNO M, 1993, NEURAL NETWORKS, V6, P861, DOI 10.1016/S0893-6080(05)80131-5
   Lin RY, 2020, Arxiv, DOI arXiv:2009.04442
   Marchand M, 1989, Complex Syst., V3, P229
   MASCIOLI FMF, 1995, IEEE T NEURAL NETWOR, V6, P794, DOI 10.1109/72.377991
   MEZARD M, 1989, J PHYS A-MATH GEN, V22, P2191, DOI 10.1088/0305-4470/22/12/019
   Parekh R, 2000, IEEE T NEURAL NETWOR, V11, P436, DOI 10.1109/72.839013
   Parekh R., 1995, Constructive Neural Network Learning Algorithms for Multi-Category Pattern Classification
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1023/A:1022643204877
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   SIVAKUMAR K, 1993, IEEE T SIGNAL PROCES, V41, P2018, DOI 10.1109/78.215329
   Smith J. W., 1988, Proceedings. The Twelfth Annual Symposium on Computer Applications in Medical Care (IEEE Cat. No.88CH2616-1), P261
   Vaswani A, 2017, ADV NEUR IN, V30
   Yang YJ, 2022, Arxiv, DOI arXiv:2203.11924
NR 53
TC 0
Z9 0
U1 5
U2 5
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104058
DI 10.1016/j.jvcir.2024.104058
EA JAN 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IE7O2
UT WOS:001164717900001
DA 2024-08-05
ER

PT J
AU Zhao, Q
   Yu, WT
   Ji, TY
AF Zhao, Qian
   Yu, Wentao
   Ji, Tangyu
TI Style Elimination and Information Restitution for generalizable person
   re-identification
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Domain generalization; Person re-identification; Instance normalization;
   Convolutional neural networks
ID CROSS-DOMAIN; ADAPTATION; NETWORK
AB Domain generalizable person re -identification (DG ReID) aims to obtain a model that can be applied directly to unseen domains once trained on a set of source domains (datasets collected from different camera networks). Among current DG ReID methods, instance normalization is a promising solution to reduce the effect of domain bias, but it inevitably filters out some discriminative information. Besides, since most pioneering approaches cannot effectively address the loss of discriminative information, this paper proposes a Style Elimination and Information Restitution (SEIR) module and constructs a generalizable framework. Specifically, we utilize instance normalization to reinforce the generalization capability on unseen domains. Then, the instance -specific mean and variance are employed to construct a feature vector that includes discriminative and style information. Besides, an encoder-decoder structure is designed to restitute the discriminative information to ensure a high recognition rate. Finally, a dual optimization strategy is devised to update the proposed module by simulating a real train -test process to enhance the generalization robustness further and prevent SEIR from overfitting to the source domain. Extensive experimental results demonstrate that the proposed method outperforms the state-of-the-art methods by a large margin on the public DG ReID benchmarks.
C1 [Zhao, Qian; Yu, Wentao; Ji, Tangyu] Shanghai Univ Elect Power, Coll Elect & Informat Engn, Shanghai 201306, Peoples R China.
C3 Shanghai University of Electric Power
RP Yu, WT (corresponding author), Shanghai Univ Elect Power, Coll Elect & Informat Engn, Shanghai 201306, Peoples R China.
EM yuuwtz@mail.shiep.edu.cn
OI ji, tangyu/0009-0001-6291-4786
CR Bai Y, 2021, PROC CVPR IEEE, P2123, DOI 10.1109/CVPR46437.2021.00216
   Chen KX, 2024, IEEE T CIRC SYST VID, V34, P357, DOI 10.1109/TCSVT.2023.3285046
   Chen PX, 2021, AAAI CONF ARTIF INTE, V35, P1054
   Chen ZN, 2024, IEEE T MULTIMEDIA, V26, P3554, DOI 10.1109/TMM.2023.3312939
   Choi S, 2021, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR46437.2021.00343
   Dai YX, 2021, PROC CVPR IEEE, P16140, DOI 10.1109/CVPR46437.2021.01588
   Delussu R, 2021, INT C PATT RECOG, P3829, DOI 10.1109/ICPR48806.2021.9412485
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Dou ZP, 2023, IEEE I CONF COMP VIS, P15801, DOI 10.1109/ICCV51070.2023.01452
   Gong TT, 2023, IEEE T CIRC SYST VID, V33, P5947, DOI 10.1109/TCSVT.2023.3262832
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Han K, 2022, AAAI CONF ARTIF INTE, P817
   He T, 2022, AAAI CONF ARTIF INTE, P879
   Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9
   Hou RB, 2022, IEEE T PATTERN ANAL, V44, P4894, DOI 10.1109/TPAMI.2021.3079910
   Huang ZP, 2022, PROC CVPR IEEE, P14268, DOI 10.1109/CVPR52688.2022.01389
   Jia J., 2019, BRIT MACHINE VISION
   Jiao BL, 2022, LECT NOTES COMPUT SC, V13674, P285, DOI 10.1007/978-3-031-19781-9_17
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Kaiyang Zhou, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P561, DOI 10.1007/978-3-030-58517-4_33
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li W, 2013, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2013.461
   Lin CS, 2021, INT C PATT RECOG, P6758, DOI 10.1109/ICPR48806.2021.9413013
   Lin YT, 2020, IEEE T IMAGE PROCESS, V29, P5481, DOI 10.1109/TIP.2020.2982826
   Liu JX, 2018, PROC CVPR IEEE, P4099, DOI 10.1109/CVPR.2018.00431
   Loy CC, 2009, PROC CVPR IEEE, P1988, DOI 10.1109/CVPRW.2009.5206827
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Ni H, 2023, IEEE I CONF COMP VIS, P11246, DOI 10.1109/ICCV51070.2023.01036
   Ni H, 2022, PROC CVPR IEEE, P2477, DOI 10.1109/CVPR52688.2022.00252
   Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29
   Qin WC, 2022, IEEE SIGNAL PROC LET, V29, P2672, DOI 10.1109/LSP.2022.3233002
   Shengcai Liao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P456, DOI 10.1007/978-3-030-58621-8_27
   Song JF, 2019, PROC CVPR IEEE, P719, DOI 10.1109/CVPR.2019.00081
   Song LC, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2019.107173
   Tan WT, 2024, IEEE T MULTIMEDIA, V26, P1600, DOI 10.1109/TMM.2023.3283878
   Thrun S, 1998, LEARNING TO LEARN, P3
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang GC, 2019, AAAI CONF ARTIF INTE, P8933
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu JJ, 2023, MULTIMED TOOLS APPL, V82, P38639, DOI 10.1007/s11042-023-14718-1
   Wu YH, 2022, AAAI CONF ARTIF INTE, P2750
   Xiang SC, 2024, MACH LEARN, V113, P1921, DOI 10.1007/s10994-023-06352-7
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Xu BQ, 2022, LECT NOTES COMPUT SC, V13674, P372, DOI 10.1007/978-3-031-19781-9_22
   Xu X, 2022, PATTERN RECOGN, V132, DOI 10.1016/j.patcog.2022.108954
   Yan SY, 2022, NEUROCOMPUTING, V486, P123, DOI 10.1016/j.neucom.2021.11.016
   Yuan Y, 2020, IEEE WINT CONF APPL, P3578, DOI [10.1109/WACV45572.2020.9093521, 10.1109/wacv45572.2020.9093521]
   Zhang L, 2023, IEEE T IMAGE PROCESS, V32, P2107, DOI 10.1109/TIP.2023.3263112
   Zhang PY, 2022, LECT NOTES COMPUT SC, V13674, P215, DOI 10.1007/978-3-031-19781-9_13
   Zhang YF, 2023, IEEE T IMAGE PROCESS, V32, P509, DOI 10.1109/TIP.2022.3229621
   Zhao J, 2023, J VIS COMMUN IMAGE R, V93, DOI 10.1016/j.jvcir.2023.103828
   Zhao YY, 2021, PROC CVPR IEEE, P6273, DOI 10.1109/CVPR46437.2021.00621
   Zheng KC, 2021, PROC CVPR IEEE, P5306, DOI 10.1109/CVPR46437.2021.00527
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng W.S., 2009, BRIT MACHINE VISION
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhou JH, 2020, PROC CVPR IEEE, P2906, DOI 10.1109/CVPR42600.2020.00298
   Zhou K, 2021, INT C LEARNING REPRE
   Zhou KY, 2022, IEEE T PATTERN ANAL, V44, P5056, DOI 10.1109/TPAMI.2021.3069237
   Zijie Zhuang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P140, DOI 10.1007/978-3-030-58610-2_9
NR 65
TC 0
Z9 0
U1 5
U2 5
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104048
DI 10.1016/j.jvcir.2024.104048
EA JAN 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GT9M8
UT WOS:001155041700001
DA 2024-08-05
ER

PT J
AU Yang, P
   Wang, QH
   Dou, J
   Dou, L
AF Yang, Peng
   Wang, Qinghui
   Dou, Jie
   Dou, Lei
TI Learning saliency-awareness Siamese network for visual object tracking
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Siamese network; Saliency-awareness; Segmentation mask; Visual tracking
ID ROBUST
AB Siamese trackers have emerged as the predominant paradigm in visual tracking owing to their robust similarity matching. However, relying on dense regression strategies for predicting the target's axis-aligned bounding box often leads to excessive background pixels. This limitation can compromise the model's accuracy, especially when tracking non-rigid targets. To tackle this issue, this paper presents a novel saliency-awareness Siamese network for visual object tracking. Compared to the bounding box regression network, our method achieves accurate pixel-level target tracking. Specifically, a two-level U-structure encode-decode model is tailored to learn the saliency of backbone features. Additionally, a dual-pipeline parallel tracking framework is proposed, allowing top-down multi-stage saliency mask prediction, by integrating the aforementioned model into the Siamese network. Finally, a convolutional head is devised to generate a precise binary mask for tracking. Extensive experiments on four benchmarks, including VOT2016, VOT2019, GOT-10k, and UAV123, demonstrate that our tracker achieves superior tracking performance.
C1 [Yang, Peng; Wang, Qinghui; Dou, Jie; Dou, Lei] Nanjing Univ Sci & Technol, Natl Key Lab Transient Phys, Nanjing 210094, Jiangsu, Peoples R China.
C3 Nanjing University of Science & Technology
RP Dou, L (corresponding author), Nanjing Univ Sci & Technol, Natl Key Lab Transient Phys, Nanjing 210094, Jiangsu, Peoples R China.
EM pyang_15@163.com; douleijs@163.com
FU National Natural Science Foundation of China [60904085]; Foundation of
   National Key Laboratory of Transient Physics; Foundation of Defence
   Technology Innovation Special Filed; Jiangsu Funding Program for
   Excellent Postdoctoral Talent
FX This work was supported in part by the National Natural Science
   Foundation of China (No. 60904085) ; in part by the Foundation of
   National Key Laboratory of Transient Physics; in part by the Foundation
   of Defence Technology Innovation Special Filed; and in part by the
   Jiangsu Funding Program for Excellent Postdoctoral Talent.
CR Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat Goutam, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P777, DOI 10.1007/978-3-030-58536-5_46
   Bottou Leon, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P421, DOI 10.1007/978-3-642-35289-8_25
   Chen Bao Xin, 2019, arXiv
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Fan H, 2019, PROC CVPR IEEE, P7944, DOI 10.1109/CVPR.2019.00814
   Fan NN, 2023, INFORM SCIENCES, V624, P606, DOI 10.1016/j.ins.2022.12.082
   Feng W, 2019, IEEE T IMAGE PROCESS, V28, P3232, DOI 10.1109/TIP.2019.2895411
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Han G, 2023, IEEE T MULTIMEDIA, V25, P430, DOI 10.1109/TMM.2021.3127357
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong CQ, 2021, COMPUT VIS IMAGE UND, V208, DOI 10.1016/j.cviu.2021.103224
   Hong CQ, 2019, IEEE T IND INFORM, V15, P3952, DOI 10.1109/TII.2018.2884211
   Hong CQ, 2015, IEEE T IMAGE PROCESS, V24, P5659, DOI 10.1109/TIP.2015.2487860
   Hou Y, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103746
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Kristan M, 2016, LECT NOTES COMPUT SC, V9914, P777, DOI 10.1007/978-3-319-48881-3_54
   Kristan Matej, 2019, P IEEE CVF INT C COM
   Lan XY, 2019, IEEE T IND ELECTRON, V66, P9887, DOI 10.1109/TIE.2019.2898618
   Laurense VA, 2017, P AMER CONTR CONF, P5586, DOI 10.23919/ACC.2017.7963824
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li Q, 2020, Arxiv, DOI arXiv:2006.04078
   Li X, 2019, KNOWL-BASED SYST, V166, P71, DOI 10.1016/j.knosys.2018.12.011
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Q, 2021, IEEE T MULTIMEDIA, V23, P2114, DOI 10.1109/TMM.2020.3008028
   Liu Q, 2023, IEEE T MULTIMEDIA, V25, P1269, DOI 10.1109/TMM.2022.3140929
   Lukezic A, 2020, PROC CVPR IEEE, P7131, DOI 10.1109/CVPR42600.2020.00716
   Meng FY, 2023, PATTERN RECOGN, V141, DOI 10.1016/j.patcog.2023.109630
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nai Ke, 2023, IEEE Trans. Multimed.
   Oh SW, 2019, IEEE I CONF COMP VIS, P9225, DOI 10.1109/iccv.2019.00932
   Paul M, 2022, LECT NOTES COMPUT SC, V13682, P571, DOI 10.1007/978-3-031-20047-2_33
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sheng B, 2021, IEEE T CYBERNETICS, V51, P1463, DOI 10.1109/TCYB.2020.2988792
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang F, 2022, PROC CVPR IEEE, P8731, DOI 10.1109/CVPR52688.2022.00854
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xu N, 2018, LECT NOTES COMPUT SC, V11209, P603, DOI 10.1007/978-3-030-01228-1_36
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Yan B, 2021, PROC CVPR IEEE, P5285, DOI 10.1109/CVPR46437.2021.00525
   Yang C, 2024, EXPERT SYST APPL, V238, DOI 10.1016/j.eswa.2023.121577
   Yang P, 2024, J VIS COMMUN IMAGE R, V98, DOI 10.1016/j.jvcir.2023.104040
   Yu J, 2022, IEEE T PATTERN ANAL, V44, P563, DOI 10.1109/TPAMI.2019.2932058
   Yu YC, 2020, PROC CVPR IEEE, P6727, DOI 10.1109/CVPR42600.2020.00676
   Zhang PP, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107130
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
   Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z
   Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7
   Zhuang YH, 2021, IEEE INT CONF BIG DA, P3223, DOI 10.1109/BigData52589.2021.9671965
   Ziang Ma, 2020, Proceedings of the 16th European Conference on Computer Vision (ECCV 2020) Workshops. Lecture Notes in Computer Science (LNCS 12539), P653, DOI 10.1007/978-3-030-68238-5_43
NR 61
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD AUG
PY 2024
VL 103
AR 104237
DI 10.1016/j.jvcir.2024.104237
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZX4J9
UT WOS:001278569300001
DA 2024-08-05
ER

PT J
AU Huang, Y
   Chen, RZ
AF Huang, Yong
   Chen, Renzhang
TI Scientific mapping and bibliometric analysis of research advancements in
   underwater image enhancement
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Underwater image enhancement; Deep learning; Underwater image
   restoration; Underwater image fusion; Underwater image denoising
ID RESTORATION; HISTOGRAM; RETINEX
AB Underwater Image Enhancement (UIE) addresses challenges in marine resource exploitation such as absorption, noise, and low contrast. This paper employs bibliometric methods and data from Web of Science to analyze UIE literature. The analysis reveals that research on UIE entered a rapid development stage starting from 2017. Three main areas of technical support for research: underwater image enhancement methods, deep learning-based methods, and underwater image restoration and depth estimation. Hotspots in research include enhancement, deep learning, restoration, feature extraction, and quality assessment. Drawing from the bibliometric analysis, this paper proposes a conceptual framework for UIE research and presents five research suggestions: enhancing method robustness and adaptability, improving real-time performance, integrating software and hardware, establishing underwater image benchmark datasets, and refining quality evaluation systems for underwater images. This study offers valuable references and guidance for the future exploration and development of the UIE field.
C1 [Huang, Yong] Jinan Univ, Lib Zhuhai Campus, 206 Qianshan Rd, Zhuhai 519070, Guangdong, Peoples R China.
   [Chen, Renzhang] Jinan Univ, Modern Educ Technol Ctr, 206 Qianshan Rd,Zhuhai Campus, Zhuhai 519070, Guangdong, Peoples R China.
C3 Jinan University; Jinan University
RP Chen, RZ (corresponding author), Jinan Univ, Modern Educ Technol Ctr, 206 Qianshan Rd,Zhuhai Campus, Zhuhai 519070, Guangdong, Peoples R China.
EM huangyong@jnu.edu.cn; jnulion@jnu.edu.cn
OI Huang, Yong/0000-0001-9166-4037; Renzhang, Chen/0000-0003-0351-8107
FU Fundamental Research Funds for the Central Universities, China
   [23IJKY02]
FX <BOLD>Funding</BOLD> This research is supported by the Fundamental
   Research Funds for the Central Universities, China (23IJKY02) .
CR Aguirre-Castro OA, 2022, NEUROCOMPUTING, V494, P148, DOI 10.1016/j.neucom.2022.04.074
   Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252
   Ancuti C, 2012, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2012.6247661
   Arun R.A., 2022, Enhancement and Detection of Objects in Underwater Images Using Image Super-Resolution and Effective Object Detection Model
   Babüroglu ES, 2024, EVOL SYST-GER, V15, P789, DOI 10.1007/s12530-023-09503-2
   Baharin MASK, 2021, IMAGING SCI J, V69, P65, DOI 10.1080/13682199.2022.2149067
   Bai LF, 2020, IEEE ACCESS, V8, P128973, DOI 10.1109/ACCESS.2020.3009161
   Cecilia M, 2022, FLUCT NOISE LETT, V21, DOI 10.1142/S0219477522500092
   Chang HH, 2019, IEEE J OCEANIC ENG, V44, P1130, DOI 10.1109/JOE.2018.2865045
   Chen RZ, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3134762
   Chen Y, 2023, EXPERT SYST APPL, V228, DOI 10.1016/j.eswa.2023.120421
   Chen YH, 2023, BIOMIMETICS-BASEL, V8, DOI 10.3390/biomimetics8030275
   Cheng N, 2023, ENG APPL ARTIF INTEL, V120, DOI 10.1016/j.engappai.2023.105905
   Chiang JY, 2012, IEEE T IMAGE PROCESS, V21, P1756, DOI 10.1109/TIP.2011.2179666
   Cong RM, 2023, IEEE T IMAGE PROCESS, V32, P4472, DOI 10.1109/TIP.2023.3286263
   Deng RH, 2023, IET IMAGE PROCESS, V17, P3841, DOI 10.1049/ipr2.12901
   Ding XY, 2022, OPT LASER ENG, V152, DOI 10.1016/j.optlaseng.2022.106971
   Drews P, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P825, DOI 10.1109/ICCVW.2013.113
   Drews PLJ, 2016, IEEE COMPUT GRAPH, V36, P24, DOI 10.1109/MCG.2016.26
   Ellegaard O, 2015, SCIENTOMETRICS, V105, P1809, DOI 10.1007/s11192-015-1645-z
   Ezugwu AE, 2023, ARCH COMPUT METHOD E, V30, P4177, DOI 10.1007/s11831-023-09930-z
   Fabbri C, 2018, IEEE INT CONF ROBOT, P7159
   Fu XY, 2014, IEEE IMAGE PROC, P4572, DOI 10.1109/ICIP.2014.7025927
   Galdran A, 2015, J VIS COMMUN IMAGE R, V26, P132, DOI 10.1016/j.jvcir.2014.11.006
   Gao SB, 2023, IEEE SIGNAL PROC LET, V30, P658, DOI 10.1109/LSP.2023.3281255
   Ge WY, 2022, OPT EXPRESS, V30, P24295, DOI 10.1364/OE.463865
   Ghani A.S.A., 2015, Homomorphic Filtering with Image Fusion for Enhancement of Details and Homogeneous Contrast of Underwater Image
   Gmür M, 2003, SCIENTOMETRICS, V57, P27, DOI 10.1023/A:1023619503005
   Gong BX, 2021, APPL OPTICS, V60, P6928, DOI 10.1364/AO.424917
   Guan MS, 2024, IEEE J-STARS, V17, P2319, DOI 10.1109/JSTARS.2023.3344453
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hou GJ, 2019, NEUROCOMPUTING, V369, P106, DOI 10.1016/j.neucom.2019.08.041
   Hu K, 2022, J MAR SCI ENG, V10, DOI 10.3390/jmse10020241
   Islam MJ, 2020, IEEE ROBOT AUTOM LET, V5, P3227, DOI 10.1109/LRA.2020.2974710
   Jian MW, 2021, SIGNAL PROCESS-IMAGE, V91, DOI 10.1016/j.image.2020.116088
   Kenger ON, 2023, NEURAL COMPUT APPL, V35, P5081, DOI 10.1007/s00521-023-08267-9
   Li CY, 2016, IEEE T IMAGE PROCESS, V26, P5664, DOI 10.1109/TIP.2016.2612882
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107038
   Li CY, 2018, IEEE SIGNAL PROC LET, V25, P323, DOI 10.1109/LSP.2018.2792050
   Li JR, 2019, OPT LASER TECHNOL, V110, P129, DOI 10.1016/j.optlastec.2018.05.034
   Li J, 2018, IEEE ROBOT AUTOM LET, V3, P387, DOI 10.1109/LRA.2017.2730363
   Li JY, 2023, MULTIMED TOOLS APPL, V82, P6625, DOI 10.1007/s11042-022-13605-5
   Li M, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3145568
   Lin YF, 2021, IEEE SIGNAL PROC LET, V28, P199, DOI 10.1109/LSP.2020.3048619
   Liu DS, 2022, OPT EXPRESS, V30, P29099, DOI 10.1364/OE.462861
   Liu H, 2020, IET IMAGE PROCESS, V14, P807, DOI 10.1049/iet-ipr.2019.0856
   Liu K, 2023, OPT EXPRESS, V31, P9688, DOI 10.1364/OE.482489
   Liu RS, 2020, IEEE T CIRC SYST VID, V30, P4861, DOI 10.1109/TCSVT.2019.2963772
   Lu SQ, 2023, J VIS COMMUN IMAGE R, V96, DOI 10.1016/j.jvcir.2023.103926
   Mao JJ, 2022, FORESTS, V13, DOI 10.3390/f13060863
   Moghimi MK, 2021, J REAL-TIME IMAGE PR, V18, P1509, DOI 10.1007/s11554-020-01052-0
   Muniraj M, 2022, COMPUT ELECTR ENG, V100, DOI 10.1016/j.compeleceng.2022.107909
   Muniraj M, 2021, NEUROCOMPUTING, V460, P211, DOI 10.1016/j.neucom.2021.07.003
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Pei XD, 2023, INT J COMPUT INT SYS, V16, DOI 10.1007/s44196-023-00225-6
   Peng LT, 2023, IEEE T IMAGE PROCESS, V32, P3066, DOI 10.1109/TIP.2023.3276332
   Peng YT, 2018, IEEE T IMAGE PROCESS, V27, P2856, DOI 10.1109/TIP.2018.2813092
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Raveendran S, 2021, ARTIF INTELL REV, V54, P5413, DOI 10.1007/s10462-021-10025-z
   Ren TD, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3205061
   Rodríguez JV, 2022, WIRES DATA MIN KNOWL, V12, DOI 10.1002/widm.1476
   Shen Z, 2023, COMPUT GRAPH-UK, V111, P77, DOI 10.1016/j.cag.2023.01.009
   Singh N, 2024, INT J INF TECH DECIS, V23, P539, DOI 10.1142/S0219622023300021
   Sood V, 2024, EXPERT SYST, V41, DOI 10.1111/exsy.13389
   Sun XD, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3290366
   Tang Yi, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P5419, DOI 10.1145/3581783.3612378
   Wang Y, 2018, COMPUT ELECTR ENG, V70, P904, DOI 10.1016/j.compeleceng.2017.12.006
   Wang YD, 2021, SIGNAL PROCESS-IMAGE, V96, DOI 10.1016/j.image.2021.116250
   Wang ZY, 2023, IEEE T CIRC SYST VID, V33, P1123, DOI 10.1109/TCSVT.2022.3212788
   Wang ZY, 2012, SCIENTOMETRICS, V90, P855, DOI 10.1007/s11192-011-0563-y
   Wu ZJ, 2022, J MAR SCI ENG, V10, DOI 10.3390/jmse10101513
   Xu S, 2022, COMPUT ELECTR ENG, V100, DOI 10.1016/j.compeleceng.2022.107822
   Yang M, 2019, IEEE ACCESS, V7, P123638, DOI 10.1109/ACCESS.2019.2932611
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Yang P, 2023, DIGIT SIGNAL PROCESS, V134, DOI 10.1016/j.dsp.2022.103900
   Yin SB, 2022, KNOWL-BASED SYST, V258, DOI 10.1016/j.knosys.2022.109997
   Zhang HJ, 2023, PHOTONICS-BASEL, V10, DOI 10.3390/photonics10020145
   Zhang S, 2023, J MAR SCI ENG, V11, DOI 10.3390/jmse11061183
   Zhang WD, 2022, IEEE J OCEANIC ENG, V47, P718, DOI 10.1109/JOE.2022.3140563
   Zhao WF, 2023, IEEE J OCEANIC ENG, V48, P147, DOI 10.1109/JOE.2022.3192089
   Zhou JC, 2023, IEEE J OCEANIC ENG, V48, P474, DOI 10.1109/JOE.2022.3223733
   Zhou JC, 2023, APPL INTELL, V53, P3594, DOI 10.1007/s10489-022-03767-y
   Zhou JC, 2022, OPT EXPRESS, V30, P17290, DOI 10.1364/OE.450858
   Zhu HB, 2021, MEAS SCI TECHNOL, V32, DOI 10.1088/1361-6501/abaa1d
NR 85
TC 0
Z9 0
U1 8
U2 8
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104166
DI 10.1016/j.jvcir.2024.104166
EA MAY 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TK7M8
UT WOS:001241222100001
DA 2024-08-05
ER

PT J
AU Zhang, XD
   Zhang, L
   Chu, MH
   Wang, S
AF Zhang, Xiaodong
   Zhang, Long
   Chu, Menghui
   Wang, Shuo
TI DU-Net: A new double U-shaped network for single image dehazing
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Image dehazing; Convolutional neural network; Double U-Net; Multi-scale
AB Convolutional neural networks have achieved remarkable success in single image dehazing tasks, and previous studies verified the dehazing performance of the U-shaped framework. However, most existing U-shaped architecture dehazing networks still face challenges in sufficiently dealing with a large area of haze with low visibility. In this paper, we propose a novel dehazing network named Double U-Net(DU-Net). Specifically, to reduce the interference of haze features in the encoder to the recovery stage when skip -connecting to the decoder directly, we develop a new architecture firstly, which is composed of an extended encoder-decoder. Besides, the hierarchical depth -wise convolution block(HDCB) is designed to gradually increase the receptive field by leveraging the depth -wise convolution, enriching the global information. Moreover, we propose a multi -branch interactive fusion(MIF) which achieves efficient cross -branch and cross -channel interaction through parallel multiple 1D convolutions. Extensive experiments on both synthetic and real -world hazy images demonstrate the effectiveness of our proposed method.
C1 [Zhang, Xiaodong; Zhang, Long; Chu, Menghui; Wang, Shuo] China Univ Petr East China, Dept Comp & Sci, Qingdao 266580, Peoples R China.
C3 China University of Petroleum
RP Zhang, L (corresponding author), China Univ Petr East China, Dept Comp & Sci, Qingdao 266580, Peoples R China.
EM zhang_long@s.upc.edu.cn
OI Chu, Menghui/0009-0006-2345-2634; Zhang, Long/0009-0008-1919-1712
FU National Natural Science Founda-tion of China [61801517]
FX This research was supported by National Natural Science Founda-tion of
   China under Grant 61801517.
CR Ancuti CO, 2020, IEEE COMPUT SOC CONF, P1798, DOI 10.1109/CVPRW50498.2020.00230
   Ancuti CO, 2019, IEEE IMAGE PROC, P1014, DOI [10.1109/ICIP.2019.8803046, 10.1109/icip.2019.8803046]
   Ates GC, 2023, Arxiv, DOI arXiv:2303.17696
   Berman D, 2020, IEEE T PATTERN ANAL, V42, P720, DOI 10.1109/TPAMI.2018.2882478
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chen Z., 2023, arXiv
   Dauphin YN, 2017, PR MACH LEARN RES, V70
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Guo CL, 2022, PROC CVPR IEEE, P5802, DOI 10.1109/CVPR52688.2022.00572
   Han N, 2022, DISPLAYS, V73, DOI 10.1016/j.displa.2022.102192
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Huang X., 2020, CVPR, P10747
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Ju MY, 2021, IEEE T IMAGE PROCESS, V30, P9043, DOI 10.1109/TIP.2021.3122088
   Ju MY, 2021, IEEE T IMAGE PROCESS, V30, P2180, DOI 10.1109/TIP.2021.3050643
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li Y, 2022, KNOWL-BASED SYST, V254, DOI 10.1016/j.knosys.2022.109579
   Li ZY, 2022, PATTERN RECOGN, V132, DOI 10.1016/j.patcog.2022.108911
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu YT, 2022, IEEE SIGNAL PROC LET, V29, P174, DOI 10.1109/LSP.2021.3130014
   Liu Y, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P50, DOI 10.1145/3474085.3475331
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu LP, 2023, Arxiv, DOI arXiv:2305.17654
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Nayar S. K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P820, DOI 10.1109/ICCV.1999.790306
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Romano Y, 2015, SIAM J IMAGING SCI, V8, P1187, DOI 10.1137/140990978
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shazeer N., 2020, ARXIV
   Song YD, 2023, IEEE T IMAGE PROCESS, V32, P1927, DOI 10.1109/TIP.2023.3256763
   Song YD, 2022, Arxiv, DOI arXiv:2209.11448
   Valanarasu JMJ, 2022, PROC CVPR IEEE, P2343, DOI 10.1109/CVPR52688.2022.00239
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Yang LJ, 2022, MEASUREMENT, V204, DOI 10.1016/j.measurement.2022.112001
   Yin SB, 2021, NEUROCOMPUTING, V437, P143, DOI 10.1016/j.neucom.2020.12.081
   Yu XY, 2023, KNOWL-BASED SYST, V260, DOI 10.1016/j.knosys.2022.110176
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zheng CY, 2022, KNOWL-BASED SYST, V240, DOI 10.1016/j.knosys.2022.108148
   Zheng ZR, 2021, PROC CVPR IEEE, P16180, DOI 10.1109/CVPR46437.2021.01592
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 47
TC 0
Z9 0
U1 3
U2 3
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104132
DI 10.1016/j.jvcir.2024.104132
EA APR 2024
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QG3F5
UT WOS:001219680300001
DA 2024-08-05
ER

PT J
AU Dave, C
   Patel, H
   Kumar, A
AF Dave, Chintan
   Patel, Hetal
   Kumar, Ahlad
TI Unsupervised single image dehazing - A contour approach
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Single image dehazing; Unsupervised learning; Contour
ID RESTORATION; COLOR
AB Small particles present in the air degrade the visual clarity of images due to light scattering phenomena caused by particles. This degradation of the image is in the context of attenuation of light intensity and poor contrast, which ultimately have an impact on the image's quality. Thus, image dehazing is a necessity for better visualization and image analysis. The proposed method uses an unsupervised approach to dehaze the image without using any ground truth image or transmission map, which overcomes the necessity of a paired dataset and a true depth map. The majority of current state-of-the-art haze removal approaches observe color shifts in the sky region. By utilizing the contour process and RGB color plane data, the proposed innovative "ContourDCP" technique can effectively identify the sky area. After applying the Dark Channel Prior (DCP) to obtain the initial transmission map, it is further reformulate through the contour method to achieve an accurate color representation of the sky area during the image dehazing process. We proposed atmospheric light estimation based on the haze concentration present in the Y -plane after conversion from RGB to YCbCr color space. These modified transmission map and atmospheric light are used in the atmospheric scattering model to recover the scene radiance. Performance evaluation of the method shows the robustness of the proposed method compared to existing state-of-the-art methods.
C1 [Dave, Chintan] Gujrat Technol Univ, Ahmadabad 382424, Gujarat, India.
   [Patel, Hetal] AD Patel Inst Technol, Dept Elect & Commun Engn, Anand 388121, India.
   [Kumar, Ahlad] Natl Forens Sci Univ, Sch Cyber Secur & Digital Forens, Gandhinagar 382007, India.
C3 Gujarat Technological University; National Forensic Sciences University
RP Patel, H (corresponding author), AD Patel Inst Technol, Dept Elect & Commun Engn, Anand 388121, India.
EM ec.hetal.patel@adit.ac.in
CR Ancuti C, 2016, IEEE IMAGE PROC, P2226, DOI 10.1109/ICIP.2016.7532754
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   CHAVEZ PS, 1988, REMOTE SENS ENVIRON, V24, P459, DOI 10.1016/0034-4257(88)90019-3
   Cozman F, 1997, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.1997.609419
   Dudhane A, 2018, IEEE WINT CONF APPL, P1397, DOI 10.1109/WACV.2018.00157
   Engin D, 2018, IEEE COMPUT SOC CONF, P938, DOI 10.1109/CVPRW.2018.00127
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Golts A, 2020, IEEE T IMAGE PROCESS, V29, P2692, DOI 10.1109/TIP.2019.2952032
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hamaguchi R, 2018, IEEE WINT CONF APPL, P1442, DOI 10.1109/WACV.2018.00162
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu Q, 2023, VISUAL COMPUT, V39, P997, DOI 10.1007/s00371-021-02380-3
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ki S, 2018, IEEE COMPUT SOC CONF, P930, DOI 10.1109/CVPRW.2018.00126
   Lee JS, 2022, J INTERNET TECHNOL, V23, P967, DOI 10.53106/160792642022092305005
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li BY, 2021, INT J COMPUT VISION, V129, P1754, DOI 10.1007/s11263-021-01431-5
   Li BY, 2020, IEEE T IMAGE PROCESS, V29, P8457, DOI 10.1109/TIP.2020.3016134
   Li JF, 2023, IEEE T MULTIMEDIA, V25, P3587, DOI 10.1109/TMM.2022.3163554
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   MCDONALD JE, 1963, J ATMOS SCI, V20, P476, DOI 10.1175/1520-0469(1963)020<0476:TSAINM>2.0.CO;2
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schechner YY, 2001, PROC CVPR IEEE, P325
   Shwartz S, 2006, 2006 IEEE COMP SOC C, P1984
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Sim H, 2018, IEEE COMPUT SOC CONF, P1025, DOI 10.1109/CVPRW.2018.00136
   SUZUKI S, 1985, COMPUT VISION GRAPH, V30, P32, DOI 10.1016/0734-189X(85)90016-7
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163
   Xu ZY, 2009, PROCEEDINGS OF THE 2009 2ND INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOLS 1-9, P508
   Yu F., 2016, 4 INT C LEARN REPR I, DOI [10.48550/arXiv.1511.07122, DOI 10.48550/ARXIV.1511.07122]
   Yuan H, 2017, IEEE ACCESS, V5, P1735, DOI 10.1109/ACCESS.2017.2660302
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 48
TC 0
Z9 0
U1 4
U2 4
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104119
DI 10.1016/j.jvcir.2024.104119
EA MAR 2024
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QN8G4
UT WOS:001221637000001
DA 2024-08-05
ER

PT J
AU Islam, AR
   Alammari, A
   Buckles, B
AF Islam, A. B. M. Rezbaul
   Alammari, Ali
   Buckles, Bill
TI Human skin detection: An unsupervised machine learning way☆
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Human skin detection; Machine learning; Unsupervised learning;
   Clustering
ID HUMAN-FACE DETECTION; COLOR; SEGMENTATION; REGIONS
AB Researchers have been involved for decades in search of an efficient skin detection method. However, current methods have not overcome the significant challenges of skin detection, such as variation of illumination, various skin tones of different ethnic groups, and many others. This research proposed a clustering and regiongrowing-based skin detection method to overcome these limitations. Together with significant insight, these methods result in a more effective algorithm. The insight concerns the capability to dynamically define the number of clusters in a collection of pixels organized as images. In Clustering for most problem domains, the number of clusters is fixed prior and does not perform effectively over a wide variety of data contents. Therefore, this research paper proposed a skin detection method that validated the above findings. The proposed method assigns the number of clusters based on image properties and ultimately allows freedom from manual thresholds or other manual operations. The dynamic determination of clustering outcomes allows for greater automation of skin detection when dealing with uncertain real-world conditions.
C1 [Islam, A. B. M. Rezbaul] Sam Houston State Univ, Huntsville, TX 77341 USA.
   [Alammari, Ali; Buckles, Bill] Univ North Texas, UNT Discovery Pk,3940 N Elm St Ste F201, Denton, TX 76207 USA.
C3 Texas State University System; Sam Houston State University; University
   of North Texas System; University of North Texas Denton
RP Islam, AR (corresponding author), Sam Houston State Univ, Huntsville, TX 77341 USA.
EM ari014@shsu.edu; AliAlammari@my.unt.edu; Bill.Buckles@unt.edu
CR Al-Mohair H.K., 2013, 1 WSEAS INT C IM PRO
   Albiol A, 2008, PATTERN RECOGN LETT, V29, P1537, DOI 10.1016/j.patrec.2008.03.017
   Basilio Jorge Alberto Marcial, 2011, Applications of Mathematics and Computer Engineering. American Conference on Applied Mathematics (AMERICAN-MATH'11). 5th WSEAS International Conference on Computer Engineering and Applications (CEA'11), P123
   Brancati N, 2017, COMPUT VIS IMAGE UND, V155, P33, DOI 10.1016/j.cviu.2016.12.001
   Byeongcheol Choi, 2011, Proceedings of the 2011 13th International Conference on Advanced Communication Technology (ICACT). Smart Service Innovation through Mobile Interactivity, P556
   Casati J.P.B., 2013, 9 WORKSH VIS COMP
   Chai D, 1999, IEEE T CIRC SYST VID, V9, P551, DOI 10.1109/76.767122
   Chai D, 2001, ISSPA 2001: SIXTH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, VOLS 1 AND 2, PROCEEDINGS, P343, DOI 10.1109/ISSPA.2001.949848
   Chaves-González JM, 2010, DIGIT SIGNAL PROCESS, V20, P806, DOI 10.1016/j.dsp.2009.10.008
   Dahm I., 2003, 7 INT WORKSH ROBOCUP
   Dahmani D, 2014, J VIS COMMUN IMAGE R, V25, P1240, DOI 10.1016/j.jvcir.2013.12.019
   Deshmukh C., 2013, Int. J., V1
   Erdem CE, 2011, INT CONF ACOUST SPEE, P1497, DOI 10.1109/ICASSP.2011.5946777
   Fan Hai Xiang, 2013, International Journal of Information and Electronics Engineering, V3, P172, DOI 10.7763/IJIEE.2013.V3.292
   Garcia C, 1999, IEEE T MULTIMEDIA, V1, P264, DOI 10.1109/6046.784465
   Gasparini F, 2008, J ELECTRON IMAGING, V17, DOI 10.1117/1.2916715
   Hsu RL, 2002, IEEE T PATTERN ANAL, V24, P696, DOI 10.1109/34.1000242
   Huynh-Thu Q, 2002, SIXTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P280, DOI 10.1109/ACV.2002.1182195
   Islam ABMR, 2019, PROC SPIE, V10993, DOI 10.1117/12.2518765
   Jaimes A, 2007, COMPUT VIS IMAGE UND, V108, P116, DOI 10.1016/j.cviu.2006.10.019
   Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198
   Karishma S., 2014, Int. J. Eng. Res. Technol., V3, P1835
   Ketenci S, 2013, 2013 IEEE EUROCON, P1647
   Kovac J, 2003, IEEE REGION 8 EUROCON 2003, VOL B, PROCEEDINGS, P144
   Mohanty R., 2016, Int. J. Adv. Res. Comput. Commun. Eng., V5, P470
   Montenegro J, 2013, 2013 10TH INTERNATIONAL CONFERENCE ON ELECTRICAL ENGINEERING, COMPUTING SCIENCE AND AUTOMATIC CONTROL (CCE), P313, DOI 10.1109/ICEEE.2013.6676048
   Nacer N., 2013, Int. J. Comput. Sci.
   Nalepa J, 2014, COMM COM INF SC, V424, P364
   Nalepa J, 2014, ADV INTEL SOFT COMPU, V242, P79, DOI 10.1007/978-3-319-02309-0_8
   Phung SL, 2005, IEEE T PATTERN ANAL, V27, P148, DOI 10.1109/TPAMI.2005.17
   Platzer C, 2014, SFCS'14: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON SECURITY AND FORENSICS IN COMMUNICATION SYSTEMS, P45, DOI 10.1145/2598918.2598920
   Poudel Rudra P. K., 2012, Proceedings of the International Conference on Computer Vision Theory and Applications. VISAPP 2012, P301
   Powar V, 2013, INT CONF COMP COMMUN
   Rautaray SS, 2015, ARTIF INTELL REV, V43, P1, DOI 10.1007/s10462-012-9356-9
   Sagheer A, 2012, 8TH INTERNATIONAL CONFERENCE ON SIGNAL IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS (SITIS 2012), P90, DOI 10.1109/SITIS.2012.25
   Schumeyer RP, 1997, P SOC PHOTO-OPT INS, V3309, P189
   Sebe N, 2004, INT C PATT RECOG, P903, DOI 10.1109/ICPR.2004.1334405
   Shakir H.R., 2015, Single face detection based on skin color and edge detection
   Sobottka K, 1998, SIGNAL PROCESS-IMAGE, V12, P263, DOI 10.1016/S0923-5965(97)00042-8
   Störring M, 2003, PATTERN RECOGN LETT, V24, P1715, DOI 10.1016/S0167-8655(02)00327-6
   Subban R., 2014, Recent Advances in Intelligent Informatics, P13
   Tang S, 2013, ADV MATER RES-SWITZ, V706-708, P1877, DOI 10.4028/www.scientific.net/AMR.706-708.1877
   Tavallali P, 2019, MULTIMED TOOLS APPL, V78, P2599, DOI 10.1007/s11042-018-6385-7
   Vadakkepat P, 2008, IEEE T IND ELECTRON, V55, P1385, DOI 10.1109/TIE.2007.903993
   Vezhnevets V., 2005, A comparative assessment of pixel-based skin detection methods
   Wang D, 2008, COMM COM INF SC, V15, P487
   Zahir NB, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING APPLICATIONS (IEEE ICSIPA 2013), P264, DOI 10.1109/ICSIPA.2013.6708015
   Zaidan AA, 2014, NEUROCOMPUTING, V131, P397, DOI 10.1016/j.neucom.2013.10.003
   Zarit B. D., 1999, Proceedings International Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems. In Conjunction with ICCV'99 (Cat. No.PR00378), P58, DOI 10.1109/RATFG.1999.799224
   Zhu Q, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P37
   Zuo HQ, 2017, IEEE SIGNAL PROC LET, V24, P289, DOI 10.1109/LSP.2017.2654803
NR 51
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104046
DI 10.1016/j.jvcir.2024.104046
EA JAN 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IR3L2
UT WOS:001168015800001
DA 2024-08-05
ER

PT J
AU Cao, M
   Ding, C
   Chen, C
   Peng, SL
AF Cao, Min
   Ding, Cong
   Chen, Chen
   Peng, Silong
TI Context-aided unicity matching for person re-identification
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Person re-identification; Unicity matching; Contextual information;
   Graph neural network
ID ALGORITHM; NETWORK
AB Most existing person re -identification methods compute the matching relations between person images based on the similarity ranking. It lacks the global viewpoint and context consideration, inevitably leading to ambiguous matching results and sub-optimal performance. Based on a natural assumption that images belonging to the same identity should not match with images belonging to different identities across views, called the unicity of person matching on the identity level, we propose an end -to -end person unicity matching architecture to learn and refine the person matching relations. We first employ the image samples' context in feature space to generate the initial soft matchings using graph neural networks, and then utilize the samples' global context to refine the soft matchings and reach the matching unicity by bipartite graph matching. Considering real -world applications, we further develop a fast algorithm without losing performance. Experimental results on five public benchmarks show the superiority of the proposed method.
C1 [Cao, Min; Ding, Cong] Soochow Univ, Sch Comp Sci & Technol, Suzhou 215006, Peoples R China.
   [Chen, Chen; Peng, Silong] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
C3 Soochow University - China; Chinese Academy of Sciences; Institute of
   Automation, CAS
RP Chen, C (corresponding author), Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
EM chen.chen@ia.ac.cn
FU National Science Foundation of China [NSFC 61906194, NSFC 62002252];
   Collaborative Innovation Center of Novel Software Technology and
   Industrialization; Liaoning Collaboration Innovation Center For CSLE,
   China
FX This work is supported by the National Science Foundation of China under
   Grant NSFC 62002252, and is also partially supported by the National
   Science Foundation of China under Grant NSFC 61906194, Collaborative
   Innovation Center of Novel Software Technology and Industrialization,
   and Liaoning Collaboration Innovation Center For CSLE, China.
CR Cao M, 2023, IEEE T MULTIMEDIA, V25, P1230, DOI 10.1109/TMM.2022.3140647
   Cao M, 2021, IEEE T MULTIMEDIA, V23, P1239, DOI 10.1109/TMM.2020.2994524
   Cao M, 2017, IEEE INT CONF COMP V, P2573, DOI 10.1109/ICCVW.2017.302
   Chen YC, 2018, IEEE T PATTERN ANAL, V40, P392, DOI 10.1109/TPAMI.2017.2666805
   Cho Y, 2022, PROC CVPR IEEE, P7298, DOI 10.1109/CVPR52688.2022.00716
   Dai ZZ, 2019, IEEE I CONF COMP VIS, P3690, DOI 10.1109/ICCV.2019.00379
   Das A, 2014, LECT NOTES COMPUT SC, V8690, P330, DOI 10.1007/978-3-319-10605-2_22
   Ding C., 2020, IEEE T PATTERN ANAL, VI, P1
   Fang PF, 2019, IEEE I CONF COMP VIS, P8029, DOI 10.1109/ICCV.2019.00812
   Fey M, 2020, Arxiv, DOI arXiv:2001.09621
   Fu DP, 2022, PROC CVPR IEEE, P2466, DOI 10.1109/CVPR52688.2022.00251
   Fu DP, 2021, PROC CVPR IEEE, P14745, DOI 10.1109/CVPR46437.2021.01451
   García J, 2017, IEEE T IMAGE PROCESS, V26, P1650, DOI 10.1109/TIP.2017.2652725
   Gray D., 2007, IEEE INT WORKSH PERF, V3, P1
   Gu H., 2022, P IEEE CVF C COMP VI, P4744
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Hong CQ, 2019, IEEE T IND INFORM, V15, P3952, DOI 10.1109/TII.2018.2884211
   Hong CQ, 2015, IEEE T IMAGE PROCESS, V24, P5659, DOI 10.1109/TIP.2015.2487860
   Hong CQ, 2015, IEEE T IND ELECTRON, V62, P3742, DOI 10.1109/TIE.2014.2378735
   Ji DY, 2021, Arxiv, DOI [arXiv:2012.04298, DOI arXiv:2012.04298.v1]
   JONKER R, 1987, COMPUTING, V38, P325, DOI 10.1007/BF02278710
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Li HJ, 2021, PROC CVPR IEEE, P6725, DOI 10.1109/CVPR46437.2021.00666
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li YY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2115, DOI 10.1145/3343031.3350982
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin J, 2017, PROC CVPR IEEE, P3396, DOI 10.1109/CVPR.2017.362
   Lingxiao He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P357, DOI 10.1007/978-3-030-58604-1_22
   Liu S, 2021, AAAI CONF ARTIF INTE, V35, P2172
   Liu XH, 2017, IEEE I CONF COMP VIS, P350, DOI 10.1109/ICCV.2017.46
   Liu YH, 2021, IEEE T IMAGE PROCESS, V30, P2060, DOI 10.1109/TIP.2021.3050839
   Luo CC, 2019, IEEE I CONF COMP VIS, P4975, DOI 10.1109/ICCV.2019.00508
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152
   Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29
   Park H, 2020, AAAI CONF ARTIF INTE, V34, P11839
   Rezatofighi SH, 2016, PROC CVPR IEEE, P136, DOI 10.1109/CVPR.2016.22
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Rokach L, 2005, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, P321, DOI 10.1007/0-387-25465-X_15
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Shen Y, 2015, IEEE I CONF COMP VIS, P3200, DOI 10.1109/ICCV.2015.366
   Shen YT, 2018, LECT NOTES COMPUT SC, V11219, P508, DOI 10.1007/978-3-030-01267-0_30
   SIBSON R, 1973, COMPUT J, V16, P30, DOI 10.1093/comjnl/16.1.30
   Wang DW, 2023, J VIS COMMUN IMAGE R, V93, DOI 10.1016/j.jvcir.2023.103822
   Wang HC, 2022, PROC CVPR IEEE, P7287, DOI 10.1109/CVPR52688.2022.00715
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P3416, DOI 10.1109/TIP.2019.2959923
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu JL, 2022, LECT NOTES COMPUT SC, V13674, P549, DOI 10.1007/978-3-031-19781-9_32
   Xu RY, 2023, J VIS COMMUN IMAGE R, V94, DOI 10.1016/j.jvcir.2023.103849
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2017, IEEE I CONF COMP VIS, P5152, DOI 10.1109/ICCV.2017.550
   Ye Zian, 2022, 2022 IEEE International Conference on Big Data (Big Data), P4210, DOI 10.1109/BigData55660.2022.10020632
   Yu J, 2022, IEEE T PATTERN ANAL, V44, P563, DOI 10.1109/TPAMI.2019.2932058
   Zhang AG, 2021, PROC CVPR IEEE, P598, DOI 10.1109/CVPR46437.2021.00066
   Zhang GW, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P516, DOI 10.1145/3474085.3475202
   Zhang Z, 2021, PROC CVPR IEEE, P12131, DOI 10.1109/CVPR46437.2021.01196
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
   Zhou JH, 2020, PROC CVPR IEEE, P2906, DOI 10.1109/CVPR42600.2020.00298
   Zhou Q., 2018, Proceedings of the AAAI Conference on Artificial Intelligence, V32
   Zhu ZH, 2020, AAAI CONF ARTIF INTE, V34, P13114
   Zijie Zhuang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P140, DOI 10.1007/978-3-030-58610-2_9
NR 64
TC 0
Z9 0
U1 2
U2 2
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104077
DI 10.1016/j.jvcir.2024.104077
EA FEB 2024
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LH0E5
UT WOS:001185772500001
DA 2024-08-05
ER

PT J
AU Zhang, GY
   Cheng, XX
   Yang, F
   Wang, AH
   Zhang, XN
   Liu, L
AF Zhang, Guoyou
   Cheng, Xiaoxue
   Yang, Fan
   Wang, Anhong
   Zhang, Xuenan
   Liu, Li
TI High-capacity multi-MSB predictive reversible data hiding in encrypted
   domain for triangular mesh models
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE 3D mesh model; Reversible data hiding; Mesh subdivision; Multi-MSB
   prediction; Positive integer mapping; Multi-layer embedding strategy
ID EXPANSION; FRAMEWORK
AB Reversible data hiding in encrypted domain (RDH-ED) is widely used in sensitive fields such as privacy protection and copyright authentication. However, the embedding capacity of existing methods is generally low due to the insufficient use of model topology. In order to improve the embedding capacity, this paper proposes a high-capacity multi-MSB predictive reversible data hiding in encrypted domain (MMPRDH-ED). Firstly, the 3D model is subdivided by triangular mesh subdivision (TMS) algorithm, and its vertices are divided into reference set and embedded set. Then, in order to make full use of the redundant space of embedded vertices, Multi-MSB prediction (MMP) and Multi-layer Embedding Strategy (MLES) are used to improve the capacity. Finally, stream encryption technology is used to encrypt the model and data to ensure data security. The experimental results show that compared with the existing methods, the embedding capacity of MMPRDH-ED is increased by 53 %, which has higher advantages.
C1 [Zhang, Guoyou; Cheng, Xiaoxue; Yang, Fan] Taiyuan Univ Sci & Technol, Coll Comp Sci & Technol, Taiyuan 030024, Peoples R China.
   [Wang, Anhong; Liu, Li] Taiyuan Univ Sci & Technol, Sch Elect Informat Engn, Taiyuan 030024, Peoples R China.
   [Zhang, Xuenan] China United Network Telecommun Corp Beijing Branc, Beijing Branch, Beijing 100031, Peoples R China.
C3 Taiyuan University of Science & Technology; Taiyuan University of
   Science & Technology
RP Zhang, GY (corresponding author), Taiyuan Univ Sci & Technol, Coll Comp Sci & Technol, Taiyuan 030024, Peoples R China.
EM zhangguoyou@tyust.edu.cn
FU Natural Science Foundation of Shanxi Province [202203021221145];
   National Natural Science Foundation of China [62072325, 20212039];
   Fundamental Research Program of Shanxi Province [202103021224272]
FX This study was funded by [the Natural Science Foundation of Shanxi
   Province under Grant No.202203021221145] , [the National Natural Science
   Foundation of China under Grant No.62072325] , [TYUST SRIF under Grant
   No.20212039] , [Fundamental Research Program of Shanxi Province under
   Grant No.202103021224272] .
CR Bhardwaj R, 2022, BIOMED SIGNAL PROCES, V73, DOI 10.1016/j.bspc.2021.103265
   Celik MU, 2006, IEEE T IMAGE PROCESS, V15, P1042, DOI 10.1109/TIP.2005.863053
   Deering M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P13, DOI 10.1145/218380.218391
   El-sayed HS, 2016, ARAB J SCI ENG, V41, P1091, DOI 10.1007/s13369-015-1956-7
   Fan GJ, 2021, SIGNAL PROCESS, V180, DOI 10.1016/j.sigpro.2020.107888
   Gao XY, 2020, SIGNAL PROCESS, V173, DOI 10.1016/j.sigpro.2020.107579
   Gujjunoori S, 2019, MULTIMED TOOLS APPL, V78, P25889, DOI 10.1007/s11042-019-07767-y
   Huang FJ, 2016, IEEE T INF FOREN SEC, V11, P2777, DOI 10.1109/TIFS.2016.2598528
   Jia YJ, 2019, SIGNAL PROCESS, V163, P238, DOI 10.1016/j.sigpro.2019.05.020
   Jiang RQ, 2018, MULTIMED TOOLS APPL, V77, P5263, DOI 10.1007/s11042-017-4430-6
   Jiang RQ, 2018, IEEE T MULTIMEDIA, V20, P55, DOI 10.1109/TMM.2017.2723244
   Kumar S, 2022, APPL INTELL, V52, P7373, DOI 10.1007/s10489-021-02789-2
   Li XL, 2013, IEEE T IMAGE PROCESS, V22, P2181, DOI 10.1109/TIP.2013.2246179
   Luo H, 2006, IIH-MSP: 2006 INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, PROCEEDINGS, P487
   Lyu WL, 2022, SIGNAL PROCESS, V201, DOI 10.1016/j.sigpro.2022.108686
   Ma KD, 2013, IEEE T INF FOREN SEC, V8, P553, DOI 10.1109/TIFS.2013.2248725
   Manupriya P, 2017, 2017 CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGY (CICT), DOI 10.1109/INFOCOMTECH.2017.8340639
   Narendra M, 2022, MULTIMEDIA SYST, V28, P623, DOI 10.1007/s00530-021-00860-z
   Puteaux P, 2018, IEEE T INF FOREN SEC, V13, P1670, DOI 10.1109/TIFS.2018.2799381
   Shah M, 2018, ARAB J SCI ENG, V43, P8145, DOI 10.1007/s13369-018-3354-4
   Sharma Shikhar, 2018, Proceedings of 2nd International Conference on Computer Vision & Image Processing. CVIP 2017. Advances in Intelligent Systems and Computing (AISC 703), P51, DOI 10.1007/978-981-10-7895-8_5
   Shi YQ, 2016, IEEE ACCESS, V4, P3210, DOI 10.1109/ACCESS.2016.2573308
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Tsai YY, 2021, IEEE T MULTIMEDIA, V23, P2286, DOI 10.1109/TMM.2020.3009492
   van Rensburg BJ, 2021, IEEE IMAGE PROC, P3068, DOI 10.1109/ICIP42928.2021.9506320
   Wu CP, 2005, IEEE T MULTIMEDIA, V7, P828, DOI 10.1109/TMM.2005.854469
   Wu QL, 2018, SYMMETRY-BASEL, V10, DOI 10.3390/sym10070284
   Xu N, 2022, COGN COMPUT, V14, P1172, DOI 10.1007/s12559-021-09919-5
   Zhang QL, 2019, MEASUREMENT, V135, P738, DOI 10.1016/j.measurement.2018.12.016
   Zhang WM, 2013, IEEE T IMAGE PROCESS, V22, P2775, DOI 10.1109/TIP.2013.2257814
NR 30
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD AUG
PY 2024
VL 103
AR 104246
DI 10.1016/j.jvcir.2024.104246
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZZ6O6
UT WOS:001279149100001
DA 2024-08-05
ER

PT J
AU Zhong, FJ
   Wang, YH
   Yu, H
   Hu, J
   Yang, Y
AF Zhong, Fujin
   Wang, Yunhe
   Yu, Hong
   Hu, Jun
   Yang, Yan
TI Mask-guided discriminative feature network for occluded person
   re-identification
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Person re-identification; Occlusion augmentation; Mask-guided
   discriminative feature network; Sparse pairwise loss
AB In recent years, although research on person re-identification (ReID) has made significant progress, occluded person ReID remains a major challenge. In real-world scenes, persons are often occluded by various obstacles such as vehicles, umbrellas, and other persons. This leads to the noisy interference and the loss of visual information, which result in poor ReID performance of occluded persons. To address this issue, we propose an end-to-end Mask-guided Discriminative Feature Network (MDFNet). First, MDFNet adopts a dual-branch architecture with a shared encoder as the Feature Extraction module for paired images. Each pair of images consist of one image from the training set and its corresponding occluded image generated through an occlusion augmentation strategy. Second, MDFNet utilizes a Mask-guided Discriminative Feature Enhancement and Fusion (MDFEF) module to achieve the fusion and enhancement of global and local features for highquality person representations. MDFEF module effectively suppresses the interference of occlusion, enriches the representation capacity of person features, and enables the model to focus more on the important features in non-occluded regions. Furthermore, MDFNet introduces a sparse pairwise loss for enabling the model to dynamically adapt to intra-class variations and reduce the negative impact of complex occlusions. The experimental results on four challenging person ReID datasets demonstrate the effectiveness of the proposed method.
C1 [Zhong, Fujin; Wang, Yunhe; Yu, Hong; Hu, Jun; Yang, Yan] Chongqing Univ Telecommun & Posts, Chongqing Key Lab Computat Intelligence, Chongqing, Peoples R China.
RP Wang, YH (corresponding author), Chongqing Univ Telecommun & Posts, Chongqing Key Lab Computat Intelligence, Chongqing, Peoples R China.
EM wangyunhe0729@163.com
OI Yu, Hong/0000-0003-0667-8413; Yang, Yan/0000-0001-8648-9692
FU National Natural Science Foundation of China [62136002, 61876027];
   Natural Science Foundation of Chongqing [cstc2019jcyj-cxttX0002,
   cstc2021ycjh-bgzxm0013]; Key cooperation project of Chongqing Municipal
   Education Commission [HZ2021008]
FX This work is supported by the National Natural Science Foundation of
   China (Grant No: 62136002, 61876027), the Natural Science Foundation of
   Chongqing (Grant No: cstc2019jcyj-cxttX0002, cstc2021ycjh-bgzxm0013),
   the key cooperation project of Chongqing Municipal Education Commission
   (Grant No: HZ2021008).
CR Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Chen GY, 2019, IEEE I CONF COMP VIS, P9636, DOI 10.1109/ICCV.2019.00973
   Chen PX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11813, DOI 10.1109/ICCV48922.2021.01162
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Chen WB, 2022, MULTIMED TOOLS APPL, V81, P4649, DOI 10.1007/s11042-020-10494-4
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Eom C, 2019, ADV NEUR IN, V32
   Fan HJ, 2024, IEEE T IND INFORM, V20, P442, DOI 10.1109/TII.2023.3266372
   Gao Z, 2024, IEEE T CIRC SYST VID, V34, P2010, DOI 10.1109/TCSVT.2023.3296680
   He LX, 2019, IEEE I CONF COMP VIS, P8449, DOI 10.1109/ICCV.2019.00854
   He LX, 2018, PROC CVPR IEEE, P7073, DOI 10.1109/CVPR.2018.00739
   He ST, 2024, IEEE T INF FOREN SEC, V19, P120, DOI 10.1109/TIFS.2023.3318956
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Hou RB, 2022, IEEE T PATTERN ANAL, V44, P4894, DOI 10.1109/TPAMI.2021.3079910
   Huang HJ, 2018, PROC CVPR IEEE, P5098, DOI 10.1109/CVPR.2018.00535
   Iodice S, 2019, LECT NOTES COMPUT SC, V11366, P101, DOI 10.1007/978-3-030-20876-9_7
   Jia MX, 2023, IEEE T MULTIMEDIA, V25, P1294, DOI 10.1109/TMM.2022.3141267
   Jin X, 2020, AAAI CONF ARTIF INTE, V34, P11173
   Kuan Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P346, DOI 10.1007/978-3-030-58580-8_21
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Liao SC, 2015, IEEE I CONF COMP VIS, P3685, DOI 10.1109/ICCV.2015.420
   Liu YJ, 2023, IMAGE VISION COMPUT, V140, DOI 10.1016/j.imavis.2023.104844
   Luo G, 2022, IEEE T IMAGE PROCESS, V31, P3386, DOI 10.1109/TIP.2021.3139234
   Ma BP, 2014, IMAGE VISION COMPUT, V32, P379, DOI 10.1016/j.imavis.2014.04.002
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shang Gao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11741, DOI 10.1109/CVPR42600.2020.01176
   Shi YX, 2022, NEUROCOMPUTING, V486, P237, DOI 10.1016/j.neucom.2021.11.038
   Shi YX, 2022, NEUROCOMPUTING, V470, P226, DOI 10.1016/j.neucom.2021.11.013
   Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562
   Somers V, 2023, IEEE WINT CONF APPL, P1613, DOI 10.1109/WACV56688.2023.00166
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tan Lei, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P531, DOI 10.1145/3503161.3547764
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang PF, 2023, IEEE T MULTIMEDIA, V25, P3154, DOI 10.1109/TMM.2022.3156282
   Wang T, 2022, AAAI CONF ARTIF INTE, P2540
   Wang ZK, 2022, PROC CVPR IEEE, P4744, DOI 10.1109/CVPR52688.2022.00471
   Xiang SC, 2020, MULTIMED TOOLS APPL, V79, P32079, DOI 10.1007/s11042-020-09569-z
   Xu BQ, 2022, IEEE T IMAGE PROCESS, V31, P4651, DOI 10.1109/TIP.2022.3186759
   Yan G, 2023, IEEE T CIRC SYST VID, V33, P4217, DOI 10.1109/TCSVT.2023.3241764
   Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35
   Yao HT, 2019, IEEE T IMAGE PROCESS, V28, P2860, DOI 10.1109/TIP.2019.2891888
   Ye M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13547, DOI 10.1109/ICCV48922.2021.01331
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yu R, 2018, LECT NOTES COMPUT SC, V11220, P196, DOI 10.1007/978-3-030-01270-0_12
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zhang XK, 2021, IEEE T CIRC SYST VID, V31, P2764, DOI 10.1109/TCSVT.2020.3033165
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng KC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4537, DOI 10.1145/3474085.3475610
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Liang, 2016, PERSON REIDENTIFICAT
   Zheng M, 2019, PROC CVPR IEEE, P5728, DOI 10.1109/CVPR.2019.00588
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhou QQ, 2020, IEEE T IMAGE PROCESS, V29, P7578, DOI 10.1109/TIP.2020.3004267
   Zhou X, 2023, PROC CVPR IEEE, P19691, DOI 10.1109/CVPR52729.2023.01886
   Zhou Y, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00679
   Zhu K., 2021, arXiv
   Zhu ML, 2023, SYMMETRY-BASEL, V15, DOI 10.3390/sym15040906
   Zhuang XY, 2024, IMAGE VISION COMPUT, V143, DOI 10.1016/j.imavis.2024.104921
   Zhuo JX, 2018, IEEE INT CON MULTI
   Zou GF, 2021, MULTIMED TOOLS APPL, V80, P26855, DOI 10.1007/s11042-021-10953-6
NR 65
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104178
DI 10.1016/j.jvcir.2024.104178
EA MAY 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TP4D6
UT WOS:001242441800001
DA 2024-08-05
ER

PT J
AU Yuan, S
   Li, JJ
   Ren, L
   Chen, Z
AF Yuan, Shuang
   Li, Jinjiang
   Ren, Lu
   Chen, Zheng
TI Multi-Frequency Field Perception and Sparse Progressive Network for
   low-light image enhancement
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Frequency domain scene perception; Sparse enhancement factor; Iterative
   progressive structure
ID QUALITY ASSESSMENT
AB Images taken in low -light conditions often suffer from various types of degradation. While most current methods primarily focus on spatial domain information to address these degradation issues, they often overlook the importance of frequency domain information. In order to better solve the degradation problems of lowlight images, the Multi -Frequency Field Perception and Sparse Progressive Network (MFSPNet) for low -light image enhancement is proposed through Leveraging the complementary strengths of the frequency domain and the spatial domain, aiming to tackle the challenges of degraded images in intricate scenarios. Specifically, we propose the frequency domain field feature filtering (FDFF) module, which utilizes image frequency distribution information to address issues such as noise and artifacts in low -light images while complementing the spatial domain. Subsequently, we embed different scales of FDFF into four heterogeneous branches to flexibly handle features at various levels of complexity. Furthermore, we design a sparse wing -shaped transformer block (SWTB) that enhances the focus on critical information and complex multi -scale details through adaptive sparse attention unit (ASAU) and illumination multi -scale fusion feedforward network (IMFFN). In addition, we propose a progressive enhancement strategy for self-knowledge sublimation to gradually improve image quality. At last, we comprehensively assess the proposed network across multiple datasets. Compared to other methods, our approach achieved the highest PSNR scores, with improvements of 3.014 dB and 0.215 dB, respectively, over the next best results. Additionally, our method exhibited the highest SSIM gain. Abundant experimental outcomes demonstrate that our approach outperforms numerous present low -light image enhancement approaches in both objective evaluation metrics and subjective visual effects, showcasing outstanding performance and significant potential.
C1 [Yuan, Shuang] Shandong Technol & Business Univ, Sch Informat & Elect Engn, Yantai 264005, Peoples R China.
   [Li, Jinjiang; Chen, Zheng] Shandong Technol & Business Univ, Sch Comp Sci & Technol, Yantai 264005, Peoples R China.
   [Ren, Lu] Inst Network Technol INT, Yantai 264005, Peoples R China.
C3 Shandong Technology & Business University; Shandong Technology &
   Business University
RP Chen, Z (corresponding author), Shandong Technol & Business Univ, Sch Comp Sci & Technol, Yantai 264005, Peoples R China.
EM chenzheng@sdtbu.edu.cn
FU National Natural Science Foun-dation of China [61772319, 62002200,
   62202268, 62272281]; Shan-dong Natural Science Foundation of China
   [ZR2021MF107, ZR2022MA076]; Yantai science and technology innovation
   develop-ment plan [2022JCYJ031]
FX This research was supported by the National Natural Science Foun-dation
   of China (61772319, 62002200, 62202268, 62272281) , Shan-dong Natural
   Science Foundation of China (ZR2021MF107, ZR2022MA076) , Yantai science
   and technology innovation develop-ment plan (2022JCYJ031) .
CR Al-Ameen Z, 2019, IET IMAGE PROCESS, V13, P1314, DOI 10.1049/iet-ipr.2018.6585
   Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652
   Chen BL, 2024, IEEE T MULTIMEDIA, V26, P3430, DOI 10.1109/TMM.2023.3312851
   Chen T., 2021, Neural Information Processing Systems
   Cheng CY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3109379
   Cui Z., 2022, BRIT MACH VIS C
   Deng G, 2011, IEEE T IMAGE PROCESS, V20, P1249, DOI 10.1109/TIP.2010.2092441
   Deng X, 2021, IEEE T IMAGE PROCESS, V30, P3098, DOI 10.1109/TIP.2021.3058764
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fan GD, 2024, IEEE T NEUR NET LEAR, V35, P1598, DOI 10.1109/TNNLS.2022.3184164
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Guo JY, 2022, PROC CVPR IEEE, P12165, DOI 10.1109/CVPR52688.2022.01186
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang J, 2022, LECT NOTES COMPUT SC, V13679, P163, DOI 10.1007/978-3-031-19800-7_10
   Jiang Q., 2022, arXiv
   Jiang QP, 2022, IEEE T INTELL TRANSP, V23, P19440, DOI 10.1109/TITS.2022.3165176
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Xu ZQJ, 2022, Arxiv, DOI arXiv:2201.07395
   Kingma D. P., 2014, arXiv
   Kong LS, 2023, PROC CVPR IEEE, P5886, DOI 10.1109/CVPR52729.2023.00570
   Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059
   Li C, 2020, PROC CVPR IEEE, P3562, DOI 10.1109/CVPR42600.2020.00362
   Li CY, 2023, Arxiv, DOI arXiv:2302.11831
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P4225, DOI 10.1109/TPAMI.2021.3063604
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Li YW, 2021, Arxiv, DOI arXiv:2104.05707
   Liang ZX, 2023, IEEE I CONF COMP VIS, P8060, DOI 10.1109/ICCV51070.2023.00743
   Lim S, 2021, IEEE T MULTIMEDIA, V23, P4272, DOI 10.1109/TMM.2020.3039361
   Liu JY, 2023, INFORM FUSION, V95, P237, DOI 10.1016/j.inffus.2023.02.027
   Lu YX, 2022, IEEE SIGNAL PROC LET, V29, P1127, DOI 10.1109/LSP.2022.3162145
   Lv F., 2018, Proc. BMVC, V220, P4
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Ma L, 2023, Arxiv, DOI arXiv:2306.01343
   Ma L, 2022, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR52688.2022.00555
   Mao XT, 2022, Arxiv, DOI [arXiv:2111.11745, DOI 10.48550/ARXIV.2111.11745]
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Pan QZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1875, DOI 10.1145/3474085.3475341
   Park Namuk, 2022, arXiv
   Pizer S. M., 1990, Proceedings of the First Conference on Visualization in Biomedical Computing (Cat. No.90TH0311-1), P337, DOI 10.1109/VBC.1990.109340
   Rao Yongming, 2021, Advances in neural information processing systems, V34
   Rasheed MT, 2023, SIGNAL PROCESS, V204, DOI 10.1016/j.sigpro.2022.108821
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren YR, 2019, IEEE T CIRC SYST VID, V29, P968, DOI 10.1109/TCSVT.2018.2828141
   Ronneberger O, 2015, Arxiv, DOI arXiv:1505.04597
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Vaswani A, 2017, ADV NEUR IN, V30
   Vonikakis V, 2018, MULTIMED TOOLS APPL, V77, P9211, DOI 10.1007/s11042-017-4783-x
   Wang LW, 2020, IEEE T IMAGE PROCESS, V29, P7984, DOI 10.1109/TIP.2020.3008396
   Wang P., 2022, arXiv
   Wang PC, 2022, Arxiv, DOI [arXiv:2106.00515, 10.48550/ARXIV.2106.00515]
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang T, 2022, Arxiv, DOI arXiv:2212.11548
   Wang Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2015, DOI 10.1145/3343031.3350983
   Wang YL, 2021, PROC CVPR IEEE, P13370, DOI 10.1109/CVPR46437.2021.01317
   Wang YF, 2022, AAAI CONF ARTIF INTE, P2604
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Wei C, 2018, Arxiv, DOI arXiv:1808.04560
   Wu WH, 2022, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR52688.2022.00581
   Xu XG, 2022, PROC CVPR IEEE, P17693, DOI 10.1109/CVPR52688.2022.01719
   Yang BC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1552, DOI 10.1145/3343031.3350878
   Ying ZQ, 2017, Arxiv, DOI arXiv:1711.00591
   Zamir S.W., 2020, EUR C COMP VIS
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang JL, 2022, Arxiv, DOI arXiv:2210.01427
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang Y., 2023, arXiv
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhao G, 2019, Sparse transformer: Concentrated attention through explicit selection
   Zhao GX, 2019, Arxiv, DOI arXiv:1912.11637
   Zheng NS, 2023, IEEE I CONF COMP VIS, P12525, DOI 10.1109/ICCV51070.2023.01154
   Zhou JC, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3293912
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 80
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104133
DI 10.1016/j.jvcir.2024.104133
EA APR 2024
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QX7M0
UT WOS:001224231800001
DA 2024-08-05
ER

PT J
AU Kumar, BP
   Kumar, A
   Pandey, R
AF Kumar, Balla Pavan
   Kumar, Arvind
   Pandey, Rajoo
TI A 4-channelled hazy image input generation and deep learning-based
   single image dehazing
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Image dehazing; Regional haze properties; 4-channel hazy image input;
   RCNN
AB The images in foggy weather are often degraded which may affect the object detection process. Hence, in recent times, several schemes have been designed to eliminate the haze effect and enhance the performance of computer vision systems. Although these methods reduce the haze effect, they also produce over -saturation and over -degradation on most occasions. These problems occur due to inadequate utilization of the regional haze properties during the dehazing process. Therefore, a new dehazing model is proposed in this paper to address the issues faced by existing dehazing algorithms. The residual convolutional neural network (RCNN) with 14 layers is developed in the present work. In this model, features to indicate the extent of haze are extracted using the regional characteristics of a hazy image and are used as the fourth channel along with the three colour channels of a hazy image. The proposed RCNN is trained with 4 -channel hazy images as input and their corresponding haze -free images as output. The results of our model are obtained with several images taken from three different datasets. Through the experimentation, quantitative and qualitative improvements are observed when compared to state-of-the-art dehazing methods.
C1 [Kumar, Balla Pavan; Kumar, Arvind; Pandey, Rajoo] Natl Inst Technol, Kurukshetra 136119, Haryana, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Kurukshetra
RP Kumar, BP (corresponding author), Natl Inst Technol, Kurukshetra 136119, Haryana, India.
EM pavan_6180103@nitkkr.ac.in
OI Balla, Pavan Kumar/0000-0001-9011-5503; Pandey,
   Rajoo/0000-0002-2775-0448
CR Agrawal SC, 2022, IEEE T CIRC SYST VID, V32, P593, DOI 10.1109/TCSVT.2021.3068625
   AMARI S, 1993, NEUROCOMPUTING, V5, P185, DOI 10.1016/0925-2312(93)90006-O
   Babu GH, 2023, J VIS COMMUN IMAGE R, V97, DOI 10.1016/j.jvcir.2023.103976
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen WT, 2022, LECT NOTES COMPUT SC, V13674, P427, DOI 10.1007/978-3-031-19781-9_25
   Chen WT, 2020, IEEE T IMAGE PROCESS, V29, P6773, DOI 10.1109/TIP.2020.2993407
   Chen Y, 2023, Multimedia Tools Appl., P1
   Chen YT, 2024, COMPUT VIS IMAGE UND, V238, DOI 10.1016/j.cviu.2023.103883
   Chen YT, 2024, INT J MACH LEARN CYB, V15, P1815, DOI 10.1007/s13042-023-01999-z
   Chen YT, 2023, J KING SAUD UNIV-COM, V35, DOI 10.1016/j.jksuci.2023.101567
   Chen YL, 2018, 2018 IEEE 3RD INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC), P374, DOI 10.1109/ICIVC.2018.8492834
   Deng G, 2021, IEEE OPEN J SIGNAL P, V2, P119, DOI 10.1109/OJSP.2021.3063076
   Ding KY, 2021, INT J COMPUT VISION, V129, P1258, DOI 10.1007/s11263-020-01419-7
   Ehsan SM, 2021, IEEE ACCESS, V9, P89055, DOI 10.1109/ACCESS.2021.3090078
   Galdran A, 2018, SIGNAL PROCESS, V149, P135, DOI 10.1016/j.sigpro.2018.03.008
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Hu K, 2024, J VIS COMMUN IMAGE R, V98, DOI 10.1016/j.jvcir.2023.104042
   Juneja A, 2023, J VIS COMMUN IMAGE R, V94, DOI 10.1016/j.jvcir.2023.103855
   Kaplan NH, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103720
   Kingma D. P., 2014, arXiv
   Koschmieder H., 1924, BEITR PHYS FREIEN AT, P33, DOI DOI 10.1007/978-3-663-04661-5_2
   Kou F, 2015, IEEE T IMAGE PROCESS, V24, P4528, DOI 10.1109/TIP.2015.2468183
   Kumar BP, 2023, SIGNAL IMAGE VIDEO P, V17, P3183, DOI 10.1007/s11760-023-02540-z
   Kumar BP, 2022, SIGNAL PROCESS-IMAGE, V100, DOI 10.1016/j.image.2021.116532
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li J, 2023, IEEE T MULTIMED
   Li Z, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21030960
   Li ZG, 2021, IEEE T IMAGE PROCESS, V30, P9270, DOI 10.1109/TIP.2021.3123551
   Liu J, 2023, IEEE T PATTERN ANAL, V45, P8845, DOI 10.1109/TPAMI.2022.3226276
   Liu XN, 2022, IEEE T MULTIMEDIA, V24, P3934, DOI 10.1109/TMM.2021.3110483
   Liu YF, 2023, DISPLAYS, V78, DOI 10.1016/j.displa.2023.102416
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P2519, DOI 10.1109/TIP.2017.2671921
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Ngo D, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21113896
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ramazzina A, 2023, Arxiv, DOI arXiv:2305.02103
   Shin J, 2022, IEEE T MULTIMEDIA, V24, P245, DOI 10.1109/TMM.2021.3050053
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song GL, 2024, J RADIAT RES APPL SC, V17, DOI 10.1016/j.jrras.2024.100822
   Su Z, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103706
   Ullah H, 2021, IEEE T IMAGE PROCESS, V30, P8968, DOI 10.1109/TIP.2021.3116790
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZB, 2023, J IMAGING, V9, DOI 10.3390/jimaging9090183
   Xie XX, 2023, J VIS COMMUN IMAGE R, V97, DOI 10.1016/j.jvcir.2023.103984
   Zhang YF, 2017, IEEE IMAGE PROC, P3205, DOI 10.1109/ICIP.2017.8296874
   Zheng Y, 2023, PROC CVPR IEEE, P5785, DOI 10.1109/CVPR52729.2023.00560
   Zuiderveld K., 1994, Graphics gems, DOI 10.1016/b978-0-12-336156-1.50061-6
NR 51
TC 1
Z9 1
U1 5
U2 5
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104099
DI 10.1016/j.jvcir.2024.104099
EA MAR 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OF5T7
UT WOS:001205871600001
DA 2024-08-05
ER

PT J
AU Hsu, LY
AF Hsu, Ling -Yuan
TI AI-assisted deepfake detection using adaptive blind image watermarking
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Deepfake detection; Blind image watermarking; Partly sign -altered;
   Mixed modulation
ID AUTHENTICATION; DCT
AB This paper proposes a new adaptive blind watermarking technology for deepfake detection, which can embed deepfake detection information into the image and verify the image's authenticity without requiring additional information. The proposed scheme utilizes mixed modulation combined with partly sign-altered mean value to embed a set of coefficients that enhance robustness against attacks while maintaining high image quality. Additionally, blind adaptive deepfake detection technology with the tamper detection mean value is employed to detect relative positions adaptively, even when face images are slightly modified or deepfaked. To further improve the performance of the proposed scheme, a gray wolf optimizer is introduced to optimize parameters, and a denoising autoencoder is employed to facilitate the identification of extracted watermarks. This technology will adaptively embed watermark information while preserving the original face image, thereby maintaining the authenticity of the face in the image and verifying the owner of the image. The code is available at https://github.com/lyhsu01/AwDD.
C1 [Hsu, Ling -Yuan] Natl Ilan Univ, Dept Elect Engn, Yilan, Taiwan.
C3 National Ilan University
RP Hsu, LY (corresponding author), Natl Ilan Univ, Dept Elect Engn, Yilan, Taiwan.
EM lyhsu@niu.edu.tw
OI Hsu, Ling-Yuan/0000-0002-9543-6872
CR Agarwal S, 2019, Arxiv, DOI arXiv:1905.03493
   Arbelaez P., BERKELEY SEGMENTATIO
   Bonomi M, 2021, J VIS COMMUN IMAGE R, V79, DOI 10.1016/j.jvcir.2021.103239
   C. V. G. University of Granada, CVG-UGR Image Database
   Cao F, 2023, J VIS COMMUN IMAGE R, V94, DOI 10.1016/j.jvcir.2023.103837
   Cao HJ, 2022, OPTIK, V262, DOI 10.1016/j.ijleo.2022.169319
   Ciftci Umur Aybars, 2020, IEEE Trans Pattern Anal Mach Intell, VPP, DOI 10.1109/TPAMI.2020.3009287
   Creswell A, 2017, Arxiv, DOI [arXiv:1708.08487, 10.48550/arXiv.1708.08487]
   Guarnera L, 2020, IEEE COMPUT SOC CONF, P2841, DOI 10.1109/CVPRW50498.2020.00341
   Hasan HR, 2019, IEEE ACCESS, V7, P41596, DOI 10.1109/ACCESS.2019.2905689
   Hasler M, 1997, IEEE T CIRCUITS-I, V44, P856, DOI 10.1109/81.633874
   Heo YJ, 2023, APPL INTELL, V53, P7512, DOI 10.1007/s10489-022-03867-9
   Hertzmann A, 2020, LEONARDO, V53, P424, DOI [10.1162/LEON_a_01930, 10.1145/3386567.3388574]
   Hjelmås E, 2001, COMPUT VIS IMAGE UND, V83, P236, DOI 10.1006/cviu.2001.0921
   Hsu LY, 2017, J VIS COMMUN IMAGE R, V46, P33, DOI 10.1016/j.jvcir.2017.03.009
   Hu HT, 2016, AEU-INT J ELECTRON C, V70, P172, DOI 10.1016/j.aeue.2015.11.003
   Ju Y., 2024, P IEEE CVF WINT C AP, P4655
   Kamili A, 2021, IEEE T IND INFORM, V17, P5108, DOI 10.1109/TII.2020.3028612
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Ki Chan Christopher Chun, 2020, 2020 IEEE/ITU International Conference on Artificial Intelligence for Good (AI4G), P55, DOI 10.1109/AI4G50087.2020.9311067
   Ko HJ, 2020, INFORM SCIENCES, V517, P128, DOI 10.1016/j.ins.2019.11.005
   Louizos C, 2017, Arxiv, DOI arXiv:1511.00830
   Maurer UM, 2000, IEEE T INFORM THEORY, V46, P1350, DOI 10.1109/18.850674
   Mirjalili S, 2014, ADV ENG SOFTW, V69, P46, DOI 10.1016/j.advengsoft.2013.12.007
   Moosazadeh M, 2017, OPTIK, V140, P975, DOI 10.1016/j.ijleo.2017.05.011
   Negi S., 2021, International Journal of Scientific Research in Computer Science, Engineering and Information Technology, P183
   Rana MS, 2022, IEEE ACCESS, V10, P25494, DOI 10.1109/ACCESS.2022.3154404
   Rasti P, 2016, J VIS COMMUN IMAGE R, V38, P838, DOI 10.1016/j.jvcir.2016.05.001
   Sage A., LLD-Large Logo Dataset
   Salih Z.A., 2022, 2022 IEEE INT C ART, P1
   Thabit R, 2023, IET IMAGE PROCESS, V17, P3938, DOI 10.1049/ipr2.12909
   Wang CP, 2023, J VIS COMMUN IMAGE R, V95, DOI 10.1016/j.jvcir.2023.103875
   Wang TY, 2024, Arxiv, DOI arXiv:2311.01357
   Wang XY, 2023, J VIS COMMUN IMAGE R, V97, DOI 10.1016/j.jvcir.2023.103986
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
NR 36
TC 1
Z9 1
U1 6
U2 6
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104094
DI 10.1016/j.jvcir.2024.104094
EA MAR 2024
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QF5F2
UT WOS:001219469200001
DA 2024-08-05
ER

PT J
AU Zhang, GQ
   Zhang, YY
   Zhang, HW
   Chen, YH
   Zheng, YH
AF Zhang, Guoqing
   Zhang, Yinyin
   Zhang, Hongwei
   Chen, Yuhao
   Zheng, Yuhui
TI Learning dual attention enhancement feature for visible-infrared person
   re-identification
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Cross-modality; Person re-identification; Shallow feature
ID NETWORK
AB Most previous visible-infrared person re-identification methods emphasized learning modality-shared features to narrow the modality differences, while neglecting the benefits of modality-specific features for feature embedding and narrowing the modality gap. To tackle this issue, our paper designs a method based on dual attention enhancement features to use shallow and deep features simultaneously. We first convert visible images into gray images to alleviate the visual difference. Then, to close the difference between modalities by learning the modality-specific features, we design a shallow feature measurement module, in which we use a class-specific maximum mean discrepancy loss to measure the distribution difference of specific features between two modalities. Finally, we design a dual attention feature enhancement module, which aims to mine more useful context information from modality-shared features to shorter the distance between classes within modalities. Specifically, our model exceeds the current SOTAs on SYSU-MM01, with 66.61% Rank -1 accuracy and 62.86% mAP.
C1 [Zhang, Guoqing; Zhang, Yinyin; Zhang, Hongwei; Chen, Yuhao; Zheng, Yuhui] Nanjing Univ Informat Sci & Technol, Sch Comp Sci, Nanjing 210044, Peoples R China.
   [Zhang, Guoqing] Nanjing Univ Sci & Technol, Jiangsu Key Lab Image & Video Understanding Social, Nanjing 210094, Peoples R China.
C3 Nanjing University of Information Science & Technology; Nanjing
   University of Science & Technology
RP Zheng, YH (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Comp Sci, Nanjing 210044, Peoples R China.
EM guoqingzhang@nuist.edu.cn; mkkdzz2428@163.com;
   hongweizhang_123@outlook.com; chinayhchen@gmail.com;
   zheng_yuhui@nuist.edu.cn
RI Zhang, Hongwei/IAR-6558-2023
OI Zhang, Hongwei/0000-0002-0646-176X
FU National Natural Science Foun-dation of China [62172231, U22B2056,
   U20B2065]; Natural Science Foundation of Jiangsu Province of China
   [BK20220107, BK20211539]
FX This research was supported by the National Natural Science Foun-dation
   of China under Grant 62172231, U22B2056 and U20B2065; and by the Natural
   Science Foundation of Jiangsu Province of China under Grant BK20220107
   and BK20211539.
CR Chen CQ, 2022, IEEE T IMAGE PROCESS, V31, P2352, DOI 10.1109/TIP.2022.3141868
   Chen S, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103749
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005
   Feng ZX, 2020, IEEE T IMAGE PROCESS, V29, P579, DOI 10.1109/TIP.2019.2928126
   Fu CY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11803, DOI 10.1109/ICCV48922.2021.01161
   Guangcong Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10565, DOI 10.1109/CVPR42600.2020.01058
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Hu B., 2021, 2021 IEEE INT C MULT, P1
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Jaderberg M, 2015, ADV NEUR IN, V28
   Li DG, 2020, AAAI CONF ARTIF INTE, V34, P4610
   Li YY, 2022, J VIS COMMUN IMAGE R, V89, DOI 10.1016/j.jvcir.2022.103689
   Liang WQ, 2018, Arxiv, DOI arXiv:1811.03768
   Liang WQ, 2021, IEEE T IMAGE PROCESS, V30, P6392, DOI 10.1109/TIP.2021.3092578
   Liu HJ, 2021, IEEE SIGNAL PROC LET, V28, P653, DOI 10.1109/LSP.2021.3065903
   Liu JN, 2022, IEEE T CIRC SYST VID, V32, P7226, DOI 10.1109/TCSVT.2022.3168999
   Mang Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P229, DOI 10.1007/978-3-030-58520-4_14
   Mnih V, 2014, ADV NEUR IN, V27
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Park H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12026, DOI 10.1109/ICCV48922.2021.01183
   Pu N, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2149, DOI 10.1145/3394171.3413673
   Seokeon Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10254, DOI 10.1109/CVPR42600.2020.01027
   Sun ZR, 2020, 2020 INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND SIGNAL PROCESSING (AISP), DOI 10.1109/aisp48273.2020.9073293
   Tang CR, 2022, J VIS COMMUN IMAGE R, V89, DOI 10.1016/j.jvcir.2022.103674
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang DW, 2023, J VIS COMMUN IMAGE R, V93, DOI 10.1016/j.jvcir.2023.103822
   Wang F, 2016, Arxiv, DOI arXiv:1601.06823
   Wang GA, 2020, AAAI CONF ARTIF INTE, V34, P12144
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang GC, 2019, AAAI CONF ARTIF INTE, P8933
   Wang GC, 2018, IEEE T CIRC SYST VID, V28, P2777, DOI 10.1109/TCSVT.2017.2748698
   Wang GR, 2021, IEEE T NEUR NET LEAR, V32, P2142, DOI 10.1109/TNNLS.2020.2999517
   Wang ZX, 2019, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.2019.00071
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Yang MX, 2022, PROC CVPR IEEE, P14288, DOI 10.1109/CVPR52688.2022.01391
   Ye HR, 2021, IEEE T IMAGE PROCESS, V30, P1583, DOI 10.1109/TIP.2020.3045261
   Ye M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13547, DOI 10.1109/ICCV48922.2021.01331
   Ye M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1092
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Ye M, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P347, DOI 10.1145/3343031.3351043
   Ye M, 2020, IEEE T INF FOREN SEC, V15, P407, DOI 10.1109/TIFS.2019.2921454
   Ye M, 2018, AAAI CONF ARTIF INTE, P7501
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zhu YX, 2020, NEUROCOMPUTING, V386, P97, DOI 10.1016/j.neucom.2019.12.100
   Zhang G., 2021, arXiv
   Zhang GQ, 2023, IEEE INT CON MULTI, P2429, DOI 10.1109/ICME55011.2023.00414
   Zhang GQ, 2023, IEEE T IMAGE PROCESS, V32, P4555, DOI 10.1109/TIP.2023.3279673
   Zhang GQ, 2023, IEEE T CIRC SYST VID, V33, P4096, DOI 10.1109/TCSVT.2023.3240001
   Zhang GQ, 2022, IEEE T CIRC SYST VID, V32, P6766, DOI 10.1109/TCSVT.2022.3169422
   Zhang GQ, 2022, J VIS COMMUN IMAGE R, V87, DOI 10.1016/j.jvcir.2022.103581
   Zhang GQ, 2021, IEEE T IMAGE PROCESS, V30, P8913, DOI 10.1109/TIP.2021.3120054
   Zhang GQ, 2021, INFORM SCIENCES, V578, P525, DOI 10.1016/j.ins.2021.07.058
   Zhang HW, 2022, IEEE T CIRC SYST VID, V32, P8599, DOI 10.1109/TCSVT.2022.3194084
   Zhang L, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3473341
   Zhang Q, 2022, PROC CVPR IEEE, P7339, DOI 10.1109/CVPR52688.2022.00720
   Zhang Q, 2021, IEEE T IMAGE PROCESS, V30, P8019, DOI 10.1109/TIP.2021.3112035
   Zhang X, 2018, Arxiv, DOI arXiv:1711.08184
   Zhao YB, 2019, IET IMAGE PROCESS, V13, P2897, DOI 10.1049/iet-ipr.2019.0699
   Zheng L, 2016, Arxiv, DOI arXiv:1610.02984
   Zhuo JX, 2018, IEEE INT CON MULTI
NR 65
TC 0
Z9 0
U1 16
U2 16
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104076
DI 10.1016/j.jvcir.2024.104076
EA FEB 2024
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NE0Z6
UT WOS:001198672800001
DA 2024-08-05
ER

PT J
AU Wang, SW
   Yang, XH
   Feng, ZQ
   Sun, JD
   Liu, J
AF Wang, Shaowen
   Yang, Xiaohui
   Feng, Zhiquan
   Sun, Jiande
   Liu, Ju
TI EMCFN: Edge-based Multi-scale Cross Fusion Network for video frame
   interpolation
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Video frame interpolation; Multi-scale cross fusion network; Optical
   flow; Frame synthesis
ID CONVOLUTIONAL NEURAL-NETWORK
AB Video frame interpolation (VFI) is used to synthesize one or more intermediate frames between two frames in a video sequence to improve the temporal resolution of the video. However, many methods still face challenges when dealing with complex scenes involving high-speed motion, occlusions, and other factors. To address these challenges, we propose an Edge-based Multi-scale Cross Fusion Network (EMCFN) for VFI. We integrate a feature enhancement module (FEM) based on edge information into the U-Net architecture, resulting in richer and more complete feature maps, while also enhancing the preservation of image structure and details. This contributes to generating more accurate and realistic interpolated frames. At the same time, we use a multi-scale cross fusion frame synthesis model (MCFM) composed of three GridNet branches to generate highquality interpolation frames. We have conducted a series of experiments and the results show that our model exhibits satisfactory performance on different datasets compared with the state-of-the-art methods.
C1 [Wang, Shaowen; Yang, Xiaohui; Feng, Zhiquan] Univ Jinan, Sch Informat Sci & Engn, Jinan 250022, Peoples R China.
   [Sun, Jiande] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Peoples R China.
   [Liu, Ju] Shandong Univ, Sch Informat Sci & Engn, Qingdao 266237, Peoples R China.
   [Yang, Xiaohui] Jinan Inspur Data Technol Co Ltd, Jinan 250101, Peoples R China.
C3 University of Jinan; Shandong Normal University; Shandong University
RP Yang, XH (corresponding author), Univ Jinan, Sch Informat Sci & Engn, Jinan 250022, Peoples R China.
EM ise_xhyang@ujn.edu.cn
FU Shandong Provincial Natural Science Foundation [ZR2023LZH013]; Jinan
   Municipal and School Integration Development Strategy Project
   [JNSX2023025, JNSX2023015]
FX This work was supported by the Shandong Provincial Natural Science
   Foundation (No. ZR2023LZH013) and the Jinan Municipal and School
   Integration Development Strategy Project (JNSX2023025, JNSX2023015) .
CR Bao WB, 2019, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2019.00382
   Bao WB, 2021, IEEE T PATTERN ANAL, V43, P933, DOI 10.1109/TPAMI.2019.2941941
   Bucilua C., 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464
   Cao JM, 2022, IEEE T IMAGE PROCESS, V31, P3726, DOI 10.1109/TIP.2022.3175432
   Cheng XH, 2022, IEEE T PATTERN ANAL, V44, P7029, DOI 10.1109/TPAMI.2021.3100714
   Cheng XH, 2020, IEEE T CIRC SYST VID, V30, P3968, DOI 10.1109/TCSVT.2019.2939143
   Danier D, 2022, PROC CVPR IEEE, P3511, DOI 10.1109/CVPR52688.2022.00351
   Ding TY, 2021, PROC CVPR IEEE, P7997, DOI 10.1109/CVPR46437.2021.00791
   Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595
   Fourure D., 2017, P BRIT MACH VIS C
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Hu MS, 2022, IEEE T CIRC SYST VID, V32, P3390, DOI 10.1109/TCSVT.2021.3110796
   Huo S, 2021, IEEE T CIRC SYST VID, V31, P1178, DOI 10.1109/TCSVT.2020.2995243
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938
   Jin X, 2023, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR52729.2023.00158
   Jing PG, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103735
   Junheum Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P109, DOI 10.1007/978-3-030-58568-6_7
   Kingma D. P., 2014, arXiv
   Kong LT, 2022, PROC CVPR IEEE, P1968, DOI 10.1109/CVPR52688.2022.00201
   Lee H, 2020, PROC CVPR IEEE, P5315, DOI 10.1109/CVPR42600.2020.00536
   Lei P., 2023, IEEE Trans. Multimed.
   Li HP, 2020, INT CONF ACOUST SPEE, P2613, DOI [10.1109/icassp40776.2020.9053987, 10.1109/ICASSP40776.2020.9053987]
   Li SY, 2021, PROC CVPR IEEE, P6583, DOI 10.1109/CVPR46437.2021.00652
   Li SY, 2019, IEEE INT CONF COMP V, P3427, DOI 10.1109/ICCVW.2019.00425
   Li Y, 2022, INT J COMPUT VISION, V130, P2980, DOI 10.1007/s11263-022-01683-9
   Liang M, 2019, PROC CVPR IEEE, P7337, DOI 10.1109/CVPR.2019.00752
   Liu YL, 2019, AAAI CONF ARTIF INTE, P8794
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCV.2017.478, 10.1109/ICCVW.2017.361]
   Long GC, 2016, LECT NOTES COMPUT SC, V9910, P434, DOI 10.1007/978-3-319-46466-4_26
   Lu LY, 2022, PROC CVPR IEEE, P3522, DOI 10.1109/CVPR52688.2022.00352
   Niklaus S, 2018, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2018.00183
   Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37
   Niklaus S, 2017, PROC CVPR IEEE, P2270, DOI 10.1109/CVPR.2017.244
   Park M, 2020, INT CONF ACOUST SPEE, P1958, DOI [10.1109/icassp40776.2020.9054744, 10.1109/ICASSP40776.2020.9054744]
   Paszke A., 2019, NeurIPS, V32
   Peleg T, 2019, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2019.00250
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Reda F, 2022, LECT NOTES COMPUT SC, V13667, P250, DOI 10.1007/978-3-031-20071-7_15
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yan N, 2019, IEEE T CIRC SYST VID, V29, P840, DOI 10.1109/TCSVT.2018.2816932
   Yang K, 2022, IEEE T CIRC SYST VID, V32, P359, DOI 10.1109/TCSVT.2021.3061153
   Zhang D., 2023, ACM T MULTIM COMPUT, V19, P1
   Zhang DY, 2023, PROCEEDINGS OF THE 2023 ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, IH&MMSEC 2023, P19, DOI 10.1145/3577163.3595098
   Zhang H, 2021, IEEE T CIRC SYST VID, V31, P1953, DOI 10.1109/TCSVT.2020.3011197
   Zhang YT, 2023, IEEE T CIRC SYST VID, V33, P2116, DOI 10.1109/TCSVT.2022.3222875
   Zhao B, 2024, IEEE T NEUR NET LEAR, V35, P1401, DOI 10.1109/TNNLS.2022.3178281
NR 53
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD AUG
PY 2024
VL 103
AR 104226
DI 10.1016/j.jvcir.2024.104226
EA JUL 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YX6A4
UT WOS:001271811700001
DA 2024-08-05
ER

PT J
AU Chen, CM
   Mou, XQ
AF Chen, Congmin
   Mou, Xuanqin
TI Shift-insensitive perceptual feature of quadratic sum of gradient
   magnitude and LoG signals for image quality assessment and image
   classification
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Image quality assessment (IQA); Gradient magnitude; Laplacian of
   Gaussian; Shift -insensitive; Texture classification
ID STRUCTURAL SIMILARITY; TEXTURE CLASSIFICATION; INFORMATION
AB Most existing full-reference (FR) Image quality assessment (IQA) models work in the premise of that the two images should be well registered. Shifting an image would lead to an inaccurate evaluation of image quality, because small spatial shifts are far less noticeable than structural distortion for human observers. To this regard, we propose to study an IQA feature that is shift-insensitive to the basic primitive structure of images, i.e., image edge. According to previous studies, the image gradient magnitude (GM) and the Laplacian of Gaussian (LoG) operator that depict the edge profiles of natural images are highly efficient structural features in IQA tasks. In this paper, we find that the Quadratic sum of the normalized GM and the LoG signals (QGL) has excellent shiftinsensitive property in representing image edges after theoretically solving the selection problem of a ratio parameter to balance the GM and LoG signals. Based on the proposed QGL feature, two FR-IQA models can be built directly by measuring the similarity map with mean and standard deviation pooling strategies, named mQGL and sQGL, respectively. Experimental results show that the proposed sQGL and mQGL work robustly on four benchmark IQA databases, and QGL-based models show great shift-insensitive property to spatial translation and image rotation while judging the image quality. In addition, we explore the feasibility of combining QGL feature with deep neural networks, and verify that it can help to promote image pattern recognition in texture classification tasks.
C1 [Chen, Congmin; Mou, Xuanqin] Xi An Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University
RP Mou, XQ (corresponding author), Xi An Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Xian 710049, Peoples R China.
EM chencongmin@stu.xjtu.edu.cn; xqmou@mail.xjtu.edu.cn
FU National Natural Science Foundation of China (NSFC) [62071375]
FX This work is supported by the National Natural Science Foundation of
   China (NSFC, No. 62071375) .
CR Ahn S, 2021, IEEE COMPUT SOC CONF, P344, DOI 10.1109/CVPRW53098.2021.00044
   Arivazhagan S, 2006, PATTERN RECOGN LETT, V27, P1976, DOI 10.1016/j.patrec.2006.05.008
   Azulay A, 2019, J MACH LEARN RES, V20
   Baqar M, 2019, 2019 IEEE INTERNATIONAL SYMPOSIUM ON HAPTIC AUDIO-VISUAL ENVIRONMENTS & GAMES (HAVE 2019), DOI 10.1109/have.2019.8921188
   Chen CM, 2023, EURASIP J IMAGE VIDE, V2023, DOI 10.1186/s13640-023-00611-2
   Chen CM, 2019, LECT NOTES COMPUT SC, V11901, P702, DOI 10.1007/978-3-030-34120-6_57
   Chen WL, 2024, IEEE T MULTIMEDIA, V26, P6398, DOI 10.1109/TMM.2024.3349929
   Cho MS, 2014, PROC CVPR IEEE, P2091, DOI 10.1109/CVPR.2014.268
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Chuan-An Cheng, 2016, 2016 International Symposium on VLSI Technology, Systems and Application (VLSI-TSA), P1, DOI 10.1109/VLSI-TSA.2016.7480487
   Dias TLB, 2022, EUR W VIS INF PROCES, DOI 10.1109/EUVIP53989.2022.9922843
   Ding KY, 2021, INT J COMPUT VISION, V129, P1258, DOI 10.1007/s11263-020-01419-7
   Ding KY, 2022, IEEE T PATTERN ANAL, V44, P2567, DOI 10.1109/TPAMI.2020.3045810
   Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816
   Gao F, 2017, NEUROCOMPUTING, V257, P104, DOI 10.1016/j.neucom.2017.01.054
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Giusti A, 2013, IEEE IMAGE PROC, P4034, DOI 10.1109/ICIP.2013.6738831
   Hansard M, 2011, NEURAL COMPUT, V23, P2324, DOI 10.1162/NECO_a_00163
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang H, 2019, 2019 2ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND PATTERN RECOGNITION (AIPR 2019), P72, DOI 10.1145/3357254.3357255
   Huang Y, 2019, J PHYS CONF SER, V1229, DOI 10.1088/1742-6596/1229/1/012030
   Jakhetiya V, 2017, IEEE T MULTIMEDIA, V19, P93, DOI 10.1109/TMM.2016.2609419
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lee Jake, 2020, Proceedings of the 16th European Conference on Computer Vision (ECCV 2020) Workshops. Lecture Notes in Computer Science (LNCS 12539), P196, DOI 10.1007/978-3-030-68238-5_15
   Li CF, 2010, SIGNAL PROCESS-IMAGE, V25, P517, DOI 10.1016/j.image.2010.03.004
   Li HX, 2015, PROC CVPR IEEE, P5325, DOI 10.1109/CVPR.2015.7299170
   Lin HH, 2019, INT WORK QUAL MULTIM
   Luo FangRui, 2023, 2023 3rd International Conference on Consumer Electronics and Computer Engineering (ICCECE), P57, DOI 10.1109/ICCECE58074.2023.10135541
   Mallikarjuna P., 2006, The KTH-TIPS2 database
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   Moorthy AK, 2009, IEEE J-STSP, V3, P193, DOI 10.1109/JSTSP.2009.2015374
   Mou XQ, 2014, PROC SPIE, V9023, DOI 10.1117/12.2038982
   Ninassi A, 2007, IEEE IMAGE PROC, P733
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Park J, 2013, IEEE T IMAGE PROCESS, V22, P610, DOI 10.1109/TIP.2012.2219551
   Ponomarenko Nikolay, 2013, 2013 4th European Workshop on Visual Information Processing (EUVIP), P106
   Prashnani E, 2018, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2018.00194
   Rawat Paresh, 2023, Computational Intelligence for Engineering and Management Applications: Select Proceedings of CIEMA 2022. Lecture Notes in Electrical Engineering (984), P205, DOI 10.1007/978-981-19-8493-8_16
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sampat MP, 2009, IEEE T IMAGE PROCESS, V18, P2385, DOI 10.1109/TIP.2009.2025923
   Schölkopf B, 2021, P IEEE, V109, P612, DOI 10.1109/JPROC.2021.3058954
   Shao W., 2013, Proceedings of IS&T/SPIE Electronic Imaging, V8660
   Sharifzadeh M, 2022, IEEE T ULTRASON FERR, V69, P1703, DOI 10.1109/TUFFC.2022.3162800
   Sheikh H.R., LIVE image quality assessment database release 2
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan M, 2016, IEEE T MED IMAGING, V35, P1719, DOI 10.1109/TMI.2016.2527619
   Tolias G., 2016, 4 INT C LEARN REPR
   Tong YB, 2010, J IMAGING SCI TECHN, V54, DOI 10.2352/J.ImagingSci.Technol.2010.54.3.030503
   TORRE V, 1986, IEEE T PATTERN ANAL, V8, P147, DOI 10.1109/TPAMI.1986.4767769
   Varma M, 2009, IEEE T PATTERN ANAL, V31, P2032, DOI 10.1109/TPAMI.2008.182
   Wang G., 2019, Machine learning for tomographic imaging
   Wang YT, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2023.103751
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZY, 2024, IEEE J OCEANIC ENG, V49, P592, DOI 10.1109/JOE.2024.3351235
   Wang Z, 2006, IEEE IMAGE PROC, P2945, DOI 10.1109/ICIP.2006.313136
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang L, 2010, IEEE IMAGE PROC, P321, DOI 10.1109/ICIP.2010.5649275
   Zhang M, 2011, IEEE SIGNAL PROC LET, V18, P315, DOI 10.1109/LSP.2011.2127473
   Zhang R, 2019, PR MACH LEARN RES, V97
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zitová B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9
NR 70
TC 0
Z9 0
U1 2
U2 2
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD JUN
PY 2024
VL 102
AR 104215
DI 10.1016/j.jvcir.2024.104215
EA JUN 2024
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XQ0Q7
UT WOS:001263032800001
DA 2024-08-05
ER

PT J
AU Ai, WW
   Yang, Z
   Chen, ZY
   Hu, X
AF Ai, Weiwei
   Yang, Zhao
   Chen, Zhiyong
   Hu, Xiao
TI Maximum open-set entropy optimization via uncertainty measure for
   universal domain adaptation
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Universal domain adaptation; Deep transfer learning; Uncertainty measure
AB Universal Domain Adaptation (UniDA) is a technology that enables the intelligent model to transfer knowledge learned from labeled source domains to related but unlabeled target domains without any prior label set relationship. The key to UniDA lies in rejecting target domain -specific "unknown" samples to achieve domain alignment on shared classes. In this paper, we propose the Maximum Open -set Entropy Optimization via uncertainty measure to adaptively reject "unknown" samples. Specifically, MOEO sets a transition region within the most easily confused class in the open -set space. We optimize the maximum open -set entropy of simple samples outside the transition region to further improve their confidence and separate difficult samples within the transition region by aggregating similar neighbors. Accordingly, a secure boundary is formed between shared samples and "unknown" samples, promoting domain alignment within shared classes. Experiments on four benchmarks show that MOEO outperforms the previous state-of-the-art significantly.
C1 [Ai, Weiwei; Yang, Zhao; Chen, Zhiyong] Guangzhou Univ, Sch Elect & Commun Engn, Guangzhou, Peoples R China.
   [Ai, Weiwei; Yang, Zhao; Chen, Zhiyong] Guangzhou Univ, Huangpu Res & Grad Sch, Guangzhou, Peoples R China.
   [Hu, Xiao] Guangzhou Univ, Sch Mech & Elect Engn, Guangzhou, Peoples R China.
C3 Guangzhou University; Guangzhou University; Guangzhou University
RP Yang, Z (corresponding author), Guangzhou Univ, Sch Elect & Commun Engn, Guangzhou, Peoples R China.
EM yangzhao@gzhu.edu.cn
FU program for excellent new-recruited doctors [YB201712]; National Natural
   Science Foundation of China [62076075]
FX This research was supported by Guangzhou University's training program
   for excellent new-recruited doctors (No. YB201712) and the National
   Natural Science Foundation of China under grant 62076075.r program for
   excellent new-recruited doctors (No. YB201712) and the National Natural
   Science Foundation of China under grant 62076075.
CR Bo Fu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P567, DOI 10.1007/978-3-030-58555-6_34
   Bucci Silvia, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P422, DOI 10.1007/978-3-030-58517-4_25
   Busto PP, 2017, IEEE I CONF COMP VIS, P754, DOI 10.1109/ICCV.2017.88
   Cao ZJ, 2018, LECT NOTES COMPUT SC, V11212, P139, DOI 10.1007/978-3-030-01237-3_9
   Chen L, 2022, AAAI CONF ARTIF INTE, P6258
   Chen QC, 2018, PROC CVPR IEEE, P7976, DOI 10.1109/CVPR.2018.00832
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Hassanin M, 2022, J VIS COMMUN IMAGE R, V83, DOI 10.1016/j.jvcir.2022.103448
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hu LQ, 2018, PROC CVPR IEEE, P1498, DOI 10.1109/CVPR.2018.00162
   Kundu JN, 2020, PROC CVPR IEEE, P4543, DOI 10.1109/CVPR42600.2020.00460
   Li GR, 2021, PROC CVPR IEEE, P9752, DOI 10.1109/CVPR46437.2021.00963
   Liu H, 2019, PROC CVPR IEEE, P2922, DOI 10.1109/CVPR.2019.00304
   Liu ZW, 2015, IEEE I CONF COMP VIS, P1377, DOI 10.1109/ICCV.2015.162
   Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2019, IEEE T PATTERN ANAL, V41, P3071, DOI 10.1109/TPAMI.2018.2868685
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Lv Q., 2023, IEEE Transactions on Circuits and Systems for Video Technology
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Paszke A., 2017, NEURIPS
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   Peng XC, 2018, IEEE COMPUT SOC CONF, P2102, DOI 10.1109/CVPRW.2018.00271
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito K., 2020, Advances in Neural Information Processing Systems, V33, P16282
   Saito K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8980, DOI 10.1109/ICCV48922.2021.00887
   Saito K, 2019, IEEE I CONF COMP VIS, P8049, DOI 10.1109/ICCV.2019.00814
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Saito K, 2018, LECT NOTES COMPUT SC, V11209, P156, DOI 10.1007/978-3-030-01228-1_10
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Tzeng E, 2014, Arxiv, DOI [arXiv:1412.3474, DOI 10.48550/ARXIV.1412.3474]
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Volpi R, 2018, PROC CVPR IEEE, P5495, DOI 10.1109/CVPR.2018.00576
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   You KC, 2019, PROC CVPR IEEE, P2715, DOI 10.1109/CVPR.2019.00283
   Zhang HJ, 2019, IEEE INT CON MULTI, P1258, DOI 10.1109/ICME.2019.00219
NR 42
TC 0
Z9 0
U1 4
U2 4
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104169
DI 10.1016/j.jvcir.2024.104169
EA MAY 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TS0P9
UT WOS:001243134200001
DA 2024-08-05
ER

PT J
AU Zhang, XF
   Yang, S
   Zhang, QY
   Yuan, FN
AF Zhang, Xiangfen
   Yang, Shuo
   Zhang, Qingyi
   Yuan, Feiniu
TI Multi-scale recurrent attention gated fusion network for single image
   dehazing
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Deep Learning; Single Image Dehazing; Multi-scale; Recurrent Attention
   Gated Fusion
ID WEATHER
AB The purpose of single image dehazing is to eliminate the bad influence of haze on images, so as to maintain more scene information of images. In recent years, the convolutional neural networks (CNN) have made significant contributions to single image dehazing. However, the visual quality of dehazed images still needs to be further improved. In view of the problems of single -scale shallow image feature extraction and the insufficient use of intermediate layer features in existing dehazing networks, we propose an end -to -end Multi -scale Recurrent Attention Gated Fusion Network (MRAGFN) to address the image dehazing task. We cascade three Dual Attention Fusion (DAF) modules to progressively form three haze -relevant features map, meanwhile, we adopt downsampling operation on the input to produce global feature map, which are used to weight the three feature maps to compensate for the missing of single -scale feature information. We present Feature Enhancement Module (FEM) to enhance the feature representation ability of these weighted feature maps. We design Recurrent Attention Gated Fusion (RAGF) module by adding attention mechanism and gating mechanism to gradually obtain more refined features based on these weighted features while eliminating redundant features. Experimental results on different hazy images demonstrate that the proposed dehazing network can restore the hazefree images and perform better than the state-of-the-art dehazing networks in terms of the objective indicators (such as PSNR, SSIM) and the subjective visual quality.
C1 [Zhang, Xiangfen; Yang, Shuo; Zhang, Qingyi; Yuan, Feiniu] Shanghai Normal Univ, Coll Informat Mech & Elect Engn, Shanghai 200000, Peoples R China.
C3 Shanghai Normal University
RP Zhang, XF (corresponding author), Shanghai Normal Univ, Coll Informat Mech & Elect Engn, Shanghai 200000, Peoples R China.
EM xiangfen@shnu.edu.cn
FU National Natural Science Foundation of China [61862029, 62171285];
   General Research Fund of Shanghai Normal University [KF2021100,
   Sk201220]
FX This work was partially supported by the National Natural Science
   Foundation of China (Project numbers: 61862029 and 62171285) , General
   Research Fund of Shanghai Normal University (Project numbers: KF2021100
   and Sk201220) .
CR Ancuti CO, 2020, IEEE COMPUT SOC CONF, P2029, DOI 10.1109/CVPRW50498.2020.00253
   Ancuti CO, 2019, IEEE IMAGE PROC, P1014, DOI [10.1109/ICIP.2019.8803046, 10.1109/icip.2019.8803046]
   Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti C, 2018, IEEE COMPUT SOC CONF, P1004, DOI 10.1109/CVPRW.2018.00134
   Ancuti C, 2018, LECT NOTES COMPUT SC, V11182, P620, DOI 10.1007/978-3-030-01449-0_52
   Berman D, 2020, IEEE T PATTERN ANAL, V42, P720, DOI 10.1109/TPAMI.2018.2882478
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Borkar K, 2020, NEUROCOMPUTING, V400, P294, DOI 10.1016/j.neucom.2020.03.027
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen ZH, 2020, VISUAL COMPUT, V36, P2189, DOI 10.1007/s00371-020-01929-y
   Cho K., 2014, P 2014 C EMP METH NA, DOI 10.3115/v1/D14-1179
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2009, PROC CVPR IEEE, P1956, DOI [10.1109/CVPRW.2009.5206515, 10.1109/CVPR.2009.5206515]
   Hou GJ, 2020, J VIS COMMUN IMAGE R, V66, DOI 10.1016/j.jvcir.2019.102732
   Huang ZL, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103694
   Jing YC, 2023, PROC CVPR IEEE, P24345, DOI 10.1109/CVPR52729.2023.02332
   Jing YC, 2021, PROC CVPR IEEE, P15704, DOI 10.1109/CVPR46437.2021.01545
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li YA, 2016, NEUROCOMPUTING, V182, P221, DOI 10.1016/j.neucom.2015.12.032
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P5432, DOI 10.1109/TIP.2015.2482903
   Liang Y., 2022, P 31 INT JOINT C ART, P1137
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S., 2022, NeurIPS, V35, P1100
   Liu SH, 2023, PROC CVPR IEEE, P3759, DOI 10.1109/CVPR52729.2023.00366
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Mo YZ, 2022, J VIS COMMUN IMAGE R, V82, DOI 10.1016/j.jvcir.2021.103431
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Nishino K, 2012, INT J COMPUT VISION, V98, P263, DOI 10.1007/s11263-011-0508-1
   Parihar AS, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103722
   Peled SR, 2019, EUR SIGNAL PR CONF, DOI 10.23919/eusipco.2019.8902547
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Shi XJ, 2015, ADV NEUR IN, V28
   Su Z, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103706
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Yang X., 2022, Adv. Neural Inf. Process. Syst., V35, P25739
   Yang XY, 2023, IEEE I CONF COMP VIS, P18892, DOI 10.1109/ICCV51070.2023.01736
   Yang XY, 2023, PROC CVPR IEEE, P22552, DOI 10.1109/CVPR52729.2023.02160
   Yang XY, 2022, LECT NOTES COMPUT SC, V13694, P73, DOI 10.1007/978-3-031-19830-4_5
   Yang XT, 2018, AAAI CONF ARTIF INTE, P7485
   Yi QS, 2021, IET IMAGE PROCESS, V15, P143, DOI 10.1049/ipr2.12013
   Yu F., 2016, ICLR, P1
   Yuan FN, 2020, COMPUT VIS IMAGE UND, V194, DOI 10.1016/j.cviu.2020.102933
   Zhang J, 2020, IEEE T IMAGE PROCESS, V29, P72, DOI 10.1109/TIP.2019.2922837
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhu HY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1234
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 53
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104171
DI 10.1016/j.jvcir.2024.104171
EA MAY 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TJ9C5
UT WOS:001241003400001
DA 2024-08-05
ER

PT J
AU Zhang, JJ
   Yang, YL
   Liu, YY
AF Zhang, Jingjian
   Yang, Youlong
   Liu, Yuanyuan
TI Locality sensitive hashing scheme based on online-learning
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Locally Sensitive Hashing; Approximate nearest-neighbour search;
   Weighted majority algorithm; Online-learning framework
ID APPROXIMATE NEAREST-NEIGHBOR
AB Locally Sensitive Hashing (LSH) algorithms are classical algorithms commonly used on the c -Approximate Nearest Neighbor (c -ANN) search problem. When using Euclidean distance to measure sample similarity and solve the c -ANN problem, the traditional approach is to utilize the Exact Euclidean Locality Sensitive Hashing (E2LSH) algorithm based on the p -stable distribution. However, the uncertainty of the p -stable distribution causes the hash buckets constructed by the E2LSH algorithm to vary in queries. Therefore, this paper proposes the OLLSH algorithm based on the Weighted Majority algorithm in the Online -Learning framework, which selects the hash buckets with more stable query accuracy by weighted voting on the hash buckets generated by the E2LSH algorithm. Then, we conduct simulation experiments on synthetic dataset and four real data sets and conclude that the proposed OLLSH algorithm improves the accuracy compared to the original algorithm with the same memory usage.
C1 [Zhang, Jingjian; Yang, Youlong; Liu, Yuanyuan] Xidian Univ, Sch Math & Stat, Xian 710126, Peoples R China.
C3 Xidian University
RP Yang, YL (corresponding author), Xidian Univ, Sch Math & Stat, Xian 710126, Peoples R China.
EM ylyang@mail.xidian.edu.cn
RI xiao, ming/KHT-1774-2024
FU National Natural Science Foundation of China [61573266]; Natural Science
   Basic Research Program of Shaanxi [2021JM-133]
FX This work is financially supported by the National Natural Science
   Foundation of China (No. 61573266) and the Natural Science Basic
   Research Program of Shaanxi (No. 2021JM-133).
CR Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   Arya S, 1998, J ACM, V45, P891, DOI 10.1145/293347.293348
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900
   Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231
   Charikar M.S., 2002, P THIRY 4 ANN ACM S, P380, DOI DOI 10.1145/509907.509965
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Cheng DD, 2024, WIREL NETW, V30, P4195, DOI 10.1007/s11276-022-02927-9
   Datar M., 2004, PROC 20 ANN S COMPUT, P253
   Gan J., 2012, P 2012 ACM SIGMOD IN, P541
   Haiquan Qiu, 2019, Journal of Physics: Conference Series, V1302, DOI 10.1088/1742-6596/1302/2/022019
   Huang Q, 2015, PROC VLDB ENDOW, V9, P1
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Joly Alexis., 2008, PROCEEDING 16 ACM IN, P209
   KIRA K, 1992, AAAI-92 PROCEEDINGS : TENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, P129
   Kulis B, 2009, IEEE T PATTERN ANAL, V31, P2143, DOI 10.1109/TPAMI.2009.151
   Kumar C, 2022, INFORM SCIENCES, V601, P207, DOI 10.1016/j.ins.2022.04.028
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LITTLESTONE N, 1994, INFORM COMPUT, V108, P212, DOI 10.1006/inco.1994.1009
   Liu J., 2023, IEEE Trans. Artif. Intell., P1
   Liu J.-X., 2023, IEEE/ACM Trans. Comput. Biol. Bioinform.
   Liu S., 2023, Appl. Soft Comput.
   Liu WQ, 2021, VLDB J, V30, P215, DOI 10.1007/s00778-020-00635-4
   Qiu HQ, 2021, INFORM SCIENCES, V558, P21, DOI 10.1016/j.ins.2021.01.017
   Simoudis E., 1996, KDD 96 P 2 INT C KNO, P226
   Tao YF, 2010, ACM T DATABASE SYST, V35, DOI 10.1145/1806907.1806912
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Wang CP, 2022, APPL INTELL, V52, P9739, DOI 10.1007/s10489-021-02940-z
   Wang XL, 2023, APPL SOFT COMPUT, V149, DOI 10.1016/j.asoc.2023.110952
   Xia JH, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108177
   Zhang BS, 2022, KNOWL-BASED SYST, V252, DOI 10.1016/j.knosys.2022.109400
   Zhang JN, 2023, EXPERT SYST APPL, V234, DOI 10.1016/j.eswa.2023.121039
   Zheng BL, 2020, PROC VLDB ENDOW, V13, P643, DOI 10.14778/3377369.3377374
NR 34
TC 0
Z9 0
U1 8
U2 8
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104036
DI 10.1016/j.jvcir.2023.104036
EA JAN 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GO2U7
UT WOS:001153554500001
DA 2024-08-05
ER

PT J
AU Hou, YQ
   Yang, B
AF Hou, Yongqi
   Yang, Bo
TI Semantic attention guided low-light image enhancement with multi-scale
   perception
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Low-light image enhancement; Semantic guidance; Attention mechanism;
   Deep learning
ID CONTRAST ENHANCEMENT; REPRESENTATION
AB Low-light environments often lead to complex degradation of captured images. However, most deep learning- based image enhancement methods for low-light conditions only learn a single-channel mapping relationship between the input image in low-light conditions and the desired image in normal light without considering semantic priors. This may cause the network to deviate from the original color of the region. In addition, deep network architectures are not suitable for low-light image recovery due to low pixel values. To address these issues, we propose a novel network called SAGNet. It consists of two branches:the main branch extracts global enhancement features at the level of the original image, and the other branch introduces semantic information through region-based feature learning and learns local enhancement features for semantic regions with multilevel perception to maintain color consistency. The extracted features are merged with the global enhancement features for semantic consistency and visualization. We also propose an unsupervised loss function to improve the network's adaptability to general scenes and reduce the effect of sparse datasets. Extensive experiments and ablation studies show that SAGNet maintains color accuracy better in all cases and keeps natural luminance consistency across the semantic range.
C1 [Hou, Yongqi; Yang, Bo] Inner Mongolia Univ, Sch Comp Sci, Hohhot 010021, Peoples R China.
C3 Inner Mongolia University
RP Hou, YQ (corresponding author), Inner Mongolia Univ, Sch Comp Sci, Hohhot 010021, Peoples R China.
EM csyb@imu.edu.cn
FU National Natural Science Foundation of China [61363051, 12074204,
   12374258]
FX <B>Acknowledgment</B> This work was financially supported by the
   National Natural Science Foundation of China (Grant nos. 61363051,
   12074204, 12374258) .
CR Cai BL, 2017, IEEE I CONF COMP VIS, P4020, DOI 10.1109/ICCV.2017.431
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Fan MH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2317, DOI 10.1145/3394171.3413757
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Gupta B, 2017, COMPUT ELECTR ENG, V62, P360, DOI 10.1016/j.compeleceng.2017.01.010
   Hao SJ, 2019, MULTIMED TOOLS APPL, V78, P3817, DOI 10.1007/s11042-018-6257-1
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059
   Lee C, 2012, IEEE T IMAGE PROCESS, V21, P80, DOI 10.1109/TIP.2011.2159387
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Ma L, 2022, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR52688.2022.00555
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shen L, 2017, Arxiv, DOI arXiv:1711.02488
   Singh K, 2015, OPTIK, V126, P2619, DOI 10.1016/j.ijleo.2015.06.060
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang LW, 2020, IEEE T IMAGE PROCESS, V29, P7984, DOI 10.1109/TIP.2020.3008396
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang WJ, 2018, IEEE INT CONF AUTOMA, P751, DOI 10.1109/FG.2018.00118
   Wang YN, 2023, J VIS COMMUN IMAGE R, V92, DOI 10.1016/j.jvcir.2023.103795
   Wei Chen, 2018, P BMVC
   Wu WH, 2022, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR52688.2022.00581
   Yan QS, 2019, PATTERN RECOGN LETT, V127, P66, DOI 10.1016/j.patrec.2018.10.008
   Yang WH, 2020, PROC CVPR IEEE, P3060, DOI 10.1109/CVPR42600.2020.00313
   Zhang Q, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P582, DOI 10.1145/3240508.3240595
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
NR 32
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD AUG
PY 2024
VL 103
AR 104242
DI 10.1016/j.jvcir.2024.104242
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A0I9N
UT WOS:001279474200001
DA 2024-08-05
ER

PT J
AU Zhang, Y
   Gong, XL
   Yu, HL
   Wu, ZJ
   Yu, L
AF Zhang, Yuan
   Gong, Xiaoli
   Yu, Hualong
   Wu, Zijun
   Yu, Lu
TI Distance-based feature repack algorithm for video coding for machines
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Collaborative intelligence; Deep feature compression; H.265/HEVC; Action
   recognition; Video coding for machines
ID FEATURE COMPRESSION
AB Nowadays, the use of video data for machine (VCM) tasks has become increasingly prevalent, with deep learning and computer vision requiring large volumes of video data for object detection, object tracking, and other tasks. However, the features required for machine tasks are different from those used by humans, and a new approach is needed to encode and compress video data for machine consumption. Video coding for machines has received considerable attention, with many approaches focusing on compressing features rather than the video itself. However, a key challenge in this process is repacking the features in an efficient and effective manner. This paper proposes a distance -based patch tiling and intra-block quilting method to repack feature sequences in a manner that is better suited for existing video codecs, based on statistical analysis of feature characteristics in the channel dimension. Experimental results demonstrate that our method achieves an 65.54% BD -rate gain compared to benchmark methods. This research has significant implications for improving the efficiency of video coding for machine applications, and future work could explore the use of feature dimensionality reduction and combination of neural network (NN) codec to optimize the repacking of features for compression.
C1 [Zhang, Yuan; Yu, Hualong; Yu, Lu] Zhejiang Univ, 38 Zheda Rd, Hangzhou, Zhejiang, Peoples R China.
   [Zhang, Yuan; Wu, Zijun] China Telecom, 1835 South Pudong Rd, Shanghai, Peoples R China.
   [Gong, Xiaoli] Xi An Jiao Tong Univ, Xian, Shaanxi, Peoples R China.
C3 Zhejiang University; China Telecom Corp. Ltd.; Xi'an Jiaotong University
RP Yu, L (corresponding author), Zhejiang Univ, 38 Zheda Rd, Hangzhou, Zhejiang, Peoples R China.
EM yuanzhang@zju.edu.cn; gcc9331@stu.xjtu.edu.cn; yuanzhang@zju.edu.cn;
   wuzj12@chinatelecom.cn; yuanzhang@zju.edu.cn
FU Key Research and Develop-ment Program of Zhejiang Province, China
   [2021C01119]
FX <B>Acknowledgments</B> This research work was supported by the Key
   Research and Develop-ment Program of Zhejiang Province, China, under
   Grant 2021C01119.
CR Bajic IV, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P8493, DOI 10.1109/ICASSP39728.2021.9413943
   Bross B, 2021, IEEE T CIRC SYST VID, V31, P3736, DOI 10.1109/TCSVT.2021.3101953
   Chen Z, 2020, IEEE IMAGE PROC, P3094, DOI 10.1109/ICIP40778.2020.9190843
   Chen Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2414, DOI 10.1145/3343031.3350849
   Chen Z, 2020, IEEE T IMAGE PROCESS, V29, P2230, DOI 10.1109/TIP.2019.2941660
   Choi H, 2018, IEEE IMAGE PROC, P3743, DOI 10.1109/ICIP.2018.8451100
   Cohen RA, 2021, IEEE OPEN J CIRCUITS, V2, P350, DOI 10.1109/OJCAS.2021.3072884
   Cosar S, 2014, J VIS COMMUN IMAGE R, V25, P864, DOI 10.1016/j.jvcir.2014.02.004
   Eshratifar AE, 2019, Arxiv, DOI arXiv:1902.00147
   Eshratifar AE, 2019, I SYMPOS LOW POWER E, DOI 10.1109/islped.2019.8824955
   Ethan S., 2022, ISO/IEC JTC 1/SC 29/WG 2: Use cases and requirements for video coding for machine, N00190
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Gao W., 2021, arXiv
   Hu YZ, 2020, IEEE I C VI COM I PR, P475, DOI 10.1109/vcip49819.2020.9301807
   Huang Z., 2021, 2021 IEEE INT C MULT, P1, DOI [10.1109/ICME51207.2021.9428417, DOI 10.1109/ICME51207.2021.9428417]
   Jiang M, 2020, J VIS COMMUN IMAGE R, V71, DOI 10.1016/j.jvcir.2020.102846
   Jiawang L., 2023, 2023 IEEE INT C VIS, P1, DOI [10.1109/VCIP59821.2023.10402702, DOI 10.1109/VCIP59821.2023.10402702]
   Kim D., 2021, ISO/IEC JTC 1/SC 29/WG2
   Korbar B, 2019, IEEE I CONF COMP VIS, P6241, DOI 10.1109/ICCV.2019.00633
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu CH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13608, DOI 10.1109/ICCV48922.2021.01337
   Liu D, 2012, J VIS COMMUN IMAGE R, V23, P100, DOI 10.1016/j.jvcir.2011.09.001
   Liu S., 2021, ISO/IEC JTC 1/SC 29/WG2: Common test conditions and evaluation methodology for video coding for machines, N00107
   Marcellin M. W., 2000, Proceedings DCC 2000. Data Compression Conference, P523, DOI 10.1109/DCC.2000.838192
   MMAction2 Contributors, 2020, Openmmlab's next generation video understanding toolbox and benchmark
   Park J, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21030896
   Singh S, 2020, IEEE IMAGE PROC, P3349, DOI 10.1109/ICIP40778.2020.9190860
   Soomro K., 2012, CoRR, V2
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Suzuki S, 2022, IEEE T CIRC SYST VID, V32, P3934, DOI 10.1109/TCSVT.2021.3107716
   Ulhaq M, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P8498, DOI 10.1109/ICASSP39728.2021.9413603
   Wang SR, 2022, IEEE T MULTIMEDIA, V24, P3169, DOI 10.1109/TMM.2021.3094300
   Wang WQ, 2023, J VIS COMMUN IMAGE R, V95, DOI 10.1016/j.jvcir.2023.103859
   Wang Z., 2023, ADJ P 2022 ACM INT J, P438, DOI [10.1145/3544793.3560405, DOI 10.1145/3544793.3560405]
   Wang ZX, 2021, J VIS COMMUN IMAGE R, V79, DOI 10.1016/j.jvcir.2021.103226
   Xu XZ, 2016, IEEE J EM SEL TOP C, V6, P409, DOI 10.1109/JETCAS.2016.2597645
   Yan N, 2020, IEEE IMAGE PROC, P3114, DOI 10.1109/ICIP40778.2020.9191184
   Yang WH, 2021, Arxiv, DOI arXiv:2110.09241
   Zhang H, 2015, J VIS COMMUN IMAGE R, V28, P28, DOI 10.1016/j.jvcir.2015.01.004
   Zhang Z., 2021, 2021 IEEE INT C MULT, P1, DOI [10.1109/ICME51207.2021.9428258, DOI 10.1109/ICME51207.2021.9428258]
NR 40
TC 0
Z9 0
U1 2
U2 2
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD JUN
PY 2024
VL 102
AR 104150
DI 10.1016/j.jvcir.2024.104150
EA JUN 2024
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WV2C6
UT WOS:001257572900001
DA 2024-08-05
ER

PT J
AU Jiang, YH
   Luo, SH
   Guo, LJ
   Zhang, R
AF Jiang, Yinhui
   Luo, Sihui
   Guo, Lijun
   Zhang, Rong
TI MCT-VHD: Multi-modal contrastive transformer for video highlight
   detection
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Multi-modal; Video highlight detection; Transformer; Contrastive
   learning
ID NEURAL-NETWORKS; RANKING
AB Autonomous highlight detection aims to identify the most captivating moments in a video, which is crucial for enhancing the efficiency of video editing and browsing on social media platforms. However, current efforts primarily focus on visual elements and often overlook other modalities, such as text information that could provide valuable semantic signals. To overcome this limitation, we propose a Multi -modal Contrastive Transformer for Video Highlight Detection (MCT-VHD). This transformer -based network mainly utilizes video and audio modalities, along with auxiliary text features (if exist) for video highlight detection. Specifically, We enhance the temporal connections within the video by integrating a convolution -based local enhancement module into the transformer blocks. Furthermore, we explore three multi -modal fusion strategies to improve highlight inference performance and employ a contrastive objective to facilitate interactions between different modalities. Comprehensive experiments conducted on three benchmark datasets validate the effectiveness of MCT-VHD, and our ablation studies provide valuable insights into its essential components.
C1 [Jiang, Yinhui; Luo, Sihui; Guo, Lijun; Zhang, Rong] Ningbo Univ, Fac Informat Sci & Engn, Ningbo, Zhejiang, Peoples R China.
C3 Ningbo University
RP Luo, SH (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo, Zhejiang, Peoples R China.
EM luosihui@nbu.edu.cn
OI GUO, Lijun/0000-0002-6133-9564
FU Natural Science Foundation Project of Zhejiang Province, China
   [LQ22F020020]; Open Project Program of the State Key Lab of CADCG
   [A2216]; Zhejiang University, China
FX <B>Acknowledgments</B> This work is supported by the Natural Science
   Foundation Project of Zhejiang Province, China (LQ22F020020) , and the
   Open Project Program of the State Key Lab of CAD&CG (Grant No. A2216) ,
   Zhejiang University, China.
CR Afouras T, 2022, IEEE T PATTERN ANAL, V44, P8717, DOI 10.1109/TPAMI.2018.2889052
   Badamdorj T, 2022, PROC CVPR IEEE, P14022, DOI 10.1109/CVPR52688.2022.01365
   Badamdorj T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8107, DOI 10.1109/ICCV48922.2021.00802
   BECKER S, 1992, NATURE, V355, P161, DOI 10.1038/355161a0
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen J., 2024, Neural Netw.
   Chen T, 2020, Arxiv, DOI [arXiv:2002.05709, DOI 10.48550/ARXIV.2002.05709]
   Fa-Ting Hong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P345, DOI 10.1007/978-3-030-58601-0_21
   Fan YF, 2023, PROC CVPR IEEE, P20029, DOI 10.1109/CVPR52729.2023.01918
   Gabeur Valentin, 2020, COMPUTER VISION ECCV, DOI [10.48550/arXiv.2007.10639, DOI 10.48550/ARXIV.2007.10639]
   Gao RH, 2020, PROC CVPR IEEE, P10454, DOI 10.1109/CVPR42600.2020.01047
   Gao TY, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P6894
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Gygli M, 2016, PROC CVPR IEEE, P1001, DOI 10.1109/CVPR.2016.114
   Hadsell R., 2006, INT C COMPUTER VISIO, P1735, DOI DOI 10.1109/CVPR.2006.100
   Han M., 2023, ICCV, P13414
   Hu KW, 2023, INT J COMPUT VISION, V131, P1741, DOI 10.1007/s11263-023-01776-z
   Hu RH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1419, DOI 10.1109/ICCV48922.2021.00147
   Jiao YF, 2018, IEEE T MULTIMEDIA, V20, P2693, DOI 10.1109/TMM.2018.2815998
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kong QQ, 2020, IEEE-ACM T AUDIO SPE, V28, P2880, DOI 10.1109/TASLP.2020.3030497
   Lei Jie, 2021, ADV NEURAL INFORM PR, V34, P3
   Lin RH, 2022, Arxiv, DOI arXiv:2210.14556
   Liu M, 2019, IEEE T IMAGE PROCESS, V28, P1235, DOI 10.1109/TIP.2018.2875363
   Liu Y, 2022, PROC CVPR IEEE, P3032, DOI 10.1109/CVPR52688.2022.00305
   Ma Shuang, 2021, INT C LEARN REPR
   Mahasseni B, 2017, PROC CVPR IEEE, P2982, DOI 10.1109/CVPR.2017.318
   Mai SJ, 2023, INFORM FUSION, V100, DOI 10.1016/j.inffus.2023.101920
   Nagrani Arsha, 2021, Neural Information Processing Systems
   Radford A, 2021, PR MACH LEARN RES, V139
   Song YL, 2015, PROC CVPR IEEE, P5179, DOI 10.1109/CVPR.2015.7299154
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun M, 2014, LECT NOTES COMPUT SC, V8689, P787, DOI 10.1007/978-3-319-10590-1_51
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Wang JJ, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P599, DOI 10.1109/ICME.2004.1394263
   Wang L., 2020, ECCV, P300
   Wang W., 2020, P IEEE CVF C COMP VI, P12695, DOI DOI 10.1109/CVPR42600.2020.01271
   Wei FY, 2022, PROC CVPR IEEE, P3063, DOI 10.1109/CVPR52688.2022.00308
   Wei Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P52, DOI 10.1007/978-3-030-58523-5_4
   Xiong B, 2019, PROC CVPR IEEE, P1258, DOI 10.1109/CVPR.2019.00135
   Xu C., 2006, PROC 14 ANN ACM INT, P221, DOI DOI 10.1145/1180639.1180699
   Xu CS, 2008, IEEE T MULTIMEDIA, V10, P421, DOI 10.1109/TMM.2008.917346
   Xu MH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7950, DOI 10.1109/ICCV48922.2021.00787
   Yao T, 2016, PROC CVPR IEEE, P982, DOI 10.1109/CVPR.2016.112
   Ye QH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7930, DOI 10.1109/ICCV48922.2021.00785
   Yu J, 2020, IEEE T CIRC SYST VID, V30, P4467, DOI 10.1109/TCSVT.2019.2947482
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Zhang M, 2019, ADV NEUR IN, V32
   Zhang M, 2020, IEEE T IMAGE PROCESS, V29, P6276, DOI 10.1109/TIP.2020.2990341
   Zhang YY, 2020, AAAI CONF ARTIF INTE, V34, P12902
   Zhou JX, 2023, IEEE T PATTERN ANAL, V45, P7239, DOI 10.1109/TPAMI.2022.3223688
   Zhu GY, 2007, IEEE T MULTIMEDIA, V9, P1167, DOI 10.1109/TMM.2007.902847
   Zolfaghari M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1430, DOI 10.1109/ICCV48922.2021.00148
NR 53
TC 0
Z9 0
U1 3
U2 3
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104162
DI 10.1016/j.jvcir.2024.104162
EA MAY 2024
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TC7A6
UT WOS:001239114100001
DA 2024-08-05
ER

PT J
AU Mukati, MU
   Zhang, X
   Wu, XL
   Forchhammer, S
AF Mukati, M. Umair
   Zhang, Xi
   Wu, Xiaolin
   Forchhammer, Soren
TI Low-complexity l∞-compression of light field images with a
   deep-decompression stage
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Light field decorrelation; High fidelity compression; Near-lossless
   encoding; Deep soft decompression; Compression artifacts removal
ID ENTROPY; NETWORK
AB To enrich the functionalities of traditional cameras, light field cameras record both the intensity and direction of light rays, so that images can be rendered with user -defined camera parameters via computations. The added capability and flexibility are gained at the cost of gathering typically more than 100 perspectives of the same scene, resulting in large data volume. To cope with this issue, several light field compression schemes have been introduced. However, their ways of exploiting correlations of multidimensional light field data are complex and are hence not suited for cost-effective light field cameras. On the other hand, existing simpler compression schemes do not offer good compression performance. In this work, we propose a novel eW-constrained light -field image compression system that has a very low -complexity DPCM encoder and a CNN -based deep decoder enhancement. Targeting high-fidelity soft -decoding (restoration), the CNN decoding capitalizes on the eW-constraint, i.e. the maximum absolute error bound, and light field properties to remove the compression artifacts. Two different architectures for CNN decoder enhancement are proposed, one is based on 2D CNNs and optimized for fast inference, and another is based on 3D CNNs to maximize e2 restoration quality. The proposed networks achieve superior performance both in inference speed and restoration quality in comparison to state-of-the-art light field network architectures. In conjunction with eW-EPIC, the proposed architecture, while satisfying a well-defined eW constraint, outperforms existing state-of-the-art e2 -based light field compression methods.
C1 [Mukati, M. Umair; Forchhammer, Soren] Tech Univ Denmark, Dept Elect & Photon Engn, DK-2800 Lyngby, Denmark.
   [Zhang, Xi; Wu, Xiaolin] McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8G 4K1, Canada.
C3 Technical University of Denmark; McMaster University
RP Mukati, MU (corresponding author), Tech Univ Denmark, Dept Elect & Photon Engn, DK-2800 Lyngby, Denmark.
EM mummu@dtu.dk
RI Zhang, Xi/AIF-1562-2022
OI Zhang, Xi/0000-0002-1993-6031; Forchhammer, Soren/0000-0002-6698-8870
FU EU's H2020 ITN Programme through the MSCA [765911]; Innovation Fund
   Denmark (IFD) [2053-00015B]; NSFC, China [62301313]
FX This work was jointly supported by the EU's H2020 ITN Programme through
   the MSCA Grant Agreement (RealVision) under Grant 765911, and the
   Innovation Fund Denmark (IFD) under Case No. 2053-00015B. Partial
   funding was provided by NSFC, China grant no. 62301313.
CR Ahmad W, 2017, IEEE IMAGE PROC, P4557, DOI 10.1109/ICIP.2017.8297145
   Astola P, 2018, EUR W VIS INF PROCES
   Bai YC, 2021, PROC CVPR IEEE, P11941, DOI 10.1109/CVPR46437.2021.01177
   CHEN KS, 1994, IEEE T MED IMAGING, V13, P538, DOI 10.1109/42.310885
   Chuah S, 2013, IEEE T IMAGE PROCESS, V22, P5271, DOI 10.1109/TIP.2013.2286324
   Conti C, 2020, IEEE ACCESS, V8, P49244, DOI 10.1109/ACCESS.2020.2977767
   de Carvalho MB, 2018, IEEE IMAGE PROC, P435, DOI 10.1109/ICIP.2018.8451684
   Alves GD, 2020, IEEE ACCESS, V8, P170807, DOI 10.1109/ACCESS.2020.3024844
   Fan HZ, 2017, IEEE IMAGE PROC, P1167, DOI 10.1109/ICIP.2017.8296465
   Google, 2014, WebP image format
   Gul MSK, 2022, IEEE ACCESS, V10, P7895, DOI 10.1109/ACCESS.2022.3142949
   Helin P, 2017, IEEE J-STSP, V11, P1146, DOI 10.1109/JSTSP.2017.2737967
   Hu ZX, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3152242
   Jin J, 2020, AAAI CONF ARTIF INTE, V34, P11141
   Ke LG, 1998, IEEE T IMAGE PROCESS, V7, P225, DOI 10.1109/83.660999
   Li YM, 2014, IEEE IMAGE PROC, P4612
   Mukati M.U., 2020, IEEE INT CONF MULTI, V2020, P1, DOI [DOI 10.1109/icmew46912.2020.9105980, 10.1109/ICMEW46912.2020.9105980]
   Mukati MU, 2021, IEEE OPEN J CIRCUITS, V2, P325, DOI 10.1109/OJCAS.2021.3073252
   Mukati MU, 2021, IEEE ACCESS, V9, P1124, DOI 10.1109/ACCESS.2020.3047073
   Ng R, 2005, PhD diss
   Pan ZQ, 2022, IEEE T CIRC SYST VID, V32, P6347, DOI 10.1109/TCSVT.2022.3161103
   PhiCong H, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9111798
   Rerabek M., 2016, JPEG Pleno Database: EPFL Light-field data set
   Rerabek M., 2016, 8 INT C QUALITY MULT, V08
   Rerabek M., 2015, ISOIEC JTC 1SC 29WG1
   Rerabek M., 2016, Call Propos. Eval. Proced.
   Santos JM, 2018, J VIS COMMUN IMAGE R, V54, P21, DOI 10.1016/j.jvcir.2018.03.003
   Schelkens P, 2019, PROC SPIE, V11137, DOI 10.1117/12.2532049
   Schiopu I, 2017, 2017 3DTV CONFERENCE: THE TRUE VISION - CAPTURE, TRANSMISSION AND DISPLAY OF 3D VIDEO (3DTV-CON)
   Srinivasan PP, 2017, IEEE I CONF COMP VIS, P2262, DOI 10.1109/ICCV.2017.246
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Vieira A, 2015, INT CONF IMAG PROC, P494, DOI 10.1109/IPTA.2015.7367195
   Weinberger MJ, 2000, IEEE T IMAGE PROCESS, V9, P1309, DOI 10.1109/83.855427
   Wu GC, 2017, PROC CVPR IEEE, P1638, DOI 10.1109/CVPR.2017.178
   Wu GC, 2019, IEEE T PATTERN ANAL, V41, P1681, DOI 10.1109/TPAMI.2018.2845393
   Wu XL, 2000, IEEE T IMAGE PROCESS, V9, P536, DOI 10.1109/83.841931
   Yeung HWF, 2019, IEEE T IMAGE PROCESS, V28, P2319, DOI 10.1109/TIP.2018.2885236
   Yeung HWF, 2018, LECT NOTES COMPUT SC, V11210, P138, DOI 10.1007/978-3-030-01231-1_9
   Yun Li, 2016, 2016 IEEE International Conference on Multimedia & Expo: Workshops (ICMEW), DOI 10.1109/ICMEW.2016.7574673
   Zhang SF, 2019, PROC CVPR IEEE, P919, DOI 10.1109/CVPR.2019.00101
   Zhang S, 2021, IEEE T IMAGE PROCESS, V30, P5956, DOI 10.1109/TIP.2021.3079805
   Zhang X, 2021, IEEE T IMAGE PROCESS, V30, P963, DOI 10.1109/TIP.2020.3040074
   Zhou JT, 2012, IEEE T IMAGE PROCESS, V21, P4797, DOI 10.1109/TIP.2012.2202672
   Zhou SY, 2021, VIS COMPUT IND BIOME, V4, DOI 10.1186/s42492-021-00096-8
NR 45
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104072
DI 10.1016/j.jvcir.2024.104072
EA FEB 2024
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KY3S8
UT WOS:001183495300001
DA 2024-08-05
ER

PT J
AU Wang, Z
   Zhang, XH
AF Wang, Zhen
   Zhang, Xiaohuan
TI Contextual recovery network for low-light image enhancement with texture
   recovery
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Low light enhancement; Texture optimization; Squeeze-excitation;
   Information aggregation
ID HISTOGRAM EQUALIZATION; FRAMEWORK; QUALITY
AB Low -light image enhancement has been a challenging topic in computer vision. In order to recover colors and detailed textures in images, several data -driven enhancement based methods have been developed and obtained encouraging results. However, the network generalization ability is not satisfactory due to the uncertainty of the collected data. In order to address this issue, we propose a network with texture enhancement (TEENet), which synergizes the image brightness recovery and recovers the texture information lost during the process. To recover image brightness, we propose a low -light image enhancement network with a squeeze -excitation operation and construct a texture -optimized network that combines contextual aggregation information to recover texture loss during the enhancement process. We conducted network performance tests using different data and ablation experiments to verify the performance of different components.
C1 [Wang, Zhen; Zhang, Xiaohuan] Huizhou Univ, Coll Comp Sci & Technol, Huizhou 516007, Peoples R China.
C3 Huizhou University
RP Zhang, XH (corresponding author), Huizhou Univ, Coll Comp Sci & Technol, Huizhou 516007, Peoples R China.
EM zhangxiaohuan@hzu.edu.cn
FU Key construction disciplines in Guangdong province "Light-weight federal
   learning paradigm and its application"; Professorial and Doctoral
   Scientific Research Foundation of Huizhou University [2022ZDJS058];
   Guangdong Overseas Famous Teacher Project: Research on artificial
   intelligence and new generation intelligent manufacturing technology;
   The 2021 practice teaching base project of integration of science,
   industry and education in Guangdong Undergraduate Universities: Huizhou
   information technology science, industry and education integration
   practice teaching base;  [2018JB007]
FX This research was funded by the fund which aims to improve scientific
   research capability of key construction disciplines in Guangdong
   province "Light-weight federal learning paradigm and its application"
   (No:2022ZDJS058) , the Professorial and Doctoral Scientific Research
   Foundation of Huizhou University (No. 2018JB007) , Guangdong Overseas
   Famous Teacher Project: Research on artificial intelligence and new
   generation intelligent manufacturing technology, 2021 practice teaching
   base project of integration of science, industry and education in
   Guangdong Undergraduate Universities: Huizhou information technology
   science, industry and education integration practice teaching base.
CR Agostinelli F., 2013, NIPS, P1493
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dhara SK, 2022, IEEE T CIRC SYST VID, V32, P3438, DOI 10.1109/TCSVT.2021.3113559
   Feng LT, 2016, J VIS COMMUN IMAGE R, V38, P451, DOI 10.1016/j.jvcir.2016.03.019
   Fu HY, 2023, PROC CVPR IEEE, P18125, DOI 10.1109/CVPR52729.2023.01738
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Fu ZQ, 2023, PROC CVPR IEEE, P22252, DOI 10.1109/CVPR52729.2023.02131
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2023, INT J COMPUT VISION, V131, P48, DOI 10.1007/s11263-022-01667-9
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hai J, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103712
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Huang SC, 2013, IEEE T IMAGE PROCESS, V22, P1032, DOI 10.1109/TIP.2012.2226047
   Ibrahim H, 2007, IEEE T CONSUM ELECTR, V53, P1752, DOI 10.1109/TCE.2007.4429280
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Lee C, 2012, IEEE IMAGE PROC, P965, DOI 10.1109/ICIP.2012.6467022
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P4225, DOI 10.1109/TPAMI.2021.3063604
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Lin J, 2022, J VIS COMMUN IMAGE R, V89, DOI 10.1016/j.jvcir.2022.103684
   Liu HD, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3261929
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lv F., 2018, Proc. BMVC, V220, P4
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Ma L, 2022, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR52688.2022.00555
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Wu WH, 2022, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR52688.2022.00581
   Wu YH, 2023, PROC CVPR IEEE, P1662, DOI 10.1109/CVPR52729.2023.00166
   Xu XG, 2023, PROC CVPR IEEE, P9893, DOI 10.1109/CVPR52729.2023.00954
   Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P3461, DOI 10.1109/TIP.2021.3062184
   Ying ZQ, 2017, Arxiv, DOI arXiv:1711.00591
   Yun SH, 2010, IEEE T CONSUM ELECTR, V56, P2763, DOI 10.1109/TCE.2010.5681167
   Zhang DH, 2023, EXPERT SYST APPL, V231, DOI 10.1016/j.eswa.2023.120842
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zhang ZJ, 2018, 2018 IEEE/ACM 26TH INTERNATIONAL SYMPOSIUM ON QUALITY OF SERVICE (IWQOS), DOI 10.1109/IWQoS.2018.8624183
   Zheng S, 2022, IEEE WINT CONF APPL, P581, DOI 10.1109/WACVW54805.2022.00064
   Zhi N., 2018, J LIAONING TU, V37, P191
   Zhou JC, 2023, IEEE J OCEANIC ENG, V48, P474, DOI 10.1109/JOE.2022.3223733
   Zhou JC, 2023, INT J COMPUT VISION, DOI 10.1007/s11263-023-01853-3
   Zhou JC, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3293912
   Zhou JC, 2023, ENG APPL ARTIF INTEL, V121, DOI 10.1016/j.engappai.2023.105952
NR 48
TC 1
Z9 1
U1 14
U2 14
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104050
DI 10.1016/j.jvcir.2024.104050
EA JAN 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JH6C2
UT WOS:001172302500001
DA 2024-08-05
ER

PT J
AU Lin, XC
   Li, N
AF Lin, Xiaocan
   Li, Nan
TI Self-supervised learning monocular depth estimation from internet photos
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Monocular depth estimation; Self-supervised learning;
   Internet-data-driven
AB Monocular depth estimation (MDE) is a fundamental problem in computer vision. Recently, self-supervised learning (SSL) approaches have attracted significant attention due to the ability to train an MDE network without ground-truth depth data. However, the performance of most existing SSL-MDE methods is yet limited by the available real training dataset, which are either binocular stereo pairs or monocular video sequences. In this paper, we propose a simple but effective generalization of SSL framework such that collections of multiple view Internet photos, a virtually unlimited source of real data, are enabled to train an MDE network. Combining the depth consistency and the mask that alleviates the interference such as moving objects, the network benefits from the real correspondences in adjacent views, thus achieving the improvement. Experiments show that the generalization of Monodepth2 via the proposed method not only leads to superior performance than itself and some data-driven MDE methods, but also stably boosts the performance of multiple state-of-the-art SSL-MDE methods. Besides, experiments on SeasonDepth, a dataset with various environmental conditions, show the good generalization capability of our proposed method.
C1 [Lin, Xiaocan; Li, Nan] Shenzhen Univ, Sch Math Sci, Shenzhen 518060, Peoples R China.
   [Li, Nan] Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
C3 Shenzhen University
RP Li, N (corresponding author), Shenzhen Univ, Sch Math Sci, Shenzhen 518060, Peoples R China.
EM nan.li@szu.edu.cn
OI Lin, Xiaocan/0000-0002-9632-1663; Li, Nan/0000-0001-5973-2829
CR Atapour-Abarghouei A, 2018, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2018.00296
   Bhat SF, 2021, PROC CVPR IEEE, P4008, DOI 10.1109/CVPR46437.2021.00400
   Bian JW, 2019, ADV NEUR IN, V32
   Casser V, 2019, AAAI CONF ARTIF INTE, P8001
   Chang Shu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P572, DOI 10.1007/978-3-030-58529-7_34
   Chen HX, 2021, IEEE SIGNAL PROC LET, V28, P334, DOI 10.1109/LSP.2021.3050712
   Chen XY, 2023, IEEE WINT CONF APPL, P5765, DOI 10.1109/WACV56688.2023.00573
   Eigen D, 2014, ADV NEUR IN, V27
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Gordon A, 2019, IEEE I CONF COMP VIS, P8976, DOI 10.1109/ICCV.2019.00907
   Guo XY, 2018, LECT NOTES COMPUT SC, V11215, P506, DOI 10.1007/978-3-030-01252-6_30
   Han WC, 2022, LECT NOTES COMPUT SC, V13698, P586, DOI 10.1007/978-3-031-19839-7_34
   He M, 2022, LECT NOTES COMPUT SC, V13687, P565, DOI 10.1007/978-3-031-19812-0_33
   Hu HJ, 2021, Arxiv, DOI arXiv:2011.04408
   Ji P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12767, DOI 10.1109/ICCV48922.2021.01255
   Jiang HZ, 2018, LECT NOTES COMPUT SC, V11215, P20, DOI 10.1007/978-3-030-01252-6_2
   Li L, 2021, IEEE T IND INFORM, V17, P3920, DOI 10.1109/TII.2020.3011067
   Li YM, 2023, PROC CVPR IEEE, P9087, DOI 10.1109/CVPR52729.2023.00877
   Li YH, 2023, AAAI CONF ARTIF INTE, P1477
   Li ZQ, 2018, PROC CVPR IEEE, P2041, DOI 10.1109/CVPR.2018.00218
   Liu Z., 2023, IEEE T CIRCUITS SYST
   Ocal M, 2020, Arxiv, DOI arXiv:2004.06267
   Peng R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15540, DOI 10.1109/ICCV48922.2021.01527
   Ranftl R, 2022, IEEE T PATTERN ANAL, V44, P1623, DOI 10.1109/TPAMI.2020.3019967
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Shao S., 2023, P IEEECVF INT C COMP, P7931
   Shao SW, 2023, Arxiv, DOI arXiv:2309.14137
   Silberman N., 2012, Computer Vision-ECCV 2012-12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part V, volume 7576 of Lecture Notes in Computer Science, DOI [DOI 10.1007/978-3-642-33715-4_54, 10.1007/978-3-642-33715-454, 10.1007/978-3-642-33715-4_54]
   Tonioni A, 2020, IEEE T PATTERN ANAL, V42, P2396, DOI 10.1109/TPAMI.2019.2940948
   Tosi F, 2019, PROC CVPR IEEE, P9791, DOI 10.1109/CVPR.2019.01003
   Wang CY, 2019, INT CONF 3D VISION, P348, DOI 10.1109/3DV.2019.00046
   Wang Q, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2023.103753
   Wong A, 2019, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR.2019.00579
   Xu XF, 2021, IEEE SIGNAL PROC LET, V28, P678, DOI 10.1109/LSP.2021.3067498
   Yan JX, 2021, INT CONF 3D VISION, P464, DOI 10.1109/3DV53792.2021.00056
   Yang B, 2019, IEEE T PATTERN ANAL, V41, P2820, DOI 10.1109/TPAMI.2018.2868195
   Yang ZH, 2018, PROC CVPR IEEE, P225, DOI 10.1109/CVPR.2018.00031
   Zhang N, 2023, PROC CVPR IEEE, P18537, DOI 10.1109/CVPR52729.2023.01778
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
NR 44
TC 0
Z9 0
U1 10
U2 10
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104063
DI 10.1016/j.jvcir.2024.104063
EA JAN 2024
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KE9K6
UT WOS:001178397000001
DA 2024-08-05
ER

PT J
AU Wang, YX
   Hu, J
   Zhang, RG
   Wang, LF
   Zhang, R
   Liu, XJ
AF Wang, Yuxi
   Hu, Jing
   Zhang, Rongguo
   Wang, Lifang
   Zhang, Rui
   Liu, Xiaojun
TI Heterogeneity constrained color ellipsoid prior image dehazing algorithm
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Heterogeneity constraints; Color ellipsoid prior; Quadtree search;
   Differential filtering
ID CONTRAST ENHANCEMENT; SINGLE
AB To address the issue of inaccurate transmission estimation in areas with sudden depth changes in hazy images, the image dehazing algorithm with Heterogeneity Constrained Color Ellipsoid Prior (HC-CEP) is proposed. Firstly, a local threshold optimization quadtree search method is designed, in order to solve the problem of inaccurate global atmospheric light estimation and avoid the overall darkening of the image caused by white objects.Then, different regions are selected by a local block heterogeneity window to construct diversity prior vectors to estimate the initial transmission. Finally, by constructing neighborhood heterogeneity weighted constraints on each pixel of the initial transmission and improving the high-order differential filtering of the Scharr operator, the optimization of the transmission is achieved, eliminating halo artifacts in the depth mutation area of the image. Both qualitative and quantitative experimental results show that the proposed algorithm comprehensively considers the heterogeneity of local pixel characteristics, and the dehazing image obtained has better color fidelity and edge detail preservation ability, which can effectively eliminate halo artifacts on the edges of hazy images.The proposed method has better dehazing performance and has a 8% improvement in terms of the FADE metric compared to state-of-the-art dehazing methods.
C1 [Wang, Yuxi; Hu, Jing; Zhang, Rongguo; Wang, Lifang; Zhang, Rui] Taiyuan Univ Sci & Technol, Sch Comp Sci & Technol, Taiyuan 030024, Peoples R China.
   [Liu, Xiaojun] Hefei Univ Technol, Sch Mech Engn, Hefei 230009, Peoples R China.
C3 Taiyuan University of Science & Technology; Hefei University of
   Technology
RP Wang, YX (corresponding author), Taiyuan Univ Sci & Technol, Sch Comp Sci & Technol, Taiyuan 030024, Peoples R China.
FU National Natural Science Foundation of China [52375178]; Natural Science
   Foundation of Shanxi Province [202203021211206]
FX The authors declare that they have no known competing financial
   interests or personal relationships that could have appeared to
   influence the work reported in this paper. We thank the financial
   support of the National Natural Science Foundation of China (52375178) ,
   the Natural Science Foundation of Shanxi Province (202203021211206,
CR Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti C, 2018, LECT NOTES COMPUT SC, V11182, P620, DOI 10.1007/978-3-030-01449-0_52
   Bai HR, 2022, IEEE T IMAGE PROCESS, V31, P1217, DOI 10.1109/TIP.2022.3140609
   Berman D, 2020, IEEE T PATTERN ANAL, V42, P720, DOI 10.1109/TPAMI.2018.2882478
   Bui TM, 2018, IEEE T IMAGE PROCESS, V27, P999, DOI 10.1109/TIP.2017.2771158
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chen ZY, 2021, PROC CVPR IEEE, P7176, DOI 10.1109/CVPR46437.2021.00710
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Dong Y, 2023, IEEE T CYBERNETICS, V53, P7238, DOI 10.1109/TCYB.2022.3221544
   Dwivedi P, 2023, IMAGE VISION COMPUT, V136, DOI 10.1016/j.imavis.2023.104747
   Galdran A, 2018, PROC CVPR IEEE, P8212, DOI 10.1109/CVPR.2018.00857
   Gibson KB, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-37
   Han XX, 2016, IEEE IMAGE PROC, P2236, DOI 10.1109/ICIP.2016.7532756
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hu HM, 2020, IEEE T MULTIMEDIA, V22, P1485, DOI 10.1109/TMM.2019.2944260
   Huang SC, 2021, IEEE T PATTERN ANAL, V43, P2623, DOI 10.1109/TPAMI.2020.2977911
   Ju MY, 2021, IEEE T IMAGE PROCESS, V30, P2180, DOI 10.1109/TIP.2021.3050643
   Ju MY, 2019, IEEE T CIRC SYST VID, V29, P2349, DOI 10.1109/TCSVT.2018.2869594
   Ju MY, 2020, IEEE T IMAGE PROCESS, V29, P3104, DOI 10.1109/TIP.2019.2957852
   Ju MY, 2017, NEUROCOMPUTING, V260, P180, DOI 10.1016/j.neucom.2017.04.034
   Jun W., 2013, Appl. Math. Sci., V7, P3913
   Juneja A, 2023, J VIS COMMUN IMAGE R, V94, DOI 10.1016/j.jvcir.2023.103855
   Kaplan NH, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103720
   Kim JH, 2013, J VIS COMMUN IMAGE R, V24, P410, DOI 10.1016/j.jvcir.2013.02.004
   Kim SE, 2020, IEEE T IMAGE PROCESS, V29, P1985, DOI 10.1109/TIP.2019.2948279
   Kim T, 2008, IEEE T CONSUM ELECTR, V54, P1803, DOI 10.1109/TCE.2008.4711238
   Kumar BP, 2024, J VIS COMMUN IMAGE R, V100, DOI 10.1016/j.jvcir.2024.104099
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li JF, 2023, IEEE T MULTIMEDIA, V25, P3587, DOI 10.1109/TMM.2022.3163554
   Ling PY, 2023, IEEE T IMAGE PROCESS, V32, P3238, DOI 10.1109/TIP.2023.3279980
   Liu XN, 2022, IEEE T MULTIMEDIA, V24, P3934, DOI 10.1109/TMM.2021.3110483
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Parihar AS, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103722
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672
   Shin J, 2022, IEEE T MULTIMEDIA, V24, P245, DOI 10.1109/TMM.2021.3050053
   Tu ZZ, 2022, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR52688.2022.00568
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie XX, 2023, J VIS COMMUN IMAGE R, V97, DOI 10.1016/j.jvcir.2023.103984
   Xiong YY, 2021, PROC CVPR IEEE, P3824, DOI 10.1109/CVPR46437.2021.00382
   Yang MM, 2018, IEEE T MULTIMEDIA, V20, P3008, DOI 10.1109/TMM.2018.2820327
   Yang Y, 2022, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR52688.2022.00208
   Zheng ZR, 2021, PROC CVPR IEEE, P16180, DOI 10.1109/CVPR46437.2021.01592
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 50
TC 0
Z9 0
U1 2
U2 2
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104177
DI 10.1016/j.jvcir.2024.104177
EA MAY 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TN1N2
UT WOS:001241852000001
DA 2024-08-05
ER

PT J
AU Wang, ZK
   Zou, YN
   Chen, HY
   Liu, PX
   Chen, JY
AF Wang, Zekun
   Zou, Yanni
   Chen, Hongyu
   Liu, Peter X.
   Chen, Junyu
TI Multi-scale features and attention guided for brain tumor segmentation
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Brain tumor segmentation; Convolutional neural network; Magnetic
   resonance images; Dilated convolution
AB Brain tumor segmentation supplies a credible reference for clinical treatment and pathological research and facilitates practitioners to diagnose more accurately. However, since the randomness and complexity of tumor shape and location, automatic brain tumor segmentation remains an extremely challenging assignment. In this study, we build an end-to-end convolutional neural network with a U-shaped structure to implement the segmentation of three lesion regions. We propose a multi-scale context block and an attention guidance block to focus on the spatial information at different scales and the interdependence between feature channels to enhance network representations and boost the learning capability of the model. Specifically, the multiscale context block draws rich feature information through 3D dilated convolution. The attention guidance block reduces the impact of learned redundant features and eliminates the interference of irrelevant regions in the overall global information. Our recommended approach is evaluated on the brain tumor segmentation 2020 validation data. The Dice scores of the enhancing tumor (ET), whole tumor (WT), and tumor core (TC) are 78.19%, 90.10%, and 83.98%, respectively. In addition, the practice is also carried out in 2019 online validation data, and the Dice scores of ET, WT, and TC are 77.31%, 89.64%, and 82.55%, respectively. Experimental results reveal that the recommended approach gains favorable performance in comparison with representative brain tumor segmentation approaches. Our present study would accurately and efficaciously segment the three brain lesion regions and has clinical practice value.
C1 [Wang, Zekun; Zou, Yanni; Liu, Peter X.; Chen, Junyu] Nanchang Univ, Sch Math & Comp Sci, Nanchang 330031, Jiangxi, Peoples R China.
   [Chen, Hongyu] Guangdong Polytech Normal Univ, Sch Literature & Commun, Guangzhou 510006, Peoples R China.
   [Liu, Peter X.] Carleton Univ, Dept Syst & Comp Engn, Ottawa, ON K1S 5B6, Canada.
C3 Nanchang University; Guangdong Polytechnic Normal University; Carleton
   University
RP Zou, YN (corresponding author), Nanchang Univ, Sch Math & Comp Sci, Nanchang 330031, Jiangxi, Peoples R China.
EM zouyanni@163.com
FU National Natural Science Founda-tion of China [62262040]; Innovation
   Fund Designated for Graduate Students of Jiangxi Province, China
   [YC2020-S091]
FX This work was supported by the National Natural Science Founda-tion of
   China under Grant 62262040. This work was also supported by the
   Innovation Fund Designated for Graduate Students of Jiangxi Province,
   China under grant YC2020-S091.
CR Ahmad P, 2020, Arxiv, DOI arXiv:2010.13082
   Ashtari P, 2021, LECT NOTES COMPUT SC, V12658, P470, DOI 10.1007/978-3-030-72084-1_42
   Chang PD, 2016, LECT NOTES COMPUT SC, V10154, P108, DOI 10.1007/978-3-319-55524-9_11
   Chen ML, 2020, LECT NOTES COMPUT SC, V11992, P142, DOI 10.1007/978-3-030-46640-4_14
   Cheng GH, 2020, MED PHYS, V47, P4885, DOI 10.1002/mp.14392
   Cicek Ozgun, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P424, DOI 10.1007/978-3-319-46723-8_49
   Guo XQ, 2020, LECT NOTES COMPUT SC, V11993, P285, DOI 10.1007/978-3-030-46643-5_28
   Hamghalam M, 2020, LECT NOTES COMPUT SC, V11992, P153, DOI 10.1007/978-3-030-46640-4_15
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jain A, 2005, PATTERN RECOGN, V38, P2270, DOI 10.1016/j.patcog.2005.01.012
   Jia ZH, 2023, COMPUT BIOL MED, V157, DOI 10.1016/j.compbiomed.2023.106751
   Jun W, 2021, LECT NOTES COMPUT SC, V12658, P183, DOI 10.1007/978-3-030-72084-1_17
   Khosravanian A, 2021, COMPUT METH PROG BIO, V198, DOI 10.1016/j.cmpb.2020.105809
   Kingma D. P., 2014, arXiv
   Li XY, 2020, LECT NOTES COMPUT SC, V11992, P163, DOI 10.1007/978-3-030-46640-4_16
   Liu CY, 2020, Arxiv, DOI arXiv:2010.15647
   Liu Y, 2022, IEEE SIGNAL PROC LET, V29, P1799, DOI 10.1109/LSP.2022.3198594
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Menze BH, 2015, IEEE T MED IMAGING, V34, P1993, DOI 10.1109/TMI.2014.2377694
   Oktay O, 2018, Arxiv, DOI [arXiv:1804.03999, DOI 10.48550/ARXIV.1804.03999]
   Pei LM, 2021, LECT NOTES COMPUT SC, V12658, P367, DOI 10.1007/978-3-030-72084-1_33
   Pereira S, 2016, IEEE T MED IMAGING, V35, P1240, DOI 10.1109/TMI.2016.2538465
   Ranjbarzadeh R, 2023, COMPUT BIOL MED, V152, DOI 10.1016/j.compbiomed.2022.106405
   Rehman MU, 2021, DIAGNOSTICS, V11, DOI 10.3390/diagnostics11020169
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russo C, 2020, Arxiv, DOI arXiv:2008.07090
   Soltaninejad M, 2021, LECT NOTES COMPUT SC, V12659, P30, DOI 10.1007/978-3-030-72087-2_3
   Sun JD, 2021, NEUROCOMPUTING, V423, P34, DOI 10.1016/j.neucom.2020.10.031
   Sundaresan V., 2021, arXiv
   Tustison NJ, 2010, IEEE T MED IMAGING, V29, P1310, DOI 10.1109/TMI.2010.2046908
   Valanarasu JMJ, 2022, IEEE T MED IMAGING, V41, P965, DOI 10.1109/TMI.2021.3130469
   Wang FF, 2020, LECT NOTES COMPUT SC, V11992, P131, DOI 10.1007/978-3-030-46640-4_13
   Wang JJ, 2021, COMPUT METH PROG BIO, V208, DOI 10.1016/j.cmpb.2021.106208
   Wang YL, 2021, COMPUT METH PROG BIO, V207, DOI 10.1016/j.cmpb.2021.106154
   Wang Y, 2022, COMPUT BIOL MED, V149, DOI 10.1016/j.compbiomed.2022.106039
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1007/s11263-019-01198-w, 10.1109/CSTIC.2018.8369274]
   Xu YW, 2019, LECT NOTES COMPUT SC, V11384, P222, DOI 10.1007/978-3-030-11726-9_20
   Zhang JP, 2021, IEEE T MED IMAGING, V40, P661, DOI 10.1109/TMI.2020.3034995
   Zhang M., 2019, 2019 INT C MED IM PH, P1
   Zhou TX, 2020, COMPUT MED IMAG GRAP, V86, DOI 10.1016/j.compmedimag.2020.101811
   Zhu ZQ, 2023, INFORM FUSION, V91, P376, DOI 10.1016/j.inffus.2022.10.022
   Zikic Darko, 2014, Proc. MICCAI-BRATS, V36, P36
NR 42
TC 1
Z9 1
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104141
DI 10.1016/j.jvcir.2024.104141
EA APR 2024
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RV8A9
UT WOS:001230516400001
DA 2024-08-05
ER

PT J
AU Qian, ZH
   Wu, W
   Wu, XT
   Chen, XD
AF Qian, Zeheng
   Wu, Wen
   Wu, Xian-Tao
   Chen, Xiao-Diao
TI Omni-supervised shadow detection with vision foundation model
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Image segmentation; Shadow detection; Segment anything model;
   Omni-supervised; Bipartite matching
AB Weakly supervised shadow detection has been spreading as cheaper labeling costs in recent years. However, these works tend to use a single annotation manner, i.e., scribble, it may not be the best choice for each shadow scene and thereby cannot exploit more valuable cues for better training. To tackle this issue, this work explores a more flexible labeling strategy, containing scribble, box and point. During the training, rather than only using cross-entropy (CE) loss on the available points, we formulate the supervisions between given annotations with the model's predictions as a bipartite matching problem, and then design efficient weak learning losses from other valuable perspectives of shadow location, quantity and size. Moreover, considering the challenge of poor boundary localization in one -stage weakly supervised image segmentation, we propose a CNN-assisted tuning strategy to inherit boundary knowledge from the vision foundation model ( i.e. , SAM), and learn task-specific knowledge from our hybrid annotations by introducing a light-weight CNN branch along with the SAM backbone, namely ShadowSAM. This approach has lower requirements on GPU memory and extracts more local information for identifying tiny shadow regions. Extensive experiments demonstrate that our proposed methods can achieve comparable performance with state -of -the -art (SOTA) fully supervised shadow detectors. Our code will be available at https://github.com/wuwen1994/omni-shadow.
C1 [Qian, Zeheng] China Agr Univ, Coll Informat & Elect Engn, Beijing 100083, Peoples R China.
   [Wu, Wen; Wu, Xian-Tao; Chen, Xiao-Diao] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China.
   [Chen, Xiao-Diao] Haihe Lab ITAI, Tianjin 300480, Peoples R China.
C3 China Agricultural University; Hangzhou Dianzi University
RP Wu, W (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China.
EM wuwen.hdu.cs@gmail.com
RI WU, Wen/HKW-7234-2023
OI WU, Wen/0000-0003-0919-3948
FU National Natural Science Foundation of China [61972120]; Haihe Lab of
   ITAI Project, China [XCHK20210102]; Joint Funds of the Zhejiang
   Provincial Natural Science Foundation of China [LQZSZ24E050001]
FX The work was supported by the National Natural Science Foundation of
   China (61972120) , the Haihe Lab of ITAI Project, China (XCHK20210102) ,
   and the Joint Funds of the Zhejiang Provincial Natural Science
   Foundation of China (LQZSZ24E050001) .
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achiam J., 2023, Gpt-4 technical report
   Alayrac J.-B., 2022, NeurIPS, V35, P23716, DOI DOI 10.48550/ARXIV.2204.14198
   Carion N., 2020, EUR C COMP VIS, P213
   Chen HJ, 2021, SEMINAR LEARNING CLI, P6900, DOI [10.1109/ICCV48922.2021.00684, DOI 10.1109/ICCV48922.2021.00684]
   Chen KY, 2023, PROC CVPR IEEE, P23518, DOI 10.1109/CVPR52729.2023.02252
   Chen XD, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3332257
   Chen ZH, 2020, PROC CVPR IEEE, P5610, DOI 10.1109/CVPR42600.2020.00565
   Guo RQ, 2011, PROC CVPR IEEE
   Hu XW, 2021, IEEE T IMAGE PROCESS, V30, P1925, DOI 10.1109/TIP.2021.3049331
   Hu XW, 2018, PROC CVPR IEEE, P7454, DOI 10.1109/CVPR.2018.00778
   Huang X, 2011, IEEE I CONF COMP VIS, P898, DOI 10.1109/ICCV.2011.6126331
   Jia C, 2021, PR MACH LEARN RES, V139
   Jia ML, 2022, LECT NOTES COMPUT SC, V13693, P709, DOI 10.1007/978-3-031-19827-4_41
   Jie LP, 2023, Arxiv, DOI arXiv:2305.11513
   Junejo IN, 2008, LECT NOTES COMPUT SC, V5302, P318, DOI 10.1007/978-3-540-88682-2_25
   Karsch K, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024191
   Ke T.-W., 2020, INT C LEARN REPR
   Khan SH, 2014, PROC CVPR IEEE, P1939, DOI 10.1109/CVPR.2014.249
   Kirillov A, 2023, IEEE I CONF COMP VIS, P3992, DOI 10.1109/ICCV51070.2023.00371
   Lalonde JF, 2012, INT J COMPUT VISION, V98, P123, DOI 10.1007/s11263-011-0501-8
   Le HE, 2018, LECT NOTES COMPUT SC, V11206, P680, DOI 10.1007/978-3-030-01216-8_41
   Lê HA, 2023, IEEE INT CONF COMP V, P1003, DOI 10.1109/ICCVW60793.2023.00107
   Lian D., 2022, Advances in Neural Information Processing Systems, V35, P109
   Ma J, 2024, Arxiv, DOI [arXiv:2304.12306, DOI 10.48550/ARXIV.2304.12306]
   Marin D, 2019, PROC CVPR IEEE, P10179, DOI 10.1109/CVPR.2019.01043
   Okabe T, 2009, IEEE I CONF COMP VIS, P1693
   Ouyang Long, 2022, Advances in Neural Information Processing Systems, V35, P27730, DOI 10.1177/01454455830072006
   Radford A, 2021, PR MACH LEARN RES, V139
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Sohn K, 2023, PROC CVPR IEEE, P19840, DOI 10.1109/CVPR52729.2023.01900
   Tang M, 2018, LECT NOTES COMPUT SC, V11220, P524, DOI 10.1007/978-3-030-01270-0_31
   Tang M, 2018, PROC CVPR IEEE, P1818, DOI 10.1109/CVPR.2018.00195
   Vincente TFY, 2016, LECT NOTES COMPUT SC, V9910, P816, DOI 10.1007/978-3-319-46466-4_49
   Nguyen V, 2017, IEEE I CONF COMP VIS, P4520, DOI 10.1109/ICCV.2017.483
   Wang B, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3663
   Wang HX, 2024, Arxiv, DOI arXiv:2303.09992
   Wang JF, 2018, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2018.00192
   Wu W, 2023, IEEE T CIRC SYST VID, V33, P6213, DOI 10.1109/TCSVT.2023.3263903
   Wu W, 2023, KNOWL-BASED SYST, V273, DOI 10.1016/j.knosys.2023.110614
   Wu W, 2022, J VIS COMMUN IMAGE R, V82, DOI 10.1016/j.jvcir.2021.103397
   Wu W, 2022, COMPUT VIS IMAGE UND, V216, DOI 10.1016/j.cviu.2021.103341
   Wu XT, 2022, COMPUT GRAPH-UK, V104, P152, DOI 10.1016/j.cag.2022.04.003
   Yang H, 2023, IEEE I CONF COMP VIS, P12641, DOI 10.1109/ICCV51070.2023.01165
   Zhang J., 2020, PROC IEEECVF C COMP, P12546
   Zhang S., 2023, INT C AC SPEECH SIGN, P1
   Zhao BR, 2022, PROC CVPR IEEE, P11943, DOI 10.1109/CVPR52688.2022.01165
   Zheng QL, 2019, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2019.00531
   Zhou K, 2022, J VIS COMMUN IMAGE R, V88, DOI 10.1016/j.jvcir.2022.103596
   Zhu JJ, 2010, PROC CVPR IEEE, P223, DOI 10.1109/CVPR.2010.5540209
   Zhu L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4682, DOI 10.1109/ICCV48922.2021.00466
   Zhu L, 2018, LECT NOTES COMPUT SC, V11210, P122, DOI 10.1007/978-3-030-01231-1_8
   Zhu YR, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6717, DOI 10.1145/3503161.3547904
NR 55
TC 0
Z9 0
U1 2
U2 2
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104146
DI 10.1016/j.jvcir.2024.104146
EA APR 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SM7Q7
UT WOS:001234939900001
DA 2024-08-05
ER

PT J
AU Liu, FQ
   Liu, T
   Yan, B
   Pan, JS
   Yang, HM
AF Liu, Feng-Qing
   Liu, Tao
   Yan, Bin
   Pan, Jeng-Shyang
   Yang, Hong-Mei
TI Non-iterative reversible information hiding in the secret sharing domain
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Secret image sharing; Secret sharing domain; Reversible information
   hiding
ID STEGANOGRAPHY
AB Long running time due to multiple iterations is the main drawback of existing information hiding in the sharing domain (IHSD) algorithm. To address this problem, we propose non -iterative reversible information hiding in the secret sharing domain (NIIHSD). We calculate the coefficients of the polynomial that make the least significant bit of the shadow pixel equal to '0' or '1' and store them in two forms (A0 and A1). According to the information to be embedded, coefficients are randomly selected in A0 and A1 to generate qualified shares by once calculation. The comparison with IHSD demonstrates our scheme improves the running speed of the algorithm by a factor of 2 to 4 for different (k, n) thresholds and secret image with different sizes. The coefficient forms only need to be created once to be reused. Our scheme has a greater advantage when sharing a large number of images.
C1 [Liu, Feng-Qing; Yan, Bin] Shandong Univ Sci & Technol, Coll Elect & Informat Engn, Qingdao 266590, Peoples R China.
   [Liu, Tao; Pan, Jeng-Shyang; Yang, Hong-Mei] Shandong Univ Sci & Technol, Coll Comp Sci & Engn, Qingdao 266590, Peoples R China.
C3 Shandong University of Science & Technology; Shandong University of
   Science & Technology
RP Yan, B (corresponding author), Shandong Univ Sci & Technol, Coll Elect & Informat Engn, Qingdao 266590, Peoples R China.
EM fengqingliu0519@163.com; fengqingliu0519@163.com;
   yanbinhit@sdust.edu.cn; jengshyangpan@gmail.com; jengshyangpan@gmail.com
FU Shandong Provincial Natural Sci-ence Foundation [ZR2021MF050]; MOE
   (Ministry of Edu-cation in China) Project of Humanities and Social
   Sciences [18YJAZH110]; National Statistics Science Project [2021 LY082]
FX This work was funded by the Shandong Provincial Natural Sci-ence
   Foundation (No. ZR2021MF050) , the MOE (Ministry of Edu-cation in China)
   Project of Humanities and Social Sciences (Project No. 18YJAZH110) , and
   the National Statistics Science Project (2021 LY082) .
CR Blakley G. R., 1979, Proceedings of the National Computer Conference, IEEE Computer Society, P313, DOI [DOI 10.1109/MARK.1979.8817296, 10.1109/MARK.1979.8817296, DOI 10.1109/AFIPS.1979.98]
   Chen D, 2019, IEEE ACCESS, V7, P107104, DOI 10.1109/ACCESS.2019.2929090
   Chen YC, 2019, IEEE T INF FOREN SEC, V14, P3332, DOI 10.1109/TIFS.2019.2914557
   Dragoi IC, 2014, IEEE T IMAGE PROCESS, V23, P1779, DOI 10.1109/TIP.2014.2307482
   Gao XB, 2011, IEEE T CIRC SYST VID, V21, P1061, DOI 10.1109/TCSVT.2011.2130410
   Kang HY, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12199629
   Li XL, 2013, IEEE T IMAGE PROCESS, V22, P2181, DOI 10.1109/TIP.2013.2246179
   Lin CC, 2004, J SYST SOFTWARE, V73, P405, DOI 10.1016/S0164-1212(03)00239-5
   Liu ZN, 2023, J VIS COMMUN IMAGE R, V93, DOI 10.1016/j.jvcir.2023.103827
   Meghrajani YK, 2019, J INF SECUR APPL, V47, P267, DOI 10.1016/j.jisa.2019.05.010
   Pan JS, 2021, ENG APPL ARTIF INTEL, V97, DOI 10.1016/j.engappai.2020.104049
   Pang LJ, 2005, APPL MATH COMPUT, V167, P840, DOI 10.1016/j.amc.2004.06.120
   Qin C, 2022, IEEE T CIRC SYST VID, V32, P1928, DOI 10.1109/TCSVT.2021.3091319
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   Thien CC, 2002, COMPUT GRAPH-UK, V26, P766
   Thodi DM, 2007, IEEE T IMAGE PROCESS, V16, P721, DOI 10.1109/TIP.2006.891046
   Thodi DM, 2004, IEEE IMAGE PROC, P1549
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Wang RZ, 2006, PATTERN RECOGN LETT, V27, P551, DOI 10.1016/j.patrec.2005.09.021
   Weng CY, 2023, ENTROPY-SWITZ, V25, DOI 10.3390/e25020209
   Wu XT, 2020, J INF SECUR APPL, V51, DOI 10.1016/j.jisa.2020.102452
   Wu XT, 2019, SIGNAL PROCESS-IMAGE, V78, P437, DOI 10.1016/j.image.2019.08.007
   Wu XT, 2019, J VIS COMMUN IMAGE R, V61, P74, DOI 10.1016/j.jvcir.2019.03.020
   Wu XT, 2018, SIGNAL PROCESS, V143, P269, DOI 10.1016/j.sigpro.2017.09.017
   Xing FY, 2022, J VIS COMMUN IMAGE R, V86, DOI 10.1016/j.jvcir.2022.103520
   Xiong LZ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3512797
   Yan XH, 2018, IEEE ACCESS, V6, P45246, DOI 10.1109/ACCESS.2018.2865421
   Yang CN, 2015, SIGNAL PROCESS-IMAGE, V31, P1, DOI 10.1016/j.image.2014.11.003
   Yu Y., 2020, Secur. Commun. Netw., V2020, P1
   Yuan JT, 2022, INFORM SCIENCES, V592, P36, DOI 10.1016/j.ins.2022.01.053
NR 30
TC 0
Z9 0
U1 9
U2 9
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104096
DI 10.1016/j.jvcir.2024.104096
EA MAR 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NS8T0
UT WOS:001202541500001
DA 2024-08-05
ER

PT J
AU Chen, ZY
   Cui, GM
   Li, ZH
   Zhao, JF
AF Chen, Ziyi
   Cui, Guangmang
   Li, Zihan
   Zhao, Jufeng
TI Lightweight Patch-Wise Casformer for dynamic scene deblurring
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Image deblurring; Deep separable attention; Double -Flow gate; Cascade
   Transformer
ID NETWORK
AB In dynamic scenes, motion blur can often occur, which is non-uniform and can be difficult to remove. Recently, the Transformer has shown excellent performance in various image-related tasks such as classification, recognition, and segmentation. Using a Transformer-based backbone network has also shown potential advantages in image deblurring. However, the computational complexity of Transformers increases quadratically with spatial resolution, making it difficult to apply to high-resolution images. To address the above issue, we propose a cascade Transformer (Casformer) that consists of two key modules: Deep Separable Attention (DSA) and DoubleFlow Gate (DFG). Our approach effectively reduces computational complexity while suppressing blurry information. Additionally, we discovered an inconsistency between training and testing images during the image restoration process. We addressed this issue by experimentally verifying an inference aggregation method (IAM) that independently predicts patches during inference to address the problem of imbalanced information distribution. Experimental results demonstrate that our design performs well on GoPro and other datasets, e.g. 29.20 dB PSNR on RealBlur-J, exceeding the previous state-of-the-art (SOTA) 0.14 dB.
C1 [Chen, Ziyi; Cui, Guangmang; Li, Zihan; Zhao, Jufeng] Hangzhou Dianzi Univ, Inst Carbon Neutral & New Energy, Sch Elect & Informat, Hangzhou 310018, Peoples R China.
   [Cui, Guangmang; Zhao, Jufeng] Hangzhou Dianzi Univ, Zhejiang Prov Key Lab Equipment Elect, Hangzhou 310018, Peoples R China.
C3 Hangzhou Dianzi University; Hangzhou Dianzi University
RP Cui, GM (corresponding author), Hangzhou Dianzi Univ, Inst Carbon Neutral & New Energy, Sch Elect & Informat, Hangzhou 310018, Peoples R China.
EM cuigm@hdu.edu.cn
OI Cui, Guangmang/0000-0001-9821-8179; , Ziyi Chen/0009-0004-3356-3480
FU National Natural Science Foundation of China [61805063]; Natural Science
   Foundation of Zhejiang Province [LY22F050002]; Graduate Scientific
   Research Foundation of Hangzhou Dianzi University [CXJJ2023058];
   Department of Education of Zhejiang Province [Y202249843]
FX This work was supported by National Natural Science Foundation of China
   under Grant No. 61805063 and the Natural Science Foundation of Zhejiang
   Province under Grant No. LY22F050002. This work was also supported by
   the Graduate Scientific Research Foundation of Hangzhou Dianzi
   University under Grant No. CXJJ2023058 and Department of Education of
   Zhejiang Province under Grant No. Y202249843.
CR Aghajanyan A., arXiv
   CHARBONNIER P, 1994, IEEE IMAGE PROC, P168
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen X, 2022, Arxiv, DOI arXiv:2209.06794
   Chen XN, 2023, Arxiv, DOI [arXiv:2302.06675, DOI 10.48550/ARXIV.2302.06675]
   Chen YF, 2023, Arxiv, DOI arXiv:2211.04168
   Chen Y, 2021, IEEE ACCESS, V9, P65638, DOI 10.1109/ACCESS.2021.3076241
   Chi ZX, 2021, PROC CVPR IEEE, P9133, DOI 10.1109/CVPR46437.2021.00902
   Cho SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4621, DOI 10.1109/ICCV48922.2021.00460
   Chu XJ, 2022, LECT NOTES COMPUT SC, V13667, P53, DOI 10.1007/978-3-031-20071-7_4
   Cui JK, 2020, NEUROCOMPUTING, V383, P39, DOI 10.1016/j.neucom.2019.11.063
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Gao HY, 2019, PROC CVPR IEEE, P3843, DOI 10.1109/CVPR.2019.00397
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hassani A, 2022, Arxiv, DOI [arXiv:2209.15001, DOI 10.48550/ARXIV.2209.15001]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu XB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4278, DOI 10.1109/ICCV48922.2021.00426
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Jaesung Rim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P184, DOI 10.1007/978-3-030-58595-2_12
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Li J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4096, DOI 10.1109/ICCV48922.2021.00408
   Li JC, 2021, IEEE T MULTIMEDIA, V23, P2986, DOI 10.1109/TMM.2021.3068561
   Liu Y, 2022, IEEE T MULTIMEDIA, V24, P2890, DOI 10.1109/TMM.2021.3090206
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu BY, 2019, PROC CVPR IEEE, P10217, DOI 10.1109/CVPR.2019.01047
   Muennighoff N, 2022, Arxiv, DOI [arXiv:2210.07316, 10.48550/arXiv.2210.07316]
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Park D., 2020, P EUR C COMP VIS
   Purohit K, 2020, AAAI CONF ARTIF INTE, V34, P11882
   Ren C, 2019, IEEE T MULTIMEDIA, V21, P731, DOI 10.1109/TMM.2018.2866362
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Ruiz N, 2023, PROC CVPR IEEE, P22500, DOI 10.1109/CVPR52729.2023.02155
   Seif G, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1468, DOI 10.1109/ICASSP.2018.8461664
   Shen ZY, 2019, IEEE I CONF COMP VIS, P5571, DOI 10.1109/ICCV.2019.00567
   Su W., 2022, arXiv
   Suin M, 2020, PROC CVPR IEEE, P3603, DOI 10.1109/CVPR42600.2020.00366
   Takase S, 2022, Arxiv, DOI arXiv:2104.06022
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Tsai FJ, 2022, LECT NOTES COMPUT SC, V13679, P146, DOI 10.1007/978-3-031-19800-7_9
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang LY, 2023, Arxiv, DOI arXiv:2308.08730
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang WH, 2023, Arxiv, DOI [arXiv:2211.05778, 10.48550/ARXIV.2211.05778]
   Wang WH, 2022, Arxiv, DOI arXiv:2208.10442
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xia B, 2023, IEEE I CONF COMP VIS, P13049, DOI 10.1109/ICCV51070.2023.01204
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Yu JH, 2022, Arxiv, DOI arXiv:2205.01917
   Yu X, 2014, IEEE T MULTIMEDIA, V16, P1510, DOI 10.1109/TMM.2014.2321734
   Yuan Y, 2020, PROC CVPR IEEE, P3552, DOI 10.1109/CVPR42600.2020.00361
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang HG, 2019, PROC CVPR IEEE, P5971, DOI 10.1109/CVPR.2019.00613
   Zhang J., P EU CHIN APP DRON W, P6
   Zhang JW, 2018, PROC CVPR IEEE, P2521, DOI 10.1109/CVPR.2018.00267
   Zhang JT, 2022, IEEE ACCESS, V10, P81390, DOI 10.1109/ACCESS.2022.3194524
   Zhang KH, 2020, PROC CVPR IEEE, P2734, DOI 10.1109/CVPR42600.2020.00281
   Zhang T, 2022, AAAI CONF ARTIF INTE, P11712
   Zhang YX, 2023, PROC CVPR IEEE, P10146, DOI 10.1109/CVPR52729.2023.00978
NR 65
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104112
DI 10.1016/j.jvcir.2024.104112
EA MAR 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OF4K7
UT WOS:001205835800001
DA 2024-08-05
ER

PT J
AU Chen, Z
   Zhang, K
   Cai, H
   Ding, XY
   Jiang, CX
   Chen, ZZ
AF Chen, Zhao
   Zhang, Kao
   Cai, Hao
   Ding, Xiaoying
   Jiang, Chenxi
   Chen, Zhenzhong
TI Audio-visual saliency prediction for movie viewing in immersive
   environments: Dataset and benchmarks
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Saliency prediction; Visual attention; Movie viewing; Virtual reality
ID INTEGRATION; GAZE
AB In this paper, an eye -tracking dataset of movie viewing in the immersive environment is developed, which contains 256 movie clips with 2K QHD resolution and corresponding movie genre labels from IMDb (Internet Movie Database). The dataset provides the audio-visual clues for studying the human visual attention when watching movie using a VR headset, by recording the eye movements using integrated eye tracker. To provide benchmarks for a saliency prediction for movie viewing in the immersive environment, fifteen computational models are evaluated on the dataset, including a newly developed multi -stream audio-visual saliency prediction model based on deep neural networks, named as MSAV. Detailed quantitative and qualitative comparisons and analyses are also provided. The developed dataset and benchmarks could help to facilitate the studies of visual saliency prediction for movie viewing in the immersive environments.
C1 [Chen, Zhao; Zhang, Kao; Cai, Hao; Jiang, Chenxi; Chen, Zhenzhong] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Peoples R China.
   [Zhang, Kao] Nanjing Univ Informat Sci & Technol, Sch Artificial Intelligence, Sch Future Technol, Nanjing 210044, Peoples R China.
   [Ding, Xiaoying] Zhongnan Univ Econ & Law, Sch Informat & Safety Engn, Wuhan 430073, Peoples R China.
C3 Wuhan University; Nanjing University of Information Science &
   Technology; Zhongnan University of Economics & Law
RP Chen, ZZ (corresponding author), Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Peoples R China.
EM zzchen@ieee.org
FU National Natural Science Foundation of China [62036005, 62201404]; China
   Postdoctoral Science Foundation [2021M692463, 2022M713503]; Natural
   Science Foundation of Hubei Province, China [2022CFB984]
FX This work was supported in part by the National Natural Science
   Foundation of China under contract No. 62036005 and No. 62201404, China
   Postdoctoral Science Foundation under contract No. 2021M692463 and No.
   2022M713503, and Natural Science Foundation of Hubei Province, China
   under contract No. 2022CFB984.
CR Anderson R, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980257
   Aytar Y, 2016, ADV NEUR IN, V29
   Bando T, 2012, DISPLAYS, V33, P76, DOI 10.1016/j.displa.2011.09.001
   Bellitto G, 2021, INT J COMPUT VISION, V129, P3216, DOI 10.1007/s11263-021-01519-y
   Borji A, 2021, IEEE T PATTERN ANAL, V43, P679, DOI 10.1109/TPAMI.2019.2935715
   Borji A, 2013, IEEE I CONF COMP VIS, P921, DOI 10.1109/ICCV.2013.118
   Bowman DA, 2007, COMPUTER, V40, P36, DOI 10.1109/MC.2007.257
   Carletta J, 2005, LECT NOTES COMPUT SC, V3869, P28
   CHERRY EC, 1953, J ACOUST SOC AM, V25, P975, DOI 10.1121/1.1907229
   Coutrot A, 2015, EUR SIGNAL PR CONF, P1531, DOI 10.1109/EUSIPCO.2015.7362640
   Coutrot A, 2014, J VISION, V14, DOI 10.1167/14.8.5
   Nguyen C, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5428, DOI 10.1145/3025453.3025675
   Ding XY, 2022, NEUROCOMPUTING, V502, P120, DOI 10.1016/j.neucom.2022.06.088
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   GOODALE MA, 1992, TRENDS NEUROSCI, V15, P20, DOI 10.1016/0166-2236(92)90344-8
   Gorji S, 2018, PROC CVPR IEEE, P7501, DOI 10.1109/CVPR.2018.00783
   Gutiérrez J, 2022, IEEE T MULTIMEDIA, V24, P3087, DOI 10.1109/TMM.2021.3093717
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Harel J., 2016, P NEURAL INFORM PROC, P545
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jain S, 2021, IEEE INT C INT ROBOT, P3520, DOI 10.1109/IROS51168.2021.9635989
   Jiang L, 2018, LECT NOTES COMPUT SC, V11218, P625, DOI 10.1007/978-3-030-01264-9_37
   Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710
   Jost T, 2005, COMPUT VIS IMAGE UND, V100, P107, DOI 10.1016/j.cviu.2004.10.009
   Khatoonabadi SH, 2015, MULTIMED TOOLS APPL, V74, P10057, DOI 10.1007/s11042-015-2802-3
   Khatoonabadi SH, 2015, PROC CVPR IEEE, P5501, DOI 10.1109/CVPR.2015.7299189
   Lai QX, 2020, IEEE T IMAGE PROCESS, V29, P1113, DOI 10.1109/TIP.2019.2936112
   Leborán V, 2017, IEEE T PATTERN ANAL, V39, P893, DOI 10.1109/TPAMI.2016.2567391
   Linardos P, 2019, Arxiv, DOI arXiv:1907.01869
   Liu YF, 2017, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2017.343
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Mahadevan V, 2010, IEEE T PATTERN ANAL, V32, P171, DOI 10.1109/TPAMI.2009.112
   Min K, 2019, IEEE I CONF COMP VIS, P2394, DOI 10.1109/ICCV.2019.00248
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3805, DOI 10.1109/TIP.2020.2966082
   Min XK, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/2996463
   Mital PK, 2011, COGN COMPUT, V3, P5, DOI 10.1007/s12559-010-9074-z
   Olsen A., 2012, P S EYE TRACKING RES, P317, DOI [10.1145/2168556.2168625, DOI 10.1145/2168556.2168625]
   Ostrand R., 2011, Proceedings of the Annual Meeting of the Cognitive Science Society, V33, P1376, DOI [10.1016/j.neuroimage.2010.12.063.Discrete, DOI 10.1016/J.NEUROIMAGE.2010.12.063.DISCRETE]
   Pei JL, 2023, APPL INTELL, V53, P6214, DOI 10.1007/s10489-022-03647-5
   Peters RJ, 2005, VISION RES, V45, P2397, DOI 10.1016/j.visres.2005.03.019
   Tavakoli HR, 2020, Arxiv, DOI arXiv:1905.10693
   Rebenitsch L, 2016, VIRTUAL REAL-LONDON, V20, P101, DOI 10.1007/s10055-016-0285-9
   Riche N, 2013, IEEE I CONF COMP VIS, P1153, DOI 10.1109/ICCV.2013.147
   Ruesch J, 2008, IEEE INT CONF ROBOT, P962, DOI 10.1109/ROBOT.2008.4543329
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Schauerte B, 2011, IEEE INT C INT ROBOT, P1173, DOI 10.1109/IROS.2011.6048857
   Stein BE, 2008, NAT REV NEUROSCI, V9, P255, DOI 10.1038/nrn2331
   Stevenson RA, 2009, NEUROIMAGE, V44, P1210, DOI 10.1016/j.neuroimage.2008.09.034
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Tatler BW, 2008, J EYE MOVEMENT RES, V2
   Tatler BW, 2005, VISION RES, V45, P643, DOI 10.1016/j.visres.2004.09.017
   Tseng PH, 2009, J VISION, V9, DOI 10.1167/9.7.4
   Tsiami A, 2020, PROC CVPR IEEE, P4765, DOI 10.1109/CVPR42600.2020.00482
   Van der Burg E, 2008, J EXP PSYCHOL HUMAN, V34, P1053, DOI 10.1037/0096-1523.34.5.1053
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P220, DOI 10.1109/TPAMI.2019.2924417
   Wehrmann J, 2017, APPL SOFT COMPUT, V61, P973, DOI 10.1016/j.asoc.2017.08.029
   Wen SJ, 2024, IEEE T CIRC SYST VID, V34, P5935, DOI 10.1109/TCSVT.2023.3342903
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Zhang K, 2021, IEEE T IMAGE PROCESS, V30, P572, DOI 10.1109/TIP.2020.3036749
   Zhang K, 2019, IEEE T CIRC SYST VID, V29, P3544, DOI 10.1109/TCSVT.2018.2883305
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhou XF, 2023, IEEE T CIRC SYST VID, V33, P7696, DOI 10.1109/TCSVT.2023.3278410
NR 65
TC 0
Z9 0
U1 2
U2 2
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104095
DI 10.1016/j.jvcir.2024.104095
EA MAR 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NU6M2
UT WOS:001203006800001
DA 2024-08-05
ER

PT J
AU Liu, ZK
   Fu, XX
   Lin, C
   Xu, HY
AF Liu, Zhenkai
   Fu, Xinxiao
   Lin, Chi
   Xu, Haiyong
TI COC-UFGAN: Underwater image enhancement based on color opponent
   compensation and dual-subnet underwater fusion generative adversarial
   network
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Underwater Image Enhancement; Color Opponent Compensation; Generative
   Adversarial Network
ID DECOMPOSITION; QUALITY; MODEL
AB Due to the complex underwater environment and the attenuation of light, the underwater image is produced with various distortions such as color loss, low contrast, noise, blur, and haze-like, which bring significant challenges to underwater image applications. To alleviate these problems, a novel color opponent compensation and a dualsubnet underwater fusion generative adversarial network (COC-UFGAN) are proposed by simulating the information processing mechanism in the human visual system (HVS). Specifically, considering the color opponent mechanism of the primary visual cortex and the characteristics of underwater imaging, an opponent domain is constructed consisting of four color opponent channels: green-red, yellow-blue, cyan-red and black-white. Then, a novel local-to-global color opponent compensation method is designed to restore color loss in underwater imaging. Furthermore, considering the complexity of underwater imaging, a dual-subnet underwater fusion generative adversarial network is designed. Finally, experimental results on synthetic and natural underwater images demonstrate the superiority of the proposed COC-UFGAN over the state-of-the-art methods qualitatively and quantitatively.
C1 [Liu, Zhenkai; Lin, Chi] Ningbo Univ, Coll Sci & Technol, Ningbo 315300, Peoples R China.
   [Fu, Xinxiao; Xu, Haiyong] Ningbo Univ, Sch Math & Stat, Ningbo 315211, Peoples R China.
C3 Ningbo University; Ningbo University
RP Xu, HY (corresponding author), Ningbo Univ, Sch Math & Stat, Ningbo 315211, Peoples R China.
EM xuhaiyong@nbu.edu.cn
CR Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252
   Anwar S, 2018, Arxiv, DOI arXiv:1807.03528
   Berman D, 2021, IEEE T PATTERN ANAL, V43, P2822, DOI 10.1109/TPAMI.2020.2977624
   BUCHSBAUM G, 1980, J FRANKLIN I, V310, P1, DOI 10.1016/0016-0032(80)90058-7
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen XY, 2019, IEEE T IND ELECTRON, V66, P9350, DOI 10.1109/TIE.2019.2893840
   Chiang JY, 2012, IEEE T IMAGE PROCESS, V21, P1756, DOI 10.1109/TIP.2011.2179666
   Codruta AO, 2020, IEEE T IMAGE PROCESS, V29, P2653, DOI 10.1109/TIP.2019.2951304
   Conway BR, 2010, J NEUROSCI, V30, P14955, DOI 10.1523/JNEUROSCI.4348-10.2010
   Drews PLJ, 2016, IEEE COMPUT GRAPH, V36, P24, DOI 10.1109/MCG.2016.26
   Fabbri C, 2018, IEEE INT CONF ROBOT, P7159
   Finlayson GD, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P37
   Fu XY, 2014, IEEE IMAGE PROC, P4572, DOI 10.1109/ICIP.2014.7025927
   Galdran A, 2015, J VIS COMMUN IMAGE R, V26, P132, DOI 10.1016/j.jvcir.2014.11.006
   Gao SB, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2919947
   Gao SB, 2015, IEEE T PATTERN ANAL, V37, P1973, DOI 10.1109/TPAMI.2015.2396053
   Gijsenij A, 2011, IEEE T PATTERN ANAL, V33, P687, DOI 10.1109/TPAMI.2010.93
   Gonzalez-Sabbagh S, 2022, Arxiv, DOI arXiv:2211.10026
   Guo YC, 2020, IEEE J OCEANIC ENG, V45, P862, DOI 10.1109/JOE.2019.2911447
   Han G., 2023, IEEE Trans. Geosci. Remote Sens.
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   JAFFE JS, 1990, IEEE J OCEANIC ENG, V15, P101, DOI 10.1109/48.50695
   Jiang QP, 2022, IEEE T CIRC SYST VID, V32, P5959, DOI 10.1109/TCSVT.2022.3164918
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kang YZ, 2023, IEEE T CIRC SYST VID, V33, P988, DOI 10.1109/TCSVT.2022.3208100
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Larson E C, 2009, Categorical Image Quality (CSIQ) database [EB/OL]
   Li CY, 2021, IEEE T IMAGE PROCESS, V30, P4985, DOI 10.1109/TIP.2021.3076367
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107038
   Li CY, 2018, IEEE SIGNAL PROC LET, V25, P323, DOI 10.1109/LSP.2018.2792050
   Li HY, 2021, SIGNAL PROCESS-IMAGE, V95, DOI 10.1016/j.image.2021.116248
   Li J, 2018, IEEE ROBOT AUTOM LET, V3, P387, DOI 10.1109/LRA.2017.2730363
   LIU YC, 1995, IEEE T CONSUM ELECTR, V41, P460, DOI 10.1109/30.468045
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Noguchi Y, 2021, IEEE J OCEANIC ENG, V46, P11, DOI 10.1109/JOE.2020.2972046
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Peng HW, 2017, IEEE T PATTERN ANAL, V39, P818, DOI 10.1109/TPAMI.2016.2562626
   Peng YT, 2018, IEEE T IMAGE PROCESS, V27, P2856, DOI 10.1109/TIP.2018.2813092
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Pizer S M., 1990, VISUALIZATION BIOMED
   Shapley R, 2011, VISION RES, V51, P701, DOI 10.1016/j.visres.2011.02.012
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song W, 2020, IEEE T BROADCAST, V66, P153, DOI 10.1109/TBC.2019.2960942
   Sun J, 2021, IEEE J OCEANIC ENG, V46, P294, DOI 10.1109/JOE.2020.2974270
   Van de Weijer J, 2007, IEEE T IMAGE PROCESS, V16, P2207, DOI 10.1109/TIP.2007.901808
   Wu SC, 2021, IEEE J OCEANIC ENG, V46, P1213, DOI 10.1109/JOE.2021.3064093
   Yang KF, 2013, PROC CVPR IEEE, P2810, DOI 10.1109/CVPR.2013.362
   Yang M, 2020, IEEE J OCEANIC ENG, V45, P521, DOI 10.1109/JOE.2018.2886093
   Yeh CH, 2022, IEEE T NEUR NET LEAR, V33, P6129, DOI 10.1109/TNNLS.2021.3072414
   Yi X., 2023, Patent No. [Displays102586, 102586]
   Yu XL, 2019, LECT NOTES COMPUT SC, V11188, P66, DOI 10.1007/978-3-030-05792-3_7
   Zhang J, 2012, LECT NOTES COMPUT SC, V7576, P312, DOI 10.1007/978-3-642-33715-4_23
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zhou JC, 2023, INT J COMPUT VISION, DOI 10.1007/s11263-023-01853-3
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 60
TC 0
Z9 0
U1 4
U2 4
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104101
DI 10.1016/j.jvcir.2024.104101
EA FEB 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NG0M3
UT WOS:001199181500001
DA 2024-08-05
ER

PT J
AU Melman, A
   Evsutin, O
AF Melman, Anna
   Evsutin, Oleg
TI Methods for countering attacks on image watermarking schemes: Overview
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Review
DE Digital images; Watermarking; Robustness; Removal attacks; Forgery
   attacks
ID ROBUST WATERMARKING; FOURIER-TRANSFORM; COLLUSION ATTACK; SVD;
   RESISTANT; RESILIENT; HYBRID; DOMAIN; PRINT; DCT
AB Image watermarking is an effective and promising technology. Robust watermarks that are resistant to various attacks allow authors and owners of digital images to protect their rights to digital content, control its distribution and confirm its authenticity. Most of the modern algorithms for robust image watermarking aim to achieve resistance to a large number of different attacks. However, some authors develop algorithms designed to counter targeted attacks. The study of such schemes allows developers of watermarking algorithms to evaluate special means of counteracting various attacks, and then use them to create new robust schemes, both targeted and universal ones. In this paper, we present an overview of robust image watermarking schemes in terms of countering targeted attacks. We review the state-of-the-art in the field of attacking robust watermarks and propose a four -level classification of attacks that includes different levels of attack implementation, including an attacker's intent, characteristics of actions, the main target and an attack type. The proposed classification considers a watermark as an object of attack and summarizes various characteristics of attacks in a hierarchical manner. We analyze the means of countering common attacks such as image processing attacks, geometric attacks, print -scan and screen capture attacks, collusion attacks, and ambiguity attacks. Based on the results of our review, we highlight the most common methods of countering attacks and formulate promising areas of research in the field of methods for improving security of embedding schemes.
C1 [Melman, Anna; Evsutin, Oleg] HSE Univ, 20 Myasnitskaya Ulitsa, Moscow 101000, Russia.
C3 HSE University (National Research University Higher School of Economics)
RP Evsutin, O (corresponding author), HSE Univ, 20 Myasnitskaya Ulitsa, Moscow 101000, Russia.
EM evsutin.oo@gmail.com
RI Melman, Anna/V-4267-2019; Evsutin, Oleg/E-6719-2017
OI Melman, Anna/0000-0001-6444-7774; Evsutin, Oleg/0000-0002-8257-2082
CR Abdelhakim AM, 2018, MULTIMED TOOLS APPL, V77, P27895, DOI 10.1007/s11042-018-6014-5
   Agarwal N, 2019, MULTIMED TOOLS APPL, V78, P8603, DOI 10.1007/s11042-018-7128-5
   Amiri SH, 2014, SIGNAL PROCESS-IMAGE, V29, P1181, DOI 10.1016/j.image.2014.07.004
   Anand A, 2021, MULTIMED TOOLS APPL, V80, P30165, DOI 10.1007/s11042-020-08801-0
   Bai R, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12199780
   Bhatti Uzair Aslam, 2022, Proceedings of International Conference on Information Technology and Applications: ICITA 2021. Lecture Notes in Networks and Systems (350), P75, DOI 10.1007/978-981-16-7618-5_7
   Bideh PN, 2018, MULTIMED TOOLS APPL, V77, P31713, DOI 10.1007/s11042-018-6218-8
   Boujerfaoui S, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12010074
   Cheddad A, 2010, SIGNAL PROCESS, V90, P727, DOI 10.1016/j.sigpro.2009.08.010
   Chen BJ, 2020, COMPUT VIS IMAGE UND, V197, DOI 10.1016/j.cviu.2020.103015
   Chen PY, 2015, IMAGING SCI J, V63, P273, DOI 10.1179/1743131X15Y.0000000010
   Chen WT, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21030701
   Chen WT, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10217494
   Ching-Sheng Hsu, 2018, Frontier Computing. Theory, Technologies and Applications (FC 2017). LNEE 464, P84, DOI 10.1007/978-981-10-7398-4_9
   Degadwala SD, 2020, PROCEDIA COMPUT SCI, V167, P213, DOI 10.1016/j.procs.2020.03.198
   Dwivedi Ranjana, 2022, Advances in VLSI, Communication, and Signal Processing: Select Proceedings of VCAS 2021. Lecture Notes in Electrical Engineering (911), P671, DOI 10.1007/978-981-19-2631-0_58
   Dzhanashia K, 2022, COMPUT ELECTR ENG, V102, DOI 10.1016/j.compeleceng.2022.108194
   Ernawan F, 2018, IEEE ACCESS, V6, P20464, DOI 10.1109/ACCESS.2018.2819424
   Evsutin O, 2022, SIGNAL PROCESS-IMAGE, V100, DOI 10.1016/j.image.2021.116523
   Fares K, 2020, OPTIK, V208, DOI 10.1016/j.ijleo.2020.164562
   Fazli S, 2016, OPTIK, V127, P964, DOI 10.1016/j.ijleo.2015.09.205
   Feng H, 2015, MULTIMED TOOLS APPL, V74, P6967, DOI 10.1007/s11042-014-1948-8
   Garg P, 2020, MULTIMED TOOLS APPL, V79, P25921, DOI 10.1007/s11042-020-09262-1
   Geng LF, 2020, J REAL-TIME IMAGE PR, V17, P631, DOI 10.1007/s11554-020-00941-8
   Ghadi M, 2019, MULTIMED TOOLS APPL, V78, P15705, DOI 10.1007/s11042-018-6851-2
   Golabi S, 2014, INFORM SCIENCES, V269, P94, DOI 10.1016/j.ins.2013.11.020
   Goli MS, 2017, 2017 3RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION AND IMAGE ANALYSIS (IPRIA), P237, DOI 10.1109/PRIA.2017.7983054
   Gull S, 2023, MULTIMED TOOLS APPL, DOI 10.1007/s11042-023-15396-9
   Haghighi BB, 2020, COGN COMPUT, V12, P863, DOI 10.1007/s12559-019-09700-9
   Han SC, 2018, OPTOELECTRON LETT, V14, P61, DOI 10.1007/s11801-018-7212-0
   Hatoum MW, 2020, MULTIMED TOOLS APPL, V79, P1887, DOI 10.1007/s11042-019-08242-4
   Holliman M, 2000, IEEE T IMAGE PROCESS, V9, P432, DOI 10.1109/83.826780
   Hsu CS, 2020, MULTIMED TOOLS APPL, V79, P11297, DOI 10.1007/s11042-019-08367-6
   Islam M, 2018, MULTIMED TOOLS APPL, V77, P14407, DOI 10.1007/s11042-017-5035-9
   Jayanthi VE, 2011, INT J ELECTRON, V98, P1565, DOI 10.1080/00207217.2011.601444
   Ji F, 2013, NEUROCOMPUTING, V106, P42, DOI 10.1016/j.neucom.2012.09.032
   Kadian P, 2021, WIRELESS PERS COMMUN, V118, P3225, DOI 10.1007/s11277-021-08177-w
   Kang XB, 2018, MULTIMED TOOLS APPL, V77, P13197, DOI 10.1007/s11042-017-4941-1
   Kasana G, 2017, OPTIK, V142, P191, DOI 10.1016/j.ijleo.2017.05.027
   Keskinarkaus A, 2012, J VIS COMMUN IMAGE R, V23, P507, DOI 10.1016/j.jvcir.2012.01.010
   Kumar S, 2020, MULTIMED TOOLS APPL, V79, P20149, DOI 10.1007/s11042-020-08881-y
   Lebcir M, 2022, MULTIMED TOOLS APPL, V81, P20561, DOI 10.1007/s11042-022-12365-6
   Lee JS, 2014, IEEE MULTIMEDIA, V21, P60, DOI 10.1109/MMUL.2014.14
   Li L, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21196554
   Li MJ, 2020, IEEE ACCESS, V8, P72308, DOI 10.1109/ACCESS.2020.2987914
   Li Q, 2022, IEEE T CIRC SYST VID, V32, P5695, DOI 10.1109/TCSVT.2021.3138795
   Li YM, 2021, INFORM SCIENCES, V551, P205, DOI 10.1016/j.ins.2020.11.020
   Lin CH, 2010, VISUAL COMPUT, V26, P1101, DOI [10.1007/s00371-010-0461-y, 10.1007/s00371-010-0461]
   Liu P, 2022, MULTIMED TOOLS APPL, V81, P2637, DOI 10.1007/s11042-021-11532-5
   Liu YN, 2015, MULTIMED TOOLS APPL, V74, P4765, DOI 10.1007/s11042-013-1838-5
   Loukhaoukha Khaled, 2017, Journal of Electrical Systems and Information Technology, V4, P359, DOI 10.1016/j.jesit.2016.12.011
   Luo YL, 2021, EXPERT SYST APPL, V168, DOI 10.1016/j.eswa.2020.114272
   Mahto DK, 2022, SOFT COMPUT, V26, P8105, DOI 10.1007/s00500-022-07155-z
   Mahto DK, 2021, COMPUT ELECTR ENG, V93, DOI 10.1016/j.compeleceng.2021.107255
   Maity SP, 2013, WIRELESS PERS COMMUN, V72, P1737, DOI 10.1007/s11277-013-1132-x
   Makbol NM, 2018, MULTIMED TOOLS APPL, V77, P26845, DOI 10.1007/s11042-018-5891-y
   Melman A, 2023, ARTIF INTELL REV, V56, P15375, DOI 10.1007/s10462-023-10537-w
   Mousavi SM, 2017, MULTIMED TOOLS APPL, V76, P10313, DOI 10.1007/s11042-016-3622-9
   Mousavi SM, 2014, J DIGIT IMAGING, V27, P714, DOI 10.1007/s10278-014-9700-5
   Nah J, 2016, MULTIMED TOOLS APPL, V75, P14917, DOI 10.1007/s11042-014-2233-6
   Nyeem H, 2014, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2014-135
   Petitcolas FAP, 1998, LECT NOTES COMPUT SC, V1525, P218
   Petitcolas FAP, 2000, IEEE SIGNAL PROC MAG, V17, P58, DOI 10.1109/79.879339
   Phiasai T, 2015, WIRELESS PERS COMMUN, V85, P421, DOI 10.1007/s11277-015-2747-x
   Pun CM, 2015, MULTIMED TOOLS APPL, V74, P7821, DOI 10.1007/s11042-014-2025-z
   Qin C, 2023, IEEE MULTIMEDIA, V30, P28, DOI 10.1109/MMUL.2022.3213004
   Rai M, 2023, CIRC SYST SIGNAL PR, V42, P4019, DOI 10.1007/s00034-023-02299-1
   Raj NRN, 2021, MULTIMED TOOLS APPL, V80, P19307, DOI 10.1007/s11042-021-10664-y
   Ray A, 2020, INT J MULTIMED INF R, V9, P249, DOI 10.1007/s13735-020-00197-9
   Roy S, 2019, IJST-T ELECTR ENG, V43, P201, DOI 10.1007/s40998-018-0109-x
   Sadeghi S, 2018, PATTERN ANAL APPL, V21, P291, DOI 10.1007/s10044-017-0678-8
   Safizadeh N, 2023, MULTIMED TOOLS APPL, V82, P22275, DOI 10.1007/s11042-023-15248-6
   Sahu AK, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03365-9
   Setiadi DIM, 2021, MULTIMED TOOLS APPL, V80, P8423, DOI 10.1007/s11042-020-10035-z
   Singh AK, 2014, P NATL A SCI INDIA A, V84, P345, DOI 10.1007/s40010-014-0140-x
   Singh HK, 2023, MULTIMED TOOLS APPL, DOI 10.1007/s11042-023-15750-x
   Singh Neha, 2020, Procedia Computer Science, V171, P1137, DOI 10.1016/j.procs.2020.04.122
   Singh OP, 2021, MULTIMED TOOLS APPL, V80, P30367, DOI 10.1007/s11042-020-09606-x
   Tanha Maryam., 2012, 2012 INT C CYBER SEC, P265, DOI DOI 10.1109/CYBERSEC.2012.6246095
   Tanveer MSR, 2019, INT CONF ELECTR ENG, DOI 10.1109/eict48899.2019.9068779
   Thahab AT, 2018, 2018 THIRD SCIENTIFIC CONFERENCE OF ELECTRICAL ENGINEERING (SCEE), P105, DOI 10.1109/SCEE.2018.8684058
   Tsougenis ED, 2012, J SYST SOFTWARE, V85, P1864, DOI 10.1016/j.jss.2012.02.045
   Vaidya SP, 2017, MULTIMED TOOLS APPL, V76, P25623, DOI 10.1007/s11042-017-4355-0
   Verma VS, 2015, IETE TECH REV, V32, P479, DOI 10.1080/02564602.2015.1042927
   Voloshynovskiy S, 2001, IEEE COMMUN MAG, V39, P118, DOI 10.1109/35.940053
   Wang CY, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8030410
   Wang CP, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12020303
   Wang KS, 2022, MULTIMED TOOLS APPL, V81, P6159, DOI 10.1007/s11042-021-11725-y
   Wang SJ, 2009, IMAGING SCI J, V57, P177, DOI 10.1179/174313109X442489
   Wang XY, 2020, PATTERN ANAL APPL, V23, P933, DOI 10.1007/s10044-019-00828-w
   Wu YD, 2006, IEEE T MULTIMEDIA, V8, P626, DOI 10.1109/TMM.2006.870721
   Ye XY, 2014, 2014 7TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP 2014), P323, DOI 10.1109/CISP.2014.7003800
   Yi-Lin Bei, 2018, 2018 International Conference on Security, Pattern Analysis, and Cybernetics (SPAC), P343, DOI 10.1109/SPAC46244.2018.8965467
   Yu XY, 2017, FUTURE INTERNET, V9, DOI 10.3390/fi9040056
   Yuan ZH, 2020, MULTIMED TOOLS APPL, V79, P30557, DOI 10.1007/s11042-020-09499-w
   Zhang H, 2011, IEEE T IMAGE PROCESS, V20, P2189, DOI 10.1109/TIP.2011.2118216
   Zhang WY, 2018, COMPUT ELECTR ENG, V67, P182, DOI 10.1016/j.compeleceng.2018.02.051
   Zhang XJ, 2013, FRONT COMPUT SCI-CHI, V7, P145, DOI 10.1007/s11704-013-2174-7
   Zhang XT, 2023, MULTIMED TOOLS APPL, V82, P27217, DOI 10.1007/s11042-023-14479-x
   Zheng PJ, 2020, MULTIMED TOOLS APPL, V79, P18343, DOI 10.1007/s11042-019-08490-4
   Zhou L, 2022, SECUR COMMUN NETW, V2022, DOI 10.1155/2022/7605595
   Zong TR, 2015, IEEE T CIRC SYST VID, V25, P717, DOI 10.1109/TCSVT.2014.2363743
NR 102
TC 0
Z9 0
U1 5
U2 5
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104073
DI 10.1016/j.jvcir.2024.104073
EA FEB 2024
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JY0Q9
UT WOS:001176607100001
DA 2024-08-05
ER

PT J
AU Zhang, JH
   Li, B
   Qiu, QH
   Mo, HQ
   Tian, LF
AF Zhang, Jinhong
   Li, Bin
   Qiu, Qianhui
   Mo, Hongqiang
   Tian, Lianfang
TI SICNet: Learning selective inter-slice context via Mask-Guided
   Self-knowledge distillation for NPC segmentation
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Nasopharyngeal carcinoma; Segmentation; Convolutional neural networks;
   Self-knowledge distillation; Selective inter-slice context
ID CARCINOMA LESION SEGMENTATION; NASOPHARYNGEAL; IMAGES
AB Accurate segmentation of nasopharyngeal carcinoma (NPC) in magnetic resonance (MR) images is crucial for radiotherapy planning. However, vanilla 2D/3D deep convolutional networks fail to gain satisfying NPC segmentation results, since 3D methods suffer from inter-slice discontinuities and 2D methods lack inter-slice context learning. To address this problem, this paper proposes a 2.5D learning Selective Inter-slice Context Network (SICNet) for NPC segmentation. The proposed SICNet comprises a main encoder that extracts target features from the middle slice and an auxiliary encoder that learns inter-slice context from adjacent slices. The features from two encoders are fused by the Inter-slice Context Fusion (ICF) module and then sent to the decoder for predictions. Furthermore, a knowledge distillation module named Mask-Guided Self-knowledge Distillation (MGSD) is proposed to help the auxiliary encoder learn context selectively. Specifically, with the guidance of mask labels, MGSD tells the auxiliary encoder where to look and how to learn, thus achieving selective inter-slice context learning and boosting segmentation accuracy. The proposed SICNet achieves a DSC of 74.38 +/- 11.99, an HD95 of 9.31 +/- 2.68, and an ASSD of 1.46 +/- 0.76 on an in-house NPC dataset, outperforming other state-of-theart segmentation methods.
C1 [Zhang, Jinhong; Li, Bin; Mo, Hongqiang; Tian, Lianfang] South China Univ Technol, Sch Automat Sci & Engn, Guangzhou 510640, Peoples R China.
   [Qiu, Qianhui] Southern Med Univ, Guangdong Acad Med Sci, Guangdong Prov Peoples Hosp, Dept Otolaryngol & Head & Neck Surg, Guangzhou 510080, Peoples R China.
C3 South China University of Technology; Southern Medical University -
   China; Guangdong Academy of Medical Sciences & Guangdong General
   Hospital
RP Li, B (corresponding author), South China Univ Technol, Sch Automat Sci & Engn, Guangzhou 510640, Peoples R China.; Qiu, QH (corresponding author), Southern Med Univ, Guangdong Acad Med Sci, Guangdong Prov Peoples Hosp, Dept Otolaryngol & Head & Neck Surg, Guangzhou 510080, Peoples R China.
EM binlee@scut.edu.cn; qiuqianhui@gdph.org.cn
FU National Natural Science Foundation of China [62273155]; Guangdong
   Provincial Science and Technology Special Fund ("Big Project + Task
   List") [210719145863737]; Key Laboratory of Autonomous Systems and
   Network Control of Ministry of Education (SCUT of China); National
   Engineering Research Center for Tissue Restoration; Guangdong Key
   Laboratory for Biomedical Engineering (SCUT of China)
FX This work is supported by National Natural Science Foundation of China
   under Grant 62273155, 2021 Guangdong Provincial Science and Technology
   Special Fund ("Big Project + Task List") under Grant 210719145863737,
   Key Laboratory of Autonomous Systems and Network Control of Ministry of
   Education (SCUT of China) , the National Engineering Research Center for
   Tissue Restoration and Reconstruction and the Guangdong Key Laboratory
   for Biomedical Engineering (SCUT of China) .
CR Ahn S, 2019, PROC CVPR IEEE, P9155, DOI 10.1109/CVPR.2019.00938
   Chen C, 2019, LECT NOTES COMPUT SC, V11765, P523, DOI 10.1007/978-3-030-32245-8_58
   Chua MLK, 2016, LANCET, V387, P1012, DOI 10.1016/S0140-6736(15)00055-0
   Cicek Ozgun, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P424, DOI 10.1007/978-3-319-46723-8_49
   Dong Z., 2022, arXiv
   Feng JJ, 2020, WD SCI P COMP ENG, V12, P882
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G., 2015, NEURIPS DEEP LEARN R, P9
   Huang HM, 2020, INT CONF ACOUST SPEE, P1055, DOI [10.1109/icassp40776.2020.9053405, 10.1109/ICASSP40776.2020.9053405]
   Huang JB, 2019, LECT NOTES COMPUT SC, V11768, P494, DOI 10.1007/978-3-030-32254-0_55
   Huang W, 2013, J DIGIT IMAGING, V26, P472, DOI 10.1007/s10278-012-9520-4
   Isensee F, 2021, NAT METHODS, V18, P203, DOI 10.1038/s41592-020-01008-z
   Ji M, 2021, PROC CVPR IEEE, P10659, DOI 10.1109/CVPR46437.2021.01052
   Jiang J, 2022, IEEE T MED IMAGING, V41, P1057, DOI 10.1109/TMI.2021.3132291
   Kavur AE, 2021, MED IMAGE ANAL, V69, DOI 10.1016/j.media.2020.101950
   Komodakis N., 2017, ICLR
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lei BY, 2020, MED IMAGE ANAL, V64, DOI 10.1016/j.media.2020.101753
   Lequan Yu, 2017, Medical Image Computing and Computer-Assisted Intervention, MICCAI 2017. 20th International Conference. Proceedings: LNCS 10434, P287, DOI 10.1007/978-3-319-66185-8_33
   Li Y, 2022, IEEE T MED IMAGING, V41, P1639, DOI 10.1109/TMI.2022.3144274
   Li ZW, 2022, IEEE T NEUR NET LEAR, V33, P6999, DOI 10.1109/TNNLS.2021.3084827
   Liang-Chieh C., 2015, INT C LEARNING REPRE
   Liu QD, 2020, IEEE T MED IMAGING, V39, P3429, DOI 10.1109/TMI.2020.2995518
   Liu YH, 2021, APPL SOFT COMPUT, V111, DOI 10.1016/j.asoc.2021.107722
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo XD, 2021, LECT NOTES COMPUT SC, V12902, P318, DOI 10.1007/978-3-030-87196-3_30
   Lustig M, 2007, MAGN RESON MED, V58, P1182, DOI 10.1002/mrm.21391
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Oktay O., 2018, P 1 C MED IM DEEP LE, P1, DOI 10.48550/arXiv.1804.03999
   Romero A., 2015, ICLR, P1
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang P, 2021, NEUROCOMPUTING, V435, P103, DOI 10.1016/j.neucom.2020.12.085
   Tang YC, 2023, ENG STRUCT, V274, DOI 10.1016/j.engstruct.2022.115158
   Tao GH, 2022, MED IMAGE ANAL, V78, DOI 10.1016/j.media.2022.102381
   Tung F, 2019, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2019.00145
   Wang K., 2020, INT C LEARNING REPRE
   Wang N, 2023, IEEE T MED IMAGING, V42, P2740, DOI 10.1109/TMI.2023.3264433
   Wang SX, 2020, IEEE T MED IMAGING, V39, P4174, DOI 10.1109/TMI.2020.3014433
   Wang Y, 2018, I S BIOMED IMAGING, P1336, DOI 10.1109/ISBI.2018.8363818
   Wang YC, 2023, PROC CVPR IEEE, P15651, DOI 10.1109/CVPR52729.2023.01502
   Wu ZH, 2023, INT J INTELL SYST, V2023, DOI 10.1155/2023/9940881
   Yu QH, 2019, Arxiv, DOI arXiv:1904.01150
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang HH, 2019, LECT NOTES COMPUT SC, V11766, P338, DOI 10.1007/978-3-030-32248-9_38
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zhou J, 2006, I S BIOMED IMAGING, P1364
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
NR 49
TC 1
Z9 1
U1 3
U2 3
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104053
DI 10.1016/j.jvcir.2024.104053
EA JAN 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ID8T5
UT WOS:001164486600001
DA 2024-08-05
ER

PT J
AU Hu, ST
   Cheng, Z
   Fan, GD
   Gan, M
   Chen, CLP
AF Hu, Shuteng
   Cheng, Zheng
   Fan, Guodong
   Gan, Min
   Chen, C. L. Philip
TI Texture-aware and color-consistent learning for underwater image
   enhancement
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Underwater image enhancement; Texture-aware; Color-consistent; Feature
   decoupling
ID MODEL
AB Texture and color are pivotal factors for evaluating the quality of underwater image enhancement. However, current methods for enhancing underwater images still exhibit deficiencies in the restoration of texture and color. Diverging from previous approaches, we conduct heuristic modeling specifically targeting color and texture, culminating in the proposal of a texture-aware and color-consistent network (TACC-Net). Specifically, it first obtains preliminary enhanced images of more global features to facilitate feature decoupling. Then, we designed a color feature decoupling (CFD) sub-module to decouple the preliminary enhanced images into colorconsistent color histograms to generate textured gray images. Finally, we designed a color space adjustment embedding (CSAE) sub-module, which is used to evaluate the similarity between features when embedding color information, and adjust the embedding of color information on textures. The experimental results indicate that our method enhances images and exhibits strong competitiveness in improving visual quality.
C1 [Hu, Shuteng; Cheng, Zheng; Fan, Guodong; Gan, Min; Chen, C. L. Philip] Qingdao Univ, Coll Comp Sci & Technol, Qingdao 266000, Shandong, Peoples R China.
   [Chen, C. L. Philip] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Peoples R China.
C3 Qingdao University; South China University of Technology
RP Fan, GD (corresponding author), Qingdao Univ, Coll Comp Sci & Technol, Qingdao 266000, Shandong, Peoples R China.
EM fgd96@outlook.com
RI Chen, C. L. Philip/O-2657-2016
OI Fan, Guodong/0000-0003-0382-6142
FU National Natural Science Foun-dation of China [62073082]; Taishan
   Scholar Pro-gram of Shandong Province
FX <B>Acknowledgment</B> This research was supported by The National
   Natural Science Foun-dation of China (Grant No. 62073082) and the
   Taishan Scholar Pro-gram of Shandong Province.
CR Ancuti C, 2012, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2012.6247661
   Chen L, 2021, IEEE T CIRC SYST VID, V31, P3078, DOI 10.1109/TCSVT.2020.3035108
   Codruta AO, 2020, IEEE T IMAGE PROCESS, V29, P2653, DOI 10.1109/TIP.2019.2951304
   Cong RM, 2023, IEEE T IMAGE PROCESS, V32, P4472, DOI 10.1109/TIP.2023.3286263
   Drews PLJ, 2016, IEEE COMPUT GRAPH, V36, P24, DOI 10.1109/MCG.2016.26
   Fan GD, 2022, IEEE T CIRC SYST VID, V32, P7403, DOI 10.1109/TCSVT.2022.3186880
   Fan GD, 2024, IEEE T NEUR NET LEAR, V35, P1598, DOI 10.1109/TNNLS.2022.3184164
   Feng YX, 2023, IEEE T CIRC SYST VID, V33, P5470, DOI 10.1109/TCSVT.2023.3256414
   Fu XY, 2014, IEEE IMAGE PROC, P4572, DOI 10.1109/ICIP.2014.7025927
   Fu ZQ, 2022, LECT NOTES COMPUT SC, V13678, P465, DOI 10.1007/978-3-031-19797-0_27
   Fu ZQ, 2022, INT CONF ACOUST SPEE, P2764, DOI 10.1109/ICASSP43922.2022.9747758
   Galdran A, 2015, J VIS COMMUN IMAGE R, V26, P132, DOI 10.1016/j.jvcir.2014.11.006
   Guo C., 2023, AAAI, V37, P702
   Guo YC, 2020, IEEE J OCEANIC ENG, V45, P862, DOI 10.1109/JOE.2019.2911447
   Hou GJ, 2024, IEEE T CIRC SYST VID, V34, P799, DOI 10.1109/TCSVT.2023.3290363
   Hou GJ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3578584
   Huang SR, 2023, PROC CVPR IEEE, P18145, DOI 10.1109/CVPR52729.2023.01740
   Islam MJ, 2020, IEEE ROBOT AUTOM LET, V5, P3227, DOI 10.1109/LRA.2020.2974710
   Jiang JX, 2023, Arxiv, DOI arXiv:2305.08824
   Jiang QP, 2022, IEEE T CIRC SYST VID, V32, P5959, DOI 10.1109/TCSVT.2022.3164918
   Jiang QP, 2022, IEEE T INTELL TRANSP, V23, P19440, DOI 10.1109/TITS.2022.3165176
   Jiang QP, 2022, IEEE T IMAGE PROCESS, V31, P2279, DOI 10.1109/TIP.2022.3154588
   Kang YZ, 2023, IEEE T CIRC SYST VID, V33, P988, DOI 10.1109/TCSVT.2022.3208100
   Li CY, 2016, IEEE T IMAGE PROCESS, V26, P5664, DOI 10.1109/TIP.2016.2612882
   Li CY, 2023, Arxiv, DOI arXiv:2302.11831
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P9396, DOI 10.1109/TPAMI.2021.3126387
   Li CY, 2021, IEEE T IMAGE PROCESS, V30, P4985, DOI 10.1109/TIP.2021.3076367
   Li CY, 2021, IEEE T CYBERNETICS, V51, P88, DOI 10.1109/TCYB.2020.2969255
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P4225, DOI 10.1109/TPAMI.2021.3063604
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107038
   Li CY, 2017, PATTERN RECOGN LETT, V94, P62, DOI 10.1016/j.patrec.2017.05.023
   Li J, 2018, IEEE ROBOT AUTOM LET, V3, P387, DOI 10.1109/LRA.2017.2730363
   Liu JY, 2023, Arxiv, DOI arXiv:2308.02097
   Liu JY, 2023, INFORM FUSION, V95, P237, DOI 10.1016/j.inffus.2023.02.027
   Liu JY, 2022, PROC CVPR IEEE, P5792, DOI 10.1109/CVPR52688.2022.00571
   Liu JY, 2022, IEEE T CIRC SYST VID, V32, P5026, DOI 10.1109/TCSVT.2022.3144455
   Liu JY, 2022, IEEE T CIRC SYST VID, V32, P105, DOI 10.1109/TCSVT.2021.3056725
   Liu RS, 2019, Arxiv, DOI arXiv:1901.05320
   Ma L, 2023, INT J COMPUT VISION, DOI 10.1007/s11263-023-01900-z
   Ma L, 2022, Arxiv, DOI arXiv:2212.14245
   Ma L, 2022, IEEE T NEUR NET LEAR, V33, P5666, DOI 10.1109/TNNLS.2021.3071245
   Ma Long, 2022, IEEE Trans. Multimed.
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Peng YT, 2018, IEEE T IMAGE PROCESS, V27, P2856, DOI 10.1109/TIP.2018.2813092
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Song W, 2020, IEEE T BROADCAST, V66, P153, DOI 10.1109/TBC.2019.2960942
   Su JN, 2023, IEEE T PATTERN ANAL, V45, P8453, DOI 10.1109/TPAMI.2022.3229689
   Wang Y, 2017, IEEE IMAGE PROC, P1382, DOI 10.1109/ICIP.2017.8296508
   Wang YD, 2021, SIGNAL PROCESS-IMAGE, V96, DOI 10.1016/j.image.2021.116250
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Yuan GJ, 2024, IEEE INTERNET THINGS, V11, P10722, DOI 10.1109/JIOT.2023.3327978
   Yuan GJ, 2022, J KING SAUD UNIV-COM, V34, P7184, DOI 10.1016/j.jksuci.2022.05.020
   Zhang DH, 2023, ENG APPL ARTIF INTEL, V125, DOI 10.1016/j.engappai.2023.106743
   Zhang DH, 2023, EXPERT SYST APPL, V231, DOI 10.1016/j.eswa.2023.120842
   Zhang Dehuan, 2023, Neural Netw
   Zhang SC, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3299442
   Zhang WD, 2022, IEEE T IMAGE PROCESS, V31, P3997, DOI 10.1109/TIP.2022.3177129
   Zhou J., 2023, IEEE J. Ocean. Eng.
   Zhou JC, 2023, INT J COMPUT VISION, DOI 10.1007/s11263-023-01853-3
   Zhou JC, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3293912
   Zhou JC, 2023, ENG APPL ARTIF INTEL, V121, DOI 10.1016/j.engappai.2023.105946
   Zhou JC, 2023, ENG APPL ARTIF INTEL, V121, DOI 10.1016/j.engappai.2023.105952
   Zhou JC, 2023, APPL INTELL, V53, P3594, DOI 10.1007/s10489-022-03767-y
   Zhuang PX, 2022, IEEE T IMAGE PROCESS, V31, P5442, DOI 10.1109/TIP.2022.3196546
NR 65
TC 0
Z9 0
U1 6
U2 6
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104051
DI 10.1016/j.jvcir.2024.104051
EA JAN 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IK4B2
UT WOS:001166194100001
DA 2024-08-05
ER

PT J
AU Jia, R
   Zhao, L
   Yang, R
   Yang, HH
   Wu, XJ
   Zhang, YM
   Li, P
   Su, YP
AF Jia, Ru
   Zhao, Li
   Yang, Rui
   Yang, Honghong
   Wu, Xiaojun
   Zhang, Yumei
   Li, Peng
   Su, Yuping
TI HFA-GTNet: Hierarchical Fusion Adaptive Graph Transformer network for
   dance action recognition
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Dance action recognition; Self-built dance datasets; Hierarchical
   Spatial Attention; Temporal Fusion Attention; Adaptive Component
AB Dance action recognition is a hot research topic in computer vision. However, current skeleton -based action recognition methods face difficulties in capturing the adequate spatial structure and temporal variations of dance actions, resulting in lower recognition accuracy. In this paper, we propose a Hierarchical Fusion Adaptive Graph Transformer network (HFA-GTNet) for dance action recognition. A Hierarchical Spatial Attention (HSAtt) module is designed to extract different levels of spatial feature information from joint to parts to group, it can effectively learn high -order dependency relationships from local joints to global poses in dance actions. Secondly, to extract the joint variations in dance actions at different speeds, we have designed a Temporal Fusion Attention (TFAtt) module. This module learns the short-term and long-term temporal dependencies among joints across frames. Additionally, to capture the variations in motion patterns and dance styles among different dancers, we introduce an Adaptive Component (AdaptC). Finally, we evaluate our model on two selfbuilt dance datasets, MSDanceAction and InDanceAction, and demonstrate its superior performance compared to other state-of-the-art methods in dance action recognition.
C1 [Jia, Ru; Zhao, Li; Yang, Rui; Yang, Honghong; Wu, Xiaojun; Zhang, Yumei; Li, Peng; Su, Yuping] Shaanxi Normal Univ, Sch Comp Sci, Xian 710062, Peoples R China.
   [Yang, Honghong; Wu, Xiaojun; Zhang, Yumei; Li, Peng; Su, Yuping] Minist Educ, Key Lab Modern Teaching Technol, Xian 710062, Peoples R China.
   [Jia, Ru; Zhao, Li; Yang, Rui; Yang, Honghong; Wu, Xiaojun; Zhang, Yumei; Li, Peng; Su, Yuping] Minist Culture & Tourism, Key Lab Intelligent Comp & Serv Technol Folk Song, Xian 710119, Peoples R China.
C3 Shaanxi Normal University
RP Wu, XJ (corresponding author), Shaanxi Normal Univ, Sch Comp Sci, Xian 710062, Peoples R China.
EM xjwu@snnu.edu.cn
RI Su, Yu-Ping/J-7534-2012
OI Zhao, Li/0009-0009-3093-6858
FU National Natural Science Foundation of China [61907028, 62377034,
   11872036]; Young science and technology stars in Shaanxi Province, China
   [2021KJXX-91]; Fundamental Research Funds for the Central Uni-versities
   of China [GK202101004]; Shaanxi Key Science and Technology Innovation
   Team Project, China [2022TD-26]; Key Laboratory of the Ministry of
   Culture and Tourism, China [2023-02]
FX <B>Acknowledgments</B> This work was partially supported the National
   Natural Science Foundation of China (No. 61907028, No. 62377034, No.
   11872036) , the Young science and technology stars in Shaanxi Province,
   China (2021KJXX-91) , the Fundamental Research Funds for the Central
   Uni-versities of China under Grant (No. GK202101004) , the Shaanxi Key
   Science and Technology Innovation Team Project, China (No. 2022TD-26) ,
   and the Key Laboratory of the Ministry of Culture and Tourism, China
   (No. 2023-02) .
CR Ahmad T, 2023, J VIS COMMUN IMAGE R, V95, DOI 10.1016/j.jvcir.2023.103892
   Barkoky A, 2022, J VIS COMMUN IMAGE R, V82, DOI 10.1016/j.jvcir.2021.103371
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chaquet JM, 2013, COMPUT VIS IMAGE UND, V117, P633, DOI 10.1016/j.cviu.2013.01.013
   Chen J, 2021, IMAGE VISION COMPUT, V112, DOI 10.1016/j.imavis.2021.104214
   Chen XY, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103707
   Chi HG, 2022, PROC CVPR IEEE, P20154, DOI 10.1109/CVPR52688.2022.01955
   Ding WW, 2022, J VIS COMMUN IMAGE R, V83, DOI 10.1016/j.jvcir.2021.103410
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Houlsby N, 2019, PR MACH LEARN RES, V97
   Huang HE, 2020, J VIS COMMUN IMAGE R, V73, DOI 10.1016/j.jvcir.2020.102925
   Kishore PVV, 2018, ADV MULTIMED, V2018, DOI 10.1155/2018/5141402
   Kitsikidis A, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 2, P789
   Kong Y, 2022, INT J COMPUT VISION, V130, P1366, DOI 10.1007/s11263-022-01594-9
   Korban Matthew, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P761, DOI 10.1007/978-3-030-58565-5_45
   Lee J, 2023, IEEE I CONF COMP VIS, P10410, DOI 10.1109/ICCV51070.2023.00958
   Li C, 2018, Arxiv, DOI arXiv:1804.06055
   Li TJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13414, DOI 10.1109/ICCV48922.2021.01318
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu YA, 2022, KNOWL-BASED SYST, V240, DOI 10.1016/j.knosys.2022.108146
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Ou YJ, 2023, J VIS COMMUN IMAGE R, V93, DOI 10.1016/j.jvcir.2023.103804
   Pareek P, 2021, ARTIF INTELL REV, V54, P2259, DOI 10.1007/s10462-020-09904-8
   Plizzari Chiara, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12663), P694, DOI 10.1007/978-3-030-68796-0_50
   Protopapadakis E, 2017, INT ARCH PHOTOGRAMM, V42-2, P587, DOI 10.5194/isprs-archives-XLII-2-W3-587-2017
   Qiu HL, 2022, Arxiv, DOI arXiv:2201.02849
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Sharma G, 2012, PROC CVPR IEEE, P3506, DOI 10.1109/CVPR.2012.6248093
   Shi L, 2020, IEEE T IMAGE PROCESS, V29, P9532, DOI 10.1109/TIP.2020.3028207
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Shi Lei, 2020, P AS C COMP VIS
   Shu XB, 2022, IEEE T PATTERN ANAL, V44, P3300, DOI 10.1109/TPAMI.2021.3050918
   Shu XB, 2021, IEEE T PATTERN ANAL, V43, P1110, DOI 10.1109/TPAMI.2019.2942030
   Si CY, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107511
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun Y, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21165339
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Xie J, 2021, NEUROCOMPUTING, V440, P230, DOI 10.1016/j.neucom.2021.02.001
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang H, 2022, IEEE T IMAGE PROCESS, V31, P164, DOI 10.1109/TIP.2021.3129117
   Yang HH, 2022, CHINESE J ELECTRON, V31, P266, DOI 10.1049/cje.2020.00.007
   Yang JY, 2021, IEEE T MULTIMEDIA, V23, P883, DOI 10.1109/TMM.2020.2990082
   Yao BP, 2010, PROC CVPR IEEE, P9, DOI 10.1109/CVPR.2010.5540234
   Zhou YX, 2023, Arxiv, DOI arXiv:2211.09590
   Zhu FL, 2021, TRAIT SIGNAL, V38, P529, DOI 10.18280/ts.380233
NR 46
TC 0
Z9 0
U1 6
U2 6
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104038
DI 10.1016/j.jvcir.2023.104038
EA JAN 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GR6O6
UT WOS:001154441300001
DA 2024-08-05
ER

PT J
AU Kumar, DA
   Kishore, PVV
   Chaithanya, TR
   Sravani, K
AF Kumar, D. Anil
   Kishore, P. V. V.
   Chaithanya, T. R.
   Sravani, K.
TI Multi frame multi-head attention learning on deep features for
   recognizing Indian classical dance poses
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Deep learning; Indian classical dance; Self-attention; Convolutional
   neural networks; Computers in art
AB The aim of this work is to develop a classifier for the Indian classical dance (ICD) online Bharatanatyam videos using deep learning techniques. Bharatanatyam is the most ancient and popular of all eight types of Indian classical dance forms recognized by the government of India. Many ICD enthusiasts struggle to synchronize their minds to the lyrics and complex dance poses during live performances. Therefore, they record performances on their mobile devices and review them later to improve their performance. The goal of this work is to help dance lovers enjoy live performances by using an AI -based dance pose recognizer. Previous ICD learning models have demonstrated that global feature representations of complex dance poses in video sequences with unpredictable background changes have unreliable performance metrics. Therefore, these models developed their own small dance datasets with few dancers posing under controlled lighting and backgrounds. However, these models have strained performance on online video data due to background, viewpoint, costume, lighting, blurring, and pose -to -background ratios. To overcome these challenges, this work proposes a multi -frame multi -head attention network that operates on multiple frames averaging on attention scores from multiple heads to improve the strained performance of deep learning models on online video data. Specifically, the multi -frame multi -head layer attention (MFMHLA) improves the random pixel distributions of the dancer in the ICD online videos by learning object relationships across frames. The MFMHLA layer works on a set of layered features across three consecutive frames per head to learn attention locations in the central frame. Similarly, multiple heads are employed to extract attention maps from overlapping consecutive frames which are further averaged to generate a semantic relational feature map. Subsequently, these averaged maps are generated at multiple feature resolutions across the ResNet-50 architecture. This results in a chronological enhancement of the dancer's pose at multiple resolutions across the depth of features in ResNet-50 architecture along with Inception, VGG, and CapsuleNet. The experiments were conducted on our online Bharatanatyam ICD(BOICDVD22) with 10 songs. The results conclude that the presence of MFMHLA has improved pose feature representations of online dance videos burdened with deformations.
C1 [Kumar, D. Anil; Sravani, K.] PACE Inst Technol & Sci, Dept Elect & Commun Engn, Ongole 523272, Andhra Pradesh, India.
   [Kishore, P. V. V.] KL Deemed Univ, Dept Elect & Commun Engn, Vaddeswaram 522302, Andhra Pradesh, India.
   [Chaithanya, T. R.] PACE Inst Technol & Sci, Dept Comp Sci & Engn, Ongole 523272, Andhra Pradesh, India.
RP Kishore, PVV (corresponding author), KL Deemed Univ, Dept Elect & Commun Engn, Vaddeswaram 522302, Andhra Pradesh, India.
EM pvvkishore@kluniversity.in
RI Kishore, P.V.V./R-3293-2017
OI Kishore, P.V.V./0000-0002-3247-3043
FU Department of Science and Technology, SERB division, SRG scheme, India
   [SERB-SRG/2021/001637]
FX This work was supported by the Department of Science and Technology,
   SERB division, SRG scheme, India, under Grant SERB-SRG/2021/001637. The
   dataset was made available from Biomechanics and Vision Computing
   research Laboratory, Koneru Lakshmaiah Education Foundation.
CR Arpitha D., 2022, 2022 4th International Conference on Smart Systems and Inventive Technology (ICSSIT), P885, DOI 10.1109/ICSSIT53264.2022.9716486
   Biswas S, 2021, 2021 SIXTH INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS, SIGNAL PROCESSING AND NETWORKING (WISPNET), P278, DOI [10.1109/WiSPNET51692.2021.9419426, 10.1109/WISPNET51692.2021.9419426]
   Challapalli JR, 2022, KNOWL INF SYST, V64, P2411, DOI 10.1007/s10115-022-01707-3
   Chan SX, 2022, IEEE T IMAGE PROCESS, V31, P1882, DOI 10.1109/TIP.2022.3148876
   Chen Y, 2022, PATTERN RECOGN LETT, V157, P90, DOI 10.1016/j.patrec.2022.03.020
   Das S, 2020, ADV INTELL SYST COMP, V1120, P83, DOI 10.1007/978-981-15-2449-3_6
   Devi M, 2016, 2016 INTERNATIONAL CONFERENCE ON ACCESSIBILITY TO DIGITAL WORLD (ICADW), P193, DOI 10.1109/ICADW.2016.7942540
   Dewan S, 2018, PROC SPIE, V10696, DOI 10.1117/12.2309445
   Fourie M, 2019, LECT NOTES ELECTR EN, V514, P317, DOI 10.1007/978-981-13-1056-0_33
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jain N, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11146253
   JishaRaj R, 2022, P INT C COMP INT SUS, P181
   Kale MR, 2019, ANNU IEEE IND CONF, DOI 10.1109/indicon47234.2019.9029006
   Kapsouras I., 2013, P 6 BALKAN C INFORMA, DOI [10.1145/2490257.2490271, DOI 10.1145/2490257.2490271]
   Kishore PVV, 2018, ADV MULTIMED, V2018, DOI 10.1155/2018/5141402
   Kumar KVV, 2018, SMART INNOV SYST TEC, V77, P659, DOI 10.1007/978-981-10-5544-7_65
   Kumar KVV, 2017, MATH PROBL ENG, V2017, DOI 10.1155/2017/6204742
   Liaqat S, 2021, IEEE SENS J, V21, P9515, DOI 10.1109/JSEN.2021.3055898
   Mohanty A, 2016, SIGNAL PROCESS-IMAGE, V47, P529, DOI 10.1016/j.image.2016.05.019
   Naik Ashwini Dayanand, 2021, Computational Vision and Bio-Inspired Computing. ICCVBIC 2020. Advances in Intelligent Systems and Computing (AISC 1318), P81, DOI 10.1007/978-981-33-6862-0_7
   Naik Ashwini Dayanand, 2020, 2020 Proceedings of the International Conference on Communication and Signal Processing (ICCSP), P1245, DOI 10.1109/ICCSP48568.2020.9182365
   Pervaiz N, 2023, VISUAL COMPUT, V39, P4087, DOI 10.1007/s00371-022-02577-0
   podium, Bharatanatyam Songs, popular songs source
   Raj RJ, 2023, VISUAL COMPUT, V39, P4049, DOI 10.1007/s00371-022-02572-5
   Rani C.J., 2023, Computational Intelligence and Data Analytics, P241
   Saha S, 2013, 2013 FIFTH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE, COMMUNICATION SYSTEMS AND NETWORKS (CICSYN), P3, DOI 10.1109/CICSYN.2013.11
   Samanta S., 2012, 2012 IEEE Workshop on Applications of Computer Vision (WACV), P265, DOI 10.1109/WACV.2012.6163050
   Samanta S, 2014, INT C PATT RECOG, P4507, DOI 10.1109/ICPR.2014.771
   Shailesh S, 2022, ENTERTAIN COMPUT, V42, DOI 10.1016/j.entcom.2022.100484
   Shailesh S., 2021, Data Engineering and Communication Technology. Proceedings of ICDECT 2020. Lecture Notes on Data Engineering and Communications Technologies (LNDECT 63), P29, DOI 10.1007/978-981-16-0081-4_4
   Shubhangi, 2017, LECT NOTES COMPUT SC, V10127, P67, DOI 10.1007/978-3-319-52503-7_6
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun YF, 2019, PROC CVPR IEEE, P393, DOI 10.1109/CVPR.2019.00048
   Tan HC, 2023, IEEE T NEUR NET LEAR, V34, P8210, DOI 10.1109/TNNLS.2022.3144163
   Voulodimos A, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/7068349
   Wang X., 2021, J. Ambient Intell. Humaniz. Comput., P1
   Wang YM, 2018, PROC CVPR IEEE, P4148, DOI 10.1109/CVPR.2018.00436
   youtube, youtube Bharatanatyam Songs
   Yuan Y, 2021, IEEE T CYBERNETICS, V51, P3562, DOI 10.1109/TCYB.2019.2931735
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zheng M, 2018, IEEE COMPUT SOC CONF, P1974, DOI 10.1109/CVPRW.2018.00251
   Zhou YL, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22166293
NR 42
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104091
DI 10.1016/j.jvcir.2024.104091
EA FEB 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NF7J0
UT WOS:001199100000001
DA 2024-08-05
ER

PT J
AU Yu, P
   Duan, ZL
   Guan, SJ
   Li, M
   Deng, SB
AF Yu, Peng
   Duan, Zhuolei
   Guan, Sujie
   Li, Min
   Deng, Shaobo
TI UnifiedTT: Visual tracking with unified transformer
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Target tracking; Feature extraction; Convolutional neural network;
   Transformer; Siamese network
AB Target tracking is an important research task in computer vision. Existing tracking algorithms based on Siamese networks often suffer from the problem of information redundancy between adjacent frames and a lack of ability to capture global dependencies. When similar backgrounds appear around the target, the tracking performance usually significantly decreases. Although target tracking algorithms based on deep convolution and the Transformer have partially addressed these issues, achieving a good balance between the two remains a challenge. In this work, we propose a unified convolution and self-attention Siamese network for target tracking. By utilizing a feature extraction backbone network based on integrated convolution and self-attention styles, we are able to capture globally important regions and key frames while greatly reducing local redundant computations, thereby improving tracking performance. We apply this approach to the task of target tracking and enhance the feature extraction capabilities of both the target template and search template. Experimental results show that our proposed tracking algorithm outperforms some recent classical tracking algorithms, especially achieving improvements of 10.7 % on the high-diversity dataset GOT-10K and 24.7 % on the large-scale and high-quality LaSOT dataset.
C1 [Yu, Peng; Duan, Zhuolei; Guan, Sujie; Li, Min; Deng, Shaobo] Nanchang Inst Technol, Sch Informat Engn, Nanchang 330099, Peoples R China.
C3 Nanchang Institute Technology
RP Deng, SB (corresponding author), Nanchang Inst Technol, Sch Informat Engn, Nanchang 330099, Peoples R China.
EM houjiyuan@nit.edu.cn
FU Natural Science Fund project of Department of Jiangxi Province Science
   and Technology [20224BAB202014]; Science and Technology Project of
   Jiangxi Provincial Education Department [GJJ211921, GJJ201917,
   GJJ190941]; National Science Foundation of China [61763032, 61562061]
FX The project is funded in part by Natural Science Fund project of
   Department of Jiangxi Province Science and Technology (No.
   20224BAB202014), the Science and Technology Project of Jiangxi
   Provincial Education Department (Nos. GJJ211921, GJJ201917 and
   GJJ190941), and the National Science Foundation of China (Nos. 61763032
   and 61562061).
CR Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Dai Z, 2021, ADV NEUR IN, V34
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Du F, 2020, PROC CVPR IEEE, P6835, DOI 10.1109/CVPR42600.2020.00687
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Feng M., 2023, IEEE Transactions on Multimedia
   Fu ZH, 2021, PROC CVPR IEEE, P13769, DOI 10.1109/CVPR46437.2021.01356
   Guo DY, 2021, PROC CVPR IEEE, P9538, DOI 10.1109/CVPR46437.2021.00942
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang LH, 2020, AAAI CONF ARTIF INTE, V34, P11037
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Jing YC, 2023, PROC CVPR IEEE, P24345, DOI 10.1109/CVPR52729.2023.02332
   Jing YC, 2022, LECT NOTES COMPUT SC, V13667, P111, DOI 10.1007/978-3-031-20071-7_7
   Jing YC, 2021, PROC CVPR IEEE, P15704, DOI 10.1109/CVPR46437.2021.01545
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li KC, 2022, Arxiv, DOI arXiv:2201.09450
   Lin Liting, 2022, ADV NEUR IN
   Liu S., 2022, arXiv
   Liu SH, 2023, PROC CVPR IEEE, P3759, DOI 10.1109/CVPR52729.2023.00366
   Liu Z, 2022, PROC CVPR IEEE, P3192, DOI 10.1109/CVPR52688.2022.00320
   Ma F, 2022, PROC CVPR IEEE, P8771, DOI 10.1109/CVPR52688.2022.00858
   Mayer C, 2022, PROC CVPR IEEE, P8721, DOI 10.1109/CVPR52688.2022.00853
   Meinhardt T, 2022, PROC CVPR IEEE, P8834, DOI 10.1109/CVPR52688.2022.00864
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shen QH, 2022, PROC CVPR IEEE, P8091, DOI 10.1109/CVPR52688.2022.00793
   Sun C, 2018, PROC CVPR IEEE, P8962, DOI 10.1109/CVPR.2018.00934
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661
   Wang GT, 2020, PROC CVPR IEEE, P6287, DOI 10.1109/CVPR42600.2020.00632
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xie F, 2021, IEEE INT CONF COMP V, P2688, DOI 10.1109/ICCVW54120.2021.00303
   Yang XY, 2023, PROC CVPR IEEE, P22552, DOI 10.1109/CVPR52729.2023.02160
   Yang XY, 2022, LECT NOTES COMPUT SC, V13694, P73, DOI 10.1007/978-3-031-19830-4_5
   Yang XY, 2022, Arxiv, DOI arXiv:2210.17409
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhao MJ, 2021, Arxiv, DOI [arXiv:2105.03817, DOI 10.48550/ARXIV.2105.03817]
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
   Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7
NR 48
TC 0
Z9 0
U1 10
U2 10
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104067
DI 10.1016/j.jvcir.2024.104067
EA FEB 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KG3M8
UT WOS:001178766200001
DA 2024-08-05
ER

PT J
AU Xiong, QM
   Gao, ZR
   Ma, JY
   Ma, Y
AF Xiong, Qiming
   Gao, Zhirong
   Ma, Jiayi
   Ma, Yong
TI Multi-image super-resolution based low complexity deep network for image
   compressive sensing reconstruction☆
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Compressive sensing reconstruction; Deep learning; Grouping initial
   reconstruction; Image super-resolution; Low complexity
ID ALGORITHM
AB Deep learning (DL) has been widely utilized in image compressive sensing (CS) to enhance the quality and speed of reconstruction. The typical deep network for CS reconstruction comprises an initial reconstruction subnetwork, followed by a cascaded deep refinement reconstruction subnetworks. This paper introduces a new low-complexity image CS deep reconstruction framework, GSRCS, which leverages multi-image based deep super-resolution technology to better address the cost constraints of practical applications. The proposed initial reconstruction module generates multiple low-resolution images in parallel by grouping the input measurements, while a high-quality, high-resolution reconstructed image is produced through a multi-image deep super-resolution network. The theoretical derivation and experimental results demonstrate that this method significantly reduces system complexity in terms of parameters and floating-point arithmetic operations, while achieving competitive reconstruction performance compared to the state-of-the-arts. Specifically, the average number of parameters is reduced by over 63%, and the computational complexity is decreased by more than 88%.
C1 [Xiong, Qiming; Ma, Jiayi; Ma, Yong] Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.
   [Gao, Zhirong] South Cent Minzu Univ, Coll Comp Sci, Wuhan 430074, Peoples R China.
C3 Wuhan University; South Central Minzu University
RP Gao, ZR (corresponding author), South Cent Minzu Univ, Coll Comp Sci, Wuhan 430074, Peoples R China.
EM qmxiong@foxmail.com; gaozhirong@mail.scuec.edu.cn; jyma2010@gmail.com;
   jyma2010@gmail.com
RI Ma, Jiayi/Y-2470-2019
OI Ma, Jiayi/0000-0003-3264-3265
FU Fundamental Research Funds for Central University, South-Central MinZu
   University [CZZ21013]; Project of State Key Laboratory of Multispectral
   Information Processing Technology [6142113210303]
FX This work has been supported by the Fundamental Research Funds for
   Central University, South-Central MinZu University under grant CZZ21013
   and the Project of State Key Laboratory of Multispectral Information
   Processing Technology under grant 6142113210303. The authors express
   their gratitude to the reviewers for their valuable comments on the
   paper, which have significantly contributed to enhancement of its
   quality.
CR Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Canh TN, 2021, IEEE T COMPUT IMAG, V7, P86, DOI 10.1109/TCI.2020.3034433
   Chen WJ, 2022, INT CONF ACOUST SPEE, P2460, DOI 10.1109/ICASSP43922.2022.9746648
   Chien JT, 2018, IEEE T PATTERN ANAL, V40, P318, DOI 10.1109/TPAMI.2017.2677439
   Croitoru FA, 2023, IEEE T PATTERN ANAL, V45, P10850, DOI 10.1109/TPAMI.2023.3261988
   Deng X, 2020, IEEE T IMAGE PROCESS, V29, P1683, DOI 10.1109/TIP.2019.2944270
   Dong WS, 2014, IEEE T IMAGE PROCESS, V23, P3618, DOI 10.1109/TIP.2014.2329449
   Donoho DL, 2009, P NATL ACAD SCI USA, V106, P18914, DOI 10.1073/pnas.0909892106
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Duarte MF, 2008, IEEE SIGNAL PROC MAG, V25, P83, DOI 10.1109/MSP.2007.914730
   Fowler JE, 2011, EUR SIGNAL PR CONF, P564
   Gan L, 2007, PROCEEDINGS OF THE 2007 15TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, P403
   Gao Z, 2018, IEEE WIREL COMMUN, V25, P144, DOI 10.1109/MWC.2017.1700147
   Gao ZR, 2019, IEEE IMAGE PROC, P2095, DOI [10.1109/icip.2019.8803124, 10.1109/ICIP.2019.8803124]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hershey J.R., 2014, Comput. Sci.
   Huang G, 2018, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2018.00291
   Jing YC, 2023, PROC CVPR IEEE, P24345, DOI 10.1109/CVPR52729.2023.02332
   Jing YC, 2021, PROC CVPR IEEE, P15704, DOI 10.1109/CVPR46437.2021.01545
   Kalipatnapu S, 2020, IEEE T VLSI SYST, V28, P1283, DOI 10.1109/TVLSI.2020.2967477
   Kelkar VA, 2021, IEEE T COMPUT IMAG, V7, P209, DOI [10.1109/TCI.2021.3049648, 10.1109/tci.2021.3049648]
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kulkarni K, 2016, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2016.55
   Lei S, 2017, IEEE GEOSCI REMOTE S, V14, P1243, DOI 10.1109/LGRS.2017.2704122
   Li C., 2010, EFFICIENT ALGORITHM
   Li CB, 2013, IEEE T BROADCAST, V59, P197, DOI 10.1109/TBC.2012.2226509
   Li YH, 2021, IEEE T IMAGE PROCESS, V30, P4840, DOI 10.1109/TIP.2021.3076285
   Liu S., 2022, NeurIPS, V35, P1100
   Liu S, 2021, NEUROCOMPUTING, V449, P357, DOI 10.1016/j.neucom.2021.03.124
   Liu SH, 2023, PROC CVPR IEEE, P3759, DOI 10.1109/CVPR52729.2023.00366
   Lucas A, 2018, IEEE SIGNAL PROC MAG, V35, P20, DOI 10.1109/MSP.2017.2760358
   Lustig M, 2008, IEEE SIGNAL PROC MAG, V25, P72, DOI 10.1109/MSP.2007.914728
   Ma Q, 2022, IEEE T IMAGE PROCESS, V31, P2950, DOI 10.1109/TIP.2022.3161834
   Ma XY, 2023, Arxiv, DOI arXiv:2312.00858
   Marivani I, 2020, IEEE T IMAGE PROCESS, V29, P8443, DOI 10.1109/TIP.2020.3014729
   Metzler CA, 2016, IEEE T INFORM THEORY, V62, P5117, DOI 10.1109/TIT.2016.2556683
   Mousavi A, 2015, ANN ALLERTON CONF, P1336, DOI 10.1109/ALLERTON.2015.7447163
   Ning Q, 2021, IEEE J-STSP, V15, P240, DOI 10.1109/JSTSP.2020.3037516
   Ravishankar S, 2020, P IEEE, V108, P86, DOI 10.1109/JPROC.2019.2936204
   Shi WZ, 2020, IEEE T IMAGE PROCESS, V29, P375, DOI 10.1109/TIP.2019.2928136
   Stanislaus J. L. V. M., 2013, 2013 International Conference on Computing, Networking and Communications (ICNC 2013), P671, DOI 10.1109/ICCNC.2013.6504167
   Tang LF, 2022, IEEE-CAA J AUTOMATIC, V9, P2121, DOI 10.1109/JAS.2022.106082
   Torkamani R, 2021, SIGNAL PROCESS-IMAGE, V95, DOI 10.1016/j.image.2021.116212
   Wang G, 2018, IEEE T MED IMAGING, V37, P1289, DOI 10.1109/TMI.2018.2833635
   Wang G, 2016, IEEE ACCESS, V4, P8914, DOI 10.1109/ACCESS.2016.2624938
   Wang XJ, 2019, PROC CVPR IEEE, P9041, DOI 10.1109/CVPR.2019.00926
   Wang XY, 2021, SIGNAL PROCESS-IMAGE, V95, DOI 10.1016/j.image.2021.116246
   Wang XY, 2023, IEEE-CAA J AUTOMATIC, V10, P78, DOI 10.1109/JAS.2022.105914
   Wang XY, 2022, INFORM FUSION, V79, P188, DOI 10.1016/j.inffus.2021.10.005
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Wu Y, 2019, PR MACH LEARN RES, V97
   Xi M, 2016, IEEE IMAGE PROC, P3224, DOI 10.1109/ICIP.2016.7532955
   Xiong CY, 2021, APPL INTELL, V51, P935, DOI 10.1007/s10489-020-01869-z
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Yang X., 2023, P IEEECVF INT C COMP, P18938
   Yang X., 2022, Adv. Neural Inf. Process. Syst., V35, P25739
   Yang XY, 2022, LECT NOTES COMPUT SC, V13694, P73, DOI 10.1007/978-3-031-19830-4_5
   Yang Y, 2020, IEEE T PATTERN ANAL, V42, P521, DOI 10.1109/TPAMI.2018.2883941
   Yao HT, 2019, NEUROCOMPUTING, V359, P483, DOI 10.1016/j.neucom.2019.05.006
   Zammit J, 2020, IEEE ACCESS, V8, P120999, DOI 10.1109/ACCESS.2020.3006861
   Zhang J, 2020, IEEE J-STSP, V14, P765, DOI 10.1109/JSTSP.2020.2977507
   Zhang J, 2018, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR.2018.00196
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang K, 2020, PROC CVPR IEEE, P3214, DOI 10.1109/CVPR42600.2020.00328
   Zhang MH, 2021, SIGNAL PROCESS-IMAGE, V99, DOI 10.1016/j.image.2021.116449
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang ZH, 2021, IEEE T IMAGE PROCESS, V30, P1487, DOI 10.1109/TIP.2020.3044472
   Zhou SW, 2021, IEEE T MULTIMEDIA, V23, P2627, DOI 10.1109/TMM.2020.3014561
NR 72
TC 0
Z9 0
U1 9
U2 9
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104071
DI 10.1016/j.jvcir.2024.104071
EA JAN 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JH6F7
UT WOS:001172306000001
OA hybrid
DA 2024-08-05
ER

PT J
AU Liu, C
AF Liu, Chang
TI Fast HEVC inter-frame coding based on LSTM neural network technology
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE HEVC; Fast coding; Inter -frame coding; LSTM; Neural network
ID DECISION
AB High Efficiency Video Coding (HEVC) is the most commonly used video coding standard. However, its high coding complexity is a heavy burden for real-time video applications. But, coding tools designed based on traditional coding frameworks have reached limits. Furthermore, existing low-complexity video coding methods have not thoroughly analyzed the characteristics of compressed video, making it impossible to develop targeted models to reduce coding complexity. Therefore, in this research, a fast HEVC inter-frame coding technique is proposed. Firstly, we perform a characteristics analysis of HEVC inter-frame coding to explore the correlation between video frames. Secondly, we develop an Inter-Frame Feature Transfer-Long Short-Term Memory (IFFT-LSTM) model to obtain the optimal coding tree unit (CTU) partition structure. Thirdly, we embed the IFFT-LSTM model into the HEVC test platform. The experimental results show that suggested method can effectively reduce the HEVC inter-frame coding complexity with a small amount of rate-distortion (RD) performance loss while maintaining acceptable subjective video quality.
C1 [Liu, Chang] Nantong Univ, Res Ctr Intelligent Informat Technol, Nantong 226019, Peoples R China.
C3 Nantong University
RP Liu, C (corresponding author), Nantong Univ, Res Ctr Intelligent Informat Technol, Nantong 226019, Peoples R China.
EM liuchang@ntu.edu.cn
FU Natural Science Research Program for Higher Education in Jiangsu
   Province [23KJB520030]
FX This paper is supported by the Natural Science Research Program for
   Higher Education in Jiangsu Province under Grant No. 23KJB520030.
CR Bjontegaard G., 2001, Calculation of Average PSNR Differences between RDcurves
   Bross B, 2021, IEEE T CIRC SYST VID, V31, P3736, DOI 10.1109/TCSVT.2021.3101953
   Ding DD, 2021, P IEEE, V109, P1494, DOI 10.1109/JPROC.2021.3059994
   Ender JHG, 2010, SIGNAL PROCESS, V90, P1402, DOI 10.1016/j.sigpro.2009.11.009
   Ericsson, 2022, Ericsson mobility report
   Feng ZQ, 2018, IEEE ACCESS, V6, P45262, DOI 10.1109/ACCESS.2018.2864881
   JCT-VC: Joint Collaborative Team on Video Coding, 2014, HM software
   Jung SH, 2016, IEEE T CIRC SYST VID, V26, P1846, DOI 10.1109/TCSVT.2015.2473303
   Kim HS, 2016, IEEE T CIRC SYST VID, V26, P130, DOI 10.1109/TCSVT.2015.2444672
   Kim N, 2016, IEEE INT SYM BROADB
   Li TY, 2017, IEEE INT CON MULTI, P1255, DOI 10.1109/ICME.2017.8019316
   Liu XG, 2019, IEEE T CIRC SYST VID, V29, P144, DOI 10.1109/TCSVT.2017.2777903
   Ma SW, 2020, IEEE T CIRC SYST VID, V30, P1683, DOI 10.1109/TCSVT.2019.2910119
   Ma ZF, 2023, APPL SOFT COMPUT, V147, DOI 10.1016/j.asoc.2023.110731
   Mallikarachchi T, 2018, IEEE T CIRC SYST VID, V28, P693, DOI 10.1109/TCSVT.2016.2619499
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shen LQ, 2014, IEEE T IMAGE PROCESS, V23, P4232, DOI 10.1109/TIP.2014.2341927
   Sherstinsky A, 2020, PHYSICA D, V404, DOI 10.1016/j.physd.2019.132306
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tan HL, 2016, IEEE T BROADCAST, V62, P128, DOI 10.1109/TBC.2015.2505406
   Wang T, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23187923
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xing FC, 2022, J VIS COMMUN IMAGE R, V82, DOI 10.1016/j.jvcir.2021.103374
   Xu M, 2018, IEEE T IMAGE PROCESS, V27, P5044, DOI 10.1109/TIP.2018.2847035
   Zhu LW, 2017, IEEE T BROADCAST, V63, P547, DOI 10.1109/TBC.2017.2711142
NR 25
TC 0
Z9 0
U1 3
U2 3
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104056
DI 10.1016/j.jvcir.2024.104056
EA JAN 2024
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IL1D6
UT WOS:001166380000001
DA 2024-08-05
ER

PT J
AU Wang, GX
   Wang, HK
   Li, H
   Yu, L
   Yin, HB
   Xu, HF
   Ye, Z
   Song, JF
AF Wang, Guoxiang
   Wang, Hongkui
   Li, Hui
   Yu, Li
   Yin, Haibing
   Xu, Haifeng
   Ye, Zhen
   Song, Junfeng
TI A survey on just noticeable distortion estimation and its applications
   in video coding
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Review
DE JND estimation; JND-based perceptual video coding; Human visual system
ID JND MODEL; BIT ALLOCATION; DIFFERENCE; PROFILE; OPTIMIZATION; LEVEL;
   PREDICTION; VISIBILITY; IMAGES
AB With the developing explosion in video data delivery, perceptual video coding (PVC) plays an increasingly significant role in video compression. The just noticeable distortion (JND) reflects the tolerance limit of human visual system (HVS) to coding distortion directly, resulting in that JND-based PVC is the most important branch of video coding. This paper provides an extensive overview of JND estimation and JND-based PVC, so as to make interested readers aware of the status quo. The main contribution of this article can be briefly outlined as follows. Firstly, the general description of JND concepts and most existing computation models for JND are to be reviewed systematically. Secondly, most related works about JND-based perceptual image and video coding schemes are introduced, including JND-based coding preprocessing and JND-based codec embedding. Thirdly, in addition to a thorough summary of JND estimation and JND-based PVC, possible future directions and opportunities are analyzed and discussed.
C1 [Wang, Guoxiang; Ye, Zhen; Song, Junfeng] Lishui Univ, Lishui, Peoples R China.
   [Wang, Hongkui] Hangzhou Dianzi Univ, Sch Commun Engn, Hangzhou, Peoples R China.
   [Wang, Hongkui; Yin, Haibing; Xu, Haifeng] Hangzhou Dianzi Univ, Lishui Inst, Hangzhou, Peoples R China.
   [Yu, Li] Huazhong Univ Sci & Technol, Coll Elect Informat & Commun, Wuhan, Peoples R China.
   [Li, Hui] ZheJiang Dahua Technol CO LTD, Hangzhou, Peoples R China.
C3 Lishui University; Hangzhou Dianzi University; Hangzhou Dianzi
   University; Huazhong University of Science & Technology
RP Wang, HK (corresponding author), Hangzhou Dianzi Univ, Sch Commun Engn, Hangzhou, Peoples R China.; Wang, HK (corresponding author), Hangzhou Dianzi Univ, Lishui Inst, Hangzhou, Peoples R China.
EM wanggx_bupt@163.com; wanghk@hdu.edu.cn; li_hui@dahuatech.com;
   hustlyu@hust.edu.cn; haibingyin@163.com; xuhaifeng@hdu.edu.cn;
   yezhen@lsu.edu.cn; dachan@126.com
RI Guoxiang, Wang/KVB-0433-2024; Ye, Zhen/KPY-2852-2024
OI Guoxiang, Wang/0000-0002-4602-0598; 
CR Ahumada A.J., 1997, Luminance-model-based DCT quantization for color image compression
   Azevedo R., 2019, INT WORKSHOP IMMERSI
   Bae SH, 2017, IEEE T CIRC SYST VID, V27, P1196, DOI 10.1109/TCSVT.2016.2539862
   Bae SH, 2016, IEEE T IMAGE PROCESS, V25, P3343, DOI 10.1109/TIP.2016.2568459
   Bae SH, 2014, IEEE T IMAGE PROCESS, V23, P3227, DOI 10.1109/TIP.2014.2327808
   Bae SH, 2013, IEEE SIGNAL PROC LET, V20, P893, DOI 10.1109/LSP.2013.2272193
   Barlow H.B., 2012, Possible Principles Underlying the Transformation of Sensory Messages
   Bondzulic B, 2021, ACTA POLYTECH HUNG, V18, P201
   Brass B., 2018, Document JVET-K1001 of JVET
   Chen Z., 2010, E Trans. Grrulls Syst. Video Teshoud, V20, P806
   Chen ZZ, 2010, IEEE T CIRC SYST VID, V20, P806, DOI 10.1109/TCSVT.2010.2045912
   Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154
   Chin YJ., 1999, BEE Tam. Circuits Syst. Viln Textind., V9, P438
   Cui J, 2019, IEEE DATA COMPR CONF, P565, DOI 10.1109/DCC.2019.00077
   De Silva DVSX, 2011, IEEE J-STSP, V5, P335, DOI 10.1109/JSTSP.2011.2108113
   Ding L, 2015, PROC SPIE, V9410, DOI 10.1117/12.2083818
   DoVale E., 2017, SMPTE Motion Imaging J., V126, P41, DOI [10.5594/JMI.2017.2749919, 10.5594/jmi.2017.2749919, DOI 10.5594/JMI.2017.2749919]
   Fan C., 2019, 11 INT C QUALITY MUL
   Fan CL, 2021, IEEE MULTIMEDIA, V28, P8, DOI 10.1109/MMUL.2021.3060831
   Fan CL, 2019, J VIS COMMUN IMAGE R, V62, P140, DOI 10.1016/j.jvcir.2019.04.016
   Feng HC, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2014.2384273
   Friston Fristras K.I., Nat Rex N, V11, P127
   Friston KJ, 2006, J PHYSIOL-PARIS, V100, P70, DOI 10.1016/j.jphysparis.2006.10.001
   Guan Y, 2019, SIGCOMM '19 - PROCEEDINGS OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P394, DOI 10.1145/3341302.3342063
   Haruvi A, 2022, FRONT COMPUT NEUROSC, V15, DOI 10.3389/fncom.2021.760561
   Houmansadr A, 2006, SIGMAP 2006: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND MULTIMEDIA APPLICATIONS, P252
   Huang H.-M., 2002, IEEE Trans. Circuits Syst. Video Technol., V7, P299
   Huang Q, 2017, IEEE DATA COMPR CONF, P42, DOI 10.1109/DCC.2017.17
   Jaballah S., 2020, 2020 IEEE INT C IMAG
   Jia YT, 2006, IEEE T CIRC SYST VID, V16, P820, DOI 10.1109/TCSVT.2006.877397
   Jiménez-Rodríguez L, 2014, IEEE SIGNAL PROC LET, V21, P35, DOI 10.1109/LSP.2013.2290317
   Jin J, 2022, IEEE T CIRC SYST VID, V32, P3452, DOI 10.1109/TCSVT.2021.3113572
   Jin L., 2016, ELECT IMAGING, V2016, P1
   Kar M., 2018, Hilk Tram Image Process, V27, P3178
   KELLY DH, 1979, J OPT SOC AM, V69, P1340, DOI 10.1364/JOSA.69.001340
   Ki S., 2000, E Accesa
   Ki S, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Ki S, 2018, IEEE T IMAGE PROCESS, V27, P3178, DOI 10.1109/TIP.2018.2818439
   Kim J, 2015, IEEE T CIRC SYST VID, V25, P1786, DOI 10.1109/TCSVT.2015.2389491
   Kim J, 2020, IEEE DATA COMPR CONF, P375, DOI 10.1109/DCC47342.2020.00087
   Lian F., 2012, Comput. Soc., V1
   Lin HH, 2022, IEEE T CIRC SYST VID, V32, P5859, DOI 10.1109/TCSVT.2022.3163860
   Lin J., 2015, Applications of Digital Image Processing XXXVIII
   Lin JY, 2015, PROC SPIE, V9599, DOI 10.1117/12.2188389
   Lin WS, 2022, IEEE T MULTIMEDIA, V24, P3706, DOI 10.1109/TMM.2021.3106503
   Liu AM, 2010, IEEE T CIRC SYST VID, V20, P1648, DOI 10.1109/TCSVT.2010.2087432
   Liu HH, 2020, IEEE T IMAGE PROCESS, V29, P641, DOI 10.1109/TIP.2019.2933743
   Liu JY, 2010, IEEE T CIRC SYST VID, V20, P967, DOI 10.1109/TCSVT.2010.2045924
   Liu XH, 2018, LECT NOTES COMPUT SC, V11164, P458, DOI 10.1007/978-3-030-00776-8_42
   Lu M, 2022, IEEE T VIS COMPUT GR, V28, P718, DOI 10.1109/TVCG.2021.3114874
   Luo ZY, 2013, IEEE T CIRC SYST VID, V23, P935, DOI 10.1109/TCSVT.2013.2240919
   Ma L., 2011, Image Comman, V26, P162
   Ma L, 2011, SIGNAL PROCESS-IMAGE, V26, P162, DOI 10.1016/j.image.2011.02.002
   Macknik SL, 1998, NAT NEUROSCI, V1, P144, DOI 10.1038/393
   Nami S, 2020, IEEE INT CONF MULTI
   NILL NB, 1985, IEEE T COMMUN, V33, P551, DOI 10.1109/TCOM.1985.1096337
   Oh H., Trans. Image Process., V22, P189
   Oh H, 2013, IEEE T IMAGE PROCESS, V22, P189, DOI 10.1109/TIP.2012.2215616
   Pan ZQ, 2022, IEEE T CIRC SYST VID, V32, P7518, DOI 10.1109/TCSVT.2022.3188991
   Pan ZQ, 2022, IEEE T CIRC SYST VID, V32, P6347, DOI 10.1109/TCSVT.2022.3161103
   Pan ZQ, 2022, IEEE T IMAGE PROCESS, V31, P1613, DOI 10.1109/TIP.2022.3144892
   Pang C, 2013, IEEE T CIRC SYST VID, V23, P990, DOI 10.1109/TCSVT.2013.2244795
   Pel X., 2016, Displays, V42, P43
   Prangnell L., 2018, 2018 DEE INT C ACOUS
   Qi F., 2015, Stereoscopic video quality assessment Based on visual attention and just-noticeable difference models
   Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580
   Shen XL, 2021, IEEE T IMAGE PROCESS, V30, P26, DOI 10.1109/TIP.2020.3029428
   Shen XL, 2020, INT CONF ACOUST SPEE, P2058, DOI [10.1109/ICASSP40776.2020.9053580, 10.1109/icassp40776.2020.9053580]
   Si JJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING (IEEE VCIP 2013)
   Startsev M., 2018, Signal Process Image Comman.
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tian T., 2021, Best, V66, P600
   Tian T, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3408320
   Vidal E, 2017, SIGNAL PROCESS-IMAGE, V52, P124, DOI 10.1016/j.image.2016.12.003
   Wan WF, 2017, IEEE ACCESS, V5, P22953, DOI 10.1109/ACCESS.2017.2699858
   Wang H, 2017, ARXIV
   Wang HG, 2016, IEEE IMAGE PROC, P1509, DOI 10.1109/ICIP.2016.7532610
   Wang HQ, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6747, DOI 10.1109/ICASSP.2018.8461571
   Wang HQ, 2018, PICT COD SYMP, P278, DOI 10.1109/PCS.2018.8456243
   Wang HK, 2023, SIGNAL PROCESS-IMAGE, V118, DOI 10.1016/j.image.2023.117019
   Wang HK, 2021, IEEE T IMAGE PROCESS, V30, P487, DOI 10.1109/TIP.2020.3037525
   Wang HK, 2020, IEEE DATA COMPR CONF, P396, DOI 10.1109/DCC47342.2020.00073
   Wang HK, 2020, IEEE SIGNAL PROC LET, V27, P181, DOI 10.1109/LSP.2019.2957647
   Wang HK, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING (IEEE VCIP)
   Wang HK, 2018, LECT NOTES COMPUT SC, V11166, P188, DOI 10.1007/978-3-030-00764-5_18
   Wang S., 2016, IEEE Trans. Image Process
   Wang SS, 2013, IEEE J-STSP, V7, P1101, DOI 10.1109/JSTSP.2013.2272240
   Watson A.B., 2013, Visual optimization of DCT quantization matrices for individual images
   Wei ZY, 2009, IEEE T CIRC SYST VID, V19, P337, DOI 10.1109/TCSVT.2009.2013518
   Wit Y., 2021, Diplays: Technol. Appl., P1
   Wu JJ, 2017, IEEE T IMAGE PROCESS, V26, P2682, DOI 10.1109/TIP.2017.2685682
   Wu JJ, 2013, IEEE T MULTIMEDIA, V15, P1705, DOI 10.1109/TMM.2013.2268053
   Xiang GQ, 2016, IEEE INT SYMP CIRC S, P2535, DOI 10.1109/ISCAS.2016.7539109
   Xiuchang Z., 2017, J. Nanjing Univ. Posts Telecommun., V37, P11
   Yan Y., 2020, 2020 IEEE INT C MULT, P1
   Yang HL, 2020, IEEE DATA COMPR CONF, P404, DOI 10.1109/DCC47342.2020.00071
   Yang KF, 2015, SIGNAL PROCESS-IMAGE, V31, P10, DOI 10.1016/j.image.2014.11.005
   Yang XK, 2005, SIGNAL PROCESS-IMAGE, V20, P662, DOI 10.1016/j.image.2005.04.001
   Yin HB, 2019, IEEE ACCESS, V7, P151215, DOI 10.1109/ACCESS.2019.2947260
   Yuan D., 2019, HEE, VA7, P20014
   Yuan D, 2019, IEEE ACCESS, V7, P29014, DOI 10.1109/ACCESS.2019.2901342
   Yue W., 2011, 2011 VIMAL COMMUNICA
   Zhang Q, 2024, Arxiv, DOI arXiv:2211.06797
   Zhang Q, 2021, INT J COMPUT VISION, V129, P2889, DOI 10.1007/s11263-021-01505-4
   Zhang X., 2020, BEE Trans. Image Process., P1
   Zhang XH, 2005, SIGNAL PROCESS, V85, P795, DOI 10.1016/j.sigpro.2004.12.002
   Zhang XY, 2019, IEEE IMAGE PROC, P4140, DOI [10.1109/ICIP.2019.8803454, 10.1109/icip.2019.8803454]
   Zhang Y., 2022, Trans, Circuits Syst. Video Techul.
   Zhang Y., 2019, WE Trans Image Procos, P1
   Zhang Y, 2022, Arxiv, DOI arXiv:2112.12284
   Zhao Y, 2011, IEEE SIGNAL PROC LET, V18, P19, DOI 10.1109/LSP.2010.2090041
   Zhong R, 2015, MULTIMED TOOLS APPL, V74, P10457, DOI 10.1007/s11042-014-2176-y
   Zhou ML, 2020, IEEE T IMAGE PROCESS, V29, P7603, DOI 10.1109/TIP.2020.3004714
   Zhu C, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954513
   Zhu JW, 2022, IEEE IMAGE PROC, P4213, DOI 10.1109/ICIP46576.2022.9897946
   Zhu X., 2021, J. Nanjing Univ. Posts Telcommun., V41, P1
   Zhu YC, 2020, IEEE T MULTIMEDIA, V22, P2331, DOI 10.1109/TMM.2019.2957986
NR 117
TC 0
Z9 0
U1 4
U2 4
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104034
DI 10.1016/j.jvcir.2023.104034
EA JAN 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HH5B8
UT WOS:001158604700001
DA 2024-08-05
ER

PT J
AU Ulrich, L
   Marcolin, F
   Vezzetti, E
   Nonis, F
   Mograbib, DC
   Scuratid, GW
   Dozio, N
   Ferrise, F
AF Ulrich, Luca
   Marcolin, Federica
   Vezzetti, Enrico
   Nonis, Francesca
   Mograbib, Daniel C.
   Scuratid, Giulia Wally
   Dozio, Nicolo
   Ferrise, Francesco
TI <inverted exclamation>CalD3r and MenD3s: Spontaneous 3D facial
   expression databases
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE 3D facial expression; Spontaneous expressions; Facial expression
   recognition; Ecological validity; Affective database; Human-computer
   interaction
ID RECOGNITION; EMOTION
AB In the last couple of decades, the research on 3D facial expression recognition has been fostered by the creation of tailored databases containing prototypical expressions of different individuals and by the advances in cost most of the currently availab effective acquisition technologies. Though, most of the currently available databases consist of exaggerated facial expressions, due to the imitation principle which they rely on. This makes these databases only partially employable for real world applications such as human-computer interaction for smart products and environ- ments, health, and industry 4.0, as algorithms learn on these 'inflated' data which do not respond to ecological validity requirements. In this work, we present two novel 2D + 3D spontaneous facial expression databases of young adults with different geographical origin, in which emotions have been evoked thanks to affective images of the acknowledged IAPS and GAPED databases, and verified with participants self-reports. To the best of our knowledge, these are the first three-dimensional facial databases with emotions elicited by validated affective stimuli.
C1 [Ulrich, Luca; Marcolin, Federica; Vezzetti, Enrico; Nonis, Francesca] Politecn Torino, Dept Management & Prod Engn, C So Duca degli Abruzzi 24, I-10129 Turin, Italy.
   [Mograbib, Daniel C.] Pontif Catholic Univ Rio de Janeiro, Dept Psychol, Rua Marques Sao Vicente 225, Gavea, RJ, Brazil.
   [Dozio, Nicolo; Ferrise, Francesco] Politecn Milan, Dept Mech Engn, Via Privata Giuseppe La Masa 1, I-20156 Milan, Italy.
   [Scuratid, Giulia Wally] Blekinge Tekn Hogskola, Dept Mech Engn, Valhallavagen 1, S-37141 Karlskrona, Sweden.
C3 Polytechnic University of Turin; Polytechnic University of Milan
RP Ferrise, F (corresponding author), Politecn Milan, Dept Mech Engn, Via Privata Giuseppe La Masa 1, I-20156 Milan, Italy.
EM federica.marcolin@polito.it
RI Ferrise, Francesco/C-6502-2008
OI Ferrise, Francesco/0000-0001-8951-8807; Ulrich,
   Luca/0000-0001-8407-0660; Dozio, Nicolo/0000-0003-3201-2519; Marcolin,
   Federica/0000-0002-4360-6905
CR BAGBY RM, 1994, J PSYCHOSOM RES, V38, P23, DOI 10.1016/0022-3999(94)90005-1
   Barrett LF, 2019, PSYCHOL SCI PUBL INT, V20, P1, DOI 10.1177/1529100619832930
   BASSILI JN, 1979, J PERS SOC PSYCHOL, V37, P2049, DOI 10.1037/0022-3514.37.11.2049
   Ben XY, 2023, IEEE T MULTIMEDIA, V25, P5429, DOI 10.1109/TMM.2022.3192727
   Ben XY, 2022, IEEE T PATTERN ANAL, V44, P5826, DOI 10.1109/TPAMI.2021.3067464
   Bradley M. M., 1999, tech. rep. no. b-2)
   Bradley MM, 2001, EMOTION, V1, P300, DOI 10.1037//1528-3542.1.3.300
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   BRUCE V, 1986, BRIT J PSYCHOL, V77, P305, DOI 10.1111/j.2044-8295.1986.tb02199.x
   Buolamwini J., 2018, C FAIRN ACC TRANSP, V81, P77, DOI DOI 10.2147/OTT.S126905
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Castiblanco Jimenez I. A., 2022, INT JOINT C MECH DES, P318
   Chen JM, 2021, BEHAV RES METHODS, V53, P371, DOI 10.3758/s13428-020-01447-8
   Colombo A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P2113, DOI 10.1109/ICCVW.2011.6130509
   Crowder J.A., 2014, Artificial Cognition Archit., P17
   Dan-Glauser ES, 2011, BEHAV RES METHODS, V43, P468, DOI 10.3758/s13428-011-0064-1
   Darwin C., 1872, P374
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Deng JK, 2019, INT J COMPUT VISION, V127, P599, DOI 10.1007/s11263-018-1134-y
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Dozio N, 2022, INT J HUM-COMPUT ST, V162, DOI 10.1016/j.ijhcs.2022.102791
   Dozio N, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-03380-y
   EKMAN P, 1992, PSYCHOL REV, V99, P550, DOI 10.1037/0033-295X.99.3.550
   Ekman P., 1978, Environ. Psychol. Nonverbal Behav.
   Ekman P., 2004, Bmj, V328, P0405184, DOI [DOI 10.1136/SBMJ.0405184, 10.1136/sbmj.0405184]
   Faltemier TC, 2007, 2007 FIRST IEEE INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS, P19
   Flynn P. J., 2011, 2011 INT JOINT C BIO, P1
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Gupta SD, 2010, METHODS MOL BIOL, V589, P97, DOI 10.1109/SSIAI.2010.5483908
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heseltine T, 2008, IMAGE VISION COMPUT, V26, P382, DOI 10.1016/j.imavis.2006.12.008
   Ho Y, 2020, IEEE ACCESS, V8, P4806, DOI 10.1109/ACCESS.2019.2962617
   Kingma D. P., 2014, arXiv
   Lang P., 2007, Series in affective science, V29, P70
   Lang PJ., 1997, International Affective Picture System (IAPS)
   Maculotti G, 2022, MEASUREMENT, V200, DOI 10.1016/j.measurement.2022.111643
   Marcolin F, 2021, IEEE COMPUT GRAPH, V41, P171, DOI 10.1109/MCG.2021.3115015
   Maria E, 2019, ELECTRON NOTES THEOR, V343, P35, DOI 10.1016/j.entcs.2019.04.009
   Meneghini A., 2012, the italian adaptation of the balanced emotional empathy scale (bees) by albert mehrabian
   Min R, 2014, IEEE T SYST MAN CY-S, V44, P1534, DOI 10.1109/TSMC.2014.2331215
   Moreno Gavabdb A., 2004, PROC, V2nd, P75
   Moriguchi Y, 2007, CEREB CORTEX, V17, P2223, DOI 10.1093/cercor/bhl130
   Mul CL, 2018, J AUTISM DEV DISORD, V48, P2953, DOI 10.1007/s10803-018-3564-3
   Namba S, 2017, CURR PSYCHOL, V36, P593, DOI 10.1007/s12144-016-9448-9
   Nonis Francesca, 2021, Universal Access in Human-Computer Interaction. Design Methods and User Experience. 15th International Conference, UAHCI 2021 Held as Part of the 23rd HCI International Conference, HCII 2021. Proceedings,, P599, DOI 10.1007/978-3-030-78092-0_42
   Nonis F, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9183904
   Olivetti EC, 2020, LECT N MECH ENG, P665, DOI 10.1007/978-3-030-31154-4_56
   Phillips P. J., 2003, 2003 IEEE International Workshop on Analysis and Modeling of Faces and Gestures
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   Redies C, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.00953
   Rhue L., 2018, SSRN ELECT J, DOI [10.2139/ssrn.3281765, DOI 10.2139/SSRN.3281765]
   Rosenberg Erika L, 2020, What the Face Reveals: Basic and Applied Studies ofSpontaneousExpressionusingtheFacialActionCodingSystem(FACS)
   Sankowski W, 2015, 2015 22ND INTERNATIONAL CONFERENCE MIXED DESIGN OF INTEGRATED CIRCUITS & SYSTEMS (MIXDES), P93, DOI 10.1109/MIXDES.2015.7208488
   Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6
   Schmuckler MA, 2001, INFANCY, V2, P419, DOI 10.1207/S15327078IN0204_02
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Ulrich L, 2020, MULTIMED TOOLS APPL, V79, P29375, DOI 10.1007/s11042-020-09479-0
   Ulrich L, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10010008
   Vezzetti E, 2018, MULTIMED TOOLS APPL, V77, P14177, DOI 10.1007/s11042-017-5025-y
   Wan SH, 2014, PATTERN RECOGN, V47, P1859, DOI 10.1016/j.patcog.2013.11.025
   Wang SF, 2013, IEEE T AFFECT COMPUT, V4, P34, DOI 10.1109/T-AFFC.2012.32
   Yang HY, 2018, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2018.00231
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Yin LJ, 2008, IEEE INT CONF AUTOMA, P116
   Zabatani A, 2020, IEEE T PATTERN ANAL, V42, P2333, DOI 10.1109/TPAMI.2019.2915841
   Zafeiriou S., 2011, 2011 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops 2011), P132, DOI 10.1109/CVPRW.2011.5981840
   Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002
   Zhang Z, 2016, PROC CVPR IEEE, P3438, DOI 10.1109/CVPR.2016.374
   Zhong C, 2007, PROC CVPR IEEE, P2371
NR 69
TC 1
Z9 1
U1 2
U2 2
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104033
DI 10.1016/j.jvcir.2023.104033
EA JAN 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HE2W3
UT WOS:001157761700001
DA 2024-08-05
ER

PT J
AU Lu, XY
   Xue, YB
   Wang, ZG
   Xu, HX
   Wen, XB
AF Lu, Xingyuan
   Xue, Yanbing
   Wang, Zhigang
   Xu, Haixia
   Wen, Xianbin
TI X-CDNet: A real-time crosswalk detector based on YOLOX
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Crosswalk detection; CD9K; Reparameterization; X-CDNet
AB As urban traffic safety becomes increasingly important, real-time crosswalk detection is playing a critical role in the transportation field. However, existing crosswalk detection algorithms must be improved in terms of accuracy and speed. This study proposes a real-time crosswalk detector called X-CDNet based on YOLOX. Based on the ConvNeXt basic module, we designed a new basic module called Rep arameterizable S parse L arge - K ernel (RepSLK) convolution that can be used to expand the model's receptive field without the addition of extra inference time. In addition, we created a new crosswalk dataset called CD9K, which is based on realistic driving scenes augmented by techniques such as synthetic rain and fog. The experimental results demonstrate that X-CDNet outperforms YOLOX in terms of both detection accuracy and speed. X-CDNet achieves a 93.3 AP50 and a real-time detection speed of 123 FPS.
C1 [Lu, Xingyuan; Xue, Yanbing; Wang, Zhigang; Xu, Haixia; Wen, Xianbin] Tianjin Univ Technol, Key Lab Comp Vis & Syst, Minist Educ, Tianjin 300384, Peoples R China.
C3 Tianjin University of Technology
RP Xue, YB (corresponding author), Tianjin Univ Technol, Key Lab Comp Vis & Syst, Minist Educ, Tianjin 300384, Peoples R China.
EM xueyb0718@tjut.edu.cn
OI Lu, Xingyuan/0009-0006-1999-1395
FU National Natural Science Foundation of China [61971309, 61872270]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61971309 and 61872270. The au-thors
   thank TopEdit ( www.topeditsci.com ) for its linguistic assistance
   during the preparation of this manuscript.
CR Akinlar C, 2011, IEEE IMAGE PROC
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen N., 2019, J. Zhejiang Univ. Sci. Technol., V6, P476
   Ding XH, 2022, PROC CVPR IEEE, P11953, DOI 10.1109/CVPR52688.2022.01166
   Ding XH, 2021, PROC CVPR IEEE, P13728, DOI 10.1109/CVPR46437.2021.01352
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2018, Arxiv, DOI [arXiv:1608.06993, 10.48550/arXiv.1608.06993]
   Huang X., 2017, Comput. Appl. Softw., V12, P202
   Jocher G., 2023, YOLO by Ultralytics
   Jocher Glenn, 2022, Zenodo, DOI 10.5281/ZENODO.3908559
   Kaya Ö, 2023, BUILDINGS-BASEL, V13, DOI 10.3390/buildings13041070
   Li CY, 2023, IEEE T PATTERN ANAL, V45, P8284, DOI 10.1109/TPAMI.2023.3234976
   Li CY, 2022, Arxiv, DOI [arXiv:2209.02976, DOI 10.48550/ARXIV.2209.02976]
   Liu Hanzhou, 2022, 2022 7th International Conference on Image, Vision and Computing (ICIVC), P114, DOI 10.1109/ICIVC55077.2022.9887023
   Liu HM, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3274926
   Liu N, 2022, LECT NOTES COMPUT SC, V13677, P423, DOI 10.1007/978-3-031-19790-1_26
   Liu SW, 2022, Arxiv, DOI [arXiv:2207.03620, DOI 10.48550/ARXIV.2207.03620, 10.48550/arXiv.2207.03620]
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Luo P, 2019, Arxiv, DOI [arXiv:1806.10779, 10.48550/arXiv.1806.10779]
   Luo WJ, 2016, ADV NEUR IN, V29
   Luo YR, 2023, Arxiv, DOI arXiv:2308.04583
   Mascetti S, 2016, PATTERN RECOGN, V60, P405, DOI 10.1016/j.patcog.2016.05.002
   Mirza MJ, 2021, IEEE INT C INTELL TR, P2719, DOI 10.1109/ITSC48978.2021.9564505
   Rahman Z, 2023, Arxiv, DOI arXiv:2307.06853
   Redmon J., 2018, CoRR
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Se S, 2000, PROC CVPR IEEE, P211, DOI 10.1109/CVPR.2000.854787
   Szegedy C, 2015, Arxiv, DOI [arXiv:1512.00567, 10.48550/arXiv.1512.00567]
   Szegedy C, 2014, Arxiv, DOI arXiv:1409.4842
   Tan R., 2020, P IEEE CVF C COMP VI, DOI [10.1109/CVPR42600.2020.01079, DOI 10.1109/CVPR42600.2020.01079]
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Trinh Thong Duc, 2022, Proceedings of the International Conference on Advanced Intelligent Systems and Informatics 2021. Lecture Notes on Data Engineering and Communications Technologies (100), P62, DOI 10.1007/978-3-030-89701-7_6
   Uddin MS, 2005, IEEE T INTELL TRANSP, V6, P439, DOI 10.1109/TITS.2005.858787
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang SH, 2014, J VIS COMMUN IMAGE R, V25, P263, DOI 10.1016/j.jvcir.2013.11.005
   Woo S, 2023, PROC CVPR IEEE, P16133, DOI 10.1109/CVPR52729.2023.01548
   Yan L, 2020, IEEE T GEOSCI REMOTE, V58, P3558, DOI 10.1109/TGRS.2019.2958123
   Yang KL, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (ROBIO), P1034, DOI 10.1109/ROBIO.2018.8665211
   Zhang ZD, 2022, NEURAL COMPUT APPL, V34, P10719, DOI 10.1007/s00521-022-07007-9
   Zhao Y, 2024, Arxiv, DOI [arXiv:2304.08069, 10.48550/arXiv.2304.08069]
   Zhou AJ, 2021, Arxiv, DOI arXiv:2102.04010
   Zhou KY, 2023, Arxiv, DOI arXiv:2305.00675
NR 48
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD JUN
PY 2024
VL 102
AR 104206
DI 10.1016/j.jvcir.2024.104206
EA JUN 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XN9Z6
UT WOS:001262495200001
DA 2024-08-05
ER

PT J
AU Tao, XF
   Kong, J
   Jiang, M
   Luo, X
   Liu, TS
AF Tao, Xuefeng
   Kong, Jun
   Jiang, Min
   Luo, Xi
   Liu, Tianshan
TI Patch-based tendency camera multi-constraint learning for unsupervised
   person re-identification
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Unsupervised person re-identification; Ranking tendency similarity;
   Tendency-based mutual complementation loss; Adaptive camera
   multi-constraint loss
AB Unsupervised person re -identification (ReID) is a task that aims to retrieve pedestrians across different cameras from unlabeled data. Existing methods rely on clustering to generate pseudo -labels, but they are inevitably noisy. Although pseudo -label refinement approaches have been presented, the essentiality of patch contours is ignored. The tendency analysis of retrieval between global and patch features has not been well investigated. In this paper, we propose a Patch -based Tendency Camera Multi -Constraint Learning (PTCML) model for unsupervised person ReID. First, to explore the tendentious retrieval of global and patch features, we design a Ranking Tendency Similarity (RTS) score by gauging the distribution discrepancy of distance changes. Second, based on RTS score, we propose a Tendency -based Mutual Complementation (TMC) loss to improve the quality of global and patch pseudo -labels. Third, to resist camera variations, we propose an Adaptive Camera Multi -Constraint (ACM) loss to optimize recognition results with camera distribution constraint and instance constraint simultaneously. Finally, numerous experiments on Market -1501 and MSMT17 demonstrate that our method can significantly surpass the state-of-the-art performance.
C1 [Tao, Xuefeng; Kong, Jun] Jiangnan Univ, Key Lab Adv Proc Control Light Ind, Minist Educ, Wuxi 214122, Peoples R China.
   [Jiang, Min; Luo, Xi] Jiangnan Univ, Jiangsu Prov Engn Lab Pattern Recognit & Computat, Wuxi 214122, Peoples R China.
   [Liu, Tianshan] Hong Kong Polytech Univ, Dept Elect & Informat Engn, Hong Kong 999077, Peoples R China.
   [Kong, Jun] Jiangnan Univ, Sch Internet Things Engn, Wuxi 214122, Peoples R China.
C3 Jiangnan University; Jiangnan University; Hong Kong Polytechnic
   University; Jiangnan University
RP Kong, J (corresponding author), Jiangnan Univ, Sch Internet Things Engn, Wuxi 214122, Peoples R China.
EM kongjun@jiangnan.edu.cn
FU Fundamental Research Funds for the Central Universities, China
   [JUSRP41908]; National Natural Science Foundation of China [62371209,
   62371208]; China Postdoctoral Science Foundation [2015M581720,
   2016M600360]; The 111 Projects [B12018]
FX This work was partially supported by the Fundamental Research Funds for
   the Central Universities, China (JUSRP41908), the National Natural
   Science Foundation of China (62371209, 62371208), China Postdoctoral
   Science Foundation (2015M581720, 2016M600360), 111 Projects under Grant
   B12018.
CR Bertocco G, 2023, IEEE T INF FOREN SEC, V18, P3876, DOI 10.1109/TIFS.2023.3289448
   Cho Y, 2022, PROC CVPR IEEE, P7298, DOI 10.1109/CVPR52688.2022.00716
   Chuanchen Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P224, DOI 10.1007/978-3-030-58555-6_14
   Dai YX, 2021, IEEE T IMAGE PROCESS, V30, P7815, DOI 10.1109/TIP.2021.3104169
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Fang Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P526, DOI 10.1007/978-3-030-58621-8_31
   Felzenszwalb P, 2008, PROC CVPR IEEE, P1984
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ge YX, 2020, Arxiv, DOI [arXiv:2001.01526, 10.48550/arXiv.2001.01526]
   Guo JY, 2019, IEEE I CONF COMP VIS, P3641, DOI 10.1109/ICCV.2019.00374
   Jaderberg M, 2015, ADV NEUR IN, V28
   Kaiwei Zeng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13654, DOI 10.1109/CVPR42600.2020.01367
   Kong J., 2022, IEEE Trans. Multimed., P1
   Lan L, 2023, IEEE T IMAGE PROCESS, V32, P3338, DOI 10.1109/TIP.2023.3278860
   Li MK, 2022, IEEE T IMAGE PROCESS, V31, P3606, DOI 10.1109/TIP.2022.3173163
   Lin YT, 2020, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR42600.2020.00345
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Pang ZQ, 2023, KNOWL-BASED SYST, V263, DOI 10.1016/j.knosys.2023.110263
   Shu J, 2019, ADV NEUR IN, V32
   Song H, 2023, IEEE T NEUR NET LEAR, V34, P8135, DOI 10.1109/TNNLS.2022.3152527
   Tao XF, 2022, IEEE T CIRC SYST VID, V32, P4404, DOI 10.1109/TCSVT.2021.3135274
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang ML, 2021, AAAI CONF ARTIF INTE, V35, P2764
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wei WY, 2022, J VIS COMMUN IMAGE R, V82, DOI 10.1016/j.jvcir.2021.103418
   Wu C, 2022, PROC CVPR IEEE, P20206, DOI 10.1109/CVPR52688.2022.01960
   Xuan SY, 2021, PROC CVPR IEEE, P11921, DOI 10.1109/CVPR46437.2021.01175
   Yang FX, 2021, PROC CVPR IEEE, P4853, DOI 10.1109/CVPR46437.2021.00482
   Yang QZ, 2019, PROC CVPR IEEE, P3628, DOI 10.1109/CVPR.2019.00375
   Yu XT, 2022, IEEE SIGNAL PROC LET, V29, P1297, DOI 10.1109/LSP.2022.3177319
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zhang G., 2023, IEEE Trans. Circuits Syst. Video Technol., P1
   Zhang X, 2021, PROC CVPR IEEE, P3435, DOI 10.1109/CVPR46437.2021.00344
   Zhang XY, 2022, PROC CVPR IEEE, P7359, DOI 10.1109/CVPR52688.2022.00722
   Zhao J, 2023, J VIS COMMUN IMAGE R, V93, DOI 10.1016/j.jvcir.2023.103828
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhu YJ, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103714
NR 44
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104180
DI 10.1016/j.jvcir.2024.104180
EA MAY 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TU2Y6
UT WOS:001243718900001
DA 2024-08-05
ER

PT J
AU Cheng, X
   Siu, WC
AF Cheng, Xi
   Siu, Wan-Chi
CA Life Fellow IEEE
TI Edge fusion back projection GAN for large scale face super resolution
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Face super resolution; Edge guidance; Back projection; Deep learning
ID IMAGE; SUPERRESOLUTION; NETWORK
AB Face super-resolution is an important low-level vision task that has wide applications. Existing deep-learningbased face super-resolution (SR) methods often optimize the image super-resolution network by directly minimizing the pixel or feature level distance between the synthetic low-resolution face and the ground truth face image. These methods usually generate blur or over smooth results and lack of high-frequency face details. These are especially true for making high super-resolution of faces. Say for example, a super-resolution of 16x, only 0.4 % of the reference data points are available. To address the problem, we propose a novel network with edge fusion, back projection, and GAN prior (EFBPGAN) which can significantly improve the visual quality and generate realistic faces. To further make use of the spatial information and keep the structural consistency, we have developed new edge fusion and spatial fusion modules. We also propose a back projection extensive based coarse to fine SR pipeline to suppress the distortion and artifacts caused by GAN. Much experimental work has been done, results of which show that our proposed EFBPGAN can outperform the state-of-the-art approaches not only on numerical metrics but also on subjective visual evaluations.
C1 [Cheng, Xi] Nanjing Univ Posts & Telecommun, Nanjing, Peoples R China.
   [Siu, Wan-Chi] Hong Kong Polytech Univ, Hong Kong, Peoples R China.
   [Cheng, Xi; Siu, Wan-Chi] St Francis Univ, Hong Kong, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Hong Kong Polytechnic
   University; Saint Francis University Hong Kong
RP Siu, WC (corresponding author), Hong Kong Polytech Univ, Hong Kong, Peoples R China.; Siu, WC (corresponding author), St Francis Univ, Hong Kong, Peoples R China.
EM enwcsiu@polyu.edu.hk
FU Hong Kong Polytechnic University, Saint Francis University in Hong Kong
   [ISG200206]; University Grants Committee (UGC) of the Hong Kong SAR
   Government [UGC/IDS (C) 11/E01/20]; Natural Science Research Start-up
   Foundation of Recruiting Talents of Nanjing University of Posts and
   Telecommunications [NY223130]
FX This work was partly supported by The Hong Kong Polytechnic University,
   Saint Francis University in Hong Kong (with Grant number ISG200206) ,
   University Grants Committee (UGC) of the Hong Kong SAR Government (with
   grant number UGC/IDS (C) 11/E01/20) , and the Natural Science Research
   Start-up Foundation of Recruiting Talents of Nanjing University of Posts
   and Telecommunications (Grant No. NY223130) .
CR Blau Y, 2019, LECT NOTES COMPUT SC, V11133, P334, DOI 10.1007/978-3-030-11021-5_21
   Chan KCK, 2021, PROC CVPR IEEE, P14240, DOI 10.1109/CVPR46437.2021.01402
   Chen CF, 2021, IEEE T IMAGE PROCESS, V30, P1219, DOI 10.1109/TIP.2020.3043093
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen XZ, 2020, NEUROCOMPUTING, V376, P119, DOI 10.1016/j.neucom.2019.09.079
   Chen Y, 2018, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2018.00264
   Cheng X, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9152992
   Choi J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14347, DOI 10.1109/ICCV48922.2021.01410
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Dou H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1891, DOI 10.1145/3394171.3413590
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Grm K, 2020, IEEE T IMAGE PROCESS, V29, P2150, DOI 10.1109/TIP.2019.2945835
   Gu JJ, 2020, PROC CVPR IEEE, P3009, DOI 10.1109/CVPR42600.2020.00308
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Hou H, 2023, IEEE T IMAGE PROCESS, V32, P1184, DOI 10.1109/TIP.2023.3240845
   Hsu CC, 2019, IEEE T IMAGE PROCESS, V28, P6225, DOI 10.1109/TIP.2019.2924554
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang DD, 2016, LECT NOTES COMPUT SC, V9967, P167, DOI 10.1007/978-3-319-46654-5_19
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Hung KW, 2012, IEEE T IMAGE PROCESS, V21, P1061, DOI 10.1109/TIP.2011.2168416
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T., 2020, P IEEE CVF C COMP VI, P8110, DOI [10.1109/cvpr42600.2020.00813, DOI 10.1109/CVPR42600.2020.00813]
   Karras T, 2018, Arxiv, DOI [arXiv:1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim J, 2016, PROC CVPR IEEE, P1646, DOI 10.1109/CVPR.2016.182
   Kim J, 2021, NEUROCOMPUTING, V446, P11, DOI 10.1016/j.neucom.2021.03.048
   Ko SL, 2021, INT C PATT RECOG, P3505, DOI 10.1109/ICPR48806.2021.9412950
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai W.-S., 2017, P IEEE C COMPUTER VI, P624, DOI [DOI 10.1109/CVPR.2017.618, 10.1109/CVPR.2017.618, 10.1109/cvpr.2017.618]
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu S, 2021, NEUROCOMPUTING, V449, P357, DOI 10.1016/j.neucom.2021.03.124
   Liu ZS, 2021, IEEE T CIRC SYST VID, V31, P1351, DOI 10.1109/TCSVT.2020.3003832
   Liu ZS, 2021, IEEE T IMAGE PROCESS, V30, P4157, DOI 10.1109/TIP.2021.3069554
   Loshchilov I, 2017, Sgdr: Stochastic gradient descent with warm restarts, DOI [10.48550/arXiv.1608.03983, DOI 10.48550/ARXIV.1608.03983]
   Lu T, 2024, IEEE T NEUR NET LEAR, V35, P3938, DOI 10.1109/TNNLS.2022.3201448
   Menon S, 2020, PROC CVPR IEEE, P2434, DOI 10.1109/CVPR42600.2020.00251
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tang YG, 2023, J VIS COMMUN IMAGE R, V93, DOI 10.1016/j.jvcir.2023.103834
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang CY, 2022, IEEE T CIRC SYST VID, V32, P7317, DOI 10.1109/TCSVT.2022.3181828
   Wang LW, 2020, IEEE T IMAGE PROCESS, V29, P7984, DOI 10.1109/TIP.2020.3008396
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang XT, 2021, IEEE INT CONF COMP V, P1905, DOI 10.1109/ICCVW54120.2021.00217
   Wang XT, 2021, PROC CVPR IEEE, P9164, DOI 10.1109/CVPR46437.2021.00905
   Wang YZ, 2024, IEEE T MULTIMEDIA, V26, P2314, DOI 10.1109/TMM.2023.3294808
   Wang YZ, 2023, IEEE T CIRC SYST VID, V33, P2533, DOI 10.1109/TCSVT.2022.3224940
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wong C.-S., 2010, P 19 INT C COMP COMM, P1
   Wong CS, 2010, EUR SIGNAL PR CONF, P309
   Xie F, 2023, J VIS COMMUN IMAGE R, V95, DOI 10.1016/j.jvcir.2023.103889
   Xin JW, 2020, AAAI CONF ARTIF INTE, V34, P12476
   Xin JW, 2019, AAAI CONF ARTIF INTE, P9054
   Yang T, 2021, PROC CVPR IEEE, P672, DOI 10.1109/CVPR46437.2021.00073
   Yu X, 2016, LECT NOTES COMPUT SC, V9909, P318, DOI 10.1007/978-3-319-46454-1_20
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang ML, 2021, IEEE T MULTIMEDIA, V23, P1938, DOI 10.1109/TMM.2020.3006414
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YL, 2021, IEEE T PATTERN ANAL, V43, P2480, DOI 10.1109/TPAMI.2020.2968521
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhu H, 2023, J VIS COMMUN IMAGE R, V95, DOI 10.1016/j.jvcir.2023.103874
NR 71
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104143
DI 10.1016/j.jvcir.2024.104143
EA APR 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SG2V9
UT WOS:001233247100001
OA hybrid
DA 2024-08-05
ER

PT J
AU Zhang, DY
   Lv, ZY
   Li, F
   Ding, XL
   Yang, GB
AF Zhang, Dengyong
   Lv, Zhenyu
   Li, Feng
   Ding, Xiangling
   Yang, Gaobo
TI A convolutional neural network based on noise residual for seam carving
   detection
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Image forensic; Seam carving; Deep learning; Local noise in-consistency
AB Seam carving is a method of resizing images based on content awareness. It can realize image retargeting while retaining the main content of the image. However, it may also be maliciously used to tamper with images, such as changing image semantic content by object removal. Therefore, seam carving detection has become important in image forensics. In this paper, a noise residual -based deep learning method is proposed to detect seam carving images. We try to learn the local noise in -consistency of images to recognize them. Firstly, a noise residual extraction segment is used to learn local noise features in the image, and a noise augmentation module is designed to enrich the features, which leverages the noise features extracted from a steganalysis rich model filter to discover the noise in -consistency between authentic and tampered regions. Then, through the feature dimensionality reduction section, the features are further learned and the size of feature maps are reduced. Finally, the output is obtained through global average pooling and a fully -connected layer. A careful testing strategy is further proposed, which greatly improves the detection performance, especially for seam carving with small scaling ratios. The experimental results demonstrate that our method achieves stateof-the-art performance at various scales, and has good robustness and generalization compared with other methods.
C1 [Zhang, Dengyong; Lv, Zhenyu; Li, Feng] Changsha Univ Sci & Technol, Hunan Prov Key Lab Intelligent Proc Big Data Trans, Changsha 410114, Peoples R China.
   [Zhang, Dengyong; Lv, Zhenyu; Li, Feng] Changsha Univ Sci & Technol, Sch Comp & Commun Engn, Changsha 410114, Peoples R China.
   [Ding, Xiangling] Hunan Univ Sci & Technol, Sch Comp Sci & Engn, Xiangtan 411004, Peoples R China.
   [Yang, Gaobo] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
C3 Changsha University of Science & Technology; Changsha University of
   Science & Technology; Hunan University of Science & Technology; Hunan
   University
RP Zhang, DY (corresponding author), Changsha Univ Sci & Technol, Hunan Prov Key Lab Intelligent Proc Big Data Trans, Changsha 410114, Peoples R China.
EM zhdy@csust.edu.cn
OI Zhang, Dengyong/0000-0002-2789-2980
FU National Natural Science Foundation of China [62172059, 62272160];
   Scientific Research Fund of Hunan Provincial Education Department of
   China [22A0200]
FX This work was funded in part by the National Natural Science Foundation
   of China under Grant 62172059, 62272160, in part by Scientific Research
   Fund of Hunan Provincial Education Department of China under Grant
   22A0200.
CR Achanta R, 2009, IEEE IMAGE PROC, P1005, DOI 10.1109/ICIP.2009.5413815
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Bas Patrick, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P59, DOI 10.1007/978-3-642-24178-9_5
   Boroumand M, 2019, IEEE T INF FOREN SEC, V14, P1181, DOI 10.1109/TIFS.2018.2871749
   Celebi N.H., 2022, J. Surveill. Secur. Saf., P88
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Cieslak LFD, 2018, 2018 IEEE 12TH INTERNATIONAL SYMPOSIUM ON APPLIED COMPUTATIONAL INTELLIGENCE AND INFORMATICS (SACI), P195, DOI 10.1109/SACI.2018.8441016
   Fillion C, 2010, PROC SPIE, V7541, DOI 10.1117/12.838647
   Frankovich M, 2011, IEEE SIGNAL PROC LET, V18, P375, DOI 10.1109/LSP.2011.2140396
   Glorot X., 2011, P 14 INT C ART INT S, V15, P315, DOI DOI 10.1002/ECS2.1832
   Gudavalli C, 2022, IEEE COMPUT SOC CONF, P1, DOI 10.1109/CVPRW56347.2022.00010
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Liu QZ, 2019, IEEE T CIRC SYST VID, V29, P1907, DOI 10.1109/TCSVT.2018.2859633
   Liu QZ, 2015, ACM T INTEL SYST TEC, V5, DOI 10.1145/2560365
   Moreira TP, 2022, LECT NOTES COMPUT SC, V13256, P447, DOI 10.1007/978-3-031-04881-4_35
   Nam SH, 2021, IEEE T CIRC SYST VID, V31, P3308, DOI 10.1109/TCSVT.2020.3037662
   Nam SH, 2019, IEEE IMAGE PROC, P106, DOI [10.1109/ICIP.2019.8802946, 10.1109/icip.2019.8802946]
   Nataraj Lakshmanan, 2021, Machine Learning, Deep Learning and Computational Intelligence for Wireless Communication. Proceedings of MDCWC 2020. Lecture Notes in Electrical Engineering (LNEE 749), P381, DOI 10.1007/978-981-16-0289-4_29
   NazarI H., 2020, 2020 4 INT S MULT ST, P1
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Sarkar A, 2009, MM&SEC'09: PROCEEDINGS OF THE 2009 ACM SIGMM MULTIMEDIA AND SECURITY WORKSHOP, P107
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   Senturk Z.K., 2019, 2019 1 INT INF SOFTW, P1
   Senturk ZK, 2021, ELEKTRON ELEKTROTECH, V27, P59, DOI 10.5755/j02.eie.29050
   Tan MX, 2019, PR MACH LEARN RES, V97
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wei JD, 2014, PATTERN RECOGN LETT, V36, P100, DOI 10.1016/j.patrec.2013.09.026
   Yan B, 2013, IEEE T CIRC SYST VID, V23, P313, DOI 10.1109/TCSVT.2012.2203740
   Yang B, 2013, NEUROCOMPUTING, V120, P365, DOI 10.1016/j.neucom.2012.10.032
   Ye JY, 2019, LECT NOTES COMPUT SC, V11378, P3, DOI 10.1007/978-3-030-11389-6_1
   Yin T, 2015, COMPUT SECUR, V55, P130, DOI 10.1016/j.cose.2015.09.003
   Zhang DY, 2020, SECUR COMMUN NETW, V2020, DOI 10.1155/2020/8830310
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhou P, 2018, PROC CVPR IEEE, P1053, DOI 10.1109/CVPR.2018.00116
NR 36
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104135
DI 10.1016/j.jvcir.2024.104135
EA APR 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RW6C1
UT WOS:001230725700001
DA 2024-08-05
ER

PT J
AU Jiang, JQ
   Li, L
   Tan, B
   Duan, LH
   Yao, J
AF Jiang, Jiaqin
   Li, Li
   Tan, Bin
   Duan, Lunhao
   Yao, Jian
TI A multi-view references image super-resolution framework for generating
   the large-FOV and high-resolution image
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Large-FOV HR images; Reference-based super-resolution; Adaptive feature
   fusion; Structural information
AB This paper investigates the generation of large field -of -view (FOV) and high -resolution (HR) panorama images for smartphones. Existing techniques like image stitching struggle to produce satisfactory results due to geometry misalignment and inconsistent appearance. To circumvent the inherent challenges of image stitching methods and generate high -quality panoramas, we treat the image stitching problem as a multiple -referencebased super -resolution problem. Specifically, one large-FOV low -resolution (LR) image and several overlapped small-FOV HR images are taken as inputs, where the LR image acts as a base and multi -view references provide rich HR information. Building on this foundation, a novel multi -view references image super -resolution framework (MVRefSR) is proposed. Within this framework, to address the residual geometric misalignment between the LR-Ref image pairs after coarse alignment, a flow -based RefSR network (FlowSRNet) is proposed, which super -resolves LR patches with corresponding HR references. To facilitate adaptive feature fusion and minimize distortion in structured regions, a fusion weight estimation module and a gradient branch are introduced in FlowSRNet. Finally, the large-FOV HR image is generated by combining these SR patches together. Furthermore, the lack of real -world RefSR datasets for smartphones is addressed by designing an innovative dataset construction pipeline. Extensive experiments demonstrate the superior performance of FlowSRNet and MVRefSR over the compared SR methods and image stitching software, which proves the effectiveness of generating panoramas from a new perspective.
C1 [Jiang, Jiaqin; Li, Li] Wuhan Univ, Sch Remote Sensing Informat Engn, Wuhan, Peoples R China.
   [Tan, Bin] Wuhan Univ, Sch Comp Sci, Wuhan, Peoples R China.
   [Li, Li] Minist Nat Resources, Key Lab Urban Land Resources Monitoring & Simulat, Shenzhen, Peoples R China.
   [Yao, Jian] Wuhan Univ Shenzhen, Res Inst, Shenzhen, Peoples R China.
C3 Wuhan University; Wuhan University; Ministry of Natural Resources of the
   People's Republic of China; Wuhan University
RP Jiang, JQ; Li, L (corresponding author), Wuhan Univ, Sch Remote Sensing Informat Engn, Wuhan, Peoples R China.; Li, L (corresponding author), Minist Nat Resources, Key Lab Urban Land Resources Monitoring & Simulat, Shenzhen, Peoples R China.; Yao, J (corresponding author), Wuhan Univ Shenzhen, Res Inst, Shenzhen, Peoples R China.
EM jiangjiaqin@whu.edu.cn; li.li@whu.edu.cn; jian.yao@whu.edu.cn
FU National Natural Science Foundation of China [42271445, U22A2009,
   42101440]; Shenzhen Science and Technology Program, China; Fundamental
   Research Funds for the Central Universities, China [KF-2022-07-023];
   Open Fund of Key Laboratory of Urban Land Resources Monitoring and
   Simulation, Ministry of Natural Resources, China [2022PGE008];
   Foundation of Anhui Province Key Laboratory of Physical Geographic
   Environment, China [JCYJ20230807090206013];  [2042023kf0174]
FX This work was partially supported by the National Natural Science
   Foundation of China (No. 42271445, No. U22A2009, No. 42101440) , the
   Shenzhen Science and Technology Program, China (JCYJ20230807090206013) ,
   the Fundamental Research Funds for the Central Universities, China
   (2042023kf0174) , the Open Fund of Key Laboratory of Urban Land
   Resources Monitoring and Simulation, Ministry of Natural Resources,
   China (KF-2022-07-023) , the Foundation of Anhui Province Key Laboratory
   of Physical Geographic Environment, China (2022PGE008) .
CR Ali S, 2012, 2012 15TH INTERNATIONAL MULTITOPIC CONFERENCE (INMIC), P209, DOI 10.1109/INMIC.2012.6511464
   [Anonymous], 2021, About Us
   Bhat G, 2021, PROC CVPR IEEE, P9205, DOI 10.1109/CVPR46437.2021.00909
   Cai JR, 2019, IEEE I CONF COMP VIS, P3086, DOI 10.1109/ICCV.2019.00318
   Cao JZ, 2022, LECT NOTES COMPUT SC, V13678, P325, DOI 10.1007/978-3-031-19797-0_19
   Chang CH, 2014, PROC CVPR IEEE, P3254, DOI 10.1109/CVPR.2014.422
   Chen C, 2014, COMPUT GEOSCI-UK, V73, P28, DOI 10.1016/j.cageo.2014.08.007
   Chen K, 2020, Arxiv, DOI arXiv:2004.02478
   Chen PM, 2024, INFORM FUSION, V103, DOI 10.1016/j.inffus.2023.102125
   Chen Y, 2023, Multimedia Tools Appl., P1
   Chen YS, 2016, LECT NOTES COMPUT SC, V9909, P186, DOI 10.1007/978-3-319-46454-1_12
   Chen YT, 2024, EXPERT SYST APPL, V245, DOI 10.1016/j.eswa.2023.123111
   Chen YT, 2023, J KING SAUD UNIV-COM, V35, DOI 10.1016/j.jksuci.2023.101567
   Chen YT, 2023, INT J MACH LEARN CYB, V14, P2945, DOI 10.1007/s13042-023-01811-y
   Chen YT, 2023, J VIS COMMUN IMAGE R, V91, DOI 10.1016/j.jvcir.2023.103776
   Chen YT, 2024, VISUAL COMPUT, V40, P489, DOI 10.1007/s00371-023-02795-0
   Cheng Ma, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7766, DOI 10.1109/CVPR42600.2020.00779
   Trinidad MC, 2019, IEEE I CONF COMP VIS, P4100, DOI 10.1109/ICCV.2019.00420
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Han Zou, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P6124, DOI 10.1109/CVPRW59228.2023.00652
   Hu Kai, 2023, Advanced Intelligent Computing Technology and Applications: 19th International Conference, ICIC 2023, Proceedings. Lecture Notes in Computer Science (14087), P98, DOI 10.1007/978-981-99-4742-3_8
   Huang Y., 2022, CVPR, P5931
   Jiang YM, 2021, PROC CVPR IEEE, P2103, DOI 10.1109/CVPR46437.2021.00214
   Kingma D. P., 2014, arXiv
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lin CC, 2015, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2015.7298719
   Lin KM, 2017, PROC CVPR IEEE, P2701, DOI 10.1109/CVPR.2017.289
   Lin KM, 2016, LECT NOTES COMPUT SC, V9907, P370, DOI 10.1007/978-3-319-46487-9_23
   Liu H, 2023, NEURAL COMPUT APPL, V35, P12331, DOI 10.1007/s00521-020-05687-9
   Liu H, 2019, INFORM SCIENCES, V473, P44, DOI 10.1016/j.ins.2018.09.018
   Lu LY, 2021, PROC CVPR IEEE, P6364, DOI 10.1109/CVPR46437.2021.00630
   Pan J, 2010, IEEE GEOSCI REMOTE S, V7, P401, DOI 10.1109/LGRS.2009.2037442
   Park J, 2016, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2016.53
   Ritter KA, 2022, VIRTUAL REAL-LONDON, V26, P571, DOI 10.1007/s10055-021-00502-9
   Shen TW, 2017, LECT NOTES COMPUT SC, V10114, P392, DOI 10.1007/978-3-319-54190-7_24
   Srinivasan PP, 2017, IEEE I CONF COMP VIS, P2262, DOI 10.1109/ICCV.2017.246
   Tan Y, 2021, IEEE T PATTERN ANAL, V43, P4291, DOI 10.1109/TPAMI.2020.2997007
   Taneja A, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P479, DOI 10.1109/3DIMPVT.2012.45
   Tian CZ, 2021, J VIS COMMUN IMAGE R, V81, DOI 10.1016/j.jvcir.2021.103324
   Wan C, 2024, Arxiv, DOI arXiv:2311.12770
   Wang DY, 2023, IEEE SIGNAL PROC LET, V30, P1717, DOI 10.1109/LSP.2023.3333228
   Wang L, 2021, J VIS COMMUN IMAGE R, V80, DOI 10.1016/j.jvcir.2021.103300
   Wang S., 2023, Vis. Comput., P1
   Wang Tengfei, 2021, PROC IEEECVF INT C C, P2001
   Wang TY, 2017, ACTA OCEANOL SIN, V36, P1, DOI 10.1007/s13131-017-0987-1
   Wang Y., 2023, arXiv
   Win KP, 2018, INT CONF INTEL INFOR, P221, DOI 10.1109/ICIIBMS.2018.8549931
   Xie RP, 2018, ISPRS J PHOTOGRAMM, V135, P43, DOI 10.1016/j.isprsjprs.2017.11.012
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yang M., 2021, P IEEE INT C COMM CO, P1
   Yu L, 2017, IEEE GEOSCI REMOTE S, V14, P729, DOI 10.1109/LGRS.2017.2676438
   Zhang J., 2024, IEEE Trans. Emerg. Top. Comput.
   Zhang JM, 2024, COMPUT ELECTR ENG, V114, DOI 10.1016/j.compeleceng.2024.109075
   Zhang L., 2023, P IEEE CVF INT C COM, P13118, DOI DOI 10.48550/ARXIV.2303.04970
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang WP, 2018, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-018-0323-5
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang Y, 2021, IEEE T VIS COMPUT GR, V27, P3198, DOI 10.1109/TVCG.2020.2965097
   Zhang ZF, 2019, PROC CVPR IEEE, P7974, DOI 10.1109/CVPR.2019.00817
   Zhao MD, 2018, IEEE T COMPUT IMAG, V4, P406, DOI 10.1109/TCI.2018.2838457
   Zheng H., 2018, P EUROPEAN C COMPUTE, P88, DOI DOI 10.1007/978-3-030-01231-16
   Zheng Haitian, 2017, P BRIT MACH VIS C 20, V1, P2, DOI DOI 10.5244/C.31.138
NR 63
TC 0
Z9 0
U1 5
U2 5
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104123
DI 10.1016/j.jvcir.2024.104123
EA MAR 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PX7Z8
UT WOS:001217458000001
DA 2024-08-05
ER

PT J
AU Jiang, Y
   Liu, BX
   Zhang, ZQ
   Yan, Y
   Guo, HT
   Li, YH
AF Jiang, Yun
   Liu, Bingxi
   Zhang, Zequn
   Yan, Yao
   Guo, Huanting
   Li, Yuhang
TI Dense-sparse representation matters: A point-based method for volumetric
   medical image segmentation
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Convolutional neural networks; Transformer; Volumetric images; Point
   cloud
ID NET
AB Deep learning methods utilizing Convolutional Neural Networks (CNNs) and Transformers have achieved remarkable success in volumetric medical image analysis. While successful, the symmetrical structure of numerous networks pays insufficient attention to the encoding phase, and the large amount of memory occupied by voxels leads to unnecessary redundancy in the network. In this paper, we present a novel approach to handle volumetric medical images by converting them into point cloud and introduce a new asymmetrical segmentation architecture. We propose a dual -path encoder that fully captures both dense and sparse representations of the input point cloud sampled from volumes. Moreover, the two obtained representations are subtracted at the skip connection as a complementary feature during the decoding stage. Experimental results on the Brain Tumor Segmentation (BraTS) and the Multi -sequence Cardiac MR Segmentation tasks demonstrate the great potential of our point -based method for volumetric medical image segmentation.
C1 [Jiang, Yun; Liu, Bingxi; Zhang, Zequn; Yan, Yao; Guo, Huanting; Li, Yuhang] Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou 730070, Gansu, Peoples R China.
C3 Northwest Normal University - China
RP Liu, BX (corresponding author), Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou 730070, Gansu, Peoples R China.
EM jiangyun@nwnu.edu.cn; 2022222292@nwnu.edu.cn; 2021222178@nwnu.edu.cn;
   2022222261@nwnu.edu.cn; 2022222240@nwnu.edu.cn; 2023222201@nwnu.edu.cn
OI ZHANG, Zequn/0000-0001-5566-761X; yan, yao/0009-0007-9993-9286
FU National Natural Science Foundation of China [61962054, 62302246]; Gansu
   Provincial Key RD Program [22YF7FA123]; Major Scientific Research
   Project Cultivation Program, Northwest Normal University, Lanzhou, China
   [NWNU-LKZD2021-06]
FX This work is supported in part by the National Natural Science
   Foundation of China [Grant 61962054, 62302246]; Gansu Provincial Key R&D
   Program [Grant 22YF7FA123]; Major Scientific Research Project
   Cultivation Program, Northwest Normal University, Lanzhou, China [Grant
   NWNU-LKZD2021-06]:
CR Bakas S, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.117
   Bakas Spyridon, 2019, Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the brats challenge
   Bui T. D., 2017, 3d densely convolutional networks for volumetric segmentation
   Cao Hu, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13803), P205, DOI 10.1007/978-3-031-25066-8_9
   Chen JP, 2021, NAT PHOTONICS, V15, P570, DOI 10.1038/s41566-021-00828-5
   Chen ZY, 2022, ISPRS J PHOTOGRAMM, V194, P58, DOI 10.1016/j.isprsjprs.2022.09.017
   Cheng Junlong, 2023, SegNetr: rethinking the local-global interactions and skip connections in u-shaped networks, P64
   Cicek Ozgun, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P424, DOI 10.1007/978-3-319-46723-8_49
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dolz J, 2019, IEEE T MED IMAGING, V38, P1116, DOI 10.1109/TMI.2018.2878669
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Gao Shangqi, 2023, arXiv
   Hatamizadeh A, 2022, IEEE WINT CONF APPL, P1748, DOI 10.1109/WACV51458.2022.00181
   Hatamizadeh Ali, 2022, UNetFormer: a unified vision transformer model and pre-training framework for 3D medical image segmentation
   He Sheng, 2023, U-netmer: u-net meets transformer for medical image segmentation
   Isensee F, 2021, LECT NOTES COMPUT SC, V12659, P118, DOI 10.1007/978-3-030-72087-2_11
   Isensee F, 2021, NAT METHODS, V18, P203, DOI 10.1038/s41592-020-01008-z
   Jiang Y, 2022, BRAIN SCI, V12, DOI 10.3390/brainsci12060797
   Jiang ZY, 2020, LECT NOTES COMPUT SC, V11992, P231, DOI 10.1007/978-3-030-46640-4_22
   Kirillov Alexander, 2023, Segment Anything, P4015
   Lai X, 2022, PROC CVPR IEEE, P8490, DOI 10.1109/CVPR52688.2022.00831
   Lee SH, 2021, J VIS COMMUN IMAGE R, V79, DOI 10.1016/j.jvcir.2021.103246
   Lequan Yu, 2017, Medical Image Computing and Computer-Assisted Intervention, MICCAI 2017. 20th International Conference. Proceedings: LNCS 10434, P287, DOI 10.1007/978-3-319-66185-8_33
   Li X, 2022, J VIS COMMUN IMAGE R, V89, DOI 10.1016/j.jvcir.2022.103641
   Li YZ, 2018, ADV NEUR IN, V31
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma Xu, 2022, INT C LEARN REPR
   Ma Xu, 2023, Image as set of points
   Menze BH, 2015, IEEE T MED IMAGING, V34, P1993, DOI 10.1109/TMI.2014.2377694
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Ho NV, 2021, LECT NOTES COMPUT SC, V12901, P644, DOI 10.1007/978-3-030-87193-2_61
   Nie YY, 2021, PROC CVPR IEEE, P4606, DOI 10.1109/CVPR46437.2021.00458
   Oktay O., 2018, P 1 C MED IM DEEP LE, P1, DOI 10.48550/arXiv.1804.03999
   Qi CR, 2017, ADV NEUR IN, V30
   Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Siddique N, 2021, IEEE ACCESS, V9, P82031, DOI 10.1109/ACCESS.2021.3086020
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang WX, 2021, LECT NOTES COMPUT SC, V12901, P109, DOI 10.1007/978-3-030-87193-2_11
   Wu FP, 2023, IEEE T PATTERN ANAL, V45, P6021, DOI 10.1109/TPAMI.2022.3215186
   Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563
   Zeng Ziyin, 2023, Small but mighty: enhancing 3D point clouds semantic segmentation with u-next framework
   Zhang ZQ, 2022, COMPUT BIOL MED, V150, DOI 10.1016/j.compbiomed.2022.106146
   Zhou Hong-Yu, 2022, Nnformer: interleaved transformer for volumetric segmentation
   Zhou Junjie, 2023, SAT: size-aware transformer for 3D point cloud semantic segmentation
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
   Zhuang XH, 2019, IEEE T PATTERN ANAL, V41, P2933, DOI 10.1109/TPAMI.2018.2869576
NR 47
TC 0
Z9 0
U1 9
U2 9
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104115
DI 10.1016/j.jvcir.2024.104115
EA MAR 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QS9U9
UT WOS:001222985800001
DA 2024-08-05
ER

PT J
AU Lin, C
   Wu, YF
   Huang, K
   Yang, H
   Deng, YQ
   Wen, YM
AF Lin, Cong
   Wu, Yufeng
   Huang, Ke
   Yang, Hai
   Deng, Yuqiao
   Wen, Yamin
TI Copy-move forgery detection using Regional Density Center clustering
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Image forensics; Copy-move forgery detection; Density search; Core
   point; Filtering policy
AB Copy-move forgery detection is a common image tampering detection technology. In this paper, a novel copymove forgery detection scheme is proposed. The proposed scheme is based on Regional Density Center (RDC) clustering and Refined Length Homogeneity Filtering (RLHF) policy. First, to obtain an adequate number of keypoints in smooth or small areas of the image, the proposed scheme employs scale normalization and adjustment of the contrast threshold of the input image. Subsequently, to speed up the feature matching process, a matching algorithm based on gray value grouping is used to match the keypoints. RLHF policy is applied to filter the mismatched pairs. To guarantee a good estimation of the affine transformation, the RDC clustering algorithm is proposed to group the matched pairs. Finally, the correlation coefficients are computed to precisely locate the tampered regions. The proposed copy-move forgery detection scheme based on RDC and RLHF can effectively identify duplicated regions of digital images. It demonstrates the effectiveness and robustness of the proposed scheme over many state-of-the-art schemes on public datasets.
C1 [Lin, Cong; Wu, Yufeng; Deng, Yuqiao; Wen, Yamin] Guangdong Univ Finance & Econ, Sch Stat & Math, Dig Data & Educ Stat Applicat Lab, Guangzhou 510320, Peoples R China.
   [Huang, Ke; Yang, Hai] Guangdong Univ Finance & Econ, Sch Informat, Guangzhou 510320, Peoples R China.
   [Wu, Yufeng; Wen, Yamin] Sun Yat Sen Univ, Guangdong Prov Key Lab Informat Secur Technol, Guangzhou 510006, Peoples R China.
C3 Guangdong University of Finance & Economics; Guangdong University of
   Finance & Economics; Sun Yat Sen University
RP Lin, C (corresponding author), Guangdong Univ Finance & Econ, Sch Stat & Math, Dig Data & Educ Stat Applicat Lab, Guangzhou 510320, Peoples R China.
EM lincong0310@gmail.com; 2891653072@qq.com; jacobi0727@163.com;
   3247695808@qq.com; 425478541@qq.com; wenyamin@gdufe.edu.cn
FU Characteristic Innovation Project of Regular Institutions of Higher
   Learning of Guangdong Province (Natural Science) [2022KTSCX041]; Basic
   and Applied Basic Research Project of Guangzhou Science and Technology
   Program [202102080316]; Opening Project of Guangdong Province Key
   Laboratory of Information Security Technology [2023B1212060026]; Science
   and Technology Program of Guangzhou Haizhu District [2022-45]
FX This work is supported by Characteristic Innovation Project of Regular
   Institutions of Higher Learning of Guangdong Province (Natural Science)
   (2022KTSCX041) ; Basic and Applied Basic Research Project of Guangzhou
   Science and Technology Program (202102080316) ; Opening Project of
   Guangdong Province Key Laboratory of Information Security Technology
   (No. 2023B1212060026) ; Science and Technology Program of Guangzhou
   Haizhu District (2022-45) .
CR Abhishek, 2021, MULTIMED TOOLS APPL, V80, P3571, DOI 10.1007/s11042-020-09816-3
   Abir NAM, 2023, MULTIMED TOOLS APPL, DOI 10.1007/s11042-023-15506-7
   Agarwal R, 2022, EVOL SYST-GER, V13, P27, DOI 10.1007/s12530-021-09367-4
   Al-Qershi OM, 2019, MULTIDIM SYST SIGN P, V30, P1671, DOI 10.1007/s11045-018-0624-y
   Alcantarilla PF, 2012, LECT NOTES COMPUT SC, V7577, P214, DOI 10.1007/978-3-642-33783-3_16
   Amerini I, 2013, SIGNAL PROCESS-IMAGE, V28, P659, DOI 10.1016/j.image.2013.03.006
   Amerini I, 2011, IEEE T INF FOREN SEC, V6, P1099, DOI 10.1109/TIFS.2011.2129512
   Babu SBGT, 2023, BIG DATA MIN ANAL, V6, P347, DOI 10.26599/BDMA.2022.9020029
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Barni M, 2021, IEEE T INF FOREN SEC, V16, P1825, DOI 10.1109/TIFS.2020.3045903
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bilal M, 2021, AUST J FORENSIC SCI, V53, P459, DOI 10.1080/00450618.2020.1715479
   Christlein V, 2012, IEEE T INF FOREN SEC, V7, P1841, DOI 10.1109/TIFS.2012.2218597
   Cozzolino D, 2015, IEEE T INF FOREN SEC, V10, P2284, DOI 10.1109/TIFS.2015.2455334
   Emam M, 2016, MULTIMED TOOLS APPL, V75, P11513, DOI 10.1007/s11042-015-2872-2
   Fatima B, 2022, MULTIMED TOOLS APPL, V81, P43805, DOI 10.1007/s11042-022-12915-y
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fridrich J., 2003, P DIG FOR RES WORKSH, P19
   Gan YN, 2022, INFORM PROCESS MANAG, V59, DOI 10.1016/j.ipm.2021.102783
   Kumar S, 2022, J VIS COMMUN IMAGE R, V89, DOI 10.1016/j.jvcir.2022.103644
   Kumar S, 2023, MULTIMED TOOLS APPL, V82, P1431, DOI 10.1007/s11042-022-12391-4
   Li YM, 2019, IEEE T INF FOREN SEC, V14, P1307, DOI 10.1109/TIFS.2018.2876837
   Li YA, 2013, FORENSIC SCI INT, V224, P59, DOI 10.1016/j.forsciint.2012.10.031
   Lin C, 2019, MULTIMED TOOLS APPL, V78, P30081, DOI 10.1007/s11042-018-6922-4
   Lin C, 2019, MULTIMED TOOLS APPL, V78, P20739, DOI 10.1007/s11042-019-7342-9
   Lin C, 2018, MULTIMED TOOLS APPL, V77, P14241, DOI 10.1007/s11042-017-5027-9
   Liu B, 2018, SIGNAL PROCESS-IMAGE, V66, P103, DOI 10.1016/j.image.2018.04.011
   Liu L, 2014, 2014 TENTH INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING (IIH-MSP 2014), P626, DOI 10.1109/IIH-MSP.2014.162
   Liu YQ, 2022, IEEE T IMAGE PROCESS, V31, P541, DOI 10.1109/TIP.2021.3132828
   Lyu QY, 2021, J VIS COMMUN IMAGE R, V76, DOI 10.1016/j.jvcir.2021.103057
   Maashi M, 2023, IEEE ACCESS, V11, P87297, DOI 10.1109/ACCESS.2023.3304237
   Mahdian B, 2007, FORENSIC SCI INT, V171, P180, DOI 10.1016/j.forsciint.2006.11.002
   McInnes L, 2017, INT CONF DAT MIN WOR, P33, DOI 10.1109/ICDMW.2017.12
   Muhammad G, 2012, DIGIT INVEST, V9, P49, DOI 10.1016/j.diin.2012.04.004
   Narasimhamurthy S.K., 2023, Int. J. Eng. Intell. Eng. Syst., V16, P12
   Nazir T, 2022, APPL SOFT COMPUT, V131, DOI 10.1016/j.asoc.2022.109778
   Niu P, 2021, J VIS COMMUN IMAGE R, V77, DOI 10.1016/j.jvcir.2021.103068
   Pan XY, 2010, IEEE T INF FOREN SEC, V5, P857, DOI 10.1109/TIFS.2010.2078506
   Rao Y, 2016, IEEE INT WORKS INFOR
   Ryu SJ, 2013, IEEE T INF FOREN SEC, V8, P1355, DOI 10.1109/TIFS.2013.2272377
   Ryu SJ, 2010, LECT NOTES COMPUT SC, V6387, P51, DOI 10.1007/978-3-642-16435-4_5
   Shehin AU, 2024, J VIS COMMUN IMAGE R, V99, DOI 10.1016/j.jvcir.2024.104075
   Shivakumar B., 2011, International Journal of Computer Science Issues (IJCSI), V8, P199
   Silva E, 2015, J VIS COMMUN IMAGE R, V29, P16, DOI 10.1016/j.jvcir.2015.01.016
   Sujin JS, 2024, SOFT COMPUT, V28, P437, DOI 10.1007/s00500-023-08209-6
   Vaishali S, 2024, MULTIMED TOOLS APPL, V83, P10839, DOI 10.1007/s11042-023-15724-z
   Vaishnavi D, 2023, J INTELL FUZZY SYST, V45, P10267, DOI 10.3233/JIFS-230291
   Wang C, 2023, IEEE T INF FOREN SEC, V18, P1064, DOI 10.1109/TIFS.2023.3234861
   Wang XY, 2017, MULTIMED TOOLS APPL, V76, P23353, DOI 10.1007/s11042-016-4140-5
   Wang XY, 2022, J VIS COMMUN IMAGE R, V89, DOI 10.1016/j.jvcir.2022.103658
   Wang YL, 2020, J INF SECUR APPL, V54, DOI 10.1016/j.jisa.2020.102536
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weng SW, 2024, IEEE T MULTIMEDIA, V26, P750, DOI 10.1109/TMM.2023.3270629
   Yang F, 2017, ENG APPL ARTIF INTEL, V59, P73, DOI 10.1016/j.engappai.2016.12.022
   Zandi M, 2016, IEEE T INF FOREN SEC, V11, P2499, DOI 10.1109/TIFS.2016.2585118
   Zhang Y., 2022, IEEE Trans. Circuits Syst. Video Technol.
   Zhong JL, 2020, IEEE T INF FOREN SEC, V15, P2134, DOI 10.1109/TIFS.2019.2957693
   Zhu Y, 2020, IEEE T IND INFORM, V16, P6714, DOI 10.1109/TII.2020.2982705
   Zhu Y, 2016, MULTIMED TOOLS APPL, V75, P3221, DOI 10.1007/s11042-014-2431-2
NR 59
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD AUG
PY 2024
VL 103
AR 104221
DI 10.1016/j.jvcir.2024.104221
EA JUL 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YL0F2
UT WOS:001268518300001
DA 2024-08-05
ER

PT J
AU Tan, YM
   Xia, HY
   Song, SX
AF Tan, Yumei
   Xia, Haiying
   Song, Shuxiang
TI Learning informative and discriminative semantic features for robust
   facial expression recognition
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Facial expression recognition; Attention; Informative features; Robust
   learning
ID DEEP; NETWORK; ATTENTION
AB Facial expression recognition (FER) becomes challenging in real -world scenarios, which requires learning informative and discriminative features from challenging datasets to obtain robust facial expression recognition. In this paper, we propose an Informative and Discriminative Semantic Features Learning (IDSFL) network for FER against occlusion and head pose in the wild. Specifically, IDSFL aims to mine informative and discriminative semantic features from both low and high levels learned features to learn robust representations. First, a multi -channel feature (MCF) modulator incorporating low-level Gabor features is introduced to learn informative semantic features by capturing adequate diverse and detailed information. Additionally, a specific emotion-aware (SEA) module is proposed to learn discriminative semantic features by aggregating highlevel emotion-specific features to focus on each expression category. Thus, IDSFL can collaboratively learn informative and discriminative representations. Extensive experiments on challenging in -the -wild datasets, including RAF -DB, FERPlus and AffectNet-7, demonstrate that our proposed method outperforms most state -of -the -art FER methods.
C1 [Tan, Yumei] Guangxi Normal Univ, Sch Comp Sci & Engn, Guilin 541004, Peoples R China.
   [Xia, Haiying; Song, Shuxiang] Guangxi Normal Univ, Sch Elect & Informat Engn, Guilin 541004, Peoples R China.
C3 Guangxi Normal University; Guangxi Normal University
RP Song, SX (corresponding author), Guangxi Normal Univ, Sch Elect & Informat Engn, Guilin 541004, Peoples R China.
EM tanyumei@stu.gxnu.edu.cn; xhy22@mailbox.gxnu.edu.cn;
   songshuxiang@mailbox.gxnu.edu.cn
FU National Natural Science Foundation of China [62106054, 62167001];
   Research Projects of Guangxi Normal University (Natural Sciences)
   [2021JC012]
FX This work was in part supported by the National Natural Science
   Foundation of China under Grants 62106054 and 62167001, and The Research
   Projects of Guangxi Normal University (Natural Sciences) (2021JC012) .
CR Amos B., 2016, Openface: A general-purpose face recognition library with mobile applications
   Barsoum E, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P279, DOI 10.1145/2993148.2993165
   Benitez-Quiroz CF, 2016, PROC CVPR IEEE, P5562, DOI 10.1109/CVPR.2016.600
   Dharanya V, 2021, J VIS COMMUN IMAGE R, V77, DOI 10.1016/j.jvcir.2021.103110
   Dias W, 2022, J VIS COMMUN IMAGE R, V82, DOI 10.1016/j.jvcir.2021.103395
   Hoai DPV, 2019, 2019 INTERNATIONAL CONFERENCE ON MULTIMEDIA ANALYSIS AND PATTERN RECOGNITION (MAPR), DOI 10.1109/mapr.2019.8743528
   Fa S, 2023, J VIS COMMUN IMAGE R, V93, DOI 10.1016/j.jvcir.2023.103826
   Fei ZX, 2020, NEUROCOMPUTING, V388, P212, DOI 10.1016/j.neucom.2020.01.034
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Li H., 2021, arXiv
   Li HD, 2020, KNOWL-BASED SYST, V204, DOI 10.1016/j.knosys.2020.106172
   Li HH, 2023, VISUAL COMPUT, V39, P4709, DOI 10.1007/s00371-022-02619-7
   Li R, 2015, INT CONF COMPUT INTE, P347, DOI 10.1109/CICN.2015.75
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Li W, 2018, IEEE T PATTERN ANAL, V40, P2583, DOI 10.1109/TPAMI.2018.2791608
   Li X, 2019, Arxiv, DOI arXiv:1905.09646
   Li YJ, 2023, IEEE T AFFECT COMPUT, V14, P451, DOI 10.1109/TAFFC.2020.3031602
   Li YJ, 2022, IEEE T CIRC SYST VID, V32, P3178, DOI 10.1109/TCSVT.2021.3103760
   Li YJ, 2019, PR MACH LEARN RES, V101, P897
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P2439, DOI 10.1109/TIP.2018.2886767
   Liao L, 2022, MACH VISION APPL, V33, DOI 10.1007/s00138-022-01288-9
   Liu C, 2023, INFORM SCIENCES, V619, P781, DOI 10.1016/j.ins.2022.11.068
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Ma FY, 2023, IEEE T AFFECT COMPUT, V14, P1236, DOI 10.1109/TAFFC.2021.3122146
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Müller R, 2019, ADV NEUR IN, V32
   Nan F, 2022, KNOWL-BASED SYST, V236, DOI 10.1016/j.knosys.2021.107678
   Olivares-Mercado J, 2019, I W BIOMETRIC FORENS, DOI 10.1109/iwbf.2019.8739178
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Picard RW, 2001, IEEE T PATTERN ANAL, V23, P1175, DOI 10.1109/34.954607
   She JH, 2021, PROC CVPR IEEE, P6244, DOI 10.1109/CVPR46437.2021.00618
   Su C, 2023, PATTERN ANAL APPL, V26, P543, DOI 10.1007/s10044-022-01124-w
   Sun Z, 2022, J VIS COMMUN IMAGE R, V85, DOI 10.1016/j.jvcir.2022.103458
   Sun Z, 2020, KNOWL-BASED SYST, V204, DOI 10.1016/j.knosys.2020.106124
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Wang C, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P238, DOI 10.1145/3343031.3350872
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang ZL, 2022, J VIS COMMUN IMAGE R, V89, DOI 10.1016/j.jvcir.2022.103679
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1007/s11263-019-01198-w, 10.1109/CSTIC.2018.8369274]
   Xia HY, 2021, IEEE MULTIMEDIA, V28, P20, DOI 10.1109/MMUL.2021.3076834
   Xia YF, 2022, IEEE T COGN DEV SYST, V14, P1143, DOI 10.1109/TCDS.2021.3100131
   Xie SY, 2019, PATTERN RECOGN, V92, P177, DOI 10.1016/j.patcog.2019.03.019
   Yang L, 2020, KNOWL-BASED SYST, V204, DOI 10.1016/j.knosys.2020.106217
   Yao LS, 2022, WIRELESS PERS COMMUN, V125, P1483, DOI 10.1007/s11277-022-09616-y
   Yovel G, 2006, J COGNITIVE NEUROSCI, V18, P580, DOI 10.1162/jocn.2006.18.4.580
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zeng D, 2022, PROC CVPR IEEE, P20259, DOI 10.1109/CVPR52688.2022.01965
   Zhang CB, 2021, IEEE T IMAGE PROCESS, V30, P5984, DOI 10.1109/TIP.2021.3089942
   Zhang H, 2023, LECT NOTES COMPUT SC, V13843, P541, DOI 10.1007/978-3-031-26313-2_33
   Zhao ZQ, 2021, AAAI CONF ARTIF INTE, V35, P3510
   Zhao ZQ, 2021, IEEE T IMAGE PROCESS, V30, P6544, DOI 10.1109/TIP.2021.3093397
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P184, DOI 10.1109/ICCV48922.2021.00025
   Zoran D., 2020, P IEEECVF C COMPUTER
NR 60
TC 0
Z9 0
U1 7
U2 7
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104062
DI 10.1016/j.jvcir.2024.104062
EA JAN 2024
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JH5V4
UT WOS:001172295600001
DA 2024-08-05
ER

PT J
AU Xu, YF
   Li, JJ
   Wei, PP
   Wang, AC
   Rao, Y
AF Xu, Yifei
   Li, Jingjing
   Wei, Pingping
   Wang, Aichen
   Rao, Yuan
TI A dual-branch residual network for inhomogeneous dehazing
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Single image dehazing; Dual-branch; Residual network
ID CONTRAST ENHANCEMENT; IMAGE
AB Image dehazing has now gained the dominant popularity in the field of image processing, particularly in inhomogeneous scene. Recent years have witnessed great progress in handling homogeneous dehazing problems. Due to the unknown haze distribution of the real world, it is extremely intractable to offer a clear view of the observed scene in limited inhomogeneous datasets. Furthermore, it is impossible to completely avoid artifacts of color distortion, over -enhancement, halo, and blur errors in order to provide reliable and stable results. To this end, we propose an end -to -end Dual -branch Residual Network (DBRN) for inhomogeneous dehazing that is composed of Hybrid Feature Subnet (HFS) and Attention Feature Fusion Subnet (AFFS). HFS explores high -quality global and local hazy features with long-range spatial fusion at different scales in an encoder-decoder manner, while AFFS makes artifact removal possible with a stack of Residual Convolution Attention Module (RCAM). Besides, the joint loss function aims to ensure that the recovered image is close to the ground -truth in the aspects of texture, color, structure index, and so on. Through this design, the model exhibits robustness in inhomogeneous hazy scenes, enabling high -quality visual restoration in scenes with varying haze densities. Extensive experimental results demonstrate that the proposed model performs favorably against the state-of-the-art methods on synthetic datasets and real -world hazy images. In addition, ablation studies are carried out to demonstrate the effectiveness of each component. The source code of the proposed method is available at https://github.com/jing-1196/DBRN/.
C1 [Xu, Yifei; Li, Jingjing; Rao, Yuan] Xi An Jiao Tong Univ, Sch Software, Xian 710054, Shaanxi, Peoples R China.
   [Xu, Yifei; Li, Jingjing; Rao, Yuan] Shaanxi Joint Key Lab Artifact Intelligence, Shaanxi 710054, Peoples R China.
   [Wei, Pingping] Xi An Jiao Tong Univ, State Key Lab Precis Micronano Mfg Technol, Xian 710054, Shaanxi, Peoples R China.
   [Wang, Aichen] Jiangsu Univ, Key Lab Modern Agr Equipment & Technol, Zhenjiang 212013, Peoples R China.
   [Xu, Yifei; Li, Jingjing; Rao, Yuan] Xian Key Lab Social Intelligence & Complex Data Pr, Shaanxi 710054, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; Jiangsu University
RP Xu, YF (corresponding author), Xi An Jiao Tong Univ, Sch Software, Xian 710054, Shaanxi, Peoples R China.
EM belonxu_1@xjtu.edu.cn; 13180651619@stu.xjtu.edu.cn;
   erin1989@xjtu.edu.cn; acwang@ujs.edu.cn; raoyuan@mail.xjtu.edu.cn
RI Li, Jingjing/T-6522-2019
FU Natural Science Basic Research Program of Shaanxi [2024JC-YBMS-498]
FX <B>Acknowledgments</B> This work was supported in part by Natural
   Science Basic Research Program of Shaanxi (Program No. 2024JC-YBMS-498)
CR Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti C, 2018, LECT NOTES COMPUT SC, V11182, P620, DOI 10.1007/978-3-030-01449-0_52
   Archa S., 2014, Int. J. Sci. Eng. Technol. Res., V3, P4808
   Banala R., 2024, QSU-net: QCNN with self-attention based U-net for single image dehazing
   Bui TM, 2018, IEEE T IMAGE PROCESS, V27, P999, DOI 10.1109/TIP.2017.2771158
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Dippel S, 2002, IEEE T MED IMAGING, V21, P343, DOI 10.1109/TMI.2002.1000258
   Fang FM, 2020, IEEE T MULTIMEDIA, V22, P2537, DOI 10.1109/TMM.2019.2958755
   Frants V, 2023, IEEE T CYBERNETICS, V53, P5448, DOI 10.1109/TCYB.2023.3238640
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gu B, 2024, IET IMAGE PROCESS, V18, P1014, DOI 10.1049/ipr2.13003
   Han HN, 2022, SIGNAL IMAGE VIDEO P, V16, P1297, DOI 10.1007/s11760-021-02081-3
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jia T., 2023, IEEE Trans. Multimed
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Ju MY, 2021, IEEE T IMAGE PROCESS, V30, P2180, DOI 10.1109/TIP.2021.3050643
   Jun W., 2013, Appl. Math. Sci., V7, P3913
   Kim JY, 2001, IEEE T CIRC SYST VID, V11, P475, DOI 10.1109/76.915354
   Kim M, 2008, IEEE T CONSUM ELECTR, V54, P1389, DOI 10.1109/TCE.2008.4637632
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li YS, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107293
   Ling PY, 2023, IEEE T IMAGE PROCESS, V32, P3238, DOI 10.1109/TIP.2023.3279980
   Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283
   Lyu Z, 2024, PATTERN RECOGN, V150, DOI 10.1016/j.patcog.2024.110290
   Memon S, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103748
   Middleton W.E.K, 1957, Vision Through the Atmosphere, P254, DOI [10.1007/978-3-642-45881-1_3, DOI 10.1007/978-3-642-45881-1_3]
   Niklaus S, 2018, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2018.00183
   Patel O., 2013, Signal Image Process, V4, P11, DOI 10.5121/sipij.2013.4502
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ramya C., 2012, Int. J. Comput. Appl, V54
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Seow MJ, 2006, NEUROCOMPUTING, V69, P954, DOI 10.1016/j.neucom.2005.07.003
   Shekar AK, 2021, INT J COMPUT VISION, V129, P1185, DOI 10.1007/s11263-020-01423-x
   Shin J, 2020, IEEE T MULTIMEDIA, V22, P30, DOI 10.1109/TMM.2019.2922127
   Tu Z., 2022, IEEE C COMPUT VIS PA, P5769
   Wang GT, 2019, PROC CVPR IEEE, P3638, DOI 10.1109/CVPR.2019.00376
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang ZY, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2486, DOI 10.1145/3219819.3219944
   Yi QS, 2022, IEEE T MULTIMEDIA, V24, P3114, DOI 10.1109/TMM.2021.3093724
   Yoon I, 2012, IEEE T CONSUM ELECTR, V58, P111, DOI 10.1109/TCE.2012.6170062
   Yu YK, 2021, IEEE COMPUT SOC CONF, P193, DOI 10.1109/CVPRW53098.2021.00028
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang QL, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2235, DOI 10.1109/ICASSP39728.2021.9414568
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang SL, 2024, Arxiv, DOI arXiv:2401.04550
   Zhang Z., 2023, ICEEIE 2023, V12922, P182
   Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
   Zheng LR, 2023, IEEE T MULTIMEDIA, V25, P6794, DOI 10.1109/TMM.2022.3214780
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 57
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD JUN
PY 2024
VL 102
AR 104191
DI 10.1016/j.jvcir.2024.104191
EA JUN 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WA1L7
UT WOS:001252055900001
DA 2024-08-05
ER

PT J
AU Tao, YR
   Hu, YS
   Chen, ZZ
AF Tao, Yiran
   Hu, Yaosi
   Chen, Zhenzhong
TI Memory-guided representation matching for unsupervised video anomaly
   detection
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Video anomaly detection; Video understanding; Representation learning
ID EVENT DETECTION
AB Recent works on Video Anomaly Detection (VAD) have made advancements in the unsupervised setting, known as Unsupervised VAD (UVAD), which brings it closer to practical applications. Unlike the classic VAD task that requires a clean training set with only normal events, UVAD aims to identify abnormal frames without any labeled normal/abnormal training data. Many existing UVAD methods employ handcrafted surrogate tasks, such as frame reconstruction, to address this challenge. However, we argue that these surrogate tasks are sub-optimal solutions, inconsistent with the essence of anomaly detection. In this paper, we propose a novel approach for UVAD that directly detects anomalies based on similarities between events in videos. Our method generates representations for events while simultaneously capturing prototypical normality patterns, and detects anomalies based on whether an event's representation matches the captured patterns. The proposed model comprises a memory module to capture normality patterns, and a representation learning network to obtain representations matching the memory module for normal events. A pseudo -label generation module as well as an anomalous event generation module for negative learning are further designed to assist the model to work under the strictly unsupervised setting. Experimental results demonstrate that the proposed method outperforms existing UVAD methods and achieves competitive performance compared with classic VAD methods.
C1 [Hu, Yaosi; Chen, Zhenzhong] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430072, Peoples R China.
   [Tao, Yiran; Chen, Zhenzhong] Hubei Luojia Lab, Wuhan 430079, Peoples R China.
   [Tao, Yiran] Wuhan Univ, Sch Foreign Languages & Literature, Wuhan 430072, Peoples R China.
C3 Wuhan University; Wuhan University
RP Chen, ZZ (corresponding author), Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430072, Peoples R China.
EM taoyiran@whu.edu.cn; ys_hu@whu.edu.cn; zzchen@whu.edu.cn
FU National Natural Science Foundation of China [62036005]; Special Fund of
   Hubei Luojia Laboratory
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62036005 and the Special Fund of Hubei
   Luojia Laboratory. This paper has been recommended for acceptance by
   Zicheng Liu.
CR Adam A, 2008, IEEE T PATTERN ANAL, V30, P555, DOI 10.1109/TPAMI.2007.70825
   Astrid M., 2021, BRIT MACH VIS C, P279
   Barbalau A, 2023, COMPUT VIS IMAGE UND, V229, DOI 10.1016/j.cviu.2023.103656
   Cai RC, 2021, AAAI CONF ARTIF INTE, V35, P938
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Del Giorno A, 2016, LECT NOTES COMPUT SC, V9909, P334, DOI 10.1007/978-3-319-46454-1_21
   Fan YX, 2020, COMPUT VIS IMAGE UND, V195, DOI 10.1016/j.cviu.2020.102920
   Feng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3490, DOI 10.1109/ICCV48922.2021.00349
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Georgescu MI, 2021, PROC CVPR IEEE, P12737, DOI 10.1109/CVPR46437.2021.01255
   Gong D, 2019, IEEE I CONF COMP VIS, P1705, DOI 10.1109/ICCV.2019.00179
   Guansong Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12170, DOI 10.1109/CVPR42600.2020.01219
   Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86
   He H., 2020, P IEEE CVF C COMP VI, P9729
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hyunjong Park, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14360, DOI 10.1109/CVPR42600.2020.01438
   Ionescu RT, 2017, IEEE I CONF COMP VIS, P2914, DOI 10.1109/ICCV.2017.315
   Koppel M, 2007, J MACH LEARN RES, V8, P1261
   Lin XR, 2022, AAAI CONF ARTIF INTE, P1620
   Liu FT, 2012, ACM T KNOWL DISCOV D, V6, DOI 10.1145/2133360.2133363
   Liu W, 2018, PROC CVPR IEEE, P6536, DOI 10.1109/CVPR.2018.00684
   Liu Y., 2018, P BMVC
   Liu ZA, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13568, DOI 10.1109/ICCV48922.2021.01333
   Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338
   Lu YW, 2019, 2019 16TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), DOI 10.1109/avss.2019.8909850
   Luo WX, 2017, IEEE INT CON MULTI, P439, DOI 10.1109/ICME.2017.8019325
   Madan N, 2023, Arxiv, DOI arXiv:2209.12148
   Mahadevan V, 2010, PROC CVPR IEEE, P1975, DOI 10.1109/CVPR.2010.5539872
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882
   Redmon J., 2018, CoRR
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sun QR, 2017, PATTERN RECOGN, V64, P187, DOI 10.1016/j.patcog.2016.09.016
   Nguyen TN, 2019, IEEE I CONF COMP VIS, P1273, DOI 10.1109/ICCV.2019.00136
   Wang SQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P636, DOI 10.1145/3240508.3240615
   Wang ZM, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2463, DOI 10.1145/3394171.3413529
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
   Yu G, 2022, PROC CVPR IEEE, P13967, DOI 10.1109/CVPR52688.2022.01360
   Yu G, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P583, DOI 10.1145/3394171.3413973
   Yunpeng Chang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P329, DOI 10.1007/978-3-030-58555-6_20
   Zhang Y, 2016, PATTERN RECOGN, V59, P302, DOI 10.1016/j.patcog.2015.11.018
NR 45
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104185
DI 10.1016/j.jvcir.2024.104185
EA MAY 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UC3O6
UT WOS:001245825500001
DA 2024-08-05
ER

PT J
AU Chen, GL
   Huang, JY
   Chen, GH
   Chen, X
   Deng, XL
   Lan, YB
   Long, YB
   Tian, Q
AF Chen, Guilong
   Huang, Jiayi
   Chen, Guanghai
   Chen, Xin
   Deng, Xiaoling
   Lan, Yubin
   Long, Yongbing
   Tian, Qi
TI GaitGMT: Global feature mapping transformer for gait recognition
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Gait recognition; Deep learning; Vision transformer; Global feature
   mapping
ID IMAGE; MODEL
AB Gait recognition is an important biometric technology that allows for the remote collection of stakeholders' characteristics, without requiring their explicit cooperation. It has gained considerable attention in the fields of criminal investigation and intelligent security. Previous studies have shown that local gait features can enhance gait recognition performance by improving robustness to disturbances. However, global gait features also play a crucial role in gait recognition. Many researchers have utilized convolutional operations to extract global features, but these operations tend to focus on features within the receptive field, neglecting those outside of it. Therefore, the potential of global gait features has not been fully explored. In this paper, we propose a gait recognition framework based on vision transformers, aiming to enhance the extraction of global gait features. We introduce an adaptive multi -frame global feature mapping (AMGM) method to address the challenge of inconsistent feature dimensions caused by variations in the number of gait frames when fusing global and local features. We evaluate our model on the latest datasets, and the experimental results demonstrate a significant breakthrough. Notably, our model achieves state-of-the-art recognition accuracy, particularly in scenarios where subjects are wearing coats. Additionally, our model achieves remarkable improvements in recognition accuracy through training with small sample sets.
C1 [Chen, Guilong; Huang, Jiayi; Chen, Guanghai; Chen, Xin; Deng, Xiaoling; Lan, Yubin; Long, Yongbing] South China Agr Univ, Coll Elect Engn, Coll Artificial Intelligence, Guangzhou 510642, Peoples R China.
   [Tian, Qi] Huawei Noahs Ark Lab, Shenzhen 518000, Peoples R China.
C3 South China Agricultural University; Huawei Technologies
RP Chen, X (corresponding author), South China Agr Univ, Coll Elect Engn, Coll Artificial Intelligence, Guangzhou 510642, Peoples R China.
EM guilongchen@stu.scau.edu.cn; JiayiHuang@stu.scau.edu.cn;
   chenguanghai@stu.scau.edu.cn; chenxin@scau.edu.cn; dengxl@scau.edu.cn;
   ylan@scau.edu.cn; yongbinglong@126.com; tian.qi1@huawei.com
FU Laboratory of Lingnan Modern Agriculture Project, China [NT2021009];
   National Natural Science Foundation of China [61906074, 61675003,
   61972178]; Guangdong Basic and Applied Basic Research Foundation, China
   [2019A1515011276]; The 111 Project, China [D18019]; Key -Area Research
   and Development Program of Guangdong Province, China [2019B020214003];
   Key-Area Research and Development Program of Guangzhou, China
   [202103000090]; Key-Areas of Artificial Intelligence in General Colleges
   and Universities of Guangdong Province, China [2019KZDZX1012];
   Fundamental Research Funds for the Central Universities, China
   [21620432]; Key - Area Research and Development Program of Guangdong
   Province, China [2019B1515120010]
FX This work was supported by Laboratory of Lingnan Modern Agriculture
   Project, China (Grant No. NT2021009), National Natural Science
   Foundation of China (Grant Nos. 61906074, 61675003, 61972178), Guangdong
   Basic and Applied Basic Research Foundation, China (Grant No.
   2019A1515011276), the 111 Project, China (D18019), Key -Area Research
   and Development Program of Guangdong Province, China (Grant No.
   2019B020214003), Key -Area Research and Development Program of
   Guangzhou, China (Grant No. 202103000090), Key -Areas of Artificial
   Intelligence in General Colleges and Universities of Guangdong Province,
   China (Grant No. 2019KZDZX1012), the Fundamental Research Funds for the
   Central Universities, China (21620432), Key - Area Research and
   Development Program of Guangdong Province, China (2019B1515120010).
CR Ben XY, 2020, IEEE T CIRC SYST VID, V30, P734, DOI 10.1109/TCSVT.2019.2893736
   Ben XY, 2019, IEEE T IMAGE PROCESS, V28, P3142, DOI 10.1109/TIP.2019.2894362
   Boulgouris NV, 2007, IEEE T IMAGE PROCESS, V16, P731, DOI 10.1109/TIP.2007.891157
   Boulgouris NV, 2013, IEEE T IMAGE PROCESS, V22, P3636, DOI 10.1109/TIP.2013.2266578
   Chao HQ, 2019, AAAI CONF ARTIF INTE, P8126
   Chen Jianyu, 2022, IEEE Transactions on Biometrics, Behavior, and Identity Science, V4, P582, DOI 10.1109/TBIOM.2022.3213545
   Chen JY, 2023, INFORM SCIENCES, V636, DOI 10.1016/j.ins.2023.03.145
   Chen X, 2021, IEEE T IMAGE PROCESS, V30, P3041, DOI 10.1109/TIP.2021.3055936
   Chen X, 2020, IEEE T NEUR NET LEAR, V31, P3962, DOI 10.1109/TNNLS.2019.2947789
   Chen X, 2018, IEEE T PATTERN ANAL, V40, P1697, DOI 10.1109/TPAMI.2017.2726061
   Connor P, 2018, COMPUT VIS IMAGE UND, V167, P1, DOI 10.1016/j.cviu.2018.01.007
   Deng MQ, 2019, IEEE T CIRC SYST VID, V29, P3636, DOI 10.1109/TCSVT.2018.2883449
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fan C., 2020, 2020 IEEE CVF C COMP
   Fan C, 2023, PROC CVPR IEEE, P9707, DOI 10.1109/CVPR52729.2023.00936
   Fan HQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6804, DOI 10.1109/ICCV48922.2021.00675
   Gao S, 2022, IET COMPUT VIS, V16, P111, DOI 10.1049/cvi2.12070
   Gupta SK, 2021, NEUROCOMPUTING, V454, P76, DOI 10.1016/j.neucom.2021.04.113
   Haritha D., 2020, IEEE 2019 INT C MED
   He YW, 2019, IEEE T INF FOREN SEC, V14, P102, DOI 10.1109/TIFS.2018.2844819
   Hermans A., 2017, ARXIV
   Hu MD, 2013, IEEE T INF FOREN SEC, V8, P2034, DOI 10.1109/TIFS.2013.2287605
   Huang SC, 2023, IEEE T NEUR NET LEAR, V34, P5122, DOI 10.1109/TNNLS.2021.3125679
   Huang TH, 2022, IEEE T CIRC SYST VID, V32, P6967, DOI 10.1109/TCSVT.2022.3175959
   Huang XX, 2012, IEEE T IMAGE PROCESS, V21, P2256, DOI 10.1109/TIP.2011.2180914
   Kim B, 2021, PROC CVPR IEEE, P74, DOI 10.1109/CVPR46437.2021.00014
   Kusakunniran Worapan, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1058, DOI 10.1109/ICCVW.2009.5457587
   Kusakunniran W, 2014, IEEE T IMAGE PROCESS, V23, P696, DOI 10.1109/TIP.2013.2294552
   Kusakunniran W, 2012, PATTERN RECOGN LETT, V33, P882, DOI 10.1016/j.patrec.2011.04.014
   Li X, 2020, PATTERN RECOGN, V105, DOI 10.1016/j.patcog.2020.107376
   Lin BB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14628, DOI 10.1109/ICCV48922.2021.01438
   Lin KV, 2021, Arxiv, DOI arXiv:2012.09760
   Makihara Y, 2006, LECT NOTES COMPUT SC, V3953, P151, DOI 10.1007/11744078_12
   Mogan J.N., 2022, IAENG Int. J. Comput. Sci., V2, P49
   Muramatsu D, 2016, IEEE T CYBERNETICS, V46, P1602, DOI 10.1109/TCYB.2015.2452577
   Muramatsu D, 2015, IEEE T IMAGE PROCESS, V24, P140, DOI 10.1109/TIP.2014.2371335
   Parmar N, 2018, PR MACH LEARN RES, V80
   Qin H, 2022, IEEE T CIRC SYST VID, V32, P2990, DOI 10.1109/TCSVT.2021.3095290
   Rijun Liao, 2017, Biometric Recognition. 12th Chinese Conference, CCBR 2017. Proceedings: LNCS 10568, P474, DOI 10.1007/978-3-319-69923-3_51
   Saihui Hou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P382, DOI 10.1007/978-3-030-58545-7_22
   Santos C., 2022, Gait recognition based on deep learning: A survey
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sepas-Moghaddam A, 2023, IEEE T PATTERN ANAL, V45, P264, DOI 10.1109/TPAMI.2022.3151865
   Shiraga K, 2016, INT CONF BIOMETR
   Sun JM, 2021, Arxiv, DOI [arXiv:2104.00680, 10.48550/arXiv.2104.00680, DOI 10.48550/ARXIV.2104.00680]
   Takemura Noriko, 2018, IPSJ Transactions on Computer Vision and Applications, V10, DOI 10.1186/s41074-018-0039-6
   Takemura N, 2019, IEEE T CIRC SYST VID, V29, P2708, DOI 10.1109/TCSVT.2017.2760835
   Tong SB, 2019, PATTERN RECOGN LETT, V125, P212, DOI 10.1016/j.patrec.2019.04.010
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan CS, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3230633
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wen JM, 2022, NEURAL PROCESS LETT, V54, P1855, DOI 10.1007/s11063-021-10709-1
   Wolf T, 2016, IEEE IMAGE PROC, P4165, DOI 10.1109/ICIP.2016.7533144
   Wu HQ, 2021, IEEE T IMAGE PROCESS, V30, P2734, DOI 10.1109/TIP.2020.3039888
   Wu YH, 2023, IEEE T PATTERN ANAL, V45, P12760, DOI 10.1109/TPAMI.2022.3202765
   Wu ZF, 2017, IEEE T PATTERN ANAL, V39, P209, DOI 10.1109/TPAMI.2016.2545669
   Xu C, 2021, IEEE T CIRC SYST VID, V31, P260, DOI 10.1109/TCSVT.2020.2975671
   Xu D, 2012, IEEE T IMAGE PROCESS, V21, P316, DOI 10.1109/TIP.2011.2160956
   Xu K, 2021, IEEE T MULTIMEDIA, V24, P3265, DOI 10.1109/TMM.2021.3095809
   Yao LX, 2023, IEEE T MULTIMEDIA, V25, P4187, DOI 10.1109/TMM.2022.3171961
   Yu SQ, 2006, INT C PATT RECOG, P441
   Yu SQ, 2017, NEUROCOMPUTING, V239, P81, DOI 10.1016/j.neucom.2017.02.006
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zhang YQ, 2020, IEEE T IMAGE PROCESS, V29, P1001, DOI 10.1109/TIP.2019.2926208
   Zhao AT, 2022, IEEE T MULTIMEDIA, V24, P846, DOI 10.1109/TMM.2021.3060280
   Zhao A, 2020, KNOWL-BASED SYST, V206, DOI 10.1016/j.knosys.2020.106273
NR 67
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104139
DI 10.1016/j.jvcir.2024.104139
EA APR 2024
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RX1T8
UT WOS:001230873500001
DA 2024-08-05
ER

PT J
AU Jin, HY
   Li, L
   Su, HN
   Zhang, YL
   Xiao, ZL
   Wang, B
AF Jin, Haiyan
   Li, Long
   Su, Haonan
   Zhang, Yuanlin
   Xiao, Zhaolin
   Wang, Bin
TI Learn to enhance the low-light image via a multi-exposure generation and
   fusion method
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Multi exposure generation and fusion; Low light image enhancement;
   Perceptual importance; Feature fusion
ID HISTOGRAM EQUALIZATION; QUALITY ASSESSMENT; NETWORK
AB In low-light image enhancement, single-exposure images contain a limited dynamic range, which hinders the restoration of contrast and texture. To address these problems, we propose a multi-exposure generation and fusion method (MEGF), which simulates multi exposure images and performs feature fusion for low light image enhancement. First, we propose a Multi-Exposure Generation (MEG) block, which generates images with different exposure levels based on the input low-light images. The MEG block employs information entropy as an evaluation measure to prevent the underexposed or overexposed image generation. Then, the Perceptual Importance based Multi-Exposure Feature Enhancement (PIMEFE) module has been developed to fuse the multi-exposure features using the Perceptual Importance-based Feature Fusion (PIFF) module. The PIFF module selects the well-exposed features from the multi-exposure features processed by the Multi Scale Recursive Feature Enhancement (MSRFE) block. Finally, the fused features are input to the Curve Adjustment (CA) block for fine-tuning and provide color enhancement to the fused features. Moreover, we propose the Multiple Exposure Recursive Fusion (MERF) module which estimates the adjustment factors for the CA block with the guidance of multi-exposure features. Experimental results demonstrate that our method outperforms other techniques in terms of image signal-to-noise ratio, structural similarity, and color accuracy on both real and synthetic datasets.
C1 [Jin, Haiyan; Li, Long; Su, Haonan; Zhang, Yuanlin; Xiao, Zhaolin; Wang, Bin] Xian Univ Technol, Dept Comp Sci & Engn, Xian 710048, Peoples R China.
C3 Xi'an University of Technology
RP Su, HN (corresponding author), Xian Univ Technol, Dept Comp Sci & Engn, Xian 710048, Peoples R China.
EM suhaonan@xaut.edu.cn
OI Zhang, Yuanlin/0000-0003-0960-3636
FU National Natural Science Foundation of China [62272383, 62371389];
   Shaanxi Provincial Edu-Department [23JP105]
FX This research has been funded in part by the National Natural Science
   Foundation of China (No. 62272383, No. 62371389) , in part Scientific
   Research Program Funded by Shaanxi Provincial Edu-Department (Program
   No. 23JP105) .
CR A.D.B.C.N. Network, Blind Image Quality Assessment Using A Deep Bilinear Convolutional Neural Network
   Abdullah-Al-Wadud M, 2007, IEEE T CONSUM ELECTR, V53, P593, DOI 10.1109/TCE.2007.381734
   Azam MA, 2022, COMPUT BIOL MED, V144, DOI 10.1016/j.compbiomed.2022.105253
   Cai BL, 2017, IEEE I CONF COMP VIS, P4020, DOI 10.1109/ICCV.2017.431
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Cheng HD, 2004, DIGIT SIGNAL PROCESS, V14, P158, DOI 10.1016/j.dsp.2003.07.002
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Golestaneh SA, 2022, IEEE WINT CONF APPL, P3989, DOI 10.1109/WACV51458.2022.00404
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Han G, 2023, J VIS COMMUN IMAGE R, V96, DOI 10.1016/j.jvcir.2023.103932
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Jin H., 2023, ICASSP 2023 2023 IEE, P1
   Jin H., 2024, INT JOINT C NEUR NET
   Jin HY, 2023, J VIS COMMUN IMAGE R, V95, DOI 10.1016/j.jvcir.2023.103887
   Ke JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5128, DOI 10.1109/ICCV48922.2021.00510
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Lu K, 2021, IEEE T MULTIMEDIA, V23, P4093, DOI 10.1109/TMM.2020.3037526
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Ma L, 2022, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR52688.2022.00555
   Parihar A.S., 2023, PREPRINT
   Parihar AS, 2021, IET IMAGE PROCESS, V15, P1410, DOI 10.1049/ipr2.12114
   Ren XT, 2020, IEEE T IMAGE PROCESS, V29, P5862, DOI 10.1109/TIP.2020.2984098
   Shu HP, 2009, 2009 INTERNATIONAL FORUM ON INFORMATION TECHNOLOGY AND APPLICATIONS, VOL 1, PROCEEDINGS, P732, DOI 10.1109/IFITA.2009.559
   Singh K, 2023, J VIS COMMUN IMAGE R, V91, DOI 10.1016/j.jvcir.2023.103780
   Singh K, 2021, J VIS COMMUN IMAGE R, V79, DOI 10.1016/j.jvcir.2021.103241
   Tao J., 2021, 2021 2 CHIN INT SAR, P1
   Wang LW, 2020, IEEE T IMAGE PROCESS, V29, P7984, DOI 10.1109/TIP.2020.3008396
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang T., 2023, P AAAI C ART INT, P2654, DOI [DOI 10.1609/AAAI.V37I3.25364, 10.1609/AAAI.V37I3.25364]
   Wang YN, 2023, J VIS COMMUN IMAGE R, V92, DOI 10.1016/j.jvcir.2023.103795
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei C, 2018, Arxiv, DOI arXiv:1808.04560
   Wu WH, 2022, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR52688.2022.00581
   Xu J, 2020, IEEE T IMAGE PROCESS, V29, P5022, DOI 10.1109/TIP.2020.2974060
   Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P2072, DOI 10.1109/TIP.2021.3050850
   Yang WH, 2020, PROC CVPR IEEE, P3060, DOI 10.1109/CVPR42600.2020.00313
   Zhang L, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1623, DOI 10.1145/3343031.3351069
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhang ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4086, DOI 10.1109/ICCV48922.2021.00407
NR 43
TC 1
Z9 1
U1 4
U2 4
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104127
DI 10.1016/j.jvcir.2024.104127
EA MAR 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QS9I8
UT WOS:001222973700001
DA 2024-08-05
ER

PT J
AU Zheng, SJ
   Wang, GC
   Yuan, YJ
   Huang, SQ
AF Zheng, Shijie
   Wang, Gaocai
   Yuan, Yujian
   Huang, Shuqiang
TI Fine-grained image classification based on TinyVit object location and
   graph convolution network
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Fine-grained image classification; TinyVit; Object location; Spatial
   relationship feature learning; Graph convolution network
AB Fine-grained image classification is a branch of image classification. Recently, vision transformer has made excellent progress in the field of image recognition. Its self -attention mechanism can extract very effective image feature information. However, feeding fixed -size image blocks into the network introduces additional noise, which is detrimental to extract discriminative features for fine-grained images. The vision transformer's network model is large, making it difficult to utilize in practice. Moreover, many of today's fine-grained image classification methods focus on mining discriminative features while ignoring the connections within the image. To address these problems, we propose a novel method based on the lightweight TinyVit backbone network. Our approach utilizes the self -attention weight values of TinyVit as a guide to construct an effective object location (OL) module that cuts and enlarges the object area, providing the network with the opportunity to concentrate on the local object. Additionally, we employ the graph convolutional network (GCN) to create a spatial relationship feature learning (SRFL) module that captures spatial context information between image blocks in TinyVit with the help of the transformer's self -attention weights. OL and SRFL collaborate to jointly guide the classification task. The experimental results show that the proposed method achieved competitive performance, with the second -highest classification faccuracy on both the CUB -200-2011 and NABirds datasets. When tested on the Stanford Dogs dataset, our approach outperformed many popular methods. Our code is uploaded on https://gith ub.com/hhhj1999/SRFL_OL.
C1 [Zheng, Shijie; Wang, Gaocai; Yuan, Yujian] Guangxi Univ, Sch Comp & Elect & Informat, Nanning 530004, Peoples R China.
   [Huang, Shuqiang] Jinan Univ, Coll Cyber Secur, Guangzhou 510632, Peoples R China.
C3 Guangxi University; Jinan University
RP Wang, GC (corresponding author), Guangxi Univ, Sch Comp & Elect & Informat, Nanning 530004, Peoples R China.
EM wanggcgx@163.com
FU National Natural Science Foundation of China [62062007, 62272198]
FX This work is suspported by the National Natural Science Foundation of
   China (Grant NO.62062007, 62272198) .
CR Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Baffour AA, 2021, J VIS COMMUN IMAGE R, V81, DOI 10.1016/j.jvcir.2021.103368
   Branson S, 2014, Arxiv, DOI arXiv:1406.2952
   Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097
   Chen S., 2022, IEEE Transactions on Pattern Analysis and Machine Intelligence
   Chou P.-Y., 2022, arXiv, DOI DOI 10.48550/ARXIV.2202.03822
   Huynh D, 2020, PROC CVPR IEEE, P4482, DOI 10.1109/CVPR42600.2020.00454
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fan Zhang, 2021, MultiMedia Modeling. 27th International Conference, MMM 2021. Proceedings. Lecture Notes in Computer Science (LNCS 12572), P136, DOI 10.1007/978-3-030-67832-6_12
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Guo-Sen Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P562, DOI 10.1007/978-3-030-58548-8_33
   He J, 2022, AAAI CONF ARTIF INTE, P852
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu T, 2019, Arxiv, DOI arXiv:1901.09891
   Hu XB, 2023, J VIS COMMUN IMAGE R, V91, DOI 10.1016/j.jvcir.2023.103755
   Hu YQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4239, DOI 10.1145/3474085.3475561
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Ke X, 2022, COMPUT VIS IMAGE UND, V218, DOI 10.1016/j.cviu.2022.103408
   Kim S, 2022, 39 INT C MACHINE LEA
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Lam M, 2017, PROC CVPR IEEE, P6497, DOI 10.1109/CVPR.2017.688
   Li ZW, 2022, IEEE T NEUR NET LEAR, V33, P6999, DOI 10.1109/TNNLS.2021.3084827
   Lin C, 2024, IEEE T NEUR NET LEAR, V35, P1305, DOI 10.1109/TNNLS.2022.3183210
   Lin TY, 2017, Arxiv, DOI [arXiv:1504.07889, 10.48550/arXiv.1504.07889]
   Liu H., 2023, IEEE Transactions on Multimedia
   Liu XD, 2022, NEUROCOMPUTING, V492, P137, DOI 10.1016/j.neucom.2022.04.037
   Liu Y, 2021, PROC CVPR IEEE, P3793, DOI 10.1109/CVPR46437.2021.00379
   Liu Z, 2022, PROC CVPR IEEE, P11999, DOI 10.1109/CVPR52688.2022.01170
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Sun HB, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5853, DOI 10.1145/3503161.3548308
   Sun M, 2018, LECT NOTES COMPUT SC, V11220, P834, DOI 10.1007/978-3-030-01270-0_49
   Tan MX, 2019, PR MACH LEARN RES, V97
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang J, 2022, Arxiv, DOI arXiv:2107.02341
   Wu K, 2022, LECT NOTES COMPUT SC, V13681, P68, DOI 10.1007/978-3-031-19803-8_5
   Xu Q, 2023, IEEE T MULTIMEDIA, V25, P9015, DOI 10.1109/TMM.2023.3244340
   Yang Z, 2021, J VIS COMMUN IMAGE R, V79, DOI 10.1016/j.jvcir.2021.103245
   Yu Y, 2019, NEURAL COMPUT, V31, P1235, DOI 10.1162/neco_a_01199
   Zhang Y, 2022, INT CONF ACOUST SPEE, P3234, DOI 10.1109/ICASSP43922.2022.9747591
   Zhao YF, 2021, PROC CVPR IEEE, P15074, DOI 10.1109/CVPR46437.2021.01483
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhu HW, 2022, PROC CVPR IEEE, P4682, DOI 10.1109/CVPR52688.2022.00465
NR 50
TC 0
Z9 0
U1 2
U2 2
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104120
DI 10.1016/j.jvcir.2024.104120
EA MAR 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QR3R3
UT WOS:001222563000001
DA 2024-08-05
ER

PT J
AU Yan, XG
   Shao, F
   Chen, HW
   Jiang, QP
AF Yan, Xingao
   Shao, Feng
   Chen, Hangwei
   Jiang, Qiuping
TI Hybrid CNN-transformer based meta-learning approach for personalized
   image aesthetics assessment
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Meta-Learning; Personalized image aesthetics assessment
ID PHOTO
AB Personalized Image Aesthetics Assessment (PIAA) is highly subjective, as people's aesthetic preferences vary greatly. Traditional generic models struggle to capture the unique preferences of each individual, and PIAA often deals with limited samples from individual users. Furthermore, it requires a holistic consideration of diverse visual features in images, including both local and global features. To address these challenges, we propose an innovative network that combines the power of transformer and Convolutional Neural Networks (CNNs) with Meta-Learning for PIAA (TCML-PIAA). Firstly, we leverage both Vision Transformer blocks and CNNs to extract long-term and short-term dependencies, mining richer and heterogeneous aesthetic attributes from these two branches. Secondly, to effectively fuse these distinct features, we introduce an Aesthetic Feature Interaction Module (AFIM), designed to seamlessly integrate the aesthetic features extracted from CNNs and ViT, enabling the interaction and fusion of aesthetic information across different modalities. We also incorporate a ChannelSpatial Attention Module (CSAM), embedding it within both the CNNs and the AFIM to enhance the perception of different regions in images, further exploring the aesthetic cues in images. Experimental results demonstrate that our TCML-PIAA outperforms existing state-of-the-art methods on benchmark databases.
C1 [Yan, Xingao; Shao, Feng; Chen, Hangwei; Jiang, Qiuping] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
C3 Ningbo University
RP Shao, F (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
EM shaofeng@nbu.edu.cn
OI Chen, Hangwei/0000-0002-3756-2029
CR Andrychowicz M, 2016, ADV NEUR IN, V29
   Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Celona L, 2022, IEEE T IMAGE PROCESS, V31, P5009, DOI 10.1109/TIP.2022.3191853
   Chen G, 2023, IEEE T CIRC SYST VID, V33, P1787, DOI 10.1109/TCSVT.2022.3215979
   Chen G, 2022, IEEE T CIRC SYST VID, V32, P6308, DOI 10.1109/TCSVT.2022.3166914
   Chen H., IEEE Trans. Neural Networks Learn. Syst.
   Chen HW, 2023, IEEE T CIRC SYST VID, V33, P3055, DOI 10.1109/TCSVT.2022.3231041
   Chen HW, 2023, IEEE T MULTIMEDIA, V25, P140, DOI 10.1109/TMM.2021.3121875
   Cheon M, 2021, IEEE COMPUT SOC CONF, P433, DOI 10.1109/CVPRW53098.2021.00054
   Cui YL, 2024, IEEE T MULTIMEDIA, V26, P5092, DOI 10.1109/TMM.2023.3330096
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Deng C, 2019, IEEE T GEOSCI REMOTE, V57, P1741, DOI 10.1109/TGRS.2018.2868851
   Deng YB, 2017, IEEE SIGNAL PROC MAG, V34, P80, DOI 10.1109/MSP.2017.2696576
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Feng TL, 2023, IEEE T PATTERN ANAL, V45, P8577, DOI 10.1109/TPAMI.2022.3232328
   Finn C, 2017, PR MACH LEARN RES, V70
   He JY, 2023, IEEE-CAA J AUTOMATIC, V10, P828, DOI 10.1109/JAS.2023.123117
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou JW, 2021, Arxiv, DOI arXiv:2104.01548
   Hou JW, 2022, IEEE T CIRC SYST VID, V32, P7386, DOI 10.1109/TCSVT.2022.3186307
   Hu BT, 2014, ADV NEUR IN, V27
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Kalchbrenner N, 2014, Arxiv, DOI [arXiv:1404.2188, 10.48550/arXiv.1404.2188]
   Ke Y., 2006, CVPR, P419, DOI DOI 10.1109/CVPR.2006.303
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kucer M, 2018, IEEE T IMAGE PROCESS, V27, P5100, DOI 10.1109/TIP.2018.2845100
   Li LD, 2020, IEEE T IMAGE PROCESS, V29, P3898, DOI 10.1109/TIP.2020.2968285
   Li Yaohui, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P896, DOI 10.1145/3503161.3548244
   Liang S, 2021, J VIS COMMUN IMAGE R, V81, DOI 10.1016/j.jvcir.2021.103356
   Liu Y, 2022, J VIS COMMUN IMAGE R, V87, DOI 10.1016/j.jvcir.2022.103586
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Lv P, 2023, IEEE T MULTIMEDIA, V25, P736, DOI 10.1109/TMM.2021.3130752
   Lv P, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1328, DOI 10.1145/3240508.3240635
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Nichol A, 2018, Arxiv, DOI arXiv:1803.02999
   O'Donovan P., 2014, Proceedings of the Workshop on Computational Aesthetics, P33
   Parikh Ankur P., 2016, A decomposable attention model for natural language inference, DOI 10.48550/arXiv.1606.01933
   Ren J, 2017, IEEE I CONF COMP VIS, P638, DOI 10.1109/ICCV.2017.76
   Snell J., 2017, Advances in Neural Information Processing Systems, V30, P4077
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinciarelli A, 2014, IEEE T AFFECT COMPUT, V5, P273, DOI 10.1109/TAFFC.2014.2330816
   Wang WN, 2019, IEEE IMAGE PROC, P1875, DOI [10.1109/ICIP.2019.8803119, 10.1109/icip.2019.8803119]
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang YT, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2023.103751
   Yang JC, 2024, IEEE T MULTIMEDIA, V26, P3604, DOI 10.1109/TMM.2023.3313256
   Yang JC, 2023, NEUROCOMPUTING, V530, P104, DOI 10.1016/j.neucom.2023.01.067
   Yang YZ, 2022, PROC CVPR IEEE, P19829, DOI 10.1109/CVPR52688.2022.01924
   Yang ZC, 2024, IEEE T MULTIMEDIA, V26, P1944, DOI 10.1109/TMM.2023.3290479
   Zaremba W, 2015, Arxiv, DOI [arXiv:1409.2329, DOI 10.48550/ARXIV.1409.2329]
   Zhang C., 2020, J. vis. Commun. Image Represent, V71
   Zhu H., 2023, ACM INT C MULTIMEDIA
   Zhu HC, 2022, IEEE T CYBERNETICS, V52, P1798, DOI 10.1109/TCYB.2020.2984670
   Zhu HC, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10224181
   Zhu HC, 2022, J VIS COMMUN IMAGE R, V89, DOI 10.1016/j.jvcir.2022.103675
   Zhu HC, 2023, IEEE T MULTIMEDIA, V25, P179, DOI 10.1109/TMM.2021.3123468
   Zhu XZ, 2021, Arxiv, DOI [arXiv:2010.04159, 10.48550/arXiv.2010.04159]
NR 63
TC 1
Z9 1
U1 2
U2 2
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104044
DI 10.1016/j.jvcir.2023.104044
EA JAN 2024
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HF2V0
UT WOS:001158021500001
DA 2024-08-05
ER

PT J
AU Rahman, CMA
   Nyeem, H
AF Rahman, Chowdhury M. Abid
   Nyeem, Hussain
TI Tensor-enhanced shock energy-driven active contours: A novel approach
   for knowledge-based image segmentation
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Active contour; Image segmentation; Level-set; Optimized shock filter;
   Variational methods
ID FITTING ENERGY; MODEL
AB In image segmentation, active contour models (ACMs) have demonstrated their proficiency in accurately delineating complex backgrounds with irregular intensity distributions, even in the presence of noise or variable intensities. However, recent ACMs, including variants of the widely used region-scalable fitting (RSF) model, encounter a specific challenge when initializing contours in regions marked by significant intensity irregularities. This challenge arises from their reliance on minimizing a quadratic loss function within the energy framework, which in turn depends on second-order edge characteristics. To overcome this limitation and enhance segmentation accuracy, we introduce a novel ACM, empowered by a tensor-based structured shock energy function. This energy function amplifies image boundaries by aligning with the sharpest gradients, thereby inducing a second-order edge effect along object boundaries. Integrating this derived energy seamlessly into a region-based level-set formulation bolsters the model's edge detection capabilities. Through extensive experimentation, we validate the effectiveness of this approach, demonstrating significantly improved tolerance for initialization in the presence of noise and intensity variations. Notably, our solution outperforms prominent region-based segmentation models. This research marks a significant advancement in image segmentation, particularly for challenging scenarios, ultimately pushing the boundaries of current state-of-the-art techniques.
C1 [Rahman, Chowdhury M. Abid] West Virginia Univ, Lane Dept Comp Sci & Elect Engn, Morgantown, WV 26506 USA.
   [Nyeem, Hussain] Mil Inst Sci & Technol MIST Mirpur Cantonment, Dept Elect Elect & Commun Engn, Dhaka 1216, Bangladesh.
C3 West Virginia University
RP Nyeem, H (corresponding author), Mil Inst Sci & Technol MIST Mirpur Cantonment, Dept Elect Elect & Commun Engn, Dhaka 1216, Bangladesh.
EM h.nyeem@eece.mist.ac.bd
OI Rahman, Chowdhury Mohammad Abid/0009-0003-3093-3370
CR Alpert S, 2012, IEEE T PATTERN ANAL, V34, P315, DOI 10.1109/TPAMI.2011.130
   Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043
   CASELLES V, 1993, NUMER MATH, V66, P1, DOI 10.1007/BF01385685
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Chen Y, 2021, OPTIK, V248, DOI 10.1016/j.ijleo.2021.168130
   Ding KY, 2018, PATTERN RECOGN LETT, V104, P29, DOI 10.1016/j.patrec.2018.01.019
   Ding KY, 2017, SIGNAL PROCESS, V134, P224, DOI 10.1016/j.sigpro.2016.12.021
   Duan YX, 2020, OPTIK, V202, DOI 10.1016/j.ijleo.2019.163667
   Fang JX, 2019, IEEE ACCESS, V7, P97492, DOI 10.1109/ACCESS.2019.2929659
   Ge PQ, 2022, EXPERT SYST APPL, V210, DOI 10.1016/j.eswa.2022.118493
   Ge PQ, 2022, PATTERN RECOGN LETT, V158, P71, DOI 10.1016/j.patrec.2022.04.025
   Kass M., 1987, International Journal of Computer Vision, V1, P321, DOI 10.1007/BF00133570
   Li CM, 2008, IEEE T IMAGE PROCESS, V17, P1940, DOI 10.1109/TIP.2008.2002304
   Liu HX, 2020, IEEE ACCESS, V8, P59412, DOI 10.1109/ACCESS.2020.2981596
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Medioni G., 2000, COMPUTATIONAL FRAMEW
   OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2
   OSHER S, 1990, SIAM J NUMER ANAL, V27, P919, DOI 10.1137/0727053
   Prantikuzzaman Md, 2022, 2022 25th International Conference on Computer and Information Technology (ICCIT), P605, DOI 10.1109/ICCIT57492.2022.10055425
   Shyam Satirtha Paul, 2022, 2022 25th International Conference on Computer and Information Technology (ICCIT), P751, DOI 10.1109/ICCIT57492.2022.10055134
   Wang GA, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3241981
   Wang L, 2017, INFORM SCIENCES, V418, P61, DOI 10.1016/j.ins.2017.06.042
   Wang L, 2009, COMPUT MED IMAG GRAP, V33, P520, DOI 10.1016/j.compmedimag.2009.04.010
   Weickert J, 2003, LECT NOTES COMPUT SC, V2781, P1
   Weng GR, 2020, KNOWL-BASED SYST, V197, DOI 10.1016/j.knosys.2020.105882
   Yan X, 2022, APPL MATH MODEL, V101, P586, DOI 10.1016/j.apm.2021.09.002
   Zhang KH, 2010, IMAGE VISION COMPUT, V28, P668, DOI 10.1016/j.imavis.2009.10.009
   Zhang KH, 2010, PATTERN RECOGN, V43, P1199, DOI 10.1016/j.patcog.2009.10.010
   Zhao WC, 2018, OPTIK, V158, P1160, DOI 10.1016/j.ijleo.2018.01.004
NR 29
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD AUG
PY 2024
VL 103
AR 104218
DI 10.1016/j.jvcir.2024.104218
EA JUL 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YK5C1
UT WOS:001268385000001
DA 2024-08-05
ER

PT J
AU Baydar, M
   Akbas, E
AF Baydar, Melih
   Akbas, Emre
TI SegIns: A simple extension to instance discrimination task for better
   localization learning
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Self-supervised learning; Contrastive learning; Non-contrastive
   learning; Instance discrimination task; Semantic segmentation; Object
   detection
AB Recent self-supervised learning methods, where instance discrimination task is a fundamental way of pretraining convolutional neural networks (CNN), excel in transfer learning performance. Even though instance discrimination task is a well suited pretraining method for classification with its image-level learning, lack of dense representation learning makes it sub-optimal for localization tasks such as object detection. In this paper, we aim to mitigate this shortcoming of instance discrimination task by extending it to jointly learn dense representations alongside image-level representations. We add a segmentation branch parallel to the imagelevel learning to predict class-agnostic masks, enhancing location-awareness of the representations. We show the effectiveness of our pretraining approach on localization tasks by transferring the learned representations to object detection and segmentation tasks, providing relative improvements by up to 1.7% AP on PASCAL VOC and 0.8% AP on COCO object detection, 0.8% AP on COCO instance segmentation and 3.6% mIoU on PASCAL VOC semantic segmentation respectively.
C1 [Baydar, Melih; Akbas, Emre] Middle East Tech Univ, Comp Engn, TR-06800 Ankara, Turkiye.
C3 Middle East Technical University
RP Baydar, M (corresponding author), Middle East Tech Univ, Comp Engn, TR-06800 Ankara, Turkiye.
EM melih.baydar@metu.edu.tr
FU The "Young Scientist Awards Program (BAGEP) " of Science Academy, Turkey
FX Dr. Akbas is supported by the "Young Scientist Awards Program (BAGEP) "
   of Science Academy, Turkey. The numerical calculations reported in this
   paper were partially performed at TUBITAK ULAKBIM, High Performance and
   Grid Computing Center (TRUBA resources) .
CR Bardes A., 2021, arXiv
   Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951
   Caron Mathilde, 2020, Advances in Neural Information Processing Systems, V33, P9912, DOI DOI 10.5555/3495724.3496555
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen XL, 2020, Arxiv, DOI arXiv:2003.04297
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Grill J.B., 2020, P 34 INT C NEUR INF, V33, P21271, DOI [10.48550/arXiv.2006.07733, DOI 10.48550/ARXIV.2006.07733]
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hénaff OJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10066, DOI 10.1109/ICCV48922.2021.00993
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Ki M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2783, DOI 10.1109/ICCV48922.2021.00280
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mo S, 2021, ADV NEUR IN, V34
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Selvaraju RR, 2021, PROC CVPR IEEE, P11053, DOI 10.1109/CVPR46437.2021.01091
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Sudre CH, 2017, LECT NOTES COMPUT SC, V10553, P240, DOI 10.1007/978-3-319-67558-9_28
   Tian YD, 2021, PR MACH LEARN RES, V139, P7279
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   Van Gansbeke W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10032, DOI 10.1109/ICCV48922.2021.00990
   Wang XL, 2021, PROC CVPR IEEE, P3023, DOI 10.1109/CVPR46437.2021.00304
   Wu Y., 2019, Detectron2
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xiao T., 2021, INT C LEARN REPR
   Xiao TT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10519, DOI 10.1109/ICCV48922.2021.01037
   Xie ZD, 2021, PROC CVPR IEEE, P16679, DOI 10.1109/CVPR46437.2021.01641
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
NR 30
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104122
DI 10.1016/j.jvcir.2024.104122
EA MAR 2024
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QR7F1
UT WOS:001222655600001
DA 2024-08-05
ER

PT J
AU Ge, PQ
   Chen, YY
   Wang, GA
   Weng, GR
AF Ge, Pengqiang
   Chen, Yiyang
   Wang, Guina
   Weng, Guirong
TI An active contour model based on Jeffreys divergence and clustering
   technology for image segmentation
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE K-medoids; Active contour model; Jeffreys divergence; Level set method;
   Data-driven
ID CHAN-VESE MODEL; FITTING ENERGY; DRIVEN
AB Classic active contour models (ACMs) generally implement Euclidean distance to measure the gap between true image and fitted one, which may cause issues such as edge leakage and falling into false boundary. In addition, some existing ACMs are sensitive to noise and different initial contours. To resolve these problems, this study raises an ACM based on Jeffreys divergence (KJD) and clustering technique for image segmentation. Firstly, the K-medoids clustering algorithm is deployed to cluster the foreground and background pixels into two sets, which forms original data -driven term and is further embedded into the theory of Jeffreys divergence to formulate a KJD energy. Next, a regularization function regularizes the ranges of optimized data -driven term and level set function respectively, which essentially produces a more stable and robust evolution environment and improves segmentation precision. In contrast with fitting function -based and recently developed ACMs on three types of images, KJD model not only raises segmentation precision, but also reduces computation expense. Experimental outcomes also verify that this model can resist different noise within limits and adapts to various initial contours.
C1 [Ge, Pengqiang; Chen, Yiyang; Wang, Guina; Weng, Guirong] Soochow Univ, Sch Mech & Elect Engn, 8 Jixue Rd, Suzhou 215137, Jiangsu, Peoples R China.
C3 Soochow University - China
RP Chen, YY (corresponding author), Soochow Univ, Sch Mech & Elect Engn, 8 Jixue Rd, Suzhou 215137, Jiangsu, Peoples R China.
EM pqgegpu@stu.suda.edu.cn; yychen90@suda.edu.cn; wangguina@suda.edu.cn;
   wgr@suda.edu.cn
RI Chen, Yiyang/T-3492-2019; Ge, Pengqiang/KJM-6748-2024
OI Chen, Yiyang/0000-0001-9960-9040; 
FU National Natural Science Foundation of China [62103293]; Natural Science
   Foundation of Jiangsu Province, China [BK20210709]; Suzhou Municipal
   Science and Technology Bureau, China [SYG202138]; Innovation Plan of
   Jiangsu Province, China [JSSCBS20210641]
FX This research paper was invested by National Natural Science Foundation
   of China under Grant 62103293, Natural Science Foundation of Jiangsu
   Province, China under Grant BK20210709, Suzhou Municipal Science and
   Technology Bureau, China under Grant SYG202138 and Entrepreneurship and
   Innovation Plan of Jiangsu Province, China under Grant JSSCBS20210641.
CR Allili MS, 2008, NEUROCOMPUTING, V71, P2001, DOI 10.1016/j.neucom.2007.10.019
   Aubert G., 2008, MATH PROBLEMIMAGE, V147, DOI DOI 10.1007/978-0-387-44588-5
   Caselles V, 1997, INT J COMPUT VISION, V22, P61, DOI 10.1023/A:1007979827043
   Chan TF, 2006, SIAM J APPL MATH, V66, P1632, DOI 10.1137/040615286
   Chen Yiyang, 2023, Optik, DOI 10.1016/j.ijleo.2023.170997
   Ding KY, 2018, PATTERN RECOGN LETT, V104, P29, DOI 10.1016/j.patrec.2018.01.019
   Ding KY, 2017, SIGNAL PROCESS, V134, P224, DOI 10.1016/j.sigpro.2016.12.021
   Ge PQ, 2022, EXPERT SYST APPL, V210, DOI 10.1016/j.eswa.2022.118493
   Ge PQ, 2022, PATTERN RECOGN LETT, V158, P71, DOI 10.1016/j.patrec.2022.04.025
   Han B, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107520
   Han B, 2018, MULTIMED TOOLS APPL, V77, P29193, DOI 10.1007/s11042-018-6127-x
   Huang QY, 2021, OPT QUANT ELECTRON, V53, DOI 10.1007/s11082-021-03000-z
   Huang QW, 2021, TSINGHUA SCI TECHNOL, V26, P833, DOI 10.26599/TST.2020.9010042
   Jain AK, 2010, PATTERN RECOGN LETT, V31, P651, DOI 10.1016/j.patrec.2009.09.011
   Jin R, 2019, NEUROCOMPUTING, V359, P408, DOI 10.1016/j.neucom.2019.06.019
   Jin R, 2019, SIGNAL PROCESS, V163, P1, DOI 10.1016/j.sigpro.2019.05.002
   Kaufman L., 2009, Encyclopedia of Database Systems, DOI [10.1007/978-0-387-39940-9_3241, DOI 10.1007/978-0-387-39940-9_3241]
   Li CM, 2008, IEEE T IMAGE PROCESS, V17, P1940, DOI 10.1109/TIP.2008.2002304
   Li CM, 2010, IEEE T IMAGE PROCESS, V19, P3243, DOI 10.1109/TIP.2010.2069690
   Li Q, 2016, SIGNAL PROCESS, V120, P185, DOI 10.1016/j.sigpro.2015.08.020
   Liu SG, 2012, PATTERN RECOGN, V45, P2769, DOI 10.1016/j.patcog.2011.11.019
   Nguyen TT, 2019, PATTERN RECOGN LETT, V117, P97, DOI 10.1016/j.patrec.2018.12.009
   Pedrycz W, 1996, PATTERN RECOGN LETT, V17, P625, DOI 10.1016/0167-8655(96)00027-X
   Peng SD, 2020, Arxiv, DOI arXiv:2001.01629
   Pratondo A, 2017, J VIS COMMUN IMAGE R, V43, P1, DOI 10.1016/j.jvcir.2016.11.019
   Wan MJ, 2023, DISPLAYS, V78, DOI 10.1016/j.displa.2023.102452
   Wang GA, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3241981
   Wang L, 2009, SIGNAL PROCESS, V89, P2435, DOI 10.1016/j.sigpro.2009.03.014
   Weng GR, 2021, EXPERT SYST APPL, V185, DOI 10.1016/j.eswa.2021.115633
   Weng GR, 2021, ENG APPL ARTIF INTEL, V104, DOI 10.1016/j.engappai.2021.104299
   Wu SF, 2015, INT J BIFURCAT CHAOS, V25, DOI 10.1142/S0218127415400398
   Yan X, 2022, APPL MATH MODEL, V101, P586, DOI 10.1016/j.apm.2021.09.002
   Yang CX, 2023, ENG APPL ARTIF INTEL, V123, DOI 10.1016/j.engappai.2023.106472
   Yang CX, 2022, SYMMETRY-BASEL, V14, DOI 10.3390/sym14112343
   Zhang KH, 2010, PATTERN RECOGN, V43, P1199, DOI 10.1016/j.patcog.2009.10.010
   Zhang T., 2022, ARXIV
NR 36
TC 1
Z9 1
U1 3
U2 3
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104069
DI 10.1016/j.jvcir.2024.104069
EA FEB 2024
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JY0L3
UT WOS:001176601500001
DA 2024-08-05
ER

PT J
AU Song, YC
   Wang, JC
   Ge, YF
   Li, LF
   Guo, J
   Dong, QX
   Liao, ZF
AF Song, Yucheng
   Wang, Jincan
   Ge, Yifan
   Li, Lifeng
   Guo, Jia
   Dong, Quanxing
   Liao, Zhifang
TI Medical image classification: Knowledge transfer via residual U-Net and
   vision transformer-based teacher-student model with knowledge
   distillation
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Knowledge distillation; Medical imaging; U -Net; Residual module;
   Attention; Vision Transformer
ID SEGMENTATION
AB With the widespread integration of deep learning techniques in the domain of medical image analysis, there is a prevailing consensus regarding their efficacy in handling high -dimensional and intricate medical image data. However, it is imperative to acknowledge that while complex deep models exhibit a remarkable capacity for processing high -dimensional and intricate data, they often necessitate a substantial allocation of computational resources and time. Furthermore, lightweight models, despite their computational efficiency, tend to underperform when compared to their more intricate counterparts in terms of performance. Hence, the prevailing aspiration is to transfer the cognitive prowess of complex models to their lightweight counterparts. Addressing the aforementioned concern, this study proposes a knowledge distillation approach that encompasses joint feature and soft label transfer. It entails the transference of knowledge from the teacher model's intermediate features and predictive outcomes to the student model. The student model leverages this knowledge to emulate the behavior of the teacher model, thereby enhancing the precision of its own predictions. Building upon this foundation, we introduce a Res -Transformer teacher model based on the U -Net architecture and a student model known as ResU-Net, which is grounded in residual modules. The Res -Transformer teacher model employs multilayer residual attention during the downsampling process to capture deep -level features of the image. Subsequently, we have incorporated a Multi -layer Perceptual Attention module (MPA) for each skip connection layer, facilitating the integration of hierarchical upsampled information to restore fine-grained details within the feature maps. The ResU-Net student model enhances network stability through the utilization of residual modules and optimizes skip connections to recover any lost image information during convolutional operations. Lastly, we conducted experimental assessments on multiple disease datasets. The results reveal that the ACC of the Res -Transformer model achieves an impressive 96.9%. Furthermore, through the knowledge distillation method, rich knowledge is effectively transferred to the ResU-Net model, resulting in a remarkable ACC improvement of 7.2%.
C1 [Song, Yucheng; Wang, Jincan; Ge, Yifan; Dong, Quanxing; Liao, Zhifang] Cent South Univ, Sch Comp Sci & Engn, Changsha 410083, Peoples R China.
   [Li, Lifeng] Univ South China, Affiliated Changsha Cent Hosp, Hengyang Med Sch, Dept Radiol, Hengyang, Hunan, Peoples R China.
   [Li, Lifeng] Nanchang Univ, Affiliated Hosp 1, Med Imaging Ctr, Nanchang, Jiangxi, Peoples R China.
   [Guo, Jia] Cent South Univ, Xiangya Sch Nursing, Changsha 410013, Peoples R China.
C3 Central South University; University of South China; Nanchang
   University; Central South University
RP Liao, ZF (corresponding author), Cent South Univ, Sch Comp Sci & Engn, Changsha 410083, Peoples R China.
EM zfliao@csu.edu.cn
RI Li, Lifeng/JRW-2275-2023
OI Li, Lifeng/0000-0001-9817-7661
FU Natural Science Foundation of China, Regional Science Fund Project
   [72264037]
FX This work was supported by National Natural Science Foundation of China,
   Regional Science Fund Project, No:72264037.
CR Archip N, 2002, IEEE T MED IMAGING, V21, P1504, DOI 10.1109/TMI.2002.806578
   Ayana G, 2022, DIAGNOSTICS, V12, DOI 10.3390/diagnostics12010135
   Brown MS, 1998, COMPUT MED IMAG GRAP, V22, P463, DOI 10.1016/S0895-6111(98)00051-2
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Cheng JL, 2022, MED IMAGE ANAL, V76, DOI 10.1016/j.media.2021.102313
   Cruz-Roa A, 2014, PROC SPIE, V9041, DOI 10.1117/12.2043872
   Dalmaz O, 2022, IEEE T MED IMAGING, V41, P2598, DOI 10.1109/TMI.2022.3167808
   Das A, 2022, APPL SOFT COMPUT, V115, DOI 10.1016/j.asoc.2021.108178
   Diakogiannis FI, 2020, ISPRS J PHOTOGRAMM, V162, P94, DOI 10.1016/j.isprsjprs.2020.01.013
   Du GT, 2020, J IMAGING SCI TECHN, V64, DOI 10.2352/J.ImagingSci.Technol.2020.64.2.020508
   Fan Y., 2022, CHIN C PATT REC COMP, P126
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hu J., 2018, Advances in neural information processing systems
   Hu J., 2018, SQUEEZE AND EXCITATI, P7132
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Kurtz C, 2014, J BIOMED INFORM, V49, P227, DOI 10.1016/j.jbi.2014.02.018
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee AWC, 2019, MED IMAGE ANAL, V57, P197, DOI 10.1016/j.media.2019.06.017
   Li CY, 2019, AAAI CONF ARTIF INTE, P6666
   Li X, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0232127
   Liu M, 2022, IEEE J BIOMED HEALTH, V26, P5025, DOI 10.1109/JBHI.2022.3187765
   Rahman T, 2021, COMPUT BIOL MED, V132, DOI 10.1016/j.compbiomed.2021.104319
   Ribeiro MX, 2008, IEEE T MULTIMEDIA, V10, P277, DOI 10.1109/TMM.2007.911837
   Shen DG, 2017, ANNU REV BIOMED ENG, V19, P221, DOI [10.1146/annurev-bioeng-071516-044442, 10.1146/annurev-bioeng-071516044442]
   Siddique N, 2021, IEEE ACCESS, V9, P82031, DOI 10.1109/ACCESS.2021.3086020
   Sonka M, 1996, IEEE T MED IMAGING, V15, P314, DOI 10.1109/42.500140
   Spanhol FA, 2016, IEEE T BIO-MED ENG, V63, P1455, DOI 10.1109/TBME.2015.2496264
   Tajbakhsh N, 2020, MED IMAGE ANAL, V63, DOI 10.1016/j.media.2020.101693
   Tian R, 2023, Arxiv, DOI arXiv:2212.00776
   Vaswani A, 2017, ADV NEUR IN, V30
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yao L, 2019, BMC MED INFORM DECIS, V19, DOI 10.1186/s12911-019-0781-4
   Zhang ZX, 2018, IEEE GEOSCI REMOTE S, V15, P749, DOI 10.1109/LGRS.2018.2802944
NR 34
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD JUN
PY 2024
VL 102
AR 104212
DI 10.1016/j.jvcir.2024.104212
EA JUN 2024
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XE4U8
UT WOS:001260003100001
DA 2024-08-05
ER

PT J
AU Jiang, JQ
   Liu, Z
   Li, J
   Tu, JM
   Li, L
   Yao, J
AF Jiang, Jiaqin
   Liu, Zhao
   Li, Jie
   Tu, Jingmin
   Li, Li
   Yao, Jian
TI iMVS: Integrating multi-view information on multiple scales for 3D
   object recognition ☆
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE 3D object recognition; Hybrid feature extraction; Multi-view information
   transfer; Multi-scale aggregation
ID CONVOLUTIONAL NEURAL-NETWORKS
AB 3D object recognition is a fundamental task in 3D computer vision. View -based methods have received considerable attention due to their high efficiency and superior performance. To better capture the longrange dependencies among multi -view images, Transformer has recently been introduced into view -based 3D object recognition and achieved excellent performance. However, the information among views on multiple scales is not utilized sufficiently in the existing Transformer -based methods. To address this limitation, we proposed a 3D object recognition method named iMVS to i ntegrate M ulti - V iew information on multiple S cales. Specifically, for the single -view image/features at each scale, we adopt a hybrid feature extraction module consisting of CNN and Transformer to jointly capture local and non -local information. For the extracted multi -view image features at each scale, we develop a feature transfer module including a view Transformer block to achieve the information transfer across views. Following a sequential process of the single -view feature extraction and multi -view feature transfer on multiple scales, the multi -view information is sufficiently interacted. Subsequently, the multi -scale features with multi -view information are fed into our designed feature aggregation module to generate a category -specific descriptor, where the adopted channel Transformer block facilitates the descriptor to be more expressive. Coupling with these designs, our method can fully exploit the information embedded within multi -view images. Experimental results on ModelNet40, ModelNet10 and a real -world dataset MVP -N demonstrate the superior performance of our method.
C1 [Jiang, Jiaqin; Li, Li; Yao, Jian] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Peoples R China.
   [Liu, Zhao; Li, Jie; Tu, Jingmin] Hubei Univ Technol, Sch Elect & Elect Engn, Wuhan 430068, Peoples R China.
   [Li, Li; Yao, Jian] Wuhan Univ, Shenzhen Res Inst, Shenzhen 518057, Peoples R China.
C3 Wuhan University; Hubei University of Technology; Wuhan University
RP Yao, J (corresponding author), Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Peoples R China.; Yao, J (corresponding author), Wuhan Univ, Shenzhen Res Inst, Shenzhen 518057, Peoples R China.
EM jian.yao@whu.edu.cn
FU National Natural Science Foundation of China [U22A2009, 42271445,
   42301515]; Fundamental Research Funds for the Central Universities
   [2042023kf0174]; Shenzhen Science and Technology Program
   [JCYJ20230807090201003]; Foundation of Anhui Province Key Laboratory of
   Physical Geographic Environment [2022PGE008]; Guangdong Basic and
   Applied Basic Research Foundation [2024A1515010218]
FX This work was partially supported by the National Natural Science
   Foundation of China (No. U22A2009, No. 42271445, No. 42301515) , the
   Fundamental Research Funds for the Central Universities (2042023kf0174)
   , the Shenzhen Science and Technology Program (JCYJ20230807090201003) ,
   the Foundation of Anhui Province Key Laboratory of Physical Geographic
   Environment (2022PGE008) , the Guangdong Basic and Applied Basic
   Research Foundation (2024A1515010218) .
CR Ben-Shabat Y, 2018, IEEE ROBOT AUTOM LET, V3, P3145, DOI 10.1109/LRA.2018.2850061
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen S., 2022, arXiv
   Chen S, 2021, Arxiv, DOI [arXiv:2110.13083, DOI 10.48550/ARXIV.2110.13083]
   Cui YM, 2021, NEUROCOMPUTING, V432, P300, DOI 10.1016/j.neucom.2020.12.067
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Hagbi N, 2011, IEEE T VIS COMPUT GR, V17, P1369, DOI 10.1109/TVCG.2010.241
   Han PF, 2015, COMPUT GRAPH-UK, V50, P36, DOI 10.1016/j.cag.2015.05.021
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P3986, DOI 10.1109/TIP.2019.2904460
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P658, DOI 10.1109/TIP.2018.2868426
   Hu M, 2021, APPL INTELL, V51, P6983, DOI 10.1007/s10489-021-02240-6
   Huang ZY, 2019, AAAI CONF ARTIF INTE, P8505
   Jia K, 2019, IEEE T IMAGE PROCESS, V28, P5121, DOI 10.1109/TIP.2019.2912356
   Jing WP, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14041036
   Kanezaki A, 2018, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR.2018.00526
   Kingma D. P., 2014, arXiv
   Kumawat S, 2019, PROC CVPR IEEE, P4898, DOI 10.1109/CVPR.2019.00504
   Li J, 2023, J VIS COMMUN IMAGE R, V95, DOI 10.1016/j.jvcir.2023.103906
   Lu DN, 2022, IEEE T INTELL TRANSP, V23, P24854, DOI 10.1109/TITS.2022.3198836
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Nie WZ, 2021, IEEE T IMAGE PROCESS, V30, P4371, DOI 10.1109/TIP.2021.3071687
   Nong LP, 2023, IEEE T MULTIMEDIA, V25, P4842, DOI 10.1109/TMM.2022.3183388
   Pylvanainen T., 2010, S 3D DAT PROC VIS TR, V737, P738
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Qi SH, 2022, IET COMPUT VIS, DOI 10.1049/cvi2.12107
   Qiu S, 2021, IEEE WINT CONF APPL, P3812, DOI 10.1109/WACV48630.2021.00386
   Ren MW, 2017, Arxiv, DOI [arXiv:1711.10108, DOI arXiv:1711.10108.null]
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Su JC, 2019, LECT NOTES COMPUT SC, V11131, P645, DOI 10.1007/978-3-030-11015-4_49
   Sun K, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3326606
   Sun K, 2021, IEEE T IMAGE PROCESS, V30, P868, DOI 10.1109/TIP.2020.3039378
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C, 2019, Arxiv, DOI arXiv:1906.01592
   Wang R., 2022, P ADV NEUR INF PROC, P20536
   Wang WJ, 2023, LECT NOTES COMPUT SC, V13841, P486, DOI 10.1007/978-3-031-26319-4_29
   Wang WJ, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14091996
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wei X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P397, DOI 10.1109/ICCV48922.2021.00046
   Wei X, 2020, PROC CVPR IEEE, P1847, DOI 10.1109/CVPR42600.2020.00192
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xu Y, 2021, IEEE T IMAGE PROCESS, V30, P5299, DOI 10.1109/TIP.2021.3082310
   Yang MM, 2023, IEEE WINT CONF APPL, P653, DOI 10.1109/WACV56688.2023.00072
   Yang YR, 2021, INT C PATT RECOG, P10235, DOI 10.1109/ICPR48806.2021.9413342
   Yang Z, 2019, IEEE I CONF COMP VIS, P7504, DOI 10.1109/ICCV.2019.00760
   Yu T, 2021, IEEE T IMAGE PROCESS, V30, P2168, DOI 10.1109/TIP.2021.3049968
   Yu T, 2018, PROC CVPR IEEE, P186, DOI 10.1109/CVPR.2018.00027
   Zhou HR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4945, DOI 10.1109/ICCV48922.2021.00492
NR 52
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 100
AR 104175
DI 10.1016/j.jvcir.2024.104175
EA MAY 2024
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TL2D2
UT WOS:001241343200001
DA 2024-08-05
ER

PT J
AU Li, XP
   Fu, ZX
   Yu, B
AF Li, Xiaopeng
   Fu, Zhengxin
   Yu, Bin
TI A novel extended secret image sharing scheme based on sharing matrix
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Review
DE Secret image sharing; No pixel expansion; Meaningful natural shares;
   Undistorted; Search window
ID VISUAL CRYPTOGRAPHY SCHEME; QUALITY; SHADOW
AB Ensuring the safety of communicating parties while sharing secret information and concealing their communication behavior in everyday activities has become an increasingly important issue. One approach to address this challenge is through a secret image sharing (SIS) scheme that utilizes meaningful shares. However, existing SIS techniques face ongoing challenges in enhancing the visual quality of those shares while ensuring lossless recovery of the secret image. To overcome these challenges, we propose a novel extended secret image sharing scheme (ESIS) based on a sharing matrix. By locating the secret pixel value within a search window centered on the coordinate of the pixel value pair of the cover images, our method achieves lossless recovery of the secret image with resulting shares that are natural images of continuous tone indistinguishable from original cover images by human visual perception. Extensive experimental results demonstrate its effectiveness and advantages, supported by theoretical analysis.
C1 [Li, Xiaopeng; Fu, Zhengxin; Yu, Bin] Strateg Support Force Informat Engn Univ, Zhengzhou 450004, Peoples R China.
C3 PLA Information Engineering University
RP Li, XP (corresponding author), Strateg Support Force Informat Engn Univ, Zhengzhou 450004, Peoples R China.
EM peng001123@sina.com; fzx2515@163.com; Byu2009@163.com
FU National Natural Science Foundation of China [61602513]
FX This research was supported by the National Natural Science Foun- dation
   of China (No. 61602513) . The authors would like to express their
   gratitude to the editor and the anonymous reviewers for their valuable
   comments.
CR Chang CC, 2014, 2014 TENTH INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING (IIH-MSP 2014), P89, DOI 10.1109/IIH-MSP.2014.29
   Chen YH, 2022, SYMMETRY-BASEL, V14, DOI 10.3390/sym14071445
   Cheng TF, 2017, MULTIMED TOOLS APPL, V76, P9337, DOI 10.1007/s11042-016-3535-7
   Cu DH, 2015, SIGNAL PROCESS, V108, P604, DOI 10.1016/j.sigpro.2014.10.011
   Fu ZX, 2019, MULTIMED TOOLS APPL, V78, P2367, DOI 10.1007/s11042-018-6364-z
   Guo C, 2016, MULTIMED TOOLS APPL, V75, P11577, DOI 10.1007/s11042-015-2885-x
   Guo T, 2014, SIGNAL PROCESS, V94, P90, DOI 10.1016/j.sigpro.2013.06.003
   Hua ZY, 2023, IEEE T DEPEND SECURE, V20, P3669, DOI 10.1109/TDSC.2022.3218570
   Lee JS, 2015, DIGIT SIGNAL PROCESS, V40, P131, DOI 10.1016/j.dsp.2015.02.012
   Liu YX, 2018, EURASIP J WIREL COMM, DOI 10.1186/s13638-018-1084-7
   Liu YJ, 2019, MATH BIOSCI ENG, V16, P1914, DOI 10.3934/mbe.2019093
   Naor M., 1995, Advances in Cryptology - EUROCRYPT '94. Workshop on the Theory and Application of Cryptographic Techniques. Proceedings, P1, DOI 10.1007/BFb0053419
   Huynh NT, 2015, J VIS COMMUN IMAGE R, V28, P105, DOI 10.1016/j.jvcir.2015.01.011
   Ren ZH, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10060864
   Suresh P.K., 2019, Int. J. Security and Networks, V14, P1
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yan XH, 2021, IEEE T CIRC SYST VID, V31, P2896, DOI 10.1109/TCSVT.2020.3025527
   Yan XH, 2018, IEEE ACCESS, V6, P45246, DOI 10.1109/ACCESS.2018.2865421
   Zhao YK, 2022, J VIS COMMUN IMAGE R, V82, DOI 10.1016/j.jvcir.2021.103408
NR 19
TC 0
Z9 0
U1 5
U2 5
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104149
DI 10.1016/j.jvcir.2024.104149
EA APR 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SE2V2
UT WOS:001232725300001
DA 2024-08-05
ER

PT J
AU Zhou, X
   Zhou, ZH
   Wang, MY
   Ning, B
   Wang, YH
   Zhu, PL
AF Zhou, Xin
   Zhou, Zihan
   Wang, Manying
   Ning, Bo
   Wang, Yanhao
   Zhu, Pengli
TI Multi-level feature enhancement network for object detection in sonar
   images
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Sonar image; Object detection; Multi-scale feature fusion; Geometric
   distortion
AB The unstable geometric features affect the accuracy of object detection in sonar images. We thus propose a novel multi-level feature enhancement network to enhance useful features for object detection in sonar images. We first introduce a deformable convolution to model variations in geometric features. In addition, spatial and channel attention modules are utilized to aggregate rich semantic information from features, improving the quality of feature extraction. We further use an adaptive multi-scale feature fusion module for feature weighting so as to enhance fine-grained features and minimize information loss during feature fusion. Then, the cascaded detection module corrects the prediction results of the previous detector with a low Intersection-over-Union (IoU) threshold, where each detector employs adaptive feature enhancement blocks to enhance region proposal features and thus improve detection performance. Experimental results on two real-world sonar image datasets show that our proposed model performs better than several mainstream object detection methods by achieving 2% to 19.4% higher accuracy rates.
C1 [Zhou, Xin; Zhou, Zihan; Wang, Manying; Ning, Bo] Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian 116026, Peoples R China.
   [Wang, Yanhao] East China Normal Univ, Sch Data Sci & Engn, Shanghai 200062, Peoples R China.
   [Zhu, Pengli] Dalian Maritime Univ, Coll Marine Engn, Dalian 116026, Peoples R China.
C3 Dalian Maritime University; East China Normal University; Dalian
   Maritime University
RP Zhou, X (corresponding author), Dalian Maritime Univ, Sch Informat Sci & Technol, Dalian 116026, Peoples R China.
EM zhouxin314159@dlmu.edu.cn; zmzzh@dlmu.edu.cn; manyingw@dlmu.edu.cn;
   ningbo@dlmu.edu.cn; yhwang@dase.ecnu.edu.cn; dlmu.p.l.zhu@gmail.com
RI Yanhao, Wang/I-7372-2019
OI Yanhao, Wang/0000-0002-7661-3917
FU National Natural Science Founda-tion of China [61976032, 62002039]
FX <B>Acknowledgments</B> This work was supported by the National Natural
   Science Founda-tion of China under grant numbers 61976032 and 62002039.
CR Alaie HK, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8010061
   Bochkovskiy A., 2020, IEEE C COMP VIS PATT
   Bore N, 2021, IEEE J OCEANIC ENG, V46, P195, DOI 10.1109/JOE.2020.2980456
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   CERVENKA P, 1993, IEEE J OCEANIC ENG, V18, P108, DOI 10.1109/48.219531
   Chen Q, 2021, PROC CVPR IEEE, P13034, DOI 10.1109/CVPR46437.2021.01284
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   [范习健 Fan Xijian], 2012, [中国图象图形学报, Journal of Image and Graphics], V17, P68
   Fan XN, 2022, MULTIMED TOOLS APPL, V81, P10091, DOI 10.1007/s11042-022-12054-4
   Fang Jie, 2020, 2020 International Conference on Information Science, Parallel and Distributed Systems (ISPDS), P25, DOI 10.1109/ISPDS51347.2020.00013
   Ge Z., 2021, ARXIV
   Guo G., 2018, Control Decis, V33, P906
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huo GY, 2020, IEEE ACCESS, V8, P47407, DOI 10.1109/ACCESS.2020.2978880
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kong WZ, 2020, IEEE SENS J, V20, P3745, DOI 10.1109/JSEN.2019.2960796
   Li Y., 2022, OCEANS 2022, P1
   Li YH, 2019, IEEE I CONF COMP VIS, P6053, DOI 10.1109/ICCV.2019.00615
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma QX, 2020, IEEE WINT CONF APPL, P718, DOI [10.1109/wacv45572.2020.9093467, 10.1109/WACV45572.2020.9093467]
   Neves G, 2020, EXPERT SYST APPL, V140, DOI 10.1016/j.eswa.2019.112870
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Redmon J, 2018, Arxiv, DOI arXiv:1804.02767
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Steiniger Y, 2021, J MAR SCI ENG, V9, DOI 10.3390/jmse9030239
   Sun YS, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14225807
   Tang YL, 2020, 2020 INTERNATIONAL CONFERENCE ON BIG DATA & ARTIFICIAL INTELLIGENCE & SOFTWARE ENGINEERING (ICBASE 2020), P348, DOI 10.1109/ICBASE51474.2020.00080
   Tang YL, 2020, IEEE ACCESS, V8, P173450, DOI 10.1109/ACCESS.2020.3024813
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tueller P, 2020, IET RADAR SONAR NAV, V14, P1940, DOI 10.1049/iet-rsn.2020.0224
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang F, 2022, J ELECTRON INF TECHN, V44, P3419, DOI 10.11999/JEIT220260
   Wang J., 2020, CoRR abs/2007.03496.
   Wang XM, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0177666
   Wang Z, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3105269
   Wang Z, 2022, IEEE SENS J, V22, P1509, DOI 10.1109/JSEN.2021.3131645
   Wang ZX, 2020, IEEE ACCESS, V8, P116569, DOI 10.1109/ACCESS.2020.3004198
   Yu YC, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13183555
   Zhang P, 2022, IEEE J-STARS, V15, P3365, DOI 10.1109/JSTARS.2022.3169339
   Zhang P, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3096011
   Zhou JC, 2024, Arxiv, DOI arXiv:2308.11918
   Zhu XK, 2021, IEEE INT CONF COMP V, P2778, DOI 10.1109/ICCVW54120.2021.00312
   Zhu XY, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2022, P443, DOI 10.1145/3512527.3531398
NR 47
TC 0
Z9 0
U1 3
U2 3
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104147
DI 10.1016/j.jvcir.2024.104147
EA APR 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TE7D8
UT WOS:001239642600001
DA 2024-08-05
ER

PT J
AU Wan, YC
   Zhao, QK
   Xu, JQ
   Wang, HZ
   Fang, LJ
AF Wan, Yingcai
   Zhao, Qiankun
   Xu, Jiqian
   Wang, Huaizhen
   Fang, Lijin
TI DAGNet: Depth-aware Glass-like objects segmentation via cross-modal
   attention
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Semantic segmentation; Transparent; Cross-modal; Self-attention
AB Transparent or specular objects, such as mirrors, glass windows, and glass walls, have a significant impact on computer vision tasks. Glass -like Objects (GLOS) encompass transparent or specular objects that lack distinctive visual appearances and specific external shapes, posing challenges for GLO segmentation. In this study, we propose a novel bidirectional cross -modal fusion framework with a shift-window cross-attention for GLO segmentation. The framework incorporates a Feature Exchange Module (FEM) and a Shifted-window Cross-attention Feature Fusion Module (SW-CAFM) in each transformer block stage to calibrate, exchange, and fuse cross -modal features. The FEM employs coordinate and spatial attention mechanisms to filter out the noise and recalibrate the features from two modalities. The Shifted-Window Cross -Modal Attention Fusion (SW-CAFM) uses cross-attention to fuse RGB and depth features, leveraging the shifted-window self-attention operation to reduce the computational complexity of cross-attention. The experimental results demonstrate the feasibility and high performance of the proposed method, achieving state -of -the -art results on various glass and mirror benchmarks. The method achieves mIoU accuracies of 90.32%, 94.24%, 88.76%, and 87.47% on the GDD, Trans10K, MSD, and RGBD-Mirror datasets, respectively.
C1 [Wan, Yingcai; Zhao, Qiankun; Xu, Jiqian; Fang, Lijin] Northeastern Univ, Fac Robot Sci & Engn, Shenyang, Peoples R China.
   [Wang, Huaizhen] Inspur Grp, Inst Shandong New Generat Informat Ind Technol, Jinan, Peoples R China.
C3 Northeastern University - China; Inspur
RP Fang, LJ (corresponding author), Northeastern Univ, Fac Robot Sci & Engn, Shenyang, Peoples R China.
EM ljfang@mail.neu.edu.cn
FU Liaoning Province Basic Research Plan Project, China
   [2022JH2/101300202]; National Natural Science Foundation of China
   [62273081]
FX This work was supported by the Liaoning Province Basic Research Plan
   Project, China: 2022JH2/101300202 and the National Natural Science
   Foundation of China under Grant: 62273081.
CR Armeni I., 2017, arXiv
   Bello I, 2019, IEEE I CONF COMP VIS, P3285, DOI 10.1109/ICCV.2019.00338
   Chang AE, 2017, Arxiv, DOI arXiv:1709.06158
   Chen L.-C., 2018, ECCV, P801, DOI [DOI 10.1007/978-3-030-01234-249, 10.1007/978-3-030-01234-2_49]
   Chen LC, 2016, Arxiv, DOI [arXiv:1412.7062, DOI 10.48550/ARXIV.1412.7062, 10.48550/ARXIV.1412.7062]
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Ding T, 2022, MEASUREMENT, V199, DOI 10.1016/j.measurement.2022.111429
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Dubey S, 2022, J VIS COMMUN IMAGE R, V89, DOI 10.1016/j.jvcir.2022.103620
   Enze Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P696, DOI 10.1007/978-3-030-58601-0_41
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   He H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15839, DOI 10.1109/ICCV48922.2021.01556
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu XW, 2018, PROC CVPR IEEE, P7454, DOI 10.1109/CVPR.2018.00778
   Huang SH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P844, DOI 10.1109/ICCV48922.2021.00090
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Klank U, 2011, IEEE INT CONF ROBOT
   Lin JY, 2021, PROC CVPR IEEE, P13410, DOI 10.1109/CVPR46437.2021.01321
   Lin JY, 2020, PROC CVPR IEEE, P3694, DOI 10.1109/CVPR42600.2020.00375
   Lin W., 2021, P IEEE CVF C COMP VI, P15990
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu W, 2015, Arxiv, DOI [arXiv:1506.04579, 10.48550/arXiv.1506.04579]
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZK, 2019, J VIS COMMUN IMAGE R, V65, DOI 10.1016/j.jvcir.2019.102651
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lv TF, 2022, APPL OPTICS, V61, P2219, DOI 10.1364/AO.449589
   Mei HY, 2021, PROC CVPR IEEE, P3043, DOI 10.1109/CVPR46437.2021.00306
   Mei HY, 2020, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR42600.2020.00374
   Miao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P374, DOI 10.1007/978-3-030-58604-1_23
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Nian Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13753, DOI 10.1109/CVPR42600.2020.01377
   Paszke A, 2019, ADV NEUR IN, V32
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Wang Q, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2023.103753
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang YY, 2021, MEASUREMENT, V170, DOI 10.1016/j.measurement.2020.108698
   Wang ZD, 2019, PROC CVPR IEEE, P1117, DOI 10.1109/CVPR.2019.00121
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiaokang Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P561, DOI 10.1007/978-3-030-58621-8_33
   Xu YC, 2015, IEEE I CONF COMP VIS, P3442, DOI 10.1109/ICCV.2015.393
   Yang X, 2019, IEEE I CONF COMP VIS, P8808, DOI 10.1109/ICCV.2019.00890
   Yin W, 2021, PROC CVPR IEEE, P204, DOI 10.1109/CVPR46437.2021.00027
   Yongri Piao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9057, DOI 10.1109/CVPR42600.2020.00908
   Youwei Pang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P235, DOI 10.1007/978-3-030-58595-2_15
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Yu LT, 2022, IEEE T IMAGE PROCESS, V31, P2920, DOI 10.1109/TIP.2022.3162709
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang JM, 2023, Arxiv, DOI arXiv:2203.04838
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou H., 2020, P IEEE CVF C COMP VI, P9138, DOI 10.1109/CVPR42600.2020.00916
   Zhu L, 2018, LECT NOTES COMPUT SC, V11210, P122, DOI 10.1007/978-3-030-01231-1_8
NR 64
TC 0
Z9 0
U1 7
U2 7
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104121
DI 10.1016/j.jvcir.2024.104121
EA MAR 2024
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QS6T7
UT WOS:001222906400001
DA 2024-08-05
ER

PT J
AU Li, W
   Huang, Y
   Zhang, XY
   Han, GJ
AF Li, Wei
   Huang, Ya
   Zhang, Xinyuan
   Han, Guijin
TI LIIS: Low-light image instance segmentation
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Instance segmentation; Low-light image; Post-processing detail
   enhancement denoising; module; Wavelet feature fusion module; W-BCNet
   instance segmentation network
AB Image features in low -light scenes become hard to distinguish and full of noise, which makes the performance of current popular instance segmentation models drastically degraded. We propose a two -stage approach for instance segmentation of low -light images with enhancement followed by segmentation. Stage -I corresponds to the Low -Light Image Enhancement (LLIE) process. We propose a post -processing Detail Enhancement Denoising Module (DEDM) to suppress degradation effects caused by the enhancement in the preprocessing stage. StageII represents the segmentation process of enhanced images. We construct the W-BCNet instance segmentation network and design a Wavelet Feature Fusion Module (WFFM) in the feature extraction stage to preserve more fine-grained features. We achieve great segmentation results on LIS, detailed comparative experiments and ablation studies show the advantages and excellent generalization ability of our model.
C1 [Li, Wei; Huang, Ya; Zhang, Xinyuan; Han, Guijin] Xian Univ Posts & Telecommun, 618 West Changan Ave, Xian 710121, Shaanxi, Peoples R China.
C3 Xi'an University of Posts & Telecommunications
RP Huang, Y (corresponding author), Xian Univ Posts & Telecommun, 618 West Changan Ave, Xian 710121, Shaanxi, Peoples R China.
EM xiyouhy@stu.xupt.edu.cn.com
FU Key Research and Development Plan General Project of Shaanxi Provincial
   Science and Technology Department, China [2023-YBGY-032]
FX <B>Acknowledgments</B> We thank Professor Han Guijin for the equipment
   and financial support. This work is supported by the Key Research and
   Development Plan General Project of Shaanxi Provincial Science and
   Technology Department, China under Grants (No. 2023-YBGY-032) .
CR Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925
   Chen LW, 2023, INT J COMPUT VISION, V131, P2198, DOI 10.1007/s11263-023-01808-8
   Gnanasambandam Abhiram, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P484, DOI 10.1007/978-3-030-58598-3_29
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo HF, 2021, INT C PATT RECOG, P5611, DOI 10.1109/ICPR48806.2021.9412802
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Ho JAT, 2020, Arxiv, DOI [arXiv:2006.11239, DOI 10.48550/ARXIV.2006.11239]
   Jiang H, 2023, Arxiv, DOI arXiv:2306.00306
   Jiang H, 2023, PROC CVPR IEEE, P15641, DOI 10.1109/CVPR52729.2023.01501
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Ke L, 2021, PROC CVPR IEEE, P4018, DOI 10.1109/CVPR46437.2021.00401
   Lee Y., 2020, CVPR, P13906
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P4225, DOI 10.1109/TPAMI.2021.3063604
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mohan R, 2021, INT J COMPUT VISION, V129, P1551, DOI 10.1007/s11263-021-01445-z
   Panagiotou S, 2023, Arxiv, DOI arXiv:2303.09627
   Picron C, 2023, Arxiv, DOI [arXiv:2307.01545, 10.48550/arXiv.2307.01545, DOI 10.48550/ARXIV.2307.01545]
   Ren JH, 2022, Arxiv, DOI [arXiv:2210.00545, 10.48550/arXiv.2210.00545, DOI 10.48550/ARXIV.2210.00545]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Song Y, 2020, Arxiv, DOI [arXiv:1907.05600, DOI 10.48550/ARXIV.1907.05600]
   Wang T., 2023, P AAAI C ART INT, P2654, DOI [DOI 10.1609/AAAI.V37I3.25364, 10.1609/AAAI.V37I3.25364]
   Wang T, 2023, Arxiv, DOI arXiv:2307.14659
   Wang YF, 2022, AAAI CONF ARTIF INTE, P2604
   Wei C, 2018, Arxiv, DOI arXiv:1808.04560
   Wu WH, 2022, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR52688.2022.00581
   Wu YR, 2023, IEEE T NETW SCI ENG, V10, P3086, DOI 10.1109/TNSE.2022.3151502
   Xinlong Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P649, DOI 10.1007/978-3-030-58523-5_38
   Yan CG, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472810
   Yuan Y, 2022, Arxiv, DOI [arXiv:2211.09206, DOI 10.48550/ARXIV.2211.09206]
   Zhang G, 2021, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR46437.2021.00679
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhang Zhenlin, 2022, arXiv
   Zhi Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P282, DOI 10.1007/978-3-030-58452-8_17
   Zhou DW, 2023, Arxiv, DOI [arXiv:2305.10028, DOI 10.48550/ARXIV.2305.10028]
   Zhou YQ, 2020, AAAI CONF ARTIF INTE, V34, P13074
NR 37
TC 0
Z9 0
U1 5
U2 5
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104116
DI 10.1016/j.jvcir.2024.104116
EA MAR 2024
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QN7K7
UT WOS:001221615000001
DA 2024-08-05
ER

PT J
AU Massa, KJL
   Grobler, H
AF Massa, Kelian J. L.
   Grobler, Hans
TI Adapting projection-based LiDAR semantic segmentation to natural domains
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Semantic analysis; Semantic segmentation; LiDAR; Natural data;
   Projection; Fusion
AB In this paper, an approach to the semantic segmentation of 3D LiDAR point clouds obtained from natural scenes is introduced. Using a state -of -the -art projection -based semantic segmentation model as the core segmentation network, several recent advances in projection -based 3D semantic segmentation methods are aggregated into a single model. These adaptions include: scan unfolding, soft-kNN post -processing, and multi -projection fusion. A novel Naive Bayesian approach to multi -projection fusion which weights class probabilities based on the outputs of the base classifiers is proposed to further increase robustness. Quantitative and qualitative evaluations on several datasets, including scenes from both urban and natural environments; show that aggregating these adaptions into a single model can further improve the accuracy of state -of -the -art projection -based approaches. Finally, it is demonstrated that the novel Naive Bayesian approach to multi -projection fusion addresses a number of the challenges inherent to natural data while also improving results on urban data.
C1 [Massa, Kelian J. L.; Grobler, Hans] Univ Pretoria, Dept Elect Elect & Comp Engn, ZA-0028 Hatfield, Pretoria, South Africa.
C3 University of Pretoria
RP Massa, KJL (corresponding author), Univ Pretoria, Dept Elect Elect & Comp Engn, ZA-0028 Hatfield, Pretoria, South Africa.
EM u17000841@tuks.co.za
OI Massa, Kelian/0009-0004-4220-6573
CR Akbari Y, 2021, ARTIF INTELL REV, V54, P3887, DOI 10.1007/s10462-020-09943-1
   Aksoy EE, 2020, IEEE INT VEH SYM, P926, DOI [10.1109/IV47402.2020.9304694, 10.13140/rg.2.2.22837.83689]
   Alnaggar YA, 2021, IEEE WINT CONF APPL, P1799, DOI 10.1109/WACV48630.2021.00184
   Ando A, 2023, PROC CVPR IEEE, P5240, DOI 10.1109/CVPR52729.2023.00507
   Behley J, 2019, IEEE I CONF COMP VIS, P9296, DOI 10.1109/ICCV.2019.00939
   Biasutti P, 2019, Arxiv, DOI arXiv:1905.08748
   Boulch A, 2018, COMPUT GRAPH-UK, V71, P189, DOI 10.1016/j.cag.2017.11.010
   Cheng H., 2022, 2022 IEEE INT C MULT, P1, DOI [10.1109/ICME52920.2022.9859693, DOI 10.1109/ICME52920.2022.9859693]
   Cortinhal Tiago, 2020, Advances in Visual Computing. 15th International Symposium, ISVC 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12510), P207, DOI 10.1007/978-3-030-64559-5_16
   Elich C, 2019, LECT NOTES COMPUT SC, V11824, P48, DOI 10.1007/978-3-030-33676-9_4
   Liong VE, 2020, Arxiv, DOI arXiv:2012.04934
   Guerry J, 2017, IEEE INT CONF COMP V, P669, DOI 10.1109/ICCVW.2017.85
   Jiang P., 2020, arXiv
   Kellner M, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22031139
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Kochanov D, 2020, Arxiv, DOI arXiv:2007.12668
   Kong LD, 2023, IEEE I CONF COMP VIS, P228, DOI 10.1109/ICCV51070.2023.00028
   Kuncheva LI, 2014, COMBINING PATTERN CLASSIFIERS: METHODS AND ALGORITHMS, 2ND EDITION, P1
   Li SJ, 2022, IEEE ROBOT AUTOM LET, V7, P738, DOI 10.1109/LRA.2021.3132059
   Maturana D., 2018, FIELD SERVICE ROBOTI, P335, DOI [10.1007/978-3-319-67361-522, DOI 10.1007/978-3-319-67361-522, DOI 10.1007/978-3-319-67361-5_22]
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Milioto A, 2019, IEEE INT C INT ROBOT, P4213, DOI 10.1109/IROS40897.2019.8967762
   Pan YC, 2020, Arxiv, DOI arXiv:2002.09147
   Qi CR, 2017, ADV NEUR IN, V30
   Qiu HB, 2022, Arxiv, DOI arXiv:2207.02605
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067
   Triess LT, 2020, IEEE INT VEH SYM, P1116, DOI [10.1109/IV47402.2020.9304631, 10.1109/iv47402.2020.9304631]
   Uhlemann E, 2021, IEEE VEH TECHNOL MAG, V16, P15, DOI 10.1109/MVT.2021.3065792
   Wu BC, 2019, IEEE INT CONF ROBOT, P4376, DOI [10.1109/ICRA.2019.8793495, 10.1109/icra.2019.8793495]
   Wu BC, 2018, IEEE INT CONF ROBOT, P1887
   Zhang JY, 2019, IEEE ACCESS, V7, P179118, DOI 10.1109/ACCESS.2019.2958671
   Zhao YM, 2021, IEEE INT C INT ROBOT, P4453, DOI 10.1109/IROS51168.2021.9636385
NR 34
TC 0
Z9 0
U1 4
U2 4
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104111
DI 10.1016/j.jvcir.2024.104111
EA MAR 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OF5Q3
UT WOS:001205868200001
OA hybrid
DA 2024-08-05
ER

PT J
AU Zheng, Q
   Guo, HL
   Yin, YH
   Zheng, B
   Jiang, HX
AF Zheng, Qian
   Guo, Hualing
   Yin, Yunhua
   Zheng, Bin
   Jiang, Hongxu
TI LFSimCC: Spatial fusion lightweight network for human pose estimation
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Lightweight model SimCC Self attention mechanism Spatial information
   fusion
AB To address the limitations of existing 2D human pose estimation methods in terms of speed and lightweight, we propose a method called Lightweight Fusion SimCC (LFSimCC). LFSimCC incorporates two modules: LiteFNet, which enhances multi-scale spatial information fusion, and LKC-GAU, which improves the modeling capability of spatial information. Specifically, LiteFNet utilizes a combination of self -attention mechanism and novel spatial convolution to enable feature maps to capture richer multi-level global feature representations within the network. On the other hand, LKC-GAU enhances SimCC's ability to capture spatial relationships between joints by incorporating a large kernel of convolution and a self -attention mechanism. Furthermore, we design a keypoint information fusion loss (IFL) that enhances the model's sensitivity to information between keypoints in the human body. Experimental results demonstrate that our method is capable of extracting more decisive information and suppressing redundant feature representations, leading to high recognition accuracy and low inference latency.
C1 [Zheng, Qian; Guo, Hualing; Zheng, Bin; Jiang, Hongxu] North Univ China, Sch Elect & Control Engn, Taiyuan 030051, Peoples R China.
   [Yin, Yunhua] Sci & Technol Transient Impact Lab, Beijing 102202, Peoples R China.
C3 North University of China
RP Guo, HL (corresponding author), North Univ China, Sch Elect & Control Engn, Taiyuan 030051, Peoples R China.
EM guohualing@nuc.edu.cn
FU Fundamental Research Program of Shanxi Province, China
   [202103021224221]; Science and Technology on Transient Impact Laboratory
   Foundation, China [6142606203208]
FX <B>Acknowledgments</B> This work was supported by the Fundamental
   Research Program of Shanxi Province, China, with the Grant/Award Number:
   202103021224221, and the Science and Technology on Transient Impact
   Laboratory Foundation, China, with the Grant/Award Number:
   6142606203208.
CR Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Cai Y, 2020, EUROPEAN C COMPUTER, P455, DOI DOI 10.1007/978-3-030-58580-8_27
   Cao YX, 2023, Arxiv, DOI arXiv:2306.12113
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Hong CQ, 2021, COMPUT VIS IMAGE UND, V208, DOI 10.1016/j.cviu.2021.103224
   Hong CQ, 2019, IEEE T IND INFORM, V15, P3952, DOI 10.1109/TII.2018.2884211
   Hong CQ, 2015, IEEE T IMAGE PROCESS, V24, P5659, DOI 10.1109/TIP.2015.2487860
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hua W., 2022, P INT C MACH LEARN, P9099
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Iqbal U, 2016, LECT NOTES COMPUT SC, V9914, P627, DOI 10.1007/978-3-319-48881-3_44
   Jiang T, 2023, Arxiv, DOI arXiv:2303.07399
   Jiang-Jiang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10093, DOI 10.1109/CVPR42600.2020.01011
   Johnson S., 2010, BMVC, V2, P5, DOI DOI 10.5244/C.24.12
   Kocabas M, 2018, LECT NOTES COMPUT SC, V11215, P437, DOI 10.1007/978-3-030-01252-6_26
   Kreiss S, 2019, PROC CVPR IEEE, P11969, DOI 10.1109/CVPR.2019.01225
   Li Q, 2022, Arxiv, DOI arXiv:2204.10762
   Li YJ, 2022, LECT NOTES COMPUT SC, V13666, P89, DOI 10.1007/978-3-031-20068-7_6
   Li YJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11293, DOI 10.1109/ICCV48922.2021.01112
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Luo ZX, 2021, PROC CVPR IEEE, P13259, DOI 10.1109/CVPR46437.2021.01306
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Mazzia V, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108487
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Niu Y, 2023, NEUROCOMPUTING, V544, DOI 10.1016/j.neucom.2023.126301
   Papandreou G, 2018, LECT NOTES COMPUT SC, V11218, P282, DOI 10.1007/978-3-030-01264-9_17
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Su K, 2019, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2019.00582
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Tang Y., 2022, Adv. Neural Inf. Process. Syst, V35, P9969
   Tang YH, 2022, PROC CVPR IEEE, P10925, DOI 10.1109/CVPR52688.2022.01066
   Tian QH, 2023, J VIS COMMUN IMAGE R, V95, DOI 10.1016/j.jvcir.2023.103891
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xie F, 2023, J VIS COMMUN IMAGE R, V95, DOI 10.1016/j.jvcir.2023.103889
   Xu LM, 2021, PROC CVPR IEEE, P16067, DOI 10.1109/CVPR46437.2021.01581
   Yang S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11782, DOI 10.1109/ICCV48922.2021.01159
   Yu CQ, 2021, PROC CVPR IEEE, P10435, DOI 10.1109/CVPR46437.2021.01030
   Yuan Y., 2021, P C NEUR INF PROC SY, P7281
   Zhang F, 2020, PROC CVPR IEEE, P7091, DOI 10.1109/CVPR42600.2020.00712
   Zhang LZ, 2023, J VIS COMMUN IMAGE R, V92, DOI 10.1016/j.jvcir.2023.103783
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zheng C, 2024, ACM COMPUT SURV, V56, DOI 10.1145/3603618
   Zhou HF, 2021, IEEE INT CONF BIG DA, P3215, DOI 10.1109/BigData52589.2021.9671770
   Zhu X., 2017, P IEEE CVF INT C COM, P4321
NR 57
TC 0
Z9 0
U1 12
U2 12
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104093
DI 10.1016/j.jvcir.2024.104093
EA FEB 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MX6C7
UT WOS:001196964700001
DA 2024-08-05
ER

PT J
AU Zhang, YZ
   Zhu, PF
   Zheng, TT
   Yu, PZ
   Wang, JM
AF Zhang, Yunzuo
   Zhu, Pengfei
   Zheng, Tingting
   Yu, Puze
   Wang, Jianming
TI Surveillance video synopsis framework base on tube set
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Surveillance video; Video synopsis; Tube set; Judge interactivity;
   Mapping model
ID OPTIMIZATION
AB Video synopsis technology can shorten the length of the video, which has attracted wide attention. However, due to the limitation of object extraction technology and the difficulty of preserving interactivity, synopsis videos will lose the semantic information of the original video. To address the above problems, we propose a video synopsis framework based on tube sets. Firstly, we propose a video tracking algorithm based on Yolov4 and Kalman Filter, which can effectively alleviate the problem that it is difficult for object tracking technology to extract occluded objects. Secondly, we propose a method that combines moving direction and dynamic threshold to judge interactivity. Accurately judging interactivity can ensure that the interactive objects are not separated in the synopsis video. Finally, we propose a tube set mapping model (TSMM) that can rearrange with the tube set as the basic unit. Experimental results demonstrate that our method can achieve superior performance than other state-of-the-art methods.
C1 [Zhang, Yunzuo; Zhu, Pengfei; Zheng, Tingting; Yu, Puze; Wang, Jianming] Shijiazhuang Tiedao Univ, Sch Informat Sci & Technol, Shijiazhuang 050043, Peoples R China.
C3 Shijiazhuang Tiedao University
RP Zhang, YZ (corresponding author), Shijiazhuang Tiedao Univ, Sch Informat Sci & Technol, Shijiazhuang 050043, Peoples R China.
EM zhangyunzuo888@sina.com
RI Zhu, Pengfei/AAB-6672-2022
OI Zhu, Pengfei/0000-0002-2566-5493
FU National Natural Science Foundation of China [61702347, 62027801];
   Natural Science Foundation of Hebei Province, China [F2022210007,
   F2017210161]; Science and Technology Project of Hebei Education
   Department, China [ZD2022100, QN2017132]; Central Guidance on Local
   Science and Technology Development Fund, China [226Z0501G]; Graduate
   innovation program, China [YC2023081]
FX This work is jointly supported by the National Natural Science
   Foundation of China (No. 61702347, No. 62027801) , the Natural Science
   Foundation of Hebei Province, China (No. F2022210007, No. F2017210161) ,
   the Science and Technology Project of Hebei Education Department, China
   (No. ZD2022100, No. QN2017132) , the Central Guidance on Local Science
   and Technology Development Fund, China (No. 226Z0501G) . Graduate
   innovation program, China (YC2023081) .
CR Blunsden S.J., 2009, Ann. BMVA
   Chou C.L., 2015, Coherent Event-Based Surveillance Video Synopsis Using Trajectory Clustering
   Fisher R., 2003, Ec funded caviar project IST 2001 37540
   Fu W, 2014, NEUROCOMPUTING, V135, P155, DOI 10.1016/j.neucom.2013.12.041
   Ghatak S, 2021, DIGIT SIGNAL PROCESS, V111, DOI 10.1016/j.dsp.2021.102988
   Ghatak S, 2020, IEEE T CONSUM ELECTR, V66, P144, DOI 10.1109/TCE.2020.2981829
   Ghatak S, 2020, MULTIMED TOOLS APPL, V79, P4429, DOI 10.1007/s11042-019-7389-7
   He Y, 2017, NEUROCOMPUTING, V225, P64, DOI 10.1016/j.neucom.2016.11.011
   Islam MR, 2019, IEEE INT CONF MULTI, P579, DOI 10.1109/ICMEW.2019.00105
   Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968
   KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671
   Kumar K, 2018, MULTIMED TOOLS APPL, V77, P7383, DOI 10.1007/s11042-017-4642-9
   Li XL, 2018, IEEE T IMAGE PROCESS, V27, P3798, DOI 10.1109/TIP.2018.2823420
   Li XL, 2016, IEEE T IMAGE PROCESS, V25, P740, DOI 10.1109/TIP.2015.2507942
   Lu ML, 2013, INT CONF ACOUST SPEE, P2292, DOI 10.1109/ICASSP.2013.6638063
   Mahamud S., 2006, 2006 IEEE COMPUTER S, V1, P1154
   Moussa MM, 2021, SIGNAL IMAGE VIDEO P, V15, P761, DOI 10.1007/s11760-020-01794-1
   Namitha K, 2020, MULTIMED TOOLS APPL, V79, P32331, DOI 10.1007/s11042-020-09493-2
   Namitha K., 2020, IEEE INT C COMMUNICA
   Nie YW, 2020, IEEE T IMAGE PROCESS, V29, P1465, DOI 10.1109/TIP.2019.2942543
   Nie YW, 2013, IEEE T VIS COMPUT GR, V19, P1664, DOI 10.1109/TVCG.2012.176
   [牛通 Niu Tong], 2021, [计算机工程与应用, Computer Engineering and Application], V57, P96
   Pappalardo G, 2019, IEEE IMAGE PROC, P664, DOI [10.1109/ICIP.2019.8803795, 10.1109/icip.2019.8803795]
   Paul M, 2019, IEEE T CIRC SYST VID, V29, P1856, DOI 10.1109/TCSVT.2018.2844780
   Pritch Y, 2007, IEEE I CONF COMP VIS, P833
   Pritch Y, 2008, IEEE T PATTERN ANAL, V30, P1971, DOI 10.1109/TPAMI.2008.29
   Pritch Y, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P195, DOI 10.1109/AVSS.2009.53
   Salehin Md Musfequs, 2017, 2017 IEEE International Conference on Multimedia and Expo: Workshops (ICMEW), P692, DOI 10.1109/ICMEW.2017.8026294
   Salehin MM, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0181636
   Salehin MM, 2017, J OPT SOC AM A, V34, P814, DOI 10.1364/JOSAA.34.000814
   Tian YM, 2016, IET COMPUT VIS, V10, P868, DOI 10.1049/iet-cvi.2016.0128
   U.K, 2004, Ec funded caviar project IST 2001 37540
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Xu L, 2015, J AMB INTEL HUM COMP, V6, P623, DOI 10.1007/s12652-015-0278-7
   Zhang J., 2014, WSEAS Trans. Circuits Syst., V13, P274
   Zhang YZ, 2023, EXPERT SYST APPL, V216, DOI 10.1016/j.eswa.2022.119467
   Zhang Yunzuo, 2023, Image Commun., V113
NR 37
TC 0
Z9 0
U1 2
U2 2
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104057
DI 10.1016/j.jvcir.2024.104057
EA JAN 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IK2R5
UT WOS:001166158200001
DA 2024-08-05
ER

PT J
AU Yang, RY
   Liu, D
   Wu, F
   Gao, W
AF Yang, Runyu
   Liu, Dong
   Wu, Feng
   Gao, Wen
TI CC-SMC: Chain coding-based segmentation map lossless compression
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Chain coding; Lossless compression; Occupancy map; Quadtree-based block
   partitioning; Segmentation map; Semantic map
ID CODE
AB A segmentation map, either static or dynamic, refers to a two-dimensional picture that may vary with time and indicates the segmentation label per pixel. Both the semantic map and the occupancy map in video-based point cloud compression (V-PCC) belong to the segmentation map we referred to. The semantic map can work for many machine vision tasks like tracking and has been used as a layer of image representation in some image compression methods. The occupancy map constitutes a part of the point cloud coding bitstream. Since segmentation maps are widely used, how to efficiently compress them is of interest. We propose a segmentation map lossless compression scheme namely CC-SMC, exploiting the nature of segmentation maps that usually contain limited colors and sharp edges. Specifically, we design a chain coding-based scheme combined with quadtree-based block partitioning. For intraframe coding, one block is partitioned recursively with a quadtree structure, until the block contains only one color, is smaller than a threshold, or satisfies the defined chain coding condition. We revise the three-orthogonal chain coding method to incorporate contextual information and design effective intraframe prediction methods. For interframe coding, one block may find a reference block; the chain difference between the current and the reference blocks is coded. We implement the proposed scheme and test it on several different kinds of segmentation maps. Compared with advanced lossless image compression techniques, our proposed scheme obtains more than 10% bits reduction as well as more than 20% decoding time-saving. The code is available at https://github.com/Yang-Runyu/CC-SMC.
C1 [Yang, Runyu; Liu, Dong; Wu, Feng] Univ Sci & Technol China, MOE Key Lab Brain Inspired Intelligent Percept & C, Hefei 230027, Peoples R China.
   [Gao, Wen] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Gao, Wen] Peking Univ, Beijing 100871, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Peng Cheng Laboratory; Peking University
RP Liu, D (corresponding author), Univ Sci & Technol China, MOE Key Lab Brain Inspired Intelligent Percept & C, Hefei 230027, Peoples R China.
EM dongeliu@ustc.edu.cn
FU Natural Science Foundation of China [61931014, 62021001]; Funda-mental
   Research Funds for the Central Universities [WK3490000006]
FX <B>Acknowledgments</B> This work was supported by the Natural Science
   Foundation of China under Grants 61931014 and 62021001, and by the
   Funda-mental Research Funds for the Central Universities under Grant No.
   WK3490000006.
CR Akbari M, 2019, INT CONF ACOUST SPEE, P2042, DOI [10.1109/ICASSP.2019.8683541, 10.1109/icassp.2019.8683541]
   Brady N, 1999, IEEE T CIRC SYST VID, V9, P1170, DOI 10.1109/76.809154
   Bribiesca E, 1999, PATTERN RECOGN, V32, P235, DOI 10.1016/S0031-3203(98)00132-0
   Chan YH, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC424
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Freeman H., 1974, Computing Surveys, V6, P57, DOI 10.1145/356625.356627
   Freeman H., 1961, Electronic Computers, IRE Transactions on, V10, P260, DOI DOI 10.1109/TEC.1961.5219197
   Guo LW, 2014, IEEE IMAGE PROC, P5556, DOI 10.1109/ICIP.2014.7026124
   ISO/IEC, 1993, ISO/IEC11544.
   ISO/IEC, 2003, ISO/IEC 15948
   Jeromel A, 2020, MULTIMED TOOLS APPL, V79, P433, DOI 10.1007/s11042-019-08126-7
   Kirillov A, 2023, IEEE I CONF COMP VIS, P3992, DOI 10.1109/ICCV51070.2023.00371
   Liu YK, 2005, PATTERN RECOGN, V38, P553, DOI 10.1016/j.patcog.2004.08.017
   loup Gailly J., 2018, Gzip
   LU CC, 1991, IEEE T COMMUN, V39, P1511, DOI 10.1109/26.103046
   Miao JX, 2021, PROC CVPR IEEE, P4131, DOI 10.1109/CVPR46437.2021.00412
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Preda M, 2017, Technical Report MPEG-w17251
   Ryabko B. Ya., 1980, Problems of Information Transmission, V16, P265
   Sánchez-Cruz H, 2005, OPT ENG, V44, DOI 10.1117/1.2052793
   Sánchez-Cruz H, 2007, PATTERN RECOGN, V40, P1660, DOI 10.1016/j.patcog.2006.10.013
   Schwarz S., 2018, Technical Report MPEG-w17766
   Sneyers J, 2016, IEEE IMAGE PROC, P66, DOI 10.1109/ICIP.2016.7532320
   Tsang SH, 2019, IEEE T MULTIMEDIA, V21, P269, DOI 10.1109/TMM.2018.2856078
   Xu JZ, 2016, IEEE T CIRC SYST VID, V26, P50, DOI 10.1109/TCSVT.2015.2478706
   Yan N, 2020, IEEE INT SYMP CIRC S, DOI 10.1109/iscas45731.2020.9180529
   Yang RY, 2020, IEEE I C VI COM I PR, P479, DOI 10.1109/vcip49819.2020.9301867
   Yu H., 2015, Technical Report JCTVC-X1015
   Yu SQ, 2006, INT C PATT RECOG, P441
   Zalik B, 2021, J VIS COMMUN IMAGE R, V75, DOI 10.1016/j.jvcir.2021.103050
   Zhao LP, 2018, IEEE T MULTIMEDIA, V20, P796, DOI 10.1109/TMM.2017.2758519
   Zhu ZJ, 2015, PATTERN RECOGN LETT, V65, P131, DOI 10.1016/j.patrec.2015.07.026
NR 32
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD AUG
PY 2024
VL 103
AR 104222
DI 10.1016/j.jvcir.2024.104222
EA JUL 2024
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YM4W9
UT WOS:001268903100001
DA 2024-08-05
ER

PT J
AU Zhang, Q
   Yan, WQ
   Zhao, YL
   Jin, Q
   Zhang, Y
AF Zhang, Qing
   Yan, Weiqi
   Zhao, Yilin
   Jin, Qi
   Zhang, Yu
TI AFINet: Camouflaged object detection via Attention Fusion and
   Interaction Network
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Camouflaged object detection; Cross-level feature fusion; Attention
   interaction and fusion; Boundary guidance
AB Since the camouflaged objects share very similar colors and textures with the surroundings, there is still a great challenge in accurately locating and segmenting target objects with the varying sizes and shapes in different scenes. In this paper, we propose a novel Attention Fusion and Interaction network (AFINet) to detect the camouflaged objects by exploring the cross -level complementary information. Specifically, we first propose a Multi -Attention Interaction (MAI) module to fuse the cross -level features containing different characteristics by the attention interaction, thereby fully making use of the specific and complementary information from different levels to deal with scale variation. Furthermore, we design a Location and Boundary Guidance (LBG) module to make each side -output feature aware of where to learn, which can avoid the disturbances of the noncamouflaged regions by distinguishing the subtle differences. Comprehensive experiments and comparisons are conducted on four widely used benchmark datasets, demonstrating that the proposed network achieves stateof-the-art performance. The code and prediction maps will be available at https://github.com/ZhangQing0329/ AFINet.
C1 [Zhang, Qing; Yan, Weiqi; Zhao, Yilin; Jin, Qi; Zhang, Yu] Shanghai Inst Technol, Dept Comp Sci & Informat Engn, Shanghai 201418, Peoples R China.
C3 Shanghai Institute of Technology
RP Zhang, Y (corresponding author), Shanghai Inst Technol, Dept Comp Sci & Informat Engn, Shanghai 201418, Peoples R China.
EM zhangyu95900@163.com
FU Natural Science Foundation of Shanghai, China [21ZR1462600, 19ZR1455300]
FX This work is supported by Natural Science Foundation of Shanghai, China
   under Grant Nos. 21ZR1462600 and 19ZR1455300.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007
   Chen G, 2022, IEEE T CIRC SYST VID, V32, P6981, DOI 10.1109/TCSVT.2022.3178173
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26
   Dong Bo, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P2131, DOI 10.1145/3581783.3612185
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P6024, DOI 10.1109/TPAMI.2021.3085766
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Fan DP, 2020, IEEE T MED IMAGING, V39, P2626, DOI 10.1109/TMI.2020.2996645
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fang YQ, 2019, LECT NOTES COMPUT SC, V11764, P302, DOI 10.1007/978-3-030-32239-7_34
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   He CM, 2023, PROC CVPR IEEE, P22046, DOI 10.1109/CVPR52729.2023.02111
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37
   Ji GP, 2022, PATTERN RECOGN, V123, DOI 10.1016/j.patcog.2021.108414
   Jia Q, 2022, PROC CVPR IEEE, P4703, DOI 10.1109/CVPR52688.2022.00467
   Li AX, 2021, PROC CVPR IEEE, P10066, DOI 10.1109/CVPR46437.2021.00994
   Li P, 2022, IEEE T IMAGE PROCESS, V31, P6396, DOI 10.1109/TIP.2022.3189828
   Li S, 2018, IEEE T IMAGE PROCESS, V27, P3918, DOI 10.1109/TIP.2018.2828329
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Lv YQ, 2021, PROC CVPR IEEE, P11586, DOI 10.1109/CVPR46437.2021.01142
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Máttyus G, 2017, IEEE I CONF COMP VIS, P3458, DOI 10.1109/ICCV.2017.372
   Mei HY, 2021, PROC CVPR IEEE, P8768, DOI 10.1109/CVPR46437.2021.00866
   Mondal A, 2020, INT J IMAGE GRAPH, V20, DOI 10.1142/S021946782050028X
   Pang YW, 2022, PROC CVPR IEEE, P2150, DOI 10.1109/CVPR52688.2022.00220
   Patel K, 2021, 2021 18TH CONFERENCE ON ROBOTS AND VISION (CRV 2021), P181, DOI [10.1109/CRV52889.2021.00032, 10.1109/crv52889.2021.00032]
   Pei JL, 2022, LECT NOTES COMPUT SC, V13678, P19, DOI 10.1007/978-3-031-19797-0_2
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Przemyslaw S., 2018, Animal camouflage analysis: Chameleon database
   Ren JJ, 2023, IEEE T CIRC SYST VID, V33, P1157, DOI 10.1109/TCSVT.2021.3126591
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruifei Zhang, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P253, DOI 10.1007/978-3-030-59725-2_25
   Silva J, 2014, INT J COMPUT ASS RAD, V9, P283, DOI 10.1007/s11548-013-0926-3
   Sudre CH, 2017, LECT NOTES COMPUT SC, V10553, P240, DOI 10.1007/978-3-319-67558-9_28
   Sun Y., 2021, P INT JOINT C ART IN, P1025
   Sun Y., 2022, P INT JOINT C ART IN, P1335, DOI DOI 10.24963/IJCAI.2022/186
   Le TN, 2019, COMPUT VIS IMAGE UND, V184, P45, DOI 10.1016/j.cviu.2019.04.006
   Vázquez D, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/4037190
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang F, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4126, DOI 10.1109/ICCV48922.2021.00411
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zhai Q, 2021, PROC CVPR IEEE, P12992, DOI 10.1109/CVPR46437.2021.01280
   Zhai W, 2023, IEEE T MULTIMEDIA, V25, P5155, DOI 10.1109/TMM.2022.3188401
   Zhang LQ, 2024, IEEE T CIRC SYST VID, V34, P534, DOI 10.1109/TCSVT.2023.3287167
   Zhang M, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5323, DOI 10.1145/3503161.3548178
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang Q, 2023, IEEE INT CON MULTI, P2441, DOI 10.1109/ICME55011.2023.00416
   Zhang Q, 2023, IEEE T CIRC SYST VID, V33, P298, DOI 10.1109/TCSVT.2022.3199780
   Zhang X, 2017, IEEE T CIRC SYST VID, V27, P2001, DOI 10.1109/TCSVT.2016.2555719
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao XQ, 2021, LECT NOTES COMPUT SC, V12901, P120, DOI 10.1007/978-3-030-87193-2_12
   Zhong YJ, 2022, PROC CVPR IEEE, P4494, DOI 10.1109/CVPR52688.2022.00446
   Zhou T, 2022, IEEE T IMAGE PROCESS, V31, P7036, DOI 10.1109/TIP.2022.3217695
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
   Zhu HW, 2022, AAAI CONF ARTIF INTE, P3608
   Zhu JC, 2021, AAAI CONF ARTIF INTE, V35, P3599
   Zhuge MC, 2022, PATTERN RECOGN, V127, DOI 10.1016/j.patcog.2022.108644
NR 64
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD JUN
PY 2024
VL 102
AR 104208
DI 10.1016/j.jvcir.2024.104208
EA JUN 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XH6P9
UT WOS:001260834300001
DA 2024-08-05
ER

PT J
AU Qiao, FJ
   Zhub, Y
   Li, GF
   Li, B
AF Qiao, Fengjuan
   Zhub, Yonggui
   Li, Guofang
   Li, Bin
TI Dual contrastive attention-guided deformable convolutional network for
   single image super-resolution
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Image super-resolution; Deformable convolution; Contrastive learning;
   Attention mechanism
AB With its powerful ability to model geometric transformations, the deformable convolutional network brings great improvements for single image super-resolution (SISR). Nevertheless, its location -variant sampling method leads to an escalation in spatial variance as the deformable convolutional layers are stacked, consequently resulting in limited performance. Hence, we propose a novel and effective approach called dual contrastive attention-guided deformable convolutional network (DCADCN) for SISR modeling. Specifically, we propose an attention-guided deformable convolutional module with joint inner and external attention mechanisms to fully exploit the correspondences between input and deformation features and preserve spatial characteristics to the extent possible. Additionally, we propose a dual mixed feature extractor consisting of two parallel sub-paths. This design allows for the learning of diverse and complementary spatial features. Furthermore, contrastive learning is applied to further amplify the role of key features and mitigate the interference of noisy features. Extensive experimental results demonstrate that DCADCN is capable of effectively handling classic SISR, SISR with blind noise, and real-world SISR tasks. Moreover, our method achieves comparable or even better performance with lower computational cost compared to state -of -the -art methods.
C1 [Qiao, Fengjuan; Li, Guofang] Commun Univ China, Sch Informat & Commun Engn, Beijing 100024, Peoples R China.
   [Zhub, Yonggui] Commun Univ China, Sch Data Sci & Intelligent Media, Beijing 100024, Peoples R China.
   [Li, Bin] Qilu Univ Technol, Shandong Acad Sci, Sch Math & Stat, Jinan 250353, Peoples R China.
C3 Communication University of China; Communication University of China;
   Qilu University of Technology
RP Zhub, Y (corresponding author), Commun Univ China, Sch Data Sci & Intelligent Media, Beijing 100024, Peoples R China.
EM qiao_fj@126.com; ygzhu@cuc.edu.cn; gfli@cuc.edu.cn; ribbenlee@126.com
FU National Natural Science Foundation of China [11571325]; Fundamental
   Research Funds for the Central Universities, China [CUC2019 A002];
   Public Computing Cloud, CUC
FX This document is the results of the research projects funded by the
   National Natural Science Foundation of China (No. 11571325) and the
   Fundamental Research Funds for the Central Universities, China (No.
   CUC2019 A002) . This work is supported by Public Computing Cloud, CUC.
CR Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen YP, 2017, ADV NEUR IN, V30
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Deng Weijian, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1712, DOI 10.1109/CVPRW59228.2023.00172
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Gao DD, 2023, EXPERT SYST APPL, V213, DOI 10.1016/j.eswa.2022.118898
   Gao GW, 2022, AAAI CONF ARTIF INTE, P661
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Huang MX, 2022, PROC CVPR IEEE, P4583, DOI 10.1109/CVPR52688.2022.00455
   Huang Y, 2021, KNOWL-BASED SYST, V231, DOI 10.1016/j.knosys.2021.107384
   Ji XZ, 2020, IEEE COMPUT SOC CONF, P1914, DOI 10.1109/CVPRW50498.2020.00241
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kong FY, 2022, IEEE COMPUT SOC CONF, P765, DOI 10.1109/CVPRW56347.2022.00092
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Liang J, 2022, LECT NOTES COMPUT SC, V13678, P574, DOI 10.1007/978-3-031-19797-0_33
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Liu Z, 2021, IEEE COMPUT SOC CONF, P463, DOI 10.1109/CVPRW53098.2021.00057
   Liu ZY, 2020, IEEE T INSTRUM MEAS, V69, P9681, DOI 10.1109/TIM.2020.3001695
   Luo ZW, 2021, IEEE COMPUT SOC CONF, P471, DOI 10.1109/CVPRW53098.2021.00058
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Qin JH, 2022, NEUROCOMPUTING, V500, P846, DOI 10.1016/j.neucom.2022.05.066
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Sun L., 2022, Advances in Neural Information Processing Systems, V35, P17314
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tian CW, 2022, IEEE T SYST MAN CY-S, V52, P3718, DOI 10.1109/TSMC.2021.3069265
   Tian CW, 2020, KNOWL-BASED SYST, V205, DOI 10.1016/j.knosys.2020.106235
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HQ, 2022, PROC CVPR IEEE, P16020, DOI 10.1109/CVPR52688.2022.01557
   Wang LG, 2021, PROC CVPR IEEE, P10576, DOI 10.1109/CVPR46437.2021.01044
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wang YQ, 2021, IEEE T IMAGE PROCESS, V30, P1057, DOI 10.1109/TIP.2020.3042059
   Wang ZX, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3569900
   Wu G, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3290038
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yeh CH, 2022, LECT NOTES COMPUT SC, V13686, P668, DOI 10.1007/978-3-031-19809-0_38
   Yoon Y, 2017, IEEE SIGNAL PROC LET, V24, P848, DOI 10.1109/LSP.2017.2669333
   Zeyde R., 2012, INT C CURV SURF, P711
   Zhang K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4771, DOI 10.1109/ICCV48922.2021.00475
   Zhang K, 2019, PROC CVPR IEEE, P1671, DOI 10.1109/CVPR.2019.00177
   Zhang Y, 2022, APPL INTELL, V52, P295, DOI 10.1007/s10489-021-02246-0
   Zhang YF, 2022, KNOWL-BASED SYST, V249, DOI 10.1016/j.knosys.2022.108984
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang ZD, 2019, IEEE T IMAGE PROCESS, V28, P1625, DOI 10.1109/TIP.2018.2877483
   Zhao CH, 2022, IEEE T IMAGE PROCESS, V31, P3838, DOI 10.1109/TIP.2022.3176537
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 55
TC 0
Z9 0
U1 6
U2 6
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104097
DI 10.1016/j.jvcir.2024.104097
EA FEB 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NH2Z1
UT WOS:001199507600001
OA hybrid
DA 2024-08-05
ER

PT J
AU Wang, Y
   Zhou, Y
   Li, MY
   Sun, YJ
   Ding, JC
AF Wang, Yi
   Zhou, Yu
   Li, Mengyu
   Sun, Yanjing
   Ding, Jicun
TI Blind omnidirectional image quality assessment based on semantic
   information replenishment
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Blind omnidirectional image quality assessment (BOIQA); Semantic
   information replenishment; Multi-scale feature representation;
   Multi-stream network
AB Blind Omnidirectional Image Quality Assessment (BOIQA) is of great significance to the development of the immersive media technology. Most BOIQA metrics are achieved by projecting the raw spherical OIs into other plane spaces for better feature representation. For these metrics, the semantic information at the shared boundaries of the adjacent views are prone to be destroyed, hindering the semantic understanding and further performance improvement. To tackle this problem, we propose a lightweight but effective BOIQA metric via replenishing the damaged semantic information. Specifically, a multi-stream semantic information replenishment module is constructed by the multi-scale feature representation, which is designed to restore the destroyed semantic information from two adjacent views. For module learning, more than 20,000 image triplets are further built. Then, the restored features are integrated for the final quality prediction. To testify the effectiveness of the proposed method, extensive experiments are conducted on the public OIQA databases, and the results prove the superior performance of the proposed method.
C1 [Wang, Yi; Zhou, Yu; Li, Mengyu; Sun, Yanjing] China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Jiangsu, Peoples R China.
   [Wang, Yi] Jiangsu Normal Univ, Kewen Coll, Xuzhou 221132, Jiangsu, Peoples R China.
   [Zhou, Yu; Ding, Jicun] Xuzhou First Peoples Hosp, Xuzhou 221116, Jiangsu, Peoples R China.
C3 China University of Mining & Technology; Jiangsu Normal University
RP Ding, JC (corresponding author), Xuzhou First Peoples Hosp, Xuzhou 221116, Jiangsu, Peoples R China.
EM djcun@126.com
FU National Natural Science Foundation of China [62001475]; Natural Science
   Foundation of Jiangsu Province [BK20200649]; Hospital Management
   Innovation Research Project of Jiangsu Hospital Association
   [JSYGY-3-2023-353]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62001475, in part by the Natural
   Science Foundation of Jiangsu Province under Grant BK20200649, in part
   by the Hospital Management Innovation Research Project of Jiangsu
   Hospital Association under Grant JSYGY-3-2023-353.
CR Chen SJ, 2018, IEEE INT CON MULTI
   Duan HY, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351786
   Fang YM, 2022, AAAI CONF ARTIF INTE, P580
   GREENE N, 1986, IEEE COMPUT GRAPH, V6, P21, DOI 10.1109/MCG.1986.276658
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jiang H, 2022, IEEE T CIRC SYST VID, V32, P4211, DOI 10.1109/TCSVT.2021.3128014
   Jiang H, 2021, IEEE T IMAGE PROCESS, V30, P2364, DOI 10.1109/TIP.2021.3052073
   Kim HG, 2020, IEEE T CIRC SYST VID, V30, P917, DOI 10.1109/TCSVT.2019.2898732
   Li LD, 2021, IEEE T MULTIMEDIA, V23, P320, DOI 10.1109/TMM.2020.2980185
   Li LD, 2020, IEEE T CIRC SYST VID, V30, P3859, DOI 10.1109/TCSVT.2019.2947450
   Lin HH, 2019, INT WORK QUAL MULTIM
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu X.L., 2024, ACM Trans. Multi. Comput., V20, P1
   Liu Y., 2021, IEEE Trans. Instrum. Meas., V70
   Liu Y, 2023, J VIS COMMUN IMAGE R, V91, DOI 10.1016/j.jvcir.2023.103770
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503
   Madhusudana PC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2921858
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Qiu MM, 2021, OPTIK, V240, DOI 10.1016/j.ijleo.2021.166858
   Sui XJ, 2022, IEEE T VIS COMPUT GR, V28, P3022, DOI 10.1109/TVCG.2021.3050888
   Sun W, 2020, IEEE J-STSP, V14, P64, DOI 10.1109/JSTSP.2019.2955024
   Sun YL, 2017, IEEE SIGNAL PROC LET, V24, P1408, DOI 10.1109/LSP.2017.2720693
   Tian SS, 2019, IEEE T MULTIMEDIA, V21, P1235, DOI 10.1109/TMM.2018.2875307
   Tian SS, 2018, IEEE T IMAGE PROCESS, V27, P1652, DOI 10.1109/TIP.2017.2781420
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu JH, 2021, IEEE T CIRC SYST VID, V31, P1724, DOI 10.1109/TCSVT.2020.3015186
   Yan JB, 2022, IEEE T IMAGE PROCESS, V31, P3896, DOI 10.1109/TIP.2022.3177127
   Yu M, 2015, 2015 IEEE International Symposium on Mixed and Augmented Reality, P31, DOI 10.1109/ISMAR.2015.12
   Yue GH, 2023, IEEE T CIRC SYST VID, V33, P5549, DOI 10.1109/TCSVT.2023.3260212
   Yue GH, 2023, IEEE T MULTIMEDIA, V25, P6499, DOI 10.1109/TMM.2022.3209889
   Zakharchenko V, 2016, PROC SPIE, V9970, DOI 10.1117/12.2235885
   Zheng XL, 2020, IEEE ACCESS, V8, P31647, DOI 10.1109/ACCESS.2020.2972158
   Zhou Y., 2024, IEEE Trans. Instrum. Meas., V73
   Zhou Y, 2023, IEEE T MULTIMEDIA, V25, P4177, DOI 10.1109/TMM.2022.3171684
   Zhou Y, 2022, IEEE T CIRC SYST VID, V32, P1767, DOI 10.1109/TCSVT.2021.3081162
   Zhou Y, 2019, IEEE T IMAGE PROCESS, V28, P4566, DOI 10.1109/TIP.2019.2912463
NR 38
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD AUG
PY 2024
VL 103
AR 104241
DI 10.1016/j.jvcir.2024.104241
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZV5H2
UT WOS:001278069900001
DA 2024-08-05
ER

PT J
AU Liu, JC
   Li, X
   Dong, CX
AF Liu, Jichuan
   Li, Xiao
   Dong, Chunxi
TI Unknown Sample Selection and Discriminative Classifier Learning for
   Generalized Category Discovery ☆
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Generalized Category Discovery; Linear subspace; Neighborhood relevance
   score; Unknown Samples Selection; Uncertainty augmentation loss
AB Traditional supervised techniques rely on labeled data, which are not available for unknown classes. Generalized Category Discovery (GCD) aims to categorize data into both known and unknown classes. We proposed a method, Unknown Sample Selection and Discriminative Classifier Learning for GCD (USSDCL). Firstly, we map the features to a linear subspace to identify dimensions that best represent the semantic differences among the known classes and introduce a neighborhood relevance score to assess the consistency of labels among neighbors. Using this score, we can efficiently distinguish between known, unknown, and uncertain samples. Next, we leverage both the known samples, pseudo -labeled unknown samples, and the uncertain samples to train a discriminative classifier. This classifier incorporates both a cross -entropy loss and an uncertainty augmentation loss to encourage closer predictions between uncertain samples and their augmentations, enhancing the classifier's discriminative capability. Our model exhibits enhanced performance in classifying both known and unknown samples.
C1 [Liu, Jichuan; Dong, Chunxi] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
   [Li, Xiao] Xidian Univ, Sch Comp Sci & Technol, Xian 710071, Peoples R China.
   [Liu, Jichuan] 54th Res Inst China Elect Technol Grp Corp, Shijiazhuang 050081, Peoples R China.
C3 Xidian University; Xidian University; China Electronics Technology Group
RP Li, X (corresponding author), Xidian Univ, Sch Comp Sci & Technol, Xian 710071, Peoples R China.
EM xiaoli@xidian.edu.cn
FU National Natural Science Foundation of China [62176197]
FX <B>Acknowledgments</B> This work is supported by the National Natural
   Science Foundation of China under (Grant No. 62176197) .
CR An W., 2023, P AAAI C ART INT, V37, P12527
   An W., 2024, P AAAI C ART INT, V38, P10856
   Cao Kaidi, 2022, INT C LEARN REPR
   Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951
   Chi H., 2021, INT C LEARN REPR
   Chiaroni F, 2023, IEEE I CONF COMP VIS, P1729, DOI 10.1109/ICCV51070.2023.00166
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fini E, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9264, DOI 10.1109/ICCV48922.2021.00915
   Han K, 2022, IEEE T PATTERN ANAL, V44, P6767, DOI 10.1109/TPAMI.2021.3091944
   Han K, 2019, IEEE I CONF COMP VIS, P8400, DOI 10.1109/ICCV.2019.00849
   Hsu Y-C, 2018, ICLR
   Jia X., 2021, P IEEE CVF INT C COM, P610
   Joseph KJ, 2022, LECT NOTES COMPUT SC, V13684, P570, DOI 10.1007/978-3-031-20053-3_33
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A., 2009, Learning multiple layers of features from tiny images
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Pisner D.A., 2020, Machine Learning, P101, DOI [DOI 10.1016/B978-0-12-815739-8.00006-7, 10.1016/B978-0-12-815739-8.00006-7]
   Pu N, 2023, PROC CVPR IEEE, P7579, DOI 10.1109/CVPR52729.2023.00732
   Qi XQ, 2017, 2017 SECOND INTERNATIONAL CONFERENCE ON MECHANICAL, CONTROL AND COMPUTER ENGINEERING (ICMCCE), P151, DOI 10.1109/ICMCCE.2017.49
   Sun YL, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103728
   Tan K.C., 2019, WORKSH FIN GRAIN VIS
   Vaze S, 2022, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR52688.2022.00734
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wen X., 2023, P IEEECVF INT C COMP, P16590
   Yang ML, 2022, PROC CVPR IEEE, P14248, DOI 10.1109/CVPR52688.2022.01387
   Yu Q, 2022, AAAI CONF ARTIF INTE, P3161
   Zhang S, 2023, PROC CVPR IEEE, P3479, DOI 10.1109/CVPR52729.2023.00339
   Zhang X., 2022, Advances in Neural Information Processing Systems, V35, P27455
   Zhao B., 2023, P IEEECVF INT C COMP, P16623
   Zhao B., 2023, P IEEECVF INT C COMP, P19137
   Zhong Z, 2021, PROC CVPR IEEE, P10862, DOI 10.1109/CVPR46437.2021.01072
   Zhong Z, 2021, PROC CVPR IEEE, P9457, DOI 10.1109/CVPR46437.2021.00934
NR 34
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD JUN
PY 2024
VL 102
AR 104203
DI 10.1016/j.jvcir.2024.104203
EA JUN 2024
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WW5T6
UT WOS:001257928200001
DA 2024-08-05
ER

PT J
AU Kamyab, S
   Azimifar, Z
AF Kamyab, Shima
   Azimifar, Zohreh
TI Deep-MDS framework for recovering the 3D shape of 2D landmarks from a
   single image
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Single view 3D human face shape recovery; Deep learning human face shape
   recovery; Using multidimensional scaling for 2D shape; recovery from a
   singe image
ID MORPHABLE MODEL
AB Using 3D reconstruction techniques within computer vision frameworks can result in more robust and accurate solutions. However, the main challenge lies in the high computation and memory resources required by such methods. To reduce the complexity of these frameworks, a practical solution is to use geometric landmarks instead of the entire image. Therefore, in this paper the problem of 3D shape recovery of a set of standard 2D landmarks, on a single human face image is faced using multi -dimensional scaling (MDS) approach to find a 3D embedding for a set of standard 2D points. Hence, MDS approach is used for the first time in this study to establish an unbiased mapping from 2D landmark space to the corresponding 3D shape space. A deep neural network learns the pairwise 3D dissimilarity among standard 2D landmarks. This scheme leads to find a symmetric dissimilarity matrix, to be fed into the MDS approach to appropriately recovering the 3D shape of corresponding 2D landmarks. In the case of complex input image formations like posedness or perspective projection causing occlusion in the input image, an autoencoder component is used in the proposed framework, as an occlusion removal part, which turns different input views of the human face into a profile view. The results of performance evaluation using variety of synthetic and real -world human face datasets, including NoW dataset, Besel Face Model (BFM), CelebA, CoMA - FLAME, and CASIA-3D, indicates the superiority of the proposed framework, despite its small number of training parameters, with the related state-of-the-art and recent 3D shape recovery of landmark methods from the literature, in terms of unbiasedness, independent projection, efficiency and accuracy. Further, ablation study is performed to find the best training scheme of the deep learning components. All codes and public data of our paper are publicly available for research purposes at https://github.com/s2kamyab/DeepMDS.
C1 [Kamyab, Shima; Azimifar, Zohreh] Shiraz Univ, Comp Engn Dept, Molasadra St, Shiraz, Fars, Iran.
C3 Shiraz University
RP Kamyab, S (corresponding author), Shiraz Univ, Comp Engn Dept, Molasadra St, Shiraz, Fars, Iran.
EM sh.kamyab@cse.shirazu.ac.ir; azimifar@cse.shirazu.ac.ir
CR Aldrian O, 2013, IEEE T PATTERN ANAL, V35, P1080, DOI 10.1109/TPAMI.2012.206
   [Anonymous], 2003, Dep. Pap. (CIS)
   [Anonymous], 2007, Artificial Intelligence and Statistics
   Bandini A, 2021, IEEE J BIOMED HEALTH, V25, P1111, DOI 10.1109/JBHI.2020.3019242
   Bartl V., 2020, 2020 DIG IM COMP TEC, P1
   Bas A, 2017, LECT NOTES COMPUT SC, V10117, P377, DOI 10.1007/978-3-319-54427-4_28
   BEALS R, 1968, PSYCHOL REV, V75, P127, DOI 10.1037/h0025470
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Bocchi A, 2020, APPL NEUROPSYCH-CHIL, V9, P31, DOI 10.1080/21622965.2018.1504218
   Cai HR, 2021, GRAPH MODELS, V115, DOI 10.1016/j.gmod.2021.101103
   Chandran P, 2023, PROC CVPR IEEE, P16858, DOI 10.1109/CVPR52729.2023.01617
   Chen S, 2018, LECT NOTES COMPUT SC, V10996, P428, DOI 10.1007/978-3-319-97909-0_46
   Chen SY, 2023, IEEE T MULTIMEDIA, V25, P3166, DOI 10.1109/TMM.2022.3156820
   Chen Y, 2023, Multimedia Tools Appl., P1
   Chen YT, 2023, J KING SAUD UNIV-COM, V35, DOI 10.1016/j.jksuci.2023.101567
   Chen YT, 2023, INT J MACH LEARN CYB, V14, P2945, DOI 10.1007/s13042-023-01811-y
   Chen YT, 2023, J VIS COMMUN IMAGE R, V91, DOI 10.1016/j.jvcir.2023.103776
   Chen YT, 2024, VISUAL COMPUT, V40, P489, DOI 10.1007/s00371-023-02795-0
   Chen YY, 2022, IEEE-ASME T MECH, V27, P3186, DOI 10.1109/TMECH.2021.3110883
   Chinaev N., 2018, EUR C COMP VIS ECCV
   Choi DY, 2020, IEEE ACCESS, V8, P121549, DOI 10.1109/ACCESS.2020.3006958
   Deng ZR, 2023, VISUAL COMPUT, V39, P5547, DOI 10.1007/s00371-022-02679-9
   Engl H.W., 2014, Inverse and Ill-posed Problems, V4
   Ghojogh B., 2020, ARXIV
   Ghojogh B, 2019, Arxiv, DOI [arXiv:1905.02845, DOI 10.48550/ARXIV.1905.02845]
   Gong X, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON IDENTITY, SECURITY AND BEHAVIOR ANALYSIS (ISBA)
   GOWER JC, 1975, PSYCHOMETRIKA, V40, P33, DOI 10.1007/BF02291478
   Guo J., 2022, EUR C COMP VIS SPRIN, P350
   He ZL, 2017, IEEE COMPUT SOC CONF, P2044, DOI 10.1109/CVPRW.2017.255
   He ZL, 2017, IEEE INT CONF AUTOMA, P200, DOI 10.1109/FG.2017.33
   Hout MC, 2013, WIRES COGN SCI, V4, P93, DOI 10.1002/wcs.1203
   Hu Tao, 2021, P IEEE CVF C COMP VI, P6002
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Komal B.H., 2020, J. Crit. Rev, V7, P7765
   Kruskal JB., 1978, MULTIDIMENSIONAL SCA
   Kuang C., 2022, EUR C COMP VIS SPRIN, P1
   Lee J, 2022, MULTIMED TOOLS APPL, V81, P38217, DOI 10.1007/s11042-022-13590-9
   Li T., 2021, J. Sensors, V2021
   Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lugaresi C, 2019, Arxiv, DOI arXiv:1906.08172
   Malti A, 2021, COMPUT VIS IMAGE UND, V202, DOI 10.1016/j.cviu.2020.103072
   Naqvi SZG, 2020, J LIAQUAT UNIV MED H, V19, P238, DOI 10.22442/jlumhs.201940697
   Ngo B.-V., 2021, Eur. J. Eng. Technol. Res, V6, P130
   Nouduri K, 2020, IEEE IMAGE PROC, P1911, DOI [10.1109/icip40778.2020.9191342, 10.1109/ICIP40778.2020.9191342]
   Olivetti EC, 2020, LECT N MECH ENG, P665, DOI 10.1007/978-3-030-31154-4_56
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Ranjan A, 2018, LECT NOTES COMPUT SC, V11207, P725, DOI 10.1007/978-3-030-01219-9_43
   Rehder E, 2017, IEEE INT VEH SYM, P1694, DOI 10.1109/IVS.2017.7995952
   Revathy R, 2020, ADV INTELL SYST COMP, V1108, P402, DOI 10.1007/978-3-030-37218-7_46
   Rohr K., 2001, LANDMARK BASED IMAGE, V21
   Moniz JRA, 2018, Arxiv, DOI arXiv:1803.09202
   Sadeghzadeh A, 2020, IET COMPUT VIS, V14, P268, DOI 10.1049/iet-cvi.2019.0244
   Sanyal S, 2019, PROC CVPR IEEE, P7755, DOI 10.1109/CVPR.2019.00795
   Sharma S, 2022, ARCH COMPUT METHOD E, V29, P3475, DOI 10.1007/s11831-021-09705-4
   Sharma S, 2020, MULTIMED TOOLS APPL, V79, P17303, DOI 10.1007/s11042-020-08688-x
   Tian W, 2018, IEEE INT CONF AUTOMA, P774, DOI 10.1109/FG.2018.00122
   TORGERSON WS, 1965, PSYCHOMETRIKA, V30, P379, DOI 10.1007/BF02289530
   Wisth D, 2021, IEEE ROBOT AUTOM LET, V6, P1004, DOI 10.1109/LRA.2021.3056380
   Wood E, 2022, LECT NOTES COMPUT SC, V13673, P160, DOI 10.1007/978-3-031-19778-9_10
   Woolson R. F., 2007, International Encyclopedia of Statistical Science, P1, DOI 10.1002/0470011815. b2a15177
   Wu FZ, 2019, PATTERN RECOGN LETT, V125, P766, DOI 10.1016/j.patrec.2019.07.017
   Zhang DQ, 2020, MED IMAGE ANAL, V61, DOI 10.1016/j.media.2020.101659
   Zhang J, 2023, COMPUT AIDED DESIGN, V159, DOI 10.1016/j.cad.2023.103483
   Zhao DP, 2022, LECT NOTES COMPUT SC, V13142, P109, DOI 10.1007/978-3-030-98355-0_10
   Zhao RQ, 2018, IEEE T PATTERN ANAL, V40, P3059, DOI 10.1109/TPAMI.2017.2772922
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
   Zou X, 2021, IEEE T CIRC SYST VID, V31, P2070, DOI 10.1109/TCSVT.2020.3006236
NR 68
TC 0
Z9 0
U1 3
U2 3
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104032
DI 10.1016/j.jvcir.2023.104032
EA JAN 2024
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JZ6J4
UT WOS:001177016200001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Liu, Q
   He, ZX
   Zhang, DH
   Zhang, WS
   Lin, ZF
   Sohel, F
AF Liu, Qian
   He, Zongxin
   Zhang, Dehuan
   Zhang, Weishi
   Lin, Zifan
   Sohel, Ferdous
TI DRC: Chromatic aberration intensity priors for underwater image
   enhancement
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Aquaculture; Underwater image; Image enhancement; Depth estimation;
   Imaging model
AB Underwater imaging technology is a crucial tool for monitoring marine flora and fauna. However, selective light absorption and scattering properties of water make underwater imagery frequently appear blurred and exhibit color biases, hindering the extraction of vital aquacultural insights. To address this challenge, we propose a method, namely DRC, which is a holistic approach to enhancing underwater image clarity and color fidelity. This method comprises three integral components: D -procedure, R -procedure, and C -procedure. The D -procedure intricately accounts for the trichromatic underwater attenuation dynamics, formulating a chromatic aberration intensity prior. This prior counters the disparities in degradation levels seen in conventional single -channel prior depth estimations, achieving a dynamic depth representation approaching binocular image precision. The R -procedure, utilizing an adaptive dark -pixel prior, pinpoints corresponding points across varied depth zones to counteract backscattering, thereby mitigating the image's hazy appearance. The C -procedure bolsters image luminance and color fidelity through opponent channel rectification and amalgamates pronounced image contrasts and intricate details via Gaussian pyramid fusion. The method was tested on several publicly available datasets and compared with nine popular underwater image enhancement techniques. Both subjective and objective assessments underscore the superiority of our DRC method over existing underwater image enhancement techniques.
C1 [Liu, Qian; Zhang, Dehuan; Zhang, Weishi] Dalian Maritime Univ, Coll Informat & Sci Technol, Dalian 116026, Peoples R China.
   [He, Zongxin] Huizhou Univ, Coll Comp Sci & Technol, Huizhou 516007, Peoples R China.
   [Lin, Zifan] Univ Western Australia, Dept Elect & Elect Engn, Perth, WA 6009, Australia.
   [Sohel, Ferdous] Murdoch Univ, Sch Informat Technol, Murdoch, WA 6150, Australia.
C3 Dalian Maritime University; Huizhou University; University of Western
   Australia; Murdoch University
RP Liu, Q (corresponding author), Dalian Maritime Univ, Coll Informat & Sci Technol, Dalian 116026, Peoples R China.
EM qianliu@dlmu.edu.cn; teesiv@dlmu.edu.cn; F.Sohel@murdoch.edu.au
RI Sohel, Ferdous/C-2428-2013
OI Sohel, Ferdous/0000-0003-1557-4907; He, Zongxin/0009-0006-3265-4764;
   Lin, Zifan/0000-0002-7046-3102
FU National Natural Science Foundation of China [61702074]; Liaoning
   Provincial Natural Science Foundation of China [20170520196];
   Fundamental Research Funds for the Central Universities, China
   [3132019205, 3132019354]; Cultivation Program for the Excellent Doctoral
   Dissertation of Dalian Maritime University; National Undergraduate
   Innovation and Entrepreneurship Training Program project is the Marine
   Ranch Ecological Environment Visualization Monitoring System
   [202210577003]; High Performance Computing Center of Dalian Maritime
   University
FX This work was supported in part by the National Natural Science
   Foundation of China (No. 61702074) , the Liaoning Provincial Natural
   Science Foundation of China (No. 20170520196) , and the Fundamental
   Research Funds for the Central Universities, China (Nos. 3132019205 and
   3132019354) , the Cultivation Program for the Excellent Doctoral
   Dissertation of Dalian Maritime University, The 2022 National
   Undergraduate Innovation and Entrepreneurship Training Program project
   is the Marine Ranch Ecological Environment Visualization Monitoring
   System (project number: 202210577003) . This work was supported in part
   by the High Performance Computing Center of Dalian Maritime University.
CR Akkaynak D, 2019, PROC CVPR IEEE, P1682, DOI 10.1109/CVPR.2019.00178
   Akkaynak D, 2018, PROC CVPR IEEE, P6723, DOI 10.1109/CVPR.2018.00703
   Akkaynak D, 2017, PROC CVPR IEEE, P568, DOI 10.1109/CVPR.2017.68
   Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252
   Ancuti C, 2012, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2012.6247661
   Berman D., 2017, P BRIT MACHINE VISIO, V1, P1
   Bhoi A, 2019, Arxiv, DOI [arXiv:1901.09402, 10.48550/arXiv.1901.09402]
   Cai KW, 2022, COMPUT ELECTRON AGR, V200, DOI 10.1016/j.compag.2022.107186
   Carlevaris-Bianco N, 2010, OCEANS-IEEE
   Drews P, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P825, DOI 10.1109/ICCVW.2013.113
   Fu ZQ, 2022, LECT NOTES COMPUT SC, V13678, P465, DOI 10.1007/978-3-031-19797-0_27
   Galdran A, 2015, J VIS COMMUN IMAGE R, V26, P132, DOI 10.1016/j.jvcir.2014.11.006
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hitam Muhammad Suzuri, 2013, 2013 INT C COMPUTER
   Hou GJ, 2020, J VIS COMMUN IMAGE R, V66, DOI 10.1016/j.jvcir.2019.102732
   Huang DM, 2018, LECT NOTES COMPUT SC, V10704, P453, DOI 10.1007/978-3-319-73603-7_37
   Islam MJ, 2020, IEEE ROBOT AUTOM LET, V5, P3227, DOI 10.1109/LRA.2020.2974710
   Islam MJ, 2020, Arxiv, DOI arXiv:2002.01155
   Li CY, 2021, IEEE T IMAGE PROCESS, V30, P4985, DOI 10.1109/TIP.2021.3076367
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107038
   Li F, 2022, COMPUT ELECTRON AGR, V199, DOI 10.1016/j.compag.2022.107180
   Li H., 2019, arXiv
   Li J, 2018, IEEE ROBOT AUTOM LET, V3, P387, DOI 10.1109/LRA.2017.2730363
   Li S. Z., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P361, DOI 10.1007/BFb0028368
   Lin Y, 2021, COMPUT ELECTRON AGR, V191, DOI 10.1016/j.compag.2021.106497
   Liu JY, 2024, INT J COMPUT VISION, V132, P1748, DOI 10.1007/s11263-023-01952-1
   Lu SQ, 2023, J VIS COMMUN IMAGE R, V96, DOI 10.1016/j.jvcir.2023.103926
   Ma L, 2023, INT J COMPUT VISION, DOI 10.1007/s11263-023-01900-z
   Marques TP, 2020, IEEE COMPUT SOC CONF, P2286, DOI 10.1109/CVPRW50498.2020.00277
   Ming Y, 2021, NEUROCOMPUTING, V438, P14, DOI 10.1016/j.neucom.2020.12.089
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Peng YT, 2018, IEEE T IMAGE PROCESS, V27, P2856, DOI 10.1109/TIP.2018.2813092
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Ren WQ, 2020, INT J COMPUT VISION, V128, P240, DOI 10.1007/s11263-019-01235-8
   Saleem A, 2023, IEEE ACCESS, V11, P10412, DOI 10.1109/ACCESS.2023.3240648
   Song W, 2018, LECT NOTES COMPUT SC, V11164, P678, DOI 10.1007/978-3-030-00776-8_62
   Sun KC, 2022, J VIS COMMUN IMAGE R, V87, DOI 10.1016/j.jvcir.2022.103587
   Sutton C, 2012, FOUND TRENDS MACH LE, V4, P267, DOI 10.1561/2200000013
   Tang Y., 2022, P AS C COMP VIS, P1403
   Venkatanath N, 2015, NATL CONF COMMUN
   Wang L, 2022, J VIS COMMUN IMAGE R, V86, DOI 10.1016/j.jvcir.2022.103545
   Wang YD, 2021, SIGNAL PROCESS-IMAGE, V96, DOI 10.1016/j.image.2021.116250
   Yang HH, 2021, IEEE INT CONF ROBOT, P685, DOI 10.1109/ICRA48506.2021.9561263
   Yuan JY, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3110575
   Yuan JY, 2021, IEEE T GEOSCI REMOTE, V59, P8117, DOI 10.1109/TGRS.2020.3033407
   Zang YH, 2023, INT J COMPUT VISION, V131, P987, DOI 10.1007/s11263-022-01738-x
   Zhang KH, 2022, INT J COMPUT VISION, V130, P2103, DOI 10.1007/s11263-022-01633-5
   Zhou J., 2024, IEEE Trans. Geosci. Remote Sens., V62, P1
   Zhou JC, 2023, IEEE J OCEANIC ENG, V48, P474, DOI 10.1109/JOE.2022.3223733
   Zhou JC, 2023, INT J COMPUT VISION, DOI 10.1007/s11263-023-01853-3
   Zhou JC, 2023, IEEE J OCEANIC ENG, V48, P1322, DOI 10.1109/JOE.2023.3275615
   Zhou JC, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3293912
   Zhou JC, 2023, ENG APPL ARTIF INTEL, V121, DOI 10.1016/j.engappai.2023.105946
NR 57
TC 0
Z9 0
U1 4
U2 4
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104065
DI 10.1016/j.jvcir.2024.104065
EA JAN 2024
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JH6A6
UT WOS:001172300900001
DA 2024-08-05
ER

PT J
AU Zhang, H
   Shen, ZN
   Zheng, BL
   Chen, Q
   Yu, DG
   Chen, YR
   Yan, CG
AF Zhang, Hua
   Shen, Zhuonan
   Zheng, Bolun
   Chen, Quan
   Yu, Dingguo
   Chen, Yiru
   Yan, Chenggang
TI Learning degradation priors for reliable no-reference image quality
   assessment
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Image quality assessment; Image priors; Band-pass filters; Multi-task
   learning
AB The goal of No -Reference Image Quality Assessment (NR-IQA) is to endow computers with a human -like ability to evaluate an image's quality without comparison to a reference. Current deep learning -based methods mainly work in the spatial domain to measure the quality, heavily rely on semantic information, and less on the degradation of the image itself, struggling to accurately judge the quality of an image in similar scenes. In this paper, we propose a novel degradation priors learning architecture to address the NR-IQA task by leveraging learnable degradation priors, along with semantic features. The multi -task learning strategy is introduced to ensure our model could obtain accurate degradation priors for the NR-IQA task. Extensive experiments on public benchmarks demonstrate that our approach outperforms state-of-the-art solutions. Besides we also collect an additional dataset namely ReD-1K to illustrate the superiority of our approach to judge the image quality in similar scenes.
C1 [Zhang, Hua; Shen, Zhuonan; Zheng, Bolun; Chen, Quan; Yan, Chenggang] Hangzhou Dianzi Univ, Hangzhou 310018, Peoples R China.
   [Yu, Dingguo] Commun Univ Zhejiang, Coll Media Engn, Hangzhou 310018, Peoples R China.
   [Chen, Yiru] Kuaishou Technol, Beijing 100000, Peoples R China.
C3 Hangzhou Dianzi University; Communication University of Zhejiang
RP Chen, Q (corresponding author), Hangzhou Dianzi Univ, Hangzhou 310018, Peoples R China.
EM zhangh@hdu.edu.cn; shenzhuonan@hdu.edu.cn; blzheng@hdu.edu.cn;
   chenquan@hdu.edu.cn; yudg@cuz.edu.cn; chenquan@hdu.edu.cn;
   cgyan@hdu.edu.cn
OI Zheng, Bolun/0000-0001-8788-1725
FU National Key R&D Program of China [2020YFB1406604]; Key R&D Program of
   Zhejiang [2023C01044]; Fundamental Research Funds for the Provincial
   Universities of Zhejiang [GK239909299001-013]; National Nature Science
   Foundation of China [62371 175]
FX This work was supported by National Key R&D Program of China under Grant
   2020YFB1406604, the Key R&D Program of Zhejiang under Grant No.
   2023C01044, the Fundamental Research Funds for the Provincial
   Universities of Zhejiang under Grants No. GK239909299001-013 and the
   National Nature Science Foundation of China No. 62371 175.
CR Bhalla K, 2022, J VIS COMMUN IMAGE R, V84, DOI 10.1016/j.jvcir.2022.103485
   Bianco S, 2018, SIGNAL IMAGE VIDEO P, V12, P355, DOI 10.1007/s11760-017-1166-8
   Bosse S., 2016, IEEE Trans. Image Process
   Cao F, 2023, J VIS COMMUN IMAGE R, V94, DOI 10.1016/j.jvcir.2023.103837
   Chen J., 2021, INT C AC SPEECH SIGN
   Chen Q, 2024, Arxiv, DOI arXiv:2403.04172
   Chen Y, 2020, IEEE T CIRC SYST VID, V30, P3282, DOI 10.1109/TCSVT.2019.2931589
   Cui Y., AdaIR: Adaptive all-in-one image restoration via frequency mining and modulation
   DALY S, 1992, P SOC PHOTO-OPT INS, V1666, P2, DOI 10.1117/12.135952
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fang RG, 2017, IEEE T CIRC SYST VID, V27, P1381, DOI 10.1109/TCSVT.2016.2539658
   Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373
   Ghadiyaram D, 2017, J VISION, V17, DOI 10.1167/17.1.32
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Golestaneh SA, 2022, IEEE WINT CONF APPL, P3989, DOI 10.1109/WACV51458.2022.00404
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hosu V., KonIQ++: Boosting no-reference image quality assessment in the wild by jointly predicting image quality and defects
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Jia H., 2014, J. Image Graph.
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kim J, 2017, IEEE J-STSP, V11, P206, DOI 10.1109/JSTSP.2016.2639328
   Kingma D. P., 2014, arXiv
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lin HH, 2019, INT WORK QUAL MULTIM
   Liu JZ, 2023, IEEE T MULTIMEDIA, V25, P5358, DOI 10.1109/TMM.2022.3190700
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   Ma KD, 2016, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2016.184
   Ma Y, 2018, DIGIT SIGNAL PROCESS, V80, P37, DOI 10.1016/j.dsp.2018.05.010
   Madhusudana PC, 2022, IEEE T IMAGE PROCESS, V31, P4149, DOI 10.1109/TIP.2022.3181496
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Mittal A, 2011, CONF REC ASILOMAR C, P723, DOI 10.1109/ACSSC.2011.6190099
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Sun SM, 2023, IEEE T MULTIMEDIA, V25, P2912, DOI 10.1109/TMM.2022.3152942
   Wang HQ, 2017, J VIS COMMUN IMAGE R, V46, P292, DOI 10.1016/j.jvcir.2017.04.009
   Wang XQ, 2023, IEEE T MULTIMEDIA, V25, P8958, DOI 10.1109/TMM.2023.3243683
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu W, 2024, J VIS COMMUN IMAGE R, V98, DOI 10.1016/j.jvcir.2023.104030
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Yan J, 2017, LECT NOTES COMPUT SC, V10114, P3, DOI 10.1007/978-3-319-54190-7_1
   Ye P., 2012, Computer Vision and Pattern Recognition
   Ye P, 2012, IEEE T IMAGE PROCESS, V21, P3129, DOI 10.1109/TIP.2012.2190086
   Ying ZQ, 2020, PROC CVPR IEEE, P3572, DOI 10.1109/CVPR42600.2020.00363
   You JY, 2021, IEEE IMAGE PROC, P1389, DOI 10.1109/ICIP42928.2021.9506075
   Zeng H., 2017, arXiv
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang WX, 2023, PROC CVPR IEEE, P14071, DOI 10.1109/CVPR52729.2023.01352
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zheng B., 2024, P AAAI C ART INT, V38, P7552
   Zheng BL, 2022, IEEE T PATTERN ANAL, V44, P7705, DOI 10.1109/TPAMI.2021.3115139
   Zheng BL, 2020, PROC CVPR IEEE, P3633, DOI 10.1109/CVPR42600.2020.00369
   Zhou M., Qin, An end-to-end blind image quality assessment method using a recurrent network and self-attention
   Zhou W, 2022, IEEE T CIRC SYST VID, V32, P1778, DOI 10.1109/TCSVT.2021.3081182
   Zhu H., 2020, P IEEECVF C COMPUTER, P14143
NR 60
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD JUN
PY 2024
VL 102
AR 104189
DI 10.1016/j.jvcir.2024.104189
EA JUN 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UX1D5
UT WOS:001251263200001
DA 2024-08-05
ER

PT J
AU Ma, LP
   Hong, DY
   Yin, SB
   Deng, WQ
   Yang, Y
   Yang, YH
AF Ma, Lunpeng
   Hong, Dongyang
   Yin, Shibai
   Deng, Wanqiu
   Yang, Yang
   Yang, Yee-Hong
TI Convolution-transformer blend pyramid network for underwater image
   enhancement ☆
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Image enhancement; Laplacian pyramid; Convolutional neural network;
   Transformer; Wavelet transform
ID FRAMEWORK
AB Underwater images suffer from different types of degradation, where color degradation occurs in the spatial domain and edge degradation in the frequency domain. The high -quality underwater image enhancement represents a crucial milestone in advancing computer vision systems tailored for marine environments. This foundational endeavor encompasses a wide array of applications in computer vision tasks, including underwater inspection, underwater archaeology, and environmental monitoring. However, current convolutional neural network (CNN) -based pyramid frameworks primarily focus on capturing local features, often overlooking the significance of global semantic features that play a crucial role in understanding underwater scenes. Moreover, these frameworks handle spatial and frequency features independently, failing to enhance images by exploring correlation among domain -specific attributes for enabling information consistency. Besides, optimizing the model using a loss function with the same domain attributes from the ground truth may not lead to a better generalization ability. To solve these problems, we propose a new Convolution -Transformer Blend Pyramid Network (CTPN), which consists of a spatial branch and several frequency branches. The CTPN has four key components: a Swin transformer encoder, a CNN -Transformer aggregated encoder-decoder (CTED) and a blend pyramid framework. The Swin transformer encoder is employed to capture global semantic features, benefiting from its ability to extract long-range and global dependencies among features. The CTED fuses local features captured by CNN layers and global semantic features captured by the Swin transformer encoder in the spatial branch, with the help of the Cross -Model Fusion Module (CFM) and Skip -Aggregation Module (SAM). Subsequently, a blend pyramid framework is designed which not only progressively expands the transformed information of the previous domain branch to the current domain branch via the CTED-based refining operation, but also utilizes the proposed Domain Affinity Block (DAB) to explore the connection between domain attributes, ensuring information consistency. The experimental results demonstrate that the proposed method outperforms existing underwater image enhancement methods quantitatively and qualitatively.
C1 [Ma, Lunpeng] Commun Univ Zhejiang, Sch Televis Arts, Hangzhou 310018, Zhejiang, Peoples R China.
   [Hong, Dongyang; Yin, Shibai; Deng, Wanqiu; Yang, Yang] Southwestern Univ Finance & Econ, Dept Comp & Artificial Intelligence, Chengdu 611130, Sichuan, Peoples R China.
   [Yang, Yee-Hong] Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2E8, Canada.
C3 Communication University of Zhejiang; Southwestern University of Finance
   & Economics - China; University of Alberta
RP Yin, SB (corresponding author), Southwestern Univ Finance & Econ, Dept Comp & Artificial Intelligence, Chengdu 611130, Sichuan, Peoples R China.
EM shibaiyin@swufe.edu.cn
OI Yang, Yee Hong/0000-0002-7194-3327
FU National Natural Science Foundation of China [61502396]; Fundamental
   Research Funds for the Central Universities [JBK2202047]; Humanities and
   Social Science Foundation of the Ministry of Education in China
   [2020070022]; Natural Sciences and Engineering Research Council of
   Canada; Financial Intelligence and Financial Engineering Key Laboratory
   of Sichuan Province, School of Economic Information Engineering
FX This work is supported by the National Natural Science Foundation of
   China (No. 61502396) , the Fundamental Research Funds for the Central
   Universities (No. JBK2202047) , Humanities and Social Science Foundation
   of the Ministry of Education in China (No. 2020070022) and the Natural
   Sciences and Engineering Research Council of Canada. In addition, this
   work is also supported by the Financial Intelligence and Financial
   Engineering Key Laboratory of Sichuan Province, School of Economic
   Information Engineering.
CR Aguirre-Castro OA, 2022, NEUROCOMPUTING, V494, P148, DOI 10.1016/j.neucom.2022.04.074
   Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252
   Anwar S, 2018, Arxiv, DOI arXiv:1807.03528
   Badran M, 2023, IEEE IMAGE PROC, P1830, DOI 10.1109/ICIP49359.2023.10222895
   Berman D, 2021, IEEE T PATTERN ANAL, V43, P2822, DOI 10.1109/TPAMI.2020.2977624
   Cheng QM, 2022, IEEE-CAA J AUTOMATIC, V9, P1532, DOI 10.1109/JAS.2022.105773
   Drews PLJ, 2016, IEEE COMPUT GRAPH, V36, P24, DOI 10.1109/MCG.2016.26
   Duarte A, 2016, OCEANS-IEEE
   Fabbri C, 2018, IEEE INT CONF ROBOT, P7159
   Fu XY, 2020, SIGNAL PROCESS-IMAGE, V86, DOI 10.1016/j.image.2020.115892
   Fu ZQ, 2022, LECT NOTES COMPUT SC, V13678, P465, DOI 10.1007/978-3-031-19797-0_27
   Galdran A, 2015, J VIS COMMUN IMAGE R, V26, P132, DOI 10.1016/j.jvcir.2014.11.006
   Hu K, 2023, ENG APPL ARTIF INTEL, V123, DOI 10.1016/j.engappai.2023.106196
   Huang ZX, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3189630
   Islam MJ, 2020, IEEE ROBOT AUTOM LET, V5, P3227, DOI 10.1109/LRA.2020.2974710
   JAFFE JS, 1990, IEEE J OCEANIC ENG, V15, P101, DOI 10.1109/48.50695
   Islam MJ, 2020, Arxiv, DOI arXiv:2002.01155
   Ji W, 2023, COMPUT ELECTRON AGR, V204, DOI 10.1016/j.compag.2022.107522
   Kang YZ, 2023, IEEE T CIRC SYST VID, V33, P988, DOI 10.1109/TCSVT.2022.3208100
   Lei B., 2022, P IEEE CVF C COMP VI, P2108
   Li CL, 2018, 2018 11TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2018)
   Li CY, 2016, IEEE T IMAGE PROCESS, V26, P5664, DOI 10.1109/TIP.2016.2612882
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107038
   Li H., 2019, arXiv
   Li K., 2022, IEEE Trans. Circuits Syst. Video Technol., P1
   Li XJ, 2020, IEEE ACCESS, V8, P197448, DOI 10.1109/ACCESS.2020.3034275
   Liang J, 2021, PROC CVPR IEEE, P9387, DOI 10.1109/CVPR46437.2021.00927
   Liu Q., 2024, J. Vis. Commun. Image Represent.
   Liu SB, 2022, IEEE ROBOT AUTOM LET, V7, P5326, DOI 10.1109/LRA.2022.3156176
   Liu Z, 2022, PROC CVPR IEEE, P3192, DOI 10.1109/CVPR52688.2022.00320
   Ma ZY, 2022, INT CONF ACOUST SPEE, P2769, DOI 10.1109/ICASSP43922.2022.9747781
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Peng LT, 2023, IEEE T IMAGE PROCESS, V32, P3066, DOI 10.1109/TIP.2023.3276332
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Raihan AJ, 2021, J ADV INFORM TECHNOL, V12, P334, DOI 10.12720/jait.12.4.334-341
   Rao BS, 2020, APPL SOFT COMPUT, V89, DOI 10.1016/j.asoc.2020.106114
   Ren TD, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3205061
   Shen Z, 2023, COMPUT GRAPH-UK, V111, P77, DOI 10.1016/j.cag.2023.01.009
   Song HJ, 2022, IEEE T PATTERN ANAL, V44, P6953, DOI 10.1109/TPAMI.2021.3097804
   Tanchenko A, 2014, J VIS COMMUN IMAGE R, V25, P874, DOI 10.1016/j.jvcir.2014.01.008
   Tang Yi, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P5419, DOI 10.1145/3581783.3612378
   Tang Y., 2022, P AS C COMP VIS, P1403
   Ummar M, 2023, ENG APPL ARTIF INTEL, V126, DOI 10.1016/j.engappai.2023.107069
   Uplavikar Pritish M., 2019, CVPR WORKSH, P1
   Wang YD, 2021, SIGNAL PROCESS-IMAGE, V96, DOI 10.1016/j.image.2021.116250
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Xie J, 2022, IEEE T CIRC SYST VID, V32, P3514, DOI 10.1109/TCSVT.2021.3115791
   Xue XW, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.109041
   Zhou Z., 2022, IEEE Trans. Instrum. Meas.
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 54
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104163
DI 10.1016/j.jvcir.2024.104163
EA MAY 2024
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TT8U6
UT WOS:001243609000001
DA 2024-08-05
ER

PT J
AU Zhang, PC
   Zheng, PX
   Guo, X
   Chen, EQ
AF Zhang, Pengcheng
   Zheng, Peixiao
   Guo, Xin
   Chen, Enqing
TI Few-shot defect classification via feature aggregation based on graph
   neural network
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Few-shot learning; Graph neural networks(GNNs); Distribution learning;
   Surface defect classification
AB The effectiveness of deep learning models is greatly dependent on the availability of a vast amount of labeled data. However, in the realm of surface defect classification, acquiring and annotating defect samples proves to be quite challenging. Consequently, accurately predicting defect types with only a limited number of labeled samples has emerged as a prominent research focus in recent years. Few -shot learning, which leverages a restricted sample set in the support set, can effectively predict the categories of unlabeled samples in the query set. This approach is particularly well -suited for defect classification scenarios. In this article, we propose a transductive few -shot surface defect classification method, which using both the instance -level relations and distribution -level relations in each few -shot learning task. Furthermore, we calculate class center features in transductive manner and incorporate them into the feature aggregation operation to rectify the positioning of edge samples in the mapping space. This adjustment aims to minimize the distance between samples of the same category, thereby mitigating the influence of unlabeled samples at category boundary on classification accuracy. Experimental results on the public dataset show the outstanding performance of our proposed approach compared to the state-of-the-art methods in the few -shot learning settings. Our code is available at https://github.com/Harry10459/CIDnet.
C1 [Zhang, Pengcheng; Zheng, Peixiao; Guo, Xin; Chen, Enqing] Zhengzhou Univ, Sch Elect & Informat Engn, 100 Sci Ave, Zhengzhou 450001, Peoples R China.
C3 Zhengzhou University
RP Guo, X (corresponding author), Zhengzhou Univ, Sch Elect & Informat Engn, 100 Sci Ave, Zhengzhou 450001, Peoples R China.
EM iexguo@zzu.edu.cn
FU National Natural Science Foundation of China [62101503]; Henan
   Provincial Key Science and Technology Research Projects [242102211017]
FX This research was supported by the National Natural Science Foundation
   of China under Grant No. 62101503 and the Henan Provincial Key Science
   and Technology Research Projects under Grant No. 242102211017.
CR [Anonymous], 2018, Guangdong industrial intelligent manufacturing innovation competition: Recognition of surface defects of aluminum profiles
   Bajpai S, 2023, WIRELESS PERS COMMUN, V131, P805, DOI 10.1007/s11277-023-10455-8
   Bajpai S, 2022, MULTIMED TOOLS APPL, V81, P33205, DOI 10.1007/s11042-022-13057-x
   Chang YH, 2023, MECH SYST SIGNAL PR, V199, DOI 10.1016/j.ymssp.2023.110462
   Chen CF, 2021, PROC CVPR IEEE, P6592, DOI 10.1109/CVPR46437.2021.00653
   Chen D, 2022, LECT NOTES COMPUT SC, V13677, P322, DOI 10.1007/978-3-031-19790-1_20
   Chen TJ, 2016, IEEE T INSTRUM MEAS, V65, P2055, DOI 10.1109/TIM.2016.2566442
   Chen YB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9042, DOI 10.1109/ICCV48922.2021.00893
   Chen Y, 2022, KNOWL-BASED SYST, V247, DOI 10.1016/j.knosys.2022.108623
   Chetverikov D, 2002, PATTERN RECOGN, V35, P2165, DOI 10.1016/S0031-3203(01)00188-1
   Chobola T, 2021, PR MACH LEARN RES, V140, P29
   Dhillon G., 2020, ICLR, P1
   Ding KZ, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P2448, DOI 10.1145/3442381.3449922
   Dong HW, 2022, IEEE T IND INFORM, V18, P1801, DOI 10.1109/TII.2021.3090036
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu YQ, 2021, LECT NOTES COMPUT SC, V12892, P487, DOI 10.1007/978-3-030-86340-1_39
   Hu YQ, 2021, INT C PATT RECOG, P8164, DOI 10.1109/ICPR48806.2021.9412076
   Jinlu Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P741, DOI 10.1007/978-3-030-58452-8_43
   Kadam Suvarna, 2020, Intelligent Systems Design and Applications. Proceedings of 18th International Conference on Intelligent Systems Design and Applications (ISDA 2018). Advances in Intelligent Systems and Computing (AISC 940), P100, DOI 10.1007/978-3-030-16657-1_10
   Kang D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8802, DOI 10.1109/ICCV48922.2021.00870
   Kim J, 2019, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2019.00010
   Krizhevsky A, 2009, CIFAR-10 dataset
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lazarou M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8731, DOI 10.1109/ICCV48922.2021.00863
   Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091
   Lin DY, 2021, Arxiv, DOI arXiv:2007.09438
   Ling Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13387, DOI 10.1109/CVPR42600.2020.01340
   Liu Y., 2018, arXiv
   Liu YB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8433, DOI 10.1109/ICCV48922.2021.00834
   Liu Y, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11111752
   Liu Y, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3165287
   O' Mahony N, 2019, PROCEDIA MANUF, V38, P186, DOI 10.1016/j.promfg.2020.01.025
   Ravi S., 2017, INT C LEARN REPR
   Rodriguez Pau, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P121, DOI 10.1007/978-3-030-58574-7_8
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Snell J, 2017, ADV NEUR IN, V30
   Song KC, 2013, APPL SURF SCI, V285, P858, DOI 10.1016/j.apsusc.2013.09.002
   Song XB, 2020, PROC CVPR IEEE, P5630, DOI 10.1109/CVPR42600.2020.00567
   Vinyals O., 2016, Advances in neural information processing systems, V29
   Wang XL, 2018, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR.2018.00717
   Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760
   Wieler M, 2007, Weakly supervised learning for industrial optical inspection
   Xiao WW, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3169547
   Xie JT, 2022, PROC CVPR IEEE, P7962, DOI 10.1109/CVPR52688.2022.00781
   Xu R, 2023, ENG APPL ARTIF INTEL, V124, DOI 10.1016/j.engappai.2023.106640
   Yiwei Lu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P125, DOI 10.1007/978-3-030-58558-7_8
   Zaremba W, 2015, Arxiv, DOI [arXiv:1409.2329, DOI 10.48550/ARXIV.1409.2329]
   Zhang DK, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3038008
   Zhao WL, 2023, MEASUREMENT, V208, DOI 10.1016/j.measurement.2023.112446
   Zhong X, 2023, IEEE T MULTIMEDIA, V25, P1979, DOI 10.1109/TMM.2022.3141886
   Zhu H, 2023, PROC CVPR IEEE, P23996, DOI 10.1109/CVPR52729.2023.02298
   Zhu H, 2022, PROC CVPR IEEE, P9068, DOI 10.1109/CVPR52688.2022.00887
NR 52
TC 1
Z9 1
U1 3
U2 3
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104172
DI 10.1016/j.jvcir.2024.104172
EA MAY 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TQ1E9
UT WOS:001242626000001
DA 2024-08-05
ER

PT J
AU Yang, KX
   Xiang, W
   Chen, ZS
   Zhang, J
   Liu, YP
AF Yang, Kaixuan
   Xiang, Wei
   Chen, Zhenshuai
   Zhang, Jian
   Liu, Yunpeng
TI A review on infrared and visible image fusion algorithms based on neural
   networks
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Review
DE Image fusion; Neural network; Infrared and visible images; Image
   processing
ID GENERATIVE ADVERSARIAL NETWORK; INFORMATION; NEST
AB Infrared and visible image fusion represents a significant segment within the image fusion domain. The recent surge in image processing hardware advancements, including GPUs, TPUs, and cloud computing platforms, has facilitated the fusion of extensive datasets from multiple sensors. Given the remarkable proficiency of neural networks in image feature extraction and fusion, their application in infrared and visible image fusion has emerged as a prominent research area in recent years. This article begins by providing an overview of the current mainstream algorithms for infrared and visible image fusion based on neural networks, detailing the principles of various image fusion algorithms, their representative works, and their respective advantages and disadvantages. Subsequently, it introduces domain -relevant datasets, evaluation metrics, and some typical application scenarios. Finally, the article conducts qualitative and quantitative evaluations of the fusion results of various state-of-the-art algorithms and offers future research prospects based on experimental results.
C1 [Yang, Kaixuan; Xiang, Wei; Chen, Zhenshuai; Zhang, Jian; Liu, Yunpeng] Chinese Acad Sci, Key Lab Optoelect Informat Proc, Shenyang 110016, Peoples R China.
   [Yang, Kaixuan; Xiang, Wei; Chen, Zhenshuai; Zhang, Jian; Liu, Yunpeng] Chinese Acad Sci, Shenyang Inst Automat, Shenyang 110016, Peoples R China.
   [Yang, Kaixuan; Chen, Zhenshuai; Zhang, Jian] Chinese Acad Sci, Inst Robot & Intelligent Mfg, Shenyang 110169, Peoples R China.
   [Yang, Kaixuan; Chen, Zhenshuai; Zhang, Jian] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Chinese Academy of Sciences; Shenyang
   Institute of Automation, CAS; Chinese Academy of Sciences; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Liu, YP (corresponding author), Chinese Acad Sci, Shenyang Inst Automat, Shenyang 110016, Peoples R China.
EM ypliu@sia.cn
OI yang, kaixuan/0009-0002-5315-9894
CR Alanazi T, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app13126916
   Ali MAS, 2023, PLOS ONE, V18, DOI 10.1371/journal.pone.0287349
   An Xiaodong, 2022, Computer Engineering and Applications, P64, DOI 10.3778/j.issn.1002-8331.2203-0600
   Arjovsky M., 2017, arXiv, DOI DOI 10.48550/ARXIV.1701.04862
   Arora S, 2017, PR MACH LEARN RES, V70
   Aslantas V, 2015, AEU-INT J ELECTRON C, V69, P160, DOI 10.1016/j.aeue.2015.09.004
   Bino T., 2023, Modality correlation aware sparse representation and kiou tracker for rgb-infrared videos
   Broussard RP, 1996, P SOC PHOTO-OPT INS, V2760, P372, DOI 10.1117/12.235981
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen X, 2023, PROC CVPR IEEE, P14572, DOI 10.1109/CVPR52729.2023.01400
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Cheng CY, 2023, INFORM FUSION, V92, P80, DOI 10.1016/j.inffus.2022.11.010
   Cho S, 2023, COMPUT ELECTRON AGR, V207, DOI 10.1016/j.compag.2023.107703
   Chu P, 2023, IEEE WINT CONF APPL, P4859, DOI 10.1109/WACV56688.2023.00485
   Ciprián-Sánchez JF, 2023, NEURAL COMPUT APPL, V35, P18201, DOI 10.1007/s00521-021-06691-3
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Ding SF, 2018, IET COMPUT VIS, V12, P377, DOI 10.1049/iet-cvi.2017.0285
   Dong LL, 2024, OPT LASER ENG, V172, DOI 10.1016/j.optlaseng.2023.107821
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Enlong W., J. Front. Comput. Sci. Technol., V1
   Eskicioglu AM, 1995, IEEE T COMMUN, V43, P2959, DOI 10.1109/26.477498
   Feng MZ, 2020, J VIS COMMUN IMAGE R, V72, DOI 10.1016/j.jvcir.2020.102881
   Fu Y, 2022, Arxiv, DOI arXiv:2107.13967
   Fu Y, 2021, INT C PATT RECOG, P10675, DOI 10.1109/ICPR48806.2021.9412293
   Fu Y, 2021, INFORM FUSION, V72, P110, DOI 10.1016/j.inffus.2021.02.019
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gu YS, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23020239
   Gui J, 2023, IEEE T KNOWL DATA EN, V35, P3313, DOI 10.1109/TKDE.2021.3130191
   Guo CX, 2023, EXPERT SYST APPL, V211, DOI 10.1016/j.eswa.2022.118631
   Haibo Zhao, 2021, 2021 International Conference on Information Technology and Biomedical Engineering (ICITBE), P71, DOI 10.1109/ICITBE54178.2021.00025
   Han MA, 2023, INFORM FUSION, V92, P268, DOI 10.1016/j.inffus.2022.12.005
   Han Y, 2013, INFORM FUSION, V14, P127, DOI 10.1016/j.inffus.2011.08.002
   Hu RH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1419, DOI 10.1109/ICCV48922.2021.00147
   Huang YP, 2023, INFRARED PHYS TECHN, V128, DOI 10.1016/j.infrared.2022.104509
   Huang ZB, 2022, LECT NOTES COMPUT SC, V13678, P539, DOI 10.1007/978-3-031-19797-0_31
   Iranmanesh SM, 2020, IMAGE VISION COMPUT, V94, DOI 10.1016/j.imavis.2019.103861
   Jian LH, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3022438
   JOHNSON JL, 1994, APPL OPTICS, V33, P6239, DOI 10.1364/AO.33.006239
   Kang B, 2023, IEEE T NEUR NET LEAR, V34, P9900, DOI 10.1109/TNNLS.2022.3161969
   Karim S, 2023, INFORM FUSION, V90, P185, DOI 10.1016/j.inffus.2022.09.019
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Kong WW, 2014, INFRARED PHYS TECHN, V65, P103, DOI 10.1016/j.infrared.2014.04.003
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Lan XY, 2020, PATTERN RECOGN LETT, V130, P12, DOI 10.1016/j.patrec.2018.10.002
   Li CL, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106977
   Li GF, 2023, IEEE T INTELL TRANSP, V24, P10118, DOI 10.1109/TITS.2023.3268063
   Li HF, 2023, INFORM FUSION, V95, P26, DOI 10.1016/j.inffus.2023.02.011
   Li HF, 2021, IEEE T IMAGE PROCESS, V30, P4070, DOI 10.1109/TIP.2021.3069339
   Li H, 2023, IEEE T PATTERN ANAL, V45, P11040, DOI 10.1109/TPAMI.2023.3268209
   Li H, 2021, INFORM FUSION, V73, P72, DOI 10.1016/j.inffus.2021.02.023
   Li H, 2020, IEEE T INSTRUM MEAS, V69, P9645, DOI 10.1109/TIM.2020.3005230
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li J, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3029360
   Li J, 2021, IEEE T MULTIMEDIA, V23, P1383, DOI 10.1109/TMM.2020.2997127
   Li JP, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3164136
   Li MY, 2023, IEEE SENS J, V23, P19692, DOI 10.1109/JSEN.2023.3295473
   Li QQ, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3186048
   Li YQ, 2020, INFORM FUSION, V58, P1, DOI 10.1016/j.inffus.2019.12.014
   Li Z., 2021, 2021 IEEE INT C MULT, P1
   Liang PW, 2022, LECT NOTES COMPUT SC, V13678, P719, DOI 10.1007/978-3-031-19797-0_41
   Liao B, 2020, IEEE ACCESS, V8, P79754, DOI 10.1109/ACCESS.2020.2990539
   Lin Liting, 2022, ADV NEUR IN
   Lindblad T., 2005, IMAGE PROCESSING USI
   Liu HP, 2008, IEEE INT CONF ROBOT, P2259, DOI 10.1109/ROBOT.2008.4543550
   Liu JY, 2023, INFORM FUSION, V91, P205, DOI 10.1016/j.inffus.2022.09.030
   Liu JY, 2023, IEEE I CONF COMP VIS, P8081, DOI 10.1109/ICCV51070.2023.00745
   Liu JY, 2022, PROC CVPR IEEE, P5792, DOI 10.1109/CVPR52688.2022.00571
   Liu JY, 2022, IEEE SIGNAL PROC LET, V29, P1614, DOI 10.1109/LSP.2022.3180672
   Liu JY, 2022, IEEE T CIRC SYST VID, V32, P105, DOI 10.1109/TCSVT.2021.3056725
   Liu Y, 2018, INT J WAVELETS MULTI, V16, DOI 10.1142/S0219691318500182
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long YZ, 2021, INFORM FUSION, V69, P128, DOI 10.1016/j.inffus.2020.11.009
   Luo YY, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app131910891
   Ma JY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3038013
   Ma JY, 2022, IEEE-CAA J AUTOMATIC, V9, P1200, DOI 10.1109/JAS.2022.105686
   Ma JY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3075747
   Ma JY, 2020, IEEE T IMAGE PROCESS, V29, P4980, DOI 10.1109/TIP.2020.2977573
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma WH, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23020599
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Özer S, 2022, PATTERN RECOGN, V129, DOI 10.1016/j.patcog.2022.108712
   Pandit V.R., 2015, IJCA, V120, P22, DOI [DOI 10.5120/21263-3846, 10.5120/21263-3846]
   Panigrahy C, 2022, NEUROCOMPUTING, V514, P21, DOI 10.1016/j.neucom.2022.09.157
   Piella G, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P173
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Rao YJ, 2023, INFORM FUSION, V92, P336, DOI 10.1016/j.inffus.2022.12.007
   Rashid Al-Bakri I.M., 2023, 2023 9 INT C ADV COM, V1, P1951, DOI [10.1109/ICACCS57279.2023.10112907, DOI 10.1109/ICACCS57279.2023.10112907]
   Ren L, 2021, INFRARED PHYS TECHN, V117, DOI 10.1016/j.infrared.2021.103839
   Roberts JW, 2008, J APPL REMOTE SENS, V2, DOI 10.1117/1.2945910
   Salimans T, 2016, ADV NEUR IN, V29
   Shen LF, 2022, NEUROCOMPUTING, V492, P370, DOI 10.1016/j.neucom.2022.04.032
   Singh S, 2023, DIGIT SIGNAL PROCESS, V137, DOI 10.1016/j.dsp.2023.104020
   Tan W, 2021, PROC SPIE, V11353, DOI 10.1117/12.2551830
   Tang LF, 2023, INFORM FUSION, V91, P477, DOI 10.1016/j.inffus.2022.10.034
   Tang LF, 2022, INFORM FUSION, V83, P79, DOI 10.1016/j.inffus.2022.03.007
   Tang LF, 2022, INFORM FUSION, V82, P28, DOI 10.1016/j.inffus.2021.12.004
   Tang Q, 2023, SIGNAL PROCESS, V213, DOI 10.1016/j.sigpro.2023.109165
   Tang W, 2023, IEEE T CIRC SYST VID, V33, P3159, DOI 10.1109/TCSVT.2023.3234340
   Tang W, 2023, IEEE T MULTIMEDIA, V25, P5413, DOI 10.1109/TMM.2022.3192661
   Tang ZY, 2023, INFORM FUSION, V99, DOI 10.1016/j.inffus.2023.101881
   Tang ZM, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3259021
   Terhörst P, 2023, IEEE WINT CONF APPL, P3473, DOI 10.1109/WACV56688.2023.00348
   Toet A, 2017, DATA BRIEF, V15, P249, DOI 10.1016/j.dib.2017.09.038
   Tooranloo HS, 2021, INT J HEALTHCARE MAN, V14, P676, DOI 10.1080/20479700.2019.1688504
   Vaswani A., 2017, Adv. Neural. Inf. Proc. Syst., V30
   Vibashan VS, 2022, IEEE IMAGE PROC, P3566, DOI 10.1109/ICIP46576.2022.9897280
   Wang CAY, 2021, IEEE INT CONF TRUST, P832, DOI 10.1109/TrustCom53373.2021.00118
   Wang D, 2023, INFORM FUSION, V98, DOI 10.1016/j.inffus.2023.101828
   Wang D, 2022, Arxiv, DOI [arXiv:2205.11876, DOI 10.48550/ARXIV.2205.11876]
   Wang FT, 2023, APPL INTELL, V53, P24709, DOI 10.1007/s10489-023-04741-y
   Wang KF, 2017, IEEE-CAA J AUTOMATIC, V4, P588, DOI 10.1109/JAS.2017.7510583
   Wang SY, 2023, IEEE T INSTRUM MEAS, V72, DOI [10.1109/TIM.2023.3300458, 10.1109/TIM.2023.3240227]
   Wang X, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3216413
   Wang XJ, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3216399
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wang ZS, 2023, IEEE T CIRC SYST VID, V33, P3677, DOI 10.1109/TCSVT.2023.3239627
   Wang ZS, 2022, IEEE T CIRC SYST VID, V32, P3360, DOI 10.1109/TCSVT.2021.3109895
   Wang ZS, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2021.3139654
   Wu ZH, 2020, Arxiv, DOI arXiv:2004.11886
   Xi C, 2010, INT CONF SIGN PROCES, P845, DOI 10.1109/ICOSP.2010.5655945
   Xiao WX, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3149101
   Xie RL, 2023, MEAS SCI TECHNOL, V34, DOI 10.1088/1361-6501/acacb8
   Xie YN, 2023, J MOD OPTIC, V70, P52, DOI 10.1080/09500340.2023.2174358
   Xing ML, 2023, OPT LASER ENG, V171, DOI 10.1016/j.optlaseng.2023.107804
   Xu H, 2023, IEEE T PATTERN ANAL, V45, P12148, DOI 10.1109/TPAMI.2023.3283682
   Xu H, 2022, PROC CVPR IEEE, P19647, DOI 10.1109/CVPR52688.2022.01906
   Xu H, 2022, COMPUT VIS IMAGE UND, V218, DOI 10.1016/j.cviu.2022.103407
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Xu H, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3954
   Yang Y, 2021, IEEE T CIRC SYST VID, V31, P4771, DOI 10.1109/TCSVT.2021.3054584
   Yang Z, 2019, ARCH COMPUT METHOD E, V26, P491, DOI 10.1007/s11831-018-9253-8
   Yao YX, 2022, IEEE T IMAGE PROCESS, V31, P2584, DOI 10.1109/TIP.2022.3157450
   Ye YX, 2022, ISPRS J PHOTOGRAMM, V188, P331, DOI 10.1016/j.isprsjprs.2022.04.011
   Yi S, 2022, INFRARED PHYS TECHN, V127, DOI 10.1016/j.infrared.2022.104405
   Yin HT, 2022, IEEE SIGNAL PROC LET, V29, P1988, DOI 10.1109/LSP.2022.3207621
   Yin WX, 2023, VISUAL COMPUT, V39, P6723, DOI 10.1007/s00371-022-02759-w
   Yuan D, 2024, IEEE T INSTRUM MEAS, V73, DOI 10.1109/TIM.2023.3338701
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang H, 2021, IEEE T COMPUT IMAG, V7, P1134, DOI 10.1109/TCI.2021.3119954
   Zhang H, 2021, INFORM FUSION, V76, P323, DOI 10.1016/j.inffus.2021.06.008
   Zhang H, 2021, INT J COMPUT VISION, V129, P2761, DOI 10.1007/s11263-021-01501-8
   Zhang H, 2020, AAAI CONF ARTIF INTE, V34, P12797
   Zhang H, 2023, SIGNAL IMAGE VIDEO P, V17, P791, DOI 10.1007/s11760-022-02289-x
   Zhang JM, 2023, INFRARED PHYS TECHN, V131, DOI 10.1016/j.infrared.2023.104629
   Zhang J, 2023, IEEE T MULTIMEDIA, V25, P8988, DOI 10.1109/TMM.2023.3243659
   Zhang X, 2021, PROC SPIE, V12065, DOI 10.1117/12.2606088
   Zhang XC, 2020, INFORM FUSION, V63, P166, DOI 10.1016/j.inffus.2020.05.002
   Zhang Y, 2023, PROC CVPR IEEE, P22056, DOI 10.1109/CVPR52729.2023.02112
   Zhao HJ, 2023, PROC CVPR IEEE, P18696, DOI 10.1109/CVPR52729.2023.01793
   Zhao WD, 2023, PROC CVPR IEEE, P13955, DOI 10.1109/CVPR52729.2023.01341
   Zhao ZX, 2023, PROC CVPR IEEE, P5906, DOI 10.1109/CVPR52729.2023.00572
   Zhao ZX, 2023, Arxiv, DOI arXiv:2303.06840
   Zhao ZX, 2021, Arxiv, DOI arXiv:2003.09210
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zheng X, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.109009
   Zhou HB, 2022, INFORM FUSION, V88, P184, DOI 10.1016/j.inffus.2022.07.016
   Zhou HB, 2023, IEEE T MULTIMEDIA, V25, P635, DOI 10.1109/TMM.2021.3129609
   Zhou XL, 2023, SIGNAL PROCESS-IMAGE, V115, DOI 10.1016/j.image.2023.116956
   Zhu ZJ, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3203000
NR 162
TC 1
Z9 1
U1 12
U2 12
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104179
DI 10.1016/j.jvcir.2024.104179
EA MAY 2024
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TN5O1
UT WOS:001241957100001
OA hybrid
DA 2024-08-05
ER

PT J
AU Raikwar, S
   Tapaswi, S
   Sharma, RK
AF Raikwar, Suresh
   Tapaswi, Shashikala
   Sharma, Rajendra Kumar
TI Structure based transmission estimation in single image dehazing
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Dark channel prior; Dehazing; Fog; Generative adversarial network; Haze;
   Structural similarity
ID FRAMEWORK; ARCHITECTURE; VISIBILITY; NETWORK; WEATHER
AB The single image dehazing (SID) is challenging because of ill-posed characteristic, since there exist multiple possible dehazed images for a single hazy image due to multiple possible values of transmission. The SID is solved using scattering model, which requires computation of two parameters (transmission and atmospheric light) for its solution. The existing methods have presented priors and techniques to estimate transmission with limited focus on structure of the transmission. This paper proposes a lower bound on transmission using structural measure. The proposed lower bound is utilized to estimate the value of transmission, while preserving structural. Further, a quality control parameter is introduced to ensure non -negative value of the transmission for images with brighter objects than atmospheric light. The accuracy and efficiency of the proposed method is established by comparing with renowned SID methods using benchmark datasets.
C1 [Raikwar, Suresh] Thapar Inst Engn & Technol, Bhadson Rd, Patiala 147001, Punjab, India.
   [Tapaswi, Shashikala] ABV Indian Inst Informat Technol & Management, Gwalior 474015, Madhya Pradesh, India.
   [Sharma, Rajendra Kumar] Jaypee Univ Informat Technol, Waknaghat 173234, Himachal Prades, India.
C3 Thapar Institute of Engineering & Technology; ABV-Indian Institute of
   Information Technology & Management, Gwalior; Jaypee University of
   Information Technology
RP Raikwar, S (corresponding author), Thapar Inst Engn & Technol, Bhadson Rd, Patiala 147001, Punjab, India.
EM suresh.raikwar@thapar.edu
CR Berman D, 2020, IEEE T PATTERN ANAL, V42, P720, DOI 10.1109/TPAMI.2018.2882478
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Cao ZW, 2021, IEEE T INTELL TRANSP, V22, P7460, DOI 10.1109/TITS.2020.3003129
   Chen WT, 2020, IEEE T IMAGE PROCESS, V29, P6773, DOI 10.1109/TIP.2020.2993407
   Engin D, 2018, IEEE COMPUT SOC CONF, P938, DOI 10.1109/CVPRW.2018.00127
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Jha DK, 2016, IET COMPUT VIS, V10, P331, DOI 10.1049/iet-cvi.2014.0449
   Kim G, 2022, IEEE T INTELL TRANSP, V23, P2494, DOI 10.1109/TITS.2021.3117868
   Kim G, 2021, IEEE T IMAGE PROCESS, V30, P5452, DOI 10.1109/TIP.2021.3084743
   Kim JH, 2013, J VIS COMMUN IMAGE R, V24, P410, DOI 10.1016/j.jvcir.2013.02.004
   Kumar R, 2021, IEEE T INTELL TRANSP, V22, P6536, DOI 10.1109/TITS.2020.2993906
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li CY, 2020, IEEE T MULTIMEDIA, V22, P704, DOI 10.1109/TMM.2019.2933334
   Li CY, 2018, IEEE ACCESS, V6, P24877, DOI 10.1109/ACCESS.2018.2818882
   Li PY, 2021, IEEE T IMAGE PROCESS, V30, P1100, DOI 10.1109/TIP.2020.3040075
   Li YN, 2021, IEEE T IMAGE PROCESS, V30, P1354, DOI 10.1109/TIP.2020.3044208
   Li YA, 2016, NEUROCOMPUTING, V182, P221, DOI 10.1016/j.neucom.2015.12.032
   Li ZG, 2022, IEEE T IMAGE PROCESS, V31, P6213, DOI 10.1109/TIP.2022.3207571
   Li ZG, 2021, IEEE T IMAGE PROCESS, V30, P9270, DOI 10.1109/TIP.2021.3123551
   Liu SL, 2017, COMPUT ELECTR ENG, V62, P345, DOI 10.1016/j.compeleceng.2016.11.021
   Liu YF, 2023, DISPLAYS, V78, DOI 10.1016/j.displa.2023.102416
   Ma KD, 2015, IEEE IMAGE PROC, P3600, DOI 10.1109/ICIP.2015.7351475
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Middleton W.E.K., 1952, Vision through the atmosphere
   Narasimhan S.G., 2004, Ph.D. thesis
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Nayar S. K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P820, DOI 10.1109/ICCV.1999.790306
   Nayar S.K., 2003, IEEE WORKSH COL PHOT
   Raikwar SC, 2020, IEEE T IMAGE PROCESS, V29, P4832, DOI 10.1109/TIP.2020.2975909
   Raikwar SC, 2020, VISUAL COMPUT, V36, P191, DOI 10.1007/s00371-018-1596-5
   Raikwar SC, 2020, MULTIMED TOOLS APPL, V79, P891, DOI 10.1007/s11042-019-08120-z
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Sahu Geet, 2019, 2019 International Conference on Information Technology (ICIT), P388, DOI 10.1109/ICIT48102.2019.00075
   Sahu G, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3271753
   Sahu G, 2024, IEEE T AUTOM SCI ENG, V21, P305, DOI 10.1109/TASE.2022.3217801
   Sahu G, 2023, IEEE T INTELL TRANSP, V24, P3027, DOI 10.1109/TITS.2022.3225797
   Sahu G, 2022, IEEE TETCI, V6, P762, DOI 10.1109/TETCI.2022.3173443
   Sahu G, 2021, J VIS COMMUN IMAGE R, V74, DOI 10.1016/j.jvcir.2020.103008
   Santra S, 2018, IEEE T IMAGE PROCESS, V27, P4598, DOI 10.1109/TIP.2018.2841198
   Schechner YY, 2001, PROC CVPR IEEE, P325
   Shi LF, 2018, IEEE T MULTIMEDIA, V20, P2503, DOI 10.1109/TMM.2018.2807593
   Shwartz S, 2006, 2006 IEEE COMP SOC C, P1984
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song YF, 2018, IEEE T MULTIMEDIA, V20, P1548, DOI 10.1109/TMM.2017.2771472
   Tan KK, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P788, DOI 10.1109/ICIP.2000.899827
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tang KT, 2014, PROC CVPR IEEE, P2995, DOI 10.1109/CVPR.2014.383
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Wang JZ, 2016, IEEE T MULTIMEDIA, V18, P1000, DOI 10.1109/TMM.2016.2544099
   Wang WC, 2017, IEEE T MULTIMEDIA, V19, P1142, DOI 10.1109/TMM.2017.2652069
   Wang WC, 2017, NEUROCOMPUTING, V238, P365, DOI 10.1016/j.neucom.2017.01.075
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   Yang D, 2018, LECT NOTES COMPUT SC, V11211, P729, DOI 10.1007/978-3-030-01234-2_43
   Yang MM, 2018, IEEE T MULTIMEDIA, V20, P3008, DOI 10.1109/TMM.2018.2820327
   Yuan F, 2018, IEEE T IMAGE PROCESS, V27, P4395, DOI 10.1109/TIP.2018.2837900
   Yuan H, 2017, IEEE ACCESS, V5, P1735, DOI 10.1109/ACCESS.2017.2660302
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang XQ, 2021, IEEE T IMAGE PROCESS, V30, P5211, DOI 10.1109/TIP.2021.3078319
   Zhao SY, 2021, IEEE T IMAGE PROCESS, V30, P3391, DOI 10.1109/TIP.2021.3060873
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 65
TC 0
Z9 0
U1 3
U2 3
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104161
DI 10.1016/j.jvcir.2024.104161
EA MAY 2024
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SZ1H5
UT WOS:001238175900001
DA 2024-08-05
ER

PT J
AU Sun, KY
   Li, XL
   Zhao, Y
AF Sun, Kuiyuan
   Li, Xiaolong
   Zhao, Yao
TI A keypoints-motion-based landmark transfer method for face reenactment
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Image generation Face reenactment Deepfake
ID MODEL
AB Face reenactment is a task to transfer the facial pose and expression from one driving face to the source face. It can be done by adding many condition information, such as the landmark, 3D Morphable Model (3DMM). Because the landmark information is easy to get and contains face structure and expression information, many works rely on it. One of the key points for the landmark -based method is to generate landmarks that keep the face shape of the source image while having the same expression and pose as the driving image. Concentrating on this problem, we propose a novel and effective method to transfer the expression and pose based on the motion of the key points between driving images. Besides, we also try to improve the fit ability of the current popular backbone by adding an enhancement module.
C1 [Sun, Kuiyuan; Li, Xiaolong; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Sun, Kuiyuan; Li, Xiaolong; Zhao, Yao] Beijing Key Lab Adv Informat Sci & Network, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University
RP Zhao, Y (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM yzhao@bjtu.edu.cn
FU Fundamental Research Funds for the Central Universities [2018JBM011];
   National Natural Science Foun-dation of China [61772066, 61972028]
FX This work was supported by Fundamental Research Funds for the Central
   Universities (2018JBM011) and National Natural Science Foun-dation of
   China (No. 61772066, No. 61972028) .
CR Babu KK, 2022, J VIS COMMUN IMAGE R, V82, DOI 10.1016/j.jvcir.2021.103382
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Chung Joon Son, 2018, arXiv
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guo XJ, 2019, Arxiv, DOI arXiv:1902.10859
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang LM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13899, DOI 10.1109/ICCV48922.2021.01366
   Jiang LM, 2020, PROC CVPR IEEE, P2886, DOI 10.1109/CVPR42600.2020.00296
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2018, Arxiv, DOI [arXiv:1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Li LZ, 2020, Arxiv, DOI arXiv:1912.13457
   Liming Jiang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P206, DOI 10.1007/978-3-030-58580-8_13
   Liu BY, 2021, 2021 IFIP NETWORKING CONFERENCE AND WORKSHOPS (IFIP NETWORKING), DOI 10.23919/IFIPNETWORKING52078.2021.9472798
   Ma LM, 2019, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2019), DOI 10.1145/3306131.3317016
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Nagrani A, 2020, COMPUT SPEECH LANG, V60, DOI 10.1016/j.csl.2019.101027
   Nirkin Y, 2019, IEEE I CONF COMP VIS, P7183, DOI 10.1109/ICCV.2019.00728
   Perov I, 2021, Arxiv, DOI [arXiv:2005.05535, DOI 10.48550/ARXIV.2005.05535]
   Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50
   Radford A, 2016, Arxiv, DOI [arXiv:1511.06434, DOI 10.48550/ARXIV.1511.06434]
   Ren YR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13739, DOI 10.1109/ICCV48922.2021.01350
   Shao H, 2021, J VIS COMMUN IMAGE R, V79, DOI 10.1016/j.jvcir.2021.103231
   Siarohin A, 2019, ADV NEUR IN, V32
   Thies J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201350
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Vlasic D, 2005, ACM T GRAPHIC, V24, P426, DOI 10.1145/1073204.1073209
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wiles O, 2018, LECT NOTES COMPUT SC, V11217, P690, DOI 10.1007/978-3-030-01261-8_41
   Zakharov Egor, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P524, DOI 10.1007/978-3-030-58610-2_31
   Zhang JN, 2020, PROC CVPR IEEE, P5325, DOI 10.1109/CVPR42600.2020.00537
   Zhao RQ, 2021, IEEE INT CONF COMP V, P1991, DOI 10.1109/ICCVW54120.2021.00226
   Zhi H, 2019, J VIS COMMUN IMAGE R, V58, P495, DOI 10.1016/j.jvcir.2018.12.012
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 36
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104138
DI 10.1016/j.jvcir.2024.104138
EA APR 2024
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RW6B9
UT WOS:001230725500001
DA 2024-08-05
ER

PT J
AU El-Nagar, G
   El-Sawy, A
   Rashad, M
AF El-Nagar, Gamal
   El-Sawy, Ahmed
   Rashad, Metwally
TI A audio-visual model for efficient video summarization
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Video Skimming; VGGish; SumMe; TVSum; Visualization of score curves
AB The adage "a picture is worth a thousand words"resonates in the digital video domain, suggesting that a video could be seen as a composition of millions of these words. Videos are composed of countless frames. Video summarization creates cohesive visual units in scenes by condensing shots from segments. Video summarization gains prominence by condensing lengthy videos while retaining crucial content. Despite effective techniques using keyframes or keyshots in video summarization, integrating audio components is imperative. This paper focuses on integrating deep learning techniques to generate dynamic summaries enriched with audio. To address that gap, an efficient model employs audio-visual features, enriching summarization for more robust and informative video summaries. The model selects keyshots based on their significance scores, safeguarding essential content. Assigning these scores to specific video shots is a pivotal yet demanding task for video summarization. The model's evaluation occurs on benchmark datasets, TVSum and SumMe. Experimental outcomes reveal its efficacy, showcasing considerable performance enhancements. On the TVSum, SumMe datasets, an F -Score metric of 79.33% and 66.78%, respectively, is achieved, surpassing previous state-of-the-art techniques.
C1 [El-Nagar, Gamal; El-Sawy, Ahmed; Rashad, Metwally] Benha Univ, Fac Comp & Artificial Intelligence, Dept Comp Sci, Banha, Egypt.
   [Rashad, Metwally] Prince Sattam Bin Abdulaziz Univ, Coll Engn, Dept Comp Engn & Informat, Al Kharj 16273, Saudi Arabia.
C3 Egyptian Knowledge Bank (EKB); Benha University; Prince Sattam Bin
   Abdulaziz University
RP Rashad, M (corresponding author), Prince Sattam Bin Abdulaziz Univ, Coll Engn, Dept Comp Engn & Informat, Al Kharj 16273, Saudi Arabia.
EM gamal.essam@fci.bu.edu.eg; ahmed.el_sawy@fci.bu.edu.eg;
   metwally.rashad@fci.bu.edu.eg
RI Rashad, Metwally/KHX-9454-2024
FU Deanship of Scientific Research, Prince Sattam bin Abdulaziz University,
   Al-Kharj, Saudi Arabia; Prince sattam bin Abdulaziz University, Saudi
   Arabia [PSAU/2024/R/1445]
FX The authors would like to thank the Deanship of Scientific Research,
   Prince Sattam bin Abdulaziz University, Al-Kharj, Saudi Arabia, for
   supporting this research via funding from Prince sattam bin Abdulaziz
   University, Saudi Arabia project number (PSAU/2024/R/1445) .
CR Abbasi M, 2023, IEEE IMAGE PROC, P425, DOI 10.1109/ICIP49359.2023.10222350
   Apostolidis E, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2022, P407, DOI 10.1145/3512527.3531404
   Apostolidis E, 2021, IEEE T CIRC SYST VID, V31, P3278, DOI 10.1109/TCSVT.2020.3037883
   Gao Y., 2021, IJCAI, P2403
   Ghauri Junaid Ahmed, 2021, 2021 IEEE International Conference on Multimedia and Expo (ICME), P1, DOI 10.1109/ICME51207.2021.9428318
   Gygli M, 2014, LECT NOTES COMPUT SC, V8695, P505, DOI 10.1007/978-3-319-10584-0_33
   Issa O, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app13106065
   Ji Z, 2021, IEEE T NEUR NET LEAR, V32, P1765, DOI 10.1109/TNNLS.2020.2991083
   Li HP, 2023, IEEE WINT CONF APPL, P5573, DOI 10.1109/WACV56688.2023.00554
   Li S., 2023, ICASSP 2023 2023 IEE, P1
   Li WL, 2023, INT J MACH LEARN CYB, V14, P2991, DOI 10.1007/s13042-023-01814-9
   Li ZT, 2021, IEEE WINT CONF APPL, P3238, DOI 10.1109/WACV48630.2021.00328
   Mokhtarabadi H, 2023, Arxiv, DOI arXiv:2303.15993
   Nair Madhu S., 2023, Journal of Ambient Intelligence and Humanized Computing, P14071, DOI 10.1007/s12652-022-04112-4
   Nair MS, 2021, SIGNAL IMAGE VIDEO P, V15, P735, DOI 10.1007/s11760-020-01791-4
   Nektaria Minaidi M., 2023, arXiv
   Park J, 2022, Arxiv, DOI arXiv:2207.01814
   Potapov D, 2014, LECT NOTES COMPUT SC, V8694, P540, DOI 10.1007/978-3-319-10599-4_35
   Rhevanth M., 2022, Advanced Machine Intelligence and Signal Processing. Lecture Notes in Electrical Engineering (858), P229, DOI 10.1007/978-981-19-0840-8_17
   Singh A, 2023, MULTIMED TOOLS APPL, DOI 10.1007/s11042-023-15431-9
   Song YL, 2015, PROC CVPR IEEE, P5179, DOI 10.1109/CVPR.2015.7299154
   Sreeja MU, 2022, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03641-8
   Terbouche Hacene, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P3143, DOI 10.1109/CVPRW59228.2023.00316
   Tonge A., 2022, 2022 INT C SIGN INF, P1, DOI [10.1109/ICoNSIP49665.2022.10007516, DOI 10.1109/ICONSIP49665.2022.10007516]
   Zhang J, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12234757
   Zhang K, 2016, LECT NOTES COMPUT SC, V9911, P766, DOI 10.1007/978-3-319-46478-7_47
   Zhang Y., 2019, P ACM TUR CEL C CHIN, P1
   Zhao B, 2023, IEEE T NEUR NET LEAR, V34, P5181, DOI 10.1109/TNNLS.2021.3119969
   Zhao B, 2022, IEEE T PATTERN ANAL, V44, P2793, DOI 10.1109/TPAMI.2021.3072117
NR 29
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104130
DI 10.1016/j.jvcir.2024.104130
EA APR 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RB9L9
UT WOS:001225328900001
DA 2024-08-05
ER

PT J
AU Li, HY
   Qiao, RC
   Yu, PF
   Li, HJ
   Tan, MC
AF Li, Haiyan
   Qiao, Renchao
   Yu, Pengfei
   Li, Haijiang
   Tan, Mingchuan
TI CTHD-Net: CNN-Transformer hybrid dehazing network via residual global
   attention and gated boosting strategy
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Image dehazing; Attention mechanism; Transformer; Gated mechanism;
   Boosting strategy
ID FRAMEWORK
AB Single image dehazing is one of crucial tasks in the field of computer vision. However, existing methods are challenged on how to handle unevenly distributed haze, capture global contextual information, and filter noise while preserving details. To overcome these limitations, a novel dehazing network with residual global attention and gated boosting strategy based on a CNN-Transformer hybrid architecture (CTHD-Net) is proposed. Firstly, a feature encoder with a residual global attention (RGA) module is presented to improve the representation capability of the entire network by adaptively assigning different weights to feature maps. Subsequently, a CNNTransformer hybrid architecture is designed to enhance the features encoding via the improved Swin-transformer and to capture the long-range dependencies among features by shifted-window Multi-head Self-attention. Finally, an effective Gated Strength-Operation-Subtract (GSOS) Boosting decoder is developed to reuse the key information required for image reconstruction in the shallow features, while effectively preventing haze noise. Extensive evaluation demonstrates that our proposed CTHD-Net significantly outperforms the previous state-ofthe-arts in terms of quantity and quality. The source code has been made available at https://github.com/RCQiao/CTHD-Net.
C1 [Li, Haiyan; Qiao, Renchao; Yu, Pengfei; Tan, Mingchuan] Yunnan Univ, Sch Informat, Kunming 650504, Peoples R China.
   [Li, Haijiang] Yunnan Commun Investment & Construct Grp Co Ltd, Kunming 650050, Peoples R China.
C3 Yunnan University
RP Yu, PF (corresponding author), Yunnan Univ, Sch Informat, Kunming 650504, Peoples R China.
EM pfyu@ynu.edu.cn
RI CHEN, BING/KHX-6659-2024
OI Yu, Pengfei/0000-0002-2094-1671
FU Famous teacher of teaching of Yunnan 10000 Talents Program; National
   Natural Science Foundation of China [62266049, 62166048, 62066046]
FX This research was supported by "Famous teacher of teaching" of Yunnan
   10000 Talents Program, The National Natural Science Foundation of China
   under Grants 62266049, 62166048 and 62066046.
CR Ancuti CO, 2020, IEEE COMPUT SOC CONF, P1798, DOI 10.1109/CVPRW50498.2020.00230
   Ancuti CO, 2019, IEEE IMAGE PROC, P1014, DOI [10.1109/ICIP.2019.8803046, 10.1109/icip.2019.8803046]
   Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Bajpai S, 2023, MULTIMED TOOLS APPL, V82, P31233, DOI 10.1007/s11042-023-14738-x
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen JP, 2021, NAT PHOTONICS, V15, P570, DOI 10.1038/s41566-021-00828-5
   Creswell A., 2018, An Overview, V53, P65
   Das SD, 2020, IEEE COMPUT SOC CONF, P1994, DOI 10.1109/CVPRW50498.2020.00249
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Girard G., 2014, Towards Quantitative Connectivity Analysis: Reducing Tractography Biases, V266, P278
   Guo CL, 2022, PROC CVPR IEEE, P5802, DOI 10.1109/CVPR52688.2022.00572
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang S.C., 2020, DSNet: Joint Semantic Learning for Object Detection in Inclement Weather Conditions, V2623, P2633
   Intelligence M., 2010, Dark Channel Prior., V2341, P2353
   Jin YY, 2023, LECT NOTES COMPUT SC, V13843, P155, DOI 10.1007/978-3-031-26313-2_10
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Mei KF, 2019, LECT NOTES COMPUT SC, V11361, P203, DOI 10.1007/978-3-030-20887-5_13
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Nayar S. K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P820, DOI 10.1109/ICCV.1999.790306
   Niu ZY, 2021, NEUROCOMPUTING, V452, P48, DOI 10.1016/j.neucom.2021.03.091
   Prakash A, 2021, PROC CVPR IEEE, P7073, DOI 10.1109/CVPR46437.2021.00700
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Romano Y, 2015, SIAM J IMAGING SCI, V8, P1187, DOI 10.1137/140990978
   Sharma D, 2020, IETE TECH REV, V37, P36, DOI 10.1080/02564602.2018.1557569
   Sindagi Vishwanath A., 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P763, DOI 10.1007/978-3-030-58568-6_45
   Singh Ayush, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12538), P166, DOI 10.1007/978-3-030-66823-5_10
   Stoyanov D., 2018, 4 INT WORKSH DLMIA 2
   Turay T., 2022, IEEE Access, V10
   Turay T, 2022, IEEE ACCESS, V10, P14076, DOI 10.1109/ACCESS.2022.3147495
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zheng ZR, 2021, PROC CVPR IEEE, P16180, DOI 10.1109/CVPR46437.2021.01592
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 39
TC 0
Z9 0
U1 16
U2 16
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104066
DI 10.1016/j.jvcir.2024.104066
EA FEB 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JX7S1
UT WOS:001176530100001
DA 2024-08-05
ER

PT J
AU Wang, YF
   Zhou, Y
   Wu, H
   Liu, XY
   Zhai, XD
   Sun, KZ
   Tian, CL
   Zhao, HX
   Li, T
   Jia, WG
   Zhang, Y
AF Wang, Yunfeng
   Zhou, Yi
   Wu, Hao
   Liu, Xiyu
   Zhai, Xiaodi
   Sun, Kuizhi
   Tian, Chengliang
   Zhao, Haixia
   Li, Tao
   Jia, Wenguang
   Zhang, Yan
TI MFCANet: A road scene segmentation network based on Multi-Scale feature
   fusion and context information aggregation
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Scene segmentation; Separable pyramidal information aggregation;
   Upsampling attention fusion; Pixel-wise information cross-fusion
ID SEMANTIC SEGMENTATION
AB Road scene segmentation is the basic task of autonomous driving. Recent representative scene segmentation methods adopt the full convolutional network based on the encoder -decoder. However, the framework can cause the loss of image fine-grained information in the process of down -sampling, feature extraction and feature fusion, resulting in blurred boundary details and chaotic segmentation effect. In this work, a road scene segmentation network based on multi -scale feature fusion and context information aggregation is proposed, in which context information is used to guide feature fusion and enhance semantic feature extraction. Three plug -and -play modules are designed to extract multi -scale features with strong semantic information from high-level features, which compensate for the loss of spatial information in the upper sampling stage, and capture the information dependence among pixels to improve pixel -by -pixel segmentation. Experimental results on Camvid and Cityscapes show that the proposed multi -scale feature fusion and context information aggregation network (MFCANet) can achieve satisfactory performance compared with the state-of-the-art segmentation methods.
C1 [Wang, Yunfeng; Zhou, Yi; Wu, Hao; Sun, Kuizhi; Zhao, Haixia; Li, Tao; Jia, Wenguang; Zhang, Yan] Qingdao Univ Sci & Technol, Coll Electromech Engn, Qingdao 266061, Peoples R China.
   [Liu, Xiyu; Zhai, Xiaodi; Zhang, Yan] Qingdao Univ Sci & Technol, Sch Math & Phys, Qingdao 266061, Peoples R China.
   [Tian, Chengliang] Qingdao Univ, Coll Comp Sci & Technol, Qingdao 266000, Peoples R China.
C3 Qingdao University of Science & Technology; Qingdao University of
   Science & Technology; Qingdao University
RP Zhang, Y (corresponding author), Qingdao Univ Sci & Technol, Coll Electromech Engn, Qingdao 266061, Peoples R China.; Zhang, Y (corresponding author), Qingdao Univ Sci & Technol, Sch Math & Phys, Qingdao 266061, Peoples R China.
EM zy@qust.edu.cn
FU Natural Science Foundation of Shandong Province, China [ZR2019MEE066]
FX This work was supported by the Natural Science Foundation of Shandong
   Province, China No. ZR2019MEE066.
CR Albawi S, 2017, I C ENG TECHNOL
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Brostow GJ, 2008, LECT NOTES COMPUT SC, V5302, P44, DOI 10.1007/978-3-540-88682-2_5
   CHANG YL, 1994, IEEE T IMAGE PROCESS, V3, P868, DOI 10.1109/83.336259
   Changqian Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12413, DOI 10.1109/CVPR42600.2020.01243
   Chen L.-C., 2018, ECCV, P801, DOI [DOI 10.1007/978-3-030-01234-249, 10.1007/978-3-030-01234-2_49]
   Chen LC, 2016, Arxiv, DOI [arXiv:1412.7062, DOI 10.48550/ARXIV.1412.7062, 10.48550/ARXIV.1412.7062]
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai YM, 2021, IEEE WINT CONF APPL, P3559, DOI 10.1109/WACV48630.2021.00360
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Hao SJ, 2020, NEUROCOMPUTING, V406, P302, DOI 10.1016/j.neucom.2019.11.118
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong YD, 2021, Arxiv, DOI arXiv:2101.06085
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu RZ, 2023, IEEE T CYBERNETICS, V53, P3651, DOI 10.1109/TCYB.2021.3128023
   Hu RZ, 2021, DISPLAYS, V69, DOI 10.1016/j.displa.2021.102045
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   John V, 2016, INT C PATT RECOG, P3763, DOI 10.1109/ICPR.2016.7900220
   Krishnan A., 2016, VEHICLE DETECTION RO
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li HC, 2018, Arxiv, DOI [arXiv:1805.10180, 10.48550/arXiv.1805.10180]
   Li X, 2019, IEEE I CONF COMP VIS, P9166, DOI 10.1109/ICCV.2019.00926
   Li ZC, 2022, IEEE T PATTERN ANAL, V44, P9904, DOI 10.1109/TPAMI.2021.3132068
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mengxing Huang, 2012, Proceedings of the 2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel & Distributed Computing (SNPD 2012), P135, DOI 10.1109/SNPD.2012.26
   Nirkin Y, 2021, PROC CVPR IEEE, P4060, DOI 10.1109/CVPR46437.2021.00405
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shafiq M, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12188972
   Shi N, 2010, 2010 THIRD INTERNATIONAL SYMPOSIUM ON INTELLIGENT INFORMATION TECHNOLOGY AND SECURITY INFORMATICS (IITSI 2010), P63, DOI 10.1109/IITSI.2010.74
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Strudel R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7242, DOI 10.1109/ICCV48922.2021.00717
   Sungha Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9370, DOI 10.1109/CVPR42600.2020.00939
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Ulku I, 2022, APPL ARTIF INTELL, V36, DOI 10.1080/08839514.2022.2032924
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang H, 2023, IEEE IMTC P, DOI 10.1109/I2MTC53148.2023.10175992
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu HK, 2019, Arxiv, DOI arXiv:1903.11816
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu FS, 2016, Arxiv, DOI arXiv:1511.07122
   Yu HS, 2018, NEUROCOMPUTING, V304, P82, DOI 10.1016/j.neucom.2018.03.037
   Yuan YH, 2021, Arxiv, DOI [arXiv:1909.11065, DOI 10.48550/ARXIV.1909.11065]
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhao H., 2020, P IEEECVF C COMPUTER, P10076, DOI [10.1109/CVPR42600.2020.01009, DOI 10.1109/CVPR42600.2020.01009]
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng K, 2020, IEEE ACCESS, V8, P140964, DOI 10.1109/ACCESS.2020.3009782
   Zhou Q, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108290
NR 54
TC 1
Z9 1
U1 15
U2 15
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104055
DI 10.1016/j.jvcir.2024.104055
EA JAN 2024
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JB0L5
UT WOS:001170574700001
DA 2024-08-05
ER

PT J
AU Hu, K
   Wu, F
   Zhan, ZF
   Luo, J
   Pu, HY
AF Hu, Ke
   Wu, Fei
   Zhan, Zhenfei
   Luo, Jun
   Pu, Huayan
TI High-low level task combination for object detection in foggy weather
   conditions
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Object detection; Foggy weather; Image dehazing; Multitask learning
AB For the object detection task in foggy weather conditions, image dehazing network is often used as preprocessing method to get a clear input. However, there is not strictly a strong positive correlation between image dehazing task and object detection task. Moreover, the preprocessing module can increase the inference time of the whole model to a certain extent. To alleviate these problems, we propose a novel High -Low level task combination network (HLNet) based on multitask learning, which can learn both high-level and low-level tasks. Specially, instead of restoring the features to clear pixel -wise feature space like common image dehazing method, we opt to perform a restoration in feature level to mitigate the influence of the Batch Normalization (BN) layer of encoder on dehazing task. HLNet jointly learn dehazing task and detection task in an end -to -end fashion, which ensures that the weather -specific information in latent feature space is suppressed. Moreover, we applied the HLNet framework on three different object detection networks, including RetinaNnet, YOLOv3 and YOLOv5s network, and achieved improvements of 1.7 percent, 2.3 percent, and 1.2 percent in mAP respectively. The experimental results demonstrate the effectiveness and generalization ability of our proposed HLNet framework in real foggy scenarios.
C1 [Hu, Ke; Wu, Fei; Luo, Jun] Chongqing Univ, Chongqing 400044, Peoples R China.
   [Zhan, Zhenfei] Chongqing Jiaotong Univ, Chongqing 400074, Peoples R China.
C3 Chongqing University; Chongqing Jiaotong University
RP Wu, F (corresponding author), Chongqing Univ, Chongqing 400044, Peoples R China.
EM kehudk@cqu.edu.cn; wufeifrank@cqu.edu.cn; zhenfeizhan@cqjtu.edu.cn;
   luojun@shu.edu.cn; phygood_2001@t.shu.edu.cn
FU China Postdoctoral Science Foundation [2022M710523]; Fundamental
   Research Funds for the Central Universities [2023CDJXY-023]; Technology
   Support Program Project of Guizhou Province [[2021] General 341]
FX This work is supported by the China Postdoctoral Science Foundation:
   [Grant ID: 2022M710523] ; The authors also wish to thank the Fundamental
   Research Funds for the Central Universities (No. 2023CDJXY-023) and
   Technology Support Program Project of Guizhou Province (Grant No. [2021]
   General 341) . Additionally, the authors thank the associate editor and
   the reviewers for their useful feedback that improved this paper.
CR Bai HR, 2022, IEEE T IMAGE PROCESS, V31, P1217, DOI 10.1109/TIP.2022.3140609
   Bijelic M., 2020, P IEEE CVF C COMP VI, P11682
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Chen LY, 2021, IEEE COMPUT SOC CONF, P182, DOI 10.1109/CVPRW53098.2021.00027
   Chen X, 2022, LECT NOTES COMPUT SC, V13677, P632, DOI 10.1007/978-3-031-19790-1_38
   Cho SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4621, DOI 10.1109/ICCV48922.2021.00460
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang SC, 2021, IEEE T PATTERN ANAL, V43, P2623, DOI 10.1109/TPAMI.2020.2977911
   Kingma D. P., 2014, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li BY, 2021, INT J COMPUT VISION, V129, P1754, DOI 10.1007/s11263-021-01431-5
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu D, 2018, Arxiv, DOI arXiv:1706.04284
   Liu W., 2021, arXiv, DOI DOI 10.1609/AAAI.V36I2.20072
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Redmon J., 2018, CoRR
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Teichmann M, 2018, IEEE INT VEH SYM, P1013, DOI 10.1109/IVS.2018.8500504
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Zhang HY, 2018, Arxiv, DOI arXiv:1710.09412
   Zhang XQ, 2020, COMPUT VIS IMAGE UND, V197, DOI 10.1016/j.cviu.2020.103003
   Zhao SY, 2021, IEEE T IMAGE PROCESS, V30, P3391, DOI 10.1109/TIP.2021.3060873
   Zheng ZR, 2021, PROC CVPR IEEE, P16180, DOI 10.1109/CVPR46437.2021.01592
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 38
TC 1
Z9 1
U1 9
U2 9
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104042
DI 10.1016/j.jvcir.2023.104042
EA JAN 2024
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JC4O4
UT WOS:001170946000001
DA 2024-08-05
ER

PT J
AU Song, W
   Shen, ZH
   Zhang, MH
   Wang, Y
   Liotta, A
AF Song, Wei
   Shen, Zhihao
   Zhang, Minghua
   Wang, Yan
   Liotta, Antonio
TI A hierarchical probabilistic underwater image enhancement model with
   reinforcement tuning
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Underwater image enhancement; Hierarchical probabilistic model;
   Reinforcement learning tuning; Underwater environment
AB Underwater Image Enhancement (UIE) is a challenging problem due to the complex underwater environment. Traditional UIE methods can hardly adapt to various underwater environments. Deep learning-based UIE methods are more powerful but often rely on a large deal of real-world underwater images with distortionfree reference images. This gives rise to two issues: First, the reference images are highly uncertain because the ground-truth images cannot be are captured directly in underwater environment. Second, learning-based methods may lack generalization ability for diverse underwater environments. To tackle these issues, we propose HPUIE-RL, a hierarchical probabilistic UIE model facilitated by reinforcement learning. This model integrates UNet with hierarchical probabilistic modules to produce various enhanced candidate images that reflect the uncertainty of the enhancement. Then, a reinforcement learning fine-tuning framework is designed to fine -tune the pretrained model in an unsupervised manner, which responds to the dynamic underwater environment. Experiments on real-world datasets from diverse underwater environments demonstrate that our HPUIE-RL model outperforms state-of-the-art UIE methods regarding visual and quantitative performance and generalizability.
C1 [Song, Wei; Shen, Zhihao; Zhang, Minghua] Shanghai Ocean Univ, 999 Huchenghuan Rd, Shanghai 201306, Peoples R China.
   [Wang, Yan] Fudan Univ, Shanghai 200433, Peoples R China.
   [Liotta, Antonio] Free Univ Bozen Bolzano, Fac Engn, I-39100 Bozen Bolzano, Italy.
C3 Shanghai Ocean University; Fudan University; Free University of
   Bozen-Bolzano
RP Song, W (corresponding author), Shanghai Ocean Univ, 999 Huchenghuan Rd, Shanghai 201306, Peoples R China.
EM wsong@shou.edu.cn; mhzhang@shou.edu.cn; yanwang19@fudan.edu.cn;
   antonio.liotta@unibz.it
RI Song, wei/KHX-7140-2024
OI Song, wei/0000-0002-0604-5563
FU National Natural Science Founda-tion of China [61972240]
FX <B>Funding</B> This work was supported by the National Natural Science
   Founda-tion of China [grant numbers 61972240] .
CR Akkaynak D, 2019, PROC CVPR IEEE, P1682, DOI 10.1109/CVPR.2019.00178
   Ancuti C, 2012, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2012.6247661
   Berman D, 2021, IEEE T PATTERN ANAL, V43, P2822, DOI 10.1109/TPAMI.2020.2977624
   Chen YW, 2022, IEEE ACCESS, V10, P90523, DOI 10.1109/ACCESS.2022.3201555
   Chiang JY, 2012, IEEE T IMAGE PROCESS, V21, P1756, DOI 10.1109/TIP.2011.2179666
   Fu XY, 2017, I S INTELL SIG PROC, P789, DOI 10.1109/ISPACS.2017.8266583
   Fu XY, 2014, IEEE IMAGE PROC, P4572, DOI 10.1109/ICIP.2014.7025927
   Fu ZQ, 2022, LECT NOTES COMPUT SC, V13678, P465, DOI 10.1007/978-3-031-19797-0_27
   Fu ZQ, 2022, AAAI CONF ARTIF INTE, P643
   Galdran A, 2015, J VIS COMMUN IMAGE R, V26, P132, DOI 10.1016/j.jvcir.2014.11.006
   Guo C., 2023, AAAI, V37, P702
   He KM, 2015, Arxiv, DOI [arXiv:1512.03385, DOI 10.48550/ARXIV.1512.03385]
   He KM, 2009, PROC CVPR IEEE, P1956, DOI [10.1109/CVPRW.2009.5206515, 10.1109/CVPR.2009.5206515]
   Hu K, 2023, ENG APPL ARTIF INTEL, V123, DOI 10.1016/j.engappai.2023.106196
   Huang DM, 2018, LECT NOTES COMPUT SC, V10704, P453, DOI 10.1007/978-3-319-73603-7_37
   Huo JY, 2006, IEEE T CONSUM ELECTR, V52, P541, DOI 10.1109/TCE.2006.1649677
   Islam MJ, 2020, IEEE ROBOT AUTOM LET, V5, P3227, DOI 10.1109/LRA.2020.2974710
   JAFFE JS, 1990, IEEE J OCEANIC ENG, V15, P101, DOI 10.1109/48.50695
   Jiang QP, 2022, IEEE T CIRC SYST VID, V32, P5959, DOI 10.1109/TCSVT.2022.3164918
   Jiang Q, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108324
   King DB, 2015, ACS SYM SER, V1214, P1
   Kohl S.A.A., 2019, PROF 33 C NEURAL INF, P1
   Kohl SAA, 2018, ADV NEUR IN, V31
   Kreutzer J., 2018, P 2018 C N AM CHAPTE, P92, DOI DOI 10.18653/V1/N18-3012
   Li CY, 2021, IEEE T IMAGE PROCESS, V30, P4985, DOI 10.1109/TIP.2021.3076367
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107038
   Liu RS, 2020, IEEE T CIRC SYST VID, V30, P4861, DOI 10.1109/TCSVT.2019.2963772
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Peng LT, 2023, IEEE T IMAGE PROCESS, V32, P3066, DOI 10.1109/TIP.2023.3276332
   Peng YT, 2018, IEEE T IMAGE PROCESS, V27, P2856, DOI 10.1109/TIP.2018.2813092
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Pinto A.S., 2023, 40 INT C MACHINE LEA, P33229
   Ren TD, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3205061
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Skalse J., 2022, Advances in Neural Information Processing Systems, V35, P9460
   Sohn K, 2015, ADV NEUR IN, V28
   Song W, 2018, LECT NOTES COMPUT SC, V11164, P678, DOI 10.1007/978-3-030-00776-8_62
   Stiennon Nisan, 2020, ADV NEURAL INF PROCE, V33, P3008
   Wang H, 2023, ISPRS J PHOTOGRAMM, V195, P462, DOI 10.1016/j.isprsjprs.2022.12.007
   Wang ZY, 2023, IEEE T IMAGE PROCESS, V32, P1442, DOI 10.1109/TIP.2023.3244647
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Yang N, 2021, SIGNAL PROCESS-IMAGE, V94, DOI 10.1016/j.image.2021.116218
   Zhou Yi, 2019, ARXIV
   Zhuang PX, 2022, IEEE T IMAGE PROCESS, V31, P5442, DOI 10.1109/TIP.2022.3196546
NR 47
TC 0
Z9 0
U1 4
U2 4
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104052
DI 10.1016/j.jvcir.2024.104052
EA JAN 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IC6B1
UT WOS:001164151900001
DA 2024-08-05
ER

PT J
AU Zhou, SL
   He, PS
   Liu, JY
   Luo, J
AF Zhou, Shenglie
   He, Peisong
   Liu, Jiayong
   Luo, Jie
TI Towards robust image watermarking via random distortion assignment based
   meta-learning
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Robust image watermarking; Open-set distortion; Meta-learning;
   Invertible neural network
AB Recently, deep learning-based image watermarking methods have been proposed for copyright protection, which are robust to common post-processing operations. However, they suffer from distinct performance drops to open-set distortions, where distortions applied on testing samples are unseen in the training stage. To address this issue, we propose a random distortion assignment-based meta-learning framework for robust image watermarking, where meta-train and meta-test tasks are constructed to simulate open-set distortion scenarios. The embedding and extraction network of watermark information is constructed based on the invertible neural network and equipped with a multi-stage distortion layer, which can conduct random combinations of basic post-processing operators. Besides, to obtain a better balance between robustness and visual imperceptibility, a hybrid loss function is designed by considering global and local similarities based on wavelet decomposition to capture multi-scale texture information. Extensive experiments are conducted by considering various open-set distortions to verify the superiority of the proposed method.
C1 [Zhou, Shenglie; He, Peisong; Liu, Jiayong] Sichuan Univ, Sch Cyber Sci & Engn, Chengdu 610065, Peoples R China.
   [Luo, Jie] Chengdu Univ Informat Technol, Sch Cybersecur, Chengdu 610225, Peoples R China.
C3 Sichuan University; Chengdu University of Information Technology
RP He, PS (corresponding author), Sichuan Univ, Sch Cyber Sci & Engn, Chengdu 610065, Peoples R China.
EM gokeyhps@scu.edu.cn
FU National Key Research and Development Program for Young Scientists
   [2022YFB4501300]; National Key Laboratory of Security Communication
   Foundation [6142103042309]
FX This work is supported by National Key Research and Development Program
   for Young Scientists under Grant 2022YFB4501300 and National Key
   Laboratory of Security Communication Foundation (2023,6142103042309).
CR Ahmadi M, 2020, EXPERT SYST APPL, V146, DOI 10.1016/j.eswa.2019.113157
   Almohammad A., 2010, 2010 2nd International Conference on Image Processing Theory, Tools and Applications (IPTA 2010), P215, DOI 10.1109/IPTA.2010.5586786
   Choudhary R, 2016, 2016 2ND INTERNATIONAL CONFERENCE ON COMMUNICATION CONTROL AND INTELLIGENT SYSTEMS (CCIS), P120, DOI 10.1109/CCIntelS.2016.7878213
   Daren H., 2001, IEEE INT C MULT EXP, P80
   Dehkordi A.B., 2011, 2011 19 IR C EL ENG, P1
   Dinh L, 2015, Arxiv, DOI [arXiv:1410.8516, 10.48550/arXiv.1410.8516]
   Dinh L, 2017, Arxiv, DOI [arXiv:1605.08803, DOI 10.48550/ARXIV.1605.08803]
   Fang H, 2023, AAAI CONF ARTIF INTE, P5054
   Gourrame K, 2016, LECT NOTES COMPUT SC, V9680, P356, DOI 10.1007/978-3-319-33618-3_36
   Guan ZY, 2023, IEEE T PATTERN ANAL, V45, P372, DOI 10.1109/TPAMI.2022.3141725
   He C, 2022, IEEE SIGNAL PROC LET, V29, P1097, DOI 10.1109/LSP.2022.3168195
   Hsieh MS, 2001, IEEE T IND ELECTRON, V48, P875, DOI 10.1109/41.954550
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jia ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P41, DOI 10.1145/3474085.3475324
   Kandi H, 2017, COMPUT SECUR, V65, P247, DOI 10.1016/j.cose.2016.11.016
   Karras T., 2018, INT C LEARNING REPRE
   Ko HJ, 2020, INFORM SCIENCES, V517, P128, DOI 10.1016/j.ins.2019.11.005
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu H, 2023, PATTERN RECOGN, V144, DOI 10.1016/j.patcog.2023.109822
   Liu Y, 2021, PROC CVPR IEEE, P13360, DOI 10.1109/CVPR46437.2021.01316
   Liu Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1509, DOI 10.1145/3343031.3351025
   Lu J., 2022, 2022 IEEE INT C MULT, P1
   Lu SP, 2021, PROC CVPR IEEE, P10811, DOI 10.1109/CVPR46437.2021.01067
   Lugmayr Andreas, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P715, DOI 10.1007/978-3-030-58558-7_42
   Luo Yuanjing, 2023, WWW '23: Proceedings of the ACM Web Conference 2023, P2340, DOI 10.1145/3543507.3583489
   Ma B, 2024, IEEE T CIRC SYST VID, V34, P2763, DOI 10.1109/TCSVT.2023.3311483
   Ma B, 2023, DIGIT SIGNAL PROCESS, V141, DOI 10.1016/j.dsp.2023.104121
   Ma R, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1532, DOI 10.1145/3503161.3547950
   Mingqing Xiao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P126, DOI 10.1007/978-3-030-58452-8_8
   Mohammed AO, 2023, MULTIMED TOOLS APPL, V82, P32855, DOI 10.1007/s11042-023-14797-0
   Mun SM, 2017, Arxiv, DOI arXiv:1704.03248
   Phaphuangwittayakul A, 2022, IEEE T MULTIMEDIA, V24, P2205, DOI 10.1109/TMM.2021.3077729
   Stephane M., 2009, A Wavelet Tour of Signal Processing, VThird, P263, DOI [10.1016/B978-0-12-374370-1.00011-2, DOI 10.1016/B978-0-12-374370-1.00011-2]
   Sun WW, 2021, IEEE T CIRC SYST VID, V31, P1208, DOI 10.1109/TCSVT.2020.2998476
   Wang JY, 2022, COMPUT GRAPH-UK, V108, P61, DOI 10.1016/j.cag.2022.09.002
   Wang RZ, 2001, PATTERN RECOGN, V34, P671, DOI 10.1016/S0031-3203(00)00015-7
   Wang SH, 2004, IEEE T IMAGE PROCESS, V13, P154, DOI 10.1109/TIP.2004.823822
   Wang XY, 2023, IEEE T CIRC SYST VID, V33, P5345, DOI 10.1109/TCSVT.2023.3252042
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu X., 2023, SepMark: Deep Separable Watermarking for Unified Source Tracing and Deepfake Detection, MM '23, P1190
   Xu GP, 2023, PATTERN RECOGN, V143, DOI 10.1016/j.patcog.2023.109819
   Xu QW, 2021, PROC CVPR IEEE, P14378, DOI 10.1109/CVPR46437.2021.01415
   Yang P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14821, DOI 10.1109/ICCV48922.2021.01457
   Zhu JR, 2018, LECT NOTES COMPUT SC, V11219, P682, DOI 10.1007/978-3-030-01267-0_40
NR 44
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD AUG
PY 2024
VL 103
AR 104238
DI 10.1016/j.jvcir.2024.104238
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZX5I3
UT WOS:001278593700001
DA 2024-08-05
ER

PT J
AU Shuang, F
   He, WB
   Li, SD
AF Shuang, Feng
   He, Wenbo
   Li, Shaodong
TI 3D hand reconstruction via aggregating intra and inter graphs guided by
   prior knowledge for hand-object interaction scenario
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE 3D hand reconstruction; Hand-object interaction scenario; MANO model;
   Mutual graph-attention
AB Recently, 3D hand reconstruction has gained more attention in human-computer cooperation, especially for hand -object interaction scenario. However, it still remains challenge to improve estimation accuracy and ensure physical plausibility from a single RGB image. To overcome the challenge, we propose a 3D hand reconstruction network combining the benefits of model -based and model -free approaches to balance accuracy and physical plausibility for hand -object interaction scenario. Firstly, we present a novel topology -aware MANO pose parameters regression module from 2D joints directly. Moreover, we further carefully design a vertexjoint mutual graph -attention module guided by MANO to jointly refine hand meshes and joints, which model the dependencies of vertex -vertex and joint -joint and capture the correlation of vertex -joint for aggregating intra-graph and inter -graph node features respectively. The experimental results demonstrate that our method achieves state-of-the-art performance on recently benchmark datasets HO3DV2 and Dex-YCB, and outperforms all only model -based approaches or model -free approaches.
C1 [Shuang, Feng; He, Wenbo; Li, Shaodong] Guangxi Univ, Sch Elect Engn, Guangxi Key Lab Intelligent Control & Maintenance, 100 Daxuedong Rd, Nanning 530004, Guangxi, Peoples R China.
C3 Guangxi University
RP Shuang, F; Li, SD (corresponding author), Guangxi Univ, Sch Elect Engn, Guangxi Key Lab Intelligent Control & Maintenance, 100 Daxuedong Rd, Nanning 530004, Guangxi, Peoples R China.
EM fshuang@gxu.edu.cn; lishaodongyx@126.com
OI Li, Shaodong/0000-0002-5034-8721
FU Natural Science Foundation of Guangxi [2023GXNSFBA026069]; Basic ability
   promotion project for young and middle-aged teachers in Guangxi's
   colleges and universities [2022KY0008]; Bagui Scholars Program of
   Guangxi Zhuang Autonomous Region
FX This work was supported in part by the Natural Science Foundation of
   Guangxi (Grant No. 2023GXNSFBA026069) , and in part by the funding of
   basic ability promotion project for young and middle-aged teach-ers in
   Guangxi's colleges and universities (Grant No. 2022KY0008) . Feng also
   acknowledges support by the Bagui Scholars Program of Guangxi Zhuang
   Autonomous Region.
CR Boukhayma A, 2019, PROC CVPR IEEE, P10835, DOI 10.1109/CVPR.2019.01110
   Chao YW, 2021, PROC CVPR IEEE, P9040, DOI 10.1109/CVPR46437.2021.00893
   Chen P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12909, DOI 10.1109/ICCV48922.2021.01269
   Chen XY, 2022, PROC CVPR IEEE, P20512, DOI 10.1109/CVPR52688.2022.01989
   Chen YJ, 2021, PROC CVPR IEEE, P10446, DOI 10.1109/CVPR46437.2021.01031
   Choi Hongsuk, 2020, COMPUTER VISION ECCV
   Conci N, 2007, IEEE IMAGE PROC, P2433
   Doosti B, 2020, PROC CVPR IEEE, P6607, DOI 10.1109/CVPR42600.2020.00664
   Gao CY, 2022, NEUROCOMPUTING, V474, P25, DOI 10.1016/j.neucom.2021.12.013
   Ge LH, 2019, PROC CVPR IEEE, P10825, DOI 10.1109/CVPR.2019.01109
   Gyeongsik Moon, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P752, DOI 10.1007/978-3-030-58571-6_44
   Hampali S, 2022, PROC CVPR IEEE, P11080, DOI 10.1109/CVPR52688.2022.01081
   Hampali S, 2020, PROC CVPR IEEE, P3193, DOI 10.1109/CVPR42600.2020.00326
   Han SC, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392452
   Hasson Y, 2019, PROC CVPR IEEE, P11799, DOI 10.1109/CVPR.2019.01208
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang L, 2023, PROC CVPR IEEE, P8969, DOI 10.1109/CVPR52729.2023.00866
   Huang L, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3136, DOI 10.1145/3394171.3413775
   Jiang ZH, 2023, PROC CVPR IEEE, P758, DOI 10.1109/CVPR52729.2023.00080
   Jung SC, 2016, 2016 INTERNATIONAL CONFERENCE ON COLLABORATION TECHNOLOGIES AND SYSTEMS (CTS), P597, DOI [10.1109/CTS.2016.0110, 10.1109/CTS.2016.108]
   Kingma D. P., 2014, arXiv
   Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463
   Kulon D, 2020, PROC CVPR IEEE, P4989, DOI 10.1109/CVPR42600.2020.00504
   Le V.-T., 2021, 2021 13 INT C KNOWL, P1
   Lin K, 2021, PROC CVPR IEEE, P1954, DOI 10.1109/CVPR46437.2021.00199
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin ZF, 2023, PROC CVPR IEEE, P12989, DOI 10.1109/CVPR52729.2023.01248
   Liu SW, 2021, PROC CVPR IEEE, P14682, DOI 10.1109/CVPR46437.2021.01445
   Mueller F, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322958
   Park J, 2022, PROC CVPR IEEE, P1486, DOI 10.1109/CVPR52688.2022.00155
   Romero Javier, 2022, arXiv
   Spurr A., 2020, EUR C COMP VIS, P211
   Tang X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11678, DOI 10.1109/ICCV48922.2021.01149
   Tse THE, 2022, PROC CVPR IEEE, P1654, DOI 10.1109/CVPR52688.2022.00171
   Wang JY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417852
   Wang R, 2023, IEEE WINT CONF APPL, P5724, DOI 10.1109/WACV56688.2023.00569
   Wang YT, 2023, IEEE WINT CONF APPL, P5664, DOI 10.1109/WACV56688.2023.00563
   Xing Liang, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12536), P278, DOI 10.1007/978-3-030-66096-3_20
   Xu H, 2023, PROC CVPR IEEE, P17048, DOI 10.1109/CVPR52729.2023.01635
   Yang LX, 2022, PROC CVPR IEEE, P2740, DOI 10.1109/CVPR52688.2022.00277
   Yin RY, 2021, ADV FUNCT MATER, V31, DOI 10.1002/adfm.202008936
   Yu ZW, 2023, PROC CVPR IEEE, P544, DOI 10.1109/CVPR52729.2023.00060
   Zhang XY, 2022, IEEE T MULTIMEDIA, V24, P166, DOI 10.1109/TMM.2020.3047552
   Zhang X, 2019, IEEE I CONF COMP VIS, P2354, DOI 10.1109/ICCV.2019.00244
   Zhao L, 2019, PROC CVPR IEEE, P3420, DOI 10.1109/CVPR.2019.00354
NR 45
TC 0
Z9 0
U1 2
U2 2
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD APR
PY 2024
VL 100
AR 104129
DI 10.1016/j.jvcir.2024.104129
EA APR 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QD3Z1
UT WOS:001218914000001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zeng, X
   Wang, HK
   Guo, Q
   Wu, YP
AF Zeng, Xin
   Wang, Huake
   Guo, Qiang
   Wu, Yunpeng
TI Correlation-attention guided regression network for efficient crowd
   counting
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Crowd counting; Crowd density estimation; Attention mechanism;
   Regression
AB As a valuable component of intelligent video surveillance, crowd counting has received lots of attention. In practice, however, crowd counting always suffers from the problem of the scale change of pedestrians. To mitigate this limitation, we propose a novel correlation -attention guided regression network to estimate the number of people, termed CGR-Net. To make the generation process of spatial attention and channel attention independent of each other, we design a parallel channel/spatial-wise attention module (PCSAM) to avoid error accumulation. A pixel -wise assisted attention module (PAAM) is developed for learning crowd uneven distribution on the different image pixels to further enhance the ability of the CGR-Net. Furthermore, we present a new loss function to ensure the effectiveness and performance of the proposed method. Comprehensive experimental results demonstrate that our model delivers enhanced representation and attains state-of-the-art performance.
C1 [Zeng, Xin] ZhengZhou Vocat Coll Finance & Taxat, Zhengzhou 450048, Peoples R China.
   [Wang, Huake] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Xian 710049, Peoples R China.
   [Guo, Qiang; Wu, Yunpeng] Zhengzhou Univ, Sch Comp & Artificial Intelligence, Zhengzhou 450001, Peoples R China.
C3 Xi'an Jiaotong University; Zhengzhou University
RP Wu, YP (corresponding author), Zhengzhou Univ, Sch Comp & Artificial Intelligence, Zhengzhou 450001, Peoples R China.
EM work.xzeng@gmail.com; wanghuake@stu.xpu.edu.cn; ieqguo@gs.zzu.edu.cn;
   ieypwu@zzu.edu.cn
RI Guo, Qiang/IUO-2296-2023
OI Guo, Qiang/0000-0003-0916-6667
FU National Natural Science Foundation of China [62002330]; Key Scientific
   Research Project of Higher Education of Henan Province [23A520057]
FX This work was supported by National Natural Science Foundation of China
   (No. 62002330) and Key Scientific Research Project of Higher Education
   of Henan Province (No. 23A520057) .
CR Al-Zaydi ZQH, 2016, J VIS COMMUN IMAGE R, V39, P218, DOI 10.1016/j.jvcir.2016.05.018
   Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45
   Gao JY, 2023, IEEE T NEUR NET LEAR, V34, P4803, DOI 10.1109/TNNLS.2021.3124272
   Gao JY, 2019, NEUROCOMPUTING, V363, P1, DOI 10.1016/j.neucom.2019.08.018
   Guo Q, 2021, KNOWL-BASED SYST, V213, DOI 10.1016/j.knosys.2020.106691
   Han R, 2024, EXPERT SYST APPL, V238, DOI 10.1016/j.eswa.2023.122087
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Kingma D.P., 2015, PROC INT C LEARN RE
   Lempitsky V., 2010, NEURIPS
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Li ZX, 2022, J VIS COMMUN IMAGE R, V87, DOI 10.1016/j.jvcir.2022.103591
   Liang DK, 2022, LECT NOTES COMPUT SC, V13661, P38, DOI 10.1007/978-3-031-19769-7_3
   Liu LB, 2019, IEEE I CONF COMP VIS, P1774, DOI 10.1109/ICCV.2019.00186
   Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524
   Ma JJ, 2023, PATTERN RECOGN, V141, DOI 10.1016/j.patcog.2023.109585
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Paszke A, 2019, ADV NEUR IN, V32
   Sam DB, 2021, IEEE T PATTERN ANAL, V43, P2739, DOI 10.1109/TPAMI.2020.2974830
   Savner SS, 2023, J VIS COMMUN IMAGE R, V94, DOI 10.1016/j.jvcir.2023.103853
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sindagi VA, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Sindagi VA, 2022, IEEE T PATTERN ANAL, V44, P2594, DOI 10.1109/TPAMI.2020.3035969
   Sindagi VA, 2019, IEEE I CONF COMP VIS, P1002, DOI 10.1109/ICCV.2019.00109
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HK, 2023, IEEE T IMAGE PROCESS, V32, P2761, DOI 10.1109/TIP.2023.3274967
   Wang L, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103718
   Wang Q, 2022, IEEE T CYBERNETICS, V52, P4675, DOI 10.1109/TCYB.2020.3033428
   Wang Q, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01365-4
   Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839
   Wang Q, 2022, IEEE T INTELL TRANSP, V23, P15233, DOI 10.1109/TITS.2021.3138896
   Wang T, 2023, KNOWL-BASED SYST, V271, DOI 10.1016/j.knosys.2023.110541
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu XJ, 2019, INT CONF ACOUST SPEE, P2382, DOI [10.1109/icassp.2019.8683744, 10.1109/ICASSP.2019.8683744]
   Xu W, 2021, IEEE SIGNAL PROC LET, V28, P1570, DOI 10.1109/LSP.2021.3096119
   Yan Z., 2021, IEEE Trans. Circuits Syst. Video Technol.
   Zeng X, 2021, IET IMAGE PROCESS, V15, P3534, DOI 10.1049/ipr2.12304
   Zeng X, 2020, EXPERT SYST APPL, V141, DOI 10.1016/j.eswa.2019.112977
   Zhang KB, 2020, APPL SOFT COMPUT, V96, DOI 10.1016/j.asoc.2020.106634
   Zhang SH, 2019, J VIS COMMUN IMAGE R, V62, P166, DOI 10.1016/j.jvcir.2019.05.003
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zhao L., 2023, IEEE Geosci. Remote Sens. Lett.
   Zhong X, 2022, IEEE SIGNAL PROC LET, V29, P1794, DOI 10.1109/LSP.2022.3198371
NR 46
TC 0
Z9 0
U1 5
U2 5
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104078
DI 10.1016/j.jvcir.2024.104078
EA FEB 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LA9T0
UT WOS:001184180500001
DA 2024-08-05
ER

PT J
AU Singh, K
   Parihar, AS
AF Singh, Kavinder
   Parihar, Anil Singh
TI MRN-LOD: Multi-exposure Refinement Network for Low-light Object
   Detection
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Object detection; Multi-exposure images; Adaptive refinement network;
   Low-light images; Computer vision; Feature extraction
ID IMAGE-ENHANCEMENT; FASTER; REPRESENTATION; ILLUMINATION; FEEDBACK
AB Low-light conditions present a myriad of intricacies for object detection, with many existing methods relying primarily on image enhancement before detection. Sometimes, the enhancement methods are unable to improve the detection performance in low-light conditions. In this paper, we present a new Multiexposure refinement network for low-light object detection (MRN-LOD) to avoid the need for enhancement before detection. The MRN-LOD contains: multi-exposure feature extractor, adaptive refinement network, and detection head. The developed multi-exposure feature extractor extracts features from the multi-exposure images generated by the low-light image. We introduced the notion of feature extraction from multi-exposure images for object detection in low light. In addition, we proposed an adaptive refinement network to refine the features of low-light images for better detection performance. The detection head uses the refined features to perform object detection. Extensive experimentation on real -world and synthetic datasets shows the superiority of the proposed MRN-LOD.
C1 [Singh, Kavinder; Parihar, Anil Singh] Delhi Technol Univ, Dept Comp Sci & Engn, Machine Learning Res Lab, Delhi, India.
C3 Delhi Technological University
RP Parihar, AS (corresponding author), Delhi Technol Univ, Dept Comp Sci & Engn, Machine Learning Res Lab, Delhi, India.
EM kavinder85@gmail.com; parihar.anil@gmail.com
RI Parihar, Anil Singh/Z-4992-2019
OI Parihar, Anil Singh/0000-0001-5339-8671
CR Agarwal S, 2002, LECT NOTES COMPUT SC, V2353, P113
   Agrawal SC, 2021, J VIS COMMUN IMAGE R, V77, DOI 10.1016/j.jvcir.2021.103087
   Alvarez-Santos V, 2014, J VIS COMMUN IMAGE R, V25, P499, DOI 10.1016/j.jvcir.2013.03.017
   Baseski E, 2010, J VIS COMMUN IMAGE R, V21, P850, DOI 10.1016/j.jvcir.2010.06.006
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Butko NJ, 2009, PROC CVPR IEEE, P2743
   Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Cui Z., 2021, P IEEECVF INT C COMP, P2553
   Dai JF, 2016, ADV NEUR IN, V29
   Elharrouss O, 2021, J VIS COMMUN IMAGE R, V77, DOI 10.1016/j.jvcir.2021.103116
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fu XY, 2015, IEEE T IMAGE PROCESS, V24, P4965, DOI 10.1109/TIP.2015.2474701
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hao SJ, 2021, COMPUT J, V64, P1028, DOI 10.1093/comjnl/bxab055
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Hong Y., 2021, BMVC, V1, P3
   Huang YT, 2019, 2019 16TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), DOI 10.1109/avss.2019.8909820
   Jiang Y., 2019, arXiv, DOI DOI 10.1109/TIP.2021.3051462
   Jiang Z., 2023, Vis. Comput., P1
   Kong T, 2020, IEEE T IMAGE PROCESS, V29, P7389, DOI 10.1109/TIP.2020.3002345
   Kong T, 2017, PROC CVPR IEEE, P5244, DOI 10.1109/CVPR.2017.557
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Li X, 2023, IEEE T PATTERN ANAL, V45, P3139, DOI 10.1109/TPAMI.2022.3180392
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu WY, 2022, AAAI CONF ARTIF INTE, P1792
   Loh YP, 2019, COMPUT VIS IMAGE UND, V178, P30, DOI 10.1016/j.cviu.2018.10.010
   Lv F., 2018, BRIT MACHINE VISION
   Mehra A, 2021, J VIS COMMUN IMAGE R, V77, DOI 10.1016/j.jvcir.2021.103137
   Panda DK, 2018, J VIS COMMUN IMAGE R, V56, P52, DOI 10.1016/j.jvcir.2018.07.014
   Parihar AS, 2021, IET IMAGE PROCESS, V15, P1410, DOI 10.1049/ipr2.12114
   Parihar AS, 2020, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING AND CONTROL SYSTEMS (ICICCS 2020), P823, DOI [10.1109/ICICCS48265.2020.9120999, 10.1109/iciccs48265.2020.9120999]
   Parihar AS, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON INVENTIVE SYSTEMS AND CONTROL (ICISC 2018), P619, DOI 10.1109/ICISC.2018.8398874
   Pinheiro PO, 2015, ADV NEUR IN, V28
   Pinheiro PO, 2016, LECT NOTES COMPUT SC, V9905, P75, DOI 10.1007/978-3-319-46448-0_5
   Redmon J., 2018, CoRR
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren K, 2023, J FRANKLIN I, V360, P4427, DOI 10.1016/j.jfranklin.2023.02.023
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren YR, 2019, IEEE T CIRC SYST VID, V29, P968, DOI 10.1109/TCSVT.2018.2828141
   Sangnoree A, 2017, J VIS COMMUN IMAGE R, V48, P88, DOI 10.1016/j.jvcir.2017.06.006
   Sasagawa Yukihiro, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P345, DOI 10.1007/978-3-030-58589-1_21
   Sermanet P., 2014, INT C LEARNING REPRE
   Shaaban KM, 2012, J VIS COMMUN IMAGE R, V23, P397, DOI 10.1016/j.jvcir.2011.12.001
   Shen ZQ, 2017, IEEE I CONF COMP VIS, P1937, DOI 10.1109/ICCV.2017.212
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Shrivastava A, 2016, LECT NOTES COMPUT SC, V9905, P330, DOI 10.1007/978-3-319-46448-0_20
   Singh K, 2023, J VIS COMMUN IMAGE R, V91, DOI 10.1016/j.jvcir.2023.103780
   Singh K, 2024, VISUAL COMPUT, V40, P121, DOI 10.1007/s00371-023-02770-9
   Singh K, 2021, J VIS COMMUN IMAGE R, V79, DOI 10.1016/j.jvcir.2021.103241
   Song XJ, 2023, VISUAL COMPUT, V39, P489, DOI 10.1007/s00371-021-02343-8
   Tian Z, 2022, IEEE T PATTERN ANAL, V44, P1922, DOI 10.1109/TPAMI.2020.3032166
   Torralba A, 2007, IEEE T PATTERN ANAL, V29, P854, DOI 10.1109/TPAMI.2007.1055
   Tsai CY, 2020, J VIS COMMUN IMAGE R, V70, DOI 10.1016/j.jvcir.2020.102798
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Ulhaq A, 2016, J VIS COMMUN IMAGE R, V40, P682, DOI 10.1016/j.jvcir.2016.08.008
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang CY, 2021, PROC CVPR IEEE, P13024, DOI 10.1109/CVPR46437.2021.01283
   Wang JQ, 2019, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2019.00308
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang SK, 2022, VISUAL COMPUT, V38, P3073, DOI 10.1007/s00371-022-02560-9
   Wu WH, 2022, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR52688.2022.00581
   Wu YR, 2023, IEEE T NETW SCI ENG, V10, P3086, DOI 10.1109/TNSE.2022.3151502
   Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975
   Yoo D, 2015, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2015.305
   Yuan ZK, 2023, IEEE T POWER DELIVER, V38, P2240, DOI 10.1109/TPWRD.2023.3269206
   Zhang R, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103711
   Zhang S., 2020, P IEEE CVF C COMP VI, P9759
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhang ZS, 2018, PROC CVPR IEEE, P5813, DOI 10.1109/CVPR.2018.00609
   Zhou P, 2018, PROC CVPR IEEE, P528, DOI 10.1109/CVPR.2018.00062
   Zhu CC, 2019, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2019.00093
   Zhu R, 2019, PROC CVPR IEEE, P2263, DOI 10.1109/CVPR.2019.00237
   Zhu YS, 2017, IEEE I CONF COMP VIS, P4146, DOI 10.1109/ICCV.2017.444
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 85
TC 1
Z9 1
U1 8
U2 8
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAR
PY 2024
VL 99
AR 104079
DI 10.1016/j.jvcir.2024.104079
EA FEB 2024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KY6M3
UT WOS:001183568500001
DA 2024-08-05
ER

PT J
AU Wang, H
   Wang, DT
   Chu, ZH
   Rao, ZH
   Yao, Y
AF Wang, Hui
   Wang, Detong
   Chu, Zhihui
   Rao, Zheheng
   Yao, Ye
TI Reversible data hiding for color images based on prediction-error value
   ordering and adaptive embedding☆
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Reversible data hiding; Prediction-error value ordering; Adaptive 2D
   modification; Improved particle swarm optimization; Color images
ID EXPANSION; WATERMARKING; SCHEME
AB Prediction-error value ordering (PEVO) is an efficient implementation of reversible data hiding (RDH), which is perfect for color images to exploit the inter-channel and intra-channel correlations synchronously. However, the existing PEVO method has a slight shortage in the mapping selection stage, the candidate mappings are selected under conditions inconsistent with actual embedding in advance, and this is not the optimal solution. Therefore, in this paper, a novel RDH method for color images based on PEVO and adaptive embedding is proposed to implement adaptive two-dimensional (2D) modification for PEVO. Firstly, an improved particle swarm optimization (IPSO) algorithm based on PEVO is designed to alleviate the high temporal complexity caused by the determination of parameters and implement adaptive 2D modification for PEVO. Next, to further optimize the mapping used in embedding, an improved adaptive 2D mapping generation strategy is proposed by introducing the position information of points. In addition, a dynamic payload partition strategy is proposed to improve the embedding performance. Finally, the experimental results show that the PSNR of the image Lena is as high as 62.94 dB and the average PSNR of the proposed method is 1.46 dB higher than that of the state-of-the-art methods for embedding capacity of 20,000 bits.
C1 [Wang, Hui; Wang, Detong; Rao, Zheheng; Yao, Ye] Hangzhou Dianzi Univ, Sch Cyberspace, Hangzhou 310018, Peoples R China.
   [Chu, Zhihui] China Tobacco Shandong Ind Co Ltd, Jinan 250014, Peoples R China.
C3 Hangzhou Dianzi University; China National Tobacco Corporation
RP Rao, ZH (corresponding author), Hangzhou Dianzi Univ, Sch Cyberspace, Hangzhou 310018, Peoples R China.
EM h.wang@hdu.edu.cn; h.wang@hdu.edu.cn; chuzhihui@sdtobacco.com.cn;
   raozheheng@hdu.edu.cn; yaoye@hdu.edu.cn
FU Humanities and Social Sciences Foundation of Ministry of Education of
   China [23YJA870013]; Zhejiang Provincial Natural Science Foundation of
   China [LY24F020017]
FX This work was supported by the Humanities and Social Sciences Foundation
   of Ministry of Education of China under Grant Number 23YJA870013, and
   the Zhejiang Provincial Natural Science Foundation of China under Grant
   Number LY24F020017.
CR Alattar AM, 2004, IEEE T IMAGE PROCESS, V13, P1147, DOI 10.1109/TIP.2004.828418
   Brar A.S, 2012, Int. J. Emerg. Technol. Adv. Eng., P32
   Celik MU, 2005, IEEE T IMAGE PROCESS, V14, P253, DOI 10.1109/TIP.2004.840686
   Celik MU, 2006, IEEE T IMAGE PROCESS, V15, P1042, DOI 10.1109/TIP.2005.863053
   Chang Q, 2022, IEEE T CIRC SYST VID, V32, P5725, DOI 10.1109/TCSVT.2022.3153796
   Chang Q, 2021, IEEE T CIRC SYST VID, V31, P4850, DOI 10.1109/TCSVT.2021.3055612
   Fan GJ, 2022, J INF SECUR APPL, V67, DOI 10.1016/j.jisa.2022.103180
   Fernandez P, 2023, P IEEE CVF INT C COM, P22466
   Fridrich J, 2002, PROC SPIE, V4675, P1, DOI 10.1117/12.465263
   Hu RW, 2023, IEEE SIGNAL PROC LET, V30, P329, DOI 10.1109/LSP.2023.3256701
   Huang LC, 2013, J SYST SOFTWARE, V86, P716, DOI 10.1016/j.jss.2012.11.024
   Kamstra L, 2005, IEEE T IMAGE PROCESS, V14, P2082, DOI 10.1109/TIP.2005.859373
   Lee SK, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1321, DOI 10.1109/icme.2006.262782
   Li J, 2013, SIGNAL PROCESS, V93, P2748, DOI 10.1016/j.sigpro.2013.01.020
   Li XL, 2015, IEEE T INF FOREN SEC, V10, P2016, DOI 10.1109/TIFS.2015.2444354
   Li XL, 2013, SIGNAL PROCESS, V93, P198, DOI 10.1016/j.sigpro.2012.07.025
   Ou B, 2019, IEEE T CIRC SYST VID, V29, P2176, DOI 10.1109/TCSVT.2018.2859792
   Ou B, 2016, J VIS COMMUN IMAGE R, V39, P12, DOI 10.1016/j.jvcir.2016.05.005
   Ou B, 2015, SIGNAL PROCESS, V108, P642, DOI 10.1016/j.sigpro.2014.10.012
   Ou B, 2014, SIGNAL PROCESS-IMAGE, V29, P760, DOI 10.1016/j.image.2014.05.003
   Ou B, 2013, IEEE T IMAGE PROCESS, V22, P5010, DOI 10.1109/TIP.2013.2281422
   Parah SA, 2017, J BIOMED INFORM, V66, P214, DOI 10.1016/j.jbi.2017.01.006
   Peng F, 2014, DIGIT SIGNAL PROCESS, V25, P255, DOI 10.1016/j.dsp.2013.11.002
   Peng S, 2023, Arxiv, DOI arXiv:2306.03436
   Qi WF, 2023, SIGNAL PROCESS, V207, DOI 10.1016/j.sigpro.2023.108956
   Qin JQ, 2019, IEEE SIGNAL PROC LET, V26, P843, DOI 10.1109/LSP.2019.2909080
   Sachnev V, 2009, IEEE T CIRC SYST VID, V19, P989, DOI 10.1109/TCSVT.2009.2020257
   Tai WL, 2009, IEEE T CIRC SYST VID, V19, P904, DOI 10.1109/TCSVT.2009.2017409
   Tang Ruixiang, 2023, ACM SIGKDD Explorations Newsletter, P43, DOI 10.1145/3606274.3606279
   Thodi DM, 2007, IEEE T IMAGE PROCESS, V16, P721, DOI 10.1109/TIP.2006.891046
   Wu Y, 2023, IEEE Trans. Circuits Syst. Video Technol.
   Xiao MY, 2019, SIGNAL PROCESS, V158, P210, DOI 10.1016/j.sigpro.2019.01.008
   Yao H, 2017, J VIS COMMUN IMAGE R, V43, P152, DOI 10.1016/j.jvcir.2017.01.004
   Zeng YW, 2023, IEEE INT CON MULTI, P1211, DOI 10.1109/ICME55011.2023.00211
   Zhang C, 2022, IEEE T CIRC SYST VID, V32, P4174, DOI 10.1109/TCSVT.2021.3125711
   Zhang T, 2020, IEEE T INF FOREN SEC, V15, P2306, DOI 10.1109/TIFS.2019.2963766
NR 36
TC 0
Z9 0
U1 0
U2 0
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD AUG
PY 2024
VL 103
AR 104239
DI 10.1016/j.jvcir.2024.104239
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZU3J8
UT WOS:001277759800001
DA 2024-08-05
ER

PT J
AU Fu, Y
   Wang, WW
   Zhu, L
   Ye, XY
   Yue, HG
AF Fu, Yun
   Wang, Wenwu
   Zhu, Lei
   Ye, Xinyue
   Yue, Huagang
TI Weakly supervised semantic segmentation based on superpixel affinity
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Weakly supervised learning; Semantic segmentation; Feature
   reorganization; Superpixel affinity
ID NETWORKS
AB Weakly supervised semantic segmentation (WSSS) usually employs the method of modifying and extending class activation map (CAM) seeds to achieve semantic segmentation. However, the down-sampling of the network weakens the edge awareness of CAMs, leading to under- or over-activation problems, which affects the segmentation quality. Considering the excellent contour attachment property of superpixels and the high semantic similarity between pixels within the same superpixel, we propose a superpixel affinity-based method that uses multi-scale features to aggregate superpixels with the same semantics, providing complete localization supervision for the generation of CAMs. In order to improve the accuracy of semantic labels for superpixels, we utilize a method of deep feature reorganization to improve the quality of network-generated CAM seeds. The experimental results indicate that the proposed method has achieved satisfactory performance on PASCAL VOC 2012 and MS COCO 2014 datasets.
C1 [Fu, Yun; Wang, Wenwu; Zhu, Lei; Ye, Xinyue; Yue, Huagang] Wuhan Univ Sci & Technol, Sch Informat Sci & Engn, Wuhan 430081, Peoples R China.
C3 Wuhan University of Science & Technology
RP Wang, WW (corresponding author), Wuhan Univ Sci & Technol, Sch Informat Sci & Engn, Wuhan 430081, Peoples R China.
EM flynet@wust.edu.cn; wangwenwu@wust.edu.cn; zhulei@wust.edu.cn;
   yxyxywust@outlook.com; yhg114@outlook.com
RI Zhu, Lei/KUD-1330-2024
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231
   Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523
   Araslanov N, 2020, PROC CVPR IEEE, P4252, DOI 10.1109/CVPR42600.2020.00431
   Bearman A, 2016, LECT NOTES COMPUT SC, V9911, P549, DOI 10.1007/978-3-319-46478-7_34
   Caesar H, 2018, PROC CVPR IEEE, P1209, DOI 10.1109/CVPR.2018.00132
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Choe J, 2021, IEEE T PATTERN ANAL, V43, P4256, DOI 10.1109/TPAMI.2020.2999099
   Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fan JS, 2020, AAAI CONF ARTIF INTE, V34, P10762
   Fan JS, 2020, PROC CVPR IEEE, P4282, DOI 10.1109/CVPR42600.2020.00434
   Guolei Sun, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P347, DOI 10.1007/978-3-030-58536-5_21
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong S, 2017, IEEE SIGNAL PROC MAG, V34, P39, DOI 10.1109/MSP.2017.2742558
   Huang ZL, 2018, PROC CVPR IEEE, P7014, DOI 10.1109/CVPR.2018.00733
   Jo S, 2021, IEEE IMAGE PROC, P639, DOI 10.1109/ICIP42928.2021.9506058
   Khoreva A, 2017, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2017.181
   Kolesnikov A, 2016, LECT NOTES COMPUT SC, V9908, P695, DOI 10.1007/978-3-319-46493-0_42
   Krahenbuhl P., 2011, NeurIPS, V24
   Kweon H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6974, DOI 10.1109/ICCV48922.2021.00691
   Lee J., 2021, P IEEE CVF C COMP VI, P4071
   Lee J, 2019, PROC CVPR IEEE, P5262, DOI 10.1109/CVPR.2019.00541
   Lee Jungbeom, 2021, Advances in Neural Information Processing Systems, V34
   Lee S, 2021, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR46437.2021.00545
   Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo WF, 2021, PATTERN RECOGN, V115, DOI 10.1016/j.patcog.2021.107858
   Pan JW, 2022, INT J COMPUT VISION, V130, P1181, DOI 10.1007/s11263-022-01590-z
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Su YK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6984, DOI 10.1109/ICCV48922.2021.00692
   Sun KY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7263, DOI 10.1109/ICCV48922.2021.00719
   Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687
   Yao Q, 2020, IEEE ACCESS, V8, P14413, DOI 10.1109/ACCESS.2020.2966647
   Yao YZ, 2021, PROC CVPR IEEE, P2623, DOI 10.1109/CVPR46437.2021.00265
   Yu-Ting Chang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8988, DOI 10.1109/CVPR42600.2020.00901
   Yude Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12272, DOI 10.1109/CVPR42600.2020.01229
   Zhang D., 2020, PROC ADV NEURAL IN, V33, P655, DOI DOI 10.5555/3495724.3495780
   Zhang F, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7222, DOI 10.1109/ICCV48922.2021.00715
   Zhang H, 2022, IEEE COMPUT SOC CONF, P2735, DOI 10.1109/CVPRW56347.2022.00309
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 45
TC 0
Z9 0
U1 1
U2 1
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104168
DI 10.1016/j.jvcir.2024.104168
EA MAY 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TR3T1
UT WOS:001242954400001
DA 2024-08-05
ER

PT J
AU Li, BH
   Huo, FS
AF Li, Bingheng
   Huo, Fushuo
TI REQA: Coarse-to-fine assessment of image quality to alleviate the range
   effect
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Blind image quality assessment; Range effect; Coarse-to-fine assessment;
   Feedback hierarchy
ID SIMILARITY
AB Blind image quality assessment (BIQA) of User Generated Content (UGC) suffers from the range effect, which indicates that on the overall quality range, mean opinion score (MOS) and predicted MOS (pMOS) are well correlated, but when focusing on a particular narrow range, the correlation is lower. To tackle this problem, a novel method is proposed from coarse -grained metric to fine-grained prediction. Concretely, we utilize global context features and local detailed features for the multi -scale distortion perception. Then, to further boost the ability of fine-grained assessment, we introduce the feedback mechanism, which is in accord with Human Vision System (HVS), to perceive detailed distortions gradually. Also, two coarse -to -fine loss functions are proposed to facilitate the feedback perception progress: a rank -and -gradient loss for coarsegrained metric keeps the assessment rank and gradient consistency between pMOS and MOS; a multi -level tolerance loss following the curriculum learning strategy is proposed to make a fine-grained prediction. Both coarse -grained and fine-grained experiments demonstrate that the proposed method outperforms the state-ofthe-art ones, which validates that our method effectively alleviates the range effect. The codes are available at https://github.com/huofushuo/REQA.
C1 [Li, Bingheng] Huawei Technol Co Ltd, Hangzhou 310051, Peoples R China.
   [Huo, Fushuo] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
C3 Huawei Technologies; Hong Kong Polytechnic University
RP Huo, FS (corresponding author), Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
EM fushuo.huo@connect.polyu.hk
OI Huo, Fushuo/0000-0003-1030-7834
FU Xidian University
FX The authors declare the following financial interests/personal
   relationships which may be considered as potential competing interests:
   Li Bingheng reports financial support was provided by Xidian University.
   Huo Fushuo reports a relationship with The Hong Kong Polytechnic
   University that includes: employment. Huo Fushuo has patent issued to
   Issued. co-author previously in Chongqing University.
CR Bae SH, 2017, IEEE T CIRC SYST VID, V27, P1196, DOI 10.1109/TCSVT.2016.2539862
   Bellet A., 2015, SYNTH LECTURESARTIF, V9, P1
   Bengio J., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P41
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   BT RECOMMENDATION ITU-R, 2002, Methodology for the subjective assessment of the quality of television pictures
   Ciancio A, 2011, IEEE T IMAGE PROCESS, V20, P64, DOI 10.1109/TIP.2010.2053549
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373
   Finn C, 2017, PR MACH LEARN RES, V70
   Gao F, 2018, PATTERN RECOGN, V81, P432, DOI 10.1016/j.patcog.2018.04.016
   Gao F, 2017, NEUROCOMPUTING, V257, P104, DOI 10.1016/j.neucom.2017.01.054
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Goldstone RL, 1998, ANNU REV PSYCHOL, V49, P585, DOI 10.1146/annurev.psych.49.1.585
   Gu J, 2019, AAAI CONF ARTIF INTE, P8336
   Hancheng Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14131, DOI 10.1109/CVPR42600.2020.01415
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochstein S, 2002, NEURON, V36, P791, DOI 10.1016/S0896-6273(02)01091-7
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Ke JJ, 2023, PROC CVPR IEEE, P10041, DOI 10.1109/CVPR52729.2023.00968
   Kim J, 2019, IEEE T NEUR NET LEAR, V30, P11, DOI 10.1109/TNNLS.2018.2829819
   Kim J, 2017, IEEE J-STSP, V11, P206, DOI 10.1109/JSTSP.2016.2639328
   Krasula Lukas, 2016, 2016 Eighth International Conference on Quality of Multimedia Experience (QoMEX), DOI 10.1109/QoMEX.2016.7498936
   Krasula L, 2017, IEEE T IMAGE PROCESS, V26, P1496, DOI 10.1109/TIP.2017.2651374
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lee SH, 2023, J VIS COMMUN IMAGE R, V94, DOI 10.1016/j.jvcir.2023.103850
   Li BW, 2022, IEEE T CIRC SYST VID, V32, P5944, DOI 10.1109/TCSVT.2022.3164467
   Li DQ, 2019, IEEE T MULTIMEDIA, V21, P1221, DOI 10.1109/TMM.2018.2875354
   Li LD, 2017, IEEE T MULTIMEDIA, V19, P1030, DOI 10.1109/TMM.2016.2640762
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Liang YD, 2016, LECT NOTES COMPUT SC, V9909, P3, DOI 10.1007/978-3-319-46454-1_1
   Lin HH, 2019, INT WORK QUAL MULTIM
   Lin KY, 2018, PROC CVPR IEEE, P732, DOI 10.1109/CVPR.2018.00083
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Liu Y, 2023, J VIS COMMUN IMAGE R, V91, DOI 10.1016/j.jvcir.2023.103770
   Ma JP, 2021, IEEE T IMAGE PROCESS, V30, P3650, DOI 10.1109/TIP.2021.3064195
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503
   Ma L, 2011, IEEE T MULTIMEDIA, V13, P824, DOI 10.1109/TMM.2011.2109701
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Pan D, 2018, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2018.00667
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Prashnani E, 2018, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2018.00194
   Rehman A, 2012, IEEE T IMAGE PROCESS, V21, P3378, DOI 10.1109/TIP.2012.2197011
   Ren HY, 2018, AAAI CONF ARTIF INTE, P7308
   Saad M.A., 2014, VQEG eLetter, V1, P62
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Seo S, 2020, Arxiv, DOI [arXiv:1902.05316, DOI arXiv:1902.05316.v4]
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shen LQ, 2019, IEEE TETCI, V3, P59, DOI 10.1109/TETCI.2018.2804885
   Shi X., 2023, ICASSP, P1
   Shi XJ, 2015, ADV NEUR IN, V28
   Song TS, 2022, IEEE T CIRC SYST VID, V32, P7592, DOI 10.1109/TCSVT.2022.3179744
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Sun SM, 2023, IEEE T MULTIMEDIA, V25, P2912, DOI 10.1109/TMM.2022.3152942
   Varga D, 2018, IEEE INT CON MULTI
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Jianyi, 2023, P AAAI C ART INT
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z., 2023, IEEE TMM, P1
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Yang JC, 2022, J VIS COMMUN IMAGE R, V87, DOI 10.1016/j.jvcir.2022.103553
   Yang S, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1383, DOI 10.1145/3343031.3350990
   Yao XW, 2023, IEEE T NEUR NET LEAR, V34, P8324, DOI 10.1109/TNNLS.2022.3149534
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Ye P, 2012, IEEE T IMAGE PROCESS, V21, P3129, DOI 10.1109/TIP.2012.2190086
   Ying ZQ, 2020, PROC CVPR IEEE, P3572, DOI 10.1109/CVPR42600.2020.00363
   Yu SD, 2023, J VIS COMMUN IMAGE R, V94, DOI 10.1016/j.jvcir.2023.103848
   Zamir AR, 2017, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2017.196
   Zeng H, 2018, IEEE IMAGE PROC, P609, DOI 10.1109/ICIP.2018.8451285
   Zhai GT, 2012, IEEE T IMAGE PROCESS, V21, P41, DOI 10.1109/TIP.2011.2161092
   Zhang KX, 2023, J VIS COMMUN IMAGE R, V94, DOI 10.1016/j.jvcir.2023.103851
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang P., 2022, ICME, P1, DOI [10.1109/ICME52920.2022.9860021, DOI 10.1109/ICME52920.2022.9860021]
   Zhang WX, 2023, PROC CVPR IEEE, P14071, DOI 10.1109/CVPR52729.2023.01352
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhang XF, 2022, IEEE T CIRC SYST VID, V32, P2746, DOI 10.1109/TCSVT.2021.3096528
NR 82
TC 0
Z9 0
U1 3
U2 3
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104043
DI 10.1016/j.jvcir.2023.104043
EA JAN 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ID7W8
UT WOS:001164463900001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Lin, RY
   Liu, S
   Jiang, J
   Li, SJ
   Li, CQ
   Kuo, CCJ
AF Lin, Ruiyuan
   Liu, Sheng
   Jiang, Jun
   Li, Shujun
   Li, Chengqing
   Kuo, C. -C. Jay
TI Recovering sign bits of DCT coefficients in digital images as an
   optimization problem
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Discrete cosine transform (DCT); Sign bit recovery; Optimization;
   Integer programming; Linear programming; Selective encryption
ID DIFFERENTIAL ENERGY WATERMARKING; BLOCK; RESTORATION; INFORMATION;
   NETWORKS; ERROR
AB Recovering unknown, missing, damaged, distorted, or lost information in DCT coefficients is a common task in multiple applications of digital image processing, including image compression, selective image encryption, and image communication. This paper investigates the recovery of sign bits in DCT coefficients of digital images, by proposing two different approximation methods to solve a mixed integer linear programming (MILP) problem, which is NP-hard in general. One method is a relaxation of the MILP problem to a linear programming (LP) problem, and the other splits the original MILP problem into some smaller MILP problems and an LP problem. We considered how the proposed methods can be applied to JPEG-encoded images and conducted extensive experiments to validate their performances. The experimental results showed that the proposed methods outperformed other existing methods by a substantial margin, both according to objective quality metrics and our subjective evaluation.
C1 [Lin, Ruiyuan; Kuo, C. -C. Jay] Univ Southern Calif, Dept Elect & Comp Engn, Multimedia Commun Lab, Los Angeles, CA USA.
   [Liu, Sheng] Hunan Univ, Sch Comp Sci & Elect Engn, Changsha 410082, Hunan, Peoples R China.
   [Jiang, Jun] Peking Univ, Sch Software & Microelect Elect & Informat Engn, Beijing, Peoples R China.
   [Li, Shujun] Univ Kent, Inst Cyber Secur Soc iCSS, Canterbury, England.
   [Li, Shujun] Univ Kent, Sch Comp, Canterbury, England.
   [Li, Chengqing] Xiangtan Univ, Sch Comp Sci, Xiangtan 411105, Hunan, Peoples R China.
C3 University of Southern California; Hunan University; Peking University;
   University of Kent; University of Kent; Xiangtan University
RP Li, SJ (corresponding author), Univ Kent, Inst Cyber Secur Soc iCSS, Canterbury, England.; Li, SJ (corresponding author), Univ Kent, Sch Comp, Canterbury, England.
EM hooklee@gmail.com
RI Liu, Sheng/GNP-1840-2022; Li, Shujun/A-9032-2008; Li,
   Chengqing/B-9388-2008; Kuo, C.-C. Jay/A-7110-2011
OI Liu, Sheng/0000-0002-1285-7381; Li, Shujun/0000-0001-5628-7328; Li,
   Chengqing/0000-0002-5385-7644; Kuo, C.-C. Jay/0000-0001-9474-5035
CR Ahmed Nasir, 1991, Digit Signal Process, V1, P4, DOI [DOI 10.1016/1051-2004(91)90086-Z, 10.1016/1051-2004(91)90086-Z]
   Ali M., 2013, P DICTA 2013, DOI DOI 10.1109/DICTA.2013.6691535
   Bingabr M, 2004, IEEE T CIRC SYST VID, V14, P441, DOI 10.1109/TCSVT.2004.825545
   Changgui Shi, 1998, Proceedings ACM Multimedia 98, P81
   Chen C, 2018, IEEE T CIRC SYST VID, V28, P1906, DOI 10.1109/TCSVT.2017.2694966
   Das TK, 2005, IEEE T SIGNAL PROCES, V53, P768, DOI 10.1109/TSP.2004.839930
   De Natale FGB, 2000, IEEE J SEL AREA COMM, V18, P1111, DOI 10.1109/49.848260
   Dufaux F, 2008, IEEE T CIRC SYST VID, V18, P1168, DOI 10.1109/TCSVT.2008.928225
   Filippov A, 2019, APSIPA TRANS SIGNAL, V8, DOI 10.1017/ATSIP.2019.6
   Hofbauer Heinz, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1986, DOI 10.1109/ICASSP.2014.6853946
   Hofbauer H, 2015, IEEE IMAGE PROC, P3740, DOI 10.1109/ICIP.2015.7351503
   Hung KW, 2018, J VIS COMMUN IMAGE R, V56, P144, DOI 10.1016/j.jvcir.2018.09.005
   Jack K., 2011, Video Demystified: A Handbook for the Digital Engineer, DOI DOI 10.1016/B978-0-7506-8395-1.X5000-7
   Jiang JM, 2002, IEEE T SIGNAL PROCES, V50, P1160, DOI 10.1109/78.995072
   Koyama J, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P385, DOI 10.1109/PCS.2012.6213370
   Lakhani G, 2000, IEEE T CIRC SYST VID, V10, P819, DOI 10.1109/76.856460
   Lakhani G, 2013, IEEE T IMAGE PROCESS, V22, P1326, DOI 10.1109/TIP.2012.2228492
   Lam EY, 2000, IEEE T IMAGE PROCESS, V9, P1661, DOI 10.1109/83.869177
   Langelaar GC, 2001, IEEE T IMAGE PROCESS, V10, P148, DOI 10.1109/83.892451
   Li SJ, 2007, IEEE T CIRC SYST VID, V17, P214, DOI 10.1109/TCSVT.2006.888840
   Li SJ, 2013, DIGIT IMAG COMPUT, P431
   Li SJ, 2011, IEEE IMAGE PROC, P1537, DOI 10.1109/ICIP.2011.6115738
   Li SJ, 2010, IEEE IMAGE PROC, P2085, DOI 10.1109/ICIP.2010.5653467
   Li WH, 2007, INT J COMPUT MATH, V84, P1367, DOI 10.1080/00207160701294376
   Ma XJ, 2016, IEEE T EMERG TOP COM, V4, P349, DOI 10.1109/TETC.2015.2460462
   Minemura K, 2017, IEEE T CIRC SYST VID, V27, P2309, DOI 10.1109/TCSVT.2016.2589742
   Minemura K, 2014, ASIAPAC SIGN INFO PR
   Minemura K, 2012, IEEE IMAGE PROC, P261, DOI 10.1109/ICIP.2012.6466845
   Ong S, 2017, SIGNAL PROCESS-IMAGE, V58, P1, DOI 10.1016/j.image.2017.06.002
   Park J, 2002, 2002 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL V, PROCEEDINGS, P245
   Park JW, 1997, IEEE T CIRC SYST VID, V7, P845, DOI 10.1109/76.644064
   Pei SC, 2022, IEEE T SIGNAL PROCES, V70, P3848, DOI 10.1109/TSP.2022.3190615
   Peng F, 2020, IEEE T CIRC SYST VID, V30, P2765, DOI 10.1109/TCSVT.2019.2924910
   Ponomarenko NN, 2007, PROC SPIE, V6497, DOI 10.1117/12.713872
   Qiu H, 2019, FUTURE GENER COMP SY, V96, P23, DOI 10.1016/j.future.2019.01.037
   Rad RM, 2013, IEEE IMAGE PROC, P4442, DOI 10.1109/ICIP.2013.6738915
   Sohn H, 2011, IEEE T CIRC SYST VID, V21, P170, DOI 10.1109/TCSVT.2011.2106250
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Suzuki K, 2022, Arxiv, DOI arXiv:2209.10712
   Tsekeridou S, 2000, IEEE T CIRC SYST VID, V10, P646, DOI 10.1109/76.845010
   Tsutake C, 2021, IEEE IMAGE PROC, P2024, DOI 10.1109/ICIP42928.2021.9506155
   Uehara T, 2006, IEEE T IMAGE PROCESS, V15, P3592, DOI 10.1109/TIP.2006.881939
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang H, 2014, SIGNAL PROCESS-IMAGE, V29, P773, DOI 10.1016/j.image.2014.05.001
   Wang YS, 2013, IEEE T CIRC SYST VID, V23, P1476, DOI 10.1109/TCSVT.2013.2248588
   Wu J, 2016, IEEE T MULTIMEDIA, V18, P893, DOI 10.1109/TMM.2016.2535727
   Xu L., 2011, P GLOBECOM 2011, DOI DOI 10.1109/GLOCOM.2011.6134276
   Young SI, 2019, IEEE T IMAGE PROCESS, V28, P343, DOI 10.1109/TIP.2018.2867943
   Zeng WJ, 2003, IEEE T MULTIMEDIA, V5, P118, DOI 10.1109/TMM.2003.808817
   Zhou SW, 2023, IEEE T MULTIMEDIA, V25, P2022, DOI 10.1109/TMM.2022.3142952
   Zhou SW, 2021, IEEE T MULTIMEDIA, V23, P2627, DOI 10.1109/TMM.2020.3014561
NR 51
TC 8
Z9 8
U1 6
U2 6
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104045
DI 10.1016/j.jvcir.2023.104045
EA JAN 2024
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HH2J4
UT WOS:001158532900001
OA Green Submitted, hybrid
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Yang, P
   Wang, QH
   Dou, J
   Dou, L
AF Yang, Peng
   Wang, Qinghui
   Dou, Jie
   Dou, Lei
TI SDCS-CF: Saliency-driven localization and cascade scale estimation for
   visual tracking
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Saliency-aware; Convolution neural network; Cascaded scale estimation;
   Discriminative correlation filter; Visual tracking
ID CORRELATION FILTER TRACKER; OBJECT DETECTION; CHANNEL; MODEL
AB Discriminative correlation filtering (DCF)-based trackers have demonstrated remarkable results in the field of visual tracking. Nevertheless, in most DCF-based trackers: (i) they apply correlation operations across the entire search area features without discriminative weights, rendering them highly susceptible to background interference; and (ii) the fixed aspect ratio scale search strategy is inadequate for accurate scale estimation under irregular scale variations. To overcome these challenges, this study proposes a new correlation filter through saliency-driven localization and cascaded scale estimation (SDCS-CF). Specifically, a U -like network is devised to generate a pixel-wise saliency map. This map is then multiplied element-wise with search area features to accentuate the target attribute, mitigating distractors and increasing localization confidence. Furthermore, a cascaded scale estimation model consisting of three one-dimensional filters is designed to refine the scale estimation process and improving the robustness. Extensive experimental results on three public datasets demonstrate that the proposed SDCS-CF outperforms most DCF-based trackers.
C1 [Yang, Peng; Wang, Qinghui; Dou, Jie; Dou, Lei] Nanjing Univ Sci & Technol, Natl Key Lab Transient Phys, Nanjing 210094, Jiangsu, Peoples R China.
C3 Nanjing University of Science & Technology
RP Dou, L (corresponding author), Nanjing Univ Sci & Technol, Natl Key Lab Transient Phys, Nanjing 210094, Jiangsu, Peoples R China.
EM pyang_15@163.com; douleijs@163.com
RI Dou, Lei/HLW-3194-2023
FU National Natural Science Foundation of China [60904085]; Foundation
   National Key Laboratory of Transient Physics, China; Foundation of
   Defence Technology Innovation Special Filed, China; Jiangsu Funding
   Program for Excellent Postdoctoral Talent, China
FX <B>Acknowledgments</B> This work was supported in part by the National
   Natural Science Foundation of China (No. 60904085) ; in part by the
   Foundation National Key Laboratory of Transient Physics, China; in part
   by the Foundation of Defence Technology Innovation Special Filed, China;
   and in part by the Jiangsu Funding Program for Excellent Postdoctoral
   Talent, China.
CR Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Cao Y, 2022, IEEE T CIRC SYST VID, V32, P3085, DOI 10.1109/TCSVT.2021.3101591
   Dai KN, 2019, PROC CVPR IEEE, P4665, DOI 10.1109/CVPR.2019.00480
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelljan M., 2014, BRIT MACH VIS C NOTT, DOI DOI 10.5244/C.28.65
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Feng W, 2019, IEEE T IMAGE PROCESS, V28, P3232, DOI 10.1109/TIP.2019.2895411
   Fu CH, 2020, IEEE T GEOSCI REMOTE, V58, P8940, DOI 10.1109/TGRS.2020.2992301
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.129, 10.1109/ICCV.2017.128]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XD, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03469-2
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Jain M, 2022, IEEE T CIRC SYST VID, V32, P715, DOI 10.1109/TCSVT.2021.3063144
   Javed S, 2023, IEEE T PATTERN ANAL, V45, P6552, DOI [10.1109/IECON49645.2022.9969084, 10.1109/TPAMI.2022.3212594]
   Jing YC, 2023, PROC CVPR IEEE, P24345, DOI 10.1109/CVPR52729.2023.02332
   Jing YC, 2021, PROC CVPR IEEE, P15704, DOI 10.1109/CVPR46437.2021.01545
   Kinga D., 2015, C TRACK P, V5, P6
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Kristan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P564, DOI 10.1109/ICCVW.2015.79
   Lan XY, 2019, IEEE T IND ELECTRON, V66, P9887, DOI 10.1109/TIE.2019.2898618
   Laurense VA, 2017, P AMER CONTR CONF, P5586, DOI 10.23919/ACC.2017.7963824
   Lee H, 2021, INFORM SCIENCES, V575, P399, DOI 10.1016/j.ins.2021.06.042
   Lee MS, 2022, AAAI CONF ARTIF INTE, P12993
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu QY, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107766
   Liu S., 2022, NeurIPS, V35, P1100
   Liu SH, 2023, PROC CVPR IEEE, P3759, DOI 10.1109/CVPR52729.2023.00366
   Liu T., 2016, IEEE T CIRC SYST VID
   Lukezic A, 2018, INT J COMPUT VISION, V126, P671, DOI 10.1007/s11263-017-1061-3
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Ma HY, 2020, CONF REC ASILOMAR C, P847, DOI 10.1109/IEEECONF51394.2020.9443534
   Ma HY, 2020, IEEE T IMAGE PROCESS, V29, P3546, DOI 10.1109/TIP.2019.2962694
   Mahadevan V, 2013, IEEE T PATTERN ANAL, V35, P541, DOI 10.1109/TPAMI.2012.98
   Mueller M, 2017, PROC CVPR IEEE, P1387, DOI 10.1109/CVPR.2017.152
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Muthuswamy K, 2016, INT C PATT RECOG, P1780, DOI 10.1109/ICPR.2016.7899894
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Sheng B, 2021, IEEE T CYBERNETICS, V51, P1463, DOI 10.1109/TCYB.2020.2988792
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Tang M, 2018, PROC CVPR IEEE, P4874, DOI 10.1109/CVPR.2018.00512
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang MM, 2017, PROC CVPR IEEE, P4800, DOI 10.1109/CVPR.2017.510
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yang XY, 2023, PROC CVPR IEEE, P22552, DOI 10.1109/CVPR52729.2023.02160
   Yang XY, 2022, LECT NOTES COMPUT SC, V13694, P73, DOI 10.1007/978-3-031-19830-4_5
   Yang Xingyi, 2022, Deep model reassembly, V35, P25739
   Yang Zhang, 2020, Pattern Recognition Letters, V136, P161, DOI 10.1016/j.patrec.2020.06.004
   Yuan D, 2020, J VIS COMMUN IMAGE R, V72, DOI 10.1016/j.jvcir.2020.102882
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang PP, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107130
   Zhang SL, 2020, IEEE T CYBERNETICS, V50, P270, DOI 10.1109/TCYB.2018.2868782
   Zhang TZ, 2018, IEEE T IMAGE PROCESS, V27, P2676, DOI 10.1109/TIP.2017.2781304
   Zhao DW, 2019, INFORM SCIENCES, V470, P78, DOI 10.1016/j.ins.2018.08.053
   Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z
   Zhou ZK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9846, DOI 10.1109/ICCV48922.2021.00972
   Zhu H, 2021, IEEE SIGNAL PROC LET, V28, P86, DOI 10.1109/LSP.2020.3039933
NR 72
TC 1
Z9 1
U1 9
U2 9
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD FEB
PY 2024
VL 98
AR 104040
DI 10.1016/j.jvcir.2023.104040
EA JAN 2024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GO2Y4
UT WOS:001153558200001
DA 2024-08-05
ER

PT J
AU Tang, N
   Zhang, DX
   Gao, JH
   Qu, YY
AF Tang, Ni
   Zhang, Dongxiao
   Gao, Juhao
   Qu, Yanyun
TI FSRDiff: A fast diffusion-based super-resolution method using GAN
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Diffusion model; GAN; Super-resolution; Sampling speed
ID SINGLE IMAGE SUPERRESOLUTION; NETWORK
AB Single image super -resolution with diffusion probabilistic models (SRDiff) is a successful diffusion model for image super -resolution that produces high -quality images and is stable during training. However, due to the long sampling time, it is slower in the testing phase than other deep learning -based algorithms. Reducing the total number of diffusion steps can accelerate sampling, but it also causes the inverse diffusion process to deviate from the Gaussian distribution and exhibit a multimodal distribution, which violates the diffusion assumption and degrades the results. To overcome this limitation, we propose a fast SRDiff (FSRDiff) algorithm that integrates a generative adversarial network (GAN) with a diffusion model to speed up SRDiff. FSRDiff employs conditional GAN to approximate the multimodal distribution in the inverse diffusion process of the diffusion model, thus enhancing its sampling efficiency when reducing the total number of diffusion steps. The experimental results show that FSRDiff is nearly 20 times faster than SRDiff in reconstruction while maintaining comparable performance on the DIV2K test set.
C1 [Tang, Ni; Zhang, Dongxiao; Gao, Juhao] Jimei Univ, Sch Sci, Xiamen 361021, Peoples R China.
   [Qu, Yanyun] Xiamen Univ, Dept Comp Sci & Technol, Xiamen 361005, Peoples R China.
C3 Jimei University; Xiamen University
RP Zhang, DX (corresponding author), Jimei Univ, Sch Sci, Xiamen 361021, Peoples R China.
EM zdx1980@jmu.edu.cn
RI ; Zhang, Dongxiao/IAP-7449-2023
OI tang, ni/0009-0007-9791-3461; Zhang, Dongxiao/0009-0002-9369-7988
FU Doctoral Research Initiation Fund of Jimei University, China
   [ZQ2023022]; National Natural Science Foundation of China [12271211,
   62176224]; Natural Science Foundation of Fujian Province, China
   [2022J011275, 2020J01710]; National Key Research and Development Program
   of China [2020AAA0108301]; CCF-Lenovo Blue Ocean Research Fund
FX This work was supported in part by the Doctoral Research Initiation Fund
   of Jimei University, China under Grant ZQ2023022; in part by the
   National Natural Science Foundation of China under Grant 12271211 and
   62176224; in part by the Natural Science Foundation of Fujian Province,
   China under Grant 2022J011275 and 2020J01710; in part by the National
   Key Research and Development Program of China under Grant
   2020AAA0108301; in part by the CCF-Lenovo Blue Ocean Research Fund.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Amaranageswarao G, 2020, J VIS COMMUN IMAGE R, V70, DOI 10.1016/j.jvcir.2020.102819
   Asad M, 2021, J VIS COMMUN IMAGE R, V75, DOI 10.1016/j.jvcir.2021.103047
   Basak Hritam, 2020, 2020 IEEE 15th International Conference on Industrial and Information Systems (ICIIS), P219, DOI 10.1109/ICIIS51140.2020.9342688
   Chan KCK, 2021, PROC CVPR IEEE, P14240, DOI 10.1109/CVPR46437.2021.01402
   Choi J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14347, DOI 10.1109/ICCV48922.2021.01410
   DOCKHORN T., 2022, ADV NEURAL INFORM PR, V35, P30150
   Fuoli D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2340, DOI 10.1109/ICCV48922.2021.00236
   Gao SC, 2023, PROC CVPR IEEE, P10021, DOI 10.1109/CVPR52729.2023.00966
   He XY, 2023, Arxiv, DOI arXiv:2201.10084
   He XY, 2019, PROC CVPR IEEE, P1732, DOI 10.1109/CVPR.2019.00183
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Huang DT, 2023, IEEE T CIRC SYST VID, V33, P6273, DOI 10.1109/TCSVT.2023.3266222
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jo Y, 2021, IEEE COMPUT SOC CONF, P364, DOI 10.1109/CVPRW53098.2021.00046
   Kim Y, 2021, IEEE COMPUT SOC CONF, P424, DOI 10.1109/CVPRW53098.2021.00053
   Kingma D. P., 2014, arXiv
   Kong Zhifeng, 2021, arXiv
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li HY, 2022, NEUROCOMPUTING, V479, P47, DOI 10.1016/j.neucom.2022.01.029
   Li JC, 2021, IEEE T CIRC SYST VID, V31, P2547, DOI 10.1109/TCSVT.2020.3027732
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu SW, 2019, 2019 IEEE 4TH INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING (ICSIP 2019), P1004, DOI [10.1109/SIPROCESS.2019.8868566, 10.1109/siprocess.2019.8868566]
   Liu ZY, 2022, IEEE T CIRC SYST VID, V32, P7418, DOI 10.1109/TCSVT.2022.3188433
   Lu C., 2022, P ADV NEUR INF PROC, V35, P5775
   Lu SQ, 2023, J VIS COMMUN IMAGE R, V96, DOI 10.1016/j.jvcir.2023.103926
   Lugmayr Andreas, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P715, DOI 10.1007/978-3-030-58558-7_42
   Luo Z., 2023, PMLR
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Misra D, 2020, Arxiv, DOI [arXiv:1908.08681, 10.48550/arXiv.1908.08681]
   Phung H, 2023, PROC CVPR IEEE, P10199, DOI 10.1109/CVPR52729.2023.00983
   Rivadeneira RE, 2020, VISAPP: PROCEEDINGS OF THE 15TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 4: VISAPP, P111, DOI 10.5220/0009173601110119
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saharia C, 2023, IEEE T PATTERN ANAL, V45, P4713, DOI 10.1109/TPAMI.2022.3204461
   Salimans Tim, 2022, arXiv, DOI 10.48550/arXiv.2202.00512
   Shang SY, 2024, Arxiv, DOI arXiv:2303.08714
   Song JM, 2022, Arxiv, DOI [arXiv:2010.02502, DOI 10.48550/ARXIV.2010.02502]
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang YH, 2022, Arxiv, DOI arXiv:2212.00490
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Whang J, 2022, PROC CVPR IEEE, P16272, DOI 10.1109/CVPR52688.2022.01581
   Xiao Z., 2021, arXiv
   Xiong YM, 2020, J VIS COMMUN IMAGE R, V73, DOI 10.1016/j.jvcir.2020.102947
   Zhang JQ, 2022, IEEE T CIRC SYST VID, V32, P1020, DOI 10.1109/TCSVT.2021.3071191
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang WL, 2022, IEEE T PATTERN ANAL, V44, P7149, DOI 10.1109/TPAMI.2021.3096327
   Zhang ZX, 2023, SIGNAL PROCESS, V212, DOI 10.1016/j.sigpro.2023.109177
NR 51
TC 0
Z9 0
U1 15
U2 15
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD MAY
PY 2024
VL 101
AR 104164
DI 10.1016/j.jvcir.2024.104164
EA MAY 2024
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TB4K5
UT WOS:001238782700001
DA 2024-08-05
ER

EF