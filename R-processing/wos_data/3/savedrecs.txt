FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Ak, KE
   Sun, Y
   Lim, JH
AF Ak, Kenan E. E.
   Sun, Ying
   Lim, Joo Hwee
TI Learning by Imagination: A Joint Framework for Text-Based Image
   Manipulation and Change Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Text-based image manipulation; change captioning; generative networks;
   GANs; reinforcement learning
AB Image and text are dual modalities of our semantic interpretation. Changing images based on text descriptions allows us to imagine and visualize the world (a.k.a. text-based image manipulation (TIM)). In this paper, we introduce a framework that combines TIM with change captioning (CC) and utilizes the benefits of co-training. CC aims to describe what has changed in a scene and can be regarded as the inverse version of TIM where both tasks rely on generative networks. These generative networks can be regarded as data producers of each other and unlike previous methods, we discover that integrating their learning procedures can benefit both. Since the CC module describes differences between two images as text, the CC module can be used as evaluation criteria and provide feedback. Furthermore, we utilize a shared attention mechanism in TIM and CC modules to localize towards prominent regions as well as enabling a change-aware discriminator. In the opposite direction, the output image synthesized by the TIM module can be assessed with the CC module, by checking whether the ground truth text description can be redescribed. Following this insight, not only do we boost the training of the TIM module, but we also utilize the TIM module as additional supervision for the CC training. Experimental results show that our framework outperforms existing TIM methods on several datasets substantially and we achieve marginal improvements in the CC module. To our best knowledge, this is the first study dedicated to the joint training of TIM and CC tasks.
C1 [Ak, Kenan E. E.; Sun, Ying; Lim, Joo Hwee] Inst Infocomm Res, Visual Intelligence, Singapore 138632, Singapore.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R)
RP Ak, KE (corresponding author), Inst Infocomm Res, Visual Intelligence, Singapore 138632, Singapore.
EM kenanea@i2r.a-star.edu.sg; suny@i2r.a-star.edu.sg;
   joohwee@i2r.a-star.edu.sg
OI Sun, Ying/0000-0002-7224-6726; LIM, Joo Hwee/0000-0002-4103-3824
FU Agency for Science, Technology and Research (A*STAR) through its AME
   Programmatic Funding Scheme [A18A2b0046]
FX & nbsp;This work was supported by the Agency for Science, Technology and
   Research (A*STAR) through its AME Programmatic Funding Scheme under
   Project A18A2b0046. The associate editor coordinating the review of this
   manuscript and approving it for publication was Mrs. Si Liu.& nbsp;
CR Ak KE, 2021, IEEE IMAGE PROC, P2693, DOI 10.1109/ICIP42928.2021.9506508
   Ak KE, 2019, IEEE INT CONF COMP V, P3121, DOI 10.1109/ICCVW.2019.00379
   Ak KE, 2020, IEEE IMAGE PROC, P1601, DOI [10.1109/ICIP40778.2020.9191228, 10.1109/icip40778.2020.9191228]
   Ak KE, 2019, IEEE I CONF COMP VIS, P10540, DOI 10.1109/ICCV.2019.01064
   Ak KE, 2018, PROC CVPR IEEE, P7708, DOI 10.1109/CVPR.2018.00804
   Ak KE, 2018, PATTERN RECOGN LETT, V112, P212, DOI 10.1016/j.patrec.2018.07.019
   Ak KE, 2018, IEEE WINT CONF APPL, P1671, DOI 10.1109/WACV.2018.00186
   Alcantarilla PF, 2018, AUTON ROBOT, V42, P1301, DOI 10.1007/s10514-018-9734-5
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Banerjee S., 2005, P ACL WORKSH INTR EX, P65
   Bengio S, 2015, ADV NEUR IN, V28
   Bruzzone L, 2000, IEEE T GEOSCI REMOTE, V38, P1171, DOI 10.1109/36.843009
   Chen C, 2019, AAAI CONF ARTIF INTE, P8142
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8191
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Dong H, 2017, IEEE I CONF COMP VIS, pCP1, DOI 10.1109/ICCV.2017.608
   El-Nouby A, 2019, IEEE I CONF COMP VIS, P10303, DOI 10.1109/ICCV.2019.01040
   Feng W, 2015, IEEE I CONF COMP VIS, P1260, DOI 10.1109/ICCV.2015.149
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Gunel M., 2018, PROC EUR C COMPUT VI
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hijmans Robert J, 2023, CRAN
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hosseinzadeh M, 2021, PROC CVPR IEEE, P2724, DOI 10.1109/CVPR46437.2021.00275
   Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93
   Huang JS, 2015, IEEE I CONF COMP VIS, P1062, DOI 10.1109/ICCV.2015.127
   Lim JH, 2017, Arxiv, DOI arXiv:1705.02894
   Jhamtani H, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4024
   Jiang WH, 2018, LECT NOTES COMPUT SC, V11206, P510, DOI 10.1007/978-3-030-01216-8_31
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma Diederik P., 2015, P 3 INT C LEARN
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li B., 2020, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, P7880
   Liu R, 2020, INT CONF ACOUST SPEE, P6274, DOI [10.1109/icassp40776.2020.9054681, 10.1109/ICASSP40776.2020.9054681]
   Mao J., 2015, PROC 3 INT C LEARN R
   Miyato T., 2018, 6 INT C LEARNING REP
   Nam S., 2018, Advances in Neural Information Processing Systems, P42
   Nilsback M.-E., 2006, IEEE C COMP VIS PATT, V2, P1447, DOI [DOI 10.1109/CVPR.2006.42, 10.1109/CVPR.2006.42]
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Park DH, 2019, IEEE I CONF COMP VIS, P4623, DOI 10.1109/ICCV.2019.00472
   Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50
   Qiao T., 2019, ADV NEUR IN
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Reed S. E., 2016, ADV NEURAL INFORM PR, V29, P217
   Reed S, 2016, PR MACH LEARN RES, V48
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Rumelhart D.E., 2013, Learning internal representations by error propagation, P399, DOI [10.1016/b978-1-4832-1446-7.50035-2, 10.1016/B978-1-4832-1446-7.50035-2]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh A, 2019, PROC CVPR IEEE, P8309, DOI 10.1109/CVPR.2019.00851
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   Tu YB, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P63
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Vo N, 2019, PROC CVPR IEEE, P6432, DOI 10.1109/CVPR.2019.00660
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wan J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P157, DOI 10.1145/2647868.2654948
   Wang X, 2018, PROC CVPR IEEE, P4213, DOI 10.1109/CVPR.2018.00443
   Xiangxi Shi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P574, DOI 10.1007/978-3-030-58568-6_34
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yang J., 2017, PROC INT C LEARN REP
   Yu SM, 2019, IEEE IMAGE PROC, P734, DOI [10.1109/ICIP.2019.8804285, 10.1109/icip.2019.8804285]
   Zhan HJ, 2022, IEEE T MULTIMEDIA, V24, P819, DOI 10.1109/TMM.2021.3059514
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang JC, 2020, IEEE T IMAGE PROCESS, V29, P6209, DOI 10.1109/TIP.2020.2988435
   Zhang JC, 2019, PROC CVPR IEEE, P8319, DOI 10.1109/CVPR.2019.00852
   Zhang TH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1893, DOI 10.1145/3474085.3475343
   Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu SZ, 2017, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2017.186
NR 76
TC 4
Z9 4
U1 2
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3006
EP 3016
DI 10.1109/TMM.2022.3154154
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200005
DA 2024-07-18
ER

PT J
AU Cao, M
   Ding, C
   Chen, C
   Dou, H
   Hu, XY
   Yan, JC
AF Cao, Min
   Ding, Cong
   Chen, Chen
   Dou, Hao
   Hu, Xiyuan
   Yan, Junchi
TI Progressive Context-Aware Graph Feature Learning for Target
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target re-identification; graph convolutional network; feature learning;
   contextual information; graph feature learning
ID PERSON REIDENTIFICATION; NEURAL-NETWORK
AB This paper aims at robust and discriminative feature learning for target re-identification (Re-ID). In addition to paying attention to the individual appearance information as in most Re-ID methods, we further utilize the abundant contextual information as additional clues to guide the feature learning. Graph as a format of structured data is used to represent the target sample with its context. It describes the first-order appearance information of the samples and the second-order topological relationship information among samples, based on which we compute the feature representation by learning a graph feature embedding. We provide a detailed analysis of graph convolutional network mechanism applied in target Re-ID and propose a novel progressive context-aware graph feature learning method, in which the message passing is dominated by a pre-defined adjacency relationship followed by a learned relationship in a self-adaptive way. The proposed method fully exploits and utilizes contextual information at a low cost for Re-ID. Extensive experiments on five Re-ID benchmarks demonstrate the state-of-the-art performance of the proposed method.
C1 [Cao, Min; Ding, Cong] Soochow Univ, Sch Comp Sci & Technol, Suzhou 215006, Peoples R China.
   [Chen, Chen; Dou, Hao] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
   [Hu, Xiyuan] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210093, Jiangsu, Peoples R China.
   [Yan, Junchi] Shanghai Jiao Tong Univ, AI Inst, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Yan, Junchi] Shanghai Jiao Tong Univ, AI Inst, MoE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China.
C3 Soochow University - China; Chinese Academy of Sciences; Institute of
   Automation, CAS; Nanjing University of Science & Technology; Shanghai
   Jiao Tong University; Shanghai Jiao Tong University
RP Chen, C (corresponding author), Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
EM caomin0719@126.com; congding.ncu@gmail.com; chen.chen@ia.ac.cn;
   douhao2015@ia.ac.cn; huxy@njust.edu.cn; yanjunchi@sjtu.edu.cn
RI Ma, Xiaodong/JAN-7473-2023; Yan, Miaochen/JLL-5061-2023; Hu,
   Xiyuan/AAF-7773-2021; Yan, Jun/IXD-7801-2023; Jiang, Yu/JEZ-9814-2023
OI Hu, Xiyuan/0000-0002-7095-6986; Yan, Junchi/0000-0001-9639-7679
FU National Science Foundation of China (NSFC) [61906194, 62002252];
   Collaborative Innovation Center of Novel Software Technology and
   Industrialization; Liaoning Collaboration Innovation Center for CSLE
FX This work was supported in part by the National Science Foundation of
   China under Grant NSFC 61906194, in part by the National
   ScienceFoundation of China under Grant NSFC 62002252, in part by the
   CollaborativeInnovation Center of Novel Software Technology and
   Industrialization, and inpart by the Liaoning Collaboration Innovation
   Center for CSLE. The associateeditor coordinating the review of this
   manuscript and approving it for publicationwas Mr. Yuming Fang.
CR [Anonymous], 2017, P IEEE C COMP VIS PA
   Bai S, 2019, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2019.00083
   Bai YS, 2019, PROCEEDINGS OF THE TWELFTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'19), P384, DOI 10.1145/3289600.3290967
   Bao LQ, 2019, IEEE COMPUT SOC CONF, P1496, DOI 10.1109/CVPRW.2019.00191
   Cai L, 2022, IEEE T PATTERN ANAL, V44, P5103, DOI 10.1109/TPAMI.2021.3080635
   Cao M, 2021, IEEE T MULTIMEDIA, V23, P1239, DOI 10.1109/TMM.2020.2994524
   Cao M, 2019, PATTERN RECOGN, V94, P218, DOI 10.1016/j.patcog.2019.05.035
   Cao M, 2017, IEEE INT CONF COMP V, P2573, DOI 10.1109/ICCVW.2017.302
   Chen JX, 2021, PROC CVPR IEEE, P8142, DOI 10.1109/CVPR46437.2021.00805
   Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Cheng D, 2018, PATTERN RECOGN, V82, P94, DOI 10.1016/j.patcog.2018.05.007
   Chu RH, 2019, IEEE I CONF COMP VIS, P8281, DOI 10.1109/ICCV.2019.00837
   Dai ZZ, 2019, IEEE I CONF COMP VIS, P3690, DOI 10.1109/ICCV.2019.00379
   Errica F., 2020, ICLR, P1
   Fang PF, 2019, IEEE I CONF COMP VIS, P8029, DOI 10.1109/ICCV.2019.00812
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Ge Y., 2020, ADV NEURAL INFORM PR
   Guo JY, 2019, IEEE I CONF COMP VIS, P3641, DOI 10.1109/ICCV.2019.00374
   Hamilton WL, 2017, ADV NEUR IN, V30
   He B, 2019, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2019.00412
   He LX, 2020, Arxiv, DOI arXiv:2006.02631
   Hu FY, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107745
   Ji D., 2021, P AAAI C ARTIFICIAL, V35, P1646, DOI 10.1609/aaai.v35i2.16257
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Khorramshahi Pirazh, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P369, DOI 10.1007/978-3-030-58568-6_22
   Kingma D. P, 2015, International Conference on Learning Representations
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Li HJ, 2021, PROC CVPR IEEE, P6725, DOI 10.1109/CVPR46437.2021.00666
   Li QM, 2018, AAAI CONF ARTIF INTE, P3538
   Li SC, 2019, IEEE I CONF COMP VIS, P6101, DOI 10.1109/ICCV.2019.00620
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li YY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2115, DOI 10.1145/3343031.3350982
   Liu HM, 2021, NEUROCOMPUTING, V423, P57, DOI 10.1016/j.neucom.2020.10.019
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Liu XC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P907, DOI 10.1145/3394171.3413578
   Liu YH, 2021, IEEE T IMAGE PROCESS, V30, P2060, DOI 10.1109/TIP.2021.3050839
   Lu JJ, 2022, IEEE T MULTIMEDIA, V24, P558, DOI 10.1109/TMM.2021.3054973
   Luo CC, 2019, IEEE I CONF COMP VIS, P4975, DOI 10.1109/ICCV.2019.00508
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Meng DC, 2020, PROC CVPR IEEE, P7101, DOI 10.1109/CVPR42600.2020.00713
   Quan RJ, 2019, IEEE I CONF COMP VIS, P3749, DOI 10.1109/ICCV.2019.00385
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Shen YT, 2018, LECT NOTES COMPUT SC, V11219, P508, DOI 10.1007/978-3-030-01267-0_30
   Sun XX, 2019, PROC CVPR IEEE, P608, DOI 10.1109/CVPR.2019.00070
   Sun YF, 2021, IEEE T PATTERN ANAL, V43, P902, DOI 10.1109/TPAMI.2019.2938523
   Sun ZR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3523, DOI 10.1145/3394171.3413541
   Taufique AMN, 2021, NEUROCOMPUTING, V463, P122, DOI 10.1016/j.neucom.2021.07.082
   Tay CP, 2019, PROC CVPR IEEE, P7127, DOI 10.1109/CVPR.2019.00730
   Tian MQ, 2018, PROC CVPR IEEE, P5794, DOI 10.1109/CVPR.2018.00607
   Tsai-Shien Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P330, DOI 10.1007/978-3-030-58536-5_20
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan CQ, 2020, IEEE T MULTIMEDIA, V22, P1605, DOI 10.1109/TMM.2019.2946486
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang Z, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4973
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu F, 2019, PR MACH LEARN RES, V97
   Wu YM, 2020, IEEE T IMAGE PROCESS, V29, P8821, DOI 10.1109/TIP.2020.3001693
   Xu K., 2019, PROC ICLR
   Yan C, 2022, IEEE T MULTIMEDIA, V24, P1665, DOI 10.1109/TMM.2021.3069562
   Yan YC, 2020, PROC CVPR IEEE, P2896, DOI 10.1109/CVPR42600.2020.00297
   Yan YC, 2019, PROC CVPR IEEE, P2153, DOI 10.1109/CVPR.2019.00226
   Yang F, 2019, PATTERN RECOGN, V86, P143, DOI 10.1016/j.patcog.2018.08.015
   Yang JR, 2020, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR42600.2020.00335
   Yao HT, 2019, IEEE T IMAGE PROCESS, V28, P2860, DOI 10.1109/TIP.2019.2891888
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Zakira J, 2021, MATHEMATICS-BASEL, V9, DOI 10.3390/math9243162
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zhang ZZ, 2019, PROC CVPR IEEE, P667, DOI 10.1109/CVPR.2019.00076
   Zhao CR, 2020, IEEE T MULTIMEDIA, V22, P3180, DOI 10.1109/TMM.2020.2972125
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng L, 2017, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2017.357
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhu YC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P646, DOI 10.1145/3394171.3413607
   Zhu ZH, 2020, AAAI CONF ARTIF INTE, V34, P13114
   Zijie Zhuang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P140, DOI 10.1007/978-3-030-58610-2_9
   Ziwei Zhang, 2022, IEEE Transactions on Knowledge and Data Engineering, V34, P249, DOI 10.1109/TKDE.2020.2981333
NR 79
TC 2
Z9 2
U1 3
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1230
EP 1242
DI 10.1109/TMM.2022.3140647
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100016
DA 2024-07-18
ER

PT J
AU Chen, JC
   Gao, BB
   Lu, ZQ
   Xue, JH
   Wang, CJ
   Liao, QM
AF Chen, Jiacheng
   Gao, Bin-Bin
   Lu, Zongqing
   Xue, Jing-Hao
   Wang, Chengjie
   Liao, Qingmin
TI APANet: Adaptive Prototypes Alignment Network for Few-Shot Semantic
   Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contrastive learning; few-shot learning; metric learning;
   self-supervised learning; semantic segmentation
AB Few-shot semantic segmentation aims to segment novel-class objects in a given query image with only a few labeled support images. Most advanced solutions exploit a metric learning framework that performs segmentation through matching each query feature to a learned class-specific prototype. However, this framework suffers from biased classification due to incomplete feature comparisons. To address this issue, we present an adaptive prototype representation by introducing class-specific and class-agnostic prototypes and thus construct complete sample pairs for learning semantic alignment with query features. The complementary features learning manner effectively enriches feature comparison and helps yield an unbiased segmentation model in the few-shot setting. It is implemented with a two-branch end-to-end network (i.e., a class-specific branch and a class-agnostic branch), which generates prototypes and then combines query features to perform comparisons. In addition, the proposed class-agnostic branch is simple yet effective. In practice, it can adaptively generate multiple class-agnostic prototypes for query images and learn feature alignment in a self-contrastive manner. Extensive experiments on PASCAL-5(i) and COCO-20(i) demonstrate the superiority of our method. At no expense of inference efficiency, our model achieves state-of-the-art results in both 1-shot and 5-shot settings for semantic segmentation.
C1 [Chen, Jiacheng; Lu, Zongqing; Liao, Qingmin] Tsinghua Univ, Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
   [Gao, Bin-Bin; Wang, Chengjie] Tencent YouTu Lab, Shenzhen 518057, Peoples R China.
   [Xue, Jing-Hao] UCL, Dept Stat Sci, London WC1E 6BT, England.
C3 Tsinghua University; Tencent; University of London; University College
   London
RP Lu, ZQ (corresponding author), Tsinghua Univ, Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.; Gao, BB (corresponding author), Tencent YouTu Lab, Shenzhen 518057, Peoples R China.
EM cjc19@mails.tsinghua.edu.cn; gaobb@lamda.nju.edu.cn;
   luzq@sz.tsinghua.edu.cn; jinghao.xue@ucl.ac.uk; jasoncjwang@tencent.com;
   liaoqm@tsinghua.edu.cn
RI Gao, Bin-Bin/ABB-7944-2021
OI Gao, Bin-Bin/0000-0003-2572-8156; Wang, Chengjie/0000-0003-4216-8090;
   Xue, Jing-Hao/0000-0003-1174-610X
FU Tencent; Special Foundation for the Development of Strategic Emerging
   Industries of Shenzhen [JCYJ20170817161056260]
FX This work was supported in part by Tencent and in part by the Special
   Foundation for the Development of Strategic Emerging Industries of
   Shenzhen under Grant JCYJ20170817161056260. This work was done when J.
   Chen was an intern at Tencent YouTu Lab. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Vasileios Mezaris. (Jiacheng Chen and Bin-Bin Gao
   contributed equally tothis work.)
CR Asano Y. M., 2020, ICLR
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bansal A, 2018, LECT NOTES COMPUT SC, V11205, P397, DOI 10.1007/978-3-030-01246-5_24
   Bautista M. A., 2016, NIPS, P3846
   Boyu Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P763, DOI 10.1007/978-3-030-58598-3_45
   Caron M, 2019, IEEE I CONF COMP VIS, P2959, DOI 10.1109/ICCV.2019.00305
   Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Dong N., 2018, BMVC, V4, P4
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Feng ZY, 2019, PROC CVPR IEEE, P10356, DOI 10.1109/CVPR.2019.01061
   Gairola S, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P573
   Gidaris S., 2018, P 6 INT C LEARNING R
   Grill Jean-Bastien, 2020, ADV NEURAL INFORM PR
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   Haochen Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P730, DOI 10.1007/978-3-030-58601-0_43
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Hu T, 2019, AAAI CONF ARTIF INTE, P8441
   Hu YF, 2021, IEEE T MULTIMEDIA, V23, P4285, DOI 10.1109/TMM.2020.3039329
   Huang HX, 2021, IEEE T MULTIMEDIA, V23, P1666, DOI 10.1109/TMM.2020.3001510
   Nguyen K, 2019, IEEE I CONF COMP VIS, P622, DOI 10.1109/ICCV.2019.00071
   KRAHENBUHL P, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472
   Li X, 2019, IEEE I CONF COMP VIS, P9166, DOI 10.1109/ICCV.2019.00926
   Liu BH, 2021, IEEE T IMAGE PROCESS, V30, P3142, DOI 10.1109/TIP.2021.3058512
   Liu WD, 2020, PROC CVPR IEEE, P4164, DOI 10.1109/CVPR42600.2020.00422
   Liu Y., 2020, P EUR C COMP VIS ECC, P142, DOI DOI 10.1007/978-3-030-58545-79
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Phaphuangwittayakul A, 2022, IEEE T MULTIMEDIA, V24, P2205, DOI 10.1109/TMM.2021.3077729
   Rakelly K., 2018, P ICLR WORKSH
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shaban A, 2017, ARXIV PREPRINT ARXIV, DOI 10.5244/C.31.167
   Siam M, 2019, IEEE I CONF COMP VIS, P5248, DOI 10.1109/ICCV.2019.00535
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tian ZT, 2022, IEEE T PATTERN ANAL, V44, P1050, DOI 10.1109/TPAMI.2020.3013717
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Xie GS, 2021, PROC CVPR IEEE, P5471, DOI 10.1109/CVPR46437.2021.00543
   Xie JY, 2016, PR MACH LEARN RES, V48
   Zhang BF, 2021, PROC CVPR IEEE, P8308, DOI 10.1109/CVPR46437.2021.00821
   Zhang C, 2019, IEEE I CONF COMP VIS, P9586, DOI 10.1109/ICCV.2019.00968
   Zhang C, 2019, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR.2019.00536
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhang XL, 2020, IEEE T CYBERNETICS, V50, P3855, DOI 10.1109/TCYB.2020.2992433
   Zhu YH, 2021, IEEE T MULTIMEDIA, V23, P1200, DOI 10.1109/TMM.2020.2993952
NR 50
TC 5
Z9 5
U1 7
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4361
EP 4373
DI 10.1109/TMM.2022.3174405
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200021
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Huang, Z
   Yang, J
   Gong, C
AF Huang, Zhuo
   Yang, Jian
   Gong, Chen
TI They are Not Completely Useless: Towards Recycling Transferable
   Unlabeled Data for Class-Mismatched Semi-Supervised Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semi-supervised learning; class mismatch; domain adaptation
AB Semi-Supervised Learning (SSL) with mismatched classes deals with the problem that the classes-of-interests in the limited labeled data are only a subset of the classes in massive unlabeled data. As a result, classical SSL methods would be misled by the classes which are only possessed by the unlabeled data. To solve this problem, some recent methods divide unlabeled data to useful in-distribution (ID) data and harmful out-of-distribution (OOD) data, among which the latter should particularly be weakened. As a result, the potential value contained by OOD data is largely overlooked. To remedy this defect, this paper proposes a "Transferable OOD data Recycling" (TOOR) method which properly utilizes ID data as well as the "recyclable" OOD data to enrich the information for conducting class-mismatched SSL. Specifically, TOOR treats the OOD data that have a close relationship with ID data and labeled data as recyclable, and employs adversarial domain adaptation to project them to the space of ID data and labeled data. In other words, the recyclability of an OOD datum is evaluated by its transferability, and the recyclable OOD data are transferred so that they are compatible with the distribution of known classes-of-interests. Consequently, our TOOR extracts more information from unlabeled data than existing methods, so it achieves an improved performance which is demonstrated by the experiments on typical benchmark datasets.
C1 [Huang, Zhuo; Gong, Chen] Nanjing Univ Sci & Technol, Key Lab Intelligent Percept & Syst High Dimens In, Minist Educ, Nanjing 210094, Peoples R China.
   [Huang, Zhuo; Gong, Chen] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Social, Nanjing 210094, Peoples R China.
   [Yang, Jian] Nankai Univ, Coll Comp Sci, Tianjin, Peoples R China.
C3 Nanjing University of Science & Technology; Nanjing University of
   Science & Technology; Nankai University
RP Gong, C (corresponding author), Nanjing Univ Sci & Technol, Key Lab Intelligent Percept & Syst High Dimens In, Minist Educ, Nanjing 210094, Peoples R China.; Gong, C (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Social, Nanjing 210094, Peoples R China.
EM hzhuo@njust.edu.cn; csjyang@njust.edu.cn; chen.gong@njust.edu.cn
RI li, xiaomin/KCX-9845-2024; WANG, SHIHAO/KHC-8263-2024; GONG,
   CHEN/JDW-5727-2023
FU NSF of China [61973162]; NSF of Jiangsu Province [BZ2021013];
   Fundamental Research Funds for the Central Universities [30920032202,
   30921013114]; CCF-Tencent Open Fund [RAGR20200101]; "111" Program
   [B13022]
FX This work was supported in part by the NSF of China under Grant
   61973162, in part by the NSF of Jiangsu Province under Grant BZ2021013,
   in part by the Fundamental Research Funds for the Central Universities
   under Grants 30920032202 and 30921013114, in part by CCF-Tencent Open
   Fund under Grant RAGR20200101, and in part by "111" Program under Grant
   B13022.
CR Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Bennett KP, 1999, ADV NEUR IN, V11, P368
   Berthelot D., 2019, arXiv preprint arXiv:1911.09785
   Berthelot D, 2019, ADV NEUR IN, V32
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Cao ZJ, 2018, LECT NOTES COMPUT SC, V11212, P139, DOI 10.1007/978-3-030-01237-3_9
   Cao ZJ, 2019, PROC CVPR IEEE, P2980, DOI 10.1109/CVPR.2019.00310
   Chapelle O., 2006, SEMISUPERVISED LEARN
   Chen YB, 2020, AAAI CONF ARTIF INTE, V34, P3569
   Chrabaszcz P, 2017, Arxiv, DOI arXiv:1707.08819
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Geng B, 2012, IEEE T PATTERN ANAL, V34, P1227, DOI 10.1109/TPAMI.2012.57
   Gong C, 2016, IEEE T IMAGE PROCESS, V25, P3249, DOI 10.1109/TIP.2016.2563981
   Gong C, 2015, PROC CVPR IEEE, P2531, DOI 10.1109/CVPR.2015.7298868
   Gong C, 2015, IEEE T NEUR NET LEAR, V26, P2261, DOI 10.1109/TNNLS.2014.2376936
   Gong C, 2015, IEEE T NEUR NET LEAR, V26, P2148, DOI 10.1109/TNNLS.2014.2376963
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Grandvalet Y., 2005, CAP, V367, P281
   Guo LZ, 2020, PR MACH LEARN RES, V119
   Hinton G., 2015, COMPUT SCI, V2
   Huang Zhuo, 2021, Advances in Neural Information Processing Systems, V34, P26714
   Iscen A, 2019, PROC CVPR IEEE, P5065, DOI 10.1109/CVPR.2019.00521
   Ke JC, 2022, IEEE T CYBERNETICS, V52, P164, DOI 10.1109/TCYB.2019.2953337
   Kingma D. P, 2015, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1412.6980
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee D.-H., 2013, PSEUDOLABEL SIMPLE E, V3, P896
   Li C.-L., 2020, Advances in neural information processing systems, V33, P596
   Li Y, 2021, IEEE T MULTIMEDIA, V23, P1354, DOI 10.1109/TMM.2020.2997185
   Li ZY, 2021, IEEE T MULTIMEDIA, V23, P2694, DOI 10.1109/TMM.2020.3015015
   Liang S., 2017, PROC INT C LEARN REP
   Luo YC, 2018, PROC CVPR IEEE, P8896, DOI 10.1109/CVPR.2018.00927
   Mesgaran M, 2021, IEEE T MULTIMEDIA, V23, P3931, DOI 10.1109/TMM.2020.3034530
   Min SB, 2021, IEEE T MULTIMEDIA, V23, P899, DOI 10.1109/TMM.2020.2990063
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Netzer Y., 2011, READING DIGITS NATUR
   Oliver A, 2018, ADV NEUR IN, V31
   Qing Yu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P438, DOI 10.1007/978-3-030-58610-2_26
   Sajjadi M, 2016, ADV NEUR IN, V29
   Samuli L., 2017, ICLR, P1
   Tarvainen A, 2017, ADV NEUR IN, V30
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wan S, 2021, AAAI CONF ARTIF INTE, V35, P10049
   Wang F, 2008, IEEE T KNOWL DATA EN, V20, P55, DOI 10.1109/TKDE.2007.190672
   Wang Q, 2019, IEEE I CONF COMP VIS, P1466, DOI 10.1109/ICCV.2019.00155
   Xia X., 2021, Instance correction for learning with open-set noisy labels
   Xia X., 2020, Extended t: Learning with mixed closed-set and open-set noisy labels
   Xia X., 2022, P INT C LEARN REPR, P1
   Yao Y, 2020, AAAI CONF ARTIF INTE, V34, P12669
   You KC, 2019, PROC CVPR IEEE, P2715, DOI 10.1109/CVPR.2019.00283
   Yu B, 2019, PROC CVPR IEEE, P10668, DOI 10.1109/CVPR.2019.01093
   Zagoruyko S., 2016, BMVC, P1
   Zhai XH, 2019, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2019.00156
   Zhang CJ, 2019, IEEE T MULTIMEDIA, V21, P2482, DOI 10.1109/TMM.2019.2903628
   Zhang H., 2017, PROC INT C LEARN REP, P1
   Zhang J, 2018, PROC CVPR IEEE, P8156, DOI 10.1109/CVPR.2018.00851
   Zhang LH, 2020, PROC CVPR IEEE, P3911, DOI 10.1109/CVPR42600.2020.00397
   Zhou DY, 2004, ADV NEUR IN, V16, P321
NR 60
TC 4
Z9 4
U1 7
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1844
EP 1857
DI 10.1109/TMM.2022.3179895
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100020
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kong, YQ
   Wang, YH
   Li, AN
   Huang, QY
AF Kong, Yongqiang
   Wang, Yunhong
   Li, Annan
   Huang, Qiuyu
TI Self-Sufficient Feature Enhancing Networks for Video Salient Object
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Object detection; Optical imaging; Spatiotemporal
   phenomena; Task analysis; Estimation; Visualization; Video salient
   object detection; deep network; joint training; feature enhancing module
ID OPTIMIZATION; ACCURATE
AB Detecting salient objects in videos is a very challenging task. Current state-of-the-art methods are dominated by motion based deep neural networks, among which optical flow is often leveraged as motion representation. Though with robust performance, these optical flow-based video salient object detection methods face at least two problems that may hinder their generalization and application. First, computing optical flow as a pre-processing step does not support direct end-to-end learning; second, little attention has been given to the quality of visual features due to high computational cost of spatiotemporal feature encoding. In this paper we propose a novel self-sufficient feature enhancing network (SFENet) for video salient object detection, which leverages optical flow estimation as an auxiliary task while being end-to-end trainable. With a joint training scheme of both salient object detection and optical flow estimation, its multi-task architecture can be totally self-sufficient for achieving good performance without any pre-processing. Furthermore, for improving feature quality, we design four lightweight modules in spatial and temporal domains, including cross-layer fusion, multi-level warping, spatial-channel attention and boundary-aware refinement. The proposed method is evaluated through extensive experiments on five video salient object detection datasets. Experimental results show that our SFENet can be easily trained with fast convergence speed. It significantly outperforms previous methods in terms of various evaluation metrics. Moreover, with optical flow estimation and unsupervised video object segmentation as example applications, our method also yields state-of-the-art results on standard datasets.
C1 [Kong, Yongqiang; Wang, Yunhong; Li, Annan; Huang, Qiuyu] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
C3 Beihang University
RP Li, AN (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM yqkong@buaa.edu.cn; yhwang@buaa.edu.cn; liannan@buaa.edu.cn;
   huangqiuyu@buaa.edu.cn
RI Li, Annan/KRP-2299-2024
OI Li, Annan/0000-0003-3497-5052
FU National Natural Science Foundation of China [U20B2069]; CCF-Tencent
   Rhino-Bird Research Fund
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant U20B2069 and in part by CCF-Tencent
   Rhino-Bird Research Fund.
CR Ballas N., 2016, INT C LEARNING REPRE
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21
   Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen CLZ, 2021, IEEE T IMAGE PROCESS, V30, P3995, DOI 10.1109/TIP.2021.3068644
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P1090, DOI 10.1109/TIP.2019.2934350
   Chen CLZ, 2018, IEEE T MULTIMEDIA, V20, P3324, DOI 10.1109/TMM.2018.2839523
   Chen CZ, 2017, IEEE T IMAGE PROCESS, V26, P3156, DOI 10.1109/TIP.2017.2670143
   Chen YH, 2018, IEEE T IMAGE PROCESS, V27, P3345, DOI 10.1109/TIP.2018.2813165
   Cong RM, 2019, IEEE T CIRC SYST VID, V29, P2941, DOI 10.1109/TCSVT.2018.2870832
   Cornia M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177745
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Fan RC, 2019, PROC CVPR IEEE, P6096, DOI 10.1109/CVPR.2019.00626
   Fang YM, 2019, IEEE T IMAGE PROCESS, V28, P2305, DOI 10.1109/TIP.2018.2885229
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Fortun Denis, 2015, Computer Vision and Image Understanding, V134, P1, DOI 10.1016/j.cviu.2015.02.008
   Fragkiadaki K, 2015, PROC CVPR IEEE, P4083, DOI 10.1109/CVPR.2015.7299035
   Gu YC, 2020, AAAI CONF ARTIF INTE, V34, P10869
   Guo F, 2020, IEEE T CIRC SYST VID, V30, P4887, DOI 10.1109/TCSVT.2019.2906226
   Han JW, 2015, IEEE T CIRC SYST VID, V25, P1309, DOI 10.1109/TCSVT.2014.2381471
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Hui TW, 2021, IEEE T PATTERN ANAL, V43, P2555, DOI 10.1109/TPAMI.2020.2976928
   Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jerripothula KR, 2019, IEEE T CIRC SYST VID, V29, P744, DOI 10.1109/TCSVT.2018.2805811
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   KRAHENBUHL P, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472
   Li GB, 2018, PROC CVPR IEEE, P3243, DOI 10.1109/CVPR.2018.00342
   Li GB, 2016, IEEE T IMAGE PROCESS, V25, P5012, DOI 10.1109/TIP.2016.2602079
   Li HF, 2019, IEEE I CONF COMP VIS, P7273, DOI 10.1109/ICCV.2019.00737
   Li J, 2018, IEEE T IMAGE PROCESS, V27, P349, DOI 10.1109/TIP.2017.2762594
   Li YX, 2020, IEEE T MULTIMEDIA, V22, P1153, DOI 10.1109/TMM.2019.2940851
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu NA, 2020, IEEE T IMAGE PROCESS, V29, P6438, DOI 10.1109/TIP.2020.2988568
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu Z, 2017, IEEE T CIRC SYST VID, V27, P2527, DOI 10.1109/TCSVT.2016.2595324
   Long J., 2015, P IEEE C COMP VIS PA, P3431, DOI DOI 10.48550/ARXIV.1411.4038
   Lu XK, 2019, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2019.00374
   Paszke A, 2019, ADV NEUR IN, V32
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Siam M, 2019, IEEE INT CONF ROBOT, P50, DOI [10.1109/ICRA.2019.8794254, 10.1109/icra.2019.8794254]
   Song HM, 2018, LECT NOTES COMPUT SC, V11215, P744, DOI 10.1007/978-3-030-01252-6_44
   Sucheng Ren, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P212, DOI 10.1007/978-3-030-58558-7_13
   Sugimoto A, 2017, BMVC, V1, P3
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Tang Y, 2019, IEEE T CIRC SYST VID, V29, P1973, DOI 10.1109/TCSVT.2018.2859773
   Le TN, 2018, IEEE T IMAGE PROCESS, V27, P5002, DOI 10.1109/TIP.2018.2849860
   Tsai YH, 2016, PROC CVPR IEEE, P3899, DOI 10.1109/CVPR.2016.423
   Tu WC, 2016, PROC CVPR IEEE, P2334, DOI 10.1109/CVPR.2016.256
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330
   Wang WG, 2020, IEEE T PATTERN ANAL, V42, P1913, DOI 10.1109/TPAMI.2019.2905607
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P2413, DOI 10.1109/TPAMI.2020.2966453
   Wang WG, 2018, PROC CVPR IEEE, P1711, DOI 10.1109/CVPR.2018.00184
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Wang Y, 2018, PROC CVPR IEEE, P4884, DOI 10.1109/CVPR.2018.00513
   Wang Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P873, DOI 10.1145/3343031.3350882
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xi T, 2017, IEEE T IMAGE PROCESS, V26, P3425, DOI 10.1109/TIP.2016.2631900
   Xu MZ, 2020, IEEE T CIRC SYST VID, V30, P2191, DOI 10.1109/TCSVT.2019.2920652
   Yan PX, 2019, IEEE I CONF COMP VIS, P7283, DOI 10.1109/ICCV.2019.00738
   Yao R, 2020, ACM T INTEL SYST TEC, V11, DOI 10.1145/3391743
   Yu F., 2015, ARXIV
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhao SY, 2020, PROC CVPR IEEE, P6277, DOI 10.1109/CVPR42600.2020.00631
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zhou TF, 2020, AAAI CONF ARTIF INTE, V34, P13066
   Zhuo T, 2020, IEEE T IMAGE PROCESS, V29, P237, DOI 10.1109/TIP.2019.2930152
NR 77
TC 4
Z9 4
U1 5
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 557
EP 571
DI 10.1109/TMM.2021.3129052
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800017
DA 2024-07-18
ER

PT J
AU Li, N
   Zhao, XB
AF Li, Na
   Zhao, Xinbo
TI A Strong and Robust Skeleton-Based Gait Recognition Method with Gait
   Periodicity Priors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Skeleton; Gait recognition; Legged locomotion;
   Indexes; Robustness; Convolution; pace; periodicity; skeleton
AB Gait recognition aims to identify people by their walking patterns. Normal human walking is a periodic movement, however, existing gait recognition methods rarely make use of gait periodicity. In this paper, we propose the gait Periodicity-inspired Temporal feature Pyramid aggregator (PTP), which introduces gait periodicity priors into gait feature extraction, resulting in a strong and robust skeleton-based gait recognition method called CycleGait. Specifically, inspired by gait periodicity, PTP adopts multiple parallel temporal convolution operators with pyramid temporal kernel sizes to extract temporal gait features. Then, PTP cooperates with the spatial Graph Convolutional Network (GCN) to form the GCN-PTP network. CycleGait uses this network to extract spatio-temporal gait features from a sequence of skeleton coordinates. In addition, to improve CycleGait's robustness and performance, we feed more gait samples with various gait cycles into CycleGait with the plug-and-play Irregular Pace Converter (IPC), which can automatically convert normal pace into irregular and reasonable paces. Extensive experiments conducted on the CASIA-B dataset and OG RGB+D dataset show that CycleGait has excellent performance in various complex scenarios, namely, cross-view and cross-walking conditions, and becomes one of the best SOTA methods, which not only outperforms the best preexisting gait recognition methods by a large margin but also exhibits a significant level of robustness.
C1 [Li, Na; Zhao, Xinbo] Northwestern Polytech Univ, Natl Engn Lab Integrated Aerosp Ground Ocean Big D, Sch Comp Sci & Engn, Xian 710072, Peoples R China.
C3 Northwestern Polytechnical University
RP Zhao, XB (corresponding author), Northwestern Polytech Univ, Natl Engn Lab Integrated Aerosp Ground Ocean Big D, Sch Comp Sci & Engn, Xian 710072, Peoples R China.
EM cvlina@mail.nwpu.edu.cn; xbozhao@163.com
FU National Natural Science Foundation of China [61871326]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61871326. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr.
   XinxiaoWu.(Corresponding author: Xinbo Zhao.)
CR An WZ, 2018, LECT NOTES COMPUT SC, V10996, P137, DOI 10.1007/978-3-319-97909-0_15
   [Anonymous], 1999, Biometrics: Personal Identification in Networked Society, chapter Automatic gait recognition
   Brown Kramer J, 2020, P IEEE CVF C COMP VI, P294, DOI DOI 10.1109/CVPRW50498.2020.00155
   Chao Fan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14213, DOI 10.1109/CVPR42600.2020.01423
   Chao HQ, 2019, AAAI CONF ARTIF INTE, P8126
   de Lima VC, 2021, PATTERN ANAL APPL, V24, P497, DOI 10.1007/s10044-020-00935-z
   He YW, 2019, IEEE T INF FOREN SEC, V14, P102, DOI 10.1109/TIFS.2018.2844819
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hien D., INTERNET DRAFT
   Jordan K, 2007, GAIT POSTURE, V26, P128, DOI 10.1016/j.gaitpost.2006.08.010
   Khosla P., 2020, ADV NEURAL INF PROCE, V33, P18661
   Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530
   Lakany H, 2008, PATTERN RECOGN, V41, P1627, DOI 10.1016/j.patcog.2007.11.004
   Li N., 2021, ARXIV
   Li N., 2020, Jointsgait: A model-based gait recognition method based on gait graph convolutional networks and joints relationship pyramid mapping
   Liao RJ, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107069
   Luo WJ, 2016, ADV NEUR IN, V29
   Rijun Liao, 2017, Biometric Recognition. 12th Chinese Conference, CCBR 2017. Proceedings: LNCS 10568, P474, DOI 10.1007/978-3-319-69923-3_51
   Smith LN, 2019, PROC SPIE, V11006, DOI 10.1117/12.2520589
   Song YF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1625, DOI 10.1145/3394171.3413802
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Teepe T, 2021, IEEE IMAGE PROC, P2314, DOI 10.1109/ICIP42928.2021.9506717
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu ZF, 2017, IEEE T PATTERN ANAL, V39, P209, DOI 10.1109/TPAMI.2016.2545669
   Xu K, 2021, 2021 IEEE INT C MULT, P1, DOI [10.1109/icme51207.2021.9428123, DOI 10.1109/ICME51207.2021.9428123]
   Xu K, 2021, IEEE T MULTIMEDIA, V24, P3265, DOI 10.1109/TMM.2021.3095809
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yu SQ, 2006, INT C PATT RECOG, P441
   Yu SQ, 2019, PATTERN RECOGN, V87, P179, DOI 10.1016/j.patcog.2018.10.019
   Yu SQ, 2017, IEEE COMPUT SOC CONF, P532, DOI 10.1109/CVPRW.2017.80
   Yu SQ, 2017, NEUROCOMPUTING, V239, P81, DOI 10.1016/j.neucom.2017.02.006
   Zhang ZY, 2019, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR.2019.00484
NR 32
TC 16
Z9 17
U1 4
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3046
EP 3058
DI 10.1109/TMM.2022.3154609
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200008
DA 2024-07-18
ER

PT J
AU Liang, TF
   Jin, Y
   Liu, W
   Li, YD
AF Liang, Tengfei
   Jin, Yi
   Liu, Wu
   Li, Yidong
TI Cross-Modality Transformer With Modality Mining for Visible-Infrared
   Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Task analysis; Feature extraction; Cameras; Convolutional
   neural networks; Lighting; Transformer cores; Cross-modality person
   re-id; modality-aware loss; modality embedding; transformer; VI-ReID
AB The visible-infrared person re-identification (VI-ReID) is a challenging ReID task, which aims to retrieve and match the same identity's images between the heterogeneous visible and infrared modalities. Thus, the core of this task is to bridge the huge gap between these two modalities. The existing methods mainly face the problem of insufficient perception of modality information, and can not learn good discriminative modality-invariant embeddings for identities, which limits their performance. To solve these problems, we propose a new cross-modality transformer-based method (CMTR) for this visible-infrared person re-identification task, which can explicitly mine the information of each modality and generate better discriminative features based on it. Specifically, to capture inherent characteristics of modalities, we design the novel modality embeddings, which are fused with token embeddings to encode modality information directly. Moreover, to enhance representation of modality embeddings and adjust the distribution of embeddings, we further propose a modality-aware enhancement loss based on the learned modality information, which contains two components to reduce intra-class distance and enlarging inter-class distance simultaneously. To our knowledge, this is the first exploration of applying pure transformer network to the cross-modality re-identification task. We implement extensive experiments on the public SYSU-MM01 and RegDB datasets, and compared with previous methods, our method achieves good performance with more compact and powerful embeddings for the cross-modality retrieval.
C1 [Liang, Tengfei; Jin, Yi; Li, Yidong] Beijing Jiaotong Univ, Sch Comp & lnformat Technol, Beijing 100044, Peoples R China.
   [Liu, Wu] JD Explore Acad, Beijing 100176, Peoples R China.
C3 Beijing Jiaotong University
RP Jin, Y (corresponding author), Beijing Jiaotong Univ, Sch Comp & lnformat Technol, Beijing 100044, Peoples R China.
EM tengfei.liang@bjtu.edu.cn; yjin@bjtu.edu.cn; liuwu@live.cn;
   ydli@bjtu.edu.cn
OI Jin, Yi/0000-0001-8408-3816
FU National key research and development program of China
FX No Statement Available
CR Brown T., 2020, P ADV NEUR INF PROC, V33, P1877
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen YHS, 2021, PROC CVPR IEEE, P587, DOI 10.1109/CVPR46437.2021.00065
   Dai J., 2021, ICLR
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Fu CY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11803, DOI 10.1109/ICCV48922.2021.01161
   Gao YJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5257, DOI 10.1145/3474085.3475643
   Ge Y., 2018, P NIPS, P1230
   Hao Y, 2019, AAAI CONF ARTIF INTE, P8385
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Huang Y, 2022, IEEE T MULTIMEDIA, V24, P1570, DOI 10.1109/TMM.2021.3067760
   Li DG, 2020, AAAI CONF ARTIF INTE, V34, P4610
   Li K, 2021, PROC CVPR IEEE, P1944, DOI 10.1109/CVPR46437.2021.00198
   Liu HJ, 2021, IEEE T MULTIMEDIA, V23, P4414, DOI 10.1109/TMM.2020.3042080
   Liu HJ, 2020, NEUROCOMPUTING, V398, P11, DOI 10.1016/j.neucom.2020.01.089
   Loshchilov I., 2019, DECOUPLED WEIGHT DEC
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Mang Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P229, DOI 10.1007/978-3-030-58520-4_14
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Pei JL, 2023, IEEE T MULTIMEDIA, V25, P1964, DOI 10.1109/TMM.2022.3141891
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang GA, 2020, AAAI CONF ARTIF INTE, V34, P12144
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang ZX, 2019, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.2019.00071
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Wu Q, 2021, PROC CVPR IEEE, P4328, DOI 10.1109/CVPR46437.2021.00431
   Yan C, 2022, IEEE T MULTIMEDIA, V24, P1665, DOI 10.1109/TMM.2021.3069562
   Yan Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13376, DOI 10.1109/CVPR42600.2020.01339
   Ye M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1092
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P347, DOI 10.1145/3343031.3351043
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zhu YX, 2020, NEUROCOMPUTING, V386, P97, DOI 10.1016/j.neucom.2019.12.100
   Zeng ZL, 2020, IEEE T MULTIMEDIA, V22, P3064, DOI 10.1109/TMM.2020.2969782
   Zhang GQ, 2022, IEEE T CIRC SYST VID, V32, P6766, DOI 10.1109/TCSVT.2022.3169422
   Zhang GQ, 2021, IEEE T IMAGE PROCESS, V30, P8913, DOI 10.1109/TIP.2021.3120054
   Zhang ZY, 2021, PATTERN RECOGN LETT, V150, P155, DOI 10.1016/j.patrec.2021.07.006
   Zhao JQ, 2023, IEEE T MULTIMEDIA, V25, P3668, DOI 10.1109/TMM.2022.3163847
   Zhao ZW, 2021, AAAI CONF ARTIF INTE, V35, P3520
   Zheng L, 2016, Arxiv, DOI arXiv:1610.02984
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhu ZH, 2020, AAAI CONF ARTIF INTE, V34, P13114
NR 48
TC 4
Z9 4
U1 12
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8432
EP 8444
DI 10.1109/TMM.2023.3237155
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000062
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, DY
   Huang, Y
   Fang, YM
   Zuo, YF
   An, P
AF Liu, Deyang
   Huang, Yan
   Fang, Yuming
   Zuo, Yifan
   An, Ping
TI Multi-Stream Dense View Reconstruction Network for Light Field Image
   Compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Image reconstruction; Redundancy; Three-dimensional
   displays; Correlation; Cameras; Decoding; Deep learning; dense view
   reconstruction; light field image compression; multi-stream network
ID GEOMETRY; STANDARD
AB Recently, many view synthesis-based methods are proposed for high-efficiency light field (LF) image compression. However, most existing methods fail to recover more texture details on occlusion regions, which reduces the compression efficiency. In this paper, we propose a multi-stream dense view reconstruction network to further improve LF image compression performance. In our method, only sparsely-sampled LF views are transmitted and the rest of the views are reconstructed at the decoder side. During the reconstruction process, we firstly constitute a multi-disparity geometry (MDG) structure based on the decoded sparse LF views, which can reflect abundant disparity characteristics. Subsequently, a multi-stream view reconstruction network (MSVRNet) is put forward to reconstruct a high-quality dense LF image, which consists of a multi-scale feature fusion sub-network, a fusion reconstruction sub-network, and a detail refinement sub-network. The multi-scale feature fusion sub-network can implicitly lean abundant multiscale geometric structure features from the constituted MDG structure. The fusion reconstruction sub-network and the detail refinement sub-network are respectively utilized to fuse the learned multiscale geometric features and restore more texture details, especially for occlusion regions. Moreover, 3D convolutional operations are adopted in the whole reconstruction process, which allow information propagation among the learned multiscale geometric features. Comprehensive experimental results demonstrate the effectiveness of the proposed method. The perceptual quality of reconstructed views and application on depth estimation also demonstrate that the proposed method can keep structural consistency of the reconstructed LF image and recover more texture details.
C1 [Liu, Deyang] Anqing Normal Univ, Anqing 246000, Peoples R China.
   [Liu, Deyang] Jiangxi Univ Finance & Econ, Anqing 246000, Peoples R China.
   [Fang, Yuming; Zuo, Yifan] Jiangxi Univ Finance & Econ, Sch Informat Management, Nanchang 330000, Peoples R China.
   [Huang, Yan] Chinese Acad Sci CASIA, Inst Automation, Ctr Res Intelligent Percept & Comp CRIPAC, Natl Lab Pattern Recognit NLPR, Beijing 100045, Peoples R China.
   [An, Ping] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
C3 Anqing Normal University; Jiangxi University of Finance & Economics;
   Jiangxi University of Finance & Economics; Chinese Academy of Sciences;
   Institute of Automation, CAS; Shanghai University
RP Fang, YM (corresponding author), Jiangxi Univ Finance & Econ, Sch Informat Management, Nanchang 330000, Peoples R China.
EM deyang.liu@hotmail.com; yan.huang@cripac.ia.ac.cn;
   leo.fangyuming@foxmail.com; kenny0410@126.com; anping@shu.edu.cn
RI Zuo, Yifan/JVZ-3041-2024; Huang, Yan/N-3447-2018
OI Zuo, Yifan/0000-0003-4980-7211; Huang, Yan/0000-0002-1363-5318
FU National Natural Science Foundation of China [62171002, 61901197,
   62132006]; Shenzhen Municipal Science and Technology Innovation Council
   [2021Szvup051]; STCSM [SKLSFO2021-05]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62171002, 62132006, and 61901197, in
   part by the Shenzhen Municipal Science and Technology Innovation Council
   under Grant 2021Szvup051, and in part by STCSM under Grant
   SKLSFO2021-05.
CR Ahmad W, 2020, IEEE T IMAGE PROCESS, V29, P4269, DOI 10.1109/TIP.2020.2969087
   [Anonymous], 2017, Tech. Rep., ISO/IEC JTC1/SC29/WG11 MPEG2017/N16718
   [Anonymous], 2015, ISO/IEC JTC1/SC29/WG1, WG1N6922
   Astola P, 2019, IEEE ACCESS, V7, P176820, DOI 10.1109/ACCESS.2019.2957934
   Bakir N., 2020, P IEEE INT C MULT EX, P1
   Bakir N, 2021, IEEE T MULTIMEDIA, V23, P2972, DOI 10.1109/TMM.2021.3068563
   Bjontegaard G., 2001, ITU-T VCEG and ISO/IEC MPEG document VCEG-MM33
   Brites C, 2021, IEEE T CIRC SYST VID, V31, P339, DOI 10.1109/TCSVT.2020.2976784
   Chen YL, 2020, IEEE SIGNAL PROC LET, V27, P1135, DOI 10.1109/LSP.2020.3003533
   Choi I, 2019, IEEE I CONF COMP VIS, P7780, DOI 10.1109/ICCV.2019.00787
   Conti C, 2020, IEEE ACCESS, V8, P49244, DOI 10.1109/ACCESS.2020.2977767
   Conti C, 2018, IEEE T MULTIMEDIA, V20, P2905, DOI 10.1109/TMM.2018.2825882
   Dai F, 2015, IEEE IMAGE PROC, P4733, DOI 10.1109/ICIP.2015.7351705
   Alves GD, 2020, IEEE ACCESS, V8, P170807, DOI 10.1109/ACCESS.2020.3024844
   Fiss J, 2014, IEEE INT CONF COMPUT
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou JH, 2019, IEEE T CIRC SYST VID, V29, P517, DOI 10.1109/TCSVT.2018.2802943
   Huang XP, 2022, IEEE T MULTIMEDIA, V24, P152, DOI 10.1109/TMM.2020.3046860
   Huang XP, 2018, IEEE INT CON MULTI
   Huang XP, 2019, OPT EXPRESS, V27, P3557, DOI 10.1364/OE.27.003557
   Ihrke I, 2016, IEEE SIGNAL PROC MAG, V33, P59, DOI 10.1109/MSP.2016.2582220
   ISO/IEC JTC 1/SC29/WG1 N84049 CTQ, 2019, P 84 JPEG M
   Jia CM, 2019, IEEE J EM SEL TOP C, V9, P177, DOI 10.1109/JETCAS.2018.2886642
   Jiang XR, 2017, IEEE INT CONF MULTI
   Jin J, 2020, AAAI CONF ARTIF INTE, V34, P11141
   Jin J, 2022, IEEE T PATTERN ANAL, V44, P1819, DOI 10.1109/TPAMI.2020.3026039
   Jin X, 2018, IEEE T IMAGE PROCESS, V27, P3954, DOI 10.1109/TIP.2018.2832449
   Kalantari NK, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980251
   Kim C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461926
   Ko K, 2021, IEEE T IMAGE PROCESS, V30, P4114, DOI 10.1109/TIP.2021.3069291
   Li L, 2017, IEEE J-STSP, V11, P1107, DOI 10.1109/JSTSP.2017.2725198
   Liu D., 2016, ARXIV PREPRINT ARXIV, P1, DOI DOI 10.1109/ICMEW.2016.7574674
   Liu DY, 2021, INFORM SCIENCES, V545, P118, DOI 10.1016/j.ins.2020.07.073
   Liu DY, 2020, IEEE T COMPUT IMAG, V6, P1507, DOI 10.1109/TCI.2020.3037413
   Liu DY, 2020, IEEE T MULTIMEDIA, V22, P846, DOI 10.1109/TMM.2019.2934426
   Miandji E, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3269980
   Monteiro RJS, 2017, IEEE J-STSP, V11, P1120, DOI 10.1109/JSTSP.2017.2721358
   Mukati M.U., 2020, IEEE INT CONF MULTI, V2020, P1, DOI [DOI 10.1109/icmew46912.2020.9105980, 10.1109/ICMEW46912.2020.9105980]
   Ng Ren, 2018, Lytro redefines photography with light field cameras
   Overbeck RS, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275031
   Peng JY, 2020, IEEE T COMPUT IMAG, V6, P682, DOI 10.1109/TCI.2020.2967148
   Ravishankar J, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21134574
   Rerabek Martin, 2016, P 8 INT C QUAL MULT
   Schiopu I., 2019, APSIPA Trans. Signal Inf. Process., V8, P1
   Senoh T, 2018, EUR SIGNAL PR CONF, P1840, DOI 10.23919/EUSIPCO.2018.8553373
   Sheng H, 2018, PATTERN RECOGN, V74, P587, DOI 10.1016/j.patcog.2017.09.010
   Shi JL, 2020, PROC CVPR IEEE, P2552, DOI 10.1109/CVPR42600.2020.00263
   Singh M, 2021, Arxiv, DOI arXiv:2106.11558
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang J, 2020, IEEE DATA COMPR CONF, P397, DOI 10.1109/DCC47342.2020.00047
   Wang YL, 2020, IEEE T COMPUT IMAG, V6, P830, DOI 10.1109/TCI.2020.2986092
   Wu GC, 2019, IEEE T IMAGE PROCESS, V28, P3261, DOI 10.1109/TIP.2019.2895463
   Wu GC, 2019, IEEE T PATTERN ANAL, V41, P1681, DOI 10.1109/TPAMI.2018.2845393
   Wu GC, 2017, IEEE J-STSP, V11, P926, DOI 10.1109/JSTSP.2017.2747126
   Yeung H. W. F., 2018, P EUR C COMP VIS, P137
   Zhang S, 2019, PROC CVPR IEEE, P11038, DOI 10.1109/CVPR.2019.01130
   Zhao ZH, 2018, IEEE INT CON MULTI
NR 60
TC 10
Z9 10
U1 7
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4400
EP 4414
DI 10.1109/TMM.2022.3175023
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200023
DA 2024-07-18
ER

PT J
AU Liu, S
   Li, AN
   Wang, JH
   Wang, YH
AF Liu, Sheng
   Li, Annan
   Wang, Jiahao
   Wang, Yunhong
TI Bidirectional Maximum Entropy Training With Word Co-Occurrence for Video
   Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video captioning; bidirectional maximum entropy; word co-occurrence
ID ALGORITHM
AB Video captioning aims to generate natural language descriptions for a given video, which is a more challenging task than static image captioning since it requires a more diverse and exhaustive result. Meanwhile, it is also important that the generated captions should be consistent with the language habits of people at a fine granularity. In this work, unlike most recent works enhancing performance with additional data modalities or complex model designs, we focus on optimizing the training process of video captioning models. Firstly, to generate a more diverse video caption, we propose the bidirectional maximum entropy (BME) training, which directly optimizes the probability distribution of neighboring words under a reinforcement learning (RL) framework. Secondly, to search for more human-like captions in the larger search space created by BME, we introduce the word co-occurrence (WCO) weighting. It adaptively guides RL algorithms with co-occurrence statistics in the training corpus. Our method can be deployed on existing captioning models in a plug-and-play manner without introducing any extra parameters. Experimental results show that our method yields up to 5.8% and 7.0% improvements considering the CIDEr score on MSVD and MSR-VTT, respectively.
C1 [Liu, Sheng; Li, Annan; Wang, Jiahao; Wang, Yunhong] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
C3 Beihang University
RP Wang, YH (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM liu_sheng@buaa.edu.cn; annan_li@buaa.edu.cn; jhwang@buaa.edu.cn;
   yhwang@buaa.edu.cn
RI Li, Annan/KRP-2299-2024
OI Li, Annan/0000-0003-3497-5052; Liu, Sheng/0000-0001-9608-0524
FU National Natural Science Foundation of China [U20B2069]
FX This work was supported by the Key Program of National Natural Science
   Foundation of China under Grant U20B2069. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Vasileios Mezaris.
CR Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Banerjee S., 2005, P ACL WORKSH INTR EX, P65
   Baraldi L, 2017, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2017.339
   Ben HX, 2022, IEEE T MULTIMEDIA, V24, P904, DOI 10.1109/TMM.2021.3060948
   Bengio S, 2015, ADV NEUR IN, V28
   Berger AL, 1996, COMPUT LINGUIST, V22, P39
   Boxiao Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10867, DOI 10.1109/CVPR42600.2020.01088
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen H, 2018, AAAI CONF ARTIF INTE, P6706
   Chen L, 2021, PROC CVPR IEEE, P16841, DOI 10.1109/CVPR46437.2021.01657
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8191
   Chen SZ, 2019, IEEE T MULTIMEDIA, V21, P2407, DOI 10.1109/TMM.2019.2896515
   Chen XL, 2015, Arxiv, DOI arXiv:1504.00325
   Chen YY, 2018, LECT NOTES COMPUT SC, V11217, P367, DOI 10.1007/978-3-030-01261-8_22
   Cherian A, 2020, IEEE WINT CONF APPL, P1606, DOI 10.1109/WACV45572.2020.9093291
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Deng JC, 2022, IEEE T CIRC SYST VID, V32, P880, DOI 10.1109/TCSVT.2021.3063423
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dong-Jin Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P718, DOI 10.1007/978-3-030-58589-1_43
   Gao JL, 2019, PROC CVPR IEEE, P6293, DOI 10.1109/CVPR.2019.00646
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Ge HW, 2019, IEEE I CONF COMP VIS, P1754, DOI 10.1109/ICCV.2019.00184
   Haarnoja T, 2019, Arxiv, DOI [arXiv:1812.05905, DOI 10.48550/ARXIV.1812.05905]
   Haarnoja T, 2018, PR MACH LEARN RES, V80
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   JAYNES ET, 1982, P IEEE, V70, P939, DOI 10.1109/PROC.1982.12425
   Krishnan Sanjay, 2018, arXiv
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Li Jiwei, 2016, NAACL, P110
   Li LJ, 2019, IEEE WINT CONF APPL, P339, DOI 10.1109/WACV.2019.00042
   Li XL, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2208
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Lin K, 2021, AAAI CONF ARTIF INTE, V35, P2047
   Liu S, 2021, IEEE T PATTERN ANAL, V43, P3259, DOI 10.1109/TPAMI.2019.2940007
   Liu SQ, 2017, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2017.100
   Ma Y., 2019, FRONT DATA COMPUT, V1, P105, DOI [DOI 10.11871/JFDC, DOI 10.11871/JFDC.ISSN.2096.742X.2019.01.011]
   Meng LX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3146, DOI 10.1145/3394171.3413499
   Pan PB, 2016, PROC CVPR IEEE, P1029, DOI 10.1109/CVPR.2016.117
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Parr R, 1998, ADV NEUR IN, V10, P1043
   Pei WJ, 2019, PROC CVPR IEEE, P8339, DOI 10.1109/CVPR.2019.00854
   Ranzato M, 2016, 4 INT C LEARN REPR I
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Ryu H, 2021, AAAI CONF ARTIF INTE, V35, P2514
   Song JK, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2737
   Song YQ, 2021, PROC CVPR IEEE, P11240, DOI 10.1109/CVPR46437.2021.01109
   Song ZL, 2021, AAAI CONF ARTIF INTE, V35, P2584
   Sun QR, 2013, IEEE IMAGE PROC, P3220, DOI 10.1109/ICIP.2013.6738663
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tillmann C, 2003, COMPUT LINGUIST, V29, P97, DOI 10.1162/089120103321337458
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S., 2015, P ANN C N AM CHAPT A, P1494, DOI 10.3115/v1/N15-1173
   Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795
   Wang JB, 2018, PROC CVPR IEEE, P7512, DOI 10.1109/CVPR.2018.00784
   Wang X, 2018, PROC CVPR IEEE, P4213, DOI 10.1109/CVPR.2018.00443
   Wang Z., 2022, P IEEECVF C COMPUTER, P1819
   Wang ZW, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P659
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
   Wu J, 2021, IEEE T MULTIMEDIA, V23, P2413, DOI 10.1109/TMM.2020.3011317
   Wu N., 2012, The Maximum Entropy Method, V32
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu WR, 2021, IEEE T MULTIMEDIA, V23, P1772, DOI 10.1109/TMM.2020.3002669
   Yan CG, 2021, IEEE T PATTERN ANAL, V43, P1445, DOI 10.1109/TPAMI.2020.2975798
   Yan CG, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3468872
   Yan CG, 2022, IEEE T CIRC SYST VID, V32, P43, DOI 10.1109/TCSVT.2021.3067449
   Yan CG, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3404374
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yan X, 2021, Arxiv, DOI arXiv:2111.10146
   Yang B, 2021, AAAI CONF ARTIF INTE, V35, P3119
   Yang LY, 2021, IEEE T MULTIMEDIA, V23, P835, DOI 10.1109/TMM.2020.2990074
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Yuan CF, 2014, IEEE T IMAGE PROCESS, V23, P658, DOI 10.1109/TIP.2013.2291319
   Zhang J, 2021, IEEE T MULTIMEDIA, V23, P92, DOI 10.1109/TMM.2020.2976552
   Zhang JC, 2020, IEEE T IMAGE PROCESS, V29, P6209, DOI 10.1109/TIP.2020.2988435
   Zhang JC, 2019, PROC CVPR IEEE, P8319, DOI 10.1109/CVPR.2019.00852
   Zhang SY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1292, DOI 10.1145/3394171.3413880
   Zhang ZW, 2021, IEEE T MULTIMEDIA, V23, P1799, DOI 10.1109/TMM.2020.3003592
   Zhang ZQ, 2021, PROC CVPR IEEE, P9832, DOI 10.1109/CVPR46437.2021.00971
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Ziqi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13275, DOI 10.1109/CVPR42600.2020.01329
NR 86
TC 2
Z9 2
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4494
EP 4507
DI 10.1109/TMM.2022.3177308
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200030
DA 2024-07-18
ER

PT J
AU Park, K
   Soh, JW
   Cho, NI
AF Park, Karam
   Soh, Jae Woong
   Cho, Nam Ik
TI A Dynamic Residual Self-Attention Network for Lightweight Single Image
   Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image reconstruction; Computational modeling; Computational efficiency;
   Task analysis; Computer architecture; Three-dimensional displays;
   Performance evaluation; attention mechanism; lightweight network; Single
   image super-resolution
ID ACCURATE
AB Deep learning methods have shown outstanding performance in many applications, including single-image super-resolution (SISR). With residual connection architecture, deeply stacked convolutional neural networks provide a substantial performance boost for SISR, but their huge parameters and computational loads are impractical for real-world applications. Thus, designing lightweight models with acceptable performance is one of the major tasks in current SISR research. The objective of lightweight network design is to balance a computational load and reconstruction performance. Most of the previous methods have manually designed complex and predefined fixed structures, which generally required a large number of experiments and lacked flexibility in the diversity of input image statistics. In this paper, we propose a dynamic residual self-attention network (DRSAN) for lightweight SISR, while focusing on the automated design of residual connections between building blocks. The proposed DRSAN has dynamic residual connections based on dynamic residual attention (DRA), which adaptively changes its structure according to input statistics. Specifically, we propose a dynamic residual module that explicitly models the DRA by finding the interrelation between residual paths and input image statistics, as well as assigning proper weights to each residual path. We also propose a residual self-attention (RSA) module to further boost the performance, which produces 3-dimensional attention maps without additional parameters by cooperating with residual structures. The proposed dynamic scheme, exploiting the combination of DRA and RSA, shows an efficient trade-off between computational complexity and network performance. Experimental results show that the DRSAN performs better than or comparable to existing state-of-the-art lightweight models for SISR.
C1 [Park, Karam; Soh, Jae Woong; Cho, Nam Ik] Seoul Natl Univ, Dept Elect & Comp Engn, INMC, Seoul 08826, South Korea.
   [Cho, Nam Ik] Seoul Natl Univ, Grad Sch Data Sci, INMC, Seoul 08826, South Korea.
C3 Seoul National University (SNU); Seoul National University (SNU)
RP Cho, NI (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, INMC, Seoul 08826, South Korea.
EM saturnian77@ispl.snu.ac.kr; soh90815@ispl.snu.ac.kr; nicho@snu.ac.kr
RI Cho, Nam Ik/I-5029-2014
OI Park, Karam/0000-0002-3612-0077
FU National Research Foundation of Korea (NRF) - Korea government (MSIT)
   [2021R1A2C2007220]; Samsung Electronics Company, Ltd
FX This work was supported in part by the National Research Foundation of
   Korea (NRF) under Grant 2021R1A2C2007220, funded by the Korea government
   (MSIT), and in part by Samsung Electronics Company, Ltd.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   [Anonymous], 2017, P AAAI C ARTIFICIAL
   Behjati P, 2021, IEEE WINT CONF APPL, P2693, DOI 10.1109/WACV48630.2021.00274
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Choi J.-S., 2018, P AS C COMP VIS, P471
   Choi JS, 2017, IEEE COMPUT SOC CONF, P1150, DOI 10.1109/CVPRW.2017.153
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Chu XX, 2021, INT C PATT RECOG, P59, DOI 10.1109/ICPR48806.2021.9413080
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Goto T, 2014, INT C PATT RECOG, P4453, DOI 10.1109/ICPR.2014.762
   Gunturk BK, 2003, IEEE T IMAGE PROCESS, V12, P597, DOI 10.1109/TIP.2003.811513
   Guo Y, 2020, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR42600.2020.00545
   Hang YC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2562, DOI 10.1145/3394171.3413564
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XY, 2019, PROC CVPR IEEE, P1732, DOI 10.1109/CVPR.2019.00183
   He ZW, 2020, IEEE T MULTIMEDIA, V22, P1042, DOI 10.1109/TMM.2019.2937688
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Jiang-Jiang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10093, DOI 10.1109/CVPR42600.2020.01011
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim KI, 2010, IEEE T PATTERN ANAL, V32, P1127, DOI 10.1109/TPAMI.2010.25
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Lan RS, 2021, IEEE T CYBERNETICS, V51, P1443, DOI 10.1109/TCYB.2020.2970104
   Ledig C., 2017, P IEEE C COMP VIS PA, P4681
   Li YC, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414838
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mnih V, 2014, ADV NEUR IN, V27
   Park K., 2021, 25 INT C PATT REC, P1
   Peled S, 2001, MAGN RESON MED, V45, P29, DOI 10.1002/1522-2594(200101)45:1<29::AID-MRM1005>3.0.CO;2-Z
   Qiu YJ, 2019, IEEE I CONF COMP VIS, P4179, DOI 10.1109/ICCV.2019.00428
   Rasti P, 2016, LECT NOTES COMPUT SC, V9756, P175, DOI 10.1007/978-3-319-41778-3_18
   Roy AG, 2018, LECT NOTES COMPUT SC, V11070, P421, DOI 10.1007/978-3-030-00928-1_48
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shi WZ, 2013, LECT NOTES COMPUT SC, V8151, P9, DOI 10.1007/978-3-642-40760-4_2
   Shi Y, 2017, IEEE T MULTIMEDIA, V19, P2804, DOI 10.1109/TMM.2017.2711263
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Thornton MW, 2006, INT J REMOTE SENS, V27, P473, DOI 10.1080/01431160500207088
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Wang CF, 2019, Arxiv, DOI arXiv:1904.02358
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xuehui Wang, 2021, Computer Vision - ACCV 2020. 15th Asian Conference on Computer Vision. Lecture Notes in Computer Science (LNCS 12623), P268, DOI 10.1007/978-3-030-69532-3_17
   Yan YT, 2021, IEEE ACCESS, V9, P52202, DOI 10.1109/ACCESS.2021.3069775
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Yang X, 2021, J VIS COMMUN IMAGE R, V75, DOI 10.1016/j.jvcir.2021.103019
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang LP, 2010, SIGNAL PROCESS, V90, P848, DOI 10.1016/j.sigpro.2009.09.002
   Zhang XY, 2021, IEEE T MULTIMEDIA, V23, P1924, DOI 10.1109/TMM.2020.3005025
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhao H, 2020, COMPUTER VISION ECCV, P56, DOI DOI 10.1007/978-3-030-67070-23
   Zoph B, 2017, PROC 5 INT C LEARN R
NR 64
TC 40
Z9 40
U1 31
U2 111
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 907
EP 918
DI 10.1109/TMM.2021.3134172
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900018
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Shen, QQ
   Yi, SY
   Liang, YS
   Chen, YY
   Liu, W
AF Shen, Qiangqiang
   Yi, Shuangyan
   Liang, Yongsheng
   Chen, Yongyong
   Liu, Wei
TI Bilateral Fast Low-Rank Representation With Equivalent Transformation
   for Subspace Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Null space; Sparse matrices; Streaming media; Singular value
   decomposition; Minimization; Matrix decomposition; Clustering
   algorithms; Distribute; equivalent transformation; fast low-rank;
   subspace clustering
ID MATRIX COMPLETION; ALGORITHM; RECOVERY; SEGMENTATION; RECOGNITION;
   MODELS
AB In recent years, low-rank representation (LRR) has received increasing attention on subspace clustering. Due to inevitable matrix inversion and singular value decomposition in each iteration, however, most of existing LRR algorithms may suffer from high computational complexity, and hence can not cope with the large-scale sample data commendably. To overcome this problem, in this paper, we propose a bilateral fast low-rank representation (BFLRR), which has a linear time complexity with respect to the number of samples. Specifically, we introduce the equivalent transformation method to remove the null spaces of both the columns and rows of the coefficient matrix so that a hypercompact coefficient matrix can be learned. Furthermore, the proposed BFLRR is embedded into a distributed framework as DFC-BFLRR to make it more efficient, which utilizes a combination of the global and local projection matrices. Extensive experiments are carried out on real datasets, and the results testify that the proposed methods not only perform faster-computing speed but also obtain favorable clustering accuracy in comparison with the competing methods among large-scale sample data.
C1 [Shen, Qiangqiang; Liang, Yongsheng] Harbin Inst Technol Shenzhen, Sch Elect & Informat Engn, Shenzhen 518055, Guangdong, Peoples R China.
   [Yi, Shuangyan] Shenzhen Inst Informat Technol, Sch Software Engn, Shenzhen 518055, Guangdong, Peoples R China.
   [Chen, Yongyong] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
   [Chen, Yongyong] Harbin Inst Technol, Guangdong Prov Key Lab Novel Secur Intelligence T, Shenzhen 518055, Guangdong, Peoples R China.
   [Liu, Wei] Harbin Inst Technol, Shenzhen Inst Informat Technol, Shenzhen 518055, Guangdong, Peoples R China.
C3 Harbin Institute of Technology; Shenzhen Institute of Information
   Technology; Harbin Institute of Technology; Harbin Institute of
   Technology; Harbin Institute of Technology; Shenzhen Institute of
   Information Technology
RP Liang, YS (corresponding author), Harbin Inst Technol Shenzhen, Sch Elect & Informat Engn, Shenzhen 518055, Guangdong, Peoples R China.
EM 1120810623@hit.edu.cn; shuangyanshuangfei@163.com; liangys@hit.edu.cn;
   YongyongChen.cn@gmail.com; liuwei@sziit.edu.cn
RI Liu, Wei/L-1951-2019; Chen, yongyong/P-3801-2016
OI Shen, Qiangqiang/0000-0002-3564-6042; Chen, yongyong/0000-0003-1970-1993
FU National Natural Science Foundation of China [62031013, 61871154,
   62106063]; Guangdong Natural Science Foundation [2022A1515010819]; Youth
   Program of National Natural Science Foundation of China [61906124];
   Basic and Applied Basic Research Fund of Guangdong Province
   [2019A1515011307]; Guangdong Provincial Key Laboratory of Novel Security
   Intelligence Technologies [2022B1212010005]; Shenzhen College Stability
   Support Plan [GXWD20201230155427003-20200824113231001]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62031013, 61871154, and 62106063, in
   part by Guangdong Natural Science Foundation under Grant
   2022A1515010819, in part by the Youth Program of National Natural
   Science Foundation of China under Grant 61906124, in part by the Basic
   and Applied Basic Research Fund of Guangdong Province under Grant
   2019A1515011307, in part by the Guangdong Provincial Key Laboratory of
   Novel Security Intelligence Technologies under Grant 2022B1212010005,
   and in part by Shenzhen College Stability Support Plan under Grant
   GXWD20201230155427003-20200824113231001. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publicationwas Prof. Xin Geng.
CR [Anonymous], 2009, Tech. Rep. UILU-ENG- 09-2215
   Bao BK, 2012, IEEE T IMAGE PROCESS, V21, P3794, DOI 10.1109/TIP.2012.2192742
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Chen Y, 2020, PROC CVPR IEEE, P4154, DOI 10.1109/CVPR42600.2020.00421
   Chen YY, 2022, IEEE T MULTIMEDIA, V24, P4054, DOI 10.1109/TMM.2021.3112230
   Chen YY, 2022, IEEE T CIRC SYST VID, V32, P92, DOI 10.1109/TCSVT.2021.3055625
   Deguang Kong, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference (ECML PKDD 2013). Proceedings: LNCS 8189, P177, DOI 10.1007/978-3-642-40991-2_12
   ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Favaro P, 2011, PROC CVPR IEEE, P1801, DOI 10.1109/CVPR.2011.5995365
   Fazel M., 2002, THESIS STANFORD U ST
   Fu ZQ, 2021, PROC CVPR IEEE, P5316, DOI 10.1109/CVPR46437.2021.00528
   Gabay D., 1976, Computers & Mathematics with Applications, V2, P17, DOI 10.1016/0898-1221(76)90003-1
   Glowinski R., 1975, OPTIMIZATION TECHNIQ, P327
   Goldstein T, 2014, SIAM J IMAGING SCI, V7, P1588, DOI 10.1137/120896219
   Hale ET, 2008, SIAM J OPTIMIZ, V19, P1107, DOI 10.1137/070698920
   Hestenes M. R., 1969, Journal of Optimization Theory and Applications, V4, P303, DOI 10.1007/BF00927673
   Hinojosa C, 2018, IEEE J-STSP, V12, P1589, DOI 10.1109/JSTSP.2018.2878293
   Hong M., 2017, PROC 34 INT C MACH L, P1529
   Hong W, 2006, IEEE T IMAGE PROCESS, V15, P3655, DOI 10.1109/TIP.2006.882016
   Horn R. A., 2013, Topics in Matrix Analysis, V2nd
   Hu Y, 2013, IEEE T PATTERN ANAL, V35, P2117, DOI 10.1109/TPAMI.2012.271
   HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440
   Javed S, 2017, IEEE T IMAGE PROCESS, V26, P5840, DOI 10.1109/TIP.2017.2746268
   Kang M, 2015, COMPUT OPTIM APPL, V62, P373, DOI 10.1007/s10589-015-9742-8
   Kang M, 2013, J SCI COMPUT, V56, P515, DOI 10.1007/s10915-013-9686-z
   Ke ZW, 2021, IEEE T MED IMAGING, V40, P3698, DOI 10.1109/TMI.2021.3096218
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Lin Z., 2011, ADV NEURAL INFORM PR, V24, P612, DOI DOI 10.1007/S11263-013-0611-6
   Liu G., 2010, P INT C MACH LEARN, P663
   Liu GC, 2019, IEEE T IMAGE PROCESS, V28, P5161, DOI 10.1109/TIP.2019.2917857
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu GC, 2011, IEEE I CONF COMP VIS, P1615, DOI 10.1109/ICCV.2011.6126422
   Liu YY, 2013, PATTERN RECOGN, V46, P163, DOI 10.1016/j.patcog.2012.07.003
   Lu CY, 2015, AAAI CONF ARTIF INTE, P1805
   Lu CY, 2014, PROC CVPR IEEE, P4130, DOI 10.1109/CVPR.2014.526
   Ma Y, 2007, IEEE T PATTERN ANAL, V29, P1546, DOI 10.1109/TP'AMI.2007.1085
   Mackey L, 2015, J MACH LEARN RES, V16, P913
   Mackey Lester W, 2011, ADV NEURAL INFORM PR, P1134, DOI DOI 10.5555/2986459.2986586
   Mao XH, 2014, 2014 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P453, DOI 10.1109/GlobalSIP.2014.7032158
   Matsushima S, 2019, ADV NEUR IN, V32
   Nie F., 2016, P 25 INT JOINT C ART, P1874
   Nie F., 2012, AAAI, P655
   Nie FP, 2019, IEEE T IMAGE PROCESS, V28, P2378, DOI 10.1109/TIP.2018.2886712
   Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720
   Peng X, 2013, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2013.62
   Powell M. J. D., 1972, A method for nonlinear constraints in minimization problems, in Optimization
   Ruta A, 2012, 2012 11TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2012), VOL 1, P380, DOI 10.1109/ICMLA.2012.69
   Shen QQ, 2022, IEEE T CIRC SYST VID, V32, P1262, DOI 10.1109/TCSVT.2021.3078327
   Shi XS, 2018, NEUROCOMPUTING, V283, P205, DOI 10.1016/j.neucom.2017.12.034
   Talwalkar A, 2013, IEEE I CONF COMP VIS, P3543, DOI 10.1109/ICCV.2013.440
   Tang YQ, 2022, IEEE T MULTIMEDIA, V24, P3920, DOI 10.1109/TMM.2021.3110098
   Tao H, 2020, IEEE T IMAGE PROCESS, V29, P8083, DOI 10.1109/TIP.2020.3010631
   Wakin M. B, 2019, P ADV NEUR INF PROC, P8422
   Wang BY, 2021, IEEE T MULTIMEDIA, V23, P216, DOI 10.1109/TMM.2020.2975394
   Wang YX, 2019, IEEE T INFORM THEORY, V65, P5406, DOI 10.1109/TIT.2019.2915593
   Xia GY, 2021, IEEE T NEUR NET LEAR, V32, P1612, DOI 10.1109/TNNLS.2020.2985817
   Xiao SJ, 2015, PROC CVPR IEEE, P4612, DOI 10.1109/CVPR.2015.7299092
   Xie DY, 2020, IET IMAGE PROCESS, V14, P1475, DOI 10.1049/iet-ipr.2018.6596
   Yang AY, 2008, COMPUT VIS IMAGE UND, V110, P212, DOI 10.1016/j.cviu.2007.07.005
   Yi SY, 2022, IEEE T KNOWL DATA EN, V34, P4812, DOI 10.1109/TKDE.2020.3047405
   Yin M, 2013, IEEE IMAGE PROC, P3770, DOI 10.1109/ICIP.2013.6738777
   Zhang CQ, 2020, IEEE T PATTERN ANAL, V42, P86, DOI 10.1109/TPAMI.2018.2877660
   Zhang CQ, 2017, PROC CVPR IEEE, P4333, DOI 10.1109/CVPR.2017.461
   Zhang HX, 2014, ELECTRON LETT, V50, P936, DOI 10.1049/el.2014.1396
   Zhang X, 2014, IEEE T KNOWL DATA EN, V26, P1293, DOI 10.1109/TKDE.2013.114
   Zheng JW, 2020, IEEE T IMAGE PROCESS, V29, P7861, DOI 10.1109/TIP.2020.3008367
   Zhou P, 2021, IEEE T PATTERN ANAL, V43, P1718, DOI 10.1109/TPAMI.2019.2954874
NR 68
TC 2
Z9 2
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6371
EP 6383
DI 10.1109/TMM.2022.3207922
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500051
DA 2024-07-18
ER

PT J
AU Sun, LY
   Mao, YX
   Zong, TY
   Liu, Y
   Wang, Y
AF Sun, Liyang
   Mao, Yixiang
   Zong, Tongyu
   Liu, Yong
   Wang, Yao
TI Live 360 Degree Video Delivery Based on User Collaboration in a
   Streaming Flock
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimedia communication; virtual reality; machine learning
ID PREDICTION
AB Streaming of live 360-degree video allows users to follow a live event from any view point and has already been deployed on some commercial platforms. However, the current systems can only stream the video at relatively low-quality because the entire 360-degree video is delivered to the users under limited bandwidth. Streaming video falling into user field of view (FoV) can improve bandwidth efficiency of 360-degree video delivery. In this paper, we propose to use the idea of "flocking" to simultaneously improve the accuracy of user FoV prediction and video delivery efficiency for live 360-degree video streaming. By assigning variable playback latencies to users in a streaming session based on their network conditions, a "streaming flock" is formed and led by "strong" users with low playback latencies in the front of the flock. We propose a long short-term memory (LSTM) based collaborative FoV prediction scheme where the FoV traces of users in the front of the flock are utilized to predict the FoV of users behind them. Given a predicted FoV, we develop an optimal rate allocation strategy to maximize the perceptual quality. By conducting experiments using real-world user FoV traces and LTE/5 G network bandwidth traces, we evaluate the gains of the proposed strategies over several benchmarks. Our experimental results demonstrate that the proposed streaming system can increase the overall quality dramatically by about 10 dB compared with heuristic FoV prediction strategy. In addition, the network-aware flocking formation can further reduce the video freeze without influencing video quality.
C1 [Sun, Liyang; Mao, Yixiang; Zong, Tongyu; Liu, Yong; Wang, Yao] NYU, Tandon Sch Engn, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
C3 New York University; New York University Tandon School of Engineering
RP Sun, LY (corresponding author), NYU, Tandon Sch Engn, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
EM ls3817@nyu.edu; yixiang.mao@nyu.edu; tz1178@nyu.edu; yongliu@nyu.edu;
   yw523@nyu.edu
RI Zong, Tongyu/AAW-3884-2021; Sun, Liyang/AAS-1573-2021
OI Sun, Liyang/0000-0001-7593-3815
FU USA NSF [CNS-1816500]
FX & nbsp;This work was supported by USA NSF under Award CNS-1816500.&
   nbsp;& nbsp;
CR Alface PR, 2012, BELL LABS TECH J, V16, P135, DOI 10.1002/bltj.20538
   [Anonymous], 2016, P ACM SIGGRAPH 2016
   [Anonymous], 2017, P 27 NOSSDAV, DOI DOI 10.1145/3083165.3083180
   Ban Y, 2018, SA'18: SIGGRAPH ASIA 2018 EMERGING TECHNOLOGIES, DOI 10.1145/3275476.3275492
   Bao YN, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P1161, DOI 10.1109/BigData.2016.7840720
   Berning Matthias., 2013, P 2013 ACM C PERVASI, P1471
   Cheung G, 2017, IEEE IMAGE PROC, P2179, DOI 10.1109/ICIP.2017.8296668
   Corbillon X, 2017, IEEE ICC, DOI 10.1109/ICC.2017.7996611
   Gaddam VR, 2016, IEEE T MULTIMEDIA, V18, P1819, DOI 10.1109/TMM.2016.2586304
   Guan Y, 2019, SIGCOMM '19 - PROCEEDINGS OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P394, DOI 10.1145/3341302.3342063
   Hosseini M, 2016, IEEE INT SYM MULTIM, P107, DOI [10.1109/ISM.2016.45, 10.1109/ISM.2016.0028]
   Hou XS, 2021, IEEE T MULTIMEDIA, V23, P716, DOI 10.1109/TMM.2020.2987693
   Jeppsson M, 2018, IEEE INT SYM MULTIM, P81, DOI 10.1109/ISM.2018.00022
   Jiang ZQ, 2020, IEEE T VEH TECHNOL, V69, P2157, DOI 10.1109/TVT.2019.2960866
   Kim I., 2014, JCT VC
   Le Feuvre J, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P329, DOI 10.1145/2910017.2910641
   Li CG, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P297, DOI 10.1109/MIPR.2019.00060
   Liu X, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P154, DOI 10.1145/3304109.3306220
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Mao YX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3696, DOI 10.1145/3394171.3413751
   Mayne DQ, 2000, AUTOMATICA, V36, P789, DOI 10.1016/S0005-1098(99)00214-9
   MUX, 2019, LOW LAT LIV STREAM L
   Nasrabadi AT, 2020, NOSSDAV '20: PROCEEDINGS OF THE 2020 WORKSHOP ON NETWORK AND OPERATING SYSTEM SUPPORT FOR DIGITAL AUDIO AND VIDEO, P34, DOI 10.1145/3386290.3396934
   Nasrabadi AT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1689, DOI 10.1145/3123266.3123414
   Nasrabadi AT, 2017, P IEEE VIRT REAL ANN, P347, DOI 10.1109/VR.2017.7892319
   Orduna M, 2020, IEEE T CONSUM ELECTR, V66, P22, DOI 10.1109/TCE.2019.2957987
   Ozcinar C, 2017, IEEE IMAGE PROC, P2174, DOI 10.1109/ICIP.2017.8296667
   Qian F., 2016, Proceedings of the 5th Workshop on All Things Cellular: Operations, Applications and Challenges, P1, DOI DOI 10.1145/2980055.2980056
   Qiao ML, 2021, IEEE T MULTIMEDIA, V23, P748, DOI 10.1109/TMM.2020.2987682
   Raca D, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P303, DOI 10.1145/3339825.3394938
   Skupin R, 2016, IEEE INT SYM MULTIM, P399, DOI [10.1109/ISM.2016.0089, 10.1109/ISM.2016.137]
   Sun LY, 2021, MMSYS '21: PROCEEDINGS OF THE 2021 MULTIMEDIA SYSTEMS CONFERENCE, P200, DOI 10.1145/3458305.3463382
   Sun LY, 2021, IEEE ACM T NETWORK, V29, P2327, DOI 10.1109/TNET.2021.3087625
   Sun LY, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P26, DOI 10.1145/3339825.3391856
   Sun LY, 2019, I C NETWORK PROTOCOL
   Sun LY, 2019, IEEE J EM SEL TOP C, V9, P43, DOI 10.1109/JETCAS.2019.2898877
   Sun LY, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P162, DOI 10.1145/3204949.3204978
   van der Hooft J, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3362101
   Xie XF, 2017, CONEXT'17: PROCEEDINGS OF THE 2017 THE 13TH INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES, P336, DOI 10.1145/3143361.3143381
   Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559
   Yadav PK, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3724, DOI 10.1145/3394171.3413550
   Yaqoob A, 2021, IEEE T BROADCAST, V67, P746, DOI 10.1109/TBC.2021.3105022
   Zare A., 2016, P 24 ACM INT C MULT, P601, DOI DOI 10.1145/2964284.2967292
NR 43
TC 5
Z9 5
U1 2
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2636
EP 2647
DI 10.1109/TMM.2022.3149642
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600014
DA 2024-07-18
ER

PT J
AU Wang, D
   Liu, S
   Wang, Q
   Tian, YM
   He, LH
   Gao, XB
AF Wang, Di
   Liu, Shuai
   Wang, Quan
   Tian, Yumin
   He, Lihuo
   Gao, Xinbo
TI Cross-Modal Enhancement Network for Multimodal Sentiment Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Context modeling; Multimodal sentiment analysis; pre-trained language
   model; transformer
AB Multimodal sentiment analysis (MSA) plays an important role in many applications, such as intelligent question-answering, computer-assisted psychotherapy and video understanding, and has attracted considerable attention in recent years. It leverages multimodal signals including verbal language, facial gestures, and acoustic behaviors to identify sentiments in videos. Language modality typically outperforms nonverbal modalities in MSA. Therefore, strengthening the significance of language in MSA will be a vital way to promote recognition accuracy. Considering that the meaning of a sentence often varies in different nonverbal contexts, combining nonverbal information with text representations is conducive to understanding the exact emotion conveyed by an utterance. In this paper, we propose a Cross-modal Enhancement Network (CENet) model to enhance text representations by integrating visual and acoustic information into a language model. Specifically, it embeds a Cross-modal Enhancement (CE) module, which enhances each word representation according to long-range emotional cues implied in unaligned nonverbal data, into a transformer-based pre-trained language model. Moreover, a feature transformation strategy is introduced for acoustic and visual modalities to reduce the distribution differences between the initial representations of verbal and nonverbal modalities, thereby facilitating the fusion of distinct modalities. Extensive experiments on benchmark datasets demonstrate the significant gains of CENet over state-of-the-art methods.
C1 [Wang, Di; Wang, Quan; Tian, Yumin] Xidian Univ, Sch Comp Sci & Technol, Key Lab Smart Human Comp Interact & Wearable Techn, Xian 710071, Peoples R China.
   [Liu, Shuai] Xidian Univ, Guangzhou Inst Technol, Guangzhou 510555, Peoples R China.
   [He, Lihuo] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
C3 Xidian University; Xidian University; Xidian University
RP Wang, Q (corresponding author), Xidian Univ, Sch Comp Sci & Technol, Key Lab Smart Human Comp Interact & Wearable Techn, Xian 710071, Peoples R China.
EM wangdi@xidian.edu.cn; sliu_4@stu.xidian.edu.cn; qwang@xidian.edu.cn;
   ymtian@mail.xidian.edu.cn; lhhe@mail.xidian.edu.cn;
   xbgao@mail.xidian.edu.cn
OI Wang, Di/0000-0001-8027-4287; Wang, Quan/0000-0001-6913-8604; Liu,
   Shuai/0000-0001-5002-4754; He, Lihuo/0000-0002-0555-3574
FU National Natural Science Foundation of China [62072354, 61972302,
   61876146, 62072355]; Key Research and Development Program of Shannxi
   Province [2022GY-057, 2019ZDLGY13-01, 2021GY-086, 2021GY-014];
   Fundamental Research Funds for Central Universities [JB210305,
   RW210419]; Youth Innovation Team of Shaanxi Universities
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62072354, 61972302, 61876146, and
   62072355, in part by the Key Research and Development Program of Shannxi
   Province under Grants 2022GY-057, 2019ZDLGY13-01, 2021GY-086, and
   2021GY-014, in part by the Fundamental Research Funds for the Central
   Universities under Grants JB210305 and RW210419, and in part by the
   Youth Innovation Team of Shaanxi Universities . The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Ichiro Ide.
CR Alhojely S, 2016, Int. J. Comput. Appl., V2, P22
   [Anonymous], 2015, P 1 INT WORKSH AFF S, DOI 10.1145/2813524.2813530
   Baccianella S, 2010, LREC 2010 - SEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION
   Campos V, 2017, IMAGE VISION COMPUT, V65, P15, DOI 10.1016/j.imavis.2017.01.011
   Chen WX, 2016, IEEE INT SYM MULTIM, P367, DOI [10.1109/ISM.2016.106, 10.1109/ISM.2016.0081]
   Chen YX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P117, DOI 10.1145/3240508.3240533
   Degottex G, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6853739
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Cheong JH, 2021, Arxiv, DOI arXiv:2104.03509
   Irsoy O, 2014, P 2014 C EMPIRICAL M, P720, DOI DOI 10.3115/V1/D14-1080
   Jindal S, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING (ICIP), P447, DOI 10.1109/INFOP.2015.7489424
   Ke P, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P6975
   Kim Y., 2014, P 2014 C EMP METH NA, P1746, DOI [DOI 10.3115/V1/D14-1181, 10.3115/v1/D14-1181]
   Kollias D, 2017, IEEE COMPUT SOC CONF, P1972, DOI 10.1109/CVPRW.2017.247
   Li YC, 2019, INTERSPEECH, P2803, DOI 10.21437/Interspeech.2019-2594
   Liu YH, 2019, Arxiv, DOI arXiv:1907.11692
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Neumann M, 2017, INTERSPEECH, P1263, DOI 10.21437/Interspeech.2017-917
   Pang B, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P79, DOI 10.3115/1118693.1118704
   Pang B., 2004, ANN M ASS COMP LING, P271, DOI [10.3115/1218955.1218990, DOI 10.3115/1218955.1218990]
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Peters M, 2018, STUD LATEINAMERIKA, V32, P1, DOI 10.5771/9783845286846
   Poria S., 2015, P 2015 C EMP METH NA, P2539, DOI DOI 10.18653/V1/D15-1303
   Poria S, 2016, IEEE DATA MINING, P439, DOI [10.1109/ICDM.2016.178, 10.1109/ICDM.2016.0055]
   Radford Alec, 2018, IMPROVING LANGUAGE U, DOI DOI 10.18653/V1/N18-1202
   Rahman W, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2359, DOI 10.18653/v1/2020.acl-main.214
   Schuller B, 2007, INTERSPEECH 2007: 8TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION, VOLS 1-4, P881
   Socher R., 2013, P 2013 C EMP METH NA, V2013, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791
   Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756
   Sun ZK, 2020, AAAI CONF ARTIF INTE, V34, P8992
   Toutanova K, 2003, HLT-NAACL 2003: HUMAN LANGUAGE TECHNOLOGY CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE MAIN CONFERENCE, P252, DOI 10.3115/1073445.1073478
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Tzirakis P, 2021, INFORM FUSION, V68, P46, DOI 10.1016/j.inffus.2020.10.011
   Tzirakis P, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5089, DOI 10.1109/ICASSP.2018.8462677
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang YS, 2019, AAAI CONF ARTIF INTE, P7216
   Wöllmer M, 2008, INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2008, VOLS 1-5, P597
   Xu H, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P1725
   Xu H, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P2324
   Xu MK, 2020, 2020 10TH ANNUAL COMPUTING AND COMMUNICATION WORKSHOP AND CONFERENCE (CCWC), P1058, DOI 10.1109/ccwc47524.2020.9031207
   Yang JF, 2018, IEEE T MULTIMEDIA, V20, P2513, DOI 10.1109/TMM.2018.2803520
   Yang KC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P521, DOI 10.1145/3394171.3413690
   Yang ZL, 2019, ADV NEUR IN, V32
   Yin D, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P3695
   You QZ, 2015, AAAI CONF ARTIF INTE, P381
   Yu WM, 2021, AAAI CONF ARTIF INTE, V35, P10790
   Zadeh A., 2017, P 2017 C EMP METH NA, P1103, DOI 10.18653/v1/D17-1115
   Zadeh A, 2019, Arxiv, DOI arXiv:1911.09826
   Zadeh A, 2016, Arxiv, DOI arXiv:1606.06259
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5642
   Zhang XY, 2016, INT SYMP PARA DISTR, P230, DOI 10.1109/ISPDC.2016.39
NR 54
TC 8
Z9 8
U1 14
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4909
EP 4921
DI 10.1109/TMM.2022.3183830
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300022
DA 2024-07-18
ER

PT J
AU Wang, W
   Gao, JY
   Xu, CS
AF Wang, Wei
   Gao, Junyu
   Xu, Changsheng
TI Weakly-Supervised Video Object Grounding via Learning Uni-Modal
   Associations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Grounding; Task analysis; Prototypes; Annotations;
   Uncertainty; Proposals; Cross-modal retrieval; weakly-supervised
   learning; video object grounding; uni-modal association
ID LANGUAGE
AB Grounding objects described in natural language to visual regions in the video is a crucial capability needed in vision-and-language fields. In this paper, we deal with the weakly-supervised video object grounding (WSVOG) task, where only video-sentence pairs are provided for learning. The essence of this task is to learn the cross-modal associations between words in textual modality and regions in visual modality. Despite the recent progress, we find that most existing methods focus on the association learning for cross-modal samples, while the rich and complementary information within uni-modal samples has not been fully exploited. To this end, we propose to explicitly learn uni-modal associations on both textual and visual sides, so as to fully exploit the useful uni-modal information for accurate video object grounding. Specifically, (1) we learn textual prototypes by considering rich contextual information of the same object in different sentences, and (2) we estimate visual prototypes in an adaptive manner so as to overcome the uncertainties in selecting object-relevant visual regions. Besides, a cross-modal correspondence is learned which not only bridges the visual and textual modalities for WSVOG task, but also tightly cooperates with the uni-modal association learning process. We conduct extensive experiments on three popular datasets, and the favorable results demonstrate the effectiveness of our method.
C1 [Wang, Wei; Gao, Junyu; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Wang, Wei; Gao, Junyu; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Xu, Changsheng] PengCheng Lab, Shenzhen 518066, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM wangwei2018@ia.ac.cn; gaojunyu2015@ia.ac.cn; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; Gao, Junyu/HDO-5516-2022
OI Gao, Junyu/0000-0002-8105-5497; xu, chang sheng/0000-0001-8343-9665
FU National Key Research & Development Plan of China [2020AAA0106200];
   National Natural Science Foundation of China [62036012, 62102415,
   U21B2044, 61721004, 62072286, 61832002, 62072455, 62002355]; Beijing
   Natural Science Foundation [L201001]; Open Research Projects of Zhejiang
   Lab [2022RC0AB02]; CCF-Hikvision Open Fund [20210004]
FX Thisworkwas supported in part by the National Key Research & Development
   Plan of China underGrant 2020AAA0106200, in part by the National Natural
   Science Foundation of China under Grants 62036012, 62102415, U21B2044,
   61721004, 62072286, 61832002, 62072455, and 62002355, in part by Beijing
   Natural Science Foundation under Grant L201001, in part by Open Research
   Projects of Zhejiang Lab under Grant 2022RC0AB02, and in part by
   CCF-Hikvision Open Fund under Grant 20210004.
CR Alomari M, 2017, AAAI CONF ARTIF INTE, P4349
   Chen JW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3789, DOI 10.1145/3394171.3413614
   Chen K, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P23, DOI 10.1145/3078971.3078976
   Chen K, 2018, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2018.00425
   Chen L, 2019, IEEE INT CONF COMP V, P1407, DOI 10.1109/ICCVW.2019.00177
   Da C, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1129, DOI 10.1145/3474085.3481539
   Datta S, 2019, IEEE I CONF COMP VIS, P2601, DOI 10.1109/ICCV.2019.00269
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Gupta T., 2020, P EUROPEAN C COMPUTE, P752
   Hong RC, 2022, IEEE T PATTERN ANAL, V44, P684, DOI 10.1109/TPAMI.2019.2911066
   Huang DA, 2018, PROC CVPR IEEE, P5948, DOI 10.1109/CVPR.2018.00623
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Koh JY, 2021, IEEE WINT CONF APPL, P237, DOI 10.1109/WACV48630.2021.00028
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lei Jie, 2020, P 58 ANN M ASS COMPU, P8211
   Liu XJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P539, DOI 10.1145/3343031.3351074
   Liu Y., 2021, P IEEECVF C COMPUTER, P5612
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Plummer BA, 2017, IEEE I CONF COMP VIS, P1946, DOI 10.1109/ICCV.2017.213
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Plummer BA, 2022, IEEE T PATTERN ANAL, V44, P2155, DOI 10.1109/TPAMI.2020.3029008
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49
   Sadhu Arka, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10414, DOI 10.1109/CVPR42600.2020.01043
   Sener O, 2015, IEEE I CONF COMP VIS, P4480, DOI 10.1109/ICCV.2015.509
   Shi J, 2019, PROC CVPR IEEE, P10436, DOI 10.1109/CVPR.2019.01069
   Sun MJ, 2021, IEEE T PATTERN ANAL, V43, P4189, DOI 10.1109/TPAMI.2021.3058684
   Wang LW, 2021, PROC CVPR IEEE, P14085, DOI 10.1109/CVPR46437.2021.01387
   Wang LW, 2019, IEEE T PATTERN ANAL, V41, P394, DOI 10.1109/TPAMI.2018.2797921
   Wang MZ, 2016, LECT NOTES COMPUT SC, V9912, P696, DOI 10.1007/978-3-319-46484-8_42
   Wang W, 2023, IEEE T PATTERN ANAL, V45, P3933, DOI 10.1109/TPAMI.2022.3180025
   Wang W, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P760, DOI 10.1145/3474085.3475245
   Xiao FY, 2017, PROC CVPR IEEE, P5253, DOI 10.1109/CVPR.2017.558
   Yang X, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1939, DOI 10.1145/3394171.3413610
   Yu H, 2017, INT J COMPUT VISION, V124, P312, DOI 10.1007/s11263-017-1018-6
   Zhang DW, 2022, IEEE T PATTERN ANAL, V44, P5866, DOI 10.1109/TPAMI.2021.3074313
   Zhang DW, 2022, IEEE T PATTERN ANAL, V44, P3349, DOI 10.1109/TPAMI.2020.3046647
   Zhang DW, 2020, IEEE T PATTERN ANAL, V42, P475, DOI 10.1109/TPAMI.2018.2881114
   Zhang YZ, 2022, Arxiv, DOI arXiv:2105.06597
   Zhang Z., 2020, Advances in NIPS, P18123
   Zhang Z, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1069
   Zhao F, 2018, PROC CVPR IEEE, P5696, DOI 10.1109/CVPR.2018.00597
   Zhou LW, 2019, PROC CVPR IEEE, P6571, DOI 10.1109/CVPR.2019.00674
   Zhou Luowei, 2018, BMVC, P50
   Zhu Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10665, DOI 10.1109/CVPR42600.2020.01068
NR 46
TC 2
Z9 2
U1 1
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6329
EP 6340
DI 10.1109/TMM.2022.3207581
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500048
DA 2024-07-18
ER

PT J
AU Weng, TY
   Xiao, J
   Yan, FL
   Jiang, HY
AF Weng, Tingyu
   Xiao, Jun
   Yan, Feilong
   Jiang, Haiyong
TI Context-Aware 3D Point Cloud Semantic Segmentation With Plane Guidance
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Point cloud compression; Geometry; Shape;
   Semantics; Task analysis; Network architecture; 3D semantic
   segmentation; holistic contexts; point cloud; plane geometry
AB Point cloud segmentation is fundamental in understanding 3D environments. However, most existing methods usually perform poorly on identifying boundaries of touching objects and large surfaces of objects. Planes in a scene usually act as supporting surfaces to separate touching objects and provide geometry priors to group points on a large surface as shown in Fig. 1. Besides, planes can roughly represent the structure of a scene, and are more efficient to encode holistic scene contexts than large scale point clouds. In light of the above advantages, we advise a plane-assisted module, coined 3D-PAM, to enhance semantic segmentation of touching objects and large surface objects. 3D-PAM consists of a plane separation network (PS-Net) and a plane relation network (PR-Net). PS-Net focuses on learning features that can robustly separate touching objects, e.g., a chair on a floor, as well as capture plane-based geometry priors to group points on a large plane, e.g., points of a desk. PR-Net encodes mutual plane relations as a proxy of a scene structure to capture holistic contexts. 3D-PAM is designed as a plug-and-play module so that it can be easily plugged into any off-the-shelf semantic segmentation network. Extensive experiments demonstrate that the method achieves large segmentation improvements on several backbones, and accomplishes superior results on most categories when using a RandLA-Net backbone (11/13 categories on S3DIS dataset and 15/20 categories on ScanNetv2 dataset). The project is available at GitHub https://github.com/windmillknight/ Context- Aware- 3DPoint- Cloud- Semantic-Segmentation-With- Plane- Guidance
C1 [Weng, Tingyu; Xiao, Jun; Jiang, Haiyong] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Yan, Feilong] Huya Live, Guangzhou 511442, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Jiang, HY (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
EM wengtingyu18@mails.ucas.ac.cn; xiaojun@ucas.ac.cn; feilongyan@huya.com;
   haiyong.jiang@ucas.ac.cn
OI Xiao, Jun/0000-0002-1799-3948
FU Strategic Priority Research Program of the Chinese Academy of Sciences
   [XDA23090304]; National Natural Science Foundation of China [62271467,
   U2003109, U21A20515, 62102393, 61802362]; Youth Innovation Promotion
   Association of the Chinese Academy of Sciences [Y201935]; State Key
   Laboratory of Robotics and Systems [SKLRS-2022-KF-11]; Fundamental
   Research Funds for the Central Universities
FX This work was supported in part by the Strategic Priority Research
   Program of the Chinese Academy of Sciences under Grant XDA23090304, in
   part by the National Natural Science Foundation of China under Grants
   62271467, U2003109, U21A20515, 62102393, and 61802362, in part by the
   Youth Innovation Promotion Association of the Chinese Academy of
   Sciences under Grant Y201935, in part by the State Key Laboratory of
   Robotics and Systems under Grant SKLRS-2022-KF-11, and in part by the
   Fundamental Research Funds for the Central Universities. The Associate
   Editor coordinating the review of this manuscript and approving it for
   publication was Prof. Sebastian Knorr.
CR Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chen SL, 2020, IEEE T GEOSCI REMOTE, V58, P2530, DOI 10.1109/TGRS.2019.2952086
   Chen ZQ, 2020, PROC CVPR IEEE, P42, DOI 10.1109/CVPR42600.2020.00012
   Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Deng BY, 2020, PROC CVPR IEEE, P31, DOI 10.1109/CVPR42600.2020.00011
   Dzitsiuk Maksym, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3976, DOI 10.1109/ICRA.2017.7989457
   Engelmann F, 2020, IEEE INT CONF ROBOT, P9463, DOI [10.1109/icra40945.2020.9197503, 10.1109/ICRA40945.2020.9197503]
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   Hackel T, 2017, ISPRS ANN PHOTOGRAMM, P91, DOI 10.5194/isprs-annals-IV-1-W1-91-2017
   Jaritz M, 2019, IEEE INT CONF COMP V, P3995, DOI 10.1109/ICCVW.2019.00494
   Jiang L, 2019, IEEE I CONF COMP VIS, P10432, DOI 10.1109/ICCV.2019.01053
   Junbo Yin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11492, DOI 10.1109/CVPR42600.2020.01151
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Li H, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21082651
   Li LX, 2019, PROC CVPR IEEE, P2647, DOI 10.1109/CVPR.2019.00276
   Li YY, 2018, ADV NEUR IN, V31
   Li YY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964947
   Liang ZD, 2019, IEEE INT CONF ROBOT, P8152, DOI [10.1109/icra.2019.8794052, 10.1109/ICRA.2019.8794052]
   Liu C, 2019, PROC CVPR IEEE, P4445, DOI 10.1109/CVPR.2019.00458
   Liu C, 2018, PROC CVPR IEEE, P2579, DOI 10.1109/CVPR.2018.00273
   Loizou M, 2020, COMPUT GRAPH FORUM, V39, P183, DOI 10.1111/cgf.14078
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Meng QH, 2022, IEEE T PATTERN ANAL, V44, P4454, DOI 10.1109/TPAMI.2021.3063611
   Monszpart A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766995
   Nan LL, 2017, IEEE I CONF COMP VIS, P2372, DOI 10.1109/ICCV.2017.258
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qinghao Meng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P515, DOI 10.1007/978-3-030-58601-0_31
   Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schnabel R, 2007, COMPUT GRAPH FORUM, V26, P214, DOI 10.1111/j.1467-8659.2007.01016.x
   Shi YF, 2018, LECT NOTES COMPUT SC, V11212, P767, DOI 10.1007/978-3-030-01237-3_46
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Xia YD, 2018, LECT NOTES COMPUT SC, V11073, P445, DOI 10.1007/978-3-030-00937-3_51
   Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6
   Yiming Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P330, DOI 10.1007/978-3-030-58571-6_20
   Yin JB, 2022, Arxiv, DOI arXiv:2207.12655
   Yin JB, 2023, IEEE T PATTERN ANAL, V45, P9822, DOI 10.1109/TPAMI.2021.3125981
   Zeyu Hu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P222, DOI 10.1007/978-3-030-58565-5_14
NR 45
TC 2
Z9 2
U1 14
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6653
EP 6664
DI 10.1109/TMM.2022.3212914
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500073
DA 2024-07-18
ER

PT J
AU Wu, HC
   Xiao, L
   Sun, L
   Jeon, B
AF Wu, Huicong
   Xiao, Liang
   Sun, Le
   Jeon, Byeungwoo
TI A Novel Video Stabilization Model With Motion Morphological Component
   Priors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cameras; Three-dimensional displays; Motion segmentation; Smoothing
   methods; Hybrid fiber coaxial cables; Trajectory; Visualization;
   Adaptive weight; motion morphological component decomposition; rapid
   motion; video stabilization
AB Video stabilization is the process of improving the video quality by removing annoying fluctuant motion caused by camera jittering. A key issue of a successful solution is the temporal adaptability to motion and the overall robustness with respect to different motion types. However, most previous methods usually produce non-motion adaptive stabilized videos. In other words, under-smoothing in slow motion segments and over-smoothing in rapid motion segments will be produced for complex shaky videos. To overcome these drawbacks, we propose a novel video stabilization approach using a motion morphological component (MMC) decomposition. Specifically, the observed motion is decomposed into three MMCs: low-frequency smoothed (LFS) motion, high-frequency compensatory (HFC) motion, and shaky motion. LFS motion helps to largely stabilize videos, and HFC motion helps to recover missing motion to deal with over-smoothing. Subsequently, we present an MMC-based model to retrieve the desired smoothed motion, in which weighted nuclear norm and autoregression priors are used for LFS motion, while a sparsity prior is adopted for HFC motion. In addition, we design an adaptive weight setting scheme to detect rapid motions and to calculate the optimal weights. Finally, we develop a stabilization algorithm under the Alternating Direction Method of Multipliers (ADMM) framework. Experimental results demonstrate that our method can achieve high-quality results compared with that of other state-of-the-art stabilization methods in terms of robustness and efficiency, both quantitatively and qualitatively.
C1 [Wu, Huicong; Xiao, Liang; Sun, Le] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing, Jiangsu, Peoples R China.
   [Sun, Le] Sungkyunkwan Univ, Sch Elect & Elect Engn, Suwon 440746, South Korea.
C3 Nanjing University of Science & Technology; Sungkyunkwan University
   (SKKU)
RP Xiao, L (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing, Jiangsu, Peoples R China.
EM njust_wuhuicong@163.com; xiaoliang@mail.njust.edu.cn;
   sunlecncom@nuist.edu.cn; bjeon@skku.edu
RI Zhang, Yunyi/JHS-3626-2023; li, yang/IQV-3559-2023; li,
   chunlin/KFS-0761-2024; Liu, Yuan/JFB-4766-2023; WANG,
   YANG/JFA-8821-2023; wang, wei/JBS-7400-2023; xiao, liang/G-2968-2010;
   Wang, Yanlin/JGC-6782-2023; lu, yang/IWE-3635-2023; li,
   jing/JEF-8436-2023; Wang, lili/IXD-9828-2023; liu,
   jiaming/IWE-3196-2023; liu, bing/JJD-5566-2023; wang,
   yingying/JSK-6741-2023; .., What/IXW-6776-2023; LI, XIAO/JCE-6169-2023;
   Yan, Jing/JFA-6705-2023; LI, XIAO/IQV-9318-2023; wang, yu/IUQ-6654-2023
OI xiao, liang/0000-0003-0178-9384; Jeon, Byeungwoo/0000-0002-5650-2881;
   Sun, Le/0000-0001-6465-8678
FU National Natural Science Foundation of China [61571230, 11431015,
   61871226]; National Major Research Plan of China [2016YFF0103604];
   Jiangsu Provincial Natural Science Foundation of China [BK20161500,
   BK20150784]; Jiangsu Provincial Social Developing Project [BE2018727];
   Jiangsu Provincial Qinglan Project of Young and Middle-aged Academic
   Leaders
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61571230, 11431015,and 61871226, in
   part by the National Major Research Plan of China under Grant
   2016YFF0103604, in part by the Jiangsu Provincial Natural Science
   Foundation of China under Grants BK20161500 and BK20150784, in part by
   the Jiangsu Provincial Social Developing Project under Grant BE2018727,
   and in part by the Jiangsu Provincial Qinglan Project of Young and
   Middle-aged Academic Leaders. The Associate Editor coordinating the
   review of this manuscript and approving it for publication was Dr. Jiebo
   Luo.
CR [Anonymous], 2011, 201103 CSTR STANF U
   [Anonymous], Video Stabilization Using Point Feature Matching web page
   Battiato S, 2010, IEEE T MULTIMEDIA, V12, P622, DOI 10.1109/TMM.2010.2060474
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bell S, 2014, LECT NOTES COMPUT SC, V8692, P294, DOI 10.1007/978-3-319-10593-2_20
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Buehler C, 2001, PROC CVPR IEEE, P609
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chang HC, 2006, J VIS COMMUN IMAGE R, V17, P659, DOI 10.1016/j.jvcir.2005.10.004
   Chen YD, 2019, IEEE T MULTIMEDIA, V21, P1934, DOI 10.1109/TMM.2018.2890361
   Cho SI, 2018, IEEE T MULTIMEDIA, V20, P1738, DOI 10.1109/TMM.2017.2781371
   Dai P, 2019, IEEE T MULTIMEDIA, V21, P1709, DOI 10.1109/TMM.2018.2885922
   Deshaker, VID STAB VIRT
   Dong WS, 2017, IEEE T MULTIMEDIA, V19, P293, DOI 10.1109/TMM.2016.2613824
   Fan R, 2017, IEEE T MULTIMEDIA, V19, P893, DOI 10.1109/TMM.2016.2642786
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gleicher M., 2007, P 15 INT C MULTIMEDI, P27, DOI DOI 10.1145/1291233.1291246
   Goldstein A, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2231816.2231824
   Grundmann M, 2013, IEEE INT CONF COMPUT
   Grundmann M, 2011, PROC CVPR IEEE, P225, DOI 10.1109/CVPR.2011.5995525
   Gu SH, 2017, INT J COMPUT VISION, V121, P183, DOI 10.1007/s11263-016-0930-5
   Guo H, 2016, IEEE T IMAGE PROCESS, V25, P5491, DOI 10.1109/TIP.2016.2607419
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P15, DOI 10.1109/TMM.2016.2599145
   Ko SJ, 1998, IEEE T CONSUM ELECTR, V44, P617, DOI 10.1109/30.713172
   Koh YJ, 2015, IEEE T IMAGE PROCESS, V24, P5260, DOI 10.1109/TIP.2015.2479918
   Lee KY, 2009, IEEE I CONF COMP VIS, P1397
   Li J, 2018, IEEE T MULTIMEDIA, V20, P1672, DOI 10.1109/TMM.2017.2777461
   Litvin A, 2003, PROC SPIE, V5022, P663, DOI 10.1117/12.476436
   Liu F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899408
   Liu F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531350
   Liu SC, 2017, IEEE T CIRC SYST VID, V27, P1922, DOI 10.1109/TCSVT.2016.2556587
   Liu SC, 2016, LECT NOTES COMPUT SC, V9910, P800, DOI 10.1007/978-3-319-46466-4_48
   Liu SC, 2014, PROC CVPR IEEE, P4209, DOI 10.1109/CVPR.2014.536
   Liu S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461995
   Liu SC, 2012, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2012.6247662
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matsushita Y, 2006, IEEE T PATTERN ANAL, V28, P1150, DOI 10.1109/TPAMI.2006.141
   Morimoto C, 1998, INT CONF ACOUST SPEE, P2789, DOI 10.1109/ICASSP.1998.678102
   Ratakonda K., 1998, ISCAS '98. Proceedings of the 1998 IEEE International Symposium on Circuits and Systems (Cat. No.98CH36187), P69, DOI 10.1109/ISCAS.1998.698760
   Starck J.L., 2005, Proc. SPIE, V5914, p59140Q, DOI [DOI 10.1117/12.615237, 10.1117/12.615237]
   Tian JD, 2018, IEEE T MULTIMEDIA, V20, P2659, DOI 10.1109/TMM.2018.2808763
   Wang M, 2019, IEEE T IMAGE PROCESS, V28, P2283, DOI 10.1109/TIP.2018.2884280
   Wang YS, 2013, IEEE T VIS COMPUT GR, V19, P1354, DOI 10.1109/TVCG.2013.11
   Wu HC, 2019, IEEE T CIRC SYST VID, V29, P2873, DOI 10.1109/TCSVT.2018.2875671
   Wu HC, 2017, IET IMAGE PROCESS, V11, P465, DOI 10.1049/iet-ipr.2016.0645
   Yang JL, 2009, IEEE T CIRC SYST VID, V19, P945, DOI 10.1109/TCSVT.2009.2020252
   Zhang FL, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2493959
   Zhang GF, 2009, VISUAL COMPUT, V25, P997, DOI 10.1007/s00371-009-0310-z
   Zhang L, 2017, IEEE T IMAGE PROCESS, V26, P2219, DOI 10.1109/TIP.2017.2676354
   Zhang L, 2017, IEEE T CIRC SYST VID, V27, P225, DOI 10.1109/TCSVT.2015.2501941
   Zhou F, 2016, IEEE T PATTERN ANAL, V38, P1492, DOI 10.1109/TPAMI.2016.2526002
   Zhou ZH, 2013, PROC CVPR IEEE, P2299, DOI 10.1109/CVPR.2013.298
NR 54
TC 1
Z9 1
U1 1
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 389
EP 404
DI 10.1109/TMM.2021.3126934
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800005
DA 2024-07-18
ER

PT J
AU Xia, W
   Wang, QQ
   Gao, QX
   Yang, M
   Gao, XB
AF Xia, Wei
   Wang, Qianqian
   Gao, Quanxue
   Yang, Ming
   Gao, Xinbo
TI Self-Consistent Contrastive Attributed Graph Clustering With
   Pseudo-Label Prompt
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Representation learning; Task analysis; SPICE; Self-supervised learning;
   Unsupervised learning; Mutual information; Data models; Graph
   representation learning; node clustering; contrastive learning;
   unsupervised
ID NETWORKS
AB Attributed graph clustering, which learns node representation from node attribute and topological graph for clustering, is a fundamental and challenging task for multimedia network-structured data analysis. Recently, graph contrastive learning (GCL)-based methods have obtained impressive clustering performance on this task. Nevertheless, there still remain some limitations to be solved: 1) most existing methods fail to consider the self-consistency between latent representations and cluster structures; and 2) most methods require a post-processing operation to get clustering labels. Such a two-step learning scheme results in models that cannot handle newly generated data, i.e., out-of-sample (OOS) nodes. To address these issues in a unified framework, a Self-consistent Contrastive Attributed Graph Clustering (SCAGC) network with pseudo-label prompt is proposed in this article. In SCAGC, by clustering labels prompt information, a self-consistent contrastive loss, which aims to maximize the consistencies of intra-cluster representations while minimizing the consistencies of inter-cluster representations, is designed for representation learning. Meanwhile, a clustering module is built to directly output clustering labels by contrasting the representation of different clusters. Thus, for the OOS nodes, SCAGC can directly calculate their clustering labels. Extensive experimental results on seven benchmark datasets have shown that SCAGC consistently outperforms 16 competitive clustering methods.
C1 [Xia, Wei; Wang, Qianqian; Gao, Quanxue] Xidian Univ, Sch Telecommun Engn, Xian 710071, Peoples R China.
   [Yang, Ming] Univ Evansville, Dept Math, Evansville, IN 47722 USA.
   [Gao, Xinbo] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
C3 Xidian University; University of Evansville; Chongqing University of
   Posts & Telecommunications
RP Gao, QX (corresponding author), Xidian Univ, Sch Telecommun Engn, Xian 710071, Peoples R China.
EM xdweixia@gmail.com; qqwang@xidian.edu.cn; qxgao@xidian.edu.cn;
   yangmingmath@gmail.com; gaoxb@cqupt.edu.cn
RI Wang, Qianqian/AHC-6753-2022; Yang, Ming/JCE-4730-2023
OI Wang, Qianqian/0000-0001-8011-171X; Yang, Ming/0000-0003-1810-1566
FU National Natural Science Foundation of China [62176203, 62036007];
   Natural Science Basic Research Plan in Shaanxi Province [2020JZ-19];
   Open Project Program of the National Laboratory of Pattern Recognition
   (NLPR) [202200035]; Natural Science Foundation of Shandong Province
   [ZR202102180986]; Fundamental Research Funds for the Central
   Universities; Innovation Fund of Xidian University; Special Project on
   Technological Innovation and Application Development [cstc2020
   jscx-dxwtB0032]; Chongqing Excellent Scientist Project
   [cstc2021ycjh-bgzxm0339]
FX The work of Wei Xia, Qianqian Wang and Quanxue Gao was supported in part
   by the National Natural Science Foundation of China under Grant
   62176203, in part by the Natural Science Basic Research Plan in Shaanxi
   Province under Grant 2020JZ-19, in part by the Open Project Program of
   the National Laboratory of Pattern Recognition (NLPR) under Grant
   202200035, in part by the Natural Science Foundation of Shandong
   Province under Grant ZR202102180986, in part by the Fundamental Research
   Funds for the Central Universities, and in part by the Innovation Fund
   of Xidian University. The work of Xinbo Gao was supported in part by the
   National Natural Science Foundation of China under Grant 62036007, in
   part by the Special Project on Technological Innovation and Application
   Development under Grant cstc2020 jscx-dxwtB0032, and in part by the
   Chongqing Excellent Scientist Project under Grant
   cstc2021ycjh-bgzxm0339. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Ichiro Ide.
CR Albelwi S, 2022, ENTROPY-SWITZ, V24, DOI 10.3390/e24040551
   Bo DY, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P1400, DOI 10.1145/3366423.3380214
   Bojchevski Aleksandar, 2018, ICLR
   Chen T, 2020, PR MACH LEARN RES, V119
   Cheng JF, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2973
   Cui GQ, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P976, DOI 10.1145/3394486.3403140
   Fan SH, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P3070, DOI 10.1145/3366423.3380079
   Fatemi B., 2021, Advances in Neural Information Processing Systems, V34, P22667
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Han YD, 2020, IEEE T CYBERNETICS, V50, P1697, DOI 10.1109/TCYB.2018.2881539
   Huang C, 2021, AAAI CONF ARTIF INTE, V35, P4115
   Jin M, 2021, P 30 INT JOINT C ART, P1477, DOI [DOI 10.24963/IJCAI.2021/204, 10.24963/ijcai.2021/204]
   Jing LL, 2021, IEEE T PATTERN ANAL, V43, P4037, DOI 10.1109/TPAMI.2020.2992393
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kingma D. P., 2015, P INT C LEARN REPR, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Kipf T. N., 2016, ARXIV161107308, V1050, P21
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Li YF, 2021, AAAI CONF ARTIF INTE, V35, P8547
   Li ZL, 2022, IEEE T MULTIMEDIA, V24, P2461, DOI 10.1109/TMM.2021.3081930
   Lin YJ, 2021, PROC CVPR IEEE, P11169, DOI 10.1109/CVPR46437.2021.01102
   Lin Z., 2021, IJCAI, P2723
   Liu Y, 2022, AAAI CONF ARTIF INTE, P7603
   London B., 2012, P 10 INT WORKSH MIN, V8
   Lv JC, 2021, IEEE T IMAGE PROCESS, V30, P5252, DOI 10.1109/TIP.2021.3079800
   Mao YQ, 2021, AAAI CONF ARTIF INTE, V35, P8893
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P1962, DOI 10.1109/TMM.2020.3006371
   Niu C, 2022, Arxiv, DOI arXiv:2103.09382
   Pan S., 2016, P 25 INT JOINT C ART, V11, P12, DOI DOI 10.1145/2623330.2623732
   Pan SR, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2609
   Pan SR, 2020, IEEE T CYBERNETICS, V50, P2475, DOI 10.1109/TCYB.2019.2932096
   Park J, 2019, IEEE I CONF COMP VIS, P6518, DOI 10.1109/ICCV.2019.00662
   Peng X, 2022, J MACH LEARN RES, V23, P1
   Peng Z, 2022, IEEE T KNOWL DATA EN, V34, P2539, DOI 10.1109/TKDE.2020.3015098
   Peng Z, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P259, DOI 10.1145/3366423.3380112
   Piao JH, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P3146, DOI 10.1145/3442381.3449849
   Qiu JZ, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1150, DOI 10.1145/3394486.3403168
   Shchur O, 2018, ARXIV
   Shi D, 2020, IEEE T NEUR NET LEAR, V31, P4424, DOI 10.1109/TNNLS.2019.2955209
   Sun F. -Y., 2020, PROC INT C LEARN REP
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   Tang J., 2008, SIGKDD, P990, DOI DOI 10.1145/1401890.1402008
   Tu W., 2022, P 31 I JOINT C ART I, P3494
   Tu WX, 2021, AAAI CONF ARTIF INTE, V35, P9978
   Valsesia D, 2021, IEEE T MULTIMEDIA, V23, P402, DOI 10.1109/TMM.2020.2976627
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Velickovic Petar, 2019, ICLR
   Wan S, 2021, AAAI CONF ARTIF INTE, V35, P10049
   Wang C, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3670
   Wang C, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P889, DOI 10.1145/3132847.3132967
   Wang Q, 2022, IEEE T PATTERN ANAL, V44, P390, DOI 10.1109/TPAMI.2020.3007673
   Wang QQ, 2021, IEEE T MULTIMEDIA, V23, P3483, DOI 10.1109/TMM.2020.3025666
   Wen J, 2021, IEEE T MULTIMEDIA, V23, P2493, DOI 10.1109/TMM.2020.3013408
   Wu JL, 2019, IEEE I CONF COMP VIS, P8149, DOI 10.1109/ICCV.2019.00824
   Wu J, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P406, DOI 10.1145/3292500.3330950
   Xia W, 2021, IEEE T MULTIMEDIA, V24, P3182, DOI 10.1109/TMM.2021.3094296
   Xie JY, 2016, PR MACH LEARN RES, V48
   Yang Z, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1666, DOI 10.1145/3394486.3403218
   You Y, 2020, ADV NEURAL INFORM PR, V33, P5812, DOI [10.48550/arXiv.2010.13902, DOI 10.48550/ARXIV.2010.13902]
   Zhang DJ, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P5419
   Zhang R, 2021, Arxiv, DOI arXiv:2106.09244
   Zhang XT, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4327
   Zhao H., 2021, P INT JOINT C ART IN, P3434
   Zhong HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9204, DOI 10.1109/ICCV48922.2021.00909
   Zhu PF, 2023, IEEE T NEUR NET LEAR, V34, P10851, DOI 10.1109/TNNLS.2022.3171583
   Zhu YQ, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P2069, DOI 10.1145/3442381.3449802
NR 65
TC 9
Z9 9
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6665
EP 6677
DI 10.1109/TMM.2022.3213208
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500074
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yang, YH
   Zhang, XZ
   Yang, ML
   Deng, C
AF Yang, Yanhua
   Zhang, Xiaozhe
   Yang, Muli
   Deng, Cheng
TI Adaptive Bias-Aware Feature Generation for Generalized Zero-Shot
   Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Semantics; Generators; Training; Generative adversarial
   networks; Data models; Benchmark testing; Zero-shot learning; generative
   adversarial network; image classification; bias problem
AB Zero-Shot Learning (ZSL) aims to recognize unseen classes that never appear during training. Recently, generative adversarial networks (GANs) have been introduced to convert ZSL into a supervised learning problem by synthesizing unseen visual features. However, since unseen classes are never experienced for the generator during training, the synthesized unseen visual features often become heavily biased towards seen classes, or sometimes there is even no meaningful class that can be assigned to them. This is known as the bias problem. In this paper, we propose a novel method, namely Adaptive Bias-Aware GAN (ABA-GAN), to alleviate generating biased visual features. For this purpose, we build a semantic adversarial network to regularize the feature generator. Specifically, an adaptive adversarial loss is proposed to constrain the feature distributions, which avoids the generation of meaningless visual features. Meanwhile, a domain divider is presented to explicitly distinguish synthesized visual features between seen and unseen domains, such that the bias towards seen classes can be alleviated. Moreover, we propose a novel metric named bias score (BS) to explicitly quantify the degree of the strong bias. Extensive experiments on four widely used benchmark datasets demonstrate that our proposed method outperforms the state-of-the-art approaches under both ZSL and GZSL protocols.
C1 [Yang, Yanhua] Xidian Univ, Sch Comp Sci & Technol, Xian 710071, Peoples R China.
   [Zhang, Xiaozhe; Yang, Muli; Deng, Cheng] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
C3 Xidian University; Xidian University
RP Deng, C (corresponding author), Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
EM yanhyang@xidian.edu.cn; xzhzhang.xd@gmail.com; muliyang.xd@gmail.com;
   chdeng.xd@gmail.com
RI chen, huan/KEC-2019-2024; LIU, Qing Yu/IWV-1159-2023; liu,
   xingwang/KCY-1277-2024; he, xi/JXN-3817-2024; Zhang,
   Lijuan/KAM-0174-2024; Lin, Yi/KEH-1784-2024
OI Yang, Muli/0000-0002-5959-2931
FU National Natural Science Foundation of China [62132016, 62171343,
   62071361, 62102293]; Key Research and Development Program of Shaanxi
   [2021ZDLGY01-03]; Fundamental Research Funds for the Central
   Universities [ZDRC2102]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62132016, 62171343, 62071361, and
   62102293, in part by the Key Research and Development Program of Shaanxi
   under Grant 2021ZDLGY01-03, and in part by the Fundamental Research
   Funds for the Central Universities ZDRC2102.
CR Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986
   Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575
   Chen XY, 2022, IEEE T MULTIMEDIA, V24, P177, DOI 10.1109/TMM.2020.3047546
   Chen Z, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P844, DOI 10.1145/3474085.3475258
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Felix R, 2018, LECT NOTES COMPUT SC, V11210, P21, DOI 10.1007/978-3-030-01231-1_2
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Gao JY, 2020, IEEE T MULTIMEDIA, V22, P3088, DOI 10.1109/TMM.2020.2969787
   Guo JC, 2021, IEEE T MULTIMEDIA, V23, P524, DOI 10.1109/TMM.2020.2984091
   Guo YC, 2017, IEEE T IMAGE PROCESS, V26, P3277, DOI 10.1109/TIP.2017.2696747
   Han ZY, 2021, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR46437.2021.00240
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang H, 2019, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.2019.00089
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jiang HJ, 2019, IEEE I CONF COMP VIS, P9764, DOI 10.1109/ICCV.2019.00986
   Jin Y, 2022, IEEE T MULTIMEDIA, V24, P1896, DOI 10.1109/TMM.2021.3073624
   Kingma D. P., 2014, arXiv
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li JJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1587, DOI 10.1145/3343031.3350901
   Li JJ, 2019, PROC CVPR IEEE, P7394, DOI 10.1109/CVPR.2019.00758
   Liu Y, 2021, PATTERN RECOGN, V118, DOI 10.1016/j.patcog.2021.108024
   Liu Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2490
   Min SB, 2021, IEEE T MULTIMEDIA, V23, P3919, DOI 10.1109/TMM.2020.3033124
   Ni J, 2019, ADV NEUR IN, V32
   Norouzi M, 2014, PROC INT C LEARN REP
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Romera-Paredes B, 2015, PR MACH LEARN RES, V37, P2152
   Song J, 2018, PROC CVPR IEEE, P1024, DOI 10.1109/CVPR.2018.00113
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Verma VK, 2018, PROC CVPR IEEE, P4281, DOI 10.1109/CVPR.2018.00450
   Vyas Maunil R., 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P70, DOI 10.1007/978-3-030-58577-8_5
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Xian YQ, 2019, PROC CVPR IEEE, P10267, DOI 10.1109/CVPR.2019.01052
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Xie GS, 2019, PROC CVPR IEEE, P9376, DOI 10.1109/CVPR.2019.00961
   Yao HT, 2022, IEEE T MULTIMEDIA, V24, P1933, DOI 10.1109/TMM.2021.3074252
   Ye YL, 2022, IEEE T MULTIMEDIA, V24, P1325, DOI 10.1109/TMM.2021.3063616
   Ye ZH, 2019, IEEE INT CON MULTI, P85, DOI 10.1109/ICME.2019.00023
   Yunlong Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14032, DOI 10.1109/CVPR42600.2020.01405
   Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321
   Zhang XX, 2020, IEEE T MULTIMEDIA, V22, P1692, DOI 10.1109/TMM.2019.2959433
   Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474
   Zhu YZ, 2018, PROC CVPR IEEE, P1004, DOI 10.1109/CVPR.2018.00111
NR 49
TC 12
Z9 12
U1 3
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 280
EP 290
DI 10.1109/TMM.2021.3125134
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9E8KM
UT WOS:000937028400020
DA 2024-07-18
ER

PT J
AU Zeng, ZL
   Wang, Z
   Yang, F
   Satoh, SI
AF Zeng, Zelong
   Wang, Zheng
   Yang, Fan
   Satoh, Shin'ichi
TI Geo-Localization via Ground-to-Satellite Cross-View Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retrieval; Task analysis; Feature extraction; Drones; Satellites;
   Location awareness; Representation learning; Cross-view; diffusion;
   image retrieval; representation learning
AB The large variation of viewpoint and irrelevant content around the target always hinder accurate image retrieval and its subsequent tasks. In this paper, we investigate an extremely challenging task: given a ground-view image of a landmark, we aim to achieve cross-view geo-localization by searching out its corresponding satellite-view images. Specifically, the challenge comes from the gap between ground-view and satellite-view, which includes not only large viewpoint changes (some parts of the landmark may be invisible from front view to top view) but also highly irrelevant background (the target landmark tend to be hidden in other surrounding buildings), making it difficult to learn a common representation or a suitable mapping. To address this issue, we take advantage of drone-view information as a bridge between ground-view and satellite-view domains. We propose a Peer Learning and Cross Diffusion (PLCD) framework. PLCD consists of three parts: 1) a peer learning across ground-view and drone-view to find visible parts to benefit ground-drone cross-view representation learning; 2) a patch-based network for satellite-drone cross-view representation learning; and 3) a cross diffusion between ground-drone space and satellite-drone space. Extensive experiments conducted on the University-Earth and University-Google datasets show that our method outperforms state-of-the-arts significantly.
C1 [Zeng, Zelong; Yang, Fan; Satoh, Shin'ichi] Univ Tokyo, Grad Sch Informat Sci & Technol, Dept Informat & Commun Engn, Bunkyo Ku, Tokyo 1018430, Japan.
   [Zeng, Zelong; Yang, Fan; Satoh, Shin'ichi] Natl Inst Informat, Digital Content & Media Sci Res Div, Tokyo, Japan.
   [Wang, Zheng] Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Sch Comp Sci, Wuhan 430072, Peoples R China.
C3 University of Tokyo; Research Organization of Information & Systems
   (ROIS); National Institute of Informatics (NII) - Japan; Wuhan
   University
RP Wang, Z (corresponding author), Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Sch Comp Sci, Wuhan 430072, Peoples R China.
EM zzlbz@nii.ac.jp; wangzwhu@whu.edu.cn; yang@nii.ac.jp; satoh@nii.ac.jp
RI Zeng, Zelong/ITU-9384-2023
OI Satoh, Shin'ichi/0000-0001-6995-6447
FU National Key R&D Program of China [2021YFC3320301]; National Natural
   Science Foundation of China [62171325]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2021YFC3320301 and in part by the National Natural Science
   Foundation of China under Grant 62171325.
CR [Anonymous], 2016, PROC INT C LEARN REP
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Bai S, 2019, IEEE T PATTERN ANAL, V41, P1213, DOI 10.1109/TPAMI.2018.2828815
   Bai S, 2017, IEEE I CONF COMP VIS, P774, DOI 10.1109/ICCV.2017.90
   Cai SD, 2019, IEEE I CONF COMP VIS, P8390, DOI 10.1109/ICCV.2019.00848
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Furlanello T., 2018, P MACHINE LEARNING R, V80, P1607, DOI DOI 10.48550/ARXIV.1805.04770
   Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15
   Gu XL, 2019, IEEE T MULTIMEDIA, V21, P1524, DOI 10.1109/TMM.2018.2876822
   Guo XX, 2018, ADV NEUR IN, V31
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hu MS, 2022, IEEE T CIRC SYST VID, V32, P3390, DOI 10.1109/TCSVT.2021.3110796
   Hu SX, 2018, PROC CVPR IEEE, P7258, DOI 10.1109/CVPR.2018.00758
   Hu SY, 2020, Arxiv, DOI arXiv:2006.13681
   Huang WX, 2021, NEURAL COMPUT APPL, V33, P961, DOI 10.1007/s00521-020-05314-7
   Jiang K, 2021, IEEE T IMAGE PROCESS, V30, P7404, DOI 10.1109/TIP.2021.3102504
   Jiang K, 2021, IEEE T CIRC SYST VID, V31, P3981, DOI 10.1109/TCSVT.2020.3044887
   Kansal K, 2020, IEEE T CIRC SYST VID, V30, P3422, DOI 10.1109/TCSVT.2019.2963721
   Liu L, 2019, IEEE I CONF COMP VIS, P2570, DOI 10.1109/ICCV.2019.00266
   Liu L, 2019, PROC CVPR IEEE, P5607, DOI 10.1109/CVPR.2019.00577
   Lu X, 2020, IEEE T MULTIMEDIA, V22, P2048, DOI 10.1109/TMM.2019.2947358
   Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374
   Olszewska J. I., 2016, ICAART 2016. 8th International Conference on Agents and Artificial Intelligence. Proceedings, P566
   Olszewska JI, 2017, ICAART: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON AGENTS AND ARTIFICIAL INTELLIGENCE, VOL 2, P687, DOI 10.5220/0006253706870692
   Pang K., 2017, BMVC, P1
   Qizhe Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10684, DOI 10.1109/CVPR42600.2020.01070
   Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566
   Ren S., 2015, ADV NEURAL INFORM PR, V28, P91
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shi Y, 2019, P NEUR INF PROC SYST, V32, P10090
   Shi YJ, 2020, AAAI CONF ARTIF INTE, V34, P11990
   Song JF, 2017, IEEE I CONF COMP VIS, P5552, DOI 10.1109/ICCV.2017.592
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Toman J, 2014, INT SYMP COMP INTELL, P55, DOI 10.1109/CINTI.2014.7028728
   Verma A, 2023, IEEE T MULTIMEDIA, V25, P364, DOI 10.1109/TMM.2021.3126404
   Vo N, 2019, PROC CVPR IEEE, P6432, DOI 10.1109/CVPR.2019.00660
   Vo NN, 2016, LECT NOTES COMPUT SC, V9905, P494, DOI 10.1007/978-3-319-46448-0_30
   Wang TY, 2022, IEEE T CIRC SYST VID, V32, P867, DOI 10.1109/TCSVT.2021.3061265
   Workman S, 2015, IEEE I CONF COMP VIS, P3961, DOI 10.1109/ICCV.2015.451
   Xu X, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414839
   Yang F, 2020, AAAI CONF ARTIF INTE, V34, P12589
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P2347, DOI 10.1109/TMM.2020.3009476
   Zeng ZL, 2020, IEEE T MULTIMEDIA, V22, P3064, DOI 10.1109/TMM.2020.2969782
   Zhai M, 2017, PROC CVPR IEEE, P4132, DOI 10.1109/CVPR.2017.440
   Zhang YT, 2016, IEEE T MULTIMEDIA, V18, P1604, DOI 10.1109/TMM.2016.2568138
   Zheng CQ, 2021, IEEE T MULTIMEDIA, V23, P4079, DOI 10.1109/TMM.2020.3037456
   Zheng ZD, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1395, DOI 10.1145/3394171.3413896
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zhong X, 2022, IEEE T CIRC SYST VID, V32, P1418, DOI 10.1109/TCSVT.2021.3072171
NR 50
TC 12
Z9 12
U1 5
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2176
EP 2188
DI 10.1109/TMM.2022.3144066
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100045
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, K
   Long, M
   Chen, J
   Liu, MZ
   Li, JJ
AF Zhang, Ke
   Long, Miao
   Chen, Jie
   Liu, Mingzhu
   Li, Jingjing
TI CFPNet: A Denoising Network for Complex Frequency Band Signal Processing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise reduction; Image denoising; Feature extraction; Frequency-domain
   analysis; Discrete cosine transforms; Convolutional neural networks;
   Signal processing; Image noise reduction; cosine transform; multi-scale;
   image denoising
ID IMAGE; SPARSE; ALGORITHM
AB The recent development of deep learning has brought breakthroughs in image denoising. However, the recovery of image detail, especially high-frequency weak information, still needs to be improved. Firstly, the noise mainly concentrates on the high-frequency signal, and the high-frequency signal is easy to be disturbed, which makes it difficult to recover; Secondly, in the process of image denoising with deep learning, feature extraction of model is used to smooth the noise for image restoration, resulting in a poor recovery effect of high-frequency signal. To solve the above problems and improve the overall image denoising performance, we propose a denoising network for complex frequency band signal processing (CFPNet), which contains three insights: 1) the image input node uses a cosine transform to segment the image noise frequency and divides different image features into signals in different frequency bands for targeted noise reduction; 2) targeted noise reduction is carried out for different frequency band signals via a fine-grained scheme; 3) different frequency band signals are fused and high-frequency signals are enhanced to improve the recovery of detailed signals. The experimental results show that the proposed CFPNet can achieve state-of-the-art performance on both real-world datasets and Gaussian noise fitting datasets.
C1 [Zhang, Ke; Long, Miao; Chen, Jie; Li, Jingjing] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610056, Peoples R China.
   [Zhang, Ke; Long, Miao; Chen, Jie; Li, Jingjing] Yangtze Delta Reg Inst Univ Elect Sci & Technol Ch, Huzhou 610056, Peoples R China.
   [Liu, Mingzhu] Univ Michigan, Ann Arbor, MI 48109 USA.
C3 University of Electronic Science & Technology of China; University of
   Michigan System; University of Michigan
RP Li, JJ (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610056, Peoples R China.; Li, JJ (corresponding author), Yangtze Delta Reg Inst Univ Elect Sci & Technol Ch, Huzhou 610056, Peoples R China.
EM kezhang@uestc.edu.cn; 202052081119@std.uestc.edu.cn;
   202052081118@std.uestc.edu.cn; mingzliu@umich.edu; lijin117@yeah.net
RI Li, Jingjing/T-6522-2019; Liu, Mingzhu/AAZ-1332-2020
OI Liu, Mingzhu/0000-0003-2973-5361; Liu, Mingzhu/0000-0002-9905-2673
FU National Natural Science Foundation of China
FX No Statement Available
CR Abdelhamed A, 2018, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2018.00182
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Anwar S, 2019, IEEE I CONF COMP VIS, P3155, DOI 10.1109/ICCV.2019.00325
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952
   Chen HA, 2022, IEEE T MULTIMEDIA, V24, P2164, DOI 10.1109/TMM.2021.3077140
   Chen LY, 2022, LECT NOTES COMPUT SC, V13667, P17, DOI 10.1007/978-3-031-20071-7_2
   Cheng S, 2021, PROC CVPR IEEE, P4894, DOI 10.1109/CVPR46437.2021.00486
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dabov K, 2006, PROC SPIE, V6064, DOI 10.1117/12.643267
   Deng J, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P3099, DOI 10.1145/2556288.2557011
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Guan JT, 2019, IEEE ACCESS, V7, P44544, DOI 10.1109/ACCESS.2019.2908720
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   Jing MM, 2023, IEEE T MULTIMEDIA, V25, P2559, DOI 10.1109/TMM.2022.3148592
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Khaw HY, 2017, IET IMAGE PROCESS, V11, P1238, DOI 10.1049/iet-ipr.2017.0374
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D, 2014, ICLR P, V2014, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lefkimmiatis S, 2017, PROC CVPR IEEE, P5882, DOI 10.1109/CVPR.2017.623
   Li JJ, 2022, IEEE T KNOWL DATA EN, V34, P5770, DOI 10.1109/TKDE.2021.3060473
   Li JJ, 2022, IEEE T PATTERN ANAL, V44, P8196, DOI 10.1109/TPAMI.2021.3109287
   Li JJ, 2021, IEEE T PATTERN ANAL, V43, P3918, DOI 10.1109/TPAMI.2020.2991050
   Li LJ, 2018, 2018 NINTH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY IN MEDICINE AND EDUCATION (ITME 2018), P105, DOI 10.1109/ITME.2018.00033
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lin K, 2019, IEEE COMPUT SOC CONF, P1717, DOI 10.1109/CVPRW.2019.00221
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu PJ, 2018, IEEE COMPUT SOC CONF, P886, DOI 10.1109/CVPRW.2018.00121
   Liu W, 2020, IEEE COMPUT SOC CONF, P1742, DOI 10.1109/CVPRW50498.2020.00224
   Loshchilov I, 2017, P 5 INT C LEARN REPR
   Ma RJ, 2022, IEEE T MULTIMEDIA, V24, P2366, DOI 10.1109/TMM.2021.3079697
   Mao XJ, 2016, ADV NEUR IN, V29
   PITAS I, 1986, IEEE T ACOUST SPEECH, V34, P573, DOI 10.1109/TASSP.1986.1164857
   Plötz T, 2018, ADV NEUR IN, V31
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K., 2014, CORR
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Xu J, 2018, LECT NOTES COMPUT SC, V11212, P21, DOI 10.1007/978-3-030-01237-3_2
   Xu J, 2018, IEEE T IMAGE PROCESS, V27, P2996, DOI 10.1109/TIP.2018.2811546
   Yang D, 2018, IEEE SIGNAL PROC LET, V25, P55, DOI 10.1109/LSP.2017.2768660
   Yang JY, 2017, IEEE IMAGE PROC, P2418, DOI 10.1109/ICIP.2017.8296716
   Yu S, 2019, IEEE COMPUT SOC CONF, P2095, DOI 10.1109/CVPRW.2019.00262
   Yue ZS, 2019, ADV NEUR IN, V32
   Zamir Syed Waqas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P492, DOI 10.1007/978-3-030-58595-2_30
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
   Zongsheng Yue, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P41, DOI 10.1007/978-3-030-58607-2_3
   Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278
NR 55
TC 2
Z9 2
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8212
EP 8224
DI 10.1109/TMM.2022.3233398
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000011
DA 2024-07-18
ER

PT J
AU Zhang, XY
   Zhang, SM
   Cui, Z
   Li, ZC
   Xie, J
   Yang, J
AF Zhang, Xiaoya
   Zhang, Shumin
   Cui, Zhen
   Li, Zechao
   Xie, Jin
   Yang, Jian
TI Tube-Embedded Transformer for Pixel Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Depth estimation; multi-task pixel learning; segmentation; tube-embedded
   transformer
AB Multi-task pixel-level learning, which aims to exploit the inter-task interactions to improve the learning of each task, is an important but challenging issue in visual perception and multimedia applications. Measuring the inter-task correlation and intra-task specificity, we propose a tube-embedded transformer (TET) framework for robust multi-task pixel prediction. To facilitate inter-task interactions, we aggregate and project all tasks into a shared tube pool to generate the latent multi-task representation during the coarse-to-fine decoding stages. The resulting task-tube interactions replace the two-by-two task-task interactions to reduce the model complexity significantly. In addition, we introduce the transformer mechanism to adaptively transfer tube features to the target task. Concretely, on the one hand, multi-task features aggregate in the tube to generate the shared feature representation bases; on the other hand, based on the task-tube association and complementarity, the tube outputs the query entry and the weighting coefficients of the target task. Experimentally, on the joint learning of semantic segmentation, depth estimation, and surface normal estimation, the comparison experiments show the superiority of the TET multi-task learning method over other state-of-the-art approaches, and the ablation experiments verify the effectiveness of the TET mechanism.
C1 [Zhang, Xiaoya; Zhang, Shumin; Cui, Zhen; Li, Zechao; Xie, Jin; Yang, Jian] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China.
C3 Nanjing University of Science & Technology
RP Cui, Z (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China.
EM zhangxiaoya@njust.edu.cn; shumin.zhang@njust.edu.cn;
   zhen.cui@njust.edu.cn; zechao.li@njust.edu.cn; csjxie@njust.edu.cn;
   csjyang@njust.edu.cn
RI li, xiaomin/KCX-9845-2024; WANG, SHIHAO/KHC-8263-2024
FU National Natural Science Foundation of China [62072244, 61876084];
   Natural Science Foundation of Jiangsu Province [BK20190019, BK20210328];
   Fundamental Research Funds for the Central Universities [30921011104]
FX & nbsp;This work was supported in part by the National Natural Science
   Foundation of China under Grants 62072244 and 61876084, in part by the
   Natural Science Foundation of Jiangsu Province under Grants BK20190019
   and BK20210328, and in part by the Fundamental Research Funds for the
   Central Universities under Grant 30921011104.& nbsp;
CR Badki A, 2020, PROC CVPR IEEE, P1597, DOI 10.1109/CVPR42600.2020.00167
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bansal A, 2016, PROC CVPR IEEE, P5965, DOI 10.1109/CVPR.2016.642
   Chen CF, 2021, IEEE T MULTIMEDIA, V23, P2335, DOI 10.1109/TMM.2020.3009499
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng YH, 2017, PROC CVPR IEEE, P1475, DOI 10.1109/CVPR.2017.161
   Deng Z, 2015, IEEE I CONF COMP VIS, P1733, DOI 10.1109/ICCV.2015.202
   Du C, 2022, IEEE T MULTIMEDIA, V24, P2018, DOI 10.1109/TMM.2021.3075025
   Eigen D, 2014, ADV NEUR IN, V27
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Evgeniou T., 2004, P 10 ACM SIGKDD INT, P109, DOI DOI 10.1145/1014052.1014067
   Fouhey DF, 2013, IEEE I CONF COMP VIS, P3392, DOI 10.1109/ICCV.2013.421
   Fouhey DF, 2014, LECT NOTES COMPUT SC, V8694, P687, DOI 10.1007/978-3-319-10599-4_44
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Geden M, 2020, AAAI CONF ARTIF INTE, V34, P654
   Guo P., 2020, ICML, P3854
   He K., 2017, IEEE C COMP VIS PATT, P2961, DOI DOI 10.1109/ICCV.2017.322
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kendall Alex, 2017, ADV NEURAL INFORM PR, V30, DOI DOI 10.5555/3295222.3295309
   Kundu JN, 2018, PROC CVPR IEEE, P2656, DOI 10.1109/CVPR.2018.00281
   Ladicky L, 2014, LECT NOTES COMPUT SC, V8693, P468, DOI 10.1007/978-3-319-10602-1_31
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Li B, 2015, PROC CVPR IEEE, P1119, DOI 10.1109/CVPR.2015.7298715
   Li Y, 2019, IEEE T MULTIMEDIA, V21, P875, DOI 10.1109/TMM.2018.2867720
   Liao S, 2019, PROC CVPR IEEE, P9751, DOI 10.1109/CVPR.2019.00999
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin GS, 2016, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2016.348
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo X, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392377
   Ma JQ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1930, DOI 10.1145/3219819.3220007
   Mousavian A, 2016, INT CONF 3D VISION, P611, DOI 10.1109/3DV.2016.69
   Park SJ, 2017, IEEE I CONF COMP VIS, P4990, DOI 10.1109/ICCV.2017.533
   Qi XJ, 2018, PROC CVPR IEEE, P283, DOI 10.1109/CVPR.2018.00037
   Qi XJ, 2017, IEEE I CONF COMP VIS, P5209, DOI 10.1109/ICCV.2017.556
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruder S, 2017, Arxiv, DOI [arXiv:1706.05098, 10.48550/arXiv.1706.05098]
   Ruder S, 2019, AAAI CONF ARTIF INTE, P4822
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Song WF, 2020, IEEE T MULTIMEDIA, V22, P1220, DOI 10.1109/TMM.2019.2941776
   Tao F, 2021, IEEE T MULTIMEDIA, V23, P1, DOI 10.1109/TMM.2020.2975922
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang P, 2016, ADV NEUR IN, V29
   Wang P, 2015, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2015.7298897
   Wang WY, 2018, LECT NOTES COMPUT SC, V11215, P144, DOI 10.1007/978-3-030-01252-6_9
   Wang XL, 2015, PROC CVPR IEEE, P539, DOI 10.1109/CVPR.2015.7298652
   Xu D, 2018, PROC CVPR IEEE, P675, DOI 10.1109/CVPR.2018.00077
   Xu D, 2018, PROC CVPR IEEE, P3917, DOI 10.1109/CVPR.2018.00412
   Xu D, 2017, PROC CVPR IEEE, P161, DOI 10.1109/CVPR.2017.25
   Zhang R, 2021, IEEE T PATTERN ANAL, V43, P2150, DOI 10.1109/TPAMI.2020.3007637
   Zhang Y, 2022, IEEE T KNOWL DATA EN, V34, P5586, DOI 10.1109/TKDE.2021.3070203
   Zhang ZY, 2019, PROC CVPR IEEE, P4101, DOI 10.1109/CVPR.2019.00423
   Zhang ZY, 2020, IEEE T PATTERN ANAL, V42, P2608, DOI 10.1109/TPAMI.2019.2926728
   Zhang ZY, 2018, LECT NOTES COMPUT SC, V11214, P238, DOI 10.1007/978-3-030-01249-6_15
   Zhao SC, 2017, IEEE T MULTIMEDIA, V19, P632, DOI 10.1109/TMM.2016.2617741
   Zhong L. W., 2012, P INT C MACH LEARN, P483
   Zhou L, 2020, PROC CVPR IEEE, P4513, DOI 10.1109/CVPR42600.2020.00457
   Zhou Q, 2016, IEEE T PATTERN ANAL, V38, P266, DOI 10.1109/TPAMI.2015.2452911
NR 58
TC 7
Z9 7
U1 4
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2503
EP 2514
DI 10.1109/TMM.2022.3147664
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600004
DA 2024-07-18
ER

PT J
AU Zhou, KL
   Zhao, LP
   Ye, ZG
   Wang, HH
   Lin, T
   Feng, S
   Yang, YF
AF Zhou, Kailun
   Zhao, Liping
   Ye, Zigao
   Wang, Huihui
   Lin, Tao
   Feng, Sheng
   Yang, Yufen
TI Equal Value String and Copy Above String Based String Prediction for SCC
   in AVS3
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio video coding standard; screen content coding; string prediction;
   equal value string; 4:2:0 format
ID MATCHING APPROACH; SCREEN; COMPRESSION; COMPLEXITY
AB String prediction (SP) is a very efficient screen content coding (SCC) tool which has been adopted in the third generation of Audio Video Standard (AVS3). It is observed that two special types of strings occur frequently. To further improve the coding efficiency for SCC on top of the original SP, a new variation of SP named Equal-value-string and Copy-above-string based SP (ECSP) is proposed. An ECSP coding unit uses only three types of strings: Equal-value-string, Copy-above-string, and Unpredictable-pixel-string. Compared with the AVS3 reference software HPM9.0 with ECSP disabled, using AVS3 SCC Common Test Condition and YUV 4:2:0 test sequences, the proposed technique achieves an average Y BD-rate reduction of 5.54 and 3.01% for All Intra and Low Delay configurations, respectively, with low additional encoding and decoding complexity. The proposed ECSP has been adopted in the AVS3 standard.
C1 [Zhou, Kailun; Ye, Zigao; Wang, Huihui; Lin, Tao; Yang, Yufen] Tongji Univ, Coll Elect & Informat Engn, VLSI Lab, Shanghai 200092, Peoples R China.
   [Zhao, Liping; Feng, Sheng] Shaoxing Univ, Dept Comp Sci & Engn, Shaoxing 312000, Peoples R China.
   [Zhao, Liping] Peking Univ, Informat Technol R&D Innovat Ctr, Shaoxing 312000, Peoples R China.
C3 Tongji University; Shaoxing University; Peking University
RP Lin, T (corresponding author), Tongji Univ, Coll Elect & Informat Engn, VLSI Lab, Shanghai 200092, Peoples R China.; Zhao, LP (corresponding author), Shaoxing Univ, Dept Comp Sci & Engn, Shaoxing 312000, Peoples R China.
EM kailun_zh@tongji.edu.cn; zhaoliping_jian@126.com;
   yzg15921864839@163.com; whh971683@163.com; lintao@tongji.edu.cn;
   fengsheng_13@aliyun.com; hk_yyf@163.com
RI Wang, Huihui/ABB-6935-2021; zhao, liping/N-4269-2017
OI Wang, Huihui/0000-0002-4098-5313; Feng, Sheng/0000-0002-3859-4516; Yang,
   Yufen/0000-0001-9724-3049
FU Natural Science Foundation of Zhejiang Province [LY19F020015,
   TY22F025548]; National Science Foundation of China [61871289]; Natural
   Science Foundation of Shanghai [18ZR1440600, 19ZR1461100]; public
   Welfare Technology Research Project of Zhejiang Province [LGG19F020007];
   Social Sciences and Humanities Youth Foundation of Ministry of Education
   [21YJCZH039]
FX This work was supported in part by the Natural Science Foundation of
   Zhejiang Province under Grant LY19F020015 and under Grant TY22F025548,
   in part by the National Science Foundation of China under Grant
   61871289, in part by the Natural Science Foundation of Shanghai under
   Grant 18ZR1440600 and under Grant 19ZR1461100, in part by the public
   Welfare Technology Research Project of Zhejiang Province under Grant
   LGG19F020007, and in part by the Social Sciences and Humanities Youth
   Foundation of Ministry of Education under Grant 21YJCZH039.
CR [Anonymous], HPM REFERENCE SOFTWA
   AVS Video Group, 2020, PROC AVS N2955 TELEC, P1
   Bjontegaard G., 2001, PROC 13 VCEG M
   [陈先义 Chen Xianyi], 2015, [电子与信息学报, Journal of Electronics & Information Technology], V37, P2685
   Chen Y, 2018, PICT COD SYMP, P41, DOI 10.1109/PCS.2018.8456249
   Choi K, 2020, IEEE SIGNAL PROC MAG, V37, P160, DOI 10.1109/MSP.2020.2971765
   Jiang D., 2020, PROC AVS M5790 TELEC, P1
   Li J., 2019, PROC AVS M4972, P1
   Li Y., 2019, PROC AVS M4859, P1
   Lin T, 2017, J ELECTRON INF TECHN, V39, P351, DOI 10.11999/JEIT160560
   Lin T, 2013, PICT COD SYMP, P369, DOI 10.1109/PCS.2013.6737760
   Lin T, 2013, IEEE T CIRC SYST VID, V23, P173, DOI 10.1109/TCSVT.2012.2223871
   Ma SW, 2020, IEEE T CIRC SYST VID, V30, P1683, DOI 10.1109/TCSVT.2019.2910119
   Ma Z, 2014, IEEE T IMAGE PROCESS, V23, P4399, DOI 10.1109/TIP.2014.2346995
   Mitrica I, 2019, IEEE T MULTIMEDIA, V21, P2157, DOI 10.1109/TMM.2019.2900168
   Peng WH, 2016, IEEE J EM SEL TOP C, V6, P393, DOI 10.1109/JETCAS.2016.2608971
   Strutz T, 2020, IEEE T MULTIMEDIA, V22, P1126, DOI 10.1109/TMM.2019.2941270
   Nguyen T, 2021, IEEE T CIRC SYST VID, V31, P3801, DOI 10.1109/TCSVT.2021.3074312
   Wang SH, 2015, MULTIMED TOOLS APPL, V74, P7753, DOI 10.1007/s11042-014-2021-3
   Wang SH, 2014, MULTIMED TOOLS APPL, V71, P1263, DOI 10.1007/s11042-012-1274-y
   Wang Y., 2020, P AVS M5221 TEL, P1
   Xu X., 2020, PROC AVS N2928 TELEC, P1
   Xu XZ, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954512
   Xu XZ, 2022, IEEE T CIRC SYST VID, V32, P839, DOI 10.1109/TCSVT.2021.3064210
   Yang YF, 2021, IEEE T CIRC SYST VID, V31, P3714, DOI 10.1109/TCSVT.2020.3029726
   Ye Z., 2020, PROC AVS M5835 TELEC, P1
   Zhang J, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954503
   Zhang YX, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9206739
   Zhao L., 2017, CHIN J COMPUT, V40, P1
   Zhao LP, 2020, IEEE T MULTIMEDIA, V22, P786, DOI 10.1109/TMM.2019.2931414
   [赵利平 Zhao Liping], 2019, [计算机学报, Chinese Journal of Computers], V42, P2100
   [赵利平 Zhao Liping], 2018, [计算机学报, Chinese Journal of Computers], V41, P2482
   Zhao LP, 2018, IEEE T MULTIMEDIA, V20, P796, DOI 10.1109/TMM.2017.2758519
   Zhao LP, 2016, IEEE T MULTIMEDIA, V18, P339, DOI 10.1109/TMM.2015.2512539
   Zhou C., 2020, PROC AVS M5146 TELEC, P1
   Zhou KL, 2018, MULTIMED TOOLS APPL, V77, P23751, DOI 10.1007/s11042-018-5624-2
   Zhou KL, 2016, IEEE J EM SEL TOP C, V6, P560, DOI 10.1109/JETCAS.2016.2599876
   Zhou QY, 2021, IEEE T MULTIMEDIA, V23, P3867, DOI 10.1109/TMM.2020.3033092
   Zhu WJ, 2013, PICT COD SYMP, P373, DOI 10.1109/PCS.2013.6737761
NR 39
TC 5
Z9 5
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 584
EP 592
DI 10.1109/TMM.2021.3129358
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800019
DA 2024-07-18
ER

PT J
AU Zhu, T
   Li, LD
   Yang, JF
   Zhao, SC
   Xiao, X
AF Zhu, Tong
   Li, Leida
   Yang, Jufeng
   Zhao, Sicheng
   Xiao, Xiao
TI Multimodal Emotion Classification With Multi-Level Semantic Reasoning
   Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Sentiment analysis; Visualization; Cognition; Feature
   extraction; Task analysis; Social networking (online); Multimodal
   emotion classification; Graph attention module; Semantic reasoning
ID SENTIMENT ANALYSIS
AB Nowadays, people are accustomed to posting images and associated text for expressing their emotions on social networks. Accordingly, multimodal sentiment analysis has drawn increasingly more attention. Most of the existing image-text multimodal sentiment analysis methods simply predict the sentiment polarity. However, the same sentiment polarity may correspond to quite different emotions, such as happiness vs. excitement and disgust vs. sadness. Therefore, sentiment polarity is ambiguous and may not convey the accurate emotions that people want to express. Psychological research has shown that objects and words are emotional stimuli and that semantic concepts can affect the role of stimuli. Inspired by this observation, this paper presents a new MUlti-Level SEmantic Reasoning network (MULSER) for fine-grained image-text multimodal emotion classification, which not only investigates the semantic relationship among objects and words respectively, but also explores the semantic relationship between regional objects and global concepts. For image modality, we first build graphs to extract objects and global representation, and employ a graph attention module to perform bilevel semantic reasoning. Then, a joint visual graph is built to learn the regional-global semantic relations. For text modality, we build a word graph and further apply graph attention to reinforce the interdependencies among words in a sentence. Finally, a cross-modal attention fusion module is proposed to fuse semantic-enhanced visual and textual features, based on which informative multimodal representations are obtained for fine-grained emotion classification. The experimental results on public datasets demonstrate the superiority of the proposed model over the state-of-the-art methods.
C1 [Zhu, Tong] China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Peoples R China.
   [Li, Leida] Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
   [Yang, Jufeng] Nankai Univ, Sch Comp & Control Engn, Tianjin 300350, Peoples R China.
   [Zhao, Sicheng] Tsinghua Univ, BNRist, Beijing 100084, Peoples R China.
   [Xiao, Xiao] Xidian Univ, Sch Telecommun Engn, Xidian 710071, Peoples R China.
C3 China University of Mining & Technology; Xidian University; Nankai
   University; Tsinghua University; Xidian University
RP Li, LD (corresponding author), Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
EM zhutong@cumt.edu.cn; reader1104@hotmail.com; yangjufeng@nankai.edu.cn;
   schzhao@gmail.com; xiaoxiao@xidian.edu.cn
OI Zhu, Tong/0000-0002-3082-7848
FU Key-Area Research and Development Program of Guangdong Province
   [2021B0101400002]; National Natural Science Foundation of China
   [62171340, 61991451, 61771473]; Key Project of Shaanxi Provincial
   Department of Education (Collaborative Innovation Center) [20JY024]
FX This work was supported in partby the Key-Area Research and Development
   Program of Guangdong Province under Grant 2021B0101400002, in part by
   the National Natural Science Foundation of China under Grants 62171340,
   61991451 and 61771473, in part by the Key Project of Shaanxi Provincial
   Department of Education (Collaborative Innovation Center) under Grant
   20JY024. The Associate Editor coordinating the review of this manuscript
   and approving it for publication was Prof. MetinSezgin.
CR [Anonymous], 2014, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), DOI [DOI 10.3115/V1/D14-1181, 10.3115/v1/D14-1181]
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Brosch T, 2010, COGNITION EMOTION, V24, P377, DOI 10.1080/02699930902975754
   Chen T, 2014, Arxiv, DOI arXiv:1410.8586
   Chen YX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P117, DOI 10.1145/3240508.3240533
   DEHOUWER J, 1994, COGNITION EMOTION, V8, P1, DOI 10.1080/02699939408408925
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy Alexey, 2021, 2021 INT C LEARN REP
   Fang Q, 2015, IEEE T MULTIMEDIA, V17, P2281, DOI 10.1109/TMM.2015.2491019
   Gao Difei, 2020, CVPR, P12746
   Gao Y, 2016, IEEE T MULTIMEDIA, V18, P2115, DOI 10.1109/TMM.2016.2581483
   Guo WY, 2021, IEEE T MULTIMEDIA, V23, P1785, DOI 10.1109/TMM.2020.3003648
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu A, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P350, DOI 10.1145/3219819.3219853
   Huang FR, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3388861
   Huang FR, 2019, KNOWL-BASED SYST, V167, P26, DOI 10.1016/j.knosys.2019.01.019
   Kingma D. P., 2014, arXiv
   Kipf TN, 2017, INT C LEARN REPR
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li Y., 2016, ICLR, P1, DOI DOI 10.48550/ARXIV.1511.05493
   Liu G, 2019, NEUROCOMPUTING, V337, P325, DOI 10.1016/j.neucom.2019.01.078
   Liu XC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P907, DOI 10.1145/3394171.3413578
   Liu XC, 2019, PROC CVPR IEEE, P3561, DOI 10.1109/CVPR.2019.00368
   Maas Andrew, 2011, P 49 ANN M ASS COMP
   Machajdik J., 2010, P 18 ACM INT C MULT, P83, DOI DOI 10.1145/1873951.1873965
   Niu T., 2016, MULTIMEDIA MODELING, P15, DOI [DOI 10.1007/978-3-319-27674-82, 10.1007/978-3-319-27674-8_2]
   Pang B, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P79, DOI 10.3115/1118693.1118704
   Paszke Adam, 2017, NIPS W
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   She DY, 2020, IEEE T MULTIMEDIA, V22, P1358, DOI 10.1109/TMM.2019.2939744
   Tai KS, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1556
   Tang Duyu, 2015, P 2015 C EMPIRICAL M, P1422
   Vaswani A, 2017, ADV NEUR IN, V30
   Velickovic Petar, 2018, INT C LEARN REPR
   Xu J, 2019, APPL SOFT COMPUT, V80, P387, DOI 10.1016/j.asoc.2019.04.010
   Xu N, 2019, AAAI CONF ARTIF INTE, P371
   Xu N, 2018, ACM/SIGIR PROCEEDINGS 2018, P929, DOI 10.1145/3209978.3210093
   Xu N, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P2399, DOI 10.1145/3132847.3133142
   Xu N, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SECURITY INFORMATICS (ISI), P152, DOI 10.1109/ISI.2017.8004895
   Yang JY, 2021, PROC CVPR IEEE, P4235, DOI 10.1109/CVPR46437.2021.00422
   Yang JY, 2021, IEEE T IMAGE PROCESS, V30, P8686, DOI 10.1109/TIP.2021.3118983
   Yang JF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3266
   Yang XC, 2021, IEEE T MULTIMEDIA, V23, P4014, DOI 10.1109/TMM.2020.3035277
   Yang XC, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1 (ACL-IJCNLP 2021), P328
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yao XX, 2021, IEEE T MULTIMEDIA, V23, P1640, DOI 10.1109/TMM.2020.3001527
   You QZ, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P13, DOI 10.1145/2835776.2835779
   You QZ, 2015, IEEE T MULTIMEDIA, V17, P2271, DOI 10.1109/TMM.2015.2487863
   Yu H, 2003, PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P129
   Yuan J., 2013, P 2 INT WORKSH ISS S, P1
   Zhang M, 2019, IEEE INT CON MULTI, P1618, DOI 10.1109/ICME.2019.00279
   Zhao SC, 2021, IEEE SIGNAL PROC MAG, V38, P59, DOI 10.1109/MSP.2021.3106895
   Zhao SC, 2022, IEEE T PATTERN ANAL, V44, P6729, DOI 10.1109/TPAMI.2021.3094362
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P47, DOI 10.1145/2647868.2654930
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1025, DOI 10.1145/2647868.2655035
   Zhu XG, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3595
NR 60
TC 3
Z9 3
U1 17
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6868
EP 6880
DI 10.1109/TMM.2022.3214989
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000012
DA 2024-07-18
ER

PT J
AU Zhuo, JB
   Wang, SH
   Huang, QM
AF Zhuo, Junbao
   Wang, Shuhui
   Huang, Qingming
TI Uncertainty Modeling for Robust Domain Adaptation Under Noisy
   Environments
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Domain Adaptation; Uncertainty; Noisy Label; Transfer Learning; Deep
   Learning
AB In this paper, we tackle the task of domain adaptation under noisy environments; this is a practical and challenging problem in which the source domain is corrupted with noise in its labels, its features, or both. Noise in the source domain leads to inaccurate visual representations and makes it harder to estimate and reduce the domain discrepancy between the source and target domains, resulting in severe performance degradation in the target domain. These challenges can be addressed with offline source sample selection following robust domain discrepancy reduction. To achieve reliable sample selection, we model the uncertainty in the predictions of a convolutional neural network (CNN) classifier and reweight the classification loss by this uncertainty. Such a reweighting mechanism reduces the contribution of noise, leading to improved noise robustness. We further propose UncertaintyRank, a novel regularizer, to encourage the uncertainty to be more sensitive to noisy labels, as label corruption brings more severe degradation. The uncertainty is also aggregated with the classification loss to eliminate the adverse effects of noisy representations while estimating the domain discrepancy. Extensive experiments validate the effectiveness of our method and verify that it performs favorably against existing state-of-the-art methods.
C1 [Zhuo, Junbao; Wang, Shuhui; Huang, Qingming] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Wang, Shuhui; Huang, Qingming] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
   [Huang, Qingming] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101408, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Peng Cheng Laboratory; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS
RP Wang, SH (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM junbao.zhuo@vipl.ict.ac.cn; wangshuhui@ict.ac.cn; qmhuang@ucas.ac.cn
OI Zhuo, Junbao/0000-0001-9587-498X
FU National Key R&D Program of China [2018AAA0102000]; National Natural
   Science Foundation of China [62022083, U21B2038, 61931008]; Fundamental
   Research Funds for the Central Universities
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018AAA0102000, in part by National Natural Science
   Foundation of China under Grants 62022083, U21B2038, and 61931008, and
   in part by the Fundamental Research Funds for the Central Universities.
CR Amini Alexander, 2020, ADV NEURAL INFORM PR, V33, P14927, DOI DOI 10.5555/3495724.3496975
   Antoran J., 2020, Advances in Neural Information Processing Systems, V33, P10620
   Arazo E, 2019, PR MACH LEARN RES, V97
   Arpit D, 2017, PR MACH LEARN RES, V70
   Ash J. T., 2019, P INT C LEARN REPR, P1
   Bergamo Alessandro, 2010, ADV NEURAL INFORM PR, V23
   Blundell C, 2015, PR MACH LEARN RES, V37, P1613
   Cui SH, 2020, PROC CVPR IEEE, P3940, DOI 10.1109/CVPR42600.2020.00400
   Deng WJ, 2021, IEEE T CIRC SYST VID, V31, P29, DOI 10.1109/TCSVT.2020.2968484
   Gal Y, 2016, PR MACH LEARN RES, V48
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Ghosh A, 2017, AAAI CONF ARTIF INTE, P1919
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Graves Alex, 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721
   Griffin G., 2007, CALTECH 256 OBJECT C
   Han ZY, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2269
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503
   Kendall A., 2017, Advances in Neural Information Processing Systems, V30, P5574
   Kohler J. M., 2019, CVPR WORKSHOPS, P33
   Lakshminarayanan B, 2017, ADV NEUR IN, V30
   Lee CY, 2019, PROC CVPR IEEE, P10277, DOI 10.1109/CVPR.2019.01053
   Li J., 2019, P INT C LEARN REPR
   Li JN, 2019, PROC CVPR IEEE, P5046, DOI 10.1109/CVPR.2019.00519
   Li M, 2020, PR MACH LEARN RES, V108, P4313
   Li YC, 2017, IEEE I CONF COMP VIS, P1928, DOI 10.1109/ICCV.2017.211
   Liu S., 2020, Adv. Neural Inf. Process. Syst, V33, P20331
   Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Lu J, 2018, PR MACH LEARN RES, V80
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Neal R. M., 2012, BAYESIAN LEARNING NE, V118, DOI DOI 10.1007/978-1-4612-0745-0
   Pawan Kumar M., 2010, NIPS
   Ren MY, 2018, PR MACH LEARN RES, V80
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Shu Y, 2019, AAAI CONF ARTIF INTE, P4951
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   van Rooyen B, 2015, ADV NEUR IN, V28
   Vandat A, 2017, ADV NEUR IN, V30
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang YS, 2018, PROC CVPR IEEE, P8688, DOI 10.1109/CVPR.2018.00906
   Wang YS, 2019, IEEE I CONF COMP VIS, P322, DOI 10.1109/ICCV.2019.00041
   Wei J., 2022, P INT C LEARN REPR
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Yan HL, 2017, PROC CVPR IEEE, P945, DOI 10.1109/CVPR.2017.107
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P2347, DOI 10.1109/TMM.2020.3009476
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P2701, DOI 10.1109/TMM.2019.2912121
   Zeng XY, 2014, LECT NOTES COMPUT SC, V8691, P472, DOI 10.1007/978-3-319-10578-9_31
   Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414
   Zhang YC, 2019, PR MACH LEARN RES, V97
   Zhang ZL, 2018, ADV NEUR IN, V31
   Zhihe Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9108, DOI 10.1109/CVPR42600.2020.00913
   Zhuo JB, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P261, DOI 10.1145/3123266.3123292
   Zuo YK, 2022, IEEE T MULTIMEDIA, V24, P1020, DOI 10.1109/TMM.2021.3097495
NR 62
TC 4
Z9 4
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6157
EP 6170
DI 10.1109/TMM.2022.3205457
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500037
DA 2024-07-18
ER

PT J
AU Arvanitis, G
   Zacharaki, EI
   Vása, L
   Moustakas, K
AF Arvanitis, Gerasimos
   Zacharaki, Evangelia I.
   Vasa, Libor
   Moustakas, Konstantinos
TI Broad-to-Narrow Registration and Identification of 3D Objects in
   Partially Scanned and Cluttered Point Clouds
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Feature extraction; Solid modeling; Shape;
   Object recognition; Histograms; Data models; point cloud registration;
   partially-scanned point clouds; saliency; weighted ICP; cluttered scene
ID MATCHING ALGORITHM; RECOGNITION; FEATURES; SURFACE; IMAGES
AB The new generation 3D scanner devices have revolutionized the way information from 3D objects is acquired, making the process of scene capturing and digitization straightforward. However, the effectiveness and robustness of conventional algorithms for real scene analysis are usually deteriorated due to challenging conditions, such as noise, low resolution, and bad perceptual quality. In this work, we present a methodology for identifying and registering partially-scanned and noisy 3D objects, lying in arbitrary positions in a 3D scene, with corresponding high-quality models. The methodology is assessed on point cloud scenes with multiple objects with large missing parts. The proposed approach does not require connectivity information and is thus generic and computationally efficient, thereby facilitating computationally demanding applications, like augmented reality. The main contributions of this work are the introduction of a layered joint registration and indexing scheme of cluttered partial point clouds using a novel multi-scale saliency extraction technique to identify distinctive regions, and an enhanced similarity criterion for object-to-model matching. The processing time of the process is also accelerated through 3D scene segmentation. Comparisons of the proposed methodology with other state-of-the-art approaches highlight its superiority under challenging conditions.
C1 [Arvanitis, Gerasimos; Zacharaki, Evangelia I.; Moustakas, Konstantinos] Univ Patras, Dept Elect & Comp Engn, Patras 26504, Greece.
   [Vasa, Libor] Univ West Bohemia, Fac Appl Sci, Dept Comp Sci & Engn, Plzen 30614, Czech Republic.
C3 University of Patras; University of West Bohemia Pilsen
RP Arvanitis, G (corresponding author), Univ Patras, Dept Elect & Comp Engn, Patras 26504, Greece.
EM arvanitis@ece.upatras.gr; ezachar@upatras.gr; lvasa@kiv.zcu.cz;
   moustakas@ece.upatras.gr
RI Vasa, Libor/F-6706-2011; Zacharaki, Evangelia/AAC-6661-2020
OI Vasa, Libor/0000-0002-0213-3769; Zacharaki,
   Evangelia/0000-0001-8228-0437; ARVANITIS, GERASIMOS/0000-0001-8149-5188
FU European Union [871738]; University Specific Student Research Project
   Synthesis and Analysis of Geometric and Computing Models [SGS-2019-016]
FX This work has received funding from the European Union's Horizon 2020
   research and innovation programme under Grant 871738 -CPSoSaware:
   Crosslayer cognitive optimization tools & methods for the lifecycle
   support of dependable CPSoS. This project was partially supported by the
   University Specific Student Research Project SGS-2019-016 Synthesis and
   Analysis of Geometric and Computing Models. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Lu Fang.
CR [Anonymous], 2006, 2006 IEEE COMPUTER S
   Arvanitis G, 2019, IEEE INTL CONF IND I, P683, DOI [10.1109/indin41052.2019.8972024, 10.1109/INDIN41052.2019.8972024]
   Arvanitis G, 2018, 2018 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P415, DOI 10.1109/CW.2018.00080
   Arvanitis G, 2018, IEEE IMAGE PROC, P3888, DOI 10.1109/ICIP.2018.8451099
   Arvanitis G, 2019, IEEE T VIS COMPUT GR, V25, P1513, DOI 10.1109/TVCG.2018.2802926
   Arvanitis G, 2017, LECT NOTES ARTIF INT, V10459, P11, DOI 10.1007/978-3-319-66471-2_2
   Bariya P, 2012, INT J COMPUT VISION, V99, P232, DOI 10.1007/s11263-012-0526-7
   Bergström P, 2017, NUMER ALGORITHMS, V74, P755, DOI 10.1007/s11075-016-0170-3
   Bergström P, 2014, COMPUT OPTIM APPL, V58, P543, DOI 10.1007/s10589-014-9643-2
   Bouaziz S, 2013, COMPUT GRAPH FORUM, V32, P113, DOI 10.1111/cgf.12178
   Buch A. G., 2016, LOCAL SHAPE FEATURE, V5
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chen HC, 2019, IEEE-CAA J AUTOMATIC, V6, P981, DOI 10.1109/JAS.2019.1911579
   Chen HH, 2020, IEEE T VIS COMPUT GR, V26, P3255, DOI 10.1109/TVCG.2019.2920817
   Darom T, 2012, IEEE T IMAGE PROCESS, V21, P2758, DOI 10.1109/TIP.2012.2183142
   Elbaz G, 2017, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2017.265
   Erus G, 2014, MED IMAGE ANAL, V18, P542, DOI 10.1016/j.media.2014.02.003
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   Fleishman S, 2005, ACM T GRAPHIC, V24, P544, DOI 10.1145/1073204.1073227
   Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224
   Gojcic Z, 2019, PROC CVPR IEEE, P5540, DOI 10.1109/CVPR.2019.00569
   Guo YL, 2014, IEEE T PATTERN ANAL, V36, P2270, DOI 10.1109/TPAMI.2014.2316828
   Guo YL, 2014, IEEE T MULTIMEDIA, V16, P1377, DOI 10.1109/TMM.2014.2316145
   Guo YL, 2013, INT J COMPUT VISION, V105, P63, DOI 10.1007/s11263-013-0627-y
   Hruda L, 2019, COMPUT GRAPH FORUM, V38, P175, DOI 10.1111/cgf.13798
   Hu W, 2020, IEEE T SIGNAL PROCES, V68, P2841, DOI 10.1109/TSP.2020.2978617
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Keaomanee Yossawee, 2018, 2018 IEEE 4th International Conference on Computer and Communications (ICCC). Proceedings, P1611, DOI 10.1109/CompComm.2018.8780922
   Khan I, 2018, IEEE T MULTIMEDIA, V20, P841, DOI 10.1109/TMM.2017.2758740
   Kim H, 2018, INT C CONTR AUTOMAT, P1620
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Lawin FJ, 2018, PROC CVPR IEEE, P3829, DOI 10.1109/CVPR.2018.00403
   Li D., 2018, P 10 INT C MEAS TECH
   Liang YQ, 2020, INTEGR COMPUT-AID E, V27, P417, DOI 10.3233/ICA-200641
   Lin Z., 2009, AUGMENTED LAGRANGE M
   Liu HB, 2019, IEEE ACCESS, V7, P73637, DOI 10.1109/ACCESS.2019.2919989
   Liu L, 2015, 2015 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P115, DOI 10.1109/CW.2015.31
   Liu SJ, 2012, IEEE COMPUT GRAPH, V32, P70, DOI 10.1109/MCG.2011.14
   Lu B, 2019, IEEE ACCESS, V7, P137570, DOI 10.1109/ACCESS.2019.2943003
   Lu J, 2019, CHIN CONTR CONF, P4439, DOI 10.23919/ChiCC.2019.8866059
   Lu M, 2014, SENSORS-BASEL, V14, P24156, DOI 10.3390/s141224156
   Luo S., 2020, DIFFERENTIABLEMANIFO, P1330, DOI DOI 10.1145/3394171.3413727
   Malassiotis S, 2007, IEEE T PATTERN ANAL, V29, P1285, DOI 10.1109/TPAMI.2007.1060
   Mattei E, 2017, COMPUT GRAPH FORUM, V36, P123, DOI 10.1111/cgf.13068
   Mavridis P, 2015, COMPUT AIDED GEOM D, V35-36, P16, DOI 10.1016/j.cagd.2015.03.022
   Mellado N, 2014, COMPUT GRAPH FORUM, V33, P205, DOI 10.1111/cgf.12446
   Mian A, 2010, INT J COMPUT VISION, V89, P348, DOI 10.1007/s11263-009-0296-z
   Mian AS, 2006, IEEE T PATTERN ANAL, V28, P1584, DOI 10.1109/TPAMI.2006.213
   Mian AS, 2006, INT J COMPUT VISION, V66, P19, DOI 10.1007/s11263-005-3221-0
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Paquet E, 2000, SIGNAL PROCESS-IMAGE, V16, P103, DOI 10.1016/S0923-5965(00)00020-5
   Pistilli F, 2021, IEEE J-STSP, V15, P402, DOI 10.1109/JSTSP.2020.3047471
   Rodolà E, 2013, INT J COMPUT VISION, V102, P129, DOI 10.1007/s11263-012-0568-x
   Rusu RB, 2010, IEEE INT C INT ROBOT, P2155, DOI 10.1109/IROS.2010.5651280
   Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011
   Sarkar K, 2018, INT CONF 3D VISION, P444, DOI 10.1109/3DV.2018.00058
   Shang LM, 2010, INT J COMPUT VISION, V89, P211, DOI 10.1007/s11263-009-0276-3
   Sheung H., 2009, 2009 SIAMACM JOINT C, P13
   Stechschulte J, 2019, IEEE INT CONF ROBOT, P7143, DOI [10.1109/ICRA.2019.8793857, 10.1109/icra.2019.8793857]
   Sun JL, 2020, IEEE ACCESS, V8, P7539, DOI 10.1109/ACCESS.2020.2963984
   Sun Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P263, DOI 10.1109/ICCV.2001.937634
   Taati B, 2011, COMPUT VIS IMAGE UND, V115, P681, DOI 10.1016/j.cviu.2010.11.021
   Tao WY, 2018, IEEE ACCESS, V6, P48062, DOI 10.1109/ACCESS.2018.2866935
   Taylor Z, 2015, J FIELD ROBOT, V32, P675, DOI 10.1002/rob.21523
   Truong G, 2019, 2019 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P200, DOI 10.1109/dicta47822.2019.8945870
   Vongkulbhisal J, 2017, PROC CVPR IEEE, P3975, DOI 10.1109/CVPR.2017.423
   Wu YQ, 2018, IEEE T SERV COMPUT, V11, P341, DOI 10.1109/TSC.2015.2501981
   Xu ZH, 2019, IEEE GEOSCI REMOTE S, V16, P286, DOI 10.1109/LGRS.2018.2872353
   YIANILOS PN, 1993, PROCEEDINGS OF THE FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P311
   Yuan C, 2016, PROCEEDINGS OF 2016 INTERNATIONAL CONFERENCE ON AUDIO, LANGUAGE AND IMAGE PROCESSING (ICALIP), P404, DOI 10.1109/ICALIP.2016.7846655
   Zaharescu A, 2012, INT J COMPUT VISION, V100, P78, DOI 10.1007/s11263-012-0528-5
   Zhang ZX, 2019, IEEE GEOSCI REMOTE S, V16, P1904, DOI 10.1109/LGRS.2019.2910546
   Zhong ZY, 2015, IEEE T MULTIMEDIA, V17, P1391, DOI 10.1109/TMM.2015.2446201
NR 74
TC 9
Z9 9
U1 6
U2 49
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2230
EP 2245
DI 10.1109/TMM.2021.3089838
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600002
OA Green Submitted, hybrid, Green Published
DA 2024-07-18
ER

PT J
AU Chen, C
   Dong, SY
   Tian, Y
   Cao, KL
   Liu, L
   Guo, YH
AF Chen, Cong
   Dong, Shouyang
   Tian, Ye
   Cao, Kunlin
   Liu, Li
   Guo, Yuanhao
TI Temporal Self-Ensembling Teacher for Semi-Supervised Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object detection; Predictive models; Training; Detectors; Data models;
   Analytical models; Transforms; Semi-supervised object detection; deep
   convolutional neural networks; knowledge distillation; temporal
   self-ensembling; focal loss
AB This paper focuses on the semi-supervised object detection (SSOD) which makes good use of unlabeled data to boost performance. We face the following obstacles when adapting the knowledge distillation (KD) framework in SSOD. (1) The teacher model serves a dual role as a teacher and a student, such that the teacher predictions on unlabeled images may limit the upper bound of the student. (2) The data imbalance issue caused by the large quantity of consistent predictions between the teacher and student hinders an efficient knowledge transfer between them. To mitigate these issues, we propose a novel SSOD model called Temporal Self-Ensembling Teacher (TSET). Our teacher model ensembles its temporal predictions for unlabeled images under stochastic perturbations. Then, our teacher model ensembles its model weights with those of the student model by an exponential moving average. These ensembling strategies ensure data and model diversity, and lead to better teacher predictions for unlabeled images. In addition, we adapt the focal loss to formulate the consistency loss for handling the data imbalance issue. Together with a thresholding method, the focal loss automatically reweights the inconsistent predictions, which preserves the knowledge for difficult objects to detect in the unlabeled images. The mAP of our model reaches 80.73% and 40.52% on the VOC2007 test set and the COCO2014 minival5k set, respectively, and outperforms a strong fully supervised detector by 2.37% and 1.49%, respectively. Furthermore, the mAP of our model (80.73%) sets a new state-of-the-art performance in SSOD on the VOC2007 test set.
C1 [Chen, Cong; Cao, Kunlin] Keya Med Technol, Shenzhen 518116, Peoples R China.
   [Dong, Shouyang] Cambricon, Software Dept, Beijing 100010, Peoples R China.
   [Tian, Ye] Tencent, Hippocrates Res Lab, Shenzhen 518052, Peoples R China.
   [Liu, Li] Natl Univ Def Technol, Coll Syst Engn, Changsha 410073, Peoples R China.
   [Liu, Li] Univ Oulu, Ctr Machine Vis & Signal Anal, Oulu 90570, Finland.
   [Guo, Yuanhao] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
C3 Tencent; National University of Defense Technology - China; University
   of Oulu; Chinese Academy of Sciences; Institute of Automation, CAS
RP Guo, YH (corresponding author), Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
EM congc@keyayun.com; shouyang.dong@gmail.com; tytian@outlook.com;
   cao@keyayun.com; dreamliu2010@gmail.com; yguo.leidenuniv@gmail.com
RI Tian, Ye/ABB-3883-2020; Guo, Yuanhao/N-2763-2019
OI Tian, Ye/0000-0002-2225-7566; Liu, li/0000-0002-2011-2873
FU National Natural Science Foundation of China [12073047]
FX This work was supported by the National Natural Science Foundation of
   China (No. 12073047).
CR [Anonymous], 2018, INT C LEARN REPR
   [Anonymous], 2008, COMPUT SCI
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], 2001, P IEEE COMP SOC C CO
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Chapelle O., 2006, SEMISUPERVISED LEARN
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Dai JF, 2016, ADV NEUR IN, V29
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   FAIR,, 2018, FASTER R CNN RETINA
   Girshick R., 2014, P 2014 IEEE C COMPUT, P580, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Goyal P, 2019, IEEE I CONF COMP VIS, P6400, DOI 10.1109/ICCV.2019.00649
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Jeong J, 2019, ADV NEUR IN, V32
   Kuznetsova A, 2020, INT J COMPUT VISION, V128, P1956, DOI 10.1007/s11263-020-01316-z
   Laine Samuli, 2017, 5 INT C LEARNING REP, DOI DOI 10.48550/ARXIV.1610.02242
   Lan X., 2018, Advances in neural information processing systems, P7528
   Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1
   Lee D.-H., 2013, PSEUDOLABEL SIMPLE E, V3, P896
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu YF, 2019, PROC CVPR IEEE, P2599, DOI 10.1109/CVPR.2019.00271
   Mandal D, 2020, IEEE T MULTIMEDIA, V22, P2345, DOI 10.1109/TMM.2019.2954741
   Masi I, 2018, SIBGRAPI, P471, DOI 10.1109/SIBGRAPI.2018.00067
   Min SB, 2021, IEEE T MULTIMEDIA, V23, P899, DOI 10.1109/TMM.2020.2990063
   Oquab M, 2015, PROC CVPR IEEE, P685, DOI 10.1109/CVPR.2015.7298668
   Phuong M, 2019, PR MACH LEARN RES, V97
   Radosavovic I, 2018, PROC CVPR IEEE, P4119, DOI 10.1109/CVPR.2018.00433
   Rasmus Antti, 2015, PROC 28 INT C NEURAL, P3546
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sajjadi M, 2016, ADV NEUR IN, V29
   Sohn K, 2020, Arxiv, DOI [arXiv:2005.04757, DOI 10.48550/ARXIV.2005.04757]
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Tang P, 2021, IEEE WINT CONF APPL, P2290, DOI 10.1109/WACV48630.2021.00234
   Tang YX, 2017, IEEE T MULTIMEDIA, V19, P393, DOI 10.1109/TMM.2016.2614862
   Tarvainen A, 2017, ADV NEUR IN, V30
   Valpola H, 2015, Adv. in Independent Component Analysis and Learning Machines, P143
   Wan F, 2018, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2018.00141
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Xia GS, 2018, PROC CVPR IEEE, P3974, DOI 10.1109/CVPR.2018.00418
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yuan MK, 2020, IEEE T MULTIMEDIA, V22, P1955, DOI 10.1109/TMM.2019.2951463
   Zhang DW, 2019, INT J COMPUT VISION, V127, P363, DOI 10.1007/s11263-018-1112-4
   Zhang YB, 2020, IEEE T MULTIMEDIA, V22, P1345, DOI 10.1109/TMM.2019.2939747
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094
   Zhu CC, 2019, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2019.00093
   Zhu X, 2009, Synthesis Lectures on Artificial Intelligence and Machine Learning, V3, P1, DOI 10.1007/978-3-031-01548-9
   Zhuang BH, 2018, PROC CVPR IEEE, P7920, DOI 10.1109/CVPR.2018.00826
NR 61
TC 3
Z9 3
U1 5
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3679
EP 3692
DI 10.1109/TMM.2021.3105807
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400002
OA Green Accepted, Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, T
   Xie, GS
   Yao, YZ
   Wang, Q
   Shen, FM
   Tang, ZM
   Zhang, J
AF Chen, Tao
   Xie, Guo-Sen
   Yao, Yazhou
   Wang, Qiong
   Shen, Fumin
   Tang, Zhenmin
   Zhang, Jian
TI Semantically Meaningful Class Prototype Learning for One-Shot Image
   Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image segmentation; Prototypes; Semantics; Training; Feature extraction;
   Task analysis; Testing; Image segmentation; one-shot learning;
   semantically meaningful prototype
ID CLASSIFICATION; NETWORK
AB One-shot semantic image segmentation aims to segment the object regions for the novel class with only one annotated image. Recent works adopt the episodic training strategy to mimic the expected situation at testing time. However, these existing approaches simulate the test conditions too strictly during the training process, and thus cannot make full use of the given label information. Besides, these approaches mainly focus on the foreground-background target class segmentation setting. They only utilize binary mask labels for training. In this paper, we propose to leverage the multi-class label information during the episodic training. It will encourage the network to generate more semantically meaningful features for each category. After integrating the target class cues into the query features, we then propose a pyramid feature fusion module to mine the fused features for the final classifier. Furthermore, to take more advantage of the support image-mask pair, we propose a self-prototype guidance branch to support image segmentation. It can constrain the network for generating more compact features and a robust prototype for each semantic class. For inference, we propose a fused prototype guidance branch for the segmentation of the query image. Specifically, we leverage the prediction of the query image to extract the pseudo-prototype and combine it with the initial prototype. Then we utilize the fused prototype to guide the final segmentation of the query image. Extensive experiments demonstrate the superiority of our proposed approach. The source codes and models have been made available at https://github.com/NUST-Machine-Intelligence-Laboratory/SMCP.
C1 [Chen, Tao; Yao, Yazhou; Wang, Qiong; Tang, Zhenmin] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Xie, Guo-Sen] Incept Inst Artificial Intelligence, Comp Vis Grp, Abu Dhabi 999041, U Arab Emirates.
   [Shen, Fumin] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Sichuan, Peoples R China.
   [Zhang, Jian] Univ Technol Sydney, Sch Elect & Data Engn, Rosebery, NSW 2018, Australia.
C3 Nanjing University of Science & Technology; University of Electronic
   Science & Technology of China; University of Technology Sydney
RP Yao, YZ (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.; Xie, GS (corresponding author), Incept Inst Artificial Intelligence, Comp Vis Grp, Abu Dhabi 999041, U Arab Emirates.
EM taochen_93@outlook.com; gsxiehm@gmail.com; yazhou.yao@njust.edu.cn;
   wangq@njust.edu.cn; fumin.shen@gmail.com; Tzm.cs@njust.edu.cn;
   Jian.Zhang@uts.edu.au
RI Shen, Fumin/R-2121-2016; Tang, Zhenmin/AAY-6058-2020; Chen,
   Tao/ABB-5983-2022; Xie, Guo-Sen/AAL-6674-2020
OI Tang, Zhenmin/0000-0001-6708-2205; Xie, Guo-Sen/0000-0002-5487-9845;
   Zhang, Jian/0000-0002-7240-3541; Chen, Tao/0000-0001-8239-1698; Yao,
   Yazhou/0000-0002-0337-9410; Wang, Qiong/0000-0003-4193-0960
FU National Natural Science Foundation of China [61976116]; Fundamental
   Research Funds for the Central Universities [30920021135]; Jiangsu
   Province Key Research and Development Plan [BE2016904]; Ark Innovation
   Fund [HT202012310145]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61976116, in part by the Fundamental
   Research Funds for the Central Universities under Grant 30920021135, in
   part by Jiangsu Province Key Research and Development Plan under Grant
   BE2016904, and in part by Ark Innovation Fund under Grant
   HT202012310145.
CR [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Boykov Y., 2020, INT J SPORT NUTR EXE, DOI DOI 10.1123/IJSNEM.2021-0090
   Cao ZY, 2019, IEEE ACCESS, V7, P166109, DOI 10.1109/ACCESS.2019.2953465
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen YH, 2017, IEEE I CONF COMP VIS, P2011, DOI 10.1109/ICCV.2017.220
   Chi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12200, DOI 10.1109/CVPR42600.2020.01222
   Chu WT, 2018, IEEE T MULTIMEDIA, V20, P2491, DOI 10.1109/TMM.2018.2801718
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong N., 2018, P BRIT MACH VIS C, V79
   Dong ZH, 2019, P INT COMP SOFTW APP, P42, DOI 10.1109/COMPSAC.2019.10181
   Dvornik N, 2019, IEEE I CONF COMP VIS, P3722, DOI 10.1109/ICCV.2019.00382
   Finn C, 2017, PR MACH LEARN RES, V70
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gan S., P ACM INT C MULT, P1837
   Gao GY, 2018, IEEE T MULTIMEDIA, V20, P3399, DOI 10.1109/TMM.2018.2838330
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu H, 2014, IEEE ACCESS, V2, P652, DOI 10.1109/ACCESS.2014.2332453
   Hu T, 2019, AAAI CONF ARTIF INTE, P8441
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Huang ZL, 2018, PROC CVPR IEEE, P7014, DOI 10.1109/CVPR.2018.00733
   Hung W. C., 2019, P BRIT MACH VIS C, V65
   Jiang PT, 2019, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2019.00216
   Kalluri T, 2019, IEEE I CONF COMP VIS, P5258, DOI 10.1109/ICCV.2019.00536
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Nguyen K, 2019, IEEE I CONF COMP VIS, P622, DOI 10.1109/ICCV.2019.00071
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li JA, 2018, IEEE T MULTIMEDIA, V20, P1645, DOI 10.1109/TMM.2017.2772796
   Li Y., 2016, P 5 INT WORKSHOP ENE, P1
   Li Y., P ACM INT C MULT, P3377
   Li Y., 2020, P IEEECVF C COMPUTER, P8553
   Li Y, 2019, IEEE T MULTIMEDIA, V21, P875, DOI 10.1109/TMM.2018.2867720
   Liang XD, 2018, PROC CVPR IEEE, P752, DOI 10.1109/CVPR.2018.00085
   Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu L, 2020, IEEE WIREL COMMUN, V27, P127, DOI 10.1109/MWC.001.1900220
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Munkhdalai T, 2017, PR MACH LEARN RES, V70
   Peng ZM, 2019, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2019.00053
   Qi GJ, 2010, IEEE T MULTIMEDIA, V12, P278, DOI 10.1109/TMM.2010.2046270
   Qiu ZF, 2018, IEEE T MULTIMEDIA, V20, P939, DOI 10.1109/TMM.2017.2759504
   Qiurui Wang, 2019, IEEE Transactions on Multimedia, V21, P1839, DOI 10.1109/TMM.2018.2890360
   Rakelly K., 2018, P INT C LEARN REPR
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shaban A, 2017, ARXIV PREPRINT ARXIV, DOI 10.5244/C.31.167
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Shen FL, 2017, PROC CVPR IEEE, P5178, DOI 10.1109/CVPR.2017.550
   Shuxin Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9159, DOI 10.1109/CVPR42600.2020.00918
   Siam M, 2019, IEEE I CONF COMP VIS, P5248, DOI 10.1109/ICCV.2019.00535
   Simonyan K., 2015, P ICLR
   Snell J, 2017, ADV NEUR IN, V30
   Souly N, 2017, IEEE I CONF COMP VIS, P5689, DOI 10.1109/ICCV.2017.606
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tao C, 2020, IEEE MTT S INT MICR, P1, DOI 10.1109/IMS30576.2020.9223957
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang P, 2017, PROC CVPR IEEE, P6212, DOI 10.1109/CVPR.2017.658
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang YQ, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3386252
   Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687
   Wei YC, 2018, PROC CVPR IEEE, P7268, DOI 10.1109/CVPR.2018.00759
   Wei YC, 2017, IEEE T PATTERN ANAL, V39, P2314, DOI 10.1109/TPAMI.2016.2636150
   Wu ZY, 2019, IEEE I CONF COMP VIS, P6658, DOI 10.1109/ICCV.2019.00676
   Yang GC, 2020, INFORM SCIENCES, V518, P225, DOI 10.1016/j.ins.2020.01.016
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yu CQ, 2018, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR.2018.00199
   Yu F, 2015, P INT C LEARN REPR
   Zhan C, 2020, IEEE T MULTIMEDIA, V22, P795, DOI 10.1109/TMM.2019.2931441
   Zhang C, 2019, IEEE I CONF COMP VIS, P9586, DOI 10.1109/ICCV.2019.00968
   Zhang C, 2019, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR.2019.00536
   Zhang H., 2020, P 28 ACM INT C MULT, P430, DOI DOI 10.1145/3394171.3413582
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang JJ, 2018, IEEE T MULTIMEDIA, V20, P2801, DOI 10.1109/TMM.2018.2812605
   Zhang L, 2019, IEEE T CIRC SYST VID, V29, P1339, DOI 10.1109/TCSVT.2018.2842206
   Zhang XL, 2020, IEEE T CYBERNETICS, V50, P3855, DOI 10.1109/TCYB.2020.2992433
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhen M., 2020, IEEE C COMPUT VIS PA, P13666, DOI DOI 10.48550/ARXIV.2004.07684
   Zhu JJ, 2019, IEEE INT CON MULTI, P610, DOI 10.1109/ICME.2019.00111
   Zlateski A, 2018, PROC CVPR IEEE, P1479, DOI 10.1109/CVPR.2018.00160
   Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI [10.1007/978-3-030-01219-9_, 10.1007/978-3-030-01219-9_18]
NR 86
TC 24
Z9 24
U1 2
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 968
EP 980
DI 10.1109/TMM.2021.3061816
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100035
OA Green Published
DA 2024-07-18
ER

PT J
AU Chen, XY
   Li, HL
   Wu, QB
   Meng, FM
   Qiu, HQ
AF Chen, Xiaoyu
   Li, Hongliang
   Wu, Qingbo
   Meng, Fanman
   Qiu, Heqian
TI Bal-R<SUP>2</SUP>CNN: High Quality Recurrent Object Detection With
   Balance Optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural network; deep learning; object detection; object
   recognition
ID NMS
AB It is a common practice to refine object detection results using recurrent detection paradigm. We evaluate the recurrent detection on Faster R-CNN, but the improvement is far away from expected. We consider that the performance bottleneck is from imbalance optimization caused by the biased distribution of training data. Low-IoU-skewed RPN proposals could suppress the contribution of High-IoU examples at the training stage. Besides, data imbalance and statistical discrepancy on regression targets between low-IoU and high-IoU examples are not considered in the regression task; this design could impede localization quality. In this work, we propose Bal-(RCNN)-C-2 for high-quality recurrent object detection. There are two new components in Bal-(RCNN)-C-2. Self-iteration box sampling collects object boxes from recurrent steps and increases the number of high-IoU training examples. loll-sensitive bounding-box regression sends proposal boxes with different IOUs to specified regression branches for more accurate hounding-box prediction. Both two new components could induce balanced optimization and be helpful. With the resulting Bal-(RCNN)-C-2 detector, evaluation on PASCAL VOC and MSCOCO reveal that our method has a significant improvement on the existing solution and could reach a better performance than several state-of-the-art methods.
C1 [Chen, Xiaoyu; Li, Hongliang; Wu, Qingbo; Meng, Fanman; Qiu, Heqian] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Li, HL (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
EM xychen@std.uestc.edu.cn; hlli@uestc.edu.cn; qbwu@uestc.edu.cn;
   fmmeng@uestc.edu.cn; hqqiu@std.uestc.edu.cn
RI Wu, Qingbo/AAF-6872-2019
OI Wu, Qingbo/0000-0003-2936-6340; Qiu, Heqian/0000-0002-0963-0311; Li,
   Hongliang/0000-0002-7481-095X
FU National Natural Science Foundation of China [61831005, 61971095,
   61871078, 61871087]
FX This work was supported in part by National Natural Science Foundation
   of China under Grants 61831005, 61971095, 61871078, and 61871087. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. ManzurMurshed.
CR [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], 2018, IEEE T MULTIMEDIA, DOI DOI 10.1109/TMM.2018.2811621
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Branco Paula, 2017, Proceedings of the First International Workshop on Learning with Imbalanced Domains: Theory and Applications, P36
   Cao Y., 2020, P IEEE CVF C COMP VI, P583
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Chen KA, 2019, PROC CVPR IEEE, P5114, DOI 10.1109/CVPR.2019.00526
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Dollár P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Gidaris S., 2016, arXiv preprint arXiv:1606.04446
   Gidaris S, 2016, PROC CVPR IEEE, P789, DOI 10.1109/CVPR.2016.92
   Gidaris S, 2015, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2015.135
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Girshick Ross, 2018, Detectron
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Jiang BR, 2018, LECT NOTES COMPUT SC, V11218, P816, DOI 10.1007/978-3-030-01264-9_48
   Kong T, 2017, PROC CVPR IEEE, P5244, DOI 10.1109/CVPR.2017.557
   Kong T, 2016, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2016.98
   Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1
   Li YH, 2019, IEEE I CONF COMP VIS, P6053, DOI 10.1109/ICCV.2019.00615
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu ST, 2019, PROC CVPR IEEE, P6452, DOI 10.1109/CVPR.2019.00662
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lu X, 2019, PROC CVPR IEEE, P7355, DOI 10.1109/CVPR.2019.00754
   Meng F, 2018, BMVC, P83
   MONIZ N., 2017, P MACHINE LEARNING R, V74, P129, DOI DOI 10.1016/J.JBI.2004.07.008
   Najibi M, 2016, PROC CVPR IEEE, P2369, DOI 10.1109/CVPR.2016.260
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Qiu HQ, 2020, IEEE T MULTIMEDIA, V22, P3039, DOI 10.1109/TMM.2020.2971175
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang JQ, 2019, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2019.00308
   Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41
   Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975
   Yoo D, 2015, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2015.305
   Zhang SJ, 2020, I S MOD ANAL SIM COM, P1, DOI 10.1109/mascots50786.2020.9285955
   Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442
   Zhang XS, 2019, ADV NEUR IN, V32
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zhou P, 2018, PROC CVPR IEEE, P528, DOI 10.1109/CVPR.2018.00062
NR 52
TC 9
Z9 10
U1 5
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1558
EP 1569
DI 10.1109/TMM.2021.3067439
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200024
DA 2024-07-18
ER

PT J
AU Hu, BW
   Liu, P
   Zheng, ZD
   Ren, MW
AF Hu, Bingwen
   Liu, Ping
   Zheng, Zhedong
   Ren, Mingwu
TI SPG-VTON: Semantic Prediction Guidance for Multi-Pose Virtual Try-on
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Clothing; Faces; Fitting; Training; Shape; Image synthesis;
   End-to-end; multi-pose; semantic prediction; virtual try-on
AB Image-based virtual try-on is challenging in fitting a target in-shop clothes onto a reference person under diverse human poses. Previous works focus on preserving clothing details (e.g., texture, logos, patterns) when transferring desired clothes onto a target person under a fixed pose. However, the performances of existing methods significantly dropped when extending existing methods to multi-pose virtual try-on. In this paper, we propose an end-to-end Semantic Prediction Guidance multi-pose Virtual Try-On Network (SPG-VTON), which can fit the desired clothing into a reference person under arbitrary poses. Specifically, SPG-VTON is composed of three sub-modules. First, a Semantic Prediction Module (SPM) generates the desired semantic map. The predicted semantic map provides more abundant guidance to locate the desired clothing region and produce a coarse try-on image. Second, a Clothes Warping Module (CWM) warps in-shop clothes to the desired shape according to the predicted semantic map and the desired pose. Specifically, we introduce a conductible cycle consistency loss to alleviate the misalignment in the clothing warping process. Third, a Try-on Synthesis Module (TSM) combines the coarse result and the warped clothes to generate the final virtual try-on image, preserving details of the desired clothes and under the desired pose. In addition, we introduce a face identity loss to refine the facial appearance and maintain the identity of the final virtual try-on result at the same time. We evaluate the proposed method on the most massive multi-pose dataset (MPV) and the DeepFashion dataset. The qualitative and quantitative experiments show that SPG-VTON is superior to the state-of-the-art methods and is robust to data noise, including background and accessory changes, i.e., hats and handbags, showing good scalability to the real-world scenario.
C1 [Hu, Bingwen] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210092, Peoples R China.
   [Hu, Bingwen; Liu, Ping; Zheng, Zhedong] Univ Technol Sydney, ReLER Lab, AAII, Ultimo, NSW 2007, Australia.
   [Liu, Ping] ASTAR, Ctr Frontier Artificial Intelligence Res CFAR, Singapore 138632, Singapore.
   [Zheng, Zhedong] Natl Univ Singapore, Sch Comp, NExT, Singapore 118404, Singapore.
   [Ren, Mingwu] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210092, Peoples R China.
C3 Nanjing University of Science & Technology; University of Technology
   Sydney; Agency for Science Technology & Research (A*STAR); National
   University of Singapore; Nanjing University of Science & Technology
RP Ren, MW (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210092, Peoples R China.
EM hubw.sky@gmail.com; pino.pingliu@gmail.com; zdzheng@nus.edu.sg;
   renmingwu@njust.edu.cn
RI hu, bingwen/KVB-0878-2024; Zheng, Zhedong/R-5314-2019
OI hu, bingwen/0000-0003-4890-1775; Zheng, Zhedong/0000-0002-2434-9050
FU National Natural Science Foundation of China [61773117, 61703209];
   National Major Scientific Instruments and Equipments Development Project
   of China [61727802]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61773117 and 61703209, and in part by
   the National Major Scientific Instruments and Equipments Development
   Project of China under Grant 61727802. The Guest Editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Jian Zhang.
CR Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chou CL, 2022, IEEE T NEUR NET LEAR, V33, P4584, DOI 10.1109/TNNLS.2021.3058379
   Dong H., ARXIV190600884, V2019
   Dong H., 2018, NeurIPS, P474
   Dong H, 2019, IEEE I CONF COMP VIS, P9025, DOI 10.1109/ICCV.2019.00912
   Esser P, 2018, PROC CVPR IEEE, P8857, DOI 10.1109/CVPR.2018.00923
   Gao X., 2021, P 29 ACM INT C MULT
   Ge YY, 2021, PROC CVPR IEEE, P8481, DOI 10.1109/CVPR46437.2021.00838
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Grigorev A, 2019, PROC CVPR IEEE, P12127, DOI 10.1109/CVPR.2019.01241
   Han XT, 2019, IEEE I CONF COMP VIS, P10470, DOI 10.1109/ICCV.2019.01057
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Han Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7847, DOI 10.1109/CVPR42600.2020.00787
   Hu BW, 2021, IEEE T CYBERNETICS, V51, P4373, DOI 10.1109/TCYB.2020.2995496
   Huang ZK, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P652
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Issenhuth Thibaut, 2019, ARXIV PREPRINT ARXIV
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kingma D. P., 2014, arXiv
   Kingma D. P., 2013, ARXIV13126114
   Li PK, 2022, IEEE T PATTERN ANAL, V44, P3260, DOI 10.1109/TPAMI.2020.3048039
   Liu M, 2019, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2019.00379
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Lu ZW, 2015, PATTERN RECOGN, V48, P605, DOI 10.1016/j.patcog.2014.08.019
   Ma LQ, 2017, ADV NEUR IN, V30
   Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mescheder L, 2018, PR MACH LEARN RES, V80
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Qi YK, 2018, IEEE T IMAGE PROCESS, V27, P3857, DOI 10.1109/TIP.2018.2797482
   Rocco I, 2017, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2017.12
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Salimans T, 2016, ADV NEUR IN, V29
   Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359
   Simonyan K, 2015, IEEE INT C ICLR
   Song SJ, 2019, PROC CVPR IEEE, P2352, DOI 10.1109/CVPR.2019.00246
   Ulyanov Dmitry, 2016, arXiv
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3350, DOI 10.1145/3474085.3475490
   Yan YC, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P199, DOI 10.1145/3123266.3123277
   Yang Y, 2021, FRONT INFORM TECH EL, V22, P1551, DOI 10.1631/FITEE.2100463
   Yu RY, 2019, IEEE I CONF COMP VIS, P10510, DOI 10.1109/ICCV.2019.01061
   Zheng N, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P266, DOI 10.1145/3343031.3350946
   Zheng ZD, 2021, IEEE T MULTIMEDIA, V23, P2683, DOI 10.1109/TMM.2020.3014488
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhou ZH, 2021, IEEE T MULTIMEDIA, V23, P1592, DOI 10.1109/TMM.2020.3001472
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu LC, 2022, IEEE T MULTIMEDIA, V24, P668, DOI 10.1109/TMM.2021.3057503
NR 57
TC 11
Z9 13
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1233
EP 1246
DI 10.1109/TMM.2022.3143712
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ZN1SD
UT WOS:000764821800019
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Huo, XY
   Xie, LX
   Wei, LH
   Zhang, XP
   Chen, X
   Li, H
   Yang, ZJ
   Zhou, WG
   Li, HQ
   Tian, Q
AF Huo, Xinyue
   Xie, Lingxi
   Wei, Longhui
   Zhang, Xiaopeng
   Chen, Xin
   Li, Hao
   Yang, Zijie
   Zhou, Wengang
   Li, Houqiang
   Tian, Qi
TI Heterogeneous Contrastive Learning: Encoding Spatial Information for
   Compact Visual Representations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Semantics; Head; Contrastive learning; pre-training;
   spatial information
AB Unsupervised pretraining is of great significance for visual representation. Especially, contrastive learning has achieved great success recently, but existing approaches mostly ignored spatial information which is often crucial for visual representation. Strong semantic embedding has an inherent advantage for classification, but dense prediction tasks require more spatial and low-level representation. This paper presents heterogeneous contrastive learning (HCL), an effective approach that adds spatial information to the encoding stage to alleviate the learning inconsistency between the contrastive objective and strong data augmentation operations. We demonstrate the effectiveness of HCL by showing that (i) it achieves higher accuracy in instance discrimination, (ii) it surpasses existing pre-training methods in a series of downstream tasks (iii) and it shrinks the pre-training costs by half for almost 800 GPU-hours. More importantly, we show that our approach achieves higher efficiency in visual representations, and thus delivers a key message to inspire the future research of self-supervised visual representation learning.
C1 [Huo, Xinyue; Wei, Longhui; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
   [Xie, Lingxi; Zhang, Xiaopeng; Chen, Xin; Tian, Qi] Huawei Cloud, Shenzhen 518100, Guangdong, Peoples R China.
   [Li, Hao] Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China.
   [Yang, Zijie] Chinese Acad Sci, Dept Comp & Control, Beijing 100089, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Huawei Technologies; Shanghai Jiao Tong University; Chinese
   Academy of Sciences
RP Tian, Q (corresponding author), Huawei Cloud, Shenzhen 518100, Guangdong, Peoples R China.
EM xinyueh@mail.ustc.edu.cn; 198808xc@gmail.com; weilh2568@gmail.com;
   zxphistory@sjtu.edu.cn; chenxin061@gmail.com; lihao0374@sjtu.edu.cn;
   yangzijie@ict.ac.cn; zhwg@ustc.edu.cn; lihq@ustc.edu.cn;
   tian.qil@huawei.com
RI Li, Houqiang Li/B-6259-2013
OI Xinyue, Huo/0000-0003-1724-9438
FU National Key R&D Program of China [2017YFB1002202]; National Natural
   Science Foundation of China [61822208]; GPU cluster built by MCC
   Laboratory of Information Science and Technology Institution, USTC
FX This work was supported in part by the National Key R&D Program of China
   under Contract 2017YFB1002202, in part by the National Natural Science
   Foundation of China under Contract 61822208, and in part by the GPU
   cluster built by MCC Laboratory of Information Science and Technology
   Institution, USTC.
CR [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen XL, 2020, Arxiv, DOI arXiv:2003.04297
   Chen XL, 2020, Arxiv, DOI arXiv:2011.10566
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Cruz RS, 2017, PROC CVPR IEEE, P6044, DOI 10.1109/CVPR.2017.640
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Dong L, 2018, IEEE T MULTIMEDIA, V20, P2012, DOI 10.1109/TMM.2017.2788205
   Dosovitskiy A, 2014, ADV NEUR IN, V27
   Fernando B, 2017, PROC CVPR IEEE, P5729, DOI 10.1109/CVPR.2017.607
   Grill J.B., 2020, arXiv, DOI DOI 10.48550/ARXIV.2006.07733
   Gupta A, 2019, PROC CVPR IEEE, P5351, DOI 10.1109/CVPR.2019.00550
   He K., 2017, P IEEE INT C COMPUTE, P2961
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang PH, 2018, PROC CVPR IEEE, P2821, DOI 10.1109/CVPR.2018.00298
   Huang YF, 2021, IEEE T MULTIMEDIA, V23, P176, DOI 10.1109/TMM.2020.2981994
   Huo XY, 2021, PROC CVPR IEEE, P1235, DOI 10.1109/CVPR46437.2021.00129
   Iandola F, 2014, Arxiv, DOI [arXiv:1404.1869, DOI 10.48550/ARXIV.1404.1869]
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Khosla P., 2020, arXiv
   Komodakis N., 2018, P INT C LEARN REPR I, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larsson G, 2017, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2017.96
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee HY, 2017, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2017.79
   Li H, 2021, Arxiv, DOI arXiv:2011.02697
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Long J., 2015, P IEEE C COMP VIS PA, P3431, DOI DOI 10.48550/ARXIV.1411.4038
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12
   Malisiewicz T., 2009, Advances in Neural Information Processing Systems, P1222
   Min EX, 2018, IEEE ACCESS, V6, P39501, DOI 10.1109/ACCESS.2018.2855437
   Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32
   Ng PC, 2003, NUCLEIC ACIDS RES, V31, P3812, DOI 10.1093/nar/gkg509
   Noroozi M, 2018, PROC CVPR IEEE, P9359, DOI 10.1109/CVPR.2018.00975
   Noroozi M, 2017, IEEE I CONF COMP VIS, P5899, DOI 10.1109/ICCV.2017.628
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Pathak D, 2017, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2017.638
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Qi GJ, 2019, IEEE I CONF COMP VIS, P8129, DOI 10.1109/ICCV.2019.00822
   Qi GJ, 2015, IEEE T MULTIMEDIA, V17, P1873, DOI 10.1109/TMM.2015.2485538
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Roh B, 2021, Arxiv, DOI arXiv:2103.06122
   Rosenfeld, 2005, SEMISUPERVISED LEARN
   Shen J, 2011, ACM MULTIMEDIA, P639, DOI DOI 10.1145/2072298.2072405
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tian K, 2017, LECT NOTES ARTIF INT, V10535, P809, DOI 10.1007/978-3-319-71246-8_49
   Tian YL, 2020, Arxiv, DOI arXiv:2005.10243
   Tian YL, 2020, Arxiv, DOI arXiv:1906.05849
   Wang T, 2020, INT C MACH LEARN, P9929, DOI DOI 10.1109/CVPR.2019.00516
   Wang WR, 2015, PR MACH LEARN RES, V37, P1083
   Wang X, 2022, Arxiv, DOI arXiv:2104.07713
   Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320
   Wang XL, 2021, Arxiv, DOI arXiv:2011.09157
   Wei C, 2019, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2019.00201
   Wei LH, 2020, Arxiv, DOI arXiv:2011.08621
   Wu Y., 2019, DETECTRON2
   Xie QK, 2021, IEEE T MULTIMEDIA, V23, P597, DOI 10.1109/TMM.2020.2985525
   Xie ZD, 2021, Arxiv, DOI arXiv:2011.10043
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P924, DOI 10.1109/TPAMI.2020.3013379
   Ye M, 2020, PROC CVPR IEEE, P5456, DOI 10.1109/CVPR42600.2020.00550
   Ye M, 2019, PROC CVPR IEEE, P6203, DOI 10.1109/CVPR.2019.00637
   Zhang LH, 2019, PROC CVPR IEEE, P2542, DOI 10.1109/CVPR.2019.00265
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhu ZL, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-78703-6
NR 70
TC 5
Z9 5
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4224
EP 4235
DI 10.1109/TMM.2021.3115335
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 4K8TP
UT WOS:000852215000006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Shao, HC
   Wang, HC
   Su, WT
   Lin, CW
AF Shao, Hao-Chiang
   Wang, Hsin-Chieh
   Su, Weng-Tai
   Lin, Chia-Wen
TI Ensemble Learning With Manifold-Based Data Splitting for Noisy Label
   Correction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Ensemble learning; label noise; data splitting; graph representation;
   label correction
AB Label noise in training data can significantly degrade a model's generalization performance for supervised learning tasks. Here we focus on the problem that noisy labels are primarily caused by mislabeled confusing samples, which tend to be concentrated near decision boundaries rather than uniformly distributed, and whose features should be equivocal. To address the problem, we propose an ensemble learning method to correct noisy labels by exploiting the local structures of feature manifolds. Different from typical ensemble strategies that increase the prediction diversity among sub-models via certain loss terms, our method trains sub-models on disjoint subsets, each being a union of randomly selected seed samples' nearest-neighbors of the same class on the data manifold. As a result, only a limited number of sub-models will be affected by locally-concentrated noisy labels, and each sub-model can learn a coarse representation of the data manifold along with a corresponding graph. The constructed graphs are used to suggest a set of label correction candidates, and accordingly, our method determines label correction results by majority decisions. Our experiments on real-world noisy label datasets demonstrate the superiority of the proposed method over existing state-of-the-arts.
C1 [Shao, Hao-Chiang] Fu Jen Catholic Univ, Dept Stat & Informat Sci, New Taipei 242, Taiwan.
   [Wang, Hsin-Chieh; Su, Weng-Tai] Nation Tsing Hua Univ, Dept Elect Engn, Hsinchu 300, Taiwan.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 300, Taiwan.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Inst Commun Engn, Hsinchu 300, Taiwan.
   [Lin, Chia-Wen] Ind Technol Res Inst, Elect & Optoelect Syst Res Labs, Hsinchu, Taiwan.
C3 Fu Jen Catholic University; National Tsing Hua University; National
   Tsing Hua University; National Tsing Hua University; Industrial
   Technology Research Institute - Taiwan
RP Lin, CW (corresponding author), Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 300, Taiwan.; Lin, CW (corresponding author), Natl Tsing Hua Univ, Inst Commun Engn, Hsinchu 300, Taiwan.
EM shao.haochiang@gmail.com; lucas85062055@gmail.com;
   wengtai2008@hotmail.com; cwlin@ee.nthu.edu.tw
RI ; Lin, Chia-Wen/M-4571-2013
OI Shao, Hao-Chiang/0000-0002-3749-234X; Lin, Chia-Wen/0000-0002-9097-2318
FU Ministry of Science and Technology, Taiwan [110-2634-F-007-015]
FX This work was supported in part by the Ministry of Science and
   Technology, Taiwan, under Grant 110-2634-F-007-015. The Guest Editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Xian-Sheng Hua.
CR Arazo E, 2019, PR MACH LEARN RES, V97
   Bontonou M., 2019, INTRO GRAPH SMOOTHNE
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Chen FH, 2018, IEEE T MULTIMEDIA, V20, P997, DOI 10.1109/TMM.2017.2757769
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Douze M, 2018, PROC CVPR IEEE, P3349, DOI 10.1109/CVPR.2018.00353
   Erdem A, 2012, NEURAL COMPUT, V24, P700, DOI 10.1162/NECO_a_00233
   Ghosh A, 2017, AAAI CONF ARTIF INTE, P1919
   Han JF, 2019, IEEE I CONF COMP VIS, P5137, DOI 10.1109/ICCV.2019.00524
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D, 2018, ADV NEUR IN, V31
   Iscen A, 2019, PROC CVPR IEEE, P5065, DOI 10.1109/CVPR.2019.00521
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Laine S., 2016, ARXIV161002242
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee KH, 2018, PROC CVPR IEEE, P5447, DOI 10.1109/CVPR.2018.00571
   Li J., 2020, DIVIDEMIX LEARNING N
   Li JN, 2019, PROC CVPR IEEE, P5046, DOI 10.1109/CVPR.2019.00519
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu S., 2020, Early-learning regularization prevents memorization of noisy labels, V33, P20331
   Nguyen D. T., 2019, SELF LEARNING FILTER
   Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Settles B., TR1648 U WISC MAD
   Shao H.-C., CONFUSING LABEL NOIS
   Sharma K., 2020, NOISERANK UNSUPERVIS
   Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582
   Tarvainen A, 2017, ADV NEUR IN, V30
   Wang R., IEEE T MULTIMEDIA, V23, P2021
   Wang RX, 2018, IEEE T NEUR NET LEAR, V29, P2568, DOI 10.1109/TNNLS.2017.2699783
   Wang YS, 2019, IEEE I CONF COMP VIS, P322, DOI 10.1109/ICCV.2019.00041
   Wei Hongxin, 2020, P IEEE C COMP VIS PA
   Xia XB, 2019, ADV NEUR IN, V32
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Yan Y, 2016, AAAI CONF ARTIF INTE, P2244
   Yi K, 2019, PROC CVPR IEEE, P7010, DOI 10.1109/CVPR.2019.00718
   Zhang Hongyi, 2017, ARXIV171009412
   Zhang WH, 2019, PROC CVPR IEEE, P7365, DOI 10.1109/CVPR.2019.00755
   Zhang ZL, 2018, ADV NEUR IN, V31
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zizhao Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9291, DOI 10.1109/CVPR42600.2020.00931
NR 43
TC 3
Z9 3
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1127
EP 1140
DI 10.1109/TMM.2021.3119861
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ZN1SD
UT WOS:000764821800010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tang, YQ
   Xie, Y
   Zhang, CY
   Zhang, WS
AF Tang, Yongqiang
   Xie, Yuan
   Zhang, Chenyang
   Zhang, Wensheng
TI Constrained Tensor Representation Learning for Multi-View
   Semi-Supervised Subspace Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Representation learning; Tensors; Correlation; Clustering algorithms;
   Minimization; Task analysis; Optimization; Multi-view learning; pairwise
   constraint; semi-supervised clustering; subspace learning; tensor
   singular value decomposition (t-SVD)
ID SELF-REPRESENTATION; CLASSIFICATION; SCALE
AB Multi-view subspace clustering is an effective method to partition data into their corresponding categories. Nevertheless, existing multi-view subspace clustering approaches generally operate in a purely unsupervised manner, while ignoring the valuable weakly supervised information that can be readily obtained in many practical applications. In this paper, we consider the weakly supervised form of sample pair constraints, and devote to promoting the performance of multi-view subspace clustering with the aid of such prior knowledge. To achieve this goal, inspired by the intrinsic block diagonal structure of ideal low-rank representation (LRR), we propose a novel regularization to integrate must-link, cannot-link and normalization constraints into a unified formulation. The proposed regularization can be regarded as a general description for sample pairwise constraints, and thus provides a flexible framework for multi-view semi-supervised subspace clustering task. Furthermore, we devise a contrained tensor representation learning (CTRL) model that takes advantage of our proposed regularization to facilitate the learning of the desired representation tensor. An efficient optimization algorithm based on alternating direction minimization strategy is carefully designed to solve the proposed CTRL model. Extensive experiments on eight challenging real-world datasets are conducted, and the results validate the effectiveness of our designed pairwise constraints regularization, as well as the superiority of the proposed CTRL model.
C1 [Tang, Yongqiang] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
   [Xie, Yuan] East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.
   [Zhang, Chenyang; Zhang, Wensheng] Chinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing 100190, Peoples R China.
   [Zhang, Chenyang; Zhang, Wensheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 101408, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; East China
   Normal University; Chinese Academy of Sciences; Institute of Automation,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS
RP Tang, YQ (corresponding author), Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.; Zhang, WS (corresponding author), Chinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing 100190, Peoples R China.
EM yongqiang.tang@ia.ac.cn; yxie@cs.ecnu.edu.cn;
   zhangchenyang2016@ia.ac.cn; zhangwenshengia@hotmail.com
RI Zhang, Chenyang/AAJ-1371-2020
OI tang, yong qiang/0000-0001-9333-8200
FU National Key Research and Development Program of China [2018AAA0102100];
   Key-Area Research and Development Program of Guangdong Province
   [2019B010153002]; National Natural Science Foundation of China
   [62106266, 61961160707, 61976212, U1936206, 62006139, 62176092];
   Shanghai Science andTechnology Commission [21511100700]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018AAA0102100, in part by the
   Key-Area Research and Development Program of Guangdong Province under
   Grant 2019B010153002, in part by the National Natural Science Foundation
   of China under Grants 62106266, 61961160707, 61976212, U1936206,
   62006139, 62176092, and in part by Shanghai Science andTechnology
   Commission underGrant 21511100700.
CR Amini M. R., 2009, NEURIPS, V22, P1, DOI DOI 10.5555/2984093.2984097
   [Anonymous], 2011, NUMERICAL ANAL, DOI DOI 10.1017/CBO9781107415324.004
   [Anonymous], 2003, P 18 INT JOINT C ART
   [Anonymous], 2001, P 18 INT C MACH LEAR
   [Anonymous], 2008, Proceedings of the 25th international conference on Machine learning
   [Anonymous], 2012, Artificial Intelligence and Statistics
   Bair E, 2013, WIRES COMPUT STAT, V5, P349, DOI 10.1002/wics.1270
   Basu S, 2009, CH CRC DATA MIN KNOW, P1
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863
   Cai D, 2005, IEEE T KNOWL DATA EN, V17, P1624, DOI 10.1109/TKDE.2005.198
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Chao Guoqing, 2021, IEEE Trans Artif Intell, V2, P146, DOI 10.1109/tai.2021.3065894
   Chao GQ, 2019, INFORM SCIENCES, V494, P278, DOI 10.1016/j.ins.2019.04.039
   Chen CH, 2016, MATH PROGRAM, V155, P57, DOI 10.1007/s10107-014-0826-5
   Chen YY, 2021, IEEE T IMAGE PROCESS, V30, P4022, DOI 10.1109/TIP.2021.3068646
   Chen YY, 2022, IEEE T NEUR NET LEAR, V33, P4712, DOI 10.1109/TNNLS.2021.3059874
   Chen YY, 2020, IEEE T MULTIMEDIA, V22, P1985, DOI 10.1109/TMM.2019.2952984
   Davidson I, 2009, DATA MIN KNOWL DISC, V18, P257, DOI 10.1007/s10618-008-0103-4
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Du L, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3476
   Dunn J. C., 1974, Journal of Cybernetics, V4, P95, DOI 10.1080/01969727408546059
   Gao HC, 2015, IEEE I CONF COMP VIS, P4238, DOI 10.1109/ICCV.2015.482
   HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Kang Z, 2022, IEEE T CYBERNETICS, V52, P8976, DOI 10.1109/TCYB.2021.3061660
   Kilmer ME, 2013, SIAM J MATRIX ANAL A, V34, P148, DOI 10.1137/110837711
   Kilmer ME, 2011, LINEAR ALGEBRA APPL, V435, P641, DOI 10.1016/j.laa.2010.09.020
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LADES M, 1993, IEEE T COMPUT, V42, P300, DOI 10.1109/12.210173
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Li ZG, 2009, PROC CVPR IEEE, P421, DOI 10.1109/CVPRW.2009.5206852
   Lin Z., 2011, ADV NEURAL INFORM PR, V24, P612, DOI DOI 10.1007/S11263-013-0611-6
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu HF, 2018, IEEE T PATTERN ANAL, V40, P2469, DOI 10.1109/TPAMI.2017.2763945
   Liu XW, 2020, IEEE T PATTERN ANAL, V42, P1191, DOI 10.1109/TPAMI.2019.2892416
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu CY, 2019, IEEE T PATTERN ANAL, V41, P487, DOI 10.1109/TPAMI.2018.2794348
   Lu ZW, 2013, INT J COMPUT VISION, V103, P306, DOI 10.1007/s11263-012-0602-z
   Mao L., 2019, Multiview Machine Learning
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Peng X, 2019, PR MACH LEARN RES, V97
   Qi XB, 2014, IEEE T PATTERN ANAL, V36, P2199, DOI 10.1109/TPAMI.2014.2316826
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Semerci O, 2014, IEEE T IMAGE PROCESS, V23, P1678, DOI 10.1109/TIP.2014.2305840
   Shental N, 2004, ADV NEUR IN, V16, P465
   Simonyan K, 2015, IEEE INT C ICLR
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   Tang YQ, 2022, IEEE T CYBERNETICS, V52, P9179, DOI 10.1109/TCYB.2021.3053057
   Tang YQ, 2021, IEEE T KNOWL DATA EN, V33, P1223, DOI 10.1109/TKDE.2019.2937027
   Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739
   Wang H, 2020, IEEE T KNOWL DATA EN, V32, P1116, DOI 10.1109/TKDE.2019.2903810
   Wang J, 2017, IEEE T CYBERNETICS, V47, P4534, DOI 10.1109/TCYB.2016.2618852
   Wu JX, 2011, IEEE T PATTERN ANAL, V33, P1489, DOI 10.1109/TPAMI.2010.224
   Xiao SJ, 2016, IEEE T NEUR NET LEAR, V27, P2268, DOI 10.1109/TNNLS.2015.2472284
   Xiao XL, 2021, IEEE T NEUR NET LEAR, V32, P1325, DOI 10.1109/TNNLS.2020.2984625
   Xie Y, 2021, IEEE T NEUR NET LEAR, V32, P868, DOI 10.1109/TNNLS.2020.2979685
   Xie Y, 2018, INT J COMPUT VISION, V126, P1157, DOI 10.1007/s11263-018-1086-2
   Xu C., 2013, Neural Computing and Applications, V23, P2031, DOI DOI 10.1007/S00521-013-1362-6
   Yang SJ, 2019, IEEE T MULTIMEDIA, V21, P2916, DOI 10.1109/TMM.2019.2912735
   Zhan K, 2019, IEEE T IMAGE PROCESS, V28, P1261, DOI 10.1109/TIP.2018.2877335
   Zhang CQ, 2020, INT J COMPUT VISION, V128, P2344, DOI 10.1007/s11263-020-01307-0
   Zhang CQ, 2020, IEEE T PATTERN ANAL, V42, P86, DOI 10.1109/TPAMI.2018.2877660
   Zhang CQ, 2015, IEEE I CONF COMP VIS, P1582, DOI 10.1109/ICCV.2015.185
   Zhang ZM, 2014, PROC CVPR IEEE, P3842, DOI 10.1109/CVPR.2014.485
   Zhao HD, 2017, AAAI CONF ARTIF INTE, P2921
   Zhou P, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4105
NR 66
TC 13
Z9 13
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3920
EP 3933
DI 10.1109/TMM.2021.3110098
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400018
DA 2024-07-18
ER

PT J
AU Xue, ZF
   Mao, WJ
   Zheng, L
AF Xue, Zhenfeng
   Mao, Weijie
   Zheng, Liang
TI Learning to Simulate Complex Scenes for Street Scene Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Engines; Lighting; Roads; Data models; Semantics; Buildings; Training;
   Content adaptation; data simulation; synthetic dataset search
AB Data simulation engines like Unity are becoming an increasingly important data source that allows us to acquire ground truth labels conveniently. Moreover, we can flexibly edit the content of an image in the engine, such as objects (position, orientation) and environments (illumination, occlusion). When using simulated data as training sets, its editable content can be leveraged to mimick the distribution of real-world data, and thus reduce the content difference between the synthetic and real domains. This paper explores content adaptation in the context of semantic segmentation, where the complex street scenes are fully synthesized using 19 classes of virtual objects from a first person driver perspective and controlled by 23 attributes. To optimize the attribute values and obtain a training set of similar content to real-world data, we propose a scalable discretization-and-relaxation (SDR) approach. Under a reinforcement learning framework, we formulate attribute optimization as a random-to-optimized mapping problem using a neural network. Our method has three characteristics. 1) Instead of editing attributes of individual objects, we focus on global attributes that have large influence on the scene structure, such as object density and illumination. 2) Attributes are quantized to discrete values, so as to reduce search space and training complexity. 3) Correlated attributes are jointly optimized in a group, so as to avoid meaningless scene structures and find better convergence points. Experiment shows our system can generate reasonable and useful scenes, from which we obtain promising real-world segmentation accuracy compared with existing synthetic training sets.
C1 [Xue, Zhenfeng; Mao, Weijie] Zhejiang Univ, Inst Cyber Syst & Control, State Key Lab Ind Control Technol, Hangzhou 310027, Peoples R China.
   [Zheng, Liang] Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT 0200, Australia.
C3 Zhejiang University; Australian National University
RP Mao, WJ (corresponding author), Zhejiang Univ, Inst Cyber Syst & Control, State Key Lab Ind Control Technol, Hangzhou 310027, Peoples R China.
EM zfxue0903@zju.edu.cn; wjmao@zju.edu.cn; liang.zheng@anu.edu.au
OI Xue, Zhenfeng/0000-0002-9593-9429; Mao, Weijie/0000-0001-5791-1823;
   Zheng, Liang/0000-0002-1464-9500
FU National Natural Science Foundation of China [61633019]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61633019. The associate editor coordinating the
   reviewof this manuscript and approving it for publication was Houqiang
   Li.
CR Bak S, 2018, LECT NOTES COMPUT SC, V11217, P193, DOI 10.1007/978-3-030-01261-8_12
   Barbosa IB, 2018, COMPUT VIS IMAGE UND, V167, P50, DOI 10.1016/j.cviu.2017.12.002
   Brostow GJ, 2008, LECT NOTES COMPUT SC, V5302, P44, DOI 10.1007/978-3-540-88682-2_5
   Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen YH, 2019, PROC CVPR IEEE, P1841, DOI 10.1109/CVPR.2019.00194
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Dosovitskiy A., 2017, P 1 ANN C ROB LEARN, P1, DOI DOI 10.48550/ARXIV.1711.03938
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Gaidon A, 2016, PROC CVPR IEEE, P4340, DOI 10.1109/CVPR.2016.470
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Hattori H, 2015, PROC CVPR IEEE, P3819, DOI 10.1109/CVPR.2015.7299006
   Hensel M, 2017, ADV NEUR IN, V30
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Hoffmann Johannes, 2016, 2016 Conference on Precision Electromagnetic Measurements (CPEM), P1, DOI 10.1109/CPEM.2016.7540615
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Juliani A., 2018, ARXIV180902627
   Kar A, 2019, IEEE I CONF COMP VIS, P4550, DOI 10.1109/ICCV.2019.00465
   Kolve E, 2017, AI2 THOR INTERACTIVE
   Li P., 2018, ARXIV PREPRINT ARXIV
   Li YJ, 2017, ADV NEUR IN, V30
   Luo YW, 2019, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2019.00261
   Pepik B, 2012, PROC CVPR IEEE, P3362, DOI 10.1109/CVPR.2012.6248075
   Prakash A, 2019, IEEE INT CONF ROBOT, P7249, DOI [10.1109/icra.2019.8794443, 10.1109/ICRA.2019.8794443]
   Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7
   Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352
   Ruiz N., 2019, P INT C LEARN REPR
   Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8
   Satkin S, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.128
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sultana M, 2021, IEEE T MULTIMEDIA, V23, P2005, DOI 10.1109/TMM.2020.3006419
   Sun XX, 2019, PROC CVPR IEEE, P608, DOI 10.1109/CVPR.2019.00070
   Tobin J, 2017, IEEE INT C INT ROBOT, P23
   Tremblay J, 2018, IEEE COMPUT SOC CONF, P1082, DOI 10.1109/CVPRW.2018.00143
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Wright SJ, 2015, MATH PROGRAM, V151, P3, DOI 10.1007/s10107-015-0892-3
   Wu Y, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351296
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522
   Yao Y., 2020, EUR C COMP VIS, P775, DOI DOI 10.1007/978-3-030
   Zhang YH, 2018, PROC CVPR IEEE, P6810, DOI 10.1109/CVPR.2018.00712
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI [10.1007/978-3-030-01219-9_, 10.1007/978-3-030-01219-9_18]
NR 53
TC 5
Z9 5
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1253
EP 1265
DI 10.1109/TMM.2021.3062497
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200001
DA 2024-07-18
ER

PT J
AU Zhang, HY
   Chau, LP
   Wang, DW
AF Zhang, Haoyuan
   Chau, Lap-Pui
   Wang, Danwei
TI Soft Warping Based Unsupervised Domain Adaptation for Stereo Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Three-dimensional displays; Task analysis; Pipelines; Neural
   networks; Adversarial machine learning; Feature extraction; stereo
   matching; unsupervised domain adaptation; adversarial learning; soft
   warping loss
ID ACCURATE
AB Stereo matching is a practical method to estimate depth information and retrieve 3D world in robot perception and autonomous driving scenarios. With the development of convolution neural networks (CNNs), deep-learning based stereo matching algorithms have significantly improved the accuracy and dominated most of the online benchmarks. However, limited labels in real world, especially in challenging weather conditions, still hinder the technology from practical usage. In this paper, we propose a new unsupervised learning mechanism for stereo matching, utilizing adversarial iterative learning and novel soft warping loss to promote the effectiveness of the networks in unseen environments. The experiments transferring the stereo matching module from synthetic domain to real-world domain demonstrate the superiority of our proposed method. Extensive experiments in challenging weathers further prove that our method shows great practical potential in strait environments.
C1 [Zhang, Haoyuan; Chau, Lap-Pui; Wang, Danwei] Nanyang Technol Univ, Dept Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Chau, LP (corresponding author), Nanyang Technol Univ, Dept Elect & Elect Engn, Singapore 639798, Singapore.
EM haoyuan001@e.ntu.edu.sg; elpchau@ntu.edu.sg; edwwang@ntu.edu.sg
RI wang, dan/JEF-0836-2023
OI Chau, Lap-Pui/0000-0003-4932-0593; Wang, Danwei/0000-0003-3400-0079
CR Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Black M. J., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P231, DOI 10.1109/ICCV.1993.378214
   Black MJ, 1996, INT J COMPUT VISION, V19, P57, DOI 10.1007/BF00131148
   Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567
   Chen L, 2017, IEEE T INTELL TRANSP, V18, P3093, DOI 10.1109/TITS.2017.2680538
   Duggal S, 2019, IEEE I CONF COMP VIS, P4383, DOI 10.1109/ICCV.2019.00448
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Geiger A., 2015, The kitti vision benchmark suite, V2
   Gretton A, 2008, Arxiv, DOI [arXiv:0805.2368, 10.48550/arXiv.0805.2368]
   Guo J, 2018, Arxiv, DOI arXiv:1809.02256
   Hannah M. J, 1974, THESIS
   Hirschmüller H, 2007, PROC CVPR IEEE, P2134
   Hirschmüller H, 2005, PROC CVPR IEEE, P807, DOI 10.1109/cvpr.2005.56
   Hoffman J, 2018, Arxiv, DOI arXiv:1805.08727
   KANADE T, 1994, IEEE T PATTERN ANAL, V16, P920, DOI 10.1109/34.310690
   Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17
   Kim T, 2017, PR MACH LEARN RES, V70
   Kordelas GA, 2016, IEEE T MULTIMEDIA, V18, P155, DOI 10.1109/TMM.2015.2505905
   Lai HY, 2019, PROC CVPR IEEE, P1890, DOI 10.1109/CVPR.2019.00199
   Lee Z, 2015, IEEE T MULTIMEDIA, V17, P792, DOI 10.1109/TMM.2015.2425141
   Lee Z, 2014, IEEE T MULTIMEDIA, V16, P2168, DOI 10.1109/TMM.2014.2355131
   Liu R., 2020, P IEEE CVF C COMP VI, P12757
   Luo WJ, 2016, PROC CVPR IEEE, P5695, DOI 10.1109/CVPR.2016.614
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Pan Y, 2017, IEEE T MULTIMEDIA, V19, P685, DOI 10.1109/TMM.2016.2646179
   Piatkowska E, 2017, IEEE COMPUT SOC CONF, P370, DOI 10.1109/CVPRW.2017.51
   Radford A, 2016, Arxiv, DOI [arXiv:1511.06434, DOI 10.48550/ARXIV.1511.06434]
   Rao ZB, 2020, APSIPA TRANS SIGNAL, V9, DOI 10.1017/ATSIP.2020.16
   Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Shaked A, 2017, PROC CVPR IEEE, P6901, DOI 10.1109/CVPR.2017.730
   Simoncelli E. P., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P310, DOI 10.1109/CVPR.1991.139707
   Sun BC, 2016, AAAI CONF ARTIF INTE, P2058
   Tonioni A, 2019, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2019.00028
   Tonioni A, 2019, PROC CVPR IEEE, P9653, DOI 10.1109/CVPR.2019.00989
   Tonioni A, 2017, IEEE I CONF COMP VIS, P1614, DOI 10.1109/ICCV.2017.178
   Tulyakov S, 2018, Arxiv, DOI arXiv:1806.01677
   Xu HF, 2020, PROC CVPR IEEE, P1956, DOI 10.1109/CVPR42600.2020.00203
   Yan JC, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P167, DOI 10.1145/2911996.2912035
   Yang GR, 2019, PROC CVPR IEEE, P899, DOI 10.1109/CVPR.2019.00099
   Ye XQ, 2017, IEEE ACCESS, V5, P18745, DOI 10.1109/ACCESS.2017.2754318
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zbontar J, 2016, J MACH LEARN RES, V17
   Zbontar J, 2015, PROC CVPR IEEE, P1592, DOI 10.1109/CVPR.2015.7298767
   Zhao H, 2019, PR MACH LEARN RES, V97
   Zhong Y., 2017, arXiv, DOI 10.48550/arXiv.1709.00930(2017).1709.00930
   Zhou C, 2017, IEEE I CONF COMP VIS, P1576, DOI 10.1109/ICCV.2017.174
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 49
TC 5
Z9 5
U1 5
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3835
EP 3846
DI 10.1109/TMM.2021.3108900
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400013
DA 2024-07-18
ER

PT J
AU Zhang, ZZ
   Lan, CL
   Zeng, WJ
   Chen, ZB
   Chang, SF
AF Zhang, Zhizheng
   Lan, Cuiling
   Zeng, Wenjun
   Chen, Zhibo
   Chang, Shih-Fu
TI Beyond Triplet Loss: Meta Prototypical N-Tuple Loss for Person
   Re-identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Person re-identification; loss design; metric learning
ID NETWORK
AB Person Re-identification (ReID) aims at matching a person of interest across images. In convolutional neural network (CNN) based approaches, loss design plays a vital role in pulling closer features of the same identity and pushing far apart features of different identities. In recent years, triplet loss achieves superior performance and is predominant in ReID. However, triplet loss considers only three instances of two classes in per-query optimization (with an anchor sample as query) and it is actually equivalent to a two-class classification. There is a lack of loss design which enables the joint optimization of multiple instances (of multiple classes) within per-query optimization for person ReID. In this paper, we introduce a multi-class classification loss, i.e., N-tuple loss, to jointly consider multiple (N) instances for per-query optimization. This in fact aligns better with the ReID test/inference process, which conducts the ranking/comparisons among multiple instances. Furthermore, for more efficient multiclass classification, we propose a new meta prototypical N-tuple loss. With the multi-class classification incorporated, our model achieves the state-of-the-art performance on the benchmark person ReID datasets
C1 [Zhang, Zhizheng; Chen, Zhibo] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230026, Anhui, Peoples R China.
   [Lan, Cuiling; Zeng, Wenjun] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Chang, Shih-Fu] Columbia Univ, New York, NY 10027 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft Research Asia; Microsoft; Columbia University
RP Chen, ZB (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230026, Anhui, Peoples R China.; Lan, CL (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.
EM zhizheng@mail.ustc.edu.cn; culan@microsoft.com; wezeng@microsoft.com;
   chenzhibo@ustc.edu.cn; sc250@columbia.edu
RI Lan, Cuiling/KCK-5597-2024; Zhang, Zhizheng/AGZ-8479-2022
FU National Key Research and Development Program of China
   [2018AAA01014-00]; NSFC [U1908209, 62021001]
FX This work was supported in part by the National Key Research and
   Development Program of China 2018AAA01014-00 and NSFC under Grants
   U1908209 and 62021001.
CR Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016
   Almazan J, 2018, Arxiv, DOI arXiv:1801.05339
   Bai X, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107036
   Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Do TT, 2019, PROC CVPR IEEE, P10396, DOI 10.1109/CVPR.2019.01065
   Fang PF, 2019, IEEE I CONF COMP VIS, P8029, DOI 10.1109/ICCV.2019.00812
   Goldberger J., 2004, P INT C NEUR INF PRO, V17, P513
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LX, 2018, Arxiv, DOI arXiv:1810.07399
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jia J., 2019, PROC BRIT MACH VIS C
   Jin X, 2020, AAAI CONF ARTIF INTE, V34, P11173
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Khamis S, 2015, LECT NOTES COMPUT SC, V8927, P134, DOI 10.1007/978-3-319-16199-0_10
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lawen H, 2020, Arxiv, DOI arXiv:1910.07038
   Li W., 2017, arXiv
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Lingxiao He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P357, DOI 10.1007/978-3-030-58604-1_22
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Paisitkriangkrai S, 2015, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2015.7298794
   Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29
   Qian X., 2020, P AS C COMP VIS NOV, P71
   Qiao SY, 2018, PROC CVPR IEEE, P7229, DOI 10.1109/CVPR.2018.00755
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Snell J, 2017, ADV NEUR IN, V30
   Sohn K, 2016, ADV NEUR IN, V29
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23
   Wang FQ, 2016, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2016.144
   Wang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1041, DOI 10.1145/3123266.3123359
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang Y, 2018, PROC CVPR IEEE, P8042, DOI [10.1109/CVPR.2018.00839, 10.1109/CVPR.2018.00736]
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279
   Weinberger K.Q., 2006, Advances in neural information processing systems, P1473
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Yan C., 2021, TMM
   Yang QZ, 2021, IEEE T PATTERN ANAL, V43, P2029, DOI 10.1109/TPAMI.2019.2960509
   Yao HT, 2019, IEEE T IMAGE PROCESS, V28, P2860, DOI 10.1109/TIP.2019.2891888
   Yu R, 2018, LECT NOTES COMPUT SC, V11220, P196, DOI 10.1007/978-3-030-01270-0_12
   Yuan Y., 2019, In
   Zhai Y, 2019, IEEE COMPUT SOC CONF, P1526, DOI 10.1109/CVPRW.2019.00194
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zhang ZZ, 2019, PROC CVPR IEEE, P667, DOI 10.1109/CVPR.2019.00076
   Zhao CR, 2020, IEEE T MULTIMEDIA, V22, P3180, DOI 10.1109/TMM.2020.2972125
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zheng L, 2016, Arxiv, DOI arXiv:1610.02984
   Zheng L, 2018, IEEE T PATTERN ANAL, V40, P1224, DOI 10.1109/TPAMI.2017.2709749
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Liang, 2017, P IEEE C COMP VIS PA, P3346, DOI DOI 10.1109/CVPR.2017.357
   Zheng Z., 2018, T CIRCUITS SYST VIDE
   Zheng ZD, 2017, Arxiv, DOI arXiv:1701.07717
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou SP, 2017, PROC CVPR IEEE, P5028, DOI 10.1109/CVPR.2017.534
NR 72
TC 6
Z9 6
U1 2
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4158
EP 4169
DI 10.1109/TMM.2021.3115451
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 4K8TP
UT WOS:000852215000001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhao, WL
   Wang, H
   Ngo, CW
AF Zhao, Wan-Lei
   Wang, Hui
   Ngo, Chong-Wah
TI Approximate <i>k</i>-NN Graph Construction: A Generic Online Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Quantization (signal); Indexing; Task analysis; Nearest
   neighbor methods; Approximation algorithms; Time complexity; k-nearest
   neighbor graph; nearest neighbor search; high-dimensional; NN-descent
ID NEAREST-NEIGHBOR SEARCH; TREES
AB Nearest neighbor search and k-nearest neighbor graph construction are two fundamental issues that arise from many disciplines such as multimedia information retrieval, data-mining, and machine learning. They become more and more imminent given the big data emerge in various fields in recent years. In this paper, a simple but effective solution both for approximate k-nearest neighbor search and approximate k-nearest neighbor graph construction is presented. These two issues are addressed jointly in our solution. On one hand, the approximate k-nearest neighbor graph construction is treated as a search task. Each sample along with its k-nearest neighbors is joined into the k-nearest neighbor graph by performing the nearest neighbor search sequentially on the graph under construction. On the other hand, the built k-nearest neighbor graph is used to support k-nearest neighbor search. Since the graph is built online, the dynamic update on the graph, which is not possible for most of the existing solutions, is supported. This solution is feasible for various distance measures. Its effectiveness both as k-nearest neighbor construction and k-nearest neighbor search approaches is verified across different types of data in different scales, various dimensions, and under different metrics.
C1 [Zhao, Wan-Lei; Wang, Hui] Xiamen Univ, Dept Comp Sci & Technol, Xiamen 361005, Peoples R China.
   [Ngo, Chong-Wah] Singapore Management Univ, Sch Comp & Informat Syst, Singapore, Singapore.
C3 Xiamen University; Singapore Management University
RP Zhao, WL (corresponding author), Xiamen Univ, Dept Comp Sci & Technol, Xiamen 361005, Peoples R China.
EM wlzhao@xmu.edu.cn; hwang2019@stu.xmu.edu.cn; cscwngo@cityu.edu.hk
OI Wang, Hui/0000-0001-8982-0571
FU National Natural Science Foundation of China [61572408, 61972326];
   Xiamen University [20720180074]
FX This work was supported in part by National Natural Science Foundation
   of China under Grants 61572408 and 61972326, and in part by Xiamen
   University under Grant 20720180074.
CR Amato Giuseppe., 2016, Proceedings of the 2016 ACM Workshop on Multimedia COMMONS, P11
   [Anonymous], 2009, P ACM INT C IM VID R, DOI DOI 10.1145/1646396.1646421
   Aumüller M, 2017, LECT NOTES COMPUT SC, V10609, P34, DOI 10.1007/978-3-319-68474-1_3
   Babenko A, 2016, PROC CVPR IEEE, P2055, DOI 10.1109/CVPR.2016.226
   BECKMANN N, 1990, SIGMOD REC, V19, P322, DOI 10.1145/93605.98741
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Bernhardsson E., 2017, Annoy: approximate nearest neighbors in C++/Python optimized for memory usage and loading/saving to disk
   Chen J, 2009, J MACH LEARN RES, V10, P1989
   Chen YJ, 2010, SENSORS-BASEL, V10, P11259, DOI 10.3390/s101211259
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   COMER D, 1979, COMPUT SURV, V11, P121, DOI 10.1145/356770.356776
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Debatty T., 2016, ARXIV160206819 CORR
   Dong W., 2011, P 20 INT C WORLD WID, P577
   Fu C, 2019, PROC VLDB ENDOW, V12, P461, DOI 10.14778/3303753.3303754
   Fu Cong, 2016, ABS160907228 CORR
   Gray RM, 1998, IEEE T INFORM THEORY, V44, P2325, DOI 10.1109/18.720541
   Guttman Antonin., 1984, P 1984 ACM SIGMOD C, P47
   Hajebi Kiana, 2011, P 22 INT JOINT C ART, P1312
   Harwood B, 2016, PROC CVPR IEEE, P5713, DOI 10.1109/CVPR.2016.616
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Johnson J, 2021, IEEE T BIG DATA, V7, P535, DOI 10.1109/TBDATA.2019.2921572
   Levina Elizaveta, 2004, Advances in neural information processing systems, V17
   Li W, 2020, IEEE T KNOWL DATA EN, V32, P1475, DOI 10.1109/TKDE.2019.2909204
   Lin P.-C., 2019, COMP STUDY HIERARCHI
   Little J. J., ARXIV14112173
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo CZ, 2016, IEEE T MULTIMEDIA, V18, P40, DOI 10.1109/TMM.2015.2495248
   Lv Q, 2007, P 33 INT C VER LARG, P950
   Malkov YA, 2020, IEEE T PATTERN ANAL, V42, P824, DOI 10.1109/TPAMI.2018.2889473
   Malkov Y, 2014, INFORM SYST, V45, P61, DOI 10.1016/j.is.2013.10.006
   Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376
   Paredes R, 2006, LECT NOTES COMPUT SC, V4007, P85
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Silpa-Anan C, 2008, PROC CVPR IEEE, P2308
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Sun YF, 2014, PROC VLDB ENDOW, V8, P1
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Wang J, 2012, PROC CVPR IEEE, P1106, DOI 10.1109/CVPR.2012.6247790
   Wang Jingdong, 2012, Acm multimedia 2012, P179
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wieschollek P, 2016, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2016.223
   Xie HT, 2019, IEEE T MULTIMEDIA, V21, P1248, DOI 10.1109/TMM.2018.2872898
   Yan-Ming Zhang, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference (ECML PKDD 2013). Proceedings: LNCS 8189, P660, DOI 10.1007/978-3-642-40991-2_42
   Zhang T, 2014, PR MACH LEARN RES, V32, P838
NR 47
TC 7
Z9 7
U1 15
U2 75
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1909
EP 1921
DI 10.1109/TMM.2021.3073811
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200010
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Hofbauer, H
   Autrusseau, F
   Uhl, A
AF Hofbauer, Heinz
   Autrusseau, Florent
   Uhl, Andreas
TI Low Quality and Recognition of Image Content
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Encryption; Databases; Observers; Image recognition; Cryptography;
   Visualization; Distortion; Selective encryption; image recognition;
   image quality; human visual system; visual quality indices
ID ENCRYPTION
AB Assessment of visual encryption of video and image content requires a reliable estimation of content recognizability and low quality. As pointed out in the literature, current methods are insufficient and research into this topic, as well as into the relation between low quality and recognizability, is still lacking. This lack of research is primarily due to a lack of data. To improve on the status-quo we have taken a recognizability database and performed a subjective quality evaluation on a subset of the images. This gives us a new database with both subjective recognizability and quality information and allows to delve into the relation between low quality and recognizability. We analyze the relationship between quality and recognizability as well as the predictive quality of state of the art visual quality indices. We show that the visual quality indices are poor indicators for the estimation of recognizability. Furthermore, we show that they must be a poor fit because of the disparity between two distinct perceptual tasks: quality and recognizability.
C1 [Hofbauer, Heinz; Uhl, Andreas] Paris Lodron Univ Salzburg, Dept Comp Sci, A-5020 Salzburg, Austria.
   [Autrusseau, Florent] Univ Nantes, Polytech Nantes, LTeN, F-44306 Nantes, France.
   [Autrusseau, Florent] Univ Nantes, INSERM, RMeS, UMR1229, F-44306 Nantes, France.
C3 Salzburg University; Nantes Universite; Institut National de la Sante et
   de la Recherche Medicale (Inserm); Ecole Nationale Veterinaire,
   Agroalimentaire et de l'Alimentation Nantes-Atlantique; Nantes
   Universite
RP Hofbauer, H (corresponding author), Paris Lodron Univ Salzburg, Dept Comp Sci, A-5020 Salzburg, Austria.
EM hofbauer@cs.sbg.ac.at; Florent.Autrusseau@univ-nantes.fr;
   uhl@cs.sbg.ac.at
RI Autrusseau, Florent/S-1200-2018
OI Autrusseau, Florent/0000-0002-2690-0029
FU Austrian Science Fund [P27776]; Austrian Science Fund (FWF) [P27776]
   Funding Source: Austrian Science Fund (FWF)
FX This work was supported in part by the Austrian Science Fund, Project
   no. P27776.
CR Abdmouleh MK, 2017, I C COMP GRAPH IM VI, P79, DOI 10.1109/CGiV.2017.10
   Abraham AS, 2017, 2017 INTERNATIONAL CONFERENCE ON NETWORKS & ADVANCES IN COMPUTATIONAL TECHNOLOGIES (NETACT), P387, DOI 10.1109/NETACT.2017.8076801
   B. Series, 2012, Recommendation ITU-R BT, V500, P500
   Barten Peter G. J, 1999, Contrast sensitivity of the human eye and its effects on image quality
   Bay H., 2008, COMPUT VIS IMAGE UND, V10, P346, DOI DOI 10.1016/j.cviu.2007.09.014
   Carosi M, 2010, PROC SPIE, V7542, DOI 10.1117/12.838627
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Chikkerur S, 2011, IEEE T BROADCAST, V57, P165, DOI 10.1109/TBC.2011.2104671
   Corsini M, 2007, IEEE T MULTIMEDIA, V9, P247, DOI 10.1109/TMM.2006.886261
   Damera-Venkata N, 2000, IEEE T IMAGE PROCESS, V9, P636, DOI 10.1109/83.841940
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Guo SW, 2020, IEEE T INF FOREN SEC, V15, P1151, DOI 10.1109/TIFS.2019.2935415
   Chehreghani MH, 2020, MACH LEARN, V109, P1779, DOI 10.1007/s10994-020-05895-3
   He JH, 2018, IEEE T MULTIMEDIA, V20, P2645, DOI 10.1109/TMM.2018.2817065
   Hofbauer Heinz, 2018, 2018 7th European Workshop on Visual Information Processing (EUVIP). Proceedings, DOI 10.1109/EUVIP.2018.8611779
   Hofbauer Heinz, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1986, DOI 10.1109/ICASSP.2014.6853946
   Hofbauer H., 2016, International Conference of the Biometrics Special Interest Group (BIOSIG), P1
   Hofbauer H., 2020, INFORM SCI, P128
   Hofbauer H, 2018, PROCEEDINGS OF THE 6TH ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY (IH&MMSEC'18), P139, DOI 10.1145/3206004.3206007
   Hofbauer H, 2016, SIGNAL PROCESS-IMAGE, V46, P60, DOI 10.1016/j.image.2016.05.001
   ITU, 2002, BT50011 ITU
   Jenisch S, 2014, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2014-6
   Kotel S, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION TECHNOLOGY (CIT), P375, DOI 10.1109/CIT.2016.45
   Lee JS, 2014, IEEE T MULTIMEDIA, V16, P564, DOI 10.1109/TMM.2013.2292590
   Lewis JP, 1994, PROC CANAD IMAG PROC, P120
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MATTHEWS BW, 1975, BIOCHIM BIOPHYS ACTA, V405, P442, DOI 10.1016/0005-2795(75)90109-9
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Pearson K., 1895, P R SOC LOND, V58, P240, DOI 10.1098/rspl.1895.0041
   Peng F, 2020, IEEE T CIRC SYST VID, V30, P2765, DOI 10.1109/TCSVT.2019.2924910
   Ponomarenko Nikolay, 2013, 2013 4th European Workshop on Visual Information Processing (EUVIP), P106
   Powers D.M.W., 2007, Technical Report, V2, P37
   Sallam AI, 2018, IEEE T MULTIMEDIA, V20, P1636, DOI 10.1109/TMM.2017.2777470
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Spearman C, 1904, AM J PSYCHOL, V15, P72, DOI 10.2307/1412159
   Stütz T, 2010, MM&SEC 2010: 2010 ACM SIGMM MULTIMEDIA AND SECURITY WORKSHOP, PROCEEDINGS, P247
   Stuetz T, 2007, MM&SEC'07: PROCEEDINGS OF THE MULTIMEDIA & SECURITY WORKSHOP 2007, P97
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Sun J, 2011, MULTIMED TOOLS APPL, V53, P75, DOI 10.1007/s11042-010-0491-5
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Telecommunication Standardization Sector of ITU, 1996, TEL TRANSM QUAL AUD
   Tian SS, 2019, IEEE T MULTIMEDIA, V21, P1235, DOI 10.1109/TMM.2018.2875307
   VQEG contributors, 2010, HYBR PERC BITSTR GRO
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu JJ, 2019, FRONT COMPUT SCI-CHI, V13, P4, DOI 10.1007/s11704-016-6213-z
   Xiang T, 2020, IEEE T CIRC SYST VID, V30, P4129, DOI 10.1109/TCSVT.2019.2955298
   Xiang T, 2016, IEEE T INF FOREN SEC, V11, P951, DOI 10.1109/TIFS.2016.2515503
   Yang C, 2021, IEEE T MULTIMEDIA, V23, P1557, DOI 10.1109/TMM.2020.3001537
   Yao Y, 2009, INFORM-J COMPUT INFO, V33, P69
   Yue GH, 2019, IEEE T MULTIMEDIA, V21, P2184, DOI 10.1109/TMM.2019.2913315
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
NR 52
TC 5
Z9 5
U1 1
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG 12
PY 2021
VL 24
BP 3595
EP 3610
DI 10.1109/TMM.2021.3103394
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NR
UT WOS:000824707600002
OA Green Submitted, hybrid
DA 2024-07-18
ER

PT J
AU Huang, D
   Feng, XY
   Zhang, HX
   Yu, ZT
   Peng, JY
   Zhao, GY
   Xia, ZQ
AF Huang, Dong
   Feng, Xiaoyi
   Zhang, Haixi
   Yu, Zitong
   Peng, Jinye
   Zhao, Guoying
   Xia, Zhaoqiang
TI Spatio-Temporal Pain Estimation Network With Measuring Pseudo Heart Rate
   Gain
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Pain; Estimation; Feature extraction; Visualization; Physiology; Videos;
   Three-dimensional displays; Pain estimation; pseudo modality;
   spatio-temporal network; 3D convolution and attention; probabilistic
   inference
ID INTENSITY ESTIMATION; RECOGNITION
AB Pain is a significant indicator that shows people are suffering from an unwell experience and its automatic estimation has attracted much interest in recent years. Of late, most estimation methods are designed to capture the dynamic pain information from visual signals while a few physiological-signal based methods can provide extra potential cues to analyze the pain more accurately. However, it is still challenging to capture the physiological data from patients as it requires contact devices and patients' cooperation. In this paper, we propose to leverage the pseudo physiological information by generating new modal data from the original visual videos and jointly estimating the pain by an end-to-end network. To extract the representations from bi-modal data, we design a spatio-temporal pain estimation network, which employs a dual-branch framework for extracting pain-aware visual and pseudo physiological features separately and fuses the features in a probabilistic way. The inherent vital sign, i.e., heart rate gain (HRG), from pseudo physiological information can be utilized as an auxiliary signal and integrated with the visual pain estimation framework. Moreover, specially-designed 3D convolution filters and attention structures are employed to extract spatio-temporal features for both branches. To use the HRG as an auxiliary way for pain estimation, we propose a probabilistic inference model by jointly considering the visual branch and physiological branch, which makes our model estimate the pain comprehensively. Experiments on two publicly-available datasets show the effectiveness of introducing the pseudo modality, and the proposed method can outperform the state-of-the-art methods.
C1 [Huang, Dong; Feng, Xiaoyi; Xia, Zhaoqiang] Northwestern Polytech Univ, Sch Elect & Informat, Xian 710129, Peoples R China.
   [Zhang, Haixi] Northwest A&F Univ, Coll Informat Engn, Yangling 712100, Shaanxi, Peoples R China.
   [Yu, Zitong; Zhao, Guoying] Univ Oulu, Ctr Machine Vis & Signal Anal, Oulu 90014, Finland.
   [Peng, Jinye] Northwest Univ, Sch Informat Sci & Technol, Xian 710127, Peoples R China.
C3 Northwestern Polytechnical University; Northwest A&F University - China;
   University of Oulu; Northwest University Xi'an
RP Xia, ZQ (corresponding author), Northwestern Polytech Univ, Sch Elect & Informat, Xian 710129, Peoples R China.
EM huangdong1007785@gmail.com; fengxiao@nwpu.edu.cn;
   zh.haixi@nnwafu.edu.cn; zitong.yu@oulu.fi; pjyxida@nwu.edu.cn;
   guoying.zhao@oulu.fi; xiazhaoqiang@gmail.com
RI Zhang, Haixi/HGB-7131-2022; Xia, Zhaoqiang/AAC-4021-2019; Peng,
   Jin/HZH-6965-2023
OI Xia, Zhaoqiang/0000-0003-0630-3339; Yu, Zitong/0000-0001-6505-3304;
   Zhao, Guoying/0000-0003-3694-206X
FU Key Research and Development Program of Shaanxi [2021ZDLGY15-01,
   2021ZDLGY09-04, 2021GY-004, 2020GY-050]; International Science and
   Technology Cooperation Research Project of Shenzhen
   [GJHZ20200731095204013]; National Natural Science Foundation of China
   [61772419]; Academy of Finland [316765, 328115]
FX This work was supported in part by the Key Research and Development
   Program of Shaanxi under Programs 2021ZDLGY15-01, 2021ZDLGY09-04,
   2021GY-004, and 2020GY-050, the International Science and Technology
   Cooperation Research Project of Shenzhen (GJHZ20200731095204013), the
   National Natural Science Foundation of China under Grant 61772419,
   Academy of Finland for project MiGA under Grant 316765, and ICT 2023
   project under Grant 328115, Infotech Oulu.
CR Appelhans BM, 2008, BIOL PSYCHOL, V77, P174, DOI 10.1016/j.biopsycho.2007.10.004
   Ashraf AB, 2009, IMAGE VISION COMPUT, V27, P1788, DOI 10.1016/j.imavis.2009.05.007
   Baltrusaitis T, 2018, IEEE INT CONF AUTOMA, P59, DOI 10.1109/FG.2018.00019
   Chang CY, 2009, CIBCB: 2009 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN BIOINFORMATICS AND COMPUTATIONAL BIOLOGY, P278
   Chu YQ, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00279
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Florea C, 2015, LECT NOTES COMPUT SC, V8927, P778, DOI 10.1007/978-3-319-16199-0_54
   Hammal Z, 2012, PATTERN RECOGN, V45, P1265, DOI 10.1016/j.patcog.2011.09.014
   Haque MA, 2018, IEEE INT CONF AUTOMA, P250, DOI 10.1109/FG.2018.00044
   Hassan MA, 2017, BIOMED SIGNAL PROCES, V38, P346, DOI 10.1016/j.bspc.2017.07.004
   Hossain MS, 2019, INFORM FUSION, V49, P69, DOI 10.1016/j.inffus.2018.09.008
   Huang D, 2020, MULTIMED TOOLS APPL, V79, P28329, DOI 10.1007/s11042-020-09397-1
   Huang D, 2019, J ELECTRON IMAGING, V28, DOI 10.1117/1.JEI.28.4.043008
   Irani Ramin, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P80, DOI 10.1109/CVPRW.2015.7301340
   Kächele M, 2016, IEEE J-STSP, V10, P854, DOI 10.1109/JSTSP.2016.2535962
   Kächele M, 2015, LECT NOTES COMPUT SC, V9132, P220, DOI 10.1007/978-3-319-20248-8_19
   Kumar K, 2018, IEEE T MULTIMEDIA, V20, P323, DOI 10.1109/TMM.2017.2741423
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Li XB, 2014, PROC CVPR IEEE, P4264, DOI 10.1109/CVPR.2014.543
   Littlewort GC, 2009, IMAGE VISION COMPUT, V27, P1797, DOI 10.1016/j.imavis.2008.12.010
   Liu D., 2017, Proceedings of the 1st IJCAI Workshop on Artificial Intelligence in Affective Computing, Proceedings of Machine Learning Research, (Melbourne, Australia), P1
   Lopez-Martinez D, 2017, INT CONF AFFECT, P181, DOI 10.1109/ACIIW.2017.8272611
   Lucey P, 2012, IMAGE VISION COMPUT, V30, P197, DOI 10.1016/j.imavis.2011.12.003
   Mäntyselka P, 2001, PAIN, V89, P175, DOI 10.1016/S0304-3959(00)00361-4
   Martinez DL, 2017, IEEE COMPUT SOC CONF, P2318, DOI 10.1109/CVPRW.2017.286
   MERSKEY H, 1979, PAIN, V7, P271, DOI 10.1016/0304-3959(79)90084-8
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Prkachin KM, 2008, PAIN, V139, P267, DOI 10.1016/j.pain.2008.04.010
   Rodriguez P., 2017, IEEE METROL AEROSPAC, DOI 10.1109/TCYB.2017.2662199
   Rudovic O, 2015, IEEE T PATTERN ANAL, V37, P944, DOI 10.1109/TPAMI.2014.2356192
   Sullivan MJL, 2006, PAIN, V125, P270, DOI 10.1016/j.pain.2006.06.019
   Tavakolian M, 2020, PATTERN RECOGN LETT, V140, P26, DOI 10.1016/j.patrec.2020.09.012
   Tavakolian M, 2019, INT J COMPUT VISION, V127, P1413, DOI 10.1007/s11263-019-01191-3
   Tavakolian M, 2018, IEEE IMAGE PROC, P1952, DOI 10.1109/ICIP.2018.8451681
   Terkelsen AJ, 2005, AUTON NEUROSCI-BASIC, V121, P101, DOI 10.1016/j.autneu.2005.07.001
   Thiam P, 2020, ICPRAM: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS, P289, DOI 10.5220/0008896102890296
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Tran Du, 2017, ARXIV170805038
   Tzirakis P, 2017, IEEE J-STSP, V11, P1301, DOI 10.1109/JSTSP.2017.2764438
   Von Korff M, 2000, SPINE, V25, P3140, DOI 10.1097/00007632-200012150-00009
   Walter S, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON CYBERNETICS (CYBCONF)
   Wang F, 2017, IEEE IMAGE PROC, P1087, DOI 10.1109/ICIP.2017.8296449
   Werner P, 2017, INT CONF AFFECT, P176, DOI 10.1109/ACIIW.2017.8272610
   Werner P, 2017, IEEE T AFFECT COMPUT, V8, P286, DOI 10.1109/TAFFC.2016.2537327
   Werner P, 2014, INT C PATT RECOG, P4582, DOI 10.1109/ICPR.2014.784
   Xia ZQ, 2020, IEEE T MULTIMEDIA, V22, P626, DOI 10.1109/TMM.2019.2931351
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xuesong Niu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P295, DOI 10.1007/978-3-030-58536-5_18
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yang RJ, 2018, INT C PATT RECOG, P3495, DOI 10.1109/ICPR.2018.8545244
   Yu ZT, 2019, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2019.00024
   Zhao R, 2016, PROC CVPR IEEE, P3466, DOI 10.1109/CVPR.2016.377
   Zheng WL, 2019, IEEE T CYBERNETICS, V49, P1110, DOI 10.1109/TCYB.2018.2797176
   Zhou J, 2016, IEEE COMPUT SOC CONF, P1535, DOI 10.1109/CVPRW.2016.191
   Zwakhalen Sandra M G, 2006, BMC Geriatr, V6, P3, DOI 10.1186/1471-2318-6-3
NR 55
TC 5
Z9 5
U1 3
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 9
PY 2021
VL 24
BP 3300
EP 3313
DI 10.1109/TMM.2021.3096080
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NT
UT WOS:000824707800003
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Somandepalli, K
   Hebbar, R
   Narayanan, S
AF Somandepalli, Krishna
   Hebbar, Rajat
   Narayanan, Shrikanth
TI Robust Character Labeling in Movie Videos: Data Resources and
   Self-Supervised Feature Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video character labeling; Self-supervision; Multiview correlation;
   Triplet loss; Face diarization; face clustering; computational media
   understanding
ID FACE; REPRESENTATION; DIARIZATION
AB Robust face clustering is a vital step in enabling computational understanding of visual character portrayal in media. Face clustering for long-form content is challenging because of variations in appearance and lack of supporting large-scale labeled data. Our work in this paper focuses on two key aspects of this problem: the lack of domain-specific training or benchmark datasets, and adapting face embeddings learned on web images to long-form content, specifically movies. First, we present a dataset of over 169000 face tracks curated from 240 Hollywood movies with weak labels on whether a pair of face tracks belong to the same or a different character. We propose an offline algorithm based on nearest-neighbor search in the embedding space to mine hard-examples from these tracks. We then investigate triplet-loss and multiview correlation-based methods for adapting face embeddings to hard-examples. Our experimental results highlight the usefulness of weakly labeled data for domain-specific feature adaptation. Overall, we find that multiview correlation-based adaptation yields more discriminative and robust face embeddings. Its performance on downstream face verification and clustering tasks is comparable to that of the state-of-the-art results in this domain. We also present the SAIL-Movie Character Benchmark corpus developed to augment existing benchmarks. It consists of racially diverse actors and provides face-quality labels for subsequent error analysis. We hope that the large-scale datasets developed in this work can further advance automatic character labeling in videos. All resources are available freely at https: //saill.use.edu/(similar to)ccmi/multiface.
C1 [Somandepalli, Krishna; Hebbar, Rajat; Narayanan, Shrikanth] Univ Southern Calif, Dept Elect & Comp Engn, Los Angeles, CA 90089 USA.
C3 University of Southern California
RP Somandepalli, K (corresponding author), Univ Southern Calif, Dept Elect & Comp Engn, Los Angeles, CA 90089 USA.
EM somandep@usc.edu; rajatheb@usc.edu; shri@sipi.usc.edu
OI Somandepalli, Krishna/0000-0002-2845-1079
FU Google Research
FX This work was done at the Center for Computational Media Intelligence at
   the University of Southern California's Signal Analysis and
   Interpretation Laboratory (USC SAIL) supported in part by Google
   Research.
CR Aljundi R., 2016, P AS C COMP VIS, P467
   Andrew G., 2013, ICML, P1247
   [Anonymous], 2020 HOLLYWOOD DIVER
   [Anonymous], 2001, MASS COMMUN SOC, DOI DOI 10.1207/S15327825MCS0403_01
   [Anonymous], 2020 FILM HIST GENDE
   [Anonymous], 4 WAYS USE XRAY PRIM
   [Anonymous], 2014, Multimedia and Expo (ICME), 2014 IEEE International Conference on
   Bian J., 2018, PROC 10 INT C DIGIT, V10806, P45
   Bojanowski P, 2013, IEEE I CONF COMP VIS, P2280, DOI 10.1109/ICCV.2013.283
   Bugeau A, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/317278
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chang FJ, 2017, IEEE INT CONF COMP V, P1599, DOI 10.1109/ICCVW.2017.188
   Chang XB, 2018, PROC CVPR IEEE, P1488, DOI 10.1109/CVPR.2018.00161
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Cinbis RG, 2011, IEEE I CONF COMP VIS, P1559, DOI 10.1109/ICCV.2011.6126415
   Cour T, 2010, PROC CVPR IEEE, P1014, DOI 10.1109/CVPR.2010.5540106
   Datta S, 2018, IEEE INT CONF AUTOMA, P135, DOI 10.1109/FG.2018.00029
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng JK, 2020, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR42600.2020.00525
   El Bolock A, 2020, LECT NOTES ELECTR EN, V603, P627, DOI 10.1007/978-981-15-0058-9_60
   El Khoury E, 2014, MULTIMED TOOLS APPL, V68, P747, DOI 10.1007/s11042-012-1080-6
   Everingham M., 2006, BMVC, V2, P6
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Ghaleb E, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P455, DOI 10.1145/2671188.2749296
   Guha T, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P31, DOI 10.1145/2818346.2820778
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Haurilet ML, 2016, IEEE WINT CONF APPL
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Huang G. B., 2008, WORKSH FAC REAL LIF
   Huo J., 2020, 2020 IEEE 23 FUSION, P1
   Kapsouras I, 2017, MULTIMED TOOLS APPL, V76, P2223, DOI 10.1007/s11042-015-3181-5
   Kärkkäinen K, 2021, IEEE WINT CONF APPL, P1547, DOI 10.1109/WACV48630.2021.00159
   Kim KG, 2018, IEEE WINT CONF APPL, P39, DOI 10.1109/WACV.2018.00011
   Kim MJ, 2017, IEEE T AFFECT COMPUT, V8, P216, DOI 10.1109/TAFFC.2016.2519888
   Kose Neslihan, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163104
   Kulshreshtha P, 2020, MULTIMED TOOLS APPL, V79, P33103, DOI 10.1007/s11042-020-09449-6
   Kulshreshtha P, 2018, IEEE IMAGE PROC, P2670, DOI 10.1109/ICIP.2018.8451343
   Kumar K, 2018, MULTIMED TOOLS APPL, V77, P26635, DOI 10.1007/s11042-018-5882-z
   Kumar K, 2018, IEEE T MULTIMEDIA, V20, P323, DOI 10.1109/TMM.2017.2741423
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Nagrani A., 2017, PROC BRIT MACH VIS C
   Nie L., 2019, SYNTHESIS LECT IMAGE, V9, P1
   Nie LQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1192, DOI 10.1145/3123266.3123313
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Pnevmatikakis A, 2009, J VIS COMMUN IMAGE R, V20, P543, DOI 10.1016/j.jvcir.2009.08.001
   Ramanathan V, 2014, LECT NOTES COMPUT SC, V8689, P95, DOI 10.1007/978-3-319-10590-1_7
   Rohrbach A, 2017, PROC CVPR IEEE, P4196, DOI 10.1109/CVPR.2017.447
   Rohrbach A, 2017, INT J COMPUT VISION, V123, P94, DOI 10.1007/s11263-016-0987-1
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sharma R, 2019, IEEE IMAGE PROC, P2991, DOI [10.1109/icip.2019.8803248, 10.1109/ICIP.2019.8803248]
   Sharma V., 2017, PROC COMPUT VIS PATT, V1, P24
   Sharma V, 2019, IEEE INT CONF AUTOMA, P360
   Shi YC, 2019, IEEE I CONF COMP VIS, P6901, DOI 10.1109/ICCV.2019.00700
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Shun Zhang, 2016, Advances in Multimedia Information Processing - PCM 2016. 17th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 9916, P497, DOI 10.1007/978-3-319-48890-5_49
   Sivic J, 2009, PROC CVPR IEEE, P1145, DOI 10.1109/CVPRW.2009.5206513
   Somandepalli K., 2019, ARXIV190401775
   Somandepalli K, 2019, INTERSPEECH, P2320, DOI 10.21437/Interspeech.2019-3130
   Somandepalli K, 2021, P IEEE, V109, P891, DOI 10.1109/JPROC.2020.3047978
   Somandepalli K, 2019, INT CONF ACOUST SPEE, P4065, DOI 10.1109/ICASSP.2019.8682314
   Somandepalli K, 2018, IEEE T MULTIMEDIA, V20, P539, DOI 10.1109/TMM.2017.2745712
   Suris D, 2021, PROC CVPR IEEE, P12602, DOI 10.1109/CVPR46437.2021.01242
   Tapaswi M, 2019, IEEE I CONF COMP VIS, P5026, DOI 10.1109/ICCV.2019.00513
   Tapaswi M, 2012, PROC CVPR IEEE, P2658, DOI 10.1109/CVPR.2012.6247986
   Ul Haq I, 2019, IEEE ACCESS, V7, P9265, DOI 10.1109/ACCESS.2018.2890560
   Vallet F, 2013, IEEE T MULTIMEDIA, V15, P509, DOI 10.1109/TMM.2012.2233724
   Van Noorden R, 2020, NATURE, V587, P354, DOI 10.1038/d41586-020-03187-3
   Vicol P, 2018, PROC CVPR IEEE, P8581, DOI 10.1109/CVPR.2018.00895
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Whitelam C, 2017, IEEE COMPUT SOC CONF, P592, DOI 10.1109/CVPRW.2017.87
   Wolf L, 2011, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2011.5995566
   Wu BY, 2013, PROC CVPR IEEE, P3507, DOI 10.1109/CVPR.2013.450
   Xiang Y, 2015, IEEE I CONF COMP VIS, P4705, DOI 10.1109/ICCV.2015.534
   Xiao SJ, 2014, LECT NOTES COMPUT SC, V8694, P123, DOI 10.1007/978-3-319-10599-4_9
   Yang S, 2016, PROC CVPR IEEE, P5525, DOI 10.1109/CVPR.2016.596
   Yi Dong, 2014, ARXIV14117923
   Youngjae Yu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P543, DOI 10.1007/978-3-030-58558-7_32
   Zhang ZP, 2016, LECT NOTES COMPUT SC, V9907, P236, DOI 10.1007/978-3-319-46487-9_15
   Zhang Z, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2951, DOI 10.1109/ICASSP.2018.8461985
NR 81
TC 1
Z9 1
U1 1
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 9
PY 2021
VL 24
BP 3355
EP 3368
DI 10.1109/TMM.2021.3096155
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NT
UT WOS:000824707800006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chiang, Y
   Hsu, CH
   Wei, HY
AF Chiang, Yao
   Hsu, Chih-Ho
   Wei, Hung-Yu
TI Collaborative Social-Aware and QoE-Driven Video Caching and Adaptation
   in Edge Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-access edge computingg; collaborative caching; quality of
   experience; social network; video adaptation
ID DELIVERY
AB With the emerging demand for high-definition videos in recent years, Multi-access Edge Computing (MEC) has become a promising solution to leverage Quality of Experience (QoE) of users in the 5G mobile network, which provides computing and cache resource at network edges to serve end users with less latency. Also, since mobile users tend to be influenced by the trends in social media, the performance of video caching will become more effective if we can extract the hidden information from interaction among them. In this paper, we propose a novel Collaborative Social-aware QoE-driven video Caching and Adaption (CSQCA) framework. Specifically, we first design a 2-tier MEC collaborative video caching architecture, which partially caches popular videos among multiple edge servers. Second, we propose a social-aware proactive cache strategy, which embeds interactions of users and video dissemination process in social networks into the caching mechanism. Third, a QoE-driven video adaptation algorithm is presented to dynamically transcode the cached videos into appropriate resolution on edge server for each request. Finally, we conduct our simulation based on real-world datasets. The simulation results show that the proposed CSQCA framework outperforms traditional cache algorithms, in terms of the average hit ratio and QoE.
C1 [Chiang, Yao; Hsu, Chih-Ho; Wei, Hung-Yu] Natl Taiwan Univ, Dept Elect Engn, Taipei 10617, Taiwan.
C3 National Taiwan University
RP Wei, HY (corresponding author), Natl Taiwan Univ, Dept Elect Engn, Taipei 10617, Taiwan.
EM d05921015@ntu.edu.tw; smyonlys@gmail.com; hywei@ntu.edu.tw
RI Wei, Hung-Yu/AFI-4515-2022
OI Wei, Hung-Yu/0000-0002-3116-306X; Chiang, Yao/0000-0002-0392-6525
FU Ministry of Science and Technology (MOST) of Taiwan
   [109-2221E-002-148-MY2, 109-2218-E-002-018]
FX This work was support by the from Ministry of Science and Technology
   (MOST) of Taiwan under Grants 109-2221E-002-148-MY2 and
   109-2218-E-002-018.
CR [Anonymous], 2016, 003 ETSI GS MEC
   [Anonymous], CISCO VIRTUAL NETWOR
   Bin Gao, 2019, IEEE INFOCOM 2019 - IEEE Conference on Computer Communications, P1459, DOI 10.1109/INFOCOM.2019.8737543
   Box GEP., 1994, Time Series Analysis, Forecasting and Control, V5th, P712
   Cha M, 2009, IEEE ACM T NETWORK, V17, P1357, DOI 10.1109/TNET.2008.2011358
   Chiang Y, 2020, IEEE ACCESS, V8, P84753, DOI 10.1109/ACCESS.2020.2991986
   Dai J, 2012, IEEE J SEL AREA COMM, V30, P458, DOI 10.1109/JSAC.2012.120226
   Ge C, 2016, PROCEEDINGS OF THE 2016 3RD ACM CONFERENCE ON INFORMATION-CENTRIC NETWORKING (ACM-ICN '16), P237, DOI 10.1145/2984356.2988522
   Han SQ, 2020, IEEE T WIREL COMMUN, V19, P218, DOI 10.1109/TWC.2019.2943552
   He S, 2017, IEEE COMMUN LETT, V21, P1027, DOI 10.1109/LCOMM.2017.2655038
   Hsu H., 2016, P IEEE INT C COMM, P1
   Ilhan N, 2015, PROCEEDINGS OF THE 2015 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM 2015), P1509, DOI 10.1145/2808797.2808913
   Jin YC, 2014, 2014 IEEE 22ND INTERNATIONAL SYMPOSIUM OF QUALITY OF SERVICE (IWQOS), P208, DOI 10.1109/IWQoS.2014.6914321
   Kan TY, 2018, IEEE GLOBE WORK
   Kan TY, 2018, WIRELESS OPTIC COMM, P129
   Kua J, 2017, IEEE COMMUN SURV TUT, V19, P1842, DOI 10.1109/COMST.2017.2685630
   Li CL, 2018, IEEE T MULTIMEDIA, V20, P965, DOI 10.1109/TMM.2017.2757761
   Liang CC, 2017, IEEE T WIREL COMMUN, V16, P6912, DOI 10.1109/TWC.2017.2734081
   Liu FM, 2013, IEEE T COMPUT, V62, P351, DOI 10.1109/TC.2011.222
   Liu FM, 2012, IEEE T PARALL DISTR, V23, P1227, DOI 10.1109/TPDS.2011.283
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Megiddo N, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P115
   Mehrabi A, 2018, IEEE ACCESS, V6, P52261, DOI 10.1109/ACCESS.2018.2870855
   Neglia G, 2018, IEEE ACM T NETWORK, V26, P302, DOI 10.1109/TNET.2017.2783623
   Nie B, 2014, IEEE CONF COMPUT, P97, DOI 10.1109/INFCOMW.2014.6849175
   Pedersen HA, 2016, IEEE ACM T NETWORK, V24, P996, DOI 10.1109/TNET.2015.2410298
   Shen HY, 2014, IEEE T PARALL DISTR, V25, P2428, DOI 10.1109/TPDS.2013.139
   Tan XB, 2010, INT CONF COMP SCI, P259, DOI 10.1109/ICCSIT.2010.5563738
   Tran TX, 2019, IEEE T MOBILE COMPUT, V18, P1965, DOI 10.1109/TMC.2018.2871147
   Tuysuz MF, 2020, IEEE T IND INFORM, V16, P7115, DOI 10.1109/TII.2020.2972931
   Wang BW, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON SMART INTERNET OF THINGS (SMARTIOT 2018), P255, DOI [10.1109/SmartIoT.2018.00053, 10.1109/SmartloT.2018.00027]
   Wang X, 2017, IEEE ACCESS, V5, P8492, DOI 10.1109/ACCESS.2017.2693440
   Wang XF, 2014, IEEE COMMUN MAG, V52, P131, DOI 10.1109/MCOM.2014.6736753
   Wang YF, 2017, IEEE COMMUN LETT, V21, P2266, DOI 10.1109/LCOMM.2017.2705695
   Xu XD, 2017, IEEE ACCESS, V5, P16406, DOI 10.1109/ACCESS.2017.2739343
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zhang N, 2014, INT WIREL COMMUN, P1136, DOI 10.1109/IWCMC.2014.6906514
   Zhang QX, 2019, IEEE INFOCOM SER, P2449, DOI [10.1109/INFOCOM.2019.8737660, 10.1109/infocom.2019.8737660]
   Zhang WW, 2013, IEEE T MULTIMEDIA, V15, P1431, DOI 10.1109/TMM.2013.2247583
   Zhou H, 2018, INT CONF WIRE COMMUN
   Zink M, 2008, PROC SPIE, V6818, DOI 10.1117/12.774903
NR 41
TC 18
Z9 18
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4311
EP 4325
DI 10.1109/TMM.2020.3040532
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900031
DA 2024-07-18
ER

PT J
AU Cui, LZ
   Su, DY
   Yang, S
   Wang, Z
   Ming, Z
AF Cui, Laizhong
   Su, Dongyuan
   Yang, Shu
   Wang, Zhi
   Ming, Zhong
TI TCLiVi: Transmission Control in Live Video Streaming Based on Deep
   Reinforcement Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Live video streaming; reinforcement learning; joint optimization;
   adaptive transmission control
AB Currently, video content accounts for the majority of network traffic. With increased live streaming, rigorous requirements have been introduced for better Quality of Experience (QoE). It is challenging to meet satisfactory QoE in live streaming, where the aim is to achieve a balance between 1) enhancing the video quality and stability and 2) reducing the rebuffering time and end-to-end delay, under different scenarios with various network conditions and user preferences, where the fluctuation in the network throughput degrades the QoE severely. In this paper, we propose an approach to improve the QoE for live video streaming based on Deep Reinforcement Learning (DRL). The new approach jointly adjusts the streaming parameters, including the video bitrate and target buffer size. With the basic DRL framework, TCLiVi can automatically generate the inference model based on the playback information, to achieve the joint optimization of the video quality, stability, rebuffering time and latency parameters. We evaluate our framework on real-world data in different live streaming broadcast scenarios, such as a talent show and a sports competition under different network conditions. We compare TCLiVi with other algorithms, such as the Double DQN, MPC and Buffer-based algorithms. The simulation results show that TCLiVi significantly improves the video quality and decreases the rebuffering time, consequently increasing the QoE score by 40.84% in average. We also show that TCLiVi is self-adaptive in different scenarios.
C1 [Cui, Laizhong; Su, Dongyuan; Yang, Shu; Ming, Zhong] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
   [Cui, Laizhong; Su, Dongyuan; Yang, Shu; Ming, Zhong] Shenzhen Univ, Guangdong Lab Artificial Intelligence & Cyber Eco, Shenzhen 518060, Peoples R China.
   [Wang, Zhi] Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
C3 Shenzhen University; Shenzhen University; Tsinghua Shenzhen
   International Graduate School; Tsinghua University
RP Yang, S (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
EM cuilz@szu.edu.cn; 1900271016@email.szu.edu.cn; yang.shu@szu.edu.cn;
   wangzhi@sz.tsinghua.edu.cn; mingz@szu.edu.cn
RI Cui, Laizhong/AAX-9571-2020; Su, Dongyuan/ABC-6059-2021
OI Su, Dongyuan/0000-0002-3543-5130; Wang, Zhi/0000-0002-5462-6178
FU National Key R&D Program of China [2018YFB1800302, 2018YFB1800805];
   National Natural Science Foundation of China [61772345, 61902258]; Major
   Fundamental Research Project in the Science and Technology Plan of
   Shenzhen [JCYJ20190808142207420]
FX This work was supported in part by the National Key R&D Program of China
   under Grants 2018YFB1800302 and 2018YFB1800805, in part by the National
   Natural Science Foundation of China under Grants 61772345 and 61902258,
   and in part by the Major Fundamental Research Project in the Science and
   Technology Plan of Shenzhen under Grant JCYJ20190808142207420. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Zixiang Xiong.
CR Aguayo M, 2018, IEEE T MULTIMEDIA, V20, P1224, DOI 10.1109/TMM.2017.2764325
   [Anonymous], 1989, LEARNING DELAYED REW
   [Anonymous], 2011, Proc. second annu. acm conf. multimed. syst.-mmsys'11, DOI DOI 10.1145/1943552.1943574
   [Anonymous], 2012, RMSPROP DIVIDE GRADI
   [Anonymous], 2019, GLOBAL TRANSMISSION
   [Anonymous], 2018, 2018 GLOBAL INTERNET
   Balachandran A, 2013, ACM SIGCOMM COMP COM, V43, P339, DOI 10.1145/2534169.2486025
   Chen CC, 2018, TELEMAT INFORM, V35, P293, DOI 10.1016/j.tele.2017.12.003
   Choi W, 2019, IEEE ACCESS, V7, P26830, DOI 10.1109/ACCESS.2019.2901279
   Cisco Visual Networking Index, 2016, CISCO GLOBAL CLOUD I, P1
   De Asis K, 2018, AAAI CONF ARTIF INTE, P2902
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Fu J, 2019, IEEE INT CON MULTI, P290, DOI 10.1109/ICME.2019.00058
   Gadaleta M, 2017, IEEE T COGN COMMUN, V3, P703, DOI 10.1109/TCCN.2017.2755007
   Ge C, 2017, IEEE T MULTIMEDIA, V19, P2222, DOI 10.1109/TMM.2017.2735301
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Han S, 2019, 33RD INTERNATIONAL CONFERENCE ON INFORMATION NETWORKING (ICOIN 2019), P176, DOI [10.1109/ICOIN.2019.8718151, 10.1109/icoin.2019.8718151]
   Hassan K. K., 2002, NONLINEAR SYSTEMS
   HORNIK K, 1990, NEURAL NETWORKS, V3, P551, DOI 10.1016/0893-6080(90)90005-6
   Huang T.Y., 2012, P 2012 ACM C INT MEA, P225, DOI 10.1145/2398776.2398800
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Khan K, 2019, IEEE T MULTIMEDIA, V21, P2012, DOI 10.1109/TMM.2019.2892304
   Kim S, 2019, IEEE T MULTIMEDIA, V21, P442, DOI 10.1109/TMM.2018.2856626
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li J., 2016, EMNLP, DOI [DOI 10.18653/V1/D16-1127.URL, 10.18653/v1/D16-1127, DOI 10.18653/V1/D16-1127]
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Lu Z, 2018, IEEE T MULTIMEDIA, V20, P1848, DOI 10.1109/TMM.2017.2772802
   Ma XT, 2019, PROCEEDINGS OF THE 29TH ACM WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO (NOSSDAV'19), P31, DOI 10.1145/3304112.3325603
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Mnih V, 2013, ARXIV
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Nihei K., 2018, P IEEE GLOB COMM C A, P1
   Palau CE, 2011, MULTIMED TOOLS APPL, V53, P591, DOI 10.1007/s11042-010-0516-0
   Puterman M., 1990, Handb Oper Res Manage Sci, V2, P331
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Sun Y, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P272, DOI 10.1145/2934872.2934898
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   Tian GB, 2016, IEEE ACM T NETWORK, V24, P2386, DOI 10.1109/TNET.2015.2464700
   van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094
   Xu L., 2016, 2016 IEEE International Workshop on Acoustic Signal Enhancement (IWAENC), P1, DOI [DOI 10.1109/IWAENC.2016.7602891, 10.1109/IWAENC.2016.7602891]
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zhang RX, 2019, PROCEEDINGS OF THE 29TH ACM WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO (NOSSDAV'19), P55, DOI 10.1145/3304112.3325607
   Zhang Tong, 2004, P 21 INT C MACH LEAR, P116, DOI DOI 10.1145/1015330.1015332
   Zhang YX, 2019, IEEE INFOCOM SER, P1252, DOI [10.1109/INFOCOM.2019.8737361, 10.1109/infocom.2019.8737361]
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
   Zhu YK, 2017, INT CONF ACOUST SPEE, P5335, DOI 10.1109/ICASSP.2017.7953175
NR 49
TC 27
Z9 27
U1 2
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 651
EP 663
DI 10.1109/TMM.2020.2985631
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QA6NL
UT WOS:000613560200010
DA 2024-07-18
ER

PT J
AU Gao, YQ
   Sang, JT
   Fu, CP
   Wang, ZJ
   Ren, TW
   Xu, CS
AF Gao, Yuqi
   Sang, Jitao
   Fu, Chengpeng
   Wang, Zhengjia
   Ren, Tongwei
   Xu, Changsheng
TI Metadata Connector: Exploiting Hashtag and Tag for Cross-OSN Event
   Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Twitter; Tagging; Videos; YouTube; Flickr; Games; Social media search;
   cross-OSN application; social multimedia
ID ENGINE
AB Social media has revolutionized the way people understand and keep track of real-world events. Various related multimedia information in different modalities such as texts, images and videos is updated on social media and reflects the events. These quantities of information distributes on different Online Social Networks (OSNs), which provides rich, wide coverage, comprehensive information about the trending events. Faced with such large amounts of data, searching has become a handy tool for event understanding and tracking on social media. However, existing single-OSN search mainly involves with single modality on single platform. Moreover, most OSNs usually focus on biased perspective of events, which significantly limits the coverage and diversity of single-OSN based event search. In this paper, we introduce a novel cross-OSN framework to help integrate these cross-OSN information regarding the same event and provide an immersive experience for information retrieval. Since social media information is widely distributed in different OSNs where semantic gap exists among these heterogeneous spaces, we propose to utilize hashtag and tag, which are user-generated metadata for organizing and labeling in many OSNs, as bridges to connect between different OSNs. In our four-stage solution framework, various methods are adopted for hashtag and tag filtering, search results representation, clustering and demonstration. Given an event query, in the first stage we generate related items with corresponding tags and hashtags from OSNs and filter the hashtags and tags we need. Then, topical representation is generated for hashtag and tag. The third stage leverages the derived representation for cross-OSN hashtag and tag clustering. Finally, demonstration for each query is produced and the results are organized hierarchically. Experiments on a dataset containing hundreds of search queries and related items demonstrate the effectiveness of our cross-OSN event search framework.
C1 [Gao, Yuqi; Ren, Tongwei] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210093, Peoples R China.
   [Sang, Jitao; Fu, Chengpeng; Wang, Zhengjia] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.
   [Sang, Jitao; Fu, Chengpeng; Wang, Zhengjia] Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
   [Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
C3 Nanjing University; Beijing Jiaotong University; Beijing Jiaotong
   University; Chinese Academy of Sciences; Institute of Automation, CAS
RP Sang, JT (corresponding author), Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.; Sang, JT (corresponding author), Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
EM gaoyq@smail.nju.edu.cn; jtsang@bjtu.edu.cn; 18120358@bjtu.edu.cn;
   17281195@bjtu.edu.cn; rentw@nju.edu.cn; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; Xu, Chang/GQP-7280-2022
OI Chengpeng, Fu/0000-0001-8883-7363; Wang, Zhengjia/0000-0003-1419-7337;
   xu, chang sheng/0000-0001-8343-9665
FU National Key R&D Program of China [2018AAA0100604]; National Natural
   Science Foundation of China [61832002, 61632004, 61672518, 61602115];
   Natural Science Foundation of Jiangsu Province [BK20191248]; Science,
   Technology and Innovation Commission of Shenzhen Municipality
   [JCYJ20180307151516166]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018AAA0100604, in part by the National Natural Science
   Foundation of China under Grants 61832002, 61632004, 61672518, and
   61602115, in part by the Natural Science Foundation of Jiangsu Province
   under Grant BK20191248, and in part by the Science, Technology and
   Innovation Commission of Shenzhen Municipality under Grant
   JCYJ20180307151516166. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Pradeep K.
   Atrey.
CR Ames M, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P971
   [Anonymous], 2012, ACM MM'12'
   Banerjee A, 2005, J MACH LEARN RES, V6, P1705
   Banerjee A, 2007, J MACH LEARN RES, V8, P1919
   Bao BK, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2730889
   Bar-Ilan J, 2006, COMPUT NETW, V50, P1448, DOI 10.1016/j.comnet.2005.10.020
   Barrington L., 2009, PROC ACM SIGKDD WORK, P7
   Baziotis C., 2017, P 11 INT WORKSHOP SE, P747, DOI [DOI 10.18653/V1/S17-2126, 10.18653/v1/S17-2126]
   Blei DM, 2004, ADV NEUR IN, V16, P17
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Denton E, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1731, DOI 10.1145/2783258.2788576
   DIACONIS P, 1977, J ROY STAT SOC B MET, V39, P262, DOI 10.1111/j.2517-6161.1977.tb01624.x
   Enrich M, 2013, LECT NOTES BUS INF P, V152, P101
   Gao YQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1924, DOI 10.1145/3123266.3123442
   Hsu WinstonH., 2007, ACM MM
   Hu YT, 2018, IEEE T MULTIMEDIA, V20, P927, DOI 10.1109/TMM.2017.2760101
   Jiang YG, 2016, IEEE T MULTIMEDIA, V18, P2161, DOI 10.1109/TMM.2016.2614233
   Langville AN, 2005, SIAM REV, V47, P135, DOI 10.1137/S0036144503424786
   Lee WY, 2018, IEEE T MULTIMEDIA, V20, P142, DOI 10.1109/TMM.2017.2726184
   Liu Dong., 2009, P 18 INT C WORLD WID, P351
   Lu D, 2016, IEEE T MULTIMEDIA, V18, P1628, DOI 10.1109/TMM.2016.2568099
   McCallum A.K., 2002, MALLET MACHINE LEARN
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Page L., 1999, PAGERANK CITATION RA, DOI DOI 10.1109/IISWC.2012.6402911
   Popescu Adrian, 2011, P 2 INT C COMP GEOSP, P1
   Rahman S, 2020, IEEE T MULTIMEDIA, V22, P242, DOI 10.1109/TMM.2019.2924511
   Sang JT, 2016, IEEE INT SYM MULTIM, P481, DOI 10.1109/ISM.2016.130
   Sang JT, 2015, IEEE T MULTIMEDIA, V17, P2259, DOI 10.1109/TMM.2015.2486524
   Shi Y, 2011, LECT NOTES COMPUT SC, V6787, P305, DOI 10.1007/978-3-642-22362-4_26
   Strehl A, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P93, DOI 10.1162/153244303321897735
   Van Thong JM, 2002, IEEE T MULTIMEDIA, V4, P88, DOI 10.1109/6046.985557
   Vaughan L, 2007, J COMPUT-MEDIAT COMM, V12, P888, DOI 10.1111/j.1083-6101.2007.00355.x
   Yan M, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/2671188.2749344
   Yan M, 2015, IEEE T MULTIMEDIA, V17, P1248, DOI 10.1109/TMM.2015.2446949
   Yang L., 2012, P 21 INT C WORLD WID, P261, DOI DOI 10.1145/2187836.2187872
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P2762, DOI 10.1109/TMM.2019.2912124
   Zhao WNX, 2011, LECT NOTES COMPUT SC, V6611, P338, DOI 10.1007/978-3-642-20161-5_34
   Zhengyu Deng, 2012, Advances in Multimedia Information Processing - PCM 2012. 13th Pacific-Rim Conference on Multimedia. Proceedings, P455, DOI 10.1007/978-3-642-34778-8_42
   Zhou DY, 2004, ADV NEUR IN, V16, P321
NR 40
TC 3
Z9 3
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 510
EP 523
DI 10.1109/TMM.2020.2982047
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600040
DA 2024-07-18
ER

PT J
AU Hu, WP
   Hu, HF
AF Hu, Weipeng
   Hu, Haifeng
TI Adversarial Disentanglement Spectrum Variations and Cross-Modality
   Attention Networks for NIR-VIS Face Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE NIR-VIS HFR task; deep learning; advanced scatter loss;
   modality-adversarial feature learning; cross-modality attention block
ID REGRESSION
AB Near-infrared and visual (NIR-VIS) matching task refers to the face recognition between the two images of different modalities, which remains a challenging task in the field of machine vision. The main problems of NIR-VIS Heterogeneous Face Recognition (HFR) tasks include two aspects: large intra-class differences caused by cross-modal data, and insufficient paired training samples. In this paper, an effective Adversarial Disentanglement spectrum variations and Cross-modality Attention Networks (ADCANs) is proposed for VIS-NIR matching task. Three key components are introduced to the ADCANs for reducing the gap of cross-modal images: Advanced Scatter Loss (ASL), Modality-adversarial Feature Learning (MaFL) and Cross-modality Attention Block (CmAB). The proposed ASL loss captures between- and within-class information of the data and embeds them to the network for more effective training, and it focuses on categories with small between-class distance and increases the distance between them. The MaFL consists of an Identity-Discriminative Feature Learning Network (IDFLN) and a Modality-Adversarial Disentanglement Network (MADN), which can enhance the identity-discriminative feature representations as well as disentangling spectrum variations via an adversarial learning. The IDFLN built by an end-to-end CNNs aims at learning identity-discriminative feature. While the MADN built by a discriminator D and a generator G focuses on removing modality-related information. Furthermore, to increase representation power as well as disentangling spectrum variations effectively, a CmAB block is introduced to the network, which sequentially applies spatial and channel attention modules to both the IDFLN and MADN. Since the channel attention module focuses on 'what' features to suppress or emphasize, an orthogonality constraint is introduced to the two channel attention modules, which allows MADN and IDFLN to focus on learning modality-related features and identity-related features, respectively. In particular, the ADCANs consists of multiple CmAB blocks to learn discriminative features and disentangle spectrum variations. A large number of experiments on three challenging HFR datasets indicate that the proposed ADCANs is effective for VIS-NIR HFR task.
C1 [Hu, Weipeng; Hu, Haifeng] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510275, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510275, Peoples R China.
EM huwp5@mail2.sysu.edu.cn; huhaif@mail.sysu.edu.cn
RI Weipeng, Hu/AAS-1819-2020
OI Weipeng, Hu/0000-0003-2886-7346; Hu, Haifeng/0000-0002-4884-323X
FU National Natural Science Foundation of China [61673402, 61273270,
   60802069]; Natural Science Foundation of Guangdong [2017A030311029];
   Science and Technology Program of Guangzhou [201704020180]; Fundamental
   Research Funds for the Central Universities of China
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61673402, 61273270, and 60802069, in
   part by the Natural Science Foundation of Guangdong under Grant
   2017A030311029, in part by the Science and Technology Program of
   Guangzhou under Grant 201704020180, and in part by the Fundamental
   Research Funds for the Central Universities of China.
CR [Anonymous], 2016, P IEEE C COMP VIS PA
   [Anonymous], 2008, PROC CVPR IEEE
   [Anonymous], 2016, P IEEE C COMP VIS PA
   [Anonymous], 2019, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2018.2842770
   [Anonymous], BIOM INT C
   [Anonymous], 2014, ARXIV14117923
   [Anonymous], PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2018.00745, DOI 10.1109/TPAMI.2019.2913372]
   [Anonymous], 2018, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2017.2737538
   [Anonymous], 2014, ARXIV PREPRINT ARXIV
   Arjovsky M, 2017, arXiv preprint arXiv:1701.07875
   Boyd S., 2009, Convex Optimization, P215
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chen BC, 2015, IEEE T MULTIMEDIA, V17, P804, DOI 10.1109/TMM.2015.2420374
   Chen J, 2009, PROC CVPR IEEE, P156, DOI 10.1109/CVPRW.2009.5206832
   Deng JK, 2019, Arxiv, DOI arXiv:1801.07698
   Deng ZY, 2019, AAAI CONF ARTIF INTE, P8239
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Fu C. Y., 2019, P C NEUR INF PROC SY, P2670
   Gong DH, 2017, IEEE T IMAGE PROCESS, V26, P2079, DOI 10.1109/TIP.2017.2651380
   Goodfellow IJ, 2014, ADV NEUR IN, P2672, DOI DOI 10.1145/3422622
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   He R, 2017, AAAI CONF ARTIF INTE, P2000
   Hu SW, 2017, IEEE INT CONF AUTOMA, P883, DOI 10.1109/FG.2017.126
   Hu W. P., 2019, IEEE T MULTIMEDIA
   Hu WP, 2019, IEEE ACCESS, V7, P75305, DOI 10.1109/ACCESS.2019.2920855
   Hu WP, 2019, COMPUT VIS IMAGE UND, V184, P9, DOI 10.1016/j.cviu.2019.04.003
   Huang D., 2012, IRIPTR12FR001
   Huang GS, 2007, 2007 7TH IEEE CONFERENCE ON NANOTECHNOLOGY, VOL 1-3, P7, DOI 10.1109/NANO.2007.4601129
   Huang XS, 2013, IEEE T IMAGE PROCESS, V22, P353, DOI 10.1109/TIP.2012.2215617
   Huo J, 2018, IEEE T CYBERNETICS, V48, P1814, DOI 10.1109/TCYB.2017.2715660
   Jaderberg M., 2015, ICLR, DOI DOI 10.48550/ARXIV.1506.02025
   Juefei-Xu Felix, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P141, DOI 10.1109/CVPRW.2015.7301308
   Kan MN, 2016, IEEE T PATTERN ANAL, V38, P188, DOI 10.1109/TPAMI.2015.2435740
   Klare BF, 2013, IEEE T PATTERN ANAL, V35, P1410, DOI 10.1109/TPAMI.2012.229
   Klare BF, 2011, IEEE T PATTERN ANAL, V33, P639, DOI 10.1109/TPAMI.2010.180
   Komodakis N, 2017, P ICLR
   Lei Z, 2009, PROC CVPR IEEE, P1123, DOI 10.1109/CVPRW.2009.5206860
   Lezama J, 2017, PROC CVPR IEEE, P6807, DOI 10.1109/CVPR.2017.720
   Li SZ, 2013, IEEE COMPUT SOC CONF, P348, DOI 10.1109/CVPRW.2013.59
   Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212
   Liu WY, 2016, PR MACH LEARN RES, V48
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Sarfraz MS, 2017, INT J COMPUT VISION, V122, P426, DOI 10.1007/s11263-016-0933-2
   Saxena S, 2016, LECT NOTES COMPUT SC, V9915, P483, DOI 10.1007/978-3-319-49409-8_40
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shao M, 2017, IEEE T NEUR NET LEAR, V28, P451, DOI 10.1109/TNNLS.2016.2517014
   Sharma A., 2013, P IEEE C COMP VIS PA, P593
   Song LX, 2018, AAAI CONF ARTIF INTE, P7355
   Song YB, 2014, LECT NOTES COMPUT SC, V8694, P800, DOI 10.1007/978-3-319-10599-4_51
   Sun Y, 2014, ADV NEUR IN, V27
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Wang F, 2018, IEEE SIGNAL PROC LET, V25, P926, DOI 10.1109/LSP.2018.2822810
   Wang KY, 2013, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2013.261
   Wang R, 2009, LECT NOTES COMPUT SC, V5558, P319, DOI 10.1007/978-3-642-01793-3_33
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu X, 2019, AAAI CONF ARTIF INTE, P9005
   Wu X, 2018, AAAI CONF ARTIF INTE, P1679
   Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032
   Wu XK, 2015, IEEE T INTELL TRANSP, V16, P2786, DOI [10.1109/TITS.2015.2422778, 10.1109/MWSYM.2015.7166900]
   Yi D., 2007, ASIA S PACIF DES AUT, P523
   Yi DW, 2015, INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ARTIFICIAL INTELLIGENCE (ICCSAI 2014), P1
   Zhang HC, 2012, INT C MULTIMED INFO, P568, DOI 10.1109/MINES.2012.65
   Zhang H, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P100
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang MJ, 2019, IEEE T IMAGE PROCESS, V28, P642, DOI 10.1109/TIP.2018.2869688
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
NR 69
TC 25
Z9 25
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 145
EP 160
DI 10.1109/TMM.2020.2980201
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600012
DA 2024-07-18
ER

PT J
AU Liu, L
   Jiang, J
   Jia, WJ
   Amirgholipour, S
   Wang, Y
   Zeibots, M
   He, XJ
AF Liu, Lei
   Jiang, Jie
   Jia, Wenjing
   Amirgholipour, Saeed
   Wang, Yi
   Zeibots, Michelle
   He, Xiangjian
TI DENet: A Universal Network for Counting Crowd With Varying Densities and
   Scales
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolution; Estimation; Feature extraction; Loss measurement; Image
   segmentation; Computer architecture; Measurement uncertainty; Crowd
   counting; density estimation; detection
AB Counting people or objects with significantly varying scales and densities has attracted much interest from the research community and yet it remains an open problem. In this paper, we propose a simple but efficient and effective network, named DENet, which is composed of two components, i.e., a detection network (DNet) and an encoder-decoder estimation network (ENet). We first run the DNet on the input image to detect and count individuals who can be segmented clearly. Then, the ENet is utilized to estimate the density maps of the remaining areas, typically with low resolution and high densities where individuals cannot be detected. For this purpose, we propose a modified Xception network as the encoder for feature extraction and a combination of dilated convolution and transposed convolution as the decoder. When evaluated on the ShanghaiTech Part A, UCF and WorldExpo'10 datasets, our DENet has achieved lower Mean Absolute Error (MAE) than those of the state-of-the-art methods.
C1 [Liu, Lei; Jiang, Jie] Beihang Univ, Sch Instrumentat Sci & Optoelect Engn, Beijing 100083, Peoples R China.
   [Jia, Wenjing; Amirgholipour, Saeed; He, Xiangjian] Univ Technol Sydney, Global Big Data Technol Ctr, Ultimo, NSW 2007, Australia.
   [Wang, Yi] Dalian Univ Technol, Sch Informat Sci & Engn, Dalian 116024, Peoples R China.
   [Wang, Yi] Dalian Univ Technol, Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian 116024, Peoples R China.
   [Zeibots, Michelle] Univ Technol Sydney, Inst Sustainable Futures, Ultimo, NSW 2007, Australia.
C3 Beihang University; University of Technology Sydney; Dalian University
   of Technology; Dalian University of Technology; University of Technology
   Sydney
RP Jiang, J (corresponding author), Beihang Univ, Sch Instrumentat Sci & Optoelect Engn, Beijing 100083, Peoples R China.; He, XJ (corresponding author), Univ Technol Sydney, Global Big Data Technol Ctr, Ultimo, NSW 2007, Australia.
EM by1417114@buaa.edu.cn; jiangjie@buaa.edu.cn; Wenjing.Jia@uts.edu.au;
   Saeed.AmirgholipourKasmani@student.uts.edu.au; dlutwangyi@dlut.edu.cn;
   Michelle.E.Zeibots@uts.edu.au; Xiangjian.He@uts.edu.au
RI Liu, Lei/P-5394-2014; He, Xiangjian/CAA-1461-2022; Jia,
   Weijia/W-6152-2019
OI Jia, Wenjing/0000-0002-0940-3338; Jiang, Jie/0000-0003-1946-1707; He,
   Xiangjian/0000-0001-8962-540X
FU National Natural Science Fund of China [61725501, 61976037]; Specialized
   Research Fund for the Doctoral Program of Higher Education of China
   [20121102110032]
FX This work was supported in part by the National Natural Science Fund of
   China under Grants 61725501 and 61976037 and in part by Specialized
   Research Fund for the Doctoral Program of Higher Education of China
   under Grant 20121102110032.
CR [Anonymous], 2016, INT C LEARNING REPRE
   [Anonymous], 2018, ECCV
   [Anonymous], P EUR C COMP VIS ECC
   Brostow G.J., 2006, CVPR, P594, DOI DOI 10.1109/CVPR.2006.320
   Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569
   Chan AB, 2012, IEEE T IMAGE PROCESS, V21, P2160, DOI 10.1109/TIP.2011.2172800
   Chan AB, 2009, IEEE I CONF COMP VIS, P545, DOI 10.1109/ICCV.2009.5459191
   Cheng ZQ, 2019, IEEE I CONF COMP VIS, P6151, DOI 10.1109/ICCV.2019.00625
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dollár P, 2008, LECT NOTES COMPUT SC, V5303, P211, DOI 10.1007/978-3-540-88688-4_16
   Fan X., IEEE T MOBILE COMPUT
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Huang SY, 2018, IEEE T IMAGE PROCESS, V27, P1049, DOI 10.1109/TIP.2017.2740160
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Lian DZ, 2019, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2019.00192
   Liu J, 2018, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2018.00545
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu YT, 2019, PROC CVPR IEEE, P6462, DOI 10.1109/CVPR.2019.00663
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Oñoro-Rubio D, 2016, LECT NOTES COMPUT SC, V9911, P615, DOI 10.1007/978-3-319-46478-7_38
   Paszke A., 2017, AUTOMATIC DIFFERENTI
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Regazzoni CS, 1996, SIGNAL PROCESS, V53, P47, DOI 10.1016/0165-1684(96)00075-8
   Sam DB, 2017, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2017.429
   Shang C, 2016, IEEE IMAGE PROC, P1215, DOI 10.1109/ICIP.2016.7532551
   Simonyan K., 2014, 14091556 ARXIV
   Sindagi VA, 2017, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2017.206
   Sindagi VA, 2018, PATTERN RECOGN LETT, V107, P3, DOI 10.1016/j.patrec.2017.07.007
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Zeng LK, 2017, IEEE IMAGE PROC, P465, DOI 10.1109/ICIP.2017.8296324
   Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684
   Zhang C, 2016, IEEE T MULTIMEDIA, V18, P1048, DOI 10.1109/TMM.2016.2542585
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
NR 38
TC 45
Z9 45
U1 2
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1060
EP 1068
DI 10.1109/TMM.2020.2992979
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300018
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Lu, YW
   Wang, WJ
   Yuan, C
   Li, XL
   Lai, ZH
AF Lu, Yuwu
   Wang, Wenjing
   Yuan, Chun
   Li, Xuelong
   Lai, Zhihui
TI Manifold Transfer Learning via Discriminant Regression Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Learning systems; Manifolds; Sparse matrices; Task analysis; Image
   classification; Convergence; Regression analysis; Manifold; transfer
   learning; regression; discriminant; image classification
ID REGULARIZATION
AB In transfer learning, how to effectively transfer useful information from the source domain to the target domain is crucial. In this paper, we propose a novel transfer learning method for image classification, named manifold transfer learning via discriminant regression analysis (MTL-DRA), to transfer the local geometry structure information from the source domain to the target domain and ensure that the transform matrix is robust or sparse so that samples from different domains can be well combined. In MTL-DRA, we encode discriminant information of the source domain to the target domain by introducing between- and within-class graphs to preserve within-class similarity and reduce between-class similarity. With different norms as constraints, MTL-DRA overcomes the disturbance of noise and avoids negative transfer learning. To improve the robustness of MTL-DRA, we encode a nuclear norm constraint and propose robust MTL-DRA (RMTL-DRA). We analyzed the convergence and complexity of the two proposed methods. To verify the performance of the proposed methods, we conducted extensive experiments on five public image benchmarks. The experimental results show that the proposed methods outperform state-of-the-art transfer learning methods.
C1 [Lu, Yuwu; Wang, Wenjing; Lai, Zhihui] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518055, Peoples R China.
   [Lu, Yuwu; Wang, Wenjing; Lai, Zhihui] Shenzhen Univ, Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
   [Lu, Yuwu; Wang, Wenjing; Lai, Zhihui] Shenzhen Univ, Guangdong Lab Artificial Intelligence & Digital E, Shenzhen 518060, Peoples R China.
   [Lu, Yuwu; Wang, Wenjing; Lai, Zhihui] Tsinghua Shenzhen Int Grad Sch & Peng Cheng Lab, Shenzhen 518060, Peoples R China.
   [Yuan, Chun] Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
   [Yuan, Chun] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Li, Xuelong] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.
   [Li, Xuelong] Northwestern Polytech Univ, Ctr OPTical IMagery Anal & Learning Optimal, Xian 710072, Peoples R China.
C3 Shenzhen University; Shenzhen University; Guangming Laboratory; Shenzhen
   University; Tsinghua Shenzhen International Graduate School; Peng Cheng
   Laboratory; Northwestern Polytechnical University; Northwestern
   Polytechnical University
RP Yuan, C (corresponding author), Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.; Yuan, C (corresponding author), Peng Cheng Lab, Shenzhen 518055, Peoples R China.
EM luyuwu2008@163.com; wangwenjing2018@email.szu.edu.cn;
   yuanc@sz.tsinghua.edu.cn; xuelong_li@nwpu.cdu.cn; lai_zhi_hui@.163.com
RI Lai, Zhihui/R-1000-2019; Li, Xue-long/AFU-6301-2022
OI Lai, Zhihui/0000-0002-4388-3080; Li, Xue-long/0000-0003-2037-2525
FU National Natural Science Foundation of China [U1833101, 61761130079,
   61732011]; Guangdong Basic and Applied Basic Research Foundation
   [2019A1515011493]; Tencent "Rhinoceros Birds"-Scientific Research
   Foundation for Young Teachers of Shenzhen University; Natural Science
   Foundation of Shenzhen University [2019046]; Science Foundation of
   Shenzhen [JCYJ20160422144110140, JCYJ20190809172201639]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants U1833101, 61761130079, 61732011, in
   part by the Guangdong Basic and Applied Basic Research Foundation under
   Grant 2019A1515011493, in part by the Tencent "Rhinoceros
   Birds"-Scientific Research Foundation for Young Teachers of Shenzhen
   University, in part by the Natural Science Foundation of Shenzhen
   University under Grant 2019046, and in part by the Science Foundation of
   Shenzhen underGrants JCYJ20160422144110140 and JCYJ20190809172201639.
CR [Anonymous], 1996, COLUMBIA OBJECT IMAG
   [Anonymous], INT C MACH LEARN
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Boyd S., 2004, CONVEX OPTIMIZATION
   Chen YM, 2019, IEEE T CYBERNETICS, V49, P1909, DOI 10.1109/TCYB.2018.2816981
   Chen YD, 2018, IEEE T MULTIMEDIA, V20, P3212, DOI 10.1109/TMM.2018.2834867
   Ding ZM, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3453
   Frank A., 2019, UCI MACHINE LEARNING
   Fu Q, 2018, IEEE T MULTIMEDIA, V20, P2114, DOI 10.1109/TMM.2018.2791803
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gholami B., 2017, P IEEE INT C COMP VI, P22
   Han YH, 2010, IEEE T CIRC SYST VID, V20, P1110, DOI 10.1109/TCSVT.2010.2057015
   Jiang WH, 2019, IEEE T KNOWL DATA EN, V31, P561, DOI 10.1109/TKDE.2018.2837085
   Jing P., 2016, IEEE T MULTIMEDIA, V18, P775
   Jing PG, 2020, IEEE T MULTIMEDIA, V22, P1555, DOI 10.1109/TMM.2019.2944749
   Jing PG, 2018, IEEE T KNOWL DATA EN, V30, P1519, DOI 10.1109/TKDE.2017.2785784
   Jing PG, 2017, IEEE T MULTIMEDIA, V19, P1050, DOI 10.1109/TMM.2016.2644866
   Kim E, 2015, PROC CVPR IEEE, P915, DOI 10.1109/CVPR.2015.7298693
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li P, 2017, IEEE T CYBERNETICS, V47, P4250, DOI 10.1109/TCYB.2016.2623638
   Li Y, 2019, IEEE T PATTERN ANAL, V41, P639, DOI 10.1109/TPAMI.2018.2810288
   Lin, 2010, ARXIV10095055
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Lu YW, 2019, IEEE T CIRC SYST VID, V29, P941, DOI 10.1109/TCSVT.2018.2822761
   Lu YW, 2019, IEEE T CYBERNETICS, V49, P1859, DOI 10.1109/TCYB.2018.2815559
   Lu YW, 2017, IEEE T MULTIMEDIA, V19, P2391, DOI 10.1109/TMM.2017.2703130
   Luo Y, 2019, IEEE T PATTERN ANAL, V41, P1013, DOI 10.1109/TPAMI.2018.2824309
   Mazumder R, 2010, J MACH LEARN RES, V11, P2287
   Nie FP, 2019, IEEE T IMAGE PROCESS, V28, P2378, DOI 10.1109/TIP.2018.2886712
   Noroozi M, 2018, PROC CVPR IEEE, P9359, DOI 10.1109/CVPR.2018.00975
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Peng ZH, 2020, IEEE T CIRC SYST VID, V30, P1022, DOI 10.1109/TCSVT.2019.2900467
   Shao L, 2015, IEEE T NEUR NET LEAR, V26, P1019, DOI 10.1109/TNNLS.2014.2330900
   Shao M, 2014, INT J COMPUT VISION, V109, P74, DOI 10.1007/s11263-014-0696-6
   Si S, 2010, IEEE T KNOWL DATA EN, V22, P929, DOI 10.1109/TKDE.2009.126
   Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154
   Su YT, 2020, IEEE SIGNAL PROC LET, V27, P740, DOI 10.1109/LSP.2020.2983831
   Tao JW, 2019, IEEE ACCESS, V7, P145406, DOI 10.1109/ACCESS.2019.2944211
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang DQ, 2020, IEEE T CYBERNETICS, V50, P4709, DOI 10.1109/TCYB.2019.2891577
   Wang SS, 2020, IEEE T IMAGE PROCESS, V29, P2424, DOI 10.1109/TIP.2019.2948480
   Xiang SM, 2012, IEEE T NEUR NET LEAR, V23, P1738, DOI 10.1109/TNNLS.2012.2212721
   Xu Y, 2016, IEEE T IMAGE PROCESS, V25, P850, DOI 10.1109/TIP.2015.2510498
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Yang J, 2017, IEEE T PATTERN ANAL, V39, P156, DOI 10.1109/TPAMI.2016.2535218
   Zhang FL, 2015, IEEE T NEUR NET LEAR, V26, P2247, DOI 10.1109/TNNLS.2014.2376530
   Zhang L., 2016, P INT JOINT C NEUR N, P24
   Zhang L, 2020, IEEE T NEUR NET LEAR, V31, P3374, DOI 10.1109/TNNLS.2019.2944455
   Zhang L, 2016, IEEE T IMAGE PROCESS, V25, P4959, DOI 10.1109/TIP.2016.2598679
   Zhang L, 2019, IEEE T NEUR NET LEAR, V30, P3759, DOI 10.1109/TNNLS.2019.2899037
   Zhang L, 2016, IEEE T IMAGE PROCESS, V25, P1177, DOI 10.1109/TIP.2016.2516952
   Zhang Z, 2017, IEEE T IMAGE PROCESS, V26, P1607, DOI 10.1109/TIP.2017.2654163
NR 53
TC 23
Z9 23
U1 3
U2 45
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2056
EP 2070
DI 10.1109/TMM.2020.3007340
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100017
DA 2024-07-18
ER

PT J
AU Malladi, SRSP
   Ram, S
   Rodríguez, JJ
AF Malladi, Sree Ramya S. P.
   Ram, Sundaresh
   Rodriguez, Jeffrey J.
TI Image Denoising Using Superpixel-Based PCA
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Principal component analysis; Noise reduction; Noise measurement;
   Transforms; Image denoising; Decorrelation; Noise level; Image
   restoration; denoising; superpixels; principal component analysis;
   patch-based method
ID PRINCIPAL COMPONENT ANALYSIS; NONLOCAL MEANS; INFORMATION; REGRESSION;
   RANK; CNN
AB Denoising is a fundamental task in image processing, aimed at estimating an unknown image from its noisy observation. In this paper, we develop a computationally simple paradigm for image denoising using superpixel grouping and principal component analysis (PCA) of similar patches within the superpixels. Our method comprises three steps. First, we perform a superpixel segmentation on the noisy images. Next, similar patches within the superpixels are grouped in order to preserve the local image structures. Finally, each group of similar patches is factorized by PCA transform and estimated by performing coefficient shrinkage in the PCA domain to remove the noise. The proposed method exploits the optimal energy compaction property of PCA on groups of similar patches in the least squares sense. The performance of our approach is experimentally verified on a variety of synthetic images at various noise levels, and on real world noisy images. Our proposed method achieves very competitive denoising performance, especially in preserving the fine image structures, compared with many existing denoising algorithms with respect to both objective measurement and visual evaluation. We also show that our proposed method is computationally more efficient than other local PCA based methods.
C1 [Malladi, Sree Ramya S. P.] Allvis IO, Pittsburgh, PA 15206 USA.
   [Ram, Sundaresh] Univ Michigan, Ctr Mol Imaging, Dept Radiol, Ann Arbor, MI 48105 USA.
   [Ram, Sundaresh] Univ Michigan, Dept Biomed Engn, Ann Arbor, MI 48105 USA.
   [Rodriguez, Jeffrey J.] Univ Arizona, Dept Elect & Comp Engn, Tucson, AZ 85721 USA.
C3 University of Michigan System; University of Michigan; University of
   Michigan System; University of Michigan; University of Arizona
RP Ram, S (corresponding author), Univ Michigan, Ctr Mol Imaging, Dept Radiol, Ann Arbor, MI 48105 USA.; Ram, S (corresponding author), Univ Michigan, Dept Biomed Engn, Ann Arbor, MI 48105 USA.
EM rmalladi@email.arizona.edu; sundarer@umich.edu;
   jjrodrig@email.arizona.edu
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2009, SIGNAL PROCESSING AD
   Awate SP, 2006, IEEE T PATTERN ANAL, V28, P364, DOI 10.1109/TPAMI.2006.64
   Barrett H. H., 2004, FDN IMAGE SCI
   Benesova W., 2014, C MACHINE VISION MAC, P1
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Buyssens P, 2014, IEEE IMAGE PROC, P4368, DOI 10.1109/ICIP.2014.7025886
   Chatterjee P, 2012, IEEE T IMAGE PROCESS, V21, P1635, DOI 10.1109/TIP.2011.2172799
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   Cho SI, 2019, IEEE T MULTIMEDIA, V21, P484, DOI 10.1109/TMM.2018.2859791
   Cho SI, 2018, IEEE T MULTIMEDIA, V20, P1738, DOI 10.1109/TMM.2017.2781371
   Deledalle CA, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.25
   Ding D, 2019, IEEE T IMAGE PROCESS, V28, P1705, DOI 10.1109/TIP.2018.2880681
   Ding D, 2018, PATTERN RECOGN, V83, P174, DOI 10.1016/j.patcog.2018.05.025
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Gu SH, 2017, INT J COMPUT VISION, V121, P183, DOI 10.1007/s11263-016-0930-5
   Guo Q, 2016, IEEE T CIRC SYST VID, V26, P868, DOI 10.1109/TCSVT.2015.2416631
   Guo ZC, 2012, IEEE T IMAGE PROCESS, V21, P958, DOI 10.1109/TIP.2011.2169272
   Hu Y, 2012, IEEE T IMAGE PROCESS, V21, P2559, DOI 10.1109/TIP.2012.2183143
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Knaus C, 2014, IEEE T IMAGE PROCESS, V23, P3114, DOI 10.1109/TIP.2014.2326771
   Li XY, 2016, SIGNAL PROCESS, V124, P173, DOI 10.1016/j.sigpro.2015.09.021
   Liu MY, 2014, IEEE T PATTERN ANAL, V36, P99, DOI 10.1109/TPAMI.2013.107
   Liu YP, 2019, IEEE T MULTIMEDIA, V21, P338, DOI 10.1109/TMM.2018.2859026
   Liu YP, 2018, IEEE J-STSP, V12, P1378, DOI 10.1109/JSTSP.2018.2873142
   Lu CY, 2020, IEEE T PATTERN ANAL, V42, P925, DOI 10.1109/TPAMI.2019.2891760
   Malladi S. R. S. P., COMPUT VIS IMAGE UND
   Malladi SRSP, 2014, IEEE SW SYMP IMAG, P145, DOI 10.1109/SSIAI.2014.6806050
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Neubert P, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.54
   Papyan V, 2016, IEEE T IMAGE PROCESS, V25, P249, DOI 10.1109/TIP.2015.2499698
   Peyré G, 2011, IEEE J-STSP, V5, P896, DOI 10.1109/JSTSP.2011.2120592
   Pyatykh S, 2013, IEEE T IMAGE PROCESS, V22, P687, DOI 10.1109/TIP.2012.2221728
   Ram S., 2012, Proceedings of the 2012 IEEE Southwest Symposium on Image Analysis & Interpretation (SSIAI 2012), P37, DOI 10.1109/SSIAI.2012.6202447
   Ram S., 2017, THESIS U ARIZONA TUC
   Ram S, 2018, IEEE T BIO-MED ENG, V65, P1617, DOI 10.1109/TBME.2017.2674521
   Ram S, 2016, IEEE T MED IMAGING, V35, P1753, DOI 10.1109/TMI.2016.2527740
   Ram S, 2016, IEEE SW SYMP IMAG, P69, DOI 10.1109/SSIAI.2016.7459177
   Ram S, 2014, IEEE SW SYMP IMAG, P121, DOI 10.1109/SSIAI.2014.6806044
   Ram S, 2013, INT CONF ACOUST SPEE, P1128, DOI 10.1109/ICASSP.2013.6637826
   Remez T, 2018, IEEE T IMAGE PROCESS, V27, P5707, DOI 10.1109/TIP.2018.2859044
   Ren C, 2019, IEEE T MULTIMEDIA, V21, P731, DOI 10.1109/TMM.2018.2866362
   Sampat MP, 2009, IEEE T IMAGE PROCESS, V18, P2385, DOI 10.1109/TIP.2009.2025923
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Stutz D, 2018, COMPUT VIS IMAGE UND, V166, P1, DOI 10.1016/j.cviu.2017.03.007
   Takeda H, 2007, IEEE T IMAGE PROCESS, V16, P349, DOI 10.1109/TIP.2006.888330
   Tasdizen T, 2009, IEEE T IMAGE PROCESS, V18, P2649, DOI 10.1109/TIP.2009.2028259
   Udell M., 2016, Generalized Low Rank Models
   Van den Bergh M, 2012, LECT NOTES COMPUT SC, V7578, P13, DOI 10.1007/978-3-642-33786-4_2
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie J, 2015, IEEE T MULTIMEDIA, V17, P1525, DOI 10.1109/TMM.2015.2457678
   Xu J., 2017, P IEEE INT C COMP VI, P1096
   Xu JL, 2018, CAMB CHINA LIBR, P1
   Yang D, 2018, IEEE SIGNAL PROC LET, V25, P55, DOI 10.1109/LSP.2017.2768660
   Yao J, 2015, PROC CVPR IEEE, P2947, DOI 10.1109/CVPR.2015.7298913
   Yaroslavsky L.P, 2012, DIGITAL PICTURE PROC, V9
   Yin JL, 2018, IEEE T MULTIMEDIA, V20, P3045, DOI 10.1109/TMM.2018.2820910
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang L, 2010, PATTERN RECOGN, V43, P1531, DOI 10.1016/j.patcog.2009.09.023
   Zhang L, 2009, IEEE T IMAGE PROCESS, V18, P797, DOI 10.1109/TIP.2008.2011384
NR 62
TC 12
Z9 12
U1 2
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2297
EP 2309
DI 10.1109/TMM.2020.3009502
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800011
DA 2024-07-18
ER

PT J
AU Mo, HY
   Liu, LB
   Zhu, WP
   Li, Q
   Yin, SY
   Wei, SJ
AF Mo, Huiyu
   Liu, Leibo
   Zhu, Wenping
   Li, Qiang
   Yin, Shouyi
   Wei, Shaojun
TI A 460 GOPS/W Improved Mnemonic Descent Method-Based Hardwired
   Accelerator for Face Alignment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face; Hardware; Feature extraction; Real-time systems; Shape;
   Optimization; Pipelines; Accelerator; convolutional network; face
   alignment; recurrent network
ID FACIAL LANDMARK DETECTION; NETWORK
AB The mnemonic descent method (MDM) algorithm is the first end-to-end recurrent convolutional system for high-accuracy face alignment. However, the heavy computational complexity and high memory access demands make it difficult to satisfy the requirements of real-time applications. To address this problem, an improved MDM (I-MDM) algorithm is proposed for efficient hardware implementation based on several hardware-oriented optimizations. First, a patch merging mechanism is introduced to dynamically cluster and eliminate redundant landmarks, which significantly reduces computational complexity with minimal accuracy loss. Second, a dedicated convolutional layer is inserted to halve the number of computations and memory access of the subsequent fully connected layer, yielding a 4.42% decrease in the failure rate. Third, a lightweight preprocessing method named dual regressors is proposed to reinitialize face images, which can greatly improve the overall accuracy. Moreover, compared with a similar method, the DR method can reduce computations and memory storage by nearly 99.9%. Overall and compared with the MDM algorithm, I-MDM not only reduces the number of computations by 23.5% but also decreases the failure rate by 17.9% on the 300 W test set. Based on the proposed I-MDM algorithm, an I-MDM-based hardwired accelerator is presented using the TSMC 65 nm CMOS process. First, compared with similar solutions, the gradient calculation operation is rearranged and loaded pixels are reused in the HoG feature extraction to eliminate all division operations and 25% off-chip memory access. Second, patch-independent central activations are used to enable patch-level pipelined operations, yielding a 2x acceleration in the overall process. This accelerator achieves 460 GOPS/W energy efficiency at 330 MHz, which is 38x higher than the most recent face alignment accelerator with the same process.
C1 [Mo, Huiyu; Liu, Leibo; Zhu, Wenping; Yin, Shouyi; Wei, Shaojun] Tsinghua Univ, Inst Microelect, Beijing 100084, Peoples R China.
   [Zhu, Wenping] Chinese Acad Sci, Inst Semicond, Beijing 100083, Peoples R China.
   [Li, Qiang] Intel Corp, Beijing 100080, Peoples R China.
C3 Tsinghua University; Chinese Academy of Sciences; Institute of
   Semiconductors, CAS; Intel Corporation
RP Liu, LB (corresponding author), Tsinghua Univ, Inst Microelect, Beijing 100084, Peoples R China.
EM mohy15@mails.tsinghua.edu.cn; liulb@tsinghua.edu.cn; zhuwp@semi.ac.cn;
   eric.q.li@intel.com; yinsy@tsinghua.edu.cn; wsj@tsinghua.edu.cn
RI Mo, Huiyu/AAC-6418-2019
OI Mo, Huiyu/0000-0002-3373-7178
FU National High Technology Research and Development Program of China
   [2012AA012701]; National Natural Science Foundation of China [61672317]
FX This work was supported by the National High Technology Research and
   Development Program of China (Grant No. 2012AA012701) and the National
   Natural Science Foundation of China (Grant No. 61672317).
CR [Anonymous], 2019, IEEE WCNC, DOI [DOI 10.1080/09593985.2019.1566940, DOI 10.1109/wcnc.2019.8885834, DOI 10.3938/jkps.77.1118]
   Asthana A, 2014, PROC CVPR IEEE, P1859, DOI 10.1109/CVPR.2014.240
   Baltrusaitis T, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P354, DOI 10.1109/ICCVW.2013.54
   Bong K, 2017, ISSCC DIG TECH PAP I, P248, DOI 10.1109/ISSCC.2017.7870354
   Chiu CH, 2017, IEEE INT CONF CLOUD, P1, DOI 10.1109/CLOUD.2017.10
   Choi S, 2018, ISSCC DIG TECH PAP I, P220, DOI 10.1109/ISSCC.2018.8310263
   Dapogny A, 2019, IEEE I CONF COMP VIS, P6892, DOI 10.1109/ICCV.2019.00699
   Dong XY, 2018, PROC CVPR IEEE, P379, DOI 10.1109/CVPR.2018.00047
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Hemmati M, 2014, 2014 17TH EUROMICRO CONFERENCE ON DIGITAL SYSTEM DESIGN (DSD), P543, DOI 10.1109/DSD.2014.60
   HU XB, 1991, IEEE T COMPUT, V40, P13, DOI 10.1109/12.67316
   Jeong M, 2018, IEEE T CIRC SYST VID, V28, P2753, DOI 10.1109/TCSVT.2017.2769096
   Kang S., 2018, P137
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Kim C, 2017, ESSCIRC 2017 - 43RD IEEE EUROPEAN SOLID STATE CIRCUITS CONFERENCE, P255, DOI 10.1109/ESSCIRC.2017.8094574
   Kim S, 2020, IEEE T CIRCUITS-I, V67, P1181, DOI 10.1109/TCSI.2020.2966243
   Kowalski M, 2017, IEEE COMPUT SOC CONF, P2034, DOI 10.1109/CVPRW.2017.254
   Liu QS, 2017, IEEE T IMAGE PROCESS, V26, P797, DOI 10.1109/TIP.2016.2633939
   Liu QS, 2016, IEEE T IMAGE PROCESS, V25, P700, DOI 10.1109/TIP.2015.2502485
   Mo HY, 2020, IEEE T CIRC SYST VID, V30, P4284, DOI 10.1109/TCSVT.2019.2955463
   Mo HY, 2019, IEEE T MULTIMEDIA, V21, P943, DOI 10.1109/TMM.2018.2867262
   O'Grady NP, 2002, CLIN INFECT DIS, V35, P1281, DOI 10.1086/502007
   Ranjan R, 2019, IEEE T PATTERN ANAL, V41, P121, DOI 10.1109/TPAMI.2017.2781233
   Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218
   Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002
   Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59
   Shi BG, 2018, IEEE T NEUR NET LEAR, V29, P183, DOI 10.1109/TNNLS.2016.2618340
   Shin D, 2017, ISSCC DIG TECH PAP I, P240, DOI 10.1109/ISSCC.2017.7870350
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Trigeorgis G, 2016, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2016.453
   Tzimiropoulos G, 2015, PROC CVPR IEEE, P3659, DOI 10.1109/CVPR.2015.7298989
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wu WY, 2018, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2018.00227
   Wu Y, 2019, INT J COMPUT VISION, V127, P115, DOI 10.1007/s11263-018-1097-z
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yan JJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P392, DOI 10.1109/ICCVW.2013.126
   Yin SY, 2019, IEEE J SOLID-ST CIRC, V54, P1120, DOI 10.1109/JSSC.2018.2881913
   Yin SY, 2018, IEEE J SOLID-ST CIRC, V53, P968, DOI 10.1109/JSSC.2017.2778281
   Yuan Z, 2018, SYMP VLSI CIRCUITS, P33, DOI 10.1109/VLSIC.2018.8502404
   Zhang ZP, 2016, IEEE T PATTERN ANAL, V38, P918, DOI 10.1109/TPAMI.2015.2469286
   Zhu ML, 2019, PROC CVPR IEEE, P3481, DOI 10.1109/CVPR.2019.00360
   Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
NR 43
TC 1
Z9 1
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1122
EP 1135
DI 10.1109/TMM.2020.2993943
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300023
DA 2024-07-18
ER

PT J
AU Schulz, P
   Klessig, H
   Simsek, M
   Fettweis, G
AF Schulz, Philipp
   Klessig, Henrik
   Simsek, Meryem
   Fettweis, Gerhard
TI Modeling QoE for Buffered Video Streaming in Interference-Limited
   Cellular Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Streaming media; Quality of experience; Wireless communication;
   Analytical models; Data models; Measurement; Interference; Interference;
   mobile communication; modeling; multimedia communication; queuing
   analysis
ID FRAMEWORK; QUALITY
AB Mobile networks have to cope with an ever increasing demand for video streaming, an application that imposes high quality requirements for user satisfaction. In this paper, we present an analytical model to calculate two important quality measures for streaming traffic, namely the video startup delay distribution and the buffer starvation probability. The queuing-theoretic model differs from related work by incorporating data flow dynamics of the considered cell, as well as the dynamics of fluctuating interference from neighboring cells, with the goal of accurately representing a multi-cellular environment. In this regard, we propose a finite-volume method to approximate the solution of the involved system of partial differential equations, where other approaches from comparable work were faced with numerical problems. We also evaluate two simplified versions of the model, where only the interference dynamics or all coupling terms are omitted, respectively. The model allows us to study the impact of different parameters on buffered video streaming performance. Additionally, we propose a user-centric metric to measure quality of experience (QoE). To the best of our knowledge, our approach is novel and has not been covered by comparable work. The presented results can help to design future cellular networks with enhanced video streaming experience.
C1 [Schulz, Philipp; Simsek, Meryem; Fettweis, Gerhard] Tech Univ Dresden, Vodafone Chair Mobile Commun Syst, D-01069 Dresden, Germany.
   [Klessig, Henrik; Simsek, Meryem] Int Comp Sci Inst, Berkeley, CA 94704 USA.
C3 Vodafone Group; Technische Universitat Dresden
RP Schulz, P (corresponding author), Tech Univ Dresden, Vodafone Chair Mobile Commun Syst, D-01069 Dresden, Germany.
EM philipp.schulz@ifn.et.tu-dresden.de; klessig@icsi.berkeley.edu;
   simsek@icsi.berkeley.edu; fettweis@ifn.et.tu-dresden.de
RI Schulz, Philipp/AAQ-2316-2021
OI Schulz, Philipp/0000-0002-0738-556X
FU Federal Ministry of Education and Research within the programme
   "Twenty20 Partnership for Innovation" [03ZZ0505B]
FX This work was partly supported by the Federal Ministry of Education and
   Research within the programme "Twenty20 Partnership for Innovation"
   under contract 03ZZ0505B - "fast wireless".
CR 3GPP Technical Specification Group Radio Access Network, 2010, 36814 3GPP TR
   Alreshoodi M., 2013, International Journal of Distributed and Parallel Systems, V4, P53
   [Anonymous], 2004, JOINT INT ACM SIGMET
   [Anonymous], 2015, NGMN 5G White Paper
   [Anonymous], 2019, The Paper
   Bhushan N, 2014, IEEE COMMUN MAG, V52, P82, DOI 10.1109/MCOM.2014.6736747
   Bonald T, 2005, EUR T TELECOMMUN, V16, P65, DOI 10.1002/ett.1032
   Bonald T., 2004, Performance Evaluation Review, V32, P378, DOI 10.1145/1012888.1005730
   Bonald T, 2015, IEEE GLOB COMM CONF, DOI 10.1109/GLOCOM.2015.7417641
   Borst S, 2007, IEEE INFOCOM SER, P1884, DOI 10.1109/INFCOM.2007.219
   Delcoigne F, 2004, PERFORM EVALUATION, V55, P185, DOI 10.1016/S0166-5316(03)00115-9
   Ericsson ERICSSON., 2018, MOBILITY REPORT
   Fehske A. J., 2012, IEEE International Conference on Communications (ICC 2012), P5102, DOI 10.1109/ICC.2012.6363999
   Feldmann A., 1999, P C APPL TECHN ARCH, V29, P301
   Fettweis G., 2015, ITG WORKSH SOUND VIS
   Fettweis G, 2017, P IEEE WCNC WORKSH
   Gerschgorin S, 1931, IZV AKAD NAUK USSR O, V7, P749
   He ZF, 2016, IEEE T MULTIMEDIA, V18, P1401, DOI 10.1109/TMM.2016.2564104
   Huang YS, 2013, IEEE T MULTIMEDIA, V15, P2137, DOI 10.1109/TMM.2013.2270457
   Klessig H, 2016, IEEE T WIREL COMMUN, V15, P938, DOI 10.1109/TWC.2015.2480760
   Klessig Henrik., 2015, Vehicular Technology Conference (VTC Fall), 2015 IEEE, V82nd, P1
   Krishnan R. K., 2012, P INT MEAS C, P211
   Leveque RandallJ., 2007, Finite Volume Methods for Hyperbolic Problems
   Luan TH, 2010, IEEE T MULTIMEDIA, V12, P64, DOI 10.1109/TMM.2009.2036294
   Masuda A., 2018, PROC EUR C OPT COMMU, P1, DOI DOI 10.1109/ECOC.2018.8535509
   Mogensen P, 2007, IEEE VTS VEH TECHNOL, P1234, DOI 10.1109/VETECS.2007.260
   Pierucci L, 2015, IEEE WIREL COMMUN, V22, P10, DOI 10.1109/MWC.2015.7224722
   Rahama YA, 2019, TELECOMMUN SYST, V71, P577, DOI 10.1007/s11235-018-0533-2
   Rappaport TS, 2013, IEEE ACCESS, V1, P335, DOI 10.1109/ACCESS.2013.2260813
   Roberts JW, 2001, IEEE COMMUN MAG, V39, P94, DOI 10.1109/35.894382
   Siomina I, 2012, IEEE T WIREL COMMUN, V11, P2287, DOI 10.1109/TWC.2012.051512.111532
   Sun LF, 2018, IEEE T MULTIMEDIA, V20, P3414, DOI 10.1109/TMM.2018.2834861
   Xu YD, 2016, IEEE T MOBILE COMPUT, V15, P2762, DOI 10.1109/TMC.2015.2510629
   Xu YD, 2014, IEEE T MULTIMEDIA, V16, P813, DOI 10.1109/TMM.2014.2300041
   Xu YD, 2013, IEEE INFOCOM SER, P2715
   Xu YD, 2012, IEEE INFOCOM SER, P1826, DOI 10.1109/INFCOM.2012.6195557
   Ye Z, 2016, JT IFIP WIREL MOB, P145, DOI 10.1109/WMNC.2016.7543982
NR 37
TC 3
Z9 3
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 911
EP 925
DI 10.1109/TMM.2020.2990078
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300007
DA 2024-07-18
ER

PT J
AU Yang, C
   Zhang, XF
   An, P
   Shen, LQ
   Kuo, CCJ
AF Yang, Chao
   Zhang, Xinfeng
   An, Ping
   Shen, Liquan
   Kuo, C. -C. Jay
TI Blind Image Quality Assessment Based on Multi-scale KLT
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Image color analysis; Kernel; Image quality;
   Distortion; Transforms; Measurement; Blind image quality assessment
   (BIQA); data-driven; human visual system (HVS); Karhunen-Loeve transform
   (KLT)
ID FREE-ENERGY PRINCIPLE
AB Blind image quality assessment (BIQA) plays an important role in image services as independent of the reference image. Herein, the perceptual relevant feature design is the core of BIQA methods, but their performance is still not satisfied at present. In this work, we propose an unsupervised feature extraction approach for BIQA based on Karhunen-Loeve transform (KLT). Specifically, a normalization operation is firstly applied to the test image by calculating its mean subtracted contrast normalized (MSCN) coefficient. Then, KLT is employed as a data-driven feature extraction approach to extract image structural features, wherein kernels with different sizes are utilized to perform multi-scale analysis. Finally, generalized Gaussian distribution (GGD) is employed to model the KLT coefficients distribution in different spectral components as quality relevant features. Extensive experiments conducted on four widely utilized IQA databases have demonstrated that the proposed Multi-scale KLT (MsKLT) BIQA metric compares favorably with existing BIQA methods in terms of high accordance with human subjective scores on both common and uncommon distortion types.
C1 [Yang, Chao; An, Ping; Shen, Liquan] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.
   [Zhang, Xinfeng] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 100049, Peoples R China.
   [Kuo, C. -C. Jay] Univ Southern Calif, Ming Hsieh Dept Elect Engn, Los Angeles, CA 90089 USA.
C3 Shanghai University; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; University of Southern California
RP Yang, C (corresponding author), Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.
EM yangchaoie@shu.edu.cn; xfzhang@ucas.ac.cn; anping@shu.edu.cn;
   jsslq@shu.edu.cn; cckuo@sipi.usc.edu
RI Zhang, Xinfeng/X-8148-2019; Shen, Liquan/D-4832-2012; Kuo, C.-C.
   Jay/A-7110-2011
OI Shen, Liquan/0000-0002-2148-6279; Kuo, C.-C. Jay/0000-0001-9474-5035;
   Yang, Chao/0000-0001-9276-5673; Zhang, Xinfeng/0000-0002-7517-3868
FU NSFC [61901252, 61828105]; Shanghai Science and Technology Commission
   [17DZ2292400, 18XD1423900]
FX This work was supported in part by the NSFC underGrants 61901252,
   61828105, and Shanghai Science and Technology Commission under Grants
   17DZ2292400 and 18XD1423900.
CR [Anonymous], 2010, Categorical image quality (CSIQ) database
   [Anonymous], 2002, INT CONF ACOUST SPEE
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Bylinskii Z., 2018, MIT SALIENCY BENCHMA
   Chen YR, 2018, PICT COD SYMP, P174, DOI 10.1109/PCS.2018.8456277
   Freitas PG, 2018, IEEE T MULTIMEDIA, V20, P3353, DOI 10.1109/TMM.2018.2839529
   GEUSEBROEK J, 2000, LECT NOTES COMPUT SC, P331
   Geusebroek JM, 2001, IEEE T PATTERN ANAL, V23, P1338, DOI 10.1109/34.977559
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Hadizadeh H, 2018, IEEE T MULTIMEDIA, V20, P392, DOI 10.1109/TMM.2017.2740023
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Jiang QP, 2018, IEEE T MULTIMEDIA, V20, P2035, DOI 10.1109/TMM.2017.2763321
   Kim J, 2018, IEEE IMAGE PROC, P291, DOI 10.1109/ICIP.2018.8451346
   Kim J, 2017, IEEE J-STSP, V11, P206, DOI 10.1109/JSTSP.2016.2639328
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuo CCJ, 2018, J VIS COMMUN IMAGE R, V50, P237, DOI 10.1016/j.jvcir.2017.11.023
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Li QH, 2016, IEEE SIGNAL PROC LET, V23, P541, DOI 10.1109/LSP.2016.2537321
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Liu YT, 2020, IEEE T CIRC SYST VID, V30, P929, DOI 10.1109/TCSVT.2019.2900472
   Liu YT, 2019, IEEE T MULTIMEDIA, V21, P135, DOI 10.1109/TMM.2018.2849602
   Liu YT, 2018, IEEE T MULTIMEDIA, V20, P379, DOI 10.1109/TMM.2017.2729020
   Ma K, 2016, ABS161201227 CORR
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Ponomarenko Nikolay, 2013, 2013 4th European Workshop on Visual Information Processing (EUVIP), P106
   Scholkopf B., 2002, Learning with Kernels
   SHARIFI K, 1995, IEEE T CIRC SYST VID, V5, P52, DOI 10.1109/76.350779
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2005, LIVE IMAGE QUALITY A, DOI DOI 10.1109/CVPR.2015.7298594
   Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193
   Tourancheau S, 2008, IEEE IMAGE PROC, P365, DOI 10.1109/ICIP.2008.4711767
   WAN Z, IN PRESS, DOI DOI 10.1109/TMM.2019.2950533
   Wang SQ, 2018, IEEE T CIRC SYST VID, V28, P1, DOI 10.1109/TCSVT.2016.2602764
   Wang XC, 2019, IEEE IMAGE PROC, P435, DOI [10.1109/icip.2019.8802943, 10.1109/ICIP.2019.8802943]
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu QB, 2016, IEEE T CIRC SYST VID, V26, P425, DOI 10.1109/TCSVT.2015.2412773
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Zhai GT, 2012, IEEE T IMAGE PROCESS, V21, P41, DOI 10.1109/TIP.2011.2161092
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang XF, 2019, IEEE IMAGE PROC, P1730, DOI [10.1109/ICIP.2019.8803184, 10.1109/icip.2019.8803184]
NR 48
TC 18
Z9 18
U1 2
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1557
EP 1566
DI 10.1109/TMM.2020.3001537
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SN5MI
UT WOS:000658333200002
DA 2024-07-18
ER

PT J
AU Yang, JY
   Liu, W
   Yuan, JS
   Mei, T
AF Yang, Jianyu
   Liu, Wu
   Yuan, Junsong
   Mei, Tao
TI Hierarchical Soft Quantization for Skeleton-Based Human Action
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Skeleton; Feature extraction; Quantization (signal); Three-dimensional
   displays; Color; Two dimensional displays; Data models; Action
   recognition; skeleton; soft quantization; congenerous feature
ID ATTENTION; FEATURES; MODEL
AB In daily life, human beings rely on hands and body parts to complete particular actions cooperatively. These selected body parts and their cooperative relationships are essential cues to distinguish these actions. However, most existing action recognition methods, which try to model the body appearance or spatial relations in skeleton sequences, often ignore the essential cooperation relationship among joints. Differently, in this paper, we propose a spatio-temporal hierarchical soft quantization method to extract the congenerous motion features, which reflect the cooperation relations among joints and body parts. Specifically, we design a hierarchical network with multiple soft quantization layers to extract congenerous features. The hierarchical network not only models the spatial hierarchy of skeleton structure for joint, part, and body, but also extracts the temporal hierarchy with sliding windows for frame, fragment, and sequence. Moreover, the features in each layer are visually explainable, which reflect the cooperation among body parts. The trainable parameters in the network are also significantly reduced, which reduces computational cost. Extensive experiments conducted on four benchmarks demonstrate that our method can provide competitive results compared with state-of-the-arts. The visualized congenerous features also validate that our approach can effectively perceive the essential cooperation relations.
C1 [Yang, Jianyu] Soochow Univ, Sch Rail Transportat, Suzhou 215006, Peoples R China.
   [Liu, Wu; Mei, Tao] JD AI Res, Beijing 100105, Peoples R China.
   [Yuan, Junsong] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
C3 Soochow University - China; State University of New York (SUNY) System;
   State University of New York (SUNY) Buffalo
RP Liu, W (corresponding author), JD AI Res, Beijing 100105, Peoples R China.
EM jyyang@suda.edu.cn; liuwu@live.cn; jsyuan@buffalo.edu; tmei@jd.com
RI LU, CX/KFB-9510-2024; wen, liang/JNR-7720-2023; Mei, Tao/GQZ-0596-2022;
   Yuan, Junsong/A-5171-2011; XIE, WANYING/JNR-9259-2023
OI Mei, Tao/0000-0002-5990-7307; Yuan, Junsong/0000-0002-7901-8793
FU National Natural Science Foundation of China NSFC [61773272]; University
   at Buffalo; Six Talent Peaks Project of Jiangsu Province, China
   [XYDXX-053]; Suzhou Research Project of Technical Innovation, Jiangsu,
   China [SYG201711]
FX This work was supported in part by the National Natural Science
   Foundation of China NSFC under Grant 61773272, in part by the Start-up
   Funds from University at Buffalo, in part by the Six Talent Peaks
   Project of Jiangsu Province, China (No. XYDXX-053), and in part by
   Suzhou Research Project of Technical Innovation, Jiangsu, China under
   Grant SYG201711.
CR [Anonymous], 2014, PROJECT REP
   [Anonymous], 2017, P IEEE C COMPUTER VI
   Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598
   Cai XY, 2016, IEEE T MULTIMEDIA, V18, P141, DOI 10.1109/TMM.2015.2505089
   Clevert D.-A., 2016, P 4 INT C LEARN REPR, P1
   Devanne M, 2015, IEEE T CYBERNETICS, V45, P1340, DOI 10.1109/TCYB.2014.2350774
   Du Y, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P579, DOI 10.1109/ACPR.2015.7486569
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Fan ZX, 2019, IEEE T MULTIMEDIA, V21, P363, DOI 10.1109/TMM.2018.2859620
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872
   Gao X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P601, DOI 10.1145/3343031.3351170
   Han Y., 2018, Proceedings of 2018 International Joint Conference on Neural Networks, P1
   Hou YH, 2018, IEEE T CIRC SYST VID, V28, P807, DOI 10.1109/TCSVT.2016.2628339
   Hu JF, 2017, IEEE T PATTERN ANAL, V39, P2186, DOI 10.1109/TPAMI.2016.2640292
   Hu JF, 2016, LECT NOTES COMPUT SC, V9905, P280, DOI 10.1007/978-3-319-46448-0_17
   Huang WB, 2019, IEEE T IMAGE PROCESS, V28, P1773, DOI 10.1109/TIP.2018.2877936
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kim TS, 2017, IEEE COMPUT SOC CONF, P1623, DOI 10.1109/CVPRW.2017.207
   Kingma D. P., 2014, arXiv
   Koniusz P, 2016, LECT NOTES COMPUT SC, V9908, P37, DOI 10.1007/978-3-319-46493-0_3
   Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115
   Li B, 2019, AAAI CONF ARTIF INTE, P8561
   Li C, 2018, IEEE INT CONF SENS, P1, DOI 10.1109/TFUZZ.2018.2878200
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273
   Liu J, 2018, IEEE T IMAGE PROCESS, V27, P1586, DOI 10.1109/TIP.2017.2785279
   Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306
   Liu K, 2018, AAAI CONF ARTIF INTE, P7138
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Mehta D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073596
   Passalis N, 2017, IEEE I CONF COMP VIS, P5766, DOI 10.1109/ICCV.2017.614
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shao ZP, 2021, IEEE T CIRC SYST VID, V31, P160, DOI 10.1109/TCSVT.2020.2965574
   Shao ZP, 2019, IEEE T CIRC SYST VID, V29, P2986, DOI 10.1109/TCSVT.2018.2871660
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Si CY, 2018, LECT NOTES COMPUT SC, V11205, P106, DOI 10.1007/978-3-030-01246-5_7
   Slama R, 2015, PATTERN RECOGN, V48, P556, DOI 10.1016/j.patcog.2014.08.011
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Sun Y, 2019, IEEE I CONF COMP VIS, P5348, DOI 10.1109/ICCV.2019.00545
   Tang YS, 2018, PROC CVPR IEEE, P5323, DOI 10.1109/CVPR.2018.00558
   Veeriah V, 2015, IEEE I CONF COMP VIS, P4041, DOI 10.1109/ICCV.2015.460
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198
   Wang P, 2016, LECT NOTES COMPUT SC, V9911, P370, DOI 10.1007/978-3-319-46478-7_23
   Xu M, 2017, IEEE INT CON MULTI, P517, DOI 10.1109/ICME.2017.8019351
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang JY, 2019, IEEE INT CON MULTI, P1078, DOI 10.1109/ICME.2019.00189
   Yang YH, 2017, IEEE T MULTIMEDIA, V19, P519, DOI 10.1109/TMM.2016.2626959
   Yang ZY, 2019, IEEE T CIRC SYST VID, V29, P2405, DOI 10.1109/TCSVT.2018.2864148
   Yao A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.67
   Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI [10.1109/ICCV.2017.233, 10.1109/ICCV.2017.231]
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhang YP, 2012, IEEE VTS VEH TECHNOL
   Zhu WT, 2016, AAAI CONF ARTIF INTE, P3697
NR 59
TC 30
Z9 31
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 883
EP 898
DI 10.1109/TMM.2020.2990082
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300005
DA 2024-07-18
ER

PT J
AU Yuan, ZQ
   Sun, SY
   Duan, LX
   Li, CS
   Wu, X
   Xu, CS
AF Yuan, Zhaoquan
   Sun, Siyuan
   Duan, Lixin
   Li, Changsheng
   Wu, Xiao
   Xu, Changsheng
TI Adversarial Multimodal Network for Movie Story Question Answering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Knowledge discovery; Motion pictures; Visualization; Task analysis;
   Generators; Gallium nitride; Natural languages; Movie question
   answering; adversarial network; multimodal understanding
AB Visual question answering by using information from multiple modalities has attracted more and more attention in recent years. However, it is a very challenging task, as the visual content and natural language have quite different statistical properties. In this work, we present a method called Adversarial Multimodal Network (AMN) to better understand video stories for question answering. In AMN, we propose to learn multimodal feature representations by finding a more coherent subspace for video clips and the corresponding texts (e.g., subtitles and questions) based on generative adversarial networks. Moreover, a self-attention mechanism is developed to enforce our newly introduced consistency constraint in order to preserve the self-correlation between the visual cues of the original video clips in the learned multimodal representations. Extensive experiments on the benchmark MovieQA and TVQA datasets show the effectiveness of our proposed AMN over other published state-of-the-art methods.
C1 [Yuan, Zhaoquan; Wu, Xiao] Southwest Jiaotong Univ, Sch Informat Sci & Technol, Chengdu 610031, Peoples R China.
   [Sun, Siyuan; Duan, Lixin] Univ Elect Sci & Technol China, Big Data Res Ctr, Chengdu 610051, Peoples R China.
   [Sun, Siyuan; Duan, Lixin] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610051, Peoples R China.
   [Li, Changsheng] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China.
   [Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
C3 Southwest Jiaotong University; University of Electronic Science &
   Technology of China; University of Electronic Science & Technology of
   China; Beijing Institute of Technology; Chinese Academy of Sciences;
   Institute of Automation, CAS
RP Duan, LX (corresponding author), Univ Elect Sci & Technol China, Big Data Res Ctr, Chengdu 610051, Peoples R China.; Duan, LX (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610051, Peoples R China.; Li, CS (corresponding author), Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China.
EM zqyuan@swjtu.edu.cn; jacobsun1996@gmail.com; lxduan@uestc.edu.cn;
   lcs@bit.edu.cn; wuxiaohk@home.swjtu.edu.cn; csxu@nlpr.ia.ac.cn
RI Xu, Chang/GQP-7280-2022; Li, Changsheng/G-9890-2015; xu,
   cj/HJZ-3488-2023; Sun, Siyuan/ITV-0582-2023
OI Li, Changsheng/0000-0003-2469-4222; Duan, Lixin/0000-0002-0723-4016; Wu,
   Xiao/0000-0002-8322-8558; xu, chang sheng/0000-0001-8343-9665
FU Major Project for New Generation of AI [2018AAA0100400]; National
   Natural Science Foundation of China [61802053, 61772436, 61772118,
   61806044]; Sichuan Science and Technology Program [2020YJ0037,
   2020YJ0207]; Foundation for Department of Transportation of Henan
   Province [2019J-2-2]; Fundamental Research Funds for the Central
   Universities [2682019CX62]
FX This work was supported in part by the Major Project for New Generation
   of AI under Grant 2018AAA0100400, in part by the National Natural
   Science Foundation of China under Grants 61802053, 61772436, 61772118,
   and 61806044, in part by the Sichuan Science and Technology Program
   under Grants 2020YJ0037 and 2020YJ0207, in part by the Foundation for
   Department of Transportation of Henan Province under Grant 2019J-2-2,
   and in part by the Fundamental Research Funds for the Central
   Universities under Grant 2682019CX62.
CR Andreas J., 2016, P 2016 C N AM CHAPT, P1545, DOI DOI 10.18653/V1/N16-1181
   Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Das A, 2017, PROC CVPR IEEE, P1080, DOI 10.1109/CVPR.2017.121
   Devlin J., 2018, BERT PRE TRAINING DE
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P2371, DOI 10.1109/TMM.2018.2796248
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Grand Gabriel, 2019, P 2 WORKSH SHORTC VI, P1
   He Kaiming, 2016, P INT C COMP VIS PAT, DOI 10.1109/CVPR.2016.90
   Hu YT, 2018, IEEE T MULTIMEDIA, V20, P927, DOI 10.1109/TMM.2017.2760101
   Ilievski I, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P415, DOI 10.1145/3126686.3126695
   Jia R., 2017, P 2017 C EMP METH NA, P2021, DOI [10.18653/v1/D17-1215, DOI 10.18653/V1/D17-1215, DOI 10.18653/V1/D17-1215.URL]
   Jin WK, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1193, DOI 10.1145/3343031.3351065
   Kembhavi A, 2017, PROC CVPR IEEE, P5376, DOI 10.1109/CVPR.2017.571
   Kim D, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P3568
   Kim D, 2018, PROC CVPR IEEE, P4167, DOI 10.1109/CVPR.2018.00438
   Kim J, 2019, IEEE IJCNN, DOI [10.1109/ijcnn.2019.8852087, 10.1007/s00779-019-01299-w]
   Kim J, 2019, PROC CVPR IEEE, P8329, DOI 10.1109/CVPR.2019.00853
   Kim KM, 2018, LECT NOTES COMPUT SC, V11219, P698, DOI 10.1007/978-3-030-01267-0_41
   Kim KM, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2016
   Lee Seanie, 2019, P 2 WORKSH MACH READ, P196
   Lei J, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P1369
   Li JZ, 2018, PROC CVPR IEEE, P3655, DOI 10.1109/CVPR.2018.00385
   Li XP, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1166, DOI 10.1145/3343031.3350971
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Liang JW, 2018, PROC CVPR IEEE, P6135, DOI 10.1109/CVPR.2018.00642
   Liu F, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1175, DOI 10.1145/3343031.3350993
   Liu Y, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1013, DOI 10.1145/3269206.3271765
   Malinowski M, 2014, ADV NEUR IN, V27
   Malinowski M, 2015, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2015.9
   Massiceti D, 2018, PROC CVPR IEEE, P6097, DOI 10.1109/CVPR.2018.00638
   Mikolov T, 2013, ICLR WORKSHOP POSTER
   Mun J, 2017, IEEE I CONF COMP VIS, P2886, DOI 10.1109/ICCV.2017.312
   Na S, 2017, IEEE I CONF COMP VIS, P677, DOI 10.1109/ICCV.2017.80
   Oh JH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4227
   Pandhre S, 2017, ARXIV170908203
   Peng L, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1202, DOI 10.1145/3343031.3350925
   Shekhar R., 2018, P 27 INT C COMPUTATI
   Strub F, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2765
   Sukhbaatar S, 2015, ADV NEUR IN, V28
   Tapaswi M, 2016, PROC CVPR IEEE, P4631, DOI 10.1109/CVPR.2016.501
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang AR, 2020, IEEE T IMAGE PROCESS, V29, P489, DOI 10.1109/TIP.2019.2931534
   Wang BN, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4123
   Wang B, 2018, AAAI CONF ARTIF INTE, P7380
   Wang JZ, 2018, IEEE T MULTIMEDIA, V20, P2578, DOI 10.1109/TMM.2018.2855081
   Wang P, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1290
   Wang P, 2018, IEEE T PATTERN ANAL, V40, P2413, DOI 10.1109/TPAMI.2017.2754246
   Wu CF, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P519, DOI 10.1145/3240508.3240513
   Wu Q, 2017, COMPUT VIS IMAGE UND, V163, P21, DOI 10.1016/j.cviu.2017.05.001
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yang TH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1184, DOI 10.1145/3343031.3350969
   Yang X, 2019, AAAI CONF ARTIF INTE, P395
   Yu T, 2020, IEEE T IMAGE PROCESS, V29, P1204, DOI 10.1109/TIP.2019.2940677
   Zhai DM, 2018, IEEE T MULTIMEDIA, V20, P675, DOI 10.1109/TMM.2017.2749160
   Zhang HM, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1025
   Zhang L, 2018, IEEE T MULTIMEDIA, V20, P128, DOI 10.1109/TMM.2017.2723841
   Zhao Z, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3690
   Zhu YK, 2016, PROC CVPR IEEE, P4995, DOI 10.1109/CVPR.2016.540
NR 59
TC 11
Z9 11
U1 3
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1744
EP 1756
DI 10.1109/TMM.2020.3002667
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300021
DA 2024-07-18
ER

PT J
AU Zhang, HM
   Xu, M
AF Zhang, Haimin
   Xu, Min
TI Weakly Supervised Emotion Intensity Prediction for Recognition of
   Emotions in Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotion recognition; Streaming media; Visualization; Feature extraction;
   Neural networks; Semantics; Annotations; Image emotion recognition;
   emotion intensity prediction; deep neural networks
AB Recognition of emotions in images is attracting increasing research attention. Recent studies show that using local region information helps to improve the recognition performance. Intuitively, emotion intensity maps provide more detailed information than image regions. Inspired by this intuition, we propose an end-to-end deep neural network for image emotion recognition leveraging emotion intensity learning. The proposed network is composed of a first classification stream, an intensity prediction stream and a second classification stream. The intensity prediction stream is built on top of the feature pyramid network to extract multilevel features. The class activation mapping technique is used to generate pseudo intensity maps from the first classification stream to guide the proposed network for emotion intensity learning. The predicted intensity map is integrated into the second classification stream for final emotion recognition. The three streams are trained cooperatively to improve the performance. We evaluate the proposed network for both emotion recognition and sentiment classification on different benchmark datasets. The experimental results demonstrate that the proposed network achieves improved performance compared to previous state-of-the-art approaches.
C1 [Zhang, Haimin; Xu, Min] Univ Technol Sydney, Sch Elect & Data Engn, Ultimo, NSW 2007, Australia.
C3 University of Technology Sydney
RP Xu, M (corresponding author), Univ Technol Sydney, Sch Elect & Data Engn, Ultimo, NSW 2007, Australia.
EM haimin.zhang@uts.edu.au; min.xu@uts.edu.au
OI Zhang, Haimin/0000-0002-0021-3634
CR [Anonymous], 2017, P 26 INT JOINT C ART
   [Anonymous], 1986, EMOTION THEORY RES E
   [Anonymous], 2001, EMOTIONS SOCIAL PSYC
   [Anonymous], 2005, TECH REP
   [Anonymous], 2014, 2 INT C LEARN REPR I
   [Anonymous], 2014, Comput. Sci.
   Borth D., 2013, P 21 ACM INT C MULT, P459
   Edman P., 1972, EMOTION HUMAN FACE G
   Ekman P., 1982, EMOTION HUMAN FACE
   Hanjalic A, 2006, IEEE SIGNAL PROC MAG, V23, P90, DOI 10.1109/MSP.2006.1621452
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu JJ, 2019, IEEE WINT CONF APPL, P1043, DOI 10.1109/WACV.2019.00116
   Hu SH, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3328997
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lang PJ, 1998, BIOL PSYCHIAT, V44, P1248, DOI 10.1016/S0006-3223(98)00275-3
   LANG PJ, 1979, PSYCHOPHYSIOLOGY, V16, P495, DOI 10.1111/j.1469-8986.1979.tb01511.x
   Lee BU, 2019, IEEE INT CONF ROBOT, P3281, DOI [10.1109/icra.2019.8794161, 10.1109/ICRA.2019.8794161]
   Lee J, 2019, IEEE I CONF COMP VIS, P10142, DOI 10.1109/ICCV.2019.01024
   Li KP, 2018, PROC CVPR IEEE, P9215, DOI 10.1109/CVPR.2018.00960
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu HY, 2016, IEEE T NEUR NET LEAR, V27, P1201, DOI 10.1109/TNNLS.2016.2553579
   Machajdik J., 2010, P 18 ACM INT C MULT, P83, DOI DOI 10.1145/1873951.1873965
   Mikels JA, 2005, BEHAV RES METHODS, V37, P626, DOI 10.3758/BF03192732
   Panda R., 2018, P EUR C COMP VIS, P579
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Paszke A, 2019, ADV NEUR IN, V32
   Peng KC, 2016, IEEE IMAGE PROC, P614, DOI 10.1109/ICIP.2016.7532430
   Rao TR, 2019, NEUROCOMPUTING, V333, P429, DOI 10.1016/j.neucom.2018.12.053
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salakhutdinov R., 2009, AISTATS
   Saxena A, 2007, IEEE I CONF COMP VIS, P1
   She DY, 2020, IEEE T MULTIMEDIA, V22, P1358, DOI 10.1109/TMM.2019.2939744
   Simonyan K., 2014, CORR
   Sobel I., 1973, PATTERN CLASSIFICATI
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu M, 2013, SIGNAL PROCESS, V93, P2140, DOI 10.1016/j.sigpro.2012.06.026
   Yang JF, 2018, IEEE T MULTIMEDIA, V20, P2513, DOI 10.1109/TMM.2018.2803520
   You QZ, 2016, AAAI CONF ARTIF INTE, P308
   You QZ, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P13, DOI 10.1145/2835776.2835779
   You QZ, 2015, AAAI CONF ARTIF INTE, P381
   Yuan J., 2013, P 2 INT WORKSH ISS S, P1
   Zhang HM, 2016, IEEE IMAGE PROC, P629, DOI 10.1109/ICIP.2016.7532433
   Zhang XL, 2018, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2018.00144
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P47, DOI 10.1145/2647868.2654930
   Zhou BL, 2014, ADV NEUR IN, V27
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 47
TC 22
Z9 22
U1 4
U2 43
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2033
EP 2044
DI 10.1109/TMM.2020.3007352
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100015
DA 2024-07-18
ER

PT J
AU Zhong, HS
   Chen, JY
   Shen, C
   Zhang, HW
   Huang, JQ
   Hua, XS
AF Zhong, Huasong
   Chen, Jingyuan
   Shen, Chen
   Zhang, Hanwang
   Huang, Jianqiang
   Hua, Xian-Sheng
TI Self-Adaptive Neural Module Transformer for Visual Question Answering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Layout; Cognition; Task analysis; Visualization; Neural networks;
   Knowledge discovery; Decoding; Visual question answering; neural module
   transformer; multi modal; self-adaptive
AB Vision and language understanding is one of the most fundamental and difficult tasks in Multimedia Intelligence. Simultaneously Visual Question Answering (VQA) is even more challenging since it requires complex reasoning steps to the correct answer. To achieve this, Neural Module Network (NMN) and its variants rely on parsing the natural language question into a module layout (i.e., a problem-solving program). In particular, this process follows a feedforward encoder-decoder pipeline: the encoder embeds the question into a static vector and the decoder generates the layout. However, we argue that such conventional encoder-decoder neglects the dynamic nature of question comprehension (i.e., we should attend to different words from step to step) and per-module intermediate results (i.e., we should discard module performing badly) in the reasoning steps. In this paper, we present a novel NMN, called Self-Adaptive Neural Module Transformer (SANMT), which adaptively adjusts both of the question feature encoding and the layout decoding by considering intermediate Q&A results. Specifically, we encode the intermediate results with the given question features by a novel transformer module to generate dynamic question feature embedding which evolves over reasoning steps. Besides, the transformer utilizes the intermediate results from each reasoning step to guide subsequent layout arrangement. Extensive experimental evaluations demonstrate the superiority of the proposed SANMT over NMN and its variants on four challenging benchmarks, including CLEVR, CLEVR-CoGenT, VQAv1.0, and VQAv2.0 (on average the relative improvement over NMN are 1.5, 2.3, 0.7 and 0.5 points with respect to accuracy).
C1 [Zhong, Huasong; Chen, Jingyuan; Shen, Chen; Huang, Jianqiang; Hua, Xian-Sheng] Alibaba Grp, Dept DAMO Acad, Hangzhou 311121, Peoples R China.
   [Zhang, Hanwang] Nanyang Technol Univ, Singapore 639798, Singapore.
C3 Alibaba Group; Nanyang Technological University
RP Chen, JY (corresponding author), Alibaba Grp, Dept DAMO Acad, Hangzhou 311121, Peoples R China.
EM zhonghsuestc@163.com; jingyuanchen91@gmail.com; zjushenchen@gmail.com;
   hanwangzhang@ntu.edu.sg; jianqiang.hjq@alibaba-inc.com;
   xiansheng.hxs@alibaba-inc.com
OI Zhang, Hanwang/0000-0001-7374-8739
CR Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Ben QW, 2015, EUR J CANCER PREV, V24, P286, DOI 10.1097/CEJ.0000000000000077
   Britz D., 2017, P 2017 C EMPIRICAL M, P1442, DOI [10.18653/v1/D17-1151, DOI 10.18653/V1/D17-1151]
   Carlsson N, 2017, IEEE T MULTIMEDIA, V19, P1637, DOI 10.1109/TMM.2017.2673412
   Fang ZW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1002, DOI 10.1145/3240508.3240662
   Gao P, 2019, PROC CVPR IEEE, P6632, DOI 10.1109/CVPR.2019.00680
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Han TT, 2017, IEEE T MULTIMEDIA, V19, P712, DOI 10.1109/TMM.2016.2631881
   Hao YB, 2017, IEEE T MULTIMEDIA, V19, P1, DOI 10.1109/TMM.2016.2610324
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   He YH, 2019, IEEE WINT CONF APPL, P1213, DOI 10.1109/WACV.2019.00134
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu RH, 2018, LECT NOTES COMPUT SC, V11211, P55, DOI 10.1007/978-3-030-01234-2_4
   Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93
   Hudson D.A., 2018, INT C LEARN REPR ICL
   Jabri A, 2016, LECT NOTES COMPUT SC, V9912, P727, DOI 10.1007/978-3-319-46484-8_44
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Johnson O, 2017, IEEE INT SYMP INFO, P898, DOI 10.1109/ISIT.2017.8006658
   Kafle K, 2017, COMPUT VIS IMAGE UND, V163, P3, DOI 10.1016/j.cviu.2017.06.005
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mascharka D, 2018, PROC CVPR IEEE, P4942, DOI 10.1109/CVPR.2018.00519
   Nie LQ, 2013, IEEE T MULTIMEDIA, V15, P426, DOI 10.1109/TMM.2012.2229971
   Perez E, 2018, AAAI CONF ARTIF INTE, P3942
   Sutskever I., 2014, ADV NEURAL INFORM PR, V4, P3104, DOI DOI 10.5555/2969033.2969173
   Vaswani Ashish, 2017, Advances in Neural Information Processing Systems (NeurIPS), V17, P6000, DOI DOI 10.48550/ARXIV.1706.03762
   Wu Q, 2017, COMPUT VIS IMAGE UND, V163, P21, DOI 10.1016/j.cviu.2017.05.001
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Zhang P, 2016, PROC CVPR IEEE, P5014, DOI 10.1109/CVPR.2016.542
   Zhang WQ, 2020, IEEE T MULTIMEDIA, V22, P1032, DOI 10.1109/TMM.2019.2935678
   Zhu WW, 2015, IEEE MULTIMEDIA, V22, P96, DOI 10.1109/MMUL.2015.66
NR 35
TC 33
Z9 35
U1 3
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1264
EP 1273
DI 10.1109/TMM.2020.2995278
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RU3SE
UT WOS:000645068200007
DA 2024-07-18
ER

PT J
AU Che, WB
   Fan, XP
   Xiong, RQ
   Zhao, DB
AF Che, Wenbin
   Fan, Xiaopeng
   Xiong, Ruiqin
   Zhao, Debin
TI Visual Relationship Embedding Network for Image Paragraph Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Semantics; Task analysis; Proposals; Automobiles;
   Buildings; Gallium nitride; Paragraph generation; image caption; region
   localization; attention network; visual relationship; GAN; LSTM
ID LANGUAGE
AB Image paragraph generation aims to produce a complete description of a given image. This task is more challenging than image captioning, which only generates one sentence to describe the entire image. Traditional paragraph generation methods usually produce paragraph descriptions based on individual regions that are detected by a Region Proposal Network (RPN). However, relationships among visual objects are either ignored or utilized in an implicit manner in previous work. In this paper, we attempt to explore more visual information through a novel paragraph generation network that explicitly incorporates visual relationship semantics when producing descriptions. First, a novel Relation Pair Generative Adversarial Network (RP-GAN) is designed to locate regions that may cover subjective or objective elements. Then, their relationships are inferred through an attention-based network. Finally, the visual features and relationship semantics of valid relation pairs are taken as inputs by a Long Short-Term Memory (LSTM) network for generating sentences. The experimental results show that by explicitly utilizing the predicted relationship information, our proposed method obtains more accurate and informative paragraph descriptions than previous methods.
C1 [Che, Wenbin; Fan, Xiaopeng; Zhao, Debin] Harbin Inst Technol, Res Ctr Intelligent Interface & Human Comp Intera, Dept Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Che, Wenbin; Fan, Xiaopeng; Zhao, Debin] PengCheng Lab, Shenzhen 518055, Peoples R China.
   [Xiong, Ruiqin] Peking Univ, Inst Digital Media, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
C3 Harbin Institute of Technology; Peking University
RP Fan, XP (corresponding author), Harbin Inst Technol, Res Ctr Intelligent Interface & Human Comp Intera, Dept Comp Sci & Technol, Harbin 150001, Peoples R China.
EM chewenbin@hit.edu.cn; fxp@hit.edu.cn; rqxiong@pku.edu.cn;
   dbzhao@hit.edu.cn
RI Zhao, Debin/JEP-0204-2023
OI Xiong, Ruiqin/0000-0001-9796-0478
FU National Science Foundation of China [61972115, 61631017, 61872116]
FX This work was supported by the National Science Foundation of China
   under Grant 61972115, Grant 61631017, and Grant 61872116.
CR Amodei D, 2016, PR MACH LEARN RES, V48
   [Anonymous], 2013, PREPRINT ARXIV 1308
   [Anonymous], 2017, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2017.345
   Baraldi L, 2017, IEEE T MULTIMEDIA, V19, P955, DOI 10.1109/TMM.2016.2644872
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Che WB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1435, DOI 10.1145/3240508.3240695
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Dai B, 2017, PROC CVPR IEEE, P3298, DOI 10.1109/CVPR.2017.352
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Donahue B, 2015, 2015 IEEE AEROSPACE CONFERENCE
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gould S, 2008, INT J COMPUT VISION, V80, P300, DOI 10.1007/s11263-008-0140-x
   Gupta A, 2009, IEEE T PATTERN ANAL, V31, P1775, DOI 10.1109/TPAMI.2009.83
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Krause J, 2017, PROC CVPR IEEE, P3337, DOI 10.1109/CVPR.2017.356
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lee CY, 2016, PROC CVPR IEEE, P2231, DOI 10.1109/CVPR.2016.245
   Li JA, 2017, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2017.211
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Mao J., 2014, COMPUT VISION PATTER
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Ordonez V., 2011, P INT C NEUR INF PRO, P1143
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Simonyan K., 2015, P 3 INT C LEARN REPR
   Simonyan K., 2015, P INT C LEARN REPR
   Tsung-Yi L., 2015, MICROSOFT COCO COMMO
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang LJ, 2017, PROC CVPR IEEE, P1978, DOI 10.1109/CVPR.2017.214
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yang Z., 2016, P INT C NEUR INF PRO, P2369
   Young P., 2014, P TACL, V2, P67
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang HW, 2017, PROC CVPR IEEE, P3107, DOI 10.1109/CVPR.2017.331
   Zhang J, 2017, PROC CVPR IEEE, P5226, DOI 10.1109/CVPR.2017.555
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
NR 47
TC 11
Z9 14
U1 1
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2307
EP 2320
DI 10.1109/TMM.2019.2954750
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NE0TL
UT WOS:000562310200008
DA 2024-07-18
ER

PT J
AU Furuta, R
   Inoue, N
   Yamasaki, T
AF Furuta, Ryosuke
   Inoue, Naoto
   Yamasaki, Toshihiko
TI PixelRL: Fully Convolutional Network With Reinforcement Learning for
   Image Processing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image color analysis; Task analysis; Image denoising; Learning systems;
   Image restoration; Convolution; Reinforcement learning (RL); image
   processing; denoising; restoration; local color enhancement;
   saliency-driven image editing
ID FRAMEWORK; REMOVAL
AB This article tackles a new problem setting: reinforcement learning with pixel-wise rewards (pixelRL) for image processing. After the introduction of the deep Q-network, deep RL has been achieving great success. However, the applications of deep reinforcement learning (RL) for image processing are still limited. Therefore, we extend deep RL to pixelRL for various image processing applications. In pixelRL, each pixel has an agent, and the agent changes the pixel value by taking an action. We also propose an effective learning method for pixelRL that significantly improves the performance by considering not only the future states of the own pixel but also those of the neighbor pixels. The proposed method can be applied to some image processing tasks that require pixel-wise manipulations, where deep RL has never been applied. Besides, it is possible to visualize what kind of operation is employed for each pixel at each iteration, which would help us understand why and how such an operation is chosen. We also believe that our technology can enhance the explainability and interpretability of the deep neural networks. In addition, because the operations executed at each pixels are visualized, we can change or modify the operations if necessary. We apply the proposed method to a variety of image processing tasks: image denoising, image restoration, local color enhancement, and saliency-driven image editing. Our experimental results demonstrate that the proposed method achieves comparable or better performance, compared with the state-of-the-art methods based on supervised learning. The source code is available on https://github.com/rfuruta/pixelRL.
C1 [Furuta, Ryosuke] Univ Tokyo, Tokyo 1138656, Japan.
   [Furuta, Ryosuke] Tokyo Univ Sci, Dept Informat & Comp Technol, Tokyo 1258585, Japan.
   [Inoue, Naoto; Yamasaki, Toshihiko] Univ Tokyo, Grad Sch Informat Sci & Technol, Dept Informat & Commun Engn, Tokyo 1138656, Japan.
C3 University of Tokyo; Tokyo University of Science; University of Tokyo
RP Furuta, R (corresponding author), Tokyo Univ Sci, Dept Informat & Comp Technol, Tokyo 1258585, Japan.
EM rfuruta@rs.tus.ac.jp; inoue@hal.t.u-tokyo.ac.jp;
   yamasaki@hal.t.u-tokyo.ac.jp
FU Japan Society for the Promotion of Science [16J07267]; JST-CREST
   [JPMJCR1686]; Grants-in-Aid for Scientific Research [16J07267] Funding
   Source: KAKEN
FX This work was supported in part by the Japan Society for the Promotion
   of Science through the Grants-in-Aid for Scientific Research under Grant
   16J07267 and in part by the JST-CREST under Grant JPMJCR1686.
CR [Anonymous], 2018, ACM TOG
   [Anonymous], 2005, TEXTURE 2005 P 4TH I
   [Anonymous], 2014, P 1 INTERNATIONALWOR, DOI DOI 10.1145/2662996.2663009
   [Anonymous], 2011, 2011 IEEE WORKSH APP
   Bertalmio M, 2003, IEEE T IMAGE PROCESS, V12, P882, DOI 10.1109/TIP.2003.815261
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Burger H., 2012, CVPR
   Caicedo JC, 2015, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2015.286
   Cao QX, 2017, PROC CVPR IEEE, P1656, DOI 10.1109/CVPR.2017.180
   Chen Y., 2019, P ACM INT C MULT AS
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Di P, 2017, CHIN CONT DECIS CONF, P2497, DOI 10.1109/CCDC.2017.7978934
   fir Nachum O, 2017, ADV NEURAL INFORM PR, P2775
   Furuta R, 2019, AAAI CONF ARTIF INTE, P3598
   Ganin Y., 2018, P MACHINE LEARNING R, P1652
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Hou YT, 2007, IEEE INFOCOM SER, P1, DOI 10.1109/INFCOM.2007.9
   Hwang SJ, 2012, LECT NOTES COMPUT SC, V7572, P569, DOI 10.1007/978-3-642-33718-5_41
   Iqbal S., 2017, ADV NEURAL INFORM PR, P6382, DOI DOI 10.48550/ARXIV.1706.02275
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jie Z., 2016, NIPS, P127
   Kim Y, 2008, IEEE T VIS COMPUT GR, V14, P772, DOI 10.1109/TVCG.2007.70624
   Kim Y, 2006, IEEE T VIS COMPUT GR, V12, P925, DOI 10.1109/TVCG.2006.174
   Kingma D. P., 2014, arXiv
   Kong XY, 2017, PROC CVPR IEEE, P7072, DOI 10.1109/CVPR.2017.748
   Lan SY, 2018, PROC CVPR IEEE, P6771, DOI 10.1109/CVPR.2018.00708
   Lang K., 1995, P 12 INT C MACHINE L, P331
   Lefkimmiatis S, 2017, PROC CVPR IEEE, P5882, DOI 10.1109/CVPR.2017.623
   Li DB, 2018, PROC CVPR IEEE, P8193, DOI 10.1109/CVPR.2018.00855
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu Y, 2019, LECT NOTES COMPUT SC, V11935, P128, DOI 10.1007/978-3-030-36189-1_11
   Luisier F, 2011, IEEE T IMAGE PROCESS, V20, P696, DOI 10.1109/TIP.2010.2073477
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   Maes F, 2009, MACH LEARN, V77, P271, DOI 10.1007/s10994-009-5140-8
   Mairal J, 2008, IEEE T IMAGE PROCESS, V17, P53, DOI 10.1109/TIP.2007.911828
   Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   Mateescu Victor. A., 2013, 2013 IEEE INT C MULT, P1
   Mathe S, 2016, PROC CVPR IEEE, P2894, DOI 10.1109/CVPR.2016.316
   Mechrez R, 2018, IEEE WINT CONF APPL, P1368, DOI 10.1109/WACV.2018.00154
   Mi JX, 2017, 2017 INTERNATIONAL CONFERENCE ON SECURITY, PATTERN ANALYSIS, AND CYBERNETICS (SPAC), P660, DOI 10.1109/SPAC.2017.8304358
   Mnih V., 2013, P ADV C NEUR INF PRO, P1
   Mnih V, 2016, PR MACH LEARN RES, V48
   Montabone S, 2010, IMAGE VISION COMPUT, V28, P391, DOI 10.1016/j.imavis.2009.06.006
   Nguyen TV, 2013, IEEE T MULTIMEDIA, V15, P1910, DOI 10.1109/TMM.2013.2272919
   Park J, 2018, PROC CVPR IEEE, P5928, DOI 10.1109/CVPR.2018.00621
   Qin Y, 2018, INT J COMPUT VISION, V126, P751, DOI 10.1007/s11263-017-1062-2
   Rao YM, 2017, IEEE I CONF COMP VIS, P3951, DOI 10.1109/ICCV.2017.424
   Razzak MI, 2018, L N COMPUT VIS BIOME, V26, P323, DOI 10.1007/978-3-319-65981-7_12
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Ren JS., 2015, ADV NEURAL INFORM PR, V1, P901
   Roth S, 2005, PROC CVPR IEEE, P860
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sakiyama Tadao, 2011, Science Report of the Yokosuka City Museum, P43
   Schulman J, 2015, PR MACH LEARN RES, V37, P1889
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Supancic J, 2017, IEEE I CONF COMP VIS, P322, DOI 10.1109/ICCV.2017.43
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Tokui S, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2002, DOI 10.1145/3292500.3330756
   Wulfmeier M., 2015, P DEEP REINF LEARN W
   Xiang Y, 2015, IEEE I CONF COMP VIS, P4705, DOI 10.1109/ICCV.2015.534
   Xie Junyuan, 2012, ADV NEURAL INFORM PR, P341, DOI [DOI 10.5555/2999134.2999173, DOI 10.1109/AGRO-GEOINFORMATICS.2012.6311605]
   Xu L, 2017, INT GEOSCI REMOTE SE, P3929, DOI 10.1109/IGARSS.2017.8127859
   Yan ZC, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2790296
   Yu F., 2017, PROC CVPR IEEE, P472, DOI [DOI 10.1109/CVPR.2017.75, 10.1109/CVPR.2017.75]
   Yu K, 2018, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR.2018.00259
   Yun S, 2017, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2017.148
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
NR 74
TC 55
Z9 60
U1 2
U2 47
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2020
VL 22
IS 7
BP 1704
EP 1719
DI 10.1109/TMM.2019.2960636
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MG4GJ
UT WOS:000545990500005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tan, WM
   Yan, B
   Lin, CM
   Niu, XJ
AF Tan, Weimin
   Yan, Bo
   Lin, Chumin
   Niu, Xuejing
TI Cycle-IR: Deep Cyclic Image Retargeting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Deep learning; Distortion; Coherence; Media; Training;
   Task analysis; Image retargeting; deep learning; cycle consistency
ID RECOGNITION
AB Supervised deep learning techniques have achieved great success in various fields due to getting rid of the limitation of handcrafted representations. However, most previous image retargeting algorithms still employ fixed design principles such as using gradient map or handcrafted features to compute saliency map, which inevitably restricts its generality. Deep learning techniques may help to address this issue, but the challenging problem is that we need to build a large-scale image retargeting dataset for the training of deep retargeting models. However, building such a dataset requires huge human efforts. In this paper, we propose a novel deep cyclic image retargeting approach, called Cycle-IR, to firstly implement image retargeting with a single deep model, without relying on any explicit user annotations. Our idea is built on the reverse mapping from the retargeted images to the given images. If the retargeted image has serious distortion or excessive loss of important visual information, the reverse mapping is unlikely to restore the input image well. We constrain this forward-reverse consistency by introducing a cyclic perception coherence loss. In addition, we propose a simple yet effective image retargeting network (IRNet) to implement the image retargeting process. Our IRNet contains a spatial and channel attention layer, which is able to discriminate visually important regions of input images effectively, especially in cluttered images. Given arbitrary sizes of input images and desired aspect ratios, our Cycle-IR can produce visually pleasing target images directly. Extensive experiments on the standard RetargetMe dataset show the superiority of our Cycle-IR.
C1 [Tan, Weimin; Yan, Bo; Lin, Chumin; Niu, Xuejing] Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 201203, Peoples R China.
C3 Fudan University
RP Yan, B (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 201203, Peoples R China.
EM 14110240025@fudan.edu.cn; byan@fudan.edu.cn; cmlin17@fudan.edu.cn;
   17210240176@fudan.edu.cn
RI Yan, Bo/AFQ-7025-2022
OI Yan, Bo/0000-0002-7775-1270
FU NSFC [61902076, 61772137]
FX This work was supported by NSFC under Grants 61902076 and 61772137.
CR [Anonymous], 2010, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2010.5539801
   [Anonymous], 2010, P 18 ACM INT C MULT
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   BRISLIN RW, 1970, J CROSS CULT PSYCHOL, V1, P185, DOI 10.1177/135910457000100301
   Chen R.C., 2010, 2010 IEEE International Conference on Fuzzy Systems, P1
   Cho D, 2017, IEEE I CONF COMP VIS, P4568, DOI 10.1109/ICCV.2017.488
   Dong WM, 2016, IEEE T VIS COMPUT GR, V22, P1088, DOI 10.1109/TVCG.2015.2440255
   Dong WM, 2014, IEEE T VIS COMPUT GR, V20, P111, DOI 10.1109/TVCG.2013.103
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fang Y, 2011, INT C PAR DISTRIB SY, P1044, DOI 10.1109/ICPADS.2011.158
   Gallea R, 2014, IEEE T MULTIMEDIA, V16, P971, DOI 10.1109/TMM.2014.2305917
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Grundmann M, 2010, PROC CVPR IEEE, P569, DOI 10.1109/CVPR.2010.5540165
   Huang QX, 2013, COMPUT GRAPH FORUM, V32, P177, DOI 10.1111/cgf.12184
   Jin Y, 2010, VISUAL COMPUT, V26, P769, DOI 10.1007/s00371-010-0472-8
   Karni Z, 2009, COMPUT GRAPH FORUM, V28, P1257, DOI 10.1111/j.1467-8659.2009.01503.x
   Krähenbühl P, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1616452.1618472, 10.1145/1618452.1618472]
   Lau CP, 2018, IEEE T IMAGE PROCESS, V27, P5787, DOI 10.1109/TIP.2018.2858146
   Lei JJ, 2017, IEEE T MULTIMEDIA, V19, P1442, DOI 10.1109/TMM.2017.2660440
   Li B, 2015, IEEE T IMAGE PROCESS, V24, P2811, DOI 10.1109/TIP.2015.2431441
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li XL, 2016, IEEE T IMAGE PROCESS, V25, P740, DOI 10.1109/TIP.2015.2507942
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P1092, DOI 10.1109/TIP.2018.2872876
   Liang Y, 2017, IEEE T VIS COMPUT GR, V23, P1099, DOI 10.1109/TVCG.2016.2517641
   Lin SS, 2014, IEEE T CIRC SYST VID, V24, P759, DOI 10.1109/TCSVT.2013.2291282
   Liu YJ, 2011, COMPUT GRAPH FORUM, V30, P583, DOI 10.1111/j.1467-8659.2011.01881.x
   Liu YL, 2019, AAAI CONF ARTIF INTE, P8794
   Mansfield A, 2010, LECT NOTES COMPUT SC, V6311, P143, DOI 10.1007/978-3-642-15549-9_11
   Nie YW, 2013, IEEE T VIS COMPUT GR, V19, P1664, DOI 10.1109/TVCG.2012.176
   Noh H., 2012, P 20 ACM INT C MULTI, P709, DOI DOI 10.1145/2393347.2396293
   Pal R., 2016, INNOVATIVE RES ATTEN, P115
   Panozzo D, 2012, COMPUT GRAPH FORUM, V31, P229, DOI 10.1111/j.1467-8659.2012.03001.x
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   Qi SY, 2012, LECT NOTES COMPUT SC, V7577, P314, DOI 10.1007/978-3-642-33783-3_23
   Qu Z, 2013, IEEE T MULTIMEDIA, V15, P1677, DOI 10.1109/TMM.2013.2267727
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Shao F, 2017, IEEE T IMAGE PROCESS, V26, P4790, DOI 10.1109/TIP.2017.2721546
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan WM, 2018, IEEE T CIRC SYST VID, V28, P3154, DOI 10.1109/TCSVT.2017.2742243
   Tan WM, 2016, IEEE T MULTIMEDIA, V18, P128, DOI 10.1109/TMM.2015.2500727
   Wang J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330512
   Wolf L, 2007, IEEE I CONF COMP VIS, P1418
   Yan B, 2017, IEEE T IMAGE PROCESS, V26, P2454, DOI 10.1109/TIP.2017.2681840
   Zhang LM, 2015, IEEE T MULTIMEDIA, V17, P1538, DOI 10.1109/TMM.2015.2451954
   Zhou TH, 2016, PROC CVPR IEEE, P117, DOI 10.1109/CVPR.2016.20
   Zhou TH, 2015, PROC CVPR IEEE, P1191, DOI 10.1109/CVPR.2015.7298723
   Zhou YZ, 2018, IEEE T IMAGE PROCESS, V27, P2301, DOI 10.1109/TIP.2017.2779272
NR 48
TC 23
Z9 24
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2020
VL 22
IS 7
BP 1730
EP 1743
DI 10.1109/TMM.2019.2959925
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MG4GJ
UT WOS:000545990500007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Agethen, S
   Hsu, WH
AF Agethen, Sebastian
   Hsu, Winston H.
TI Deep Multi-Kernel Convolutional LSTM Networks and an Attention-Based
   Mechanism for Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Kernel; Videos; Task analysis; Convolution; Feature extraction; YouTube;
   Mathematical model; Computational and artificial intelligence; neural
   networks; feedforward neural networks; recurrent neural networks
ID ACTION RECOGNITION; FUSION
AB Action recognition greatly benefits motion understanding in video analysis. Recurrent networks such as long short-term memory (LSTM) networks are a popular choice for motion-aware sequence learning tasks. Recently, a convolutional extension of LSTM was proposed, in which input-to-hidden and hidden-to-hidden transitions are modeled through convolution with a single kernel. This implies an unavoidable trade-off between effectiveness and efficiency. Herein, we propose a new enhancement to convolutional LSTM networks that supports accommodation of multiple convolutional kernels and layers. This resembles a Network-in-LSTM approach, which improves upon the aforementioned concern. In addition, we propose an attention-based mechanism that is specifically designed for our multi-kernel extension. We evaluated our proposed extensions in a supervised classification setting on the UCF-101 and Sports-1M datasets, with the findings showing that our enhancements improve accuracy. We also undertook qualitative analysis to reveal the characteristics of our system and the convolutional LSTM baseline.
C1 [Agethen, Sebastian; Hsu, Winston H.] Natl Taiwan Univ, Taipei 10617, Taiwan.
C3 National Taiwan University
RP Agethen, S (corresponding author), Natl Taiwan Univ, Taipei 10617, Taiwan.
EM d01944015@ntu.edu.tw; whsu@ntu.edu.tw
OI HSU, WINSTON/0000-0002-3330-0638; Agethen, Sebastian/0000-0002-5010-4452
FU Ministry of Science and Technology, Taiwan [MOST 108-2634-F-002-004];
   NVIDIA Grants; DGX-1 AI Supercomputer
FX This work was supported in part by the Ministry of Science and
   Technology, Taiwan, under Grant MOST 108-2634-F-002-004, in part by
   NVIDIA Grants, and in part by DGX-1 AI Supercomputer.
CR [Anonymous], 2015, ARXIV150602078
   [Anonymous], 2008, PROC BRIT MACH VIS C
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], ARXIV180103150
   [Anonymous], 2012, CoRR
   Bregonzio M, 2009, PROC CVPR IEEE, P1948, DOI 10.1109/CVPRW.2009.5206779
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Fan ZX, 2019, IEEE T MULTIMEDIA, V21, P363, DOI 10.1109/TMM.2018.2859620
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kay W., 2017, CORR ABS170506950
   Kim Seongchan, 2017, ARXIV171102316
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   LeCun Y, 2004, PROC CVPR IEEE, P97
   Li Z., 2016, ARXIV160701794
   Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1109/PLASMA.2013.6634954, 10.1017/S1368980013002176]
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Rochan M., 2018, BMVC
   Shi XJ, 2015, ADV NEUR IN, V28
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Simonyan K, 2014, ADV NEUR IN, V27
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sutskever I, 2014, ADV NEUR IN, V27
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Venugopalan Subhashini, 2014, P 2015 C N AM CHAPT
   Walker J, 2017, IEEE I CONF COMP VIS, P3352, DOI 10.1109/ICCV.2017.361
   Wang QR, 2019, IEEE T MULTIMEDIA, V21, P930, DOI 10.1109/TMM.2018.2869277
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wu Yonghui, 2016, P C ASS MACH TRANSL
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
NR 39
TC 21
Z9 23
U1 3
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2020
VL 22
IS 3
BP 819
EP 829
DI 10.1109/TMM.2019.2932564
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KU2ZL
UT WOS:000519576700020
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tu, GY
   Fu, YW
   Li, BY
   Gao, JR
   Jiang, YG
   Xue, XY
AF Tu, Guoyun
   Fu, Yanwei
   Li, Boyang
   Gao, Jiarui
   Jiang, Yu-Gang
   Xue, Xiangyang
TI A Multi-Task Neural Approach for Emotion Attribution, Classification,
   and Summarization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Feature extraction; Emotion recognition; Task analysis;
   Visualization; Psychology; Computer architecture; Artificial neural
   networks; multi-layer neural network; image sequences; machine vision
ID VIDEO; MODEL
AB Emotional content is a crucial ingredient in user-generated videos. However, the sparsity of emotional expressions in the videos poses an obstacle to visual emotion analysis. In this paper, we propose a new neural approach, Bi-stream Emotion Attribution-Classification Network (BEAC-Net), to solve three related emotion analysis tasks: emotion recognition, emotion attribution, and emotion-oriented summarization, in a single integrated framework. BEAC-Net has two major constituents, an attribution network and a classification network. The attribution network extracts the main emotional segment that classification should focus on in order to mitigate the sparsity issue. The classification network utilizes both the extracted segment and the original video in a bi-stream architecture. We contribute a new dataset for the emotion attribution task with human-annotated ground-truth labels for emotion segments. Experiments on two video datasets demonstrate superior performance of the proposed framework and the complementary nature of the dual classification streams.
C1 [Tu, Guoyun; Gao, Jiarui] Fudan Univ, Shanghai 200086, Peoples R China.
   [Xue, Xiangyang] Fudan Univ, Comp Sci, Shanghai 200086, Peoples R China.
   [Fu, Yanwei] Fudan Univ, Sch Data Sci, Shanghai 200433, Peoples R China.
   [Li, Boyang] Baidu Res, Big Data Lab, Sunnyvale, CA 94089 USA.
   [Jiang, Yu-Gang] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
   [Jiang, Yu-Gang] Jilian Technol Grp Video, Shanghai 200080, Peoples R China.
C3 Fudan University; Fudan University; Fudan University; Baidu; Fudan
   University
RP Jiang, YG (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
EM 14307110177@fudan.edu.cn; yanweifu@fudan.edu.cn; boyangli@baidu.com;
   jrgao14@fudan.edu.cn; ygj@fudan.edu.cn; xyxue@fudan.edu.cn
RI Fu, Yanwei/JTT-7059-2023; Li, Boyang Albert/AAG-4957-2021
OI Li, Boyang Albert/0000-0002-6230-2376; Xue,
   Xiangyang/0000-0002-4897-9209; Fu, Yanwei/0000-0002-6595-6893
FU National Natural Science Foundation of China [61572134, 61572138,
   U1611461]; Shanghai Sailing Program [17YF1427500]; Fudan
   University-CIOMP Joint Fund [FC2017-006]; STCSM Project [16JC1420400];
   Shanghai Municipal Science and Technology Major Project [2017SHZDZX01,
   2018SHZDZX01]; ZJLab
FX This work was supported in part by the National Natural Science
   Foundation of China Projects under Grant 61572134, Grant 61572138, and
   Grant U1611461, in part by the Shanghai Sailing Program (17YF1427500),
   in part by the Fudan University-CIOMP Joint Fund (FC2017-006), in part
   by the STCSM Project under Grant 16JC1420400, in part by the Shanghai
   Municipal Science and Technology Major Project under Grant 2017SHZDZX01
   and Grant 2018SHZDZX01, and in part by ZJLab. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. James She. (Guoyun Tu and Yanwei Fu contributed
   equally to this work.)
CR Alameda-Pineda X, 2016, PROC CVPR IEEE, P5240, DOI 10.1109/CVPR.2016.566
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2011, P 1 INT ACM WORKSHOP, DOI DOI 10.1145/2072529.2072532
   Arifin S, 2008, IEEE T MULTIMEDIA, V10, P1325, DOI 10.1109/TMM.2008.2004911
   Barrett LF, 2006, PERSPECT PSYCHOL SCI, V1, P28, DOI 10.1111/j.1745-6916.2006.00003.x
   Baveye Y, 2015, IEEE T AFFECT COMPUT, V6, P43, DOI 10.1109/TAFFC.2015.2396531
   Benini S, 2011, IEEE T MULTIMEDIA, V13, P1356, DOI 10.1109/TMM.2011.2163058
   Borth D., 2013, P 21 ACM INT C MULTI, P223, DOI 10.1145/2502081.2502282
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Cardona-Rivera R., 2016, P 12 AAAI C ART INT, P1
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen S., 2017, P 7 ANN WORKSH AUD V, P19
   Chen T., 2014, DeepSentiBank: Visual Sentiment Concept Classification with Deep Convolutional Neural Networks
   Clore GL, 2009, COGN SYST RES, V10, P21, DOI 10.1016/j.cogsys.2008.03.002
   Damasio A., 1994, DESCARTESS ERROR EMO
   Ekman P., 1971, Nebraska symposium on motivation, V19, P207
   Ekman P., 1999, HDB COGNITION EMOTIO
   Fontaine JRJ, 2007, PSYCHOL SCI, V18, P1050, DOI 10.1111/j.1467-9280.2007.02024.x
   Gao JR, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P78, DOI 10.1145/3078971.3079030
   Guadagno RE, 2013, COMPUT HUM BEHAV, V29, P2312, DOI 10.1016/j.chb.2013.04.016
   Hu WM, 2016, IEEE T MULTIMEDIA, V18, P76, DOI 10.1109/TMM.2015.2496372
   Huang GS, 2007, 2007 7TH IEEE CONFERENCE ON NANOTECHNOLOGY, VOL 1-3, P7, DOI 10.1109/NANO.2007.4601129
   Huang J., 2017, P 7 ANN WORKSH AUD V, P11
   Ikizler-Cinbis N, 2012, IEEE T MULTIMEDIA, V14, P1031, DOI 10.1109/TMM.2012.2187180
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jiang YG, 2014, AAAI CONF ARTIF INTE, P73
   Joho H., 2009, Proceedings of the ACM international conference on image and video retrieval, page, P31
   Jou B, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P213, DOI 10.1145/2647868.2656408
   Kahou SE, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P543, DOI 10.1145/2522848.2531745
   King DB, 2015, ACS SYM SER, V1214, P1
   Krizhevsky A., 2012, ADV NEURAL INF PROCE, V25, P1097
   Lai JL, 2012, J VIS COMMUN IMAGE R, V23, P114, DOI 10.1016/j.jvcir.2011.08.005
   Li B., 2015, P 3 ANN C ADV COGN S, P57
   Li Z, 2005, IEEE T CIRC SYST VID, V15, P1245, DOI 10.1109/TCSVT.2005.854230
   Lin CH, 2017, PROC CVPR IEEE, P2252, DOI 10.1109/CVPR.2017.242
   Lindquist KA, 2013, PSYCHOL BULL, V139, P255, DOI 10.1037/a0029038
   Liu MY, 2014, PROC CVPR IEEE, P1749, DOI 10.1109/CVPR.2014.226
   Lövheim H, 2012, MED HYPOTHESES, V78, P341, DOI 10.1016/j.mehy.2011.11.016
   Lu XL, 2012, THEORIES AND TECHNOLOGIES FOR HIGH-SPEED TRANSPORTATION INFRASTRUCTURE, P83
   Ma Y.-F., 2002, ACM MULTIMEDIA, P533
   Machajdik J., 2010, P 18 ACM INT C MULT, P83, DOI DOI 10.1145/1873951.1873965
   Mao QR, 2014, IEEE T MULTIMEDIA, V16, P2203, DOI 10.1109/TMM.2014.2360798
   Nummenmaa L, 2014, P NATL ACAD SCI USA, V111, P646, DOI 10.1073/pnas.1321664111
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Peng KC, 2015, PROC CVPR IEEE, P860, DOI 10.1109/CVPR.2015.7298687
   Plutchik R, 1980, EMOTION THEORY RES E, P3
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Schuller B, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P401
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh KK, 2016, LECT NOTES COMPUT SC, V9910, P753, DOI 10.1007/978-3-319-46466-4_45
   Somandepalli K, 2018, IEEE T MULTIMEDIA, V20, P539, DOI 10.1109/TMM.2017.2745712
   Song Y, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P237, DOI 10.1145/2522848.2522851
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Wang F, 2012, IEEE T MULTIMEDIA, V14, P76, DOI 10.1109/TMM.2011.2165531
   Wang HL, 2006, IEEE T CIRC SYST VID, V16, P689, DOI 10.1109/TCSVT.2006.873781
   Wang M, 2012, IEEE T MULTIMEDIA, V14, P975, DOI 10.1109/TMM.2012.2185041
   Wang SF, 2015, IEEE T AFFECT COMPUT, V6, P410, DOI 10.1109/TAFFC.2015.2432791
   Wang X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P849, DOI 10.1145/2647868.2655013
   Xu BH, 2018, IEEE T AFFECT COMPUT, V9, P255, DOI 10.1109/TAFFC.2016.2622690
   Xu BH, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P15, DOI 10.1145/2911996.2912006
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu WR, 2017, IEEE T MULTIMEDIA, V19, P1494, DOI 10.1109/TMM.2017.2674622
   Yadati K, 2014, IEEE T MULTIMEDIA, V16, P15, DOI 10.1109/TMM.2013.2282128
   Yang S., 2021, WATER AIR SOIL POLL, DOI [DOI 10.1007/s11270-007-9372-6, DOI 10.1007/S11270-007-9372-6, DOI 10.1016/J.AMJMS.2021.03.001,00089-6]
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   You QZ, 2015, AAAI CONF ARTIF INTE, P381
   Zeng ZH, 2007, IEEE T MULTIMEDIA, V9, P424, DOI 10.1109/TMM.2006.886310
   Zhang SQ, 2018, IEEE T MULTIMEDIA, V20, P1576, DOI 10.1109/TMM.2017.2766843
   Zhao Sicheng, 2011, P 19 ACM INT C MULTI, P1473, DOI [10.1145/2072298.2072043, DOI 10.1145/2072298.2072043]
   Zhen QK, 2016, IEEE T MULTIMEDIA, V18, P1438, DOI 10.1109/TMM.2016.2557063
NR 70
TC 26
Z9 26
U1 0
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2020
VL 22
IS 1
BP 148
EP 159
DI 10.1109/TMM.2019.2922129
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA KB6AZ
UT WOS:000506577000014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xu, MZ
   Liu, B
   Fu, P
   Li, JB
   Hu, YH
AF Xu, Mingzhu
   Liu, Bing
   Fu, Ping
   Li, Junbao
   Hu, Yu Hen
TI Video Saliency Detection via Graph Clustering With Motion Energy and
   Spatiotemporal Objectness
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Spatiotemporal phenomena; Visualization; Dynamics; Proposals; Microsoft
   Windows; Feature extraction; Fuses; Video saliency; motion energy;
   spatiotemporal objectness; graph clustering
ID VISUAL-ATTENTION; OPTIMIZATION; CONTRAST
AB We present a novel, robust estimation method to distinguish salient objects from complicated, dynamic backgrounds in videos. In this method, we propose a novel approach to model motion energy based on motion magnitude, motion orientation, gradient flow field, and spatial gradient of the video frame. Furthermore, an effective spatiotemporal objectness map is also proposed to estimate a compact object-like region in the current video frame leveraging both the objectness proposals and the saliency map of the previous frame. Then the current video frame is oversegmented into the granularity of superpixels using the simple linear iterative clustering algorithm. Each superpixel is designated as a node of a graph. The similarity between adjacent superpixels will be assigned as the weight of an edge that connects these two nodes. The feature values of motion energy and spatiotemporal objectness within each superpixel will be averaged respectively, and used to graphically cluster similar superpixels to form the detected salient object. Extensive experiments comparing this proposed new method against twelve existing salient object detection (SOD) methods have been performed using the benchmark datasets unconstrained video saliency detection and densely annotated video segmentation. Superior performance of this proposed SOD method has been observed through three well-known performance metrics: precision-recall curves, F-measure curves, and the mean absolute error.
C1 [Xu, Mingzhu; Liu, Bing; Fu, Ping; Li, Junbao] Harbin Inst Technol, Dept Measurement & Control Engn, Sch Elect & Informat Engn, Harbin 150001, Heilongjiang, Peoples R China.
   [Hu, Yu Hen] Univ Wisconsin Madison, Dept Elect & Comp Engn, Madison, WI 53706 USA.
C3 Harbin Institute of Technology; University of Wisconsin System;
   University of Wisconsin Madison
RP Liu, B (corresponding author), Harbin Inst Technol, Dept Measurement & Control Engn, Sch Elect & Informat Engn, Harbin 150001, Heilongjiang, Peoples R China.
EM xumingzhu@hit.edu.cn; liubing66@hit.edu.cn; fuping@hit.edu.cn;
   lijunbao@hit.edu.cn; yhhu@wisc.edu
OI Liu, Bing/0000-0001-6831-5779; Xu, Mingzhu/0000-0002-1492-0970
FU National Natural Science Foundation of China (NSFC) [61671170]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant 61671170. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Vasileios Mezaris.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   [Anonymous], 2006, P CVPR
   Borji A., 2012, 2012 IEEE COMPUTER S, P23, DOI 10.1109/CVPRW.2012.6239191
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143
   Chen CLZ, 2018, IEEE T MULTIMEDIA, V20, P3324, DOI 10.1109/TMM.2018.2839523
   Chen CLZ, 2018, IEEE SIGNAL PROC LET, V25, P154, DOI 10.1109/LSP.2017.2775212
   Chen CZ, 2017, IEEE T IMAGE PROCESS, V26, P3156, DOI 10.1109/TIP.2017.2670143
   Chen WJ, 2016, ACM T ARCHIT CODE OP, V13, DOI 10.1145/2948976
   Chen YH, 2018, IEEE T IMAGE PROCESS, V27, P3345, DOI 10.1109/TIP.2018.2813165
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cong RM, 2019, IEEE T CIRC SYST VID, V29, P2941, DOI 10.1109/TCSVT.2018.2870832
   Cong RM, 2019, IEEE T CYBERNETICS, V49, P233, DOI 10.1109/TCYB.2017.2771488
   Cong RM, 2018, IEEE T IMAGE PROCESS, V27, P568, DOI 10.1109/TIP.2017.2763819
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Faktor Alon, 2014, BMVC
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fang YM, 2014, IEEE T IMAGE PROCESS, V23, P3910, DOI 10.1109/TIP.2014.2336549
   Fang YM, 2012, IEEE T IMAGE PROCESS, V21, P3888, DOI 10.1109/TIP.2012.2199126
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Guo F, 2018, IEEE T CYBERNETICS, V48, P3159, DOI 10.1109/TCYB.2017.2761361
   Huang RH, 2017, 2017 INTERNATIONAL CONFERENCE ON SOCIAL SCIENCES, ARTS AND HUMANITIES (SSAH 2017), P218
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Ji QG, 2013, SIGNAL PROCESS-IMAGE, V28, P241, DOI 10.1016/j.image.2012.11.008
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Kim W, 2014, IEEE T CIRC SYST VID, V24, P646, DOI 10.1109/TCSVT.2013.2290579
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Lee YJ, 2012, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2012.6247820
   Leordeanu M, 2012, LECT NOTES COMPUT SC, V7575, P516, DOI 10.1007/978-3-642-33765-9_37
   Li GB, 2016, IEEE T IMAGE PROCESS, V25, P5012, DOI 10.1109/TIP.2016.2602079
   Li J, 2018, IEEE T IMAGE PROCESS, V27, P349, DOI 10.1109/TIP.2017.2762594
   Li JH, 2015, SIGNAL PROCESS-IMAGE, V38, P100, DOI 10.1016/j.image.2015.04.014
   Li WT, 2013, IEEE T IMAGE PROCESS, V22, P2600, DOI 10.1109/TIP.2013.2253483
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu TM, 2002, 2002 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL IV, PROCEEDINGS, P149
   Liu Z, 2017, IEEE T CIRC SYST VID, V27, P2527, DOI 10.1109/TCSVT.2016.2595324
   Liu Z, 2014, IEEE T CIRC SYST VID, V24, P1522, DOI 10.1109/TCSVT.2014.2308642
   Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Ren JR, 2018, J VIS COMMUN IMAGE R, V50, P227, DOI 10.1016/j.jvcir.2017.12.002
   Ren ZX, 2014, IEEE T CIRC SYST VID, V24, P769, DOI 10.1109/TCSVT.2013.2280096
   Ren ZX, 2013, IEEE T IMAGE PROCESS, V22, P3120, DOI 10.1109/TIP.2013.2259837
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Wei LN, 2017, IEEE IMAGE PROC, P4197, DOI 10.1109/ICIP.2017.8297073
   Wenguan Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3395, DOI 10.1109/CVPR.2015.7298961
   Wu T, 2018, MULTIMED TOOLS APPL, V77, P19481, DOI 10.1007/s11042-017-5334-1
   Xie YF, 2019, MULTIMED TOOLS APPL, V78, P10353, DOI 10.1007/s11042-018-6614-0
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang B, 2017, NEUROCOMPUTING, V266, P165, DOI 10.1016/j.neucom.2017.05.032
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhong S.-h., 2013, AAAI Conference on Artificial Intelligence, P1063
   Zhou XF, 2018, IEEE T MULTIMEDIA, V20, P2993, DOI 10.1109/TMM.2018.2829605
   Zhou XF, 2018, J VIS COMMUN IMAGE R, V51, P131, DOI 10.1016/j.jvcir.2018.01.014
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 64
TC 28
Z9 29
U1 0
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2019
VL 21
IS 11
BP 2790
EP 2805
DI 10.1109/TMM.2019.2914889
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JJ7VX
UT WOS:000494363000008
DA 2024-07-18
ER

PT J
AU Yan, CG
   Li, L
   Zhang, CJ
   Liu, BT
   Zhang, YD
   Dai, QH
AF Yan, Chenggang
   Li, Liang
   Zhang, Chunjie
   Liu, Bingtao
   Zhang, Yongdong
   Dai, Qionghai
TI Cross-Modality Bridging and Knowledge Transferring for Image
   Understanding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object and scene recognition; image semantic search; cross-modality
   bridging; multi-task learning; knowledge transferring
ID CLASSIFICATION; RETRIEVAL; OBJECT
AB The understanding of web images has been a hot research topic in both artificial intelligence and multimedia content analysis domains. The web images are composed of various complex foregrounds and backgrounds, which makes the design of an accurate and robust learning algorithm a challenging task. To solve the above significant problem, first, we learn a cross-modality bridging dictionary for the deep and complete understanding of a vast quantity of web images. The proposed algorithm leverages the visual features into the semantic concept probability distribution, which can construct a global semantic description for images while preserving the local geometric structure. To discover and model the occurrence patterns between intra- and inter-categories, multi-task learning is introduced for formulating the objective formulation with Capped-l(1) penalty, which can obtain the optimal solution with a higher probability and outperform the traditional convex function-based methods. Second, we propose a knowledge-based concept transferring algorithm to discover the underlying relations of different categories. This distribution probability transferring among categories can bring the more robust global feature representation, and enable the image semantic representation to generalize better as the scenario becomes larger. Experimental comparisons and performance discussion with classical methods on the ImageNet, Caltech-256, SUN397, and Scene15 datasets show the effectiveness of our proposed method at three traditional image understanding tasks.
C1 [Yan, Chenggang; Liu, Bingtao] Hangzhou Dianzi Univ, Inst Informat & Control, Hangzhou 310005, Zhejiang, Peoples R China.
   [Li, Liang] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Li, Liang] Univ Chinese Acad Sci, Coll Comp & Control Engn, Beijing 100190, Peoples R China.
   [Zhang, Chunjie] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
   [Zhang, Yongdong] Chinese Acad Sci, Inst Comp Technol, Adv Comp Res Lab, Beijing 100190, Peoples R China.
   [Dai, Qionghai] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
C3 Hangzhou Dianzi University; Chinese Academy of Sciences; Institute of
   Computing Technology, CAS; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; Institute
   of Automation, CAS; Chinese Academy of Sciences; Institute of Computing
   Technology, CAS; Tsinghua University
RP Li, L (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM cgyan@hdu.edu.cn; liang.li@ict.ac.cn; chunjie.zhang@ia.ac.cn;
   liubingtao@hud.edu.cn; zhyd73@ustc.edu.cn; qhdai@tsinghua.edu.cn
RI Dai, Qionghai/ABD-5298-2021
OI Dai, Qionghai/0000-0001-7043-3061; zhang, chunjie/0000-0002-1161-8995;
   Li, Liang/0000-0002-1943-8219
FU National Basic Research Program of China (973-Program) [2015CB351802];
   National Natural Science Foundation of China [61771457, 61732007,
   61572488, 61472389, 61872362, U163621, 61671196, 61525206, 61672497];
   Key Research Program of Frontier Sciences, CAS [QYZDJ-SSW-SYS013];
   Zhejiang Province Nature Science Foundation of China [LR17F030006];
   National Key Research and Development Program of China [2017YFC0820600]
FX This work was supported by the National Basic Research Program of China
   (973-Program) under Grant 2015CB351802, in part by the National Natural
   Science Foundation of China under Grant 61771457, Grant 61732007, Grant
   61572488, Grant 61472389, Grant 61872362, Grant U163621, Grant 61671196,
   Grant 61525206, and Grant 61672497, in part by the Key Research Program
   of Frontier Sciences, CAS, under Grant QYZDJ-SSW-SYS013, in part by the
   Zhejiang Province Nature Science Foundation of China LR17F030006, and in
   part by the National Key Research and Development Program of China
   (2017YFC0820600).
CR [Anonymous], 1992, COLING 1992, DOI DOI 10.3115/992133.992154
   [Anonymous], 2017, P 26 INT JOINT C ART
   [Anonymous], INT J COMPUT VIS
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], CNSTR2007001 CAL I T
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], 2011, P 25 AAAI C ART INT
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bertsekas D. P., 1999, Nonlinear Program, V2nd
   Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P394, DOI 10.1109/TPAMI.2007.61
   Chadha A, 2017, IEEE T MULTIMEDIA, V19, P1596, DOI 10.1109/TMM.2017.2673415
   Chen K, 2017, EUR SPINE J, V26, P1893, DOI 10.1007/s00586-016-4826-4
   Dai W., 2008, NIPS, P353, DOI DOI 10.5555/2981780.2981825
   Dai W., 2007, Proceedings of the National Conference on Artificial Intelligence, V1
   Dai WY, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P210
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding GG, 2016, IEEE T IMAGE PROCESS, V25, P5427, DOI 10.1109/TIP.2016.2607421
   Gong PH, 2013, J MACH LEARN RES, V14, P2979
   Grippo L, 2000, OPER RES LETT, V26, P127, DOI 10.1016/S0167-6377(99)00074-7
   Gu QQ, 2009, IEEE DATA MINING, P159, DOI 10.1109/ICDM.2009.32
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jin ZW, 2017, IEEE T MULTIMEDIA, V19, P598, DOI 10.1109/TMM.2016.2617078
   Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121
   Karpathy A, 2017, IEEE T PATTERN ANAL, V39, P664, DOI 10.1109/TPAMI.2016.2598339
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li L, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1092, DOI 10.1145/3240508.3240649
   Li L, 2016, NEUROCOMPUTING, V174, P384, DOI 10.1016/j.neucom.2015.04.108
   Li L, 2015, J VIS COMMUN IMAGE R, V31, P231, DOI 10.1016/j.jvcir.2015.06.008
   Li L, 2013, IEEE MULTIMEDIA, V20, P13, DOI 10.1109/MMUL.2013.15
   Li L, 2012, IEEE T MULTIMEDIA, V14, P1401, DOI 10.1109/TMM.2012.2194993
   Li XH, 2014, PROCEEDINGS OF THE THIRD NORTHEAST ASIA INTERNATIONAL SYMPOSIUM ON LANGUAGE, LITERATURE AND TRANSLATION, VOLS 1 AND 2, P1
   Li YQ, 2015, AAAI CONF ARTIF INTE, P2750
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1234, DOI 10.1109/TMM.2016.2646180
   Nister David, 2006, CVPR
   Niu ZX, 2012, PROC CVPR IEEE, P2743, DOI 10.1109/CVPR.2012.6247997
   Pan YW, 2017, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2017.111
   Pan YW, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P717, DOI 10.1145/2600428.2609568
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Peng YX, 2016, IEEE T CIRC SYST VID, V26, P583, DOI 10.1109/TCSVT.2015.2400779
   Philbin J., 2008, PROC IEEE C COMPUT V, P1
   Qi GJ, 2011, PROC CVPR IEEE, P897, DOI 10.1109/CVPR.2011.5995312
   Raina R., 2007, P 24 INT C MACH LEAR, P759
   Rasiwasia N., 2008, PROC IEEE COMPUT VIS, P1
   Rasiwasia N, 2012, IEEE T PATTERN ANAL, V34, P902, DOI 10.1109/TPAMI.2011.175
   Rockafellar R. T., 2015, CONVEX ANAL, DOI DOI 10.1515/9781400873173
   Rui T, 2017, NEUROCOMPUTING, V230, P66, DOI 10.1016/j.neucom.2016.11.054
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Shao M, 2017, IEEE T NEUR NET LEAR, V28, P451, DOI 10.1109/TNNLS.2016.2517014
   Song XH, 2015, PROC CVPR IEEE, P1312, DOI 10.1109/CVPR.2015.7298736
   Sun LF, 2017, IEEE T MULTIMEDIA, V19, P609, DOI 10.1109/TMM.2016.2635589
   Tang L, 2009, MINES 2009: FIRST INTERNATIONAL CONFERENCE ON MULTIMEDIA INFORMATION NETWORKING AND SECURITY, VOL 1, PROCEEDINGS, P244, DOI 10.1109/MINES.2009.194
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang ZY, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1171, DOI 10.1145/2733373.2806309
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu J, 2017, IEEE T MULTIMEDIA, V19, P1156, DOI 10.1109/TMM.2017.2652065
   Wu W T, 2012, P 2012 ACM SIGMOD IN, P481, DOI DOI 10.1145/2213836.2213891
   Wu YC, 2016, IEEE T MULTIMEDIA, V18, P2135, DOI 10.1109/TMM.2016.2614220
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Yan CG, 2018, IEEE T MULTIMEDIA, V20, P3389, DOI 10.1109/TMM.2018.2838320
   Yang SJ, 2017, PROC CVPR IEEE, P7053, DOI 10.1109/CVPR.2017.746
   Ye Jieping, 2012, SIGKDD Explor, V14, P4
   Zhan MF, 2017, IEEE INT CON MULTI, P1398, DOI 10.1109/ICME.2017.8019310
   Zhang CJ, 2015, NEUROCOMPUTING, V156, P79, DOI 10.1016/j.neucom.2014.12.083
   Zhang CJ, 2014, NEUROCOMPUTING, V142, P248, DOI 10.1016/j.neucom.2014.03.059
   Zhang T, 2013, BERNOULLI, V19, P2277, DOI 10.3150/12-BEJ452
   Zhao SC, 2017, IEEE T MULTIMEDIA, V19, P632, DOI 10.1109/TMM.2016.2617741
   Zhou BL, 2014, ADV NEUR IN, V27
   Zhu XW, 2013, PROCEEDINGS OF THE 2013 INTERNATIONAL CONFERENCE ON THE MODERN DEVELOPMENT OF HUMANITIES AND SOCIAL SCIENCE, P387
NR 72
TC 102
Z9 106
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2019
VL 21
IS 10
BP 2675
EP 2685
DI 10.1109/TMM.2019.2903448
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JD1IM
UT WOS:000489728400020
DA 2024-07-18
ER

PT J
AU Beugnon, S
   Puech, W
   Pedeboy, JP
AF Beugnon, Sebastien
   Puech, William
   Pedeboy, Jean-Pierre
TI Format-Compliant Selective Secret 3-D Object Sharing Scheme
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D selective encryption; secret sharing; information sharing; 3D object;
   content protection
ID STEGANOGRAPHY; PROTECTION
AB This paper presents a new method of secret three-dimensional object sharing (S3DOS), which allows sharing of three-dimensional (3-D) objects, while preserving its file format by selectively encrypting a 3-D object in order to sufficiently protect the visual nature of the content. This scheme, named format-compliant selective (k, n) S3DOS, modifies selected bits of the vertices of a 3-D object to protect the visual content by inducing geometrical distortions. These distortions are controlled thanks to the application of a degradation level at the beginning of the sharing process. To reconstruct the secret 3-D object, at least k shared 3-D objects among n have to be combined in order to remove the geometrical distortions and recover the exact original 3-D object. Experimental results are presented and evaluated to showcase the feasibility of the proposed scheme.
C1 [Beugnon, Sebastien] Lab Informat Robot & Microelect Montpellier, F-34095 Montpellier, France.
   [Puech, William] Lab Informat Robot & Microelect Montpellier, ICAR Team Image & Interact, F-34095 Montpellier, France.
   [Pedeboy, Jean-Pierre] Strategies, F-94150 Rungis, France.
C3 Universite de Montpellier; Universite de Montpellier
RP Puech, W (corresponding author), Lab Informat Robot & Microelect Montpellier, ICAR Team Image & Interact, F-34095 Montpellier, France.
EM sebastien.beugnon@lirmm.fr; william.puech@lirmm.fr;
   jp.pedeboy@cadwin.com
OI Puech, William/0000-0001-9383-2401
CR Anbarasi J., 2015, INT ARAB J INF TECHN, V12, P708
   Aspert N, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P705, DOI 10.1109/ICME.2002.1035879
   Blakley G. R., 1979, 1979 International Workshop on Managing Requirements Knowledge (MARK), P313, DOI 10.1109/MARK.1979.8817296
   Bogomjakov A, 2008, COMPUT GRAPH FORUM, V27, P637, DOI 10.1111/j.1467-8659.2008.01161.x
   Çiftçi S, 2018, IEEE T MULTIMEDIA, V20, P68, DOI 10.1109/TMM.2017.2728479
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   del Rey AM, 2005, APPL MATH COMPUT, V170, P1356, DOI 10.1016/j.amc.2005.01.026
   Deutsch Peter., 1996, Zlib compressed data format specification version 3.3
   Dong L, 2015, IEEE T MULTIMEDIA, V17, P2174, DOI 10.1109/TMM.2015.2484221
   Elsheh E, 2011, EXPERT SYST APPL, V38, P13906, DOI 10.1016/j.eswa.2011.04.197
   Éluard M, 2014, IEEE IMAGE PROC, P4787, DOI 10.1109/ICIP.2014.7025970
   Gschwandtner M., 2009, PROTECTED PROGRESSIV
   HERRMANN LR, 1976, J ENG MECH DIV-ASCE, V102, P749
   HUFFMAN DA, 1952, P IRE, V40, P1098, DOI 10.1109/JRPROC.1952.273898
   Jeong SW, 2017, IEEE T MULTIMEDIA, V19, P2692, DOI 10.1109/TMM.2017.2710802
   Jiang RQ, 2018, IEEE T MULTIMEDIA, V20, P55, DOI 10.1109/TMM.2017.2723244
   Lavoué G, 2010, IEEE T MULTIMEDIA, V12, P636, DOI 10.1109/TMM.2010.2060475
   Lee SS, 2017, MULTIMED TOOLS APPL, V76, P243, DOI 10.1007/s11042-015-3032-4
   Lin CC, 2004, J SYST SOFTWARE, V73, P405, DOI 10.1016/S0164-1212(03)00239-5
   Lin CH, 2013, INT J INNOV COMPUT I, V9, P1321
   Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI 10.1145/37401.37422
   del Rey AM, 2015, EXPERT SYST APPL, V42, P2114, DOI 10.1016/j.eswa.2014.10.035
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   Rossignac J, 1999, IEEE T VIS COMPUT GR, V5, P47, DOI 10.1109/2945.764870
   Said A., 2005, IEEE INT C IM PROC 2, pII
   Shahid Z, 2014, IEEE T MULTIMEDIA, V16, P24, DOI 10.1109/TMM.2013.2281029
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   TAUBIN G, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P852, DOI 10.1109/ICCV.1995.466848
   Thien CC, 2002, COMPUT GRAPH-UK, V26, P766
   Thien CC, 2003, IEEE T CIRC SYST VID, V13, P1161, DOI 10.1109/TCSVT.2003.819176
   Tsai YY, 2016, 3D RES, V7, DOI 10.1007/s13319-015-0078-z
   Wang K, 2008, IEEE T MULTIMEDIA, V10, P1513, DOI 10.1109/TMM.2008.2007350
   Wang RZ, 2006, PATTERN RECOGN LETT, V27, P551, DOI 10.1016/j.patrec.2005.09.021
   Yang CC, 2004, APPL MATH COMPUT, V151, P483, DOI 10.1016/S0096-3003(03)00355-2
   Yang CN, 2007, J SYST SOFTWARE, V80, P1070, DOI 10.1016/j.jss.2006.11.022
   Zhao JJ, 2007, COMPUT STAND INTER, V29, P138, DOI 10.1016/j.csi.2006.02.004
   Zheng YF, 2017, IEEE T MULTIMEDIA, V19, P251, DOI 10.1109/TMM.2016.2612760
   Zuras D., 2008, 7542008 IEEE, V754-2008, P1, DOI [DOI 10.1109/IEEESTD.2008.4610935, DOI 10.1109/IEEESTD.2008.5976968]
NR 38
TC 8
Z9 8
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2019
VL 21
IS 9
BP 2171
EP 2183
DI 10.1109/TMM.2019.2900905
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IT6VX
UT WOS:000483015200002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Gao, GY
   Zhang, HZ
   Hu, H
   Wen, YG
   Cai, JF
   Luo, C
   Zeng, WJ
AF Gao, Guanyu
   Zhang, Huaizheng
   Hu, Han
   Wen, Yonggang
   Cai, Jianfei
   Luo, Chong
   Zeng, Wenjun
TI Optimizing Quality of Experience for Adaptive Bitrate Streaming via
   Viewer Interest Inference
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive video streaming; QoE; rate adaptation; viewer interest; video
   semantics
ID VISUAL-ATTENTION; VIDEO; DELIVERY; REGION
AB Rate adaptation is widely adopted in video streaming to improve the quality of experience (QoE). However, most of the existing rate adaptation approaches neglect the underlying video semantic information. In fact, influenced by video semantics and viewer preferences, the viewer may have different degrees of interest on different parts of a video. The interesting parts of a video can draw more visual attention from the viewer and have higher visual importance. As such, delivering the parts of a video that are interesting to the viewer in a higher quality can improve the perceptual video quality, compared with the semantics-agnostic approaches that treat each part of a video equally. Thus, it is natural to wonder: how to allocate bitrate budgets temporally over a video session under time-varying bandwidth while considering viewer interest ? As an exploratory study, we propose an interest-aware rate adaptation approach for improving QoE by inferring viewer interest based on video semantics. We adopt the deep learning method to recognize the scenes of video frames and leverage the term frequency-inverse document frequency method to analyze the degrees of an individual viewer's interest on different types of scenes. The bandwidth, buffer occupancy, and viewer interest are jointly considered under the model predictive control framework for selecting appropriate bitrates for maximizing QoE. The objective and subjective evaluations measured in a real environment show that our method can achieve a higher QoE compared with the semantics-agnostic approaches.
C1 [Gao, Guanyu; Zhang, Huaizheng; Hu, Han; Wen, Yonggang; Cai, Jianfei] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Luo, Chong; Zeng, Wenjun] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Nanyang Technological University; Microsoft Research Asia; Microsoft
RP Gao, GY (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
EM ggao001@ntu.edu.sg; huaizhen001@ntu.edu.sg; hhu@ntu.edu.sg;
   ygwen@ntu.edu.sg; asjfcai@ntu.edu.sg; chong.luo@microsoft.com;
   wezeng@microsoft.com
RI Gao, Guanyu/ACR-3456-2022; Wen, Yonggang/P-9406-2017; Wen,
   Yonggang/B-8848-2011; Zhang, Huaizheng/JUU-2766-2023; Cai,
   Jianfei/A-3691-2011
OI Wen, Yonggang/0000-0002-2751-5114; Zhang, Huaizheng/0000-0002-0153-6400;
   Hu, Han/0000-0001-7532-0496; Cai, Jianfei/0000-0002-9444-3763
CR Aizawa A, 2003, INFORM PROCESS MANAG, V39, P45, DOI 10.1016/S0306-4573(02)00021-3
   [Anonymous], MPEG DASH CONTENT GE
   [Anonymous], P IR SIGN SYST C
   [Anonymous], 2007, MODEL PREDICTIVE CON
   [Anonymous], 2017, CVX: Matlab software for disciplined convex programming
   [Anonymous], 2015, 9 INT WORKSH VID PRO
   [Anonymous], 2015, CISCO, White Paper
   [Anonymous], DEF QUAL EXP OUTP 5
   [Anonymous], P ACM MULT C
   [Anonymous], 2016, Tech. Rep.
   [Anonymous], 2016, MULT C
   [Anonymous], PLACES2 LARGE SCALE
   [Anonymous], P910042008 ITUT
   BenoisPineau J, 2017, MULTIMED SYST APPL, P1, DOI 10.1007/978-3-319-57687-9
   Bentaleb A, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1296, DOI 10.1145/2964284.2964332
   Bokani A, 2015, IEEE T MULTIMEDIA, V17, P2297, DOI 10.1109/TMM.2015.2494458
   Bradley AP, 2003, J VIS COMMUN IMAGE R, V14, P232, DOI 10.1016/S1047-3203(03)00037-3
   Cavallaro A, 2005, IEEE T CIRC SYST VID, V15, P1200, DOI 10.1109/TCSVT.2005.854240
   Ciubotaru B, 2009, IEEE T BROADCAST, V55, P202, DOI 10.1109/TBC.2009.2020448
   Gao GY, 2015, IEEE ICC, P6880, DOI 10.1109/ICC.2015.7249422
   Gulliver SR, 2004, IEEE T SYST MAN CY A, V34, P472, DOI 10.1109/TSMCA.2004.826309
   Gygli M, 2014, LECT NOTES COMPUT SC, V8695, P505, DOI 10.1007/978-3-319-10584-0_33
   Gygli M, 2013, IEEE I CONF COMP VIS, P1633, DOI 10.1109/ICCV.2013.205
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Joseph V, 2014, IEEE INFOCOM SER, P82, DOI 10.1109/INFOCOM.2014.6847927
   Li Y, 2009, IEEE T MULTIMEDIA, V11, P1182, DOI 10.1109/TMM.2009.2026102
   Li Z., 2014, Proceedings of the 5th ACM Multimedia Systems Conference, MMSys '14, P248
   Lin S, 2013, IEEE INT SYMP CIRC S, P2864, DOI 10.1109/ISCAS.2013.6572476
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Min XK, 2017, INFORM SCIENCES, V420, P417, DOI 10.1016/j.ins.2017.08.040
   Moorthy AK, 2012, IEEE J-STSP, V6, P652, DOI 10.1109/JSTSP.2012.2212417
   Nam H., 2016, IEEE INFOCOM 2016-The 35th Annual IEEE International Conference on Computer Communications, P1
   Qin Y., 2017, PROC IEEE C COMPUT C, P1
   Riiser Haakon, 2013, P 4 ACM MULT SYST C, P114, DOI DOI 10.1145/2483977.2483991
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Sun Y, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P272, DOI 10.1145/2934872.2934898
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Vetro A, 2001, IEEE T CIRC SYST VID, V11, P387, DOI 10.1109/76.911163
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Xiao MB, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P22, DOI 10.1145/2964284.2964313
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhou BL, 2014, ADV NEUR IN, V27
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
   Zhu XQ, 2010, IEEE T CIRC SYST VID, V20, P1462, DOI 10.1109/TCSVT.2010.2077492
   Zhu Y, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P801, DOI 10.1145/2964284.2964330
   Zou XK, 2015, 16TH INTERNATIONAL WORKSHOP ON MOBILE COMPUTING SYSTEMS AND APPLICATIONS (HOTMOBILE' 15), P57, DOI 10.1145/2699343.2699359
NR 52
TC 29
Z9 32
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2018
VL 20
IS 12
BP 3399
EP 3413
DI 10.1109/TMM.2018.2838330
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HA4EY
UT WOS:000450212600018
DA 2024-07-18
ER

PT J
AU Islam, MB
   Wong, LK
   Low, KL
   Wong, CO
AF Islam, Md Baharul
   Wong, Lai-Kuan
   Low, Kok-Lim
   Wong, Chee-Onn
TI Aesthetics-Driven Stereoscopic 3-D Image Recomposition With Depth
   Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image aesthetics; depth adaptation; stereo image recomposition; stereo
   image retargeting; tearable warping
ID PHOTO COMPOSITION; ENHANCEMENT
AB Due to the availability and affordability of the stereoscopic equipment (e.g., stereo camera, lens, and display devices), stereoscopic image manipulation has been receiving considerable research attention in recent years. In this paper, we present a semiautomatic, aesthetic-driven stereoscopic image recomposition approach, which capacitates the change of the spatial position of the foreground object(s) in a given stereoscopic image to enhance human visual aesthetics. Our algorithm recomposes both the left and right stereo images simultaneously using a global optimization algorithm. To maximize image aesthetics, our algorithm minimizes a set of aesthetic quality errors, which is derived from selected photographic composition rules. In addition, depth adaptation is applied to the resized objects and change in vertical disparity of the resulting stereo image pair is minimized to ensure a pleasant three-dimensional (3-D) viewing experience. Our method can be used to perform stereoscopic image retargeting and recomposition simultaneously by providing the target image scale as the input. Empirical evaluations demonstrate the effectiveness of our approach in enhancing the aesthetics of stereoscopic 3-D images. Notably, depth adaptation is shown to play an important role in aesthetics enhancement.
C1 [Islam, Md Baharul; Wong, Lai-Kuan] Multimedia Univ, Fac Comp & Informat, Cyberjaya 63100, Malaysia.
   [Low, Kok-Lim] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
   [Wong, Chee-Onn] Multimedia Univ, Fac Creat Multimedia, Cyberjaya 63100, Malaysia.
C3 Multimedia University; National University of Singapore; Multimedia
   University
RP Islam, MB (corresponding author), Multimedia Univ, Fac Comp & Informat, Cyberjaya 63100, Malaysia.
EM bislam.eng@gmail.com; lkwong@mmu.edu.my; lowkl@comp.nus.edu.sg;
   cowong@mmu.edu.my
RI Wong, Lai Kuan/AAO-7014-2021; Wong, Chee Onn/AGW-5722-2022; Islam, Md
   Baharul/R-3751-2019
OI Wong, Lai Kuan/0000-0002-4517-0391; Islam, Md
   Baharul/0000-0002-9928-5776; Wong, Chee Onn/0000-0002-9020-7405
FU Fundamental Research Grant Scheme Grant [EP20130326018]; Multimedia
   University [IP20131108001]
FX This work was supported in part by the Fundamental Research Grant Scheme
   Grant EP20130326018 and in part by the Multimedia University Internal
   Grant IP20131108001.
CR Abbeloos W., 2010, THESIS
   [Anonymous], 2012, P 30 COMP GRAPH INT
   [Anonymous], 2010, ACM T GRAPH P SIGGRA
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Basha T, 2011, IEEE I CONF COMP VIS, P1816, DOI 10.1109/ICCV.2011.6126448
   Bhattacharya S, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2037676.2037678
   Chang CH, 2011, IEEE T MULTIMEDIA, V13, P589, DOI 10.1109/TMM.2011.2116775
   Chang HT, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P957, DOI 10.1145/2647868.2654976
   Chang HT, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P927, DOI 10.1145/2733373.2806366
   Cheng MM, 2015, COMPUT GRAPH FORUM, V34, P193, DOI 10.1111/cgf.12758
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Fang C, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1105, DOI 10.1145/2647868.2654979
   Farid MS, 2014, MULTIMED TOOLS APPL, V71, P1699, DOI 10.1007/s11042-012-1303-x
   Farid MS, 2018, MULTIMED TOOLS APPL, V77, P20839, DOI 10.1007/s11042-017-5546-4
   Grant M., 2014, CVX MATLAB SOFTWARE
   Greco L, 2013, LECT NOTES COMPUT SC, V8157, P151, DOI 10.1007/978-3-642-41184-7_16
   Guo YW, 2012, COMPUT GRAPH FORUM, V31, P2193, DOI 10.1111/j.1467-8659.2012.03212.x
   Islam MB, 2017, MULTIMED TOOLS APPL, V76, P9517, DOI 10.1007/s11042-016-3561-5
   Islam MB, 2015, Proceedings 3rd IAPR Asian Conference on Pattern Recognition ACPR 2015, P645, DOI 10.1109/ACPR.2015.7486582
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jin Y, 2012, COMPUT GRAPH-UK, V36, P955, DOI 10.1016/j.cag.2012.07.007
   Jin Y, 2010, VISUAL COMPUT, V26, P769, DOI 10.1007/s00371-010-0472-8
   Kempf J, 2013, IEEE INT CONF CL NET, P1, DOI 10.1109/CloudNet.2013.6710551
   Kwok TH, 2010, IEEE T IMAGE PROCESS, V19, P3106, DOI 10.1109/TIP.2010.2052270
   Lee KY, 2012, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2012.6247657
   Li B, 2015, IEEE T IMAGE PROCESS, V24, P2811, DOI 10.1109/TIP.2015.2431441
   Li K, 2015, SIGNAL PROCESS-IMAGE, V39, P509, DOI 10.1016/j.image.2015.07.005
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Lin SS, 2014, IEEE T CIRC SYST VID, V24, P759, DOI 10.1109/TCSVT.2013.2291282
   Liu LG, 2010, COMPUT GRAPH FORUM, V29, P469, DOI 10.1111/j.1467-8659.2009.01616.x
   Lo WY, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866173
   Mendiburu Bernard, 2012, 3D movie making: stereoscopic digital cinema from script to screen
   Niu YZ, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366202
   Niu YZ, 2012, IEEE T MULTIMEDIA, V14, P783, DOI 10.1109/TMM.2012.2186122
   Park J, 2012, IEEE IMAGE PROC, P2741, DOI 10.1109/ICIP.2012.6467466
   Peng Y, 2015, IEEE T IMAGE PROCESS, V24, P1, DOI 10.1109/TIP.2014.2378060
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Samii A, 2015, COMPUT GRAPH FORUM, V34, P141, DOI 10.1111/cgf.12465
   Santella A., 2006, Conference on Human Factors in Computing Systems. CHI2006, P771
   Shaoyu Qi, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P457, DOI 10.1007/978-3-642-37447-0_35
   Strandmark P, 2010, PROC CVPR IEEE, P2085, DOI 10.1109/CVPR.2010.5539886
   Tan C.-H., 2015, PAC RIM S IM VID TEC, P257
   Tao MW, 2013, INT J COMPUT VISION, V103, P178, DOI 10.1007/s11263-012-0579-7
   Tasli HE, 2013, SIGNAL PROCESS-IMAGE, V28, P1374, DOI 10.1016/j.image.2013.08.003
   Tong RF, 2013, IEEE T VIS COMPUT GR, V19, P1375, DOI 10.1109/TVCG.2012.319
   Urvoy M, 2013, ANN TELECOMMUN, V68, P641, DOI 10.1007/s12243-013-0394-3
   Vineet Vibhav, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563095
   Wang M., 2016, COMPUT VIS MEDIA, V1, P3, DOI DOI 10.1007/s41095-016-0037-5
   Wang WC, 2016, IEEE T IMAGE PROCESS, V25, P4608, DOI 10.1109/TIP.2016.2593345
   Wong L.-K., 2012, PROC 20 ACM INT C MU, P845
   Yan JZ, 2013, PROC CVPR IEEE, P971, DOI 10.1109/CVPR.2013.130
   Yan T, 2013, INT J COMPUT VISION, V102, P293, DOI 10.1007/s11263-012-0593-9
   Yoo JW, 2013, IEEE SIGNAL PROC LET, V20, P519, DOI 10.1109/LSP.2013.2252165
   Zemene E, 2016, LECT NOTES COMPUT SC, V9912, P278, DOI 10.1007/978-3-319-46484-8_17
   Zhang FL, 2013, IEEE T MULTIMEDIA, V15, P1480, DOI 10.1109/TMM.2013.2268051
   Zhang MJ, 2005, 2005 IEEE International Conference on Multimedia and Expo (ICME), Vols 1 and 2, P438
NR 56
TC 13
Z9 15
U1 1
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 2964
EP 2979
DI 10.1109/TMM.2018.2820324
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800008
DA 2024-07-18
ER

PT J
AU Gao, F
   Zhang, XF
   Huang, YC
   Luo, Y
   Li, XM
   Duan, LY
AF Gao, Feng
   Zhang, Xinfeng
   Huang, Yicheng
   Luo, Yong
   Li, Xiaoming
   Duan, Ling-Yu
TI Data-Driven Lightweight Interest Point Selection for Large-Scale Visual
   Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visual search; interest point selection; compact descriptors;
   regression; classification; feature selection; MPEG-CDVS; MPEG-CDVA
ID DESCRIPTORS; GEOMETRY
AB With the explosive increase of images and videos, visual analysis has become an essential technique in dealing with the big visual data, which utilizes the visual feature descriptors to search or recognize the images or frames with target objects or events. Subject to the constraints of resources (e.g., memory, bandwidth, storage, etc.), interest point selection is crucial to generate robust compact descriptors for high-efficiency visual analysis by selecting and aggregating the most discriminative local feature descriptors, which has been demonstrated in the state-of-the-art low bit rate visual search works. In this paper, we propose a data-driven lightweight interest point selection approach to significantly improve the performance of visual search, while ameliorating the efficiency of extracting feature descriptors. Comprehensive experimental results over benchmarks have shown that the proposed interest point selection algorithm has significantly improved image matching and retrieval performance in the completed MPEG Compact Descriptors for Visual Search (CDVS) standard as well as the emerging MPEG Compact Descriptors for Video Analytics (CDVA) standard, say 20% mAP gain by data-driven selection against random selection of interest points. In particular, the presented data-driven interest point selection has been adopted by MPEG-CDVS and MPEG-CDVA as a normative technique to improve the aggregation of handcrafted features, which has contributed to the combination of handcrafted features and deep learning (CNN) features as well.
C1 [Gao, Feng; Huang, Yicheng; Duan, Ling-Yu] Peking Univ, Inst Digital Media, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
   [Zhang, Xinfeng] Univ Southern Calif, Ming Hsieh Dept Elect Engn, Los Angeles, CA 90007 USA.
   [Luo, Yong] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Li, Xiaoming] Peking Univ, Inst Network Comp & Informat Syst, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
C3 Peking University; University of Southern California; Nanyang
   Technological University; Peking University
RP Duan, LY (corresponding author), Peking Univ, Inst Digital Media, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
EM gaof@pku.edu.cn; xinfengz@usc.edu; anorange0409@pku.edu.cn;
   yluo180@gmail.com; lxm@pku.edu.cn; lingyu@pku.edu.cn
RI Huang, Yicheng/AFV-4905-2022; Zhang, Xinfeng/X-8148-2019
OI Huang, Yicheng/0000-0002-0293-6844; Luo, Yong/0000-0002-2296-6370;
   Zhang, Xinfeng/0000-0002-7517-3868
FU National Natural Science Foundation of China [61661146005, U1611461,
   61390515]; National Key Research and Development Program of China
   [2016YFB1001501]; PKU-NTU Joint Research Institute (JRI)
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61661146005, U1611461, and 61390515; in
   part by the National Key Research and Development Program of China under
   Grant 2016YFB1001501; and in part by the PKU-NTU Joint Research
   Institute (JRI) sponsored by a donation from the Ng Teng Fong Charitable
   Foundation. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Jian Zhang.
CR Alcantarilla PF, 2012, LECT NOTES COMPUT SC, V7577, P214, DOI 10.1007/978-3-642-33783-3_16
   [Anonymous], 2011, JTC1SC29WG11N12202 I
   [Anonymous], 2010, PROC ACM SIGMM INT C
   [Anonymous], ARXIV170408141
   [Anonymous], JTC1SC29WG11M23822 I
   [Anonymous], JTC1SC29WG11M30256 I
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Buoncompagni S, 2015, LECT NOTES COMPUT SC, V9281, P209, DOI 10.1007/978-3-319-23222-5_26
   Buoncompagni S, 2015, PATTERN RECOGN LETT, V62, P32, DOI 10.1016/j.patrec.2015.04.019
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Das Bhattacharjee S, 2016, IEEE T MULTIMEDIA, V18, P726, DOI 10.1109/TMM.2016.2532601
   Demirci MF, 2016, IEEE INT C SEMANT CO, P17, DOI 10.1109/ICSC.2016.46
   Dorkó G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P634
   Duan LY, 2018, IEEE T IMAGE PROCESS, V27, P2201, DOI 10.1109/TIP.2018.2794203
   Duan LY, 2016, IEEE T IMAGE PROCESS, V25, P179, DOI 10.1109/TIP.2015.2500034
   Duan LY, 2015, IEEE T MULTIMEDIA, V17, P828, DOI 10.1109/TMM.2015.2419973
   Dymczyk M, 2016, INT CONF 3D VISION, P572, DOI 10.1109/3DV.2016.66
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Girod B, 2011, IEEE SIGNAL PROC MAG, V28, P61, DOI 10.1109/MSP.2011.940881
   Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8
   Guyon Isabelle, 2003, J MACH LEARN RES, V3, P1157, DOI DOI 10.1162/153244303322753616
   Hartmann W, 2014, PROC CVPR IEEE, P9, DOI 10.1109/CVPR.2014.9
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jiang YG, 2015, IEEE T MULTIMEDIA, V17, P1174, DOI 10.1109/TMM.2015.2436813
   Kim HJ, 2015, IEEE I CONF COMP VIS, P1170, DOI 10.1109/ICCV.2015.139
   Knopp J, 2010, LECT NOTES COMPUT SC, V6311, P748, DOI 10.1007/978-3-642-15549-9_54
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Li S, 2015, IEEE INT CONF ROBOT, P6374, DOI 10.1109/ICRA.2015.7140094
   Lin J, 2017, IEEE T MULTIMEDIA, V19, P1968, DOI 10.1109/TMM.2017.2713410
   Lin J, 2014, IEEE SIGNAL PROC LET, V21, P195, DOI 10.1109/LSP.2013.2296532
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   Liu ZL, 2016, IEEE IMAGE PROC, P276, DOI 10.1109/ICIP.2016.7532362
   Lou YH, 2017, IEEE DATA COMPR CONF, P420, DOI 10.1109/DCC.2017.31
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo Y, 2016, IEEE T IMAGE PROCESS, V25, P414, DOI 10.1109/TIP.2015.2495116
   Luo Y, 2015, IEEE T KNOWL DATA EN, V27, P3111, DOI 10.1109/TKDE.2015.2445757
   Luo Y, 2015, IEEE T IMAGE PROCESS, V24, P2355, DOI 10.1109/TIP.2015.2421309
   Luo Y, 2013, IEEE T NEUR NET LEAR, V24, P709, DOI 10.1109/TNNLS.2013.2238682
   Mukherjee P., 2016, 2016 22 NATL C COMMU, P1
   Philbin J., 2008, P CVPR, P1
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Tolias G., 2016, INT C LEARN REPR SAN
   Tolias G, 2014, PATTERN RECOGN, V47, P3466, DOI 10.1016/j.patcog.2014.04.007
   Turcot Panu, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P2109, DOI 10.1109/ICCVW.2009.5457541
   Yu T, 2017, AAAI CONF ARTIF INTE, P4320
   Yu T, 2017, PROC CVPR IEEE, P3195, DOI 10.1109/CVPR.2017.340
   Zhang XF, 2018, IEEE T CIRC SYST VID, V28, P3387, DOI 10.1109/TCSVT.2017.2748382
   Zhou W, 2016, INT C INTEL HUM MACH, P85, DOI 10.1109/IHMSC.2016.212
NR 52
TC 6
Z9 6
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2018
VL 20
IS 10
SI SI
BP 2774
EP 2787
DI 10.1109/TMM.2018.2818012
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GT9YH
UT WOS:000444903000019
DA 2024-07-18
ER

PT J
AU Tang, YB
   Wu, XQ
AF Tang, Youbao
   Wu, Xiangqian
TI Scene Text Detection Using Superpixel-Based Stroke Feature Transform and
   Deep Learning Based Region Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Scene text detection; superpixel based stroke feature transform; deep
   learning; convolutional neural network; fully connected network; region
   classification
ID READING TEXT; IMAGES; LOCALIZATION; COMPETITION
AB Scene text detection is a crucial step in end-to-end scene text recognition, a greatly challenging problem in computer vision. This paper proposes a novel scene text detection method that involves superpixel-based stroke feature transform (SSFT) and deep learning based region classification (DLRC). The SSFT is developed for candidate character region (CCR) extraction, which consists in partitioning an input image into several regions via superpixel-based clustering, removing mast regions based on predefined criteria satisfied by the characters, and refining the remaining regions to obtain CCRs by computing a stroke width map. The character regions are identified from the CCRs using DLRC, in which several hand-crafted low-level features, i.e., color, texture, and geometric features, and some deep convolution neural network (CNN) based high-level features are first extracted from the regions, and then these features are fused by using two fully connected networks (FCNs) for region classification. In the DLRC step, the deep feature extraction CNN and the feature fusion FCNs are jointly trained. Next, the extracted character regions are merged to form candidate text regions, from which the final scene texts are detected. The proposed method is evaluated on three publicly available datasets: ICDAR2011, ICDAR2013, and street view text. It achieves F-measures of 0.876, 0.885, and 0.631, respectively, which demonstrate the effectiveness of the proposed scene text detection method.
C1 [Tang, Youbao; Wu, Xiangqian] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
C3 Harbin Institute of Technology
RP Wu, XQ (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
EM tangyoubao@hit.edu.cn; xqwu@hit.edu.cn
RI Wu, Xiangqian/HKE-1904-2023; Tang, Youbao/L-7328-2019
OI Tang, Youbao/0000-0001-8719-3375
FU Natural Science Foundation of China [61472102, 61672194]; ShanDong
   Provincial Natural Science Foundation, China [ZR2016FM04]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 61472102 and 61672194 and in part by ShanDong
   Provincial Natural Science Foundation, China (ZR2016FM04).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2015, ARXIV150904232
   Bissacco A, 2013, IEEE I CONF COMP VIS, P785, DOI 10.1109/ICCV.2013.102
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Chen XR, 2004, PROC CVPR IEEE, P366
   Cho H, 2016, PROC CVPR IEEE, P3566, DOI 10.1109/CVPR.2016.388
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dollár P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715
   Duan LY, 2014, IEEE T MULTIMEDIA, V16, P346, DOI 10.1109/TMM.2013.2293063
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Fraz M, 2015, INT J DOC ANAL RECOG, V18, P153, DOI 10.1007/s10032-015-0239-x
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   Hastie T., 2009, The elements of statistical learning: data mining, inference, and prediction, V2
   He T, 2016, IEEE T IMAGE PROCESS, V25, P2529, DOI 10.1109/TIP.2016.2547588
   Huang WL, 2013, IEEE I CONF COMP VIS, P1241, DOI 10.1109/ICCV.2013.157
   Huang WL, 2014, LECT NOTES COMPUT SC, V8692, P497, DOI 10.1007/978-3-319-10593-2_33
   Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z
   Jaderberg M, 2014, LECT NOTES COMPUT SC, V8692, P512, DOI 10.1007/978-3-319-10593-2_34
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Kim J, 2016, IEEE T IMAGE PROCESS, V25, P9, DOI 10.1109/TIP.2015.2495122
   Kim KI, 2003, IEEE T PATTERN ANAL, V25, P1631, DOI 10.1109/TPAMI.2003.1251157
   Koo HI, 2013, IEEE T IMAGE PROCESS, V22, P2296, DOI 10.1109/TIP.2013.2249082
   Li Y, 2014, IEEE T IMAGE PROCESS, V23, P1666, DOI 10.1109/TIP.2014.2302896
   Liu XQ, 2012, IEEE T MULTIMEDIA, V14, P482, DOI 10.1109/TMM.2011.2177646
   Lu SJ, 2015, INT J DOC ANAL RECOG, V18, P125, DOI 10.1007/s10032-015-0237-z
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Mishra A., 2012, CVPR
   Neumann L, 2016, IEEE T PATTERN ANAL, V38, P1872, DOI 10.1109/TPAMI.2015.2496234
   Neumann L, 2015, PROC INT CONF DOC, P746, DOI 10.1109/ICDAR.2015.7333861
   Neumann L, 2013, IEEE I CONF COMP VIS, P97, DOI 10.1109/ICCV.2013.19
   Neumann L, 2012, PROC CVPR IEEE, P3538, DOI 10.1109/CVPR.2012.6248097
   Neumann L, 2011, LECT NOTES COMPUT SC, V6494, P770, DOI 10.1007/978-3-642-19318-7_60
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pan YF, 2011, IEEE T IMAGE PROCESS, V20, P800, DOI 10.1109/TIP.2010.2070803
   Qin Siyang., 2016, Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on, P1
   Ren XH, 2017, IEEE T MULTIMEDIA, V19, P506, DOI 10.1109/TMM.2016.2625259
   Shahab A, 2011, PROC INT CONF DOC, P1491, DOI 10.1109/ICDAR.2011.296
   Shi CZ, 2013, PATTERN RECOGN LETT, V34, P107, DOI 10.1016/j.patrec.2012.09.019
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun L, 2014, INT C PATT RECOG, P2715, DOI 10.1109/ICPR.2014.469
   Tan WM, 2016, IEEE T MULTIMEDIA, V18, P128, DOI 10.1109/TMM.2015.2500727
   Tang Y, 2017, IEEE T IMAGE PROCESS, V26, P994, DOI [10.1109/TIP.2016.2639440, 10.1109/TIP.2017.2656474]
   Tang YB, 2016, INT CONF FRONT HAND, P156, DOI [10.1109/ICFHR.2016.37, 10.1109/ICFHR.2016.0040]
   Tian SX, 2015, IEEE I CONF COMP VIS, P4651, DOI 10.1109/ICCV.2015.528
   Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang K, 2010, LECT NOTES COMPUT SC, V6311, P591, DOI 10.1007/978-3-642-15549-9_43
   Wu L, 2015, IEEE T MULTIMEDIA, V17, P1137, DOI 10.1109/TMM.2015.2443556
   Xu CS, 2008, IEEE T MULTIMEDIA, V10, P1342, DOI 10.1109/TMM.2008.2004912
   Xu H., 2014, 12 ASIAN C COMPUTER, P195
   Yao C, 2014, IEEE T IMAGE PROCESS, V23, P4737, DOI 10.1109/TIP.2014.2353813
   Yao C, 2014, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2014.515
   Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787
   Ye QX, 2015, IEEE T PATTERN ANAL, V37, P1480, DOI 10.1109/TPAMI.2014.2366765
   Yi Chucai, 2011, IEEE Trans Image Process, V20, P2594, DOI 10.1109/TIP.2011.2126586
   Yi CC, 2011, PROC INT CONF DOC, P177, DOI 10.1109/ICDAR.2011.44
   Yi CC, 2013, COMPUT VIS IMAGE UND, V117, P182, DOI 10.1016/j.cviu.2012.11.002
   Yin XC, 2015, IEEE T PATTERN ANAL, V37, P1930, DOI 10.1109/TPAMI.2014.2388210
   Yin XC, 2014, IEEE T PATTERN ANAL, V36, P970, DOI 10.1109/TPAMI.2013.182
   Zamberletti A, 2015, LECT NOTES COMPUT SC, V9009, P91, DOI 10.1007/978-3-319-16631-5_7
   Zhang Q, 2014, PROC CVPR IEEE, P2830, DOI 10.1109/CVPR.2014.362
   Zhang Z, 2015, PROC CVPR IEEE, P2558, DOI 10.1109/CVPR.2015.7298871
   Zhu SY, 2016, PROC CVPR IEEE, P625, DOI 10.1109/CVPR.2016.74
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 66
TC 39
Z9 42
U1 1
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2018
VL 20
IS 9
BP 2276
EP 2288
DI 10.1109/TMM.2018.2802644
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GR1ZX
UT WOS:000442358200004
DA 2024-07-18
ER

PT J
AU Fujihashi, T
   Saruwatari, S
   Watanabe, T
AF Fujihashi, Takuya
   Saruwatari, Shunsuke
   Watanabe, Takashi
TI Multiview Video Transmission Over Underwater Acoustic Path
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-view video; underwater acoustic path
ID SENSOR NETWORKS; DELAY
AB Existing studies on multiview video streaming are proposed for terrestrial environments. They can be classified into request-reply and all camera transmission models, while both models suffer from a long switching delay and low video quality in an underwater acoustic path. The implementation of multiview video transmission in an underwater acoustic path requires reduction of the switching delay and the maintenance of high video quality. To this end, we propose a video transmission scheme, namely, Dolphin. Dolphin comprises time-shifted slot assignment, stochastic transmission, correlation-based compensation, and greedy transmission. Dolphin assigns asymmetric and time-shifted transmission slots to an encoder node and a user node according to a propagation delay. Stochastic transmission sends a potential camera's video that will be the most likely to be played back by the user before playback. If the stochastic transmission fails, the video frames of the user's desired camera are encoded with the mispredicted video frames to prevent switching delay from increasing. Moreover, greedy transmission sends all potential video frames, which are potentially displayed by a user, and determines rate allocation of the video frames based on the display probability. Evaluations using Mitsubishi Electric Research Laboratories' benchmark test sequences demonstrated that Dolphin achieved a short switching delay with a slight degradation in video quality irrespective of communication distance and acoustic path quality. For example, Dolphin decreases the switching delay by 92.7% compared with an existing request-reply model with 0.4-dB quality degradation at a communication distance of 300 m when a user tends to gaze at one camera.
C1 [Fujihashi, Takuya] Ehime Univ, Grad Sch Sci & Engn, Matsuyama, Ehime 7908577, Japan.
   [Saruwatari, Shunsuke; Watanabe, Takashi] Osaka Univ, Grad Sch Informat Sci & Technol, Suita, Osaka 5650871, Japan.
C3 Ehime University; Osaka University
RP Fujihashi, T (corresponding author), Ehime Univ, Grad Sch Sci & Engn, Matsuyama, Ehime 7908577, Japan.
EM fujihashi@cs.ehime-u.ac.jp; saru@ist.osaka-u.ac.jp;
   watanabe@ist.osaka-u.ac.jp
RI Fujihashi, Takuya/Y-1527-2019
OI Fujihashi, Takuya/0000-0002-6960-0122; WATANABE,
   TAKASHI/0000-0002-3227-9048
FU JSPS KAKENHI [JP17K12672, JP16H01718, JP15K12018]; Grants-in-Aid for
   Scientific Research [16H01718] Funding Source: KAKEN
FX This work was supported by JSPS KAKENHI under Grants JP17K12672,
   JP16H01718, and JP15K12018. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Xiaoqing
   Zhu.
CR Alface PR, 2012, BELL LABS TECH J, V16, P135, DOI 10.1002/bltj.20538
   [Anonymous], 6 ACM INT WORKSH UND
   [Anonymous], 2014, OCEANS 2014 TAIPEI, DOI DOI 10.1109/OCEANS-TAIPEI.2014.6964355
   [Anonymous], 2010, OCEANS, DOI DOI 10.1109/OCEANS.2010.5663839
   [Anonymous], 2008, 14496102008 ISOIEC
   [Anonymous], 2005, JTC1SC29WG11 ISOIEC
   Chen KY, 2014, IEEE COMMUN SURV TUT, V16, P1433, DOI 10.1109/SURV.2014.013014.00032
   Cho C, 2004, 6TH INTERNATIONAL CONFERENCE ON ADVANCED COMMUNICATION TECHNOLOGY, VOLS 1 AND 2, PROCEEDINGS, P971
   De Abreu A, 2015, IEEE J-STSP, V9, P487, DOI 10.1109/JSTSP.2015.2407320
   Diab  K, 2017, P 36 ANN IEEE INT C, P1036
   Fan G., 2013, P 8 ACM INT C UND NE, P1
   Fujihashi T, 2014, IEEE T MULTIMEDIA, V16, P228, DOI 10.1109/TMM.2013.2281588
   Hoeberechts M., 2015, OCEANS 2015-MTS/ IEEE Washington, P1, DOI [10.23919/oceans.2015.7404592, DOI 10.23919/OCEANS.2015.7404592, 10.23919/OCEANS.2015.7404592]
   Huang H, 2012, IEEE INFOCOM SER, P2791, DOI 10.1109/INFCOM.2012.6195701
   Ikeya K., 2016, ITE Trans. Media Technol. Appl., V4, P349
   Jennehag U., 2011, 2011 IEEE Consumer Communications and Networking Conference (CCNC 2011), P766, DOI 10.1109/CCNC.2011.5766595
   Joint Video Team Of ITU-T VCEG And ISO/IEC MPEG, 2008, JMVC JOINT MULT VID
   M. Corporation, 2003, XP002392343 INT ENG
   Magadza T. B. H. T., 2015, 2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions), P1, DOI 10.1109/ICRITO.2015.7359236
   Manzato DAG, 2013, IEEE COMMUN MAG, V51, P120, DOI 10.1109/MCOM.2013.6576349
   Maugey T, 2013, IEEE T MULTIMEDIA, V15, P1070, DOI 10.1109/TMM.2013.2246147
   Nath Siddhartha, 2014, 2014 DES AUT TEST EU, P1
   Ng HH, 2013, IEEE J OCEANIC ENG, V38, P547, DOI 10.1109/JOE.2012.2227553
   Niu T, 2016, ONCOTARGET, V7, P50017, DOI 10.18632/oncotarget.10312
   Noh Y., 2010, IEEE T MOBILE COMPUT, V12, P1676
   Oda S, 2015, C LOCAL COMPUT NETW, P390, DOI 10.1109/LCN.2015.7366335
   Osada J., 2012, P AS PAC C COMM, P1
   Pan  Z., 2011, P IEEE ICC, P1
   Pompili D, 2010, IEEE T WIREL COMMUN, V9, P2924, DOI 10.1109/TWC.2010.062910.100137
   Ramos FMV, 2011, SIGNAL PROCESS-IMAGE, V26, P400, DOI 10.1016/j.image.2011.03.005
   Ren DN, 2015, IEEE T MULTIMEDIA, V17, P307, DOI 10.1109/TMM.2015.2389714
   Sannomiya H., 2014, P IEEE INT WORKSH TE, P1
   Shiau YH, 2012, INT J ADV COMPUT SC, V3, P45
   Stojanovic M, 2007, P 1 WORKSH UND NETW, P41, DOI [10.1145/1161039.1161049, DOI 10.1145/1161039.1161049, 10.1145/1347364.1347373]
   Suenaga R., 2015, P SPIE 3 DIM IM PROC
   Sullivan G., 2002, JVTB063 ISOIEC MPEG
   Syed A.A., 2006, P 25 IEEE INT C COMP, P1, DOI [DOI 10.1109/INFOCOM.2006.161, 10.1109/INFOCOM.2006. 161, 10.1109/INFOCOM.2006.161]
   Tanaka S, 2017, 2017 31ST INTERNATIONAL CONFERENCE ON INFORMATION NETWORKING (ICOIN), P500, DOI 10.1109/ICOIN.2017.7899544
   Tanimoto M, 2006, SIGNAL PROCESS-IMAGE, V21, P454, DOI 10.1016/j.image.2006.03.009
   Xiu XY, 2012, IEEE T MULTIMEDIA, V14, P1109, DOI 10.1109/TMM.2012.2191267
   Xu X., 2016, 2016 First International Workshop on Sensing, Processing and Learning for Intelligent Machines (SPLINE), P1
   Yamauchi Y, 1999, 1999 IEEE INTERNATIONAL CONFERENCE ON PERSONAL WIRELESS COMMUNICATIONS, P24, DOI 10.1109/ICPWC.1999.759578
   Yu L, 2019, INT REV SOCIOL SPORT, V54, P711, DOI 10.1177/1012690217740114
NR 43
TC 5
Z9 5
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2018
VL 20
IS 8
BP 2166
EP 2181
DI 10.1109/TMM.2018.2791800
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GN8BR
UT WOS:000439378600020
DA 2024-07-18
ER

PT J
AU Zhang, FL
   Wu, X
   Li, RL
   Wang, J
   Zheng, ZH
   Hu, SM
AF Zhang, Fang-Lue
   Wu, Xian
   Li, Rui-Long
   Wang, Jue
   Zheng, Zhao-Heng
   Hu, Shi-Min
TI Detecting and Removing Visual Distractors for Video Aesthetic
   Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video distractor; machine learning; visual quality
ID OBJECT SEGMENTATION; EVENT DETECTION; QUALITY; COLOR; MODEL
AB Personal videos often contain visual distractors, which are objects that are accidentally captured and can distract viewers from focusing on the main subjects. We propose a method to automatically detect and localize these distractors through learning from a manually labeled dataset. To achieve spatially and temporally coherent detection, we propose extracting features at the temporal-superpixel level using a traditional supporting vector machine based learning framework. We also experiment with end-to-end learning using convolutional neural networks, which achieves slightly higher performance than other methods. The classification result is further refined in a post-processing step based on graph-cut optimization. Experimental results show that our method achieves an accuracy of 81% and a recall of 86%. We demonstrate several ways of removing the detected distractors to improve the video quality, including video hole filling, video frame replacement, and camera path replanning. The user study results show that our method can significantly improve the aesthetic quality of videos.
C1 [Zhang, Fang-Lue] Victoria Univ Wellington, Sch Engn & Comp Sci, Wellington 6012, New Zealand.
   [Wu, Xian; Li, Rui-Long; Zheng, Zhao-Heng; Hu, Shi-Min] Tsinghua Univ, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
   [Wang, Jue] Megvii Face Res US, Redmond, WA 98033 USA.
   [Hu, Shi-Min] Cardiff Univ, Cardiff CF10 3AT, S Glam, Wales.
C3 Victoria University Wellington; Tsinghua University; Cardiff University
RP Hu, SM (corresponding author), Tsinghua Univ, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
EM zianglue@gmail.com; xw59uht@163.com; li-rl12@mail.tisinghua.edu.cn;
   arphid@gmail.com; zhengzh13.tsinghua@gmail.com; shimin@tsinghua.edu.cn
RI Wang, Jue/GVU-0480-2022; Hu, Shi-Min/AAW-1952-2020
OI Wang, Jue/0000-0002-3641-3136; Li, Ruilong/0000-0003-0511-8019; Wu,
   Xian/0000-0003-4003-6975; Hu, Shi-Min/0000-0001-7507-6542
FU Research Establishment Grant of Victoria University of Wellington
   [8-1620-216786-3744]; Natural Science Foundation of China [61521002];
   Research Grant of Beijing Higher Institution Engineering Research Center
FX This work was supported in part by the Research Establishment Grant of
   Victoria University of Wellington (Project 8-1620-216786-3744), in part
   by the Natural Science Foundation of China (Project 61521002), and in
   part by the Research Grant of Beijing Higher Institution Engineering
   Research Center. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Chang-Su Kim.
CR Adam A, 2008, IEEE T PATTERN ANAL, V30, P555, DOI 10.1109/TPAMI.2007.70825
   Anegekuh L, 2015, IEEE T MULTIMEDIA, V17, P1323, DOI 10.1109/TMM.2015.2444098
   [Anonymous], 2008, PROC INT CONF PATTER
   [Anonymous], 2010, P 18 ACM INT C MULT
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bai X, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531376
   BERTHOUZOZ F, 2012, ACM T GRAPHIC, V31
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chang J, 2013, PROC CVPR IEEE, P2051, DOI 10.1109/CVPR.2013.267
   Choi JH, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1003, DOI 10.1145/2733373.2806386
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Delong A, 2012, INT J COMPUT VISION, V96, P1, DOI 10.1007/s11263-011-0437-z
   Dong YY, 2016, IEEE T MULTIMEDIA, V18, P549, DOI 10.1109/TMM.2016.2522639
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fang YM, 2014, IEEE T CIRC SYST VID, V24, P27, DOI 10.1109/TCSVT.2013.2273613
   Fragkiadaki K, 2015, PROC CVPR IEEE, P4083, DOI 10.1109/CVPR.2015.7299035
   Fried O, 2015, PROC CVPR IEEE, P1703, DOI 10.1109/CVPR.2015.7298779
   Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872
   Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676
   Gleicher ML, 2008, ACM T MULTIM COMPUT, V5, DOI 10.1145/1404880.1404882
   Hameed A, 2016, IEEE T MULTIMEDIA, V18, P764, DOI 10.1109/TMM.2016.2525862
   Jain A, 2013, PROC CVPR IEEE, P2571, DOI 10.1109/CVPR.2013.332
   Jang S, 2016, IEEE T MULTIMEDIA, V18, P1808, DOI 10.1109/TMM.2016.2581582
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kim H, 2015, IEEE T MULTIMEDIA, V17, P2198, DOI 10.1109/TMM.2015.2493367
   Kim H, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2425544
   Kim J, 2014, PROC CVPR IEEE, P883, DOI 10.1109/CVPR.2014.118
   Li C, 2010, P ACM INT C MULT, P827, DOI DOI 10.1145/1873951.1874089
   Li CC, 2009, IEEE J-STSP, V3, P236, DOI 10.1109/JSTSP.2009.2015077
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338
   Lu SP, 2015, IEEE T MULTIMEDIA, V17, P577, DOI 10.1109/TMM.2015.2412879
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Matsushita Y, 2006, IEEE T PATTERN ANAL, V28, P1150, DOI 10.1109/TPAMI.2006.141
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Mital PK, 2011, COGN COMPUT, V3, P5, DOI 10.1007/s12559-010-9074-z
   Newson A, 2014, SIAM J IMAGING SCI, V7, P1993, DOI 10.1137/140954933
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Ochs P, 2014, IEEE T PATTERN ANAL, V36, P1187, DOI 10.1109/TPAMI.2013.242
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Rudoy D, 2013, PROC CVPR IEEE, P1147, DOI 10.1109/CVPR.2013.152
   Scott MJ, 2016, IEEE T MULTIMEDIA, V18, P1796, DOI 10.1109/TMM.2016.2574623
   Sun LF, 2017, IEEE T MULTIMEDIA, V19, P609, DOI 10.1109/TMM.2016.2635589
   Tang K, 2013, PROC CVPR IEEE, P2483, DOI 10.1109/CVPR.2013.321
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wu O., 2011, Proc. of WSDM, P337
   Wu O, 2016, IEEE T MULTIMEDIA, V18, P1062, DOI 10.1109/TMM.2016.2538722
   Wu ZF, 2015, IEEE T MULTIMEDIA, V17, P1960, DOI 10.1109/TMM.2015.2477681
   Xu SB, 2015, IEEE T IMAGE PROCESS, V24, P2182, DOI 10.1109/TIP.2015.2416654
   Yu JG, 2016, IEEE T MULTIMEDIA, V18, P273, DOI 10.1109/TMM.2015.2505908
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhang D, 2005, PROC CVPR IEEE, P611
   Zhang DW, 2017, IEEE T PATTERN ANAL, V39, P865, DOI 10.1109/TPAMI.2016.2567393
   Zhang D, 2013, PROC CVPR IEEE, P628, DOI 10.1109/CVPR.2013.87
   Zhang FL, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980243
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
NR 61
TC 15
Z9 15
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2018
VL 20
IS 8
BP 1987
EP 1999
DI 10.1109/TMM.2018.2790163
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GN8BR
UT WOS:000439378600006
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Zhou, YP
   Gu, XH
   Wu, D
   Chen, M
   Chan, TH
   Ho, SW
AF Zhou, Yipeng
   Gu, Xuhong
   Wu, Di
   Chen, Min
   Chan, Terence H.
   Ho, Siu Wai
TI Statistical Study of View Preferences for Online Videos With
   Cross-Platform Information
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE User view preference; view count; movie rating; comments
ID COST-EFFICIENT
AB The knowledge of view preferences of users is crucial for online video providers to improve their system operations and video recommendations. However, it is challenging to accurately acquire this knowledge by merely relying on a single online video system. In this paper, we conduct a joint statistical study using the cross-platform information obtained from Douban, the largest online video database with video rating functionality in China, and Youku, one of the largest online video streaming systems in China. The Douban dataset includes feedbacks (e.g., movie ratings, comments, and reviews) from all users of different online video systems, and movie metadata (e.g., release date, actors, and directors), based on which we can statistically explore effective and significant factors attributing to video view counts. Meanwhile, our study unveils user behaviors that are latent when only observing a single video system. Finally, a multiple correlation analysis reveals that factors extracted from Douban can significantly increase our ability to predict video view counts. Our study can benefit video caching, video procurement, and advertisement campaign for online video providers.
C1 [Zhou, Yipeng; Gu, Xuhong; Wu, Di] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Guangdong, Peoples R China.
   [Zhou, Yipeng; Chan, Terence H.; Ho, Siu Wai] Univ South Australia, Inst Telecommun Res, Adelaide, SA 5095, Australia.
   [Gu, Xuhong; Wu, Di] Guangdong Prov Key Lab Big Data Anal & Proc, Guangzhou 510006, Guangdong, Peoples R China.
   [Chen, Min] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
   [Chen, Min] Wuhan Natl Lab Optoelect, Wuhan 430074, Peoples R China.
C3 Sun Yat Sen University; University of South Australia; Huazhong
   University of Science & Technology; Huazhong University of Science &
   Technology
RP Wu, D (corresponding author), Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Guangdong, Peoples R China.
EM yipeng.job@gmail.com; 1965963961@qq.com; wudi27@mail.sysu.edu.cn;
   minchen@ieee.org; hlchan6@gmail.com; siuwai.ho@unisa.edu.au
RI Chen, Min/N-9350-2015; wu, di/IYS-9217-2023; Wu, Di/HNP-3772-2023; Chan,
   Terence/C-8787-2013
OI Chen, Min/0000-0002-0960-4447; Chan, Terence/0000-0002-6550-7203; Zhou,
   Yipeng/0000-0003-1533-0865; Ho, Siu-Wai/0000-0002-8630-494X
FU National Key R&D Program of China [2016YFB0201900]; National Natural
   Science Foundation of China [61572538]; Fundamental Research Funds for
   the Central Universities [17LGJC23]; Australian Research Council
   [DP150103658]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2016YFB0201900, in part by the National Natural Science
   Foundation of China under Grant 61572538, in part by the Fundamental
   Research Funds for the Central Universities under Grant 17LGJC23, and in
   part by the Australian Research Council under Discovery Projects
   DP150103658.
CR Ali-Eldin A., 2015, P 6 ACM MULT SYST C, P189
   [Anonymous], 2014, 2014 IEEE INT C MULT, DOI DOI 10.1109/ICME.2014.6890255
   [Anonymous], 2007, P 6 ACM INT C IM VID
   [Anonymous], 2013, P 2013 C INT MEAS C, DOI [DOI 10.1145/2504730.2504748, 10.1145/2504730.2504748]
   Bao BK, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2730889
   Barragáns-Martínez AB, 2010, INFORM SCIENCES, V180, P4290, DOI 10.1016/j.ins.2010.07.024
   Borghol Y., 2012, ACM SIGKDD International Knowledge Discovery and Data Mining, P1186, DOI [DOI 10.1145/2339530.2339717, 10.1145/2339530.2339717]
   Cha M, 2009, IEEE ACM T NETWORK, V17, P1357, DOI 10.1109/TNET.2008.2011358
   Chen M., IEEE Transactions on Big Data
   Chen M., 2016, P IEEE GLOB WASH DC, P1552
   Chen M., 2015, P 16 ACM INT S MOB A, P399, DOI [10.1145/2746285.2764928, DOI 10.1145/2746285.2764928]
   Cheng X, 2013, IEEE T MULTIMEDIA, V15, P1184, DOI 10.1109/TMM.2013.2265531
   Cui LZ, 2017, CONCURR COMP-PRACT E, V29, DOI 10.1002/cpe.3900
   Davidson James, 2010, P 4 ACM C REC SYST, P293, DOI [DOI 10.1145/1864708.1864770, 10.1145/1864708]
   Deng Zhengyu., 2013, IEEE INT C MULTIMEDI, P1
   Gao GY, 2015, IEEE ICC, P6880, DOI 10.1109/ICC.2015.7249422
   Gao GY, 2015, IEEE T MULTIMEDIA, V17, P1286, DOI 10.1109/TMM.2015.2438713
   Gu X., 2017, P 27 ACM 27 WORKSH N, P31
   Han TT, 2017, IEEE T MULTIMEDIA, V19, P712, DOI 10.1109/TMM.2016.2631881
   Hossain M. S., FUTURE GENER COMPUT
   Hu H., 2014, proceeding of The IEEE International Conference on Multimedia and Expo (ICME), P1, DOI 10.1109/ICME.2014.6890134
   Hu H, 2017, IEEE J SEL AREA COMM, V35, P935, DOI 10.1109/JSAC.2017.2676598
   Hu H, 2017, IEEE J SEL AREA COMM, V35, P545, DOI 10.1109/JSAC.2017.2659478
   Hu H, 2016, IEEE T CIRC SYST VID, V26, P1320, DOI 10.1109/TCSVT.2015.2455712
   Koren Y, 2010, COMMUN ACM, V53, P89, DOI 10.1145/1721654.1721677
   Krishnan SS, 2013, IEEE ACM T NETWORK, V21, P2001, DOI 10.1109/TNET.2013.2281542
   Li H, 2013, TRANSGENIC RES, V22, P169, DOI 10.1007/s11248-012-9623-1
   Li YC, 2016, PROC INT CONF DATA, P505, DOI 10.1109/ICDE.2016.7498266
   Liu NathanNan., 2010, RecSys, P95
   Liu Z, 2016, PROCEEDINGS OF THE 2016 3RD ACM CONFERENCE ON INFORMATION-CENTRIC NETWORKING (ACM-ICN '16), P197, DOI 10.1145/2984356.2985227
   Min WQ, 2015, IEEE T MULTIMEDIA, V17, P1787, DOI 10.1109/TMM.2015.2463226
   Sikdar S, 2016, PROCEEDINGS OF THE 19TH ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING COMPANION, P405, DOI 10.1145/2818052.2869103
   Sun LF, 2017, IEEE T MULTIMEDIA, V19, P609, DOI 10.1109/TMM.2016.2635589
   Wang Z, 2015, IEEE T PARALL DISTR, V26, P775, DOI 10.1109/TPDS.2014.2314103
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Wu JQ, 2015, 2015 IEEE 23RD INTERNATIONAL SYMPOSIUM ON QUALITY OF SERVICE (IWQOS), P141, DOI 10.1109/IWQoS.2015.7404724
   Xu YD, 2017, IEEE T MOBILE COMPUT, V16, P2228, DOI 10.1109/TMC.2016.2616402
   Yoshihara T, 2013, P INST NAVIG PAC PNT, P1
   Yu HL, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P869, DOI 10.1145/2647868.2655037
   Zhang C., 2015, ACM Workshop on Network and Operating Systems Support for Digital Audio and Video, P55, DOI DOI 10.1145/2736084.2736091
   Zhang J., 2017, P 27 WORKSH NETW OP, P37
   Zhao Xiaojian, 2011, P 19 ACM INT C MULTI, P1521
   Zhida Guo, 2013, Passive and Active Measurement. 14th International Conference, PAM 2013. Proceedings, P166, DOI 10.1007/978-3-642-36516-4_17
   Zhou L, 2017, IEEE T CIRC SYST VID, V27, P84, DOI 10.1109/TCSVT.2016.2539698
   Zhou L, 2016, IEEE T MULTIMEDIA, V18, P905, DOI 10.1109/TMM.2016.2537782
   Zhou P, 2016, IEEE T MULTIMEDIA, V18, P1217, DOI 10.1109/TMM.2016.2537216
NR 46
TC 8
Z9 8
U1 3
U2 39
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1512
EP 1524
DI 10.1109/TMM.2017.2769807
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400018
DA 2024-07-18
ER

PT J
AU Zhu, SY
   Li, MY
   Chen, C
   Liu, SC
   Zeng, B
AF Zhu, Shuyuan
   Li, Mingyu
   Chen, Chen
   Liu, Shuaicheng
   Zeng, Bing
TI Cross-Space Distortion Directed Color Image Compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distortion; downsampling; error compensation; image compression;
   interpolation
ID LEAST-SQUARES; INTERPOLATION; PREDICTION; TRANSFORMS; DEBLOCKING;
   SIGNALS; DCT
AB Traditional color image compression is usually conducted in the YCbCr space but many color displayers only accept RGB signals as inputs. Due to the use of a non-unitary matrix in the YCbCr-RGB conversion, low distortion achieved in the YCbCr space cannot guarantee low distortion for the RGB signals. To solve this problem, we propose a novel compression scheme for color images through defining a cross-space distortion so as to reduce as much as possible the distortion in the RGB space. To this end, we first derive the relationship between the distortions in the YCbCr space and RGB space. Then, we develop two solutions to implement color image compression for the most popular 4:2:0 chroma format. The first solution focuses on the design of a new spatial downsampling method to generate the 4:2:0 YCbCr image for a high-efficiency compression. The second one provides a novel way to reduce the distortion of the compressed color image by controlling the quantization error of the 4:2:0 YCbCr image, especially the one generated by using the traditional spatial downsampling. Experimental results show that both proposed solutions offer a remarkable quality gain over some state-of-the-art approaches when tested on various textured color images.
C1 [Zhu, Shuyuan; Li, Mingyu; Chen, Chen; Liu, Shuaicheng; Zeng, Bing] Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 600731, Sichuan, Peoples R China.
   [Chen, Chen] Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Hong Kong, Peoples R China.
C3 University of Electronic Science & Technology of China; Hong Kong
   University of Science & Technology
RP Liu, SC; Zeng, B (corresponding author), Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 600731, Sichuan, Peoples R China.
EM eezsy@uestc.edu.cn; limingyu2300@gmail.com; cchenaf@ust.hk;
   liushuaicheng@uestc.edu.cn; eezeng@uestc.edu.cn
RI Liu, Shuaicheng/JUF-7089-2023; Liu, Shuaicheng/ABP-0493-2022; Liu,
   Shuaicheng/GLN-4107-2022; Li, Ming-Yu/AAH-6879-2020; Chen,
   Chen/GLN-8238-2022
OI Liu, Shuaicheng/0000-0002-8815-5335; Liu,
   Shuaicheng/0009-0005-1290-7157; Li, Ming-Yu/0000-0003-4812-8604; Chen,
   Chen/0000-0003-3498-2527
FU Fundamental Research Funds for Central Universities of China
   [ZYGX2016J038]; "111" Projects [B17008]; Science and Technology Support
   Program of Sichuan Province [2016GZ0124]; National Natural Science
   Foundation of China [61672134, 61502079, 61370148]; National Key Basic
   Research Program of China [2015CB351804]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61672134, Grant 61502079, and Grant
   61370148; in part by the National Key Basic Research Program of China
   under Grant 2015CB351804; in part by the Fundamental Research Funds for
   Central Universities of China under Grant ZYGX2016J038; by the "111"
   Projects under Grant B17008; and in part by the Science and Technology
   Support Program of Sichuan Province under Grant 2016GZ0124.
CR [Anonymous], 2000, 154441 ISOIEC
   [Anonymous], 14 INT C IM PROC
   [Anonymous], 2008, MATRIX HDB STAT
   [Anonymous], 2002, PAR VAL HIGH DEF TEL
   Chen HM, 2012, IEEE SIGNAL PROC LET, V19, P344, DOI 10.1109/LSP.2012.2195172
   Cheng L., 2007, PROC 24 INT C MACHIN, P161, DOI DOI 10.1145/1273496.1273517
   Foi A, 2007, IEEE T IMAGE PROCESS, V16, P1395, DOI 10.1109/TIP.2007.891788
   Frajka T, 2004, SIGNAL PROCESS-IMAGE, V19, P257, DOI 10.1016/j.image.2003.10.003
   Horiuchi T, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/158273
   Hung KW, 2012, IEEE T IMAGE PROCESS, V21, P1061, DOI 10.1109/TIP.2011.2168416
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim WS, 2004, IEEE IMAGE PROC, P785
   Kim YH, 2009, IEEE T CIRC SYST VID, V19, P1051, DOI 10.1109/TCSVT.2009.2020251
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Lin WS, 2006, IEEE T IMAGE PROCESS, V15, P2513, DOI 10.1109/TIP.2006.877415
   Liu XM, 2011, IEEE T IMAGE PROCESS, V20, P3455, DOI 10.1109/TIP.2011.2150234
   Lu WS, 2006, 2006 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS, P378
   Lucas LFR, 2010, IEEE IMAGE PROC, P1329, DOI 10.1109/ICIP.2010.5653834
   Ma L, 2011, IEEE INT SYMP CIRC S, P97
   MALVAR HS, 1988, IEEE T COMMUN, V36, P67, DOI 10.1109/26.2730
   Noda H, 2010, TENCON IEEE REGION, P1657, DOI 10.1109/TENCON.2010.5686027
   Peter P, 2017, IEEE T IMAGE PROCESS, V26, P860, DOI 10.1109/TIP.2016.2627800
   SULLIVAN G, 2003, JVT1017 ISOIEC JTC1S
   Sun C, 2015, IEEE T IMAGE PROCESS, V24, P886, DOI 10.1109/TIP.2014.2383324
   Tran TD, 2003, IEEE T SIGNAL PROCES, V51, P1557, DOI 10.1109/TSP.2003.811222
   Wang SQ, 2016, IEEE T CIRC SYST VID, V26, P1595, DOI 10.1109/TCSVT.2015.2461891
   Wu XL, 2009, IEEE T IMAGE PROCESS, V18, P552, DOI 10.1109/TIP.2008.2010638
   Xu JZ, 2010, IEEE T IMAGE PROCESS, V19, P85, DOI 10.1109/TIP.2009.2032344
   Yang EH, 2009, IEEE T IMAGE PROCESS, V18, P63, DOI 10.1109/TIP.2008.2007609
   Yoo SB, 2014, IEEE T MULTIMEDIA, V16, P1536, DOI 10.1109/TMM.2014.2327563
   Zeng B, 2008, IEEE T CIRC SYST VID, V18, P305, DOI 10.1109/TCSVT.2008.918455
   Zhai GT, 2008, IEEE T MULTIMEDIA, V10, P735, DOI 10.1109/TMM.2008.922849
   Zhang J, 2016, IEEE T IMAGE PROCESS, V25, P1246, DOI 10.1109/TIP.2016.2515985
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhang XJ, 2008, IEEE T IMAGE PROCESS, V17, P887, DOI 10.1109/TIP.2008.924279
   Zhang XY, 2014, IEEE T IMAGE PROCESS, V23, P274, DOI 10.1109/TIP.2013.2288007
   Zhang YB, 2011, IEEE T IMAGE PROCESS, V20, P3291, DOI 10.1109/TIP.2011.2158226
   Zhu SY, 2016, IEEE T MULTIMEDIA, V18, P1707, DOI 10.1109/TMM.2016.2593039
   Zhu SY, 2010, IEEE SIGNAL PROC LET, V17, P961, DOI 10.1109/LSP.2010.2080266
   Zhu SY, 2009, IEEE SIGNAL PROC LET, V16, P861, DOI 10.1109/LSP.2009.2026115
NR 40
TC 14
Z9 14
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2018
VL 20
IS 3
BP 525
EP 538
DI 10.1109/TMM.2017.2749162
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FW5ZH
UT WOS:000425397500002
DA 2024-07-18
ER

PT J
AU Kalampogia, A
   Koutsakis, P
AF Kalampogia, Athina
   Koutsakis, Polychronis
TI H.264 and H.265 Video Bandwidth Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bandwidth prediction; H.264; H.265; traffic modeling; video streaming
ID CODING STANDARD; MODEL; MPEG-4; QUALITY
AB The explosive growth of multimedia applications renders the efficiency of network resource allocation a problem of major importance. The burstiness of video traffic, in particular, calls for traffic control solutions that will help prevent significant packet losses. Such losses can lead to unacceptable quality of service (QoS) and quality of experience (QoE) to users. In this paper, we focus on a large variety of H.264- and H.265-encoded video traces with different GoP patterns. Different versions of each trace, in low, medium, and high quality have been used in our study. We evaluate the accuracy of an existing video traffic prediction approach for the size of B-frames, and we propose a new Markovian model that predicts B-frames' sizes with significantly higher accuracy. B-frame size prediction can be used in order to reduce bandwidth requirements and smooth the encoded video stream, by selective B-frame dropping, when the model predicts larger upcoming B-frame traffic than the network can handle.
C1 [Kalampogia, Athina] Tech Univ Crete, Sch Elect & Comp Engn, Khania 73100, Greece.
   [Koutsakis, Polychronis] Murdoch Univ, Sch Engn & Informat Technol, Murdoch, WA 6150, Australia.
C3 Technical University of Crete; Murdoch University
RP Koutsakis, P (corresponding author), Murdoch Univ, Sch Engn & Informat Technol, Murdoch, WA 6150, Australia.
EM akalampogia@gmail.com; p.koutsakis@murdoch.edu.au
OI Koutsakis, Polychronis/0000-0002-4168-0888
CR [Anonymous], 2010, P 1 ANN ACM SIGMM C
   [Anonymous], 1991, SIMULATION MODELING
   Bhattacharya A, 2003, IEEE T SIGNAL PROCES, V51, P2177, DOI 10.1109/TSP.2003.814470
   Chakareski J, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P763, DOI 10.1109/ICME.2005.1521535
   Chandra K, 1999, IEEE ACM T NETWORK, V7, P398, DOI 10.1109/90.779209
   Cisco White Paper, 2017, CISCO VIRTUAL NETWOR
   Dai M, 2009, IEEE T MULTIMEDIA, V11, P1010, DOI 10.1109/TMM.2009.2021802
   Dawood AM, 1999, IEEE T MULTIMEDIA, V1, P77, DOI 10.1109/6046.748173
   Frey M, 2000, IEEE ACM T NETWORK, V8, P710, DOI 10.1109/90.893868
   Greengrass J, 2009, IEEE INTERNET COMPUT, V13, P70, DOI 10.1109/MIC.2009.14
   HEYMAN D, 1994, SERVING HUMANITY THROUGH COMMUNICATIONS, VOLS 1-3, P1744, DOI 10.1109/ICC.1994.368735
   Heyman DP, 1992, IEEE T CIRC SYST VID, V2, P49, DOI 10.1109/76.134371
   Heyman DP, 1997, IEEE ACM T NETWORK, V5, P554, DOI 10.1109/90.649513
   Kalampogia A., TECH REP
   Kalbkhani H, 2017, IEEE T MULTIMEDIA, V19, P999, DOI 10.1109/TMM.2016.2639379
   Kholaif AM, 2010, IEEE T MULTIMEDIA, V12, P142, DOI 10.1109/TMM.2009.2037380
   Lanfranchi LI, 2008, IEEE T BROADCAST, V54, P741, DOI 10.1109/TBC.2008.2001244
   Lazaris A, 2008, PERFORM EVALUATION, V65, P51, DOI 10.1016/j.peva.2007.02.004
   Li Y, 2009, IEEE T MULTIMEDIA, V11, P1182, DOI 10.1109/TMM.2009.2026102
   LUCANTONI DM, 1994, IEEE ACM T NETWORK, V2, P176, DOI 10.1109/90.298435
   Marpe D, 2006, IEEE COMMUN MAG, V44, P134, DOI 10.1109/MCOM.2006.1678121
   Miller K, 2015, IEEE T MULTIMEDIA, V17, P1309, DOI 10.1109/TMM.2015.2441002
   Qadir QM, 2015, IEEE T MULTIMEDIA, V17, P711, DOI 10.1109/TMM.2015.2416637
   Qi X, 2016, IEEE T MULTIMEDIA, V18, P1640, DOI 10.1109/TMM.2016.2572001
   Ren Q, 1998, IEEE J SEL AREA COMM, V16, P679, DOI 10.1109/49.700905
   Sarkar UK, 2003, IEEE ACM T NETWORK, V11, P638, DOI 10.1109/TNET.2003.815292
   Spanou I., 2013, 2013 IEEE International Conference on Communications (ICC), P2386, DOI 10.1109/ICC.2013.6654888
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tanwir S, 2013, IEEE COMMUN SURV TUT, V15, P1778, DOI 10.1109/SURV.2013.010413.00071
   Van der Auwera G, 2008, IEEE T BROADCAST, V54, P698, DOI 10.1109/TBC.2008.2000422
   Wang YY, 2010, PROCEEDINGS OF THE ASME INTERNATIONAL PIPELINE CONFERENCE 2010, VOL 4, P165, DOI 10.1109/PV.2010.5706834
   Xiao Y, 2011, IEEE SYST J, V5, P474, DOI 10.1109/JSYST.2011.2165596
   Xu SG, 1998, IEEE T CIRC SYST VID, V8, P138, DOI 10.1109/76.664098
NR 33
TC 19
Z9 20
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2018
VL 20
IS 1
BP 171
EP 182
DI 10.1109/TMM.2017.2713642
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FQ3WS
UT WOS:000418289700014
DA 2024-07-18
ER

PT J
AU Ma, L
   Li, HL
   Meng, FM
   Wu, QB
   Ngan, KN
AF Ma, Lei
   Li, Hongliang
   Meng, Fanman
   Wu, Qingbo
   Ngan, King Ngi
TI Learning Efficient Binary Codes From High-Level Feature Representations
   for Multilabel Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Efficient binary codes; image semantic retrieval; nonnegative matrix
   factorization
ID DIMENSIONALITY REDUCTION; OBJECT; SCENE; QUANTIZATION; ALGORITHM; MODEL;
   HASH
AB Due to the efficiency and effectiveness of hashing technologies, they have become increasingly popular in large-scale image semantic retrieval. However, existing hash methods suppose that the data distributions satisfy the manifold assumption that semantic similar samples tend to lie on a low-dimensional manifold, which will be weakened due to the large intraclass variation. Moreover, these methods learn hash functions by relaxing the discrete constraints on binary codes to real value, which will introduce large quantization loss. To tackle the above problems, this paper proposes a novel unsupervised hashing algorithm to learn efficient binary codes from high-level feature representations. More specifically, we explore nonnegative matrix factorization for learning high-level visual features. Ultimately, binary codes are generated by performing binary quantization in the high-level feature representations space, which will map images with similar (visually or semantically) high-level feature representations to similar binary codes. To solve the corresponding optimization problem involving nonnegative and discrete variables, we develop an efficient optimization algorithm to reduce quantization loss with guaranteed convergence in theory. Extensive experiments show that our proposed method outperforms the state-of-the-art hashing methods on several multilabel real-world image datasets.
C1 [Ma, Lei; Li, Hongliang; Meng, Fanman; Wu, Qingbo] Univ Elect Sci & Technol China, Intelligent Visual Informat Proc & Commun Lab IVI, Sch Elect Engn, Chengdu 611731, Sichuan, Peoples R China.
   [Ngan, King Ngi] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
   [Ngan, King Ngi] Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 611731, Sichuan, Peoples R China.
C3 University of Electronic Science & Technology of China; Chinese
   University of Hong Kong; University of Electronic Science & Technology
   of China
RP Li, HL (corresponding author), Univ Elect Sci & Technol China, Intelligent Visual Informat Proc & Commun Lab IVI, Sch Elect Engn, Chengdu 611731, Sichuan, Peoples R China.
EM leima_uestc@163.com; hlli@uestc.edu.cn; fmmeng@uestc.edu.cn;
   wqb.uestc@gmail.com; knngan@ee.cuhk.edu.hk
RI Wu, Qingbo/AAF-6872-2019; Ngan, N/E-8240-2014
OI Wu, Qingbo/0000-0003-2936-6340; Ngan, N/0000-0003-1946-3235; Li,
   Hongliang/0000-0002-7481-095X
FU National Natural Science Foundation of China [61525102, 61502084,
   61601102]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61525102, Grant 61502084, and Grant
   61601102. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Enrico Magli.
   (Corresponding author: Hongliang Li.)
CR [Anonymous], 2001, NIPS
   [Anonymous], 2004, P 20 ACM S COMP
   [Anonymous], 2012, NIPS
   [Anonymous], P 31 INT C INT C MAC
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2009, NIPS
   [Anonymous], 2006, PROC 12 ACM SIGKDD I
   Bao Y, 2014, AAAI CONF ARTIF INTE, P2
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Berry M. W., 2005, Computational & Mathematical Organization Theory, V11, P249, DOI 10.1007/s10588-005-5380-5
   Carreira-Perpinan MA, 2010, ICML 10, P167
   Chen ZX, 2017, IEEE T MULTIMEDIA, V19, P123, DOI 10.1109/TMM.2016.2620604
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745
   Ge TZ, 2014, LECT NOTES COMPUT SC, V8695, P250, DOI 10.1007/978-3-319-10584-0_17
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gray RM, 1998, IEEE T INFORM THEORY, V44, P2325, DOI 10.1109/18.720541
   Guttman Antonin., 1984, P 1984 ACM SIGMOD C, P47
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Huang KJ, 2014, IEEE T SIGNAL PROCES, V62, P211, DOI 10.1109/TSP.2013.2285514
   Irie G, 2014, PROC CVPR IEEE, P2123, DOI 10.1109/CVPR.2014.272
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jolliffe I. T., 2002, PRINCIPAL COMPONENT
   Lai HJ, 2016, IEEE T IMAGE PROCESS, V25, P2469, DOI 10.1109/TIP.2016.2545300
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Li HL, 2013, IEEE T MULTIMEDIA, V15, P1896, DOI 10.1109/TMM.2013.2271476
   Li Z, 2010, PATTERN RECOGN LETT, V31, P905, DOI 10.1016/j.patrec.2009.12.023
   Lin Kevin, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P27, DOI 10.1109/CVPRW.2015.7301269
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Meng FM, 2012, IEEE T MULTIMEDIA, V14, P1429, DOI 10.1109/TMM.2012.2197741
   Mukherjee L, 2015, IEEE I CONF COMP VIS, P4184, DOI 10.1109/ICCV.2015.476
   Norouzi M.E., 2011, ICML
   Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen FM, 2013, PROC CVPR IEEE, P1562, DOI 10.1109/CVPR.2013.205
   Song TC, 2014, IEEE INT SYMP CIRC S, P886, DOI 10.1109/ISCAS.2014.6865278
   Song TC, 2014, IEEE SIGNAL PROC LET, V21, P93, DOI 10.1109/LSP.2013.2293335
   Song TC, 2013, PATTERN RECOGN LETT, V34, P1323, DOI 10.1016/j.patrec.2013.04.020
   Song TC, 2013, IEEE SIGNAL PROC LET, V20, P59, DOI 10.1109/LSP.2012.2229273
   Strecha C, 2012, IEEE T PATTERN ANAL, V34, P66, DOI 10.1109/TPAMI.2011.103
   Sun YY, 2010, AAAI CONF ARTIF INTE, P587
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Trigeorgis G, 2014, PR MACH LEARN RES, V32, P1692
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang Jingdong., 2017, IEEE Transactions on Pattern Analysis and Machine Intelligence
   Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Weiss Y., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P975, DOI 10.1109/ICCV.1999.790354
   Weiss Y., 2008, PROC ADV NEURAL INFO, V21, P1753
   Wu CX, 2013, IEEE T KNOWL DATA EN, V25, P1380, DOI 10.1109/TKDE.2012.76
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Yang Y, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1286, DOI 10.1145/2964284.2964319
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
NR 61
TC 26
Z9 29
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2017
VL 19
IS 11
BP 2545
EP 2560
DI 10.1109/TMM.2017.2703089
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FJ9CT
UT WOS:000413068200015
DA 2024-07-18
ER

PT J
AU Lin, J
   Duan, LY
   Wang, SQ
   Bai, Y
   Lou, YH
   Chandrasekhar, V
   Huang, TJ
   Kot, A
   Gao, W
AF Lin, Jie
   Duan, Ling-Yu
   Wang, Shiqi
   Bai, Yan
   Lou, Yihang
   Chandrasekhar, Vijay
   Huang, Tiejun
   Kot, Alex
   Gao, Wen
TI HNIP: Compact Deep Invariant Representations for Video Matching,
   Localization, and Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural networks (CNNs); deep global descriptors;
   handcrafted descriptors; hybrid nested invariance pooling; MPEG CDVA;
   MPEG CDVS
ID FEATURES
AB With emerging demand for large-scale video analysis, MPEG initiated the compact descriptor for video analysis (CDVA) standardization in 2014. Beyond handcrafted descriptors adopted by the current MPEG-CDVA reference model, we study the problem of deep learned global descriptors for video matching, localization, and retrieval. First, inspired by a recent invariance theory, we propose a nested invariance pooling (NIP) method to derive compact deep global descriptors from convolutional neural networks (CNNs), by progressively encoding translation, scale, and rotation invariances into the pooled descriptors. Second, our empirical studies have shown that a sequence of well designed pooling moments (e.g.,max or average) may drastically impact video matching performance, which motivates us to design hybrid pooling operations via NIP (HNIP). HNIP has further improved the discriminability of deep global descriptors. Third, the technical merits and performance improvements by combining deep and handcrafted descriptors are provided to better investigate the complementary effects. We evaluate the effectiveness of HNIP within the well-established MPEG-CDVA evaluation framework. The extensive experiments have demonstrated that HNIP outperforms the state-of-the-art deep and canonical handcrafted descriptors with significant mAP gains of 5.5% and 4.7%, respectively. In particular the combination of HNIP incorporated CNN descriptors and handcrafted global descriptors has significantly boosted the performance of CDVA core techniques with comparable descriptor size.
C1 [Lin, Jie; Chandrasekhar, Vijay] ASTAR, Inst Infocomm Res, Singapore 138632, Singapore.
   [Duan, Ling-Yu; Bai, Yan; Lou, Yihang; Huang, Tiejun; Gao, Wen] Peking Univ, Inst Digital Media, Beijing 100080, Peoples R China.
   [Wang, Shiqi] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   [Chandrasekhar, Vijay; Kot, Alex] Nanyang Technol Univ, Singapore 639798, Singapore.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); Peking University; City University of Hong
   Kong; Nanyang Technological University
RP Duan, LY (corresponding author), Peking Univ, Inst Digital Media, Beijing 100080, Peoples R China.
EM lin-j@i2r.a-star.edu.sg; lingyu@pku.edu.cn; shiqwang@cityu.edu.hk;
   yanbai@pku.edu.cn; yihang@pku.edu.cn; EACKOT@ntu.edu.sg;
   tjhuang@pku.edu.cn; vijay@i2r.a-star.edu.sg; wgao@pku.edu.cn
RI Wang, Shiqi/AAR-5013-2020; Huang, Tiejun/D-6161-2011
OI Wang, Shiqi/0000-0002-6338-1432; 
FU National Hightech R&D Program of China [2015AA016302]; National Natural
   Science Foundation of China [U1611461, 61661146005]; PKU-NTU Joint
   Research Institute (JRI) - Ng Teng Fong Charitable Foundation
FX This work was supported in part by the National Hightech R&D Program of
   China under Grant 2015AA016302, in part by the National Natural Science
   Foundation of China under Grant U1611461 and Grant 61661146005, and in
   part by the PKU-NTU Joint Research Institute (JRI) sponsored by a
   donation from the Ng Teng Fong Charitable Foundation. The guest editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Cees Snoek. (Corresponding author: Ling-Yu Duan.)
CR [Anonymous], IJCAI P INT JOINT C
   [Anonymous], CORR
   [Anonymous], 2009, NEURIPS
   Anselmi F., 2014, P CTR BRAINS MINDS
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Araujo Andre, 2015, 2015 IEEE International Conference on Image Processing (ICIP). Proceedings, P1519, DOI 10.1109/ICIP.2015.7351054
   Azizpour Hossein, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P36, DOI 10.1109/CVPRW.2015.7301270
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Ballas N., 2014, P TRECVID 2014 WORKS
   Baroffio L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2445294
   Baroffio L, 2014, IEEE IMAGE PROC, P2794, DOI 10.1109/ICIP.2014.7025565
   Baroffio L, 2014, IEEE T IMAGE PROCESS, V23, P2262, DOI 10.1109/TIP.2014.2312617
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Call for Proposals for Compact Descriptors for Video Analysis (CDVA)-Search and Retrieval, 2015, ISOIECJTC1SC29WG11NI
   Calonder M, 2012, IEEE T PATTERN ANAL, V34, P1281, DOI 10.1109/TPAMI.2011.222
   Cdva Experimentation Model (cxm) 0. 2, 2015, ISOIECJTC1SC29WG11W1
   Chandrasekhar Vijay, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2504, DOI 10.1109/CVPRW.2009.5206733
   Chandrasekhar V., 2009, P IS T SPIE EL IMAG
   Chao JS, 2016, IEEE T MULTIMEDIA, V18, P25, DOI 10.1109/TMM.2015.2502552
   Chen D, 2013, SIGNAL PROCESS, V93, P2316, DOI 10.1016/j.sigpro.2012.06.005
   Chen DM, 2015, IEEE T MULTIMEDIA, V17, P1019, DOI 10.1109/TMM.2015.2427744
   Chen DM, 2014, IEEE DATA COMPR CONF, P33, DOI 10.1109/DCC.2014.8
   Chen DM, 2008, IEEE DATA COMPR CONF, P143, DOI 10.1109/DCC.2009.33
   Chiyuan Zhang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6984, DOI 10.1109/ICASSP.2014.6854954
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   CIMPOI M, 2015, PROC CVPR IEEE, P3828, DOI DOI 10.1109/CVPR.2015.7299007
   Compact Descriptors for Video Analysis: Objectives Applications and Use Cases, 2014, ISOIECJTC1SC29WG11N1
   Description of Core Experiments in CDVA, 2016, ISOIECJTC1SC29WG11W1
   Diba Ali, 2017, CORR
   Duan LY, 2016, IEEE T IMAGE PROCESS, V25, P179, DOI 10.1109/TIP.2015.2500034
   Girod B, 2011, IEEE SIGNAL PROC MAG, V28, P61, DOI 10.1109/MSP.2011.940881
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15
   Husain S. S., IEEE T PATT IN PRESS
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2014, PROC CVPR IEEE, P3310, DOI 10.1109/CVPR.2014.417
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Kalantidis Yannis, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P685, DOI 10.1007/978-3-319-46604-0_48
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   LENC K, 2015, PROC CVPR IEEE, P991
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Liao Q., 2013, Advances in Neural Information Processing Systems
   Lin J, 2014, IEEE SIGNAL PROC LET, V21, P195, DOI 10.1109/LSP.2013.2296532
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Makar M, 2014, IEEE T IMAGE PROCESS, V23, P3352, DOI 10.1109/TIP.2014.2331136
   Nister D., 2006, IEEE COMP SOC C COMP, P2161, DOI [10.1109/cvpr.2006.264, DOI 10.1109/CVPR.2006.264, 10.1109/CVPR]
   PERRONNIN F, 2010, PROC CVPR IEEE, P3384, DOI DOI 10.1109/CVPR.2010.5540009
   Radenovic F, 2016, LECT NOTES COMPUT SC, V9905, P3, DOI 10.1007/978-3-319-46448-0_1
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Redondi A, 2013, IEEE INT WORKSH MULT, P278, DOI 10.1109/MMSP.2013.6659301
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Shi MJ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P407, DOI 10.1145/2647868.2654895
   Simonyan K., 2015, P 3 INT C LEARN REPR
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Test Model 14: Compact Descriptors for Visual Search, 2011, ISOIECJTC1SC29WG11W1
   Tolias G., 2016, INT C LEARN REP
   Uijlings JRR, 2010, IEEE T MULTIMEDIA, V12, P665, DOI 10.1109/TMM.2010.2052027
   Weiss Yair, 2009, Advances in Neural Information Processing Systems, P1753, DOI DOI 10.5555/2981780.2981999
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Yandex Artem Babenko, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1269, DOI 10.1109/ICCV.2015.150
   Zhang SL, 2014, IEEE T IMAGE PROCESS, V23, P3671, DOI 10.1109/TIP.2014.2330794
   Zhu CZ, 2013, IEEE I CONF COMP VIS, P1705, DOI 10.1109/ICCV.2013.214
NR 65
TC 29
Z9 30
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2017
VL 19
IS 9
BP 1968
EP 1983
DI 10.1109/TMM.2017.2713410
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5XC
UT WOS:000411244200002
DA 2024-07-18
ER

PT J
AU Shen, FM
   Yang, Y
   Liu, L
   Liu, W
   Tao, DC
   Shen, HT
AF Shen, Fumin
   Yang, Yang
   Liu, Li
   Liu, Wei
   Tao, Dacheng
   Shen, Heng Tao
TI Asymmetric Binary Coding for Image Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hashing; binary codes; retrieval
ID SCALE; SUBSPACE
AB Learning to hash has attracted broad research interests in recent computer vision and machine learning studies, due to its ability to accomplish efficient approximate nearest neighbor search. However, the closely related task, maximum inner product search (MIPS), has rarely been studied in this literature. To facilitate the MIPS study, in this paper, we introduce a general binary coding framework based on asymmetric hash functions, named asymmetric inner-product binary coding (AIBC). In particular, AIBC learns two different hash functions, which can reveal the inner products between original data vectors by the generated binary vectors. Although conceptually simple, the associated optimization is very challenging due to the highly nonsmooth nature of the objective that involves sign functions. We tackle the nonsmooth optimization in an alternating manner, by which each single coding function is optimized in an efficient discrete manner. We also simplify the objective by discarding the quadratic regularization term which significantly boosts the learning efficiency. Both problems are optimized in an effective discrete way without continuous relaxations, which produces high-quality hash codes. In addition, we extend the AIBC approach to the supervised hashing scenario, where the inner products of learned binary codes are forced to fit the supervised similarities. Extensive experiments on several benchmark image retrieval databases validate the superiority of the AIBC approaches over many recently proposed hashing algorithms.
C1 [Shen, Fumin; Yang, Yang; Shen, Heng Tao] Univ Elect Sci & Technol China, Ctr Future Media, Chengdu 611731, Sichuan, Peoples R China.
   [Shen, Fumin; Yang, Yang; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Sichuan, Peoples R China.
   [Liu, Li] Malong Technol Co Ltd, Shenzhen 518081, Peoples R China.
   [Liu, Wei] Tencent AI Lab, Shenzhen 518057, Peoples R China.
   [Tao, Dacheng] Univ Sydney, Sch Informat Technol, Sydney, NSW 2006, Australia.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China; Tencent; University of Sydney
RP Shen, HT (corresponding author), Univ Elect Sci & Technol China, Ctr Future Media, Chengdu 611731, Sichuan, Peoples R China.; Shen, HT (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Sichuan, Peoples R China.
EM fumin.shen@gmail.com; dlyyang@gmail.com; li.liu@malongtech.cn;
   wliu@ee.columbia.edu; dacheng.tao@sydney.edu.au; shenhengtao@hotmail.com
RI Lang, Ming/HIK-0758-2022; Shen, Fumin/R-2121-2016; Tao,
   Dacheng/A-5449-2012; Liu, Wei/L-1951-2019; yang, yang/GVT-5210-2022;
   yang, yang/HGT-7999-2022; Shen, Heng Tao/ABD-5331-2021; liu,
   li/ADL-2178-2022
OI Tao, Dacheng/0000-0001-7225-5449; liu, li/0000-0002-6669-9382; Liu,
   Wei/0000-0002-3865-8145
FU National Natural Science Foundation of China [61502081, 61572108,
   61673299, 61632007]; Fundamental Research Funds for the Central
   Universities [ZYGX2014Z007]
FX This work was supported in part by the National Natural Science
   Foundation of China under Project 61502081, Project 61572108, Project
   61673299, and Project 61632007, and in part by the Fundamental Research
   Funds for the Central Universities under Project ZYGX2014Z007. The guest
   editor coordinating the review of this manuscript and approving it for
   publication was Mr. Jingkuan Song. (Corresponding author: Heng Tao
   Shen.)
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 2012, NIPS
   [Anonymous], 2009, NIPS
   [Anonymous], 2013, NeurIPS
   [Anonymous], 2008, Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Kulis B., 2009, P IEEE INT C COMP VI, P1092
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Li ZJ, 2016, FRONT ARTIF INTEL AP, V285, P64, DOI 10.3233/978-1-61499-672-9-64
   Lin GS, 2014, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2014.253
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu L., P IEEE C CO IN PRESS
   Liu L, 2017, IEEE T IMAGE PROCESS, V26, P107, DOI 10.1109/TIP.2016.2619262
   Liu TL, 2017, IEEE T PATTERN ANAL, V39, P227, DOI 10.1109/TPAMI.2016.2544314
   Liu W., 2014, P NEURAL INF PROCESS, P3419
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Liu XL, 2016, IEEE T CYBERNETICS, V46, P2252, DOI 10.1109/TCYB.2015.2474742
   Liu XL, 2016, IEEE T IMAGE PROCESS, V25, P907, DOI 10.1109/TIP.2015.2505180
   Liu XL, 2014, PATTERN RECOGN, V47, P748, DOI 10.1016/j.patcog.2013.08.022
   Lu JW, 2017, IEEE T IMAGE PROCESS, V26, P2352, DOI 10.1109/TIP.2017.2678163
   Lu JW, 2015, IEEE T PATTERN ANAL, V37, P2041, DOI 10.1109/TPAMI.2015.2408359
   Luo YD, 2018, PATTERN RECOGN, V75, P128, DOI 10.1016/j.patcog.2017.02.034
   Lv YM, 2015, IEEE T MULTIMEDIA, V17, P1225, DOI 10.1109/TMM.2015.2437712
   Neyshabur B, 2015, PR MACH LEARN RES, V37, P1926
   Norouzi M.E., 2011, ICML
   Qin J., P IEEE C CO IN PRESS
   Shen FM, 2016, IEEE T IMAGE PROCESS, V25, P5610, DOI 10.1109/TIP.2016.2612883
   Shen FM, 2015, IEEE I CONF COMP VIS, P4148, DOI 10.1109/ICCV.2015.472
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen FM, 2015, IEEE T IMAGE PROCESS, V24, P1839, DOI 10.1109/TIP.2015.2405340
   Shen FM, 2013, PROC CVPR IEEE, P1562, DOI 10.1109/CVPR.2013.205
   Shrivastava Anshumali, 2014, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, P2321
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Song JK, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P827, DOI 10.1145/2733373.2806341
   Song J, 2016, IEEE T IMAGE PROCESS, V25, P4999, DOI 10.1109/TIP.2016.2601260
   Song JK, 2016, IEEE T MULTIMEDIA, V18, P484, DOI 10.1109/TMM.2016.2515990
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Tao D., 2007, IEEE T PATTERN ANAL, V29
   Tao DC, 2009, IEEE T PATTERN ANAL, V31, P260, DOI 10.1109/TPAMI.2008.70
   Tao DC, 2006, IEEE T PATTERN ANAL, V28, P1088, DOI 10.1109/TPAMI.2006.134
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Weiss Y., 2008, PROC ADV NEURAL INFO, V21, P1753
   Weiss Y, 2012, LECT NOTES COMPUT SC, V7576, P340, DOI 10.1007/978-3-642-33715-4_25
   Wolf L, 2011, CVPR, DOI DOI 10.1109/CVPR.2011.5995566
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Xu C, 2015, IEEE T PATTERN ANAL, V37, P2531, DOI 10.1109/TPAMI.2015.2417578
   Yang Yang, 2015, IEEE Transactions on Big Data, V1, P162, DOI 10.1109/TBDATA.2016.2516024
   Yang Y, 2014, IEEE T MULTIMEDIA, V16, P1677, DOI 10.1109/TMM.2014.2323014
   Zhang HW, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P325, DOI 10.1145/2911451.2911502
   Zhang HW, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P781, DOI 10.1145/2964284.2964308
NR 57
TC 103
Z9 106
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2017
VL 19
IS 9
BP 2022
EP 2032
DI 10.1109/TMM.2017.2699863
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5XC
UT WOS:000411244200006
DA 2024-07-18
ER

PT J
AU Luo, B
   Li, HL
   Meng, FM
   Wu, QB
   Huang, C
AF Luo, Bing
   Li, Hongliang
   Meng, Fanman
   Wu, Qingbo
   Huang, Chao
TI Video Object Segmentation via Global Consistency Aware Query Strategy
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Energy minimization; global consistency; video object segmentation
ID GRAPH CUTS; ENERGY MINIMIZATION; CO-SEGMENTATION; EXTRACTION; TRACKING;
   PROPAGATION; ALGORITHMS
AB In this paper, we propose a video object segmentation method via global consistency aware query strategy. The aim is to obtain higher segmentation accuracy with less user annotation. Intuitively, we hope to annotate some frames to obtain better segmentation performance than to annotate other frames, which can be modeled by active learning framework. Specifically, we first generate a sample space of potential annotation regions via an object proposals method for each frame. Then, the annotation likelihood for the region is calculated in terms of annotation history and global consistency for the object in the video. Third, the segmentation result of the annotation region can be obtained by minimizing an MRF energy function. Fourth, the algorithm will provide the user with the most valuable frame to annotate, which has high annotation likelihood and large segmentation result change. Finally, the annotation is added to the framework to begin the next iteration. Experiments on a number of video sequences demonstrate that the proposed method can reduce the user effort and obtain the higher segmentation accuracy compared with the state-of-the-art methods.
C1 [Luo, Bing; Li, Hongliang; Meng, Fanman; Wu, Qingbo; Huang, Chao] Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Luo, B (corresponding author), Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 611731, Peoples R China.
EM Mathild1987@163.com; hlli@uestc.edu.cn; fmmeng@uestc.edu.cn;
   wqb.uestc@gmail.com; huangchao_uestc@aliyun.com
RI Huang, Chao/JJD-0553-2023; Huang, Chao/L-1445-2019; Wu,
   Qingbo/AAF-6872-2019
OI Huang, Chao/0000-0001-8775-3192; Wu, Qingbo/0000-0003-2936-6340; Li,
   Hongliang/0000-0002-7481-095X
FU National Natural Science Foundation of China [61525102, 61502084,
   61601102]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61525102, Grant 61502084, and Grant 61601102. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Sen-Ching Samson Cheung.
CR Alahari K, 2013, IEEE I CONF COMP VIS, P2112, DOI 10.1109/ICCV.2013.263
   Banica D, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P283, DOI 10.1109/ICCVW.2013.45
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21
   Chockalingam P, 2009, IEEE I CONF COMP VIS, P1530, DOI 10.1109/ICCV.2009.5459276
   Faktor Alon, 2014, BMVC
   Felzenszwalb PF, 2011, IEEE T PATTERN ANAL, V33, P721, DOI 10.1109/TPAMI.2010.135
   Fidler S, 2013, PROC CVPR IEEE, P3294, DOI 10.1109/CVPR.2013.423
   FISHER ML, 1978, MATH PROGRAM STUD, V8, P73, DOI 10.1007/BFb0121195
   Fragkiadaki K, 2015, PROC CVPR IEEE, P4083, DOI 10.1109/CVPR.2015.7299035
   Fragkiadaki K, 2013, PROC CVPR IEEE, P2059, DOI 10.1109/CVPR.2013.268
   Grundmann M, 2010, PROC CVPR IEEE, P2141, DOI 10.1109/CVPR.2010.5539893
   Guo JM, 2013, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2013.278
   Jain SD, 2014, LECT NOTES COMPUT SC, V8692, P656, DOI 10.1007/978-3-319-10593-2_43
   Karasev V, 2014, PROC CVPR IEEE, P2131, DOI 10.1109/CVPR.2014.273
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Lee YJ, 2011, IEEE I CONF COMP VIS, P1995, DOI 10.1109/ICCV.2011.6126471
   Li CL, 2016, IEEE T IMAGE PROCESS, V25, P1947, DOI 10.1109/TIP.2016.2537211
   Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273
   Li Y, 2005, ACM T GRAPHIC, V24, P595, DOI 10.1145/1073204.1073234
   Lim T, 2013, IEEE I CONF COMP VIS, P833, DOI 10.1109/ICCV.2013.108
   Liu C.-H., 2008, Global Telecommunications Conference, P1
   Lu JS, 2015, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2015.7299000
   Luo B, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1187, DOI 10.1145/2733373.2806313
   Ma SG, 2015, PROC CVPR IEEE, P5024, DOI 10.1109/CVPR.2015.7299137
   Ma SG, 2013, IEEE I CONF COMP VIS, P2744, DOI 10.1109/ICCV.2013.341
   Ma TY, 2012, PROC CVPR IEEE, P670, DOI 10.1109/CVPR.2012.6247735
   Malik J., 2007, Computer Vision and Pattern Recognition, P1
   Meng FM, 2013, IEEE T MULTIMEDIA, V15, P2186, DOI 10.1109/TMM.2013.2280893
   Meng FM, 2012, IEEE T MULTIMEDIA, V14, P1429, DOI 10.1109/TMM.2012.2197741
   Milan A, 2015, PROC CVPR IEEE, P5397, DOI 10.1109/CVPR.2015.7299178
   Mortensen E. N., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P191, DOI 10.1145/218380.218442
   Nagaraja NS, 2015, IEEE I CONF COMP VIS, P3235, DOI 10.1109/ICCV.2015.370
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Ochs P, 2014, IEEE T PATTERN ANAL, V36, P1187, DOI 10.1109/TPAMI.2013.242
   Ochs P, 2012, PROC CVPR IEEE, P614, DOI 10.1109/CVPR.2012.6247728
   Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223
   Perazzi F, 2015, IEEE I CONF COMP VIS, P3227, DOI 10.1109/ICCV.2015.369
   Price BL, 2009, IEEE I CONF COMP VIS, P779, DOI 10.1109/ICCV.2009.5459293
   Ramakanth SA, 2014, PROC CVPR IEEE, P376, DOI 10.1109/CVPR.2014.55
   Rantalankila P, 2014, PROC CVPR IEEE, P2417, DOI 10.1109/CVPR.2014.310
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Sun C., IEEE T CIRC IN PRESS
   Taylor B, 2015, PROC CVPR IEEE, P4268, DOI 10.1109/CVPR.2015.7299055
   Tsai D, 2012, INT J COMPUT VISION, V100, P190, DOI 10.1007/s11263-011-0512-5
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Varas D, 2014, PROC CVPR IEEE, P3470, DOI 10.1109/CVPR.2014.444
   Vezhnevets A, 2012, PROC CVPR IEEE, P3162, DOI 10.1109/CVPR.2012.6248050
   Vijayanarasimhan S, 2012, LECT NOTES COMPUT SC, V7576, P496, DOI 10.1007/978-3-642-33715-4_36
   Wu ZY, 2015, PROC CVPR IEEE, P4194, DOI 10.1109/CVPR.2015.7299047
   Yang YC, 2015, IEEE I CONF COMP VIS, P4408, DOI 10.1109/ICCV.2015.501
   Zhang B., 2012, P 20 ACM INT C MULT, P801
   Zhang D, 2013, PROC CVPR IEEE, P628, DOI 10.1109/CVPR.2013.87
   Zhao L, 2005, MATH PROGRAM, V102, P167, DOI 10.1007/s10107-004-0510-2
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 57
TC 20
Z9 20
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2017
VL 19
IS 7
BP 1482
EP 1493
DI 10.1109/TMM.2017.2671447
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5ZN
UT WOS:000404061800007
DA 2024-07-18
ER

PT J
AU Li, J
   Lan, XG
   Li, XW
   Wang, J
   Zheng, NN
   Wu, Y
AF Li, Jin
   Lan, Xuguang
   Li, Xiangwei
   Wang, Jiang
   Zheng, Nanning
   Wu, Ying
TI Online Variable Coding Length Product Quantization for Fast Nearest
   Neighbor Search in Mobile Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Approximate nearest neighbor search; Gaussian mixture model;
   low-complexity quantizer design; variable coding length
AB Quantization methods are crucial for efficient nearest neighbor search in many applications such as image, music, or product search. As mobile devices are becoming increasingly more popular, the quantization methods on mobile devices are more important, because a large portion of the search queries are becoming performed on mobile devices. One important characteristic of the communication on mobile devices is the inherent unreliability of their communication channels. In order to adapt the quality changes of the communication channels, we need to change the coding length of the quantization accordingly. The existing quantization methods use fixed-length codebooks, and it is expensive to retrain another codebook with different coding length. In this paper, we propose a novel variable length product quantization framework that consists of a set of fast universal scalar quantizers. The framework is capable of producing variable length quantization without retraining the codebook. Each data vector is transformed into a new space to reduce the correlation across dimensions. A proper number of bits is allocated to represent the scalar component in each dimension according to the given coding length. For each component, we estimate its probability density function (PDF) and design an efficient universal scalar quantizer based on the PDF and the allocated bits. To reduce distortion, we learn a Gaussian mixture model for the data. The experimental results show that, compared to state-of-theart product quantization methods, our approach can construct the codebooks online for variable coding lengths and achieve the comparable performance.
C1 [Li, Jin; Lan, Xuguang; Li, Xiangwei] Xi An Jiao Tong Univ, Xian 710049, Peoples R China.
   [Wang, Jiang] Baidu Inst Deep Learning, Sunnyvale, CA 94089 USA.
   [Zheng, Nanning] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Dept Automat Sci & Technol, Xian 710049, Peoples R China.
   [Wu, Ying] Northwestern Univ, Evanston, IL 60208 USA.
C3 Xi'an Jiaotong University; Baidu; Xi'an Jiaotong University;
   Northwestern University
RP Lan, XG (corresponding author), Xi An Jiao Tong Univ, Xian 710049, Peoples R China.
EM j.lixjtu@gmail.com; xglan@mail.xjtu.edu.cn; m9zone@stu.xjtu.edu.cn;
   wangjiangb@gmail.com; nnzheng@mail.xjtu.edu.cn; yingwu@northwestern.edu
RI Wu, Yiping/JJF-6185-2023; Lan, Xuguang/N-8814-2019; wu,
   yiping/JEF-4104-2023; Wu, Ying/B-7283-2009
OI Wu, Yiping/0009-0000-6223-5786; Lan, Xuguang/0000-0002-3422-944X;
   Koochak, Atousa/0000-0001-6547-2728; Jin, Li/0000-0002-0260-3169
FU National Key Research and Development Program of China [2016YFB1000903];
   National Natural Science Foundation of China [61573268]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2016YFB1000903, and in part by
   the National Natural Science Foundation of China under Grant 61573268.
CR [Anonymous], P IEEE INT C MULT EX
   [Anonymous], CORR
   Avrithis Y, 2013, IEEE I CONF COMP VIS, P3024, DOI 10.1109/ICCV.2013.376
   Babenko A, 2015, PROC CVPR IEEE, P4240, DOI 10.1109/CVPR.2015.7299052
   Babenko A, 2014, PROC CVPR IEEE, P931, DOI 10.1109/CVPR.2014.124
   Babenko A, 2012, PROC CVPR IEEE, P3069, DOI 10.1109/CVPR.2012.6248038
   Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598
   Brandt J, 2010, PROC CVPR IEEE, P1815, DOI 10.1109/CVPR.2010.5539852
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Chen DM, 2015, IEEE T MULTIMEDIA, V17, P1019, DOI 10.1109/TMM.2015.2427744
   Chen J, 2012, INT CONF ACOUST SPEE, P965, DOI 10.1109/ICASSP.2012.6288045
   Cisco, 2016, CISC VIS NETW IND GL
   Denton E, 2014, ADV NEUR IN, V27
   Feng YJ, 2016, IEEE T IMAGE PROCESS, V25, P343, DOI 10.1109/TIP.2015.2500030
   Ge TZ, 2014, IEEE T PATTERN ANAL, V36, P744, DOI 10.1109/TPAMI.2013.240
   Girod B, 2011, IEEE MULTIMEDIA, V18, P86, DOI 10.1109/MMUL.2011.48
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gray RM, 1998, IEEE T INFORM THEORY, V44, P2325, DOI 10.1109/18.720541
   Hou J, 2016, IEEE T NEUR NET LEAR, V27, P1368, DOI 10.1109/TNNLS.2015.2461552
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Ji RR, 2012, INT J COMPUT VISION, V96, P290, DOI 10.1007/s11263-011-0472-9
   Jordan MI, 2015, SCIENCE, V349, P255, DOI 10.1126/science.aaa8415
   Kalantidis Y, 2014, PROC CVPR IEEE, P2329, DOI 10.1109/CVPR.2014.298
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Le A, 2016, IEEE ACM T NETWORK, V24, P2705, DOI 10.1109/TNET.2015.2501349
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li XX, 2013, J APPL MATH, DOI 10.1155/2013/245963
   LINDE Y, 1980, IEEE T COMMUN, V28, P84, DOI 10.1109/TCOM.1980.1094577
   Liu QF, 2015, PROC CVPR IEEE, P1329, DOI 10.1109/CVPR.2015.7298738
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388
   Obdr?zalek S., 2005, PROC 16 BRIT MACHINE, V1, P1
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Parkinson M B., 1997, Proceedings of the ASME Design Engineering Technical Conferences, 23rd Design Automation Conference, DETC97/DAC-3763, P1
   Pauleve L, 2010, PATTERN RECOGN LETT, V31, P1348, DOI 10.1016/j.patrec.2010.04.004
   Seetharam A, 2015, IEEE T MOBILE COMPUT, V14, P619, DOI 10.1109/TMC.2014.2331963
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Strecha C, 2012, IEEE T PATTERN ANAL, V34, P66, DOI 10.1109/TPAMI.2011.103
   Subramaniam AD, 2006, IEEE T AUDIO SPEECH, V14, P524, DOI 10.1109/TSA.2005.855839
   Sun JZ, 2009, IEEE INT SYMP INFO, P6, DOI 10.1109/ISIT.2009.5205695
   Tabrizi FM, 2013, IEEE T MOBILE COMPUT, V12, P995, DOI 10.1109/TMC.2012.56
   Torralba A., 2008, PROC IEEE C COMPUT V, P1
   Turpin A., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P11, DOI 10.1145/1148170.1148176
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Wang XJ, 2016, PROC CVPR IEEE, P2018, DOI 10.1109/CVPR.2016.222
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Xia Y, 2013, IEEE I CONF COMP VIS, P3416, DOI 10.1109/ICCV.2013.424
   Xu L, 1996, NEURAL COMPUT, V8, P129, DOI 10.1162/neco.1996.8.1.129
   Yang XY, 2012, INT CON DISTR COMP S, P92, DOI 10.1109/ICDCS.2012.51
   Yang XY, 2015, IEEE T IMAGE PROCESS, V24, P1709, DOI 10.1109/TIP.2015.2411433
   Zhang T, 2015, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2015.7299085
NR 53
TC 10
Z9 10
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2017
VL 19
IS 3
BP 559
EP 570
DI 10.1109/TMM.2016.2617089
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN2VW
UT WOS:000395869400011
DA 2024-07-18
ER

PT J
AU Mok, RKP
   Chang, RKC
   Li, WC
AF Mok, Ricky K. P.
   Chang, Rocky K. C.
   Li, Weichao
TI Detecting Low-Quality Workers in QoE Crowdtesting: A Worker
   Behavior-Based Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Crowdsourcing; cheater detection; QoE Crowdtesting; QoE worker behavior
AB QoE crowdtesting is increasingly popular among researchers to conduct subjective assessments of network services. Experimenters can easily access a huge pool of human subjects through crowdsourcing platforms. Without any supervision, lowquality workers, however, can threaten the reliability of the assessments. One of the approaches in classifying the quality of workers is to analyze their behavior during the experiments, such as mouse cursor trajectory. However, existing works analyze the trajectory coarsely, which cannot fully extract the imbedded information. In this paper, we propose a novel method to detect low-quality workers in QoE crowdtesting by analyzing the worker behavior. Our approach is to construct a predictive model by using supervised learning algorithms. A quality score is computed by applying existing anti-cheating techniques and human inspections to label the workers. We define a set of ten worker behavior metrics, which quantifies different types of worker behavior, including finer-grained cursor trajectory analysis. A multiclass Na " ive Bayes classifier is applied to train a model to predict the quality ofworkers from the metrics. We have conducted video QoE assessments on Amazon Mechanical Turk and CrowdFlower to collect the worker behavior. Our results show that the error rates of the model trained from four metrics are equal or less than 30%. We further find that combining the predictions from the four different 5-point Likert scale ratingmethods can improve the success rate in detecting lowquality workers to around 80%. Finally, our method is 16.5% and 42.9% better in precision and recall than CrowdMOS.
C1 [Mok, Ricky K. P.; Chang, Rocky K. C.; Li, Weichao] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Hong Kong, Peoples R China.
C3 Hong Kong Polytechnic University
RP Mok, RKP (corresponding author), Hong Kong Polytech Univ, Dept Comp, Hong Kong, Hong Kong, Peoples R China.
EM cs.rickymok@connect.polyu.hk; csrchang@comp.polyu.edu.hk;
   csweicli@comp.polyu.edu.hk
RI Li, Weichao/Q-8453-2018
OI CHANG, Kow Chuen Rocky/0000-0002-2648-5814
FU Hong Kong Polytechnic University under Grant G-YBAK; Joint Universities
   Computer Centre of Hong Kong [H-ZL17]
FX This work was supported in part by the Hong Kong Polytechnic University
   under Grant G-YBAK, and in part by the Joint Universities Computer
   Centre of Hong Kong under Grant H-ZL17.
CR [Anonymous], P INTERSPEECH
   [Anonymous], 2014, PROC ACM WORKSHOP QU
   [Anonymous], 1998, SUBJ AUD QUAL ASS ME
   [Anonymous], JUDGMENT DECISION MA
   [Anonymous], 2011, Image Processing (ICIP), 2011 18th IEEE International Conference on
   Arapakis I., 2014, Proceedings of the 23rd ACM International Conference on Information and Knowledge Management, P1439, DOI [DOI 10.1145/2661829, DOI 10.1145/2661829.2661909]
   Arapakis I, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P599, DOI 10.1145/2911451.2911505
   Chen KT, 2010, IEEE NETWORK, V24, P28, DOI 10.1109/MNET.2010.5430141
   Chen M. C., 2001, CHI 01 HUM FACT COMP, P281, DOI DOI 10.1145/634067.634234
   Costagliola G., 2007, P 6 INT C ADV WEB BA, P452
   Demartini G., 2012, P 21 INT C WORLD WID, P469
   Difallah D.E., 2012, CROWDSEARCH 2012 WOR
   Downs JS, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2399
   Eickhoff C, 2013, INFORM RETRIEVAL, V16, P121, DOI 10.1007/s10791-011-9181-9
   Eickhoff C, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P871, DOI 10.1145/2348283.2348400
   Freeman JB, 2010, BEHAV RES METHODS, V42, P226, DOI 10.3758/BRM.42.1.226
   Gardlo B, 2014, IEEE ICC, P1070, DOI 10.1109/ICC.2014.6883463
   GitHub Inc, 2014, RATEIT PLUB JQUERY
   Guo Q., 2012, Proceedings of the International Conference on Information and Knowledge Management, P2050, DOI [10.1145/2396761.2398570, DOI 10.1145/2396761.2398570]
   Guo Q., 2012, PROC 21 INT C WORLD, P569, DOI DOI 10.1145/2187836.2187914
   Guo Q., 2008, P 31 ANN INT ACM SIG, P107
   Hirth M, 2015, COMPUT NETW, V90, P85, DOI 10.1016/j.comnet.2015.07.003
   Hirth M, 2014, 2014 IEEE FIFTH INTERNATIONAL CONFERENCE ON COMMUNICATIONS AND ELECTRONICS (ICCE), P510, DOI 10.1109/CCE.2014.6916756
   Hossfeld Tobias, 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P494, DOI 10.1109/ISM.2011.87
   Hossfeld T, 2014, IEEE T MULTIMEDIA, V16, P541, DOI 10.1109/TMM.2013.2291663
   Huang J, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1225
   Hwang F, 2005, BEHAV INFORM TECHNOL, V24, P205, DOI 10.1080/01449290412331327474
   Joglekar M, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P686
   Kaza G, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P267, DOI 10.1145/2835776.2835835
   Keimel C., 2012, 2012 Proceedings of the 19th International Packet Video Workshop (PV 2012), P155, DOI 10.1109/PV.2012.6229729
   Keimel C, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P245, DOI 10.1109/PCS.2012.6213338
   Kraft Sebastian, 2014, P LINUX AUD C
   MEYER DE, 1988, PSYCHOL REV, V95, P340, DOI 10.1037/0033-295X.95.3.340
   Mok R.K., 2012, P 3 MULTIMEDIA SYSTE, P11
   Mok RKP, 2016, IEEE J SEL AREA COMM, V34, P1914, DOI 10.1109/JSAC.2016.2559078
   Mok RKP, 2015, 2015 IEEE 23RD INTERNATIONAL SYMPOSIUM ON QUALITY OF SERVICE (IWQOS), P201, DOI 10.1109/IWQoS.2015.7404734
   Navalpakkam Vidhya., 2012, P SIGCHI C HUMAN FAC, P2963, DOI [10.1145/2207676.2208705, DOI 10.1145/2207676.2208705]
   Phillips JG, 2001, ERGONOMICS, V44, P527, DOI 10.1080/00140130121560
   Rainer B., 2013, P 5 INT WORKSH QUAL
   Redi J., 2013, Proceedings of the 2nd ACM international workshop on Crowdsourcing for multimedia (CrowdMM '13), P29
   Rennie JD. M., 1973, Proceedings of the Twentieth International Conference on Machine Learning (ICML)-2003), V20, P616, DOI DOI 10.1186/1477-3155-8-16
   Ribeiro F, 2011, INT CONF ACOUST SPEE, P2416
   Rzeszotarski JM, 2012, UIST'12: PROCEEDINGS OF THE 25TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P55
   Rzeszotarski Jeffrey M., 2011, P 24 ANN ACM S US IN, P1, DOI 10.1145/2047196.2047199
   Salas O., 2013, Proceedings of the 2nd ACM international workshop on Crowdsourcing for multimedia (CrowdMM '13), P23
   Soberon G., 2013, P 1 INT C CROWDSOURC, V1030, P45
   Soler M. D., 2010, Albeitar, P4
   Song JH, 2009, TRENDS COGN SCI, V13, P360, DOI 10.1016/j.tics.2009.04.009
   Wu CC, 2013, IEEE T MULTIMEDIA, V15, P1121, DOI 10.1109/TMM.2013.2241043
   Yen Y-C, 2013, P 9 AS INT ENG C, P65
NR 50
TC 17
Z9 21
U1 1
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2017
VL 19
IS 3
BP 530
EP 543
DI 10.1109/TMM.2016.2619901
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA EN2VW
UT WOS:000395869400009
DA 2024-07-18
ER

PT J
AU Kumano, S
   Otsuka, K
   Ishii, R
   Yamato, J
AF Kumano, Shiro
   Otsuka, Kazuhiro
   Ishii, Ryo
   Yamato, Junji
TI Collective First-Person Vision for Automatic Gaze Analysis in Multiparty
   Conversations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Conversation; egocentric vision; eye contact; first-person vision; gaze;
   head pose estimation; mutual gaze; self-calibration; speaker
   diarization; speech; wearable camera
ID HEAD POSE; JUDGMENTS; ATTENTION; EMPATHY; FOCUS
AB This paper targets small-to medium-sized-group face-to-face conversations where each person wears a dual-view camera, consisting of inward-and outward-looking cameras, and presents an almost fully automatic but accurate ofline gaze analysis framework that does not require users to perform any calibration steps. Our collective first-person vision framework, where captured audio-visual signals are gathered and processed in a centralized system, jointly undertakes the fundamental functions required for group gaze analysis, including speaker detection, face tracking, and gaze tracking. Of particular note is our self-calibration of gaze trackers by exploiting a general conversation rule, namely that listeners are likely to look at the speaker. From the rough conversational prior knowledge, our system visualizes fine-grained participants' gaze behavior as a gazee-centered heat map, which quantitatively reveals what parts of the gazee's body the participant looked at and for how long while the gazer was speaking or listening. An experiment using conversations amounting to a total of 140 min, each lasting an average of 8.7 min and engaged in by 37 participants in groups of three to six, achieves a mean absolute error of 2.8. in gaze tracking. A statistical test reveals neither a group size effect nor a conversation type effect. Our method achieves F-scores of over 0.89 and 0.87 in gazee and eye contact recognition, respectively, in comparison with human annotation.
C1 [Kumano, Shiro; Otsuka, Kazuhiro; Ishii, Ryo; Yamato, Junji] NTT Corp, Yokosuka, Kanagawa 2430198, Japan.
   [Yamato, Junji] Kogakuin Univ, Tokyo 1638677, Japan.
C3 Nippon Telegraph & Telephone Corporation; Kogakuin University
RP Kumano, S (corresponding author), NTT Corp, Yokosuka, Kanagawa 2430198, Japan.
EM kumano@ieee.org; kazuhiro.otsuka@lab.ntt.co.jp; ishii.ryo@lab.ntt.co.jp;
   yamato@cc.kogakuin.ac.jp
RI Otsuka, Kazuhiro/AAP-7814-2020; Yamato, Junji/R-2851-2019
OI Otsuka, Kazuhiro/0000-0003-4352-3955; Yamato, Junji/0000-0003-1634-7984
CR Alnajar F, 2013, IEEE I CONF COMP VIS, P137, DOI 10.1109/ICCV.2013.24
   [Anonymous], 2010, SURVEY RECENT ADV FA
   Arev I, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601198
   Ba SO, 2011, IEEE T PATTERN ANAL, V33, P101, DOI 10.1109/TPAMI.2010.69
   Baltruaitis T, 2016, 2016 IEEE WINT C APP, DOI [DOI 10.1109/WACV.2016.7477553, 10.1109/WACV.2016.7477553]
   BEATTY J, 1982, PSYCHOL BULL, V91, P276, DOI 10.1037/0033-2909.91.2.276
   Betancourt A, 2015, IEEE T CIRC SYST VID, V25, P744, DOI 10.1109/TCSVT.2015.2409731
   Bojko A, 2009, LECT NOTES COMPUT SC, V5610, P30, DOI 10.1007/978-3-642-02574-7_4
   Chen F, 2011, IEEE T MULTIMEDIA, V13, P1381, DOI 10.1109/TMM.2011.2166379
   Chen JX, 2011, PROC CVPR IEEE, P609, DOI 10.1109/CVPR.2011.5995675
   Chen YX, 2004, IEEE INT CONF ROBOT, P243
   COHEN J, 1992, PSYCHOL BULL, V112, P155, DOI 10.1037/0033-2909.112.1.155
   Coutrot A, 2014, J VISION, V14, DOI 10.1167/14.8.5
   D'Argembeau A, 2003, APPL COGNITIVE PSYCH, V17, P281, DOI 10.1002/acp.856
   DAVIS MH, 1983, J PERS SOC PSYCHOL, V44, P113, DOI 10.1037/0022-3514.44.1.113
   Dong W, 2013, IEEE T MULTIMEDIA, V15, P83, DOI 10.1109/TMM.2012.2225039
   DOVIDIO JF, 1982, SOC PSYCHOL QUART, V45, P106, DOI 10.2307/3033933
   el Kaliouby R, 2006, ANN NY ACAD SCI, V1093, P228, DOI 10.1196/annals.1382.016
   Fathi A, 2012, PROC CVPR IEEE, P1226, DOI 10.1109/CVPR.2012.6247805
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   FLEISS JL, 1973, EDUC PSYCHOL MEAS, V33, P613, DOI 10.1177/001316447303300309
   FLEISS JL, 1971, PSYCHOL BULL, V76, P378, DOI 10.1037/h0031619
   Gao T., 2014, 025 CTR BRAINS MINDS
   Gatica-Perez D., 2006, 2006 IEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (IEEE Cat. No. 06TH8908), P41, DOI 10.1109/MFI.2006.265658
   Goodwin C., 1981, CONVERSATIONAL ORG I
   Gorga S., 2010, P INT C MULT INT WOR
   Hall Edward Twitchell, 1966, HIDDEN DIMENSION
   Hansen DW, 2010, IEEE T PATTERN ANAL, V32, P478, DOI 10.1109/TPAMI.2009.30
   Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246
   HELLSTROM A, 1994, EUR J SOC PSYCHOL, V24, P693, DOI 10.1002/ejsp.2420240606
   Hughes C, 2010, IEEE T PATTERN ANAL, V32, P2289, DOI 10.1109/TPAMI.2010.159
   Itti L, 2000, VISION RES, V40, P1489, DOI 10.1016/S0042-6989(99)00163-7
   Kakumanu P, 2007, PATTERN RECOGN, V40, P1106, DOI 10.1016/j.patcog.2006.06.010
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kanade T, 2012, P IEEE, V100, P2442, DOI 10.1109/JPROC.2012.2200554
   KENDON A, 1967, ACTA PSYCHOL, V26, P22, DOI 10.1016/0001-6918(67)90005-4
   Kumano S., 2015, P INT WORKSH EMOSPAC
   Kumano S, 2015, IEEE T AFFECT COMPUT, V6, P324, DOI 10.1109/TAFFC.2015.2417561
   Kumano S, 2009, INT J COMPUT VISION, V83, P178, DOI 10.1007/s11263-008-0185-x
   Larsen RJ, 1996, PERS INDIV DIFFER, V21, P907, DOI 10.1016/S0191-8869(96)00148-1
   Nguyen LS, 2014, IEEE T MULTIMEDIA, V16, P1018, DOI 10.1109/TMM.2014.2307169
   Li Y, 2013, IEEE I CONF COMP VIS, P3216, DOI 10.1109/ICCV.2013.399
   Loy G, 2003, IEEE T PATTERN ANAL, V25, P959, DOI 10.1109/TPAMI.2003.1217601
   Marcos-Ramiro A, 2015, IEEE T MULTIMEDIA, V17, P1721, DOI 10.1109/TMM.2015.2464152
   Martin-Laurent F., 2013, IN PRESS, P1, DOI [10.1007/s11356-013-1839-y,inpress, DOI 10.1007/S11356-013-1839-Y,INPRESS]
   Milan A, 2014, IEEE T PATTERN ANAL, V36, P58, DOI 10.1109/TPAMI.2013.103
   Minotto VP, 2015, IEEE T MULTIMEDIA, V17, P1694, DOI 10.1109/TMM.2015.2463722
   MIYAMOTO K, 1964, J OPT SOC AM, V54, P1060, DOI 10.1364/JOSA.54.001060
   Mora KAF, 2013, IEEE IMAGE PROC, P2787, DOI 10.1109/ICIP.2013.6738574
   Murphy-Chutorian E, 2009, IEEE T PATTERN ANAL, V31, P607, DOI 10.1109/TPAMI.2008.106
   Nihonyanagi S., 2014, P WORKSH GAZ, P33
   Okada S, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P15, DOI 10.1145/2818346.2820757
   Otsuka K, 2011, IEEE SIGNAL PROC MAG, V28, P127, DOI 10.1109/MSP.2011.941100
   Panero J., 1979, HUMAN DIMENSION INTE
   Park H. S., 2012, Advances in Neural Information Processing Systems, V1, P422
   Park HS, 2015, PROC CVPR IEEE, P4777, DOI 10.1109/CVPR.2015.7299110
   Partala T, 2003, INT J HUM-COMPUT ST, V59, P185, DOI 10.1016/S1071-5819(03)00017-X
   Perra D, 2015, PROC CVPR IEEE, P4146, DOI 10.1109/CVPR.2015.7299042
   Poole A., 2004, ENCY HUMAN COMPUTER
   Salvucci D.D., 2000, P 2000 S EYE TRACK R, P71, DOI [DOI 10.1145/355017.355028, 10.1145/355017.355028]
   Schwiegerling J., 2004, FIELD GUIDE VISUAL O
   Sokolova M, 2009, INFORM PROCESS MANAG, V45, P427, DOI 10.1016/j.ipm.2009.03.002
   Soleymani M, 2014, IEEE T MULTIMEDIA, V16, P1075, DOI 10.1109/TMM.2014.2305573
   Speer LL, 2007, AUTISM, V11, P265, DOI 10.1177/1362361307076925
   Stiefelhagen R, 2002, IEEE T NEURAL NETWOR, V13, P928, DOI 10.1109/TNN.2002.1021893
   Stiefelhagen R., 2002, THESIS
   SUBRAMANIAN R, 2013, ICM P 2013 ACM INT, P3, DOI DOI 10.1145/2522848.2522862
   Sugano Y, 2013, IEEE T PATTERN ANAL, V35, P329, DOI 10.1109/TPAMI.2012.101
   Tsukada A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P2084, DOI 10.1109/ICCVW.2011.6130505
   Valenti R, 2012, IEEE T PATTERN ANAL, V34, P1785, DOI 10.1109/TPAMI.2011.251
   Vertegaal R., 1998, THESIS
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang YJ, 2001, PATTERN RECOGN, V34, P1983, DOI 10.1016/S0031-3203(00)00119-9
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Ye Z, 2012, UBICOMP 12 P 2012 AC
   Yonetani R, 2016, PROC CVPR IEEE, P2629, DOI 10.1109/CVPR.2016.288
   Zhang X., 2015, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, DOI 10.1109/CVPR.2015.7299081
NR 77
TC 9
Z9 9
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2017
VL 19
IS 1
BP 107
EP 122
DI 10.1109/TMM.2016.2608002
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA EH0SX
UT WOS:000391475200009
DA 2024-07-18
ER

PT J
AU Zhang, LY
   Wong, KW
   Zhang, YS
   Zhou, JT
AF Zhang, Leo Yu
   Wong, Kwok-Wo
   Zhang, Yushu
   Zhou, Jiantao
TI Bi-level Protected Compressive Sampling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compressive sampling (CS); encryption; known/chosen-plaintext attack;
   random projection; restricted isometry property (RIP)
ID RESTRICTED ISOMETRY PROPERTY; IMAGE ENCRYPTION; PRIVACY PROTECTION;
   OPTICAL ENCRYPTION; SIGNAL RECOVERY; SECURITY; SECRECY; ATTACKS
AB Some pioneering works have investigated embedding cryptographic properties in compressive sampling (CS) in a way similar to one-time pad symmetric cipher. This paper tackles the problem of constructing a CS-based symmetric cipher under the key reuse circumstance, i.e., the cipher is resistant to common attacks even when a fixed measurement matrix is used multiple times. To this end, we suggest a bi-level protected CS (BLPCS) model which makes use of the advantage of measurement matrix construction without restricted isometry property (RIP). Specifically, two kinds of artificial basis mismatch techniques are investigated to construct key-related sparsifying bases. It is demonstrated that the encoding process of BLP-CS is simply a random linear projection, which is the same as the basic CS model. However, decoding the linear measurements requires knowledge of both the key-dependent sensing matrix and its sparsifying basis. The proposed model is exemplified by sampling images as a joint data acquisition and protection layer for resource-limited wireless sensors. Simulation results and numerical analyses have justified that the new model can be applied in circumstances where the measurement matrix can be reused.
C1 [Zhang, Leo Yu; Wong, Kwok-Wo] City Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
   [Zhang, Yushu] Southwest Univ, Sch Elect & Informat Engn, Chongqing 400715, Peoples R China.
   [Zhou, Jiantao] Univ Macau, Dept Comp & Informat Sci, Fac Sci & Technol, Macau 999078, Peoples R China.
C3 City University of Hong Kong; Southwest University - China; University
   of Macau
RP Zhang, LY (corresponding author), City Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
EM tyu@gmail.com; itkwwong@cityu.edu.hk; yushuboshi@163.com; jtzhou@umac.mo
RI Zhang, Leo Yu/K-2043-2013
OI Zhang, Leo Yu/0000-0001-9330-2662
FU Macau Science and Technology Development Fund [FDCT/009/2013/A1,
   FDCT/046/2014/A1]; Research Committee within the University of Macau
   [MRG007/ZJT/2015/FST, MRG021/ZJT/2013/FST, MYRG2014-00031-FST,
   MYRG2015-00056-FST]; National Natural Science Foundation of China
   [61502399, 61402547, 61572089, U1536204]
FX This work was supported in part by the Macau Science and Technology
   Development Fund under Grant FDCT/009/2013/A1 and Grant
   FDCT/046/2014/A1, in part by the Research Committee within the
   University of Macau under Grant MRG007/ZJT/2015/FST, Grant
   MRG021/ZJT/2013/FST, Grant MYRG2014-00031-FST, and Grant
   MYRG2015-00056-FST, and in part by the National Natural Science
   Foundation of China under Grant 61502399, Grant 61402547, Grant
   61572089, and Grant U1536204. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Shahram Shirani.
CR [Anonymous], 2011, VISUAL COMMUN-US, DOI DOI 10.1109/VCIP.2011.6115917
   [Anonymous], 2005, SIGNALS SYSTEMS COMP, DOI DOI 10.1109/ACSSC.2005.1600024
   [Anonymous], P IEEE INT C AC SPEE
   Baraniuk R, 2008, CONSTR APPROX, V28, P253, DOI 10.1007/s00365-007-9003-x
   Baraniuk RG, 2007, IEEE SIGNAL PROC MAG, V24, P6, DOI 10.1109/MSP.2007.909718
   Barrows RC, 1996, J AM MED INFORM ASSN, V3, P139, DOI 10.1136/jamia.1996.96236282
   Boyd S., 2004, CONVEX OPTIMIZATION
   Cambareri V, 2015, IEEE T INF FOREN SEC, V10, P2182, DOI 10.1109/TIFS.2015.2450676
   Cambareri V, 2015, IEEE T SIGNAL PROCES, V63, P2183, DOI 10.1109/TSP.2015.2407315
   Cambareri V, 2013, IEEE INT SYMP CIRC S, P1356, DOI 10.1109/ISCAS.2013.6572106
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   Candès EJ, 2011, IEEE T INFORM THEORY, V57, P7235, DOI 10.1109/TIT.2011.2161794
   Cariolaro G, 2002, IEEE T SIGNAL PROCES, V50, P902, DOI 10.1109/78.992138
   Carnicer A, 2005, OPT LETT, V30, P1644, DOI 10.1364/OL.30.001644
   Clemente P, 2013, OPT LETT, V38, P2524, DOI 10.1364/OL.38.002524
   Dautov R., 2013, 2013 International Conference on Computing, Networking and Communications (ICNC 2013), P354, DOI 10.1109/ICCNC.2013.6504108
   Deepan B, 2014, APPL OPTICS, V53, P4539, DOI 10.1364/AO.53.004539
   Dimakis AG, 2009, ANN ALLERTON CONF, P8, DOI 10.1109/ALLERTON.2009.5394826
   Do TT, 2012, IEEE T SIGNAL PROCES, V60, P139, DOI 10.1109/TSP.2011.2170977
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Duarte MF, 2008, IEEE SIGNAL PROC MAG, V25, P83, DOI 10.1109/MSP.2007.914730
   Dufaux F, 2008, IEEE T CIRC SYST VID, V18, P1168, DOI 10.1109/TCSVT.2008.928225
   Engel D., 2005, P 7 WORKSH MULT SEC, P63
   Fang H, 2014, IEEE T SIGNAL PROCES, V62, P196, DOI 10.1109/TSP.2013.2284762
   Fowler JE, 2011, EUR SIGNAL PR CONF, P564
   Frauel Y., 2007, OPT EXPRESS, V15
   Gan L, 2007, PROCEEDINGS OF THE 2007 15TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, P403
   Huang XL, 2015, SECUR COMMUN NETW, V8, P3659, DOI 10.1002/sec.1289
   JAVIDI B, 1999, Patent No. 5903648
   Kueng R, 2014, LINEAR ALGEBRA APPL, V441, P110, DOI 10.1016/j.laa.2013.04.018
   Laska JN, 2011, APPL COMPUT HARMON A, V31, P429, DOI 10.1016/j.acha.2011.02.002
   Li J, 2015, SCI REP-UK, V5, DOI 10.1038/srep10374
   Liu HX, 2014, IEEE T MULTIMEDIA, V16, P1549, DOI 10.1109/TMM.2014.2328324
   Liu XY, 2013, OPTIK, V124, P6590, DOI 10.1016/j.ijleo.2013.05.092
   Lustig M, 2007, MAGN RESON MED, V58, P1182, DOI 10.1002/mrm.21391
   Mun S, 2012, EUR SIGNAL PR CONF, P1424
   Pande A, 2013, IEEE MULTIMEDIA, V20, P50, DOI 10.1109/MMUL.2012.29
   Pande A, 2012, J REAL-TIME IMAGE PR, V7, P131, DOI 10.1007/s11554-010-0165-6
   Rachlin Y, 2008, ANN ALLERTON CONF, P813, DOI 10.1109/ALLERTON.2008.4797641
   Rawat N, 2015, APPL OPTICS, V54, P1782, DOI 10.1364/AO.54.001782
   REFREGIER P, 1995, OPT LETT, V20, P767, DOI 10.1364/OL.20.000767
   Rivenson Y, 2010, J DISP TECHNOL, V6, P506, DOI 10.1109/JDT.2010.2042276
   SHANNON CE, 1949, BELL SYST TECH J, V28, P656, DOI 10.1002/j.1538-7305.1949.tb00928.x
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Unnikrishnan G, 2000, OPT LETT, V25, P887, DOI 10.1364/OL.25.000887
   Venturini I, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P205
   Winkler T, 2014, ACM COMPUT SURV, V47, DOI 10.1145/2545883
   Wu X., 2014, CORR
   Yang ZY, 2015, IEEE T EMERG TOP COM, V3, P363, DOI 10.1109/TETC.2014.2372151
   YEUNG SKA, 2012, P IEEE INT C IM PROC, P2637, DOI DOI 10.1109/ICIP.2012.6467440
   Zeng B, 2014, IEEE T INF FOREN SEC, V9, P309, DOI 10.1109/TIFS.2013.2293955
   Zeng L, 2012, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2012-257
   Zeng WJ, 2003, IEEE T MULTIMEDIA, V5, P118, DOI 10.1109/TMM.2003.808817
   Zhang LY, 2015, IEEE INT SYMP CIRC S, P2744, DOI 10.1109/ISCAS.2015.7169254
   Zhang YS, 2016, NEUROCOMPUTING, V205, P472, DOI 10.1016/j.neucom.2016.04.053
   Zhao J, 2003, P 1 INT C EMB NETW S, P1, DOI [DOI 10.1145/958491.958493, 10.1145/958491.958493]
NR 58
TC 72
Z9 73
U1 1
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2016
VL 18
IS 9
BP 1720
EP 1732
DI 10.1109/TMM.2016.2581593
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DT4GI
UT WOS:000381437800004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Meng, JJ
   Yuan, JS
   Yang, J
   Wang, G
   Tan, YP
AF Meng, Jingjing
   Yuan, Junsong
   Yang, Jiong
   Wang, Gang
   Tan, Yap-Peng
TI Object Instance Search in Videos via Spatio-Temporal Trajectory
   Discovery
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object instance search; spatio-temporal trajectory; video
ID LOCALIZATION; DESCRIPTORS
AB Given a specific object as query, object instance search aims to not only retrieve the images or frames that contain the query, but also locate all its occurrences. In this work, we explore the use of spatio-temporal cues to improve the quality of object instance search from videos. To this end, we formulate this problem as the spatio-temporal trajectory search problem, where a trajectory is a sequence of bounding boxes that locate the object instance in each frame. The goal is to find the top-trajectories that are likely to contain the target object. Despite the large number of trajectory candidates, we build on a recent spatio-temporal search algorithm for event detection to efficiently find the optimal spatio-temporal trajectories in large video volumes, with complexity linear to the video volume size. We solve the key bottleneck in applying this approach to object instance search by leveraging a randomized approach to enable fast scoring of any bounding boxes in the video volume. In addition, we present a new dataset for video object instance search. Experimental results on a 73-hour video dataset demonstrate that our approach improves the performance of video object instance search and localization over the state-of-the-art search and tracking methods.
C1 [Meng, Jingjing; Yuan, Junsong; Yang, Jiong; Wang, Gang; Tan, Yap-Peng] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Meng, JJ (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM jingjing.meng@ntu.edu.sg; jsyuan@ntu.edu.sg; yang0374@e.ntu.edu.sg;
   wanggang@ntu.edu.sg; eyptan@ntu.edu.sg
RI Yuan, Junsong/A-5171-2011; Tan, Yap-Peng/A-5158-2011; Yuan,
   Junsong/R-4352-2019; meng, jingjing/HDM-6615-2022
OI meng, jingjing/0000-0002-8515-6893; Yuan, Junsong/0000-0002-7901-8793
FU National Research Foundation, Singapore, under its Interactive Digital
   Media (IDM) Strategic Research Programme
FX This research was carried out at the Rapid-Rich Object Search (ROSE) Lab
   at the Nanyang Technological University, Singapore. The ROSE Lab is
   supported by the National Research Foundation, Singapore, under its
   Interactive Digital Media (IDM) Strategic Research Programme.
CR Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   [Anonymous], 2013, P 2013 IEEE PES AS P
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], P 2 ACM INT C MULT R
   [Anonymous], P TRECVID WORKSH
   [Anonymous], P EUR C COMPUT VIS
   [Anonymous], P TRECVID
   [Anonymous], 2007, IEEE C COMP VIS PATT
   [Anonymous], 2012, P 20 ACM INT C MULTI
   [Anonymous], 2014, P TRECVID
   [Anonymous], P EUR C COMPUT VIS
   [Anonymous], 2010, ONLINE MULTIMEDIA AD
   [Anonymous], 2015, P 6 ACM MULTIMEDIA S
   [Anonymous], P 22 IEEE I IN PRESS
   [Anonymous], P 22 IEEE I IN PRESS
   Arandjelovic R, 2013, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR.2013.207
   Araujo A, 2014, IEEE IMAGE PROC, P3082, DOI 10.1109/ICIP.2014.7025623
   Cai-Zhi Zhu, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4304, DOI 10.1109/ICASSP.2014.6854414
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951
   Tran D, 2014, IEEE T PATTERN ANAL, V36, P404, DOI 10.1109/TPAMI.2013.137
   Du Tran, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3321, DOI 10.1109/CVPR.2011.5995416
   Duan LY, 2014, IEEE MULTIMEDIA, V21, P30, DOI 10.1109/MMUL.2013.66
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hosang J.H., 2014, CORR
   Jain M, 2014, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2014.100
   Jiang YN, 2015, IEEE T IMAGE PROCESS, V24, P1748, DOI 10.1109/TIP.2015.2405337
   Jiang YN, 2012, PROC CVPR IEEE, P3100, DOI 10.1109/CVPR.2012.6248042
   Kawanishi T., 2010, PROC TRECVID, P1
   Lampert C. H., 2008, PROC IEEE C COMPUT V, P1
   Lampert CH, 2009, IEEE I CONF COMP VIS, P987, DOI 10.1109/ICCV.2009.5459359
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Meng J., 2010, P INT C ACM MULT, P1147
   Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331
   Perd'och M, 2009, PROC CVPR IEEE, P9, DOI 10.1109/CVPRW.2009.5206529
   Perronnin F., 2007, P IEEE CVPR, P1
   Perronnin F, 2010, PROC CVPR IEEE, P3384, DOI 10.1109/CVPR.2010.5540009
   Philbin J., 2008, P CVPR, P1
   Prest A, 2012, PROC CVPR IEEE, P3282, DOI 10.1109/CVPR.2012.6248065
   Shen XH, 2012, PROC CVPR IEEE, P3013, DOI 10.1109/CVPR.2012.6248031
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Sivic J, 2009, IEEE T PATTERN ANAL, V31, P591, DOI 10.1109/TPAMI.2008.111
   Tao R, 2014, PROC CVPR IEEE, P2099, DOI 10.1109/CVPR.2014.269
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Yang Y, 2013, IEEE IMAGE PROC, P3972, DOI 10.1109/ICIP.2013.6738818
   Yuen J, 2009, IEEE I CONF COMP VIS, P1451, DOI 10.1109/ICCV.2009.5459289
NR 49
TC 27
Z9 28
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2016
VL 18
IS 1
BP 116
EP 127
DI 10.1109/TMM.2015.2500734
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CZ5JW
UT WOS:000367139700011
DA 2024-07-18
ER

PT J
AU Sapru, A
   Bourlard, H
AF Sapru, Ashtosh
   Bourlard, Herve
TI Automatic Recognition of Emergent Social Roles in Small Group
   Interactions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Conditional random fields; crowdsourcing; small group interactions;
   social roles
ID FUNCTIONAL ROLES; PERSONALITY; DYNAMICS
AB This paper investigates the automatic recognition of social roles that emerge naturally in small groups. These roles represent a flexible classification scheme that can generalize across different scenarios of small group interaction. We systematically investigate various verbal and non-verbal cues extracted from turn-taking patterns, vocal expression, and linguistic style to model speakers behavior. The influence of social roles on the behavior cues exhibited by a speaker is modeled using a discriminative approach based on conditional random fields. Experiments performed on several hours of meeting data reveal that social role recognition using conditional random fields achieves an accuracy of 74% in classifying four social roles and outperforms the baseline method on all social role categories. Furthermore, we also demonstrate the effectiveness of our approach by evaluating it on previously unseen scenarios of small group interactions.
C1 [Sapru, Ashtosh; Bourlard, Herve] Idiap Res Inst, CH-1920 Martigny, Switzerland.
   [Sapru, Ashtosh; Bourlard, Herve] Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Sapru, A (corresponding author), Idiap Res Inst, CH-1920 Martigny, Switzerland.
EM asapru@idiap.ch; bourlard@idiap.ch
CR AMBADY N, 1992, PSYCHOL BULL, V111, P256, DOI 10.1037/0033-2909.111.2.256
   [Anonymous], 2003, P EUROSPEECH
   [Anonymous], 2008, Proc. ACM Multimedia, DOI [10.1145/1459359.1459462, DOI 10.1145/1459359.1459462]
   [Anonymous], P 12 ANN C INT SPEEC
   [Anonymous], 2004, Advances in Neural Information Processing Systems (NIPS)
   Bales R.F., 1970, PERSONALITY INTERPER
   Bales RF, 1950, AM SOCIOL REV, V15, P257, DOI 10.2307/2086790
   Banerjee S., 2004, Proc. Int. Conf. Spoken Language Processing, P2189
   Barzilay R, 2000, SEVENTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2001) / TWELFTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-2000), P679
   Benne KD, 1948, J SOC ISSUES, V4, P41, DOI 10.1111/j.1540-4560.1948.tb01783.x
   Biddle B. J., 2013, Role theory: Expectations, identities, and behaviors
   Carletta J, 1996, COMPUT LINGUIST, V22, P249
   Carletta J, 2007, LANG RESOUR EVAL, V41, P181, DOI 10.1007/s10579-007-9040-x
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Damnati G, 2011, INT CONF ACOUST SPEE, P5684
   Demsar J, 2006, J MACH LEARN RES, V7, P1
   Dong W, 2013, IEEE T MULTIMEDIA, V15, P83, DOI 10.1109/TMM.2012.2225039
   Eyben Florian, 2010, P 18 ACM INT C MULT, P1459
   Favre S., 2009, P 17 ACM INT C MULT, P585
   FLEISS JL, 1971, PSYCHOL BULL, V76, P378, DOI 10.1037/h0031619
   Gatica-Perez D, 2009, IMAGE VISION COMPUT, V27, P1775, DOI 10.1016/j.imavis.2009.01.004
   Gonzales AL, 2010, COMMUN RES, V37, P3, DOI 10.1177/0093650209351468
   Gunawardana Asela., 2005, Proceedings of Nineth European Conference on Speech Communication and Technology (EuroSpeech 2005), P1117
   Hain T., 2006, P INTERSPEECH, P17
   Hain T, 2007, INT CONF ACOUST SPEE, P357
   HARE AP, 1994, SMALL GR RES, V25, P433, DOI 10.1177/1046496494253005
   Hutchinson B, 2010, INT CONF ACOUST SPEE, P5322, DOI 10.1109/ICASSP.2010.5494958
   Janin A, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P364
   Jayagopi DB, 2010, IEEE T MULTIMEDIA, V12, P790, DOI 10.1109/TMM.2010.2065218
   Knapp M.L., 2005, Nonverbal communication in human interaction, V6th
   Laskowski Kornel., 2008, P ISCA ACL SIGDIAL W, P148
   LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116
   Liu Y., 2006, Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, P81
   Mead G. H., 1934, MIND SELF SOC, V111
   Mehl MR, 2006, J PERS SOC PSYCHOL, V90, P862, DOI 10.1037/0022-3514.90.5.862
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Pennebaker JW, 2003, ANNU REV PSYCHOL, V54, P547, DOI 10.1146/annurev.psych.54.101601.145041
   Polzehl T, 2010, IEEE INT C SEMANT CO, P134, DOI 10.1109/ICSC.2010.41
   SACKS H, 1974, LANGUAGE, V50, P696, DOI 10.2307/412243
   Salamin H, 2009, IEEE T MULTIMEDIA, V11, P1373, DOI 10.1109/TMM.2009.2030740
   Sanchez-Cortes D., 2012, P 11 INT C MOB UB MU
   Sapru A, 2013, INTERSPEECH, P1529
   Sapru A, 2013, INT CONF AFFECT, P324, DOI 10.1109/ACII.2013.60
   Sapru A, 2012, INT CONF ACOUST SPEE, P5057, DOI 10.1109/ICASSP.2012.6289057
   Schuller B, 2013, INTERSPEECH, P148
   Slater PE, 1955, AM SOCIOL REV, V20, P300, DOI 10.2307/2087389
   Vinciarelli A, 2006, INT C PATT RECOG, P1154
   Vinciarelli Alessandro., 2007, Proceedings of the 15th international conference on Multimedia, MULTIMEDIA '07, P261, DOI DOI 10.1145/1291233.1291287
   Weng CY, 2009, IEEE T MULTIMEDIA, V11, P256, DOI 10.1109/TMM.2008.2009684
   Weninger F, 2012, IEEE T AFFECT COMPUT, V3, P496, DOI 10.1109/T-AFFC.2012.15
   Wilson T., 2011, P 16 INT C INT US IN, P419
   Yaman S, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4, P2870
   Zancanaro M., 2006, Proceedings of the 8th international conference on Multimodal interfaces, P28
NR 53
TC 23
Z9 25
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2015
VL 17
IS 5
BP 746
EP 760
DI 10.1109/TMM.2015.2408437
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA CG3AH
UT WOS:000353148300015
DA 2024-07-18
ER

PT J
AU Weng, CQ
   Yuan, JS
AF Weng, Chaoqun
   Yuan, Junsong
TI Efficient Mining of Optimal AND/OR Patterns for Visual Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE AND/OR patterns; branch-and-bound search; co-occurrence features;
   frequent pattern mining
ID BOOSTING ALGORITHMS; FEATURES; SCENE
AB The co-occurrence features are the composition of base features that have more discriminative power than individual base features. Although they show promising performance in visual recognition applications such as object, scene, and action recognition, the discovery of optimal co-occurrence features is usually a computationally demanding task. Unlike previous feature mining methods that fix the order of the co-occurrence features or rely on a two-stage frequent pattern mining to select the optimal co-occurrence feature, we propose a novel branch-and-bound search-based co-occurrence feature mining algorithm that can directly mine both optimal conjunctions (AND) and disjunctions (OR) of individual features at arbitrary orders simultaneously. This feature mining process is integrated into a multi-class boosting framework Adaboost. MH such that the weighted training error is minimized by the discovered co-occurrence features in each boosting step. Experiments on UCI benchmark datasets, the scene recognition dataset, and the action recognition dataset validate both the effectiveness and efficiency of our proposed method.
C1 [Weng, Chaoqun; Yuan, Junsong] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Weng, CQ (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM weng0018@e.ntu.edu.sg; jsyuan@ntu.edu.sg
RI Yuan, Junsong/A-5171-2011; Yuan, Junsong/R-4352-2019
OI Yuan, Junsong/0000-0002-7901-8793
FU Nanyang Assistant Professorship [M58040015.040]; Singapore MoE
   [M4011272.040]
FX Manuscript received September 19, 2014; revised January 19, 2015;
   accepted February 07, 2015. Date of publication March 18, 2015; date of
   current version April 15, 2015. This work was supported in part by
   Nanyang Assistant Professorship M58040015.040 and by the Singapore MoE
   Tier-1 under Grant M4011272.040. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof. K.
   Selcuk Candan.
CR Ahonen T., 2004, P EUR C COMPUT VIS, P2037
   [Anonymous], 2008, 2008 2 INT S SYSTEMS
   [Anonymous], 2013, P ASME 2013 DYN SYST
   [Anonymous], 2007, IEEE C COMP VIS PATT
   [Anonymous], 2007, 2007 IEEE C COMP VIS
   [Anonymous], 2009, P ADV NEUR INF PROC
   [Anonymous], 2009, P 26 ANN INT C MACH, DOI [10.1145/1553374.1553439, DOI 10.1145/1553374.1553439]
   Bosch A, 2008, IEEE T PATTERN ANAL, V30, P712, DOI 10.1109/TPAMI.2007.70716
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danielsson O, 2009, IEEE I CONF COMP VIS, P917, DOI 10.1109/ICCV.2009.5459338
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Fernando Basura, 2014, International Journal of Computer Vision, V108, P186, DOI 10.1007/s11263-014-0700-1
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Han JW, 2000, SIGMOD RECORD, V29, P1
   Ito S, 2010, LECT NOTES COMPUT SC, V6315, P701
   Jiang YN, 2012, PROC CVPR IEEE, P3100, DOI 10.1109/CVPR.2012.6248042
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li J.Y., 2007, IEEE, V1, P1
   Muller Meinard., 2006, P ACM SIGGRAPHEUROGR, P137
   Mita T, 2008, IEEE T PATTERN ANAL, V30, P1257, DOI 10.1109/TPAMI.2007.70767
   Novak PK, 2009, J MACH LEARN RES, V10, P377
   Nowozin S., 2007, Computer Vision and Pattern Recognition 2007. CVPR '07, P1, DOI DOI 10.1109/CVPR.2007.383171
   Nowozin S, 2007, IEEE I CONF COMP VIS, P1727
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Saberian M, 2014, J MACH LEARN RES, V15, P2569
   Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Torralba A, 2007, IEEE T PATTERN ANAL, V29, P854, DOI 10.1109/TPAMI.2007.1055
   Tsai S.-F., 2011, ACM Multimedia, P1361
   Voravuthikunchai W, 2014, PROC CVPR IEEE, P224, DOI 10.1109/CVPR.2014.36
   Wang G, 2009, IEEE I CONF COMP VIS, P537, DOI 10.1109/ICCV.2009.5459194
   Wang J., 2012, P AS C MACH LEARN, P491
   Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wu JX, 2011, IEEE T PATTERN ANAL, V33, P1489, DOI 10.1109/TPAMI.2010.224
   Yang Y, 2011, IEEE I CONF COMP VIS, P1465, DOI 10.1109/ICCV.2011.6126403
   Yao BP, 2010, PROC CVPR IEEE, P9, DOI 10.1109/CVPR.2010.5540234
   Yuan J, 2008, INT C PATT RECOG, P172
   Yuan JS, 2011, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2011.5995476
   Yuan JS, 2010, IEEE T MULTIMEDIA, V12, P705, DOI 10.1109/TMM.2010.2051868
   Zhang SG, 2010, PROCEEDINGS OF THE ASME JOINT RAIL CONFERENCE, VOL 2, P501, DOI 10.1145/1873951.1874018
NR 41
TC 5
Z9 5
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2015
VL 17
IS 5
BP 626
EP 635
DI 10.1109/TMM.2015.2414720
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CG3AH
UT WOS:000353148300005
DA 2024-07-18
ER

PT J
AU Wang, TH
   Chiu, CW
   Wu, WC
   Wang, JW
   Lin, CY
   Chiu, CT
   Liou, JJ
AF Wang, Tsun-Hsien
   Chiu, Cheng-Wen
   Wu, Wei-Chen
   Wang, Jen-Wen
   Lin, Chun-Yi
   Chiu, Ching-Te
   Liou, Jing-Jia
TI Pseudo-Multiple-Exposure-Based Tone Fusion With Local Region Adjustment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contrast enhancement; high dynamic range (HDR); inverse tone mapping;
   local region enhancement; low dynamic range (LDR); multiple-core system;
   tone mapping
ID HIGH-DYNAMIC-RANGE; ENHANCEMENT
AB New generations of display technologies provide a significantly improved dynamic range compared to conventional display devices. Inverse tone mapping methods have been proposed to convert low dynamic range (LDR) images to HDR ones, and several of them require multiple exposure LDR images of the same scene as inputs. However, the vast majority of LDR images and videos available have only one single exposure. In this paper, we propose a region-based enhancement of the pseudo-exposures to generate an HDR image. First, we present an exposure dependent curve to convert one LDR image to the pseudo-multiple-exposures. Only certain regions of the pseudo-exposures contain noticeable detail information. We propose a region-based enhancement on the pseudo-exposures to boost details in the most distinct region. Thereby the region-enhanced pseudo-exposures are fused into an HDR image. The fused image thus enhances details in the bright region of the dark image and the dark region of the bright image. Compared with other inverse tone mapped methods, our method generates lower total contrast error measured under the dynamic range independent image quality assessment method in [1].
C1 [Wang, Tsun-Hsien; Liou, Jing-Jia] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 30013, Taiwan.
   [Chiu, Cheng-Wen] Shin Kong Life Insurance Co Ltd, Taichung 40758, Taiwan.
   [Wu, Wei-Chen] Taiwan Semicond Mfg Co, Hsinchu 30078, Taiwan.
   [Wang, Jen-Wen] ASUSTeK Comp Inc, Taipei 11259, Taiwan.
   [Lin, Chun-Yi; Chiu, Ching-Te] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30013, Taiwan.
C3 National Tsing Hua University; Taiwan Semiconductor Manufacturing
   Company; ASUSTek Computer; National Tsing Hua University
RP Wang, TH (corresponding author), Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 30013, Taiwan.
EM tshsien@gmail.com; vigle7@gmail.com; reli731206.cs96@g2.nctu.edu.tw;
   angela28582594@gmail.com; aaron19891218@hotmail.com;
   ctchiu@cs.nthu.edu.tw; jjliou@ee.nthu.edu.tw
FU Novatek; National Science Council of Taiwan [NSC102-2220-E-007-019]
FX This work was supported in part by Novatek and by the National Science
   Council of Taiwan under Contract NSC102-2220-E-007-019. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Pal Halvorsen.
CR Akyüz AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276425
   [Anonymous], 2013, EUROGRAPHICS 2013 SH
   [Anonymous], 2002, RenderMan in Production
   Aydin TO, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360668
   Banterle F, 2011, ADVANCED HIGH DYNAMIC RANGE IMAGING: THEORY AND PRACTICE, P1
   Banterle F., 2006, P 4 INT C COMP GRAPH, P349
   Banterle F, 2007, VISUAL COMPUT, V23, P467, DOI 10.1007/s00371-007-0124-9
   Banterle F, 2009, COMPUT GRAPH FORUM, V28, P13, DOI 10.1111/j.1467-8659.2008.01176.x
   Biswas KK, 2005, THIRTEENTH COLOR IMAGING CONFERENCE, FINAL PROGRAM AND PROCEEDINGS, P291
   Chalmers A., 2009, P 2 ACM SIGGRAPH C E, P71
   Daly S, 2004, PROC SPIE, V5292, P130, DOI 10.1117/12.526937
   Daly S, 2003, P SOC PHOTO-OPT INS, V5008, P455, DOI 10.1117/12.472016
   Debevec P. E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P369, DOI 10.1145/258734.258884
   Didyk P, 2008, COMPUT GRAPH FORUM, V27, P1265, DOI 10.1111/j.1467-8659.2008.01265.x
   Durand F., 2002, P SIGGRAPH, V31, P53
   Francesco M. D., 2011, P EUROGRAPHICS APR, P57
   Heidrich Wolfgang, ERIK REINHARD
   Huo Y., 2014, P 9 INT S LIN DRIV A, P733
   Ikeda E., 1998, U.S. Patent, Patent No. 5801773
   Kovaleski RP, 2009, VISUAL COMPUT, V25, P539, DOI 10.1007/s00371-009-0327-3
   Ledda P., 2010, P 24 SPRING C COMP G, P33
   Lee JW, 2011, IEEE T CONSUM ELECTR, V57, P209, DOI 10.1109/TCE.2011.5735504
   MANN S, 1995, IS&T'S 48TH ANNUAL CONFERENCE - IMAGING ON THE INFORMATION SUPERHIGHWAY, FINAL PROGRAM AND PROCEEDINGS, P442
   Mantiuk R, 2005, PROC SPIE, V5666, P204, DOI 10.1117/12.586757
   Mantiuk R., 2011, P ACM SIGGRAPH
   Masia B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618506
   Mertens T, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P382, DOI 10.1109/PG.2007.17
   Messina G, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P549
   Meylan L., 2006, COLOR IMAGING C, V1, P333, DOI [10.2352/CIC.2006.14.1.ART00061, DOI 10.2352/CIC.2006.14.1.ART00061]
   Meylan L, 2007, PROC SPIE, V6492, DOI 10.1117/12.706472
   Nayar SK, 2000, PROC CVPR IEEE, P472, DOI 10.1109/CVPR.2000.855857
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Reinhard E, 2007, J SOC INF DISPLAY, V15, P997, DOI 10.1889/1.2825110
   Rempel AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239490
   Robertson M. A., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P159, DOI 10.1109/ICIP.1999.817091
   Saito K., 1996, Japanese Patent, Patent No. [08-340486, 08340486]
   Schober M., 2013, P SPIE EL IM
   Seetzen H, 2006, SID Symp Dig Tech Pap, V37, P1229
   Seetzen H., 2003, SID Symposium Digest of Technical Papers, V34, P1450
   STREET RA, 1998, Patent No. 5789737
   Wang L., 2007, P ACM SIGGRAPH 2007
NR 41
TC 70
Z9 80
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2015
VL 17
IS 4
BP 470
EP 484
DI 10.1109/TMM.2015.2403612
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CE1QH
UT WOS:000351586300002
DA 2024-07-18
ER

PT J
AU Chou, CL
   Chen, HT
   Lee, SY
AF Chou, Chien-Li
   Chen, Hua-Tsung
   Lee, Suh-Yin
TI Pattern-Based Near-Duplicate Video Retrieval and Localization on
   Web-Scale Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Near-duplicate video localization; near-duplicate video retrieval; video
   copy detection; web-scale video analysis
ID COPY-DETECTION; FRAMEWORK
AB With the exponential growth of web multimedia contents, the Internet is rife with near-duplicate videos, the video copies applied with visual/temporal transformations and/or post productions. Two critical issues, copyright infringement and search result redundancy, arise accordingly. To resolve these problems, this paper proposes a spatiotemporal pattern-based approach under the hierarchical filter-and-refine framework for efficient and effective near-duplicate video retrieval and localization. Firstly, non-near-duplicate videos are fast filtered out through a computationally efficient data structure, termed pattern-based index tree (PI-tree). Then, an m-pattern-based dynamic programming (mPDP) algorithm is designed to localize near-duplicate segments and to re-rank the videos retrieved. The influence of time shift misalignment can be alleviated by time-shift m-pattern similarity (TPS) measurement. Comprehensive experiments on the five datasets are conducted to verify the effectiveness, efficiency, robustness, and scalability of the proposed approach. Convincing results demonstrate that our proposed approach outperforms the state-of-the-art approaches in terms of mean average precision (MAP) and normalized detection cost rate (NDCR) on the testing datasets. Furthermore, the proposed approach can achieve high quality of near-duplicate video localization in terms of quality frames (QF) and mean F1.
C1 [Chou, Chien-Li; Lee, Suh-Yin] Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu 30010, Taiwan.
   [Chen, Hua-Tsung] Natl Chiao Tung Univ, Informat & Commun Technol Lab, Hsinchu 30010, Taiwan.
C3 National Yang Ming Chiao Tung University; National Yang Ming Chiao Tung
   University
RP Chou, CL (corresponding author), Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu 30010, Taiwan.
EM fallwind@cs.nctu.edu.tw; huatsung@cs.nctu.edu.tw; sylee@cs.nctu.edu.tw
FU Ministry of Science and Technology, Taiwan [NSC-101-2221-E-009-087-MY3,
   NSC-102-2221-E-009-031, MOST-103-2221-E-009-154]
FX This work was supported in part by the Ministry of Science and
   Technology, Taiwan, under Grant NSC-101-2221-E-009-087-MY3, Grant
   NSC-102-2221-E-009-031, and Grant MOST-103-2221-E-009-154. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Lexing Xie.
CR [Anonymous], 2009, ACM INT C IM VID RET
   [Anonymous], 2011, P 19 ACM INT C MULT
   [Anonymous], 2009, P ACM INT C MULT, DOI DOI 10.1145/1631272.1631295
   [Anonymous], P C COMP VIS PATT RE
   [Anonymous], OFFICIAL YEAR END SE
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], TRECVID 2011 CONTENT
   Bahmani B, 2012, PROC VLDB ENDOW, V5, P622, DOI 10.14778/2180912.2180915
   Bay H., 2008, COMPUT VIS IMAGE UND, V10, P346, DOI DOI 10.1016/j.cviu.2007.09.014
   CHANG PK, 2004, P IEEE 6 INT S MULT, P354
   Chaudhry R, 2009, PROC CVPR IEEE, P1932, DOI 10.1109/CVPRW.2009.5206821
   Chiu CY, 2008, IEEE T CIRC SYST VID, V18, P412, DOI 10.1109/TCSVT.2008.918447
   Douze M, 2010, IEEE T MULTIMEDIA, V12, P257, DOI 10.1109/TMM.2010.2046265
   Esmaeili MM, 2011, IEEE T INF FOREN SEC, V6, P213, DOI 10.1109/TIFS.2010.2097593
   Hartung F, 1998, SIGNAL PROCESS, V66, P283, DOI 10.1016/S0165-1684(98)00011-5
   Law-To J., 2007, Muscle-VCD-2007: a live benchmark for video copy detection
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Ligang Zheng, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2537, DOI 10.1109/ICIP.2011.6116179
   Liu H, 2013, IEEE T KNOWL DATA EN, V25, P1706, DOI 10.1109/TKDE.2012.92
   Liu J, 2013, ACTA GEOL SIN-ENGL, V87, P1501, DOI 10.1111/1755-6724.12154
   Menglin Jiang, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P374, DOI 10.1109/ICME.2012.189
   Ren J., 2012, PROC 2 ACM INT C MUL, P1
   Roopalakshmi R, 2013, SIGNAL PROCESS, V93, P2339, DOI 10.1016/j.sigpro.2012.06.004
   Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275
   Shang L., 2010, Proceedings of the International Conference on Multimedia, P531
   Song J., 2011, P 19 ACM INT C MULT, P423, DOI DOI 10.1145/2072298.2072354
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Su JH, 2010, EXPERT SYST APPL, V37, P5068, DOI 10.1016/j.eswa.2009.12.003
   Tian YH, 2015, ACM T INFORM SYST, V33, DOI 10.1145/2699662
   Tian YH, 2013, IEEE MULTIMEDIA, V20, P72, DOI 10.1109/MMUL.2012.62
   Uchida Y, 2012, INT CONF ACOUST SPEE, P1029, DOI 10.1109/ICASSP.2012.6288061
   Wu A. G., 2007, P ACM MM, P218
   Wu X, 2009, IEEE T MULTIMEDIA, V11, P196, DOI 10.1109/TMM.2008.2009673
   Wu ZP, 2014, INT J MULTIMED INF R, V3, P1, DOI 10.1007/s13735-013-0049-1
   Yonghong Tian, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3629, DOI 10.1109/ICIP.2011.6116504
   Zhang J. R., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P842, DOI 10.1109/ICME.2012.111
   Zhou XM, 2012, IEEE T MULTIMEDIA, V14, P1220, DOI 10.1109/TMM.2012.2194481
NR 37
TC 59
Z9 69
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2015
VL 17
IS 3
BP 382
EP 395
DI 10.1109/TMM.2015.2391674
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CE1QB
UT WOS:000351585700010
DA 2024-07-18
ER

PT J
AU Zhou, XZ
   Xie, L
   Huang, Q
   Cox, SJ
   Zhang, YN
AF Zhou, Xiangzeng
   Xie, Lei
   Huang, Qiang
   Cox, Stephen J.
   Zhang, Yanning
TI Tennis Ball Tracking Using a Two-Layered Data Association Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Ball tracking; data association; layered; tennis; trajectory
ID ALGORITHM
AB Ball tracking is a key technology in processing and analyzing a ball game. Because of the complexity of visual scenes, a large number of objects are often selected as candidates for the ball, leading to incorrect identification, and conversely, the true position of the ball may sometimes be missed because of occlusion and blur, which can both be frequent and severe. Several tennis ball tracking algorithms have been reported in literature. In this paper, we propose a two-layered data association method to improve the robustness of tennis ball tracking. At the local layer, a shift token transfer method is proposed, based on shift window processing, to generate a set of short trajectories or "trajectorylets." At the global layer, a unique ball trajectory is obtained by applying a dynamic programming based splice method to a directed acyclic graph consisting of trajectorylets. We evaluated our approach on tennis matches from the Australian Open and the U.S. Open, and the results obtained show that our approach outperforms current state-of-the-art approach.
C1 [Zhou, Xiangzeng; Xie, Lei; Zhang, Yanning] Northwestern Polytech Univ, Sch Comp Sci, Xian 710129, Peoples R China.
   [Huang, Qiang; Cox, Stephen J.] Univ E Anglia, Sch Comp Sci, Norwich NR4 7TJ, Norfolk, England.
C3 Northwestern Polytechnical University; University of East Anglia
RP Zhou, XZ (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Xian 710129, Peoples R China.
EM xzzhou@nwpu-aslp.org; xenuts@gmail.com; h.qiang@uea.ac.uk;
   s.j.cox@uea.ac.uk; ynzhang@nwpu.edu.cn
RI Xie, Lei/JWO-8567-2024
FU National Natural Science Foundation of China [61175018, 61301194,
   61231016, 61363046]; Fok Ying Tung Education Foundation [131059];
   Research Fund for the Doctoral Program of Higher Education of China
   [20126102120055]; U.K. Engineering and Physical Sciences Research
   Council [EP/F069626/1]; EPSRC [EP/F069626/1] Funding Source: UKRI
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61175018, 61301194, 61231016, and 61363046, the Fok
   Ying Tung Education Foundation under Grant 131059, the Research Fund for
   the Doctoral Program of Higher Education of China under Grant
   20126102120055, and the U.K. Engineering and Physical Sciences Research
   Council under Grant EP/F069626/1. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Chia-Wen Lin. (Corresponding authors: Xiangzeng Zhou and Lei Xie.)
CR [Anonymous], 2007, P 15 ACM INT C MULTI
   [Anonymous], TRACKING DATA ASS
   Bar-Shalom Y., 1995, MULTITARGET MULTISEN
   Berclaz J, 2011, IEEE T PATTERN ANAL, V33, P1806, DOI 10.1109/TPAMI.2011.21
   Brendel W, 2011, PROC CVPR IEEE, P1273, DOI 10.1109/CVPR.2011.5995395
   Burkard R., 2009, Assignment Problems
   Collins RT, 2012, PROC CVPR IEEE, P1744, DOI 10.1109/CVPR.2012.6247870
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dubrofsky E., 2009, Ph.D. Thesis
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P1114, DOI 10.1109/TMM.2005.858397
   Huang Q., 2012, Signal Information Processing Association Annual Summit and Conference (APSIPA ASC), 2012 Asia-Pacific, P1
   Huang Q, 2011, IEEE T AUDIO SPEECH, V19, P1925, DOI 10.1109/TASL.2010.2103059
   Jian-Yu Hsieh, 2007, Proceedings of the Fifth IASTED International Conference on Circuits, Signals, and Systems, P1, DOI 10.1145/1329125.1329139
   Lepetit V, 2003, PROC CVPR IEEE, P281
   MOREFIELD CL, 1977, IEEE T AUTOMAT CONTR, V22, P302, DOI 10.1109/TAC.1977.1101500
   Oh S, 2009, IEEE T AUTOMAT CONTR, V54, P481, DOI 10.1109/TAC.2009.2012975
   Papageorgiou DJ, 2009, LECT NOTES CONTR INF, V381, P235, DOI 10.1007/978-3-540-88063-9_15
   Pirsiavash H, 2011, PROC CVPR IEEE, P1201, DOI 10.1109/CVPR.2011.5995604
   QUACH T, 1994, IEEE DECIS CONTR P, P271, DOI 10.1109/CDC.1994.410918
   Rasmussen C, 2001, IEEE T PATTERN ANAL, V23, P560, DOI 10.1109/34.927458
   REID DB, 1979, IEEE T AUTOMAT CONTR, V24, P843, DOI 10.1109/TAC.1979.1102177
   SMITH P, 1975, IEEE T AUTOMAT CONTR, VAC20, P101, DOI 10.1109/TAC.1975.1100851
   Yan F, 2008, IEEE T PATTERN ANAL, V30, P1814, DOI 10.1109/TPAMI.2007.70834
   Yang ZG, 2007, IEEE IMAGE PROC, P3001
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Zhang L., 2008, PROC 2008 IEEE C COM, P1
   Zhou XZ, 2013, INT CONF ACOUST SPEE, P2317, DOI 10.1109/ICASSP.2013.6638068
NR 29
TC 20
Z9 24
U1 0
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2015
VL 17
IS 2
BP 145
EP 156
DI 10.1109/TMM.2014.2380914
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AZ4RN
UT WOS:000348210500001
DA 2024-07-18
ER

PT J
AU Jiang, HQ
   Zhang, GF
   Wang, HY
   Bao, HJ
AF Jiang, Hanqing
   Zhang, Guofeng
   Wang, Huiyan
   Bao, Hujun
TI Spatio-Temporal Video Segmentation of Static Scenes and Its Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D reconstruction; spatio-temporal segmentation; video editing
ID DEPTH MAPS; IMAGE; TEXTURE; MOTION; COLOR
AB Extracting spatio-temporally consistent segments from a video sequence is a challenging problem due to the complexity of color, motion and occlusions. Most existing spatio-temporal segmentation approaches have inherent difficulties in handling large displacement with significant occlusions. This paper presents a novel framework for spatio-temporal segmentation. With the estimated depth data beforehand by a multi-view stereo technique, we project the pixels to other frames for collecting the boundary and segmentation statistics in a video, and incorporate them into the segmentation energy for spatio-temporal optimization. In order to effectively solve this problem, we introduce an iterative optimization scheme by first initializing segmentation maps for each frame independently, and then link the correspondences among different frames and iteratively refine them with the collected statistics, so that a set of spatio-temporally consistent volume segments are finally achieved. The effectiveness and usefulness of our automatic framework are demonstrated via its applications for 3D reconstruction, video editing and semantic segmentation on a variety of challenging video examples.
C1 [Jiang, Hanqing; Zhang, Guofeng; Bao, Hujun] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
   [Wang, Huiyan] Zhejiang Gongshang Univ, Sch Comp Sci & Informat Engn, Hangzhou 310018, Peoples R China.
C3 Zhejiang University; Zhejiang Gongshang University
RP Zhang, GF (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
EM jianghq@cad.zju.edu.cn; zhangguofeng@cad.zju.edu.cn;
   cederic@zjgsu.edu.cn; bao@cad.zju.edu.cn
OI Wang, Huiyan/0000-0002-7942-605X; Jiang, Hanqing/0000-0001-9582-5539
FU National Key Technology Research and Development Program of the Ministry
   of Science and Technology of China [2014BAK14B01]; NSF of China
   [61232011, 61272048, 61472362]; Specialized Research Fund for the
   Doctoral Program of Higher Education of China [20110101130011];
   Foundation for the Author of National Excellent Doctoral Dissertation of
   PR China [201245]; Zhejiang Province Science and Technology Plan Project
   [2014C33070]
FX This work was supported in part by the National Key Technology Research
   and Development Program of the Ministry of Science and Technology of
   China under Grant 2014BAK14B01, the NSF of China under Grant 61232011,
   Grant 61272048, and Grant 61472362, the Specialized Research Fund for
   the Doctoral Program of Higher Education of China under Grant
   20110101130011, Foundation for the Author of National Excellent Doctoral
   Dissertation of PR China under Grant 201245, and the Zhejiang Province
   Science and Technology Plan Project under Grant 2014C33070. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Shin'ichi Satoh. (Corresponding
   author: Guofeng Zhang.)
CR Abramov A., 2012, 2012 IEEE Workshop on Applications of Computer Vision (WACV), P457, DOI 10.1109/WACV.2012.6163000
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], LAMPTR094CSTR4403 U
   [Anonymous], LAMPTR090CAR978CSTR4
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   [Anonymous], P ECCV
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], UCIICS0302 SCH INF C
   Bhat P., 2007, P ESRT GREN FRANC, P327
   Brendel W, 2009, IEEE I CONF COMP VIS, P833, DOI 10.1109/ICCV.2009.5459242
   Chang J, 2013, PROC CVPR IEEE, P2051, DOI 10.1109/CVPR.2013.267
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Correia PL, 2003, IEEE T IMAGE PROCESS, V12, P186, DOI 10.1109/TIP.2002.807355
   Cremers D, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P886
   De Smet P, 2000, PROC SPIE, V3974, P759, DOI 10.1117/12.383013
   Deng YN, 2001, IEEE T PATTERN ANAL, V23, P800, DOI 10.1109/34.946985
   Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4
   Fowlkes C, 2001, PROC CVPR IEEE, P231
   Galasso Fabio., 2012, ACCV, P760
   Gallup D, 2010, PROC CVPR IEEE, P1418, DOI 10.1109/CVPR.2010.5539804
   Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1
   Greenspan H, 2004, IEEE T PATTERN ANAL, V26, P384, DOI 10.1109/TPAMI.2004.1262334
   Grundmann M, 2010, PROC CVPR IEEE, P2141, DOI 10.1109/CVPR.2010.5539893
   Kang H.-W., 2006, P IEEE C COMP VIS PA, P1331, DOI [DOI 10.1109/CVPR.2006.284, 10.1109/cvpr.2006.2842, DOI 10.1109/CVPR.2006.2842, 10.1109/cvpr.2006.284]
   Kang SB, 2004, INT J COMPUT VISION, V58, P139, DOI 10.1023/B:VISI.0000015917.35451.df
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Khan S, 2001, PROC CVPR IEEE, P746
   Kumar MP, 2008, INT J COMPUT VISION, V76, P301, DOI 10.1007/s11263-007-0064-x
   Lezama J., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3369, DOI 10.1109/CVPR.2011.6044588
   Li Y, 2005, ACM T GRAPHIC, V24, P595, DOI 10.1145/1073204.1073234
   Liu S., 2008, IEEE International Conference on Computer Vision and Pattern Recognition, P1
   Mobahi H, 2011, INT J COMPUT VISION, V95, P86, DOI 10.1007/s11263-011-0444-0
   Paris S, 2009, INT J COMPUT VISION, V81, P24, DOI 10.1007/s11263-007-0110-8
   Quan L, 2007, INT J COMPUT VISION, V75, P135, DOI 10.1007/s11263-007-0044-1
   Reso M, 2013, IEEE I CONF COMP VIS, P385, DOI 10.1109/ICCV.2013.55
   Sand P, 2004, ACM T GRAPHIC, V23, P592, DOI 10.1145/1015706.1015765
   Scharstein D, 2003, PROC CVPR IEEE, P195
   Seitz S. M., 2006, 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), V1, P519
   Seitz SM, 1999, INT J COMPUT VISION, V35, P151, DOI 10.1023/A:1008176507526
   Sharon E, 2006, NATURE, V442, P810, DOI 10.1038/nature04977
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Shi JB, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P1154, DOI 10.1109/ICCV.1998.710861
   Sorkine O., 2004, P 2004 EUR ACM SIGGR, P179
   Van den Bergh M, 2013, IEEE I CONF COMP VIS, P377, DOI 10.1109/ICCV.2013.54
   Wang J, 2004, ACM T GRAPHIC, V23, P574, DOI 10.1145/1015706.1015763
   Wang J, 2004, LECT NOTES COMPUT SC, V3022, P238
   Wang Y, 2005, PROC CVPR IEEE, P264
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Xiao J., 2007, P ICCV, P1
   Xiao JJ, 2006, P IEEE VIRT REAL ANN, P127
   Xu CL, 2012, LECT NOTES COMPUT SC, V7577, P626, DOI 10.1007/978-3-642-33783-3_45
   Zhang CX, 2010, LECT NOTES COMPUT SC, V6314, P708, DOI 10.1007/978-3-642-15561-1_51
   Zhang G., 2007, P CVPR, P1
   Zhang GF, 2009, IEEE T VIS COMPUT GR, V15, P828, DOI 10.1109/TVCG.2009.47
   Zhang GF, 2009, IEEE T PATTERN ANAL, V31, P974, DOI 10.1109/TPAMI.2009.52
   Zhang SH, 2011, IEEE T MULTIMEDIA, V13, P1286, DOI 10.1109/TMM.2011.2165052
   Zitnick CL, 2005, IEEE I CONF COMP VIS, P1308
NR 58
TC 93
Z9 102
U1 4
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2015
VL 17
IS 1
BP 3
EP 15
DI 10.1109/TMM.2014.2368273
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AX6QW
UT WOS:000347047400002
DA 2024-07-18
ER

PT J
AU Murata, M
   Nagano, H
   Mukai, R
   Kashino, K
   Satoh, S
AF Murata, Masaya
   Nagano, Hidehisa
   Mukai, Ryo
   Kashino, Kunio
   Satoh, Shin'ichi
TI BM25 With Exponential IDF for Instance Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE BM25 with exponential IDF; content-based video retrieval (CBVR);
   instance video search
ID SCALE; TEXT
AB This paper deals with a novel concept of an exponential IDF in the BM25 formulation and compares the search accuracy with that of the BM25 with the original IDF in a content-based video retrieval (CBVR) task. Our video retrieval method is based on a bag of keypoints (local visual features) and the exponential IDF estimates the keypoint importance weights more accurately than the original IDF. The exponential IDF is capable of suppressing the keypoints from frequently occurring background objects in videos, and we found that this effect is essential for achieving improved search accuracy in CBVR. Our proposed method is especially designed to tackle instance video search, one of the CBVR tasks, and we demonstrate its effectiveness in significantly enhancing the instance search accuracy using the TRECVID2012 video retrieval dataset.
C1 [Murata, Masaya; Nagano, Hidehisa; Mukai, Ryo; Kashino, Kunio; Satoh, Shin'ichi] NTT Corp, NTT Commun Sci Labs, Atsugi, Kanagawa 2430198, Japan.
   [Kashino, Kunio; Satoh, Shin'ichi] Res Org Informat & Syst, Natl Inst Informat, Chiyoda Ku, Tokyo 1018430, Japan.
C3 Nippon Telegraph & Telephone Corporation; Research Organization of
   Information & Systems (ROIS); National Institute of Informatics (NII) -
   Japan
RP Murata, M (corresponding author), NTT Corp, NTT Commun Sci Labs, Atsugi, Kanagawa 2430198, Japan.
EM murata.masaya@lab.ntt.co.jp
OI Murata, Masaya/0000-0001-6394-4103
CR Aly R, 2013, INFORM RETRIEVAL, V16, P557, DOI 10.1007/s10791-012-9207-y
   Amati G, 2002, ACM T INFORM SYST, V20, P357, DOI 10.1145/582415.582416
   [Anonymous], 1998, SIDLWP19990120
   Blanco R, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P921, DOI 10.1145/2348283.2348406
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   de Vries AP, 2004, IEEE IMAGE PROC, P2387
   Hoi SCH, 2008, IEEE T MULTIMEDIA, V10, P607, DOI 10.1109/TMM.2008.921735
   Hui Fang, 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P49
   Jeon J., 2003, P 26 ANN INT ACM SIG
   Lavrenko V., 2003, NIPS
   Le D. D., 2012, P TRECVID WORKSH 201, P276
   Liu Jingjing., 2007, MULTIMEDIA 07, P208
   Liu YA, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P500, DOI 10.1145/1571941.1572027
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lv YH, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P1103
   Matas J., 2002, BRIT MACH VISION COM, V22, P761
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Nister David, 2006, CVPR
   Peng Y., 2012, P TRECVID WORKSH 201, P305
   Phillips J.C., 2008, 2008 SC - International Conference for High Performance Computing, Networking, Storage and Analysis, P1, DOI DOI 10.1145/1413370.1413379
   Ponte JM, 1998, P 21 ANN INT ACM SIG, P275, DOI DOI 10.1145/290941.291008
   Robertson Stephen, 2009, Foundations and Trends in Information Retrieval, V3, P333, DOI 10.1561/1500000019
   Robertson S., 2004, P 13 ACM INT C INF K, P42
   Robertson S. E., 1994, SIGIR '94. Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, P232
   Robertson S. E., 1996, P 4 TEXT RETRIEVAL C, P21
   ROBERTSON SE, 1976, J AM SOC INFORM SCI, V27, P129, DOI 10.1002/asi.4630270302
   ROBERTSON SE, 1977, J DOC, V33, P294, DOI 10.1108/eb026647
   Robertson S, 2008, J INF SCI, V34, P439, DOI 10.1177/0165551507086989
   Singhal A., 2001, IEEE DATA ENG B, V24, P35
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   SPARCKJONES K, 1972, J DOC, V28, P11, DOI 10.1108/eb026526
   Squire DM, 2000, PATTERN RECOGN LETT, V21, P1193, DOI 10.1016/S0167-8655(00)00081-7
   Svore Krysta M., 2009, Proceedings of the 18th ACM conference on Information and knowledge management, P1811
   van de Sande Koen E. A., 2008, CGIV 2008/MCS'08. 4th European Conference on Colour in Graphics, Imaging and Vision. 10th International Symposium on Multispectral Colour Science, P378
   VRIES A, 1999, THESIS U TWENTE ENSC
   Vries A. P., 1998, P 20 ANN BCS IRSG C
   Yan R, 2007, INFORM RETRIEVAL, V10, P445, DOI 10.1007/s10791-007-9031-y
   Zhang W., 2012, P TRECVID WORKSH 201, P420
   Zhao Z., 2012, P TRECVID WORKSH 201, P66
   Zhou W., 2010, P 18 ACM INT C MULTI, P511
   Zhu C.-Z., 2012, P 2 ACM INT C MULT R
NR 41
TC 15
Z9 17
U1 3
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1690
EP 1699
DI 10.1109/TMM.2014.2323945
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200017
DA 2024-07-18
ER

PT J
AU Gao, SH
   Chia, LT
   Tsang, IWH
   Ren, ZX
AF Gao, Shenghua
   Chia, Liang-Tien
   Tsang, Ivor Wai-Hung
   Ren, Zhixiang
TI Concurrent Single-Label Image Classification and Annotation via
   Efficient Multi-Layer Group Sparse Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image annotation; image classification; kernel trick; sparse coding
ID MODEL
AB We present a multi-layer group sparse coding framework for concurrent single-label image classification and annotation. By leveraging the dependency between image class label and tags, we introduce a multi-layer group sparse structure of the reconstruction coefficients. Such structure fully encodes the mutual dependency between the class label, which describes image content as a whole, and tags, which describe the components of the image content. Therefore we propose a multi-layer group based tag propagation method, which combines the class label and subgroups of instances with similar tag distribution to annotate test images. To make our model more suitable for nonlinear separable features, we also extend our multi-layer group sparse coding in the Reproducing Kernel Hilbert Space (RKHS), which further improves performances of image classification and annotation. Moreover, we also integrate our multi-layer group sparse coding with kNN strategy, which greatly improves the computational efficiency. Experimental results on the LabelMe, UIUC-Sports and NUS-WIDE-Object databases show that our method outperforms the baseline methods, and achieves excellent performances in both image classification and annotation tasks.
C1 [Gao, Shenghua] Adv Digital Sci Ctr, Singapore, Singapore.
   [Chia, Liang-Tien; Tsang, Ivor Wai-Hung; Ren, Zhixiang] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Gao, SH (corresponding author), Adv Digital Sci Ctr, Singapore, Singapore.
EM shenghua.gao@adsc.com.sg
RI Tsang, Ivor W/E-8653-2011
OI Tsang, Ivor/0000-0001-8095-4637
FU research grant for the Human Sixth Sense Programme at the Advanced
   Digital Sciences Center from Singapore's Agency for Science, Technology
   and Research (A*STAR)
FX This work was supported in part by the research grant for the Human
   Sixth Sense Programme at the Advanced Digital Sciences Center from
   Singapore's Agency for Science, Technology and Research (A*STAR). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Shin'ichi Satoh.
CR Ameesh M., 2008, P EUR C COMP VIS
   Ameesh M., 2010, INT J COMPUT VISION
   [Anonymous], P INT C MACH LEARN
   [Anonymous], 2006, PROC IEEE C COMPUTER
   [Anonymous], 2005, P IEEE C COMP VIS PA
   [Anonymous], 2010, P IEEE C COMP VIS PA
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2009, NUSWIDE: A real-world web image database from National University of Singapore, DOI DOI 10.1145/1646396.1646452
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2003, P IEEE INT C COMP VI
   [Anonymous], P ACM MULT
   [Anonymous], 2010, P IEEE C COMP VIS PA
   [Anonymous], 2009, P IEEE C COMP VIS PA
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   Bengio S., 2009, P C NEUR INF PROC SY
   Boureau Y.-L., 2011, P IEEE C COMP VIS
   Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P394, DOI 10.1109/TPAMI.2007.61
   Chen X., 2010, TECH REP
   Cusano C., 2004, SPIE
   Dalal N., CVPR, P886, DOI [DOI 10.1109/CVPR.2005.177, 10.1109/CVPR.2005.177]
   Friedman J., 2010, Tech Rep, P1
   Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01
   Gao SH, 2013, IEEE T PATTERN ANAL, V35, P92, DOI 10.1109/TPAMI.2012.63
   Gao SH, 2011, PROC CVPR IEEE
   Gao SH, 2010, LECT NOTES COMPUT SC, V6314, P1
   Grangier D, 2008, IEEE T PATTERN ANAL, V30, P1371, DOI 10.1109/TPAMI.2007.70791
   Guillaumin M., 2009, PROC IEEE CONF COMPU
   Jenatton R., 2010, P INT C MACH LEARN
   Jia Y., 2012, P IEEE C COMP VIS PA
   Lee H., 2006, ADV NEURAL INF PROCE
   Li L.- J., 2009, P IEEE C COMP VIS PA
   Liu X., 2010, P IEEE C COMP VIS PA
   LIU X, 2009, P ACM INT C MULT
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Meier L, 2008, J R STAT SOC B, V70, P53, DOI 10.1111/j.1467-9868.2007.00627.x
   NASIERDING G, 2009, P IEEE INT C SYST MA
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Petterson J., 2010, P NIPS
   Rosenblum K., 2010, P AAAI C ART INT
   Scholkopf B., 1997, P INT C ART NEUR NET, P583
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Tang JH, 2012, IEEE T IMAGE PROCESS, V21, P2354, DOI 10.1109/TIP.2011.2180916
   WANG C., 2009, P IEEE C COMP VIS PA
   Wang C., 2009, P INT C AC SPEECH SI
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yang M., 2011, P IEEE C COMP VIS PA
   Yuan M., 2006, Journal of the Royal Statistical Society, Series B, V70, P53
   Yuan X.-T., 2010, P IEEE C COMP VIS PA
   Zhang S, 2010, P IEEE C COMP VIS PA
NR 49
TC 32
Z9 35
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 762
EP 771
DI 10.1109/TMM.2014.2299516
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500016
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Chen, YS
   Zhang, BX
   Liu, Y
   Zhu, W
AF Chen, Yishuai
   Zhang, Baoxian
   Liu, Yong
   Zhu, Wei
TI Measurement and Modeling of Video Watching Time in a Large-Scale
   Internet Video-on-Demand System
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Terms-Measurement; modeling; streaming media; videos; consumer behavior
AB Video watching time is a crucial measure for studying user watching behavior in online Internet video-on-demand (VoD) systems. It is important for system planning, user engagement understanding, and system quality evaluation. However, due to the limited access of user data in large-scale streaming systems, a systematic measurement, analysis, and modeling of video watching time is still missing. In this paper, we measure PPLive, one of the most popular commercial Internet VoD systems in China, over a three week period. We collect accurate user watching data of more than 100 million streaming sessions of more than 100 thousand distinct videos. Based on the measurement data, we characterize the distribution of watching time of different types of videos and reveal a number of interesting characteristics regarding the relation between video watching time and various video-related features (including video type, duration, and popularity). We further build a suite of mathematical models for characterizing these relationships. Extensive performance evaluation shows the high accuracy of these models as compared with commonly used data-mining based models. Our measurement and modeling results bring forth important insights for simulation, design, deployment, and evaluation of Internet VoD systems.
C1 [Chen, Yishuai] Beijing Jiaotong Univ, Sch Elect & Informat Engn, Beijing 100044, Peoples R China.
   [Zhang, Baoxian] Univ Chinese Acad Sci, Res Ctr Ubiquitous Sensor Networks, Beijing 100049, Peoples R China.
   [Liu, Yong] Polytech Inst New York Univ, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
   [Zhu, Wei] PPLive Inc, Shanghai 201203, Peoples R China.
C3 Beijing Jiaotong University; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; New York University; New York
   University Tandon School of Engineering
RP Chen, YS (corresponding author), Beijing Jiaotong Univ, Sch Elect & Informat Engn, Beijing 100044, Peoples R China.
EM yschen@bjtu.edu.cn; bxzhang@ucas.ac.cn; yongliu@poly.edu;
   zhuwei324@hotmail.com
RI Zhu, Weiguo/W-1555-2019
FU Direct For Computer & Info Scie & Enginr; Division Of Computer and
   Network Systems [0953682] Funding Source: National Science Foundation
CR Acharya S., 2000, P MMCN 00 SAN JOS CA
   Adhikari VK, 2012, IEEE INFOCOM SER, P1620, DOI 10.1109/INFCOM.2012.6195531
   Adhikari VK, 2012, IEEE CONF COMPUT, P7, DOI 10.1109/INFCOMW.2012.6193524
   Almeida J.M., 2001, Proceedings of the 11th international workshop on Network and operating systems support for digital audio and video, P21, DOI 10.1145/378344.378348
   Almeroth KC, 1996, PROCEEDINGS OF THE FIFTH IEEE INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE DISTRIBUTED COMPUTING, P209, DOI 10.1109/HPDC.1996.546190
   [Anonymous], PEER TO PEER COMPUTI
   [Anonymous], 2011, CISC VIS NETW IND GL
   [Anonymous], 2011, DATA MINING R LEARNI
   AZZALINI A, 1985, SCAND J STAT, V12, P171
   Azzalini A., 2011, SN SKEW NORMAL SKEW
   Blewitt ME, 2008, NAT GENET, V40, P663, DOI 10.1038/ng.142
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   C Wu, 2012, ACM T MULTIMEDIA C S, V8s, P13
   Cha M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P1
   Cha M, 2008, IMC'08: PROCEEDINGS OF THE 2008 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P71
   Chen Y., MEASUREMENT MODELING
   Cherkasova Ludmila., 2002, NOSSDAV 02 P 12 INT, P33
   Chesire M, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX SYMPOSIUM ON INTERNET TECHNOLOGIES AND SYSTEMS, P1
   Conover W. J., 1998, PRACTICAL NONPARAMET, V3rd
   Costa C., 2004, P 13 INT C WORLD WID, P534
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Fan Qiu, 2011, 2011 IEEE Consumer Communications and Networking Conference (CCNC 2011), P410, DOI 10.1109/CCNC.2011.5766502
   Finamore A., 2011, ACM IMC, P345, DOI DOI 10.1145/2068816.2068849
   Gao P, 2009, L N INST COMP SCI SO, V2, P24
   García R, 2009, COMPUT NETW, V53, P2038, DOI 10.1016/j.comnet.2009.03.011
   Gill P, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P15
   Guo L, 2008, PODC'08: PROCEEDINGS OF THE 27TH ANNUAL ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING, P283, DOI 10.1145/1400751.1400789
   Hei XJ, 2007, IEEE J SEL AREA COMM, V25, P1640, DOI 10.1109/JSAC.2007.071204
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   Hongliang Yu, 2006, Operating Systems Review, V40, P333, DOI 10.1145/1218063.1217968
   Huang C, 2007, ACM SIGCOMM COMP COM, V37, P133, DOI 10.1145/1282427.1282396
   Huang Y, 2008, ACM SIGCOMM COMP COM, V38, P375, DOI 10.1145/1402946.1403001
   iResearch Inc, 2012, PPTV RANK 1 DAIL US
   Li ZG, 2012, PLANT SCI, V185, P185, DOI 10.1016/j.plantsci.2011.10.006
   Lienhart R, 2002, IEEE INTERNET COMPUT, V6, P73, DOI 10.1109/4236.978372
   MITCHELL T, 1989, ANNU REV COMPUT SCI, V4, P417
   Qiu TQ, 2009, IMC'09: PROCEEDINGS OF THE 2009 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P430
   Rao A., 2011, P CONEXT 11 TOK JAP
   Research A. N. M., 2012, PEOPL
   Saxena Mohit., 2008, NOSSDAV '08: Proceedings of the 18th International Workshop on Network and Operating Systems Support for Digital Audio and Video, P39
   Silverston T., 2006, P2P IPTV measurement
   Spoto S, 2009, INT PARALL DISTRIB P, P1798, DOI 10.1109/IPDPS.2009.5160956
   Sripanidkulchai K., 2004, Proceedings of the 4th ACM SIGCOMM conference on Internet measurement, P41
   Tang W, 2007, COMPUT NETW, V51, P336, DOI 10.1016/j.comnet.2006.05.003
   Therneau T., 2019, Rpart: Recursive partitioning and regression trees
   Torgo L., 2011, Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, P787
   Veloso E, 2006, IEEE ACM T NETWORK, V14, P133, DOI 10.1109/TNET.2005.863709
   Vilas M, 2005, EUROMICRO-SEAA 2005: 31st EUROMICRO Conference on Software Engineering and Advanced Applications, Proceedings, P330
   Vu L., 2007, P QSHINE 07 VANC BC
   Wang J, 2008, MULTIMED TOOLS APPL, V36, P89, DOI 10.1007/s11042-006-0075-6
   Wu D, 2010, IEEE ACM T NETWORK, V18, P1248, DOI 10.1109/TNET.2009.2038910
   Yin H, 2009, IMC'09: PROCEEDINGS OF THE 2009 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P442
   Yu G., 2009, Broadband Multimedia Systems and Broadcasting, P1
NR 53
TC 52
Z9 58
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 2087
EP 2098
DI 10.1109/TMM.2013.2280123
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900029
DA 2024-07-18
ER

PT J
AU Wu, CC
   Chen, KT
   Chang, YC
   Lei, CL
AF Wu, Chen-Chi
   Chen, Kuan-Ta
   Chang, Yu-Chun
   Lei, Chin-Laung
TI Crowdsourcing Multimedia QoE Evaluation: A Trusted Framework
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Crowdsourcing; mean opinion score; paired comparison; probabilistic
   choice model; quality of experience; subjective test
ID QUALITY PREDICTION
AB Crowdsourcing has emerged in recent years as a potential strategy to enlist the general public to solve a wide variety of tasks. With the advent of ubiquitous Internet access, it is now feasible to ask an Internet crowd to conduct QoE (Quality of Experience) experiments on their personal computers in their own residences rather than in a laboratory. The considerable size of the Internet crowd allows researchers to crowdsource their experiments to a more diverse set of participant pool at a relatively low economic cost. However, as participants carry out experiments without supervision, the uncertainty of the quality of their experiment results is a challenging problem.
   In this paper, we propose a crowdsourceable framework to quantify the QoE of multimedia content. To overcome the aforementioned quality problem, we employ a paired comparison method in our framework. The advantages of our framework are: 1) trustworthiness due to the support for cheat detection; 2) a simpler rating procedure than that of the commonly-used but more difficult mean opinion score (MOS), which places less burden on participants; 3) economic feasibility since reliable QoE measures can be acquired with less effort compared with MOS; and 4) generalizability across a variety of multimedia content. We demonstrate the effectiveness and efficiency of the proposed framework by a comparison with MOS. Moreover, the results of four case studies support our assertion that the framework can provide reliable QoE evaluation at a lower cost.
C1 [Wu, Chen-Chi; Chang, Yu-Chun; Lei, Chin-Laung] Natl Taiwan Univ, Dept Elect Engn, Taipei 106, Taiwan.
   [Chen, Kuan-Ta] Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.
   [Chen, Kuan-Ta] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 115, Taiwan.
C3 National Taiwan University; Academia Sinica - Taiwan; Academia Sinica -
   Taiwan
RP Wu, CC (corresponding author), Natl Taiwan Univ, Dept Elect Engn, Taipei 106, Taiwan.
EM bipa@fractal.ee.ntu.edu.tw; ktchen@iis.sinica.edu.tw;
   congo@fractal.ee.ntu.edu.tw; lei@cc.ee.ntu.edu.tw
FU National Science Council [NSC101-2221-E-001-012-MY3,
   NSC101-2221-E-002-190-MY3]
FX This work was supported in part by the National Science Council under
   the grants NSC101-2221-E-001-012-MY3 and NSC101-2221-E-002-190-MY3. A
   preliminary version of this paper [13] appeared in the Proceedings of
   the 17th ACM International Conference on Multimedia (ACM Multimedia
   2009). The associate editor coordinating the review of this manuscript
   and approving it for publication was Prof. Charles D. (Chuck) Creusere.
CR Alonso Omar, 2008, SIGIR Forum, V42, P9, DOI 10.1145/1480506.1480508
   [Anonymous], P IEEE ACM NETGAMES
   [Anonymous], P SPIE
   [Anonymous], PERCEPTUAL EVALUATIO
   [Anonymous], P INTERSPEECH
   [Anonymous], P SPIE
   [Anonymous], 2011, ACM INT C MULTIMEDIA
   [Anonymous], P SPIE HUMAN VISION
   [Anonymous], P IEEE INT S BROADB
   [Anonymous], P ACM SIGCOMM 2006 P
   [Anonymous], INDIVIDUAL CHOICE BE
   [Anonymous], P IEEE INFOCOM 2009
   [Anonymous], FDN MEASUREMENT
   [Anonymous], P IEEE WORKSH STREAM
   [Anonymous], G107 ITUT
   [Anonymous], 102643 ETSI TR
   [Anonymous], 2010, 2010 IEEE COMPUTER S, DOI DOI 10.1109/CVPRW.2010.5543189
   [Anonymous], 2011, ACM C MULT SYST FEBR, DOI DOI 10.1145/1943552.1943563
   [Anonymous], BIOMETRIKA
   [Anonymous], METHODS SUBJECTIVE D
   [Anonymous], 2006, WIRED MAG
   [Anonymous], OBJECTIVE PERCEPTUAL
   [Anonymous], 2011, Image Processing (ICIP), 2011 18th IEEE International Conference on
   [Anonymous], METHOD PAIRED COMP
   [Anonymous], 2010, P 11 ACM C EL COMM, DOI DOI 10.1145/1807342.1807376
   [Anonymous], 2008, Subjective video quality assessment methods for multimedia applications. Recommendation P.910
   [Anonymous], P ACM MULT 2011 NOV
   Basso A, 2009, COMPUT SECUR, V28, P174, DOI 10.1016/j.cose.2008.11.002
   Birnbaum M.H., 2000, Psychological experiments on the Internet
   BLAND JM, 1986, LANCET, V1, P307, DOI 10.1016/s0140-6736(86)90837-8
   Brabham D. C, 2008, Convergence, V14, P75, DOI DOI 10.1177/1354856507084420
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.1093/biomet/39.3-4.324
   Carnero B, 1999, IEEE T SIGNAL PROCES, V47, P1622, DOI 10.1109/78.765133
   Chan Y, 2011, IEEE T NANOTECHNOL, V10, P947, DOI 10.1109/TNANO.2010.2090170
   Chen K.-T., 2009, P 17 ACM INT C MULTI, P491
   Chen KT, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/797159
   Choisel S, 2007, J ACOUST SOC AM, V121, P388, DOI 10.1121/1.2385043
   Ding LJ, 2003, GLOB TELECOMM CONF, P3974, DOI 10.1109/GLOCOM.2003.1258975
   Dittrich R, 1998, J ROY STAT SOC C-APP, V47, P511, DOI 10.1111/1467-9876.00125
   Doan A, 2011, COMMUN ACM, V54, P86, DOI 10.1145/1924421.1924442
   Eichhorn A, 2010, NOSSDAV 2010: PROCEEDINGS OF THE 20TH INTERNATIONAL WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO, P63
   ELLIOTT EO, 1963, AT&T TECH J, V42, P1977, DOI 10.1002/j.1538-7305.1963.tb00955.x
   GILBERT EN, 1960, BELL SYST TECH J, V39, P1253, DOI 10.1002/j.1538-7305.1960.tb03959.x
   GIROD B, 1989, P SOC PHOTO-OPT INS, V1077, P178
   Hill RJ, 1953, AM SOCIOL REV, V18, P564, DOI 10.2307/2087442
   Ho C.-J., 2007, AAAI 07 P 22 NATL C, P1359
   Ho C.-J., 2009, HCOMP 09 P ACM SIGKD, P62
   Hossfeld Tobias, 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P494, DOI 10.1109/ISM.2011.87
   Ito Y, 2005, IEEE T MULTIMEDIA, V7, P572, DOI 10.1109/TMM.2005.846785
   Jain R, 2004, IEEE MULTIMEDIA, V11, P96, DOI 10.1109/MMUL.2004.1261114
   Jain S, 2009, 10TH ACM CONFERENCE ON ELECTRONIC COMMERCE - EC 2009, P129
   Kendall MG, 1939, ANN MATH STAT, V10, P275, DOI 10.1214/aoms/1177732186
   Kittur A, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P453
   Knott C. L., 2004, International Transactions in Operational Research, V11, P87, DOI 10.1111/j.1475-3995.2004.00442.x
   LARSONPOWERS N, 1978, J FOOD SCI, V43, P41, DOI 10.1111/j.1365-2621.1978.tb09732.x
   Lee CT, 2009, 2009 IEEE/WIC/ACM INTERNATIONAL JOINT CONFERENCES ON WEB INTELLIGENCE (WI) AND INTELLIGENT AGENT TECHNOLOGIES (IAT), VOL 1, P116
   Lee JS, 2011, IEEE T MULTIMEDIA, V13, P882, DOI 10.1109/TMM.2011.2157333
   MATTHEWS JNS, 1995, J R STAT SOC C-APPL, V44, P243
   Pastrana-Vidal RR, 2004, P SOC PHOTO-OPT INS, V5292, P194, DOI 10.1117/12.526648
   Peterson GL, 1998, LAND ECON, V74, P240, DOI 10.2307/3147054
   Rand DG, 2009, SCIENCE, V325, P1272, DOI 10.1126/science.1177418
   RAO PV, 1967, J AM STAT ASSOC, V62, P194, DOI 10.2307/2282923
   Ribeiro F, 2011, INT CONF ACOUST SPEE, P2416
   Rossi PE, 2001, J AM STAT ASSOC, V96, P20, DOI 10.1198/016214501750332668
   SAATY TL, 1977, J MATH PSYCHOL, V15, P234, DOI 10.1016/0022-2496(77)90033-5
   SHROUT PE, 1979, PSYCHOL BULL, V86, P420, DOI 10.1037/0033-2909.86.2.420
   Sorokin A., 2008, PROC IEEE C COMPUTER, P1
   Sun LF, 2006, IEEE T MULTIMEDIA, V8, P809, DOI 10.1109/TMM.2006.876279
   Tasaka S., 2008, Proceedings of ACM international conference on Multimedia, MULTIMEDIA'08, P259
   Watson A., 1998, Proceedings ACM Multimedia 98, P55, DOI 10.1145/290747.290755
   Xu Q, 2012, IEEE T MULTIMEDIA, V14, P844, DOI 10.1109/TMM.2012.2190924
   Zixia H., 2012, P 3 MULTIMEDIA SYSTE, P29
NR 72
TC 55
Z9 64
U1 0
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2013
VL 15
IS 5
BP 1121
EP 1137
DI 10.1109/TMM.2013.2241043
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222WT
UT WOS:000324763600015
DA 2024-07-18
ER

PT J
AU Wu, YD
   Wei, Z
   Deng, RH
AF Wu, Yongdong
   Wei, Zhuo
   Deng, Robert H.
TI Attribute-Based Access to Scalable Media in Cloud-Assisted Content
   Sharing Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Access control; cloud computing; data security and privacy; scalable
   media content
ID ENCRYPTION; PROTECTION
AB This paper presents a novel Multi-message Ciphertext Policy Attribute-Based Encryption (MCP-ABE) technique, and employs the MCP-ABE to design an access control scheme for sharing scalable media based on data consumers' attributes (e. g., age, nationality, or gender) rather than an explicit list of the consumers' names. The scheme is efficient and flexible because MCP-ABE allows a content provider to specify an access policy and encrypt multiple messages within one ciphertext such that only the users whose attributes satisfy the access policy can decrypt the ciphertext. Moreover, the paper shows how to support resource-limited mobile devices by offloading computational intensive operations to cloud servers while without compromising data privacy.
C1 [Wu, Yongdong] Inst Infocomm Res, Cryptog & Secur Dept, Singapore 138632, Singapore.
   [Wei, Zhuo; Deng, Robert H.] Singapore Management Univ, Sch Informat Syst, Singapore 178902, Singapore.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); Singapore Management University
RP Wu, YD (corresponding author), Inst Infocomm Res, Cryptog & Secur Dept, Singapore 138632, Singapore.
EM wydong@i2r.a-star.edu.sg; phdzwei@gmail.com; robertdeng@smu.edu.sg
RI DENG, Robert H./E-8547-2012
OI Deng, Robert/0000-0003-3491-8146
FU A*STAR SERC in Singapore [102 101 0027]
FX This work was supported in part by A*STAR SERC under Grant 102 101 0027
   in Singapore. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Chang Wen Chen.
CR Akinyele JosephA., 2011, P 1 ACM WORKSHOP SEC, P75, DOI DOI 10.1145/2046614.2046628
   Algin GB, 2011, J VIS COMMUN IMAGE R, V22, P353, DOI 10.1016/j.jvcir.2011.02.005
   [Anonymous], 144962 ISOIEC
   [Anonymous], 2009, NETWORK WORLD
   [Anonymous], NETWORKWORLD FEB
   [Anonymous], FIPS PUBL
   [Anonymous], 2006, P 13 ACM C COMP COMM
   [Anonymous], ENCY CRYPTOGRAPHY SE
   [Anonymous], USENIX SECURITY
   [Anonymous], CCS07 P 14 ACM C
   [Anonymous], CIPHERTEXT POLICY AT
   Baden R, 2009, ACM SIGCOMM COMP COM, V39, P135, DOI 10.1145/1594977.1592585
   Bethencourt J, 2007, P IEEE S SECUR PRIV, P321, DOI 10.1109/sp.2007.11
   Carbunar B, 2010, ACM T SENSOR NETWORK, V6, DOI 10.1145/1689239.1689244
   Gergely V, 2009, LECT NOTES COMPUT SC, V5733, P51, DOI 10.1007/978-3-642-03700-9_6
   Goyal V., 2006, P 2006 INT C PRIVACY, P1
   Hellwagner H, 2009, SIGNAL PROCESS-IMAGE, V24, P740, DOI 10.1016/j.image.2009.07.002
   Jahid Sonia., 2011, Proceedings of the 6th ACM Symposium on Information, Computer and Communications Security, P411
   Li CH, 2008, LECT NOTES COMPUT SC, V5353, P496
   Lian SG, 2010, TELECOMMUN SYST, V45, P21, DOI 10.1007/s11235-009-9233-2
   Mell P., 2010, NIST DEFINITION CLOU
   Park SW, 2008, NCM 2008 : 4TH INTERNATIONAL CONFERENCE ON NETWORKED COMPUTING AND ADVANCED INFORMATION MANAGEMENT, VOL 1, PROCEEDINGS, P371, DOI 10.1109/NCM.2008.259
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Wee SJ, 2001, IEEE IMAGE PROC, P437, DOI 10.1109/ICIP.2001.959047
   Won YG, 2006, LECT NOTES COMPUT SC, V4283, P407
   Wu YD, 2007, IEEE T MULTIMEDIA, V9, P1314, DOI 10.1109/TMM.2007.902865
   Yu S., 2010, PROC IEEE INT C COMP, P1
   Zhang XW, 2008, ACM T INFORM SYST SE, V11, DOI 10.1145/1330295.1330298
   Zhu BB, 2005, IEEE T MULTIMEDIA, V7, P222, DOI 10.1109/TMM.2005.843340
NR 29
TC 53
Z9 62
U1 0
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2013
VL 15
IS 4
BP 778
EP 788
DI 10.1109/TMM.2013.2238910
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148FK
UT WOS:000319228500007
OA Green Published
DA 2024-07-18
ER

PT J
AU Ozkan, D
   Morency, LP
AF Ozkan, Derya
   Morency, Louis-Philippe
TI Latent Mixture of Discriminative Experts
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Backchannel feedback; evaluation metric; mixture of experts; multimodal
   integration; multimodal prediction models; sparse regularization
AB In this paper, we introduce a new model called Latent Mixture of Discriminative Experts which can automatically learn the temporal relationship between different modalities. Since, we train separate experts for each modality, LMDE is capable of improving the prediction performance even with limited amount of data. For model interpretation, we present a sparse feature ranking algorithm that exploits L-1 regularization. An empirical evaluation is provided on the task of listener backchannel prediction (i.e., head nod). We introduce a new error evaluation metric called User-adaptive Prediction Accuracy that takes into account the difference in people's backchannel responses. Our results confirm the importance of combining five types of multimodal features: lexical, syntactic structure, part-of-speech, visual and prosody. Latent Mixture of Discriminative Experts model outperforms previous approaches.
C1 [Ozkan, Derya; Morency, Louis-Philippe] Univ So Calif, Inst Creat Technol, Playa Vista, CA 90094 USA.
C3 University of Southern California
RP Ozkan, D (corresponding author), Univ So Calif, Inst Creat Technol, Playa Vista, CA 90094 USA.
EM ozkan@ict.usc.edu; morency@ict.usc.edu
RI Morency, Louis-Philippe/B-2006-2008
FU National Science Foundation [0917321]; U.S. Army Research, Development,
   and Engineering Command (RDECOM); Div Of Information & Intelligent
   Systems; Direct For Computer & Info Scie & Enginr [0917321] Funding
   Source: National Science Foundation
FX Manuscript received May 28, 2011; revised December 14, 2011 and April
   18, 2012; accepted June 12, 2012. Date of publication November 21, 2012;
   date of current version January 15, 2013. This work was supported in
   part by the National Science Foundation under Grant No. 0917321 and in
   part by the U.S. Army Research, Development, and Engineering Command
   (RDECOM). The content does not necessarily reflect the position or the
   policy of the Government, and no official endorsement should be
   inferred. The associate editor coordinating the review of this
   manuscript and approving it for publication was Daniel Gatica-Perez.
CR Andrew G., 2007, ICML 07
   [Anonymous], P INT C MACH LEARN I
   [Anonymous], MACHINE LEARNING
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Barnard Mark., 2005, IEEE International Conference on Multimedia and Expo (ICME), P1150, DOI DOI 10.1109/ICME.2005.1521630
   Bavelas JB, 2000, J PERS SOC PSYCHOL, V79, P941, DOI 10.1037/0022-3514.79.6.941
   Bishop C., 2003, P C UNC ART INT UAI
   Burgoon J.K., 1995, INTERPERSONAL ADAPTA
   Cassell J., 1999, P C ART INT AAAI
   Cathcart N, 2003, EACL 2003: 10TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P51
   Eyben P., 2009, PROC IEEE 4 INT HUMA, P576
   Foo SW, 2004, IEEE T CIRC SYST VID, V14, P693, DOI 10.1109/TCSVT.2004.826773
   Fox N, 2003, LECT NOTES COMPUT SC, V2688, P743
   FUCHS D, 1987, TOP EARLY CHILD SPEC, V7, P90, DOI 10.1177/027112148700700309
   Fujie S, 2004, RO-MAN 2004: 13TH IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, PROCEEDINGS, P159, DOI 10.1109/ROMAN.2004.1374748
   Garg A, 2003, P IEEE, V91, P1355, DOI 10.1109/JPROC.2003.817119
   Goldberg SB, 2005, NEGOTIATION J, V21, P365, DOI 10.1111/j.1571-9979.2005.00069.x
   Gratch J., 2007, P INT VIRT AG IVA
   JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181
   Kang S., 2008, P INT C AUT AG MULT
   Kendon A., 2004, GESTURE VISIBLE ACTI
   Kumar S., 2003, P INT C COMP VIS ICC
   Lafferty John D, 2001, CONDITIONAL RANDOM F
   Maatman M., 2005, P INT VIRT AG IVA
   Marcus M., 1994, HUMAN LANGUAGE TECHN, P114, DOI 10.3115/1075812.1075835
   MCCALLUM A, 2003, P C UNC ART INT
   McCowan I, 2005, IEEE T PATTERN ANAL, V27, P305, DOI 10.1109/TPAMI.2005.49
   McNeill D., 1992, Hand and Mind: What Gestures Reveal about Thought
   Morency L.-P., 2007, P IEE C COMP VIS PAT
   Morency L.-P., 2008, P C INT VIRT AG IVA
   Nakano, 2003, P ASS COMP LING ACL
   Nakano Y., 2007, Association for Computational Linguistics (ACL), P121
   Nishimura R, 2007, LECT NOTES ARTIF INT, V4629, P599
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Oliver N, 2004, COMPUT VIS IMAGE UND, V96, P163, DOI 10.1016/j.cviu.2004.02.004
   Oviatt S., 1999, COMMUN ACM
   Pavlovic V, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P343, DOI 10.1109/ICIP.1998.723492
   Peng J., 2009, NIPS
   Perkins S., 2003, Journal of Machine Learning Research, V3, P1333, DOI 10.1162/153244303322753698
   Quek F., 2003, P INT C COMP VIS ICC
   RIEZLER S, 2004, P C EMP METH NAT LAN
   Sagae K., 2007, EMNLP CONLL 2007 P 2, P1044
   Sminchisescu C, 2005, PROC CVPR IEEE, P390
   Smith A., 2005, ACL, P18
   Smith A., 2005, P INT JOINT C NAT LA
   Snoek C. G. M., 2005, P ACM INT C MULT
   Terry LH, 2008, IEEE IMAGE PROC, P1316, DOI 10.1109/ICIP.2008.4712005
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   VAIL D, 2007, P IEEE RSJ INT C INT
   Ward N, 2000, J PRAGMATICS, V32, P1177, DOI 10.1016/S0378-2166(99)00109-5
NR 50
TC 2
Z9 2
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2013
VL 15
IS 2
BP 326
EP 338
DI 10.1109/TMM.2012.2229263
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 075HN
UT WOS:000313875500009
DA 2024-07-18
ER

PT J
AU Sánchez, F
   Alduán, M
   Alvarez, F
   Menéndez, JM
   Báez, O
AF Sanchez, Faustino
   Alduan, Maria
   Alvarez, Federico
   Manuel Menendez, Jose
   Baez, Orlando
TI Recommender System for Sport Videos Based on User Audiovisual
   Consumption
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audiovisual consumption; hidden Markov model (HMM); recommender system;
   sport videos
AB This paper describes a recommender system for sport videos, transmitted over the Internet and/or broadcast, in the context of large-scale events, which has been tested for the Olympic Games. The recommender is based on audiovisual consumption and does not depend on the number of users, running only on the client side. This avoids the concurrence, computation and privacy problems of central server approaches in scenarios with a large number of users, such as the Olympic Games.
   The system has been designed to take advantage of the information available in the videos, which is used along with the implicit information of the user and the modeling of his/her audiovisual content consumption. The system is thus transparent to the user, who does not need to take any specific action.
   Another important characteristic is that the system can produce recommendations for both live and recorded events.
   Testing has showed advantages compared to previous systems, as will be shown in the results.
C1 [Sanchez, Faustino; Alduan, Maria] Univ Politecn Madrid, Visual Telecommun Applicat Grp, Signals Syst & Radio Commun Dept, ETS Ingenieros Telecomunicac, E-28040 Madrid, Spain.
   [Manuel Menendez, Jose] Univ Politecn Madrid, Visual Telecommun Applicat Res Grp, E-28040 Madrid, Spain.
   [Baez, Orlando] Univ Politecn Madrid, DIE, E-28040 Madrid, Spain.
C3 Universidad Politecnica de Madrid; Universidad Politecnica de Madrid;
   Universidad Politecnica de Madrid
RP Sánchez, F (corresponding author), Univ Politecn Madrid, Visual Telecommun Applicat Grp, Signals Syst & Radio Commun Dept, ETS Ingenieros Telecomunicac, Ciudad Univ, E-28040 Madrid, Spain.
EM fsg@gatv.ssr.upm.es; mam@gatv.ssr.upm.es; fag@gatv.ssr.upm.es;
   jmm@gatv.ssr.upm.es; obr@gatv.ssr.upm.es
RI Menéndez, José Manuel/AAS-8430-2020; Alvarez, Federico/AAA-7628-2019;
   Menendez, Jose-Manuel/L-1159-2014
OI Menéndez, José Manuel/0000-0003-0584-2250; Alvarez,
   Federico/0000-0001-7400-9591; Menendez, Jose-Manuel/0000-0003-0584-2250
FU Spanish Ministry of Industry, Tourism and Trade; Spanish Ministry of
   Science and Innovation-CDTI under the contract of the CENIT Programme
   [CEN-20091026]
FX This work was supported in part by the Spanish Ministry of Industry,
   Tourism and Trade under the research contract JOI ("Interactive Olympic
   Games") and in part by the Spanish Ministry of Science and
   Innovation-CDTI under the contract of the CENIT Programme, project
   "BUSCAMEDIA" (CEN-20091026) (http://www.cenit-buscamedia.es). The
   associate editor coordinating the review
CR Adomavicius G, 2005, IEEE T KNOWL DATA EN, V17, P734, DOI 10.1109/TKDE.2005.99
   [Anonymous], 2008, SPORTSML 2 0 IPTC G2
   [Anonymous], 2010, 102796 ETSI TS
   [Anonymous], 1973, PATTERN RECOGNITION
   Ashkezari S., 2010, P 2010 IEEE INT C FU, P1
   Martínez ABB, 2009, IEEE T CONSUM ELECTR, V55, P286, DOI 10.1109/TCE.2009.4814447
   Billsus D., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P46
   Ding Y., 2007, IEEE C COMP VIS PATT, P1
   Forsati R, 2009, 2009 IEEE/ACS INTERNATIONAL CONFERENCE ON COMPUTER SYSTEMS AND APPLICATIONS, VOLS 1 AND 2, P579, DOI 10.1109/AICCSA.2009.5069385
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P1114, DOI 10.1109/TMM.2005.858397
   Johnston I., 2000, ILL GIVE YOU DEFINIT
   Merkel K, 2011, EL MED TECHN CEMT 20, P1
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Pazzani MJ, 1999, ARTIF INTELL REV, V13, P393, DOI 10.1023/A:1006544522159
   Pizard S., 2007, CLANFI CLASIFICADOR
   Quinlan R., 1983, MACHINE LEARNING ART, V1
   RODGERS JL, 1988, AM STAT, V42, P59, DOI 10.2307/2685263
   Sanchez J. L., 2008, Second IEEE International Conference on Digital Ecosystems and Technologies (IEEE DEST 2008), P432, DOI 10.1109/DEST.2008.4635147
   SARWAR B, 2001, P 10 INT C WWW, P295
   Shani G., 2007, P WORKSH INT TECHN W
   Shin H, 2009, IEEE T CONSUM ELECTR, V55, P1417, DOI 10.1109/TCE.2009.5278008
   Sollenborn M, 2002, LECT NOTES ARTIF INT, V2416, P395
   Sotelo R, 2009, IEEE T CONSUM ELECTR, V55, P248, DOI 10.1109/TCE.2009.4814442
   Syeda-Mahmood T., 2001, P 9 ACM INT C MULT O
   Wu X, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P369, DOI 10.1109/ICME.2008.4607448
   Yu C, 2009, PROC INT CONF DATA, P1299, DOI 10.1109/ICDE.2009.225
   Yu Z., 2010, MULTIMEDIA SYST  MAY
   Zimmerman J., 2004, DESIGN TV SHOW RECOM
NR 28
TC 16
Z9 17
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2012
VL 14
IS 6
BP 1546
EP 1557
DI 10.1109/TMM.2012.2217121
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046YE
UT WOS:000311800400005
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Song, SB
   Moustafa, H
   Afifi, H
AF Song, Songbo
   Moustafa, Hassnaa
   Afifi, Hossam
TI Advanced IPTV Services Personalization Through Context-Aware Content
   Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content recommendation; Internet Protocol Television (IPTV); Next
   Generation Network (NGN); personalization; quality of experience (QoE);
   user-centric
ID NEXT-GENERATION
AB The advances in Internet Protocol Television (IPTV) technology enable new user-centric and interactive TV model, in which context-awareness is promising in making users' interaction with the TV dynamic and transparent. This paper considers user-centric personalized IPTV services applying context-awareness. We present a solution for enriched IPTV services personalization introducing context-awareness on top of IPTV architecture to gather different information on the user and his environment allowing each user to be distinguished by the system in a unique and real-time manner. Based on the different context information gathered, a novel solution for content recommendation is presented allowing to customize IPTV content according to the context of each user and his environment and hence guaranteeing better users' experience. We implemented the proposed solution on top of an IPTV platform considering the NGN IPTV architecture as a proof of concept and as a mean to evaluate the performance.
C1 [Song, Songbo] ALTRAN Technol, F-75017 Paris, France.
   [Moustafa, Hassnaa] France Telecom R&D Orange Labs, Issy Les Moulineaux, France.
   [Afifi, Hossam] Telecom & Management S Paris Inst Telecom, Dept Wireless Networks & Multimedia Serv, Evry, France.
C3 Orange SA
RP Song, SB (corresponding author), ALTRAN Technol, F-75017 Paris, France.
EM songbo.song@gmail.com; hassnaa.moustafa@orange.com;
   hossam.afifi@it-sudparis.eu
RI Akalugwu, Kenneth/F-4815-2014
FU Eureka Celtic UP-TO-US (User-Centric Personalized IPTV UbiquitOus and
   SecUre Services) European project
FX This work was supported by the Eureka Celtic UP-TO-US (User-Centric
   Personalized IPTV UbiquitOus and SecUre Services) European project. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Zhu Liu.
CR *3GPP TS, 29229 3GPP TS
   Adomavicius G, 2005, IEEE T KNOWL DATA EN, V17, P734, DOI 10.1109/TKDE.2005.99
   [Anonymous], NEURAL NETWORKS ARTI
   [Anonymous], 2008, 182028 ETSI TS
   [Anonymous], TR126 BROADB FOR
   [Anonymous], 2008, ITU T IPTV FOC GROUP
   [Anonymous], 2011, 282003 ETSI ES
   Ardissono L, 2003, LECT NOTES ARTIF INT, V2829, P474
   Chen Y., IEEE T CONSUM ELECT, V55, P707
   Chen YC, 2009, IEEE T CONSUM ELECTR, V55, P707, DOI 10.1109/TCE.2009.5174443
   Landt J., 2001, Shrouds of time: The history of RFID
   Martinez J. M., ISO IECJTC1 SC29 WG1
   Santos da Silva F., 2009, P 7 EUR C INT TV VID
   Schulzrinne H., 2003, P IETF RFC, V3550
   Schulzrinne H., 2006, P IETF RFC, V4880
   Song S., 2009, P IFIP IEEE MAN MULT
   Song S., 2010, P FMN
   Thawni A., 2004, P 4 WORKSH PERS FUT
   Yu Z., 2003, P 2 INT C MACH LEARN
NR 19
TC 18
Z9 22
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2012
VL 14
IS 6
BP 1528
EP 1537
DI 10.1109/TMM.2012.2217118
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046YE
UT WOS:000311800400003
DA 2024-07-18
ER

PT J
AU Feng, XY
   Cox, IJ
   Doërr, G
AF Feng, Xiaoying
   Cox, Ingemar J.
   Doerr, Gwenael
TI Normalized Energy Density-Based Forensic Detection of Resampled Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Multimedia and Expo (ICME)
CY JUL 11-15, 2011
CL Univ Ramon Llull, La Salle, Barcelona, SPAIN
SP IEEE, IEEE Signal Proc Soc, IEEE Circuits & Syst Soc, IEEE Comp Soc, IEEE Commun Soc
HO Univ Ramon Llull, La Salle
DE Image forensics; normalized energy density; resampling detection
AB We propose a new method to detect resampled imagery. The method is based on examining the normalized energy density present within windows of varying size in the second derivative of the image in the frequency domain, and exploiting this characteristic to derive a 19-D feature vector that is used to train a SVM classifier. Experimental results are reported on 7500 raw images from the BOSS database. Comparison with prior work reveals that the proposed algorithm performs similarly for resampling rates greater than 1, and is superior to prior work for resampling rates less than 1. Experiments are performed for both bilinear and bicubic interpolations, and qualitatively similar results are observed for each. Results are also provided for the detection of resampled imagery with noise corruption and JPEG compression. As expected, some degradation in performance is observed as the noise increases or the JPEG quality factor declines.
C1 [Feng, Xiaoying; Cox, Ingemar J.] UCL, Dept Comp Sci, London WC1E 6BT, England.
   [Doerr, Gwenael] Secur & Content Protect Labs, Technicolor R&D, F-35576 Cesson Sevigne, France.
C3 University of London; University College London; Technicolor SA
RP Feng, XY (corresponding author), UCL, Dept Comp Sci, Mortimer St, London WC1E 6BT, England.
EM x.feng@cs.ucl.ac.uk
OI Cox, Ingemar J./0000-0002-6662-417X; Doerr, Gwenael/0009-0005-5124-3111
CR [Anonymous], 2003, P DIG FOR RES WORKSH
   [Anonymous], 2006, P 8 WORKSHOP MULTIME, DOI DOI 10.1145/1161366.1161375
   Bas P, P 13 INT WORKSH INF, P59
   Chen XL, 2002, SIXTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P32, DOI 10.1109/ACV.2002.1182151
   Cox IJ, 2008, MKS MULTIMED INFORM, P1
   Dalgaard N, 2010, IEEE IMAGE PROC, P1753, DOI 10.1109/ICIP.2010.5652358
   David V., 2011, Workshop on Information Forensics and Security, P1
   Duda R., 1973, Pattern Classification and Scene Analysis
   Farid H, 2009, IEEE SIGNAL PROC MAG, V26, P16, DOI 10.1109/MSP.2008.931079
   Fawcett T., 2003, HP INVEN
   Feng X., 2010, P SOC PHOTO-OPT INS, V7541, P1
   Feng X., 2011, 2011 IEEE International Conference on Multimedia and Expo, P1
   Gallagher AC, 2005, 2nd Canadian Conference on Computer and Robot Vision, Proceedings, P65, DOI 10.1109/CRV.2005.33
   Kirchner M, 2010, MM&SEC 2010: 2010 ACM SIGMM MULTIMEDIA AND SECURITY WORKSHOP, PROCEEDINGS, P13
   Kirchner M, 2009, IEEE INT WORKS INFOR, P21, DOI 10.1109/WIFS.2009.5386489
   Kirchner M, 2008, MM&SEC'08: PROCEEDINGS OF THE MULTIMEDIA & SECURITY WORKSHOP 2008, P11, DOI 10.1145/1411328.1411333
   Liu Q, 2009, P 1 ACM WORKSH MULT, P43, DOI [10.1145/1631081.1631092, DOI 10.1145/1631081.1631092]
   Mahdian B, 2008, IEEE T INF FOREN SEC, V3, P529, DOI 10.1109/TIFS.2004.924603
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Pevny T, 2008, IEEE T INF FOREN SEC, V3, P247, DOI 10.1109/TIFS.2008.922456
   Popescu AC, 2004, LECT NOTES COMPUT SC, V3200, P128
   Popescu AC, 2005, IEEE T SIGNAL PROCES, V53, P758, DOI 10.1109/TSP.2004.839932
   Uccheddu F., 2010, P 18 EUR SIGN PROC C, P24
NR 23
TC 96
Z9 106
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 536
EP 545
DI 10.1109/TMM.2012.2191946
PN 1
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 943ZG
UT WOS:000304166300005
DA 2024-07-18
ER

PT J
AU Win, LL
   Thomas, T
   Emmanuel, S
AF Win, Lei Lei
   Thomas, Tony
   Emmanuel, Sabu
TI Privacy Enabled Digital Rights Management Without Trusted Third Party
   Assumption
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Multimedia and Expo (ICME)
CY JUL 11-15, 2011
CL Univ Ramon Llull, La Salle, Barcelona, SPAIN
SP IEEE, IEEE Signal Proc Soc, IEEE Circuits & Syst Soc, IEEE Comp Soc, IEEE Commun Soc
HO Univ Ramon Llull, La Salle
DE Accountability; blind decryption; digital rights management (DRM); hash
   chain; privacy
ID AUTHENTICATION
AB Digital rights management systems are required to provide security and accountability without violating the privacy of the entities involved. However, achieving privacy along with accountability in the same framework is hard as these attributes are mutually contradictory. Thus, most of the current digital rights management systems rely on trusted third parties to provide privacy to the entities involved. However, a trusted third party can become malicious and break the privacy protection of the entities in the system. Hence, in this paper, we propose a novel privacy preserving content distribution mechanism for digital rights management without relying on the trusted third party assumption. We use simple primitives such as blind decryption and one way hash chain to avoid the trusted third party assumption. We prove that our scheme is not prone to the "oracle problem" of the blind decryption mechanism. The proposed mechanism supports access control without degrading user's privacy as well as allows revocation of even malicious users without violating their privacy.
C1 [Win, Lei Lei; Emmanuel, Sabu] Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
   [Thomas, Tony] Indian Inst Informat Technol & Management, Trivandrum 695581, Kerala, India.
   [Win, Lei Lei] Nanyang Technol Univ, Temasek Lab, Singapore, Singapore.
C3 Nanyang Technological University; Kerala University of Digital Sciences,
   Innovation & Technology (Digital University Kerala); Nanyang
   Technological University
RP Win, LL (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
EM leiwin@ntu.edu.sg; tony.thomas@iiitmk.ac.in; asemmanuel@ntu.edu.sg
RI Emmanuel, Sabu/A-3690-2011
OI Thomas, Tony/0000-0002-9323-6607
CR Ahmad R., 2002, U.S. Patent Application, Patent No. 20020143703
   [Anonymous], 2009, P 4 ACM INT WORKSH M
   Au MH, 2006, LECT NOTES COMPUT SC, V4116, P111
   Au MH, 2005, LECT NOTES COMPUT SC, V3797, P332
   Camenisch J, 2006, LECT NOTES COMPUT SC, V4116, P141
   Chaum D., 1983, Advances in Cryptology, Proceedings of Crypto 82, P199
   Chong CN, 2002, LECT NOTES COMPUT SC, V2515, P339
   Chong D., 2006, Proceedings of the ACM workshop on Digital rights management, P37
   Durahim Ahmet Onur, 2010, Proceedings of the Fifth International Conference on Internet Monitoring and Protection (ICIMP 2010), P54, DOI 10.1109/ICIMP.2010.16
   Feng M, 2008, CONSUM COMM NETWORK, P1075, DOI 10.1109/ccnc08.2007.244
   Kim YS, 2010, INT CONF ADV COMMUN, P1583
   LAMPORT L, 1981, COMMUN ACM, V24, P770, DOI 10.1145/358790.358797
   Michiels S., 2005, DIGITAL RIGHTS MANAG
   Perlman R., 2010, P 9 S IDENTITY TRUST, P69
   Sakurai K., 1996, Information Hiding. First International Workshop Proceedings, P257
   Sun M. K., 2009, P IEEE CONS COMM NET, P1
   Teranishi I, 2006, LECT NOTES COMPUT SC, V3958, P525
   Thomas T, 2009, IEEE T INF FOREN SEC, V4, P758, DOI 10.1109/TIFS.2009.2033229
   Tsang PP, 2008, CCS'08: PROCEEDINGS OF THE 15TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P333
   Tsiounis Y., ANONYMITY PRIVACY IN
   Wenjing Lou, 2009, IEEE Wireless Communications, V16, P80, DOI 10.1109/MWC.2009.5281259
   Win L. L., 2009, LNCS, V5879, P1315
   Win L. L., 2011, MULTIMEDIA TOOLS APP
   Win L. L., 2011, P IEEE INT C MULT EX, P1
   Zhang J, 2005, 2005 IEEE International Conference on Multimedia and Expo (ICME), Vols 1 and 2, P257
NR 25
TC 14
Z9 16
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 546
EP 554
DI 10.1109/TMM.2012.2189983
PN 1
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 943ZG
UT WOS:000304166300006
DA 2024-07-18
ER

PT J
AU Bobarshad, H
   van der Schaar, M
   Aghvami, AH
   Dilmaghani, RS
   Shikh-Bahaei, MR
AF Bobarshad, Hossein
   van der Schaar, Mihaela
   Aghvami, A. Hamid
   Dilmaghani, Reza S.
   Shikh-Bahaei, Mohammad R.
TI Analytical Modeling for Delay-Sensitive Video Over WLAN
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-layer; delay; expired-time; retransmission; video; wireless local
   area network (WLAN)
ID WIRELESS MULTIMEDIA TRANSMISSION; NETWORKS; SYSTEM; STRATEGIES;
   PROTECTION; QOS
AB Delay-sensitive video transmission over IEEE 802.11 wireless local area networks (WLANs) is analyzed in a cross-layer optimization framework. The effect of delay constraint on the quality of received packets is studied by analyzing "expired-time packet discard rate". Three analytical models are examined and it is shown that M/M/1 model is quite an adequate model for analyzing delay-limited applications such as live video transmission over WLAN. The optimal MAC retry limit corresponding to the minimum "total packet loss rate" is derived by exploiting both mathematical analysis and NS-2 simulations.
   We have shown that there is an interaction between "packet overflow drop" and "expired-time packet discard" processes in the queue. Subsequently, by introducing the concept of virtual buffer size, we will obtain the optimal buffer size in order to avoid "packet overflow drop". We finally introduced a simple and yet effective real-time algorithm for retry-limit adaptation over IEEE 802.11 MAC in order to maintain a loss protection for delay-critical video traffic transmission, and showed that the average link-layer throughput can be improved by using our adaptive scheme.
C1 [Bobarshad, Hossein] Tarbiat Modares Univ, Dept Engn, Tehran, Iran.
   [van der Schaar, Mihaela] Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
   [Aghvami, A. Hamid; Dilmaghani, Reza S.; Shikh-Bahaei, Mohammad R.] Kings Coll London, Ctr Telecommun Res, London WC2R 2LS, England.
C3 Tarbiat Modares University; University of California System; University
   of California Los Angeles; University of London; King's College London
RP Bobarshad, H (corresponding author), Tarbiat Modares Univ, Dept Engn, Tehran, Iran.
EM hossein.bobarshad@ieee.org; mihaela@ee.ucla.edu;
   hamid.aghvami@kcl.ac.uk; reza.shams_dilmaghani@kcl.ac.uk;
   m.sbahaei@kcl.ac.uk
OI Shikh-Bahaei, Mohammad/0000-0001-7450-7574
CR [Anonymous], 1999, 80211 IEEE WG 11
   Bobarshad H., 2009, P IEEE WIR COMM NETW
   Bobarshad H, 2010, IEEE T MULTIMEDIA, V12, P427, DOI 10.1109/TMM.2010.2050734
   Gross D., 1998, FUNDAMENTALS QUEUEIN
   Han SC, 2011, IEEE J SEL AREA COMM, V29, P1032, DOI 10.1109/JSAC.2011.110513
   Hsu JL, 2009, IEEE SIGNAL PROC LET, V16, P268, DOI 10.1109/LSP.2008.2010821
   Hua S, 2011, IEEE T MULTIMEDIA, V13, P402, DOI 10.1109/TMM.2010.2103929
   Kleinrock L., 1975, Queueing Systems-Volume 1: Theory, V1
   Krunz MM, 2001, IEEE J SEL AREA COMM, V19, P384, DOI 10.1109/49.914515
   Lee H, 2011, IEEE T MULTIMEDIA, V13, P813, DOI 10.1109/TMM.2011.2134840
   Li Q, 2004, IEEE T MULTIMEDIA, V6, P278, DOI 10.1109/TMM.2003.822792
   Li Q, 2007, IEEE T VEH TECHNOL, V56, P3533, DOI 10.1109/TVT.2007.901927
   LU MH, 2005, P IEEE INT C MULT EX
   Luan TH, 2010, IEEE T MULTIMEDIA, V12, P64, DOI 10.1109/TMM.2009.2036294
   Luo HY, 2010, IEEE COMMUN MAG, V48, P102, DOI 10.1109/MCOM.2010.5402671
   N SS, 2007, IEEE T VEH TECHNOL, V56, P2346, DOI 10.1109/TVT.2007.897646
   Ong LT, 2008, IEEE T COMMUN, V56, P177, DOI 10.1109/TCOMM.2008.060142
   Shadmand A, 2010, EURASIP J WIREL COMM, DOI 10.1155/2010/458472
   Shojaeifard A, 2011, IEEE T WIREL COMMUN, V10, P3278, DOI 10.1109/TWC.2011.080311.101301
   The network Simulator 2 (NS-2) Information Sciences Institute (ISI), 2002, NETW SIM 2 NS 2 INF
   Tournoux PU, 2011, IEEE T MULTIMEDIA, V13, P797, DOI 10.1109/TMM.2011.2126564
   van der Schaar M, 2003, IEEE J SEL AREA COMM, V21, P1752, DOI 10.1109/JSAC.2003.815231
   van der Schaar M, 2006, IEEE T MOBILE COMPUT, V5, P755, DOI 10.1109/TMC.2006.81
   van der Schaar M, 2005, IEEE WIREL COMMUN, V12, P50, DOI 10.1109/MWC.2005.1497858
   van der Schaar M, 2007, IEEE T MULTIMEDIA, V9, P185, DOI 10.1109/TMM.2006.886384
   van der Schaar M, 2006, IEEE T MULTIMEDIA, V8, P1082, DOI 10.1109/TMM.2006.879827
NR 26
TC 30
Z9 34
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2012
VL 14
IS 2
BP 401
EP 414
DI 10.1109/TMM.2011.2173477
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OZ
UT WOS:000302702500014
DA 2024-07-18
ER

PT J
AU Salamin, H
   Vinciarelli, A
AF Salamin, Hugues
   Vinciarelli, Alessandro
TI Automatic Role Recognition in Multiparty Conversations: An Approach
   Based on Turn Organization, Prosody, and Conditional Random Fields
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Conditional random fields (CRFs); prosody; role recognition; turn
   organization
ID PERSONALITY
AB Roles are a key aspect of social interactions, as they contribute to the overall predictability of social behavior (a necessary requirement to deal effectively with the people around us), and they result in stable, possibly machine-detectable behavioral patterns (a key condition for the application of machine intelligence technologies). This paper proposes an approach for the automatic recognition of roles in conversational broadcast data, in particular, news and talk shows. The approach makes use of behavioral evidence extracted from speaker turns and applies conditional random fields to infer the roles played by different individuals. The experiments are performed over a large amount of broadcast material (around 50 h), and the results show an accuracy higher than 85%.
C1 [Salamin, Hugues; Vinciarelli, Alessandro] Univ Glasgow, Dept Comp Sci, Glasgow G12 8QQ, Lanark, Scotland.
   [Vinciarelli, Alessandro] Idiap Res Inst CP592, CH-1920 Martigny, Switzerland.
C3 University of Glasgow
RP Salamin, H (corresponding author), Univ Glasgow, Dept Comp Sci, Glasgow G12 8QQ, Lanark, Scotland.
EM hsalamin@dcs.gla.ac.uk; vincia@dcs.gla.ac.uk
RI Vinciarelli, Alessandro/HZI-8274-2023; Vinciarelli,
   Alessandro/C-1651-2012
OI Vinciarelli, Alessandro/0000-0002-9048-0524
FU European Community [231287]; Swiss National Science Foundation under the
   National Centre for Competence in Research IM2 (Interactive Multimodal
   Information Management); Scottish Research Council via the Scottish
   Information and Computer Science Alliance (SICSA)
FX Manuscript received December 17, 2010; revised April 11, 2011; accepted
   October 12, 2011. Date of publication October 27, 2011; date of current
   version March 21, 2012. This work was supported in part by the European
   Community's Seventh Framework Programme (FP7/2007-2013) under Grant
   231287 (SSPNet), the Swiss National Science Foundation under the
   National Centre for Competence in Research IM2 (Interactive Multimodal
   Information Management), and the Scottish Research Council via the
   Scottish Information and Computer Science Alliance (SICSA). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Qibin Sun.
CR ADDINGTON D, 1968, COMMUN MONOGRAPHS, V35, P492
   AJMERA J, 2004, P IEEE WORKSH AUT SP, P411
   [Anonymous], WIRED SPEECH
   Bales RF, 1950, AM SOCIOL REV, V15, P257, DOI 10.2307/2086790
   BANERJEE S, 2004, P INT C SPOK LANG PR
   Barzilay R, 2000, SEVENTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2001) / TWELFTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-2000), P679
   BIDDLE BJ, 1986, ANNU REV SOCIOL, V12, P67, DOI 10.1146/annurev.so.12.080186.000435
   BIGOT B, 2010, P INT WORKSH SEARCH, P5
   BILMES J, 1988, LANG SOC, V17, P161, DOI 10.1017/S0047404500012744
   Boersma P., 2018, Praat: doing phonetics by computer, DOI DOI 10.1097/AUD.0B013E31821473F7
   Dong W, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P271
   EKMAN P, 1980, J PERS SOC PSYCHOL, V38, P270, DOI 10.1037/0022-3514.38.2.270
   Garg N. P., 2008, P ACM INT C MULT, P693
   Kohavi R., 1995, STUDY CROSS VALIDATI, DOI DOI 10.1067/MOD.2000.109031
   Koller D., 2009, Probabilistic graphical models: principles and techniques
   Lafferty John, 2001, INT C MACH LEARN ICM
   Laskowski Kornel., 2008, P ISCA ACL SIGDIAL W, P148
   LIU Y, 2006, P HUM LANG TECHN C N, P81
   MATENA L, 2008, P MOBILEHCI 2008 10, P503
   Pianesi F, 2008, PERS UBIQUIT COMPUT, V12, P181, DOI 10.1007/s00779-007-0144-5
   Psathas G., 1995, CONVERSATION ANAL ST, DOI https://doi.org/10.4135/9781412983792
   RAY GB, 1986, COMMUN MONOGR, V53, P266, DOI 10.1080/03637758609376141
   SACKS H, 1974, LANGUAGE, V50, P696, DOI 10.2307/412243
   Salamin H, 2009, IEEE T MULTIMEDIA, V11, P1373, DOI 10.1109/TMM.2009.2030740
   Scherer KR, 2003, SPEECH COMMUN, V40, P227, DOI 10.1016/S0167-6393(02)00084-5
   Scott J., 2005, A dictionary of sociology, V3rd
   Slater PE, 1955, AM SOCIOL REV, V20, P300, DOI 10.2307/2087389
   Song CM, 2010, SCIENCE, V327, P1018, DOI 10.1126/science.1177170
   Tischler H.L., 1990, Introduction to Sociology, V3rd
   Vinciarelli A, 2007, IEEE T MULTIMEDIA, V9, P1215, DOI 10.1109/TMM.2007.902882
   Vinciarelli A, 2006, INT C PATT RECOG, P1154
   Vinciarelli A, 2009, IMAGE VISION COMPUT, V27, P1743, DOI 10.1016/j.imavis.2008.11.007
   Vinciarelli A, 2009, IEEE SIGNAL PROC MAG, V26, P133, DOI 10.1109/MSP.2009.933382
   Weng CY, 2009, IEEE T MULTIMEDIA, V11, P256, DOI 10.1109/TMM.2008.2009684
   ZANCANARO M, 2006, P INT C MULT MOD INT, P47
NR 35
TC 17
Z9 19
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2012
VL 14
IS 2
BP 338
EP 345
DI 10.1109/TMM.2011.2173927
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OZ
UT WOS:000302702500009
DA 2024-07-18
ER

PT J
AU Chu, WT
   Tsai, SY
AF Chu, Wei-Ta
   Tsai, Shang-Yin
TI Rhythm of Motion Extraction and Rhythm-Based Cross-Media Alignment for
   Dance Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Background music replacement; motion trajectory; music beat; music video
   generation; rhythm of motion
ID BACKGROUND MUSIC; BEAT ANALYSIS; SYSTEMS; TEMPO
AB We present how to extract rhythm information in dance videos and music, and accordingly correlate them based on rhythmic representation. From dancer's movement, we construct motion trajectories, detect turnings, and stops of trajectories, and then estimate rhythm of motion (ROM). For music, beats are detected to describe rhythm of music. Two modalities are therefore represented as sequences of rhythm information to facilitate finding cross-media correspondence. Two applications, i.e., background music replacement and music video generation, are developed to demonstrate the practicality of cross-media correspondence. We evaluate performance of ROM extraction, and conduct subjective/objective evaluation to show that rich browsing experience can be provided by the proposed applications.
C1 [Chu, Wei-Ta] Natl Chung Cheng Univ, Dept Comp Sci & Informat Engn, Chiayi, Taiwan.
C3 National Chung Cheng University
RP Chu, WT (corresponding author), Natl Chung Cheng Univ, Dept Comp Sci & Informat Engn, Chiayi, Taiwan.
EM wtchu@cs.ccu.edu.tw; shouyinz@hotmail.com
RI Chu, Wei-Ta/AAE-8471-2022
OI Chu, Wei-Ta/0000-0001-5722-7239
FU National Science Council of Taiwan, Republic of China [NSC
   100-2221-E-194-061, NSC 99-2221-E-194-036]
FX This work was supported in part by the National Science Council of
   Taiwan, Republic of China under research contract NSC 100-2221-E-194-061
   and NSC 99-2221-E-194-036. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Maja Pantic.
CR [Anonymous], 2007, EMBODIED MUSIC COGNI, DOI DOI 10.7551/MITPRESS/7476.001.0001
   [Anonymous], 2009, PROC 10 INT SOC MUSI
   [Anonymous], DIGITAL SIGNAL PROCE
   [Anonymous], INT C INT MULT COMP
   [Anonymous], P IEEE CVF INT C COM
   Borer T.-J., 2002, U. S. Patent, Patent No. [6442202B1, 6442202]
   Bouguet J-Y, 1999, Pyramidal implementation of the Lucas Kanade feature tracker
   Camurri A, 2000, COMPUT MUSIC J, V24, P57, DOI 10.1162/014892600559182
   Cutler R, 2000, IEEE T PATTERN ANAL, V22, P781, DOI 10.1109/34.868681
   Denman H., 2005, P 2005 INT WORKSH MU, P183
   Dixon S, 2001, J NEW MUSIC RES, V30, P39, DOI 10.1076/jnmr.30.1.39.7119
   Dowling W.J., 1985, MUSIC COGNITION
   Eyben F., 2010, Proceedings of the 11th International Society for Music Information Retrieval Conference, P589, DOI 10.5281/zenodo.1417131
   Foote J., 2002, P ACM MULTIMEDIA 200, P553, DOI DOI 10.1145/641007.641119
   Gauldin Robert., 2004, HARMONIC PRACTICE TO
   Godoy R. I., 2004, LECT NOTES ARTIF INT, V2915, P55
   GOLDFELD SM, 1966, ECONOMETRICA, V34, P541, DOI 10.2307/1909768
   Gouyon F, 2005, COMPUT MUSIC J, V29, P34, DOI 10.1162/comj.2005.29.1.34
   Grosche P., 2010, P INT SOC MUSIC INFO, P649
   Guedes C., 2006, P SOUND MUS COMP C S, P25
   Hua X. -S., 2004, PROC 12 ANN ACM INT, P472
   Huber R, 2006, IEEE T AUDIO SPEECH, V14, P1902, DOI 10.1109/TASL.2006.883259
   Kim TH, 2003, ACM T GRAPHIC, V22, P392, DOI 10.1145/882262.882283
   Laptev I., 2005, P INT C COMP VIS
   LEE HC, 2005, P EUROGRAPHICS 2005, P353
   MARQUES JS, 1989, IEEE T ACOUST SPEECH, V37, P763, DOI 10.1109/29.17571
   MIN J, 2004, P IEEE INT C COMP VI, P118
   NAKAMURA JI, 1994, J VISUAL COMP ANIMAT, V5, P247, DOI 10.1002/vis.4340050405
   Oliveira J. L., 2010, P INT SOC MUS INF RE
   Ratner LeonardG., 1956, MUSIC QUART, V42, P439
   Sadler B. M., 1986, IEEE T SIGNAL PROCES, V46, P2990
   Scheirer ED, 1998, J ACOUST SOC AM, V103, P588, DOI 10.1121/1.421129
   Sethares W. A., 2007, Rhythm and Transforms
   Shiratori T, 2003, PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON MULTISENSOR FUSION AND INTEGRATION FOR INTELLIGENT SYSTEMS, P89, DOI 10.1109/MFI-2003.2003.1232638
   Su CW, 2007, IEEE T MULTIMEDIA, V9, P1193, DOI 10.1109/TMM.2007.902875
   Toiviainen P, 2010, MUSIC PERCEPT, V28, P59, DOI 10.1525/MP.2010.28.1.59
   Wang J., 2005, P ACM MULTIIMEDIA, P735
   Yoon J.-C., 2005, P INT C ADV COMP ENT, P353
   Yoon JC, 2006, LECT NOTES COMPUT SC, V4153, P205
   Yoon JC, 2009, MULTIMED TOOLS APPL, V41, P197, DOI 10.1007/s11042-008-0225-0
NR 40
TC 10
Z9 11
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2012
VL 14
IS 1
BP 129
EP 141
DI 10.1109/TMM.2011.2172401
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science; Telecommunications
GA 924OL
UT WOS:000302701100013
DA 2024-07-18
ER

PT J
AU Chen, F
   Delannay, D
   De Vleeschouwer, C
AF Chen, Fan
   Delannay, Damien
   De Vleeschouwer, Christophe
TI An Autonomous Framework to Produce and Distribute Personalized
   Team-Sport Video Summaries: A Basketball Case Study
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content repurposing; multi-sensored processing; production of
   personalized video summarization
ID TRACKING; PEOPLE
AB Democratic and personalized production of multimedia content is a challenge that content providers will have to face in the near future. In this paper, we address this challenge by building on computer vision tools to automate the collection and distribution of audiovisual content. Especially, we proposed a complete production process of personalized video summaries in a typical application scenario, where the sensor network for media acquisition is composed of multiple cameras, which, for example, cover a basketball field. Distributed analysis and interpretation of the scene are exploited to decide what to show or not to show about the event, so as to produce a video composed of a valuable subset of the streams provided by each individual camera. Interestingly, the selection of the streams subsets to forward to each user depends on his/her individual preferences, making the process adaptive and personalized. The process involves numerous integrated technologies and methodologies, including but not limited to automatic scene analysis, camera viewpoint selection, adaptive streaming, and generation of summaries through automatic organization of stories. The proposed technology provides practical solutions to a wide range of applications, such as personalized access to local sport events through a web portal, cost-effective and fully automated production of content dedicated to small-audience, or even automatic log in of annotations.
C1 [Chen, Fan] Japan Adv Inst Sci & Technol, Sch Informat Sci, Nomi 9231211, Japan.
   [Delannay, Damien] Catholic Univ Louvain, Inst Informat & Commun Technol, Elect & Appl Math ICTEAM, B-1348 Louvain, Belgium.
C3 Japan Advanced Institute of Science & Technology (JAIST); Universite
   Catholique Louvain
RP Chen, F (corresponding author), Japan Adv Inst Sci & Technol, Sch Informat Sci, Nomi 9231211, Japan.
EM chen-fan@jaist.ac.jp; damien.de-lannay@uclouvain.be;
   christophe.devleeschouwer@uclouvain.be
FU FP7 European project APIDIS; WIST2 WALCOMO; WIST3 SPORTIC Walloon Region
   projects; Belgian NSF; Japan [23700110]; Grants-in-Aid for Scientific
   Research [23700110] Funding Source: KAKEN
FX Manuscript received November 01, 2010; revised March 05, 2011; accepted
   August 12, 2011. Date of publication August 30, 2011; date of current
   version November 18, 2011. This work was supported in part by the FP7
   European project APIDIS, by the WIST2 WALCOMO and WIST3 SPORTIC Walloon
   Region projects, by the Belgian NSF, and by the Japan Grant-in-Aid for
   Young Scientists (B) No.23700110. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Yap-Peng Tan.
CR Alahi A., 2009, J MATH IMAGING VIS, V41, P39
   Alahi A., 2009, P ICDSC 09 COM IT
   Albanese M, 2006, INFORM SYST, V31, P679, DOI 10.1016/j.is.2005.12.003
   [Anonymous], P MMSP 05
   Bömcke E, 2009, IEEE INT CON MULTI, P1554, DOI 10.1109/ICME.2009.5202804
   Chen BW, 2009, IEEE T MULTIMEDIA, V11, P295, DOI 10.1109/TMM.2008.2009703
   Chen F, 2011, IEEE T CIRC SYST VID, V21, P193, DOI 10.1109/TCSVT.2011.2106271
   Chen F, 2010, IEEE INT CON MULTI, P837, DOI 10.1109/ICME.2010.5582561
   Chen F, 2010, COMPUT VIS IMAGE UND, V114, P667, DOI 10.1016/j.cviu.2010.01.005
   De Vleeschouwer C., 2008, P NEM SUMM 08
   Delannay D, 2009, 2009 THIRD ACM/IEEE INTERNATIONAL CONFERENCE ON DISTRIBUTED SMART CAMERAS, P15
   Devaux F.-O., 2010, EVENT DETECTION ALGO
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   EVERETT H, 1963, OPER RES, V11, P399, DOI 10.1287/opre.11.3.399
   Ferman AM, 2003, IEEE T MULTIMEDIA, V5, P244, DOI 10.1109/TMM.2003.811617
   Fleuret F, 2008, IEEE T PATTERN ANAL, V30, P267, DOI 10.1109/TPAMI.2007.1174
   Gong Y., 2004, Nippon Electric, Patent No. [US6751776 (B1), 6751776]
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jung C., 2006, Samsung Electronics, Patent No. [JP2006148932, 2006148932]
   Khan SM, 2009, IEEE T PATTERN ANAL, V31, P505, DOI 10.1109/TPAMI.2008.102
   Khan SM, 2006, LECT NOTES COMPUT SC, V3954, P133
   Kubicek R., 2008, P ICCVG 08, P1
   Li Z, 2005, IEEE T CIRC SYST VID, V15, P1245, DOI 10.1109/TCSVT.2005.854230
   MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003
   Murphy N., 2004, Publication info, Patent No. [IE20040412 (A1), 20040412]
   Murphy N., 2005, Audio-visual sequence analysis, Patent No. [WO2005124686 A1, 2005124686]
   Owens J., 2006, TELEVISION SPORTS PR, V4th
   Pan H., 2004, US Patent, Patent No. [US 20040017389A1, 20040017389]
   Papaoulakis N., 2008, AREA 08 OCT 2008 VAN, P105
   Parisot P., 2011, P ICIP 11
   Qian R., 2004, Method for automatic extraction of semantically significant events from video, Patent No. [US6721454 (B1), 6721454]
   Suh Bongwon, 2003, P ACM S US INT SOFTW, P95, DOI DOI 10.1145/964696.964707
   Tseng BL, 2003, P SOC PHOTO-OPT INS, V5242, P14, DOI 10.1117/12.512987
   Vronay D., 2006, US Patent, Patent No. 20060251384
   Vronay D., 2006, US Patent, Patent No. 20060251383
   Xie X, 2006, IEEE T MULTIMEDIA, V8, P707, DOI 10.1109/TMM.2006.876294
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Yong Rui, 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P450
NR 38
TC 43
Z9 51
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2011
VL 13
IS 6
BP 1381
EP 1394
DI 10.1109/TMM.2011.2166379
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 852GD
UT WOS:000297343400017
DA 2024-07-18
ER

PT J
AU Chen, MJ
   Xu, MT
   Fränti, P
AF Chen, Minjie
   Xu, Mantao
   Franti, Pasi
TI Adaptive Context-Tree-Based Statistical Filtering for Raster Map Image
   Denoising
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Context-tree modeling; raster map image; statistical filtering
ID LOSSLESS COMPRESSION; SPARSE; FIELDS; NOISE
AB Filtering of raster map images is chosen as a case study of a more general class of palette-indexed images for the denoising problem of images with a discrete number of output colors. Statistical features of local context are analyzed to avoid damage to pixel-level patterns, which is frequently caused by conventional filters. We apply a universal statistical filter using context-tree modeling via a selective context expansion capturing those pixel combinations that are present in the image. The selective context expansion makes it possible to use a much larger spatial neighborhood, with a feasible time and memory complexity, than fixed-size templates. We improve the existing context-tree approaches in two aspects: Firstly, in order to circumvent the context contamination problem, a context-merging strategy is applied where multiple similar contexts are considered in the conditional probability estimation procedure. Secondly, we study a specific continuous-input-finite-output problem in which the map images are corrupted by additive Gaussian noise. Performance comparisons with competitive filters demonstrate that the proposed algorithm provides robust noise filtering performance and good structure preservation in all test cases without any a priori information on the statistical properties of the noise.
C1 [Chen, Minjie; Franti, Pasi] Univ Eastern Finland, Sch Comp, Joensuu, Finland.
   [Xu, Mantao] Shanghai Dianji Univ, Sch Elect Engn, Shanghai, Peoples R China.
C3 University of Eastern Finland; Shanghai Dianji University
RP Chen, MJ (corresponding author), Univ Eastern Finland, Sch Comp, Joensuu, Finland.
EM mchen@cs.joensuu.fi; xumt@sdju.edu.cn; franti@cs.joensuu.fi
OI Chen, Minjie/0000-0002-5335-1352
FU East Finland Graduate School in Computer Science and Engineering (ECSE);
   EU (EAKR); Tekniikan edistamissaatio (TES); Nokia Scholarship; China NSF
   [61072146]; Shanghai SCST [10PJ1404400]
FX Manuscript received October 31, 2010; revised March 08, 2011 and June
   29, 2011; accepted August 15, 2011. Date of publication August 30, 2011;
   date of current version November 18, 2011. The work of M. Chen was
   supported by East Finland Graduate School in Computer Science and
   Engineering (ECSE), MOPSI project EU (EAKR), Tekniikan edistamissaatio
   (TES), and Nokia Scholarship. The work of M. Xu was supported by China
   NSF (Grant No 61072146) and Shanghai SCST (Grant No 10PJ1404400). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Zicheng Liu.
CR Akimov A, 2007, IEEE T IMAGE PROCESS, V16, P114, DOI 10.1109/TIP.2006.887721
   [Anonymous], 2002, ACM SIGMOD RECORD
   Barbu A, 2009, IEEE T IMAGE PROCESS, V18, P2451, DOI 10.1109/TIP.2009.2028254
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Chen MJ, 2009, IEEE IMAGE PROC, P3953, DOI 10.1109/ICIP.2009.5414038
   Chen MJ, 2010, IEEE INT CON MULTI, P394, DOI 10.1109/ICME.2010.5583054
   Chen Y., 2009, IMAGE SIGNAL PROCESS
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Gemelos GA, 2006, IEEE T SIGNAL PROCES, V54, P2263, DOI 10.1109/TSP.2006.874295
   Gimel'farb G, 2004, LECT NOTES COMPUT SC, V3138, P477
   Henderson Thomas C., 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P376, DOI 10.1109/ICDAR.2009.31
   Khotanzad A, 2003, IEEE T PATTERN ANAL, V25, P18, DOI 10.1109/TPAMI.2003.1159943
   Kopylov P, 2004, PROCEEDINGS OF THE FOURTH IASTED INTERNATIONAL CONFERENCE ON VISUALIZATION, IMAGING, AND IMAGE PROCESSING, P837
   Kopylov P, 2004, IEEE IMAGE PROC, P267
   Leyk S, 2010, GEOINFORMATICA, V14, P1, DOI 10.1007/s10707-008-0074-z
   Lukac R, 2003, PATTERN RECOGN LETT, V24, P1889, DOI 10.1016/S0167-8655(03)00016-3
   Mavridis D, 2009, IEEE IMAGE PROC, P1633, DOI 10.1109/ICIP.2009.5413720
   Morillas S, 2009, IEEE T IMAGE PROCESS, V18, P1452, DOI 10.1109/TIP.2009.2019305
   Ordentlich E, 2005, 2005 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY (ISIT), VOLS 1 AND 2, P1270
   Ordentlich E, 2003, IEEE IMAGE PROC, P117
   Podlasov A., 2007, P INT C SIGN IM TECH, P467
   Portilla J, 2003, IEEE T IMAGE PROCESS, V12, P1338, DOI 10.1109/TIP.2003.818640
   RISSANEN J, 1983, IEEE T INFORM THEORY, V29, P656, DOI 10.1109/TIT.1983.1056741
   Roth S, 2005, PROC CVPR IEEE, P860
   Smolka B, 2005, REAL-TIME IMAGING, V11, P389, DOI 10.1016/j.rti.2005.07.003
   Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weinberger MJ, 1996, IEEE T IMAGE PROCESS, V5, P575, DOI 10.1109/83.491334
   Weissman T, 2005, IEEE T INFORM THEORY, V51, P5, DOI 10.1109/TIT.2004.839518
   Yu JM, 2006, IEEE T INFORM THEORY, V52, P4789, DOI 10.1109/TIT.2006.883626
   Zhai GT, 2009, IEEE IMAGE PROC, P3845, DOI 10.1109/ICIP.2009.5414252
   Zhao Q., 2009, P IEEE INT C IM PROC, P2397
   Zhou H., 2008, SIGNAL PROCESS, V18, P406
   Zhu X, 2010, IEEE IMAGE PROC, P1145, DOI 10.1109/ICIP.2010.5651376
NR 35
TC 5
Z9 6
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2011
VL 13
IS 6
BP 1195
EP 1207
DI 10.1109/TMM.2011.2166538
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 852GD
UT WOS:000297343400002
DA 2024-07-18
ER

PT J
AU Ni, BB
   Song, Z
   Yan, SC
AF Ni, Bingbing
   Song, Zheng
   Yan, Shuicheng
TI Web Image and Video Mining Towards Universal and Robust Age Estimator
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Age estimation; face detection; internet vision; multi-instance
   regression; video tracking
ID REGRESSION
AB In this paper, we present an automatic web image and video mining framework with the ultimate goal of building a universal human age estimator based on facial information, which is applicable to all ethnic groups and various image qualities. On one hand, a large (391 k) yet noisy human aging image database is collected from Flickr and Google Image using a set of human age-related text queries. Multiple human face detectors based on distinctive techniques are adopted for noise-prune face detection. For each image, the detected faces with high detection confidences constitute a bag of face instances. We further remove the outliers via principal component analysis (PCA), which results in a condensed image database with about 175 k face instances. A robust multi-instance regressor learning algorithm is then developed to learn the kernel regression-based human age estimator in the presence of bag label noises. On the other hand, about 10 k video clips are downloaded from YouTube. We extract tracked face sequences from these video clips. Although their age labels are unknown, the tracked faces within a sequence are naturally with identical ages. This age-consistence constraint for face pairs is used as an extra regularizer to enhance the robustness of the age estimator. The derived human age estimator is extensively evaluated on three benchmark human aging databases, and without taking any images from these benchmark databases as training samples, comparable age estimation accuracies with the state-of-the-art results are achieved.
C1 [Ni, Bingbing] Adv Digital Sci Ctr, Singapore 138632, Singapore.
   [Song, Zheng; Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore.
C3 National University of Singapore
RP Ni, BB (corresponding author), Adv Digital Sci Ctr, Singapore 138632, Singapore.
EM bingbing.ni@adsc.com.sg; zheng.s@nus.edu.sg; eleyans@nus.edu.sg
RI Yan, Shuicheng/HCI-1431-2022
FU CSIDM [CSIDM-200803]; National Research Foundation (NRF)
FX Manuscript received December 22, 2010; revised July 12, 2011; accepted
   August 25, 2011. Date of publication September 08, 2011; date of current
   version November 18, 2011. This work was supported in part by CSIDM
   Project No. CSIDM-200803 under a grant from the National Research
   Foundation (NRF) administered by the Media Development Authority (MDA)
   of Singapore. A short version of this manuscript was published in ACM
   Multimedia 2009 [26]. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Jia Li.
CR [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2000, International Conference on Machine Learning (ICML)
   [Anonymous], 2008, INT J COMPUT VIS
   [Anonymous], FG NET FG NET AGING
   Chen YX, 2006, IEEE T PATTERN ANAL, V28, P1931, DOI 10.1109/TPAMI.2006.248
   Cheng B., 2009, P ACM INT C MULT, P291
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Fu Y, 2008, IEEE T MULTIMEDIA, V10, P578, DOI 10.1109/TMM.2008.921847
   Fu Y, 2010, IEEE T PATTERN ANAL, V32, P1955, DOI 10.1109/TPAMI.2010.36
   Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733
   Guo G., 2008, P IEEE INT WORKSH AP
   Guo G., 2008, P INT WORKSH SEM LEA
   Guo G., 2010, P IEEE INT WORKSH AN
   Guo G., 2009, P IEEE INT C COMP VI
   Guo G., 2009, P IEEE INT WORKSH HU
   Guo GD, 2008, IEEE T IMAGE PROCESS, V17, P1178, DOI 10.1109/TIP.2008.924280
   Hao Q., 2010, P INT WORLD WID WEB
   Hayashi J, 2001, VSMM 2001: SEVENTH INTERNATIONAL CONFERENCE ON VIRTUAL SYSTEMS AND MULTIMEDIA, PROCEEDINGS, P439
   Hinton G., 2002, P 15 INT C NEUR INF
   Ji Rongrong., 2009, P 17 ASS COMP MACH I, P105, DOI DOI 10.1145/1631272.1631289
   Joliffe I.T., 1986, Principal Component Analysis
   Keeler J. D., 1991, Advances in neural information processing systems, P557
   Kwon YH, 1999, COMPUT VIS IMAGE UND, V74, P1, DOI 10.1006/cviu.1997.0549
   Lanitis A, 2004, IEEE T SYST MAN CY B, V34, P621, DOI 10.1109/TSMCB.2003.817091
   Li Z., 2010, P IEEE INT WORKSH AN
   Maron O, 1998, ADV NEUR IN, V10, P570
   Ni B., 2009, Proceedings of the 17th ACM international conference on Multimedia, P85
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Rudin W., 1978, Principles of mathematical analysis, Vthird
   Soumya R., 2001, ICML, P425
   Su Y., 2010, P IEEE INT C AC SPEE
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Viola P., 2005, P NEUR INF PROC SYST
   Yan S, 2008, PR IEEE COMP DESIGN, P142, DOI 10.1109/ICCD.2008.4751853
   Yan SC, 2008, IEEE T INF FOREN SEC, V3, P698, DOI 10.1109/TIFS.2008.2006585
   Yanai K., 2006, P INT WORLD WID WEB
   ZHANG Q, 2001, P NEUR INF PROC SYST
   Zhou Z., 2007, P NEUR INF PROC SYST
NR 38
TC 48
Z9 51
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2011
VL 13
IS 6
BP 1217
EP 1229
DI 10.1109/TMM.2011.2167317
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 852GD
UT WOS:000297343400004
DA 2024-07-18
ER

PT J
AU Yang, LJ
   Geng, B
   Cai, Y
   Hanjalic, A
   Hua, XS
AF Yang, Linjun
   Geng, Bo
   Cai, Yang
   Hanjalic, Alan
   Hua, Xian-Sheng
TI Object Retrieval Using Visual Query Context
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Context; image retrieval; object retrieval
ID IMAGE RETRIEVAL; ATTENTION
AB Object retrieval aims at retrieving images containing objects similar to the query object captured in the region of interest (ROI) of the query image. Boosted by the invention and wide popularity of SIFT image features and bag-of-visual-words image representation, object retrieval has progressed significantly in the past years and has already found deployment in real-life applications and products. While existing object retrieval methods perform well in many cases, they may fail to return satisfactory results if the ROI specified by the user is inaccurate or if the object captured there is too small to be represented using discriminative features and consequently to be matched with similar objects in the image collection. In order to improve the object retrieval performance also in these difficult cases, we propose in this paper an object retrieval method that exploits the information about the visual context of the query object and employ it to compensate for possible uncertainty in feature-based query object representation. Contextual information is drawn from the visual elements surrounding the query object in the query image. We consider the ROI as an uncertain observation of the latent search intent and the saliency map detected for the query image as a prior. Then a language modeling approach is employed to devise a contextual object retrieval (COR) model. There, the relevance score is determined based on the search intent scores that are inferred from the uncertain ROI and the saliency prior. The usefulness of the contextual information for object retrieval and the effectiveness of the proposed COR model are demonstrated and evaluated on three representative image datasets.
C1 [Yang, Linjun; Hua, Xian-Sheng] Microsoft Res Asia, Beijing, Peoples R China.
   [Geng, Bo] Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing 100871, Peoples R China.
   [Cai, Yang] Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.
   [Hanjalic, Alan] Delft Univ Technol, Delft Multimedia Informat Retrieval Lab, Delft, Netherlands.
C3 Microsoft Research Asia; Microsoft; Peking University; Zhejiang
   University; Delft University of Technology
RP Yang, LJ (corresponding author), Microsoft Res Asia, Beijing, Peoples R China.
EM linjuny@microsoft.com; gengbo@cis.pku.edu.cn; caiyang@zju-cadcg.cn;
   a.hanjalic@tudelft.nl; xshua@microsoft.com
RI Cai, Yang/AAM-8962-2020
OI Cai, Yang/0000-0002-1071-7158
CR Allan J., 2003, SIGIR FORUM, V37, P31, DOI DOI 10.1145/945546.945549
   [Anonymous], 2009, P 17 ACM INT C MULTI
   Bai XF, 2007, IEEE IC COMP COM NET, P1
   Belkin Nicholas J., 2008, SIGIR Forum, V42, P47, DOI 10.1145/1394251.1394261
   Callan J., 2003, REP DISC GROUP CONT
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Galleguillos C, 2010, COMPUT VIS IMAGE UND, V114, P712, DOI 10.1016/j.cviu.2010.02.004
   Geng B, 2009, INT CONF DAT MIN WOR, P158, DOI 10.1109/ICDMW.2009.114
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Kennedy Lyndon., 2007, Proceedings of the 15th International Conference on Multimedia, P631, DOI DOI 10.1145/1291233.1291384
   Lafferty J., 2001, SIGIR, P111, DOI DOI 10.1145/383952.383970
   Lawrence S., 2000, IEEE DATA ENG B, V23, P25
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo JB, 2009, IEEE T MULTIMEDIA, V11, P193, DOI 10.1109/TMM.2008.2009179
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Manning Christopher D., 2008, INTRO INFORM RETRIEV
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Oliva A, 2007, TRENDS COGN SCI, V11, P520, DOI 10.1016/j.tics.2007.09.009
   Rabinovich A, 2007, IEEE I CONF COMP VIS, P1237, DOI 10.1109/iccv.2007.4408986
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Sinha P., 2008, P IEEE INT C SEM COM
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Torralba A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P273
   TSOTSOS JK, 1995, ARTIF INTELL, V78, P507, DOI 10.1016/0004-3702(95)00025-9
   Wang J., 2007, IMAGE VIDEO MATTING, V3
   Yang X., 2010, P INT WORKSH MOB VIS
   Zha ZJ, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1823746.1823747
   Zhai C., 2009, Statistical Language Models for Information Retrieval
   Zhai CX, 2004, ACM T INFORM SYST, V22, P179, DOI 10.1145/984321.984322
   Zhou XS, 2003, MULTIMEDIA SYST, V8, P536, DOI 10.1007/s00530-002-0070-3
NR 34
TC 25
Z9 26
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2011
VL 13
IS 6
BP 1295
EP 1307
DI 10.1109/TMM.2011.2162399
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 852GD
UT WOS:000297343400010
DA 2024-07-18
ER

PT J
AU Mehrotra, S
   Li, J
   Huang, YZ
AF Mehrotra, Sanjeev
   Li, Jin
   Huang, Ying-Zong
TI Optimizing FEC Transmission Strategy for Minimizing Delay in Lossless
   Sequential Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Forward error correction; lossless data streaming; real-time
   communications
AB As cloud computing is taking off, the presence of high-performance interactive Internet applications is exploding. By nature, these applications require responsive client-server data exchange and lossless, in-order delivery. Previous work has shown that by using forward error correction (FEC), it is possible to reduce the data streaming latency caused by retransmissions of lost packets. However, the prior schemes only send FEC packets when there are no original packets pending transmission. In this paper, we further expand the hybrid FEC-ARQ protocol and show that sometimes, the transmission latency can be further reduced by pre-empting original data packets with FEC packets. We have formulated the decision of whether to send new original data packets, FEC packets, or resend original data packets as a transmission policy. An optimal transmission policy is selected to minimize the delay experienced by the application subject to a constraint on the amount of overhead. By using this optimal policy, we significantly improve the delay performance over straightforward FEC schemes while controlling the amount of overhead due to FEC.
C1 [Mehrotra, Sanjeev; Li, Jin] Microsoft Res, Redmond, WA 98033 USA.
   [Huang, Ying-Zong] MIT, Cambridge, MA 02139 USA.
C3 Microsoft; Massachusetts Institute of Technology (MIT)
RP Mehrotra, S (corresponding author), Microsoft Res, Redmond, WA 98033 USA.
EM sanjeevm@microsoft.com; jinl@microsoft.com; zong@mit.edu
CR [Anonymous], P INT C MULT COMP
   [Anonymous], P INFOCOM
   [Anonymous], P INT C MULT EXP JUL
   [Anonymous], WORLDWIDE VIRTUAL MA
   [Anonymous], P IEEE MULT SIGN PRO
   [Anonymous], Smooth Streaming: The Official Microsoft IIS Site" Available
   [Anonymous], P ACM SIGCOMM 98
   [Anonymous], P INFOCOM APR
   [Anonymous], P INT C MULT EXP JUN
   [Anonymous], P IEEE IWQOS JUN
   [Anonymous], P WORKSH MULT SIGN P
   [Anonymous], MSRTR2010126
   [Anonymous], P PASS ACT MEAS C PA
   [Anonymous], P ACM SIGMETRICS PER
   [Anonymous], P IEEE GLOB DEC
   Chou P. A., 2000, Proceedings DCC 2000. Data Compression Conference, P440, DOI 10.1109/DCC.2000.838184
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   Floyd S., 1999, RFC
   Rizzo Luigi., 1998, ACM SIGMOBILE MOBILE, V2, P23, DOI DOI 10.1145/584017.584020
   Tan K., 2006, P INF, P1
   Tsugawa Tomoaki, 2007, PACK VID 2007, P294
   Wei DX, 2006, IEEE ACM T NETWORK, V14, P1246, DOI 10.1109/TNET.2006.886335
NR 22
TC 6
Z9 9
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 1066
EP 1076
DI 10.1109/TMM.2011.2153193
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300019
DA 2024-07-18
ER

PT J
AU Song, MS
   Zhang, C
   Florencio, D
   Kang, HG
AF Song, Myung-Suk
   Zhang, Cha
   Florencio, Dinei
   Kang, Hong-Goo
TI An Interactive 3-D Audio System With Loudspeakers
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Head tracking; loudspeaker; room modeling; spatial audio; 3-D audio
AB Traditional 3-D audio systems using two loudspeakers often have a limited sweet spot and may suffer from poor performance in reverberant environments. This paper presents a novel binaural 3-D audio system that actively combines head tracking and room modeling into 3-D audio synthesis. The user's head position and orientation are first tracked by a webcam-based 3-D head tracker. The system then improves its robustness to head movement and strong early reflections by incorporating the tracking information and an explicit room model into the binaural synthesis and crosstalk cancellation process. Sensitivity analysis on the room model shows that the method is reasonably robust to modeling errors. Subjective listening tests confirm that the proposed 3-D audio system significantly improves the users' perception and ability for localization.
C1 [Song, Myung-Suk; Kang, Hong-Goo] Yonsei Univ, Sch Elect & Elect Engn, Seoul 120749, South Korea.
   [Zhang, Cha; Florencio, Dinei] Microsoft Res, Redmond, WA 98052 USA.
C3 Yonsei University; Microsoft
RP Song, MS (corresponding author), Yonsei Univ, Sch Elect & Elect Engn, Seoul 120749, South Korea.
EM earth112@dsp.yonsei.ac.kr; chazhang@microsoft.com; dinei@microsoft.com;
   hgkang@yonsei.ac.kr
RI Kang, Hong-Goo/G-8545-2012
CR ALLEN JB, 1979, J ACOUST SOC AM, V65, P943, DOI 10.1121/1.382599
   [Anonymous], 1994, HRTF MEASUREMENTS KE
   BA D, 2010, P INT C AC SPEECH SI
   Bauck J, 1996, J AUDIO ENG SOC, V44, P683
   BERANEK L, 1996, J ACOUST SOC AM, P436
   BERKHOUT AJ, 1993, J ACOUST SOC AM, V93, P2764, DOI 10.1121/1.405852
   CAI Q, 2010, P IEEE WORKSH AN MOD
   COOPER DH, 1989, J AUDIO ENG SOC, V37, P3
   FOO K, 1998, P 104 CONV AUD ENG S
   GARDNER WG, 1997, THESIS MIT CAMBRIDGE
   GEORGIOU P, 2000, P 109 CONV AUD ENG S
   Kim S, 2008, J AUDIO ENG SOC, V56, P243
   KIMBER D, 2009, P INT C IMM TEL
   Kubota A, 2007, IEEE SIGNAL PROC MAG, V24, P10, DOI 10.1109/MSP.2007.905873
   Kyriakakis C, 1998, P IEEE, V86, P941, DOI 10.1109/5.664281
   KYRIAKAKIS C, 1998, P 135 M AC SOC AM SE, V6, P3753
   KYRIAKAKIS C, 1998, P 105 CONV AUD ENG S
   LENTZ T, 2002, P 21 AUD ENG SOC C A
   Lentz T, 2006, J AUDIO ENG SOC, V54, P283
   LIM J, 2001, P INT C AC SPEECH SI
   Lopez J. J., 1999, P 2 COST G6 WORKSH D
   LOPEZ JJ, 1999, P AUD ENG SOC 16 INT
   Malham DG, 1995, COMPUT MUSIC J, V19, P58, DOI 10.2307/3680991
   Mouchtaris A, 1998, 1998 IEEE SECOND WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P155, DOI 10.1109/MMSP.1998.738928
   Mouchtaris A, 2000, IEEE T MULTIMEDIA, V2, P77, DOI 10.1109/6046.845012
   NELSON PA, 1992, IEEE T SIGNAL PROCES, V40, P1621, DOI 10.1109/78.143434
   Pulkki V, 1997, J AUDIO ENG SOC, V45, P456
   PULLKI V, 2001, THESIS HELSINKI U TE
   RIBEIRO F, 2010, P IEEE INT C MULT EX
   Ribeiro F, 2010, IEEE T AUDIO SPEECH, V18, P1781, DOI 10.1109/TASL.2010.2052250
   SONG M, 2010, P IEEE INT WORKSH HO
   SONG M, 2010, P IEEE INT WORKSH MU
   Song PJ, 2007, PACIFIC ASIA CONFERENCE ON INFORMATION SYSTEMS 2007, SECTIONS 1-6
   TOOLE FE, 1986, J AUDIO ENG SOC, V34, P227
   ZHOU Y, 2003, P IEEE C COMP VIS PA
NR 35
TC 12
Z9 15
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 844
EP 855
DI 10.1109/TMM.2011.2162581
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300002
DA 2024-07-18
ER

PT J
AU Chen, KW
   Lai, CC
   Lee, PJ
   Chen, CS
   Hung, YP
AF Chen, Kuan-Wen
   Lai, Chih-Chuan
   Lee, Pei-Jyun
   Chen, Chu-Song
   Hung, Yi-Ping
TI Adaptive Learning for Target Tracking and True Linking Discovering
   Across Multiple Non-Overlapping Cameras
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Brightness transfer function; camera network; non-overlapping cameras;
   spatio-temporal relationship; visual surveillance; visual tracking
ID OBJECTS; TIME
AB To track targets across networked cameras with disjoint views, one of the major problems is to learn the spatio-temporal relationship and the appearance relationship, where the appearance relationship is usually modeled as a brightness transfer function. Traditional methods learning the relationships by using either hand-labeled correspondence or batch-learning procedure are applicable when the environment remains unchanged. However, in many situations such as lighting changes, the environment varies seriously and hence traditional methods fail to work. In this paper, we propose an unsupervised method which learns adaptively and can be applied to long-term monitoring. Furthermore, we propose a method that can avoid weak links and discover the true valid links among the entry/exit zones of cameras from the correspondence. Experimental results demonstrate that our method outperforms existing methods in learning both the spatio-temporal and the appearance relationship, and can achieve high tracking accuracy in both indoor and outdoor environment.
C1 [Chen, Kuan-Wen; Hung, Yi-Ping] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10764, Taiwan.
   [Lai, Chih-Chuan; Lee, Pei-Jyun; Chen, Chu-Song; Hung, Yi-Ping] Natl Taiwan Univ, Inst Networking & Multimedia, Taipei 10764, Taiwan.
   [Chen, Chu-Song; Hung, Yi-Ping] Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
   [Chen, Chu-Song] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 115, Taiwan.
   [Chen, Chu-Song; Hung, Yi-Ping] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10764, Taiwan.
C3 National Taiwan University; National Taiwan University; Academia Sinica
   - Taiwan; Academia Sinica - Taiwan; National Taiwan University
RP Chen, KW (corresponding author), Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10764, Taiwan.
EM song@iis.sinica.edu.tw; hung@csie.ntu.edu.tw
RI Chen, Kuan-Wen/AAJ-3870-2020
OI Chen, Kuan-Wen/0000-0002-4159-201X
FU National Science Council, Taiwan [NSC 98-2221-E-002-127-MY3, NSC
   98-2221-E-001-012-MY3]; Ministry of Economic Affairs, Taiwan
   [99-EC-17-A-02-S1-032]
FX This work was supported in part by the National Science Council, Taiwan,
   under Grants NSC 98-2221-E-002-127-MY3 and NSC 98-2221-E-001-012-MY3,
   and in part by the Ministry of Economic Affairs, Taiwan, under Grant
   99-EC-17-A-02-S1-032. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Paal Halvorsen.
CR [Anonymous], 2008, P BRIT MACH VIS C
   Cai Q, 1999, IEEE T PATTERN ANAL, V21, P1241, DOI 10.1109/34.809119
   Chen K. W., 2008, P IEEE C COMP VIS PA, P1
   Collins RT, 2001, P IEEE, V89, P1456, DOI 10.1109/5.959341
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761
   Cormen TH, 1999, INTRO ALGORITHMS
   Dellaert F., 2000, Addressing the correspondence problem: A Markov chain Monte Carlo approach
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   DETMOLD H, 2007, P ACM IEEE INT C DIS, P195
   DICK AR, 2004, P AUSTR JOINT C ART, P160
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Gheissari N., 2006, 2006 IEEE computer society conference on computer vision and pattern recognition (CVPR'06), V2, P1528
   Gilbert A, 2008, COMPUT VIS IMAGE UND, V111, P43, DOI 10.1016/j.cviu.2007.06.005
   Girgensohn A, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1167
   HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.1093/biomet/57.1.97
   Huang T, 1997, INT JOINT CONF ARTIF, P1276
   Javed O, 2008, COMPUT VIS IMAGE UND, V109, P146, DOI 10.1016/j.cviu.2007.01.003
   Kettnaker V., 1999, IEEE C COMPUTER VISI, P252
   Khan S, 2003, IEEE T PATTERN ANAL, V25, P1355, DOI 10.1109/TPAMI.2003.1233912
   Kuan-Wen Chen, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P145, DOI 10.1109/ICPR.2010.44
   Kuo CH, 2010, LECT NOTES COMPUT SC, V6311, P383
   Makris D, 2005, IEEE T SYST MAN CY B, V35, P397, DOI 10.1109/TSMCB.2005.846652
   Makris D, 2004, PROC CVPR IEEE, P205
   Mittal A, 2000, PROC CVPR IEEE, P160, DOI 10.1109/CVPR.2000.854767
   Nguyen HT, 2007, IEEE T PATTERN ANAL, V29, P52, DOI 10.1109/TPAMI.2007.250599
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pasula H, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P1160
   Porikli F, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P653
   Shafique K., 2008, 2008 IEEE WORKSH APP, P1
   Song MZ, 2005, PROC SPIE, V5803, P174, DOI 10.1117/12.601724
   Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677
   Stauffer C, 2003, PROC CVPR IEEE, P259
   STAUFFER C, 2005, P IEEE WORKSH MOT VI, P96
   Swain D., 1990, 3 INT C COMPUTER VIS, P390, DOI [DOI 10.1109/ICCV.1990.139558, 10.1109/ICCV.1990.139558]
   Tieu K, 2005, IEEE I CONF COMP VIS, P1842
   Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196
   Wang X., 2008, P IEEE C COMP VIS PA
   Xie BL, 2004, IMAGE VISION COMPUT, V22, P117, DOI 10.1016/j.imavis.2003.07.003
NR 38
TC 49
Z9 52
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2011
VL 13
IS 4
BP 625
EP 638
DI 10.1109/TMM.2011.2131639
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 805EX
UT WOS:000293710300004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ndjiki-Nya, P
   Köppel, M
   Doshkov, D
   Lakshman, H
   Merkle, P
   Müller, K
   Wiegand, T
AF Ndjiki-Nya, Patrick
   Koeppel, Martin
   Doshkov, Dimitar
   Lakshman, Haricharan
   Merkle, Philipp
   Mueller, Karsten
   Wiegand, Thomas
TI Depth Image-Based Rendering With Advanced Texture Synthesis for 3-D
   Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Depth image based rendering; inpainting; texture synthesis; view
   synthesis; 3-D video
AB A depth image-based rendering (DIBR) approach with advanced inpainting methods is presented. The DIBR algorithm can be used in 3-D video applications to synthesize a number of different perspectives of the same scene, e. g., from a multiview-video-plus-depth (MVD) representation. This MVD format consists of video and depth sequences for a limited number of original camera views of the same natural scene. Here, DIBR methods allow the computation of additional new views. An inherent problem of the view synthesis concept is the fact that image information which is occluded in the original views may become visible, especially in extrapolated views beyond the viewing range of the original cameras. The presented algorithm synthesizes these occluded textures. The synthesizer achieves visually satisfying results by taking spatial and temporal consistency measures into account. Detailed experiments show significant objective and subjective gains of the proposed method in comparison to the state-of-the-art methods.
C1 [Ndjiki-Nya, Patrick; Koeppel, Martin; Doshkov, Dimitar; Lakshman, Haricharan; Merkle, Philipp; Mueller, Karsten; Wiegand, Thomas] Heinrich Hertz Inst HHI, Fraunhofer Inst Telecommun, Image Proc Dept, D-10587 Berlin, Germany.
   [Wiegand, Thomas] Berlin Inst Technol, Dept Telecommun Syst, Sch Elect Engn & Comp Sci, D-10587 Berlin, Germany.
C3 Fraunhofer Gesellschaft; Technical University of Berlin
RP Ndjiki-Nya, P (corresponding author), Heinrich Hertz Inst HHI, Fraunhofer Inst Telecommun, Image Proc Dept, D-10587 Berlin, Germany.
EM patrick.ndjiki-nya@hhi.fraunhofer.de; martin.koppel@hhi.fraun-hofer.de;
   dimitar.doshkov@hhi.fraunhofer.de;
   haricharan.lakshman@hhi.fraunhofer.de; philipp.merkle@hhi.fraunhofer.de;
   karsten.mueller@hhi.fraun-hofer.de; twiegand@ieee.org
CR [Anonymous], P SIGGRAPH
   [Anonymous], P SIGGRAPH
   [Anonymous], 2001, Schooling for Tomorrow
   [Anonymous], P SIGGRAPHS, DOI DOI 10.1145/218380.218446
   [Anonymous], THESIS U N CAROLINA
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Bishop C. M., 1995, NEURAL NETWORKS PATT
   Chen WY, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P1315
   Cheng C.M., 2008, Proc. IEEE MILCOM 2008, P1
   Cheng K, 2010, INT CONF COMP SCI, P181, DOI 10.1109/ICCSIT.2010.5565092
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   DEBONET JS, 1997, P SIGGRAPH 97, P361, DOI DOI 10.1145/258734.258882
   DORETTO G, 2004, INT J COM VISION FEB, P91
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Georgiev T., 2004, Workshop on Applications of Computer Vision (ECCV 2004), P1, DOI DOI 10.1145/1050330.1050437
   Georgiev T, 2006, LECT NOTES COMPUT SC, V3954, P56
   HAYES J, 2007, P ACM SIGGRAPH SAN D, P1
   International Telecommunication Union Telecommunication Standardization Sector (ITU-T) and International Organization for Standardization/International Electrotechnical Commission (ISO/IEC) JTC 1, 2004, ADV VID COD GEN AUD
   International Telecommunication Union Telecommunication Standardization Sector (ITU-T) and International Organization for Standardization/International Electrotechnical Commission (ISO/IEC) JTC 1, 2003, ADV VID COD GEN AUD
   International Telecommunication Union Telecommunication Standardization Sector (ITU-T) and International Organization for Standardization/International Electrotechnical Commission (ISO/IEC) JTC 1, 2005, ADV VID COD GEN AUD
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lakshman H, 2010, INT CONF ACOUST SPEE, P786, DOI 10.1109/ICASSP.2010.5494974
   LEE P.W., 2010, SAE Technical Paper, 2010-01-1676, P1
   MORI Y, 2009, IMAGE COMMUNICATION, V24, P65
   Müller K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/438148
   NDJIKINYA P, 2009, P INT S VIS COMP LAS, P1144
   Oh KJ, 2009, PCS: 2009 PICTURE CODING SYMPOSIUM, P233
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Schmeing M, 2010, 2010 3DTV C TRUE VIS, P1, DOI DOI 10.1109/3DTV.2010.5506596
   Schreer O, 2005, 3D VIDEOCOMMUNICATION: ALGORITHMS, CONCEPTS AND REAL-TIME SYSTEMS IN HUMAN CENTRED COMMUNICATION, P1, DOI 10.1002/0470022736
   Tanimoto M., 2009, ISOIECJTC1SC29WG11M1
   Tanimoto M., 2008, ISOIECJTC1SC29WG11M1
   Telea A., 2004, Journal of Graphics Tools, V9, P23, DOI 10.1080/10867651.2004.10487596
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xue Jiufei, 2010, Proceedings of the 2010 Asia-Pacific Conference on Wearable Computing Systems (APWC 2010), P147, DOI 10.1109/APWCS.2010.43
   Zhang L, 2005, IEEE T BROADCAST, V51, P191, DOI 10.1109/TBC.2005.846190
   Zinger S, 2010, J VIS COMMUN IMAGE R, V21, P533, DOI 10.1016/j.jvcir.2010.01.004
NR 39
TC 159
Z9 174
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2011
VL 13
IS 3
BP 453
EP 465
DI 10.1109/TMM.2011.2128862
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765UI
UT WOS:000290733700005
OA Green Published
DA 2024-07-18
ER

PT J
AU Wang, M
   Yang, KY
   Hua, XS
   Zhang, HJ
AF Wang, Meng
   Yang, Kuiyuan
   Hua, Xian-Sheng
   Zhang, Hong-Jiang
TI Towards a Relevant and Diverse Search of Social Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Diversity; social image search; tag
ID SIMILARITY; TIME
AB Recent years have witnessed the great success of social media websites. Tag-based image search is an important approach to accessing the image content on these websites. However, the existing ranking methods for tag-based image search frequently return results that are irrelevant or not diverse. This paper proposes a diverse relevance ranking scheme that is able to take relevance and diversity into account by exploring the content of images and their associated tags. First, it estimates the relevance scores of images with respect to the query term based on both the visual information of images and the semantic information of associated tags. Then, we estimate the semantic similarities of social images based on their tags. Based on the relevance scores and the similarities, the ranking list is generated by a greedy ordering algorithm which optimizes average diverse precision, a novel measure that is extended from the conventional average precision. Comprehensive experiments and user studies demonstrate the effectiveness of the approach. We also apply the scheme for web image search reranking, and it is shown that the diversity of search results can be enhanced while maintaining a comparable level of relevance.
C1 [Wang, Meng] Microsoft Res Asia, Media Comp Grp, Beijing 100080, Peoples R China.
   [Hua, Xian-Sheng] Microsoft Res Asia, Internet Media Grp, Beijing 100080, Peoples R China.
   [Yang, Kuiyuan] Univ Sci & Technol China, Dept Automat, Hefei 230027, Anhui, Peoples R China.
   [Zhang, Hong-Jiang] Microsoft Adv Technol Ctr, Beijing 100080, Peoples R China.
C3 Microsoft; Microsoft Research Asia; Microsoft; Microsoft Research Asia;
   Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft
RP Wang, M (corresponding author), Microsoft Res Asia, Media Comp Grp, Beijing 100080, Peoples R China.
EM eric.mengwang@gmail.com; yky@ustc.edu; xshua@microsoft.com;
   hjzhang@microsoft.com
CR [Anonymous], P ACM INT C MULT
   [Anonymous], P MULT ACM
   [Anonymous], 2008, 2008 IEEE C COMPUTER, DOI DOI 10.1109/CVPR.2008.4587383
   Carbonell J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P335, DOI 10.1145/290941.291025
   Cilibrasi RL, 2007, IEEE T KNOWL DATA EN, V19, P370, DOI 10.1109/TKDE.2007.48
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   GOFFMAN W, 1964, INFORM STORAGE RET, V2, P73, DOI 10.1016/0020-0271(64)90006-3
   Hsu W. H., 2006, MULTIMEDIA '06, P35
   Jaimes A, 2003, ICICS-PCM 2003, VOLS 1-3, PROCEEDINGS, P16
   Kennedy L., 2009, Proc. Workshop on Web-scale Multimedia Corpus, P17
   Kennedy L.S., 2006, Proc. ACM Multimedia Information Retrieval, P249, DOI DOI 10.1145/1178677.1178712
   King BruceM., 2003, STAT REASONING PSYCH, VFourth
   Li J, 2008, IEEE T PATTERN ANAL, V30, P985, DOI 10.1109/TPAMI.2007.70847
   Li Xirong., 2008, Proceedings of the 1st ACM International Conference on Multimedia Information Retrieval, MIR '08, P180
   Liu D, 2009, IEEE INT CON MULTI, P350, DOI 10.1109/ICME.2009.5202506
   Nah FFH, 2004, BEHAV INFORM TECHNOL, V23, P153, DOI 10.1080/01449290410001669914
   ROBERTSON SE, 1977, J DOC, V33, P294, DOI 10.1108/eb026647
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Song K., 2006, ACM Multimedia, P707
   Srinivasan SH., 2008, MM 08, P881, DOI DOI 10.1145/1459359.1459512
   Sun A., 2009, Proc. SIGMM Workshop on Social Media, P19
   Van Leuken Reinier H, 2009, P 18 INT C WORLD WID, P341, DOI 10.1145/1526709.1526756
   Wang B, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P353, DOI 10.1109/ICME.2006.262509
   Wang M., 2009, P ACM WORKSH WEB SCA
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   WEINBERGER KQ, 2008, MM 08, P111
   Wu L., 2008, P ACM INT C MULTIMED, P31, DOI DOI 10.1145/1459359.1459364
   ZHAI C, 2006, INFORM PROCESS MANAG, P31
   Zhai C., 2006, INFORM PROCESS MANAG, P10
   Zhao WL, 2009, IEEE T IMAGE PROCESS, V18, P412, DOI 10.1109/TIP.2008.2008900
   Zhu Jianke., 2008, P 16 ACM INT C MULTI, P41
NR 31
TC 146
Z9 150
U1 0
U2 51
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2010
VL 12
IS 8
BP 829
EP 842
DI 10.1109/TMM.2010.2055045
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 681ZW
UT WOS:000284365100005
DA 2024-07-18
ER

PT J
AU Nimmagadda, Y
   Kumar, K
   Lu, YH
AF Nimmagadda, Yamini
   Kumar, Karthik
   Lu, Yung-Hsiang
TI Adaptation of Multimedia Presentations for Different Display Sizes in
   the Presence of Preferences and Temporal Constraints
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content adaptation; display size; multimedia presentations; preferences;
   spatio-temporal layout
ID INTERNET CONTENT; PROXY
AB Multimedia content adaptation has become important due to many devices with different amounts of resources like display sizes, memories, and computation capabilities. Existing studies perform content adaptation on web pages and other media files that have the same start times and durations. In this paper, we present a content adaptation method for multimedia presentations constituting media files with different start times and durations. We perform adaptation based on preferences and temporal constraints specified by authors and generate an order of importance among media files. Our method can automatically generate layouts by computing the locations, start times, and durations of the media files. We compare three solutions to generate layouts: 1) exhaustive search, 2) dynamic programming, and 3) greedy algorithms. We analyze the presentations by varying screen resolutions, media files, preferences, and temporal constraints. Our analysis shows that screen utilizations are 92%, 85%, and 80% for the three methods, respectively. The time to generate layouts for a presentation with 100 media files is 1200, 17, and 10 s, respectively, for the three methods.
C1 [Nimmagadda, Yamini; Kumar, Karthik; Lu, Yung-Hsiang] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47906 USA.
C3 Purdue University System; Purdue University
RP Nimmagadda, Y (corresponding author), Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47906 USA.
EM ynimmaga@purdue.edu; kumar25@purdue.edu
FU NSF [CNS-0347466, CCF-0541267]
FX Manuscript received July 24, 2009; revised January 30, 2010; accepted
   May 15, 2010. Date of publication June 07, 2010; date of current version
   October 15, 2010. This work was supported in part by NSF CNS-0347466 and
   CCF-0541267. Any opinions, findings, and conclusions or recommendations
   in the projects are those of the investigators and do not necessarily
   reflect the views of the sponsors. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Nadia Magnenat-Thalmann.
CR Ali K, 2008, LECT NOTES COMPUT SC, V5166, P247, DOI 10.1007/978-3-540-85412-8_24
   ALLEN JF, 1983, COMMUN ACM, V26, P832, DOI 10.1145/182.358434
   Bazargan K, 2000, IEEE DES TEST COMPUT, V17, P68, DOI 10.1109/54.825678
   BLEKAS A, 2006, P 2006 INT CROSS DIS, P79
   Borning A, 2000, MULTIMEDIA SYST, V8, P177, DOI 10.1007/s005300000043
   Boutilier C, 2004, COMPUT INTELL-US, V20, P137, DOI 10.1111/j.0824-7935.2004.00234.x
   Brafman RI, 2004, ACM T INFORM SYST, V22, P503, DOI 10.1145/1028099.1028100
   BRAFMAN RI, 2006, P PREF SPEC INF APPL
   Buchanan M. C., 1993, Proceedings ACM Multimedia 93, P341, DOI 10.1145/166266.168415
   BUCHANAN MC, 1993, P INT WORKSH NETW OP, P237
   Cardellini V., 2000, Proceedings of the Ninth International Conference on Information and Knowledge Management. CIKM 2000, P520, DOI 10.1145/354756.354861
   Chang AY, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P2211, DOI 10.1109/ICME.2004.1394709
   Edelkamp S., 2006, INT C AUTOMATED PLAN, P38
   He J, 2007, IEEE T KNOWL DATA EN, V19, P127, DOI 10.1109/TKDE.2007.250590
   Hsiao JL, 2008, IEEE T MULTIMEDIA, V10, P646, DOI 10.1109/TMM.2008.921852
   JAN RH, 2006, INT J COMPUT TELECOM, P953
   JORDAN M, 1998, P EUR C ART INT WORK
   JOU C, 2007, P INT C WEB INF SYST, P230
   KINNO A, 2004, P INT MULT MOD C
   Kong J., 2006, ACM Transactions on Computer-Human Interaction, V13, P268, DOI 10.1145/1165734.1165739
   Kong J, 2004, 2004 IEEE SYMPOSIUM ON VISUAL LANGUAGES AND HUMAN CENTRIC COMPUTING: PROCEEDINGS, P99
   Kong J, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P629
   Lee E, 2007, MUE: 2007 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND UBIQUITOUS ENGINEERING, PROCEEDINGS, P620
   Li CS, 1998, INT CONF ACOUST SPEE, P3789, DOI 10.1109/ICASSP.1998.679709
   LUM WY, 2005, P INT C PAR DISTR SY
   MEGINO FB, 2008, P INT WORKSH IM AN M, P223
   Mohan R, 1999, IEEE T MULTIMEDIA, V1, P104, DOI 10.1109/6046.748175
   NAGAO K, 1990, P PAC RIM INT C ART
   NIMMAGADDA Y, 2009, P IEEE INT C MULT EX
   Nimmagadda Y., SOURCE CODE EXHAUSTI
   Shen B, 2004, IEEE T MULTIMEDIA, V6, P375, DOI 10.1109/TMM.2003.822791
   Solon AJ, 2007, IEEE J-STSP, V1, P254, DOI 10.1109/JSTSP.2007.901521
   Stephens R., SOURCE CODE PACKING
   TANG X, 2002, P INT C PAR PROC
   Thang TC, 2005, IEE P-VIS IMAGE SIGN, V152, P374, DOI 10.1049/ip-vis:20045084
   YANG SJH, 2007, P INT C INN COMP INF
NR 36
TC 8
Z9 8
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2010
VL 12
IS 7
BP 650
EP 664
DI 10.1109/TMM.2010.2052024
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 670SV
UT WOS:000283448500004
DA 2024-07-18
ER

PT J
AU Zhang, SL
   Huang, QM
   Jiang, SQ
   Gao, W
   Tian, Q
AF Zhang, Shiliang
   Huang, Qingming
   Jiang, Shuqiang
   Gao, Wen
   Tian, Qi
TI Affective Visualization and Retrieval for Music Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Affective content analysis; affective visualization; dimensional
   affective model; support vector regression
AB In modern times, music video (MV) has become an important favorite pastime to people because of its conciseness, convenience, and the ability to bring both audio and visual experiences to audiences. As the amount of MVs is explosively increasing, it has become an important task to develop new techniques for effective MV analysis, retrieval, and management. By stimulating the human affective response mechanism, affective video content analysis extracts the affective information contained in videos, and, with the affective information, natural, user-friendly, and effective MV access strategies could be developed. In this paper, a novel integrated system (i.MV) is proposed for personalized MV affective analysis, visualization, and retrieval. In i.MV, we not only perform the personalized MV affective analysis, which is a challenging and insufficiently covered problem in current affective content analysis field, but also propose novel affective visualization to convert the abstract affective states intuitive and friendly to users. Based on the affective analysis and visualization, affective information based MV retrieval is achieved. Both comprehensive experiments and subjective user studies on a large MV dataset demonstrate that our personalized affective analysis is more effective than the previous algorithms. In addition, affective visualization is proved to be more suitable for affective information-based MV retrieval than the commonly used affective state representation strategies.
C1 [Zhang, Shiliang; Jiang, Shuqiang; Gao, Wen] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Huang, Qingming] Chinese Acad Sci, Grad Univ, Beijing 100190, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; University of Texas System; University of Texas at San Antonio
   (UTSA)
RP Zhang, SL (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM slzhang@jdl.ac.cn; qitian@cs.utsa.edu
RI Huang, Qingming/GLR-3473-2022
OI Huang, Qingming/0000-0002-3025-7099
FU National Natural Science Foundation of China [60833006, 60702035];
   National Hi-Tech Development Program under the 863 Program of China
   [2006AA01Z117]; Beijing Natural Science Foundation [4092042]
FX Manuscript received December 09, 2009; revised March 23, 2010; accepted
   April 19, 2010. Date of current version September 15, 2010. This work
   was supported in part by the National Natural Science Foundation of
   China under Grant 60833006 and Grant 60702035, the National Hi-Tech
   Development Program under the 863 Program of China under Grant
   2006AA01Z117, and by the Beijing Natural Science Foundation under Grant
   4092042. The associate editor coordinating the review of this manuscript
   and approving it for publication was Dr. Nicu Sebe.
CR Adams B, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P2025, DOI 10.1109/ICME.2006.262611
   AGNIHOTRI L, 2005, P ACM INT WORKSH MIR, P31
   [Anonymous], USER MODEL USER ADAP
   [Anonymous], ACM MULTIMEDIA
   [Anonymous], P INT C IM PROC
   [Anonymous], ACM MULTIMEDIA
   [Anonymous], ACM MULTIMEDIA
   [Anonymous], ACM MULTIMEDIA
   ARIFIN S, 2006, P IEEE ICIP, P433
   Bordwell D., 2004, FILM ART INTRO
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Chen L., 1998, Proceedings of the Second International Conference on Autonomous Agents, P132, DOI 10.1145/280765.280789
   Ching Hau Chan, 2005, 13th Annual ACM International Conference on Multimedia, P427, DOI 10.1145/1101149.1101243
   Dimitrova N., 2000, ACM MULTIMEDIA, P499
   Hanjalic A, 2006, IEEE SIGNAL PROC MAG, V23, P90, DOI 10.1109/MSP.2006.1621452
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P1114, DOI 10.1109/TMM.2005.858397
   Hirsh H, 2000, COMMUN ACM, V43, P102, DOI 10.1145/345124.345159
   Isbister K., 2006, CHI 06, P1163, DOI [DOI 10.1145/1124772.1124946, 10.1145/1124772.1124946.]
   Kang H.-B., 2003, Proceedings of the 11th ACM International Conference on Multimedia, P259
   Lang Peter., 1993, PERSPECTIVES ANGER E, P109
   Liu X., 2007, PROC ACM INT C MULTI, P461
   Lu L, 2006, IEEE T AUDIO SPEECH, V14, P5, DOI 10.1109/TSA.2005.860344
   MADDAGE NC, 2004, P 12 ANN ACM INT C M, P112, DOI DOI 10.1145/1027527.1027549
   Magnini B, 2004, USER MODEL USER-ADAP, V14, P239, DOI 10.1023/B:USER.0000028980.13669.44
   Más I, 2008, IEEE COMMUN MAG, V46, P156, DOI 10.1109/MCOM.2008.4689259
   Mehrabian A, 1996, CURR PSYCHOL, V14, P261, DOI 10.1007/BF02686918
   Nguyen GP, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1324287.1324294
   RUSSELL JA, 1977, J RES PERS, V11, P273, DOI 10.1016/0092-6566(77)90037-X
   SCHLOSBERG H, 1954, PSYCHOL REV, V61, P81, DOI 10.1037/h0054570
   SEBE N, 2007, P MIR 2007 ACM AUGSB, P299
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Venkatesh S, 2008, P IEEE, V96, P697, DOI 10.1109/JPROC.2008.916378
   Wang HL, 2006, IEEE T CIRC SYST VID, V16, P689, DOI 10.1109/TCSVT.2006.873781
   Webb GI, 2001, USER MODEL USER-ADAP, V11, P19, DOI 10.1023/A:1011117102175
   Xu M, 2005, 2005 IEEE International Conference on Multimedia and Expo (ICME), Vols 1 and 2, P622
   XU M, 2007, P IEEE SCIISP, P386
   Xu M., 2008, Proceeding of the 16th ACM International Conference on Multimedia, P677, DOI DOI 10.1145/1459359.1459457
   Yang Y.-H., 2007, P INT WORKSHOP HUMAN, P13, DOI DOI 10.1145/1290128.1290132
   Zettl H., 1998, Sight, Sound, Motion: Applied Media Aesthetics, V3rd
   Zhang SL, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P1369, DOI 10.1109/ICME.2008.4607698
   ZHANG SL, 2008, ACM MULTIMEDIA, P985
   Zhang T, 2001, IEEE T SPEECH AUDI P, V9, P441, DOI 10.1109/89.917689
NR 43
TC 62
Z9 67
U1 2
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2010
VL 12
IS 6
BP 510
EP 522
DI 10.1109/TMM.2010.2059634
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 668TB
UT WOS:000283291900005
DA 2024-07-18
ER

PT J
AU Sun, CH
   Tsao, YM
   Chien, SY
AF Sun, Chih-Hao
   Tsao, You-Ming
   Chien, Shao-Yi
TI High-Quality Mipmapping Texture Compression With Alpha Maps for Graphics
   Processing Units
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Graphics hardware; graphics processing unit; mobile graphics; texture
   compression
AB Texture compression is an important technique in graphics processing units (GPUs) for saving memory bandwidth. This paper presents a high-quality mipmapping texture compression (MTC) system with alpha maps. Based upon the wavelet transform, a hierarchical approach is adopted for mipmapping textures in the YCbCr color space and alpha channel. By inspecting the similarity between the alpha and luminance channels, the two channels are efficiently encoded together with linear prediction in the differential mode. In addition, the split mode manages textures with no strong relationship between the alpha and luminance channels. A layer overlapping technique is also proposed to reduce the texture memory bandwidth. Simulation results show that MTC can reduce the texture access traffic by 80% to 90% and provides high image quality as well. Compared with DirectX texture compression (DXTC), the most well-known texture compression with alpha maps, MTC reduces the texture access bandwidth by 30% more. VLSI implementation results show that the hardware cost of MTC is similar to that of DXTC and that MTC is suitable for integration in GPUs to provide high-quality textures with low memory bandwidth requirements.
C1 [Sun, Chih-Hao] Natl Taiwan Univ, Grad Inst Elect Engn, Media IC & Syst Lab, Taipei 106, Taiwan.
   Natl Taiwan Univ, Dept Elect Engn, Taipei 106, Taiwan.
C3 National Taiwan University; National Taiwan University
RP Sun, CH (corresponding author), Natl Taiwan Univ, Grad Inst Elect Engn, Media IC & Syst Lab, Taipei 106, Taiwan.
EM r95943038@ntu.edu.tw; eddie@video.ee.ntu; sychien@cc.ec.ntu.edu.tw
OI Chien, Shao-Yi/0000-0002-0634-6294
CR Akenine-Möller T, 2003, ACM T GRAPHIC, V22, P801, DOI 10.1145/882262.882348
   [Anonymous], 2002, JPEG2000: Image Compression Fundamentals, Standards, and Practice
   [Anonymous], 2003, HWWS'03: Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware
   Beers A. C., 1996, P SIGGRAPH 96
   DELP EJ, 1979, IEEE T COMMUN, V27, P1335, DOI 10.1109/TCOM.1979.1094560
   HAKURA ZS, 1997, P 24 INT S COMP ARCH, P108
   IGEHY H, 1998, P GRAPH HARDW 1998
   Iourcha K., 1999, U.S. Patent, Patent No. 5956431
   LIU D, 1994, IEEE J SOLID-ST CIRC, V29, P663, DOI 10.1109/4.293111
   *MIRC, 2006, DIRECTX 10 0 SDK
   PEERCY M, 1997, P 24 ANN C COMP GRAP, P303, DOI DOI 10.1145/258734.258873
   Pereberin A. V., 1999, P GRAPHICON 99, P195
   PRATT W.K., 1991, DIGITAL IMAGE PROCES, V2
   Stachera J., 2006, P WSCG 2006, P108
   STROM J, 2004, SKETCHES PROGRAM SIG
   Strom J., 2007, Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics Hardware, P49, DOI [10.2312/EGGH/EGGH07/049-054, DOI 10.2312/EGGH/EGGH07/049-054]
   Strom Jacob, 2005, Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics Hardware, P63, DOI DOI 10.1145/1071866.1071877
   WILLIAMS L, 1983, P SIGGRAPH 83
   [No title captured]
NR 19
TC 11
Z9 12
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2009
VL 11
IS 4
BP 589
EP 599
DI 10.1109/TMM.2009.2017637
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 449LA
UT WOS:000266330900002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tu, W
   Steinbach, E
   Muhammad, M
   Li, XT
AF Tu, Wei
   Steinbach, Eckehard
   Muhammad, Muhammad
   Li, Xiaoting
TI Proxy Caching for Video-on-Demand Using Flexible Starting Point
   Selection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Early playout; popularity-aware caching; proxy caching; segment-based
   caching; video-on-demand; video streaming
ID ALGORITHMS
AB In this paper, we propose a novel proxy caching scheme for video-on-demand (VoD) services. Our approach is based on the observation that streaming video users searching for some specific content or scene pay most attention to the initial delay, while a small shift of the starting point is acceptable. We present results from subjective VoD tests that relate waiting time and starting point deviation to user satisfaction. Based on this relationship as well as the dynamically changing popularity of video segments, we propose an efficient segment-based caching algorithm, which maximizes the user satisfaction by trading off between the initial delay and the deviation of starting point. Our caching scheme supports interactive video cassette recorder (VCR) functionalities and enables cache replacement with a much finer granularity compared to previously proposed segment-based approaches. Our experimental results show a significantly improved user satisfaction for our scheme compared to conventional caching schemes.
C1 [Tu, Wei; Steinbach, Eckehard] Tech Univ Munich, Inst Media Technol LMT, D-80290 Munich, Germany.
   [Muhammad, Muhammad] German Aerosp Ctr, Oberpfaffenhofen, Germany.
   [Li, Xiaoting] Fujitsu Microelect Europe GmbH, D-63225 Langen, Germany.
C3 Technical University of Munich; Helmholtz Association; German Aerospace
   Centre (DLR); Fujitsu Ltd
RP Tu, W (corresponding author), Tech Univ Munich, Inst Media Technol LMT, D-80290 Munich, Germany.
EM wei.tu@mytum.de; eckehard.steinbach@tum.de; mmuhammed@hotmail.com;
   xiaoling.li@fme.fujitsu.com
RI TU, Wei/H-4073-2014
OI TU, Wei/0000-0002-0255-4037; Steinbach, Eckehard/0000-0001-8853-2703
CR ACHARYA S, 1998, P ACM SPIE MULT COMP
   *AZ STAT U, VID TRAC NETW PERF E
   BALAFOUTIS E, 2003, P IEEE INT C TEL ICT
   Chang SH, 2005, COMPUT COMMUN, V28, P1852, DOI 10.1016/j.comcom.2005.01.010
   Chen SQ, 2007, IEEE T MULTIMEDIA, V9, P1062, DOI 10.1109/TMM.2007.898943
   Chen SQ, 2005, IEEE MULTIMEDIA, V12, P59, DOI 10.1109/MMUL.2005.56
   GRIWODZ C, 1997, P ACM INT C MULT SEA
   HOFMANN M, 1999, BL01134599040904TM
   Jin SD, 2003, MULTIMEDIA SYST, V9, P386, DOI 10.1007/s00530-003-0109-0
   Kao CF, 2007, IEEE T MULTIMEDIA, V9, P221, DOI 10.1109/TMM.2006.886259
   LI K, 2005, P IEEE INT C WEB INT
   LIU J, 2004, P IEEE INT C COMP CO
   Ma WH, 2002, IEEE T MULTIMEDIA, V4, P539, DOI 10.1109/TMM.2002.806536
   MIAO Z, 1999, P INT PACK VID WORKS
   Miao ZR, 2002, IEEE J SEL AREA COMM, V20, P1315, DOI 10.1109/JSAC.2002.802061
   Oh HR, 2007, IEEE T MULTIMEDIA, V9, P1535, DOI 10.1109/TMM.2007.906590
   Oh HR, 2006, J VIS COMMUN IMAGE R, V17, P57, DOI 10.1016/j.jvcir.2005.01.003
   PARK SH, 2001, P IEEE INT PAR DISTR
   QU W, 2005, P IEEE INT C SEM KNO
   REJAIE R, 2001, P INT WORKSH NETW OP
   SEN S, 1999, P IEEE INT C COMP CO
   SHEN L, 2007, P IEEE INT C MULT EX
   VideoLAN, VLC MED PLAYER
   Wang JZ, 2007, IEEE T MULTIMEDIA, V9, P147, DOI 10.1109/TMM.2006.886379
   Wu KL, 2004, IEEE T MULTIMEDIA, V6, P770, DOI 10.1109/TMM.2004.834870
   WU KL, 2001, P INT C WORLD WID WE
   YAN H, 2003, P INT C PAR DISTR CO
   YU J, 2006, P IEEE INT C ADV INF
   Zhang ZL, 2000, IEEE ACM T NETWORK, V8, P429, DOI 10.1109/90.865072
   Zheng C., 2005, P ACM WORKSH ADV PEE
   Zipf GK, 1950, J CLIN PSYCHOL, V6, P306
NR 31
TC 21
Z9 25
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2009
VL 11
IS 4
BP 716
EP 729
DI 10.1109/TMM.2009.2017621
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 449LA
UT WOS:000266330900013
DA 2024-07-18
ER

PT J
AU Borges, PVK
   Mayer, J
   Izquierdo, E
AF Borges, Paulo Vinicius Koerich
   Mayer, Joceli
   Izquierdo, Ebroul
TI Robust and Transparent Color Modulation for Text Data Hiding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Color modulation; hardcopy watermarking; print and scan
ID IMAGE; FREQUENCY; FEATURES
AB This paper improves the use of text color modulation (TCM) as a reliable text document data hiding method. Using TCM, the characters in a document have their color components modified (possibly unperceptually) according to a side message to be embedded. This work presents a detection metric and an analysis determining the detection error rate in TCM, considering an assumed print and scan (PS) channel model. In addition, a perceptual impact model is employed to evaluate the perceptual difference between a modified and a non-modified character. Combining this perceptual model and the results from the detection error analysis it is possible to determine the optimum color modulation values. The proposed detection metric also exploits the orientation characteristics of color halftoning to reduce the error rate. In particular, because color halftoning algorithms use different screen orientation angles for each color channel, this is used as an effective feature to detect the embedded message. Experiments illustrate the validity of the analysis and the applicability of the method.
C1 [Borges, Paulo Vinicius Koerich; Izquierdo, Ebroul] Univ London, Dept Elect Engn, Multimedia & Vis Res Grp, London E1 4NS, England.
   [Mayer, Joceli] Univ Fed Santa Catarina, Dept Elect Engn, LPDS, BR-88040900 Florianopolis, SC, Brazil.
C3 University of London; Universidade Federal de Santa Catarina (UFSC)
RP Borges, PVK (corresponding author), Univ London, Dept Elect Engn, Multimedia & Vis Res Grp, London E1 4NS, England.
EM vini@ieee.org; mayer@eel.ufsc.br; ebroul.izquierdo@elec.qmul.ac.uk
RI Mayer, Joceli/B-2035-2008; Borges, Paulo/B-7538-2011
OI Borges, Paulo/0000-0001-8137-7245
FU CNPq [202288/2006-4]
FX Manuscript received September 10, 2007; revised June 24, 2008. Current
   version published December 10, 2008. This work was supported by CNPq,
   Proc. No. 202288/2006-4. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Deepa Kundur.
CR [Anonymous], 1992, R. woods digital image processing
   BALA R, 2003, DIGITAL COLOR IMAGIN, pCH5
   Baqai FA, 2005, IEEE SIGNAL PROC MAG, V22, P87, DOI 10.1109/MSP.2005.1407718
   Borges PVK, 2008, IEEE T MULTIMEDIA, V10, P1277, DOI 10.1109/TMM.2008.2004906
   BORGES PV, 2007, SIGNAL PROCESS, V87
   BORGES PVK, 2007, P IEEE INT C AC SPEE
   Bovik A.C., 2000, HDB IMAGE VIDEO PROC
   Brassil JT, 1999, P IEEE, V87, P1181, DOI 10.1109/5.771071
   *CIE, 1971, PSYCH COL TERMS P S2, V15
   Davey MC, 2001, IEEE T INFORM THEORY, V47, P687, DOI 10.1109/18.910582
   DEGUILLAUME F, 2004, Patent No. 10949318
   Duda R. O., 2000, PATTERN CLASSIFICATI
   Glassner A, 1997, IEEE COMPUT GRAPH, V17, P97, DOI 10.1109/38.626975
   Haley GM, 1999, IEEE T IMAGE PROCESS, V8, P255, DOI 10.1109/83.743859
   Huang D, 2001, IEEE T CIRC SYST VID, V11, P1237, DOI 10.1109/76.974678
   Langelaar GC, 2000, IEEE SIGNAL PROC MAG, V17, P20, DOI 10.1109/79.879337
   LENNIE P, 2000, CURRENT BIOL, V10
   Liu F, 1996, IEEE T PATTERN ANAL, V18, P722, DOI 10.1109/34.506794
   Manolakis D. G., 2000, Statistical and Adaptive Signal Processing
   MOJSILOVIC A, 1999, P IEEE INT C IM PROC, V3
   MULLIGAN JB, 1990, P SOC PHOTO-OPT INS, V1249, P261, DOI 10.1117/12.19676
   Prins N, 2003, J OPT SOC AM A, V20, P401, DOI 10.1364/JOSAA.20.000401
   QUINTELA ND, 2003, P SPIE
   TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999
   VILLAN R, 2005, P SPIE PHOTONICS W E, V7
   VILLAN R, 2006, P SPIE ELECT IMAGING
   VILLAN R, 2007, P SPIE IST ELECT IMA, V9
   VINICIUS P, 2007, P IEEE INT C IM PROC
   VINICIUS P, 2006, P IEEE INT C AC SPEE
   ZHANG X, 1997, P SPIE
   Zhang XM, 1998, SIGNAL PROCESS, V70, P201, DOI 10.1016/S0165-1684(98)00125-X
NR 31
TC 21
Z9 24
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2008
VL 10
IS 8
BP 1479
EP 1489
DI 10.1109/TMM.2008.2007294
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 390KS
UT WOS:000262163600005
DA 2024-07-18
ER

PT J
AU Zhu, WT
AF Zhu, Wen Tao
TI A Cost-Efficient Secure Multimedia Proxy System
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimedia proxy; security and performance; multikey RSA; digital
   content protection; media key block
ID REVERSIBLE PARAMETRIC SEQUENCES; IMPLEMENTATION; DESIGN
AB Due to limited server and network capacities, proxies are introduced for streaming applications to cache multimedia content from the media source to enable a scalable service and to improve the user experience. In this paper we first review the security aspect of a proxy encryption framework recently presented by Yeung et al. featuring the multikey RSA technique. Then addressing the performance aspect, we propose a redesigned cost-efficient architecture, which is based on a media key management mechanism substantially different from Yeung et al.'s framework and improves the overall system significantly.
C1 Chinese Acad Sci, Graz Univ, State Key Lab Informat Secur, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences
RP Zhu, WT (corresponding author), Chinese Acad Sci, Graz Univ, State Key Lab Informat Secur, Beijing 100049, Peoples R China.
EM wtzhu@ieee.org
CR [Anonymous], 2001, LECT NOTES COMPUTER
   Kuo WC, 2007, IEEE T MULTIMEDIA, V9, P420, DOI 10.1109/TMM.2006.886386
   Lotspiech J, 2004, P IEEE, V92, P898, DOI 10.1109/JPROC.2004.827353
   Lotspiech J, 2002, COMPUTER, V35, P57, DOI 10.1109/MC.2002.1023789
   Schneier B., 2015, APPL CRYPTOGRAPHY, VSecond
   Yeung SF, 2005, IEEE T MULTIMEDIA, V7, P330, DOI 10.1109/TMM.2005.843361
NR 6
TC 8
Z9 11
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 1214
EP 1220
DI 10.1109/TMM.2008.2001376
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600023
DA 2024-07-18
ER

PT J
AU Jie, S
   Zi, HA
   Shen, HT
   Zhou, XF
   Lim, EP
   Li, YJ
AF Jie Shao
   Zi Huang
   Shen, Heng Tao
   Zhou, Xiaofang
   Lim, Ee-Peng
   Li, Yijun
TI Batch nearest neighbor search for video retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content-based retrieval; high-dimensional indexing; multimedia
   databases; query processing
ID SIMILARITY SEARCH; INDEXING METHOD; TREE; ALGORITHM
AB To retrieve similar videos to a query clip from a large database, each video is often represented by a sequence of high-dimensional feature vectors. Typically, given a query video containing m feature vectors, an independent nearest neighbor (NN) search for each feature vector is often first performed. After completing all the NN searches, an overall similarity is then computed, i.e., a single content-based video retrieval usually involves m individual NN searches. Since normally nearby feature vectors in a video are similar, a large number of expensive random disk accesses are expected to repeatedly occur, which crucially affects the overall query performance. Batch nearest neighbor (BNN) search is stated as a batch operation that performs a number of individual NN searches. This paper presents a novel approach towards efficient high-dimensional BNN search called dynamic query ordering (DQO) for advanced optimizations of both I/O and CPU costs. Observing the overlapped candidates (or search space) of a pervious query may help to further reduce the candidate sets of subsequent queries, DQO aims at progressively finding a query order such that the common candidates among queries are fully utilized to maximally reduce the total number of candidates. Modelling the candidate set relationship of queries by a candidate overlapping graph (COG), DQO iteratively selects the next query to be executed based on its estimated pruning power to the rest of queries with the dynamically updated COG. Extensive experiments are conducted on real video datasets and show the significance of our BNN query processing strategy.
C1 [Jie Shao; Shen, Heng Tao; Zhou, Xiaofang] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
   [Zi Huang] Open Univ, Knowledge Media Inst, Milton Keynes MK7 6AA, Bucks, England.
   [Lim, Ee-Peng] Nanyang Technol Univ, Div Informat Syst, Sch Comp Engn, Singapore, Singapore.
   [Li, Yijun] Nielsen Media Res, Brisbane, Qld 4000, Australia.
C3 University of Queensland; Open University - UK; Nanyang Technological
   University
RP Jie, S (corresponding author), Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
EM jshao@itee.uq.edu.au; h.huang@open.ac.uk; shenht@itee.uq.edu.au;
   zxf@itee.uq.edu.au; aseplim@ntu.edu.sg; yijun.li@nielsen.com
RI LIM, Ee Peng/E-8562-2012; Shen, Heng Tao/ABD-5331-2021; Zhou,
   Xiangfeng/KDO-8724-2024; Zhou, Xiaofang/C-6169-2013
OI Zhou, Xiaofang/0000-0001-6343-1455; HUANG, ZI/0000-0002-9738-4949
FU Australian Research Council [DP0663272]; Australian Research Council
   [DP0663272] Funding Source: Australian Research Council
FX This work was supported by the Australian Research Council under Grant
   DP0663272. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Yong Rui.
CR Arya S, 1998, J ACM, V45, P891, DOI 10.1145/293347.293348
   Berchtold S, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P28
   Berchtold S., 2000, Proceedings of 16th International Conference on Data Engineering (Cat. No.00CB37073), P577, DOI 10.1109/ICDE.2000.839456
   Böhm C, 2000, ACM T DATABASE SYST, V25, P129, DOI 10.1145/357775.357776
   BOHM C, 2001, P 2001 ACM SIGMOD IN, P379
   Braunmuller B., 2000, Proceedings of 16th International Conference on Data Engineering (Cat. No.00CB37073), P256, DOI 10.1109/ICDE.2000.839418
   Cha GH, 2002, IEEE T MULTIMEDIA, V4, P76
   Cha GH, 2002, IEEE T MULTIMEDIA, V4, P235, DOI 10.1109/TMM.2002.1017736
   Cheung SCS, 2003, IEEE T CIRC SYST VID, V13, P59, DOI 10.1109/TCSVT.2002.808080
   Ciaccia P, 1997, PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL CONFERENCE ON VERY LARGE DATABASES, P426
   Fagin R, 2002, SIGMOD REC, V31, P109
   Ferhatosmanoglu H, 2006, INFORM SYST, V31, P512, DOI 10.1016/j.is.2005.01.001
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Jagadish HV, 2005, ACM T DATABASE SYST, V30, P364, DOI 10.1145/1071610.1071612
   KANG MH, 1994, DATA KNOWL ENG, V14, P57, DOI 10.1016/0169-023X(94)90009-4
   Kiranyaz S, 2007, IEEE T MULTIMEDIA, V9, P102, DOI 10.1109/TMM.2006.886362
   Korn F, 2000, SIGMOD REC, V29, P201, DOI 10.1145/335191.335415
   Koudas N, 2004, PROC INT CONF DATA, P6, DOI 10.1109/ICDE.2004.1319980
   Lu GJ, 2002, IEEE T MULTIMEDIA, V4, P372, DOI 10.1109/TMM.2002.802831
   Lu H, 2006, IEEE T KNOWL DATA EN, V18, P1544, DOI 10.1109/TKDE.2006.174
   Papadias D, 2004, PROC INT CONF DATA, P301, DOI 10.1109/ICDE.2004.1320006
   Papatsenko D, 2005, NAT METHODS, V2, P529, DOI 10.1038/nmeth0705-529
   Roy P., 2000, SIGMOD Record, V29, P249, DOI 10.1145/335191.335419
   Santos RF, 2001, PROC INT CONF DATA, P623, DOI 10.1109/ICDE.2001.914877
   SELLIS TK, 1988, ACM T DATABASE SYST, V13, P23, DOI 10.1145/42201.42203
   Shao J, 2007, PROC INT CONF DATA, P1370
   Shen H.T., 2005, P ACM SIGMOD INT C M, P730
   SHEN HT, 2006, P INT C DAT ENG
   SHIM K, 1994, DATA KNOWL ENG, V12, P197, DOI 10.1016/0169-023X(94)90014-0
   Tao Yufei., 2002, Proceedings of the 28th International Conference on Very Large Databases, P287
   Wang HL, 2003, J VIS COMMUN IMAGE R, V14, P150, DOI 10.1016/S1047-3203(03)00019-1
   Weber R., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P194
   XIA C, 2004, P 30 INT C VER LARG, P756
   YANG Z, 2004, P ICME, P743
   Zhang J, 2004, 16TH INTERNATIONAL CONFERENCE ON SCIENTIFIC AND STATISTICAL DATABASE MANAGEMENT, PROCEEDINGS, P297
NR 35
TC 11
Z9 11
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 409
EP 420
DI 10.1109/TMM.2008.917339
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100010
DA 2024-07-18
ER

PT J
AU Hernández, PC
   Czyz, J
   Marqués, F
   Umeda, T
   Marichal, X
   Macq, B
AF Hernandez, Pedro Correa
   Czyz, Jacek
   Marques, Ferran
   Umeda, Toshiyuki
   Marichal, Xavier
   Macq, Benoit
TI Bayesian approach for morphology-based 2-D human motion capture
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bayesian approach; geodesic distance; gestural interfaces; mathematic
   morphology; motion capture; silhouette analysis
ID 3D HUMAN MOTION
AB This paper presents a novel technique for 2-D human motion capture using a single non calibrated camera. The user's five extremities (head, hands and feet) are extracted, labeled and tracked after silhouette segmentation. As they are the minimal number of points that can be used in order to enable whole body gestural interaction, we will henceforth refer to these features as crucial points. The crucial point candidates are defined as the local maxima of the geodesic distance with respect to the center of gravity of the actor region that lie on the silhouette boundary. In order to disambiguate the selected crucial points into head, left and right foot and left and right hand classes, we propose a Bayesian framework that combines a MAP approach weighted by a prior model and the intensities of the tracked crucial points. Due to its low computational complexity, the system can run at real-time paces on standard personal computers, with an average error rate range between 2% and 7% in realistic situations, depending on the context and segmentation quality.
C1 Catholic Univ Louvain, Commun & Remote Sensing Lab, B-1348 Louvain, Belgium.
C3 Universite Catholique Louvain
RP Hernández, PC (corresponding author), Catholic Univ Louvain, Commun & Remote Sensing Lab, B-1348 Louvain, Belgium.
EM correa@tele.ucl.ac.be
RI Marques, Ferran/H-7342-2013
CR [Anonymous], P ECCV
   [Anonymous], **NON-TRADITIONAL**
   [Anonymous], P BMVC
   Baumberg A. M., 1995, LEARNING SPATIOTEMPO
   BOLT RA, 1980, P 7 ANN C COMP GRAPH, P262, DOI [DOI 10.1145/800250.807503, 10.1145/965105.807503, DOI 10.1145/965105.807503]
   Brand M., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1237, DOI 10.1109/ICCV.1999.790422
   CORREA P, 2005, P IEEE INT C IM PROC, V3, P836
   Davis JW, 1997, PROC CVPR IEEE, P928, DOI 10.1109/CVPR.1997.609439
   Delamarre Q., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P716, DOI 10.1109/ICCV.1999.790292
   DEMIRDJIAN D, 2002, P INT C MULT INT PIT
   Deutscher J, 2000, PROC CVPR IEEE, P126, DOI 10.1109/CVPR.2000.854758
   Dijkstra E. W., 1959, NUMER MATH, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]
   Felzenszwalb PF, 2000, PROC CVPR IEEE, P66, DOI 10.1109/CVPR.2000.854739
   Forsyth DA, 1997, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.1997.609399
   FUA P, 2002, INT ARCH PHOTOGRAMME, V34, P256
   Fujiyoshi H, 1998, FOURTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV'98, PROCEEDINGS, P15, DOI 10.1109/ACV.1998.732852
   GAITANIS K, 2006, P IEEE INT C IM PROC
   Gavrila DM, 1999, COMPUT VIS IMAGE UND, V73, P82, DOI 10.1006/cviu.1998.0716
   GUEZ A, 1988, P IEEE INT C NEUR NE, V2, P617
   Haralick R. M., 1992, COMPUTER ROBOT VISIO
   Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683
   Hodgins JessicaK., 1995, Proceedings of the 22nd Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 1995, Los Angeles, CA, USA, August 6-11, P71, DOI DOI 10.1145/218380.218414
   Howe NR, 2000, ADV NEUR IN, V12, P820
   Ioffe S, 2001, INT J COMPUT VISION, V43, P45, DOI 10.1023/A:1011179004708
   JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378
   JU S, 1996, P INT C AUT FAC GEST, V34, P38
   Krueger M.K., 1991, Artificial Reality, V2
   Krueger Myron W., 1985, Proceedings of the SIGCHI conference on Human factors in computing systems, P35, DOI [10.1145/1165385.317463, DOI 10.1145/1165385.317463, 10.1145/317456.317463]
   Marichal X, 2003, P SOC PHOTO-OPT INS, V5150, P41, DOI 10.1117/12.509861
   Micilotta A., 2006, P EUR C COMP VIS GRA
   Moeslund TB, 2001, COMPUT VIS IMAGE UND, V81, P231, DOI 10.1006/cviu.2000.0897
   MOESLUND TB, 1999, LIA9901 U AALB
   Mori G, 2002, LECT NOTES COMPUT SC, V2352, P666
   Pavlovic VI, 1997, IEEE T PATTERN ANAL, V19, P677, DOI 10.1109/34.598226
   PLANKERS R, 2001, COMPUT VIS IMATE UND, V81
   QUAM DL, 1990, PROC NAECON IEEE NAT, P755, DOI 10.1109/NAECON.1990.112862
   Ren XF, 2005, IEEE I CONF COMP VIS, P824
   Rosales R, 2000, PROC CVPR IEEE, P721, DOI 10.1109/CVPR.2000.854946
   Rose C., 1996, Dans Proc. SIGGRAPH '96, P147
   Serra J., 1994, MATH MORPHOLOGY ITS
   Sminchisescu C, 2005, PROC CVPR IEEE, P390
   Sminchisescu C., 2002, P INT C CENTR EUR CO
   Soille P., 2003, Morphological image analysis: principles and applications, Vsecond
   SORIANO M, 2000, P 15 INT C PATT REC
   STURMAN DJ, 1994, IEEE COMPUT GRAPH, V14, P30, DOI 10.1109/38.250916
   WANG LCT, 1991, IEEE T ROBOTIC AUTOM, V7, P489, DOI 10.1109/70.86079
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Yilmaz A, 2005, PROC CVPR IEEE, P984
NR 48
TC 8
Z9 9
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2007
VL 9
IS 4
BP 754
EP 765
DI 10.1109/TMM.2007.893342
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 170NW
UT WOS:000246671200008
DA 2024-07-18
ER

PT J
AU Khayam, SA
   Karande, SS
   Ilyas, MU
   Radha, H
AF Khayam, Syed Ali
   Karande, Shirish S.
   Ilyas, Muhammad Usman
   Radha, Hayder
TI Header detection to improve multimedia quality over wireless networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE communication systems; multimedia communication; video signal
   processing; wireless LAN
AB Wireless multimedia studies have revealed that forward error correction (FEC) on corrupted packets yields better bandwidth utilization and lower delay than retransmissions. To facilitate FEC-based recovery, corrupted packets should not dropped so that maximum number of packets is relayed to a wire- less receiver's FEC decoder. Previous studies proposed to mitigate wireless packet drops by a partial checksum that ignored payload errors. Such schemes require modifications to both transmitters and receivers, and incur packet-losses due to header errors. In this paper, we introduce a receiver-based scheme which uses the history of active multimedia sessions to detect transmitted values of corrupted packet headers, thereby improving wireless multimedia throughput. Header detection is posed as the decision-the-oretic problem of multihypothesis detection of known parameters in noise. Performance of the proposed scheme is evaluated using trace-driven video simulations on an 802.11b local area network. We show that header detection with application layer FEC provides significant throughput and video quality improvements over the conventional UDP/IP/802.11 protocol stack.
C1 Michigan State Univ, Dept Elect & Comp Engn, E Lansing, MI 48824 USA.
C3 Michigan State University
RP Khayam, SA (corresponding author), Michigan State Univ, Dept Elect & Comp Engn, E Lansing, MI 48824 USA.
EM khayamsy@egr.msu.edu; karandes@egr.msu.edu; ilyasmuh@egr.msu.edu;
   radha@egr.msu.edu
RI Ilyas, Muhammad/A-2596-2008; Ilyas, Muhammad Usman/Q-4791-2019
OI Ilyas, Muhammad Usman/0000-0003-0308-4361
CR BLAHUT RE, 1984, THEORY PRACTICE ERRO
   *ISO IEC JTC, 2001, 1SC29WG11 ISO IEC JT
   *ISO IEC JTC, 2003, 1SC29WG11 ISO IEC JT
   Khayam SA, 2003, SIGNAL PROCESS-IMAGE, V18, P575, DOI 10.1016/S0923-5965(03)00053-5
   KHAYAM SA, 2003, P IEEE INT C MULT EX
   Larzon L., 1999, P IEEE INT C COMM JU
   Larzon L., 1999, P IEEE INT WORKSH MO
   Singh A., 2001, P ACM INT WORKSH NET
   van Trees H. L, 2001, Detection, Estimation, and Modulation Theory: Part III. Radar-sonar Signal Processing and Gaussian Signal in Noise
   Zheng H., 2003, P IEEE INT C MULT EX
   Zheng HT, 2001, IEEE T MULTIMEDIA, V3, P356, DOI 10.1109/6046.944478
NR 11
TC 5
Z9 5
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2007
VL 9
IS 2
BP 377
EP 385
DI 10.1109/TMM.2006.889051
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 135CO
UT WOS:000244131900015
DA 2024-07-18
ER

PT J
AU Wang, Y
   Fang, T
   Chan, LP
   Yap, KH
AF Wang, Yu
   Fang, Tao
   Chan, Lap-Pui
   Yap, Kim-Hui
TI Two-dimensional channel coding scheme for MCTF-based scalable video
   coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE channel coding; FEC; MCTF; scalable video coding; UEP
ID UNEQUAL ERROR PROTECTION; IMAGE TRANSMISSION; INTERNET; CODES
AB The motion-compensated temporal filtering (MCTF)-based scalable video coding (SVC) provides a full scalability including spatial, temporal and signal-to-noise ratio (SNR) scalability with fine granularity, each of which may result in different visual effect. This paper addresses a novel approach of two-dimensional unequal error protection (2-D UEP) for the scalable video with a combined temporal and quality (SNR) scalability over packet-erasure channel. The bit-stream is divided into scalable subbitstreams based on the structure of MCTF. Each subbitstream is further divided into several quality layers. Unequal quantities of bits are allocated to protect different layers to obtain acceptable quality video with smooth degradation under different transmission error conditions. Experimental results are presented to show the advantage of the proposed 2-D UEP scheme over the traditional one-dimensional unequal error protection (1-D UEP) scheme. Comparing the proposed method with the 1-D UEP scheme on SNR layers, our method gives up to 0.81-dB improvement for some video sequences.
C1 Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Wang, Y (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM ywang@pmail.ntu.edu.sg; tfang@pmail.ntu.edu.sg; elpchau@ntu.edu.sg;
   ekhyap@ntu.edu.sg
RI fang, tao/IQU-3074-2023; Yap, Kim-Hui/A-5157-2011; Chau,
   Lap-Pui/A-5149-2011
OI Yap, Kim-Hui/0000-0003-1933-4986; Chau, Lap-Pui/0000-0003-4932-0593
CR [Anonymous], 1989, SEARCH OPT MACHINE L
   Bystrom M, 2000, IEEE J SEL AREA COMM, V18, P880, DOI 10.1109/49.848242
   Chang SF, 2005, P IEEE, V93, P148, DOI 10.1109/JPROC.2004.839600
   Chen S, 2004, INT J CONTROL AUTOM, V2, P1
   Choi SJ, 1999, IEEE T IMAGE PROCESS, V8, P155, DOI 10.1109/83.743851
   ELLIOTT EO, 1965, AT&T TECH J, V44, P89, DOI 10.1002/j.1538-7305.1965.tb04139.x
   Gallant M, 2001, IEEE T CIRC SYST VID, V11, P357, DOI 10.1109/76.911161
   Grangetto M, 2004, IEEE T IMAGE PROCESS, V13, P751, DOI 10.1109/TIP.2004.827233
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P511, DOI 10.1109/TCSVT.2002.800313
   Lan CF, 2004, IEEE T COMMUN, V52, P1092, DOI 10.1109/TCOMM.2004.831406
   *MPEG COMM ISO IEC, 2004, 1SC29WG11N6717 ISOIE
   Nosratinia A, 2003, IEEE T COMMUN, V51, P186, DOI 10.1109/TCOMM.2003.809256
   REICHEL J, 2004, 1SC29WG11N6716 ISOIE
   Sachs DG, 2000, PROC SPIE, V3974, P300, DOI 10.1117/12.382963
   Yang XK, 2003, SIGNAL PROCESS-IMAGE, V18, P157, DOI 10.1016/S0923-5965(02)00128-5
NR 15
TC 19
Z9 19
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2007
VL 9
IS 1
BP 37
EP 45
DI 10.1109/TMM.2006.886339
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 119VE
UT WOS:000243041500005
DA 2024-07-18
ER

PT J
AU Chen, MH
   Zakhor, A
AF Chen, Minghua
   Zakhor, Avideh
TI Multiple TFRC connections based rate control for wireless networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE All-In-One TFRC; MULTFRC; parallel connections; streaming rate control;
   TFRC; video transmission; video streaming; video over wireless; wireless
   communications
ID CONGESTION
AB Rate control is an important issue in video streaming applications for both wired and wireless networks. A widely accepted rate control method in wired networks is equation based rate control [1], in which the TCP friendly rate is determined as a function of packet loss rate, round trip time and packet size. This approach, also known as TCP friendly rate control (TFRC), assumes that packet loss in wired networks is primarily due to congestion, and as such is not applicable to wireless networks in which the bulk of packet loss is due to error at the physical layer. In this paper, we propose multiple TFRC connections as an end-to-end rate control solution for wireless video streaming. We show that this approach not only avoids modifications to the network infrastructure or network protocol, but also results in full utilization of the wireless channel. NS-2 simulations, actual experiments over 1 x RTT CDMA wireless data network, and and video streaming simulations using traces from the actual experiments, are carried out to validate, and characterize the performance of our proposed approach.
C1 Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Video & Image Proc Lab, Berkeley, CA 94720 USA.
C3 University of California System; University of California Berkeley
RP Chen, MH (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Video & Image Proc Lab, Berkeley, CA 94720 USA.
EM minghua@eecs.berkeley.edu; avz@eecs.berkeley.edu
RI Zakhor, Avideh/GYA-1602-2022; Chen, Minghua/A-7476-2012
OI Zakhor, Avideh/0000-0003-4770-6353; Chen, Minghua/0000-0003-4763-0037
CR Akan OB, 2004, IEEE ACM T NETWORK, V12, P634, DOI 10.1109/TNET.2004.833155
   [Anonymous], P ACM SIGCOMM 98
   [Anonymous], Network Simulation Version 2
   Balakrishnan H, 1997, IEEE ACM T NETWORK, V5, P756, DOI 10.1109/90.650137
   Balakrishnan H, 1998, P IEEE GLOB INT MIN
   BANSAL D, 2001, ACM SIGCOMM 2001 SAN
   Barman D, 2002, 10TH IEEE INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, PROCEEDINGS, P2, DOI 10.1109/ICNP.2002.1181381
   Biaz S, 1999, ASSET'99: 1999 IEEE SYMPOSIUM ON APPLICATION-SPECIFIC SYSTEMS AND SOFTWARE ENGINEERING & TECHNOLOGY - PROCEEDINGS, P10, DOI 10.1109/ASSET.1999.756746
   Biaz S., 1998, P 7 INT C COMP COMM
   BRAKMO LS, 1995, IEEE J SEL AREA COMM, V13, P1465, DOI 10.1109/49.464716
   Cen S, 2003, IEEE ACM T NETWORK, V11, P703, DOI 10.1109/TNET.2003.818187
   CF C, 2001, P IEEE GLOB TEL C PI, P1766
   CHEN M, 2005, P IEEE WIR COM S MUL
   Choi JH, 2002, 2002 4TH INTERNATIONAL WORKSHOP ON MOBILE AND WIRELESS COMMUNICATION NETWORK, P592, DOI 10.1109/MWCN.2002.1045834
   CROWCROFT J, 1998, ACM COMPUTER COM JUL, V28
   Ding W, 2001, PIMRC2001 SAN DIEG C, pB
   eun Kim T., 1999, Proc. ICPP Workshop, P140
   Floyd S, 2000, ACM SIGCOMM COMP COM, V30, P43, DOI 10.1145/347057.347397
   Floyd S., 1994, Computer Communication Review, V24, P8, DOI 10.1145/205511.205512
   Floyd S, 1999, IEEE ACM T NETWORK, V7, P458, DOI 10.1109/90.793002
   JA C, 1995, P IEEE S COMP COMM L, P262
   LEE JJ, 2003, P SPIE SAN JOS US JA, P104
   Liu J, 2003, P WIOPT
   MAHDAVI J, 1997, TCP FRIENDLY UNICAST
   MATHIS M, 1997, ACM COMPUTER COM JUL
   OTT DE, 2004, P IEEE INFOCOM HONG
   Parsa C., 1999, Proceedings Seventh International Conference on Network Protocols (ICNP'99), P213, DOI 10.1109/ICNP.1999.801940
   RATNAM K, 2003, INT J COMM SYST  FEB, P47
   Rendón J, 2002, 13TH IEEE INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR AND MOBILE RADIO COMMUNICATIONS, VOL 1-5, PROCEEDINGS, P1156, DOI 10.1109/PIMRC.2002.1045209
   Samaraweera NKG, 1999, IEE P-COMMUN, V146, P222, DOI 10.1049/ip-com:19990597
   Sinha P, 2002, WIREL NETW, V8, P301, DOI 10.1023/A:1013702428498
   Sinha P, 1999, P IEEE WIR COMM NETW, P953
   Tang J, 2001, IEEE INFOCOM SER, P114, DOI 10.1109/INFCOM.2001.916693
   Tobe Y., 2000, P 25 ANN IEEE C LOC
   YANG F, 2004, P IEEE INFOCOM HONGK
   Yang G, 2004, P ACM MMNS SAN DIEG
   Yang Y., 2001, Proc. of IEEE Global Telecommunications Conference, Piscataway, NJ, P2026
NR 37
TC 32
Z9 39
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2006
VL 8
IS 5
BP 1045
EP 1062
DI 10.1109/TMM.2006.879837
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 091TY
UT WOS:000241052400016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Palazzi, CE
   Ferretti, S
   Cacciaguerra, S
   Roccetti, M
AF Palazzi, Claudio E.
   Ferretti, Stefano
   Cacciaguerra, Stefano
   Roccetti, Marco
TI Interactivity-loss avoidance in event delivery synchronization for
   mirrored game architectures
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE consistency; event delivery service; interactivity; massively
   multiplayer online game; online entertainment
AB Since the expansion of their market and their challenging requirements, Massively Multiplayer Online Games are gaining increasing attention in the scientific community. One of the key factors in this kind of application is represented by the ability to rapidly deliver game events among the various players over the network. Employing in this context Mirrored Game Server architectures and adapting RED (Random Early Detection) techniques borrowed from network queuing management, we are able to show sensible benefits in upholding interactivity and scalability, whilst preserving game state consistency and game evolution fluency at the player's side.
C1 Univ Bologna, Dipartimento Sci Informaz, I-40127 Bologna, Italy.
   Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA.
C3 University of Bologna; University of California System; University of
   California Los Angeles
RP Palazzi, CE (corresponding author), Univ Bologna, Dipartimento Sci Informaz, I-40127 Bologna, Italy.
EM cpalazzi@cs.unibo.it; sferrett@cs.unibo.it; scacciag@cs.unibo.it;
   roccetti@cs.unibo.it
OI CACCIAGUERRA, Stefano/0000-0002-0605-6799; ROCCETTI,
   MARCO/0000-0003-1264-8595; PALAZZI, Claudio Enrico/0000-0002-8877-0848;
   Ferretti, Stefano/0000-0002-1911-4708
CR Armitage G, 2003, ICON 2003: 11TH IEEE INTERNATIONAL CONFERENCE ON NETWORKS, P137
   Borella MS, 2000, COMPUT COMMUN, V23, P403, DOI 10.1016/S0140-3664(99)00197-8
   Cheriton D.R., 1993, P 14 ACM S OP SYST P, P44
   Cronin E, 2004, MULTIMED TOOLS APPL, V23, P7, DOI 10.1023/B:MTAP.0000026839.31028.9f
   Farber J., 2002, P 1 WORKSHOP NETWORK, P53
   Feng W., 1999, CSETR38799 UM
   Feng WC, 1999, IEEE INFOCOM SER, P1320, DOI 10.1109/INFCOM.1999.752150
   FERRETTI S, 2004, INT J INTELLIGENT GA, V3, P7
   Fitzek F., 2002, Proc. ACM NetGames, P58
   Floyd S., ADAPTIVE RED ALGORIT
   Floyd S, 1993, IEEE ACM T NETWORK, V1, P397, DOI 10.1109/90.251892
   JEFFERSON DR, 1985, ACM T PROGR LANG SYS, V7, P404, DOI 10.1145/3916.3988
   Kunniyur SS, 2004, IEEE ACM T NETWORK, V12, P286, DOI 10.1109/TNET.2004.826291
   Lapsley D, 1999, GLOBECOM'99: SEAMLESS INTERCONNECTION FOR UNIVERSAL SERVICES, VOL 1-5, P1747, DOI 10.1109/GLOCOM.1999.832461
   Mauve M, 2004, IEEE T MULTIMEDIA, V6, P47, DOI 10.1109/TMM.2003.819751
   MILLS DL, 1991, IEEE T COMMUN, V39, P1482, DOI 10.1109/26.103043
   Morse KL, 2000, PRESENCE-TELEOP VIRT, V9, P52, DOI 10.1162/105474600566619
   PALAZZI CE, 2004, P NIME 04 DALL TX US, P157
   Pantel Lothar., 2002, NOSSDAV 02 P 12 INT, P23
   Singhal S., 1999, Networked Virtual Environments
   Srinivasan S, 1996, 1996 WINTER SIMULATION CONFERENCE PROCEEDINGS, P946, DOI 10.1145/256562.256867
   Wright S, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P1380, DOI 10.1109/ICC.2004.1312738
NR 22
TC 20
Z9 20
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2006
VL 8
IS 4
BP 874
EP 879
DI 10.1109/TMM.2006.876229
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 069BS
UT WOS:000239420300020
DA 2024-07-18
ER

PT J
AU Bertini, M
   Cucchiara, R
   Del Bimbo, A
   Prati, A
AF Bertini, Marco
   Cucchiara, Rita
   Del Bimbo, Alberto
   Prati, Andrea
TI Semantic adaptation of sport videos with user-centred performance
   analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE automatic annotation; object- and event-level compression; performance
   analysis; performance indices; semantic video adaptation; soccer sport
   video
ID MPEG-21
AB In semantic video adaptation measures of performance must consider the impact of the errors in the automatic annotation over the adaptation in relationship with the preferences and expectations of the user. In this paper, we define two new performance measures Viewing Quality Loss and Bit-rate Cost Increase, that are obtained from classical peak signal-to-noise ration (PSNR) and bit rate, and relate the results of semantic adaptation to the errors in the annotation of events and objects and the user's preferences and expectations. We present and discuss results obtained with a system that performs automatic annotation of soccer sport video highlights and applies different coding strategies to different parts of the video according to their relative importance for the end user. With reference to this framework, we analyze how highlights' statistics and the errors of the annotation engine influence the performance of semantic adaptation and reflect into the quality of the video displayed at the user's client and the increase of transmission costs.
C1 Univ Florence, Dipartimento Sistemi & Informat, I-50139 Florence, Italy.
   Univ Modena, Dipartimento Ingn Informaz, I-41100 Modena, Italy.
C3 University of Florence; Universita di Modena e Reggio Emilia
RP Bertini, M (corresponding author), Univ Florence, Dipartimento Sistemi & Informat, I-50139 Florence, Italy.
EM bertini@dsi.unifi.it; cucchiara.rita@unimore.it; del-bimbo@dsi.unifi.it;
   prati.andrea@unimore.it
RI Bertini, Marco/X-1325-2019; Prati, Andrea/B-7440-2014; Cucchiara,
   Rita/L-3006-2015
OI Bertini, Marco/0000-0002-1364-218X; Prati, Andrea/0000-0002-1211-529X;
   DEL BIMBO, ALBERTO/0000-0002-1052-8322; Cucchiara,
   Rita/0000-0002-2239-283X
CR Assfalg E, 2003, COMPUT VIS IMAGE UND, V92, P285, DOI 10.1016/j.cviu.2003.06.004
   BALDI G, 1999, P 3 INT C VIS INF SY, P171
   BERTINI M, IN PRESS MULT TOOLS
   BERTINI M, 2004, P IEEE INT C PATT RE
   Cavallaro A, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P597
   CUCCHIARA R, 2003, INT J IMAGE GRAPHICS, V3, P145
   CUCCHIARA R, 2004, J INTERNET TECHNOL S, V5, P31
   Damera-Venkata N, 2000, IEEE T IMAGE PROCESS, V9, P636, DOI 10.1109/83.841940
   Ebrahimi T, 1998, P IEEE, V86, P1109, DOI 10.1109/5.687832
   EBRAHIMI T, 2000, APS COMM NETW MULTIM, P585
   Hartley R, 2000, MULTIPLE VIEW GEOMET
   Intille SS, 2001, COMPUT VIS IMAGE UND, V81, P414, DOI 10.1006/cviu.2000.0896
   Keesman G, 1996, SIGNAL PROCESS-IMAGE, V8, P481, DOI 10.1016/0923-5965(95)00067-4
   Kim JG, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P281
   Lei ZJ, 2002, INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: CODING AND COMPUTING, PROCEEDINGS, P425
   Liang YQ, 2001, IEEE IMAGE PROC, P429, DOI 10.1109/ICIP.2001.959045
   LIN CY, 2003, P IEEE INT C IM PROC, V2, P53
   LIN CY, 2002, P IEEE INT C MULT EX, V2, P73
   Ma Y.-F., 2002, ACM MULTIMEDIA, P533
   Mohan R, 1999, IEEE T MULTIMEDIA, V1, P104, DOI 10.1109/6046.748175
   MOSS S, 2000, 2000012202 SAE
   Nagao K., 2001, IEEE Multimedia, V8, P69, DOI 10.1109/93.917973
   NELSON RC, 1994, IEEE T PATTERN ANAL, V16, P519, DOI 10.1109/34.291445
   Shanableh T, 2000, IEEE T MULTIMEDIA, V2, P101, DOI 10.1109/6046.845014
   SHIHFU C, 2001, 121 ADVENT
   Tseng BL, 2004, IEEE MULTIMEDIA, V11, P42, DOI 10.1109/MMUL.2004.1261105
   Vetro A, 2004, IEEE MULTIMEDIA, V11, P84, DOI 10.1109/MMUL.2004.1261111
   Vetro A, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P417
   WANG Z, 2002, P IEEE C AC SPEECH S
   Werner O, 1999, IEEE T IMAGE PROCESS, V8, P179, DOI 10.1109/83.743853
NR 30
TC 14
Z9 14
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2006
VL 8
IS 3
BP 433
EP 443
DI 10.1109/TMM.2006.870762
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046OX
UT WOS:000237822000001
DA 2024-07-18
ER

PT J
AU Cheng, CC
   Hsu, CT
AF Cheng, Chih-Chieh
   Hsu, Chiou-Ting
TI Fusion of audio and motion information on HMM-based highlight extraction
   for baseball games
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio feature; camera motion; hidden Markov model; highlight extraction;
   likelihood model; object motion intensity
ID VIDEO; SEGMENTATION
AB This paper aims to extract baseball game highlights based on audio-motion integrated cues. In order to better describe different audio and motion characteristics in baseball game highlights, we propose a novel representation method based on likelihood models. The proposed likelihood models measure the "likeliness" of low-level audio features and motion features to a set of predefined audio types and motion categories, respectively. Our experiments show that using the proposed likelihood representation is more robust than using low-level audio/motion features to extract the highlight. With the proposed likelihood models, we then construct an integrated feature representation by symmetrically fusing the audio and motion likelihood models. Finally, we employ a hidden Markov model (HMM) to model and detect the transition of the integrated representation for highlight segments. A series of experiments have been conducted on a 12-h video database to demonstrate the effectiveness of our proposed method and show that the proposed framework achieves promising results over a variety of baseball game sequences.
C1 ITRI, Comp & Commun Res Labs, Hsinchu 310, Taiwan.
   Natl Tsing Hua Univ, Dept Comp Sci, Multimedia Proc Lab, Hsinchu 30055, Taiwan.
C3 Industrial Technology Research Institute - Taiwan; National Tsing Hua
   University
RP Cheng, CC (corresponding author), ITRI, Comp & Commun Res Labs, Hsinchu 310, Taiwan.
EM br872514@cs.nthu.edu.tw; cthsu@cs.nthu.edu.tw
OI Hsu, Chiou-Ting/0000-0001-8857-2481
CR ALBIOL A, 2003, P ICASSP 03
   ASSFALG J, 2002, P ICES 02, V3, P1059
   BONZANINI A, 2001, P ICME 01
   Chang P, 2002, IEEE IMAGE PROC, P609
   Chang YL, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P306, DOI 10.1109/MMCS.1996.534992
   CHEN SC, 2002, P IEEE ICME, V2, P365
   CHENG CC, 2002, P PCM 2002       DEC
   Dahyot R, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P561
   Deller J.R., 2000, Discrete-Time Processing of Speech Signals, Vsecond, DOI DOI 10.1109/9780470544402.CH11
   Divakaran A, 2002, IEEE IMAGE PROC, P932
   Dufaux F, 2000, IEEE T IMAGE PROCESS, V9, P497, DOI 10.1109/83.826785
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   Figueiredo MAT, 2002, IEEE T PATTERN ANAL, V24, P381, DOI 10.1109/34.990138
   Figueiredo MAT, 2000, INT C PATT RECOG, P87, DOI 10.1109/ICPR.2000.906023
   Gatica-Perez D, 2003, IEEE T CIRC SYST VID, V13, P539, DOI 10.1109/TCSVT.2003.813428
   GONG Y, 2002, P ICME 02, V1, P26
   Gong Yihong., 2003, Multimedia Systems, V9
   Hanjalic A, 2003, IEEE IMAGE PROC, P1
   Hanjalic A, 2002, IEEE T CIRC SYST VID, V12, P90, DOI 10.1109/76.988656
   Hastie T., 1996, J ROY STAT SOC B
   Heng WJ, 2002, IEEE T MULTIMEDIA, V4, P434, DOI 10.1109/TMM.2002.806532
   Hua W, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P821, DOI 10.1109/ICME.2002.1035908
   Kawashima T, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P871, DOI 10.1109/ICIP.1998.723657
   KONDOZ AM, 1994, DIGITAL SPEECH
   Koprinska I, 2001, SIGNAL PROCESS-IMAGE, V16, P477, DOI 10.1016/S0923-5965(00)00011-4
   LEONARDI R, 2003, P ICIP 2003, V3, P401
   LI B, 2003, P ICIP 03, V1, P27
   MA YF, 2002, P ICIP 2002      SEP
   Pan H, 2001, INT CONF ACOUST SPEE, P1649, DOI 10.1109/ICASSP.2001.941253
   PETKOVIC M, 2003, P ICIP 03        SEP
   PEYRARD N, 2003, P ICIP 03        SEP
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   ROACH MJ, 2001, P ICASSP 01
   Rui Y., 2000, Proceedings ACM Multimedia 2000, P105, DOI 10.1145/354384.354443
   Shao X, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P547
   SMITH M, 1997, P CVPR 97
   SUNDARAM H, 2002, P ACM MULTIMEDIA
   Tan YP, 2000, IEEE T CIRC SYST VID, V10, P133, DOI 10.1109/76.825867
   TOVINKERE V, 2001, P ICME 01
   TSE YT, 1991, P ICASSP 91
   Wang Y, 2000, IEEE SIGNAL PROC MAG, V17, P12, DOI 10.1109/79.888862
   XIE L, 2002, P ICASSP 02, V1
   Xiong ZY, 2003, IEEE IMAGE PROC, P5
   Xiong ZY, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P401
   Xu G, 2002, INT C PATT RECOG, P831, DOI 10.1109/ICPR.2002.1048431
   XU G, 2003, P ICIP, V1, P25
   Zhang T, 2001, IEEE T SPEECH AUDI P, V9, P441, DOI 10.1109/89.917689
   ZHONG D, 2001, P IEEE INT C MULT EX, P713
NR 49
TC 52
Z9 56
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2006
VL 8
IS 3
BP 585
EP 599
DI 10.1109/TMM.2006.870726
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046OX
UT WOS:000237822000015
DA 2024-07-18
ER

PT J
AU Cheng, I
   Boulanger, P
AF Cheng, Irene
   Boulanger, Pierre
TI Adaptive online transmission of 3-D TexMesh using scale-space and visual
   perception analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE bandwidth adaptation; feature extraction; LOD; perceptual
   evaluation,,scale-space analysis; texture reduction
AB Efficient online visualization of three-dimensional (3-D) mesh, mapped with photo realistic texture, is essential for a variety of applications such as museum exhibits and medical images. In these applications synthetic texture or color per vertex loses authenticity and resolution. An image-based view dependent approach requires too much overhead to generate a 360 degrees display for online applications.' We propose using a mesh simplification algorithm based on scale-space analysis of the feature point distribution, combined with an associated visual perception analysis of the surface texture, to address the needs of adaptive online transmission of high quality 3-D objects. The premise of the proposed textured mesh (TexMesh) simplification, taking the human visual system into consideration, is the following: given limited bandwidth, texture quality in low feature density surfaces can be reduced, without significantly affecting human perception. The advantage of allocating higher bandwidth, and thus higher quality, to dense feature density surfaces, is to improve the overall visual fidelity. Statistics on feature point distribution and their associated texture fragments are gathered during preprocessing. Online transmission is based on these statistics, which can be retrieved in constant time. Using an initial estimated bandwidth, a scaled mesh is first transmitted. Starting from a default texture quality, we apply an efficient Harmonic Time Compensation Algorithm based on the current bandwidth and a time limit, to adaptively adjust the texture quality of the next fragment to be transmitted. Properties of the algorithm are proved. Experimental results show the usefulness of our approach.
C1 Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2E8, Canada.
C3 University of Alberta
RP Cheng, I (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2E8, Canada.
EM lin@cs.ualberta.ca; pierreb@cs.ualberta.ca
RI Boulanger, Pierre/AFV-2750-2022
CR BALMELLI L, 2002, EUR 02 C P, P411
   BAUER D, 2002, EUROGRAPHICS IEEE TC, P233
   BOULANGER P, 2002, P 15 INT C VIS INT C
   Brodsky D, 2000, PROC GRAPH INTERF, P221
   Cheng I, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P141, DOI 10.1109/ICME.2004.1394145
   Cheng I, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P109
   CHENG I, 2001, P IEEE EUROCON
   Cohen J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P119, DOI 10.1145/237170.237220
   COHEN J, 1998, SIGGRAPH 98, P115
   COHENOR D, 1999, SIGGRAPH        0808, P260
   Cornsweet T.N., 1970, Visual Perception
   Ferwerda J. A., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P143, DOI 10.1145/258734.258818
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Garland M, 1998, VISUALIZATION '98, PROCEEDINGS, P263, DOI 10.1109/VISUAL.1998.745312
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   KUIJPER A, 2001, LOGICAL FILTERING SC
   LINDEBERG T, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P134, DOI 10.1109/ICCV.1995.466795
   Lindstrom P, 2000, ACM T GRAPHIC, V19, P204, DOI 10.1145/353981.353995
   LINDSTROM P, TR9506 GRAPH GEORGIA
   LUEBKE D, 2001, 12 EUR WORKSH REND, P223
   Luebke DP, 2001, IEEE COMPUT GRAPH, V21, P24, DOI 10.1109/38.920624
   NAGATA S, 1984, P SID, V25, P239
   Okuda M, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P632, DOI 10.1109/ICIP.2000.899533
   Pan YX, 2005, IEEE T MULTIMEDIA, V7, P269, DOI 10.1109/TMM.2005.843364
   RUSHMEIER H, 2000, P SOC PHOTO-OPT INS, V3935, P372
   Sander PV, 2001, COMP GRAPH, P409, DOI 10.1145/383259.383307
   Shaffer E, 2001, IEEE VISUAL, P127, DOI 10.1109/VISUAL.2001.964503
   Soucy M, 1996, VISUAL COMPUT, V12, P503, DOI 10.1007/s003710050082
   TURK G, 1991, COMP GRAPH, V25, P289, DOI 10.1145/127719.122749
   TURK G, 1992, COMP GRAPH, V26, P55, DOI 10.1145/142920.134008
   Witkin A.P., 1983, P INT JOINT C ART IN, P1019, DOI DOI 10.1007/978-3-8348-9190-729
   Xia JC, 1997, IEEE T VIS COMPUT GR, V3, P171, DOI 10.1109/2945.597799
   YU Y, SIGGRAPH 2000 SKETCH
   Yu YZ, 2003, IEEE T MULTIMEDIA, V5, P466, DOI 10.1109/TMM.2003.814725
NR 35
TC 4
Z9 5
U1 1
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2006
VL 8
IS 3
BP 550
EP 563
DI 10.1109/TMM.2006.870722
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046OX
UT WOS:000237822000012
DA 2024-07-18
ER

PT J
AU Burnett, IS
   Davis, SJ
   Drury, GM
AF Burnett, IS
   Davis, SJ
   Drury, GM
TI MPEG-21 Digital Item Declaration and Identification - Principles and
   compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE MPEG; multimedia communication; multimedia computing; multimedia
   databases; multimedia systems
AB At the core of the MPEG-21 Multimedia Framework is the concept of the Digital Item, a virtual container for a hierarchical structure of metadata and resources. This paper considers the Digital Item Declaration Language (DIDL), gives examples of its usage, and discusses how it is used to integrate other parts of MPEG-21. The paper then discusses how Digital Item Identification integrates with the DIDL to allow MPEG-21 to utilize standard identifiers from many application spaces. Finally, an alternative, compressed form of the XML Digital Item Declaration is described. This uses schema-based compression
C1 Univ Wollongong, Telecommun & Informat Technol Res Inst, Wollongong, NSW 2522, Australia.
   Univ Wollongong, Sch Elect Comp & Telecommun Engn, Wollongong, NSW 2522, Australia.
C3 University of Wollongong; University of Wollongong
RP Univ Wollongong, Telecommun & Informat Technol Res Inst, Wollongong, NSW 2522, Australia.
EM i.burnett@elec.uow.edu.au; sjd11@uow.edu.au; gerrard@titr.uow.edu.au
OI Burnett, Ian/0000-0003-3795-7722
CR Chiariglione L, 1998, P IEEE, V86, P1222, DOI 10.1109/5.687836
   MALY K, 1999, DLIB MAG, V5
   MANJUNATH BS, 2002, INTRO MPEG7
   PEREIRA F, 2002, MPEG4 BOOK
   Scheffler IE, 2001, MITOCHONDRION, V1, P3, DOI 10.1016/S1567-7249(00)00002-7
   Verbert K, 2004, ED-MEDIA 2004: World Conference on Educational Multimedia, Hypermedia & Telecommunications, Vols. 1-7, P202
   Vetro A, 2005, IEEE T MULTIMEDIA, V7, P418, DOI 10.1109/TMM.2005.846795
   Wang X, 2005, IEEE T MULTIMEDIA, V7, P408, DOI 10.1109/TMM.2005.846788
NR 8
TC 28
Z9 31
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2005
VL 7
IS 3
BP 400
EP 407
DI 10.1109/TMM.2005.846789
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 949OO
UT WOS:000230798200002
OA Green Published
DA 2024-07-18
ER

PT J
AU Kettebekov, S
   Yeasin, M
   Sharma, R
AF Kettebekov, S
   Yeasin, M
   Sharma, R
TI Prosody based audiovisual coanalysis for coverbal gesture recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE gesture recognition; multimodal fusion; prosody; human-computer
   interaction
ID MODEL
AB Despite recent advances in vision-based gesture recognition, its applications remain largely limited to artificially defined and well-articulated gesture signs used for human-computer interaction. A key reason for this is the low recognition rates for "natural" gesticulation. Previous attempts of using speech cues to reduce error-proneness of visual classification have been mostly limited to keyword-gesture coanalysis. Such scheme inherits complexity and delays associated with natural language processing. This paper offers a novel "signal-level" perspective, where prosodic manifestations in speech and hand kinematics are considered as a basis for coanalyzing loosely coupled modalities. We present a computational framework for improving continuous gesture recognition based on two phenomena that capture voluntary (coarticulation) and involuntary (physiological) contributions of prosodic synchronization. Physiological constraints, manifested as signal interruptions during multimodal production, are exploited in an audiovisual feature integration framework using hidden Markov models. Coarticulation is analyzed using a Bayesian network of naive classifiers to explore alignment of intonationally prominent speech segments and hand kinematics. The efficacy of the proposed approach was demonstrated on a multimodal corpus created from the Weather Channel broadcast. Both schemas were found to contribute uniquely by reducing different error types, which subsequently improves the performance of continuous gesture recognition.
C1 Penn State Univ, Dept Comp Sci & Engn, University Pk, PA 16802 USA.
C3 Pennsylvania Commonwealth System of Higher Education (PCSHE);
   Pennsylvania State University; Pennsylvania State University -
   University Park
RP MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM kettebek@csail.mit.edu; myeasin@memphis.edu;
   rsharma@advancedinterfaces.com
CR [Anonymous], 1997, PROSODY SPEECH UNDER
   [Anonymous], 1992, HAND MIND WHAT HANDS
   Assan M., 1997, PROC GESTURE WORKSHO, P97
   AZOZ Y, 1998, IEEE C COMP VIS PATT
   Beckman ME, 1996, LANG COGNITIVE PROC, V11, P17, DOI 10.1080/016909696387213
   BECKMAN ME, 1992, LANG SPEECH, V35, P45, DOI 10.1177/002383099203500205
   Bobick AF, 1997, IEEE T PATTERN ANAL, V19, P1325, DOI 10.1109/34.643892
   Boersma P., 2002, PRAAT
   BOLT RA, 1980, SIGGRAPH COMPUT GRAP
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   CHICKERING DM, 1997, 13 C UNC ART INT PRO
   CHICKERNG DM, 2002, MSRTR2002103
   Frohlich M., 1998, Gesture and Sign Language in Human-Computer Interaction. International Gesture Workshop Proceedings, P173
   HIRSHBERG J, 1998, ICPL 98 SYDN AUSTR
   HOFMANN FG, 1997, LECT NOTES ARTIF INT, V1371, P81
   Kendon A., 1980, The relationship of verbal and nonverbal communication, P207, DOI [10.1515/9783110813098.207, DOI 10.1515/9783110813098.207]
   KENDON A, 1990, CONDUCTNG INTERACTIO
   Kettebekov S., 2000, International Journal on Artificial Intelligence Tools (Architectures, Languages, Algorithms), V9, P205, DOI 10.1142/S021821300000015X
   KETTEBEKOV S, 2002, WORKSH MULT RES MULT
   KETTEBEKOV S, 2001, LECT NOTES COMPUTER, P133
   KITA S, 1997, INT GEST WORKSH
   KRAHNSTOEVER N, 2002, INT C MULT INT PITTS
   KRUM M, 2002, IEEE VIRT REAL 2002
   Lee HK, 1999, IEEE T PATTERN ANAL, V21, P961, DOI 10.1109/34.799904
   Pavlovic VI, 1997, IEEE T PATTERN ANAL, V19, P677, DOI 10.1109/34.598226
   Pearl J., 1988, PROBABILISTIC REASON
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   RAUSCHERT I, 2002, 10 ACM INT S ADV GEO
   RIGOLL G, 1997, LECT NOTES ARTIF INT, V1371, P69
   SHARMA R, 2000, INT C FAC GEST REC F
   Sowa T, 1999, LECT NOTES ARTIF INT, V1739, P291
   SOWA T, 1999, C GEST MEAN US PORT
   TAYLOR P, 1994, SPEECH COMMUN, V15, P169, DOI 10.1016/0167-6393(94)90050-7
   Wexelblat A., 1997, International Gesture Workshop, P1
   WILSON AD, 2001, HIDDEN MARKOV MODELS, P123
   Yeo IK, 2000, BIOMETRIKA, V87, P954, DOI 10.1093/biomet/87.4.954
NR 36
TC 11
Z9 13
U1 1
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2005
VL 7
IS 2
BP 234
EP 242
DI 10.1109/TMM.2004.840590
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 909OO
UT WOS:000227869400005
DA 2024-07-18
ER

PT J
AU Nascimento, JC
   Marques, JS
AF Nascimento, JC
   Marques, JS
TI Robust shape tracking in the presence of cluttered background
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE data association; deformable contours; object tracking; robust
   filtering; shape analysis
ID MODELS
AB Many object-tracking algorithms are based on low-level features detected in the image. Typically, the object shape and position are estimated to tit the observed features. Unfortunately, image analysis methods often produce invalid features (outliers) which do not belong to the object boundary. These features have a strong influence on the shape estimates, leading to meaningless tracking results. This paper proposes a robust tracking algorithm which is able to deal with outliers, inspired in the probabilistic data association filter proposed in the context of point tracking. The algorithm is based on two key concepts. First, middle level features (strokes) are used instead of low-level ones (edge points). Second, two labels (valid/invalid) are considered for each stroke. Since the stroke labels are unknown all labeling sequences are considered and a probability (confidence degree) is assigned to each of them. In this way, all the strokes contribute to track the moving object but with different weights. This allows a robust performance of the tracker in the presence of outliers. Experimental tests are provided to assess the performance of the proposed algorithm in lip and gesture tracking and surveillance applications.
C1 Univ Tecn Lisboa, Inst Super Tecn, P-1049001 Lisbon, Portugal.
C3 Universidade de Lisboa
RP Univ Tecn Lisboa, Inst Super Tecn, P-1049001 Lisbon, Portugal.
RI Marques, Jorge/C-1427-2010; Nascimento, Jacinto/B-6128-2009
OI Marques, Jorge/0000-0002-3800-7756; Nascimento,
   Jacinto/0000-0001-7468-5127
CR [Anonymous], AUTOM SPEECH RECOGNI
   Bar-Shalom Y., 1988, TRACKING DATA ASS
   BASU S, 1999, P IEEE 3 WORKSH MULT, P475
   BAUMBERG A, 1997, MOTION BASED RECOGNI, P39
   Blake A., 1998, ACTIVE CONTOURS
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Duda R. O., 1973, PATTERN CLASSIFICATI, V3
   Evans JS, 1999, AUTOMATICA, V35, P1769, DOI 10.1016/S0005-1098(99)00086-2
   FARUQUIE T, 2000, P IEEE INT C PATT RE, V3, P110
   HUTTENLOCHER DP, 1990, INT J COMPUT VISION, V5, P195, DOI 10.1007/BF00054921
   Isard M, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P107, DOI 10.1109/ICCV.1998.710707
   ISARD M, 1996, P EUR C COMP VIS CAM, V1, P343
   Jain A. K., 1989, Fundamentals of Digital Image Processing
   LATORRE FD, 2000, P IEEE INT C PATT RE, V3, P1118
   LUCEY S, 2000, P IEEE INT C PATT RE, V3, P182
   Maciel J, 2003, IEEE T PATTERN ANAL, V25, P187, DOI 10.1109/TPAMI.2003.1177151
   MACIEL J, 1999, P IEEE INT C AC SPEE, V6, P3545
   Mori G, 2002, LECT NOTES COMPUT SC, V2352, P666
   Nascimento J, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P82, DOI 10.1109/ICIP.2000.899300
   Nascimento JC, 2002, PATTERN RECOGN, V35, P2711, DOI 10.1016/S0031-3203(01)00244-8
   NEY H, 2000, P IEEE INT C PATT RE, V3, P25
   North B, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P384, DOI 10.1109/ICCV.1998.710747
   Potamianos G, 1998, INT CONF ACOUST SPEE, P3733, DOI 10.1109/ICASSP.1998.679695
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Rajagopalan AN, 2000, IEEE IMAGE PROC, P351, DOI 10.1109/ICIP.2000.900967
   Terzopoulos D., 1992, Active Vision, P3
   TUGNAIT JK, 1982, AUTOMATICA, V18, P607, DOI 10.1016/0005-1098(82)90012-7
NR 27
TC 24
Z9 26
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2004
VL 6
IS 6
BP 852
EP 861
DI 10.1109/TMM.2004.837253
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 872QU
UT WOS:000225224200008
DA 2024-07-18
ER

PT J
AU Tryfonas, C
   Varma, A
AF Tryfonas, C
   Varma, A
TI Efficient algorithms for computation of the burstiness curve of video
   sources
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
ID NETWORKS; DELAY
AB The burstiness of a video source can be characterized by its burstiness curve. The burstiness curve is useful in the optimal allocation of resources to satisfy a desired quality of service for the video stream in a packet network. In this paper, we present deterministic algorithms for exact computation of the burstiness curve of a video source, for both elementary video streams and MPEG-2 Transport Streams. The algorithms exploit the piecewise linearity of the burstiness curve and compute only the points at which the slope of the burstiness curve changes. We also present approximate versions of these algorithms, which save computational effort by considering only a small number of candidate points at which the slope of the burstiness curve may change. The approximate algorithm was able to compute the burstiness curve of a 2-h long elementary video stream in approximately 10 s, as compared to over 6 h for the exact algorithm, with virtually no loss of accuracy in the computation. The efficiency of the proposed algorithms makes them suitable for quality-of-service (QoS) provisioning not only in off-line environments such as in video-on-demand (VoD) servers, but also in real-time applications such as in live TV distribution systems.
C1 Kazeon Syst Inc, Mountain View, CA 94043 USA.
   Univ Calif Santa Cruz, Dept Comp Engn, Santa Cruz, CA 95064 USA.
C3 University of California System; University of California Santa Cruz
RP Tryfonas, C (corresponding author), Kazeon Syst Inc, Mountain View, CA 94043 USA.
EM tryfonas@kazeon.com; varma@cse.ucsc.edu
CR *ATM FOR TECH COMM, 1996, TRAFF MAN SPEC VERS
   Casilari E., 1999, P INT TEL C ITC 16 9
   CHANG CS, 1994, IEEE T AUTOMAT CONTR, V39, P913, DOI 10.1109/9.284868
   CRUZ RL, 1991, IEEE T INFORM THEORY, V37, P114, DOI 10.1109/18.61109
   Elsayed K. M. F., 1995, PROC 6 IFIP DATA COM, P257
   ELWALID AI, 1993, P IEEE INF 93, V1, P256
   GAO J, 2001, MULTIFRACTAL MODELIN
   GARRETT M, 1993, THESIS COLUMBIA U NE
   GRAF M, 1994, P 4 OP WORKSH HIGH S
   GUERIN R, 1991, IEEE J SEL AREA COMM, V9, P968, DOI 10.1109/49.103545
   *ISO IEC, 1996, 138181 ISOIEC
   KESIDIS G, 1994, P 2 INT WORKSH MOD A, P318
   KNIGHTLY E, 1995, P ACM SIGMETRICS 95
   LI GL, 2000, SELF SIM TRAFF PERF
   LOW S, 1991, P GLOBECOM 91 DEC, V3, P1633
   STILIADIS D, 1995, UCSCCRL9538 DEP COMP
   STRALEY JC, 2001, P INT C ADV INFR EL
   TRYFONAS C, 1996, THESIS U CALIFORNIA
   TRYFONAS C, 1998, UCSCCRL9813 DEP COMP
   ZHANG H, 1995, P 5 INT WORKSH NETW
   [No title captured]
NR 21
TC 0
Z9 0
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2004
VL 6
IS 6
BP 862
EP 875
DI 10.1109/TMM.2004.835177
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 872QU
UT WOS:000225224200009
DA 2024-07-18
ER

PT J
AU Li, FWB
   Lau, RWH
   Ng, FFC
AF Li, FWB
   Lau, RWH
   Ng, FFC
TI VSculpt: A distributed virtual sculpting environment for collaborative
   design
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE collaborative environments; deformable object rendering; distributed
   collaboration; virtual sculpting
AB A collaborative virtual sculpting system supports a team of geographically separated designers/engineers connected by networks to participate in designing three-dimensional (3-D) virtual engineering tools or sculptures. It encourages international collaboration at a minimal cost. However, in order for the system to be useful, two factors need to be addressed: intuitiveness and real-time interaction. Although a lot of effort has been put into developing virtual sculpting environments, only limited work addresses collaborative virtual sculpting. This is because in order to support real-time collaborative virtual sculpting, many challenging issues need to be addressed. In this paper, we propose a collaborative virtual sculpting framework, called vSculpt. Through adapting some techniques we developed earlier and integrating them with some techniques developed here, the proposed framework provides a real-time intuitive environment for collaborative design. In particular, it addresses issues on efficient rendering and transmission of deformable objects, intuitive object deformation using the CyberGlove and concurrent object deformation by multiple clients. We demonstrate and evaluate the performance of the proposed framework through a number of experiments.
C1 City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM rynson@cs.cityu.edu.hk
RI Li, Frederick W. B./AAM-6662-2021
OI Li, Frederick W. B./0000-0002-4283-4228; LAU, Rynson W
   H/0000-0002-8957-8129
CR Alliez P, 2001, COMP GRAPH, P195, DOI 10.1145/383259.383281
   [Anonymous], P C VIRT REAL SOFTW
   Anupam V., 1994, IEEE Multimedia, V1, P39, DOI 10.1109/93.311655
   BABSKI C, 1999, PRESENCE-TELEOP VIRT, V8, P218
   Balmelli L., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P487, DOI 10.1109/ICIP.1999.822944
   CALVIN J, 1993, IEEE VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM, P450, DOI 10.1109/VRAIS.1993.380745
   CARLSSON C, 1993, IEEE VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM, P394, DOI 10.1109/VRAIS.1993.380753
   Chim J, 2003, IEEE T MULTIMEDIA, V5, P503, DOI 10.1109/TMM.2003.819094
   Chim J. H. P., 1998, Proceedings ACM Multimedia 98, P171, DOI 10.1145/290747.290769
   COHEN E, 1980, COMPUT GRAPH IMAGE P, V14
   Cutler L. D., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P107, DOI 10.1145/253284.253315
   DECASTELJAU P, 1959, S A ANDRE CITROEN
   GALYEAN TA, 1991, COMP GRAPH, V25, P267, DOI 10.1145/127719.122747
   GARGANTINI I, 1982, COMMUN ACM, V25, P905, DOI 10.1145/358728.358741
   Greenhalgh C, 1997, LECT NOTES COMPUT SC, V1242, P113, DOI 10.1007/BFb0037348
   KAMEYAMA K, 1997, P ACM S VIRT REAL SO, P197
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   KUESTER F, 1999, P WORKSH NEW PAR INF, P92
   Lau RWH, 1997, P IEEE VIRT REAL ANN, P20, DOI 10.1109/VRAIS.1997.583040
   LI F, 1997, P EUR 97 SEPT, V16, P47
   Li F. W. B., 1999, Journal of Graphics Tools, V4, P37, DOI 10.1080/10867651.1999.10487514
   Macedonia M.R., 1994, PRESENCE, V3, P265, DOI 10.1162/pres.1994.3.4.265
   NISHINO H, 1999, P ACM VRST 99, P116
   PANDZIC I, 1997, P EUR 97 SEPT, V16, P177
   Piegl L, 1995, NURBS BOOK
   Roberts DJ, 1997, SIXTH IEEE WORKSHOPS ON ENABLING TECHNOLOGIES: INFRASTRUCTURE FOR COLLABORATIVE ENTERPRISES, PROCEEDINGS, P161, DOI 10.1109/ENABL.1997.630808
   Shaw CD, 1997, MULTIMEDIA SYST, V5, P126, DOI 10.1007/s005300050048
   Wong JPY, 2000, J VISUAL COMP ANIMAT, V11, P155, DOI 10.1002/1099-1778(200007)11:3<155::AID-VIS225>3.0.CO;2-7
NR 28
TC 16
Z9 19
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2003
VL 5
IS 4
BP 570
EP 580
DI 10.1109/TTM.2003.814795
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 742VA
UT WOS:000186537700007
DA 2024-07-18
ER

PT J
AU Cheng, H
   Zhou, JT
   Tay, WP
   Wen, BH
AF Cheng, Hao
   Zhou, Joey Tianyi
   Tay, Wee Peng
   Wen, Bihan
TI Graph Neural Networks With Triple Attention for Few-Shot Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Training; Feature extraction; Graph neural networks;
   Benchmark testing; Standards; Scalability; Graph neural network;
   self-attention mechanism; few-shot classification; meta learning
AB Recent advances in Graph Neural Networks (GNNs) have achieved superior results in many challenging tasks, such as few-shot learning. Despite its capacity to learn and generalize a model from only a few annotated samples, GNN is limited in scalability, as deep GNN models usually suffer from severe over-fitting and over-smoothing. In this work, we propose a novel GNN framework with a triple-attention mechanism, i.e., node self-attention, neighbor attention, and layer memory attention, to tackle these challenges. We provide both theoretical analysis and illustrations to explain why the proposed attentive modules can improve GNN scalability for few-shot learning tasks. Our experiments show that the proposed Attentive GNN model outperforms the state-of-the-art few-shot learning methods using both GNN and non-GNN approaches. The improvement is consistent over the mini-ImageNet, tiered-ImageNet, CUB-200-2011, and Flowers-102 benchmarks, using both ConvNet-4 and ResNet-12 backbones, and under both the inductive and transductive settings. Furthermore, we demonstrate the superiority of our method for few-shot fine-grained and semi-supervised classification tasks with extensive experiments.
C1 [Cheng, Hao; Tay, Wee Peng; Wen, Bihan] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Zhou, Joey Tianyi] ASTAR Ctr Frontier AI Res, Singapore 138632, Singapore.
C3 Nanyang Technological University
RP Wen, BH (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM hao006@e.ntu.edu.sg; zhouty@cfar.a-star.edu.sg; wptay@ntu.edu.sg;
   bihan.wen@ntu.edu.sg
RI Zhou, Joey Tianyi/AAC-5115-2019; Wen, Bihan/B-3123-2017; Tay, Wee
   Peng/A-5110-2011
OI Zhou, Joey Tianyi/0000-0002-4675-7055; Wen, Bihan/0000-0002-6874-6453;
   Tay, Wee Peng/0000-0002-1543-195X; hao, cheng/0000-0003-4823-0908
FU Joey Tianyi Zhou#x0027;s A*STAR SERC Central Research Fund
FX No Statement Available
CR Afrasiyabi A, 2022, PROC CVPR IEEE, P9004, DOI 10.1109/CVPR52688.2022.00881
   Bruna J., 2014, ABS13126203 CORR, P1, DOI [10.48550/arXiv.1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Cao Kaidi, 2021, P INT C LEARN REPR
   Chamberlain B., 2021, INT C MACHINE LEARNI, P1407
   Chamberlain B., 2021, Adv. Neural Inform. Process. Syst., V34, P1594
   Chen CF, 2021, PROC CVPR IEEE, P6592, DOI 10.1109/CVPR46437.2021.00653
   Chen W.Y., 2019, ICLR, DOI DOI 10.1109/MSR.2015.54
   Chen YB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9042, DOI 10.1109/ICCV48922.2021.00893
   Cheng Hao, 2022, 2022 IEEE 5th International Conference on Multimedia Information Processing and Retrieval (MIPR), P152, DOI 10.1109/MIPR54900.2022.00033
   Cheng J., 2016, LONG SHORT TERM MEMO, DOI DOI 10.18653/V1/D16-1053
   Chi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12200, DOI 10.1109/CVPR42600.2020.01222
   Defferrard M, 2016, ADV NEUR IN, V29
   Feng YB, 2023, IEEE T MULTIMEDIA, V25, P3204, DOI [10.1109/TMM.2022.3156938, 10.1109/IECON49645.2022.9969044]
   Finn C, 2017, PR MACH LEARN RES, V70
   Guo LQ, 2022, IEEE T IMAGE PROCESS, V31, P1311, DOI 10.1109/TIP.2022.3140918
   Guo Y., 2021, PROC IEEE INT C VIS, P1
   Hamilton WL, 2017, ADV NEUR IN, V30
   Han-Jia Ye, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8805, DOI 10.1109/CVPR42600.2020.00883
   Hariharan B, 2017, IEEE I CONF COMP VIS, P3037, DOI 10.1109/ICCV.2017.328
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G., 2015, COMPUT SCI, V2
   Hou RB, 2019, ADV NEUR IN, V32
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang HX, 2021, IEEE T MULTIMEDIA, V23, P1666, DOI 10.1109/TMM.2020.3001510
   Ji F, 2020, INT CONF ACOUST SPEE, P3332, DOI [10.1109/icassp40776.2020.9054104, 10.1109/ICASSP40776.2020.9054104]
   Kang D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8802, DOI 10.1109/ICCV48922.2021.00870
   Ke L, 2020, INT CONF ACOUST SPEE, P2233, DOI [10.1109/ICASSP40776.2020.9053509, 10.1109/icassp40776.2020.9053509]
   Kim J, 2019, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2019.00010
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee J, 2019, PR MACH LEARN RES, V97
   Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li GH, 2023, IEEE T PATTERN ANAL, V45, P6923, DOI 10.1109/TPAMI.2021.3074057
   Li HY, 2019, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2019.00009
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Li QM, 2018, AAAI CONF ARTIF INTE, P3538
   Ling J, 2022, PROC CVPR IEEE, P14544, DOI 10.1109/CVPR52688.2022.01416
   Liu Y, 2022, PROC CVPR IEEE, P14391, DOI 10.1109/CVPR52688.2022.01401
   Liu YH, 2019, INT J PSYCHIAT CLIN, V23, P164, DOI 10.1080/13651501.2019.1569238
   Luo YW, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107451
   Ma YQ, 2022, IEEE T NEUR NET LEAR, V33, P6652, DOI 10.1109/TNNLS.2021.3082928
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Oono K., 2020, INT C LEARNING REPRE
   Oreshkin BN, 2018, ADV NEUR IN, V31
   Parikh AP., 2016, EMNLP
   Phaphuangwittayakul A, 2022, IEEE T MULTIMEDIA, V24, P2205, DOI 10.1109/TMM.2021.3077729
   Ravi S., 2016, INT C LEARNING REPRE
   Ren M., 2018, INT C LEARNING REPRE, DOI DOI 10.1109/IPFA.2018.8452547
   Rong Y., 2019, ARXIV, DOI [10.48550/arXiv.1907.10903, DOI 10.48550/ARXIV.1907.10903]
   Satorras V.G., 2018, ICLR
   Snell J, 2017, ADV NEUR IN, V30
   Sperduti A, 1997, IEEE T NEURAL NETWOR, V8, P714, DOI 10.1109/72.572108
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tang SX, 2021, PROC CVPR IEEE, P2329, DOI 10.1109/CVPR46437.2021.00236
   Vaswani A, 2017, ADV NEUR IN, V30
   Velickovic Petar, 2018, INT C LEARN REPR
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P56, DOI 10.1007/978-3-030-01225-0_4
   Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760
   Wen BH, 2015, INT J COMPUT VISION, V114, P137, DOI 10.1007/s11263-014-0761-1
   Xie JT, 2022, PROC CVPR IEEE, P7962, DOI 10.1109/CVPR52688.2022.00781
   Xu H., 2021, P IEEECVF INT C COMP, P8812
   Xu K., 2019, PROC ICLR
   Xu KYL, 2018, PR MACH LEARN RES, V80
   Xu SL, 2022, AAAI CONF ARTIF INTE, P2911
   Yang S., 2021, P INT C LEARN REPR, P1
   Zhang HG, 2023, IEEE T MULTIMEDIA, V25, P2111, DOI 10.1109/TMM.2022.3142955
   Zhang Ji, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P2586, DOI 10.1145/3503161.3547835
   Zhang J, 2022, IEEE T CIRC SYST VID, V32, P5916, DOI 10.1109/TCSVT.2022.3164190
   Zhang JH, 2021, IEEE WINT CONF APPL, P3481, DOI 10.1109/WACV48630.2021.00352
   Zhang M., 2021, PROC INT C LEARN REP, P1
   Zhong X, 2023, IEEE T MULTIMEDIA, V25, P1979, DOI 10.1109/TMM.2022.3141886
   Zhu YH, 2021, IEEE T MULTIMEDIA, V23, P1200, DOI 10.1109/TMM.2020.2993952
NR 76
TC 3
Z9 3
U1 23
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8225
EP 8239
DI 10.1109/TMM.2022.3233442
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000013
OA hybrid
DA 2024-07-18
ER

PT J
AU Guo, XB
   Kot, A
   Kong, AWK
AF Guo, Xiaobao
   Kot, Alex
   Kong, Adams Wai-Kin
TI Pace-Adaptive and Noise-Resistant Contrastive Learning for Multimodal
   Feature Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimodal fusion; multimodal contrastive learning; modality invariance
AB Multimodal feature fusion aims to draw complementary information from different modalities to achieve better performance. Contrastive learning is effective at discriminating coexisting semantic features (positive) from irrelative ones (negative) in multimodal signals. However, positive and negative pairs learn at separate rates, which undermines the overall performance of multimodal contrastive learning (MCL). Moreover, the learned representation model is not robust, as MCL utilizes supervision signals from potentially noisy modalities. To address these issues, a novel multimodal contrastive learning objective, Pace-adaptive and Noise-resistant Noise-Contrastive Estimation (PN-NCE), is proposed for multimodal fusion by directly using unimodal features. PN-NCE encourages the positive and negative pairs reaching to their optimal similarity scores adaptively and shows less susceptibility to noisy inputs during training. A theoretical analysis is performed on its robustness. Maximizing modality invariance information in the fused representation is expected to benefit the overall performance and therefore, an estimator that measures the difference between the fused representation and its unimodal representations is integrated into MCL to obtain a more modality-invariant fusion output. The proposed method is model-agnostic and can be adapted to various multimodal tasks. It also bears less performance degradation when reducing the number of training samples at the linear probing stage. With different networks and modality inputs from three multi-modal datasets, experimental results show that PN-NCE achieves consistent enhancements compared with previous state-of-the-art approaches.
C1 [Guo, Xiaobao; Kong, Adams Wai-Kin] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Guo, Xiaobao] Nanyang Technol Univ, Interdisciplinary Grad Programme, Rapid Rich Object Search ROSE Lab, Singapore 639798, Singapore.
   [Kot, Alex] Nanyang Technol Univ, Sch Elect & Elect Engn, Nanyang, Singapore.
C3 Nanyang Technological University; Nanyang Technological University;
   Nanyang Technological University
RP Kong, AWK (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
EM XIAOBAO001@e.ntu.edu.sg; eackot@ntu.edu.sg; AdamsKong@ntu.edu.sg
OI Kot, Alex/0000-0001-6262-8125
FU Ministry of Education - Singapore
FX No Statement Available
CR Alayrac J.-B., 2020, NeurIPS, V33, P25
   Arora S, 2019, PR MACH LEARN RES, V97
   Baltrusaitis T, 2016, IEEE WINT CONF APPL
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Bao HB, 2020, PR MACH LEARN RES, V119
   Cai YT, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2506
   CHEN T, 2020, INT C MACH LEARN, P1597, DOI DOI 10.48550/ARXIV.2002.05709
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Chuang CY, 2022, PROC CVPR IEEE, P16649, DOI 10.1109/CVPR52688.2022.01617
   Chuang Ching-Yao, 2020, ADV NEURAL INFORM PR, V33, P8765, DOI DOI 10.48550/ARXIV.2007.00224
   Degottex G, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6853739
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Gadzicki K, 2020, PROCEEDINGS OF 2020 23RD INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION 2020), P292
   Gao TY, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P6894
   Ghosh A, 2017, AAAI CONF ARTIF INTE, P1919
   Giorgi J, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1 (ACL-IJCNLP 2021), P879
   Gutmann Michael, 2010, P MACHINE LEARNING R, P297, DOI DOI 10.1145/3292500.3330651
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   Han TD, 2019, IEEE INT CONF COMP V, P1483, DOI 10.1109/ICCVW.2019.00186
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Henaff OJ, 2020, PR MACH LEARN RES, V119
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Jing L., 2022, P INT C LEARN REPR, P1
   Khosla P., 2020, C NEUR INF PROC SYST
   Le-Khac PH, 2020, IEEE ACCESS, V8, P193907, DOI 10.1109/ACCESS.2020.3031549
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Liu Yunze, 2021, P IEEECVF INT C COMP, P754
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Nagrani A, 2021, ADV NEUR IN, V34
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Ramachandram D, 2017, IEEE SIGNAL PROC MAG, V34, P96, DOI 10.1109/MSP.2017.2738401
   Saeed A, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P3875, DOI 10.1109/ICASSP39728.2021.9413528
   Schneider S, 2019, INTERSPEECH, P3465, DOI 10.21437/Interspeech.2019-1873
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang F, 2018, IEEE SIGNAL PROC LET, V25, P926, DOI 10.1109/LSP.2018.2822810
   Wang T., 2020, INT C MACHINE LEARNI, P9929
   Wang Y., 2020, P 34 ADV NEUR INF PR, P4835
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
   Zadeh A., 2017, P 2017 C EMP METH NA, P1103, DOI 10.18653/v1/D17-1115
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zhang C, 2020, IEEE J-STSP, V14, P478, DOI 10.1109/JSTSP.2020.2987728
   Zhang X, 2018, Arxiv, DOI arXiv:1809.04157
   Zhu R., 2021, PROC IEEECVF INT C C, P10306
NR 49
TC 1
Z9 1
U1 16
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9437
EP 9448
DI 10.1109/TMM.2023.3252270
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP5R1
UT WOS:001133278300006
DA 2024-07-18
ER

PT J
AU Hu, ZJ
   Liu, Y
   Chen, G
   Liu, YX
AF Hu, Zhejing
   Liu, Yan
   Chen, Gong
   Liu, Yongxu
TI Can Machines Generate Personalized Music? A Hybrid Favorite-Aware Method
   for User Preference Music Transfer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Artificial music intelligence; automatic music generation; music style
   transfer
AB User preference music transfer (UPMT) is a new problem in music style transfer that can be applied to many scenarios but remains understudied. Transferring an arbitrary song to fit a user's preferences increases musical diversity and improves user engagement, which can greatly benefit individuals' mental health. Most music style transfer approaches rely on data-driven methods. In general, however, constructing a large training dataset is challenging because users can rarely provide enough of their favorite songs. To address this problem, this paper proposes a novel hybrid method called User Preference Transformer (UP-Transformer) which uses prior knowledge of only one piece of a user's favorite music. Based on the distribution of music events in the provided music, we propose a new favorite-aware loss function to fine-tune the Transformer-based model. Two steps are proposed in the transfer phase to achieve UPMT based on the extracted music pattern in a user's favorite music. Additionally, to alleviate the problem of evaluating melodic similarity in music style transfer, we propose a new concept called pattern similarity (PS) to measure the similarity between two pieces of music. Statistical tests indicate that the results of PS are consistent with the similarity score in a qualitative experiment. Furthermore, experimental results on subjects show that the transferred music achieves better performance in musicality, similarity, and user preferences.
C1 [Hu, Zhejing; Liu, Yan; Chen, Gong; Liu, Yongxu] Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China.
C3 Hong Kong Polytechnic University
RP Liu, Y (corresponding author), Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China.
EM zhejing.hu@connect.polyu.hk; csyliu@comp.polyu.edu.hk;
   gong-a.chen@polyu.edu.hk; yongxu.liu@connect.polyu.hk
RI liu, yan/HGV-1365-2022
OI Liu, Yongxu/0000-0002-7267-7972; hu, zhejing/0000-0002-9424-9071; Chen,
   Gong/0000-0003-2098-1291
FU DaSAIL-Music Generation via Machine Composition [P0030934]
FX This work was supported by DaSAIL-Music Generation via Machine
   Composition under Grant P0030934.
CR Gatys LA, 2015, Arxiv, DOI arXiv:1508.06576
   Huang CZA, 2018, Arxiv, DOI [arXiv:1809.04281, 10.48550/arXiv.1809.04281, DOI 10.48550/ARXIV.1809.04281]
   [Anonymous], 1996, Style and music: Theory, history, and ideology
   Bogdanov D., 2010, P ACM C REC SYST WOR, P33
   Bradt J, 2013, COCHRANE DB SYST REV, DOI 10.1002/14651858.CD006577.pub3
   Brunner G, 2018, PROC INT C TOOLS ART, P786, DOI 10.1109/ICTAI.2018.00123
   Brunner Gino, 2018, ARXIV180907600, P747
   Celma O, 2010, MUSIC RECOMMENDATION AND DISCOVERY, P43, DOI 10.1007/978-3-642-13287-2_3
   Choi K., 2020, International Conference on Machine Learning, P1899
   Cifka O., 2019, INT SOC MUS INF RETR, P588
   Cífka O, 2020, IEEE-ACM T AUDIO SPE, V28, P2638, DOI 10.1109/TASLP.2020.3019642
   Dai ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2978
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Donadello I., 2018, Ph.D. Thesis
   Donadello I, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1596
   Donahue C., 2019, P 20 INT SOC MUS INF, P685
   Dunn PG, 2012, PSYCHOL MUSIC, V40, P411, DOI 10.1177/0305735610388897
   Garcez AD, 2019, J APPL LOG-IFCOLOG, V6, P611
   Hsieh TI, 2019, ADV NEUR IN, V32
   Hu ZJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1189, DOI 10.1145/3394171.3414070
   Huang Sicong, 2019, 7 INT C LEARN REPR I
   Huang YSA, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1180, DOI 10.1145/3394171.3413671
   Hung YN, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4697
   Jiang JY, 2020, INT CONF ACOUST SPEE, P516, DOI [10.1109/ICASSP40776.2020.9054554, 10.1109/icassp40776.2020.9054554]
   Jing YC, 2020, IEEE T VIS COMPUT GR, V26, P3365, DOI 10.1109/TVCG.2019.2921336
   Kanehira Ren, 2018, 2018 14th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD), P1374, DOI 10.1109/FSKD.2018.8686951
   Liu A., 2020, PROC MACH LEARN MEDI
   Lu CY, 2019, AAAI CONF ARTIF INTE, P1061
   Lu Wei Tsung, 2018, P INT SOC MUS INF RE, P740
   Malik I, 2017, Arxiv, DOI arXiv:1708.03535
   Marino K, 2017, PROC CVPR IEEE, P20, DOI 10.1109/CVPR.2017.10
   Mornhinweg G C, 1992, J Holist Nurs, V10, P101, DOI 10.1177/089801019201000202
   Noam Mor A. P., 2019, P INT C LEARN REPR
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Park J., 2019, P 20 INT SOC MUS INF, P620
   Pasini M, 2019, Arxiv, DOI arXiv:1910.03713
   Rawlings D., 1997, Psychology of Music, V25, P120, DOI [DOI 10.1177/0305735697252003, 10.1177/0305735697252003]
   Ren Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1198, DOI 10.1145/3394171.3413721
   Vaswani A, 2017, ADV NEUR IN, V30
   Volk A, 2012, MUSIC SCI, V16, P317, DOI 10.1177/1029864912448329
   Xia G.S., 2018, P 2018 IEEE C COMP V, P1, DOI DOI 10.5281/ZENODO.4285180
   Yang R., 2019, 19 INT C NEW INT MUS, P307
   Yang Ruihan, 2019, P 20 INT SOC MUSIC I, P596
   Ye H., 2020, J PHYS C SER, V1631
NR 44
TC 2
Z9 2
U1 10
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2296
EP 2308
DI 10.1109/TMM.2022.3146002
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100054
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jin, J
   Zhang, XX
   Meng, LL
   Lin, WS
   Liang, J
   Zhang, HX
   Zhao, Y
AF Jin, Jian
   Zhang, Xingxing
   Meng, Lili
   Lin, Weisi
   Liang, Jie
   Zhang, Huaxiang
   Zhao, Yao
TI Auto-Weighted Layer Representation Based View Synthesis Distortion
   Estimation for 3-D Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D video; view synthesis distortion (VSD); depth coding;
   depth-image-based rendering (DIBR)
ID QUALITY ASSESSMENT; BIT ALLOCATION; DEPTH; PLUS; DIBR; MODEL
AB Recently, various view synthesis distortion estimation models have been studied to better serve 3-D video coding. However, they can hardly model the relationship quantitatively among different levels of depth changes, texture degeneration, and view synthesis distortion (VSD), which is crucial for rate-distortion optimization and rate allocation. In this paper, an auto-weighted layer representation based view synthesis distortion estimation model is developed. Firstly, sub-VSD (S-VSD) is defined according to the level of depth changes and their associated texture degeneration. After that, a set of theoretical derivations demonstrate that the VSD can be approximately decomposed into the S-VSDs multiplied by their associated weights. To obtain the S-VSDs efficiently, a layer-based representation method is developed, where all the pixels with the same level of depth changes are represented with a layer. It enables the S-VSD calculation at the layer level. Meanwhile, a nonlinear mapping function is learnt to accurately represent the relationship between the VSD and S-VSDs, automatically providing weights for the S-VSDs during VSD estimation. To learn such a function, a dataset of the VSD and its associated S-VSDs are built, termed as VSDSet. Experimental results show that the VSD can be accurately estimated with the weights learnt by the nonlinear mapping function once its associated S-VSDs are available. The proposed method outperforms the relevant state-of-the-art methods in both accuracy and efficiency. The VSDSet and source code of the proposed method will be available at https://github.com/jianjin008/.
C1 [Jin, Jian; Lin, Weisi] Nanyang Technol Univ, Sch Engn & Comp Sci, Singapore 639798, Singapore.
   [Jin, Jian; Lin, Weisi] Nanyang Technol Univ, Alibaba NTU Singapore Joint Res Inst, Singapore 639798, Singapore.
   [Zhang, Xingxing] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Meng, Lili; Zhang, Huaxiang] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250014, Peoples R China.
   [Meng, Lili] Simon Fraser Univ, Sch Engn Sci, Burnaby, BC, Canada.
   [Zhao, Yao] Beijing Jiao Tong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Zhao, Yao] Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
C3 Nanyang Technological University; Nanyang Technological University;
   Tsinghua University; Shandong Normal University; Simon Fraser
   University; Beijing Jiaotong University; Beijing Jiaotong University
RP Lin, WS (corresponding author), Nanyang Technol Univ, Sch Engn & Comp Sci, Singapore 639798, Singapore.
EM jianj008@gmail.com; xxzhang2020@mail.tsinghua.edu.cn;
   mengll_83@hotmail.com; wslin@ntu.edu.sg; jiel@sfu.ca;
   huaxzhang@hotmail.com; yzhao@bjtu.edu.cn
RI li, jixiang/JXN-7599-2024; Chen, Nuo/JZD-0344-2024; Lin,
   Wei/D-3353-2012; Wang, Jiachen/KFT-0161-2024; Zhang,
   Xingxing/HGE-4445-2022; Yu, Shicheng/KHU-3059-2024; Lin,
   Weisi/A-3696-2011
OI Zhang, Xingxing/0000-0002-9838-6962; Zhao, Yao/0000-0002-8581-9554; Jin,
   Jian/0000-0003-4250-1519; Lin, Weisi/0000-0001-9866-1947; zhang, hua
   xiang/0000-0001-6259-7533; Liang, Jie/0000-0003-3003-4343
FU Alibaba Group through Alibaba Innovative Research Program; Alibaba-NTU
   Singapore Joint Research Institute, Nanyang Technological University,
   Singapore; National Key R&D Program of China [2021ZD0112100]; National
   NSF of China [U1936212, 62120106009]; Natural Science Foundation of
   Shandong, China [ZR2020MF042]
FX This work was supported in part by Alibaba Group through Alibaba
   Innovative Research Program, in part by Alibaba-NTU Singapore Joint
   Research Institute, Nanyang Technological University, Singapore, in part
   by the National Key R&D Program of China under Grant 2021ZD0112100, in
   part by the National NSF of China underGrants U1936212 and 62120106009,
   and in part by the Natural Science Foundation of Shandong, China under
   Grant ZR2020MF042.
CR [Anonymous], 2008, Electron Telecommun. Res. Inst. Gwangju Inst. Sci. Technol.
   Chen TQ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2939672.2939785
   Cheung G, 2011, IEEE IMAGE PROC, P129, DOI 10.1109/ICIP.2011.6115673
   Daniele B., 2022, Electron. Imag., V34, P1
   de Oliveira AQ, 2021, IEEE T IMAGE PROCESS, V30, P6408, DOI 10.1109/TIP.2021.3092817
   Domanski M, 2009, ISO/IEC JTC1/SC29/WG11 MPEG/M17050
   Fang L, 2014, IEEE T IMAGE PROCESS, V23, P185, DOI 10.1109/TIP.2013.2287608
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Fraunhofer Heinrich Hertz Institute, 2013, 3DV Sequences of HHI
   Gao P, 2019, IEEE T IMAGE PROCESS, V28, P5266, DOI 10.1109/TIP.2019.2919198
   Gao P, 2014, IEEE T MULTIMEDIA, V16, P1797, DOI 10.1109/TMM.2014.2331013
   Gu K, 2020, IEEE T BROADCAST, V66, P127, DOI 10.1109/TBC.2019.2906768
   Jia BQ, 2021, MULTIMED TOOLS APPL, V80, P20619, DOI 10.1007/s11042-021-10732-3
   Jin J, 2020, IEEE T CIRC SYST VID, V30, P2229, DOI 10.1109/TCSVT.2019.2918248
   Jin J, 2019, IEEE T CIRC SYST VID, V29, P1754, DOI 10.1109/TCSVT.2018.2844743
   Jin J, 2016, IEEE T MULTIMEDIA, V18, P953, DOI 10.1109/TMM.2016.2539825
   Jin J, 2015, LECT NOTES COMPUT SC, V9315, P390, DOI 10.1007/978-3-319-24078-7_39
   Joint collaborative team for 3DV, 2013, 3D-HTM Software Platform
   Kim WS, 2009, IEEE IMAGE PROC, P721, DOI 10.1109/ICIP.2009.5414304
   Li LD, 2021, IEEE T MULTIMEDIA, V23, P320, DOI 10.1109/TMM.2020.2980185
   Li S, 2018, IEEE T MULTIMEDIA, V20, P1948, DOI 10.1109/TMM.2018.2791810
   Ling SY, 2021, IEEE T MULTIMEDIA, V23, P4245, DOI 10.1109/TMM.2020.3038305
   Merkle P, 2007, IEEE IMAGE PROC, P201
   Nagoya University, 2008, 3DVSequences of Nagoya University
   Rusanovskyy D., 2011, ISO/IEC JTC1/SC29/WG11, Doc. M20028
   Rusanovskyy D., 2013, P 3 M ITU T ISO IEC, P1
   Sadbhawna, 2022, IEEE T IMAGE PROCESS, V31, P2027, DOI 10.1109/TIP.2022.3147981
   Sadbhawna, 2022, IEEE T IMAGE PROCESS, V31, P1737, DOI 10.1109/TIP.2022.3145997
   Shao F, 2012, IEEE T MULTIMEDIA, V14, P157, DOI 10.1109/TMM.2011.2169045
   Tian SS, 2021, NEUROCOMPUTING, V423, P158, DOI 10.1016/j.neucom.2020.09.062
   Tian SS, 2019, IEEE T MULTIMEDIA, V21, P1235, DOI 10.1109/TMM.2018.2875307
   Tian SS, 2018, IEEE T IMAGE PROCESS, V27, P1652, DOI 10.1109/TIP.2017.2781420
   Wang GC, 2020, IEEE T IMAGE PROCESS, V29, P1802, DOI 10.1109/TIP.2019.2945675
   Yuan H, 2016, IEEE T BROADCAST, V62, P134, DOI 10.1109/TBC.2015.2492461
   Yuan H, 2011, IEEE T CIRC SYST VID, V21, P485, DOI 10.1109/TCSVT.2011.2125610
   Zhang D, 2015, IEEE T CIRC SYST VID, V25, P827, DOI 10.1109/TCSVT.2014.2363746
   Zhang H, 2022, IEEE T CIRC SYST VID, V32, P5080, DOI 10.1109/TCSVT.2022.3147788
   Zhang J., 2011, ISO/IEC JTC1/SC29/WG11, Doc. M20027
   Zhang Y, 2013, IEEE T IMAGE PROCESS, V22, P3497, DOI 10.1109/TIP.2013.2265883
   Zhao LJ, 2015, KSII T INTERNET INF, V9, P4108
   Zheng ZQ, 2018, IEEE SIGNAL PROC LET, V25, P417, DOI 10.1109/LSP.2017.2723910
   Zhou Y, 2011, IEEE T CIRC SYST VID, V21, P1679, DOI 10.1109/TCSVT.2011.2133390
NR 42
TC 0
Z9 0
U1 2
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5775
EP 5788
DI 10.1109/TMM.2022.3199102
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, Y
   Hao, Q
   Hu, JG
   Pan, XM
   Li, ZC
   Cui, Z
AF Li, Yong
   Hao, Qiang
   Hu, Jianguo
   Pan, Xinmiao
   Li, Zechao
   Cui, Zhen
TI 3D3M: 3D Modulated Morphable Model for Monocular Face Reconstruction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Faces; Three-dimensional displays; Shape; Image reconstruction; Face
   recognition; Solid modeling; Codes; 3D face reconstruction; dense shape
   correspondence; self-supervised learning
ID SINGLE IMAGE; SHAPE
AB 3D face reconstruction from a single image is a vital task in various multimedia applications. A key challenge for 3D face shape reconstruction is to build the correct dense face correspondence between the monocular input face and the deformable mesh. Most existing methods rely on shape labels fitted by traditional methods or strong priors such as multi-view geometry consistency. In contrast, we propose an innovative 3D Modulated Morphable Model (3D3M) to learn the dense shape correspondence from monocular images in a self-supervised manner. Specifically, given a batch of input faces, 3D3M encodes their 3DMM attributes (shape, texture, lighting, etc.) and then randomly shuffles the 3DMM attributes to generate the attribute-changed faces. The attribute-changed faces can be encoded and rendered back in a cycle-consistent manner, which enables us to utilize the self-supervised consistencies in dense mesh vertices and reconstructed pixels. The dense shape and pixel correspondence enable us to adopt a series of self-supervised constraints to fit the 3D face model accurately and learn the per-vertex correctives end-to-end. 3D3M builds excellent high-quality 3D face reconstruction results from monocular images. Both quantitative and qualitative experimental results have verified the superiority of 3D3M over prior arts on 3D face reconstruction and face alignment.
C1 [Li, Yong; Li, Zechao; Cui, Zhen] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, PCA Lab,Minist Educ, Key Lab Intelligent Percept & Syst High Dimens In, Nanjing 210094, Peoples R China.
   [Li, Yong; Li, Zechao; Cui, Zhen] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Socia, Nanjing 210094, Peoples R China.
   [Hao, Qiang; Hu, Jianguo; Pan, Xinmiao] MINIVISION Co Ltd, Nanjing 210000, Peoples R China.
C3 Nanjing University of Science & Technology; Nanjing University of
   Science & Technology
RP Cui, Z (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, PCA Lab,Minist Educ, Key Lab Intelligent Percept & Syst High Dimens In, Nanjing 210094, Peoples R China.; Cui, Z (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Socia, Nanjing 210094, Peoples R China.
EM yong.li@njust.edu.cn; haoqiang@minivision.cn; hujianguo@minivision.cn;
   panxinmiao@minivision.cn; zechao.li@njust.edu.cn; zhen.cui@njust.edu.cn
OI Li, Yong/0000-0002-6521-5921
FU National Natural Science Foundation of China [62102180]; Natural Science
   Foundation of Jiangsu Province [BK20210329]; Shuangchuang Program of
   Jiangsu Province [JSSCBS20210210]; Natural Science Foundation of
   Shandong Province [ZR2020LZH008]; State Key Laboratory of High-end
   Server and Storage Technology
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62102180, in part by the Natural Science
   Foundation of Jiangsu Province under Grant BK20210329, in part by
   Shuangchuang Program of Jiangsu Province under Grant JSSCBS20210210, in
   part by the Natural Science Foundation of Shandong Province under Grant
   ZR2020LZH008, and in part by State Key Laboratory of High-end Server and
   Storage Technology. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Dr. Juyong Zhang.
CR Amberg B, 2008, IEEE INT CONF AUTOMA, P667
   Asthana A, 2013, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2013.442
   Bagdanov Andrew D, 2011, P JOINT ACM WORKSH H
   Bhagavatula C, 2017, IEEE I CONF COMP VIS, P4000, DOI 10.1109/ICCV.2017.429
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Bulat A, 2017, IEEE I CONF COMP VIS, P3726, DOI 10.1109/ICCV.2017.400
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Zavan FHD, 2016, LECT NOTES COMPUT SC, V9914, P581, DOI 10.1007/978-3-319-48881-3_40
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Egger B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3395208
   Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33
   Gao D, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323042
   Gao ZP, 2020, IEEE COMPUT SOC CONF, P1426, DOI 10.1109/CVPRW50498.2020.00182
   Genova K, 2018, PROC CVPR IEEE, P8377, DOI 10.1109/CVPR.2018.00874
   Gou C, 2016, LECT NOTES COMPUT SC, V9914, P604, DOI 10.1007/978-3-319-48881-3_42
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Güler RA, 2017, PROC CVPR IEEE, P2614, DOI 10.1109/CVPR.2017.280
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jianzhu Guo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P152, DOI 10.1007/978-3-030-58529-7_10
   Jiaxiang Shang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P53, DOI 10.1007/978-3-030-58555-6_4
   Kim H, 2018, PROC CVPR IEEE, P4625, DOI 10.1109/CVPR.2018.00486
   Kim J, 2013, PROC CVPR IEEE, P2307, DOI 10.1109/CVPR.2013.299
   Kingma D. P, 2015, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1412.6980
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Lee GH, 2020, PROC CVPR IEEE, P6099, DOI 10.1109/CVPR42600.2020.00614
   Li C, 2015, PROC CVPR IEEE, P4621, DOI 10.1109/CVPR.2015.7299093
   Li Y, 2022, IEEE T PATTERN ANAL, V44, P302, DOI 10.1109/TPAMI.2020.3011063
   Lin JK, 2020, PROC CVPR IEEE, P5890, DOI 10.1109/CVPR42600.2020.00593
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3
   McDonagh J, 2016, LECT NOTES COMPUT SC, V9914, P569, DOI 10.1007/978-3-319-48881-3_39
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317
   Romdhani S, 2005, PROC CVPR IEEE, P986
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sánta Z, 2016, LECT NOTES COMPUT SC, V9914, P521, DOI 10.1007/978-3-319-48881-3_36
   Tewari A, 2018, PROC CVPR IEEE, P2549, DOI 10.1109/CVPR.2018.00270
   Tewari A, 2017, IEEE INT CONF COMP V, P1274, DOI 10.1109/ICCVW.2017.153
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Tran L, 2019, PROC CVPR IEEE, P1126, DOI 10.1109/CVPR.2019.00122
   Tu XG, 2021, IEEE T MULTIMEDIA, V23, P1160, DOI 10.1109/TMM.2020.2993962
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Yu R, 2017, IEEE I CONF COMP VIS, P4733, DOI 10.1109/ICCV.2017.506
   Zhu WB, 2020, PROC CVPR IEEE, P4957, DOI 10.1109/CVPR42600.2020.00501
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
   Zollhöfer M, 2018, COMPUT GRAPH FORUM, V37, P523, DOI 10.1111/cgf.13382
NR 50
TC 0
Z9 0
U1 5
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6642
EP 6652
DI 10.1109/TMM.2022.3212282
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500072
DA 2024-07-18
ER

PT J
AU Lv, X
   Xiang, T
   Yang, Y
   Liu, HT
AF Lv, Xiao
   Xiang, Tao
   Yang, Ying
   Liu, Hantao
TI Blind Dehazed Image Quality Assessment: A Deep CNN-Based Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image quality; Feature extraction; Visualization; Image color analysis;
   Convolution; Task analysis; Quality assessment; Channel attention;
   dehazed image quality assessment; multiscale convolution; patch
   attention; residual concatenation
ID CONVOLUTIONAL NEURAL-NETWORK; MULTISCALE
AB Research on image dehazing has made the need for a suitable dehazed image quality assessment (DIQA) method even more urgent. The performance of existing DIQA methods heavily relies on handcrafted haze-related features. Since hazy images with uneven haze density distributions will result in uneven quality distributions after dehazing, the manually extracted feature expression is neither accurate nor robust. In this paper, we design a deep CNN-based DIQA method without a handcrafted feature requirement. Specifically, we propose a blind dehazed image quality assessment model (BDQM), which consists of three components: image preprocessing, a haze-related feature extraction network (HFNet), and an improved regression network (IRNet). In HFNet, we design a perceptual information enhancement (PIE) module to learn powerful feature representations and enhance network capability according to channel attention, multiscale convolution and residual concatenation. IRNet aims to aggregate all patch information for the quality prediction of the whole image, where the effect of inhomogeneous distortion from the dehazing procedure is attenuated via a specifically designed patch attention (PA) mechanism. Experimental results on benchmark datasets demonstrate the effectiveness and superiority of the proposed network architecture over state-of-the-art methods.
C1 [Lv, Xiao; Xiang, Tao] Chongqing Univ, Coll Comp Sci, Chongqing 400044, Peoples R China.
   [Yang, Ying] Chinese Univ Hong Kong, Dept Math, Hong Kong 999077, Peoples R China.
   [Liu, Hantao] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 3AA, Wales.
C3 Chongqing University; Chinese University of Hong Kong; Cardiff
   University
RP Xiang, T (corresponding author), Chongqing Univ, Coll Comp Sci, Chongqing 400044, Peoples R China.
EM xiaolv@cqu.edu.cn; txiang@cqu.edu.cn; yyang@math.cuhk.edu.hk;
   liuh35@cardiff.ac.uk
RI 吕, 潇/JYQ-4468-2024; Xiang, Tao/N-3706-2016
OI Xiang, Tao/0000-0002-9439-4623; Yang, Ying/0000-0002-0617-455X; Lv,
   Xiao/0009-0003-7117-1456
FU National Key Ramp;D Program of China
FX No Statement Available
CR Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti C, 2018, LECT NOTES COMPUT SC, V11182, P620, DOI 10.1007/978-3-030-01449-0_52
   Ancuti C, 2016, IEEE IMAGE PROC, P2226, DOI 10.1109/ICIP.2016.7532754
   [Anonymous], 2002, Int. Telecommun. Union
   Anthimopoulos M, 2016, IEEE T MED IMAGING, V35, P1207, DOI 10.1109/TMI.2016.2535865
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Bianco S, 2018, SIGNAL IMAGE VIDEO P, V12, P355, DOI 10.1007/s11760-017-1166-8
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Ding KY, 2022, IEEE T PATTERN ANAL, V44, P2567, DOI 10.1109/TPAMI.2020.3045810
   Ding XH, 2019, IEEE I CONF COMP VIS, P1911, DOI 10.1109/ICCV.2019.00200
   Fang S, 2011, CHIN CONT DECIS CONF, P610, DOI 10.1109/CCDC.2011.5968254
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Geusebroek JM, 2001, IEEE T PATTERN ANAL, V23, P1338, DOI 10.1109/34.977559
   Gu K, 2017, IEEE T IND ELECTRON, V64, P3903, DOI 10.1109/TIE.2017.2652339
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hirschmüller H, 2007, PROC CVPR IEEE, P2134
   Hsieh CH, 2017, INT CONF AWARE SCI, P279
   KAILATH T, 1967, IEEE T COMMUN TECHN, VCO15, P52, DOI 10.1109/TCOM.1967.1089532
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kao YY, 2017, IEEE T IMAGE PROCESS, V26, P1482, DOI 10.1109/TIP.2017.2651399
   Kingma D. P., 2014, arXiv
   Lai YH, 2015, IEEE T CIRC SYST VID, V25, P1, DOI 10.1109/TCSVT.2014.2329381
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li F, 2021, IEEE T CIRC SYST VID, V31, P4798, DOI 10.1109/TCSVT.2021.3055197
   Li LRH, 2020, IEEE T IMAGE PROCESS, V29, P2766, DOI 10.1109/TIP.2019.2952690
   Li Y, 2023, DIGIT COMMUN NETW, V9, P1061, DOI 10.1016/j.dcan.2021.12.001
   Liu W, 2021, IEEE T IMAGE PROCESS, V30, P176, DOI 10.1109/TIP.2020.3033402
   Liu YT, 2020, IEEE T CIRC SYST VID, V30, P929, DOI 10.1109/TCSVT.2019.2900472
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Ma KD, 2015, IEEE IMAGE PROC, P3600, DOI 10.1109/ICIP.2015.7351475
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Min XK, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3470970
   Min XK, 2019, IEEE T INTELL TRANSP, V20, P2879, DOI 10.1109/TITS.2018.2868771
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Min XK, 2019, IEEE T MULTIMEDIA, V21, P2319, DOI 10.1109/TMM.2019.2902097
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   MORRONE MC, 1986, NATURE, V324, P250, DOI 10.1038/324250a0
   Park J, 2020, IEEE T IMAGE PROCESS, V29, P4721, DOI 10.1109/TIP.2020.2975986
   Po LM, 2019, IEEE T CIRC SYST VID, V29, P1223, DOI 10.1109/TCSVT.2019.2891159
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Shen LL, 2021, NEUROCOMPUTING, V424, P132, DOI 10.1016/j.neucom.2020.10.024
   SHEN W, 2017, J NETW INTELL, V2, P139
   Song YC, 2017, J PHYS CONF SER, V844, DOI 10.1088/1742-6596/844/1/012045
   Tarel JP, 2012, IEEE INTEL TRANSP SY, V4, P6, DOI 10.1109/MITS.2012.2189969
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Tarel JP, 2010, IEEE INT VEH SYM, P478, DOI 10.1109/IVS.2010.5548128
   Video Quality Experts Group, 2000, P VOEG MAT OTT ON CA, P1
   Wang CS, 2019, IEEE INT SYMP PARAL, P1545, DOI 10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00227
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xia GS, 2017, IEEE T GEOSCI REMOTE, V55, P3965, DOI 10.1109/TGRS.2017.2685945
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   Yan QS, 2019, IEEE T IMAGE PROCESS, V28, P2200, DOI 10.1109/TIP.2018.2883741
   Yang JC, 2022, IEEE T CIRC SYST VID, V32, P4854, DOI 10.1109/TCSVT.2021.3133859
   Yang JC, 2021, IEEE T INF FOREN SEC, V16, P4234, DOI 10.1109/TIFS.2021.3102487
   Yang JC, 2021, IEEE T MULTIMEDIA, V23, P797, DOI 10.1109/TMM.2020.2990075
   Yang Y, 2021, IEEE T CIRC SYST VID, V31, P3293, DOI 10.1109/TCSVT.2020.3036854
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhai GT, 2019, DIGIT SIGNAL PROCESS, V91, P11, DOI 10.1016/j.dsp.2019.02.017
   Zhang JH, 2022, IEEE T INTELL TRANSP, V23, P3087, DOI 10.1109/TITS.2020.3030673
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhao SY, 2020, IEEE T IMAGE PROCESS, V29, P6947, DOI 10.1109/TIP.2020.2995264
   Zhou W, 2021, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS51556.2021.9401285
NR 70
TC 3
Z9 3
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9410
EP 9424
DI 10.1109/TMM.2023.3252267
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200029
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Pei, EC
   Zhao, Y
   Oveneke, MC
   Jiang, DM
   Sahli, H
AF Pei, Ercheng
   Zhao, Yong
   Oveneke, Meshia Cedric
   Jiang, Dongmei
   Sahli, Hichem
TI A Bayesian Filtering Framework for Continuous Affect Recognition From
   Facial Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bayes methods; Computational modeling; Visualization; Data models;
   Kalman filters; Emotion recognition; Adaptation models; Recurrent neural
   network; Bayesian filter; adaptive alignment; continuous affect
   recognition
ID FUSION; NETWORK
AB Continuous affective state estimation from facial information is a task which requires the prediction of time series of emotional state outputs from a facial image sequence. Modeling the spatial-temporal evolution of facial information plays an important role in affective state estimation. One of the most widely used methods is Recurrent Neural Networks (RNN). RNNs provide an attractive framework for propagating information over a sequence using a continuous-valued hidden layer representation. In this work, we propose to instead learn rich affective state dynamics. We model human affect as a dynamical system and define the affective state in terms of valence, arousal and their higher-order derivatives. We then pose the affective state estimation problem as a jointly trained state estimator for high-dimensional input images, combining an RNN and a Bayesian Filter, i.e. Kalman filters (KF) and Extended Kalman filters (EKF), so that all weights in the resulting network can be trained using backpropagation. We use a recently proposed general framework for designing and learning discriminative state estimators framed as computational graphs. Such approach can handle high dimensional observations and efficiently optimize, in an end-to-end fashion, the state estimator. In addition, to deal with the asynchrony between emotion labels and input images, caused by the inherent reaction lag of the annotators, we introduce a convolutional layer that aligns features with emotion labels. Experimental results, on the RECOLA and SEMAINE datasets for continuous emotion prediction, illustrate the potential of the proposed framework compared to recent state-of-the-art models.
C1 [Pei, Ercheng] Xian Univ Posts & Telecommun, Sch Comp Sci & Technol, Xian 710121, Shaanxi, Peoples R China.
   [Pei, Ercheng] Shaanxi Key Lab Network Data Anal & Intelligent P, Xian 710121, Shaanxi, Peoples R China.
   [Pei, Ercheng] Xian Key Lab Big Data & Intelligent Comp Sci, Xian 710121, Shaanxi, Peoples R China.
   [Pei, Ercheng] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.
   [Zhao, Yong; Jiang, Dongmei] Northwestern Polytech Univ, Natl Engn Lab Integrated Aerospace Ground Ocean B, Shaanxi Key Lab Speech & Image Informat Proc, NPU VUB Joint AVSP Res Lab,Sch Comp Sci, Xian 710072, Peoples R China.
   [Oveneke, Meshia Cedric] Vrije Univ Brussel VUB, Dept ETRO, B-1050 Brussels, Belgium.
   [Jiang, Dongmei] Peng Cheng Lab, Shenzhen 518055, Guangdong, Peoples R China.
   [Sahli, Hichem] Vrije Univ Brussel VUB, Dept Elect & Informat ETRO, VUB NPU Joint AVSP Res Lab, B-1050 Brussels, Belgium.
C3 Xi'an University of Posts & Telecommunications; Northwestern
   Polytechnical University; Northwestern Polytechnical University; Vrije
   Universiteit Brussel; Peng Cheng Laboratory; Vrije Universiteit Brussel
RP Jiang, DM (corresponding author), Northwestern Polytech Univ, Natl Engn Lab Integrated Aerospace Ground Ocean B, Shaanxi Key Lab Speech & Image Informat Proc, NPU VUB Joint AVSP Res Lab,Sch Comp Sci, Xian 710072, Peoples R China.
EM peiercheng@mail.nwpu.edu.cn; yzhao@etrovub.be; mcovenek@etrovub.be;
   jiangdm@nwpu.edu.cn; hsahli@etrovub.be
OI Jiang, Dongmei/0000-0002-6238-8499
FU Chinese Scholarship Council (CSC) [201706290115]; Shaanxi Provincial
   International Science and Technology Collaboration Project
   [2017KW-ZD-14]; VUB Interdisciplinary Research Program through the
   EMO-App Project; Agency for Innovation by Science and Technology in
   Flanders (IWT) - Ph.D. [131814]; Flemish Government (AI Research
   Program); Special Construction Fund for Key Disciplines of Shaanxi
   Provincial Higher Education
FX This work was supported in part by the Chinese Scholarship Council (CSC)
   under Grant 201706290115, in part by the Shaanxi Provincial
   International Science and Technology Collaboration Project under Grant
   2017KW-ZD-14, in part by the VUB Interdisciplinary Research Program
   through the EMO-App Project, in part by the Agency for Innovation by
   Science and Technology in Flanders (IWT) - Ph.D. under Grant 131814, in
   part by the Flemish Government (AI Research Program), and in part by the
   Special Construction Fund for Key Disciplines of Shaanxi Provincial
   Higher Education. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Dr. Dan Zeng. (Ercheng
   Pei and Yong Zhao contributed equally to this work.)
CR Amirian M, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON AUDIO/VISUAL EMOTION CHALLENGE (AVEC'16), P67, DOI 10.1145/2988257.2988260
   [Anonymous], 2016, IJCAI
   [Anonymous], 2013, 2013 10 IEEE INT C W, DOI [10.1109/FG.2013.6553805, DOI 10.1109/FG.2013.6553805]
   Baltrusaitis Tadas, 2018, 2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018), P59, DOI 10.1109/FG.2018.00019
   Battaglia, 2018, ARXIV180601261
   Bishop Christopher M., 2006, Pattern Recognition and Machine Learning, V4
   Brady K, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON AUDIO/VISUAL EMOTION CHALLENGE (AVEC'16), P97, DOI 10.1145/2988257.2988264
   Chao L, 2015, P 5 INT WORKSH AUD V, P65, DOI [DOI 10.1145/2808196.2811634, 10.1145/2808196.2811634]
   Chen S., 2017, P 7 ANN WORKSH AUD V, P19
   Chen Z., 2003, Statistics, V182, P1, DOI [10.1080/02331888.2012.708030, DOI 10.1080/02331880309257]
   Cho Kyunghyun, 2014, SYNTAX SEMANTICS STR, P5, DOI [10.3115/v1/w14-4012, 10.3115 /v1/D14-1179, DOI 10.3115/V1/D14-1179]
   Coskun H, 2017, IEEE I CONF COMP VIS, P5525, DOI 10.1109/ICCV.2017.589
   Dang T, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4929, DOI 10.1109/ICASSP.2018.8461321
   Dong YZ, 2019, IEEE T MULTIMEDIA, V21, P3150, DOI 10.1109/TMM.2019.2918739
   Downey C, 2017, ADV NEUR IN, V30
   EKMAN Paul., 2003, Unmasking the Face-A Guide to Recognizing Emotions from Facial Expressions
   Ergen T, 2018, IEEE T NEUR NET LEAR, V29, P3772, DOI 10.1109/TNNLS.2017.2741598
   Gao XJ, 2020, IEEE ICC, DOI 10.1109/icc40277.2020.9149379
   Gers F.A., 2002, European Symposium on Artificial Neural Networks (ESANN), P369
   Gers FA, 1999, IEE CONF PUBL, P850, DOI [10.1049/cp:19991218, 10.1162/089976600300015015]
   Gu JW, 2017, PROC CVPR IEEE, P1531, DOI 10.1109/CVPR.2017.167
   Haarnoja T., 2016, ADV NEURAL INFORM PR, P4376
   HAMRICK JB, 2018, RELATIONAL INDUCTIVE
   He Lang, 2015, P 5 INT WORKSH AUD V
   Hecht-Nielsen R., 1989, IJCNN: International Joint Conference on Neural Networks (Cat. No.89CH2765-6), P593, DOI 10.1109/IJCNN.1989.118638
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang DS, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), DOI 10.1109/ICPHM51084.2021.9486527
   HUANG J, 2017, NETWORK PROC 7TH ANN, P11
   Huang J, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6837, DOI 10.1109/ICASSP.2018.8461963
   Huang Z, 2015, P 5 INT WORKSH AUD V, P41, DOI 10.1145/2808196.2811640
   Huang ZC, 2017, INTERSPEECH, P3301, DOI 10.21437/Interspeech.2017-1707
   Khorram S, 2021, IEEE T AFFECT COMPUT, V12, P1069, DOI [10.1109/TAFFC.2019.2917047, 10.1109/taffc.2019.2917047]
   KINGMA DP, 2015, P 3RD INT C LEARN RE
   Kollias D, 2020, INT J COMPUT VISION, V128, P1455, DOI 10.1007/s11263-020-01304-3
   KRISHNAN RG, 2015, MACH LEARN
   Lee J, 2020, IEEE T IMAGE PROCESS, V29, P6977, DOI 10.1109/TIP.2020.2996086
   Li HB, 2017, IEEE T MULTIMEDIA, V19, P2816, DOI 10.1109/TMM.2017.2713408
   Lin SS, 2021, IEEE T MULTIMEDIA, V23, P1581, DOI 10.1109/TMM.2020.3001497
   Markov K, 2015, EUR SIGNAL PR CONF, P2077, DOI 10.1109/EUSIPCO.2015.7362750
   McKeown G, 2010, IEEE INT CON MULTI, P1079, DOI 10.1109/ICME.2010.5583006
   Nicolaou MA, 2011, IEEE T AFFECT COMPUT, V2, P92, DOI 10.1109/T-AFFC.2011.9
   Oveneke MC, 2022, IEEE T AFFECT COMPUT, V13, P426, DOI 10.1109/TAFFC.2019.2944603
   Paszke A., 2019, ADV NEURAL INFORM PR, P8026, DOI DOI 10.48550/ARXIV.1912.01703
   PEI E, 2016, P IEEE INT C MULT EX, P1, DOI DOI 10.1109/ICMEW.2016.7574729
   Pei EC, 2021, IEEE T MULTIMEDIA, V23, P3540, DOI 10.1109/TMM.2020.3026894
   Pei EC, 2019, MULTIMED TOOLS APPL, V78, P19387, DOI 10.1007/s11042-019-7313-1
   Pei EC, 2015, INT CONF AFFECT, P208, DOI 10.1109/ACII.2015.7344573
   Pérez-Ortiz JA, 2003, NEURAL NETWORKS, V16, P241, DOI 10.1016/S0893-6080(02)00219-8
   Ringeval F, 2015, PATTERN RECOGN LETT, V66, P22, DOI 10.1016/j.patrec.2014.11.007
   RUSSELL JA, 1980, PERS SOC PSYCHOL, V39
   Savran A, 2015, IEEE T CYBERNETICS, V45, P1927, DOI 10.1109/TCYB.2014.2362101
   Savran A, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P485
   Schoneveld L, 2021, PATTERN RECOGN LETT, V146, P1, DOI 10.1016/j.patrec.2021.03.007
   Schuller B, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P449
   Somandepalli K, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON AUDIO/VISUAL EMOTION CHALLENGE (AVEC'16), P59, DOI 10.1145/2988257.2988259
   Song SY, 2019, IEEE INT CONF COMP V, P1608, DOI 10.1109/ICCVW.2019.00200
   Todorovic B, 2017, STUD FUZZ SOFT COMP, V349, P173, DOI 10.1007/978-3-319-48317-7_11
   Tzirakis P, 2017, IEEE J-STSP, V11, P1301, DOI 10.1109/JSTSP.2017.2764438
   Valstar M, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON AUDIO/VISUAL EMOTION CHALLENGE (AVEC'16), P3, DOI 10.1145/2988257.2988258
   WLLMER M, 2008, P 9TH ANNU C INT SPE, P597
   Wöllmer M, 2013, IMAGE VISION COMPUT, V31, P153, DOI 10.1016/j.imavis.2012.03.001
   Xia ZQ, 2020, IEEE T MULTIMEDIA, V22, P626, DOI 10.1109/TMM.2019.2931351
   Xie SY, 2019, IEEE T MULTIMEDIA, V21, P211, DOI 10.1109/TMM.2018.2844085
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhu JJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2899, DOI 10.1145/3394171.3413718
   Zhu XY, 2015, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2015.7298679
NR 66
TC 4
Z9 4
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3709
EP 3722
DI 10.1109/TMM.2022.3164248
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500014
DA 2024-07-18
ER

PT J
AU Peng, F
   Long, B
   Long, M
AF Peng, Fei
   Long, Bo
   Long, Min
TI A Semi-Fragile Reversible Watermarking for Authenticating 3D Models
   Based on Virtual Polygon Projection and Double Modulation Strategy
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Watermarking; Solid modeling; Three-dimensional displays;
   Authentication; Distortion; Modulation; Hash functions; Double
   modulation strategy; integrity authentication; semi-fragile reversible
   watermarking; virtual polygon projection
ID INVARIANT
AB Aiming to reduce the embedding distortion and improve tampering location precision of reversible watermarking for authenticating three-dimensional(3D) models, a semi-fragile reversible watermarking based on virtual polygon projection and double modulation strategy is proposed. During the embedding, it first constructs virtual adjacent vertices for each vertex and obtains a corresponding virtual polygon, and then a watermark is generated according to the projection value of the current vertex on the corresponding polygon. For each vertex, double modulation is used to move the vertex to realize watermark embedding. For the verification, it first obtains the vertex position and extracts the watermark, and then regenerates a watermark according to the restored vertex. If the extracted watermark is consistent with the regenerated one, it means that the vertex has not been tampered, and the 3D model can be lossless recovered; otherwise, the vertex is tampered. Experimental results and analysis show that the proposed scheme outperforms the existing methods in embedding distortion and tampering location precision. It has potential application in the integrity authentication of 3D models.
C1 [Peng, Fei; Long, Bo] Hunan Univ, Sch Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
   [Long, Min] Changsha Univ Sci & Technol, Sch Comp & Commun Engn, Changsha 410114, Peoples R China.
C3 Hunan University; Changsha University of Science & Technology
RP Peng, F (corresponding author), Hunan Univ, Sch Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
EM eepengf@gmail.com; longbo@hnu.edu.cn; caslongm@aliyun.com
RI Long, Min/AGW-6059-2022; Peng, Fei/H-6951-2017
OI Peng, Fei/0000-0001-8053-4587
FU National Natural Science Foundation of China [92067104, U1936115,
   62072055]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 92067104, U1936115, and 62072055.
CR [Anonymous], 2009, P INT C E BUS INF SY
   Borah S, 2019, ARAB J SCI ENG, V44, P3867, DOI 10.1007/s13369-018-03714-5
   Cheng-Hung Chuang, 2010, IET International Conference on Frontier Computing. Theory, Technologies and Applications, P77, DOI 10.1049/cp.2010.0541
   Cheung YM, 2007, IEEE T CIRC SYST VID, V17, P1007, DOI 10.1109/TCSVT.2007.903553
   Chou CM, 2006, COMPUT AIDED DESIGN, V38, P1154, DOI 10.1016/j.cad.2006.06.009
   Chou CM, 2009, IEEE COMPUT GRAPH, V29, P72, DOI 10.1109/MCG.2009.20
   Hao-tian Wu, 2008, 2008 IEEE 10th Workshop on Multimedia Signal Processing (MMSP), P797, DOI 10.1109/MMSP.2008.4665183
   Huang YH, 2015, 3D RES, V6, DOI 10.1007/s13319-015-0051-x
   Jhou CY, 2007, 2007 THIRD INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, VOL 1, PROCEEDINGS, P365
   Jiang RQ, 2018, MULTIMED TOOLS APPL, V77, P5263, DOI 10.1007/s11042-017-4430-6
   Jiang RQ, 2018, IEEE T MULTIMEDIA, V20, P55, DOI 10.1109/TMM.2017.2723244
   Lee SH, 2013, DIGIT SIGNAL PROCESS, V23, P1505, DOI 10.1016/j.dsp.2013.04.012
   Li L, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12030347
   Lin HYS, 2005, IEEE T MULTIMEDIA, V7, P997, DOI 10.1109/TMM.2005.858412
   Liu J, 2018, IEEE ACCESS, V6, P56122, DOI 10.1109/ACCESS.2018.2872783
   Ohbuchi R., 1997, Interactive Distributed Multimedia Systems and Telecommunication Services. 4th International Workshop, IDMS '97. Proceedings, P1, DOI 10.1007/BFb0000334
   Peng F, 2021, IEEE T CIRC SYST VID, V31, P411, DOI 10.1109/TCSVT.2020.2969464
   Peng F, 2011, INT J DIGIT CRIME FO, V3, P53, DOI 10.4018/jdcf.2011010104
   Pérez-González F, 2005, IEEE T SIGNAL PROCES, V53, P3960, DOI 10.1109/TSP.2005.855407
   Tsai YY, 2021, IEEE T MULTIMEDIA, V23, P2286, DOI 10.1109/TMM.2020.3009492
   Tsai YY, 2017, SECUR COMMUN NETW, P1, DOI 10.1155/2017/1096463
   Wang NN, 2013, MULTIMED TOOLS APPL, V67, P709, DOI 10.1007/s11042-012-1333-4
   Wang PC, 2007, J INF SCI ENG, V23, P1889
   Wu HT, 2005, 2005 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE, PROCEEDINGS, P774
   Yeo BL, 1999, IEEE COMPUT GRAPH, V19, P36, DOI 10.1109/38.736467
   Zhang QL, 2019, MULTIMED TOOLS APPL, V78, P29713, DOI 10.1007/s11042-018-6219-7
NR 26
TC 8
Z9 8
U1 3
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 892
EP 906
DI 10.1109/TMM.2021.3134159
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900017
DA 2024-07-18
ER

PT J
AU Qi, QS
   Zhang, AX
   Liao, Y
   Sun, WY
   Wang, YL
   Li, XB
   Liu, S
AF Qi, Qiaosong
   Zhang, Aixi
   Liao, Yue
   Sun, Wenyu
   Wang, Yongliang
   Li, Xiaobo
   Liu, Si
TI Simultaneously Training and Compressing Vision-and-Language Pre-Training
   Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transmission line measurements; Adaptation models; Computational
   modeling; Training; Task analysis; Pipelines; Costs;
   Vision-and-Language; model compression; pre-training Model
ID KNOWLEDGE DISTILLATION; IMAGE
AB Model compression is an essential step for large-scale pre-training models toward practical application and deployment on the edge device. However, when conventional compression methods following 'pre-training then compressing' two-phase pipeline are applied to Vision-and-Language Pre-training (VLP) models, it will lead to a high calculation and memory overhead. In this work, we break the two-phase pipeline and propose an efficient and effective one-phase VLP model compression mechanism, named REDUCER, which stands for 'simultaneously training and compREssing' VLP model via progressive moDUle replaCing and nEtwork Rewiring. Specifically, REDUCER consists of three insightful designs. Firstly, we design a one-phase compression framework to train and compress the VLP model simultaneously to avoid the extra calculation and memory cost caused by an isolated model compression phase in the conventional two-phase pipeline. Secondly, we propose an adaptive progressive module replacing mechanism to compress the model depth free from explicit knowledge distillation losses, relieving the multi-task optimization problems. Thirdly, we integrate pruning techniques into VLP model compression to simultaneously compress the model in width and depth. Overall, we obtain a lightweight VLP model with only one pre-training phase, and it is the first one-phase compression method for VLP models. Extensive experiments have been conducted on representative VLP models, i.e., ClipBERT and VICTOR, and the experimental results show a superior trade-off between performance and efficiency.
C1 [Qi, Qiaosong; Zhang, Aixi; Sun, Wenyu; Wang, Yongliang; Li, Xiaobo] Alibaba Grp, Beijing 100012, Peoples R China.
   [Liao, Yue; Liu, Si] Beihang Univ, Inst Artificial Intelligence, Beijing 100083, Peoples R China.
C3 Alibaba Group; Beihang University
RP Liao, Y (corresponding author), Beihang Univ, Inst Artificial Intelligence, Beijing 100083, Peoples R China.
EM kuqs@foxmail.com; zhangaixi2008@gmail.com; liaoyue.ai@gmail.com;
   sunwenyu.swy@alibaba-inc.com; wangyong-liang.wyl@alibaba-inc.com;
   xiaobo.lixb@alibaba-inc.com; liusi@buaa.edu.cn
OI liu, si/0000-0002-9180-2935
FU National Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], 2016, PRUNING CONVOLUTIONA
   Ba LJ, 2014, ADV NEUR IN, V27
   Bhat P, 2021, IEEE COMPUT SOC CONF, P2672, DOI 10.1109/CVPRW53098.2021.00301
   Brown Askell T., 2020, P NEUR INF PROC SYST, P8877
   Chen W, 2022, IEEE T MULTIMEDIA, V24, P1844, DOI 10.1109/TMM.2021.3073279
   Chen XL, 2015, Arxiv, DOI arXiv:1504.00325
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Fan A., 2019, P INT C LEARN REPR
   Fang ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1408, DOI 10.1109/ICCV48922.2021.00146
   Gan Z, 2022, AAAI CONF ARTIF INTE, P652
   Hao ZW, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1803, DOI 10.1145/3474085.3475329
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hou L., 2020, P 34 INT C NEURAL IN, P9782
   Jiao XQ, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P4163
   Kim K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6547, DOI 10.1109/ICCV48922.2021.00650
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lan Z., 2019, P INT CONJ LEARN REP
   Lei CY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2567, DOI 10.1145/3474085.3475431
   Lei J, 2021, PROC CVPR IEEE, P7327, DOI 10.1109/CVPR46437.2021.00725
   Li YA, 2021, AAAI CONF ARTIF INTE, V35, P8518
   Li Zhuohan, 2020, INT C MACHINE LEARNI, P5958
   Liu W., 2020, P 58 ANN M ASS COMPU, P6035, DOI 10.18653/v1/2020.acl-main.537
   Liu XY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3238, DOI 10.1145/3474085.3475474
   Liu YH, 2019, Arxiv, DOI arXiv:1907.11692
   Lu JS, 2019, ADV NEUR IN, V32
   Luan YT, 2019, Arxiv, DOI arXiv:1911.09418
   Luo HS, 2020, Arxiv, DOI arXiv:2002.06353
   Lyu F, 2019, IEEE T MULTIMEDIA, V21, P1971, DOI 10.1109/TMM.2019.2894964
   Michel P, 2019, ADV NEUR IN, V32
   Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272
   Mukherjee S, 2020, Arxiv, DOI arXiv:1910.01769
   Raffel C, 2020, J MACH LEARN RES, V21
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Romero A., 2014, ARXIV14126550
   Sanh V, 2020, Arxiv, DOI arXiv:1910.01108
   Shen XB, 2022, IEEE T MULTIMEDIA, V24, P1116, DOI 10.1109/TMM.2021.3119868
   Song X, 2022, IEEE T MULTIMEDIA, V24, P2914, DOI 10.1109/TMM.2021.3090595
   Su WJ, 2019, ANN NUTR METAB, V75, P31, DOI 10.1159/000501710
   Sun C, 2019, Arxiv, DOI arXiv:1906.05743
   Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756
   Sun SQ, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4323
   Sun ZQ, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2158
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Voita E, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P5797
   Wang DZ, 2019, IEEE T MULTIMEDIA, V21, P2985, DOI 10.1109/TMM.2019.2920620
   Wang GZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2974, DOI 10.1145/3474085.3481034
   Wang H, 2022, IEEE T MULTIMEDIA, V24, P2515, DOI 10.1109/TMM.2021.3083109
   Wang Y., 2021, IJCAI, P1122
   Xu CW, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P7859
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Yang C., 2018, P IEEE C COMP VIS PA
   Yang Z., 2019, ADV NEURAL INFORM PR, P5754
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yuan MK, 2020, IEEE T MULTIMEDIA, V22, P1955, DOI 10.1109/TMM.2019.2951463
   Zagoruyko S., 2016, ARXIV160507146, DOI DOI 10.5244/C.30.87
   Zhang CJ, 2019, IEEE T MULTIMEDIA, V21, P2482, DOI 10.1109/TMM.2019.2903628
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P2518, DOI 10.1109/TMM.2019.2907453
   Zhang LB, 2021, IEEE T MULTIMEDIA, V23, P4158, DOI 10.1109/TMM.2020.3037502
   Zhang LF, 2019, IEEE I CONF COMP VIS, P3712, DOI 10.1109/ICCV.2019.00381
   Zhang PF, 2022, IEEE T MULTIMEDIA, V24, P466, DOI 10.1109/TMM.2021.3053766
   Zhang WQ, 2020, IEEE T MULTIMEDIA, V22, P1032, DOI 10.1109/TMM.2019.2935678
   Zhou LW, 2018, AAAI CONF ARTIF INTE, P7590
NR 64
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8194
EP 8203
DI 10.1109/TMM.2022.3233258
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000026
DA 2024-07-18
ER

PT J
AU Qing, ZW
   Huang, ZY
   Zhang, SW
   Tang, MQ
   Gao, CX
   Jin, R
   Ang , MH Jr
   Sang, N
AF Qing, Zhiwu
   Huang, Ziyuan
   Zhang, Shiwei
   Tang, Mingqian
   Gao, Changxin
   Jin, Rong
   Ang Jr, Marcelo H.
   Sang, Nong
TI ParamCrop: Parametric Cubic Cropping for Video Contrastive Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Parametric cropping; contrastive learning; video representation learning
AB The central idea of contrastive learning is to discriminate between different instances and force different views from the same instance to share the same representation. To avoid trivial solutions, augmentation plays an important role in generating different views, among which random cropping is shown to be effective for the model to learn a generalized and robust representation. Commonly used random crop operation keeps the distribution of the difference between two views unchanged along the training process. In this work, we show that adaptively controlling the disparity between two augmented views along the training process enhances the quality of the learned representations. Specifically, we present a parametric cubic cropping operation, ParamCrop, for video contrastive learning, which automatically crops a 3D cubic by differentiable 3D affine transformations. ParamCrop is trained simultaneously with the video backbone using an adversarial objective, so that it learns to increase the contrastive loss and thus gradually reduces the shared contents between two cropped views. Experiments show that this adaptive and gradual increase in the disparity yielded by ParamCrop is beneficial to learning a strong and generalized representation for downstream tasks, which is shown to be effective on multiple contrastive learning frameworks and video backbones.
C1 [Qing, Zhiwu; Gao, Changxin; Sang, Nong] Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Key Lab Image Proc & Intelligent Control, Wuhan 430074, Peoples R China.
   [Huang, Ziyuan; Ang Jr, Marcelo H.] Natl Univ Singapore, Adv Robot Ctr, Singapore 119077, Singapore.
   [Zhang, Shiwei; Tang, Mingqian; Jin, Rong] Alibaba Grp, Hangzhou 311100, Peoples R China.
C3 Huazhong University of Science & Technology; National University of
   Singapore; Alibaba Group
RP Sang, N (corresponding author), Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Key Lab Image Proc & Intelligent Control, Wuhan 430074, Peoples R China.; Zhang, SW (corresponding author), Alibaba Grp, Hangzhou 311100, Peoples R China.
EM qzw@hust.edu.cn; ziyuan.huang@u.nus.edu; zhangjin.zsw@alibaba-inc.com;
   mingqian.tmq@alibaba-inc.com; cgao@hust.edu.cn;
   jinrong.jr@alibaba-inc.com; mpeangh@nus.edu.sg; nsang@hust.edu.cn
RI LI, LIXIN/KFS-0074-2024; Zhang, Xiaoyu/JXR-6386-2024; Ang,
   Marcelo/B-4469-2010; Gao, Changxin/L-4841-2016
OI sang, nong/0000-0002-9167-1496; Ang, Marcelo/0000-0001-8277-6408; Gao,
   Changxin/0000-0003-2736-3920
FU National Natural Science Foundation of China
FX No Statement Available
CR Alayrac J.-B., 2020, NeurIPS, V33, P25
   Benaim S., 2020, CVPR, P9919
   Bengio Y., 2009, P 26 ANN INT C MACH, V60, P6, DOI [DOI 10.1145/1553374.1553380, 10.1145/1553374.1553380]
   Caron M., 2020, Advances in neural information processing systems, V33, P9912
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen Minghao, 2022, CVPR, P13801
   Chen PH, 2021, AAAI CONF ARTIF INTE, V35, P1045
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020
   Diba A., 2021, P IEEE CVF INT C COM, P1502
   Diba A, 2019, IEEE I CONF COMP VIS, P6191, DOI 10.1109/ICCV.2019.00629
   Duan H., 2022, PROC IEEECVF C COMPU, P3000
   Feichtenhofer C, 2021, PROC CVPR IEEE, P3298, DOI 10.1109/CVPR46437.2021.00331
   Gidaris S., 2018, P 6 INT C LEARNING R
   Grill J.-B., 2020, P ADV NEUR INF PROC, V33, P21271
   Han T., 2019, PROC IEEE INT C COMP, P1
   Han Tengda, 2020, Adv. Neural Inf. Process. Syst., NIPS, V33, P5679
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Hendrycks D., 2020, ICLR, P1
   Ho D, 2019, PR MACH LEARN RES, V97
   Hu K., 2021, P IEEECVF INT C COMP, P7939
   Huang ZY, 2021, PROC CVPR IEEE, P1276, DOI 10.1109/CVPR46437.2021.00133
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jenni S., 2021, P IEEE INT C COMP VI, P9970
   Jing LL, 2019, Arxiv, DOI arXiv:1811.11387
   JueWang Gedas Bertasius, 2022, CVPR, P14010
   Kim D, 2019, AAAI CONF ARTIF INTE, P8545
   Kingma D. P., 2015, P INT C LEARN REPR, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Kong Quan, 2020, Advances in Neural Information Processing Systems, V33, P8089
   Kuang H., 2021, P IEEE CVF INT C COM, P3195
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lee HY, 2017, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2017.79
   Lim S, 2019, ADV NEUR IN, V32
   Liu Y, 2022, IEEE T IMAGE PROCESS, V31, P1978, DOI 10.1109/TIP.2022.3147032
   Luo DZ, 2020, AAAI CONF ARTIF INTE, V34, P11701
   Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Pan T, 2021, PROC CVPR IEEE, P11200, DOI 10.1109/CVPR46437.2021.01105
   Park J., 2022, P IEEE C COMP VIS PA, P14711
   Peng XY, 2022, PROC CVPR IEEE, P16010, DOI 10.1109/CVPR52688.2022.01556
   Qian R., 2021, arXiv
   Qian R., 2021, P IEEE CVF INT C COM, P7990
   Qian R, 2021, PROC CVPR IEEE, P6960, DOI 10.1109/CVPR46437.2021.00689
   Ranasinghe K, 2022, PROC CVPR IEEE, P2864, DOI 10.1109/CVPR52688.2022.00289
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Sun C, 2019, Arxiv, DOI arXiv:1906.05743
   Tamkin A., 2021, P INT C LEARN REPR
   Tarvainen A, 2017, ADV NEUR IN, V30
   Tengda Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P312, DOI 10.1007/978-3-030-58580-8_19
   Tian Y., 2020, NeurIPS, V33, P6827
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   Wang J., 2021, PROC AAAI C ARTIF IN, V35, p10 129
   Wang JL, 2022, IEEE T PATTERN ANAL, V44, P3791, DOI 10.1109/TPAMI.2021.3057833
   Wang JL, 2019, PROC CVPR IEEE, P4001, DOI 10.1109/CVPR.2019.00413
   Wang Jiangliu, 2020, ECCV, P504, DOI DOI 10.1007/978-3-030-58520-430
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xiao F., 2022, P IEEECVF C COMPUTER, P9727
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xu DJ, 2019, PROC CVPR IEEE, P10326, DOI 10.1109/CVPR.2019.01058
   Yonggang Li, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P580, DOI 10.1007/978-3-030-58542-6_35
   You Y, 2017, Arxiv, DOI arXiv:1708.03888
   Yuan L., 2022, IEEE C COMPUTER VISI, P13977
   Zhang P., 2018, P INT C LEARN REPR
   Zhang Y, 2022, AAAI CONF ARTIF INTE, P3380
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhukov Dimitri, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P470, DOI 10.1007/978-3-030-58526-6_28
NR 69
TC 2
Z9 2
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9002
EP 9014
DI 10.1109/TMM.2023.3244126
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Sun, HY
   Li, WH
   Duan, YQ
   Zhou, J
   Lu, JW
AF Sun, Hongyi
   Li, Wanhua
   Duan, Yueqi
   Zhou, Jie
   Lu, Jiwen
TI Learning Adaptive Patch Generators for Mask-Robust Image Inpainting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image inpainting; mask-robust agent; adaptive patch generators
ID FILLING-IN
AB In this paper, we propose a Mask-Robust Inpainting Network (MRIN) approach to recover the masked areas of an image. Most existing methods learn a single model for image inpainting, under a basic assumption that all masks are from the same type. However, we discover that the masks are usually complex and exhibit various shapes and sizes at different locations of an image, where a single model cannot fully capture the large domain gap across different masks. To address this, we learn to decompose a complex mask area into several basic types and recover the damaged image in a patch-wise manner with a type-specific generator. More specifically, our MRIN consists of a mask-robust agent and an adaptive patch generative network. The mask-robust agent contains a mask selector and a patch locator, which generates mask attention maps to select a patch at each step. Based on the predicted mask attention maps, the adaptive patch generative network inpaints the selected patch with the generators bank, so that it sequentially inpaints each patch with different patch generators according to its mask type. Extensive experiments demonstrate that our approach outperforms most state-of-the-art approaches on the Place2, CelebA, and Paris Street View datasets.
C1 [Sun, Hongyi; Li, Wanhua; Duan, Yueqi; Zhou, Jie; Lu, Jiwen] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRist, Beijing 100084, Peoples R China.
   [Sun, Hongyi; Li, Wanhua; Duan, Yueqi; Zhou, Jie; Lu, Jiwen] Tsinghua Univ, Dept Automation, Beijing 100084, Peoples R China.
C3 Tsinghua University; Tsinghua University
RP Lu, JW (corresponding author), Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRist, Beijing 100084, Peoples R China.; Lu, JW (corresponding author), Tsinghua Univ, Dept Automation, Beijing 100084, Peoples R China.
EM sunhongy18@mails.tsinghua.edu.cn; li-wh17@mails.tsinghua.edu.cn;
   duanyueqi@tsinghua.edu.cn; jzhou@tsinghua.edu.cn;
   lujiwen@tsinghua.edu.cn
RI Li, Wanhua/AAE-9197-2021
FU National Natural Science Foundation of China [62125603, U1813218];
   Beijing Academy of Artificial Intelligence (BAAI)
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62125603 and U1813218, and in part by
   the Beijing Academy of Artificial Intelligence (BAAI).
CR Ballester C, 2001, IEEE T IMAGE PROCESS, V10, P1200, DOI 10.1109/83.935036
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bertalmio M, 2003, IEEE T IMAGE PROCESS, V12, P882, DOI 10.1109/TIP.2003.815261
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Caicedo JC, 2015, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2015.286
   Cao QX, 2017, PROC CVPR IEEE, P1656, DOI 10.1109/CVPR.2017.180
   Chan TF, 2001, J VIS COMMUN IMAGE R, V12, P436, DOI 10.1006/jvci.2001.0487
   Dai QQ, 2020, IEEE T MULTIMEDIA, V22, P2564, DOI 10.1109/TMM.2019.2958760
   Ding D, 2019, IEEE T IMAGE PROCESS, V28, P1705, DOI 10.1109/TIP.2018.2880681
   Doersch C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185597
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Goodrich B., 2012, P 2012 IEEE COMPUTER, P19
   Gregor K, 2015, PR MACH LEARN RES, V37, P1462
   Guo ZY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2496, DOI 10.1145/3343031.3351022
   Hongyu Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P725, DOI 10.1007/978-3-030-58536-5_43
   Vo HV, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1948, DOI 10.1145/3240508.3240678
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Jie ZQ, 2016, ADV NEUR IN, V29
   Jingyuan Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7757, DOI 10.1109/CVPR42600.2020.00778
   Kingma D. P., 2014, arXiv
   Krull A, 2017, PROC CVPR IEEE, P2566, DOI 10.1109/CVPR.2017.275
   Lahiri Avisek, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13693, DOI 10.1109/CVPR42600.2020.01371
   Li JY, 2019, IEEE I CONF COMP VIS, P5961, DOI 10.1109/ICCV.2019.00606
   Li KS, 2016, SOFT COMPUT, V20, P885, DOI 10.1007/s00500-014-1547-7
   Liang Liao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P683, DOI 10.1007/978-3-030-58583-9_41
   Liang XD, 2017, PROC CVPR IEEE, P4408, DOI 10.1109/CVPR.2017.469
   Liao L, 2021, PROC CVPR IEEE, P6535, DOI 10.1109/CVPR46437.2021.00647
   Lillicrap, 2015, ARXIV150902971, P1
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P3252, DOI 10.1109/TMM.2018.2831636
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Ma CH, 2020, SIGMOD'20: PROCEEDINGS OF THE 2020 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1051, DOI 10.1145/3318464.3389697
   Mnih V, 2014, ADV NEUR IN, V27
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Nazeri K., 2019, arXiv
   Oh SW, 2019, IEEE I CONF COMP VIS, P4402, DOI 10.1109/ICCV.2019.00450
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Schmeing M, 2015, IEEE T MULTIMEDIA, V17, P2160, DOI 10.1109/TMM.2015.2476372
   Shetty R.R., 2018, Advances in Neural Information Processing Systems, P7706
   Silver D, 2014, PR MACH LEARN RES, V32
   Song LS, 2019, AAAI CONF ARTIF INTE, P2506
   Sun Y, 2015, Arxiv, DOI arXiv:1502.00873
   Tang NC, 2011, IEEE T MULTIMEDIA, V13, P602, DOI 10.1109/TMM.2011.2112642
   van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094
   Wang Q, 2021, IEEE T MULTIMEDIA, V23, P429, DOI 10.1109/TMM.2020.2978633
   Wang TF, 2021, PROC CVPR IEEE, P5116, DOI 10.1109/CVPR46437.2021.00508
   Wang ZY, 2016, PR MACH LEARN RES, V48
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Xiong CM, 2016, PR MACH LEARN RES, V48
   Xiong W, 2019, PROC CVPR IEEE, P5833, DOI 10.1109/CVPR.2019.00599
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan ZY, 2018, LECT NOTES COMPUT SC, V11218, P3, DOI 10.1007/978-3-030-01264-9_1
   Yanhong Zeng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P528, DOI 10.1007/978-3-030-58517-4_31
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yi Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P752, DOI 10.1007/978-3-030-58595-2_45
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou YQ, 2021, PROC CVPR IEEE, P2266, DOI 10.1109/CVPR46437.2021.00230
NR 63
TC 5
Z9 5
U1 3
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4240
EP 4252
DI 10.1109/TMM.2022.3174413
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200013
DA 2024-07-18
ER

PT J
AU Wang, P
   Yang, Y
   Xia, YL
   Wang, K
   Zhang, XY
   Wang, S
AF Wang, Pei
   Yang, Yun
   Xia, Yuelong
   Wang, Kun
   Zhang, Xingyi
   Wang, Song
TI Information Maximizing Adaptation Network With Label Distribution Priors
   for Unsupervised Domain Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Information theory; label distribution priors; mutual information;
   unsupervised domain adaptation
AB Unsupervised domain adaptation, which transfers knowledge from the source domain to the target domain, has still been a challenging problem. However, previous domain adaptation methods typically minimize the domain discrepancy by using the pseudo target labels. Since the pseudo labels can be noisy, which may cause misalignment and unsatisfying adaptation performance. To address the above challenges, we propose an information maximization adaptation network with label distribution priors. We revisit feature alignment in unsupervised domain adaptation from the perspective of distribution alignment, and find that learning discriminant feature representation requires to minimizing distribution discrepancy and maximizing source mutual information between the outputs of the classifier and feature representations. Due to domain shift, maximizing target mutual information may align features to incorrect class directly. We propose a weighted target mutual information by re-weighting the estimated mutual information via the mean prediction confidence in mini-batch, which can eliminate the negative impact of inaccurate estimation. In addition, we introduce a regularization term of label priors distribution to encourage the similarity to the real label distribution. Extensive experimental results on three benchmark datasets show that our proposed method can achieve remarkable results compared with previous methods.
C1 [Wang, Pei; Xia, Yuelong; Wang, Kun] Yunnan Univ, Sch Informat Sci & Engn, Kunming 650504, Yunnan, Peoples R China.
   [Yang, Yun] Yunnan Univ, Natl Pilot Sch Software, Kunming 650091, Yunnan, Peoples R China.
   [Zhang, Xingyi] Anhui Univ, Sch Comp Sci & Technol, Key Lab Intelligent Comp & Signal Proc, Minist Educ, Hefei 230039, Peoples R China.
   [Wang, Song] Univ South Carolina, Coll Engn & Comp, Columbia, SC 29208 USA.
C3 Yunnan University; Yunnan University; Anhui University; University of
   South Carolina System; University of South Carolina Columbia
RP Yang, Y (corresponding author), Yunnan Univ, Natl Pilot Sch Software, Kunming 650091, Yunnan, Peoples R China.
EM peiwang@mail.ynu.edu.cn; yangyan19@hotmail.com; xyl@mail.ynu.edu.cn;
   kunwang@mail.ynu.edu.cn; xyzhanghust@gmail.com; songwang@cec.sc.edu
OI Wang, Song/0000-0003-4152-5295; Yang, Yun/0000-0002-9893-3436; Wang,
   Pei/0000-0003-2467-9321; Zhang, Xingyi/0000-0002-5052-000X
FU ChineseNatural Science Foundation [61876166, 61663046]; Yunnan
   provincial major science and technology special plan Projects:
   Digitization Research and Application Demonstration of Yunnan
   characteristic industry [202002AD080001]
FX This work was supported in part by the ChineseNatural Science Foundation
   under Grants 61876166 and 61663046, and in part by Yunnan provincial
   major science and technology special plan Projects: Digitization
   Research and Application Demonstration of Yunnan characteristic
   industry, under Grant 202002AD080001.
CR Ahmed SM, 2021, PROC CVPR IEEE, P10098, DOI 10.1109/CVPR46437.2021.00997
   Ben-David S., 2007, ADV NEURAL INFORM PR, V19
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Chen JH, 2022, Arxiv, DOI arXiv:2201.03102
   Chen MH, 2019, IEEE I CONF COMP VIS, P2090, DOI 10.1109/ICCV.2019.00218
   Chen XY, 2019, PR MACH LEARN RES, V97
   Cui SH, 2020, PROC CVPR IEEE, P3940, DOI 10.1109/CVPR42600.2020.00400
   des Combes Remi Tachet, 2020, NeurIPS, V33, P19276
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Donahue J, 2014, PR MACH LEARN RES, V32
   French G., 2018, P INT C LEARN REPR V
   Ganin Y, 2016, J MACH LEARN RES, V17
   Grandvalet Y, 2004, Advances in neural information processing systems, V17
   Gretton A., 2012, NEURAL INFORM PROCES
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He T, 2022, AAAI CONF ARTIF INTE, P879
   Hu WH, 2017, PR MACH LEARN RES, V70
   Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503
   Li S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9082, DOI 10.1109/ICCV48922.2021.00897
   Li S, 2017, IEEE T NEUR NET LEAR, V28, P1682, DOI 10.1109/TNNLS.2016.2538282
   Li T, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P8203, DOI 10.1109/ICASSP39728.2021.9414930
   Liang J., 2020, INT C MACH LEARN, P6028, DOI DOI 10.48550/ARXIV.2002.08546
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2019, IEEE T PATTERN ANAL, V41, P3071, DOI 10.1109/TPAMI.2018.2868685
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Morerio P, 2020, IEEE WINT CONF APPL, P3119, DOI [10.1109/WACV45572.2020.9093579, 10.1109/wacv45572.2020.9093579]
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pan YW, 2019, PROC CVPR IEEE, P2234, DOI 10.1109/CVPR.2019.00234
   Park C, 2020, Arxiv, DOI arXiv:2006.10297
   Pei ZY, 2018, AAAI CONF ARTIF INTE, P3934
   Prabhu V, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8538, DOI 10.1109/ICCV48922.2021.00844
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Shermin T, 2021, IEEE T MULTIMEDIA, V23, P2732, DOI 10.1109/TMM.2020.3016126
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wu XF, 2020, Arxiv, DOI arXiv:2002.01690
   Wu XX, 2021, IEEE T CYBERNETICS, V51, P2676, DOI 10.1109/TCYB.2019.2921559
   Xiao N, 2021, PROC CVPR IEEE, P15237, DOI 10.1109/CVPR46437.2021.01499
   Xie SA, 2018, PR MACH LEARN RES, V80
   Xu MH, 2020, AAAI CONF ARTIF INTE, V34, P6502
   Xu RJ, 2019, IEEE I CONF COMP VIS, P1426, DOI 10.1109/ICCV.2019.00151
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Yan HL, 2017, PROC CVPR IEEE, P945, DOI 10.1109/CVPR.2017.107
   Yang Y, 2021, J BIOMED INFORM, V117, DOI 10.1016/j.jbi.2021.103736
   Yang Y, 2022, IEEE T CYBERNETICS, V52, P9194, DOI 10.1109/TCYB.2021.3061147
   Yang Y, 2020, IEEE J TRANSL ENG HE, V8, DOI 10.1109/JTEHM.2020.2984601
   Ying Jin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P464, DOI 10.1007/978-3-030-58589-1_28
   Zellinger W., 2017, P INT C LEARN REPR T
   Zhang WC, 2018, PROC CVPR IEEE, P3801, DOI 10.1109/CVPR.2018.00400
   Zhu YC, 2021, IEEE T NEUR NET LEAR, V32, P1713, DOI 10.1109/TNNLS.2020.2988928
NR 55
TC 5
Z9 5
U1 7
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6026
EP 6039
DI 10.1109/TMM.2022.3203574
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500028
DA 2024-07-18
ER

PT J
AU Wang, XD
   Zheng, ZD
   He, Y
   Yan, F
   Zeng, ZQ
   Yang, Y
AF Wang, Xiaodong
   Zheng, Zhedong
   He, Yang
   Yan, Fei
   Zeng, Zhiqiang
   Yang, Yi
TI Progressive Local Filter Pruning for Image Retrieval Acceleration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retrieval; Information filters; Geometry; Computational modeling;
   Feature extraction; Convolutional neural networks; Training; Deep
   learning; image retrieval; local geometry; network pruning; person
   re-identification
AB Most image retrieval works aim at learning discriminative visual features, while little attention is paid to the retrieval efficiency. The speed of feature extraction is key to the real-world system. Therefore, in this article, we focus on network pruning for image retrieval acceleration. Different from the classification models predicting discrete categories, image retrieval models usually extract continuous features for retrieval, which are more sensitive to network pruning. Such different characteristics of the retrieval and classification models make the traditional pruning method sub-optimal for image retrieval acceleration. Two points are critical for pruning image retrieval models: preserving the local geometry structure of filters and maintaining the model capacity during pruning. In view of the above considerations, we propose a Progressive Local Filter Pruning (PLFP) method. Specifically, we analyze the local geometry of filter distribution in every layer and select redundant filters according to one new criterion that the filter can be replaced locally by other similar filters. Furthermore, to preserve the model capacity of the original model, the proposed method progressively prune the filter by decreasing the scale of filter weights gradually. We evaluate our method on four scene retrieval datasets, i.e., Oxford5K, Oxford105K, Paris6K, and Paris106K, and one person re-identification dataset, i.e., Market-1501. Extensive experiments show that the proposed method (1) preserves the original model capacity while pruning (2) and achieves superior performance to other widely-used pruning methods.
C1 [Wang, Xiaodong; Yan, Fei; Zeng, Zhiqiang] Xiamen Univ Technol, Coll Comp & Informat Engn, Xiamen 361024, Peoples R China.
   [Zheng, Zhedong; Yang, Yi] Univ Technol Sydney, ReLER Lab, Sydney, NSW, Australia.
   [He, Yang] ASTAR, Inst High Performance Comp IHPC, Singapore 138632, Singapore.
   [Yang, Yi] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310058, Peoples R China.
C3 Xiamen University of Technology; University of Technology Sydney; Agency
   for Science Technology & Research (A*STAR); A*STAR - Institute of High
   Performance Computing (IHPC); Zhejiang University
RP Zeng, ZQ (corresponding author), Xiamen Univ Technol, Coll Comp & Informat Engn, Xiamen 361024, Peoples R China.
EM xdwangjsj@xmut.edu.cn; zhedong.zheng@student.uts.edu.au;
   he_yang@cfar.a-star.edu.sg; fyan@xmut.edu.cn; zqzeng@xmut.edu.cn;
   yi.yang@uts.edu.au
RI Shen, Yan/KEJ-4617-2024; zhou, zhou/HPE-9525-2023; wang,
   xiao/HZI-9156-2023; bai, yu/KHU-2608-2024; Zheng, Zhedong/R-5314-2019;
   He, Yang/D-7386-2017; Zhou, Zhiqiang/HIR-4954-2022; liu,
   miao/KGL-7043-2024
OI Zheng, Zhedong/0000-0002-2434-9050; He, Yang/0000-0002-2257-6073; 
FU University of Technology Sydney
FX No Statement Available
CR Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Chen T., 2016, PROC INT C LEARN REP
   Deng C, 2020, IEEE T IMAGE PROCESS, V29, P8892, DOI 10.1109/TIP.2020.3020383
   Deng C, 2020, IEEE T NEUR NET LEAR, V31, P2189, DOI 10.1109/TNNLS.2019.2929068
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Ding GD, 2019, IEEE T MULTIMEDIA, V21, P2891, DOI 10.1109/TMM.2019.2916456
   Fang JW, 2023, Arxiv, DOI arXiv:2211.00385
   Felzenszwalb P, 2008, PROC CVPR IEEE, P1984
   Frankle J., 2018, ARXIV180303635
   Graf, 2017, ARXIV160808710, P1, DOI DOI 10.48550/ARXIV.1608.08710
   Gu YZ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1768, DOI 10.1145/3343031.3351081
   Han  S., 2015, ARXIV151000149
   Han S, 2015, ADV NEUR IN, V28
   Hassibi B., 1993, PROC INT ADV C NEURA
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2234
   He Y, 2019, PROC CVPR IEEE, P4335, DOI 10.1109/CVPR.2019.00447
   He Y, 2020, IEEE T CYBERNETICS, V50, P3594, DOI 10.1109/TCYB.2019.2933477
   He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155
   Hinton G., 2015, COMPUT SCI, V2
   Jaderberg M., 2014, CORR
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Kim J, 2018, ADV NEUR IN, V31
   Kingma D. P., 2015, P INT C LEARN REPR, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   LeCun Y., 1989, P INT ADV C NEUR INF
   Li CL, 2023, IEEE T PATTERN ANAL, V45, P4430, DOI 10.1109/TPAMI.2022.3194044
   Li XL, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2201
   Li YC, 2019, PROC CVPR IEEE, P2795, DOI 10.1109/CVPR.2019.00291
   Lin HY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1676, DOI 10.1145/3343031.3350900
   Lin J, 2017, IEEE T MULTIMEDIA, V19, P1968, DOI 10.1109/TMM.2017.2713410
   Lin JL, 2022, IEEE T IMAGE PROCESS, V31, P3780, DOI 10.1109/TIP.2022.3175601
   Lin SH, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2425
   Lin SH, 2020, IEEE T NEUR NET LEAR, V31, P574, DOI 10.1109/TNNLS.2019.2906563
   Lin SH, 2019, PROC CVPR IEEE, P2785, DOI 10.1109/CVPR.2019.00290
   Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298
   Liu ZM, 2019, IEEE I CONF COMP VIS, P6121, DOI 10.1109/ICCV.2019.00622
   Molchanov P., 2017, INT C LEARN REPR ICL, P1, DOI DOI 10.1002/9781118786352.WBIEG1156
   Philbin J., 2007, PROC IEEE C COMPUT V, P1
   Philbin J, 2008, PROC CVPR IEEE, P2285
   Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40
   Qian XL, 2020, IEEE T PATTERN ANAL, V42, P371, DOI 10.1109/TPAMI.2019.2928294
   Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Ren PZ, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3447582
   Schönberger JL, 2015, PROC CVPR IEEE, P5126, DOI 10.1109/CVPR.2015.7299148
   Shen C, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3309881
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srinivas S., 2015, P BRIT MACH VIS C 20, DOI DOI 10.5244/C.29.31
   Sun TX, 2020, AAAI CONF ARTIF INTE, V34, P8936
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wang XD, 2022, IEEE T CYBERNETICS, V52, P13293, DOI 10.1109/TCYB.2021.3130047
   Wang ZZ, 2020, IEEE T MULTIMEDIA, V22, P2126, DOI 10.1109/TMM.2019.2950523
   Xie D, 2020, IEEE T IMAGE PROCESS, V29, P3626, DOI 10.1109/TIP.2020.2963957
   Yan CX, 2022, IEEE T PATTERN ANAL, V44, P9733, DOI 10.1109/TPAMI.2021.3127346
   Yang EK, 2020, IEEE T CYBERNETICS, V50, P1473, DOI 10.1109/TCYB.2018.2882908
   Yang FX, 2020, AAAI CONF ARTIF INTE, V34, P12597
   Yang Y., 2011, P 22 INT JOINT C ART, P1589
   Yang Y, 2021, FRONT INFORM TECH EL, V22, P1551, DOI 10.1631/FITEE.2100463
   Ye Jianbo, 2018, P INT C LEARN REPR
   Zhang XY, 2019, IEEE I CONF COMP VIS, P8221, DOI 10.1109/ICCV.2019.00831
   Zheng L, 2016, Arxiv, DOI arXiv:1610.02984
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Zhedong, 2024, IEEE Trans Neural Netw Learn Syst, V35, P7534, DOI 10.1109/TNNLS.2022.3214834
   Zheng ZD, 2021, IEEE T MULTIMEDIA, V23, P2683, DOI 10.1109/TMM.2020.3014488
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
NR 72
TC 9
Z9 9
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9597
EP 9607
DI 10.1109/TMM.2023.3256092
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200018
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wu, LT
   Xu, Y
   Hou, JH
   Chen, CLP
   Liu, CL
AF Wu, Lintai
   Xu, Yong
   Hou, Junhui
   Chen, C. L. Philip
   Liu, Cheng-Lin
TI A Two-Level Rectification Attention Network for Scene Text Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Scene text recognition; text rectification; spatial transformer network;
   optical character recognition
ID EFFICIENT
AB Scene text recognition is a challenging task in the computer vision field due to the diversity of text styles and the complexity of the image backgrounds. In recent decades, numerous text rectification and recognition methods have been proposed to solve these problems. However, most of these methods rectify texts at the geometry level or pixel level. The former is limited by geometric constraints, and the latter is prone to blurring the text. In this paper, we propose a two-level rectification attention network (TRAN) to rectify and recognize texts. This network consists of two parts: a two-level rectification network (TORN) and an attention-based recognition network (ABRN). Specifically, the TORN first rectifies texts at the geometry level and then performs a pixel-level adjustment, which not only eliminates the geometric constraints but also renders clear texts. The ABRN's role is to recognize text in the rectified images. To improve the feature extraction ability of our model, we design a new channel-wise and kernel-wise attention unit, which enables the network to handle significant variations of character size and channel interdependencies. Furthermore, we propose a skip training strategy to make our model converge smoothly. We conduct experiments on various benchmarks, including regular and irregular datasets. The experimental results show that our method achieves a state-of-the-art performance.
C1 [Wu, Lintai; Xu, Yong] Harbin Inst Technol, Biocomp Res Ctr, Shenzhen 518055, Guangdong, Peoples R China.
   [Wu, Lintai; Xu, Yong] Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Guangdong, Peoples R China.
   [Wu, Lintai; Hou, Junhui] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Xu, Yong] Peng Cheng Lab, Shenzhen 518055, Guangdong, Peoples R China.
   [Hou, Junhui] City Univ Hong Kong, Shenzhen Res Inst, Hong Kong, Peoples R China.
   [Chen, C. L. Philip] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Chen, C. L. Philip] Pazhou Lab, Guangzhou 510335, Peoples R China.
   [Liu, Cheng-Lin] Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.
   [Liu, Cheng-Lin] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
C3 Harbin Institute of Technology; City University of Hong Kong; Peng Cheng
   Laboratory; Shenzhen Research Institute, City University of Hong Kong;
   City University of Hong Kong; South China University of Technology;
   Pazhou Lab; Chinese Academy of Sciences; Institute of Automation, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Xu, Y (corresponding author), Harbin Inst Technol, Biocomp Res Ctr, Shenzhen 518055, Guangdong, Peoples R China.
EM wulintai@stu.hit.edu.cn; yongxu@ymail.com; jh.hou@cityu.edu.hk;
   philip.chen@ieee.org; liucl@nlpr.ia.ac.cn
RI Wu, Lintai/JWP-6353-2024; wang, hang/JND-8481-2023; ZHANG,
   XUCHEN/KBB-7989-2024; Chen, C. L. Philip/O-2657-2016; Liu,
   Cheng-Lin/JCO-6642-2023; Yao, Chen/JVD-6226-2023
OI Wu, Lintai/0000-0002-9260-0980; Liu, Cheng-Lin/0000-0002-6743-4175; Hou,
   Junhui/0000-0003-3431-2021
FU National Nature Science Foundation of China [61876051]; Shenzhen Key
   Laboratory of Visual Object Detection and Recognition
   [ZDSYS20190902093015527]
FX This work was supported in part by the National Nature Science
   Foundation of China under Grant 61876051 and in part by the Shenzhen Key
   Laboratory of Visual Object Detection and Recognition under Grant
   ZDSYS20190902093015527.
CR Alsharif O., 2014, PROC INT C LEARN REP, P1
   Atienza R, 2021, LECT NOTES COMPUT SC, V12821, P319, DOI 10.1007/978-3-030-86549-8_21
   Bai F, 2018, PROC CVPR IEEE, P1508, DOI 10.1109/CVPR.2018.00163
   Bartz C, 2017, Arxiv, DOI arXiv:1707.08831
   Bartz C, 2018, AAAI CONF ARTIF INTE, P6674
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Chen XX, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3440756
   Cheng ZZ, 2018, PROC CVPR IEEE, P5571, DOI 10.1109/CVPR.2018.00584
   Cheng ZZ, 2017, IEEE I CONF COMP VIS, P5086, DOI 10.1109/ICCV.2017.543
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Zeiler MD, 2012, Arxiv, DOI [arXiv:1212.5701, DOI 10.48550/ARXIV.1212.5701]
   Dai PW, 2020, IEEE T MULTIMEDIA, V22, P1969, DOI 10.1109/TMM.2019.2952978
   Deli Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12110, DOI 10.1109/CVPR42600.2020.01213
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Gao H., 2021, IEEE T DATA SCI, V2, p15:1
   Gao YZ, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2710-7
   Gao YZ, 2019, NEUROCOMPUTING, V339, P161, DOI 10.1016/j.neucom.2019.01.094
   Graves A., 2006, P 23 INT C MACHINE L, P369
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He P, 2016, AAAI CONF ARTIF INTE, P3501
   Hu J., 2018, PROC IEEECVF C COMPU, P7132, DOI DOI 10.1109/CVPR.2018.00745
   Hu WY, 2020, AAAI CONF ARTIF INTE, V34, P11005
   Huang C, 2022, IEEE T IND INFORM, V18, P5171, DOI 10.1109/TII.2021.3122801
   Huang YL, 2020, NEUROCOMPUTING, V376, P202, DOI 10.1016/j.neucom.2019.10.010
   Jaderberg M., 2015, ICLR, DOI DOI 10.48550/ARXIV.1506.02025
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z
   Jaderberg M, 2014, LECT NOTES COMPUT SC, V8692, P512, DOI 10.1007/978-3-319-10593-2_34
   Jaderberg Max, 2014, WORKSH DEEP LEARN NI
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee CY, 2016, PROC CVPR IEEE, P2231, DOI 10.1109/CVPR.2016.245
   Li H, 2019, AAAI CONF ARTIF INTE, P8610
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Liao MH, 2019, AAAI CONF ARTIF INTE, P8714
   Liu W., 2016, PROC BRIT MACH VIS C, V2
   Liu W., 2018, P AAAI C ART INT
   Liu XB, 2018, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2018.00595
   Liu Y, 2018, LECT NOTES COMPUT SC, V11209, P449, DOI 10.1007/978-3-030-01228-1_27
   Liu ZC, 2018, AAAI CONF ARTIF INTE, P7194
   Long SB, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01369-0
   Lucas SM, 2003, PROC INT CONF DOC, P682
   Luo CJ, 2021, INT J COMPUT VISION, V129, P960, DOI 10.1007/s11263-020-01411-1
   Luo CJ, 2019, PATTERN RECOGN, V90, P109, DOI 10.1016/j.patcog.2019.01.020
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Mishra A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.127
   Neumann L, 2011, LECT NOTES COMPUT SC, V6494, P770, DOI 10.1007/978-3-642-19318-7_60
   Ren HQ, 2019, PATTERN RECOGN, V93, P179, DOI 10.1016/j.patcog.2019.04.015
   Risnumawan A, 2014, EXPERT SYST APPL, V41, P8027, DOI 10.1016/j.eswa.2014.07.008
   Shi BG, 2019, IEEE T PATTERN ANAL, V41, P2035, DOI 10.1109/TPAMI.2018.2848939
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Shi BG, 2016, PROC CVPR IEEE, P4168, DOI 10.1109/CVPR.2016.452
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Su BL, 2015, LECT NOTES COMPUT SC, V9003, P35, DOI 10.1007/978-3-319-16865-4_3
   Sun YP, 2019, LECT NOTES COMPUT SC, V11363, P83, DOI 10.1007/978-3-030-20893-6_6
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Phan TQ, 2013, IEEE I CONF COMP VIS, P569, DOI 10.1109/ICCV.2013.76
   Wang C, 2018, INT CONF FRONT HAND, P62, DOI 10.1109/ICFHR-2018.2018.00020
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang TW, 2020, AAAI CONF ARTIF INTE, V34, P12216
   Wen J, 2021, IEEE T CYBERNETICS, V51, P101, DOI 10.1109/TCYB.2020.2987164
   Wu XP, 2021, IEEE T MULTIMEDIA, V23, P3427, DOI 10.1109/TMM.2020.3025696
   Xie ZC, 2019, PROC CVPR IEEE, P6531, DOI 10.1109/CVPR.2019.00670
   Yao C, 2014, IEEE T IMAGE PROCESS, V23, P4737, DOI 10.1109/TIP.2014.2353813
   Yao C, 2014, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2014.515
   Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787
   Yue X., 2020, ECCV, VVolume 12364, P135, DOI [DOI 10.1007/978-3-030-58529-7_9, 10.1007/978-3-030-58529-7_9]
   Yun XL, 2022, IEEE T MULTIMEDIA, V24, P2580, DOI 10.1109/TMM.2021.3087000
   Zhan FN, 2019, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2019.00216
   Zhang CY, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106979
   Zhang M, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P4175, DOI 10.1109/ICASSP39728.2021.9413534
   Zhaoyi Wan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11422, DOI 10.1109/CVPR42600.2020.01144
   Zhi Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13525, DOI 10.1109/CVPR42600.2020.01354
NR 75
TC 3
Z9 3
U1 3
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2404
EP 2414
DI 10.1109/TMM.2022.3146779
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100062
DA 2024-07-18
ER

PT J
AU Zhou, SJ
   Wang, Z
   Hu, CH
   Mao, YA
   Yan, HP
   Zhang, SH
   Wu, C
   Zhu, WW
AF Zhou, Shiji
   Wang, Zhi
   Hu, Chenghao
   Mao, Yinan
   Yan, Haopeng
   Zhang, Shanghang
   Wu, Chuan
   Zhu, Wenwu
TI Caching in Dynamic Environments: A Near-Optimal Online Learning Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Heuristic algorithms; Streaming media; Size measurement; Reinforcement
   learning; Proposals; Optimization; Area measurement; Dynamic
   environment; dynamic regret; online learning
ID POLICIES
AB The rapid growth of rich multimedia data in today's Internet, especially video traffic, has challenged the content delivery networks (CDNs). Caching serves as an important means to reduce user access latency so as to enable faster content downloads. Motivated by the dynamic nature of the real-world edge traces, this paper introduces a provably well online caching policy in dynamic environments where: 1) the popularity is highly dynamic; 2) no regular stochastic pattern can model this dynamic evaluation process. First, we design an online optimization framework, which aims to minimize the dynamic regret that finds the distance between an online caching policy and the best dynamic policy in hindsight. Second, we propose a dynamic online learning method to solve the non-stationary caching problem formulated in the previous framework. Compared to the linear dynamic regret of previous methods, our proposal is proved to achieve a sublinear dynamic regret, from which it is guaranteed to be nearly optimal. We verify the design using both synthetic and real-world traces: the proposed policy achieves the best performance in the synthetic traces with different levels of dynamicity, which verifies the dynamic adaptation; our proposal consistently achieves at least 9.4% improvement than the baselines, including LRU, LFU, Static Online Learning based replacement, and Deep Reinforcement Learning based replacement, in random edge areas from real-world traces (from iQIYI), further verifying the effectiveness and robustness on the edge.
C1 [Zhou, Shiji] Tsinghua Berkeley Shenzhen Inst, Shenzhen 518000, Peoples R China.
   [Wang, Zhi] Tsinghua Shenzhen Int Grad Sch, Shenzhen 518000, Peoples R China.
   [Wang, Zhi] Peng Cheng Lab, Shenzhen 518000, Peoples R China.
   [Hu, Chenghao] Univ Toronto, Elect & Comp Engn, Toronto, ON M4Y 1M7, Canada.
   [Mao, Yinan] Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen 518000, Peoples R China.
   [Yan, Haopeng; Zhu, Wenwu] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100080, Peoples R China.
   [Zhang, Shanghang] Univ Calif Berkeley, EECS, Berkeley, CA USA.
   [Wu, Chuan] Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Tsinghua Shenzhen International Graduate School; Tsinghua Shenzhen
   International Graduate School; Peng Cheng Laboratory; University of
   Toronto; Tsinghua University; Tsinghua Shenzhen International Graduate
   School; Tsinghua University; University of California System; University
   of California Berkeley; University of Hong Kong
RP Wang, Z (corresponding author), Tsinghua Shenzhen Int Grad Sch, Shenzhen 518000, Peoples R China.; Zhu, WW (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100080, Peoples R China.
EM zsj17@mails.tsinghua.edu.cn; wangzhi@sz.tsinghua.edu.cn;
   ch.hu@mail.utoronto.ca; 525067649@qq.com; yhjp18@mails.tsinghua.edu.cn;
   shz@eecs.berkeley.edu; cwu@cs.hku.hk; wwzhu@tsinghua.edu.cn
RI Zhu, Wenwu/C-5025-2018; Zhang, Lisa/AAW-9795-2021; Wang,
   Zhi/GZB-2713-2022; Wu, Chuan/E-9919-2010
OI Wang, Zhi/0000-0001-6952-8848; Wang, Zhi/0000-0002-5462-6178; Wu,
   Chuan/0000-0002-3144-4398
FU National Key Research and Development Program of China under
   [2020AAA0106300]; National Natural Science Foundation ofChina under
   [62050110, 61872215]; Shenzhen Scienceand Technology Program under Grant
   [RCYX20200714114523079]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020AAA0106300, in part by the
   National Natural Science Foundation ofChina under Grants 62050110 and
   61872215, in part by the Shenzhen Scienceand Technology Program under
   Grant RCYX20200714114523079, and in part by the Kuaishou for sponsoring
   the research. T
CR Ahlehagh H, 2014, IEEE ACM T NETWORK, V22, P1444, DOI 10.1109/TNET.2013.2294111
   AHO AV, 1971, J ACM, V18, P80, DOI 10.1145/321623.321632
   [Anonymous], 2014, CISCO VISUAL NETWORK
   Bhattacharjee R, 2020, P ACM MEAS ANAL COMP, V4, DOI 10.1145/3392143
   Blasco P, 2014, IEEE ICC, P1897, DOI 10.1109/ICC.2014.6883600
   Casale G, 2018, IEEE INFOCOM SER, P432, DOI 10.1109/INFOCOM.2018.8485894
   Cheng P, 2019, IEEE T COMMUN, V67, P1663, DOI 10.1109/TCOMM.2018.2878231
   Fricker C, 2012, 2012 24TH INTERNATIONAL TELETRAFFIC CONGRESS (ITC 24), P57
   Garg N, 2020, IEEE T COMMUN, V68, P1087, DOI 10.1109/TCOMM.2019.2956041
   Garg N, 2019, INT CONF ACOUST SPEE, P3092, DOI 10.1109/ICASSP.2019.8682841
   Golrezaei N., 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P2781, DOI 10.1109/ISIT.2012.6284029
   Golrezaei N, 2012, IEEE INFOCOM SER, P1107, DOI 10.1109/INFCOM.2012.6195469
   Guo KY, 2017, IEEE WCNC
   Hachem Jad, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P756, DOI 10.1109/INFOCOM.2015.7218445
   Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013
   Hu W, 2016, J COMPUT SCI TECH-CH, V31, P1072, DOI 10.1007/s11390-016-1683-x
   Ji MY, 2016, IEEE J SEL AREA COMM, V34, P176, DOI 10.1109/JSAC.2015.2452672
   Leconte Mathieu, 2016, IEEE INFOCOM
   Lee D, 2001, IEEE T COMPUT, V50, P1352, DOI 10.1109/tc.2001.970573
   Li SH, 2016, IEEE T MULTIMEDIA, V18, P2503, DOI 10.1109/TMM.2016.2596042
   Liu J., 2018, PROC IEEEACM 26 INT, P1
   Ma G, 2017, IEEE J SEL AREA COMM, V35, P1076, DOI 10.1109/JSAC.2017.2680958
   Maddah-Ali MA, 2014, IEEE T INFORM THEORY, V60, P2856, DOI 10.1109/TIT.2014.2306938
   Maggi L, 2018, COMPUT COMMUN, V116, P159, DOI 10.1016/j.comcom.2017.11.015
   Mehrizi S., 2019, IEEE WCNC, P1, DOI DOI 10.1109/wcnc.2019.8885590
   Mehrizi S, 2019, IEEE ACCESS, V7, P92341, DOI 10.1109/ACCESS.2019.2927494
   Müller S, 2017, IEEE T WIREL COMMUN, V16, P1024, DOI 10.1109/TWC.2016.2636139
   Muller S., 2016, ICC, P1
   O'Neil E. J., 1993, SIGMOD Record, V22, P297, DOI 10.1145/170036.170081
   Paschos GS, 2019, IEEE INFOCOM SER, P235, DOI [10.1109/infocom.2019.8737446, 10.1109/INFOCOM.2019.8737446]
   Poularakis K, 2016, IEEE INFOCOM SER
   Qi KQ, 2019, IEEE ACCESS, V7, P120788, DOI 10.1109/ACCESS.2019.2936866
   Roberts J, 2013, 2013 25TH INTERNATIONAL TELETRAFFIC CONGRESS (ITC)
   Sadeghi A, 2018, IEEE INT WORK SIGN P, P66
   Sadeghi A, 2018, IEEE J-STSP, V12, P180, DOI 10.1109/JSTSP.2017.2787979
   SLEATOR DD, 1985, COMMUN ACM, V28, P202, DOI 10.1145/2786.2793
   Somuyiwa S. O., 2018, PROC 15 INT S WIRELE, P1
   Somuyiwa SO, 2018, IEEE J SEL AREA COMM, V36, P1331, DOI 10.1109/JSAC.2018.2844985
   Sung J, 2021, IEEE T MULTIMEDIA, V23, P2442, DOI 10.1109/TMM.2020.3011330
   Tan Y., 2017, P IEEE 85 VEH TECHN
   Traverso S, 2013, ACM SIGCOMM COMP COM, V43, P6
   Wang FX, 2020, IEEE INFOCOM SER, P2499, DOI [10.1109/infocom41043.2020.9155373, 10.1109/INFOCOM41043.2020.9155373]
   Zhang N., 2018, 2018 IEEE INT C COMM, P1
   Zhong C, 2020, IEEE T COGN COMMUN, V6, P48, DOI 10.1109/TCCN.2020.2968326
   Zhong C, 2018, 2018 52ND ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS (CISS), DOI 10.1109/ciss.2018.8362276
   Zinkevich M., 2003, ICML, V20, P928
NR 46
TC 6
Z9 6
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 792
EP 804
DI 10.1109/TMM.2021.3132156
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900009
DA 2024-07-18
ER

PT J
AU Deng, SY
   Xiang, G
   Gao, QX
   Xia, W
   Gao, XB
AF Deng, Siyang
   Xiang, Gang
   Gao, Quanxue
   Xia, Wei
   Gao, Xinbo
TI Zero-Shot Learning Based on Quality-Verifying Adversarial Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Zero-shot learning; generative adversarial network; quality-verifying;
   class metric constraint; l(12)-norm constraint
AB Recently, generative adversarial network (GAN)based zero-shot learning methods have attracted widespread attention. However, due to the randomness of GAN generation, mast existing methods cannot well guarantee to generate sufficiently reliable features and have good generalization ability. Targeting at these problems, we propose an effective Quality-Verifying Adversarial Network (QVAN) that consists of one generator and double discriminators. Adversarial learning between the former discriminator and generator is to generate visual features, which can be partitioned into pseudo-generated features and reliable-generated features. The latter discriminator is used for quality-verifying that will guide the generator to generate more reliable features that are near the real visual features. To avoid over-fitting and ensure intra-class diversity, we set the threshold for each class to distinguish pseudo-generated features and reliable-generated features. To further preserve both compactness and discrim inability of the samples, we introduce the class metric constraint, which are more conducive to classification. Moreover, we introduce l(1,2)-norm constraint to fully consider the specific distribution among different classes, thus making the generated features more discriminant. Extensive experiments on several real-world datasets show the effectiveness of the proposed approach, which demonstrate the advantage over the state-of-the-art methods.
C1 [Deng, Siyang; Gao, Quanxue; Xia, Wei] Xidian Univ, State key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Deng, Siyang] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
   [Deng, Siyang] Bei hang Univ, Beihang Univ, Control Inst, China & also Sch Automationand Elect Engn, Beijing 100191, Peoples R China.
   [Gao, Quanxue] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
C3 Xidian University; Chongqing University of Posts & Telecommunications;
   Beihang University; Chongqing University of Posts & Telecommunications
RP Gao, QX (corresponding author), Xidian Univ, State key Lab Integrated Serv Networks, Xian 710071, Peoples R China.; Gao, QX (corresponding author), Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
EM xdsiyangdeng@163.com; xianggang@buaa.edu.cn; qxgao@xidian.edu.cn;
   xd.weixia@gmail.com; xbgao@mail.xidian.edu.cn
OI Xiang, Gang/0000-0002-6278-3085
FU National Natural Science Foundation of China [62176203, 62036007,
   62050175]; Equipment Pre-research Key Laboratory Funds [6142501200201];
   Guangdong v2x data security key technology and expanded application R&D
   Industry Education Integration In-novation Platform [PT2021C002]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62176203, 62036007,and 62050175, in
   part by Equipment Pre-research Key Laboratory Funds un-der Grants
   6142501200201, and in part by Guangdong v2x data security key technology
   and expanded application R&D Industry Education Integration In-novation
   Platform under Grant PT2021C002. The Associate Editor coordinatingthe
   review of this manuscript and approving it for School of Automation
   andElectrical publication was Prof. Guo-Jun Qi.
CR Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575
   Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4
   Chen L, 2018, PROC CVPR IEEE, P1043, DOI 10.1109/CVPR.2018.00115
   Dong J., 2020, IEEE ACCESS, V8
   Elhoseiny M, 2017, PROC CVPR IEEE, P6288, DOI 10.1109/CVPR.2017.666
   Elhoseiny M, 2017, IEEE T PATTERN ANAL, V39, P2539, DOI 10.1109/TPAMI.2016.2643667
   Elhoseiny M, 2013, IEEE I CONF COMP VIS, P2584, DOI 10.1109/ICCV.2013.321
   Felix R, 2018, LECT NOTES COMPUT SC, V11210, P21, DOI 10.1007/978-3-030-01231-1_2
   Gao JY, 2020, IEEE T MULTIMEDIA, V22, P3088, DOI 10.1109/TMM.2020.2969787
   Gao R, 2020, IEEE T IMAGE PROCESS, V29, P3665, DOI 10.1109/TIP.2020.2964429
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352
   Guo YC, 2017, IEEE T IMAGE PROCESS, V26, P3277, DOI 10.1109/TIP.2017.2696747
   Huang H, 2019, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.2019.00089
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jiamin Wu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12764, DOI 10.1109/CVPR42600.2020.01278
   Karessli N, 2017, PROC CVPR IEEE, P6412, DOI 10.1109/CVPR.2017.679
   Kong D., 2014, Advances in Neural Information Processing Systems, P1655
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li J, 2021, IEEE T MULTIMEDIA, V23, P1383, DOI 10.1109/TMM.2020.2997127
   Li JJ, 2019, PROC CVPR IEEE, P7394, DOI 10.1109/CVPR.2019.00758
   Long Y, 2017, PROC CVPR IEEE, P6165, DOI 10.1109/CVPR.2017.653
   Ming D, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3158
   Ming D, 2019, AAAI CONF ARTIF INTE, P4586
   Narayan Sanath, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P479, DOI 10.1007/978-3-030-58542-6_29
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Qi GJ, 2020, INT J COMPUT VISION, V128, P1118, DOI 10.1007/s11263-019-01265-2
   Qi GJ, 2018, PROC CVPR IEEE, P1517, DOI 10.1109/CVPR.2018.00164
   Qi GJ, 2017, IEEE T PATTERN ANAL, V39, P1360, DOI 10.1109/TPAMI.2016.2587643
   Qiao RZ, 2016, PROC CVPR IEEE, P2249, DOI 10.1109/CVPR.2016.247
   Rahman S, 2020, IEEE T MULTIMEDIA, V22, P242, DOI 10.1109/TMM.2019.2924511
   Romeraparedes B., 2015, PROC INT C MACH LEAR, P22161
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   Sariyildiz MB, 2019, PROC CVPR IEEE, P2163, DOI 10.1109/CVPR.2019.00227
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Welinder P., 2010, Technical Report CNS-TR-2010-001
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Ye ZH, 2019, IEEE INT CON MULTI, P85, DOI 10.1109/ICME.2019.00023
   Yunlong Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14032, DOI 10.1109/CVPR42600.2020.01405
   Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321
   Zhang LH, 2018, ADV NEUR IN, V31
   Zhang XX, 2020, IEEE T MULTIMEDIA, V22, P1692, DOI 10.1109/TMM.2019.2959433
   Zhang ZZ, 2018, PROC CVPR IEEE, P6199, DOI 10.1109/CVPR.2018.00649
   Zhu LW, 2020, IEEE T MULTIMEDIA, V22, P45, DOI 10.1109/TMM.2019.2924591
   Zhu PK, 2019, PROC CVPR IEEE, P2990, DOI 10.1109/CVPR.2019.00311
   Zhu YZ, 2018, PROC CVPR IEEE, P1004, DOI 10.1109/CVPR.2018.00111
   Zhu YZ, 2019, IEEE I CONF COMP VIS, P9843, DOI 10.1109/ICCV.2019.00994
NR 50
TC 2
Z9 2
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4526
EP 4537
DI 10.1109/TMM.2021.3119854
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 7B6NT
UT WOS:000899248400007
DA 2024-07-18
ER

PT J
AU Fujihashi, T
   Koike-Akino, T
   Watanabe, T
   Orlik, PV
AF Fujihashi, Takuya
   Koike-Akino, Toshiaki
   Watanabe, Takashi
   Orlik, Philip V.
TI HoloCast plus : Hybrid Digital-Analog Transmission for Graceful Point
   Cloud Delivery With Graph Fourier Transform
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point Cloud; Hybrid Digital-Analog Coding; Graph Signal Processing;
   Wireless Transmission
ID POWER ALLOCATION; VIDEO; COMPRESSION
AB Point cloud is an emerging data format useful for various applications such has holographic display, autonomous vehicle, and augmented reality. Conventionally, communications of point cloud data have relied on digital compression and digital modulation for three-dimensional (3D) data streaming. However, such digital-based delivery schemes have fundamental issues called cliff and leveling effects, where the 3D reconstruction quality is a step function in terms of wireless channel quality. We propose a novel scheme of point cloud delivery, called HoloCast+, to overcome cliff and leveling effects. Specifically, our method utilizes hybrid digital-analog coding, integrating digital compression and analog coding based on graph Fourier transform (GFT), to gracefully improve 3D reconstruction quality with the improvement of channel quality. We demonstrate that HoloCast+ offers better 3D reconstruction quality in terms of the symmetric mean square error (sMSE) by up to 183 dB and 10.5 dB, respectively, compared to conventional digital-based and analog-based delivery methods in wireless fading environments.
C1 [Fujihashi, Takuya; Watanabe, Takashi] Osaka Univ, Grad Sch Informat Sci & Technol, Suita, Osaka 5650871, Japan.
   [Koike-Akino, Toshiaki; Orlik, Philip V.] Mitsubishi Elect Res Labs MERL, Cambridge, MA 02139 USA.
C3 Osaka University
RP Fujihashi, T (corresponding author), Osaka Univ, Grad Sch Informat Sci & Technol, Suita, Osaka 5650871, Japan.
EM fujihashi.takuya@ist.osaka-u.ac.jp; koike@merl.com;
   watanabe@ist.osaka-u.ac.jp; porlik@merl.com
RI Fujihashi, Takuya/Y-1527-2019; Koike-Akino, Toshiaki/T-2062-2019
OI Fujihashi, Takuya/0000-0002-6960-0122; Koike-Akino,
   Toshiaki/0000-0002-2578-5372
FU JSPS KAKENHI [JPH01101, JP20K19783]; Grants-in-Aid for Scientific
   Research [20K19783] Funding Source: KAKEN
FX This work was supported by JSPS KAKENHI under Grants JPH01101 and
   JP20K19783. The associate editor coordinating the reviewof this
   manuscript and approving it for publication was Prof. Abderrahim
   Benslimane.
CR [Anonymous], 2014, P IEEE INT C MULT EX
   Blanche PA, 2010, NATURE, V468, P80, DOI 10.1038/nature09521
   Cheung G, 2018, P IEEE, V106, P907, DOI 10.1109/JPROC.2018.2799702
   Chou PA, 2020, IEEE T IMAGE PROCESS, V29, P2203, DOI 10.1109/TIP.2019.2908095
   Cui Hao, 2013, P ACM INT C MOD AN S, P273
   De Queiroz R.L, 2017, DYNAMIC POLYGON CLOU
   de Queiroz RL, 2016, IEEE T IMAGE PROCESS, V25, P3947, DOI 10.1109/TIP.2016.2575005
   Fan Xiaopeng, 2011, P INT C MOB UB MULT, P226
   Fujihashi T., 2019, PROC IEEE INT C COMM, P1
   Fujihashi T., 2015, IEEE GLOBAL COMMUNIC, P1
   Fujihashi T., 2021, P ICC 2021 IEEE INT, P1
   Fujihashi T, 2019, IEEE T MULTIMEDIA, V21, P1000, DOI 10.1109/TMM.2018.2870074
   Fujihashi T, 2018, IEEE T MULTIMEDIA, V20, P473, DOI 10.1109/TMM.2017.2743984
   Fujihashi Takuya, 2020, P IEEE INT C COMM, P1
   Graziosi D, 2020, APSIPA TRANS SIGNAL, V9, DOI 10.1017/ATSIP.2020.12
   Gu S, 2020, IEEE SIGNAL PROC LET, V27, P176, DOI 10.1109/LSP.2019.2963793
   Gui YQ, 2021, IEEE T MOBILE COMPUT, V20, P3251, DOI 10.1109/TMC.2020.2999195
   Hadizadeh H, 2021, IEEE T MULTIMEDIA, V23, P12, DOI 10.1109/TMM.2020.2975420
   He DL, 2015, ACM S MODEL ANAL SIM, P327, DOI 10.1145/2811587.2811601
   Horaud R., 2009, A short tutorial on graph laplacians, laplacian embedding, and spectral clustering
   Jakubczak S., 2011, PROC MOBICOM, P289
   Kammerl J, 2012, IEEE INT CONF ROBOT, P778, DOI 10.1109/ICRA.2012.6224647
   Li P, 2020, IEEE ACCESS, V8, p85 128
   Liu XM, 2017, IEEE T IMAGE PROCESS, V26, P509, DOI 10.1109/TIP.2016.2627807
   Lu Y., 2020, P IEEE INT C COMM JU, P1
   Luo L, 2019, IEEE T MULTIMEDIA, V21, P2973, DOI 10.1109/TMM.2019.2919474
   Mekuria R, 2016, IEEE DATA COMPR CONF, P620, DOI 10.1109/DCC.2016.91
   Ortega A, 2018, P IEEE, V106, P808, DOI 10.1109/JPROC.2018.2820126
   Park J, 2019, IEEE J EM SEL TOP C, V9, P149, DOI 10.1109/JETCAS.2019.2898622
   Pavez E, 2020, IEEE IMAGE PROC, P2726, DOI 10.1109/ICIP40778.2020.9191183
   Prabhakaran VM, 2011, IEEE T INFORM THEORY, V57, P4573, DOI 10.1109/TIT.2011.2145210
   Pudlewski S, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2342202
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Sandri G, 2019, IEEE IMAGE PROC, P4370, DOI [10.1109/ICIP.2019.8803553, 10.1109/icip.2019.8803553]
   Schnabel R., 2006, S POINT BAS GRAPH 20, P111
   Shen J, 2018, IEEE T MULTIMEDIA, V20, P2788, DOI 10.1109/TMM.2018.2811622
   Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187
   Su P, 2016, J DISP TECHNOL, V12, P1688, DOI 10.1109/JDT.2016.2553440
   Tang XW, 2020, IEEE T VEH TECHNOL, V69, P9896, DOI 10.1109/TVT.2020.3003478
   Tang XW, 2020, MOBILE NETW APPL, V25, P2495, DOI 10.1007/s11036-020-01592-6
   Ticao Zhang, 2019, IEEE Networking Letters, V1, P84, DOI 10.1109/LNET.2019.2912831
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yahampath Pradeepa, 2020, Signal Processing: Image Communication, V87, P1
   Yu H, 2017, NAT PHOTONICS, V11, P186, DOI [10.1038/NPHOTON.2016.272, 10.1038/nphoton.2016.272]
   Yu L, 2015, IEEE T CIRC SYST VID, V25, P436, DOI 10.1109/TCSVT.2014.2347532
   Yu L, 2014, IEEE T CIRC SYST VID, V24, P331, DOI 10.1109/TCSVT.2013.2273675
   Zhang C, 2014, IEEE IMAGE PROC, P2066, DOI 10.1109/ICIP.2014.7025414
   Zhang J, 2019, IEEE T CIRC SYST VID, V29, P1806, DOI 10.1109/TCSVT.2018.2851252
   Zhao J, 2019, IEEE J EM SEL TOP C, V9, P58, DOI 10.1109/JETCAS.2019.2898750
   Zhao X, 2016, IEEE T CIRC SYST VID, V26, P1117, DOI 10.1109/TCSVT.2015.2444753
NR 50
TC 10
Z9 10
U1 2
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2179
EP 2191
DI 10.1109/TMM.2021.3077772
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200031
OA hybrid
DA 2024-07-18
ER

PT J
AU Huang, YJ
   Liu, YH
   Jing, MG
   Zeng, XY
   Fan, YB
AF Huang, Yujie
   Liu, Yuhao
   Jing, Minge
   Zeng, Xiaoyang
   Fan, Yibo
TI Tear the Image Into Strips for Style Transfer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Strips; Correlation; Pipelines; Signal processing algorithms; Memory
   management; Task analysis; Image resolution; Style Transfer; Image
   Signal Processor; Deep Learning
ID COLOR
AB Recently, Deep Convolutional Neural Networks (DCNNs) have achieved remarkable progress in computer vision community, including in style transfer tasks. Normally, most methods feed the full image to the DCNN. Although high-quality results can be achieved in this manner, several underlying problems arise. For one, with the increase in image resolution, the memory footprint will increase dramatically, leading to high latency and massive power consumption. Furthermore, these methods are usually unable to integrate with the commercial image signal processor (ISP), which processes the image in a line-sequential manner. To solve the above problems, we propose a novel ISP-friendly deep learning-based style transfer algorithm: SequentialStyle. A brand new line-sequential processing mode is proposed, where the image is torn into strips, and each strip is sequentially processed, contributing to less memory demand. We further propose a Spatial-Temporal Synergistic (STS) mechanism that decouples the previously simplex 2-D image style transfer into spatial feature processing (in-strip) and temporal correlation transmission (in-between strips). Compared with the SOTA style transfer algorithms, experimental results show that our SequentialStyle is competitive. Besides, SequentialStyle has less demand for memory consumption, even for the images whose resolutions are 4 k or higher.
C1 [Huang, Yujie; Jing, Minge; Zeng, Xiaoyang; Fan, Yibo] Fudan Univ, Coll Microelect, State Key Lab ASIC & Syst, Shanghai 200000, Peoples R China.
   [Liu, Yuhao] Dalian Univ Technol, Sch Comp Sci & Technol, Dalian 116024, Peoples R China.
   [Fan, Yibo] ZTE Corp, State Key Lab Mobile Network & Mobile Multimedia, Shenzhen 518057, Peoples R China.
C3 Fudan University; Dalian University of Technology; ZTE
RP Fan, YB (corresponding author), Fudan Univ, Coll Microelect, State Key Lab ASIC & Syst, Shanghai 200000, Peoples R China.
EM 19112020091@fudan.edu.cn; yuhaoLiu7456@gmail.com; mejing@fudan.edu.cn;
   xyzeng@fudan.edu.cn; fanyibo@fudan.edu.cn
RI yang, kun/JGM-4169-2023; Wang, Xintong/JJE-1189-2023; fan,
   yi/GYU-1036-2022; Wang, Shiyao/JLL-7826-2023; Li, Kun/JLL-6505-2023
OI Li, Kun/0000-0002-3638-2974; Liu, Yuhao/0000-0003-0550-4788; Huang,
   Yujie/0000-0001-7934-7872
FU National Natural Science Foundation of China [61525401, 62031009];
   Shanghai Science and Technology Committee (STCSM) [19511104300]; Alibaba
   Innovative Research (AIR) Program; Innovation Program of Shanghai
   Municipal Education Commission; Fudan-ZTE Joint Laboratory; Fudan
   University-CIOMP Joint Fund [FC2019-001]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62031009, in part by the National
   Natural Science Foundation of China under Grant 61525401, in part by
   Shanghai Science and Technology Committee (STCSM) under Grant
   19511104300, in part by Alibaba Innovative Research (AIR) Program, in
   part by the Innovation Program of Shanghai Municipal Education
   Commission, in part by Fudan University-CIOMP Joint Fund(FC2019-001),
   and in part by FudanZTE Joint Laboratory.
CR Abadi M., 2016, arXiv, DOI DOI 10.48550/ARXIV.1603.04467
   Afifi M, 2020, PROC CVPR IEEE, P1394, DOI 10.1109/CVPR42600.2020.00147
   [Anonymous], 1997, NEURAL COMPUT
   Brown TB, 2020, Arxiv, DOI [arXiv:2005.14165, DOI 10.48550/ARXIV.2005.14165]
   Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Carion N, 2020, Arxiv, DOI [arXiv:2005.12872, DOI 10.48550/ARXIV.2005.12872]
   Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296
   Chen M, 2020, PR MACH LEARN RES, V119
   Chen TS, 2015, IEEE MICRO, V35, P24, DOI 10.1109/MM.2015.41
   Chen XY, 2019, IEEE T IMAGE PROCESS, V28, P546, DOI 10.1109/TIP.2018.2869695
   Chen YH, 2016, CONF PROC INT SYMP C, P367, DOI 10.1109/ISCA.2016.40
   Cheng MM, 2020, IEEE T IMAGE PROCESS, V29, P909, DOI 10.1109/TIP.2019.2936746
   Dosovitskiy Alexey, 2020, ABS201011929 CORR
   Dumoulin V.., 2017, P INT C LEARN REPR I
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Gu SY, 2018, PROC CVPR IEEE, P8222, DOI 10.1109/CVPR.2018.00858
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kingma D.P., 2014, ARXIV14126980
   Kotovenko D, 2019, PROC CVPR IEEE, P10024, DOI 10.1109/CVPR.2019.01027
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1
   Lee J, 2018, ISSCC DIG TECH PAP I, P218, DOI 10.1109/ISSCC.2018.8310262
   Li YJ, 2017, ADV NEUR IN, V30
   Li YJ, 2017, PROC CVPR IEEE, P266, DOI 10.1109/CVPR.2017.36
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Park E, 2018, CONF PROC INT SYMP C, P688, DOI 10.1109/ISCA.2018.00063
   Ramanath R, 2005, IEEE SIGNAL PROC MAG, V22, P34, DOI 10.1109/MSP.2005.1407713
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sajjadi MSM, 2018, PROC CVPR IEEE, P6626, DOI 10.1109/CVPR.2018.00693
   Shen FL, 2018, PROC CVPR IEEE, P8061, DOI 10.1109/CVPR.2018.00841
   Shen W, 2020, PROC CVPR IEEE, P5113, DOI 10.1109/CVPR42600.2020.00516
   Shi XJ, 2015, ADV NEUR IN, V28
   Simonyan K., 2015, P 3 INT C LEARN REPR
   Song ZC, 2017, IEEE T MULTIMEDIA, V19, P702, DOI 10.1109/TMM.2016.2631123
   Sutskever I, 2014, ADV NEUR IN, V27
   Svoboda Jan, 2020, CVPR
   Tao X, 2017, IEEE I CONF COMP VIS, P4482, DOI 10.1109/ICCV.2017.479
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Ulyanov D, 2016, PR MACH LEARN RES, V48
   Vaswani A, 2017, ADV NEUR IN, V30
   Virtusio JJ, 2021, IEEE T MULTIMEDIA, V23, P2273, DOI 10.1109/TMM.2020.3009484
   Wang H, 2020, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR42600.2020.00193
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wen-Chung Kao, 2006, 2006 IEEE International Symposium on Circuits and Systems (IEEE Cat. No. 06CH37717C)
   Xia XD, 2020, Arxiv, DOI arXiv:2004.10955
   Yao Y, 2019, PROC CVPR IEEE, P1467, DOI 10.1109/CVPR.2019.00156
   Zhang C, 2019, AAAI CONF ARTIF INTE, P1254
   Zhang H, 2017, Arxiv, DOI arXiv:1703.06953
   Zhang KH, 2020, PROC CVPR IEEE, P2734, DOI 10.1109/CVPR42600.2020.00281
   Zhang KH, 2019, IEEE T IMAGE PROCESS, V28, P291, DOI 10.1109/TIP.2018.2867733
   Zhang W, 2013, IEEE T MULTIMEDIA, V15, P1594, DOI 10.1109/TMM.2013.2265675
   Zhizhong Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7786, DOI 10.1109/CVPR42600.2020.00781
NR 60
TC 2
Z9 2
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3978
EP 3988
DI 10.1109/TMM.2021.3111515
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400021
DA 2024-07-18
ER

PT J
AU Li, X
   Yang, F
   Luo, AO
   Jiao, ZC
   Cheng, H
   Liu, ZC
AF Li, Xin
   Yang, Fan
   Luo, Ao
   Jiao, Zhicheng
   Cheng, Hong
   Liu, Zicheng
TI EFRNet: Efficient Feature Reconstructing Network for Real-Time Scene
   Parsing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image reconstruction; Semantics; Encoding; Feature extraction;
   Convolutional codes; Real-time systems; Task analysis; Scene parsing;
   dictionary learning; representation learning
ID NEURAL-NETWORK; SEGMENTATION
AB In this paper, we introduce a light-weight and powerful convolutional neural network, termed as efficient feature reconstructing network (EFRNet), for real-time scene parsing. Our key idea is to decompose the process of learning high-resolution representations into two stages: i) bottom-up codebook/coding matrix learning and ii) top-down feature reconstructing. Specifically, the bottom-up process focuses on learning image-specific codewords (codebook) using deep-layer features and generating a coding matrix with the shallow-layer feature map. In the top-down process, the learned codebook and coding matrix are used to rebuild high-resolution features via a lightweight feature reconstructing operator (FRO). In addition, our EFRNet is constructed on a new building block, named efficient adaptive abstraction (EAA) block, to further reduce the overall network parameters and achieve a significant speed up. Extensive experiments are conducted on challenging benchmarks, such as CamVid and Cityscapes. The results show that EFRNet demonstrates state-of-the-art performance with an optimal balance between accuracy and speed.
C1 [Li, Xin; Yang, Fan] Grp 42 G42, Abu Dhabi 51133, U Arab Emirates.
   [Luo, Ao; Cheng, Hong] Univ Elect Sci & Technol China, Ctr Robot, Chengdu 611731, Peoples R China.
   [Jiao, Zhicheng] Univ Penn, Artificial Intelligence Biomed Imaging Lab AIBIL, Philadelphia, PA 19104 USA.
   [Liu, Zicheng] Microsoft AI Percept & Mixed Real, Redmond, WA 98101 USA.
C3 University of Electronic Science & Technology of China; University of
   Pennsylvania
RP Yang, F (corresponding author), Grp 42 G42, Abu Dhabi 51133, U Arab Emirates.
EM xinli_uestc@hotmail.com; fanyang_uestc@hotmail.com;
   aoluo_uestc@hotmail.com; Zhicheng.Jiao@Pennmedicine.upenn.edu;
   hcheng@uestc.edu.cn; zliu@microsoft.com
RI Luo, Ao/AAS-8194-2020
OI Luo, Ao/0000-0003-3494-8062; Yang, Fan/0000-0002-1157-8719; Liu,
   Zicheng/0000-0001-5894-7828; Li, Xin/0000-0001-8047-9610; Jiao,
   Zhicheng/0000-0002-6968-0919
CR [Anonymous], 2020, PROC INT C LEARN REP
   Ao Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P346, DOI 10.1007/978-3-030-58610-2_21
   Arnold S, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION (IEEE ICIA 2017), P201, DOI 10.1109/ICInfA.2017.8078906
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Boutell MR, 2007, IEEE T MULTIMEDIA, V9, P136, DOI 10.1109/TMM.2006.886372
   Brostow GJ, 2008, LECT NOTES COMPUT SC, V5302, P44, DOI 10.1007/978-3-540-88682-2_5
   Chen L, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278067
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Christoph B., 2017, P 20 INT C MED IM CO, P311
   Cong Y, 2017, IEEE T IMAGE PROCESS, V26, P185, DOI 10.1109/TIP.2016.2619260
   Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Ding HH, 2018, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2018.00254
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gao BB, 2017, IEEE T IMAGE PROCESS, V26, P2825, DOI 10.1109/TIP.2017.2689998
   Geng X, 2013, IEEE T PATTERN ANAL, V35, P2401, DOI 10.1109/TPAMI.2013.51
   Gu Z., 2020, IEEE T MULTIMEDIA
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu XL, 2021, COMPUT ELECTRON AGR, V185, DOI 10.1016/j.compag.2021.106135
   Huang YZ, 2014, IEEE T PATTERN ANAL, V36, P493, DOI 10.1109/TPAMI.2013.113
   Huang YZ, 2011, PROC CVPR IEEE, P1649, DOI 10.1109/CVPR.2011.5995655
   Jégou S, 2017, IEEE COMPUT SOC CONF, P1175, DOI 10.1109/CVPRW.2017.156
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Kingma D.P., 2014, ARXIV14126980
   Kornblith S, 2019, PROC CVPR IEEE, P2656, DOI 10.1109/CVPR.2019.00277
   Kreso I, 2021, IEEE T INTELL TRANSP, V22, P4951, DOI 10.1109/TITS.2020.2984894
   Li HC, 2019, PROC CVPR IEEE, P9514, DOI 10.1109/CVPR.2019.00975
   Li X, 2019, IEEE I CONF COMP VIS, P9166, DOI 10.1109/ICCV.2019.00926
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li X, 2019, PROC CVPR IEEE, P9137, DOI 10.1109/CVPR.2019.00936
   Li X, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P439, DOI 10.1145/3123266.3123290
   Li X, 2018, LECT NOTES COMPUT SC, V11219, P370, DOI 10.1007/978-3-030-01267-0_22
   Li Y, 2019, IEEE T MULTIMEDIA, V21, P875, DOI 10.1109/TMM.2018.2867720
   Liang XD, 2016, IEEE T MULTIMEDIA, V18, P1175, DOI 10.1109/TMM.2016.2542983
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Liu S, 2015, IEEE T MULTIMEDIA, V17, P1347, DOI 10.1109/TMM.2015.2443559
   Liu S, 2014, IEEE T MULTIMEDIA, V16, P253, DOI 10.1109/TMM.2013.2285526
   Liu YP, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2568
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo A, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107671
   Luo A, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107308
   Makantasis K, 2019, IEEE IMAGE PROC, P3148, DOI [10.1109/ICIP.2019.8803268, 10.1109/icip.2019.8803268]
   Makantasis K, 2018, IEEE T GEOSCI REMOTE, V56, P6884, DOI 10.1109/TGRS.2018.2845450
   Mehta S, 2019, PROC CVPR IEEE, P9182, DOI 10.1109/CVPR.2019.00941
   Mehta S, 2018, LECT NOTES COMPUT SC, V11214, P561, DOI 10.1007/978-3-030-01249-6_34
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Orsic M, 2019, PROC CVPR IEEE, P12599, DOI 10.1109/CVPR.2019.01289
   Paszke A., 2016, ARXIV160602147
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Perronnin F, 2007, PROC CVPR IEEE, P2272
   Poudel R.P.K., 2019, ARXIV190204502, P289
   Protopapadakis E, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13030371
   Qi MS, 2020, IEEE T IMAGE PROCESS, V29, P5420, DOI 10.1109/TIP.2020.2983567
   Qiurui Wang, 2019, IEEE Transactions on Multimedia, V21, P1839, DOI 10.1109/TMM.2018.2890360
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Romera E, 2018, IEEE T INTELL TRANSP, V19, P263, DOI 10.1109/TITS.2017.2750080
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi HC, 2018, IEEE T MULTIMEDIA, V20, P2670, DOI 10.1109/TMM.2018.2812600
   Tsai CC, 2020, IEEE T MULTIMEDIA, V22, P1016, DOI 10.1109/TMM.2019.2936803
   van Gemert Jan C., 2008, Computer Vision. Proceedings 10th European Conference on Computer Vision, ECCV 2008, P696, DOI 10.1007/978-3-540-88690-7_52
   Wang L., 2020, IEEE T MULTIMEDIA, P1287
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34
   Wu Z., 2017, ARXIVABS171200213
   Xiang W, 2019, IEEE WINT CONF APPL, P1789, DOI 10.1109/WACV.2019.00195
   Xiang ZZ, 2020, IEEE ROBOT AUTOM LET, V5, P596, DOI 10.1109/LRA.2020.2965075
   Yan B, 2020, IEEE T MULTIMEDIA, V22, P676, DOI 10.1109/TMM.2019.2932566
   Yan CG, 2021, IEEE T PATTERN ANAL, V43, P1445, DOI 10.1109/TPAMI.2020.2975798
   Yan CG, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3404374
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P3014, DOI 10.1109/TMM.2020.2967645
   Yang F., 2020, IEEE T IMAGE PROCESS
   Yang F, 2018, AAAI CONF ARTIF INTE, P7461
   Yang H., 2021, ARXIV210402570
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu F., P INT C LEARN REPR, P1
   Yu Z, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P395, DOI 10.1145/2600428.2609563
   Zhai Q, 2021, PROC CVPR IEEE, P12992, DOI 10.1109/CVPR46437.2021.01280
   Zhang Q, 2015, SYNTH LECT IMAGE VID, V8, P1
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang YH, 2019, PROC CVPR IEEE, P11633, DOI 10.1109/CVPR.2019.01191
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou SH, 2020, IEEE T IMAGE PROCESS, V29, P461, DOI 10.1109/TIP.2019.2919937
NR 92
TC 1
Z9 1
U1 2
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2852
EP 2865
DI 10.1109/TMM.2021.3089422
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000013
DA 2024-07-18
ER

PT J
AU Liu, HF
   Zhang, CY
   Yao, YZ
   Wei, XS
   Shen, FM
   Tang, ZM
   Zhang, J
AF Liu, Huafeng
   Zhang, Chuanyi
   Yao, Yazhou
   Wei, Xiu-Shen
   Shen, Fumin
   Tang, Zhenmin
   Zhang, Jian
TI Exploiting Web Images for Fine-Grained Visual Recognition by Eliminating
   Open-Set Noise and Utilizing Hard Examples
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise measurement; Training; Visualization; Image recognition;
   Annotations; Robustness; Data models; Noisy web images; robust learning;
   fine-grained recognition
ID CLASSIFICATION; CATEGORY; SENSES; PARTS
AB Labeling objects at a subordinate level typically requires expert knowledge, which is not always available when using random annotators. As such, learning directly from web images for fine-grained recognition has attracted broad attention. However, the presence of label noise and hard examples in web images are two obstacles for training robust fine-grained recognition models. Therefore, in this paper, we propose a novel approach for removing irrelevant samples from real-world web images during training, while employing useful hard examples to update the network. Thus, our approach can alleviate the harmful effects of irrelevant noisy web images and hard examples to achieve better performance. Extensive experiments on three commonly used fine-grained datasets demonstrate that our approach is far superior to current state-of-the-art web-supervised methods. The data and source code of this work have been made publicly available at: https://github.com/NUST-Machine-Intelligence-Laboratory/Advanced-Softly-Update-Drop.
C1 [Liu, Huafeng; Zhang, Chuanyi; Yao, Yazhou; Wei, Xiu-Shen; Tang, Zhenmin] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Shen, Fumin] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Zhang, Jian] Univ Technol Sydney, Global Big Data Technol Ctr, Ultimo, NSW 2007, Australia.
C3 Nanjing University of Science & Technology; University of Electronic
   Science & Technology of China; University of Technology Sydney
RP Yao, YZ (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
EM liu.hua.feng@outlook.com; zhangchuanyi@njust.edu.cn;
   yazhou.yao@njust.edu.cn; weixiushen@megvii.com; fumin.shen@gmail.com;
   Tzm.cs@njust.edu.cn; Jian.Zhang@uts.edu.au
RI Tang, Zhenmin/AAY-6058-2020; Shen, Fumin/R-2121-2016
OI Tang, Zhenmin/0000-0001-6708-2205; Zhang, Chuanyi/0000-0001-8724-5796;
   Yao, Yazhou/0000-0002-0337-9410; Zhang, Jian/0000-0002-7240-3541; Liu,
   Huafeng/0000-0001-5396-3183
FU National Natural Science Foundation of China [61976116]; Fundamental
   Research Funds for the Central Universities [30920021135]; Jiangsu
   Province Key Research and Development Plan [BE2016904]; Ark Innovation
   Fund [HT202012310145]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61976116, in part by Fundamental
   Research Funds for the Central Universities under Grant 30920021135, in
   part by Jiangsu Province Key Research and Development Plan (No.
   BE2016904), and in part by Ark Innovation Fund under Grant
   HT202012310145.
CR [Anonymous], 2017, LEARNING DEEP FEATUR
   [Anonymous], 2018, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2018.00749
   Arpit D., 2017, P 34 INT C MACH LEAR, P233, DOI DOI 10.48550/ARXIV.1706.05394
   Chen T, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102965
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432
   Cui Y, 2016, PROC CVPR IEEE, P1153, DOI 10.1109/CVPR.2016.130
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding LZ, 2020, IEEE T NEUR NET LEAR, V31, P4881, DOI 10.1109/TNNLS.2019.2958922
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Ge WF, 2019, PROC CVPR IEEE, P3029, DOI 10.1109/CVPR.2019.00315
   Goldberger J., 2016, PROC INT C LEARN REP, P1
   Guo-Sen Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P562, DOI 10.1007/978-3-030-58548-8_33
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Hinton G., 2015, COMPUT SCI, V2
   Hu Benyi, 2020, P 28 ACM INT C MULT, P4461
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Jiang Lu, 2018, P MACHINE LEARNING R, V80
   Korsch D, 2019, LECT NOTES COMPUT SC, V11824, P62, DOI 10.1007/978-3-030-33676-9_5
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Lam M, 2017, PROC CVPR IEEE, P6497, DOI 10.1109/CVPR.2017.688
   Liang S., 2018, PROC 6 INT C LEARN R, P1
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Lu JR, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102890
   Luo HN, 2019, IEEE I CONF COMP VIS, P9666, DOI 10.1109/ICCV.2019.00976
   Malach E, 2017, ADV NEUR IN, V30
   Niu L, 2015, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2015.7298894
   Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240
   Ranjan Rajeev, 2017, L2-constrained softmax loss for discriminative face verification
   Reed S, 2014, Training deep neural networks on noisy labels with bootstrapping
   Schroff F, 2011, IEEE T PATTERN ANAL, V33, P754, DOI 10.1109/TPAMI.2010.133
   Sun Z., 2020, ACM MM, P92
   Sun Z., PATTERN RECOGN, V110, P2021
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Vedaldi A., 2013, Technical report
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1041, DOI 10.1145/3123266.3123359
   Wang F, 2018, IEEE SIGNAL PROC LET, V25, P926, DOI 10.1109/LSP.2018.2822810
   Wang WW, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P853
   Wang YM, 2018, PROC CVPR IEEE, P4148, DOI 10.1109/CVPR.2018.00436
   Wang YS, 2018, PROC CVPR IEEE, P8688, DOI 10.1109/CVPR.2018.00906
   Wei XS, 2018, PATTERN RECOGN, V76, P704, DOI 10.1016/j.patcog.2017.10.002
   Wu Q ., 2020, PROC INT C NEURAL IN, P1
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Xie GS, 2019, PROC CVPR IEEE, P9376, DOI 10.1109/CVPR.2019.00961
   Xu Z, 2018, IEEE T PATTERN ANAL, V40, P1100, DOI 10.1109/TPAMI.2016.2637331
   Xu Z, 2015, IEEE I CONF COMP VIS, P2524, DOI 10.1109/ICCV.2015.290
   Yao HT, 2016, IEEE T IMAGE PROCESS, V25, P4858, DOI 10.1109/TIP.2016.2599102
   Yao Y., 2020, ACM MM, P1735
   Yao Y., 2016, P 24 ACM INT C MULT, P212
   Yao YZ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P996
   Yao YZ, 2020, IEEE T NEUR NET LEAR, V31, P2348, DOI 10.1109/TNNLS.2020.2966644
   Yao YZ, 2020, IEEE T KNOWL DATA EN, V32, P1199, DOI 10.1109/TKDE.2019.2903036
   Yao YZ, 2018, AAAI CONF ARTIF INTE, P523
   Yao YZ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1085
   Yao YZ, 2019, IEEE T MULTIMEDIA, V21, P184, DOI 10.1109/TMM.2018.2847248
   Yao YZ, 2019, IEEE T IMAGE PROCESS, V28, P436, DOI 10.1109/TIP.2018.2869721
   Yao YZ, 2017, IEEE T MULTIMEDIA, V19, P1771, DOI 10.1109/TMM.2017.2684626
   Yao YZ, 2016, IEEE INT CON MULTI, DOI 10.1109/ICME.2016.7552988
   Yi K, 2019, PROC CVPR IEEE, P7010, DOI 10.1109/CVPR.2019.00718
   Zeng HQ, 2017, PROC INT CONF RECON
   Zhang C., 2020, ACM MM, P2372
   Zhang C ., 2020, P IEEE INT C MULT EX, P1
   Zhang CY, 2020, AAAI CONF ARTIF INTE, V34, P12781
   Zhang HF, 2021, IEEE T MULTIMEDIA, V23, P3400, DOI 10.1109/TMM.2020.3025000
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhou T., 2020, DAVIS CHALLENGE VIDE, P1
   Zhou TF, 2020, AAAI CONF ARTIF INTE, V34, P13066
   Zhuang BH, 2017, PROC CVPR IEEE, P2915, DOI 10.1109/CVPR.2017.311
NR 71
TC 18
Z9 18
U1 3
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 546
EP 557
DI 10.1109/TMM.2021.3055024
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100003
OA Green Published
DA 2024-07-18
ER

PT J
AU Liu, SG
   Zhu, T
AF Liu, Shiguang
   Zhu, Ting
TI Structure-Guided Arbitrary Style Transfer for Artistic Image and Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image edge detection; Feature extraction; Distortion; Visualization;
   Optimization; Graphical models; Distribution functions; Image and video
   style transfer; refine network; region structure loss; cycle-temporal
   loss; structure-guided
AB Recently, neural style transfer has become a popular task in both academic research and industrial applications. Although the existing methods made great progress in terms of quality and efficiency, most of them mainly focus on extracting high-level features. Therefore, it is still challenging to display the hierarchical structure of the content image due to lack of texture information, which causes blurred boundaries and distortion of the stylized image. In this paper, a novel neural image and video style transfer scheme is proposed to suppress distortion and preserve the semantic content of the content image, which is capable of yielding satisfactory stylized images and videos of a variety of scenarios. We first propose to assemble a refine network into an auto-encoder framework to guide style transfer, which can ensure that the stylized image have diverse levels of details. Then, we introduce the global content loss and the local region structure loss to train the model and enhance the robustness of the model. In addition, in order to produce a high-quality stylized video, our method not only preserves the image structure, but also introduces a temporal consistency loss and a cycle-temporal loss to avoid temporal incoherence and motion blur as far as possible. Our approach is also friendly for photographic and exposed image and video style transfer. Both quantitative and qualitative evaluation demonstrated the effectiveness of our method.
C1 [Liu, Shiguang; Zhu, Ting] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
   [Liu, Shiguang] Tianjin Univ, Tianjin Key Lab Cognit Comp & Applicat, Tianjin 300350, Peoples R China.
C3 Tianjin University; Tianjin University
RP Liu, SG (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.; Liu, SG (corresponding author), Tianjin Univ, Tianjin Key Lab Cognit Comp & Applicat, Tianjin 300350, Peoples R China.
EM lsg@tju.edu.cn; sunshineting12@163.com
FU Natural Science Foundation of China [62072328, 61672375]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 62072328 and 61672375. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Vasileios Mezaris.
CR Aberman K, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392469
   [Anonymous], 2018, INT J COMPUT VISION, DOI DOI 10.1007/s11263-018-1089-z
   Bertasius G, 2015, IEEE I CONF COMP VIS, P504, DOI 10.1109/ICCV.2015.65
   Bertasius G, 2015, PROC CVPR IEEE, P4380, DOI 10.1109/CVPR.2015.7299067
   Chen DD, 2017, IEEE I CONF COMP VIS, P1114, DOI 10.1109/ICCV.2017.126
   Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296
   Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Chen TC, 2017, AGEING SOC, V37, P1798, DOI 10.1017/S0144686X16000623
   Chen Weifeng, 2016, ADV NEURAL INFORM PR, V29, P730, DOI DOI 10.5555/3157096.3157178
   Chen XY, 2019, IEEE T IMAGE PROCESS, V28, P546, DOI 10.1109/TIP.2018.2869695
   Cheng MM, 2020, IEEE T IMAGE PROCESS, V29, P909, DOI 10.1109/TIP.2019.2936746
   Duck S. Y., 2016, Painter by numbers
   Dumoulin V.., 2017, P INT C LEARN REPR I
   Frigo O, 2019, VISUAL COMPUT, V35, P429, DOI 10.1007/s00371-018-1474-1
   Gao Chang, 2018, P AS C COMP VIS, P2
   Gao W, 2020, IEEE WINT CONF APPL, P3211, DOI 10.1109/WACV45572.2020.9093420
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Ghiasi G., 2017, The British Machine Vision Conference (BMVC), P1
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He JZ, 2019, PROC CVPR IEEE, P3823, DOI 10.1109/CVPR.2019.00395
   Huang HZ, 2017, PROC CVPR IEEE, P7044, DOI 10.1109/CVPR.2017.745
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kozlovtsev K., 2019, DEPTH PRESERVING REA
   Lai WS, 2018, LECT NOTES COMPUT SC, V11219, P179, DOI 10.1007/978-3-030-01267-0_11
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Li W., 2018, ASIAN C COMPUTER VIS, P232
   Li XT, 2019, PROC CVPR IEEE, P3804, DOI 10.1109/CVPR.2019.00393
   Li YJ, 2018, LECT NOTES COMPUT SC, V11207, P468, DOI 10.1007/978-3-030-01219-9_28
   Li YT, 2017, ADV NEUR IN, V30
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu X.C., 2017, Proceedings of the Symposium on Non-Photorealistic Animation and Rendering, page, P1
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Ma GX, 2020, IEEE T MULTIMEDIA, V22, P324, DOI 10.1109/TMM.2019.2929943
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Ruder M, 2016, LECT NOTES COMPUT SC, V9796, P26, DOI 10.1007/978-3-319-45886-1_3
   Sheng L, 2018, PROC CVPR IEEE, P8242, DOI 10.1109/CVPR.2018.00860
   Simonyan K., 2014, 14091556 ARXIV
   Ulyanov D., 2019, PROC IEEE C COMPUT V, P4105
   Ulyanov D, 2016, PR MACH LEARN RES, V48
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xu Z., 2019, ACMEUROGRAPHICS EXPR, P21, DOI DOI 10.2312/EXP.20191073
   Yao Y, 2019, PROC CVPR IEEE, P1467, DOI 10.1109/CVPR.2019.00156
   Yu ZD, 2017, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2017.191
   Zhang C, 2019, AAAI CONF ARTIF INTE, P1254
   Zhu J, 2018, LECT NOTES COMPUT SC, V10704, P93, DOI 10.1007/978-3-319-73603-7_8
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu T, 2020, PROC IEEE INT C MULT, P1
NR 50
TC 19
Z9 20
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1299
EP 1312
DI 10.1109/TMM.2021.3063605
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200005
DA 2024-07-18
ER

PT J
AU Shao, ZF
   Cheng, G
   Ma, JY
   Wang, ZY
   Wang, JM
   Li, DR
AF Shao, Zhenfeng
   Cheng, Gui
   Ma, Jiayi
   Wang, Zhongyuan
   Wang, Jiaming
   Li, Deren
TI Real-Time and Accurate UAV Pedestrian Detection for Social Distancing
   Monitoring in COVID-19 Pandemic
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Head; Social factors; Human factors; Feature extraction; Real-time
   systems; Object detection; COVID-19; UAV; COVID-19; pedestrian
   detection; spatial attention; social distancing monitoring
ID OBJECT DETECTION
AB Coronavirus Disease 2019 (COVID-19) is a highly infectious virus that has created a health crisis for people all over the world. Social distancing has proved to be an effective non-pharmaceutical measure to slow down the spread of COVID-19. As unmanned aerial vehicle (UAV) is a flexible mobile platform, it is a promising option to use UAV for social distance monitoring. Therefore, we propose a lightweight pedestrian detection network to accurately detect pedestrians by human head detection in real-time and then calculate the social distancing between pedestrians on UAV images. In particular, our network follows the PeleeNet as backbone and further incorporates the multi-scale features and spatial attention to enhance the features of small objects, like human heads. The experimental results on Merge-Head dataset show that our method achieves 92.22% AP (average precision) and 76 FPS (frames per second), outperforming YOLOv3 models and SSD models and enabling real-time detection in actual applications. The ablation experiments also indicate that multi-scale feature and spatial attention significantly contribute the performance of pedestrian detection. The test results on UAV-Head dataset show that our method can also achieve high precision pedestrian detection on UAV images with 88.5% AP and 75 FPS. In addition, we have conducted a precision calibration test to obtain the transformation matrix from images (vertical images and tilted images) to real-world coordinate. Based on the accurate pedestrian detection and the transformation matrix, the social distancing monitoring between individuals is reliably achieved.
C1 [Shao, Zhenfeng; Cheng, Gui; Wang, Jiaming; Li, Deren] Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & R, Wuhan 430079, Peoples R China.
   [Ma, Jiayi] Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.
   [Wang, Zhongyuan] Natl Engn Res Ctr Multimedia Software, Wuhan 430079, Peoples R China.
C3 Wuhan University; Wuhan University
RP Cheng, G (corresponding author), Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & R, Wuhan 430079, Peoples R China.
EM shaozhenfeng@whu.edu.cn; 2019206190077@whu.edu.cn; jyma2010@gmail.com;
   wzy_hope@163.com; wjmecho@163.com; drli@whu.edu.cn
RI Wang, Zhongyuan/ABD-2189-2020; Wang, Jiaming/V-8621-2019; Ma,
   Jiayi/Y-2470-2019
OI Ma, Jiayi/0000-0003-3264-3265; Wang, Zhongyuan/0000-0002-9796-488X;
   Cheng, Gui/0000-0002-7186-5605
FU National Key Research and Development Program of China [2018YFB2100501];
   Key Research and Development Program of Yunnan province in China
   [2018IB023]; National Natural Science Foundation of China [42090012,
   41771452, 41771454, 41901340]; Consulting research project of Chinese
   Academy of Engineering [2020ZD16]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018YFB2100501, in part by the
   Key Research and Development Program of Yunnan province in China under
   Grant 2018IB023, in part by the National Natural Science Foundation of
   China under Grants 42090012, 41771452, 41771454, and 41901340, and in
   part by the Consulting research project of Chinese Academy of
   Engineering under Grant 2020ZD16.
CR Aguilar WG, 2017, LECT NOTES COMPUT SC, V10306, P563, DOI 10.1007/978-3-319-59147-6_48
   AlDahoul N, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/1639561
   Bai YC, 2018, LECT NOTES COMPUT SC, V11217, P210, DOI 10.1007/978-3-030-01261-8_13
   Chan JFW, 2020, LANCET, V395, P514, DOI 10.1016/S0140-6736(20)30154-9
   Chen J, 2020, IEEE GEOSCI REMOTE S, V17, P681, DOI 10.1109/LGRS.2019.2930462
   Chengbin Zeng, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2069, DOI 10.1109/ICPR.2010.509
   Courtemanche C, 2020, HEALTH AFFAIR, V39, P1237, DOI 10.1377/hlthaff.2020.00608
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Dubrofsky E, 2009, DIPLOMOVA PRACE, V5
   Fu C.-Y., 2017, DSSD: Deconvolutional Single Shot Detector
   Gao CQ, 2016, NEUROCOMPUTING, V208, P108, DOI 10.1016/j.neucom.2016.01.097
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   Howard A. G., 2017, arXiv
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jeong J., 2018, ARXIV170509587
   Jiang K, 2019, IEEE T GEOSCI REMOTE, V57, P5799, DOI 10.1109/TGRS.2019.2902431
   Larochelle Hugo, 2010, ADV NEURAL INFORM PR, V23
   Li JA, 2018, IEEE T MULTIMEDIA, V20, P1645, DOI 10.1109/TMM.2017.2772796
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li Q., 2020, NEW ENGL J MED, V382, P1199, DOI [DOI 10.1056/NEJMOA2001316, DOI 10.1056/NEJMoa2001316, 10.1056/NEJMoa2001316]
   Li W, 2020, IEEE T CIRC SYST VID, V30, P482, DOI 10.1109/TCSVT.2019.2890840
   Li YL, 2016, IEEE IMAGE PROC, P594, DOI 10.1109/ICIP.2016.7532426
   Li Z., 2017, CORR
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Ling H., 2018, Vision Meets Drones: A Challenge
   Liu Q., 2011, P INT ARCH PHOT REM, V38
   Ma JY, 2020, IEEE T IND ELECTRON, V67, P5687, DOI 10.1109/TIE.2019.2934071
   Ma YL, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16040446
   Ma'sum MA, 2013, INT C ADV COMP SCI I, P161, DOI 10.1109/ICACSIS.2013.6761569
   Moranduzzo T, 2014, IEEE T GEOSCI REMOTE, V52, P6356, DOI 10.1109/TGRS.2013.2296351
   Nguyen CT, 2020, IEEE ACCESS, V8, P153479, DOI 10.1109/ACCESS.2020.3018140
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257
   Peng DZ, 2018, INT C PATT RECOG, P2528, DOI 10.1109/ICPR.2018.8545068
   Punn N. S., ARXIV200501385, V2020
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Riou J, 2020, EUROSURVEILLANCE, V25, P7, DOI 10.2807/1560-7917.ES.2020.25.4.2000058
   Shao ZF, 2020, IEEE T CIRC SYST VID, V30, P781, DOI 10.1109/TCSVT.2019.2897980
   Shao ZF, 2018, IEEE T MULTIMEDIA, V20, P2593, DOI 10.1109/TMM.2018.2865686
   Stewart R, 2016, PROC CVPR IEEE, P2325, DOI 10.1109/CVPR.2016.255
   Tang P, 2020, IEEE T PATTERN ANAL, V42, P176, DOI 10.1109/TPAMI.2018.2876304
   Torrey L., 2010, IGI Global, P242, DOI 10.4018/978-1-60566-766-9.CH011
   Vu TH, 2015, IEEE I CONF COMP VIS, P2893, DOI 10.1109/ICCV.2015.331
   Vora Aditya, 2018, ARXIV180908766
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang HN, 2019, IEEE INT CONF COMP V, P279, DOI 10.1109/ICCVW.2019.00037
   Wang R. J., 2018, P 2018 ANN C NEUR IN, P1963
   Wang SG, 2018, IEEE T MULTIMEDIA, V20, P3148, DOI 10.1109/TMM.2018.2829602
   Wang S, 2013, IEEE IMAGE PROC, P2822, DOI 10.1109/ICIP.2013.6738581
   Wang YY, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (IEEE ROBIO 2017), P296, DOI 10.1109/ROBIO.2017.8324433
   Wang ZQ, 2017, CHIN CONTR CONF, P11104, DOI 10.23919/ChiCC.2017.8029130
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang D., 2020, A vision-based social distancing and critical density detection system for covid-19
   Yi P, 2020, IEEE T CIRC SYST VID, V30, P2503, DOI 10.1109/TCSVT.2019.2925844
   Zhang JL, 2021, IEEE T MULTIMEDIA, V23, P3085, DOI 10.1109/TMM.2020.3020691
   Zhang J, 2020, NEUROCOMPUTING, V398, P555, DOI 10.1016/j.neucom.2019.03.102
   Zhang RQ, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12193140
   Zhang SF, 2018, LECT NOTES COMPUT SC, V11207, P657, DOI 10.1007/978-3-030-01219-9_39
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhou K, 2016, DESTECH TRANS COMP
   Zhou LG, 2019, IEEE T NEUR NET LEAR, V30, P3275, DOI 10.1109/TNNLS.2018.2890550
NR 67
TC 68
Z9 69
U1 22
U2 116
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2069
EP 2083
DI 10.1109/TMM.2021.3075566
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200023
PM 35582598
OA Bronze, Green Published
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Song, SY
   Miao, ZJ
   Yu, HK
   Fang, JW
   Zheng, K
   Ma, C
   Wang, S
AF Song, Shaoyue
   Miao, Zhenjiang
   Yu, Hongkai
   Fang, Jianwu
   Zheng, Kang
   Ma, Cong
   Wang, Song
TI Deep Domain Adaptation Based Multi-Spectral Salient Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; Synchronization; Object detection; Task analysis;
   Lighting; Adaptation models; Meteorology; Domain adaptation;
   multi-spectral; salient object detection
ID CO-SEGMENTATION; CONSTRAINT
AB Salient Object Detection (SOD) plays an important role in many image-related multimedia applications. Although there are many existing research works about the salient object detection in traditional RGB (visible-light spectrum) images, there are still many complex situations that regular RGB images cannot provide enough cues for the accurate SOD, such as the shadow effect, similar appearance between background and foreground, strong or insufficient illumination, etc. Because of the success of near-infrared spectrum in many computer vision tasks, we explore the multi-spectral SOD in the synchronized RGB images and near-infrared (NIR) images for the both simple and complex situations. We assume that the RGB SOD in the existing RGB image datasets could provide references for the multi-spectral SOD problem. In this paper, we mainly model this research problem as a deep learning based domain adaptation from the traditional RGB image data (source domain) to the multi-spectral data (target domain), and an adversarial deep domain adaptation model is proposed. We first collect and will publicize a large multi-spectral dataset, RGBN-SOD dataset, including 780 synchronized RGB and NIR image pairs for the multi-spectral SOD problem in the simple and complex situations. Intensive experimental results show the effectiveness and accuracy of the proposed deep domain adaptation for the multi-spectral SOD. Besides, due to the absence of research on the field of multi-spectral co-saliency detection, we also collect 200 synchronized RGB and NIR image pairs in addition to explore the multi-spectral co-saliency detection.
C1 [Song, Shaoyue; Miao, Zhenjiang; Ma, Cong] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Song, Shaoyue; Miao, Zhenjiang; Ma, Cong] Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
   [Yu, Hongkai] Cleveland State Univ, Dept Elect Engn & Comp Sci, Cleveland, OH 44115 USA.
   [Fang, Jianwu] Changan Univ, Coll Transportat Engn, Xian 710100, Peoples R China.
   [Zheng, Kang; Wang, Song] Univ South Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USA.
C3 Beijing Jiaotong University; Beijing Jiaotong University; University
   System of Ohio; Cleveland State University; Chang'an University;
   University of South Carolina System; University of South Carolina
   Columbia
RP Miao, ZJ (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.; Miao, ZJ (corresponding author), Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.; Yu, HK (corresponding author), Cleveland State Univ, Dept Elect Engn & Comp Sci, Cleveland, OH 44115 USA.
EM 14112060@bjtu.edu.cn; zjmiao@bjtu.edu.cn; h.yu19@csuohio.edu;
   j.w.fangit@gmail.com; zheng37@email.sc.edu; 13112063@bjtu.edu.cn;
   songwang@cec.sc.edu
RI 房, 建武/IWD-9461-2023
OI Song, Shaoyue/0000-0002-9854-0907; Wang, Song/0000-0003-4152-5295
FU NSFC [61672089, 61703436, 61572064, 61273274, 61806022]; Fundamental
   Research Funds for the Central Universities, CHD [300102320202]; AWS
   Cloud Credits for Research Award;  [NSFC-61672376];  [NSFC-U 1803264]
FX This work was supported in part by the NSFC 61672089, 61703436,
   61572064, 61273274, CELFA; NSFC-61672376, NSFC-U 1803264; and NSFC
   61806022, in part by Fundamental Research Funds for the Central
   Universities, CHD (No.300102320202), and in part by AWS Cloud Credits
   for Research Award. This article was presented in part at the Conference
   on Artificial Intelligence, 2020. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Engin Erzin.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], ARXIV171200075
   Benjdira B, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111369
   Bolte JA, 2019, IEEE COMPUT SOC CONF, P1404, DOI 10.1109/CVPRW.2019.00181
   Brown M, 2011, PROC CVPR IEEE, P177, DOI 10.1109/CVPR.2011.5995637
   Chen HZ, 2018, INT J AUTOM COMPUT, V15, P559, DOI 10.1007/s11633-018-1139-6
   Chen S., 2020, ECCV, P520, DOI DOI 10.1007/978-3-030-58598-3_31
   Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347
   Cong RM, 2020, IEEE T CYBERNETICS, V50, P3627, DOI 10.1109/TCYB.2019.2932005
   Cong RM, 2019, IEEE T MULTIMEDIA, V21, P1660, DOI 10.1109/TMM.2018.2884481
   Cong RM, 2018, IEEE T IMAGE PROCESS, V27, P568, DOI 10.1109/TIP.2017.2763819
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Du L, 2019, IEEE T VEH TECHNOL, V68, P4249, DOI 10.1109/TVT.2019.2905598
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo DZ, 2020, IEEE T IMAGE PROCESS, V29, P782, DOI 10.1109/TIP.2019.2936111
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   He R, 2019, IEEE T PATTERN ANAL, V41, P1761, DOI 10.1109/TPAMI.2018.2842770
   He SF, 2014, LECT NOTES COMPUT SC, V8691, P110, DOI 10.1007/978-3-319-10578-9_8
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jiang JH, 2019, IEEE ACCESS, V7, P20607, DOI 10.1109/ACCESS.2019.2896128
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   LI J, IEEE T MULTIMEDIA
   Li SZ, 2007, IEEE T PATTERN ANAL, V29, P627, DOI 10.1109/TPAMI.2007.1014
   Lin YW, 2017, IEEE T CYBERNETICS, V47, P1090, DOI 10.1109/TCYB.2016.2538199
   Liu S., J ROBOT, V2019
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Matsui Sosuke, 2011, Information and Media Technologies, V6, P202
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Perone CS, 2019, NEUROIMAGE, V194, P1, DOI 10.1016/j.neuroimage.2019.03.026
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Radford A., 2015, ARXIV151106434
   Salamati N, 2009, SEVENTEENTH COLOR IMAGING CONFERENCE - COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, AND APPLICATIONS, P216
   Schaul L, 2009, IEEE IMAGE PROC, P1629, DOI 10.1109/ICIP.2009.5413700
   Shang-Hua Gao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P702, DOI 10.1007/978-3-030-58539-6_42
   Shashikar S., 2017, PROC INT C IMAGE INF, P1
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Shen XY, 2015, IEEE T PATTERN ANAL, V37, P2518, DOI 10.1109/TPAMI.2015.2417569
   Shin DK, 2018, IEEE ACCESS, V6, P61748, DOI 10.1109/ACCESS.2018.2875720
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singha A, 2019, IEEE IMAGE PROC, P2936, DOI [10.1109/ICIP.2019.8804411, 10.1109/icip.2019.8804411]
   Son C.-H., 2016, ARXIV161000175
   Song SY, 2020, AAAI CONF ARTIF INTE, V34, P12023
   Song SY, 2019, IEEE GEOSCI REMOTE S, V16, P1324, DOI 10.1109/LGRS.2019.2896411
   Song SY, 2019, NEUROCOMPUTING, V358, P166, DOI 10.1016/j.neucom.2019.05.009
   Sun FD, 2019, PATTERN RECOGN LETT, V120, P62, DOI 10.1016/j.patrec.2019.01.009
   Tang JH, 2015, IEEE T MULTIMEDIA, V17, P1899, DOI 10.1109/TMM.2015.2476660
   Tang YB, 2019, IEEE T MULTIMEDIA, V21, P2237, DOI 10.1109/TMM.2019.2900908
   Tu ZZ, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P141, DOI 10.1109/MIPR.2019.00032
   Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Vanmali AV, 2017, SADHANA-ACAD P ENG S, V42, P1063, DOI 10.1007/s12046-017-0673-1
   Vicente S, 2008, PROC CVPR IEEE, P767
   Wada K., 2017, PYTORCH FCN PYTORCH
   Wang JD, 2017, INT J COMPUT VISION, V123, P251, DOI 10.1007/s11263-016-0977-3
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang Q, 2013, COMPUT VIS IMAGE UND, V117, P1748, DOI 10.1016/j.cviu.2013.07.002
   Wang Q, 2013, PATTERN RECOGN LETT, V34, P34, DOI 10.1016/j.patrec.2012.06.002
   Wang W., 2019, ARXIV190409146
   Wang WG, 2020, IEEE T PATTERN ANAL, V42, P1913, DOI 10.1109/TPAMI.2019.2905607
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wang WG, 2018, PROC CVPR IEEE, P1711, DOI 10.1109/CVPR.2018.00184
   Wang X, 2018, PROC CVPR IEEE, P1354, DOI 10.1109/CVPR.2018.00147
   Wang XH, 2019, INT CONF ACOUST SPEE, P3807, DOI [10.1109/icassp.2019.8682692, 10.1109/ICASSP.2019.8682692]
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Yu HK, 2018, AAAI CONF ARTIF INTE, P7509
   Yu HK, 2020, IEEE T MULTIMEDIA, V22, P3051, DOI 10.1109/TMM.2020.2972165
   Yu HK, 2014, IEEE IMAGE PROC, P4412, DOI 10.1109/ICIP.2014.7025895
   Zhang F, 2015, IEEE T GEOSCI REMOTE, V53, P2175, DOI 10.1109/TGRS.2014.2357078
   Zhang J., 2020, P IEEE CVF C COMP VI, P8579, DOI DOI 10.1109/CVPR42600.2020.00861
   Zhang LH, 2020, IEEE T IMAGE PROCESS, V29, P3534, DOI 10.1109/TIP.2019.2962688
   Zhang PP, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107130
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang Q, 2020, IEEE T IMAGE PROCESS, V29, P3321, DOI 10.1109/TIP.2019.2959253
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhou SP, 2020, IEEE T IMAGE PROCESS, V29, P8417, DOI 10.1109/TIP.2020.3011554
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu X, 2019, ENTROPY-SWITZ, V21, DOI 10.3390/e21040374
NR 89
TC 11
Z9 11
U1 6
U2 44
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 128
EP 140
DI 10.1109/TMM.2020.3046868
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300010
DA 2024-07-18
ER

PT J
AU Song, X
   Chen, JJ
   Wu, ZX
   Jiang, YG
AF Song, Xue
   Chen, Jingjing
   Wu, Zuxuan
   Jiang, Yu-Gang
TI Spatial-Temporal Graphs for Cross-Modal Text2Video Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Semantics; Bit error rate; Encoding; Task analysis;
   Feature extraction; Microphones; Cross-modal retrieval; video retrieval;
   spatial-temporal graphs; cross-modal learning
ID IMAGE
AB Cross-modal text to video retrieval aims to find relevant videos given text queries, which is crucial for various real-world applications. The key to address this task is to build the correspondence between video and text such that related samples from different modalities can be aligned. As the text (sentence) contains both nouns and verbs representing objects as well as their interactions, retrieving relevant videos requires a fine-grained understanding of video contents-not only the semantic concepts (i.e., objects) but also the interactions between them. Nevertheless, current approaches mostly represent videos with aggregated frame-level features for the learning of joint space and ignore the information of object interactions, which usually results in suboptimal retrieval performance. To improve the performance of cross-modal video retrieval, this paper proposes a framework that models videos as spatial-temporal graphs where nodes correspond to visual objects and edges correspond to the relations/interactions between objects. With the spatial-temporal graphs, object interactions in frame sequences can be captured to enrich the video representations for joint space learning. Specifically, Graph Convolutional Network is introduced to learn the representations on spatial-temporal graphs, aiming to encode spatial-temporal interactions between objects; while BERT is introduced to dynamically encode the sentence according to the context for cross-modal retrieval. Extensive experiments verify the effectiveness of the proposed framework and it achieves promising performances on both MSR-VTT and LSMDC datasets.
C1 [Song, Xue; Chen, Jingjing; Wu, Zuxuan; Jiang, Yu-Gang] Fudan Univ, Shanghai 200433, Peoples R China.
C3 Fudan University
RP Jiang, YG (corresponding author), Fudan Univ, Shanghai 200433, Peoples R China.
EM 18210860029@fudan.edu.cn; chenjingjing@fudan.edu.cn; zxwu@fudan.edu.cn;
   ygj@fudan.edu.cn
RI chen, JJ/HGB-6029-2022
OI Song, Xue/0000-0003-0324-277X
FU National Natural Science Foundation of China [62032006]; Shanghai
   Pujiang Program [20PJ1401900]
FX This work was supported in part by National Natural Science Foundation
   of China under Grant 62032006 and in part by Shanghai Pujiang Program
   under Grant 20PJ1401900.
CR [Anonymous], T ASS COMPUT LINGUIS
   [Anonymous], 2016, ARXIV160908124
   [Anonymous], 2016, arXiv preprint arXiv:1610.02947
   Bain Max, 2021, ARXIV210400650
   Brendel W, 2011, IEEE I CONF COMP VIS, P778, DOI 10.1109/ICCV.2011.6126316
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen JJ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1020, DOI 10.1145/3240508.3240627
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dong JF, 2019, PROC CVPR IEEE, P9338, DOI 10.1109/CVPR.2019.00957
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Feng ZR, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1005
   Gao JY, 2020, IEEE T MULTIMEDIA, V22, P3088, DOI 10.1109/TMM.2020.2969787
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   JAIN A, 2016, PROC CVPR IEEE, P5308, DOI DOI 10.1109/CVPR.2016.573
   Kaufman D, 2017, IEEE I CONF COMP VIS, P94, DOI 10.1109/ICCV.2017.20
   King DB, 2015, ACS SYM SER, V1214, P1
   Kipf TN, 2017, INT C LEARN REPR
   Koppula Hema, 2013, P ICML, P792
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lev G, 2016, LECT NOTES COMPUT SC, V9910, P833, DOI 10.1007/978-3-319-46466-4_50
   Li CX, 2019, IEEE T MULTIMEDIA, V21, P2863, DOI 10.1109/TMM.2019.2912714
   Lin DH, 2014, PROC CVPR IEEE, P2657, DOI 10.1109/CVPR.2014.340
   Miech Antoine, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9876, DOI 10.1109/CVPR42600.2020.00990
   Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272
   Mithun NC, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/3206025.3206064
   Patrick M., 2020, ARXIV201002824, P1
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P5585, DOI 10.1109/TIP.2018.2852503
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Rohrbach A, 2017, INT J COMPUT VISION, V123, P94, DOI 10.1007/s11263-016-0987-1
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tu ZZ, 2020, IEEE T MULTIMEDIA, V22, P160, DOI 10.1109/TMM.2019.2924578
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu KYL, 2018, PR MACH LEARN RES, V80
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yu Y, 2018, LECT NOTES COMPUT SC, V11211, P487, DOI 10.1007/978-3-030-01234-2_29
   Yu Y, 2017, PROC CVPR IEEE, P3261, DOI 10.1109/CVPR.2017.347
   Yuan Y, 2017, IEEE I CONF COMP VIS, P1819, DOI 10.1109/ICCV.2017.200
   Zhang JJ, 2019, PROC CVPR IEEE, P5468, DOI 10.1109/CVPR.2019.00562
   Zhang L, 2018, IEEE T MULTIMEDIA, V20, P128, DOI 10.1109/TMM.2017.2723841
   Zhu L., 2020, P IEEE CVF C COMP VI, DOI 10.1109/CVPR42600.2020.00877
NR 49
TC 37
Z9 38
U1 2
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2914
EP 2923
DI 10.1109/TMM.2021.3090595
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000018
DA 2024-07-18
ER

PT J
AU Wang, HL
   Tang, PJ
   Li, QY
   Cheng, M
AF Wang, Hanli
   Tang, Pengjie
   Li, Qinyu
   Cheng, Meng
TI Emotion Expression With Fact Transfer for Video Description
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Semantics; Sentiment analysis; Measurement; Training;
   Databases; Video description; Convolutional neural network; emotion;
   fact transfer; long short-term memory; video description
AB Translating a video into natural language is a fundamental but challenging task in visual understanding, since there is a great gap between visual content and linguistic sentence. More attention has been paid to this research field and a number of state-of-the-art results are achieved in recent years. However, the emotions in videos are usually overlooked, leading to the generated description sentences being boring and colorless. In this work, we construct a new dataset for video description with emotion expression, which consists of two parts: a re-annotated subset of the MSVD dataset with emotion embedded and another subset annotated with long sentences and rich emotions based on a video emotion recognition dataset. A fact transfer based framework is designed, which incorporates a fact stream and an emotion stream to generate sentences with emotion expression for video description. In addition, we propose a novel approach for sentence evaluation by balancing facts and emotions. A group of experiments are conducted, and the experimental results demonstrate the effectiveness of the proposed methods, including the idea of dataset construction for video description with emotion expression, model training and testing, and the emotion evaluation metric. The project page (including the code and dataset) can be found in https://mic.tongji.edu.cn/ce/70/c9778a183920/page.htm.
C1 [Wang, Hanli; Li, Qinyu; Cheng, Meng] Tongji Univ, Dept Comp Sci & Technol, Key Lab Embedded Syst & Serv Comp, Minist Educ, Shanghai 201804, Peoples R China.
   [Wang, Hanli; Li, Qinyu; Cheng, Meng] Tongji Univ, Shanghai Inst Intelligent Sci & Technol, Shanghai 201804, Peoples R China.
   [Tang, Pengjie] Jinggangshan Univ, Coll Elect & Informat Engn, Jian, Jiangxi, Peoples R China.
   [Tang, Pengjie] Jinggangshan Univ, Jiangxi Engn Lab IoT Technol Crop Growth, Jian, Jiangxi, Peoples R China.
C3 Tongji University; Tongji University; Jinggangshan University;
   Jinggangshan University
RP Wang, HL (corresponding author), Tongji Univ, Dept Comp Sci & Technol, Key Lab Embedded Syst & Serv Comp, Minist Educ, Shanghai 201804, Peoples R China.; Wang, HL (corresponding author), Tongji Univ, Shanghai Inst Intelligent Sci & Technol, Shanghai 201804, Peoples R China.
EM hanliwang@tongji.edu.cn; tangpengjie@jgsu.edu.cn;
   qinyu.li@tongji.edu.cn; chengmeng@tongji.edu.cn
RI yuan, lin/JDW-7387-2023; Wang, Hanli/G-5111-2014
OI Wang, Hanli/0000-0002-9999-4871
FU National Natural Science Foundation of China [61976159, 62062041,
   61962003]; Shanghai Innovation Action Project of Science and Technology
   [20511100700]; Ph.D. Research Initiation Project of Jinggangshan
   University [JZB1923]; Shanghai Engineering Research Center of Industrial
   Vision Perception and Intelligent Computing [17DZ2251600]
FX This work was supported in part by National Natural Science Foundation
   of China under Grants 61976159, 62062041, and 61962003, in part by
   Shanghai Innovation Action Project of Science and Technology under Grant
   20511100700 in part by Ph.D. Research Initiation Project of Jinggangshan
   University under Grant JZB1923, and in part by Shanghai Engineering
   Research Center of Industrial Vision Perception and Intelligent
   Computing (17DZ2251600).
CR Banerjee S, 2005, ACL WORKSHOP INTRINS, P65
   Baraldi L, 2017, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2017.339
   Borth D., 2013, P 21 ACM INT C MULT, P459
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen TL, 2018, LECT NOTES COMPUT SC, V11214, P527, DOI 10.1007/978-3-030-01249-6_32
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan SJ, 2018, PROC CVPR IEEE, P7521, DOI 10.1109/CVPR.2018.00785
   Gan C, 2017, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2017.108
   Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang H., 2012, P ISCTCS 12, P1218
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang YG, 2014, AAAI CONF ARTIF INTE, P73
   Jonghwan Mun, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P6581, DOI 10.1109/CVPR.2019.00675
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Krishnadas N, 2013, PRINCIPLES, METHODOLOGIES, AND SERVICE-ORIENTED APPROACHES FOR CLOUD COMPUTING, P1, DOI 10.4018/978-1-4666-2854-0.ch001
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li YH, 2016, LECT NOTES COMPUT SC, V9911, P203, DOI 10.1007/978-3-319-46478-7_13
   Lin Chin-Yew, 2004, P 42 ANN M AOC COMP, P605, DOI [DOI 10.3115/.1218955.1219032, DOI 10.3115/1218955.1219032]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu TL, 2020, IEEE T MULTIMEDIA, V22, P1098, DOI 10.1109/TMM.2019.2936805
   Mao J, 2014, CELL DEATH DIS, V5, DOI 10.1038/cddis.2013.515
   Mathews A, 2016, AAAI CONF ARTIF INTE, P3574
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Pan YW, 2017, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2017.111
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Plutchik R, 1980, EMOTION PSYCHOEVOLUT, V93
   Pu YC, 2018, AAAI CONF ARTIF INTE, P7284
   She DY, 2020, IEEE T MULTIMEDIA, V22, P1358, DOI 10.1109/TMM.2019.2939744
   Simonyan K, 2014, ADV NEUR IN, V27
   Tang PJ, 2018, NEUROCOMPUTING, V312, P154, DOI 10.1016/j.neucom.2018.05.086
   Thomason J., 2014, COLING, P1218
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S., 2015, P ANN C N AM CHAPT A, P1494, DOI 10.3115/v1/N15-1173
   Venugopalan S., 2016, EMNLP, P1961
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wei SK, 2010, IEEE T KNOWL DATA EN, V22, P1191, DOI 10.1109/TKDE.2009.145
   Xu J, 2019, KNOWL-BASED SYST, V178, P61, DOI 10.1016/j.knosys.2019.04.018
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yadav A, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P219, DOI [10.1109/BigMM.2019.00040, 10.1109/BigMM.2019.00-22]
   Yang JF, 2018, IEEE T MULTIMEDIA, V20, P2513, DOI 10.1109/TMM.2018.2803520
   Yang M., 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, P527
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   You QZ, 2017, AAAI CONF ARTIF INTE, P231
   You QZ, 2015, AAAI CONF ARTIF INTE, P381
   You Quanzeng, 2018, ARXIV180110121
   Young P., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Zhao Z, 2017, AAAI CONF ARTIF INTE, P3532
NR 53
TC 12
Z9 12
U1 3
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 715
EP 727
DI 10.1109/TMM.2021.3058555
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100016
DA 2024-07-18
ER

PT J
AU Wei, X
   Shi, YY
   Zhou, L
AF Wei, Xin
   Shi, Yingying
   Zhou, Liang
TI Haptic Signal Reconstruction for Cross-Modal Communications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Haptic signal reconstruction; cross-modal communications; cloud-edge
   collaboration
ID MULTIMEDIA; AUDIO; INTELLIGENCE; QUALITY; SURFACE
AB The emerging multi-modal services, characterized as the integration of audio, visual, and haptic signals, will become the killer applications in 5 G and beyond 5 G era. In order to support multi-modal services, cross-modal communications come into being. However, when adopting cross-modal communications to haptic-dominant multi-modal services, there still face several technical challenges. On the one hand, haptic signals are very sensitive to interference and easy to be damaged or even missing during transmission. On the other hand, it needs to generate virtual haptic signals when real touch sensory information is hard to be gathered. To get over the dilemma, this paper proposes a haptic signal reconstruction strategy for cross-modal communications. First, a cloud-edge collaboration-based cross-modal communication architecture is constructed. Then, an audio-visual-aided haptic signal reconstruction (AVHR) approach under this architecture is designed by leveraging the potential correlation among modalities. It can be further divided into three components: feature extraction by cloud-edge transfer, shared semantic learning by multi-modal fusion, and haptic signal generation by semantic constraints. Finally, experiments on a standard audio-visual-haptic dataset and a practical cross-modal communication platform show that the proposed AVHR approach has better reconstruction performance when compared with the competing schemes.
C1 [Wei, Xin; Shi, Yingying; Zhou, Liang] Nanjing Univ Posts & Telecommun, Coll Telecommun & Informat Engn, Nanjing 210003, Peoples R China.
   [Wei, Xin; Shi, Yingying; Zhou, Liang] Nanjing Univ Posts & Telecommun, Key Lab Broadband Wireless Commun & Sensor Networ, Minist Educ, Nanjing 210003, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Nanjing University of
   Posts & Telecommunications
RP Zhou, L (corresponding author), Nanjing Univ Posts & Telecommun, Coll Telecommun & Informat Engn, Nanjing 210003, Peoples R China.; Zhou, L (corresponding author), Nanjing Univ Posts & Telecommun, Key Lab Broadband Wireless Commun & Sensor Networ, Minist Educ, Nanjing 210003, Peoples R China.
EM xwei@njupt.edu.cn; 1219012815@njupt.edu.cn; liang.zhou@ieee.org
RI shi, yingying/E-1112-2015
FU National Natural Science Foundation of China [62071254]; Key Lab of
   Broadband Wireless Communication and Sensor Network Technology, Ministry
   of Education [JZNY202111]; Jiangsu Higher Education Institutions
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62071254, in part by the open research
   fund of Key Lab of Broadband Wireless Communication and Sensor Network
   Technology, Ministry of Education under Grant JZNY202111, and in part by
   the Priority Academic Program Development of Jiangsu Higher Education
   Institutions.
CR Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Cizmeci B, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3063594
   Da-Hye Kim, 2010, Proceedings of the 5th International Conference on Computer Sciences and Convergence Information Technology (ICCIT 2010), P903, DOI 10.1109/ICCIT.2010.5711187
   Danieau F, 2014, IEEE MULTIMEDIA, V21, P11, DOI 10.1109/MMUL.2013.64
   Eid M, 2011, IEEE T INSTRUM MEAS, V60, P21, DOI 10.1109/TIM.2010.2065530
   Gao Y, 2021, IEEE NETWORK, V35, P236, DOI 10.1109/MNET.011.2000474
   Gaudina M, 2012, IEEE T INSTRUM MEAS, V61, P3103, DOI 10.1109/TIM.2012.2202071
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Gui M, 2020, IEEE HAPTICS SYM, P134, DOI [10.1109/HAPTICS45997.2020.ras.HAP20.18.6a51e1e1, 10.1109/haptics45997.2020.ras.hap20.18.6a51e1e1]
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Jin Z, 2020, IEEE T MULTIMEDIA, V22, P1055, DOI 10.1109/TMM.2019.2938340
   Lin TL, 2018, IEEE SENS J, V18, P9792, DOI 10.1109/JSEN.2018.2865916
   Liu HP, 2021, IEEE T AUTOM SCI ENG, V18, P521, DOI 10.1109/TASE.2020.2971713
   Papetti S, 2021, IEEE T HAPTICS, V14, P635, DOI 10.1109/TOH.2021.3060625
   Purri Matthew, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P1, DOI 10.1007/978-3-030-58583-9_1
   Strese M, 2017, IEEE T HAPTICS, V10, P226, DOI 10.1109/TOH.2016.2625787
   Tai YH, 2021, IEEE T IND INFORM, V17, P6519, DOI 10.1109/TII.2021.3052788
   Takahashi K, 2019, IEEE INT CONF ROBOT, P8951, DOI [10.1109/ICRA.2019.8794285, 10.1109/icra.2019.8794285]
   Tauböck G, 2021, IEEE J-STSP, V15, P104, DOI 10.1109/JSTSP.2020.3046422
   Wei X, 2021, IEEE WIREL COMMUN, V28, P182, DOI 10.1109/MWC.001.2000448
   Xu CQ, 2018, IEEE T MOBILE COMPUT, V17, P2114, DOI 10.1109/TMC.2018.2794970
   Xu X, 2016, IEEE ACCESS, V4, P425, DOI 10.1109/ACCESS.2016.2517926
   Yuan ZH, 2015, IEEE T MULTIMEDIA, V17, P104, DOI 10.1109/TMM.2014.2371240
   Zhang HT, 2019, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2019.00281
   Zhou H, 2019, IEEE I CONF COMP VIS, P283, DOI 10.1109/ICCV.2019.00037
   Zhou L, 2021, IEEE J SEL AREA COMM, V39, P426, DOI 10.1109/JSAC.2020.3021543
   Zhou L, 2020, IEEE WIREL COMMUN, V27, P112, DOI 10.1109/MWC.001.1900201
   Zhu WW, 2020, IEEE T MULTIMEDIA, V22, P1823, DOI 10.1109/TMM.2020.2969791
NR 30
TC 10
Z9 10
U1 4
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4514
EP 4525
DI 10.1109/TMM.2021.3119860
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 7B6NT
UT WOS:000899248400006
DA 2024-07-18
ER

PT J
AU Xu, Q
   Mei, YM
   Liu, JP
   Li, CL
AF Xu, Qin
   Mei, Yiming
   Liu, Jinpei
   Li, Chenglong
TI Multimodal Cross-Layer Bilinear Pooling for RGBT Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Target tracking; Aggregates; Visualization; Fuses;
   Deep learning; Task analysis; RGBT tracking; cross-layer bilinear
   pooling; channel attention; quality-aware fusion
ID NETWORK; FUSION
AB Hierarchical deep features can provide multilevel abstractions of target objects, which play an important role in target localization and classification. Determining how to effectively aggregate abstract information from different levels in RGB and thermal modalities is the key to exploiting their complementary advantages for robust RGBT tracking. However, existing RGBT tracking algorithms either focus on the semantic information of the last layer or aggregate hierarchical deep features from each modal using simple operations (e.g., summation and concatenation), which limit the capability of the multimodal tracker. To address these issues, in this paper, we propose a novel multimodal cross-layer bilinear pooling network for RGBT tracking. In our network, firstly, to boost the performance of the tracker, we use a channel attention mechanism to implement the adaptive calibration of feature channels for all convolutional layer features before realizing hierarchical feature fusion. Then, a bilinear pooling operation is performed on any two layers through the cross product, which is a second-order computation that effectively aggregates the deep semantic and shallow texture information of the target. Finally, a quality-aware fusion module is designed to aggregate the bilinear pooling features of different layer interactions between different modalities in an adaptive manner. The results of a large number of experiments on two public benchmark datasets demonstrate the effectiveness of our tracker compared with other state-of-the-art tracking methods.
C1 [Xu, Qin; Mei, Yiming; Li, Chenglong] Anhui Univ, Sch Comp Sci & Technol, Minist Educ, Key Lab Intelligent Comp & Signal Proc, Hefei 230601, Peoples R China.
   [Liu, Jinpei] Anhui Univ, Sch Business, Hefei 230601, Peoples R China.
C3 Anhui University; Anhui University
RP Li, CL (corresponding author), Anhui Univ, Sch Comp Sci & Technol, Minist Educ, Key Lab Intelligent Comp & Signal Proc, Hefei 230601, Peoples R China.
EM xuqin2013@aliyun.com; yimingmeiahu@163.com; liujinpei2012@163.com;
   lcl1314@foxmail.com
RI Li, Chenglong/AAH-4234-2019
OI Liu, Jinpei/0000-0002-0658-6247; Xu, Qin/0000-0002-9020-2006
FU National Natural Science Foundation of China [61976003, 72071001,
   61502003]; Key Research Project of Humanities and Social Sciences in
   Colleges and Universities of Anhui Province [SK2019A0013]; Science
   Foundation of Anhui Province Universities [KJ2019A0005]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61976003, 72071001, and 61502003, in
   part by the Key Research Project of Humanities and Social Sciences in
   Colleges and Universities of Anhui Province under Grant SK2019A0013, and
   by the Science Foundation of Anhui Province Universities under Grant
   KJ2019A0005.
CR [Anonymous], 2018, IEEE T NEUR NET LEAR, DOI DOI 10.1109/TNNLS.2018.2817340
   Changzheng Liu, 2011, 2011 6th International Forum on Strategic Technology (IFOST 2011), P1049, DOI 10.1109/IFOST.2011.6021200
   Dai XY, 2017, PROC CVPR IEEE, P6100, DOI 10.1109/CVPR.2017.646
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gade R, 2014, MACH VISION APPL, V25, P245, DOI 10.1007/s00138-013-0570-5
   Gadzicki K, 2020, PROCEEDINGS OF 2020 23RD INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION 2020), P292
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Gao Y, 2019, IEEE INT CONF COMP V, P91, DOI 10.1109/ICCVW.2019.00017
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Ge SM, 2021, IEEE T NEUR NET LEAR, V32, P1276, DOI 10.1109/TNNLS.2020.2984256
   Ge SM, 2020, IEEE T IMAGE PROCESS, V29, P2610, DOI 10.1109/TIP.2019.2950508
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Han K, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P2040, DOI 10.1145/3240508.3240550
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   Hazirbas C, 2017, LECT NOTES COMPUT SC, V10111, P213, DOI 10.1007/978-3-319-54181-5_14
   Hong XP, 2009, PROC CVPR IEEE, P1802, DOI 10.1109/CVPRW.2009.5206742
   Hu JF, 2018, LECT NOTES COMPUT SC, V11211, P346, DOI 10.1007/978-3-030-01234-2_21
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hwang S, 2015, PROC CVPR IEEE, P1037, DOI 10.1109/CVPR.2015.7298706
   Jung I, 2018, LECT NOTES COMPUT SC, V11208, P89, DOI 10.1007/978-3-030-01225-0_6
   Kang B, 2020, IEEE T IMAGE PROCESS, V29, P3401, DOI 10.1109/TIP.2019.2959912
   Kim HU, 2015, IEEE I CONF COMP VIS, P3011, DOI 10.1109/ICCV.2015.345
   Lan XY, 2019, IEEE T IND ELECTRON, V66, P9887, DOI 10.1109/TIE.2019.2898618
   Lan XY, 2018, AAAI CONF ARTIF INTE, P7008
   Li CL, 2019, IEEE INT CONF COMP V, P2262, DOI 10.1109/ICCVW.2019.00279
   Li CL, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106977
   Li CL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1856, DOI 10.1145/3123266.3123289
   Li CL, 2018, LECT NOTES COMPUT SC, V11217, P831, DOI 10.1007/978-3-030-01261-8_49
   Li CL, 2018, SIGNAL PROCESS-IMAGE, V68, P207, DOI 10.1016/j.image.2018.08.004
   Li CL, 2018, NEUROCOMPUTING, V281, P78, DOI 10.1016/j.neucom.2017.11.068
   Li CL, 2017, IEEE T SYST MAN CY-S, V47, P673, DOI 10.1109/TSMC.2016.2627052
   Li CL, 2016, IEEE T IMAGE PROCESS, V25, P5743, DOI 10.1109/TIP.2016.2614135
   Li YH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2230
   Liang NX, 2018, IEEE T MULTIMEDIA, V20, P2289, DOI 10.1109/TMM.2018.2803518
   Lin M., 2013, ARXIV13124400
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu HP, 2012, SCI CHINA INFORM SCI, V55, P590, DOI 10.1007/s11432-011-4536-9
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Pu S., 2018, ADV NEURAL INFORM PR, V31, P1931, DOI DOI 10.1016/J.PATCOG.2018.10.005
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Simonyan K, 2015, IEEE INT C ICLR
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun YX, 2019, IEEE ROBOT AUTOM LET, V4, P2576, DOI 10.1109/LRA.2019.2904733
   Tan M., 2019, IEEE ACCESS, V7, p117 944
   Tu ZZ, 2020, IEEE T MULTIMEDIA, V22, P160, DOI 10.1109/TMM.2019.2924578
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang JH, 2016, LECT NOTES COMPUT SC, V9909, P664, DOI 10.1007/978-3-319-46454-1_40
   Wang QR, 2019, IEEE T MULTIMEDIA, V21, P930, DOI 10.1109/TMM.2018.2869277
   Wang ZX, 2019, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.2019.00071
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Wu L, 2019, IEEE T CYBERNETICS, V49, P1791, DOI 10.1109/TCYB.2018.2813971
   Wu YF, 2011, PROCEEDINGS OF THE ASME INTERNATIONAL MANUFACTURING SCIENCE AND ENGINEERING CONFERENCE 2011, VOL 1, P1
   Xiao Y, 2022, IEEE T INTELL TRANSP, V23, P537, DOI 10.1109/TITS.2020.3013234
   Xu D, 2017, PROC CVPR IEEE, P4236, DOI 10.1109/CVPR.2017.451
   Yang R., 2019, 2019 IEEE INT C IMAG, P3975, DOI DOI 10.1109/ICIP.2019.8803528
   Yu CJ, 2018, LECT NOTES COMPUT SC, V11220, P595, DOI 10.1007/978-3-030-01270-0_35
   Zhai SL, 2019, NEUROCOMPUTING, V334, P172, DOI 10.1016/j.neucom.2019.01.022
   Zhang H, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20020393
   Zhang JM, 2020, MULTIMED TOOLS APPL, V79, P15095, DOI 10.1007/s11042-018-6562-8
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang LC, 2019, IEEE INT CONF COMP V, P2252, DOI 10.1109/ICCVW.2019.00278
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu YB, 2021, IEEE T INTELL VEHICL, V6, P121, DOI 10.1109/TIV.2020.2980735
   Zhu YB, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P465, DOI 10.1145/3343031.3350928
   Zhu Z, 2018, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2018.00064
NR 74
TC 38
Z9 42
U1 2
U2 49
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 567
EP 580
DI 10.1109/TMM.2021.3055362
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100005
DA 2024-07-18
ER

PT J
AU Yang, BH
   Ran, W
   Wang, L
   Lu, H
   Chen, YPP
AF Yang, Bohong
   Ran, Wu
   Wang, Lin
   Lu, Hong
   Chen, Yi-Ping Phoebe
TI Multi-Classes and Motion Properties for Concurrent Visual SLAM in
   Dynamic Environments
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Simultaneous localization and mapping; Semantics; Visualization;
   Trajectory; Feature extraction; Optimization; Labeling; Multi-classes;
   concurrent visual SLAM; motion properties; simultaneous localization and
   mapping; moving objects
ID RECOGNITION
AB Working in a dynamic environment is a challenging problem for visual simultaneous localization and mapping (visual SLAM). Most of the existing visual SLAM algorithms fail resulting in significant error or losing in tracking when moving objects dominate the scene. We found two reasons cause these issues: (i) Previous approaches use information from all regions in the image; (ii) Existing algorithms use just two groups and block all feature points from moveable objects. In this paper, we propose a novel Multi-classes and motion properties for Concurrent Visual SLAM (MCV-SLAM) algorithm, which defines classes into five categories and concurrently fuses prior knowledge and observation of moving objects with semantic segmentation to ensure visual SLAM works properly for dynamic environments in real time. We also propose an adaptive method to optimize camera pose by using more potential inlier feature points with continuous weights, while eliminating the impact of moving objects. Our experiments are performed on public datasets of both indoor and outdoor scenes with moving objects in dynamic environments. The experimental results demonstrate that our method outperforms previous works with greater robustness and smaller tracking errors, and our MCV-SLAM can deal with the situations (i.e., the dominance of moving objects, lack of matching points), which lead misestimating occurs in existing SLAMs.
C1 [Yang, Bohong; Ran, Wu; Wang, Lin; Lu, Hong] Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 200438, Peoples R China.
   [Chen, Yi-Ping Phoebe] La Trobe Univ, Dept Comp Sci & Informat Technol, Melbourne, Vic 3086, Australia.
C3 Fudan University; La Trobe University
RP Lu, H (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 200438, Peoples R China.
EM 15110240016@fudan.edu.cn; 19210240072@fudan.edu.cn;
   17210240215@fudan.edu.cn; honglu@fudan.edu.cn;
   phoebe.chen@latrobe.edu.au
RI wang, lin/HZK-4145-2023; Ran, Wu/JKK-3245-2023; Yang,
   Bohong/ISS-5088-2023; Chen, Yi-Ping Phoebe/B-8844-2008
OI Ran, Wu/0000-0001-8478-0750; Yang, Bohong/0000-0001-7284-688X; Chen,
   Yi-Ping Phoebe/0000-0002-4122-3767; Lu, Hong/0000-0002-4572-2854
FU National Key R&D Program of China [2019YFC1711800]; National Natural
   Science Foundation of China [62072112]
FX This work was supported in part by National Key R&D Program of China
   under Grant 2019YFC1711800 and in part by National Natural Science
   Foundation of China under Grant 62072112.
CR Bescos B, 2018, IEEE ROBOT AUTOM LET, V3, P4076, DOI 10.1109/LRA.2018.2860039
   Brasch N, 2018, IEEE INT C INT ROBOT, P393, DOI 10.1109/IROS.2018.8593828
   Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961
   Davison AJ, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1403
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dubey R., 2018, ARXIV180210217, V80, P1349
   Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577
   Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54
   Gaidon A, 2016, PROC CVPR IEEE, P4340, DOI 10.1109/CVPR.2016.470
   Gálvez-López D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Kaneko M, 2018, IEEE COMPUT SOC CONF, P371, DOI 10.1109/CVPRW.2018.00063
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Luo HC, 2019, IEEE T MULTIMEDIA, V21, P470, DOI 10.1109/TMM.2018.2859034
   Mouragnon E, 2009, IMAGE VISION COMPUT, V27, P1178, DOI 10.1016/j.imavis.2008.11.006
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Piao JC, 2019, IEEE T MULTIMEDIA, V21, P2827, DOI 10.1109/TMM.2019.2913324
   Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Rünz M, 2018, INT SYM MIX AUGMENT, P10, DOI 10.1109/ISMAR.2018.00024
   Siam M, 2018, IEEE IMAGE PROC, P1603, DOI 10.1109/ICIP.2018.8451495
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Strasdat H., 2012, Local accuracy and global consistency for efficient visual slam
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Tateno K, 2017, PROC CVPR IEEE, P6565, DOI 10.1109/CVPR.2017.695
   Torr PHS, 1998, PHILOS T R SOC A, V356, P1321, DOI 10.1098/rsta.1998.0224
   Xu BB, 2019, IEEE INT CONF ROBOT, P5231, DOI [10.1109/icra.2019.8794371, 10.1109/ICRA.2019.8794371]
   Yang SC, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P1222, DOI 10.1109/IROS.2016.7759204
   Yu C, 2018, IEEE INT C INT ROBOT, P1168, DOI 10.1109/IROS.2018.8593691
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
NR 35
TC 11
Z9 11
U1 7
U2 49
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3947
EP 3960
DI 10.1109/TMM.2021.3110667
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400020
DA 2024-07-18
ER

PT J
AU Ye, ZH
   Hu, FY
   Lyu, F
   Li, LY
   Huang, KZ
AF Ye, Zihan
   Hu, Fuyuan
   Lyu, Fan
   Li, Linyan
   Huang, Kaizhu
TI Disentangling Semantic-to-Visual Confusion for Zero-Shot Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Semantics; Training; Manganese; Extraterrestrial
   measurements; Generative adversarial networks; Search problems;
   Zero-shot learning; generative adversarial network; representation
   learning; deep learning
ID CLASSIFICATION
AB Using generative models to synthesize visual features from semantic distribution is one of the most popular solutions to ZSL image classification in recent years. The triplet loss (TL) is popularly used to generate realistic visual distributions from semantics by automatically searching discriminative representations. However, the traditional TL cannot search reliable unseen disentangled representations due to the unavailability of unseen classes in ZSL. To alleviate this drawback, we propose in this work a multi-modal triplet loss (MMTL) which utilizes multi-modal information to search a disentangled representation space. As such, all classes can interplay which can benefit learning disentangled class representations in the searched space. Furthermore, we develop a novel model called Disentangling Class Representation Generative Adversarial Network (DCR-GAN) focusing on exploiting the disentangled representations in training, feature synthesis, and final recognition stages. Benefiting from the disentangled representations, DCR-GAN could fit a more realistic distribution over both seen and unseen features. Extensive experiments show that our proposed model can lead to superior performance to the state-of-the-arts on four benchmark datasets.
C1 [Ye, Zihan; Hu, Fuyuan] Suzhou Univ Sci & Technol, Suzhou 215009, Peoples R China.
   [Lyu, Fan] Tianjin Univ, Tianjin 300000, Peoples R China.
   [Li, Linyan] Suzhou Inst Trade & Commerce, Suzhou 215009, Jiangsu, Peoples R China.
   [Huang, Kaizhu] Xian Jiaotong Liverpool Univ, Dept Elect & Elect Engn, Suzhou 215123, Peoples R China.
C3 Suzhou University of Science & Technology; Tianjin University; Xi'an
   Jiaotong-Liverpool University
RP Hu, FY (corresponding author), Suzhou Univ Sci & Technol, Suzhou 215009, Peoples R China.; Huang, KZ (corresponding author), Xian Jiaotong Liverpool Univ, Dept Elect & Elect Engn, Suzhou 215123, Peoples R China.
EM zihhye@outlook.com; fuyuanhu@mail.usts.edu.cn; fanlyu@tju.edu.cn;
   610468970@qq.com; Kaizhu.Huang@xjtlu.edu.cn
RI Ye, Zihan/AAS-3792-2021; Lyu, Fan/AAC-3865-2020; Huang,
   Kaizhu/O-4721-2014
OI Ye, Zihan/0000-0001-6462-6799; Lyu, Fan/0000-0002-0878-5485; Huang,
   Kaizhu/0000-0002-3034-9639; Hu, Fuyuan/0000-0002-6818-2221
FU National Natural Science Foundation of China [61876121, 62002254,
   61801323, 61876155]; Jiangsu Provincial Key Research and Development
   Program [BE2017663, BE2020006-4B]; Natural Science Foundation of the
   Jiangsu Higher Education Institutions of China [19KJB520054]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61876121, 62002254, 61801323, and
   61876155, in part by the Jiangsu Provincial Key Research and Development
   Program under Grants BE2017663, and BE2020006-4B, and in part by the
   Natural Science Foundation of the Jiangsu Higher Education Institutions
   of China under Grant 19KJB520054.
CR Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986
   Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Annadani Y, 2018, PROC CVPR IEEE, P7603, DOI 10.1109/CVPR.2018.00793
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575
   Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4
   Chen Z., 2021, ARXIV210103292
   Ding ZM, 2019, IEEE T PATTERN ANAL, V41, P2861, DOI 10.1109/TPAMI.2018.2867870
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Felix R, 2018, LECT NOTES COMPUT SC, V11210, P21, DOI 10.1007/978-3-030-01231-1_2
   Filos A, 2020, PR MACH LEARN RES, V119
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang H, 2019, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.2019.00089
   Jiang HJ, 2018, LECT NOTES COMPUT SC, V11214, P121, DOI 10.1007/978-3-030-01249-6_8
   Karessli N, 2017, PROC CVPR IEEE, P6412, DOI 10.1109/CVPR.2017.679
   Keshari Rohit, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13297, DOI 10.1109/CVPR42600.2020.01331
   Kingma D. P., 2013, ARXIV13126114
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Lee H, 2018, IEEE INT CONF COMM
   Li JJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1587, DOI 10.1145/3343031.3350901
   Li Y, 2018, PROC CVPR IEEE, P7463, DOI 10.1109/CVPR.2018.00779
   Liu Y., 2021, ARXIV210303433
   Long Y, 2017, PROC CVPR IEEE, P6165, DOI 10.1109/CVPR.2017.653
   Lyu F, 2021, AAAI CONF ARTIF INTE, V35, P8819
   Lyu F, 2019, IEEE T MULTIMEDIA, V21, P1971, DOI 10.1109/TMM.2019.2894964
   Maaten L. v. d., 2018, J MACH LEARN RES, V9, P2579
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Min S., 2020, P IEEE C COMP VIS PA, p12 661
   Ni J, 2019, ADV NEUR IN, V32
   Patterson G, 2014, INT J COMPUT VISION, V108, P59, DOI 10.1007/s11263-013-0695-z
   Qi GJ, 2017, IEEE T PATTERN ANAL, V39, P1360, DOI 10.1109/TPAMI.2016.2587643
   Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13
   Romera-Paredes B, 2015, PR MACH LEARN RES, V37, P2152
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sean M., 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), P4171
   Shigeto Y, 2015, LECT NOTES ARTIF INT, V9284, P135, DOI 10.1007/978-3-319-23528-8_9
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Song J, 2018, LECT NOTES COMPUT SC, V11213, P474, DOI 10.1007/978-3-030-01240-3_29
   Vyas Maunil R., 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P70, DOI 10.1007/978-3-030-58577-8_5
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang YQ, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3386252
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2017, PROC CVPR IEEE, P3077, DOI 10.1109/CVPR.2017.328
   Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15
   Yang Y, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1286, DOI 10.1145/2964284.2964319
   Ye ZH, 2019, COGN COMPUT, V11, P869, DOI 10.1007/s12559-019-09633-3
   Ye ZH, 2019, IEEE INT CON MULTI, P85, DOI 10.1109/ICME.2019.00023
   Yu LJ, 2020, IEEE COMPUT SOC CONF, P2534, DOI 10.1109/CVPRW50498.2020.00305
   Yuan Y, 2020, IEEE COMPUT SOC CONF, P1454, DOI 10.1109/CVPRW50498.2020.00185
   Zhang Fei, 2019, PR MACH LEARN RES, P7434
   Zhang XP, 2017, IEEE T MULTIMEDIA, V19, P2736, DOI 10.1109/TMM.2017.2710803
   Zhang ZM, 2016, PROC CVPR IEEE, P6034, DOI 10.1109/CVPR.2016.649
   Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474
   Zhu YH, 2021, IEEE T MULTIMEDIA, V23, P1200, DOI 10.1109/TMM.2020.2993952
NR 59
TC 7
Z9 7
U1 1
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2828
EP 2840
DI 10.1109/TMM.2021.3089017
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, X
   Wu, ZX
   Jiang, YG
AF Zhang, Xing
   Wu, Zuxuan
   Jiang, Yu-Gang
TI SAM: Modeling Scene, Object and Action With Semantics Attention Modules
   for Video Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video recognition; scene; object; feature fusion; semantics attention
ID LATE FUSION
AB Video recognition aims at understanding semantic contents that normally involve the interactions of humans and related objects under certain scenes. A common practice to improve recognition accuracy is to combine object, scene and action features for classification directly, assuming that they are explicitly complementary. In this paper, we break down the fusion of three features into two pairwise feature relation modeling processes, which mitigates the difficulty of correlation learning in high dimensional features. Towards this goal, we introduce a Semantics Attention Module that captures the relations of a pair of features by refining the relatively "weak" feature with the guidance from the "strong" feature using attention mechanisms. The refined representation is further combined with the "strong" feature using a residual design for downstream tasks. Two SAMs are applied in a Semantics Attention Network (SAN) for improving video recognition. Extensive experiments are conducted on two large-scale video benchmarks, FCVID and ActivityNet v1.3-the proposed approach achieves better results while requiring much less computational effort than alternative methods.
C1 [Zhang, Xing] Fudan Univ, Acad Engn & Technol, Shanghai 200433, Peoples R China.
   [Wu, Zuxuan; Jiang, Yu-Gang] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
C3 Fudan University; Fudan University
RP Jiang, YG (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
EM zhangxing18@fudan.edu.cn; zxwu@cs.umd.edu; ygj@fudan.edu.cn
OI Zhang, Xing/0000-0002-1721-035X
FU National Key R&D Program of China [2018YFB1004300]
FX This work was supported by National Key R&D Program of China under Grant
   2018YFB1004300.
CR Abu-El-Haija Sami, 2016, arXiv
   [Anonymous], 2017, ADV NEURAL INFORM PR
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Du WB, 2018, IEEE T IMAGE PROCESS, V27, P1347, DOI 10.1109/TIP.2017.2778563
   Nguyen DK, 2018, PROC CVPR IEEE, P6087, DOI 10.1109/CVPR.2018.00637
   Dwibedi Debidatta., 2018, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, P1111
   Fan HH, 2017, IEEE I CONF COMP VIS, P736, DOI 10.1109/ICCV.2017.86
   Feichtenhofer C, 2020, PROC CVPR IEEE, P200, DOI 10.1109/CVPR42600.2020.00028
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Gao RH, 2020, PROC CVPR IEEE, P10454, DOI 10.1109/CVPR42600.2020.01047
   Girdhar R, 2017, PROC CVPR IEEE, P3165, DOI 10.1109/CVPR.2017.337
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hori Chiori, 2018, CVPR WORKSH
   Hussein N, 2019, PROC CVPR IEEE, P254, DOI 10.1109/CVPR.2019.00034
   Jain M, 2015, PROC CVPR IEEE, P46, DOI 10.1109/CVPR.2015.7298599
   Jiang YG, 2018, IEEE T PATTERN ANAL, V40, P352, DOI 10.1109/TPAMI.2017.2670560
   Jiang YG, 2011, IEEE T CIRC SYST VID, V21, P674, DOI 10.1109/TCSVT.2011.2129870
   Kay W., 2017, CORR ABS170506950
   Lee J, 2019, LECT NOTES COMPUT SC, V11132, P193, DOI 10.1007/978-3-030-11018-5_18
   Limin Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P30, DOI 10.1109/CVPRW.2015.7301333
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu D, 2013, PROC CVPR IEEE, P803, DOI 10.1109/CVPR.2013.109
   Long X, 2018, PROC CVPR IEEE, P7834, DOI 10.1109/CVPR.2018.00817
   Meng LL, 2019, IEEE INT CONF COMP V, P1513, DOI 10.1109/ICCVW.2019.00189
   Mettes P, 2017, IEEE I CONF COMP VIS, P4453, DOI 10.1109/ICCV.2017.476
   Miech A., 2017, ARXIV PREPRINT ARXIV
   Monfort M, 2020, IEEE T PATTERN ANAL, V42, P502, DOI 10.1109/TPAMI.2019.2901464
   Munro J, 2020, PROC CVPR IEEE, P119, DOI 10.1109/CVPR42600.2020.00020
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Ramanathan V, 2016, PROC CVPR IEEE, P3043, DOI 10.1109/CVPR.2016.332
   Ray J, 2018, LECT NOTES COMPUT SC, V11218, P660, DOI 10.1007/978-3-030-01264-9_39
   Shao D, 2020, PROC CVPR IEEE, P2613, DOI 10.1109/CVPR42600.2020.00269
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Simonyan K., 2014, 14091556 ARXIV
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh G., 2016, ActivityNet Large Scale Activity Recognition Challenge
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399, DOI 10.1145/1101149.1101236
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Sudhakaran S, 2019, PROC CVPR IEEE, P9946, DOI 10.1109/CVPR.2019.01019
   Sun N, 2019, IEEE T CIRC SYST VID, V29, P1715, DOI 10.1109/TCSVT.2018.2848543
   Tan JH, 2019, IEEE T MULTIMEDIA, V21, P2686, DOI 10.1109/TMM.2019.2904878
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang G, 2018, IEEE T MULTIMEDIA, V20, P2921, DOI 10.1109/TMM.2018.2829163
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang L., 2016, P ECCV
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu WH, 2019, IEEE I CONF COMP VIS, P6231, DOI 10.1109/ICCV.2019.00632
   Wu ZX, 2016, PROC CVPR IEEE, P3112, DOI 10.1109/CVPR.2016.339
   Wu ZX, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P461, DOI 10.1145/2733373.2806222
   Xie CY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1639
   Xie D, 2019, AAAI CONF ARTIF INTE, P9030
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Yao T, 2015, IEEE I CONF COMP VIS, P28, DOI 10.1109/ICCV.2015.12
   Ye GN, 2012, PROC CVPR IEEE, P3021, DOI 10.1109/CVPR.2012.6248032
   Zhang Q, 2018, AAAI CONF ARTIF INTE, P5674
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zheng YD, 2020, IEEE T IMAGE PROCESS, V29, P7970, DOI 10.1109/TIP.2020.3007826
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou ZH, 2019, SCI CHINA INFORM SCI, V62, DOI 10.1007/s11432-018-9801-4
   Zhu C, 2018, LECT NOTES COMPUT SC, V11209, P139, DOI 10.1007/978-3-030-01228-1_9
   Zhu LC, 2020, PROC CVPR IEEE, P4343, DOI 10.1109/CVPR42600.2020.00440
NR 69
TC 4
Z9 4
U1 1
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 313
EP 322
DI 10.1109/TMM.2021.3050058
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300024
DA 2024-07-18
ER

PT J
AU Zhou, WJ
   Zhu, Y
   Lei, JS
   Wan, J
   Yu, L
AF Zhou, Wujie
   Zhu, Yun
   Lei, Jingsheng
   Wan, Jian
   Yu, Lu
TI CCAFNet: Crossflow and Cross-Scale Adaptive Fusion Network for Detecting
   Salient Objects in RGB-D Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Semantics; Adaptation models; Data mining; Streaming
   media; Predictive models; Logic gates; Channel fusion module; crossflow
   adaptive fusion; cross-scale; RGB-D information; salient object
   detection; spatial fusion module
AB Owing to the widespread adoption of depth sensors, salient object detection (SOD) supported by depth maps for reliable complementary information is being increasingly investigated. Existing SOD models mainly exploit the relation between an RGB image and its corresponding depth information across three fusion domains: input RGB-D images, extracted feature maps, and output salient object. However, these models do not leverage the crossflows between high- and low-level information well. Moreover, the decoder in these models uses conventional convolution that involves several calculations. To further improve RGB-D SOD, we propose a crossflow and cross-scale adaptive fusion network (CCAFNet) to detect salient objects in RGB-D images. First, a channel fusion module allows for effective fusing depth and high-level RGB features. This module extracts accurate semantic information features from high-level RGB features. Meanwhile, a spatial fusion module combines low-level RGB and depth features with accurate boundaries and subsequently extracts detailed spatial information from low-level depth features. Finally, a purification loss is proposed to precisely learn the boundaries of salient objects and obtain additional details of the objects. The results of comprehensive experiments on seven common RGB-D SOD datasets indicate that the performance of the proposed CCAFNet is comparable to those of state-of-the-art RGB-D SOD models.
C1 [Zhou, Wujie; Zhu, Yun; Lei, Jingsheng; Wan, Jian] Zhejiang Univ Sci & Technol, Sch Informat & Elect Engn, Hangzhou 310023, Peoples R China.
   [Yu, Lu] Zhejiang Univ, Inst Informat & Commun Engn, Hangzhou 310027, Peoples R China.
C3 Zhejiang University of Science & Technology; Zhejiang University
RP Zhou, WJ (corresponding author), Zhejiang Univ Sci & Technol, Sch Informat & Elect Engn, Hangzhou 310023, Peoples R China.
EM wujiezhou@163.com; zhuyun@zust.edu.cn; leijingsheng@zust.edu.cn;
   wanjian@zust.edu.cn; yul@zju.edu.cn
OI Zhu, Yun/0009-0000-2121-548X; zhou, wujie/0000-0002-3055-2493
FU National Natural Science Foundation of China [61502429, 61972357,
   61972358]; Zhejiang Provincial Natural Science Foundation of China
   [LY18F020012]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61502429, 61972357, and 61972358, and
   in part by the Zhejiang Provincial Natural Science Foundation of China
   under Grants LY18F020012. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. D.
   Crandall.
CR Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Chen H, 2019, ARXIV190909309
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2020, IEEE T CYBERNETICS, V50, P4808, DOI 10.1109/TCYB.2019.2934986
   Chen H, 2018, IEEE INT C INT ROBOT, P6821, DOI 10.1109/IROS.2018.8594373
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Chen Q, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107740
   Chen ZY, 2021, IEEE T IMAGE PROCESS, V30, P7012, DOI 10.1109/TIP.2020.3028289
   Cheng Y, 2014, MOBIHOC'14: PROCEEDINGS OF THE 15TH ACM INTERNATIONAL SYMPOSIUM ON MOBILE AD HOC NETWORKING AND COMPUTING, P23, DOI 10.1145/2632951.2632978
   Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347
   Cong RM, 2020, IEEE T CYBERNETICS, V50, P3627, DOI 10.1109/TCYB.2019.2932005
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fan DP, 2018, LECT NOTES COMPUT SC, V11219, P196, DOI 10.1007/978-3-030-01267-0_12
   Fan J.Z. D.-P., 2020, ARXIV200903075
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Guangrui Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P440, DOI 10.1007/978-3-030-58568-6_26
   Guo J., 2016, P ANN REL MAINT S, P1, DOI DOI 10.1109/RAMS.2016.7448068
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   Howard A. G., 2017, PREPRINT
   Hu XW, 2021, IEEE T CIRC SYST VID, V31, P1079, DOI 10.1109/TCSVT.2020.2995220
   Huang NAC, 2021, IEEE T MULTIMEDIA, V23, P2428, DOI 10.1109/TMM.2020.3011327
   Huang PS, 2018, INT CONF DIGIT SIG
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P1343, DOI 10.1109/TMM.2020.2997184
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   King DB, 2015, ACS SYM SER, V1214, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li CH, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376209
   Li CY, 2021, IEEE T CYBERNETICS, V51, P88, DOI 10.1109/TCYB.2020.2969255
   Li GY, 2020, IEEE T IMAGE PROCESS, V29, P4873, DOI 10.1109/TIP.2020.2976689
   Li JX, 2021, IEEE T MULTIMEDIA, V23, P1397, DOI 10.1109/TMM.2020.2997192
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu D, 2021, IEEE T MULTIMEDIA, V23, P967, DOI 10.1109/TMM.2020.2991523
   Liu N., 2020, ARXIV201005537
   Liu ZY, 2020, NEUROCOMPUTING, V387, P210, DOI 10.1016/j.neucom.2020.01.045
   Liu ZY, 2019, NEUROCOMPUTING, V363, P46, DOI 10.1016/j.neucom.2019.07.012
   Ma GX, 2020, IEEE T MULTIMEDIA, V22, P324, DOI 10.1109/TMM.2019.2929943
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Shigematsu R, 2017, IEEE INT CONF COMP V, P2749, DOI 10.1109/ICCVW.2017.323
   Simonya K., VERY DEEP CONVOLUTIO
   Wang NN, 2019, IEEE ACCESS, V7, P55277, DOI 10.1109/ACCESS.2019.2913107
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Wu JW, 2021, SIGNAL PROCESS, V178, DOI 10.1016/j.sigpro.2020.107766
   Yongri Piao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9057, DOI 10.1109/CVPR42600.2020.00908
   Zhang J., 2020, P IEEE CVF C COMP VI, P8579, DOI DOI 10.1109/CVPR42600.2020.00861
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhang PP, 2019, IEEE T IMAGE PROCESS, V28, P3048, DOI 10.1109/TIP.2019.2893535
   Zhang YF, 2021, NEUROCOMPUTING, V423, P463, DOI 10.1016/j.neucom.2020.10.079
   Zhang Z, 2021, IEEE T IMAGE PROCESS, V30, P1949, DOI 10.1109/TIP.2021.3049959
   Zhao J, 2020, IEEE T FUZZY SYST, V28, P2287, DOI 10.1109/TFUZZ.2019.2930492
   Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z
   Zhou WJ, 2023, IEEE T COGN DEV SYST, V15, P476, DOI 10.1109/TCDS.2021.3051010
   Zhou WJ, 2022, IEEE T CIRC SYST VID, V32, P1224, DOI 10.1109/TCSVT.2021.3077058
   Zhou WJ, 2021, IEEE T MULTIMEDIA, V23, P3388, DOI 10.1109/TMM.2020.3025166
   Zhou WJ, 2021, IEEE INTELL SYST, V36, P73, DOI 10.1109/MIS.2020.2999462
   Zhou WJ, 2020, IEEE T COMPUT IMAG, V6, P883, DOI 10.1109/TCI.2020.2993640
   Zhou WJ, 2021, IEEE T SYST MAN CY-S, V51, P3641, DOI 10.1109/TSMC.2019.2957386
   Zhou Y, 2020, IEEE T CIRC SYST VID, V30, P1569, DOI 10.1109/TCSVT.2019.2904463
   Zhu CB, 2019, IEEE INT CON MULTI, P199, DOI 10.1109/ICME.2019.00042
   Zhu CB, 2017, IEEE INT CONF COMP V, P1509, DOI 10.1109/ICCVW.2017.178
   Zhu CB, 2017, IEEE INT CONF COMP V, P3008, DOI 10.1109/ICCVW.2017.355
   Zhu SP, 2020, IEEE T CIRC SYST VID, V30, P1946, DOI 10.1109/TCSVT.2019.2911396
NR 69
TC 89
Z9 92
U1 9
U2 85
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2192
EP 2204
DI 10.1109/TMM.2021.3077767
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200032
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Bidgoli, NM
   Maugey, T
   Roumy, A
AF Bidgoli, Navid Mahmoudian
   Maugey, Thomas
   Roumy, Aline
TI Fine Granularity Access in Interactive Compression of 360-Degree Images
   Based on Rate-adaptive Channel Codes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Decoding; Two dimensional displays; Encoding; Correlation;
   Servers; Data mining; 360-degree content; incremental codes; interactive
   compression; intra prediction; user-dependent transmission
ID FRAME; CONTEXT
AB In this paper, we propose a new interactive compression scheme for omnidirectional images. This requires two characteristics: efficient compression of data, to lower the storage cost, and random access ability to extract part of the compressed stream requested by the user (for reducing the transmission rate). For efficient compression, data needs to be predicted by a series of references that have been pre-defined and compressed. This contrasts with the spirit of random accessibility. We propose a solution for this problem based on incremental codes implemented by rate-adaptive channel codes. This scheme encodes the image while adapting to any user request and leads to an efficient coding that is flexible in extracting data depending on the available information at the decoder. Therefore, only the information that is needed to be displayed at the user's side is transmitted during the user's request, as if the request was already known at the encoder. The experimental results demonstrate that our coder obtains a better transmission rate than the state-of-the-art tile-based methods at a small cost in storage. Moreover, the transmission rate grows gradually with the size of the request and avoids a staircase effect, which shows the perfect suitability of our coder for interactive transmission.
C1 [Bidgoli, Navid Mahmoudian; Maugey, Thomas; Roumy, Aline] Univ Rennes, INRIA, F-35042 Rennes, France.
C3 Universite de Rennes; Inria
RP Bidgoli, NM (corresponding author), Univ Rennes, INRIA, F-35042 Rennes, France.
EM navid.mahmoudian-bidgoli@inria.fr; thomas.maugey@inria.fr;
   aline.roumy@inria.fr
OI ROUMY, Aline/0000-0002-6352-8166; Mahmoudian Bidgoli,
   Navid/0000-0003-2808-3859
FU Cominlabs excellence laboratory; French National Research Agency
   [ANR-10-LABX-07-01]; Brittany Region [ARED 9582 InterCOR]
FX This work was supported in part by the Cominlabs excellence laboratory
   with funding from the French National Research Agency
   (ANR-10-LABX-07-01) and in part by the Brittany Region under Grant ARED
   9582 InterCOR. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr Sen-Ching Samson
   Cheung.
CR AbuBaker A, 2007, ICSPC: 2007 IEEE INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND COMMUNICATIONS, VOLS 1-3, PROCEEDINGS, P1283
   [Anonymous], 2014, High Efficiency Video Coding. Coding Tools and Specification
   Bidgoli N. M., 2019, P IEEE INT C IM PROC
   Birkbeck N, 2019, IEEE T CIRCUITS SYST, P1
   Chen J., 2019, Document JVET-N1002
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P744, DOI 10.1109/TIP.2010.2070074
   Cheung N.-M., 2008, P IS T SPIE VIS COMM
   Cheung NM, 2009, PCS: 2009 PICTURE CODING SYMPOSIUM, P269
   Corbillon X, 2017, IEEE ICC, DOI 10.1109/ICC.2017.7996611
   Cover T. M., 1991, ELEMENTS INFORM THEO
   Dai W, 2013, IEEE IMAGE PROC, P1787, DOI 10.1109/ICIP.2013.6738368
   Dupraz E, 2019, PHYS COMMUN-AMST, V37, DOI 10.1016/j.phycom.2019.100845
   Fu CW, 2009, IEEE T MULTIMEDIA, V11, P634, DOI 10.1109/TMM.2009.2017626
   Hannuksela MM, 2019, IEEE DATA COMPR CONF, P418, DOI 10.1109/DCC.2019.00050
   Hosseini M, 2017, P IEEE VIRT REAL ANN, P423, DOI 10.1109/VR.2017.7892357
   Hosseini M, 2016, IEEE INT SYM MULTIM, P107, DOI [10.1109/ISM.2016.45, 10.1109/ISM.2016.0028]
   Karczewicz M, 2003, IEEE T CIRC SYST VID, V13, P637, DOI 10.1109/TCSVT.2003.814969
   Kuzyakov E., 2016, Next-generation video encoding techniques for 360 video and VR-Engineering at Meta
   Le Feuvre J, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P329, DOI 10.1145/2910017.2910641
   Marpe D, 2003, IEEE T CIRC SYST VID, V13, P620, DOI 10.1109/TCSVT.2003.815173
   Maugey T, 2020, IEEE T SIGNAL INF PR, V6, P251, DOI 10.1109/TSIPN.2020.2981263
   Motz B, 2016, IEEE IMAGE PROC, P1369, DOI 10.1109/ICIP.2016.7532582
   Ng KT, 2005, IEEE T CIRC SYST VID, V15, P82, DOI 10.1109/TCSVT.2004.839989
   Ozcinar C, 2019, IEEE J EM SEL TOP C, V9, P217, DOI 10.1109/JETCAS.2019.2895096
   Qian F., 2016, Proceedings of the 5th Workshop on All Things Cellular: Operations, Applications and Challenges, P1, DOI DOI 10.1145/2980055.2980056
   Rai Y, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P205, DOI 10.1145/3083187.3083218
   Rossi S., 2017, 2017 IEEE 19 INT WOR, P1, DOI DOI 10.1109/MMSP.2017.8122230
   Roumy A, 2015, IEEE IMAGE PROC, P1870, DOI 10.1109/ICIP.2015.7351125
   Saurty K, 2015, 2015 FIFTH INTERNATIONAL CONFERENCE ON DIGITAL INFORMATION AND COMMUNICATION TECHNOLOGY AND ITS APPLICATIONS (DICTAP), P143, DOI 10.1109/DICTAP.2015.7113187
   Sayood Sayood K. K., INTRO DATA COMPRESSI
   Snyder JP., 1993, FLATTENING EARTH 200
   Tariq J, 2015, IEEE SYS MAN CYBERN, P1776, DOI 10.1109/SMC.2015.311
   Toto-Zarasoa V, 2012, IEEE T CIRC SYST VID, V22, P174, DOI 10.1109/TCSVT.2011.2159429
   Westerlaken Ronald P., 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, pII
   Ye FH, 2020, IEEE T NEUR NET LEAR, V31, P2903, DOI 10.1109/TNNLS.2019.2933850
   Yen S.-C., 2019, P 24 ACM WORKSH PACK, P7
   Yu M, 2015, 2015 IEEE International Symposium on Mixed and Augmented Reality, P31, DOI 10.1109/ISMAR.2015.12
   Zare A., 2016, P 24 ACM INT C MULT, P601, DOI DOI 10.1145/2964284.2967292
   Zhu Y., 2020, IEEE T MULTIMEDIA, V22, P1
   Zhu YC, 2018, SIGNAL PROCESS-IMAGE, V69, P15, DOI 10.1016/j.image.2018.05.010
NR 40
TC 1
Z9 1
U1 2
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2868
EP 2882
DI 10.1109/TMM.2020.3017890
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600026
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Cao, YJ
   Lin, C
   Li, YJ
AF Cao, Yi-Jun
   Lin, Chuan
   Li, Yong-Jie
TI Learning Crisp Boundaries Using Deep Refinement Network and Adaptive
   Weighting Loss
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contour detection; decode network; deep refinement network; multi-scale
   integration
AB Significant progress has been made in boundary detection with the help of convolutional neural networks. Recent boundary detection models not only focus on real object boundary detection but also "crisp" boundaries (precisely localized along the object's contour). There are two methods to evaluate crisp boundary performance. One uses more strict tolerance to measure the distance between the ground truth and the detected contour. The other focuses on evaluating the contour map without any postprocessing. In this study, we analyze both methods and conclude that both methods are two aspects of crisp contour evaluation. Accordingly, we propose a novel network named deep refinement network (DRNet) that stacks multiple refinement modules to achieve richer feature representation and a novel loss function, which combines cross-entropy and dice loss through effective adaptive fusion. Experimental results demonstrated that we achieve state-of-the-art performance for several available datasets.
C1 [Cao, Yi-Jun; Lin, Chuan] Guangxi Univ Sci & Technol, Coll Elect & Informat Engn, Liuzhou 545006, Peoples R China.
   [Li, Yong-Jie] Univ Elect Sci & Technol China, Sch Life Sci & Technol, Chengdu 610054, Peoples R China.
C3 Guangxi University of Science & Technology; University of Electronic
   Science & Technology of China
RP Lin, C (corresponding author), Guangxi Univ Sci & Technol, Coll Elect & Informat Engn, Liuzhou 545006, Peoples R China.
EM yijuncaoo@gmail.com; chuanlin@gxust.edu.cn; liyj@uestc.edu.cn
RI lin, chuan/HIK-1290-2022; lin, chuan/JBJ-7047-2023; luo,
   chuan/IVH-5370-2023; lin, chuan/HHD-2571-2022
OI lin, chuan/0000-0003-1779-1753; Li, Yongjie/0000-0002-7395-3131; Cao,
   Yi-Jun/0000-0002-1934-2698
FU National Natural Science Foundation of China [61866002]; Guangxi Natural
   Science Foundation [2018GXNSFAA138122, 2015GXNSFAA139293]; Innovation
   Project of Guangxi Graduate Education [YCSW2018203]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61866002, in part by the Guangxi Natural
   Science Foundation under Grants 2018GXNSFAA138122 and 2015GXNSFAA139293,
   and in part by the Innovation Project of Guangxi Graduate Education
   under Grant YCSW2018203.
CR [Anonymous], 2012, LECT NOTES COMPUT SC, DOI [10.1007/978-3-642-33715-4_54, DOI 10.1007/978-3-642-33715-4_54]
   Arbeláez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bertasius G, 2015, PROC CVPR IEEE, P4380, DOI 10.1109/CVPR.2015.7299067
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng RX, 2018, LECT NOTES COMPUT SC, V11210, P570, DOI 10.1007/978-3-030-01231-1_35
   Dollar P., 2006, 2006 IEEE COMP SOC C, V2, P1964, DOI DOI 10.1109/CVPR.2006.298
   Dollár P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715
   Ganin Y, 2015, LECT NOTES COMPUT SC, V9004, P536, DOI 10.1007/978-3-319-16808-1_36
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79
   He JZ, 2019, PROC CVPR IEEE, P3823, DOI 10.1109/CVPR.2019.00395
   Isola P, 2014, LECT NOTES COMPUT SC, V8691, P799, DOI 10.1007/978-3-319-10578-9_52
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Khoreva A, 2016, PROC CVPR IEEE, P183, DOI 10.1109/CVPR.2016.27
   Kokkinos, 2015, P INT C REPR LEARN
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li XX, 2017, PROC CVPR IEEE, P6459, DOI 10.1109/CVPR.2017.684
   Li Y, 2016, PROC CVPR IEEE, P1619, DOI 10.1109/CVPR.2016.179
   Liang LM, 2016, IEEE T MULTIMEDIA, V18, P2282, DOI 10.1109/TMM.2016.2614219
   Lim JJ, 2013, PROC CVPR IEEE, P3158, DOI 10.1109/CVPR.2013.406
   Lin C, 2018, IET IMAGE PROCESS, V12, P993, DOI 10.1049/iet-ipr.2017.0679
   Lin C, 2016, J ELECTRON IMAGING, V25, DOI 10.1117/1.JEI.25.4.043018
   Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849
   Maninis KK, 2016, LECT NOTES COMPUT SC, V9905, P580, DOI 10.1007/978-3-319-46448-0_35
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Mély DA, 2016, VISION RES, V120, P93, DOI 10.1016/j.visres.2015.11.007
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Paszke Adam, 2017, NIPS 2017 WORKSH AUT
   Pont-Tuset J, 2016, IEEE T PATTERN ANAL, V38, DOI [10.1109/TPAMI.2015.2481406, 10.1109/TPAMI.2016.2537320]
   Prewitt J. M., 1970, Picture processing and psychopictorics, V10, P15
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   REN X, 2008, P 10 EUR C COMP VI 3, P533
   Ren X, 2012, ADV NEURAL INFORM PR, P584, DOI DOI 10.1634/THEONCOLOGIST.8-3-252
   Shen W, 2015, PROC CVPR IEEE, P3982, DOI 10.1109/CVPR.2015.7299024
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wang S, 2015, IEEE T MULTIMEDIA, V17, P1045, DOI 10.1109/TMM.2015.2431492
   Wang YP, 2017, PROC CVPR IEEE, P1724, DOI 10.1109/CVPR.2017.187
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xu D., 2017, NIPS, P3961, DOI [DOI 10.48550/ARXIV.1801.00524, 10.48550/arXiv.1801.00524]
   Yang JM, 2016, PROC CVPR IEEE, P193, DOI 10.1109/CVPR.2016.28
   Yang KF, 2014, IEEE T IMAGE PROCESS, V23, P5020, DOI 10.1109/TIP.2014.2361210
   Zhang YT, 2016, IEEE T MULTIMEDIA, V18, P1604, DOI 10.1109/TMM.2016.2568138
   Zhang ZZ, 2016, PROC CVPR IEEE, P251, DOI 10.1109/CVPR.2016.34
NR 47
TC 29
Z9 31
U1 3
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 761
EP 771
DI 10.1109/TMM.2020.2987685
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QA6NL
UT WOS:000613560200017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Gao, Z
   Gao, LS
   Zhang, H
   Cheng, ZY
   Hong, RC
   Chen, SY
AF Gao, Zan
   Gao, Lishuai
   Zhang, Hua
   Cheng, Zhiyong
   Hong, Richang
   Chen, Shengyong
TI DCR: A Unified Framework for Holistic/Partial Person ReID
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Cameras; Image reconstruction; Training; Feature
   extraction; Skeleton; Collaboration; Dcr; deep mutual learning; fcn;
   holistic person reid; partial person reid; unified framework
ID REIDENTIFICATION; NETWORK
AB According to the occlusions, Person reidentification (ReID) can be divided into holistic person ReID and partial person ReID tasks. Occlusions commonly exist in the partial person ReID task but pose or observation perspective changes often occur in the holistic person ReID task; thus, many different network architectures have been designed for each task. However, this approach increases the cost in practice and hinders the development of ReID techniques. To solve this problem, in this work, a unified framework is proposed for holistic/partial person ReID, which can effectively and efficiently address changes in pose or observation perspective and the occlusions in both tasks. In detail, we first employ a fully convolutional network to generate feature maps for an arbitrarily sized image and then use spatial pyramid pooling to obtain its spatial pyramid feature. Thereafter, to efficiently solve the matching problem between the query image and gallery images, we build a deep spatial pyramid feature collaborative reconstruction model (DCR). Experimental results on two partial person ReID datasets and three holistic person ReID datasets demonstrate that DCR outperforms state-of-the-art approaches on both tasks and all datasets. Specifically, it outperforms all competitors with a large margin and achieves an improvement of 9.07% and 5.95% over DSR on Partial ReID and Partial-iLIDS datasets with Rank-1, respectively. Similarly, it also achieves an improvement of 5.08% over VPM on the DukeMTMC-ReID dataset with Rank-1. Additionally, the running time of our method for each query is more than 7x faster than that of DSR or DuATM methods.
C1 [Gao, Zan; Cheng, Zhiyong] Qilu Univ Technol, Shandong Acad Sci, Shandong Artificial Intelligence Inst, Jinan 250014, Peoples R China.
   [Gao, Zan; Cheng, Zhiyong] Shandong Comp Sci Ctr, Natl Supercomp Ctr, Jinan 250014, Peoples R China.
   [Gao, Lishuai; Zhang, Hua; Chen, Shengyong] Tianjin Univ Technol, Sch Comp Sci & Technol, Key Lab Comp Vis & Syst, Minist Educ, Tianjin 300384, Peoples R China.
   [Hong, Richang] Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Peoples R China.
C3 Qilu University of Technology; Qilu University of Technology; Tianjin
   University of Technology; Hefei University of Technology
RP Gao, LS; Zhang, H (corresponding author), Tianjin Univ Technol, Sch Comp Sci & Technol, Key Lab Comp Vis & Syst, Minist Educ, Tianjin 300384, Peoples R China.
EM zangaonsh4522@gmail.com; shuai0425@foxmail.com; hzhang62@163.com;
   jason.zy.cheng@gmail.com; hongrc.hfut@gmail.com; sy@ieee.org
RI Chen, S./H-3083-2011
OI Chen, S.Y./0000-0002-6705-3831; , zan/0000-0003-2182-5741
FU National Natural Science Foundation of China [61872270, 61572357];
   National Key R&D Program of China [2019YFBB1404700]; Young creative team
   in universities of Shandong Province [2020KJN012]; Jinan 20 projects in
   universities [2018GXRC014]; Tianjin New Generation Artificial
   Intelligence Major Program [18ZXZNGX00150]; Tianjin Natural Science
   Foundation of China [18JCYBJC85500]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61872270 and 61572357), in part by the
   National Key R&D Program of China under Grant 2019YFBB1404700, in part
   by the Young creative team in universities of Shandong Province under
   Grant 2020KJN012, in part by Jinan 20 projects in universities under
   Grant 2018GXRC014, in part by Tianjin New Generation Artificial
   Intelligence Major Program under Grant 18ZXZNGX00150, and in part by the
   Tianjin Natural Science Foundation of China under Grant 18JCYBJC85500.
   This article was presented in part at the conference of ACM MM 2019 with
   the title of Deep Spatial Pyramid Feature Collaborative Reconstruction
   for Partial Person ReID. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Jianguo
   Zhang. (Corresponding Author: Lishuai Gao and Hua Zhang). (Corresponding
   Author: Lishuai Gao and Hua Zhang.)
CR [Anonymous], 2016, arXiv preprint arXiv:1611.05244
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Cheng D, 2020, IEEE T CYBERNETICS, V50, P561, DOI 10.1109/TCYB.2018.2869739
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Ding GD, 2019, IEEE T MULTIMEDIA, V21, P2891, DOI 10.1109/TMM.2019.2916456
   Donahue J, 2014, PR MACH LEARN RES, V32
   Dong X, 2019, J VIS COMMUN IMAGE R, V58, P187, DOI 10.1016/j.jvcir.2018.11.030
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Fu Y, 2019, AAAI CONF ARTIF INTE, P8287
   Gao Z, 2020, NEURAL NETWORKS, V125, P290, DOI 10.1016/j.neunet.2020.02.017
   Gao Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1879, DOI 10.1145/3343031.3350861
   Gao Z, 2019, IEEE INTERNET THINGS, V6, P9280, DOI 10.1109/JIOT.2019.2911669
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Guo YW, 2018, IEEE T CYBERNETICS, V48, P2402, DOI 10.1109/TCYB.2017.2739338
   He LX, 2018, PROC CVPR IEEE, P7073, DOI 10.1109/CVPR.2018.00739
   Hermans Alexander, 2017, ARXIV170307737
   Jaderberg M, 2015, ADV NEUR IN, V28
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   King DB, 2015, ACS SYM SER, V1214, P1
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liao SC, 2013, IEEE T PATTERN ANAL, V35, P1193, DOI 10.1109/TPAMI.2012.191
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Liu AA, 2017, IEEE T PATTERN ANAL, V39, P102, DOI 10.1109/TPAMI.2016.2537337
   Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762
   Liu JX, 2018, PROC CVPR IEEE, P4099, DOI 10.1109/CVPR.2018.00431
   Qi L, 2019, WORLD WIDE WEB
   Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun YF, 2019, PROC CVPR IEEE, P393, DOI 10.1109/CVPR.2019.00048
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tao DP, 2015, IEEE T CYBERNETICS, V45, P242, DOI 10.1109/TCYB.2014.2323992
   Wang Z, 2018, IEEE T CYBERNETICS, V48, P3006, DOI 10.1109/TCYB.2017.2755044
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu Y, 2019, IEEE T IMAGE PROCESS, V28, P2872, DOI 10.1109/TIP.2019.2891895
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Xuan Z., 2017, ARXIV171108184
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Yu HX, 2019, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR.2019.00225
   Zhang SZ, 2021, IEEE T MULTIMEDIA, V23, P281, DOI 10.1109/TMM.2020.2977528
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zhao YR, 2019, PROC CVPR IEEE, P4908, DOI 10.1109/CVPR.2019.00505
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng WS, 2015, IEEE I CONF COMP VIS, P4678, DOI 10.1109/ICCV.2015.531
   Zheng WS, 2011, PROC CVPR IEEE, P649, DOI 10.1109/CVPR.2011.5995598
   Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
   Zhu PF, 2014, IEEE T INF FOREN SEC, V9, P1120, DOI 10.1109/TIFS.2014.2324277
NR 59
TC 20
Z9 20
U1 3
U2 49
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3332
EP 3345
DI 10.1109/TMM.2020.3023784
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000030
DA 2024-07-18
ER

PT J
AU Hassen, R
   Gülecyüz, B
   Steinbach, E
AF Hassen, Rania
   Guelecyuez, Basak
   Steinbach, Eckehard
TI PVC-SLP: Perceptual Vibrotactile-Signal Compression Based-on Sparse
   Linear Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Sensitivity; Codecs; Encoding; Acceleration; Surface texture; Haptic
   interfaces; Discrete cosine transforms; Perceptual coding; Tactile
   Sensitivity Function; Sparse Linear Prediction; Perceptual quality
   assessment
ID FREQUENCY-SELECTIVITY; TACTILE; QUANTIZATION; THRESHOLDS; CHANNELS; AREA
AB Developing a signal compression technique that is able to achieve a low bit rate while maintaining high perceptual signal quality is a classical signal processing problem vigorously studied for audio, speech, image, and video type of signals. Yet, until recently, there has been limited effort directed toward the compression of vibrotactile signals, which represent a crucial element of rich touch (haptic) information. A vibrotactile signal; produced when stroking a textured surface with a tool-tip or bare-finger; like other signals contains a great deal of redundant and imperceptible information that can be exploited for efficient compression. This paper presents PVC-SLP, a vibrotactile perceptual coding approach. PVC-SLP employs a model of tactile sensitivity; called ASF (Acceleration Sensitivity Function); for perceptual coding. The ASF is inspired by the four channels model that mediate the perception of vibrotactile stimuli in the glabrous skin. The compression algorithm introduces sparsity constraints in a linear prediction scheme both on the residual and the predictor coefficients. The perceptual quantization of the residual is developed through the use of ASF. The quantization parameters of the residual and the predictor coefficients were jointly optimized; by means of both squared error and perceptual quality measures; to find the sweet spot of the rate-distortion curve. PVC-SLP coding performance is evaluated using two publicly available databases that collectively comprise 1281 vibrotactile signals covering 193 material classes. Furthermore, we compare PVC-SLP with a recent vibrotactile compression method and show that PVC-SLP perceptually outperforms existing method by a sizable margin. Most recently, PVC-SLP has been selected to become part of the haptic codec standard currently under preparation by IEEE P1918.1.1, aka Haptic Codecs for the Tactile Internet.
C1 [Hassen, Rania; Guelecyuez, Basak; Steinbach, Eckehard] Tech Univ Munich, Dept Elect & Comp Engn, D-80333 Munich, Germany.
   [Hassen, Rania] Assiut Univ, Comp Sci Dept, Assiut, Egypt.
C3 Technical University of Munich; Egyptian Knowledge Bank (EKB); Assiut
   University
RP Hassen, R (corresponding author), Tech Univ Munich, Dept Elect & Comp Engn, D-80333 Munich, Germany.
EM rania.hassen@tum.de; basak.guelecyuez@tum.de; eckehard.steinbach@tum.de
OI Steinbach, Eckehard/0000-0001-8853-2703; Gulecyuz,
   Basak/0000-0001-9280-5276
FU German Research Foundation (DFG, Deutsche Forschungsgemeinschaft)
   -Cluster of Excellence "Centre for Tactile Internet with
   Human-in-the-Loop" (CeTI) of the Technical University of Dresden
   [390696704]; Alexander von Humbolt foundation
FX This work was supported in part by the German Research Foundation (DFG,
   Deutsche Forschungsgemeinschaft) as part of Germany's Excellence
   Strategy -EXC 2050/1 -Project ID 390696704 -Cluster of Excellence
   "Centre for Tactile Internet with Human-in-the-Loop" (CeTI) of the
   Technical University of Dresden. The work of Rania Hassen was supported
   by the Alexander von Humbolt foundation and hosted by the Chair of Media
   Technology, Technical University of Munich, Germany. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. L. Cao.
CR [Anonymous], 2002, Handbook of Psychology, DOI DOI 10.1002/0471264385.WEI0406
   Bensmaïa S, 2005, PERCEPT PSYCHOPHYS, V67, P842, DOI 10.3758/BF03193537
   Bensmaïa SJ, 2003, SOMATOSENS MOT RES, V20, P33, DOI 10.1080/0899022031000083825
   BOLANOWSKI SJ, 1988, J ACOUST SOC AM, V84, P1680, DOI 10.1121/1.397184
   Chaudhari R., 2012, P ACM MULT NAR JAP O, P409, DOI [10.1145/2393347.2393407, DOI 10.1145/2393347.2393407]
   Chaudhariu R, 2015, IEEE J-STSP, V9, P462, DOI 10.1109/JSTSP.2014.2374574
   Choi S, 2013, P IEEE, V101, P2093, DOI 10.1109/JPROC.2012.2221071
   Culbertson Heather, 2012, 2012 IEEE Haptics Symposium (HAPTICS), P385, DOI 10.1109/HAPTIC.2012.6183819
   DURBIN J, 1959, BIOMETRIKA, V46, P306, DOI 10.1093/biomet/46.3-4.306
   Eid M., 2018, P19181 IEEE
   Gescheider GA, 2002, SOMATOSENS MOT RES, V19, P114, DOI 10.1080/08990220220131505
   Gescheider GA, 2001, SOMATOSENS MOT RES, V18, P191
   Giacobello D, 2012, IEEE T AUDIO SPEECH, V20, P1644, DOI 10.1109/TASL.2012.2186807
   GRAY AH, 1976, IEEE T ACOUST SPEECH, V24, P459, DOI 10.1109/TASSP.1976.1162857
   HAMER RD, 1983, J ACOUST SOC AM, V73, P1293, DOI 10.1121/1.389278
   HANSEN PC, 1993, SIAM J SCI COMPUT, V14, P1487, DOI 10.1137/0914086
   Hassen R, 2018, 2018 10 INT C QUAL M, P1
   Hassen R, 2020, IEEE T HAPTICS, V13, P25, DOI 10.1109/TOH.2019.2962446
   Hassen R, 2019, 2019 IEEE WORLD HAPTICS CONFERENCE (WHC), P301, DOI [10.1109/WHC.2019.8816110, 10.1109/whc.2019.8816110]
   Hernando J, 1997, IEEE T SPEECH AUDI P, V5, P80, DOI 10.1109/89.554273
   Holland O, 2019, P IEEE, V107, P256, DOI 10.1109/JPROC.2018.2885541
   IEEE Communications Society, 2016, P19181 IEEE COMM SOC
   Israr A, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P472, DOI 10.1109/IROS.2006.282353
   Jensen TL, 2013, INT CONF ACOUST SPEE, P8184, DOI 10.1109/ICASSP.2013.6639260
   Kalluri M, 2019, IEEE T MULTIMEDIA, V21, P39, DOI 10.1109/TMM.2018.2847228
   Kirsch J., 2018, 2018 IEEE INT S HAPT, P1
   Koloda J, 2013, IEEE T MULTIMEDIA, V15, P957, DOI 10.1109/TMM.2013.2238524
   Kuchenbecker KJ, 2011, SPRINGER TRAC ADV RO, V70, P245
   Li LD, 2016, IEEE T MULTIMEDIA, V18, P1085, DOI 10.1109/TMM.2016.2545398
   Liebchen T, 2009, J ACOUST SOC KOREA, V28, P618
   Liu X, 2020, IEEE T MULTIMEDIA, V22, P921, DOI 10.1109/TMM.2019.2936305
   MAKHOUL J, 1975, P IEEE, V63, P561, DOI 10.1109/PROC.1975.9792
   Manfredi LR, 2014, J NEUROPHYSIOL, V111, P1792, DOI 10.1152/jn.00680.2013
   Meyer DJ, 2013, 2013 WORLD HAPTICS CONFERENCE (WHC), P43, DOI 10.1109/WHC.2013.6548382
   Morioka M, 2005, SOMATOSENS MOT RES, V22, P281, DOI 10.1080/08990220500420400
   Muschter E., 2020, P191811 IEEE
   Muschter E., 2019, P191811 IEEE
   Noboru Harada, 2011, MPEG 4 ALS REFERENCE
   Noll A, 2020, IEEE HAPTICS SYM, P854, DOI 10.1109/HAPTICS45997.2020.ras.HAP20.6.422bbc6e
   Okamoto S., 2010, Proceedings of the 2010 IEEE/SICE International Symposium on System Integration (SII 2010), P384, DOI 10.1109/SII.2010.5708356
   Okamoto S, 2013, IEEE T HAPTICS, V6, P81, DOI [10.1109/ToH.2012.32, 10.1109/TOH.2012.32]
   Okamoto S, 2013, IEEE T HAPTICS, V6, P69, DOI [10.1109/TOH.2012.18, 10.1109/ToH.2012.18]
   Romano JM, 2012, IEEE T HAPTICS, V5, P109, DOI [10.1109/TOH.2011.38, 10.1109/ToH.2011.38]
   Romano JM, 2010, IEEE INT CONF ROBOT, P1815, DOI 10.1109/ROBOT.2010.5509853
   Satopaa V., 2011, Proceedings of the 2011 31st International Conference on Distributed Computing Systems Workshops (ICDCS Workshops), P166, DOI 10.1109/ICDCSW.2011.20
   SCHROEDER J, 1989, SIGNAL PROCESS, V17, P19, DOI 10.1016/0165-1684(89)90069-8
   Schwartz O, 2001, NAT NEUROSCI, V4, P819, DOI 10.1038/90526
   Steinbach E, 2019, P IEEE, V107, P447, DOI 10.1109/JPROC.2018.2867835
   Strese M, 2020, IEEE T HAPTICS, V13, P404, DOI 10.1109/TOH.2019.2952118
   Strese M, 2017, IEEE T HAPTICS, V10, P226, DOI 10.1109/TOH.2016.2625787
   Technical University of Munich, TACT REF DAT TRAC
   Verillo R.T., 1966, PSYCHON SCI, V4, P135, DOI DOI 10.3758/BF03342215
   Verrillo R.T., 1992, TACTILE AIDS HEARING, P1
   VERRILLO RT, 1963, J ACOUST SOC AM, V35, P1962, DOI 10.1121/1.1918868
   VERRILLO RT, 1971, PERCEPT PSYCHOPHYS, V9, P329, DOI 10.3758/BF03208688
   VERRILLO RT, 1975, EXPT SENSORY PSYCHOL, P159
   Weber AI, 2013, P NATL ACAD SCI USA, V110, P17107, DOI 10.1073/pnas.1305509110
   Winfield L, 2007, WORLD HAPTICS 2007: SECOND JOINT EUROHAPTICS CONFERENCE AND SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS, PROCEEDINGS, P421
NR 58
TC 14
Z9 15
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4455
EP 4468
DI 10.1109/TMM.2020.3042674
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XM6HD
UT WOS:000728924800005
DA 2024-07-18
ER

PT J
AU Ma, X
   Guo, JD
   Sansom, A
   McGuire, M
   Kalaani, A
   Chen, Q
   Tang, SH
   Yang, Q
   Fu, S
AF Ma, Xu
   Guo, Jingda
   Sansom, Andrew
   McGuire, Mara
   Kalaani, Andrew
   Chen, Qi
   Tang, Sihai
   Yang, Qing
   Fu, Song
TI Spatial Pyramid Attention for Deep Convolutional Neural Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object detection; Feature extraction; Convolutional codes; Computer
   architecture; Benchmark testing; Topology; Task analysis; Attention
   mechanism; convolutional neural network; image classification; object
   detection; spatial pyramid structure; structural regularization;
   structural information
AB Attention mechanisms have shown great success in computer vision. However, the commonly used global average pooling in some implementations aggregates a three-dimensional feature map to a one-dimensional attention map, leading a significant loss of structural information in the attention learning. In this article, we present a novel Spatial Pyramid Attention Network (SPANet), which exploits the structural information and channel relationships for better feature representation. SPANet enhances a base network by adding Spatial Pyramid Attention (SPA) blocks laterally. By rethinking the self-attention mechanism design, we further present three topology structures of attention path connection for our SPANet. They can be flexibly applied to various CNN architectures. SPANet is conceptually simple but practically powerful. It uses both structural regularization and structural information to achieve better learning capability. We have comprehensively evaluated the performance of SPANet on four benchmark datasets for different visual tasks. The experimental results show that SPANet significantly improves the recognition accuracy without adding much computation overhead. Using SPANet, we achieve an improvement of 1.6% top-1 classification accuracy on the ImageNet 2012 benchmark based on ResNet50, and SPANet outperforms SENet and other attention methods. SPANet also significantly improves the object detection performance by a clear margin with negligible additional computation overhead. When applying SPANet to RetinaNet based on the ResNet50 backbone, we improve the performance of the baseline model by 2.3 mAP and the enhanced model outperforms SENet and GCNet by 1.1 mAP and 1.7 mAP respectively. The code of SPANet is made publicly available.(1) (1) [Online]. Available: https://github.com/13952522076/SPANet_TMM
C1 [Ma, Xu; Guo, Jingda; Chen, Qi; Tang, Sihai; Yang, Qing; Fu, Song] Univ North Texas, Dept Comp Sci & Engn, Denton, TX 76203 USA.
   [Ma, Xu] Nanjing Forestry Univ, Coll Informat Sci & Technol, Nanjing 210037, Peoples R China.
   [Sansom, Andrew] Univ North Texas, Dept Math, Denton, TX 76203 USA.
   [McGuire, Mara] Texas A&M Univ Corpus Christi, Dept Math & Stat, Corpus Christi, TX 78412 USA.
   [Kalaani, Andrew] Georgia Southern Univ, Dept Elect & Comp Engn, Statesboro, GA 30458 USA.
C3 University of North Texas System; University of North Texas Denton;
   Nanjing Forestry University; University of North Texas System;
   University of North Texas Denton; Texas A&M University System; Texas A&M
   University Corpus Christi; University System of Georgia; Georgia
   Southern University
RP Fu, S (corresponding author), Univ North Texas, Dept Comp Sci & Engn, Denton, TX 76203 USA.
EM xuma@my.unt.edu; jingdaguo@my.unt.edu; AndrewSansom@my.unt.edu;
   maracm17@gmail.com; ak04526@georgiasouthern.edu; qichen@my.unt.edu;
   sihaitang@my.unt.edu; qing.yang@unt.edu; Song.Fu@unt.edu
OI Sansom, Andrew/0000-0002-2276-7224; Tang, Sihai/0000-0002-2438-4163; Ma,
   Xu/0000-0002-5794-119X; Fu, Song/0000-0002-7705-0829; Yang,
   Qing/0000-0002-0683-5848
FU National Science Foundation [CNS-1852134, OAC-2017564, ECCS-2010332,
   CNS-2037982, CNS-1563750]; Fujitsu Laboratories of America Inc.
FX This work was supported in part by the National Science Foundation under
   Grants CNS-1852134, OAC-2017564, ECCS-2010332, CNS2037982, and
   CNS-1563750, and a research grant from Fujitsu Laboratories of America
   Inc. A preliminary version of this article was accepted by 2020 IEEE
   InternationalConference on Multimedia and Expo (ICME) and received the
   Best Student Paper Award [49]. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof. J.
   Zhang.
CR Bachlechner Thomas, 2020, C UNC ART INT
   Bello I, 2019, IEEE I CONF COMP VIS, P3285, DOI 10.1109/ICCV.2019.00338
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chen K., ARXIV190607155, V2019
   Chrabaszcz P., 2017, A downsampled variant of imagenet as an alternative to the cifar datasets
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guo JD, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102906
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Howard A. G., 2017, PREPRINT
   Hu J, 2018, ADV NEURAL INFORM PR, P9401, DOI DOI 10.5555/3327546.3327612
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   HU Y, 2018, ARXIV180708920
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W., 2015, ARXIV150604579
   Loshchilov I., 2016, Proc. 5th International Conf. on Learning Representations
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Nguyen TV, 2018, INT J COMPUT VISION, V126, P86, DOI 10.1007/s11263-017-1042-6
   Paszke A, 2019, ADV NEUR IN, V32
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava RK, 2015, ARXIV150500387
   Srivastava RK, 2015, ADV NEUR IN, V28
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tan MX, 2019, PROC CVPR IEEE, P2815, DOI [arXiv:1807.11626, 10.1109/CVPR.2019.00293]
   Tan MX, 2019, PR MACH LEARN RES, V97
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang B, 2019, ADV NEUR IN, V32
   Yinpeng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11027, DOI 10.1109/CVPR42600.2020.01104
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 49
TC 21
Z9 22
U1 5
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3048
EP 3058
DI 10.1109/TMM.2021.3068576
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000008
OA hybrid
DA 2024-07-18
ER

PT J
AU Quintanilla, E
   Rawat, Y
   Sakryukin, A
   Shah, M
   Kankanhalli, M
AF Quintanilla, Erik
   Rawat, Yogesh
   Sakryukin, Andrey
   Shah, Mubarak
   Kankanhalli, Mohan
TI Adversarial Learning for Personalized Tag Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Tagging; Deep learning; Convolutional neural networks;
   Encoding; Social networking (online); Tensors; Deep neural networks;
   user preference; image tagging; adversarial learning
ID IMAGES
AB We have recently seen great progress in image classification due to the success of deep convolutional neural networks and the availability of large-scale datasets. Most of the existing work focuses on single-label image classification. However, there are usually multiple tags associated with an image. The existing works on multi-label classification are mainly based on lab curated labels. Humans assign tags to their images differently, which is mainly based on their interests and personal tagging behavior. In this paper, we address the problem of personalized tag recommendation and propose an end-to-end deep network which can be trained on large-scale datasets. The user-preference is learned within the network in an unsupervised way where the network performs joint optimization for user-preference and visual encoding. A joint training of user-preference and visual encoding allows the network to efficiently integrate the visual preference with tagging behavior for a better user recommendation. In addition, we propose the use of adversarial learning, which enforces the network to predict tags resembling user-generated tags. We demonstrate the effectiveness of the proposed model on two different large-scale and publicly available datasets, YFCC100 M and NUS-WIDE. The proposed method achieves significantly better performance on both the datasets when compared to the baselines and other state-of-the-art methods. The code is publicly available at https://github.com/vyzuer/ALTReco.
C1 [Quintanilla, Erik] IIT, Chicago, IL 60616 USA.
   [Rawat, Yogesh; Shah, Mubarak] Univ Cent Florida, CRCV, Orlando, FL 32816 USA.
   [Sakryukin, Andrey; Kankanhalli, Mohan] Natl Univ Singapore, Singapore 119077, Singapore.
C3 Illinois Institute of Technology; State University System of Florida;
   University of Central Florida; National University of Singapore
RP Rawat, Y (corresponding author), Univ Cent Florida, CRCV, Orlando, FL 32816 USA.
EM equintanilla@hawk.iit.edu; yogesh@u.nus.edu; asakryukin@gmail.com;
   shah@cs.ucf.edu; mohan@comp.nus.edu.sg
RI Sahoo, Sarat Kumar/J-8765-2014; Kankanhalli, Mohan/Q-9284-2019
OI Sahoo, Sarat Kumar/0000-0001-5734-6844; Kankanhalli,
   Mohan/0000-0002-4846-2015; Rawat, Yogesh/0000-0003-4052-6798; Shah,
   Mubarak/0000-0001-6172-5572
FU NSF CNS [1461121]; Division Of Computer and Network Systems; Direct For
   Computer & Info Scie & Enginr [1461121] Funding Source: National Science
   Foundation
FX The work of Erik Quintanilla was supported by NSF CNS under Grant
   1461121.
CR [Anonymous], 2013, P 21 ACM INT C MULT
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   Babbar Sch, 2018, ARXIV180301570
   Cai Yuanzhe., 2011, Proceedings of the Fourth ACM International Conference on Web Search and Data Mining, WSDM'11, P695, DOI DOI 10.1145/1935826.1935920
   Chen A., 2013, ICML, P1274
   Chen Meihao, 2017, ARXIV171004908
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Cui P, 2018, IEEE T MULTIMEDIA, V20, P198, DOI 10.1109/TMM.2017.2724843
   Cui P, 2016, IEEE MULTIMEDIA, V23, P92, DOI 10.1109/MMUL.2016.8
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding XM, 2016, IEEE T MULTIMEDIA, V18, P1616, DOI 10.1109/TMM.2016.2572000
   Donahue J., 2016, ARXIV160509782
   Fang SC, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P715
   Fang SC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P248, DOI 10.1145/3240508.3240571
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Gong Yunchao., 2013, Deep convolutional ranking for multilabel image annotation
   Goodfellow IJ, 2014, ADV NEUR IN, P2672, DOI DOI 10.1145/3422622
   Guan ZY, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P540, DOI 10.1145/1571941.1572034
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P309, DOI 10.1109/ICCV.2009.5459266
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2015, PROC CVPR IEEE, P5353, DOI 10.1109/CVPR.2015.7299173
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang Y, 2015, IEEE T MULTIMEDIA, V17, P1923, DOI 10.1109/TMM.2015.2476658
   HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732
   Johnson J, 2015, IEEE I CONF COMP VIS, P4624, DOI 10.1109/ICCV.2015.525
   Karras T., 2018, arXiv, DOI [10.48550/arXiv.1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee CW, 2018, PROC CVPR IEEE, P1576, DOI 10.1109/CVPR.2018.00170
   Li L, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1092, DOI 10.1145/3240508.3240649
   [李晓江 Li Xiaojiang], 2014, [城市规划学刊, Urban Planning Forum], P1
   Li XR, 2019, IEEE T MULTIMEDIA, V21, P2347, DOI 10.1109/TMM.2019.2896494
   Liao S, 2015, IEEE T MULTIMEDIA, V17, P1058, DOI 10.1109/TMM.2015.2436057
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin ZJ, 2014, COMPUT VIS IMAGE UND, V124, P42, DOI 10.1016/j.cviu.2014.03.012
   Liu J, 2014, IEEE T MULTIMEDIA, V16, P588, DOI 10.1109/TMM.2014.2302732
   Liu M.-Y., 2016, P ADV NEUR INF PROC, P469
   Liu YC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P700, DOI 10.1145/3240508.3240567
   Makadia A, 2008, LECT NOTES COMPUT SC, V5304, P316, DOI 10.1007/978-3-540-88690-7_24
   McParlane PJ, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P965
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Nguyen HTH, 2017, LECT NOTES ARTIF INT, V10535, P705, DOI 10.1007/978-3-319-71246-8_43
   Nguyen HTH, 2017, LECT NOTES ARTIF INT, V10234, P186, DOI 10.1007/978-3-319-57454-7_15
   Rae Adam., 2010, Adaptivity, Personalization and Fusion of Heterogeneous Information, P92
   Rafailidis D, 2013, IEEE T SYST MAN CY-S, V43, P673, DOI 10.1109/TSMCA.2012.2208186
   Rawat Y. S., 2016, MM 16, P1102, DOI DOI 10.1145/2964284.2984068
   Rendle Steffen, 2010, P 3 ACM INT C WEB SE, P81, DOI DOI 10.1145/1718487.1718498
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shah RR, 2016, IEEE INT SYM MULTIM, P486, DOI [10.1109/ISM.2016.108, 10.1109/ISM.2016.0109]
   Sigurbjornsson B., 2008, Proceeding of the 17th International Conference on World Wide Web
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun LF, 2017, IEEE T MULTIMEDIA, V19, P609, DOI 10.1109/TMM.2016.2635589
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Tian Y, 2015, IEEE MULTIMEDIA, V22, P93, DOI 10.1109/MMUL.2015.61
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang YL, 2017, AAAI CONF ARTIF INTE, P210
   Wei Yunchao., 2014, CoRR
   Weston J, 2011, IJCAI
   Weston J, 2010, MACH LEARN, V81, P21, DOI 10.1007/s10994-010-5198-3
   Wu BY, 2018, PROC CVPR IEEE, P7967, DOI 10.1109/CVPR.2018.00831
   Wu BY, 2017, IEEE T MULTIMEDIA, V19, P1670, DOI 10.1109/TMM.2017.2655881
   Xie HT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231737
   Xu D, 2014, IEEE MULTIMEDIA, V21, P76, DOI 10.1109/MMUL.2014.62
   Yao JC, 2018, IEEE T MULTIMEDIA, V20, P224, DOI 10.1109/TMM.2017.2716829
   Zeiler Matthew D, 2012, ARXIV12125701
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P2762, DOI 10.1109/TMM.2019.2912124
   Zhang J, 2017, IEEE T MULTIMEDIA, V19, P2439, DOI 10.1109/TMM.2017.2701641
   Zhu J.-Y., 2017, IEEE I CONF COMP VIS, P2223, DOI DOI 10.1109/ICCV.2017.244
   Zhu WW, 2015, IEEE MULTIMEDIA, V22, P96, DOI 10.1109/MMUL.2015.66
NR 73
TC 17
Z9 18
U1 1
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1083
EP 1094
DI 10.1109/TMM.2020.2992941
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300020
OA Green Submitted, Bronze
DA 2024-07-18
ER

PT J
AU Shi, WX
   Wang, C
   Jiang, Y
   Li, Q
   Shen, GB
   Muntean, GM
AF Shi, Wanxin
   Wang, Chao
   Jiang, Yong
   Li, Qing
   Shen, Gengbiao
   Muntean, Gabriel-Miro
TI CoLEAP: Cooperative Learning-Based Edge Scheme With Caching and
   Prefetching for DASH Video Delivery
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Streaming media; Prefetching; Quality of experience; Bit rate; Servers;
   Optimization; Bandwidth; DASH; caching; prefetching; QoE; edge computing
ID MANAGEMENT
AB The outstanding increase in video traffic, puts increasing pressure on network transmission. Since the Dynamic Adaptive Streaming over HTTP (DASH) adjusts the delivery to the dynamic network conditions, it has emerged as a popular approach for video transmissions. However, bitrate switching and video rebuffering may still occur and influence negatively quality of experience (QoE). Additionally the popular videos are transmitted multiple times, which leads to high bandwidth consumption, despite large transmission redundancy. In this context, we propose a Cooperative Learning-based scheme for the smart Edge servers with cAching and Prefetching (CoLEAP) to improve the QoE of adaptive video streaming. CoLEAP employs edge servers which cache the most beneficial contents to reduce redundant video transmissions and prefetches content to decrease network transmission delay. Considering user-related information and the state of network, CoLEAP intelligently makes the most advantageous decisions of caching and prefetching by employing a novel QoE-oriented deep neural network model. To demonstrate the performance of our scheme, we test the proposed solution in comprehensive simulated scenarios and against four alternative solutions. When compared with the existing schemes, CoLEAP increases average bitrate by up to 181.8%, reduces video rebuffering by up to 70.8% as well as decreases response time by up to 28.0%. These values result in minimum improvements of 57.4% and 29.0%, respectively in terms of cache hit rate and QoE.
C1 [Shi, Wanxin; Jiang, Yong; Shen, Gengbiao] Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
   [Wang, Chao; Li, Qing] Peng Cheng Lab PCL, PCL Res Ctr Networks & Commun, Shenzhen 518066, Peoples R China.
   [Wang, Chao] Tsinghua Univ, Tsinghua Berkeley Shenzhen Inst, Shenzhen 518055, Peoples R China.
   [Jiang, Yong] Peng Cheng Lab PCL, Shenzhen 518066, Peoples R China.
   [Li, Qing] Southern Univ Sci & Technol, Shenzhen 518055, Peoples R China.
   [Muntean, Gabriel-Miro] Dublin City Univ, Sch Elect Engn, Performance Engn Lab, Galsnevin Campus, Dublin D09, Ireland.
C3 Tsinghua Shenzhen International Graduate School; Tsinghua University;
   Tsinghua Shenzhen International Graduate School; Southern University of
   Science & Technology; Dublin City University
RP Li, Q (corresponding author), Southern Univ Sci & Technol, Shenzhen 518055, Peoples R China.
EM shiwx17@mails.tsinghua.edu.cn; wangchao17@mails.tsinghua.edu.cn;
   jiangy@sz.tsinghua.edu.cn; liq8@sustech.edu.cn;
   sgb16@mails.tsinghua.edu.cn; gabriel.muntean@dcu.ie
RI Muntean, Gabriel-Miro/U-6783-2019; Li, Qing/U-8995-2018
OI Muntean, Gabriel-Miro/0000-0002-9332-4770; Li, Qing/0000-0002-6071-473X;
   , Wanxin/0000-0001-5394-1478
FU National Natural Science Foundation of China [61972189]; Guangdong
   Province Key Area RD Program [2018B010113001]; Project "PCL Future
   Regional Network Facilities for Large-scale Experiments and
   Applications" [PCL2018KP001]; Shenzhen Key Laboratory of Software
   Defined Networking [ZDSYS20140509172959989]
FX This work is supported in part by the National Natural Science
   Foundation of China under Grant 61972189, in part by Guangdong Province
   Key Area R&D Program under Grant 2018B010113001, in part by the Project
   "PCL Future Regional Network Facilities for Large-scale Experiments and
   Applications (PCL2018KP001)", and in part by the Shenzhen Key Laboratory
   of Software Defined Networking underGrant ZDSYS20140509172959989. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Sanjeev Mehrotra. (Corresponding
   author: Qing Li.)
CR Ahmed A., 2017, PROC IEEE 25 INT C N, P1
   Akhshabi Saamer, 2013, Proceeding of the 23rd ACM Workshop on Network and Operating Systems Support for Digital Audio and Video-NOSSDAV'13
   Akhtar Z, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P44, DOI 10.1145/3230543.3230558
   Altamimi S, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3397227
   [Anonymous], 2018, IQIYI OPEN CACHE PRO
   [Anonymous], 2019, LEAP
   [Anonymous], 2019, PRELOAD WEBPACK PLUG
   [Anonymous], 2015, P 6 ACM MULTIMEDIA S
   [Anonymous], 2018, NGINX HIGH PERFORMAN
   [Anonymous], 2018, APACHE TRAFFIC SERVE
   Bagci KT, 2017, IEEE T MULTIMEDIA, V19, P2152, DOI 10.1109/TMM.2017.2736638
   Batalla JM, 2016, IEEE J SEL AREA COMM, V34, P2154, DOI 10.1109/JSAC.2016.2577360
   Benno S, 2011, BELL LABS TECH J, V16, P101, DOI 10.1002/bltj.20505
   Berger DS, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P483
   Chiariotti F, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P77, DOI 10.1145/2910017.2910603
   CISCO, 2018, CISCO VISUAL NETWORK, P1
   Detti A, 2016, IEEE INFOCOM SER
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   FREEMAN PR, 1983, INT STAT REV, V51, P189, DOI 10.2307/1402748
   Ge C, 2017, IEEE T MULTIMEDIA, V19, P2222, DOI 10.1109/TMM.2017.2735301
   Gracia Chithra D, 2016, IEEE ICETETS
   Hu W, 2018, IEEE T CIRC SYST VID, V28, P1665, DOI 10.1109/TCSVT.2017.2684302
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Hubert B., 2002, Netherlabs BV, V1, P99
   Jiang JC, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P393
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Juluri P, 2015, SIGCOMM'15: PROCEEDINGS OF THE 2015 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P599, DOI 10.1145/2785956.2790015
   Krishnamoorthi V, 2013, I S MOD ANAL SIM COM, P182, DOI 10.1109/MASCOTS.2013.26
   Leconte Mathieu, 2016, IEEE INFOCOM
   Li CL, 2018, IEEE T MULTIMEDIA, V20, P965, DOI 10.1109/TMM.2017.2757761
   Li J, 2019, IEEE T MULTIMEDIA, V21, P2589, DOI 10.1109/TMM.2019.2903722
   Liu J., 2018, P IEEE 26 INT S QUAL, P1
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Vo PL, 2016, 2016 IEEE SIXTH INTERNATIONAL CONFERENCE ON COMMUNICATIONS AND ELECTRONICS (ICCE), P417, DOI 10.1109/CCE.2016.7562673
   Plesca C, 2016, J NETW COMPUT APPL, V69, P40, DOI 10.1016/j.jnca.2016.05.002
   Sánchez-Hernández JJ, 2015, IEEE T MULTIMEDIA, V17, P1829, DOI 10.1109/TMM.2015.2470595
   Sengupta S, 2018, I C NETWORK PROTOCOL, P165, DOI 10.1109/ICNP.2018.00026
   Shi WX, 2019, INT WORKSH QUAL SERV, DOI 10.1145/3326285.3329051
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Summers J., 2016, P IEEE INT S WORKL C, P1
   Tang LP, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P111
   Tanzil SMS, 2017, IEEE ACCESS, V5, P5870, DOI 10.1109/ACCESS.2017.2678990
   Teng WG, 2005, IEEE T PARALL DISTR, V16, P444, DOI 10.1109/TPDS.2005.56
   Wang J. Z., 2006, IEEE T MULTIMEDIA, V9, P147
   Wang YM, 2017, MOBILE NETW APPL, V22, P72, DOI 10.1007/s11036-016-0689-5
   Wang Z, 2013, IEEE T MULTIMEDIA, V15, P698, DOI 10.1109/TMM.2012.2237022
   Wu DP, 2019, IEEE T MULTIMEDIA, V21, P1788, DOI 10.1109/TMM.2018.2885931
   Wu DP, 2001, IEEE T CIRC SYST VID, V11, P282, DOI 10.1109/76.911156
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zanforlin M, 2014, 2014 12TH INTERNATIONAL SYMPOSIUM ON MODELING AND OPTIMIZATION IN MOBILE, AD HOC, AND WIRELESS NETWORKS (WIOPT), P656, DOI 10.1109/WIOPT.2014.6850361
   Zhang LX, 2014, ACM SIGCOMM COMP COM, V44, P66, DOI 10.1145/2656877.2656887
   Zhang Q, 2001, IEEE T MULTIMEDIA, V3, P339, DOI 10.1109/6046.944477
   Zhang S, 2018, IEEE T MOBILE COMPUT, V17, P1791, DOI 10.1109/TMC.2017.2780834
   Zhang WW, 2013, IEEE T MULTIMEDIA, V15, P1431, DOI 10.1109/TMM.2013.2247583
   Zhong C, 2018, 2018 52ND ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS (CISS), DOI 10.1109/ciss.2018.8362276
   Zou J., 2019, IEEE J SEL TOPICS SI, V14, P161
NR 57
TC 6
Z9 6
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3631
EP 3645
DI 10.1109/TMM.2020.3029893
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100016
DA 2024-07-18
ER

PT J
AU Song, GL
   Wang, SH
   Huang, QM
   Tian, Q
AF Song, Guoli
   Wang, Shuhui
   Huang, Qingming
   Tian, Qi
TI Learning Feature Representation and Partial Correlation for Multimodal
   Multi-Label Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Correlation; Task analysis; Data models; Learning systems;
   Kernel; Deep learning; Cross-modal retrieval; correlation learning;
   feature learning; partial correlation
ID CANONICAL CORRELATION-ANALYSIS; KERNEL; IMAGES
AB User-provided annotations in existing multimodal datasets sometimes are inappropriate for model learning and can hinder the task of cross-modal retrieval. To handle this issue, we propose a discriminative and noise-robust cross-modal retrieval method, called FLPCL, which consists of deep feature learning and partial correlation learning. Deep feature learning is implemented by utilizing label supervised information to guide the training of deep neural network for each modality, which aims to find modality-specific deep feature representations that preserve the similarity and discrimination information among multimodal data. Based on deep feature learning, partial correlation learning is proposed to infer direct association between different modalities by removing the effect of common underlying semantics from each modality. It is achieved by maximizing the canonical correlation of the feature representations of different modalities conditioned on the label modality. Different from existing works that build indirect association between modalities via incorporating semantic labels, our FLPCL method can learn more effective and robust multimodal latent representations by explicitly preserving both intra-modal and inter-modal relationship among multimodal data. Extensive experiments on three cross-modal datasets show that our method outperforms state-of-the-art methods on cross-modal retrieval tasks.
C1 [Song, Guoli; Huang, Qingming] Peng Cheng Lab, Shenzhen, Peoples R China.
   [Wang, Shuhui; Huang, Qingming] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing, Peoples R China.
   [Huang, Qingming] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101408, Peoples R China.
   [Tian, Qi] Huawei, Cloud BU, Shenzhen 518129, Peoples R China.
C3 Peng Cheng Laboratory; Chinese Academy of Sciences; Institute of
   Computing Technology, CAS; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Huawei Technologies
RP Wang, SH (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing, Peoples R China.
EM songgl@pcl.ac.cn; wangshuhui@ict.ac.cn; qmhuang@ucas.ac.cn;
   tian.qi1@huawei.com
OI Song, Guoli/0000-0002-5452-3697
FU National Key R&D Program of China [2018AAA0102003]; National Natural
   Science Foundation of China [61672497, 61836002, 61620106009, U1636214,
   61931008]; Key Research Program of Frontier Sciences of CAS
   [QYZDJ-SSW-SYS013]
FX Manuscript receivedOctober 14, 2019; revisedMay 8, 2020; accepted June
   11, 2020. Date of publication June 26, 2020; date of current version
   June 25, 2021. This work was supported in part by the National Key R&D
   Program of China under Grant 2018AAA0102003, in part by National Natural
   Science Foundation of China underGrants 61672497, 61836002, 61620106009,
   U1636214, and 61931008, and in part byKey Research Program of Frontier
   Sciences ofCASunder Grant QYZDJ-SSW-SYS013. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Professor Abdulmotaleb El Saddik.
CR Agrawal A, 2017, INT J COMPUT VISION, V123, P4, DOI 10.1007/s11263-016-0966-6
   Andrew G., 2013, ICML, P1247
   [Anonymous], 2011, P ICML
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Grauman K., 2010, P BRIT MACH VIS C, P1
   Hardoon DR, 2011, MACH LEARN, V83, P331, DOI 10.1007/s10994-010-5222-7
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He JF, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P227, DOI 10.1145/2964284.2967216
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Hwang SJ, 2012, INT J COMPUT VISION, V100, P134, DOI 10.1007/s11263-011-0494-3
   Jia YQ, 2011, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2011.6126524
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Kingma D. P., 2014, arXiv
   Lai PL, 2000, IEEE IJCNN, P614
   Lawrence N, 2005, J MACH LEARN RES, V6, P1783
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li Y, 2018, PROC CVPR IEEE, P7492, DOI 10.1109/CVPR.2018.00782
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1234, DOI 10.1109/TMM.2016.2646180
   Liu H, 2017, PROC CVPR IEEE, P6345, DOI 10.1109/CVPR.2017.672
   Mandal D, 2019, IEEE T IMAGE PROCESS, V28, P102, DOI 10.1109/TIP.2018.2863040
   Manning Christopher D., 2008, INTRO INFORM RETRIEV
   Melzer T, 2003, PATTERN RECOGN, V36, P1961, DOI 10.1016/S0031-3203(03)00058-X
   Michaeli T, 2016, PR MACH LEARN RES, V48
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466
   Rao B.R., 1969, Trabajoso de Estadistica y de Investigacion operative, V20, P211
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Rasiwasia N, 2014, JMLR WORKSH CONF PRO, V33, P823
   Rosipal R, 2006, LECT NOTES COMPUT SC, V3940, P34, DOI 10.1007/11752790_2
   Rotman G, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P910
   Sargin ME, 2007, IEEE T MULTIMEDIA, V9, P1396, DOI 10.1109/TMM.2007.906583
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Song GL, 2017, IEEE T IMAGE PROCESS, V26, P4168, DOI 10.1109/TIP.2017.2713045
   Song GL, 2015, IEEE I CONF COMP VIS, P4050, DOI 10.1109/ICCV.2015.461
   Sun LA, 2011, IEEE T PATTERN ANAL, V33, P194, DOI 10.1109/TPAMI.2010.160
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Thiel C, 2008, LECT NOTES ARTIF INT, V5177, P65, DOI 10.1007/978-3-540-85563-7_14
   Uurtio V, 2019, PR MACH LEARN RES, V97
   Vinod HD, 1976, J ECONOMETRICS, V4, P147, DOI DOI 10.1016/0304-4076(76)90010-5
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang KY, 2016, IEEE T PATTERN ANAL, V38, P2010, DOI 10.1109/TPAMI.2015.2505311
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang SH, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1398, DOI 10.1145/3240508.3240535
   Wang W., 2016, PROC INT C LEARN REP
   Wang WR, 2015, PR MACH LEARN RES, V37, P1083
   Wang WR, 2015, ANN ALLERTON CONF, P688, DOI 10.1109/ALLERTON.2015.7447071
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu JL, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1663, DOI 10.1145/3269206.3269296
   Wu Q, 2018, IEEE T PATTERN ANAL, V40, P1367, DOI 10.1109/TPAMI.2017.2708709
   Wu YL, 2021, IEEE T MULTIMEDIA, V23, P559, DOI 10.1109/TMM.2020.2985540
   Xie Pengtao., 2013, Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, P1806
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Zhang Y, 2018, LECT NOTES COMPUT SC, V11205, P707, DOI 10.1007/978-3-030-01246-5_42
   Zhuang Y., 2013, P 27 AAAI C ART INT, P1070
NR 62
TC 8
Z9 8
U1 3
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1882
EP 1894
DI 10.1109/TMM.2020.3004963
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100005
DA 2024-07-18
ER

PT J
AU Tan, JP
   Shi, YK
   Yang, ZJ
   Wen, CZ
   Lin, L
AF Tan, Junpeng
   Shi, Yukai
   Yang, Zhijing
   Wen, Caizhen
   Lin, Liang
TI Unsupervised Multi-View Clustering by Squeezing Hybrid Knowledge From
   Cross View and Each View
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Sparse matrices; Clustering algorithms; Matrix decomposition; Feature
   extraction; Data mining; Adaptation models; Clustering methods;
   Multi-view clustering (MVC); low-rank; sparse subspace clustering (SSC);
   adaptive graph regularization (AGR)
ID LOW-RANK; GRAPH; REPRESENTATION; ALGORITHM
AB Multi-view clustering methods have been a focus in recent years because of their superiority in clustering performance. However, typical traditional multi-view clustering algorithms still have shortcomings in some aspects, such as removal of redundant information, utilization of various views and fusion of multi-view features. In view of these problems, this paper proposes a new multi-view clustering method, low-rank subspace multi-view clustering based on adaptive graph regularization. We construct two new data matrix decomposition models into a unified optimization model. In this framework, we address the significance of the common knowledge shared by the cross view and the unique knowledge of each view by presenting new low-rank and sparse constraints on the sparse subspace matrix. To ensure that we achieve effective sparse representation and clustering performance on the original data matrix, adaptive graph regularization and unsupervised clustering constraints are also incorporated in the proposed model to preserve the internal structural features of the data. Finally, the proposed method is compared with several state-of-the-art algorithms. Experimental results for five widely used multi-view benchmarks show that our proposed algorithm surpasses other state-of-the-art methods by a clear margin.
C1 [Tan, Junpeng; Shi, Yukai; Yang, Zhijing; Wen, Caizhen] Guangdong Univ Technol, Sch Informat Engn, Guangzhou 510006, Peoples R China.
   [Lin, Liang] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510275, Peoples R China.
C3 Guangdong University of Technology; Sun Yat Sen University
RP Shi, YK (corresponding author), Guangdong Univ Technol, Sch Informat Engn, Guangzhou 510006, Peoples R China.
EM tjeepnb@163.com; ykshi@gdut.edu.cn; yzhj@gdut.edu.cn;
   wcz93762@gmail.com; linliang@ieee.org
RI LU, LU/JEZ-4760-2023; L, J/JEF-9564-2023; Li, Li/IAQ-0885-2023; Lin,
   L/HKO-8213-2023; l, j/JVZ-8480-2024; Lin, Liang/IQR-8601-2023; Li,
   Jiaxi/HTS-3430-2023; zhang, cl/JDW-6549-2023; l, j/HNC-5728-2023
OI Lin, Liang/0000-0003-2248-3755; Li, Jiaxi/0000-0002-8197-8590; Shi,
   Yukai/0000-0002-9413-6528
FU Technology Project of Guangdong Province [2019A050513011,
   2017B090901056]; Guangzhou Science and Technology Plan Project
   [202002030386]; Guangdong Graduate Education Innovation Project
   [2020XSLT16]; National Nature Science Foundation of China [U1701266];
   Guangdong Provincial Key Laboratory of Intellectual Property and Big
   Data [2018B030322016]
FX This work was supported in part by Technology Project of Guangdong
   Province under Grants 2019A050513011, 2017B090901056, in part by
   Guangzhou Science and Technology Plan Project 202002030386, in part by
   Guangdong Graduate Education Innovation Project under Grant 2020XSLT16,
   in part by National Nature Science Foundation of China under Grant
   U1701266, and in part by Guangdong Provincial Key Laboratory of
   Intellectual Property and Big Data under Grant 2018B030322016.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Brbic M, 2020, IEEE T CYBERNETICS, V50, P1711, DOI 10.1109/TCYB.2018.2883566
   Brbic M, 2018, PATTERN RECOGN, V73, P247, DOI 10.1016/j.patcog.2017.08.024
   Cao QX, 2017, PROC CVPR IEEE, P1656, DOI 10.1109/CVPR.2017.180
   Cao XC, 2015, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2015.7298657
   Chen JK, 2018, IEEE T AFFECT COMPUT, V9, P38, DOI 10.1109/TAFFC.2016.2593719
   Ding ZM, 2018, IEEE T NEUR NET LEAR, V29, P1986, DOI 10.1109/TNNLS.2017.2690970
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Gao HC, 2015, IEEE I CONF COMP VIS, P4238, DOI 10.1109/ICCV.2015.482
   Hou CP, 2017, IEEE T KNOWL DATA EN, V29, P1998, DOI 10.1109/TKDE.2017.2681670
   Kumar A., 2011, P 28 INT C MACHINE L, P393
   Lee TS, 1996, IEEE T PATTERN ANAL, V18, P959, DOI 10.1109/34.541406
   Li CG, 2016, IEEE T SIGNAL PROCES, V64, P6557, DOI 10.1109/TSP.2016.2613070
   Li HR, 2020, SOFT COMPUT, V24, P6851, DOI 10.1007/s00500-019-04324-5
   Li XL, 2022, IEEE T PATTERN ANAL, V44, P330, DOI 10.1109/TPAMI.2020.3011148
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu CY, 2019, IEEE T PATTERN ANAL, V41, P487, DOI 10.1109/TPAMI.2018.2794348
   Lu G., IN PRESS
   Lu GF, 2016, IEEE T IMAGE PROCESS, V25, P2196, DOI 10.1109/TIP.2016.2542919
   Luo SR, 2018, AAAI CONF ARTIF INTE, P3730
   Ma C, 2019, IEEE T MULTIMEDIA, V21, P1169, DOI 10.1109/TMM.2018.2875512
   Nie FP, 2018, IEEE T IMAGE PROCESS, V27, P1501, DOI 10.1109/TIP.2017.2754939
   Nie FP, 2017, AAAI CONF ARTIF INTE, P2408
   Selesnick I, 2017, IEEE T SIGNAL PROCES, V65, P4481, DOI 10.1109/TSP.2017.2711501
   Shi YK, 2020, IEEE SIGNAL PROC LET, V27, P481, DOI 10.1109/LSP.2020.2978410
   Shi YK, 2020, IEEE T PATTERN ANAL, V42, P2809, DOI 10.1109/TPAMI.2019.2915301
   Shi Y, 2017, IEEE T MULTIMEDIA, V19, P2804, DOI 10.1109/TMM.2017.2711263
   Tang C, 2019, AAAI CONF ARTIF INTE, P5101
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   Tulsiani S, 2017, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2017.30
   Wang QJ, 2017, NEURAL COMPUT APPL, V28, P2293, DOI 10.1007/s00521-016-2189-8
   Wang XB, 2019, PATTERN RECOGN, V88, P50, DOI 10.1016/j.patcog.2018.09.009
   Wen J, 2020, IEEE T CYBERNETICS, V50, P1418, DOI 10.1109/TCYB.2018.2884715
   Wen J, 2018, NEURAL NETWORKS, V108, P83, DOI 10.1016/j.neunet.2018.08.007
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Xiaobo Wang, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P1, DOI 10.1109/CVPR.2017.8
   Xu C, 2015, IEEE T PATTERN ANAL, V37, P2531, DOI 10.1109/TPAMI.2015.2417578
   Xu JL, 2016, PROC CVPR IEEE, P5356, DOI 10.1109/CVPR.2016.578
   Xue Z, 2019, IEEE SIGNAL PROC LET, V26, P1177, DOI 10.1109/LSP.2019.2923857
   Yang SJ, 2019, IEEE T MULTIMEDIA, V21, P2916, DOI 10.1109/TMM.2019.2912735
   Yong JS, 2019, APPL MATH SER B, V34, P480, DOI 10.1007/s11766-019-3714-1
   Yuan GL, 2020, APPL NUMER MATH, V152, P1, DOI 10.1016/j.apnum.2020.01.019
   Zhan K, 2019, IEEE T KNOWL DATA EN, V31, P1984, DOI 10.1109/TKDE.2018.2872061
   Zhan K, 2019, IEEE T IMAGE PROCESS, V28, P1261, DOI 10.1109/TIP.2018.2877335
   Zhan K, 2018, IEEE T CYBERNETICS, V48, P2887, DOI 10.1109/TCYB.2017.2751646
   Zhan SH, 2019, NEURAL NETWORKS, V109, P56, DOI 10.1016/j.neunet.2018.10.001
   Zhang CQ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3054
   Zhang CQ, 2017, PROC CVPR IEEE, P4333, DOI 10.1109/CVPR.2017.461
   Zhang RM, 2019, IEEE T IMAGE PROCESS, V28, P4870, DOI 10.1109/TIP.2019.2911488
   Zhang SL, 2015, IEEE T MULTIMEDIA, V17, P265, DOI 10.1109/TMM.2015.2390044
   Zhang XQ, 2019, INFORM SCIENCES, V477, P430, DOI 10.1016/j.ins.2018.10.049
   Zhao HD, 2018, IEEE T IMAGE PROCESS, V27, P236, DOI 10.1109/TIP.2017.2754942
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
NR 53
TC 16
Z9 16
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2943
EP 2956
DI 10.1109/TMM.2020.3019683
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600032
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, R
   Wu, XJ
   Kittler, J
AF Wang, Rui
   Wu, Xiao-Jun
   Kittler, Josef
TI Graph Embedding Multi-Kernel Metric Learning for Image Set
   Classification With Grassmannian Manifold-Valued Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Manifolds; Measurement; Feature extraction; Kernel; Geometry; Emotion
   recognition; Data models; Image set classification; Grassmannian
   manifold; Feature extraction; Graph embedding multi-kernel metric
   learning
ID GEOMETRY
AB In the domain of video-based image set classification, a considerable advance has been made by modeling a sequence of video frames (image set) as a linear subspace, which typically resides on a Grassmannian manifold. As a consequence of the large intra-class variations of the video data, there are two open challenges for the modeling task: how to establish appropriate image set models to encode these variations, and how to effectively measure the similarity between any two image sets. As a possible way to tackle these issues, this paper presents a graph embedding multi-kernel metric learning (GEMKML) algorithm for image set classification. The proposed GEMKML implements set modeling, feature extraction, and classification in two steps. Firstly, the proposed framework constructs a novel cascaded feature learning architecture on Grassmannian manifold with the aim of producing more effective Grassmannian manifold-valued feature representations. To make a better use of these learned features, a graph embedding multi-kernel metric learning scheme is then devised to map them into a lower-dimensional Euclidean space, where the inter-class distances are maximized and the intra-class distances are minimized. We evaluate the proposed GEMKML on five different visual classification tasks using widely adopted datasets. The extensive classification results confirm its superiority over the state-of-the-art methods.
C1 [Wang, Rui; Wu, Xiao-Jun] Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Jiangsu, Peoples R China.
   [Wang, Rui; Wu, Xiao-Jun] Jiangnan Univ, Jiangsu Prov Engn Lab Pattern Recognit & Computat, Wuxi 214122, Jiangsu, Peoples R China.
   [Kittler, Josef] Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
C3 Jiangnan University; Jiangnan University; University of Surrey
RP Wu, XJ (corresponding author), Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Jiangsu, Peoples R China.
EM cs_wr@jiangnan.edu.cn; xiaojun_wu_jnu@163.com; j.kittler@surrey.ac.uk
OI Wu, Xiao-Jun/0000-0002-0310-5778; Kittler, Josef/0000-0002-8110-9205;
   Wang, Rui/0000-0002-9984-1752
FU National Key Research and Development Program of China [2017YFC1601800];
   National Natural Science Foundation of China [61672265, U1836218]; 111
   Project of Ministry of Education of China [B12018]; U.K. EPSRC
   [EP/N007743/1, EP/R018456/1]; EPSRC [EP/N007743/1, EP/R013616/1,
   EP/R018456/1] Funding Source: UKRI
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2017YFC1601800, in part by the
   National Natural Science Foundation of China under Grants 61672265 and
   U1836218, in part by the 111 Project of Ministry of Education of China
   under Grant B12018, and in part by the U.K. EPSRC (EP/N007743/1,
   MURI/EPSRC/DSTL, EP/R018456/1).
CR Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1
   [Anonymous], 2015, PROC CVPR IEEE
   Arsigny V, 2007, SIAM J MATRIX ANAL A, V29, P328, DOI 10.1137/050637996
   Baudat G, 2000, NEURAL COMPUT, V12, P2385, DOI 10.1162/089976600300014980
   Bregman L. M., 1967, USSR COMP MATH MATH, V7, P200, DOI [10.1016/0041-5553(67)90040-7, DOI 10.1016/0041-5553(67)90040-7]
   Cai HY, 2018, IEEE T KNOWL DATA EN, V30, P1616, DOI 10.1109/TKDE.2018.2807452
   Chen SK, 2013, PROC CVPR IEEE, P452, DOI 10.1109/CVPR.2013.65
   Cheng G, 2018, IEEE T IMAGE PROCESS, V27, P281, DOI 10.1109/TIP.2017.2760512
   Davis J. V., 2007, ICML, P209
   Dhall A., 2014, PROC ICMI, P461, DOI DOI 10.1145/2663204.2666275
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Garcia-Hernando G, 2018, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2018.00050
   Garcia-Hernando G, 2017, PROC CVPR IEEE, P407, DOI 10.1109/CVPR.2017.51
   Gonen M., 2008, ICML, P352, DOI DOI 10.1145/1390156.1390201
   Hamm Jihun, 2008, P 25 INT C MACH LEAR, P376, DOI DOI 10.1145/1390156.1390204
   Harandi M. T., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2705, DOI 10.1109/CVPR.2011.5995564
   Harandi M, 2018, IEEE T PATTERN ANAL, V40, P48, DOI 10.1109/TPAMI.2017.2655048
   Harandi M, 2015, IEEE I CONF COMP VIS, P4112, DOI 10.1109/ICCV.2015.468
   Harandi M, 2013, IEEE I CONF COMP VIS, P3120, DOI 10.1109/ICCV.2013.387
   Harandi MT, 2012, LECT NOTES COMPUT SC, V7573, P216, DOI 10.1007/978-3-642-33709-3_16
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Huang ZW, 2018, AAAI CONF ARTIF INTE, P3279
   Huang ZW, 2018, IEEE T PATTERN ANAL, V40, P2827, DOI 10.1109/TPAMI.2017.2776154
   Huang ZW, 2017, AAAI CONF ARTIF INTE, P2036
   Huang ZW, 2015, PROC CVPR IEEE, P140, DOI 10.1109/CVPR.2015.7298609
   Huang ZW, 2015, PATTERN RECOGN, V48, P3113, DOI 10.1016/j.patcog.2015.03.011
   Kim TS, 2017, IEEE COMPUT SOC CONF, P1623, DOI 10.1109/CVPRW.2017.207
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1234, DOI 10.1109/TMM.2016.2646180
   Lu JW, 2018, IEEE T PATTERN ANAL, V40, P1979, DOI 10.1109/TPAMI.2017.2737538
   Lu JW, 2015, PROC CVPR IEEE, P1137, DOI 10.1109/CVPR.2015.7298717
   Lu JW, 2013, IEEE I CONF COMP VIS, P329, DOI 10.1109/ICCV.2013.48
   Nguyen XS, 2019, PROC CVPR IEEE, P12028, DOI 10.1109/CVPR.2019.01231
   Rahmani H, 2016, PROC CVPR IEEE, P1506, DOI 10.1109/CVPR.2016.167
   Rao YM, 2019, INT J COMPUT VISION, V127, P701, DOI 10.1007/s11263-018-1135-x
   Shroff N, 2010, PROC CVPR IEEE, P1911, DOI 10.1109/CVPR.2010.5539864
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun HL, 2017, PROC CVPR IEEE, P6240, DOI 10.1109/CVPR.2017.661
   Tekin B, 2019, PROC CVPR IEEE, P4506, DOI 10.1109/CVPR.2019.00464
   Wang R., 2020, IEEE TRANS BIG DATA, DOI [10.1109/TB-DATA.2020.2982146, DOI 10.1109/TB-DATA.2020.2982146]
   Wang R. P., 2010, P C COMP VIS PATT RE, P2496
   Wang R, 2018, INT C PATT RECOG, P627, DOI 10.1109/ICPR.2018.8546030
   Wang W, 2015, PROC CVPR IEEE, P3395, DOI 10.1109/CVPR.2015.7298816
   Wiliem A, 2016, PATTERN RECOGN LETT, V80, P121, DOI 10.1016/j.patrec.2016.06.006
   Wu J, 2015, I C INTELL COMPUT TE, P726, DOI 10.1109/ICICTA.2015.184
   Wu XJ, 2004, PATTERN RECOGN, V37, P1949, DOI 10.1016/j.patcog.2003.07.014
   Wu ZF, 2015, IEEE T MULTIMEDIA, V17, P1960, DOI 10.1109/TMM.2015.2477681
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Zhang L, 2017, IEEE T MULTIMEDIA, V19, P1220, DOI 10.1109/TMM.2016.2646219
   Zhu WH, 2016, PROC INT CONF ANTI, P1, DOI 10.1109/ICASID.2016.7873885
NR 52
TC 26
Z9 26
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 228
EP 242
DI 10.1109/TMM.2020.2981189
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600018
DA 2024-07-18
ER

PT J
AU Xu, JH
   Tian, HD
   Wang, ZY
   Wang, Y
   Kang, WX
   Chen, F
AF Xu, Jiahao
   Tian, Hongda
   Wang, Zhiyong
   Wang, Yang
   Kang, Wenxiong
   Chen, Fang
TI Joint Input and Output Space Learning for Multi-Label Image
   Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Correlation; Task analysis; Semantics; Deep
   learning; Visualization; Benchmark testing; Multi-label image
   classification; label-specific feature; label correlations; graph
   convolutional network; deep learning
ID FEATURE-SELECTION
AB Multi-label image classification aims to predict the labels associated with a given image. While most existing methods utilize unified image representations, extracting label-specific features through input space learning would improve the discriminative power of the learned features. On the other hand, most feature learning studies often ignore the learning in the output label space, although taking advantage of label correlations can boost the classification performance. In this paper, we propose a deep learning framework that incorporates flexible modules which can learn from both input and output spaces for multi-label image classification. For the input space learning, we devise a label-specific feature pooling method to refine convolutional features for obtaining features specific to each label. For the output space learning, we design a Two-Stream Graph Convolutional Network (TSGCN) to learn multi-label classifiers by mapping spatial object relationships and semantic label correlations. More specifically, we build object spatial graphs to characterize the spatial relationships among objects in an image, which supplements the label semantic graphs modelling the semantic label correlations. Experimental results on two popular benchmark datasets (i.e., Pascal VOC and MS-COCO) show that our proposed method achieves superior performance over the state-of-the-arts.
C1 [Xu, Jiahao; Wang, Zhiyong] Univ Sydney, Sch Comp Sci, Sydney, NSW 2006, Australia.
   [Tian, Hongda; Wang, Zhiyong; Chen, Fang] Univ Technol Sydney, Sydney, NSW 2007, Australia.
   [Kang, Wenxiong] South China Univ Technol, Sch Automat Sci & Engn, Guangzhou 510641, Peoples R China.
C3 University of Sydney; University of Technology Sydney; South China
   University of Technology
RP Xu, JH (corresponding author), Univ Sydney, Sch Comp Sci, Sydney, NSW 2006, Australia.
EM jiahao.xu@sydney.edu.au; hongda.tian@uts.edu.au;
   zhiyong.wang@sydney.edu.au; yang.wang@uts.edu.au; auwxkang@scut.edu.cn;
   fang.chen@uts.edu.au
RI Tian, Hongda/AAD-4744-2020
OI Tian, Hongda/0000-0002-2889-6158; Kang, Wenxiong/0000-0001-9023-7252;
   Wang, Zhiyong/0000-0002-8043-0312; Wang, Yang/0000-0002-6815-0879; Chen,
   Fang/0000-0003-4971-8729
FU Data61-CISRO Postgraduate Scholarship
FX This work was supported by Data61-CISRO Postgraduate Scholarship.
CR [Anonymous], 2014, ICLR
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Chen SF, 2018, AAAI CONF ARTIF INTE, P6714
   Chen TS, 2018, AAAI CONF ARTIF INTE, P6730
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Ding XM, 2016, IEEE T MULTIMEDIA, V18, P1616, DOI 10.1109/TMM.2016.2572000
   Elisseeff A, 2002, ADV NEUR IN, V14, P681
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Feng J, 2017, AAAI CONF ARTIF INTE, P1884
   Ge WF, 2018, PROC CVPR IEEE, P1277, DOI 10.1109/CVPR.2018.00139
   He Kaiming, 2016, P INT C COMP VIS PAT, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Huang J, 2018, IEEE T CYBERNETICS, V48, P876, DOI 10.1109/TCYB.2017.2663838
   Huang J, 2015, IEEE DATA MINING, P181, DOI 10.1109/ICDM.2015.67
   Huang SJ, 2019, IEEE T PATTERN ANAL, V41, P2614, DOI 10.1109/TPAMI.2018.2861732
   Jalali Ali, 2010, ADV NEURAL INFORM PR, P964
   Jiang SY, 2019, IEEE ACCESS, V7, P10362, DOI 10.1109/ACCESS.2018.2889572
   Kim S, 2009, BIOINFORMATICS, V25, pI204, DOI 10.1093/bioinformatics/btp218
   Kong DG, 2012, PROC CVPR IEEE, P2352, DOI 10.1109/CVPR.2012.6247947
   Lee CW, 2018, PROC CVPR IEEE, P1576, DOI 10.1109/CVPR.2018.00170
   Li Q, 2016, PROC CVPR IEEE, P2977, DOI 10.1109/CVPR.2016.325
   Li X, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P430
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu D, 2011, IEEE T MULTIMEDIA, V13, P702, DOI 10.1109/TMM.2011.2134078
   Lu SY, 2014, IEEE T MULTIMEDIA, V16, P1497, DOI 10.1109/TMM.2014.2319778
   Luo Y, 2019, IEEE COMPUT SOC CONF, P820, DOI 10.1109/CVPRW.2019.00110
   Ma ZG, 2012, IEEE T MULTIMEDIA, V14, P1021, DOI 10.1109/TMM.2012.2187179
   Pan SR, 2017, IEEE T CYBERNETICS, V47, P744, DOI 10.1109/TCYB.2016.2526058
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Read J, 2008, IEEE DATA MINING, P995, DOI 10.1109/ICDM.2008.74
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34
   Vens C, 2008, MACH LEARN, V73, P185, DOI 10.1007/s10994-008-5077-3
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang M, 2016, IEEE T IMAGE PROCESS, V26, P5678, DOI 10.1109/TIP.2016.2612829
   Wang SF, 2015, IEEE T MULTIMEDIA, V17, P2185, DOI 10.1109/TMM.2015.2484966
   Wang ZX, 2017, IEEE I CONF COMP VIS, P464, DOI 10.1109/ICCV.2017.58
   Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929
   Xu JH, 2021, IEEE T CYBERNETICS, V51, P3510, DOI 10.1109/TCYB.2019.2909779
   Yan P., 2016, P JOINT EUR C MACH L, P540
   Yang H, 2016, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2016.37
   Yao YY, 2017, INFORM SCIENCES, V418, P601, DOI 10.1016/j.ins.2017.08.038
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
   Zhang ML, 2015, IEEE T PATTERN ANAL, V37, P107, DOI 10.1109/TPAMI.2014.2339815
   Zhang ML, 2009, INFORM SCIENCES, V179, P3218, DOI 10.1016/j.ins.2009.06.010
   Zhou Z.-H., 2007, P ADV NEUR INF PROC, P1609, DOI DOI 10.1016/J.PATCOG.2006.12.019
   Zhou ZH, 2012, ARTIF INTELL, V176, P2291, DOI 10.1016/j.artint.2011.10.002
   Zhu F, 2017, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2017.219
NR 48
TC 27
Z9 27
U1 5
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1696
EP 1707
DI 10.1109/TMM.2020.3002185
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300017
DA 2024-07-18
ER

PT J
AU Yang, F
   Wu, Y
   Wang, Z
   Li, X
   Sakti, S
   Nakamura, S
AF Yang, Fan
   Wu, Yang
   Wang, Zheng
   Li, Xiang
   Sakti, Sakriani
   Nakamura, Satoshi
TI Instance-Level Heterogeneous Domain Adaptation for Limited-Labeled
   Sketch-to-Photo Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Training data; Data models; Image retrieval; Connectors;
   Training; Entropy; Domain adaptation; cross-modal image retrieval;
   sketch; person re-identification
ID REIDENTIFICATION; ATTRIBUTE
AB Although sketch-to-photo retrieval has a wide range of applications, it is costly to obtain paired and rich-labeled ground truth. Differently, photo retrieval data is easier to acquire. Therefore, previous works pre-train their models on rich-labeled photo retrieval data (i.e., source domain) and then fine-tune them on the limited-labeled sketch-to-photo retrieval data (i.e., target domain). However, without co-training source and target data, source domain knowledge might be forgotten during the fine-tuning process, while simply co-training them may cause negative transfer due to domain gaps. Moreover, identity label spaces of source data and target data are generally disjoint and therefore conventional category-level Domain Adaptation (DA) is not directly applicable. To address these issues, we propose an Instance-level Heterogeneous Domain Adaptation (IHDA) framework. We apply the fine-tuning strategy for identity label learning, aiming to transfer the instance-level knowledge in an inductive transfer manner. Meanwhile, labeled attributes from the source data are selected to form a shared label space for source and target domains. Guided by shared attributes, DA is utilized to bridge cross-dataset domain gaps and heterogeneous domain gaps, which transfers instance-level knowledge in a transductive transfer manner. Experiments show that our method has set a new state of the art on three sketch-to-photo image retrieval benchmarks without extra annotations, which opens the door to train more effective models on limited-labeled heterogeneous image retrieval tasks.
C1 [Yang, Fan; Wu, Yang; Sakti, Sakriani; Nakamura, Satoshi] Nara Inst Sci & Technol, Nara 6300192, Japan.
   [Yang, Fan; Sakti, Sakriani; Nakamura, Satoshi] RIKEN Ctr Adv Intelligence Project AIP, Nara 6300192, Japan.
   [Wu, Yang] Kyoto Univ, Comp Vis Lab, Kyoto 6068501, Japan.
   [Wang, Zheng] Natl Inst Informat, Digital Content & Media Sci Res Div, Chiyoda Ku, Tokyo 1018430, Japan.
   [Li, Xiang] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 Nara Institute of Science & Technology; RIKEN; Kyoto University;
   Research Organization of Information & Systems (ROIS); National
   Institute of Informatics (NII) - Japan; Nanyang Technological University
RP Wang, Z (corresponding author), Natl Inst Informat, Digital Content & Media Sci Res Div, Chiyoda Ku, Tokyo 1018430, Japan.
EM yang.fan.xv6@is.naist.jp; wu.yang.8c@kyoto-u.ac.jp; wangz@nii.ac.jp;
   me@shawnli.me; ssakti@is.naist.jp; s-nakantura@is.naist.jp
OI Yang, Fan/0000-0001-7185-5688; Sakti, Sakriani/0000-0001-5509-8963
FU JSPS KAKENHI [JP17H06101]; MSRA Collaborative Research 2019 Grant -
   Microsoft Research Asia
FX This work was supported in part by JSPS KAKENHI under Grant JP17H06101
   and in part by a MSRA Collaborative Research 2019 Grant Awarded by
   Microsoft Research Asia. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Lei Zhang.
CR Aljundi R., 2018, P EUR C COMP VIS ECC, P139
   [Anonymous], 2017, ADV NEURAL INF PROCE
   Bhatt HS, 2012, IEEE T INF FOREN SEC, V7, P1522, DOI 10.1109/TIFS.2012.2204252
   Cao ZJ, 2018, LECT NOTES COMPUT SC, V11212, P139, DOI 10.1007/978-3-030-01237-3_9
   Chechik G, 2010, J MACH LEARN RES, V11, P1109
   Choi J, 2019, IEEE T MULTIMEDIA, V21, P2083, DOI 10.1109/TMM.2019.2892301
   Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Das Bhattacharjee S, 2018, IEEE T MULTIMEDIA, V20, P2761, DOI 10.1109/TMM.2018.2814338
   Deng ZY, 2019, AAAI CONF ARTIF INTE, P8239
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185527
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gebru T, 2017, IEEE I CONF COMP VIS, P1358, DOI 10.1109/ICCV.2017.151
   Grandvalet Y, 2004, Advances in neural information processing systems, V17
   Guyon I., 2003, Journal of Machine Learning Research, V3, P1157, DOI 10.1162/153244303322753616
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kingma D. P., 2014, arXiv
   Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591
   Li H., 2018, P BRIT MACH VIS C, P19
   Li H., 2020, P INT C LEARN REPR
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Liu DC, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P835
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Long M., 2017, Proc Mach Learn Res, V70, P2208
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Luo Z., 2017, Advances in Neural Information Processing Systems, P165
   Ouyang SX, 2016, PROC CVPR IEEE, P5571, DOI 10.1109/CVPR.2016.601
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pang L, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P609, DOI 10.1145/3240508.3240606
   Pei ZY, 2018, AAAI CONF ARTIF INTE, P3934
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   Rozantsev A, 2019, IEEE T PATTERN ANAL, V41, P801, DOI 10.1109/TPAMI.2018.2814042
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saito K, 2019, IEEE I CONF COMP VIS, P8049, DOI 10.1109/ICCV.2019.00814
   Saito K, 2017, PR MACH LEARN RES, V70
   Sangkloy P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925954
   Shu Y, 2019, AAAI CONF ARTIF INTE, V33, P4951
   Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8
   Song J., 2016, P BRIT MACH VIS C
   Song JF, 2017, IEEE I CONF COMP VIS, P5552, DOI 10.1109/ICCV.2017.592
   Song LC, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2019.107173
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang JD, 2017, IEEE DATA MINING, P1129, DOI 10.1109/ICDM.2017.150
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wang M, 2018, NEUROCOMPUTING, V312, P135, DOI 10.1016/j.neucom.2018.05.083
   Wang S, 2015, IEEE T MULTIMEDIA, V17, P1045, DOI 10.1109/TMM.2015.2431492
   Wang Z, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4973
   Wang Z, 2020, IEEE T IMAGE PROCESS, V29, P2013, DOI 10.1109/TIP.2019.2946975
   Wang Z, 2019, IEEE T MULTIMEDIA, V21, P2376, DOI 10.1109/TMM.2019.2898753
   Wang Z, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P72, DOI 10.1145/3240508.3240510
   Wang Z, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1267, DOI 10.1145/2733373.2806400
   Wu X, 2018, AAAI CONF ARTIF INTE, P1679
   Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032
   Yu A, 2017, IEEE I CONF COMP VIS, P5571, DOI 10.1109/ICCV.2017.594
   Yu Q, 2016, PROC CVPR IEEE, P799, DOI 10.1109/CVPR.2016.93
   Zeng ZL, 2020, IEEE T MULTIMEDIA, V22, P3064, DOI 10.1109/TMM.2020.2969782
   Zhang J, 2017, PROC CVPR IEEE, P5150, DOI 10.1109/CVPR.2017.547
   Zhang YT, 2016, IEEE T MULTIMEDIA, V18, P1604, DOI 10.1109/TMM.2016.2568138
   Zhao H, 2019, PR MACH LEARN RES, V97
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zou H, 2019, AAAI CONF ARTIF INTE, P5997
   Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI [10.1007/978-3-030-01219-9_, 10.1007/978-3-030-01219-9_18]
NR 63
TC 18
Z9 20
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2347
EP 2360
DI 10.1109/TMM.2020.3009476
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800015
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Yang, S
   Hu, YY
   Yang, WH
   Duan, LY
   Liu, JY
AF Yang, Shuai
   Hu, Yueyu
   Yang, Wenhan
   Duan, Ling-Yu
   Liu, Jiaying
TI Towards Coding for Human and Machine Vision: Scalable Face Image Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Machine vision; Task analysis; Image reconstruction;
   Visualization; Feature extraction; Image color analysis; Generative
   compression; image coding; scalable coding; video coding for machine
ID COMPRESSION
AB The past decades have witnessed the rapid development of image and video coding techniques in the era of big data. However, the signal fidelity-driven coding pipeline design limits the capability of the existing image/video coding frameworks to fulfill the needs of both machine and human vision. In this paper, we come up with a novel face image coding framework by leveraging both the compressive and the generative models, to support machine vision and human perception tasks jointly. Given an input image, the feature analysis is first applied, and then the generative model is employed to reconstruct image with compact structure and color features, where sparse edges are extracted to connect both kinds of vision and a key reference pixel selection method is proposed to determine the priorities of the reference color pixels for scalable coding. The compact edge map serves as the basic layer for machine vision tasks, and the reference pixels act as an enhanced layer to guarantee signal fidelity for human vision. By introducing advanced generative models, we train a decoding network to reconstruct images from compact structure and color representations, which is flexible to accept inputs in a scalable way and to control the imagery effect of the outputs between signal fidelity and visual realism. Experimental results and comprehensive performance analysis over the face image dataset demonstrate the superiority of our framework in both human vision tasks and machine vision tasks, which provide useful evidence on the emerging standardization efforts on MPEG VCM (Video Coding for Machine).
C1 [Yang, Shuai; Hu, Yueyu; Yang, Wenhan; Duan, Ling-Yu; Liu, Jiaying] Peking Univ, Beijing 100080, Peoples R China.
C3 Peking University
RP Liu, JY (corresponding author), Peking Univ, Beijing 100080, Peoples R China.
EM williamyang@pku.edu.cn; huyy@pku.edu.cn; yangwenhan@pku.edu.cn;
   lingyu@pku.edu.cn; liujiaying@pku.edu.cn
RI Liu, JY/GYJ-0138-2022
OI Hu, Yueyu/0000-0003-4919-4515; Liu, Jiaying/0000-0002-0468-9576
FU National Key Research and Development Program of China [2018AAA0102702];
   Fundamental Research Funds for the Central Universities; National
   Natural Science Foundation of China [61772043]
FX This work was supported in part by theNationalKey Research and
   Development Program of China under Grant 2018AAA0102702, in part by the
   Fundamental Research Funds for the Central Universities and in part by
   the National Natural Science Foundation of China under Contract
   61772043. The guest editor coordinating the review of this manuscript
   and approving it for publication was Dr. M. Mrak.
CR Alakuijala J, 2019, ACM T INFORM SYST, V37, DOI 10.1145/3231935
   Alvar SR, 2020, INT CONF ACOUST SPEE, P4342, DOI [10.1109/icassp40776.2020.9054770, 10.1109/ICASSP40776.2020.9054770]
   Alvar SR, 2019, IEEE IMAGE PROC, P1705, DOI [10.1109/ICIP.2019.8803110, 10.1109/icip.2019.8803110]
   Balle J, 2017, 5 INT C LEARN REPR I
   Balle J., 2018, INT C LEARN REPR ICL, P1
   Bragilevsky L, 2020, IEEE ACCESS, V8, P41162, DOI 10.1109/ACCESS.2020.2977050
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chang JH, 2019, IEEE IMAGE PROC, P694, DOI [10.1109/ICIP.2019.8803805, 10.1109/icip.2019.8803805]
   Choi H, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1792, DOI 10.1109/ICASSP.2018.8462653
   Christopoulos C, 2000, IEEE T CONSUM ELECTR, V46, P1103, DOI 10.1109/30.920468
   CLEARY JG, 1984, IEEE T COMMUN, V32, P396, DOI 10.1109/TCOM.1984.1096090
   Cohen R. A., 2020, 2020 IEEE International Conference on Multimedia and Expo (ICME), P1
   Cristinacce D., 2006, BRIT MACH VIS C, V1, P3
   Dekel T, 2018, PROC CVPR IEEE, P3511, DOI 10.1109/CVPR.2018.00370
   Ding K., 2020, IMAGE QUALITY ASSESS
   Dollár P, 2013, IEEE I CONF COMP VIS, P1841, DOI 10.1109/ICCV.2013.231
   Duan LY, 2019, IEEE MULTIMEDIA, V26, P44, DOI 10.1109/MMUL.2018.2873844
   Duan LY, 2016, IEEE T IMAGE PROCESS, V25, P179, DOI 10.1109/TIP.2015.2500034
   Duan LY, 2020, IEEE T IMAGE PROCESS, V29, P8680, DOI 10.1109/TIP.2020.3016485
   Elad M, 2007, IEEE T IMAGE PROCESS, V16, P2379, DOI 10.1109/TIP.2007.903259
   Erol B, 2000, IEEE T MULTIMEDIA, V2, P129, DOI 10.1109/6046.845016
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gregor K, 2016, ADV NEUR IN, V29
   He TY, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954525
   Hensel M, 2017, ADV NEUR IN, V30
   Hu Y., 2020, P IEEE C MULT EXP, P1
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Khosla A, 2015, IEEE I CONF COMP VIS, P2390, DOI 10.1109/ICCV.2015.275
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   McPhee S, 2020, NVIDIA ANNOUNCES CLO
   Meier T, 1998, IEEE T CIRC SYST VID, V8, P525, DOI 10.1109/76.718500
   Miyato T., 2018, Proceedings of the 6th International Conference on Learning Representations, P1
   Nazeri K, 2019, IEEE INT CONF COMP V, P3265, DOI 10.1109/ICCVW.2019.00408
   Ono S, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2299067
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Rabbat R, 2010, WebP, a New Image Format for the Web
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Serengil S.I., 2020, 2020 INNOVATIONS INT, P1, DOI DOI 10.1109/ASYU50717.2020.9259802
   Shuai Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P601, DOI 10.1007/978-3-030-58555-6_36
   Simonyan K, 2015, IEEE INT C ICLR
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Theis L., 2017, ICLR
   Torfason R., 2018, P INT C LEARN REPR, P1
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang SR, 2019, IEEE IMAGE PROC, P2691, DOI [10.1109/ICIP.2019.8803255, 10.1109/icip.2019.8803255]
   Wang XT, 2019, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2019.00179
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weber M, 2004, AUTOTRACE PROGRAM CO
   Xia SF, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102843
   Xiong B, 2011, IEEE T CIRC SYST VID, V21, P917, DOI 10.1109/TCSVT.2011.2133530
   Xu JZ, 2016, IEEE T CIRC SYST VID, V26, P50, DOI 10.1109/TCSVT.2015.2478706
   Xu SX, 2021, IEEE T CIRC SYST VID, V31, P1308, DOI 10.1109/TCSVT.2020.3001267
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu JY, 2017, ADV NEUR IN, V30
NR 61
TC 28
Z9 29
U1 2
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2957
EP 2971
DI 10.1109/TMM.2021.3068580
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000001
DA 2024-07-18
ER

PT J
AU Yao, XX
   She, DY
   Zhang, HW
   Yang, JF
   Cheng, MM
   Wang, L
AF Yao, Xingxu
   She, Dongyu
   Zhang, Haiwei
   Yang, Jufeng
   Cheng, Ming-Ming
   Wang, Liang
TI Adaptive Deep Metric Learning for Affective Image Retrieval and
   Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Visualization; Semantics; Feature extraction; Task
   analysis; Image analysis; Image retrieval; Affective image retrieval;
   convolutional neural network; deep metric learning; visual sentiment
   analysis
ID REPRESENTATIONS; SIMILARITY; FEATURES
AB An image is worth a thousand words. Many researchers have conducted extensive studies to understand visual emotions since an increasing number of users express emotions via images and videos online. However, most existing methods based on convolutional neural networks aim to retrieve and classify affective images in a discrete label space while ignoring both the hierarchical and complex nature of emotions. On the one hand, different from concrete and isolated object concepts (e.g., cat and dog), a hierarchical relationship exists among emotions. On the other hand, most widely used deep methods depend on the representation from fully connected layers, which lacks the essential texture information for recognizing emotions. In this work, we address the above problems via adaptive deep metric learning. Specifically, we design an adaptive sentiment similarity loss, which is able to embed affective images considering the emotion polarity and adaptively adjust the margin between different image pairs. To effectively distinguish affective images, we further propose the sentiment vector that captures the texture information extracted from multiple convolutional layers. Finally, we develop a unified multi-task deep framework to simultaneously optimize both retrieval and classification goals. Extensive and thorough evaluations on four benchmark datasets demonstrate that the proposed framework performs favorably against the state-of-the-art methods.
C1 [Yao, Xingxu; She, Dongyu; Zhang, Haiwei; Yang, Jufeng; Cheng, Ming-Ming] Nankai Univ, Coll Comp Sci, Tianjin 300350, Peoples R China.
   [Wang, Liang] Chinese Acad Sci, Inst Automat, CAS Ctr Excellence Brain Sci & Intelligence Techn, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
C3 Nankai University; Chinese Academy of Sciences; Institute of Automation,
   CAS
RP Zhang, HW (corresponding author), Nankai Univ, Coll Comp Sci, Tianjin 300350, Peoples R China.
EM yxx_hbgd@163.com; sherry6656@163.com; zhhaiwei@nankai.edu.cn;
   yangjufeng@nankai.edu.cn; cmm@nankai.edu.cn; wangliang@nlpr.ia.ac.cn
RI Cheng, Ming-Ming/A-2527-2009
OI Cheng, Ming-Ming/0000-0001-5550-8758; Zhang, Haiwei/0000-0002-5852-0426
FU Major Project for New Generation of AI [2018AAA0100403]; NSFC [61876094,
   U1933114]; Natural Science Foundation of Tianjin, China [18JCYBJC15400,
   18ZXZNGX00110]; Open Project Program of the National Laboratory of
   Pattern Recognition (NLPR); Fundamental Research Funds for the Central
   Universities
FX This work was supported in part by theMajor Project for New Generation
   of AI underGrant NO.2018AAA0100403, NSFC under Grants 61876094,
   U1933114, in part by the Natural Science Foundation of Tianjin, China
   under Grants 18JCYBJC15400, 18ZXZNGX00110, in part by the Open Project
   Program of the National Laboratory of Pattern Recognition (NLPR), and in
   part by the Fundamental Research Funds for the Central Universities.
CR [Anonymous], FEBS LETT
   Bai S, 2017, AAAI CONF ARTIF INTE, P3967
   Bellet A., 2013, arXiv
   Borth D., 2013, P 21 ACM INT C MULT, P459
   Cakir F, 2019, PROC CVPR IEEE, P1861, DOI 10.1109/CVPR.2019.00196
   Chechik G, 2010, J MACH LEARN RES, V11, P1109
   Chen T., 2014, DeepSentiBank: Visual Sentiment Concept Classification with Deep Convolutional Neural Networks
   Chen T, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P367, DOI 10.1145/2647868.2654935
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Cui Y, 2016, PROC CVPR IEEE, P1153, DOI 10.1109/CVPR.2016.130
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng C, 2019, IEEE T IMAGE PROCESS, V28, P4032, DOI 10.1109/TIP.2019.2903661
   Deng C, 2019, IEEE T GEOSCI REMOTE, V57, P1741, DOI 10.1109/TGRS.2018.2868851
   Deng C, 2018, PATTERN RECOGN, V77, P306, DOI 10.1016/j.patcog.2017.10.007
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Detenber BH, 1998, J BROADCAST ELECTRON, V42, P113
   Duan YQ, 2018, PROC CVPR IEEE, P2780, DOI 10.1109/CVPR.2018.00294
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   Fan SJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P217, DOI 10.1145/3123266.3123445
   Fan SJ, 2018, PROC CVPR IEEE, P7521, DOI 10.1109/CVPR.2018.00785
   Ge WF, 2018, LECT NOTES COMPUT SC, V11210, P272, DOI 10.1007/978-3-030-01231-1_17
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8
   Harwood B, 2017, IEEE I CONF COMP VIS, P2840, DOI 10.1109/ICCV.2017.307
   He XW, 2018, PROC CVPR IEEE, P1945, DOI 10.1109/CVPR.2018.00208
   He YW, 2020, NEURAL PROCESS LETT, V51, P2077, DOI 10.1007/s11063-019-10035-7
   He Zhang, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P166, DOI 10.1007/978-3-642-42051-1_22
   Hermans Alexander, 2017, ARXIV170307737
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Idrissa M, 2002, PATTERN RECOGN LETT, V23, P1095, DOI 10.1016/S0167-8655(02)00056-9
   Jou B, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P159, DOI 10.1145/2733373.2806246
   Kim HR, 2018, IEEE T MULTIMEDIA, V20, P2980, DOI 10.1109/TMM.2018.2827782
   Kim S, 2019, PROC CVPR IEEE, P2283, DOI 10.1109/CVPR.2019.00239
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lang P.J., 1997, INT AFFECTIVE PICTUR, P39, DOI DOI 10.1027/0269-8803/A000147
   Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762
   Liu X, 2019, J VIS COMMUN IMAGE R, V58, P576, DOI 10.1016/j.jvcir.2018.12.032
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Lu Xin, 2012, Proc ACM Int Conf Multimed, V2012, P229, DOI 10.1145/2393347.2393384
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Machajdik J., 2010, P 18 ACM INT C MULT, P83, DOI DOI 10.1145/1873951.1873965
   Mikels JA, 2005, BEHAV RES METHODS, V37, P626, DOI 10.3758/BF03192732
   Nicolaou M.A., 2011, Proceedings of the 19th ACM international conference on Multimedia, P933
   Panda R, 2018, LECT NOTES COMPUT SC, V11206, P594, DOI 10.1007/978-3-030-01216-8_36
   Qian Q, 2019, IEEE I CONF COMP VIS, P6459, DOI 10.1109/ICCV.2019.00655
   Qian Shengsheng, 2016, P 24 ACM INT C MULTI
   Ragusa E, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8070783
   Rao TR, 2020, NEURAL PROCESS LETT, V51, P2043, DOI 10.1007/s11063-019-10033-9
   Rao TR, 2019, NEUROCOMPUTING, V333, P429, DOI 10.1016/j.neucom.2018.12.053
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sartori A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P311, DOI 10.1145/2733373.2806250
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sohn K, 2016, ADV NEUR IN, V29
   Solli M, 2009, LECT NOTES COMPUT SC, V5702, P573, DOI 10.1007/978-3-642-03767-2_70
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang WN, 2008, IEEE IMAGE PROC, P117, DOI 10.1109/ICIP.2008.4711705
   Wang XH, 2013, IEEE IMAGE PROC, P3230, DOI 10.1109/ICIP.2013.6738665
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu BY, 2017, IEEE T MULTIMEDIA, V19, P1670, DOI 10.1109/TMM.2017.2655881
   Xiong HT, 2019, AAAI CONF ARTIF INTE, P363
   Xu C., 2014, Visual sentiment prediction with deep convolutional neural networks
   Yang E, 2018, IEEE T NEUR NET LEAR, V29, P5292, DOI 10.1109/TNNLS.2018.2793863
   Yang JF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3266
   Yang JF, 2018, AAAI CONF ARTIF INTE, P491
   Yang JF, 2018, PROC CVPR IEEE, P7584, DOI 10.1109/CVPR.2018.00791
   Yang JF, 2018, IEEE T MULTIMEDIA, V20, P2513, DOI 10.1109/TMM.2018.2803520
   Yanulevskaya V, 2008, IEEE IMAGE PROC, P101, DOI 10.1109/ICIP.2008.4711701
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   You QZ, 2016, AAAI CONF ARTIF INTE, P308
   You QZ, 2015, AAAI CONF ARTIF INTE, P381
   Yuan J., 2013, P 2 INT WORKSH ISS S, P1
   Zhao SC, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5534
   Zhao SC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1319, DOI 10.1145/3240508.3240591
   Zhao SC, 2019, AAAI CONF ARTIF INTE, P2620
   Zhao SC, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4669
   Zhao SC, 2017, IEEE T MULTIMEDIA, V19, P632, DOI 10.1109/TMM.2016.2617741
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P47, DOI 10.1145/2647868.2654930
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1025, DOI 10.1145/2647868.2655035
   Zhao Sicheng, 2016, P 24 ACM INT C MULT, P1385, DOI DOI 10.1145/2964284.2964289
   Zheng WZ, 2019, PROC CVPR IEEE, P72, DOI 10.1109/CVPR.2019.00016
   Zhu XG, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3595
NR 88
TC 34
Z9 37
U1 3
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1640
EP 1653
DI 10.1109/TMM.2020.3001527
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300013
DA 2024-07-18
ER

PT J
AU Zhang, YP
   Ma, B
   Wu, JH
   Huang, LH
   Shen, JB
AF Zhang, Yuping
   Ma, Bo
   Wu, Jiahao
   Huang, Lianghua
   Shen, Jianbing
TI Capturing Relevant Context for Visual Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Local neighborhood graph; long-range dependencies; long-term tracking;
   visual object tracking
ID OBJECT TRACKING; NETWORKS
AB Studies have shown that contextual information can promote the robustness of trackers. However, trackers based on convolutional neural networks (CNNs) only capture local features, which limits their performance. We propose a novel relevant context block (RCB), which employs graph convolutional networks to capture the relevant context. In particular, it selects the k largest contributors as nodes for each query position (unit) that contain meaningful and discriminative contextual information and updates the nodes by aggregating the differences between the query position and its contributors. This operation can be easily incorporated into the existing networks and can be easily end-to-end trained using a standard backpropagation algorithm. To verify the effectiveness of RCB, we apply it to two trackers, SiamFC and GlobalTrack, respectively, and the two improved trackers are referred to as Siam-RCB andGlobalTrack-RCB. Extensive experiments onOTB, VOT, UAV123, LaSOT, TrackingNet, OxUvA, and VOT2018LT show the superiority of our method. For example, our Siam-RCB outperforms SiamFC by a very large margin (up to 11.2% in the success score and 7.8% in the precision score) on the OTB-100 benchmark.
C1 [Zhang, Yuping; Ma, Bo; Wu, Jiahao] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
   [Huang, Lianghua] Chinese Acad Sci, Ctr Res Intelligent Syst & Engn, Inst Automat, Beijing 100190, Peoples R China.
   [Huang, Lianghua] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Shen, Jianbing] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.
C3 Beijing Institute of Technology; Chinese Academy of Sciences; Institute
   of Automation, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS
RP Ma, B (corresponding author), Beijing Inst Technol, Sch Comp Sci & Technol, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
EM zhangyuping@bit.edu.cn; bma000@bit.edu.cn; wujiahao@bit.edu.cn;
   huanglianghua2017@ia.ac.cn; shenjianbingcg@gmail.com
RI Li, Li/IAQ-0885-2023; Zhang, Yuting/JRW-3937-2023; ZHANG,
   YUTING/HOH-4131-2023
OI Huang, Lianghua/0000-0002-9686-9354
FU National Key Research and Development Program of China [2020YFC0832502];
   National Natural Science Foundation of China [62072042, 61961015]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020YFC0832502, and in part by
   the National Natural Science Foundation of China under Grants 62072042
   and 61961015.
CR [Anonymous], 2016, Software available from tensorflow org
   Baisa NL, 2017, PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2017), VOL 6, P192, DOI 10.5220/0006117301920203
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2018, LECT NOTES COMPUT SC, V11206, P493, DOI 10.1007/978-3-030-01216-8_30
   Bruna J., 2013, INT C LEARNING REPRE
   Cao Y., 2019, CORR, P1, DOI [DOI 10.1109/ICCVW.2019.00246, 10.1109/ICCVW.2019.00246]
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Dai KN, 2020, PROC CVPR IEEE, P6297, DOI 10.1109/CVPR42600.2020.00633
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Defferrard M, 2016, ADV NEUR IN, V29
   Dong XP, 2019, IEEE T IMAGE PROCESS, V28, P3516, DOI 10.1109/TIP.2019.2898567
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Fan H, 2019, PROC CVPR IEEE, P7944, DOI 10.1109/CVPR.2019.00814
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fan H, 2017, IEEE COMPUT SOC CONF, P2217, DOI 10.1109/CVPRW.2017.275
   Fiaz M., 2018, ARXIV180203098
   Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   Hamilton WL, 2017, ADV NEUR IN, V30
   He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hu HW, 2019, IEEE T MULTIMEDIA, V21, P510, DOI 10.1109/TMM.2018.2859831
   Huang LH, 2020, AAAI CONF ARTIF INTE, V34, P11037
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kipf TN, 2017, INT C LEARN REPR
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Kristan M, 2017, IEEE INT CONF COMP V, P1949, DOI 10.1109/ICCVW.2017.230
   Kristan M, 2016, IEEE T PATTERN ANAL, V38, P2137, DOI 10.1109/TPAMI.2016.2516982
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li PX, 2019, IEEE I CONF COMP VIS, P6161, DOI 10.1109/ICCV.2019.00626
   Li PX, 2018, PATTERN RECOGN, V76, P323, DOI 10.1016/j.patcog.2017.11.007
   Liang ZY, 2020, IEEE T IMAGE PROCESS, V29, P3351, DOI 10.1109/TIP.2019.2959256
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Luo WJ, 2016, ADV NEUR IN, V29
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Marvasti-Zadeh S. Mojtaba, 2019, ARXIV191200535
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Müller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Senna P, 2017, SIBGRAPI, P338, DOI 10.1109/SIBGRAPI.2017.51
   Shen JB, 2020, IEEE T CYBERNETICS, V50, P3068, DOI 10.1109/TCYB.2019.2936503
   Shuai B, 2016, PROC CVPR IEEE, P3620, DOI 10.1109/CVPR.2016.394
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Vojir T, 2014, PATTERN RECOGN LETT, V49, P250, DOI 10.1016/j.patrec.2014.03.025
   Wang T, 2018, IEEE T PATTERN ANAL, V40, P1494, DOI 10.1109/TPAMI.2017.2716350
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yan B, 2019, IEEE I CONF COMP VIS, P2385, DOI 10.1109/ICCV.2019.00247
   Yang M, 2009, IEEE T PATTERN ANAL, V31, P1195, DOI 10.1109/TPAMI.2008.146
   Yang TY, 2018, LECT NOTES COMPUT SC, V11213, P153, DOI 10.1007/978-3-030-01240-3_10
   Yao L, 2019, AAAI CONF ARTIF INTE, P7370
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang Y., 2018, STRUCTURED SIAMESE N, P355
   Zhang Yunhua, 2018, ARXIV180904320
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
   Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7
   Zhu Z, 2017, IEEE INT CONF COMP V, P1973, DOI 10.1109/ICCVW.2017.231
NR 72
TC 9
Z9 9
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4232
EP 4244
DI 10.1109/TMM.2020.3038310
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900025
DA 2024-07-18
ER

PT J
AU Bui, T
   Cooper, D
   Collomosse, J
   Bell, M
   Green, A
   Sheridan, J
   Higgins, J
   Das, A
   Keller, JR
   Thereaux, O
AF Bui, Tu
   Cooper, Daniel
   Collomosse, John
   Bell, Mark
   Green, Alex
   Sheridan, John
   Higgins, Jez
   Das, Arindra
   Keller, Jared Robert
   Thereaux, Olivier
TI Tamper-Proofing Video With Hierarchical Attention Autoencoder Hashing on
   Blockchain
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Blockchain; Feature extraction; Visualization; Distributed ledger; Task
   analysis; Transcoding; Streaming media; Distributed Ledger Technology;
   content aware hashing; autoencoder; LSTM; attention network; content
   integrity; blockchain
ID MODEL
AB We present ARCHANGEL; a novel distributed ledger based system for assuring the long-term integrity of digital video archives. First, we introduce a novel deep network architecture using a hierarchical attention autoencoder (HAAE) to compute temporal content hashes (TCHs) from minutes or hour-long audio-visual streams. Our TCHs are sensitive to accidental or malicious content modification (tampering). The focus of our self-supervised HAAE is to guard against content modification such as frame truncation or corruption but ensure invariance against format shift (i.e. codec change). This is necessary due to the curatorial requirement for archives to format shift video over time to ensure future accessibility. Second, we describe how the TCHs (and the models used to derive them) are secured via a proof-of-authority blockchain distributed across multiple independent archives. We report on the efficacy of ARCHANGEL within the context of a trial deployment in which the national government archives of the United Kingdom, United States of America, Estonia, Australia and Norway participated.
C1 [Bui, Tu; Cooper, Daniel] Univ Surrey, Ctr Vis Speech & Signal Proc CVSSP, Guildford GU2 7XH, Surrey, England.
   [Collomosse, John] Univ Surrey, Guildford GU2 7XH, Surrey, England.
   [Collomosse, John] Adobe Res, San Jose, CA 95110 USA.
   [Bell, Mark; Green, Alex; Sheridan, John] Natl Arch, Richmond TW9 4DU, Surrey, England.
   [Higgins, Jez; Das, Arindra; Keller, Jared Robert; Thereaux, Olivier] Open Data Inst, London EC2A 4JE, England.
C3 University of Surrey; University of Surrey; Adobe Systems Inc.
RP Bui, T (corresponding author), Univ Surrey, Ctr Vis Speech & Signal Proc CVSSP, Guildford GU2 7XH, Surrey, England.
EM t.bui@surrey.ac.uk; d.cooper@surrey.ac.uk; j.collomosse@surrey.ac.uk;
   mark.bell@NATIONALARCHIVES.GOV.UK; alex.green@NATIONALARCHIVES.GOV.UK;
   jsheridan@NATIONALARC-HIVES.GOV.UK; jez@JEZUK.CO.UK;
   arindra@HUMANXDLAB.COM; jared.keller@THEODI.ORG; ot@theodi.org
RI Sheridan, John/B-9820-2009; Bui, Tu/AAQ-3929-2021
OI Bui, Tu/0000-0001-6622-9703; Keller, Jared/0000-0002-6847-4398; Bell,
   Mark/0000-0002-2153-6686
FU  [EP/P03151X/1]; EPSRC [EP/P03151X/1] Funding Source: UKRI
FX This work is supported by Archangel, the 2-year EPSRC Research Grant
   EP/P03151X/1.
CR [Anonymous], 2012, ISO147212012
   Baluja S, 2008, PATTERN RECOGN, V41, P3467, DOI 10.1016/j.patcog.2008.05.006
   Bestagini P, 2013, IEEE INT WORKSH MULT, P488, DOI 10.1109/MMSP.2013.6659337
   Bin Y, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P436, DOI 10.1145/2964284.2967258
   Breitinger C., 2016, MCIS 2016 P, P51
   Bui T., 2019, P C COMP VIS PATT RE
   Cao Liangliang., 2012, Proceedings of the 20th ACM International Conference on Multimedia, P299
   Carlini N., 2018, ARXIV180208232
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chung Junyoung, 2014, ARXIV14123555
   Collomosse J., 2018, P ACM S DOC ENG 2018, P1
   Coskun B, 2006, IEEE T MULTIMEDIA, V8, P1190, DOI 10.1109/TMM.2006.884614
   DAvino D., 2017, Media Watermarking, Security, and Forensics 2017, Burlingame, CA, USA, 29 January 2017-2 February 2017, V29, P92
   Gilbert H, 2004, LECT NOTES COMPUT SC, V3006, P175
   Gu Y., 2016, P AA COMP MACH MULT
   Guo DY, 2019, IEEE INT CONF MULTI, P687, DOI 10.1109/ICMEW.2019.00134
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Holmes C., 2018, Tech. Rep., UK Gov. Office for Science, V1, P1
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jacobi M., 2007, Proceedings of the Second IASTED International Conference on Environmental Modelling and Simulation, P1
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Johnson V., 2013, PROC DIGITAL HERITAG
   Kaselimi M, 2019, IEEE ACCESS, V7, P81047, DOI 10.1109/ACCESS.2019.2923742
   Kaselimi M, 2019, INT CONF ACOUST SPEE, P2747, DOI 10.1109/ICASSP.2019.8683110
   Khelifi F, 2019, IEEE T CIRC SYST VID, V29, P50, DOI 10.1109/TCSVT.2017.2776159
   Kim D, 2019, AAAI CONF ARTIF INTE, P8545
   Lemieux V. L., 2016, TECH REP, DOI [10.13140/RG.2.2.21736.67842, DOI 10.13140/RG.2.2.21736.67842]
   Li YZ, 2018, IEEE INT WORKS INFOR
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   Liu MY, 2017, ADV NEUR IN, V30
   Liu YQ, 2017, MULTIMEDIA SYST, V23, P223, DOI [10.1007/s00530-015-0478-1, 10.1007/s00530-015-0461-x]
   Long C., 2019, CVPR WORKSHOPS, P1
   Lu CW, 2019, INT J COMPUT VISION, V127, P993, DOI 10.1007/s11263-018-1129-8
   Messer K, 2002, INT C PATT RECOG, P1005, DOI 10.1109/ICPR.2002.1048475
   Nakamoto S, 2008, BITCOIN PEER TO PEER, DOI DOI 10.1007/S10838-008-9062-0
   Narayanan Arvind, 2016, Bitcoin and cryptocurrency technologies: a comprehensive introduction
   Olszewska JI, 2019, PROCEEDINGS OF THE 11TH INTERNATIONAL CONFERENCE ON AGENTS AND ARTIFICIAL INTELLIGENCE (ICAART), VOL 2, P850, DOI 10.5220/0007585208500856
   Peddinti V, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P3214
   Pimentel MAF, 2014, SIGNAL PROCESS, V99, P215, DOI 10.1016/j.sigpro.2013.12.026
   Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63, DOI 10.1007/978-3-540-28650-9_4
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sabir E., 2019, INTERFACES GUI, V3, P80
   Sahidullah M, 2012, SPEECH COMMUN, V54, P543, DOI 10.1016/j.specom.2011.11.004
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Senoussaoui M, 2014, IEEE-ACM T AUDIO SPE, V22, P217, DOI 10.1109/TASLP.2013.2285474
   Sharifi M., 2016, US Patent, Patent No. [9,275,427, 9275427]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sitara K, 2016, DIGIT INVEST, V18, P8, DOI 10.1016/j.diin.2016.06.003
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Subramanyam AV, 2012, IEEE INT WORKSH MULT, P89, DOI 10.1109/MMSP.2012.6343421
   Sultani W, 2018, PROC CVPR IEEE, P6479, DOI 10.1109/CVPR.2018.00678
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   WAIBEL A, 1989, IEEE T ACOUST SPEECH, V37, P328, DOI 10.1109/29.21701
   Walport M., 2016, Distributed ledger technology: Beyond blockchain, P1
   Wilson AC, 2017, ADV NEUR IN, V30
   Wood G., 2014, Ethereum project yellow paper, V151, P1
   Wu GS, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3076
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Ye GN, 2013, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2013.282
   Zhang HW, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P781, DOI 10.1145/2964284.2964308
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 66
TC 16
Z9 17
U1 0
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2020
VL 22
IS 11
BP 2858
EP 2872
DI 10.1109/TMM.2020.2967640
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OJ8YR
UT WOS:000584239900008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Dai, PW
   Zhang, H
   Cao, XC
AF Dai, Pengwen
   Zhang, Hua
   Cao, Xiaochun
TI Deep Multi-Scale Context Aware Feature Aggregation for Curved Scene Text
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Curved text detection; text-related feature enhancement; pyramid pooling
   attention; box-aware segmentation
AB Scene text plays a significant role in image and video understanding, which has made great progress in recent years. Most existing models on text detection in the wild have the assumption that all the texts are surrounded by a rotated rectangle or quadrangle. While there also exist lots of curved texts in the wild, which would not be bounded by a regular bounding box. In this paper, we develop a novel architecture to localize the text regions, which can deal with curved-shape scene texts. Specifically, we first design a text-related feature enhancement module by incorporating the prior knowledge of the text shape to enhance the feature representations. After that, based on the enhanced features, we employ a region proposal network to generate the candidate boxes of scene texts. For each text candidate, a pyramid region-of-interest pooling attention module is utilized to extract the fixed-size features. Finally, we exploit the box-aware context-based text segmentation module and box refinement network to obtain the location of scene text. Experiments are conducted on four challenging benchmarks CTW1500, totalTEXT, ICDAR-2015 and MLT, and the experimental results have demonstrated the superiority of our model.
C1 [Dai, Pengwen; Zhang, Hua; Cao, Xiaochun] Chinese Acad Sci, State Key Lab Informat Secur, Inst Informat Engn, Beijing 100195, Peoples R China.
   [Dai, Pengwen; Cao, Xiaochun] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100049, Peoples R China.
   [Cao, Xiaochun] Cyberspace Secur Res Ctr, Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Cao, Xiaochun] Shenzhen Res Inst Big Data, Shenzhen 518000, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Information Engineering, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Peng Cheng Laboratory; Shenzhen Research Institute of Big Data
RP Cao, XC (corresponding author), Chinese Acad Sci, State Key Lab Informat Secur, Inst Informat Engn, Beijing 100195, Peoples R China.; Cao, XC (corresponding author), Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100049, Peoples R China.; Cao, XC (corresponding author), Cyberspace Secur Res Ctr, Peng Cheng Lab, Shenzhen 518055, Peoples R China.
EM daipengwen@iie.ac.cn; zhanghua@iie.ac.cn; caoxiaochun@iie.ac.cn
RI Dai, Pengwen/HQZ-2709-2023
OI Dai, Pengwen/0000-0002-6262-982X
FU National Key R&D Program of China [2018YFB0803701]; Beijing Natural
   Science Foundation [4172068]; Key Program of the Chinese Academy of
   Sciences [QYZDB-SSW-JSC003]; Open Research Fund from Shenzhen Research
   Institute of Big Data [2019ORF01010]; National Natural Science
   Foundation of China [U1636214, 61602464]; Peng Cheng Laboratory Project
   of Guangdong Province [PCL2018KP004]; CCF-Tencent Open Research Fund
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018YFB0803701, in part by the Beijing Natural Science
   Foundation under Grant 4172068, in part by the Key Program of the
   Chinese Academy of Sciences under Grant QYZDB-SSW-JSC003, in part by The
   Open Research Fund from Shenzhen Research Institute of Big Data, under
   Grant 2019ORF01010, in part by the National Natural Science Foundation
   of China under Grant U1636214, in part by the Peng Cheng Laboratory
   Project of Guangdong Province under Grant PCL2018KP004, and in part by
   the CCF-Tencent Open Research Fund and National Natural Science
   Foundation of China under Grant 61602464. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Benoit Huet
CR Bouman KL, 2011, IEEE T MULTIMEDIA, V13, P922, DOI 10.1109/TMM.2011.2154317
   Busta M, 2017, IEEE I CONF COMP VIS, P2223, DOI 10.1109/ICCV.2017.242
   Ch'ng CK, 2017, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2017.157
   Chen XR, 2004, PROC CVPR IEEE, P366
   Dai YC, 2018, INT C PATT RECOG, P3604, DOI 10.1109/ICPR.2018.8546066
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Fabrizio J, 2016, INT J DOC ANAL RECOG, V19, P99, DOI 10.1007/s10032-016-0264-4
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   Hanif Shehzad Muhammad, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P1, DOI 10.1109/ICDAR.2009.172
   He DF, 2017, PROC CVPR IEEE, P474, DOI 10.1109/CVPR.2017.58
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He P, 2017, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2017.331
   He T, 2018, PROC CVPR IEEE, P5020, DOI 10.1109/CVPR.2018.00527
   He T, 2016, IEEE T IMAGE PROCESS, V25, P2529, DOI 10.1109/TIP.2016.2547588
   He WH, 2017, IEEE I CONF COMP VIS, P745, DOI 10.1109/ICCV.2017.87
   Hu H, 2017, IEEE I CONF COMP VIS, P4950, DOI 10.1109/ICCV.2017.529
   Huang WL, 2013, IEEE I CONF COMP VIS, P1241, DOI 10.1109/ICCV.2013.157
   Huang ZD, 2019, IEEE WINT CONF APPL, P764, DOI 10.1109/WACV.2019.00086
   Jaderberg M, 2014, LECT NOTES COMPUT SC, V8692, P512, DOI 10.1007/978-3-319-10593-2_34
   Kang L, 2014, PROC CVPR IEEE, P4034, DOI 10.1109/CVPR.2014.514
   Karaoglu S, 2017, IEEE T IMAGE PROCESS, V26, P3965, DOI 10.1109/TIP.2017.2707805
   Karaoglu S, 2017, IEEE T MULTIMEDIA, V19, P1063, DOI 10.1109/TMM.2016.2638622
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Li H., 2018, P BRIT MACH VIS C
   LI Y, 2017, PROC CVPR IEEE, P4438, DOI [DOI 10.1109/CVPR.2017.472, DOI 10.1109/CVPR.2017.199]
   Li Y, 2014, IEEE T IMAGE PROCESS, V23, P1666, DOI 10.1109/TIP.2014.2302896
   Liao MH, 2018, PROC CVPR IEEE, P5909, DOI 10.1109/CVPR.2018.00619
   Liao MH, 2017, AAAI CONF ARTIF INTE, P4161
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu XB, 2018, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2018.00595
   Liu Y, 2017, COMMUN MATH BIOL NEU, DOI 10.1080/10408398.2017.1329704
   Liu YL, 2017, PROC CVPR IEEE, P3454, DOI 10.1109/CVPR.2017.368
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Lyu PY, 2018, PROC CVPR IEEE, P7553, DOI 10.1109/CVPR.2018.00788
   Lyu PY, 2018, LECT NOTES COMPUT SC, V11218, P71, DOI 10.1007/978-3-030-01264-9_5
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Mateos LA, 2019, IEEE INT CONF ROBOT, P7933, DOI [10.1109/icra.2019.8793525, 10.1109/ICRA.2019.8793525]
   Mira F, 2016, 2016 22ND INTERNATIONAL CONFERENCE ON AUTOMATION AND COMPUTING (ICAC), P555
   Neumann L, 2012, PROC CVPR IEEE, P3538, DOI 10.1109/CVPR.2012.6248097
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Pan YF, 2011, IEEE T IMAGE PROCESS, V20, P800, DOI 10.1109/TIP.2010.2070803
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren XH, 2017, IEEE T MULTIMEDIA, V19, P506, DOI 10.1109/TMM.2016.2625259
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Tang Y, 2017, IEEE T IMAGE PROCESS, V26, P994, DOI [10.1109/TIP.2016.2639440, 10.1109/TIP.2017.2656474]
   Tang YB, 2018, IEEE T MULTIMEDIA, V20, P2276, DOI 10.1109/TMM.2018.2802644
   Tian SX, 2017, IEEE I CONF COMP VIS, P1501, DOI 10.1109/ICCV.2017.166
   Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wolf C, 2006, INT J DOC ANAL RECOG, V8, P280, DOI 10.1007/s10032-006-0014-0
   Wu D, 2017, PROC INT CONF DOC, P826, DOI 10.1109/ICDAR.2017.140
   Wu L, 2015, IEEE T MULTIMEDIA, V17, P1137, DOI 10.1109/TMM.2015.2443556
   Xue CH, 2018, LECT NOTES COMPUT SC, V11220, P370, DOI 10.1007/978-3-030-01270-0_22
   Yang Y, 2018, MECHANICS AND MATERIALS SCIENCE, P1071
   Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787
   Ye QX, 2015, IEEE T PATTERN ANAL, V37, P1480, DOI 10.1109/TPAMI.2014.2366765
   Yin XC, 2014, IEEE T PATTERN ANAL, V36, P970, DOI 10.1109/TPAMI.2013.182
   Zhang S, 2018, AAAI CONF ARTIF INTE, P2612
   Zhang Z, 2015, PROC CVPR IEEE, P2558, DOI 10.1109/CVPR.2015.7298871
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
   Zhu YY, 2016, FRONT COMPUT SCI-CHI, V10, P19, DOI 10.1007/s11704-015-4488-0
NR 63
TC 32
Z9 35
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2020
VL 22
IS 8
BP 1969
EP 1984
DI 10.1109/TMM.2019.2952978
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MR2MS
UT WOS:000553424500005
DA 2024-07-18
ER

PT J
AU Chen, XT
   Wang, WM
AF Chen, Xiongtao
   Wang, Wenmin
TI Uni-and-Bi-Directional Video Prediction via Learning Object-Centric
   Transformation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Kernel; Predictive models; Task analysis; Bidirectional control; Optical
   imaging; Image reconstruction; Visualization; Object-centric motion
   transformation; uni-and-bi-directional prediction; video prediction;
   visual attention
AB Video prediction, including uni-directional prediction for future frames and bi-directional prediction for in-between frames, is a challenging task and a problem worth exploring in multimedia and computer vision fields. Existing practices usually make predictions by learning global motion information from the whole given image. However, humans often focus on key objects carrying vital motion information instead of the entire frame. Besides, different objects often show different movement and deformation, even in the same scene. In this connection, we build a novel model of object-centric video prediction, in which the motion signals of key objects are particularly learned. This model can predict new frames by repeatedly transforming objects into the original input images. To focus on these objects automatically, we create an attention module with substitutable strategies. Our method requires no annotated data, and we also use adversarial training to improve sharpness of the predictions. We evaluate our model through Moving MNIST, UCF101 and Penn Action datasets and achieve competitive results in both quantity and quality, compared to existing methods. The experiments demonstrate that our uni-and-bi-directional network can well predict motions for different objects and generate plausible future and in-between frames.
C1 [Chen, Xiongtao; Wang, Wenmin] Peking Univ, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China.
   [Wang, Wenmin] Macau Univ Sci & Technol, Int Inst Next Generat Internet, Macau, Peoples R China.
C3 Peking University; Macau University of Science & Technology
RP Wang, WM (corresponding author), Macau Univ Sci & Technol, Int Inst Next Generat Internet, Macau, Peoples R China.
EM cxt@pku.edu.cn; wmwang@must.edu.mo
RI Wang, Wenmin/W-3511-2019
OI Wang, Wenmin/0000-0003-2664-4413
FU National Engineering Laboratory for Video Technology - Shenzhen
   Division; National Natural Science Foundation of China (NSFC)
   [U1613209]; Science and Technology Development Fund of Macau (FDCT)
   [0016/2019/A1]
FX This work was supported in part by the National Engineering Laboratory
   for Video Technology - Shenzhen Division, in part by the National
   Natural Science Foundation of China (NSFC, U1613209), and in part by the
   Science and Technology Development Fund of Macau (FDCT, 0016/2019/A1).
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Amersfoort J.V., 2017, ABS170108435 CORR
   [Anonymous], 2016, ARXIV160305106
   [Anonymous], 2016, PROC 4 INT C LEARN R
   [Anonymous], TECH REP
   [Anonymous], P 34 INT C MACH LEAR
   [Anonymous], 2012, CoRR
   Arjovsky M., 2017, ARXIV170107875
   Ba J., 2014, ARXIV PREPRINT ARXIV
   Babaeizadeh M., 2017, ARXIV171011252
   Byeon W, 2018, LECT NOTES COMPUT SC, V11220, P781, DOI 10.1007/978-3-030-01270-0_46
   Chen XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1503, DOI 10.1145/3123266.3123349
   Dosovitskiy A., 2016, Advances in Neural Information Processing Systems, P658
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595
   Goodfellow I., 2016, ADV NEURAL INFORM PR, P64
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu P, 2018, IEEE T MULTIMEDIA, V20, P2814, DOI 10.1109/TMM.2018.2815784
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jia X., 2016, Advances in Neural Information Processing Systems, V29, P667
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kitani KM, 2012, LECT NOTES COMPUT SC, V7575, P201, DOI 10.1007/978-3-642-33765-9_15
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Liang XD, 2017, IEEE I CONF COMP VIS, P1762, DOI 10.1109/ICCV.2017.194
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCVW.2017.361, 10.1109/ICCV.2017.478]
   Liu ZW, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661277
   Lu CC, 2017, PROC CVPR IEEE, P2137, DOI 10.1109/CVPR.2017.230
   Meyer S, 2015, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2015.7298747
   Hoai M, 2014, INT J COMPUT VISION, V107, P191, DOI 10.1007/s11263-013-0683-3
   Niklaus S, 2017, PROC CVPR IEEE, P2270, DOI 10.1109/CVPR.2017.244
   Oh J., 2015, P NEURIPS, P2863
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Patraucean V., 2015, arXiv
   Qiu ZF, 2018, IEEE T MULTIMEDIA, V20, P939, DOI 10.1109/TMM.2017.2759504
   Radford A., 2015, ARXIV
   Ranzato M., 2014, ARXIV14126604
   Shi XJ, 2015, ADV NEUR IN, V28
   Song JK, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2737
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Szeliski R., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P781, DOI 10.1109/ICCV.1999.790301
   Villegas R., 2017, P ICLR
   Villegas R., 2017, International Conference on Machine Learning, P3560
   Volz S, 2011, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2011.6126359
   Vondrick C, 2016, 30 C NEURAL INFORM P, V29
   Vondrick C, 2017, PROC CVPR IEEE, P2992, DOI 10.1109/CVPR.2017.319
   Vukotic V, 2017, LECT NOTES COMPUT SC, V10484, P140, DOI 10.1007/978-3-319-68560-1_13
   Walker J, 2016, LECT NOTES COMPUT SC, V9911, P835, DOI 10.1007/978-3-319-46478-7_51
   Walker J, 2015, IEEE I CONF COMP VIS, P2443, DOI 10.1109/ICCV.2015.281
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xue Tianfan, 2016, Advances in Neural Information Processing Systems, P91
   Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293
   Yuen J, 2010, LECT NOTES COMPUT SC, V6312, P707, DOI 10.1007/978-3-642-15552-9_51
   Zhang SW, 2018, IEEE T MULTIMEDIA, V20, P769, DOI 10.1109/TMM.2017.2758524
   Zhang WY, 2013, IEEE I CONF COMP VIS, P2248, DOI 10.1109/ICCV.2013.280
NR 63
TC 9
Z9 9
U1 3
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2020
VL 22
IS 6
BP 1591
EP 1604
DI 10.1109/TMM.2019.2946475
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU8YA
UT WOS:000538033100018
DA 2024-07-18
ER

PT J
AU Cheung, M
   She, J
AF Cheung, Ming
   She, James
TI Detecting Social Signals in User-Shared Images for Connection Discovery
   Using Deep Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Social networking (online); Feature extraction; Visualization; Deep
   learning; Object recognition; Image recognition; Image coding; Big data
   system; user-shared images; connection; discovery; recommendation;
   social network analysis; convolutional neural network
ID NETWORKS; CLUSTERS; NUMBER
AB With the advance of mobile technology and social media, image sharing has become part of our daily lives. For many applications, such as follower/followee recommendation, shared images are an excellent source to discover connections among users who shared them. Shared images on social media are like invitations for user interactions, such as comment, like, and more. Social signals are in those images, and those signals can be objects that interest related users such that they will start to interact. Conventionally, connections among users are discovered through recognizing objects among those shared images, such as a using convolutional neural network (CNN) to extract features that are sensitive to object recognition. However, social signals are not limited to object, they can be colour, textual, or even a concept that may not be captured effectively by conventional CNN. This paper proposes a CNN-based analytic framework to detect social signals among users. The CNN is optimized using a triplet network with user-shared images, and the relationships among users who upload them. It is observed that images from 2 users with a connection have a shorter distance after encoding, than 2 users without a connection. A framework is implemented, which is verified with over 1.7 million images by over 2000 users from two image-oriented social networks, Skyrock and Flickr. It is proven that the proposed analytic framework shows an up to 89 improvement on approaches using object recognition for follower/followee recommendation. To the best of our knowledge, this paper is the first to propose an analytic framework to detect social signals from visual features for connection discovery.
C1 [Cheung, Ming; She, James] Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Cheung, M (corresponding author), Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Hong Kong, Peoples R China.
EM cpming@ust.hk; eejames@ust.hk
OI Cheung, Ming/0000-0003-4646-1980
FU HKUST-NIE Social Media
FX This work was supported in part by HKUST-NIE Social Media and in part by
   CyPhyMedia Ltd. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Wenwu Zhu.
CR [Anonymous], 2015, NATURE, DOI [DOI 10.1038/NATURE14539, 10.1038/nature14539]
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2009, P 26 ANN INT C MACH
   [Anonymous], 2006, PROC IEEE COMPUT SOC
   [Anonymous], 2009, P ACM INT C IM VID R
   [Anonymous], 2014, ARXIV14032802
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Cheung M., 2014, P 4 INT C ADV INF MI, P83
   Cheung M, 2018, IEEE T BIG DATA, V4, P447, DOI 10.1109/TBDATA.2017.2762719
   Cheung M, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3115951
   Cheung M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON DATA SCIENCE AND DATA INTENSIVE SYSTEMS, P204, DOI 10.1109/DSDIS.2015.113
   Cheung M, 2015, IEEE T MULTIMEDIA, V17, P1417, DOI 10.1109/TMM.2015.2460192
   Elkahky A, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P278, DOI 10.1145/2736277.2741667
   Feng H, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1521
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Jiang Meng, 2012, P 21 ACM INT C INFOR, P45
   Kennedy Lyndon., 2007, Proceedings of the 15th International Conference on Multimedia, P631, DOI DOI 10.1145/1291233.1291384
   Krackhardt D, 1999, J PERS SOC PSYCHOL, V76, P770, DOI 10.1037/0022-3514.76.5.770
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Leroy Vincent, 2010, P 16 INT C KNOWL DIS, P393
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lü LY, 2011, PHYSICA A, V390, P1150, DOI 10.1016/j.physa.2010.11.027
   Luo CZ, 2016, IEEE T MULTIMEDIA, V18, P40, DOI 10.1109/TMM.2015.2495248
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Shaw AD, 1997, ANAL CHIM ACTA, V348, P357, DOI 10.1016/S0003-2670(97)00037-8
   Sugar CA, 2003, J AM STAT ASSOC, V98, P750, DOI 10.1198/016214503000000666
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Thiollière R, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P3179
   Tibshirani R, 2001, J ROY STAT SOC B, V63, P411, DOI 10.1111/1467-9868.00293
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26, DOI DOI 10.1007/S12654-012-0173-1
   Trzcinski T, 2017, IEEE T MULTIMEDIA, V19, P2561, DOI 10.1109/TMM.2017.2695439
   Vinciarelli A, 2009, IMAGE VISION COMPUT, V27, P1743, DOI 10.1016/j.imavis.2008.11.007
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320
   Wang XX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P627, DOI 10.1145/2647868.2654940
   Xu R, 2005, IEEE T NEURAL NETWOR, V16, P645, DOI 10.1109/TNN.2005.845141
   Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064
   Zahálka J, 2015, IEEE T MULTIMEDIA, V17, P2235, DOI 10.1109/TMM.2015.2480007
   Zhao WZ, 2009, LECT NOTES COMPUT SC, V5931, P674, DOI 10.1007/978-3-642-10665-1_71
   Zhou TC, 2010, AAAI CONF ARTIF INTE, P1486
NR 45
TC 6
Z9 6
U1 7
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2020
VL 22
IS 2
BP 407
EP 420
DI 10.1109/TMM.2019.2930043
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KH5FZ
UT WOS:000510676300010
DA 2024-07-18
ER

PT J
AU Xiao, W
   Huang, XL
   He, F
   Silva, J
   Emrani, S
   Chaudhuri, A
AF Xiao, Wei
   Huang, Xiaolin
   He, Fan
   Silva, Jorge
   Emrani, Saba
   Chaudhuri, Arin
TI Online Robust Principal Component Analysis With Change Point Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Principal component analysis; Sparse matrices; Matrix decomposition;
   Microsoft Windows; Surveillance; Big Data; Synthetic aperture sonar;
   Change point detection; online; principal component analysis
ID PCA; SPARSE
AB Robust principal component analysis (PCA) is a key technique for dynamical high-dimensional data analysis, including background subtraction for surveillance video. Typically, robust PCA requires all observations to be stored in memory before processing. The batch manner makes robust PCA inefficient for big data. In this paper, we develop an efficient online robust PCA method, namely, online moving window robust principal component analysis (OMWRPCA). Unlike the existing algorithms, OMWRPCA can successfully track not only slowly changing subspaces but also abruptly changing subspaces. Embedding hypothesis testing into the algorithm enables OMWRPCA to detect change points of the underlying subspaces. Extensive numerical experiments, including real-time background subtraction, demonstrate the superior performance of OMWRPCA compared with other state-of-the-art approaches.
C1 [Xiao, Wei] SAS Inst, Cary, NC 27513 USA.
   [Xiao, Wei] Amazon Inc, Seattle, WA 98109 USA.
   [Huang, Xiaolin; He, Fan] Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Inst Med Robot, Shanghai 200240, Peoples R China.
   [Huang, Xiaolin; He, Fan] MOE Key Lab Syst Control & Informat Proc, Shanghai 200240, Peoples R China.
   [Silva, Jorge; Emrani, Saba; Chaudhuri, Arin] SAS Inst Inc, Cary, NC 27513 USA.
   [Emrani, Saba] Apple Inc, Cupertino, CA 95014 USA.
C3 SAS Institute Inc; Amazon.com; Shanghai Jiao Tong University; SAS
   Institute Inc; Apple Inc
RP Xiao, W (corresponding author), SAS Inst, Cary, NC 27513 USA.; Xiao, W (corresponding author), Amazon Inc, Seattle, WA 98109 USA.; Huang, XL (corresponding author), Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Inst Med Robot, Shanghai 200240, Peoples R China.
EM wxiao@ncsu.edu; xiaolinhuang@sjtu.edu.cn; hf-inspire@sjtu.edu.cn;
   jorge.silva@sas.com; semrani@ncsu.edu; arin.chaudhuri@sas.com
OI Huang, Xiaolin/0000-0003-4285-6520; He, Fan/0000-0003-3497-3700
FU National Natural Science Foundation of China [61603248, IMR2018QY01]
FX The work of X. Huang was supported by the National Natural Science
   Foundation of China under Grant 61603248 and Grant IMR2018QY01. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Jingdong Wang.
CR [Anonymous], 2010, 100920105055 ARXIV
   [Anonymous], P INT C MACH LEARN
   [Anonymous], 2011, PRINCIPAL COMPONENT, DOI DOI 10.1007/978-3-642-04898-2_455
   Aravkin A, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P32
   Balzano L., 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P704, DOI 10.1109/ALLERTON.2010.5706976
   Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153
   Bouwmans T, 2017, COMPUT SCI REV, V23, P1, DOI 10.1016/j.cosrev.2016.11.001
   Bouwmans T, 2014, COMPUT VIS IMAGE UND, V122, P22, DOI 10.1016/j.cviu.2013.11.009
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Candès EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   De la Torre F, 2003, INT J COMPUT VISION, V54, P117, DOI 10.1023/A:1023709501986
   Feng Jiashi, 2013, Advances in Neural Information Processing Systems, P404
   Guo H, 2014, IEEE T SIGNAL PROCES, V62, P4284, DOI 10.1109/TSP.2014.2331612
   He J, 2012, PROC CVPR IEEE, P1568, DOI 10.1109/CVPR.2012.6247848
   He Jun, 2011, ARXIV11093827
   Hsu D, 2011, IEEE T INFORM THEORY, V57, P7221, DOI 10.1109/TIT.2011.2158250
   Hunt XJ, 2019, IEEE T PATTERN ANAL, V41, P1173, DOI 10.1109/TPAMI.2018.2829189
   Krim H, 1996, IEEE SIGNAL PROC MAG, V13, P67, DOI 10.1109/79.526899
   Kwak N, 2008, IEEE T PATTERN ANAL, V30, P1672, DOI 10.1109/TPAMI.2008.114
   Lee H, 2016, IEEE T MULTIMEDIA, V18, P2093, DOI 10.1109/TMM.2016.2595262
   Li LY, 2004, IEEE T IMAGE PROCESS, V13, P1459, DOI 10.1109/TIP.2004.836169
   Li XG, 2015, IEEE T SIGNAL PROCES, V63, P1792, DOI 10.1109/TSP.2015.2401536
   Lin Z., 2009, P INT WORKSH COMP AD, V61
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu GC, 2017, IEEE T PATTERN ANAL, V39, P47, DOI 10.1109/TPAMI.2016.2539946
   Liu Y, 2016, IEEE T MULTIMEDIA, V18, P351, DOI 10.1109/TMM.2016.2514848
   McCoy M, 2011, ELECTRON J STAT, V5, P1123, DOI 10.1214/11-EJS636
   Narayanamurthy P, 2018, IEEE INT SYMP INFO, P376, DOI 10.1109/ISIT.2018.8437747
   Nie F., 2011, INT JOINT C ART INT, V22, P1433
   Qiu CL, 2014, IEEE T INFORM THEORY, V60, P5007, DOI 10.1109/TIT.2014.2331344
   Rahmani M, 2017, IEEE T SIGNAL PROCES, V65, P2004, DOI 10.1109/TSP.2017.2649482
   Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835
   Roweis S, 1998, ADV NEUR IN, V10, P626
   Tian JD, 2018, IEEE T MULTIMEDIA, V20, P2659, DOI 10.1109/TMM.2018.2808763
   Wright J, 2009, ADV NEURAL INFORM PR, P2080, DOI DOI 10.1109/NNSP.2000.889420
   Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156
   Zhang T, 2014, J MACH LEARN RES, V15, P749
   Zhou ZH, 2010, IEEE INT SYMP INFO, P1518, DOI 10.1109/ISIT.2010.5513535
NR 38
TC 16
Z9 19
U1 4
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2020
VL 22
IS 1
BP 59
EP 68
DI 10.1109/TMM.2019.2923097
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KB6AZ
UT WOS:000506577000007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yan, B
   Bare, B
   Ma, CX
   Li, K
   Tan, WM
AF Yan, Bo
   Bare, Bahetiyaer
   Ma, Chenxi
   Li, Ke
   Tan, Weimin
TI Deep Objective Quality Assessment Driven Single Image Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Image resolution; Image quality; Quality assessment;
   Signal resolution; Deep learning; Measurement; Single image
   super-resolution; full-reference quality assessment; generative
   adversarial networks; image enhancement
ID INFORMATION; WATERMARKING; FINGERPRINT; SIMILARITY
AB Single-image super-resolution (SISR) is a classic problem in the image processing community, which aims at generating a high-resolution image from a low-resolution one. In recent years, deep learning based SISR methods emerged and achieved a performance leap than previous methods. However, because the evaluation metrics of SISR methods is peak signal-to-noise ratio (PSNR), previous methods usually choose L2-norm as the loss function. This leads to a significant improvement in the final PSNR value but little improvement in perceptual quality. In this paper, in order to achieve better results in both perceptual quality and PSNR values, we propose an objective quality assessment driven SISR method. First, we propose a novel full-reference image quality assessment approach for SISR and employ it as a loss function, namely super-resolution image quality assessment (SR-IQA) loss. Then, we combine SR-IQA loss with L2-norm to guide our proposed SISR method to achieve better results. Besides that, our proposed SISR method consists of several proposed highway units. Furthermore, in order to verify the generalization ability of our new kind of loss function, we integrate SR-IQA loss to generative adversarial networks based SR method and achieve better perceptual quality. Experimental results prove that our proposed SISR method achieves better performance than other methods both qualitatively and quantitatively in most of the cases.
C1 [Yan, Bo; Bare, Bahetiyaer; Ma, Chenxi; Li, Ke; Tan, Weimin] Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 200433, Peoples R China.
C3 Fudan University
RP Yan, B (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 200433, Peoples R China.
EM byan@fudan.edu.cn; 16110240015@fudan.edu.cn; 17210240039@fudan.edu.cn;
   15110240012@fudan.edu.cn; 14110240025@fudan.edu.cn
RI bare, bahetiyaer/AAH-5096-2019; Yan, Bo/AFQ-7025-2022
OI Yan, Bo/0000-0002-7775-1270; , chenxi/0000-0002-5577-5773
FU National Natural Science Foundation of China [61772137]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61772137. The associate editor coordinating the
   reviewof this manuscript and approving it for publication was Prof.
   Abdulmotaleb El Saddik.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   [Anonymous], 2018, P EUR C COMP VIS
   [Anonymous], LECT NOTES COMPUT SC
   [Anonymous], P IEEE INT C IM PROC
   [Anonymous], 2018, PROC EUR C COMPUT VI
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2017, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2017.618
   [Anonymous], ARXIV190102840
   [Anonymous], 2018, ARXIV181109150
   [Anonymous], 2019, P 33 AAAI C ART INT
   Bare B, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1223, DOI 10.1109/ICASSP.2018.8461931
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong Weisheng, 2011, IEEE Trans Image Process, V20, P1838, DOI 10.1109/TIP.2011.2108306
   DUCHON CE, 1979, J APPL METEOROL, V18, P1016, DOI 10.1175/1520-0450(1979)018<1016:LFIOAT>2.0.CO;2
   Fang YM, 2018, MULTIMED TOOLS APPL, V77, P29829, DOI 10.1007/s11042-018-5805-z
   Fang YM, 2016, IEEE IMAGE PROC, P2057, DOI 10.1109/ICIP.2016.7532720
   Fierrez-Aguilar J, 2006, LECT NOTES COMPUT SC, V3832, P213
   Galbally J, 2014, IEEE T IMAGE PROCESS, V23, P710, DOI 10.1109/TIP.2013.2292332
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Goodall TR, 2016, IEEE SIGNAL PROC LET, V23, P1801, DOI 10.1109/LSP.2016.2603842
   Hinton G. E., 2012, ARXIV PREPRINT ARXIV
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Karybali IG, 2006, IEEE T INF FOREN SEC, V1, P256, DOI 10.1109/TIFS.2006.873652
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim J, 2017, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2017.213
   Koz A, 2008, IEEE T CIRC SYST VID, V18, P326, DOI 10.1109/TCSVT.2008.918446
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Ledig C., 2017, P IEEE C COMP VIS PA, P4681
   Li K, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1478, DOI 10.1109/ICASSP.2018.8462007
   Liang YD, 2016, LECT NOTES COMPUT SC, V9909, P3, DOI 10.1007/978-3-319-46454-1_1
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Liu YT, 2018, IEEE T MULTIMEDIA, V20, P379, DOI 10.1109/TMM.2017.2729020
   Lukes T, 2013, INT WORK QUAL MULTIM, P42, DOI 10.1109/QoMEX.2013.6603205
   Ma C, 2017, COMPUT VIS IMAGE UND, V158, P1, DOI 10.1016/j.cviu.2016.12.009
   Ma KD, 2015, IEEE T IMAGE PROCESS, V24, P3086, DOI [10.1109/TIP.2015.2436340, 10.1109/TIP.2015.2456638]
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Rehman A, 2012, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2012-16
   Rehman A, 2010, IEEE IMAGE PROC, P289, DOI 10.1109/ICIP.2010.5653508
   Reibman AR, 2006, IEEE IMAGE PROC, P2017, DOI 10.1109/ICIP.2006.312895
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409106
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang CA, 2017, IEEE T IMAGE PROCESS, V26, P1833, DOI 10.1109/TIP.2017.2666742
   Wang GC, 2017, IEEE IMAGE PROC, P3145, DOI 10.1109/ICIP.2017.8296862
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P89, DOI 10.1109/ICIP.2001.958431
   Wang Z, 2006, IEEE T IMAGE PROCESS, V15, P1680, DOI 10.1109/TIP.2005.864165
   Wu JJ, 2013, IEEE T MULTIMEDIA, V15, P1700, DOI 10.1109/TMM.2013.2266093
   Wu QB, 2018, IEEE T CIRC SYST VID, V28, P2078, DOI 10.1109/TCSVT.2017.2710419
   Wu QB, 2018, IEEE T IMAGE PROCESS, V27, P2499, DOI 10.1109/TIP.2018.2799331
   Wu QB, 2017, IEEE T MULTIMEDIA, V19, P2490, DOI 10.1109/TMM.2017.2700206
   Wu QB, 2016, IEEE T CIRC SYST VID, V26, P425, DOI 10.1109/TCSVT.2015.2412773
   Xiong ZW, 2013, IEEE T MULTIMEDIA, V15, P1458, DOI 10.1109/TMM.2013.2264654
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yang CY, 2014, LECT NOTES COMPUT SC, V8692, P372, DOI 10.1007/978-3-319-10593-2_25
   Yang JC, 2013, PROC CVPR IEEE, P1059, DOI 10.1109/CVPR.2013.141
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang MC, 2013, IEEE T MULTIMEDIA, V15, P498, DOI 10.1109/TMM.2012.2232646
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Yeganeh H, 2012, IEEE IMAGE PROC, P1481, DOI 10.1109/ICIP.2012.6467151
   Yuan L, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239452
   Yue GH, 2018, IEEE T MULTIMEDIA, V20, P2722, DOI 10.1109/TMM.2018.2807589
   Yue GH, 2018, IEEE T IND ELECTRON, V65, P2525, DOI 10.1109/TIE.2017.2739708
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhu X, 2010, IEEE T IMAGE PROCESS, V19, P3116, DOI 10.1109/TIP.2010.2052820
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
NR 89
TC 33
Z9 36
U1 3
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2019
VL 21
IS 11
BP 2957
EP 2971
DI 10.1109/TMM.2019.2914883
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JJ7VX
UT WOS:000494363000021
DA 2024-07-18
ER

PT J
AU Maddala, TKK
   Kishore, PVV
   Eepuri, KK
   Dande, AK
AF Maddala, Teja Kiran Kumar
   Kishore, P. V. V.
   Eepuri, Kiran Kumar
   Dande, Anil Kumar
TI YogaNet: 3-D Yoga Asana Recognition Using Joint Angular Displacement
   Maps With ConvNets
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Joint angular displacement maps; convolution neural networks; yoga
   action recognition
ID PATTERNS; GESTURE
AB Representing 3-D motion-capture sensor data with 2-D color-coded joint distance maps (JDMs) as input to a deep neural network has been shown to be effective for 3-D skeletal-based human action recognition tasks. However, the joint distances are limited by their ability to represent rotational joint movements, which account for a considerable amount of information in human action classification tasks. Moreover, for the subject, view and time invariance in the recognition process, the deep classifier needs training on JDMs along different coordinate axes from multiple streams. To overcome the above shortcomings of JDMs, we propose integrating joint angular movements along with the joint distances in a spatiotemporal color-coded image called a joint angular displacement map (JADM). In the literature, multistream deep convolutional neural networks (CNNs) have been employed to achieve invariance across subjects and views for 3-D human action data, which is achieved by sacrificing training time for accuracy. To improve the recognition accuracy with reduced training times, we propose to test our JADMs with a single-stream deep CNN model. To test and analyze the proposed method, we chose video sequences of yoga. The 3-D motion-capture data represent a complex set of actions with lateral and rotational spatiotemporal variations. We validated the proposed method using 3-D traditional human action data from the publicly available datasets HDM05 and CMU. The proposed model can accurately recognize 3-D yoga actions, which may help in building a 3-D model-based yoga assistant tool.
C1 [Maddala, Teja Kiran Kumar; Eepuri, Kiran Kumar; Dande, Anil Kumar] Koneru Lakshmaiah Educ Fdn, Guntur 522502, Andhra Pradesh, India.
   [Kishore, P. V. V.] Koneru Lakshmaiah Educ Fdn, Dept Elect & Commun Engn, Biomech & Vis Comp Res Ctr, Guntur 522502, Andhra Pradesh, India.
C3 Koneru Lakshmaiah Education Foundation (K L Deemed to be University);
   Koneru Lakshmaiah Education Foundation (K L Deemed to be University)
RP Kishore, PVV (corresponding author), Koneru Lakshmaiah Educ Fdn, Dept Elect & Commun Engn, Biomech & Vis Comp Res Ctr, Guntur 522502, Andhra Pradesh, India.
EM mtejakiran@kluniversity.in; pvvkishore@kluniversity.in;
   kiraneepuri@kluniversity.in; danilmurali@kluniversity.in
RI D, Anil Kumar/AAY-6919-2020; Eepuri, Kiran Kumar/R-3308-2017; Maddala,
   Teja Kiran Kumar/V-1644-2017; Kishore, P.V.V./R-3293-2017
OI D, Anil Kumar/0000-0002-6051-2169; Eepuri, Kiran
   Kumar/0000-0001-8963-8454; Maddala, Teja Kiran
   Kumar/0000-0002-2345-6647; Kishore, P.V.V./0000-0002-3247-3043
CR Abdelkader MF, 2011, COMPUT VIS IMAGE UND, V115, P439, DOI 10.1016/j.cviu.2010.10.006
   Aggarwal JK, 2014, PATTERN RECOGN LETT, V48, P70, DOI 10.1016/j.patrec.2014.04.011
   Aggarwal JK, 1997, IEEE NONRIGID AND ARTICULATED MOTION WORKSHOP, PROCEEDINGS, P90, DOI 10.1109/NAMW.1997.609859
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 2016, Electron Imag, DOI DOI 10.2352/ISSN.2470-1173.2016.1.VDA-476
   [Anonymous], 2013, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2012.59
   [Anonymous], CORR
   [Anonymous], COMPUT ANIMATION VIR
   [Anonymous], 2012, J Comput Vis Image Process
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], PROC CVPR IEEE
   [Anonymous], P 30 INT C NEUR INF
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], 2015, Computer Vision and Pattern Recognition Workshops (CVPRW), 2015 IEEE Conference on
   [Anonymous], EVIDENCE BASED COMPL
   [Anonymous], 2016, Advances in Ergonomics of Manufacturing: Managing the Enterprise of the Future
   [Anonymous], 2013, Consumer Depth Cameras for Computer Vision, DOI DOI 10.1007/978-1-4471-4640-710
   [Anonymous], 2017, IEEE CVPR
   [Anonymous], 2011, 22 INT JT C ART INT, DOI 10.5555/2283516.2283603
   Auli Michael, 2013, P 2013 C EMPIRICAL M, P1044
   Bingbing Ni, 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1147, DOI 10.1109/ICCVW.2011.6130379
   Bloom V., 2012, 2012 IEEE COMP SOC C, P7, DOI [DOI 10.1109/CVPRW.2012.6239175, 10.1109/CVPRW.2012.6239175]
   Cai XY, 2016, IEEE T MULTIMEDIA, V18, P141, DOI 10.1109/TMM.2015.2505089
   Chen KM, 2009, INT J NURS STUD, V46, P154, DOI 10.1016/j.ijnurstu.2008.09.005
   Corrado G., 2012, P 25 INT C NEUR INF, P1223
   DAVIS RB, 1991, HUM MOVEMENT SCI, V10, P575, DOI 10.1016/0167-9457(91)90046-Z
   Deboeverie F, 2016, IEEE CONF COMPU INTE
   Draelos M, 2015, IEEE IMAGE PROC, P2520, DOI 10.1109/ICIP.2015.7351256
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Durska E, 2015, NEUES JAHRB GEOL P-A, V276, P315, DOI 10.1127/njgpa/2015/0491
   GAO HUANG Z L., 2016, CoRR
   Gavrila DM, 1996, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.1996.517056
   Gavrilova ML, 2018, IEEE CONSUM ELECTR M, V7, P88, DOI 10.1109/MCE.2017.2755498
   Han F, 2017, COMPUT VIS IMAGE UND, V158, P85, DOI 10.1016/j.cviu.2017.01.011
   Haubeck K, 2013, INT ARCH PHOTOGRAMM, P195
   Ho Y. S., 2013, U.S. Patent, Patent No. [8,351,685, 8351685]
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang C, 2007, IEEE T PATTERN ANAL, V29, P671, DOI 10.1109/TPAMI.2007.1011
   Hussein, 2013, INT JOINT C ART INT
   Ke QH, 2017, IEEE SIGNAL PROC LET, V24, P731, DOI 10.1109/LSP.2017.2690339
   Khalsa Sat Bir S, 2012, J Behav Health Serv Res, V39, P80, DOI 10.1007/s11414-011-9249-8
   Koppula HS, 2013, INT J ROBOT RES, V32, P951, DOI 10.1177/0278364913478446
   Kumar DA, 2018, MULTIMED TOOLS APPL, V77, P32063, DOI 10.1007/s11042-018-6199-7
   Li CK, 2017, IEEE SIGNAL PROC LET, V24, P624, DOI 10.1109/LSP.2017.2678539
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273
   Lin KY, 2011, EVID-BASED COMPL ALT, V2011, P1, DOI 10.1155/2011/659876
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Liu YL, 2017, IEEE INT C COMPUT, P623, DOI 10.1109/CSE-EUC.2017.115
   Mahasseni B, 2016, PROC CVPR IEEE, P3054, DOI 10.1109/CVPR.2016.333
   Maurice P, 2018, IEEE ROBOT AUTOM LET, V3, P249, DOI 10.1109/LRA.2017.2737048
   Qi TQ, 2017, NEUROCOMPUTING, V267, P475, DOI 10.1016/j.neucom.2017.06.041
   Ross A, 2013, COMPLEMENT THER MED, V21, P313, DOI 10.1016/j.ctim.2013.04.001
   Salem GJ, 2013, EVID-BASED COMPL ALT, V2013, DOI 10.1155/2013/165763
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Sempena Samsu., 2011, P INT C EL ENG INF, P1, DOI DOI 10.1109/ICEEI.2011.6021605
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Simonyan K., 2014, 14091556 ARXIV
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Vemulapalli R, 2016, PROC CVPR IEEE, P4471, DOI 10.1109/CVPR.2016.484
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Visceglia E, 2011, J ALTERN COMPLEM MED, V17, P601, DOI 10.1089/acm.2010.0075
   Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339
   Wang L., 2016, P ECCV
   Wang MY, 2013, BMC COMPLEM ALTERN M, V13, DOI 10.1186/1472-6882-13-8
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Weinland D, 2011, COMPUT VIS IMAGE UND, V115, P224, DOI 10.1016/j.cviu.2010.10.002
   Wu D, 2014, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2014.98
   Wu ZF, 2017, IEEE T PATTERN ANAL, V39, P209, DOI 10.1109/TPAMI.2016.2545669
   Xia L., 2012, CVPR 2012 HAU3D Workshop, P20
   Xu M, 2017, IEEE INT CON MULTI, P517, DOI 10.1109/ICME.2017.8019351
   Yang YH, 2017, IEEE T MULTIMEDIA, V19, P519, DOI 10.1109/TMM.2016.2626959
   Zhang H, 2016, IEEE T CIRC SYST VID, V26, P541, DOI 10.1109/TCSVT.2014.2376139
   Zhu WT, 2016, AAAI CONF ARTIF INTE, P3697
NR 74
TC 19
Z9 19
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2019
VL 21
IS 10
BP 2492
EP 2503
DI 10.1109/TMM.2019.2904880
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JD1IM
UT WOS:000489728400006
DA 2024-07-18
ER

PT J
AU Yue, GH
   Hou, CP
   Gu, K
   Zhou, TW
   Liu, HT
AF Yue, Guanghui
   Hou, Chunping
   Gu, Ke
   Zhou, Tianwei
   Liu, Hantao
TI No-Reference Quality Evaluator of Transparently Encrypted Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quality evaluation; visual security; encrypted image; transparent
   encryption; no-reference
ID MULTIPLY-DISTORTED IMAGES; NATURAL SCENE STATISTICS; SELECTIVE
   ENCRYPTION; SECURITY; INFORMATION; VIDEO; MODEL
AB In past years, various encrypted algorithms have been proposed to fully or partially protect the multimedia content in view of practical applications. In the context of digital TV broadcasting, transparent encryption only protects partial content and fulfills both security and quality requirements. To date, only a few reference-based works have been reported to evaluate the quality of transparently encrypted images. However, these works are incapable of reference-unavailable conditions. In this paper, we conduct the first attempt that proposes a novel quality evaluator in the absence of reference images. The key strategy of the proposed metric lies in extracting features by considering the motivation of transparently encrypted images. Specifically, given that encrypted images prevent content from being easily recognized, several features, including correlation coefficient, information entropy, and intensity statistic, are preliminarily extracted to estimate visual recognizability. Meanwhile, considering that encrypted images are avoided since they are of extremely low quality, we also capture many features to measure the distortions on multiple quality-sensitive image attributes, such as naturalness, structure, and texture. Finally, the quality evaluator is built by bridging all extracted features and corresponding quality scores via a regression module. Experimental results demonstrate that the proposed method is superior to the mainstream no-reference quality evaluation methods designed for synthetically distorted images and possesses a close approximation to state-of-the-art reference-based methods designed for encrypted images.
C1 [Yue, Guanghui] Shenzhen Univ, Natl Reg Key Technol Engn Lab Med Ultrasound, Guangdong Key Lab Biomed Measurements & Ultrasoun, Sch Biomed Engn,Hlth Sci Ctr, Shenzhen 518060, Peoples R China.
   [Hou, Chunping] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Gu, Ke] Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
   [Zhou, Tianwei] Shenzhen Univ, Coll Management, Shenzhen 518060, Peoples R China.
   [Liu, Hantao] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 3AA, S Glam, Wales.
C3 Shenzhen University; Tianjin University; Beijing University of
   Technology; Shenzhen University; Cardiff University
RP Zhou, TW (corresponding author), Shenzhen Univ, Coll Management, Shenzhen 518060, Peoples R China.
EM guanghuiyue.doctor@gmail.com; hcp@tju.edu.cn; guke.doctor@gmail.com;
   tianweizhoudr@gmail.com; liuh35@cardiff.ac.uk
RI Gu, Ke/AAJ-9684-2021; Zhou, Tianwei/GSI-8460-2022
OI Yue, Guanghui/0000-0001-9020-1735; Zhou, Tianwei/0000-0003-3533-7204
FU National Natural Science Foundation of China [61731003, 61520106002,
   61871274, 61801305, 81571758]; National Natural Science Foundation of
   Guangdong Province [2017A030313377, 2016A030313047]; Shenzhen Peacock
   Plan [KQTD2016053112051497]; Shenzhen Key Basic Research Project
   [JCYJ20170818142347251, JCYJ20170818094109846]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61731003, 61520106002, 61871274,
   61801305, and 81571758, in part by the National Natural Science
   Foundation of Guangdong Province under Grants 2017A030313377 and
   2016A030313047, in part by the Shenzhen Peacock Plan under Grant
   KQTD2016053112051497, and in part by the Shenzhen Key Basic Research
   Project under Grants JCYJ20170818142347251 and JCYJ20170818094109846.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Marco Grangetto.
CR [Anonymous], 2003, Final report from the video quality experts group on the validation of objective models of video quality assessment
   Autrusseau F., 2010, Subjective quality assessment of selective encryption techniques
   Bovik AC, 2013, P IEEE, V101, P2008, DOI 10.1109/JPROC.2013.2257632
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Elkamchouchi HM, 2005, RAD SCI C 2005 NRSC, P277, DOI DOI 10.1109/NRSC.2005.194011
   Fang YM, 2015, IEEE SIGNAL PROC LET, V22, P838, DOI 10.1109/LSP.2014.2372333
   Farid MS, 2018, INFORM FUSION, V43, P47, DOI 10.1016/j.inffus.2017.11.007
   Farid MS, 2015, IEEE IMAGE PROC, P3720, DOI 10.1109/ICIP.2015.7351499
   Grangetto M, 2006, IEEE T MULTIMEDIA, V8, P905, DOI 10.1109/TMM.2006.879919
   Gu K, 2017, IEEE T CYBERNETICS, V47, P4559, DOI 10.1109/TCYB.2016.2575544
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Han L, 2017, IEEE INT CON MULTI, P139, DOI 10.1109/ICME.2017.8019479
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Hassen R, 2013, IEEE T IMAGE PROCESS, V22, P2798, DOI 10.1109/TIP.2013.2251643
   Hofbauer H, 2016, SIGNAL PROCESS-IMAGE, V46, P60, DOI 10.1016/j.image.2016.05.001
   Jolfaei Alireza, 2010, International Journal of Computer and Network Security, V2, P38
   Jolfaei A, 2016, IEEE T INF FOREN SEC, V11, P235, DOI 10.1109/TIFS.2015.2489178
   Kovesi P., 1999, Videre, V1
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li LD, 2017, IEEE T MULTIMEDIA, V19, P1030, DOI 10.1109/TMM.2016.2640762
   Li LD, 2014, IEEE SIGNAL PROC LET, V21, P122, DOI 10.1109/LSP.2013.2294333
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Li QH, 2016, IEEE SIGNAL PROC LET, V23, P541, DOI 10.1109/LSP.2016.2537321
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu HT, 2010, IEEE T CIRC SYST VID, V20, P529, DOI 10.1109/TCSVT.2009.2035848
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Mao YN, 2006, IEEE T IMAGE PROCESS, V15, P2061, DOI 10.1109/TIP.2006.873426
   Mao YN, 2004, IEEE IMAGE PROC, P569
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Qin BD, 2015, IEEE T INF FOREN SEC, V10, P1384, DOI 10.1109/TIFS.2015.2410137
   RAN XN, 1995, IEEE T IMAGE PROCESS, V4, P401, DOI 10.1109/83.370671
   Ruderman DanielL., 1994, Advances in neural information processing systems, V73, P551
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Shahid Z, 2014, IEEE T MULTIMEDIA, V16, P24, DOI 10.1109/TMM.2013.2281029
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P1918, DOI 10.1109/TIP.2005.854492
   Stallings W., 2006, Cryptography and Network Security, V4th
   Sun QB, 2005, IEEE T MULTIMEDIA, V7, P480, DOI 10.1109/TMM.2005.846776
   Tong L., 2010, P 18 ACM INT C MULT, P835
   Wang C, 2013, IEEE T EMERG TOP COM, V1, P166, DOI 10.1109/TETC.2013.2273797
   Wang SQ, 2018, IEEE COMPUT GRAPH, V38, P47, DOI 10.1109/MCG.2016.46
   Wang SG, 2016, NEUROCOMPUTING, V174, P310, DOI 10.1016/j.neucom.2014.12.117
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiang T, 2016, IEEE T INF FOREN SEC, V11, P951, DOI 10.1109/TIFS.2016.2515503
   Yao Y, 2009, INFORM-J COMPUT INFO, V33, P69
   Yao Y, 2008, IEEE INT C NETW SENS, P1131
   Yue GH, 2018, IEEE T MULTIMEDIA, V20, P2722, DOI 10.1109/TMM.2018.2807589
   Zhou Y, 2018, IEEE T MULTIMEDIA, V20, P3019, DOI 10.1109/TMM.2018.2829607
NR 53
TC 26
Z9 27
U1 1
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2019
VL 21
IS 9
BP 2184
EP 2194
DI 10.1109/TMM.2019.2913315
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IT6VX
UT WOS:000483015200003
DA 2024-07-18
ER

PT J
AU Guo, XP
   Nie, RC
   Cao, JD
   Zhou, DM
   Mei, LY
   He, KJ
AF Guo, Xiaopeng
   Nie, Rencan
   Cao, Jinde
   Zhou, Dongming
   Mei, Liye
   He, Kangjian
TI FuseGAN: Learning to Fuse Multi-Focus Image via Conditional Generative
   Adversarial Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Conditional generative adversarial network; convolutional conditional
   random fields; images-to-image; multifocus image fusion; synthesize
   dataset
ID FUSION; SEGMENTATION; ENHANCEMENT
AB We study the problem of multi-focus image fusion, where the key challenge is detecting the focused regions accurately among multiple partially focused source images. Inspired by the conditional generative adversarial network (cGAN) to image-to-image task, we propose a novel FuseGAN to fulfill the images-to-image for multi-focus image fusion. To satisfy the requirement of dual input-to-one output, the encoder of the generator in FuseGAN is designed as a Siamese network. The least square GAN objective is employed to enhance the training stability of FuseGAN, resulting in an accurate confidence map for focus region detection. Also, we exploit the convolutional conditional random fields technique on the confidence map to reach a refined final decision map for better focus region detection. Moreover, due to the lack of a large-scale standard dataset, we synthesize a large enough multi-focus image dataset based on a public natural image dataset PASCAL VOC 2012, where we utilize a normalized disk point spread function to simulate the defocus and separate the background and foreground in the synthesis for each image. We conduct extensive experiments on two public datasets to verify the effectiveness of the proposed method. Results demonstrate that the proposed method presents accurate decision maps for focus regions in multi-focus images, such that the fused images are superior to 11 recent state-of-the-art algorithms, not only in visual perception, but also in quantitative analysis in terms of five metrics.
C1 [Guo, Xiaopeng; Nie, Rencan; Zhou, Dongming; Mei, Liye; He, Kangjian] Yunnan Univ, Sch Informat Sci & Engn, Kunming 650091, Yunnan, Peoples R China.
   [Nie, Rencan] Southeast Univ, Sch Automat, Nanjing 210096, Jiangsu, Peoples R China.
   [Cao, Jinde] Southeast Univ, Sch Math, Nanjing 210096, Jiangsu, Peoples R China.
C3 Yunnan University; Southeast University - China; Southeast University -
   China
RP Nie, RC (corresponding author), Yunnan Univ, Sch Informat Sci & Engn, Kunming 650091, Yunnan, Peoples R China.
EM xiaopengguo@mail.ynu.edu.cn; rcnie@ynu.edu.cn; jdcao@seu.edu.cn;
   zhoudm@ynu.edu.cn; liyemei@mail.ynu.edu.cn; Hekangjian92@126.com
RI Cao, Jinde/D-1482-2012; He, Kangjian/CAG-0300-2022; mei,
   liye/AGO-2907-2022; He, Kangjian/R-6183-2016
OI Cao, Jinde/0000-0003-3133-7119; mei, liye/0000-0002-2555-9199; He,
   Kangjian/0000-0001-6207-9728; Guo, Xiaopeng/0000-0003-1111-2035; Zhou,
   Dongming/0000-0003-0139-9415; Nie, Rencan/0000-0003-0568-1231
FU National Natural Science Foundation of China [61463052, 61365001]; China
   Postdoctoral Science Foundation [171740]; Postgraduate research
   innovation project of Yunnan University [YDY17111]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61463052 and Grant 61365001, in part by
   the China Postdoctoral Science Foundation under Grant 171740, and in
   part by the Postgraduate research innovation project of Yunnan
   University under Grant YDY17111. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Vasileios Mezaris. (Corresponding author: Rencan Nie.)
CR [Anonymous], 2017, ARXIV170105957V2
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2017, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2017.632
   [Anonymous], IEEE T CIRCUITS SYST
   [Anonymous], 1998, OPT PHOTON NEWS
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], IMAGE VISION COMPUT
   [Anonymous], 2017, Retinal Vessel Segmentation in Fundoscopic Images with Generative Adversarial Networks
   [Anonymous], 2014, ARXIV PREPRINT ARXIV
   Arjovsky M., 2017, P INT C LEARNING RE
   Bhatnagar G, 2013, IEEE T MULTIMEDIA, V15, P1014, DOI 10.1109/TMM.2013.2244870
   Chen Y, 2009, IMAGE VISION COMPUT, V27, P1421, DOI 10.1016/j.imavis.2007.12.002
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Du CB, 2017, IEEE ACCESS, V5, P15750, DOI 10.1109/ACCESS.2017.2735019
   Dumoulin V., 2016, P INT C LEARNING REP
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Goshtasby AA, 2007, INFORM FUSION, V8, P114, DOI 10.1016/j.inffus.2006.04.001
   Hossny M, 2008, ELECTRON LETT, V44, P1066, DOI 10.1049/el:20081754
   Huang W, 2007, PATTERN RECOGN LETT, V28, P1123, DOI 10.1016/j.patrec.2007.01.013
   King DB, 2015, ACS SYM SER, V1214, P1
   Kou F, 2018, IEEE T MULTIMEDIA, V20, P484, DOI 10.1109/TMM.2017.2743988
   Laganà MM, 2013, IEEE T MULTIMEDIA, V15, P1039, DOI 10.1109/TMM.2013.2244871
   Lewis JJ, 2007, INFORM FUSION, V8, P119, DOI 10.1016/j.inffus.2005.09.006
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Li S, 2013, INFORM FUSION, V14, P147, DOI 10.1016/j.inffus.2011.07.001
   Liu Y, 2018, INFORM FUSION, V42, P158, DOI 10.1016/j.inffus.2017.10.007
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Liu Y, 2016, IEEE SIGNAL PROC LET, V23, P1882, DOI 10.1109/LSP.2016.2618776
   Liu Y, 2015, INFORM FUSION, V23, P139, DOI 10.1016/j.inffus.2014.05.004
   Liu Z, 2001, PATTERN RECOGN LETT, V22, P929, DOI 10.1016/S0167-8655(01)00047-2
   Liu Z, 2012, IEEE T PATTERN ANAL, V34, P94, DOI 10.1109/TPAMI.2011.109
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Nejati M, 2015, INFORM FUSION, V25, P72, DOI 10.1016/j.inffus.2014.10.004
   Nowozin S., 2016, Advances in Neural Information Processing Systems, P271
   Oliveira JP, 2014, IEEE T IMAGE PROCESS, V23, P466, DOI 10.1109/TIP.2013.2286328
   Paszke Adam, 2017, NIPS W
   Reed S, 2016, PR MACH LEARN RES, V48
   Saeedi J, 2013, PATTERN ANAL APPL, V16, P365, DOI 10.1007/s10044-011-0235-9
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Shreyamsha Kumar BK, 2015, SIGNAL IMAGE VIDEO P, V9, P1193, DOI 10.1007/s11760-013-0556-9
   Shreyamsha Kumar BK, 2013, SIGNAL IMAGE VIDEO P, V7, P1125, DOI 10.1007/s11760-012-0361-x
   Shutao Li, 2001, Information Fusion, V2, P169, DOI 10.1016/S1566-2535(01)00038-0
   Staal J, 2004, IEEE T MED IMAGING, V23, P501, DOI 10.1109/TMI.2004.825627
   Teichmann M. T. T., 2018, COMPUT VIS PATTERN R
   Tian J, 2012, SIGNAL PROCESS, V92, P2137, DOI 10.1016/j.sigpro.2012.01.027
   Wang TH, 2015, IEEE T MULTIMEDIA, V17, P470, DOI 10.1109/TMM.2015.2403612
   Xydeas C. S, 2000, MIL TECH COUR, V56, P181, DOI 10.5937/vojtehg0802181B
   Yan X., 2018, ARXIV180607272
   Yang C, 2016, INT J DIGIT EARTH, V9, P156, DOI 10.1080/17538947.2014.967318
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064
   Zhang Q, 2009, SIGNAL PROCESS, V89, P1334, DOI 10.1016/j.sigpro.2009.01.012
   Zhang Y, 2017, INFORM FUSION, V35, P81, DOI 10.1016/j.inffus.2016.09.006
   Zhao WD, 2018, IEEE T MULTIMEDIA, V20, P866, DOI 10.1109/TMM.2017.2760100
   Zheng YF, 2007, INFORM FUSION, V8, P177, DOI 10.1016/j.inffus.2005.04.003
NR 56
TC 137
Z9 144
U1 7
U2 83
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2019
VL 21
IS 8
BP 1982
EP 1996
DI 10.1109/TMM.2019.2895292
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IK7YM
UT WOS:000476809700008
DA 2024-07-18
ER

PT J
AU Zhao, XC
   Lin, YP
   Liu, L
   Heikkilä, J
   Zheng, WM
AF Zhao, Xiaochao
   Lin, Yaping
   Liu, Li
   Heikkila, Janne
   Zheng, Wenming
TI Dynamic Texture Classification Using Unsupervised 3D Filter Learning and
   Local Binary Encoding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic texture; motion; feature extraction; local binary pattern
ID RECOGNITION; PATTERN; VIDEO; REPRESENTATION; SCALE
AB Local binary descriptors, such as local binary pattern (LBP) and its various variants, have been studied extensively in texture and dynamic texture analysis due to their outstanding characteristics, such as grayscale invariance, low computational complexity and good discriminability. Most existing local binary feature extraction methods extract spatio-temporal features from three orthogonal planes of a spatio-temporal volume by viewing a dynamic texture in 3D space. For a given pixel in a video, only a proportion of its surrounding pixels is incorporated in the local binary feature extraction process. We argue that the ignored pixels contain discriminative information that should be explored. To fully utilize the information conveyed by all the pixels in a local neighborhood, we propose extracting local binary features from the spatio-temporal domain with 3D filters that are learned in an unsupervised manner so that the discriminative features along both the spatial and temporal dimensions are captured simultaneously. The proposed approach consists of three components: 1) 3D filtering; 2) binary hashing; and 3) joint histogramming. Densely sampled 3D blocks of a dynamic texture are first normalized to have zero mean and are then filtered by 3D filters that are learned in advance. To preserve more of the structure information, the filter response vectors are decomposed into two complementary components, namely, the signs and the magnitudes, which are further encoded separately into binary codes. The local mean pixels of the 3D blocks are also converted into binary codes. Finally, three types of binary codes are combined via joint or hybrid histograms for the final feature representation. Extensive experiments are conducted on three commonly used dynamic texture databases: 1) UCLA; 2) DynTex; and 3) YUVL. The proposed method provides comparable results to, and even outperforms, many state-of-the-art methods.
C1 [Zhao, Xiaochao; Lin, Yaping] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Hunan, Peoples R China.
   [Zhao, Xiaochao; Liu, Li; Heikkila, Janne] Univ Oulu, Ctr Machine Vis & Signal Anal, FI-90014 Oulu, Finland.
   [Liu, Li] Natl Univ Def Technol, Coll Syst Engn, Changsha 410073, Hunan, Peoples R China.
   [Zheng, Wenming] Southeast Univ, Sch Biol Sci & Med Engn, Minist Educ, Key Lab Child Dev & Learning Sci, Nanjing 210096, Jiangsu, Peoples R China.
C3 Hunan University; University of Oulu; National University of Defense
   Technology - China; Southeast University - China
RP Lin, YP (corresponding author), Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Hunan, Peoples R China.
EM s12103017@hnu.edu.cn; yplin@hnu.edu.cn; li.liu@oulu.fi;
   Janne.Heikkila@oulu.fi; wenming_zheng@seu.edu.cn
FU China Scholarship Council
FX This work was supported by the scholarship from China Scholarship
   Council.
CR Andrearczyk V, 2018, PATTERN RECOGN, V76, P36, DOI 10.1016/j.patcog.2017.10.030
   [Anonymous], 2013, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2012.59
   [Anonymous], 1995, Studies in Optics
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2018, Int. J. Comput. Vis.
   [Anonymous], 2012, ARXIV12013612
   [Anonymous], P 6 ACM SIGMM INT WO
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2005, WACV MOTION
   [Anonymous], 2012, CoRR
   Arashloo SR, 2017, J VIS COMMUN IMAGE R, V43, P89, DOI 10.1016/j.jvcir.2016.12.015
   Arashloo SR, 2014, IEEE T MULTIMEDIA, V16, P2099, DOI 10.1109/TMM.2014.2362855
   Baktashmotlagh M, 2014, IEEE T PATTERN ANAL, V36, P2353, DOI 10.1109/TPAMI.2014.2339851
   Cannons KJ, 2010, LECT NOTES COMPUT SC, V6314, P511, DOI 10.1007/978-3-642-15561-1_37
   Chan A. B., 2008, PROC IEEE C COMPUT V, P1
   Chan A. B., 2007, PROC IEEE C COMPUT V, P1
   Chan AB, 2005, 2005 IEEE INTELLIGENT VEHICLES SYMPOSIUM PROCEEDINGS, P771
   Chan TH, 2015, IEEE T IMAGE PROCESS, V24, P5017, DOI 10.1109/TIP.2015.2475625
   Chen J, 2013, IEEE T IMAGE PROCESS, V22, P326, DOI 10.1109/TIP.2012.2210234
   Chen J, 2010, IEEE T PATTERN ANAL, V32, P1705, DOI 10.1109/TPAMI.2009.155
   Chetverikov D, 2005, ADV SOFT COMP, P17
   Coates Adam, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P561, DOI 10.1007/978-3-642-35289-8_30
   Derpanis KG, 2012, IEEE T PATTERN ANAL, V34, P1193, DOI 10.1109/TPAMI.2011.221
   Derpanis KG, 2010, PROC CVPR IEEE, P191, DOI 10.1109/CVPR.2010.5540213
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Doretto G, 2003, INT J COMPUT VISION, V51, P91, DOI 10.1023/A:1021669406132
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Duan YQ, 2018, IEEE T PATTERN ANAL, V40, P1139, DOI 10.1109/TPAMI.2017.2710183
   Dubois S, 2015, SIGNAL IMAGE VIDEO P, V9, P819, DOI 10.1007/s11760-013-0532-4
   Ghanem B., 2010, LECT NOTES COMPUT SC, P223
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Harandi M, 2013, IEEE I CONF COMP VIS, P3120, DOI 10.1109/ICCV.2013.387
   Hong S, 2018, MULTIDIM SYST SIGN P, V29, P279, DOI 10.1007/s11045-016-0463-7
   Ji H, 2013, IEEE T IMAGE PROCESS, V22, P286, DOI 10.1109/TIP.2012.2214040
   Jing Huang, 2010, 2010 Proceedings of 3rd International Congress on Image and Signal Processing (CISP 2010), P1887, DOI 10.1109/CISP.2010.5647609
   Kannala J, 2012, INT C PATT RECOG, P1363
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Lei Z, 2014, IEEE T PATTERN ANAL, V36, P289, DOI 10.1109/TPAMI.2013.112
   Lei Z, 2012, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2012.6247967
   Liu L, 2017, PATTERN RECOGN, V62, P135, DOI 10.1016/j.patcog.2016.08.032
   Lu JW, 2015, IEEE T PATTERN ANAL, V37, P2041, DOI 10.1109/TPAMI.2015.2408359
   Mandelbrot B. B., 1982, FRACTAL GEOMETRY NAT
   Mumtaz A, 2015, IEEE T PATTERN ANAL, V37, P697, DOI 10.1109/TPAMI.2014.2359432
   Mumtaz A, 2013, IEEE T PATTERN ANAL, V35, P1606, DOI 10.1109/TPAMI.2012.236
   Ngiam J., 2011, Advances in Neural Information Processing Systems, V24
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Péteri R, 2005, LECT NOTES COMPUT SC, V3523, P223
   Péteri R, 2010, PATTERN RECOGN LETT, V31, P1627, DOI 10.1016/j.patrec.2010.05.009
   Qi XB, 2016, NEUROCOMPUTING, V171, P1230, DOI 10.1016/j.neucom.2015.07.071
   Quan YH, 2016, PROC CVPR IEEE, P308, DOI 10.1109/CVPR.2016.40
   Quan YH, 2015, IEEE I CONF COMP VIS, P73, DOI 10.1109/ICCV.2015.17
   Rahtu E, 2012, IMAGE VISION COMPUT, V30, P501, DOI 10.1016/j.imavis.2012.04.001
   Rivera AR, 2015, IEEE T PATTERN ANAL, V37, P2146, DOI 10.1109/TPAMI.2015.2392774
   Ravichandran A, 2013, IEEE T PATTERN ANAL, V35, P342, DOI 10.1109/TPAMI.2012.83
   Ravichandran A, 2009, PROC CVPR IEEE, P1651, DOI 10.1109/CVPRW.2009.5206847
   Ren JF, 2014, IEEE SIGNAL PROC LET, V21, P1346, DOI 10.1109/LSP.2014.2336252
   Ren JF, 2013, INT CONF ACOUST SPEE, P2400, DOI 10.1109/ICASSP.2013.6638085
   Saisan P, 2001, PROC CVPR IEEE, P58
   Soatto S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P439, DOI 10.1109/ICCV.2001.937658
   Szummer M, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P823, DOI 10.1109/ICIP.1996.560871
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Tiwari D, 2016, COMPUT VIS IMAGE UND, V150, P58, DOI 10.1016/j.cviu.2016.04.010
   Tiwari D, 2016, MULTIDIM SYST SIGN P, V27, P563, DOI 10.1007/s11045-015-0319-6
   Töreyin BU, 2006, PATTERN RECOGN LETT, V27, P49, DOI 10.1016/j.patrec.2005.06.015
   Wang Y, 2015, NEUROCOMPUTING, V154, P217, DOI 10.1016/j.neucom.2014.12.001
   Woolfe F, 2006, LECT NOTES COMPUT SC, V3952, P549
   Xiaobai Li, 2018, IEEE Transactions on Affective Computing, V9, P563, DOI 10.1109/TAFFC.2017.2667642
   Xu Y, 2012, COMPUT VIS IMAGE UND, V116, P999, DOI 10.1016/j.cviu.2012.05.003
   Xu Y, 2011, IEEE I CONF COMP VIS, P1219, DOI 10.1109/ICCV.2011.6126372
   Yan X, 2014, LECT NOTES COMPUT SC, V8692, P215, DOI 10.1007/978-3-319-10593-2_15
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao GY, 2009, IEEE T MULTIMEDIA, V11, P1254, DOI 10.1109/TMM.2009.2030637
   Zhao XC, 2017, IEEE IMAGE PROC, P4152, DOI 10.1109/ICIP.2017.8297064
   Zhao XC, 2018, IEEE T MULTIMEDIA, V20, P552, DOI 10.1109/TMM.2017.2750415
   Zhao Y, 2013, NEUROCOMPUTING, V106, P68, DOI 10.1016/j.neucom.2012.10.017
NR 75
TC 25
Z9 27
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2019
VL 21
IS 7
BP 1694
EP 1708
DI 10.1109/TMM.2018.2890362
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IF6IF
UT WOS:000473183700007
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Mackin, A
   Zhang, F
   Bull, DR
AF Mackin, Alex
   Zhang, Fan
   Bull, David R.
TI A Study of High Frame Rate Video Formats
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE High frame rates; video database; immersive video; UHDTV; HFR
ID RATE UP-CONVERSION; QUALITY ASSESSMENT; IMAGE QUALITY; MOTION;
   VISIBILITY
AB High frame rates are acknowledged to increase the perceived quality of certain video content. However, the lack of high frame rate test content has previously restricted the scope of research in this area-especially in the context of immersive video formats. This problem has been addressed through the publication of a high frame rate video database BVI-HFR, which was captured natively at 120 fps. BVI-HFR spans a variety of scenes, motions, and colors, and is shown to be representative of BBC broadcast content. In this paper, temporal down-sampling is utilized to enable both subjective and objective comparisons across a range frame rates. A large-scale subjective experiment has demonstrated that high frame rates lead to increases in perceived quality, and that a degree of content dependence exists-notably related to camera motion. Various image and video quality metrics have been benchmarked on these subjective evaluations, and analysis shows that those which explicitly account for temporal distortions (e.g., FRQM) provide improved correlation with subjective opinions compared to generic quality metrics such as PSNR.
C1 [Mackin, Alex; Zhang, Fan; Bull, David R.] Univ Bristol, Bristol Vis Inst, Bristol BS8 1QU, Avon, England.
C3 University of Bristol
RP Zhang, F (corresponding author), Univ Bristol, Bristol Vis Inst, Bristol BS8 1QU, Avon, England.
EM a.mackin@bristol.ac.uk; fan.zhang@bristol.ac.uk; dave.bull@bristol.ac.uk
OI Zhang, Fan/0000-0001-6623-9936
FU Engineering and Physical Sciences Research Council [EP/M000885/1]; BBC
   Research and Development; EPSRC [EP/M000885/1] Funding Source: UKRI
FX This work was supported by the Engineering and Physical Sciences
   Research Council under Grant EP/M000885/1, and funding and support from
   BBC Research and Development. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Meng
   Wang.
CR Allison RS, 2016, J IMAGING SCI TECHN, V60, DOI 10.2352/J.ImagingSci.Technol.2016.60.6.060402
   [Anonymous], 2012, METHODOLOGY SUBJECTI
   [Anonymous], CITY
   [Anonymous], 2015, IEEE INT WORKSHOP MU
   [Anonymous], 2013, Technical Report JCTVC-L1100
   [Anonymous], 2015, Parameter values for ultra-high definition television systems for production and international programme exchange. Recommendation BT.2020-2
   [Anonymous], 2017, SMPTE MOTION IMAG J
   Armstrong J, 2008, 2008 7TH INTERNATIONAL CONFERENCE ON THE OPTICAL INTERNET (COIN), P169
   Barten Peter G. J, 1999, Contrast sensitivity of the human eye and its effects on image quality
   BEX PJ, 1995, J EXP PSYCHOL HUMAN, V21, P231, DOI 10.1037/0096-1523.21.2.231
   Bull DR, 2014, COMMUNICATING PICTURES: A COURSE IN IMAGE AND VIDEO CODING, P1
   Butterworth B., 2013, BBC INTERNET BLOG
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Chen Y, 2016, IEEE T MULTIMEDIA, V18, P576, DOI 10.1109/TMM.2016.2525010
   Cho S, 2015, IEEE T MULTIMEDIA, V17, P778, DOI 10.1109/TMM.2015.2418995
   Daly S, 1998, P SOC PHOTO-OPT INS, V3299, P180, DOI 10.1117/12.320110
   Daly S., 2014, SMPTE C SOC MOT PICT, V2014, P1
   Debattista K, 2018, COMPUT GRAPH FORUM, V37, P363, DOI 10.1111/cgf.13302
   Dong YY, 2016, IEEE T MULTIMEDIA, V18, P549, DOI 10.1109/TMM.2016.2522639
   Emoto M, 2014, J DISP TECHNOL, V10, P635, DOI 10.1109/JDT.2014.2312233
   Ge C, 2017, IEEE T MULTIMEDIA, V19, P2222, DOI 10.1109/TMM.2017.2735301
   Gizmodo, 2014, HOBB UN MAST WHY 48
   Haak M., 2009, Proceeding of the 1st driver car interaction and interface DCII 2008, P35
   Hadizadeh H, 2018, IEEE T MULTIMEDIA, V20, P392, DOI 10.1109/TMM.2017.2740023
   Hoffman DM, 2011, J SOC INF DISPLAY, V19, P271, DOI 10.1889/JSID19.3.271
   Huang Q, 2016, IEEE T BROADCAST, V62, P640, DOI 10.1109/TBC.2016.2570022
   Huang YL, 2017, IEEE T CIRC SYST VID, V27, P2739, DOI 10.1109/TCSVT.2016.2596198
   International Telecom Union, 2014, TUT OBJ PERC ASS VID
   Kaviani HR, 2016, IEEE T CIRC SYST VID, V26, P1581, DOI 10.1109/TCSVT.2015.2469120
   Kime S, 2016, J DISP TECHNOL, V12, P1372, DOI 10.1109/JDT.2016.2603222
   Kleiner M, 2007, PERCEPTION, V36, P14
   Kurdoglu E, 2018, IEEE T MULTIMEDIA, V20, P1876, DOI 10.1109/TMM.2017.2781362
   Kuroki Y, 2007, J SOC INF DISPLAY, V15, P61, DOI 10.1889/1.2451560
   Kuroki Y, 2014, J SOC INF DISPLAY, V22, P191, DOI 10.1002/jsid.237
   Lanman D., 2014, P ACM SIGGRAPH COURS, P1
   Lim H, 2015, IEEE T CIRC SYST VID, V25, P518, DOI 10.1109/TCSVT.2014.2352557
   Ma Z, 2012, IEEE T CIRC SYST VID, V22, P671, DOI 10.1109/TCSVT.2011.2177143
   Mackin A, 2017, IEEE IMAGE PROC, P295, DOI 10.1109/ICIP.2017.8296290
   Mackin A, 2016, IEEE IMAGE PROC, P2435, DOI 10.1109/ICIP.2016.7532796
   Mackin A, 2015, IEEE IMAGE PROC, P3407, DOI 10.1109/ICIP.2015.7351436
   Moss FM, 2016, IEEE IMAGE PROC, P2425, DOI 10.1109/ICIP.2016.7532794
   Moss FM, 2016, SIGNAL PROCESS-IMAGE, V48, P38, DOI 10.1016/j.image.2016.08.005
   Moss FM, 2016, IEEE T CIRC SYST VID, V26, P1977, DOI 10.1109/TCSVT.2015.2461971
   Nasiri RM, 2017, IEEE IMAGE PROC, P3475, DOI 10.1109/ICIP.2017.8296928
   Noland K, 2014, 282 BBC RES DEV
   Ou YF, 2014, IEEE T IMAGE PROCESS, V23, P2473, DOI 10.1109/TIP.2014.2303636
   Ou YF, 2011, IEEE T CIRC SYST VID, V21, P286, DOI 10.1109/TCSVT.2010.2087833
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Ribeiro FML, 2018, IEEE T MULTIMEDIA, V20, P1, DOI 10.1109/TMM.2017.2714425
   Selfridge R., 2016, P 13 EUR C VIS MED P, P3
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Silva AF, 2016, IEEE T MULTIMEDIA, V18, P2446, DOI 10.1109/TMM.2016.2601027
   Sugawara M, 2009, 2009 SID INTERNATIONAL SYMPOSIUM DIGEST OF TECHNICAL PAPERS, VOL XL, BOOKS I - III, P1200
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tag Benjamin, 2016, P 2016 CHI C HUM FAC, P2321
   Templin K, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925879
   Torralba A, 2003, NETWORK-COMP NEURAL, V14, P391, DOI 10.1088/0954-898X/14/3/302
   Vollmer Michael, 2011, Physics Education, V46, P191, DOI 10.1088/0031-9120/46/2/007
   Vu P. V., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2505, DOI 10.1109/ICIP.2011.6116171
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Watson Andrew B., 2013, Motion Imaging Journal, V122, P18
   WATSON AB, 1986, J OPT SOC AM A, V3, P300, DOI 10.1364/JOSAA.3.000300
   Wilcox LM, 2015, ACM T APPL PERCEPT, V12, DOI 10.1145/2810039
   Winkler S, 2008, IEEE T BROADCAST, V54, P660, DOI 10.1109/TBC.2008.2000733
   Winkler S, 2012, IEEE J-STSP, V6, P616, DOI 10.1109/JSTSP.2012.2215007
   Wu JY, 2016, IEEE J SEL AREA COMM, V34, P2231, DOI 10.1109/JSAC.2016.2577178
   Wu JY, 2016, IEEE T WIREL COMMUN, V15, P2713, DOI 10.1109/TWC.2015.2509063
   Wu JY, 2015, IEEE T CIRC SYST VID, V25, P1988, DOI 10.1109/TCSVT.2015.2441412
   Zhang F, 2017, IEEE IMAGE PROC, P300, DOI 10.1109/ICIP.2017.8296291
NR 71
TC 36
Z9 38
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2019
VL 21
IS 6
BP 1499
EP 1512
DI 10.1109/TMM.2018.2880603
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IA1QQ
UT WOS:000469337400013
OA Green Submitted, hybrid
DA 2024-07-18
ER

PT J
AU Tao, QY
   Yang, H
   Cai, JF
AF Tao, Qingyi
   Yang, Hao
   Cai, Jianfei
TI Exploiting Web Images for Weakly Supervised Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Weakly supervised learning; object detection; curriculum learning
AB In recent years, the performance of object detection has advanced significantly with the evolution of deep convolutional neural networks. However, the state-of-the-art object detection methods still rely on accurate bounding box annotations that require extensive human labeling. Object detection without bounding box annotations, that is, weakly supervised detection methods, are still lagging far behind. As weakly supervised detection only uses image level labels and does not require the ground truth of bounding box location and label of each object in an image, it is generally very difficult to distill knowledge of the actual appearances of objects. Inspired by curriculum learning, this paper proposes an easy-to-hard knowledge transfer scheme that incorporates easy web images to provide prior knowledge of object appearance as a good starting point. While exploiting large-scale free web imagery, we introduce a sophisticated labor-free method to construct a web dataset with good diversity in object appearance. After that, semantic relevance and distribution relevance are introduced and utilized in the proposed curriculum training scheme. Our end-to-end learning with the constructed web data achieves remarkable improvement across most object classes, especially for the classes that are often considered hard in other works.
C1 [Tao, Qingyi; Cai, Jianfei] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Tao, Qingyi] NVIDIA AI Technol Ctr, Singapore 138522, Singapore.
   [Yang, Hao] Amazon, Seattle, WA 98133 USA.
C3 Nanyang Technological University; Amazon.com
RP Tao, QY (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
EM qtao002@e.ntu.edu.sg; lancelot365@gmail.com; asjfcai@ntu.edu.sg
RI Cai, Jianfei/A-3691-2011
OI Cai, Jianfei/0000-0002-9444-3763
FU MoE [2016-T2-2-065]; NTU CoE;  [2018-T1-001-115]
FX This work was supported in part by MoE Tier-2 under Grant 2016-T2-2-065,
   in part by Tier-1 under Grant 2018-T1-001-115, and in part by NTU CoE
   under Grant 2016. This work was done when Hao Yang was at Nanyang
   Technological University.
CR [Anonymous], 2017, P BRIT MACH VIS C IM
   Arenhart JRB, 2016, LOG ANAL, P301, DOI 10.2143/LEA.235.0.3170111
   Bengio Y., 2009, P 26 ANN INT C MACH, V60, P6, DOI [DOI 10.1145/1553374.1553380, 10.1145/1553374.1553380]
   Bilen H., 2014, P BRIT MACH VIS C, V3
   Bilen H, 2016, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR.2016.311
   Bilen H, 2015, PROC CVPR IEEE, P1081, DOI 10.1109/CVPR.2015.7298711
   Chen XL, 2015, IEEE I CONF COMP VIS, P1431, DOI 10.1109/ICCV.2015.168
   Cinbis RG, 2017, IEEE T PATTERN ANAL, V39, P189, DOI 10.1109/TPAMI.2016.2535231
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Diba A, 2017, PROC CVPR IEEE, P5131, DOI 10.1109/CVPR.2017.545
   Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3
   Divvala SK, 2014, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2014.412
   Fu JL, 2015, IEEE I CONF COMP VIS, P1985, DOI 10.1109/ICCV.2015.230
   Ionescu RT, 2016, PROC CVPR IEEE, P2157, DOI 10.1109/CVPR.2016.237
   Jiang L, 2015, AAAI CONF ARTIF INTE, P2694
   Kantorov V, 2016, LECT NOTES COMPUT SC, V9909, P350, DOI 10.1007/978-3-319-46454-1_22
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Niu L, 2017, IEEE T NEUR NET LEAR, V28, P1985, DOI 10.1109/TNNLS.2016.2557349
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Reed S, 2014, Training deep neural networks on noisy labels with bootstrapping
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Su SC, 2016, PROC CVPR IEEE, pCP40, DOI 10.1109/CVPR.2016.382
   Tang P, 2017, PROC CVPR IEEE, P3059, DOI 10.1109/CVPR.2017.326
   Tang YX, 2017, IEEE T MULTIMEDIA, V19, P393, DOI 10.1109/TMM.2016.2614862
   Tao QY, 2018, LECT NOTES COMPUT SC, V11215, P387, DOI 10.1007/978-3-030-01252-6_23
   Teh E. W., 2016, P BRIT MACH VIS C
   Wang C, 2014, LECT NOTES COMPUT SC, V8694, P431, DOI 10.1007/978-3-319-10599-4_28
   Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Xu Z, 2015, IEEE I CONF COMP VIS, P2524, DOI 10.1109/ICCV.2015.290
   Yao YZ, 2017, IEEE T MULTIMEDIA, V19, P1771, DOI 10.1109/TMM.2017.2684626
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
   2016, P IEEE C COMP VIS PA, P3548, DOI DOI 10.1109/CVPR.2016.386
NR 34
TC 12
Z9 13
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2019
VL 21
IS 5
BP 1135
EP 1146
DI 10.1109/TMM.2018.2875597
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HV8HY
UT WOS:000466223600005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhou, QQ
   Zhong, BN
   Zhang, YL
   Li, J
   Fu, Y
AF Zhou, Qinqin
   Zhong, Bineng
   Zhang, Yulun
   Li, Jun
   Fu, Yun
TI Deep Alignment Network Based Multi-Person Tracking With Occlusion and
   Motion Reasoning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-person tracking; alignment network; occlusion reasoning; motion
   reasoning
ID MULTIPLE; PATTERNS
AB Tracking-by-detection is one of the typical paradigms for multi-person tracking, due to the availability of automatic pedestrian detectors. However, existing multi-person trackers are greatly challenged by misalignment in the pedestrian detectors (i.e., excessive background and part missing) and occlusion. To effectively handle these problems, we propose a deep alignment network-based multi-person tracking method with occlusion and motion reasoning. Specifically, the inaccurate detections are first corrected via a deep alignment network, in which an alignment estimation module is used to automatically learn the spatial transformation of these detections. As a result, the deep features from our alignment network will have better representation power and, thus, lead to more consistent tracks. Then, a coarse-to-fine schema is designed for construing a discriminative association cost matrix with spatial, motion, and appearance information. Meanwhile, a principled approach is developed to allowour method to handle occlusion with motion reasoning and the reidentification ability of the pedestrian alignment network. Finally, the association problem is solved via a simple yet real-time Hungarian algorithm. Comprehensive experiments on MOT16, ISSIA soccer, PETS09, and TUD datasets validate the effectiveness and robustness of our proposed tracker.
C1 [Zhou, Qinqin; Zhong, Bineng] Huaqiao Univ, Coll Comp Sci & Technol, Xiamen 361021, Peoples R China.
   [Zhang, Yulun; Li, Jun; Fu, Yun] Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA.
C3 Huaqiao University; Northeastern University
RP Zhong, BN (corresponding author), Huaqiao Univ, Coll Comp Sci & Technol, Xiamen 361021, Peoples R China.
EM 1611414018@hqu.edu.cn; bnzhong@hqu.edu.cn; yulun100@gmail.com;
   junl.mldl@gmail.com; yunfu@ece.neu.edu
OI Zhang, Yulun/0000-0002-2288-5079; Zhou, Qinqin/0000-0003-3941-3534
FU Natural Science Foundation of China [61572205, 61802135]; Natural
   Science Foundation of Fujian Province [2017J01113]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 61572205 and 61802135, and in part by the Natural
   Science Foundation of Fujian Province under Grant 2017J01113.
CR Andriluka M., 2008, PROC IEEE C COMPUT V, P1
   [Anonymous], P EUR C COMPUT VIS
   [Anonymous], 2016, ARXIV PREPRINT ARXIV
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2016, ARXIV
   [Anonymous], ARXIV171201059
   [Anonymous], 2015, ARXIV150401942
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2017, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2017.394
   [Anonymous], 2017, 2017 14 IEEE INT C A, DOI DOI 10.1109/AVSS.2017.8078481
   [Anonymous], IEEE T MULTIMEDIA
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2017, ARXIV170903572
   [Anonymous], PROC 12TH IEEE INT W
   [Anonymous], 2014, ARXIV14097618
   [Anonymous], 2017, ARXIV170307402
   [Anonymous], ARXIV170600672
   [Anonymous], 2015, PROC 28 INT C NEURAL
   Baisa NL, 2018, PROCEEDINGS OF THE 13TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2018), VOL 5: VISAPP, P429, DOI 10.5220/0006564504290438
   Ban YT, 2016, LECT NOTES COMPUT SC, V9914, P52, DOI 10.1007/978-3-319-48881-3_5
   Barnard M, 2014, IEEE T MULTIMEDIA, V16, P864, DOI 10.1109/TMM.2014.2301977
   Berclaz J, 2011, IEEE T PATTERN ANAL, V33, P1806, DOI 10.1109/TPAMI.2011.21
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Choi WG, 2015, IEEE I CONF COMP VIS, P3029, DOI 10.1109/ICCV.2015.347
   Chu CT, 2013, IEEE T MULTIMEDIA, V15, P1602, DOI 10.1109/TMM.2013.2266634
   Cui P, 2009, IEEE T MULTIMEDIA, V11, P333, DOI 10.1109/TMM.2008.2009722
   D'Orazio T, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P559, DOI 10.1109/AVSS.2009.69
   D'Orazio Tiziana., 2007, Image Analysis for Multimedia Interactive Services, P34
   DEHGHAN A, 2015, PROC CVPR IEEE, P4091, DOI DOI 10.1109/CVPR.2015
   Dicle C, 2013, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2013.286
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Feng PM, 2017, IEEE T MULTIMEDIA, V19, P725, DOI 10.1109/TMM.2016.2638206
   Geiger A, 2014, IEEE T PATTERN ANAL, V36, P1012, DOI 10.1109/TPAMI.2013.185
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Karanam S., 2016, ARXIV PREPRINT ARXIV
   Kim C, 2015, IEEE I CONF COMP VIS, P4696, DOI 10.1109/ICCV.2015.533
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Lan XY, 2018, IEEE T IMAGE PROCESS, V27, P2022, DOI 10.1109/TIP.2017.2777183
   Lan XY, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2481325
   Lan XY, 2014, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2014.156
   Leibe B, 2008, IEEE T PATTERN ANAL, V30, P1683, DOI 10.1109/TPAMI.2008.170
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mazzeo P. L., 2008, 2008 IEEE Fifth International Conference on Advanced Video and Signal Based Surveillance, P326, DOI 10.1109/AVSS.2008.33
   Milan A, 2014, IEEE T PATTERN ANAL, V36, P58, DOI 10.1109/TPAMI.2013.103
   Milan A, 2013, PROC CVPR IEEE, P3682, DOI 10.1109/CVPR.2013.472
   Osep Aljosa, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P1988, DOI 10.1109/ICRA.2017.7989230
   Pirsiavash H, 2011, PROC CVPR IEEE, P1201, DOI 10.1109/CVPR.2011.5995604
   Possegger H, 2014, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2014.170
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Rezatofighi SH, 2016, PROC CVPR IEEE, P136, DOI 10.1109/CVPR.2016.22
   Rezatofighi SH, 2015, IEEE I CONF COMP VIS, P3047, DOI 10.1109/ICCV.2015.349
   Sanchez-Matilla R, 2016, LECT NOTES COMPUT SC, V9914, P84, DOI 10.1007/978-3-319-48881-3_7
   Simonyan K., 2014, 14091556 ARXIV
   Song Y., 2016, 2016 IEEE INT C VEHI, P1, DOI DOI 10.1109/ICVES.2016.7548171
   Xiang Y, 2015, IEEE I CONF COMP VIS, P4705, DOI 10.1109/ICCV.2015.534
   Yang B, 2012, PROC CVPR IEEE, P1918, DOI 10.1109/CVPR.2012.6247892
   Yoon JH, 2016, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR.2016.155
   Yu FW, 2016, LECT NOTES COMPUT SC, V9914, P36, DOI 10.1007/978-3-319-48881-3_3
   Yuan Li, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2953, DOI 10.1109/CVPRW.2009.5206735
   Zhang L, 2008, INT C WAVEL ANAL PAT, P11, DOI 10.1109/ICWAPR.2008.4635742
   Zhang SP, 2017, IEEE T CIRC SYST VID, V27, P421, DOI 10.1109/TCSVT.2016.2539860
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Z., 2017, IEEE Trans. on Circuits and Systems for Video Technology
NR 64
TC 64
Z9 68
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2019
VL 21
IS 5
BP 1183
EP 1194
DI 10.1109/TMM.2018.2875360
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HV8HY
UT WOS:000466223600009
DA 2024-07-18
ER

PT J
AU Pang, SM
   Ma, J
   Zhu, JH
   Xue, JR
   Tian, Q
AF Pang, Shanmin
   Ma, Jin
   Zhu, Jihua
   Xue, Jianru
   Tian, Qi
TI Improving Object Retrieval Quality by Integration of Similarity
   Propagation and Query Expansion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object retrieval; query expansion; similarity propagation; re-ranking
ID IMAGE RETRIEVAL; SCALE
AB Re-ranking is an essential step for accurate image retrieval, due to its well-known power in performance improvement. Although numerous works have been proposed for re-ranking, many of them are only customized for a certain image representation model. In contrast to most existing techniques, we develop generalized re-ranking algorithms that are applicable to different kinds of image encodings in this paper. We first employ a quite successful theory of similarity propagation to reconstruct vectors of a query and its top ranked images and, subsequently, get a re-ranked list by comparing the new image vectors. Furthermore, considering that the just mentioned strategy is directly compatible with query expansion and, thus, in order to leverage advantages of this milestone, we then propose integrating them into a unified framework for maximizing re-ranking benefits. Our re-ranking algorithms are memory and computation efficient, and experimental results on benchmark datasets demonstrate that they compare favorably with the state of the art. Our code is available at https://github.com/MaJinWakeUp/rerank.
C1 [Pang, Shanmin; Ma, Jin; Zhu, Jihua] Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Shaanxi, Peoples R China.
   [Xue, Jianru] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Huawei Noahs Ark Lab, San Antonio, TX 78249 USA.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; University of
   Texas System; University of Texas at San Antonio (UTSA); University of
   Texas System; University of Texas at San Antonio (UTSA)
RP Pang, SM (corresponding author), Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Shaanxi, Peoples R China.; Tian, Q (corresponding author), Univ Texas San Antonio, Huawei Noahs Ark Lab, San Antonio, TX 78249 USA.; Tian, Q (corresponding author), Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
EM pangsm@xjtu.edu.cn; m799133891@stu.xjtu.edu.cn; zhujh@xjtu.edu.cn;
   jrxue@xjtu.edu.cn; qi.tian@utsa.edu
RI Pang, Shanmin/KBQ-6978-2024
OI Ma, Jin/0000-0002-4089-6898
FU National Key Research and Development Plan [2016YFB1001004]; National
   Natural Science Foundation of China [61603289, 61429201]; China
   Postdoctoral Science Foundation [2016M602823]; Fundamental Research
   Funds for the Central Universities [xjj2017118]; ARO [W911NF-15-1-0290];
   NEC Laboratory of America; NEC Laboratory of Blippar
FX This work was supported in part by the National Key Research and
   Development Plan under Grant 2016YFB1001004, in part by the National
   Natural Science Foundation of China under Grants 61603289 and 61429201,
   in part by the China Postdoctoral Science Foundation under Grant
   2016M602823, and in part by the Fundamental Research Funds for the
   Central Universities under Grant xjj2017118. The work of Q. Tian was
   supported in part by the ARO under Grant W911NF-15-1-0290 and in part by
   the Faculty Research Gift Awards by NEC Laboratories of America and
   Blippar. The associate editor coordinating the review of this manuscript
   and approving it for publication was Prof. Benoit Huet. (Corresponding
   authors: Shanmin Pang; Qi Tian.)
CR Ah-Pine J, 2015, ACM T INFORM SYST, V33, DOI 10.1145/2699668
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], 1999, WEB C
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Battiato S, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P397, DOI 10.1145/2911996.2912024
   Chadha A, 2017, IEEE T MULTIMEDIA, V19, P1596, DOI 10.1109/TMM.2017.2673415
   Chen ZX, 2018, IEEE T MULTIMEDIA, V20, P2126, DOI 10.1109/TMM.2017.2785253
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2011, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2011.5995601
   Gao ZN, 2016, IEEE T MULTIMEDIA, V18, P1661, DOI 10.1109/TMM.2016.2568748
   Gialampoukidis I, 2017, MULTIMED TOOLS APPL, V76, P22383, DOI 10.1007/s11042-017-4797-4
   Hsu WinstonH., 2007, ACM MM
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2014, PROC CVPR IEEE, P3310, DOI 10.1109/CVPR.2014.417
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Kalantidis Yannis, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P685, DOI 10.1007/978-3-319-46604-0_48
   Langville AN, 2005, SIAM REV, V47, P135, DOI 10.1137/S0036144503424786
   Li XC, 2015, PROC CVPR IEEE, P5153, DOI 10.1109/CVPR.2015.7299151
   Li ZY, 2018, MED IMAGE ANAL, V43, P66, DOI 10.1016/j.media.2017.09.007
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu DY, 2017, IEEE T MULTIMEDIA, V19, P1299, DOI 10.1109/TMM.2016.2646181
   Lynch C, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P541, DOI 10.1145/2939672.2939728
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Mikulik A, 2013, INT J COMPUT VISION, V103, P163, DOI 10.1007/s11263-012-0600-1
   Mohedano E, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P327, DOI 10.1145/2911996.2912061
   Murray N, 2017, IEEE T PATTERN ANAL, V39, P1797, DOI 10.1109/TPAMI.2016.2615621
   Pang SM, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P59, DOI 10.1145/3126686.3126704
   Pang SM, 2018, PATTERN RECOGN, V83, P150, DOI 10.1016/j.patcog.2018.05.010
   Pang SM, 2018, SIGNAL PROCESS-IMAGE, V63, P1, DOI 10.1016/j.image.2018.01.004
   Pang SM, 2015, NEUROCOMPUTING, V165, P423, DOI 10.1016/j.neucom.2015.03.040
   Pang SM, 2014, COMPUT VIS IMAGE UND, V128, P51, DOI 10.1016/j.cviu.2014.06.006
   Perronnin F, 2010, PROC CVPR IEEE, P3384, DOI 10.1109/CVPR.2010.5540009
   Philbin J., 2008, PROC IEEE C COMPUT V, P1
   Razavian A. S., 2016, ITE Trans. Media Technol. Appl., V4, P251, DOI [DOI 10.3169/MTA.4.251, 10.3169/mta.4.251]
   Safadi Bahjat, 2014, ICMR 2014 P ACM INT, P265
   Shen XH, 2014, IEEE T PATTERN ANAL, V36, P1229, DOI 10.1109/TPAMI.2013.237
   Siddiquie Behjat., 2014, Proceedings of the International Conference on Multimedia Retrieval, page, P321
   Simonyan K., 2014, 14091556 ARXIV
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Song YF, 2016, IEEE T MULTIMEDIA, V18, P1542, DOI 10.1109/TMM.2016.2568743
   Tolias G., 2016, Conference Track Proceedings,
   Tolias G, 2013, IEEE I CONF COMP VIS, P1401, DOI 10.1109/ICCV.2013.177
   Valsesia D, 2015, IEEE T MULTIMEDIA, V17, P1439, DOI 10.1109/TMM.2015.2455417
   van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Xie HT, 2014, IEEE T MULTIMEDIA, V16, P1104, DOI 10.1109/TMM.2014.2305909
   Xu J., 2018, 2018 IEEE 19th Workshop on Control and Modeling for Power Electronics (COMPEL), P1, DOI [10.1109/COMPEL.2018.8459937, DOI 10.1109/COMPEL.2018.8459937, 10.1109/COMPEL.2018.8460140]
   Zhang ST, 2015, IEEE T PATTERN ANAL, V37, P803, DOI 10.1109/TPAMI.2014.2346201
   Zhang W, 2015, IEEE T MULTIMEDIA, V17, P1236, DOI 10.1109/TMM.2015.2440997
   Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528
   Zhong ZY, 2015, IEEE T MULTIMEDIA, V17, P1391, DOI 10.1109/TMM.2015.2446201
   Zhou DY, 2004, ADV NEUR IN, V16, P169
NR 55
TC 10
Z9 11
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2019
VL 21
IS 3
BP 760
EP 770
DI 10.1109/TMM.2018.2866230
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HN6YJ
UT WOS:000460333800019
DA 2024-07-18
ER

PT J
AU Liu, YP
   Long, Z
   Zhu, C
AF Liu, Yipeng
   Long, Zhen
   Zhu, Ce
TI Image Completion Using Low Tensor Tree Rank and Total Variation
   Minimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Tensor tree decomposition; tensor completion; image denoise; total
   variation; low rank tensor approximation
ID MONTE-CARLO ALGORITHMS; MATRIX; APPROXIMATION; FACTORIZATION; RECOVERY;
   DECOMPOSITIONS; OPTIMIZATION
AB Tensor completion recovers missing entries of multiway data. Most of the current methods exploit the low-rank tensor structure for image completion applications. In this paper, we simultaneously exploit the globally multidimensional structure and locally piecewise smoothness to further enhance the performance. In the proposed optimization model, the low tensor tree rank minimization is used for the global data structure, and the total variation minimization is used for the local structure. Two kinds of total variation functions are discussed. The optimization problem is transformed into several subproblems by alternating direction method of multipliers. The subproblem on low tensor tree rank minimization is solved by singular value thresholding, and the subproblem on total variation minimization can be solved by soft thresholding. Numerical experiments on color images and light field images demonstrate that the proposed method outperforms most of the state-of-the-art methods in terms of recovery accuracy and computational complexity.
C1 [Liu, Yipeng; Long, Zhen; Zhu, Ce] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Sichuan, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Liu, YP (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Sichuan, Peoples R China.
EM yipengliu@uestc.edu.cn; longzhen@std.uestc.edu.cn; eczhu@uestc.edu.cn
RI Zhu, Ce/AEN-1875-2022; Liu, Yipeng/M-5434-2016
OI Liu, Yipeng/0000-0003-2084-8781
FU National Natural Science Foundation of China [61602091, 61571102];
   Fundamental Research Funds for the Central Universities [ZYGX2016J199,
   ZYGX2014Z003]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61602091 and 61571102 and in part by
   the Fundamental Research Funds for the Central Universities under Grants
   ZYGX2016J199 and ZYGX2014Z003. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Enrico Magli.
CR Acar E, 2011, CHEMOMETR INTELL LAB, V106, P41, DOI 10.1016/j.chemolab.2010.08.004
   [Anonymous], 1998, MODELS ALGORITHMS
   Ballani J, 2013, LINEAR ALGEBRA APPL, V438, P639, DOI 10.1016/j.laa.2011.08.010
   Bengua JA, 2016, 2016 10TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND COMMUNICATION SYSTEMS (ICSPCS), DOI 10.1109/ICSPCS.2016.7843326
   Bengua JA, 2017, IEEE T IMAGE PROCESS, V26, P2466, DOI 10.1109/TIP.2017.2672439
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Cao WF, 2016, IEEE T IMAGE PROCESS, V25, P4075, DOI 10.1109/TIP.2016.2579262
   Chambolle A, 2004, J MATH IMAGING VIS, V20, P89
   Cichocki A., 2009, NONNEGATIVE MATRIX T, DOI 10.1002/9780470747278
   Cichocki A, 2007, LECT NOTES COMPUT SC, V4666, P169
   Cichocki A, 2015, IEEE SIGNAL PROC MAG, V32, P145, DOI 10.1109/MSP.2013.2297439
   Da Silva C, 2015, LINEAR ALGEBRA APPL, V481, P131, DOI 10.1016/j.laa.2015.04.015
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1324, DOI 10.1137/S0895479898346995
   Drineas P, 2006, SIAM J COMPUT, V36, P158, DOI 10.1137/S0097539704442696
   Du B, 2017, IEEE T MULTIMEDIA, V19, P67, DOI 10.1109/TMM.2016.2608780
   Filipovic M, 2015, MULTIDIM SYST SIGN P, V26, P677, DOI 10.1007/s11045-013-0269-9
   Franz T, 2009, LECT NOTES COMPUT SC, V5823, P213, DOI 10.1007/978-3-642-04930-9_14
   Frieze A, 2004, J ACM, V51, P1025, DOI 10.1145/1039488.1039494
   Gandy S, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/2/025010
   Geng X, 2011, IEEE T SYST MAN CY B, V41, P881, DOI 10.1109/TSMCB.2010.2097588
   Hackbusch W, 2009, J FOURIER ANAL APPL, V15, P706, DOI 10.1007/s00041-009-9094-9
   Han X, 2014, ABSTR APPL ANAL, DOI 10.1155/2014/765782
   Hillar CJ, 2013, J ACM, V60, DOI 10.1145/2512329
   Hitchcock F. L., 1927, J MATH PHYS, V6, P189, DOI [10.1002/sapm192761164, DOI 10.1002/SAPM192761164]
   Ji TY, 2016, INFORM SCIENCES, V326, P243, DOI 10.1016/j.ins.2015.07.049
   Kamal MH, 2016, COMPUT VIS IMAGE UND, V145, P172, DOI 10.1016/j.cviu.2015.11.004
   Karatzoglou Alexandros, 2010, Multiverse recommendation: N-dimensional tensor factorization for context-aware collaborative filtering, P79, DOI DOI 10.1145/1864708.1864727
   Kilmer ME, 2013, SIAM J MATRIX ANAL A, V34, P148, DOI 10.1137/110837711
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Kressner D, 2014, BIT, V54, P447, DOI 10.1007/s10543-013-0455-z
   Le Pendu M, 2018, IEEE T IMAGE PROCESS, V27, P1981, DOI 10.1109/TIP.2018.2791864
   Lin Z., 2011, ADV NEURAL INFORM PR, V24, P612, DOI DOI 10.1007/S11263-013-0611-6
   Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39
   Liu YP, 2017, IEEE T MED IMAGING, V36, P2148, DOI 10.1109/TMI.2017.2717502
   Liu YP, 2015, IEEE T BIO-MED ENG, V62, P2055, DOI 10.1109/TBME.2015.2411672
   Liu YP, 2013, IEEE T BIO-MED ENG, V60, P2794, DOI 10.1109/TBME.2013.2264772
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mu C, 2014, PR MACH LEARN RES, V32, P73
   Nie XS, 2017, IEEE T MULTIMEDIA, V19, P785, DOI 10.1109/TMM.2016.2629758
   Osher S, 2005, MULTISCALE MODEL SIM, V4, P460, DOI 10.1137/040605412
   Rauhut H, 2017, LINEAR ALGEBRA APPL, V523, P220, DOI 10.1016/j.laa.2017.02.028
   Rauhut H, 2015, APPL NUMER HARMON AN, P419, DOI 10.1007/978-3-319-16042-9_14
   Romera-Paredes B., 2013, Adv. Neural Inf. Process. Syst., P2967
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Signoretto M, 2011, IEEE SIGNAL PROC LET, V18, P403, DOI 10.1109/LSP.2011.2151856
   TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464
   Vogel CR, 1996, SIAM J SCI COMPUT, V17, P227, DOI 10.1137/0917016
   Wang WQ, 2017, IEEE I CONF COMP VIS, P5698, DOI 10.1109/ICCV.2017.607
   Xu YY, 2015, INVERSE PROBL IMAG, V9, P601, DOI 10.3934/ipi.2015.9.601
   Xu YY, 2013, SIAM J IMAGING SCI, V6, P1758, DOI 10.1137/120887795
   Yang Y., 2015, ARXIV150302216
   Yang YN, 2015, IEEE SIGNAL PROC LET, V22, P1633, DOI 10.1109/LSP.2015.2420592
   Yokota T., 2017, P IEEE C COMP VIS PA, P3732
   Yokota T, 2016, IEEE T SIGNAL PROCES, V64, P5423, DOI 10.1109/TSP.2016.2586759
   Yuan QQ, 2012, IEEE T GEOSCI REMOTE, V50, P3660, DOI 10.1109/TGRS.2012.2185054
   Zhang ZM, 2014, PROC CVPR IEEE, P3842, DOI 10.1109/CVPR.2014.485
   Zhao Q., 2016, Tensor Ring Decomposition
   Zhao QB, 2016, IEEE T NEUR NET LEAR, V27, P736, DOI 10.1109/TNNLS.2015.2423694
   Zhao QB, 2015, IEEE T PATTERN ANAL, V37, P1751, DOI 10.1109/TPAMI.2015.2392756
NR 59
TC 61
Z9 64
U1 0
U2 66
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2019
VL 21
IS 2
BP 338
EP 350
DI 10.1109/TMM.2018.2859026
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HJ7GT
UT WOS:000457364400006
DA 2024-07-18
ER

PT J
AU Xiao, HX
   Feng, JS
   Wei, YC
   Zhang, MJ
   Yan, SC
AF Xiao, Huaxin
   Feng, Jiashi
   Wei, Yunchao
   Zhang, Maojun
   Yan, Shuicheng
TI Deep Salient Object Detection With Dense Connections and Distraction
   Diagnosis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Neural networks; deep learning; salient object detection; model
   interpretability
AB In this paper, we propose two novel components for improving deep salient object detection models. The first component, called saliency detection network (S-Net), introduces dense short- and long-range connections that effectively integrate multiscale features to better exploit contexts at multiple levels. Benefiting from the direct access to low- and high-level features, the S-Net can not only exploit the object context but also preserve the object boundary sharply, leading to enhanced saliency detection performance. Second, a distraction detection network (D-Net) is developed to learn to diagnose which regions of an input image are distracting and harmful for saliency prediction of the S-Net. With such distraction diagnosis, the regions that are distracting to S-Net are removed in hindsight from the input image and the resulted distraction-free image is fed to S-Net for saliency prediction. To train the D-Net, a distraction mining approach is proposed to localize the model-specific distracting regions through examining the sensitiveness of the S-Net to image regions in a principled manner. Besides, the distraction mining approach also provides a way to interpret decisions made by deep neural network (DNN) saliency detection models, which relieves the black-box issues of DNNs to some extent. Extensive experiments on seven popular benchmark datasets demonstrate the effectiveness of the combined S-Net and D-Net, which provides new state of the arts.
C1 [Xiao, Huaxin; Zhang, Maojun] Natl Univ Def Technol, Coll Informat Syst & Management, Changsha 410073, Hunan, Peoples R China.
   [Feng, Jiashi; Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117583, Singapore.
   [Wei, Yunchao] Univ Illinois Urbana Champaign Urbana, Beckman Inst, Urbana, IL 61801 USA.
C3 National University of Defense Technology - China; National University
   of Singapore
RP Xiao, HX (corresponding author), Natl Univ Def Technol, Coll Informat Syst & Management, Changsha 410073, Hunan, Peoples R China.
EM xiaohuaxin@nudt.edu.cn; elefjia@nus.edu.sg; wychao1987@gmail.com;
   mjzbang@nudt.edu.cn; eleyans@nus.edu.sg
RI Feng, Jiashi/AGX-6209-2022; Yan, Shuicheng/HCI-1431-2022
OI Xiao, Huaxin/0000-0003-4524-2698; Feng, Jiashi/0000-0001-6843-0064
FU MOE Tier-I [R-263-000-C21-112]; NUS [R-263-000-C67-646,
   R-263-000-C08-133]; ECRA [R-263-000-C87-133]
FX The work of J. Feng was supported in part by NUS startup
   R-263-000-C08-133, in part by MOE Tier-I R-263-000-C21-112, in part by
   NUS IDS R-263-000-C67-646, and in part by ECRA R-263-000-C87-133.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2011, ADV NEURAL INF PROCE
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2018, P IEEE 21 INT C ELEC
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Hu P, 2017, PROC CVPR IEEE, P540, DOI 10.1109/CVPR.2017.65
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang J, 2011, IEEE T MULTIMEDIA, V13, P653, DOI 10.1109/TMM.2011.2127463
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kim J, 2016, LECT NOTES COMPUT SC, V9908, P455, DOI 10.1007/978-3-319-46493-0_28
   Kuen J, 2016, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2016.399
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78
   Lei JJ, 2016, IEEE T MULTIMEDIA, V18, P1783, DOI 10.1109/TMM.2016.2592325
   Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2016, IEEE T IMAGE PROCESS, V25, P5012, DOI 10.1109/TIP.2016.2602079
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li HY, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440174
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Ma C, 2017, IEEE T MULTIMEDIA, V19, P2415, DOI 10.1109/TMM.2017.2694219
   Mahadevan V, 2009, PROC CVPR IEEE, P1007, DOI 10.1109/CVPRW.2009.5206573
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Simonyan K., 2014, 14091556 ARXIV
   Tang YB, 2016, LECT NOTES COMPUT SC, V9912, P809, DOI 10.1007/978-3-319-46484-8_49
   Tang YX, 2017, IEEE T MULTIMEDIA, V19, P393, DOI 10.1109/TMM.2016.2614862
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50
   Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433
   Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687
   Wei YC, 2017, IEEE T PATTERN ANAL, V39, P2314, DOI 10.1109/TPAMI.2016.2636150
   Wei YC, 2016, PATTERN RECOGN, V59, P234, DOI 10.1016/j.patcog.2016.01.015
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yujun W., 2018, 2018 10 IAPR WORKSH, P1
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhang YT, 2016, IEEE T MULTIMEDIA, V18, P1604, DOI 10.1109/TMM.2016.2568138
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
NR 52
TC 48
Z9 50
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2018
VL 20
IS 12
BP 3239
EP 3251
DI 10.1109/TMM.2018.2830098
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HA4EY
UT WOS:000450212600005
DA 2024-07-18
ER

PT J
AU Zou, Y
   Zhao, X
   Liu, YC
AF Zou, Yi
   Zhao, Xu
   Liu, Yuncai
TI Measuring Crowd Collectiveness by Macroscopic and Microscopic Motion
   Consistencies
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Crowd motion analysis; collectiveness; collective group; motion
   consistency; maximum consistency path
AB As a scene-independent descriptor of crowd motions, crowd collectiveness quantifies the degree of constituent individuals moving as a union in a crowd scene. An effective measurement on crowd collectiveness is of great importance for applications in surveillance of public safety, human dynamics, and other areas. To this end, we propose a novel framework to measure crowd collectiveness by combining macroscopic and microscopic motion consistencies and define quantitatively the global and local consistency of crowd motions. The defined global consistency represents the likelihood of pairwise individuals belonging to the same collective group, whereas the local consistency reflects the degree of conformity in a local region. Based on the proposed collectiveness measure, a new algorithm, named group mining, is proposed to detect collective groups from a crowd. We validate the effectiveness of the proposed method on several synthetic particle systems and a real-world crowd database with human-labeled collectiveness. Experimental results show that, compared with the previous approaches, our collectiveness measure is more consistent with human perception, and the collective groups detected by our group mining algorithm are more accurate and robust.
C1 [Zou, Yi; Zhao, Xu; Liu, Yuncai] Shanghai Jiao Tong Univ, Minist Educ China, Dept Automat, Shanghai 200240, Peoples R China.
   [Zou, Yi; Zhao, Xu; Liu, Yuncai] Shanghai Jiao Tong Univ, Minist Educ China, Key Lab Syst Control & Informat Proc, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University
RP Zhao, X; Liu, YC (corresponding author), Shanghai Jiao Tong Univ, Minist Educ China, Dept Automat, Shanghai 200240, Peoples R China.; Zhao, X; Liu, YC (corresponding author), Shanghai Jiao Tong Univ, Minist Educ China, Key Lab Syst Control & Informat Proc, Shanghai 200240, Peoples R China.
EM jbyiiiii@sjtu.edu.cn; zhaoxu@sjtu.edu.cn; whomliu@sjtu.edu.cn
OI Zou, Yi/0000-0002-4078-8248
FU National Natural Science Foundation of China [61375019, 61673269,
   61273285]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61375019, 61673269, and 61273285. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Elisa Ricci. (Corresponding author: Xu Zhao, Yuncai
   Liu.)
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Ali S, 2008, LECT NOTES COMPUT SC, V5303, P1, DOI 10.1007/978-3-540-88688-4_1
   Ali S, 2007, PROC CVPR IEEE, P65
   [Anonymous], P 2008 IEEE COMP SOC, DOI DOI 10.1109/CVPR.2008.4587718
   [Anonymous], 2006, COMPUTER VISION PATT, DOI 10.1109/CVPR.2006.92
   Bolei Zhou, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3441, DOI 10.1109/CVPR.2011.5995459
   Brostow G.J., 2006, CVPR, P594, DOI DOI 10.1109/CVPR.2006.320
   Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21
   Chan AB, 2008, IEEE T PATTERN ANAL, V30, P909, DOI 10.1109/TPAMI.2007.70738
   Couzin I, 2007, NATURE, V445, P715, DOI 10.1038/445715a
   Feinerman O, 2017, J EXP BIOL, V220, P73, DOI 10.1242/jeb.143891
   Ge WN, 2012, IEEE T PATTERN ANAL, V34, P1003, DOI 10.1109/TPAMI.2011.176
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Hu M, 2008, INT C PATT RECOG, P9
   Hughes RL, 2003, ANNU REV FLUID MECH, V35, P169, DOI 10.1146/annurev.fluid.35.101101.161136
   Kim K, 2011, IEEE I CONF COMP VIS, P1164, DOI 10.1109/ICCV.2011.6126365
   Kratz L, 2012, IEEE T PATTERN ANAL, V34, P987, DOI 10.1109/TPAMI.2011.173
   Kratz L, 2009, PROC CVPR IEEE, P1446, DOI 10.1109/CVPRW.2009.5206771
   Li XL, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2854000
   Lin D, 2010, CHILD ISS LAWS PROGR, P1
   Liu WX, 2016, IEEE T MULTIMEDIA, V18, P2398, DOI 10.1109/TMM.2016.2598091
   Mahadevan V, 2010, PROC CVPR IEEE, P1975, DOI 10.1109/CVPR.2010.5539872
   Mehran R, 2010, LECT NOTES COMPUT SC, V6313, P439
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Montes de Oca MA, 2011, SWARM INTELL-US, V5, P305, DOI 10.1007/s11721-011-0062-z
   Morris BT, 2011, IEEE T PATTERN ANAL, V33, P2287, DOI 10.1109/TPAMI.2011.64
   Moussaid M, 2009, TOP COGN SCI, V1, P469, DOI 10.1111/j.1756-8765.2009.01028.x
   Rodriguez M, 2009, IEEE I CONF COMP VIS, P1389, DOI 10.1109/ICCV.2009.5459301
   Saleemi I, 2010, PROC CVPR IEEE, P2069, DOI 10.1109/CVPR.2010.5539884
   Shao J, 2014, PROC CVPR IEEE, P2227, DOI 10.1109/CVPR.2014.285
   Sniedovich M, 2006, CONTROL CYBERN, V35, P599
   VICSEK T, 1995, PHYS REV LETT, V75, P1226, DOI 10.1103/PhysRevLett.75.1226
   Wang CJ, 2015, IEEE IMAGE PROC, P1805, DOI 10.1109/ICIP.2015.7351112
   Wang XG, 2011, INT J COMPUT VISION, V95, P287, DOI 10.1007/s11263-011-0459-6
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P539, DOI 10.1109/TPAMI.2008.87
   Wu SD, 2010, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2010.5539882
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P346, DOI 10.1109/TMM.2015.2393635
   Zhang C, 2016, IEEE T MULTIMEDIA, V18, P1048, DOI 10.1109/TMM.2016.2542585
   Zhou BL, 2015, INT J COMPUT VISION, V111, P50, DOI 10.1007/s11263-014-0735-3
   Zhou BL, 2012, LECT NOTES COMPUT SC, V7573, P857, DOI 10.1007/978-3-642-33709-3_61
   Zhou BL, 2014, IEEE T PATTERN ANAL, V36, P1586, DOI 10.1109/TPAMI.2014.2300484
   Zhou BL, 2013, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2013.392
NR 42
TC 4
Z9 5
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2018
VL 20
IS 12
BP 3311
EP 3323
DI 10.1109/TMM.2018.2832601
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HA4EY
UT WOS:000450212600011
DA 2024-07-18
ER

PT J
AU Shi, HC
   Li, HL
   Meng, FM
   Wu, QB
   Xu, LF
   Ngan, KN
AF Shi, Hengcan
   Li, Hongliang
   Meng, Fanman
   Wu, Qingbo
   Xu, Linfeng
   Ngan, King Ngi
TI Hierarchical Parsing Net: Semantic Scene Parsing From Global Scene to
   Objects
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantic scene parsing; scene-aware feature encoding; context learning
ID CO-SEGMENTATION
AB This paper proposes a novel Hierarchical Parsing Net (HPN) for semantic scene parsing. Unlike previous methods, which separately classify each object, HPN leverages global scene semantic information and the context among multiple objects to enhance scene parsing. On the one hand, HPN uses the global scene category to constrain the semantic consistency between the scene and each object. On the other hand, the context among all objects is also modeled to avoid incompatible object predictions. Specifically, HPN consists of four steps. In the first step, we extract scene and local appearance features. Based on these appearance features, the second step is to encode a contextual feature for each object, which models both the scene-object context (the context between the scene and each object) and the interobject context (the context among different objects). In the third step, we classify the global scene and then use the scene classification loss and a backpropagation algorithm to constrain the scene feature encoding. In the fourth step, a label map for scene parsing is generated from the local appearance and contextual features. Our model outperforms many state-of-the-art deep scene parsing networks on five scene parsing databases.
C1 [Shi, Hengcan; Li, Hongliang; Meng, Fanman; Wu, Qingbo; Xu, Linfeng; Ngan, King Ngi] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Sichuan, Peoples R China.
   [Ngan, King Ngi] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
C3 University of Electronic Science & Technology of China; Chinese
   University of Hong Kong
RP Li, HL (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Sichuan, Peoples R China.
EM shihengcan@gmail.com; hlli@uestc.edu.cn; fmmeng@uestc.edu.cn;
   qbwu@uestc.edu.cn; lfxu@uestc.edu.cn; knngan@ee.cuhk.edu.hk
RI Ngan, N/E-8240-2014; Xu, Linfeng/HME-1913-2023; Wu, Qingbo/AAF-6872-2019
OI Ngan, N/0000-0003-1946-3235; Xu, Linfeng/0000-0002-9934-0958; Wu,
   Qingbo/0000-0003-2936-6340; Li, Hongliang/0000-0002-7481-095X
FU National Natural Science Foundation of China [61525102, 61502084,
   61601102]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61525102, 61502084, and 61601102. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. David Crandall.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], P EUR C COMPUT VIS
   [Anonymous], 2016, P IJCAI
   [Anonymous], 2016, PROC INT C LEARN RE
   [Anonymous], P EUR C COMPUT VIS
   [Anonymous], PROC CVPR IEEE
   [Anonymous], P INT C LEARN REPR W
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], 2015, CoRR
   Bell S, 2015, PROC CVPR IEEE, P3479, DOI 10.1109/CVPR.2015.7298970
   Byeon W, 2015, PROC CVPR IEEE, P3547, DOI 10.1109/CVPR.2015.7298977
   Chen L.-C., 2017, IEEE C COMP VIS PATT
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai JF, 2015, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2015.7299025
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Gupta S, 2015, INT J COMPUT VISION, V112, P133, DOI 10.1007/s11263-014-0777-6
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   Hearst MA, 1998, IEEE INTELL SYST APP, V13, P18, DOI 10.1109/5254.708428
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Li HL, 2014, IEEE T CIRC SYST VID, V24, P789, DOI 10.1109/TCSVT.2013.2280851
   Li HL, 2013, IEEE T MULTIMEDIA, V15, P1896, DOI 10.1109/TMM.2013.2271476
   Li Z, 2016, LECT NOTES COMPUT SC, V9906, P541, DOI 10.1007/978-3-319-46475-6_34
   Liang XD, 2016, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2016.347
   Liang XD, 2016, LECT NOTES COMPUT SC, V9905, P125, DOI 10.1007/978-3-319-46448-0_8
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Luo B, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1187, DOI 10.1145/2733373.2806313
   Meng FM, 2015, IEEE T CIRC SYST VID, V25, P1735, DOI 10.1109/TCSVT.2015.2402891
   Meng FM, 2013, IEEE T IMAGE PROCESS, V22, P4809, DOI 10.1109/TIP.2013.2278461
   Meng FM, 2012, IEEE T MULTIMEDIA, V14, P1429, DOI 10.1109/TMM.2012.2197741
   Mostajahi M, 2015, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2015.7298959
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Pinheiro PO, 2014, PR MACH LEARN RES, V32
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Theis L, 2015, ADV NEUR IN, V28
   van den Oord A, 2016, PR MACH LEARN RES, V48
   Wang GR, 2017, PROC CVPR IEEE, P5235, DOI 10.1109/CVPR.2017.556
   Yao BZ, 2010, P IEEE, V98, P1485, DOI 10.1109/JPROC.2010.2050411
   Zeng K, 2009, ACM T GRAPHIC, V29, DOI 10.1145/1640443.1640445
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
NR 48
TC 23
Z9 24
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2018
VL 20
IS 10
SI SI
BP 2670
EP 2682
DI 10.1109/TMM.2018.2812600
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GT9YH
UT WOS:000444903000011
DA 2024-07-18
ER

PT J
AU Narwaria, M
   Krasula, L
   Le Callet, P
AF Narwaria, Manish
   Krasula, Lukas
   Le Callet, Patrick
TI Data Analysis in Multimedia Quality Assessment: Revisiting the
   Statistical Tests
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Assumption of normality; homogeneity of variance; multimedia quality;
   statistical analysis
ID VIDEO
AB Assessment of multimedia quality relies heavily on subjective assessment, and is typically done by human subjects in the form of preferences or continuous ratings. Such data are crucial for analysis of different multimedia-processing algorithms as well as validation of objective (computational) methods for the said purpose. To that end, statistical testing provides a theoretical framework toward drawing meaningful inferences, and making well-grounded conclusions and recommendations. While parametric tests (such as t test, ANOVA, and error estimates like confidence intervals) are popular and widely used in the community, there appears to be a certain degree of confusion in the application of such tests. Specifically, the assumptions of normality and homogeneity of variance are often not well understood, leading to incorrect application and/or interpretation of the statistical test results. Therefore, the main goal of this paper is to present new guidelines toward proper use of statistical tests and, hence, fix some of the issues in multimedia quality assessment. The said guidelines are derived based on theoretical analysis of sampling distribution of test statistics, and consider practical aspects of data analysis in the said domain. Experimental results on both simulated and real data are presented to support the arguments made. Software that implements the said recommendations is also made publicly available, in order to help researchers and practitioners perform correct statistical comparison of models.
C1 [Narwaria, Manish] Dhirubhai Ambani Inst Informat & Commun Technol, Gandhinagar 382007, India.
   [Krasula, Lukas; Le Callet, Patrick] Univ Nantes, IPI Grp LS2N, F-44306 Nantes, France.
C3 Dhirubhai Ambani Institute of Information & Communication Technology;
   Nantes Universite
RP Narwaria, M (corresponding author), Dhirubhai Ambani Inst Informat & Commun Technol, Gandhinagar 382007, India.
EM manish_narwaria@daiict.ac.in; lukas.krasula@univ-nantes.fr;
   patrick.lecallet@univ-nantes.fr
RI Le Callet, Patrick/F-5772-2010
OI Narwaria, Manish/0000-0001-7789-5322
CR [Anonymous], P WORKSH QOE MULT CO
   [Anonymous], 2012, TECH REP
   [Anonymous], 2003, Final report from the video quality experts group on the validation of objective models of video quality assessment
   [Anonymous], 2015, TECH REP
   [Anonymous], 2009, TECH REP
   Belmudez B., 2016, T LABS SERIES TELECO
   Coverdale P, 2011, IEEE SIGNAL PROC MAG, V28, P91, DOI 10.1109/MSP.2011.942467
   Gu K, 2017, IEEE T IMAGE PROCESS, V26, P4005, DOI 10.1109/TIP.2017.2711279
   Gu K, 2016, IEEE T BROADCAST, V62, P446, DOI 10.1109/TBC.2015.2511624
   Hossfeld T, 2011, INT WORK QUAL MULTIM, P131, DOI 10.1109/QoMEX.2011.6065690
   Hossfeld T, 2014, IEEE T MULTIMEDIA, V16, P541, DOI 10.1109/TMM.2013.2291663
   ITU-T Tutorial, 2005, TECH REP
   Janowski L, 2015, IEEE T MULTIMEDIA, V17, P2210, DOI 10.1109/TMM.2015.2484963
   Krasula L, 2017, IEEE J-STSP, V11, P64, DOI 10.1109/JSTSP.2016.2637168
   Li LD, 2016, IEEE T MULTIMEDIA, V18, P1085, DOI 10.1109/TMM.2016.2545398
   Lim WK, 2016, J MOD APPL STAT METH, V15, P67, DOI 10.22237/jmasm/1478001960
   Lumley T, 2002, ANNU REV PUBL HEALTH, V23, P151, DOI 10.1146/annurev.publhealth.23.100901.140546
   Melo M, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0094-1
   Min XK, 2017, INFORM SCIENCES, V420, P417, DOI 10.1016/j.ins.2017.08.040
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Min XK, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/2996463
   Mocanu DC, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.6.061208
   Moreno-Roldán JM, 2015, SENSORS-BASEL, V15, P31723, DOI 10.3390/s151229882
   Moss FM, 2016, IEEE T CIRC SYST VID, V26, P1977, DOI 10.1109/TCSVT.2015.2461971
   Narwaria M, 2013, OPT ENG, V52, DOI 10.1117/1.OE.52.10.102008
   Ninassi A, 2008, IEEE IMAGE PROC, P1180, DOI 10.1109/ICIP.2008.4711971
   Pflug G., 1983, STAT PROBABIL LETT, V1, P323
   Pinson MH, 2012, IEEE J-STSP, V6, P640, DOI 10.1109/JSTSP.2012.2215306
   Rainer B, 2017, IEEE T MULTIMEDIA, V19, P849, DOI 10.1109/TMM.2016.2629761
   Ribeiro FML, 2018, IEEE T MULTIMEDIA, V20, P1, DOI 10.1109/TMM.2017.2714425
   Roussas G.G., 2015, INTRO PROBABILITY ST
   Sawilowsky SS., 2002, J MOD APP STAT METH, V1, P461
   Schmider E, 2010, METHODOLOGY-EUR, V6, P147, DOI 10.1027/1614-2241/a000016
   Scott MJ, 2016, IEEE T MULTIMEDIA, V18, P1796, DOI 10.1109/TMM.2016.2574623
   Siahaan E, 2016, IEEE T MULTIMEDIA, V18, P1338, DOI 10.1109/TMM.2016.2559942
   Silva AF, 2016, IEEE T MULTIMEDIA, V18, P2446, DOI 10.1109/TMM.2016.2601027
   Tan TK, 2016, IEEE T CIRC SYST VID, V26, P76, DOI 10.1109/TCSVT.2015.2477916
   Tourancheau S, 2008, IEEE IMAGE PROC, P365, DOI 10.1109/ICIP.2008.4711767
   Welch BL, 1938, BIOMETRIKA, V29, P350, DOI 10.1093/biomet/29.3-4.350
   Wu CC, 2013, IEEE T MULTIMEDIA, V15, P1121, DOI 10.1109/TMM.2013.2241043
NR 40
TC 17
Z9 18
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2018
VL 20
IS 8
BP 2063
EP 2072
DI 10.1109/TMM.2018.2794266
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GN8BR
UT WOS:000439378600012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhao, Q
   Dai, F
   Ma, YK
   Wan, L
   Zhang, JW
   Zhang, YD
AF Zhao, Qiang
   Dai, Feng
   Ma, Yike
   Wan, Liang
   Zhang, Jiawan
   Zhang, Yongdong
TI Spherical Superpixel Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Superpixel; segmentation; spherical image; panorama; clustering;
   Hammersley; SLIC
ID RECOGNITION
AB These days, superpixel algorithms are widely used in computer vision and multimedia applications. However, existing algorithms are designed for planar images, which are less suited to deal with wide angle images. In this paper, we present a superpixel segmentation method for 360 degrees spherical images. Unlike previous methods, our approach explicitly considers the geometry for spherical images and makes clustering to spherical image pixels. It starts with the seeds defined by Hammersley points sampled on the sphere, then iterates between assignment step and update step, which are both based on the distance metric respecting spherical geometry. We evaluate our method on the transformed Berkeley segmentation dataset and panorama segmentation dataset collected by ourselves. Experimental results show that our method can gain better performance in terms of adherence to image boundaries and superpixel structural regularity. Furthermore, superpixels generated by our method can reserve the coherence across image boundaries and all have closed contours.
C1 [Zhao, Qiang; Dai, Feng; Ma, Yike; Zhang, Yongdong] Chinese Acad Sci, Key Lab Intelligent Informat Proc, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Zhao, Qiang] Tianjin Univ, Tianjin 300072, Peoples R China.
   [Wan, Liang; Zhang, Jiawan] Tianjin Univ, Sch Comp Software, Tianjin 300072, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Tianjin University; Tianjin University
RP Wan, L (corresponding author), Tianjin Univ, Sch Comp Software, Tianjin 300072, Peoples R China.
EM zhaoqiang@ict.ac.cn; fdai@ict.ac.cn; ykma@ict.ac.cn; lwan@tju.edu.cn;
   jwzhang@tju.edu.cn; zhyd@ict.ac.cn
OI Zhang, Jiawan/0000-0002-0667-6744; Zhao, Qiang/0000-0002-5867-3828
FU National Natural Science Foundation of China [61327013, 61572354,
   61525206, 61702479, 61771458]; Key Research Program of the Chinese
   Academy of Sciences [KFZD-SW-407]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61327013, 61572354, 61525206, 61702479,
   and 61771458, and in part by Key Research Program of the Chinese Academy
   of Sciences under Grant KFZD-SW-407.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2009, IEEE I CONF COMP VIS, DOI 10.1109/ICCV.2009.5459175
   Buss SR, 2001, ACM T GRAPHIC, V20, P95, DOI 10.1145/502122.502124
   Cabral R, 2014, PROC CVPR IEEE, P628, DOI 10.1109/CVPR.2014.546
   Chaurasia G, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487238
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Cui JJ, 1997, SIAM J SCI COMPUT, V18, P595, DOI 10.1137/S1064827595281344
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Fu HZ, 2014, IEEE T MULTIMEDIA, V16, P1165, DOI 10.1109/TMM.2014.2305571
   Hornik K, 2012, J STAT SOFTW, V50, P1
   Jiang HQ, 2015, IEEE T MULTIMEDIA, V17, P3, DOI 10.1109/TMM.2014.2368273
   Kosecka Jana, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P590, DOI 10.1007/978-3-642-37447-0_45
   Lei JJ, 2016, IEEE T MULTIMEDIA, V18, P1783, DOI 10.1109/TMM.2016.2592325
   Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96
   Li L, 2013, PROC CVPR IEEE, P3174, DOI 10.1109/CVPR.2013.408
   Liang H, 2014, IEEE T MULTIMEDIA, V16, P1241, DOI 10.1109/TMM.2014.2306177
   Liu S, 2014, IEEE T MULTIMEDIA, V16, P253, DOI 10.1109/TMM.2013.2285526
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Micusik Branislav, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2906, DOI 10.1109/CVPRW.2009.5206535
   Ming-Yu Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2097, DOI 10.1109/CVPR.2011.5995323
   Nowozin S, 2010, LECT NOTES COMPUT SC, V6316, P98, DOI 10.1007/978-3-642-15567-3_8
   OSSERMAN R, 1978, B AM MATH SOC, V84, P1182, DOI 10.1090/S0002-9904-1978-14553-4
   Pagani A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P375, DOI 10.1109/ICCVW.2011.6130266
   Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10
   Sakurada K., 2015, P BRIT MACH VIS C BM, P611
   Schick A, 2014, PATTERN RECOGN LETT, V43, P71, DOI 10.1016/j.patrec.2013.09.013
   Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1
   Tezuka S., 1995, Uniform Random Numbers. Theory and Practice
   Tighe J, 2010, LECT NOTES COMPUT SC, V6315, P352, DOI 10.1007/978-3-642-15555-0_26
   Van den Bergh M, 2012, LECT NOTES COMPUT SC, V7578, P13, DOI 10.1007/978-3-642-33786-4_2
   Vazquez-Reina A, 2010, LECT NOTES COMPUT SC, V6315, P268, DOI 10.1007/978-3-642-15555-0_20
   Veksler O, 2010, LECT NOTES COMPUT SC, V6315, P211, DOI 10.1007/978-3-642-15555-0_16
   Wang C, 2015, IEEE T MULTIMEDIA, V17, P29, DOI 10.1109/TMM.2014.2374357
   Wong T-T, 1997, J GRAPHICS TOOLS, V2, P9, DOI DOI 10.1080/10867651.1997.10487471
   Xiao JX, 2012, PROC CVPR IEEE, P2695, DOI 10.1109/CVPR.2012.6247991
   Yang H, 2016, PROC CVPR IEEE, P5422, DOI 10.1109/CVPR.2016.585
   Yu JG, 2016, IEEE T MULTIMEDIA, V18, P273, DOI 10.1109/TMM.2015.2505908
   Zeng G, 2011, IEEE I CONF COMP VIS, P447, DOI 10.1109/ICCV.2011.6126274
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P470, DOI 10.1109/TMM.2013.2293424
   Zhang YD, 2014, LECT NOTES COMPUT SC, V8694, P668, DOI 10.1007/978-3-319-10599-4_43
   Zhao Q., 2016, ENERGY CONVERSION C, P1
   Zhao Q, 2015, INT J COMPUT VISION, V113, P143, DOI 10.1007/s11263-014-0787-4
   Zhao Q, 2013, IEEE T MULTIMEDIA, V15, P1745, DOI 10.1109/TMM.2013.2280249
   Zitnick C. L., INT J COMPUT VIS, V75, P49
   Zorin D., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P257, DOI 10.1145/218380.218449
NR 45
TC 10
Z9 10
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1406
EP 1417
DI 10.1109/TMM.2017.2772842
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400010
DA 2024-07-18
ER

PT J
AU Zhou, ZQ
   Wang, B
   Ma, JL
AF Zhou, Zhiqiang
   Wang, Bo
   Ma, Jinlei
TI Scale-Aware Edge-Preserving Image Filtering via Iterative Global
   Optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Scale-aware smoothing; iterative global optimization (IGO);
   edge-preserving filter; gradient suppression
AB Presently, few filters are able to smooth images in a scale-aware manner like Gaussian filtering while not blurring the edges of large-scale features, whereas this kind of filter can be important in many visual applications requiring scale-aware manipulation while avoiding halos. In this paper, we propose a filtering technique through iterative global optimization (IGO), enabling to achieve both good scale-aware and edge-preserving performance. Our method is based on a filtering idea of selective gradient suppression and guidance gradient correction in the framework of IGO, which has the advantages of avoiding halos and preventing oversharpening of edges, and a scale-aware measure can be introduced to further control the way of gradient suppression. The proposed measure is spatially varying and oriented by coarse-scale local extrema at each pixel to better preserve the natural boundaries of large-scale structures. Besides, we show that our method can be fast implemented with a sequence of 1-D filtering. In the experiments, we demonstrate the effectiveness of our method by comparing it with current state-of-the-art filtering methods and using it in a variety of applications.
C1 [Zhou, Zhiqiang; Wang, Bo; Ma, Jinlei] Beijing Inst Technol, Sch Automat, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Zhou, ZQ (corresponding author), Beijing Inst Technol, Sch Automat, Beijing 100081, Peoples R China.
EM zhzhzhou@bit.edu.cn; wangbo.bitauto@gmail.com; majinlei121@163.com
RI Zhou, Zhiqiang/HIR-4954-2022; zhou, zhou/HPE-9525-2023
OI Zhou, Zhiqiang/0000-0001-6871-8236
FU National Natural Science Foundation of China [61403033]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61403033.
CR [Anonymous], 2008, 2008 IEEE C COMP VIS, DOI [10.1109/CVPR.2008.4587843, DOI 10.1109/CVPR.2008.4587843]
   [Anonymous], ACM T GRAPH
   Aujol JF, 2006, INT J COMPUT VISION, V67, P111, DOI 10.1007/s11263-006-4331-z
   Black MJ, 1998, IEEE T IMAGE PROCESS, V7, P421, DOI 10.1109/83.661192
   Cho H, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601188
   Criminisi A, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1857907.1857910
   DeCarlo D, 2002, ACM T GRAPHIC, V21, P769, DOI 10.1145/566570.566650
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   Ham B, 2018, IEEE T PATTERN ANAL, V40, P192, DOI 10.1109/TPAMI.2017.2669034
   Ham B, 2015, PROC CVPR IEEE, P4823, DOI 10.1109/CVPR.2015.7299115
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Jeon J, 2016, COMPUT GRAPH FORUM, V35, P77, DOI 10.1111/cgf.13005
   Karacan L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508403
   Kass M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778837
   Koutis I, 2011, COMPUT VIS IMAGE UND, V115, P1638, DOI 10.1016/j.cviu.2011.05.013
   Krishnan D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461992
   Lindeberg T., 1994, SCALE SPACE THEORY C
   Lindeberg T., 1994, Journal of AppliedStatistics, V21, P225
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma ZY, 2013, IEEE I CONF COMP VIS, P49, DOI 10.1109/ICCV.2013.13
   Min DB, 2014, IEEE T IMAGE PROCESS, V23, P5638, DOI 10.1109/TIP.2014.2366600
   Paris S, 2006, LECT NOTES COMPUT SC, V3954, P568
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Su Z, 2013, IEEE T MULTIMEDIA, V15, P535, DOI 10.1109/TMM.2012.2237025
   Subr K., 2009, ACM T GRAPHIC, V28
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   van de Weijer J, 2001, PROC CVPR IEEE, P428
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Weiss B, 2006, ACM T GRAPHIC, V25, P519, DOI 10.1145/1141911.1141918
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Witkin A., 1984, Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP '84, VVolume 9, P150, DOI DOI 10.1109/ICASSP.1984.1172729
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yang QX, 2009, PROC CVPR IEEE, P557, DOI 10.1109/CVPRW.2009.5206542
   Zhang Q, 2014, PROC CVPR IEEE, P2830, DOI 10.1109/CVPR.2014.362
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
   Zhu LF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366146
NR 38
TC 25
Z9 27
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1392
EP 1405
DI 10.1109/TMM.2017.2772438
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400009
DA 2024-07-18
ER

PT J
AU Can, G
   Odobez, JM
   Gatica-Perez, D
AF Can, Gulcan
   Odobez, Jean-Marc
   Gatica-Perez, Daniel
TI Maya Codical Glyph Segmentation: A Crowdsourcing Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Crowdsourcing; Maya glyph; classification
ID EXPERTS
AB This paper focuses on the crowd-annotation of an ancient Maya glyph dataset derived from the three ancient codices that survived up to date. More precisely, nonexpert annotators are asked to segment glyph-blocks into their constituent glyph entities. As a means of supervision, available glyph variants are provided to the annotators during the crowdsourcing task. Compared to object recognition in natural images or handwriting transcription tasks, designing an engaging task and dealing with crowd behavior is challenging in our case. This challenge originates from the inherent complexity of Maya writing and an incomplete understanding of the signs and semantics in the existing catalogs. We elaborate on the evolution of the crowdsourcing task design, and discuss the choices for providing supervision during the task. We analyze the distributions of similarity and task difficulty scores, and the segmentation performance of the crowd. A unique dataset of over 9000 Maya glyphs from 291 categories individually segmented from the three codices was created and will be made publicly available thanks to this process. This dataset lends itself to automatic glyph classification tasks. We provide baseline methods for glyph classification using traditional shape descriptors and convolutional neural networks.
C1 [Can, Gulcan; Odobez, Jean-Marc; Gatica-Perez, Daniel] Idiap Res Inst, Martigny, Switzerland.
   [Can, Gulcan; Odobez, Jean-Marc; Gatica-Perez, Daniel] Ecole Polytech Fed Lausanne, Sch Engn, Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Can, G (corresponding author), Idiap Res Inst, Martigny, Switzerland.; Can, G (corresponding author), Ecole Polytech Fed Lausanne, Sch Engn, Lausanne, Switzerland.
EM gcan@idiap.ch; odobez@idiap.ch; gatica@idiap.ch
RI Odobez, Jean-Marc/B-1426-2010
OI Odobez, Jean-Marc/0000-0002-9537-9898
FU Swiss National Science Foundation
FX The authors would like to thank C. P. Gayol (University of Bonn), G.
   Krempel (University of Bonn), J. Spotak (Comenius University in
   Bratislava) for generating the glyph-block dataset and for providing the
   glyph annotations, R. Hu (Idiap) for discussions, and Swiss National
   Science Foundation for funding the MAAYA project.
CR [Anonymous], DIEG LAND WIK FREE E
   [Anonymous], P ANN C MUS WEB
   [Anonymous], 2016, INT C LEARNING REPRE
   [Anonymous], 2015, ARXIV PREPRINT ARXIV
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, ARXIV150107873
   [Anonymous], PROC 11 EUR CONF
   [Anonymous], IDIAPINTERNALRR34201
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], APPL ELECT COMPUTERS
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], ACM SIGGRAPH
   [Anonymous], P INT WORKSH CROWDS
   [Anonymous], LECT NOTES COMPUTER
   [Anonymous], RELATION CHOSES YUCA
   [Anonymous], 2013, P 21 ACM INT C MULT, DOI DOI 10.1145/2502081.2502199
   [Anonymous], P ECCV WORKSH COMP V
   [Anonymous], 2014, CORR
   [Anonymous], NEW CATALOG MAYA HIE
   [Anonymous], ARCHAEOL INT
   [Anonymous], HIEROGLYPHEN MAYA HA
   Biel JI, 2013, IEEE T MULTIMEDIA, V15, P41, DOI 10.1109/TMM.2012.2225032
   Can G, 2016, ACM J COMPUT CULT HE, V9, DOI 10.1145/2905369
   Causer T, 2014, DIGIT RES ARTS HUM, P57
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Fischer A., 2010, P 9 IAPR INT WORKSH, P3, DOI [10.1145/1815330.1815331, DOI 10.1145/1815330.1815331]
   Fortson L, 2012, CH CRC DATA MIN KNOW, P213
   Gatos B, 2014, 2014 11TH IAPR INTERNATIONAL WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS (DAS 2014), P237, DOI 10.1109/DAS.2014.23
   Geoffrey EHinton., 2012, Improving neural networks by preventing co-adaptation of feature detectors
   Gottlieb L, 2014, IEEE T MULTIMEDIA, V16, P2075, DOI 10.1109/TMM.2014.2347268
   Gurari D, 2015, IEEE WINT CONF APPL, P1169, DOI 10.1109/WACV.2015.160
   Gurari Danna., 2014, Conference_on_medical image_computing_and_computer_assisted_intervention_(MICCAI):_Interactive_medical_image computation_(IMIC)_workshop, page, P9
   Hu R, 2015, IEEE SIGNAL PROC MAG, V32, P75, DOI 10.1109/MSP.2015.2411291
   Irshad H, 2015, BIOCOMPUT-PAC SYM, P294
   Larson M, 2012, IEEE MULTIMEDIA, V19, P15, DOI 10.1109/MMUL.2012.27
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liwicki M, 2005, PROC INT CONF DOC, P956, DOI 10.1109/ICDAR.2005.132
   Macri MarthaJ., 2003, NEW CATALOG MAYA HIE
   MASSEY FJ, 1951, J AM STAT ASSOC, V46, P68, DOI 10.2307/2280095
   Nguyen LS, 2016, IEEE T MULTIMEDIA, V18, P1422, DOI 10.1109/TMM.2016.2557058
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Razavian A. S., 2016, ITE Trans. Media Technol. Appl., V4, P251, DOI [DOI 10.3169/MTA.4.251, 10.3169/mta.4.251]
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Roman-Rangel E, 2011, INT J COMPUT VISION, V94, P101, DOI 10.1007/s11263-010-0387-x
   Rudinac S, 2013, IEEE T MULTIMEDIA, V15, P1231, DOI 10.1109/TMM.2013.2261481
   Siahaan E, 2016, IEEE T MULTIMEDIA, V18, P1338, DOI 10.1109/TMM.2016.2559942
   Simonyan K., 2014, 14091556 ARXIV
   Thompson J.Eric S., 1962, A Catalog of Maya Hieroglyphs
   Tozzer AlfredM., 1941, Landa's Relacion de las Cosas de Yucatan, V18
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   von Ahn L, 2008, SCIENCE, V321, P1465, DOI 10.1126/science.1160379
   Yosinski J, 2014, ADV NEUR IN, V27
NR 52
TC 4
Z9 4
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2018
VL 20
IS 3
BP 711
EP 725
DI 10.1109/TMM.2017.2755985
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FW5ZH
UT WOS:000425397500016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Shao, F
   Yuan, QZ
   Lin, WS
   Jiang, GY
AF Shao, Feng
   Yuan, Qizheng
   Lin, Weisi
   Jiang, Gangyi
TI No-Reference View Synthesis Quality Prediction for 3-D Videos Based on
   Color-Depth Interactions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Color depth interactions; 3D synthesized video; no-reference quality
   prediction; view synthesis quality prediction; view synthesis distortion
ID FULL-REFERENCE VIDEO; STEREOSCOPIC IMAGES; BIT ALLOCATION; OBJECTIVE
   VIDEO; MULTIVIEW VIDEO; DISTORTION; OPTIMIZATION; COMPRESSION; MODEL;
   MAPS
AB In a 3-D video system, automatically predicting the quality of synthesized 3-D video based on the inputs of color and depth videos is an urgent but very difficult task, while the existing full-reference methods usually measure the perceptual quality of the synthesized video. In this paper, a high-efficiency view synthesis quality prediction (HEVSQP) metric for view synthesis is proposed. Based on the derived VSQP model that quantifies the influences of color and depth distortions and their interactions in determining the perceptual quality of 3-D synthesized video, color-involved VSQP and depth-involved VSQP indices are predicted, respectively, and are combined to yield an HEVSQP index. Experimental results on our constructed NBU-3D Synthesized Video Quality Database demonstrate that the proposed HEVSOP has good performance evaluated on the entire synthesized video-quality database, compared with other full-reference and no-reference video-quality assessment metrics.
C1 [Shao, Feng; Yuan, Qizheng; Jiang, Gangyi] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Zhejiang, Peoples R China.
   [Lin, Weisi] Nanyang Technol Univ, Sch Comp Engn, Ctr Multimedia & Network Technol, Singapore 639798, Singapore.
C3 Ningbo University; Nanyang Technological University
RP Shao, F (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Zhejiang, Peoples R China.
EM shaofeng@nbu.edu.cn; 15724251045@163.com; wslin@ntu.edu.sg;
   jianggangyi@nbu.edu.cn
RI Lin, Weisi/A-8011-2012; Lin, Weisi/A-3696-2011; jiang,
   gang/KII-8233-2024
OI Lin, Weisi/0000-0001-9866-1947; 
FU National Natural Science Foundation of China [61622109, 61271021]; K.C.
   Wong Magna Fund in Ningbo University
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61622109 and 61271021. and in part by
   K.C. Wong Magna Fund in Ningbo University.
CR [Anonymous], 2012, JTC1SC29WG11 ISOIEC
   [Anonymous], 2013, 2013 VISUAL COMMUNIC
   [Anonymous], P SPIE
   [Anonymous], 1999, SUBJ VID QUAL ASS ME
   [Anonymous], 3DTV C TRUE VIS CAPT
   [Anonymous], 2010, P 5 INT WORKSH VID P
   Appina B, 2016, SIGNAL PROCESS-IMAGE, V43, P1, DOI 10.1016/j.image.2016.02.001
   Benoit A, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/659024
   Bensalma R, 2013, MULTIDIM SYST SIGN P, V24, P281, DOI 10.1007/s11045-012-0178-3
   Bosc E., 2013, P SPIE, V8648
   Bosc E, 2011, IEEE J-STSP, V5, P1332, DOI 10.1109/JSTSP.2011.2166245
   Chen MJ, 2013, SIGNAL PROCESS-IMAGE, V28, P1143, DOI 10.1016/j.image.2013.05.006
   De Silva V, 2013, IEEE T IMAGE PROCESS, V22, P3392, DOI 10.1109/TIP.2013.2268422
   Ekmekcioglu E., 2012, LECT NOTES I COMPUTE, P76
   Fehn C, 2006, P IEEE, V94, P524, DOI 10.1109/JPROC.2006.870688
   Galkandage C, 2017, IEEE J-STSP, V11, P102, DOI 10.1109/JSTSP.2016.2632045
   Han Y, 2016, IEEE T BROADCAST, V62, P654, DOI 10.1109/TBC.2016.2529294
   He LH, 2012, PROC CVPR IEEE, P1146, DOI 10.1109/CVPR.2012.6247795
   Hewage CTER, 2009, IEEE J-STSP, V3, P304, DOI 10.1109/JSTSP.2009.2014805
   Hu SD, 2013, IEEE T IMAGE PROCESS, V22, P585, DOI 10.1109/TIP.2012.2219549
   ISO/ IEC JTC1/ SC29/ WG11, 2014, M34570 ISOIEC JTC1SC
   ISO/ IEC JTC1/ SC29/ WG11 and ITU-T SG16 Q. 6, 2007, 100 ISOIEC JTC1SC29W
   IVY LAB, 2014, IVY LAB STEREOSCOPIC
   Jang WD, 2015, IEEE T CIRC SYST VID, V25, P1099, DOI 10.1109/TCSVT.2014.2372343
   Jung YJ, 2016, IEEE T CIRC SYST VID, V26, P1201, DOI 10.1109/TCSVT.2015.2430632
   Kim W.-S., 2010, P SOC PHOTO-OPT INS, V7543
   Li SN, 2012, IEEE T CIRC SYST VID, V22, P1100, DOI 10.1109/TCSVT.2012.2190473
   Li XL, 2016, IEEE T IMAGE PROCESS, V25, P3329, DOI 10.1109/TIP.2016.2568752
   Li YM, 2016, IEEE T CIRC SYST VID, V26, P1044, DOI 10.1109/TCSVT.2015.2430711
   Lin YH, 2014, IEEE T IMAGE PROCESS, V23, P1527, DOI 10.1109/TIP.2014.2302686
   Liu XK, 2015, IEEE T IMAGE PROCESS, V24, P4847, DOI 10.1109/TIP.2015.2469140
   Mittal A, 2016, IEEE T IMAGE PROCESS, V25, P289, DOI 10.1109/TIP.2015.2502725
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Mocanu DC, 2014, IEEE IMAGE PROC, P758, DOI 10.1109/ICIP.2014.7025152
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Müller K, 2011, P IEEE, V99, P643, DOI 10.1109/JPROC.2010.2091090
   Oh BT, 2014, IEEE T CIRC SYST VID, V24, P1006, DOI 10.1109/TCSVT.2013.2290577
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Qi F, 2016, SIGNAL IMAGE VIDEO P, V10, P737, DOI 10.1007/s11760-015-0802-4
   Ryu S, 2014, IEEE IMAGE PROC, P585, DOI 10.1109/ICIP.2014.7025117
   Ryu S, 2014, IEEE T CIRC SYST VID, V24, P591, DOI 10.1109/TCSVT.2013.2279971
   Saad MA, 2014, IEEE T IMAGE PROCESS, V23, P1352, DOI 10.1109/TIP.2014.2299154
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sandic-Stankovic D., 2017, EURASIP J IMAGE VIDE, V2017
   Seo J, 2012, CIRC SYST SIGNAL PR, V31, P1089, DOI 10.1007/s00034-011-9369-7
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P335, DOI 10.1109/TIP.2009.2034992
   Shao F, 2016, IEEE T IMAGE PROCESS, V25, P2059, DOI 10.1109/TIP.2016.2538462
   Shao F, 2016, IEEE T BROADCAST, V62, P94, DOI 10.1109/TBC.2015.2496818
   Shao F, 2015, IEEE T IMAGE PROCESS, V24, P2971, DOI 10.1109/TIP.2015.2436332
   Shao F, 2014, IEEE J EM SEL TOP C, V4, P106, DOI 10.1109/JETCAS.2014.2298314
   Shao F, 2013, IEEE T MULTIMEDIA, V15, P1843, DOI 10.1109/TMM.2013.2269897
   Shao F, 2013, IEEE T IMAGE PROCESS, V22, P1940, DOI 10.1109/TIP.2013.2240003
   Shao F, 2012, IEEE T MULTIMEDIA, V14, P157, DOI 10.1109/TMM.2011.2169045
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Soares JRS, 2014, IEEE IMAGE PROC, P763, DOI 10.1109/ICIP.2014.7025153
   Su CC, 2015, IEEE T IMAGE PROCESS, V24, P1685, DOI 10.1109/TIP.2015.2409558
   Vu P. V., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2505, DOI 10.1109/ICIP.2011.6116171
   Vu PV, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.1.013016
   Wang K, 2013, PROC SPIE, V8648, DOI 10.1117/12.2003664
   Wang Z, 2004, SIGNAL PROCESS-IMAGE, V19, P121, DOI 10.1016/S0923-5965(03)00076-6
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiang S, 2016, IEEE T IMAGE PROCESS, V25, P1479, DOI 10.1109/TIP.2015.2511586
   Xu JT, 2014, IEEE IMAGE PROC, P491, DOI 10.1109/ICIP.2014.7025098
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yuan H, 2016, IEEE T BROADCAST, V62, P134, DOI 10.1109/TBC.2015.2492461
   Yuan H, 2014, IEEE T BROADCAST, V60, P614, DOI 10.1109/TBC.2014.2361964
   Yuan H, 2014, IEEE T CIRC SYST VID, V24, P443, DOI 10.1109/TCSVT.2013.2280071
   Zhang D, 2015, IEEE T CIRC SYST VID, V25, P827, DOI 10.1109/TCSVT.2014.2363746
   Zhang LJ, 2012, IEEE T IMAGE PROCESS, V21, P2379, DOI 10.1109/TIP.2012.2183879
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, P5877, DOI 10.1109/TIP.2016.2615290
   Zhang Y, 2014, IEEE T IMAGE PROCESS, V23, P4879, DOI 10.1109/TIP.2014.2355715
   Zhang Y, 2013, IEEE T IMAGE PROCESS, V22, P3497, DOI 10.1109/TIP.2013.2265883
   Zhao Y, 2010, PROC SPIE, V7744, DOI 10.1117/12.863478
NR 76
TC 20
Z9 22
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2018
VL 20
IS 3
BP 659
EP 674
DI 10.1109/TMM.2017.2748460
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FW5ZH
UT WOS:000425397500012
DA 2024-07-18
ER

PT J
AU Gao, YY
   Hu, HM
   Li, B
   Guo, Q
AF Gao, Yuanyuan
   Hu, Hai-Miao
   Li, Bo
   Guo, Qiang
TI Naturalness Preserved Nonuniform Illumination Estimation for Image
   Enhancement Based on Retinex
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Illumination estimation; image enhancement; non-uniform illumination;
   Retinex
ID ALGORITHM
AB Illumination estimation is important for image enhancement based on Retinex. However, since illumination estimation is an ill-posed problem, it is difficult to achieve accurate illumination estimation for nonuniform illumination images. The conventional illumination estimation algorithms fail to comprehensively take all the constraints into the consideration, such as spatial smoothness, sharp edges on illumination boundaries, and limited range of illumination. Thus, these algorithms cannot effectively and efficiently estimate illumination, while preserving naturalness. In this paper, we present a naturalness preserved illumination estimation algorithm based on the proposed joint edge-preserving filter, which exploits all the abovementioned constraints. Moreover, a fast estimation is implemented based on the box filter. Experimental results demonstrate that the proposed algorithm can achieve the adaptive smoothness of illumination beyond edges and ensure the range of the estimated illumination. When compared with other state-of-the-art algorithms, it can achieve better quality from both subjective and objective aspects.
C1 [Gao, Yuanyuan; Hu, Hai-Miao; Li, Bo; Guo, Qiang] Beihang Univ, Beijing Key Lab Digital Media, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
   [Hu, Hai-Miao; Li, Bo] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
C3 Beihang University; Beihang University
RP Hu, HM (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM gyy002005@163.com; frank0139@163.com; boli@buaa.edu.cn;
   1263836618@qq.com
RI Li, Bo/AAA-8968-2020; Li, bo/IWL-9318-2023
OI Li, Bo/0000-0002-7294-6888; 
FU National Key Research and Development Program [2016YFC0801003]; National
   Natural Science Foundation of China [61370121]
FX This work was supported in part by the National Key Research and
   Development Program under Grant 2016YFC0801003, and in part by the
   National Natural Science Foundation of China under Grant 61370121. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Judith Redi. (Y. Gao and H.-M. Hu
   contributed equally to this work.) (Corresponding author: Hai-Miao Hu.)
CR Adams A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531327
   [Anonymous], 2005, SCALE SPACE PDE METH
   Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712
   Beigpour S, 2011, IEEE I CONF COMP VIS, P327, DOI 10.1109/ICCV.2011.6126259
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Chen SH, 2009, IEEE IMAGE PROC, P1813, DOI 10.1109/ICIP.2009.5413362
   Crow F. C., 1984, Computers & Graphics, V18, P207
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu XY, 2015, IEEE T IMAGE PROCESS, V24, P4965, DOI 10.1109/TIP.2015.2474701
   Gastal ESL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185529
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   Henz B, 2017, VISUAL COMPUT, V33, P33, DOI 10.1007/s00371-015-1150-7
   Hu HM, 2017, IEEE T MULTIMEDIA, V19, P2706, DOI 10.1109/TMM.2017.2711422
   Hu HM, 2017, NEUROCOMPUTING, V266, P361, DOI 10.1016/j.neucom.2017.05.052
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Jung C, 2013, NEUROCOMPUTING, V113, P130, DOI 10.1016/j.neucom.2013.01.038
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Ng MK, 2011, SIAM J IMAGING SCI, V4, P345, DOI 10.1137/100806588
   Rahman Z, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P1003, DOI 10.1109/ICIP.1996.560995
   Rahman ZU, 2004, J ELECTRON IMAGING, V13, P100, DOI 10.1117/1.1636183
   Sawant H., 2010, INT J COMPUT TECHNOL, V1, P39
   Shen CT, 2009, IEEE IMAGE PROC, P3141, DOI 10.1109/ICIP.2009.5414427
   Shin Y, 2015, IET IMAGE PROCESS, V9, P662, DOI 10.1049/iet-ipr.2014.0437
   Tsai CY, 2012, IEEE T MULTIMEDIA, V14, P1140, DOI 10.1109/TMM.2012.2190390
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Xing Yan, 2010, Proceedings 2010 International Conference on Digital Image Computing: Techniques and Applications (DICTA 2010), P486, DOI 10.1109/DICTA.2010.88
   Ye Y., 2007, P 2007 IEEE INT C CO, P313
   Zhang J, 2014, IEEE IMAGE PROC, P4557, DOI 10.1109/ICIP.2014.7025924
NR 31
TC 76
Z9 82
U1 4
U2 39
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2018
VL 20
IS 2
BP 335
EP 344
DI 10.1109/TMM.2017.2740025
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FT2ER
UT WOS:000422953200007
DA 2024-07-18
ER

PT J
AU Wu, JY
   Cheng, B
   Wang, M
AF Wu, Jiyan
   Cheng, Bo
   Wang, Ming
TI Improving Multipath Video Transmission With Raptor Codes in
   Heterogeneous Wireless Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Heterogeneous wireless networks; Raptor codes; stream control
   transmission protocol; wireless video communication
ID UTILITY MAXIMIZATION; COMPRESSED VIDEO; RATE ALLOCATION; PACKET LOSS;
   QUALITY; ALGORITHM
AB Supported by the latest technical innovations, mobile users are able to simultaneously receive real-time streaming services with different radio access technologies (e.g., LTE and Wi-Fi). The stream control transmission protocol (SCTP) is a vitally important transport protocol to enable concurrent multipath transfer (CMT) in heterogeneous wireless networks with multihomed terminals. However, enabling CMT of real-time video streaming to multihomed mobiles is challenged with key technical dilemmas: 1) high-quality real-time video transmission is constrained by stringent requirements in delay and throughput; 2) wireless networks are bandwidth-limited and error-prone; and 3) the congestion control and packet retransmission modules in the SCTP may incur frequent deadline violations and throughput fluctuations. Motivated by addressing these challenging problems, this research proposes a Video and Raptor code aware CMT (CMT-VR) solution. First, we develop a mathematical model to formulate the utility maximization problem of multipath real-time video delivery over parallel wireless networks. Second, we present a transmission framework that includes online packet scheduling, Raptor coding adaptation, and retransmission control algorithms. CMT-VR is distinct from the existing SCTPs in leveraging the video frame priority and rateless Raptor coding. The performance verification is conducted by means of system evaluations over real wireless networks and extensive semiphysical emulations in the Exata platform. Evaluation results demonstrate that CMT-VR achieves appreciable improvements over the reference schemes in perceived video quality, goodput, and end-to-end delay.
C1 [Wu, Jiyan; Cheng, Bo; Wang, Ming] Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.
C3 Beijing University of Posts & Telecommunications
RP Cheng, B (corresponding author), Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.
EM wujiyan@bupt.edu.cn; chengbo@bupt.edu.cn; wangming_bupt@bupt.edu.cn
FU National High-tech R&D Program of China (863 Program) [2013AA102301]
FX This work was supported by the National High-tech R&D Program of China
   (863 Program) under Grant 2013AA102301. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Shiwen Mao. (Corresponding author: Bo Cheng.)
CR Ahmad S, 2011, IEEE T MULTIMEDIA, V13, P92, DOI 10.1109/TMM.2010.2093511
   [Anonymous], 2016, USERLAND SCTP IMPLEM
   [Anonymous], 2017, EXATA
   [Anonymous], 2008, TECHN SPEC GROUP SER
   [Anonymous], 2007, 4960 IETF RFC
   [Anonymous], 2017, VIS NETW IND GLOB MO
   [Anonymous], 6182 IETF RFC
   Aydin I., 2016, SCTP QUALNET SIMULAT
   BELLMAN R, 1957, J MATH MECH, V6, P679, DOI 10.1512/iumj.1957.6.56038
   Borgnat P, 2009, IEEE INFOCOM SER, P711, DOI 10.1109/INFCOM.2009.5061979
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Chen MH, 2012, IEEE ACM T NETWORK, V20, P1681, DOI 10.1109/TNET.2012.2201166
   Freris N. M., 2010, Proceedings 2010 IEEE International Symposium on Multimedia (ISM 2010), P9, DOI 10.1109/ISM.2010.12
   Freris NM, 2013, IEEE ACM T NETWORK, V21, P469, DOI 10.1109/TNET.2012.2203608
   GILBERT EN, 1960, BELL SYST TECH J, V39, P1253, DOI 10.1002/j.1538-7305.1960.tb03959.x
   Go Y, 2015, IEEE T MULTIMEDIA, V17, P1646, DOI 10.1109/TMM.2015.2451951
   Gong C, 2014, IEEE T WIREL COMMUN, V13, P49, DOI 10.1109/TWC.2013.112613.121033
   Han SC, 2011, IEEE J SEL AREA COMM, V29, P1032, DOI 10.1109/JSAC.2011.110513
   Hasslinger G., 2008, The Gilbert-Elliott Model for Packet Loss in Real Time Services on the Internet, P1
   Iyengar JR, 2006, IEEE ACM T NETWORK, V14, P951, DOI 10.1109/TNET.2006.882843
   Jurca D, 2007, IEEE T MULTIMEDIA, V9, P1227, DOI 10.1109/TMM.2007.902852
   Khalili R, 2013, IEEE ACM T NETWORK, V21, P1651, DOI 10.1109/TNET.2013.2274462
   Kompella S, 2009, IEEE ACM T NETWORK, V17, P212, DOI 10.1109/TNET.2008.925942
   Konrad A., 2001, WIREL NETW, V9, P189
   Kwon OC, 2015, IEEE T MOBILE COMPUT, V14, P1903, DOI 10.1109/TMC.2014.2364042
   Lee D, 2011, IEEE T MULTIMEDIA, V13, P788, DOI 10.1109/TMM.2011.2124448
   Liang YJ, 2008, IEEE T CIRC SYST VID, V18, P861, DOI 10.1109/TCSVT.2008.923139
   Luby M, 2002, ANN IEEE SYMP FOUND, P271, DOI 10.1109/SFCS.2002.1181950
   Luo ZY, 2013, IEEE T MULTIMEDIA, V15, P2208, DOI 10.1109/TMM.2013.2280561
   Maymounkov P., 2002, RT2002833 NEW YORK U
   Neely MJ, 2013, IEEE ACM T NETWORK, V21, P41, DOI 10.1109/TNET.2012.2191157
   Oh HR, 2011, COMPUT NETW, V55, P2746, DOI 10.1016/j.comnet.2011.05.001
   Sanneck HA, 1999, ELECT IMAGING, V3969, P177
   Sgardoni V, 2015, IEEE T MOBILE COMPUT, V14, P401, DOI 10.1109/TMC.2014.2331967
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shokrollahi A, 2006, IEEE T INFORM THEORY, V52, P2551, DOI 10.1109/TIT.2006.874390
   Tuexen M., 2011, 6458 IETF RFC
   Tuexen M., 2013, 7053 IETF RFC
   Wallace TD, 2014, IEEE T MOBILE COMPUT, V13, P2510, DOI 10.1109/TMC.2014.2307330
   Wallace TD, 2012, IEEE COMMUN SURV TUT, V14, P565, DOI 10.1109/SURV.2011.051111.00096
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu J. F., 2016, J SENSORS, V2016, P1, DOI DOI 10.1371/J0URNAL.P0NE.0146261
   Wu JY, 2017, IEEE ACM T NETWORK, V25, P2701, DOI 10.1109/TNET.2017.2701153
   Wu JY, 2016, IEEE T COMMUN, V64, P2477, DOI 10.1109/TCOMM.2016.2553138
   Wu JY, 2016, IEEE J SEL AREA COMM, V34, P1160, DOI 10.1109/JSAC.2016.2551483
   Wu JY, 2016, IEEE T MOBILE COMPUT, V15, P641, DOI 10.1109/TMC.2015.2426710
   Wu JY, 2016, IEEE T PARALL DISTR, V27, P710, DOI 10.1109/TPDS.2015.2416736
   Wu JY, 2013, IEEE GLOB COMM CONF, P1723, DOI 10.1109/GLOCOM.2013.6831322
   Wu JY, 2015, IEEE T PARALL DISTR, V26, P2286, DOI 10.1109/TPDS.2014.2347031
   Wu JY, 2015, IEEE T MOBILE COMPUT, V14, P688, DOI 10.1109/TMC.2014.2334592
   Wu JY, 2013, EURASIP J WIREL COMM, DOI 10.1186/1687-1499-2013-283
   Wu YQ, 2014, IEEE T CIRC SYST VID, V24, P1047, DOI 10.1109/TCSVT.2014.2302151
   Xing M, 2014, IEEE J SEL AREA COMM, V32, P795, DOI 10.1109/JSAC.2014.140411
   Xu CQ, 2015, IEEE T CIRC SYST VID, V25, P1175, DOI 10.1109/TCSVT.2014.2376138
   Xu CQ, 2013, IEEE T MOBILE COMPUT, V12, P2193, DOI 10.1109/TMC.2012.189
   Yajnik M, 1999, IEEE INFOCOM SER, P345, DOI 10.1109/INFCOM.1999.749301
NR 56
TC 35
Z9 37
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2018
VL 20
IS 2
BP 457
EP 472
DI 10.1109/TMM.2017.2741425
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FT2ER
UT WOS:000422953200016
DA 2024-07-18
ER

PT J
AU Çiftçi, S
   Akyüz, AO
   Ebrahimi, T
AF Ciftci, Serdar
   Akyuz, Ahmet Oguz
   Ebrahimi, Touradj
TI A Reliable and Reversible Image Privacy Protection Based on False Colors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE False color; JPEG; privacy protection
AB Protection of visual privacy has become an indispensable component of video surveillance systems due to pervasive use of video cameras for surveillance purposes. In this paper, we propose two fully reversible privacy protection schemes implemented within the JPEG architecture. In both schemes, privacy protection is accomplished by using false colors with the first scheme being adaptable to other privacy protection filters while the second is false color-specific. Both schemes support either a lossless mode in which the original unprotected content can be fully extracted or a lossy mode, which limits file size while still maintaining intelligibility. Our method is not region-of-interest (ROI)-based and can be applied on entire frames without compromising intelligibility. This frees the user from having to define ROIs and improves security as tracking ROIs under dynamic content may fail, exposing sensitive information. Our experimental results indicate the favorability of our method over other commonly used solutions to protect visual privacy.
C1 [Ciftci, Serdar; Akyuz, Ahmet Oguz] Middle East Tech Univ, Dept Comp Engn, TR-06800 Ankara, Turkey.
   [Ebrahimi, Touradj] Ecole Polytech Fed Lausanne, Multimedia Signal Proc Grp, CH-1015 Lausanne, Switzerland.
C3 Middle East Technical University; Swiss Federal Institutes of Technology
   Domain; Ecole Polytechnique Federale de Lausanne
RP Çiftçi, S (corresponding author), Middle East Tech Univ, Dept Comp Engn, TR-06800 Ankara, Turkey.
EM sciftci@ceng.metu.edu.tr; akyuz@ceng.metu.edu.tr;
   touradj.ebrahimi@epfl.ch
RI Akyuz, Ahmet O/A-7956-2018; Ciftci, Serdar/AAZ-8635-2020
OI Ciftci, Serdar/0000-0001-7074-2876; Akyuz, Ahmet/0000-0001-7685-5572;
   Ebrahimi, Touradj/0000-0002-9900-3687
FU Scientific and Technological Research Council of Turkey (TUBITAK)
   [114E445]
FX This work was supported by The Scientific and Technological Research
   Council of Turkey (TUBITAK) under Project 114E445. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Sen-Ching Samson Cheung.
CR Abraham A. R., 2012, INT J COMPUTER APPL, V56, P43
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Akyüz AO, 2016, ACM T APPL PERCEPT, V14, DOI 10.1145/2911986
   [Anonymous], 2013, 18 INT C DIG SIGN PR
   [Anonymous], 1996, 1950 RFC
   Artusi A., 2015, J REAL-TIME IMAGE PR, P1, DOI DOI 10.1007/S11554-015-0547-X#ABOUTCONTENT
   B. S. I. Association, 2013, PICT IS NOT CLEAR MA
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Chinomi K, 2008, LECT NOTES COMPUT SC, V4903, P144
   Ciftci S., 2015, MEDIAEVAL BENCHMARKI
   Ciftci S., 2015, P IS T SPIE EL IM
   Dufaux F., 2011, P SPIE DEF SEC SENS
   Dufaux F, 2008, IEEE T CIRC SYST VID, V18, P1168, DOI 10.1109/TCSVT.2008.928225
   Dufaux F, 2010, IEEE INT CON MULTI, P66, DOI 10.1109/ICME.2010.5583552
   Fleck S, 2008, P IEEE, V96, P1698, DOI 10.1109/JPROC.2008.928765
   Furht B., 2004, MULTIMEDIA SECURITY, V4
   Granados M, 2012, COMPUT GRAPH FORUM, V31, P219, DOI 10.1111/j.1467-8659.2012.03000.x
   Gross R., 2005, Privacy Enhancing Technologies. 5th International Workshop, PET 2005. Revised Selected Papers (Lecture Notes in Computer Science Vol. 3856), P227
   Gross R., 2006, 2006 C COMPUTER VISI, P161
   Gross R., 2009, Protecting privacy in video surveillance, P129
   Hogue A., 2007, P 2007 C FUTURE PLAY, P174
   KitwarePublics, COL
   Korshunov P., 2013, P SPIE OPT ENG APPL
   Korshunov P, 2013, 2013 10TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2013), P208, DOI 10.1109/AVSS.2013.6636641
   Lander K, 2001, APPL COGNITIVE PSYCH, V15, P101, DOI 10.1002/1099-0720(200101/02)15:1<101::AID-ACP697>3.0.CO;2-7
   Mantiuk R, 2006, ACM T GRAPHIC, V25, P713, DOI 10.1145/1141911.1141946
   Newton EM, 2005, IEEE T KNOWL DATA EN, V17, P232, DOI 10.1109/TKDE.2005.32
   Padilla-Lopez Jose Ramon, 2014, Ubiquitous Computing and Ambient Intelligence. Personalisation and User Adapted Services. 8th International Conference, UCAmI 2014. Proceedings: LNCS 8867, P333, DOI 10.1007/978-3-319-13102-3_55
   Pennebaker W. B., 1992, JPEG STILL IMAGE DAT
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Qian ZX, 2014, IEEE T MULTIMEDIA, V16, P1486, DOI 10.1109/TMM.2014.2316154
   Padilla-López JR, 2015, EXPERT SYST APPL, V42, P4177, DOI 10.1016/j.eswa.2015.01.041
   Ribaric S, 2016, SIGNAL PROCESS-IMAGE, V47, P131, DOI 10.1016/j.image.2016.05.020
   Rodriguez-Silva D. A., 2012, 2012 IEEE 5th International Conference on Cloud Computing (CLOUD), P991, DOI 10.1109/CLOUD.2012.44
   Rogowitz BE, 2001, IEEE VISUAL, P183, DOI 10.1109/VISUAL.2001.964510
   Sadimon SB, 2010, 2010 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2010), P383, DOI 10.1109/CW.2010.33
   Saini M. K., 2013, INT J TRUST MANAGEME, V1, P23
   Saini M, 2010, IEEE INT CON MULTI, P60, DOI 10.1109/ICME.2010.5583334
   Spaulding K. E., 2003, PICS C, P307
   Sweeney L, 2002, INT J UNCERTAIN FUZZ, V10, P557, DOI 10.1142/S0218488502001648
   Tang L., 1997, P 4 ACM INT C MULT, P219
   Tansuriyavong S., 2001, Proceedings of the 2001 workshop on Perceptive user interfaces, P1, DOI DOI 10.1145/971478.971519
   Tesic J, 2005, IEEE MULTIMEDIA, V12, P86, DOI 10.1109/MMUL.2005.50
   Turk M. A., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P586, DOI 10.1109/CVPR.1991.139758
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Ward G., 2006, P ACM SIGGRAPH 2006
   Ward Greg., 2004, Proceedings of the 1st Symposium on Applied Perception in Graphics and Visualization, APGV '04, P83
   WILLIAMS A., 2006, DISTRIBUTED SMART CA
   Yuan L, 2015, IEEE CONF COMPUT, P185, DOI 10.1109/INFCOMW.2015.7179382
   Yuan XL, 2016, IEEE T MULTIMEDIA, V18, P2002, DOI 10.1109/TMM.2016.2602758
   Zeng WJ, 2003, IEEE T MULTIMEDIA, V5, P118, DOI 10.1109/TMM.2003.808817
   [张红英 ZHANG Hongying], 2007, [中国图象图形学报, Journal of Image and Graphics], V12, P1
   Zhang WM, 2016, IEEE T MULTIMEDIA, V18, P1469, DOI 10.1109/TMM.2016.2569497
NR 56
TC 44
Z9 45
U1 0
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2018
VL 20
IS 1
BP 68
EP 81
DI 10.1109/TMM.2017.2728479
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FQ3WS
UT WOS:000418289700006
DA 2024-07-18
ER

PT J
AU Yuan, H
   Wei, XK
   Yang, FZ
   Xiao, JM
   Kwong, S
AF Yuan, Hui
   Wei, Xuekai
   Yang, Fuzheng
   Xiao, Jimin
   Kwong, Sam
TI Cooperative Bargaining Game-Based Multiuser Bandwidth Allocation for
   Dynamic Adaptive Streaming Over HTTP
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cooperative game; dynamic adaptive streaming over HTTP (DASH); Nash
   bargaining solution (NBS); resource allocation; video streaming
ID SCALABLE VIDEO; FAIRNESS; QOE
AB Dynamic adaptive streaming over HTTP (DASH) has emerged as an efficient technology for video streaming. For a DASH system, a most common case is that a limited server bandwidth is competed by multiusers. In order to improve user quality of experience (QoE) and guarantee fairness, we propose to use the game theory in a proxy server to allocate the bandwidth collaboratively for multiusers. By taking user buffer length, received video bit rates, video qualities, etc., into account, the bandwidth allocation problem is formulated as a cooperative bargaining problem and the Nash bargaining solution (NBS) is obtained by convex optimization. The requested bit rate of users will be rewritten as the proxy calculated bit rate (i.e., NBS) when the user requested bit rate is larger. Experimental results demonstrate that user QoE and fairness can be improved significantly, i.e., the delay frequency and duration are smaller, and the received video qualities are higher and more stable, when comparing the proposed method with existing methods.
C1 [Yuan, Hui; Wei, Xuekai] Shandong Univ, Sch Informat Sci & Engn, Jinan 250100, Shandong, Peoples R China.
   [Yang, Fuzheng] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R China.
   [Xiao, Jimin] Xian Jiaotong Liverpool Univ, Dept Elect & Elect Engn, Suzhou 215123, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
C3 Shandong University; Xidian University; Xi'an Jiaotong-Liverpool
   University; City University of Hong Kong
RP Yuan, H (corresponding author), Shandong Univ, Sch Informat Sci & Engn, Jinan 250100, Shandong, Peoples R China.
EM huiyuan@sdu.edu.cn; weixk@foxmail.com; fzhyang@mail.xidian.edu.cn;
   jimin.xiao@xjtlu.edu.cn; cssamk@cityu.edu.hk
RI Yuan, Hui/HDO-3699-2022; Kwong, Sam/C-9319-2012
OI Yuan, Hui/0000-0001-5212-3393; Kwong, Sam/0000-0001-7484-7261; WEI,
   Xuekai/0000-0002-3761-1759
FU National Natural Science Foundation of China [61571274, 61501379];
   Shandong Natural Science Funds for Distinguished Young Scholar
   [JQ201614]; Young Scholars Program of Shandong University [2015WLJH39];
   Jiangsu Science and Technology Programme [BK20150375]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61571274 and Grant 61501379, in part by
   the Shandong Natural Science Funds for Distinguished Young Scholar under
   Grant JQ201614, in part by the Young Scholars Program of Shandong
   University under Grant 2015WLJH39, and in part by Jiangsu Science and
   Technology Programme under Grant BK20150375. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Shiwen Mao.
CR Adobe, OP SOURC MED FRAM OS
   Akhshabi S., 2011, P ACM MMSYS11 FEB, P169
   [Anonymous], TECH REP
   [Anonymous], 2014, TECH REP
   [Anonymous], TR301 DEC RES
   [Anonymous], 2014, P 13 ACM WORKSH HOT
   Bing Zhou, 2012, 2012 International Conference on Computing, Networking and Communications (ICNC), P734, DOI 10.1109/ICCNC.2012.6167520
   Bokani A, 2015, IEEE T MULTIMEDIA, V17, P2297, DOI 10.1109/TMM.2015.2494458
   Boyd S., 2008, CONVEX OPTIMIZATION
   Cicalò S, 2016, IEEE T CIRC SYST VID, V26, P2284, DOI 10.1109/TCSVT.2015.2504732
   Cranley N, 2006, INT J HUM-COMPUT ST, V64, P637, DOI 10.1016/j.ijhcs.2005.12.002
   Dai H., 2011, Acta Horticulturae, P169, DOI 10.1145/1943552.1943575
   Dräxler M, 2013, INT WIREL COMMUN, P1181, DOI 10.1109/IWCMC.2013.6583724
   El Essaili A, 2015, IEEE T CIRC SYST VID, V25, P988, DOI 10.1109/TCSVT.2014.2367355
   Fallah YP, 2008, IEEE T CIRC SYST VID, V18, P875, DOI 10.1109/TCSVT.2008.920745
   Huang T.Y., 2013, P 2013 ACM SIGCOMM W, P9, DOI [DOI 10.1145/2491172, DOI 10.1145/2491172.2491179]
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Lederer S., 2012, P ACM MULT SYST C FE, P22
   Nash JF, 1950, ECONOMETRICA, V18, P155, DOI 10.2307/1907266
   Park H, 2007, IEEE T SIGNAL PROCES, V55, P3496, DOI 10.1109/TSP.2007.893755
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Song JR, 2016, IEEE T MULTIMEDIA, V18, P444, DOI 10.1109/TMM.2016.2520090
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Wang X, 2014, IEEE T IMAGE PROCESS, V23, P4010, DOI 10.1109/TIP.2014.2341951
   Watson M., 2011, TECH REP
   Wei DX, 2006, IEEE ACM T NETWORK, V14, P1246, DOI 10.1109/TNET.2006.886335
   Xiao K., 2016, Proc. of GLOBECOM, P1
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zhang WW, 2013, IEEE T MULTIMEDIA, V15, P1431, DOI 10.1109/TMM.2013.2247583
   Zhao MC, 2015, IEEE T CIRC SYST VID, V25, P451, DOI 10.1109/TCSVT.2014.2357094
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
   Zhou C, 2014, IEEE T CIRC SYST VID, V24, P681, DOI 10.1109/TCSVT.2013.2290580
NR 33
TC 29
Z9 31
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2018
VL 20
IS 1
BP 183
EP 197
DI 10.1109/TMM.2017.2724850
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FQ3WS
UT WOS:000418289700015
DA 2024-07-18
ER

PT J
AU Ercoli, S
   Bertini, M
   Del Bimbo, A
AF Ercoli, Simone
   Bertini, Marco
   Del Bimbo, Alberto
TI Compact Hash Codes for Efficient Visual Descriptors Retrieval in Large
   Scale Databases
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural network (CNN); hashing; nearest neighbor search;
   retrieval; SIFT
ID LEARNING BINARY-CODES; PRODUCT QUANTIZATION; IMAGE; SEARCH; MULTIINDEX
AB In this paper, we present an efficient method for visual descriptors retrieval based on compact hash codes computed using a multiple k-means assignment. The method has been applied to the problem of approximate nearest neighbor (ANN) search of local and global visual content descriptors, and it has been tested on different datasets: three large scale standard datasets of engineered features of up to one billion descriptors (BIGANN) and, supported by recent progress in convolutional neural networks (CNNs), on CIFAR-10, MNIST, INRIA Holidays, Oxford 5K, and Paris 6K datasets; also, the recent DEEP1B dataset, composed by one billion CNN-based features, has been used. Experimental results show that, despite its simplicity, the proposed method obtains a very high performance that makes it superior to more complex state-of-the-art methods.
C1 [Ercoli, Simone; Bertini, Marco; Del Bimbo, Alberto] Univ Florence, Media Integrat & Commun Ctr, I-50139 Florence, Italy.
C3 University of Florence
RP Bertini, M (corresponding author), Univ Florence, Media Integrat & Commun Ctr, I-50139 Florence, Italy.
EM simone.ercoli@unifi.it; marco.bertini@unifi.it;
   alberto.delbimbo@unifi.it
RI Bertini, Marco/X-1325-2019
OI Bertini, Marco/0000-0002-1364-218X
FU Social Museum and Smart Tourism [CTN01_00034_231545]; Office of the
   Director of National Intelligence; Intelligence Advanced Research
   Projects Activity, via IARPA [2014-14071600011]
FX This work was supported in part by the "Social Museum and Smart Tourism"
   under Project CTN01_00034_231545, in part by the Office of the Director
   of National Intelligence, and in part by the Intelligence Advanced
   Research Projects Activity, via IARPA under Contract 2014-14071600011.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Zhu Li. (Corresponding author:
   Marco Bertini.)
CR [Anonymous], 2004, P 20 ACM S COMP
   [Anonymous], 2012, P 20 ACM MULTIMEDIA
   [Anonymous], 2009, NEURIPS
   [Anonymous], 2015, CORR
   [Anonymous], P INT C AC SPEECH SI
   [Anonymous], 2009, Technical report
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2009, NIPS
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], 2009, NEURIPS
   [Anonymous], CORR
   [Anonymous], 2010, P INT MOB MULT WORKS
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Babenko A, 2016, PROC CVPR IEEE, P2055, DOI 10.1109/CVPR.2016.226
   Babenko A, 2012, PROC CVPR IEEE, P3069, DOI 10.1109/CVPR.2012.6248038
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Chandrasekhar V, 2015, IEEE DATA COMPR CONF, P333, DOI 10.1109/DCC.2015.54
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen CC, 2015, J VIS COMMUN IMAGE R, V30, P86, DOI 10.1016/j.jvcir.2015.02.014
   Chen YJ, 2010, SENSORS-BASEL, V10, P11259, DOI 10.3390/s101211259
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Du SZ, 2014, INT C PATT RECOG, P2685, DOI 10.1109/ICPR.2014.464
   Ge TZ, 2014, IEEE T PATTERN ANAL, V36, P744, DOI 10.1109/TPAMI.2013.240
   Ge TZ, 2013, PROC CVPR IEEE, P2946, DOI 10.1109/CVPR.2013.379
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2013, PROC CVPR IEEE, P484, DOI 10.1109/CVPR.2013.69
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15
   Guo D, 2016, NEUROCOMPUTING, V217, P92, DOI 10.1016/j.neucom.2016.04.061
   He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378
   Heo JP, 2015, IEEE T PATTERN ANAL, V37, P2304, DOI 10.1109/TPAMI.2015.2408363
   Jain M., 2011, Proceedings of the 19th ACM international conference on Multimedia, P1441
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jin ZM, 2014, IEEE T CYBERNETICS, V44, P1362, DOI 10.1109/TCYB.2013.2283497
   Kalantidis Y, 2014, PROC CVPR IEEE, P2329, DOI 10.1109/CVPR.2014.298
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li P, 2013, IEEE T MULTIMEDIA, V15, P141, DOI 10.1109/TMM.2012.2199970
   Lin J, 2016, IEEE DATA COMPR CONF, P397, DOI 10.1109/DCC.2016.23
   Lin Kevin, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P27, DOI 10.1109/CVPRW.2015.7301269
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Lv Q, 2007, P 33 INT C VER LARG, P950
   Lv YM, 2015, IEEE T MULTIMEDIA, V17, P1225, DOI 10.1109/TMM.2015.2437712
   Norouzi M.E., 2011, ICML
   Norouzi M, 2014, IEEE T PATTERN ANAL, V36, P1107, DOI 10.1109/TPAMI.2013.231
   Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388
   Pauleve L, 2010, PATTERN RECOGN LETT, V31, P1348, DOI 10.1016/j.patrec.2010.04.004
   Philbin J., 2008, PROC IEEE C COMPUT V, P1
   Ren GX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P769, DOI 10.1145/2647868.2654956
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Uricchio T, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P1020, DOI 10.1109/ICCVW.2015.134
   van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132
   Wan J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P157, DOI 10.1145/2647868.2654948
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Zhang Z., 2015, CORR, Vabs/1502.04689
   Zheng L, 2016, INT J COMPUT VISION, V120, P1, DOI 10.1007/s11263-016-0889-2
   Zheng L, 2014, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2014.250
   Zhou WG, 2014, IEEE T MULTIMEDIA, V16, P601, DOI 10.1109/TMM.2014.2301979
NR 62
TC 33
Z9 35
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2017
VL 19
IS 11
BP 2521
EP 2532
DI 10.1109/TMM.2017.2697824
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FJ9CT
UT WOS:000413068200013
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, XS
   Jia, J
   Gao, K
   Zhang, YD
   Zhang, DM
   Li, JT
   Tian, Q
AF Zhang, Xishan
   Jia, Jia
   Gao, Ke
   Zhang, Yongdong
   Zhang, Dongming
   Li, Jintao
   Tian, Qi
TI Trip Outfits Advisor: Location-Oriented Clothing Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clothing recommendation; fashion analysis; location-based multimedia
   system
AB When packing for a journey, have you ever asked "what clothes should I take with me?" Wearing appropriate and aesthetically pleasing clothing when traveling is a concern for many of us. Our data observation of photos from several popular travel websites reveals that people's choice of clothing items and their color combinations have strong correlations with the weather, the season, and themain type of attraction at the destination. This leads to an interesting and novel problem: can the correlation between clothing and locations be automatically learned from social photos and leveraged for location-oriented clothing recommendations? In this paper, we systematically study this problem and propose a hybrid multilabel convolutional neural network combined with the support vector machine (mCNN-SVM) approach to capture the intrinsic and complex correlations between clothing attributes and location attributes. Specifically, we adapt the CNN architecture to multilabel learning and fine-tune it using each fine-grained clothing item. Then, the recognized items are fed to the SVM to learn the correlations. Experiments on three fashion datasets and a benchmark journey outfit dataset show that our proposed approach outperforms several baselines by over 10.52-16.38% in terms of the mAP for clothing item recognition and outperforms several alternative methods by over 9.59-29.41% in terms of the mAP when ranking clothing by appropriateness for travel destinations. Finally, an interesting case study demonstrates the effectiveness of our method by answering what items to wear, how to match them, and howto dress in an aesthetically pleasing manner for a journey.
C1 [Zhang, Xishan; Gao, Ke; Zhang, Yongdong; Li, Jintao] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Zhang, Yongdong] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Jia, Jia] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Zhang, Dongming] Coordinat Ctr China, Natl Comp Network Emergency Response Tech Team, Beijing 100029, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Tsinghua University; University of Texas System; University of
   Texas at San Antonio (UTSA)
RP Zhang, YD (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM zhangxishan@ict.ac.cn; jjia@mail.tsinghua.edu.cn; kegao@ict.ac.cn;
   zhyd@ict.ac.cn; dmzhang@ict.ac.cn; jtli@ict.ac.cn; qitian@cs.utsa.edu
RI jia, jia/JKJ-5720-2023; li, xiaomin/KCX-9845-2024
OI Zhang, Dongming/0000-0002-1237-7177
FU National Nature Science Foundation of China [61525206, 61271428,
   61672495, 61429201, 61370023, 61521002]; National Key Research and
   Development Plan of China [2016YFB0801203, 2016YFB1001200,
   2016YFB0801200, 2016IM010200]; Beijing Advanced Innovation Center for
   Imaging Technology [BAICIT-2016009]; ARO [W911NF-15-1-0290]; NEC
   Laboratories of America; Blippar
FX This work was supported in part by the National Nature Science
   Foundation of China under Grant 61525206, Grant 61271428, Grant
   61672495, Grant 61429201, Grant 61370023, and Grant 61521002, in part by
   the National Key Research and Development Plan of China under Grant
   2016YFB0801203, Grant 2016YFB1001200, Grant 2016YFB0801200, and Grant
   2016IM010200, in part by Beijing Advanced Innovation Center for Imaging
   Technology under Grant BAICIT-2016009, in part by ARO under Grant
   W911NF-15-1-0290, and in part by the Faculty Research Gift Awards by NEC
   Laboratories of America and Blippar. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Benoit Huet. (Corresponding author: Yongdong Zhang.)
CR [Anonymous], 2004, P 21 INT C MACH LEAR
   [Anonymous], 2013, PROC THE 21 ACM INT
   [Anonymous], 2012, P ACM INT C MULT
   [Anonymous], P INT C MULTIMEDIA M
   [Anonymous], 1995, ART COLOR COMBINATIO
   Bao J, 2015, GEOINFORMATICA, V19, P525, DOI 10.1007/s10707-014-0220-8
   Chen KT, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P177, DOI 10.1145/2733373.2809930
   Gu B, 2015, IEEE T NEUR NET LEAR, V26, P1403, DOI 10.1109/TNNLS.2014.2342533
   Hidayati SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P197, DOI 10.1145/2647868.2656405
   Hu Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P129, DOI 10.1145/2733373.2806239
   Huang S.J., 2012, 26 AAAI C ART INT, P949
   Izadinia H, 2012, LECT NOTES COMPUT SC, V7575, P430, DOI 10.1007/978-3-642-33765-9_31
   Jagadeesh V, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1925, DOI 10.1145/2623330.2623332
   Jia J, 2016, AAAI CONF ARTIF INTE, P1216
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang SH, 2015, IEEE T MULTIMEDIA, V17, P907, DOI 10.1109/TMM.2015.2417506
   Jing LP, 2015, PROC CVPR IEEE, P1483, DOI 10.1109/CVPR.2015.7298755
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Khan R, 2013, PROC CVPR IEEE, P2866, DOI 10.1109/CVPR.2013.369
   Kiapour MH, 2015, IEEE I CONF COMP VIS, P3343, DOI 10.1109/ICCV.2015.382
   Kiapour MH, 2014, LECT NOTES COMPUT SC, V8689, P472, DOI 10.1007/978-3-319-10590-1_31
   Kou NM, 2015, GEOINFORMATICA, V19, P693, DOI 10.1007/s10707-015-0226-x
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liu S, 2015, PROC CVPR IEEE, P1419, DOI 10.1109/CVPR.2015.7298748
   Liu S, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P467
   Liu S, 2014, IEEE T MULTIMEDIA, V16, P253, DOI 10.1109/TMM.2013.2285526
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Nowozin S, 2010, FOUND TRENDS COMPUT, V6, pX, DOI 10.1561/0600000033
   Oquab M, 2015, PROC CVPR IEEE, P685, DOI 10.1109/CVPR.2015.7298668
   Pinheiro PO, 2015, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2015.7298780
   Sang Jitao., 2012, SIGSPATIALGIS, P402
   Simo-Serra E, 2015, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2015.7298688
   van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809
   Veit A, 2015, IEEE I CONF COMP VIS, P4642, DOI 10.1109/ICCV.2015.527
   Vittayakorn S, 2015, IEEE WINT CONF APPL, P951, DOI 10.1109/WACV.2015.131
   Wang J, 2016, PROC CVPR IEEE, P2295, DOI 10.1109/CVPR.2016.252
   Wang XH, 2013, IEEE IMAGE PROC, P3230, DOI 10.1109/ICIP.2013.6738665
   Wei L., 2012, P 18 ACM SIGKDD INT, P195, DOI [10.1145/2339530.2339562, DOI 10.1145/2339530.2339562]
   Wen XZ, 2015, INFORM SCIENCES, V295, P395, DOI 10.1016/j.ins.2014.10.040
   Yamaguchi K, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P773, DOI 10.1145/2647868.2654958
   Yamaguchi K, 2013, IEEE I CONF COMP VIS, P3519, DOI 10.1109/ICCV.2013.437
   Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101
   Yang S.-J., 2013, Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, P1862
   Yang W, 2014, PROC CVPR IEEE, P3182, DOI 10.1109/CVPR.2014.407
   Yu T., 2009, P 26 ANN INT C MACH, P1169, DOI DOI 10.1145/1553374.1553523
   Yu ZW, 2016, IEEE T HUM-MACH SYST, V46, P151, DOI 10.1109/THMS.2015.2446953
   Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39
NR 47
TC 54
Z9 58
U1 1
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2017
VL 19
IS 11
BP 2533
EP 2544
DI 10.1109/TMM.2017.2696825
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FJ9CT
UT WOS:000413068200014
DA 2024-07-18
ER

PT J
AU Liu, SC
   Shao, JR
   Lu, HT
AF Liu, Shicong
   Shao, Junru
   Lu, Hongtao
TI Generalized Residual Vector Quantization and Aggregating Tree for Large
   Scale Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE High dimensional data; large scale data; nearest neighbor search;
   similarity search; vector quantization
ID IMAGE SEARCH; PRODUCT QUANTIZATION; CODEBOOK
AB Vector quantization is an essential tool for tasks involving large scale data, for example, large scale similarity search, which is crucial for content-based information retrieval and analysis. In this paper, we propose a novel vector quantization framework that iteratively minimizes quantization error. First, we provide a detailed review on a relevant vector quantization method named residual vector quantization (RVQ). Next, we propose generalized residual vector quantization (GRVQ) to further improve over RVQ. Many vector quantization methods can be viewed as special cases of our proposed method. To enable GRVQ on billion scale data, we introduce a nonexhaustive search scheme named aggregating tree (A-Tree) for high dimensional data that uses GRVQ encodings to build a radix tree and perform the nearest neighbor search by beam search. To search accurately and efficiently, VQ-encodings should satisfy locally aggregating encoding criterion: For any node of the corresponding A-Tree, neighboring vectors should aggregate in fewer subtrees to make beam search efficient. We show that the proposed GRVQ encodings best satisfy the suggested criterion, and the joint use of GRVQ and A-Tree shows significantly better performances on billion scale datasets. Our methods are validated on several standard benchmark datasets. Experimental results and empirical analysis show the superior efficiency and effectiveness of our proposed methods compared to the state-of-the-art for large scale search.
C1 [Liu, Shicong; Shao, Junru; Lu, Hongtao] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Key Lab Shanghai Educ Commiss Intelligent Interac, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Lu, HT (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Key Lab Shanghai Educ Commiss Intelligent Interac, Shanghai 200240, Peoples R China.
EM artheru@sjtu.edu.cn; yz_sjr@sjtu.edu.cn; htlu@sjtu.edu.cn
FU NSFC [61272247, 61533012, 61472075]; 863 National High Technology
   Research and Development Program of China [SS2015AA020501]; Basic
   Research Project of "Innovation Action Plan" [16JC1402800]; Major Basic
   Research Program of the Shanghai Science and Technology Committee
   [15JC1400103]
FX This work was supported in part by the NSFC under Grant 61272247, Grant
   61533012, and Grant 61472075, in part by the 863 National High
   Technology Research and Development Program of China (SS2015AA020501),
   in part by the Basic Research Project of "Innovation Action Plan" under
   Grant 16JC1402800, and in part by the Major Basic Research Program under
   Grant 15JC1400103 of the Shanghai Science and Technology Committee. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Guillaume Gravier. (Corresponding
   author: Hongtao Lu.)
CR Agrawal R., 1998, SIGMOD Record, V27, P94, DOI 10.1145/276305.276314
   [Anonymous], 2004, P 20 ACM S COMP
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], P 31 INT C INT C MAC
   [Anonymous], SENSORS
   [Anonymous], INT C COMP VIS IM CO
   [Anonymous], P 1 ACM INT C MULT R
   Babenko A, 2015, PROC CVPR IEEE, P4240, DOI 10.1109/CVPR.2015.7299052
   Babenko A, 2014, PROC CVPR IEEE, P931, DOI 10.1109/CVPR.2014.124
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Babenko A, 2012, PROC CVPR IEEE, P3069, DOI 10.1109/CVPR.2012.6248038
   Bardeli R, 2009, IEEE T MULTIMEDIA, V11, P68, DOI 10.1109/TMM.2008.2008920
   Biing-Hwang Juang, 1982, Proceedings of ICASSP 82. IEEE International Conference on Acoustics, Speech and Signal Processing, P597
   Bradley P. S., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P91
   Chu LY, 2013, IEEE T MULTIMEDIA, V15, P1982, DOI 10.1109/TMM.2013.2270455
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding C., 2004, P 21 INT C MACH LEAR, P29, DOI DOI 10.1145/1015330.1015408
   Dong L, 2016, IEEE T MULTIMEDIA, V18, P714, DOI 10.1109/TMM.2016.2530399
   Feris RS, 2012, IEEE T MULTIMEDIA, V14, P28, DOI 10.1109/TMM.2011.2170666
   Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745
   Ge TZ, 2013, PROC CVPR IEEE, P2946, DOI 10.1109/CVPR.2013.379
   Gray R. M., 1984, IEEE ASSP Magazine, V1, P4, DOI 10.1109/MASSP.1984.1162229
   Guan T, 2013, IEEE T MULTIMEDIA, V15, P1688, DOI 10.1109/TMM.2013.2265674
   He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2011, INT CONF ACOUST SPEE, P861
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang YG, 2013, IEEE T MULTIMEDIA, V15, P442, DOI 10.1109/TMM.2012.2231061
   Kalantidis Y, 2014, PROC CVPR IEEE, P2329, DOI 10.1109/CVPR.2014.298
   Liu X., 2013, PROC ICCREM 2013 CON, P626, DOI DOI 10.1061/9780784413135.05
   Liu XL, 2015, IEEE I CONF COMP VIS, P1107, DOI 10.1109/ICCV.2015.132
   Liu Z, 2015, IEEE T MULTIMEDIA, V17, P538, DOI 10.1109/TMM.2015.2399851
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Long YS, 2016, IEEE INT CONF CON AU, P1, DOI [10.1007/s00170-016-9151-x, 10.1109/VLSI-TSA.2016.7480514, 10.1109/ICCA.2016.7505243]
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lv YM, 2015, IEEE T MULTIMEDIA, V17, P1225, DOI 10.1109/TMM.2015.2437712
   Matsui Y, 2015, IEEE I CONF COMP VIS, P1940, DOI 10.1109/ICCV.2015.225
   Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376
   Nister David, 2006, CVPR
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Otani Mayu, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P651, DOI 10.1007/978-3-319-46604-0_46
   Perronnin F., 2007, PROC IEEE C COMPUT V, P1
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Spyromitros-Xioufis E, 2014, IEEE T MULTIMEDIA, V16, P1713, DOI 10.1109/TMM.2014.2329648
   Sun Xinghai., 2013, Proceedings of the International Conference On Multimedia (MM), P233
   Tiakas E, 2013, IEEE T MULTIMEDIA, V15, P1415, DOI 10.1109/TMM.2013.2247989
   Tian XM, 2015, IEEE T MULTIMEDIA, V17, P79, DOI 10.1109/TMM.2014.2368714
   Weber R., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P194
   Wei BC, 2014, IEEE MULTIMEDIA, V21, P41, DOI 10.1109/MMUL.2013.65
   Weiss Yair, 2009, Advances in Neural Information Processing Systems, P1753, DOI DOI 10.5555/2981780.2981999
   Wu L, 2010, IEEE T IMAGE PROCESS, V19, P1908, DOI 10.1109/TIP.2010.2045169
   Yao SS, 2015, IEEE T MULTIMEDIA, V17, P1450, DOI 10.1109/TMM.2015.2460121
   Zhang T, 2015, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2015.7299085
   Zhang YD, 2014, IEEE T MULTIMEDIA, V16, P1127, DOI 10.1109/TMM.2014.2306392
   Zhao WL, 2016, IEEE T MULTIMEDIA, V18, P1843, DOI 10.1109/TMM.2016.2585023
   Zheng L, 2015, IEEE T MULTIMEDIA, V17, P648, DOI 10.1109/TMM.2015.2408563
   Zhou WG, 2015, IEEE T IMAGE PROCESS, V24, P967, DOI 10.1109/TIP.2015.2389624
   Zhou WG, 2014, IEEE T MULTIMEDIA, V16, P601, DOI 10.1109/TMM.2014.2301979
NR 63
TC 7
Z9 7
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2017
VL 19
IS 8
BP 1785
EP 1797
DI 10.1109/TMM.2017.2692181
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5VV
UT WOS:000411240400008
DA 2024-07-18
ER

PT J
AU Carlsson, N
   Eager, D
   Krishnamoorthi, V
   Polishchuk, T
AF Carlsson, Niklas
   Eager, Derek
   Krishnamoorthi, Vengatanathan
   Polishchuk, Tatiana
TI Optimized Adaptive Streaming of Multi-video Stream Bundles
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE HTTP-based adaptive streaming (HAS); multi-view video streaming;
   multi-video stream bundles; optimized prefetching; seamless stream
   switching
ID SYSTEM
AB In contrast to traditional video, multi-view video streaming allows viewers to interactively switch among multiple perspectives provided by different cameras. One approach to achieve such a service is to encode the video from all of the cameras into a single stream, but this has the disadvantage that only a portion of the received video data will be used, namely that required for the selected view at each point in time. In this paper, we introduce the concept of a "multi-video stream bundle" that consists of multiple parallel video streams that are synchronized in time, each providing the video from a different camera capturing the same event or movie. For delivery we leverage the adaptive features and time-based chunking of HTTP-based adaptive streaming, but now employing adaptation in both content and rate. Users are able to change their viewpoint on-demand and the client player adapts the rate at which data are retrieved from each stream based on the user's current view, the probabilities of switching to other views, and the user's current bandwidth conditions. A crucial component of such a system is the prefetching policy. For this we present an optimization model as well as a simpler heuristic that can balance the playback quality and the probability of playback interruptions. After analytically and numerically characterizing the optimal solution, we present a prototype implementation and sample results. Our prefetching and buffer management solution is shown to provide close to seamless playback switching when there is sufficient bandwidth to prefetch the parallel streams.
C1 [Carlsson, Niklas; Krishnamoorthi, Vengatanathan; Polishchuk, Tatiana] Linkoping Univ, S-58183 Linkoping, Sweden.
   [Eager, Derek] Univ Saskatchewan, Saskatoon, SK S7N 5C9, Canada.
C3 Linkoping University; University of Saskatchewan
RP Carlsson, N (corresponding author), Linkoping Univ, S-58183 Linkoping, Sweden.
EM niklas.carlsson@liu.se; eager@cs.usask.ca;
   Ven-gatanathan.Krishnamoorthi@liu.se; tessa3d@gmail.com
OI Carlsson, Niklas/0000-0003-1367-1594
FU Swedish Research Council (VR); Center for Industrial Information
   Technology (CENIIT); Natural Sciences and Engineering Research Council
   of Canada
FX This work was supported in part by the Swedish Research Council (VR), in
   part by the Center for Industrial Information Technology (CENIIT), and
   in part by the Natural Sciences and Engineering Research Council of
   Canada. The associate editor coordinating the review of this manuscript
   and approving it for publication was Dr. Yonggang Wen.
CR [Anonymous], TECH REP
   [Anonymous], 2011, Proc. second annu. acm conf. multimed. syst.-mmsys'11, DOI DOI 10.1145/1943552.1943574
   [Anonymous], P IET IBC
   [Anonymous], 2002, P ACM MULT JUAN LES
   [Anonymous], P 7 ACM MULT SYST C
   [Anonymous], P 7 ACM MULT SYST C
   [Anonymous], 2015, P 6 ACM MULT SYST C, DOI DOI 10.1145/2713168.2713189
   [Anonymous], 2009, P 17 INY PACK VID WO
   [Anonymous], P 4 WORKSH MOB VID C
   [Anonymous], 2015, P IEEE BMSB
   Batalla JM, 2016, IEEE J SEL AREA COMM, V34, P2154, DOI 10.1109/JSAC.2016.2577360
   Carlsson N, 2008, IEEE T MULTIMEDIA, V10, P871, DOI 10.1109/TMM.2008.922847
   Cha M, 2008, IMC'08: PROCEEDINGS OF THE 2008 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P71
   Chakareski J, 2013, IEEE T IMAGE PROCESS, V22, P3473, DOI 10.1109/TIP.2013.2269801
   Chakareski J, 2013, IEEE COMMUN MAG, V51, P94, DOI 10.1109/MCOM.2013.6515052
   Chen ZB, 2009, IEEE INT SYMP CIRC S, P1795, DOI 10.1109/ISCAS.2009.5118125
   Cheung G, 2008, 2008 IEEE 10TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, VOLS 1 AND 2, P454
   Cheung NM, 2009, PCS: 2009 PICTURE CODING SYMPOSIUM, P269
   De Abreu A, 2015, J VIS COMMUN IMAGE R, V33, P255, DOI 10.1016/j.jvcir.2015.09.010
   Devloo J, 2013, LECT NOTES COMPUT SC, V7943, P25, DOI 10.1007/978-3-642-38998-6_3
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Fujihashi T, 2014, IEEE T MULTIMEDIA, V16, P228, DOI 10.1109/TMM.2013.2281588
   Guo X, 2005, IEEE INT SYMP CIRC S, P3471
   Hossfeld T, 2015, COMPUT NETW, V81, P320, DOI 10.1016/j.comnet.2015.02.015
   Huang H, 2012, IEEE INFOCOM SER, P2791, DOI 10.1109/INFCOM.2012.6195701
   Huang T.Y., 2012, P 2012 ACM C INT MEA, P225, DOI 10.1145/2398776.2398800
   Huang TY, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P187, DOI 10.1145/2619239.2626296
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Johansen D, 2012, MULTIMED TOOLS APPL, V61, P419, DOI 10.1007/s11042-011-0847-5
   Krishnamoorthi V, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P551, DOI 10.1145/2733373.2806270
   Krishnamoorthi V, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P317, DOI 10.1145/2647868.2654951
   Krishnamoorthi V, 2013, I S MOD ANAL SIM COM, P182, DOI 10.1109/MASCOTS.2013.26
   Kurutepe E, 2007, IEEE T CIRC SYST VID, V17, P1558, DOI 10.1109/TCSVT.2007.903664
   Kurutepe E, 2008, IEEE IMAGE PROC, P3088, DOI 10.1109/ICIP.2008.4712448
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Maugey T, 2013, IEEE T MULTIMEDIA, V15, P1070, DOI 10.1109/TMM.2013.2246147
   Mavlankar A., 2010, 2010 18th International Packet Video Workshop (PV 2010), P64, DOI 10.1109/PV.2010.5706821
   Mavlankar A, 2008, IEEE IMAGE PROC, P2296, DOI 10.1109/ICIP.2008.4712250
   Niamut OmarA., 2013, P 4 ACM MULTIMEDIA S, P249
   Qiu TQ, 2009, PERF E R SI, V37, P275
   Rizzo L., 1997, Computer Communication Review, V27, P31, DOI 10.1145/251007.251012
   Seo D, 2014, MULTIMEDIA SYST, V20, P707, DOI 10.1007/s00530-013-0333-1
   Su TY, 2016, J REAL-TIME IMAGE PR, V12, P329, DOI 10.1007/s11554-015-0504-8
   Summers J., 2016, PROC IEEE INT S WORK, P1
   Sun Y, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P272, DOI 10.1145/2934872.2934898
   Tekalp AM, 2007, IEEE SIGNAL PROC MAG, V24, P77, DOI 10.1109/MSP.2007.905878
   Toni L, 2013, IEEE INT WORKSH MULT, P446, DOI 10.1109/MMSP.2013.6659330
   van Brandenburg R., 2011, 2011 15th International Conference on Intelligence in Next Generation Networks (ICIN): "From Bits to Data, from Pipes to Clouds", P151, DOI 10.1109/ICIN.2011.6081064
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Xiu XY, 2012, IEEE T MULTIMEDIA, V14, P1109, DOI 10.1109/TMM.2012.2191267
   Yin XQ, 2015, SIGCOMM'15: PROCEEDINGS OF THE 2015 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P325, DOI 10.1145/2785956.2787486
   Zhang WZ, 2016, COMPUT COMMUN, V85, P89, DOI 10.1016/j.comcom.2016.04.001
   Zhao MC, 2015, 2015 PICTURE CODING SYMPOSIUM (PCS) WITH 2015 PACKET VIDEO WORKSHOP (PV), P221, DOI 10.1109/PCS.2015.7170079
   Zhao YP, 2007, IEEE ACM T NETWORK, V15, P1149, DOI 10.1109/TNET.2007.896534
   Zou XK, 2015, 16TH INTERNATIONAL WORKSHOP ON MOBILE COMPUTING SYSTEMS AND APPLICATIONS (HOTMOBILE' 15), P57, DOI 10.1145/2699343.2699359
NR 56
TC 35
Z9 39
U1 2
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2017
VL 19
IS 7
BP 1637
EP 1653
DI 10.1109/TMM.2017.2673412
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5ZN
UT WOS:000404061800019
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, FZ
   Nagano, H
   Kashino, K
   Igarashi, T
AF Wang, Fangzhou
   Nagano, Hidehisa
   Kashino, Kunio
   Igarashi, Takeo
TI Visualizing Video Sounds With Sound Word Animation to Enrich User
   Experience
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Animation; entertainment; environmental sound processing; soundword;
   sound visualization; user experience; video annotation.
ID CLASSIFICATION; RECOGNITION; MODEL; ATTENTION; TEXT
AB Sound information in videos plays an important role in shaping the user experience. When sound is not accessible in videos, text captions can provide sound information. However, conventional text captions are not very expressive for nonverbal sounds because they are designed to visualize speech sounds. Here, we present a framework to automatically transform nonverbal video sounds into animated soundwords and position them near the sound source objects in the video for visualization. This provides natural visual representation of nonverbal sounds with rich information about the sound category and dynamics. To evaluate how the animated sound words generated by our framework affect the user experience, we implemented an experimental system and conducted a user study involving over 300 participants from an online crowdsourcing service. The results of the user study show that the animated sound words can effectively and naturally visualize the dynamics of sound while clarifying the position of the sound source as well as contribute to making video-watching more enjoyable and increasing the visual impact of videos.
C1 [Wang, Fangzhou; Igarashi, Takeo] Univ Tokyo, Tokyo 1138654, Japan.
   [Wang, Fangzhou] Google Inc, Mountain View, CA 94043 USA.
   [Nagano, Hidehisa; Kashino, Kunio] NTT Corp, NTT Commun Sci Labs, Atsugi, Kanagawa 2430198, Japan.
C3 University of Tokyo; Google Incorporated; Nippon Telegraph & Telephone
   Corporation
RP Wang, FZ (corresponding author), Google Inc, Mountain View, CA 94043 USA.
EM fzw@acm.org; nagano.hidehisa@lab.ntt.co.jp; kashino.kunio@lab.ntt.co.jp;
   takeo@acm.org
RI Igarashi, Takeo/ITT-5921-2023
CR Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Carrier David., 2000, The Aesthetics of Comics
   Chachada S, 2014, APSIPA TRANS SIGNAL, V3, DOI 10.1017/ATSIP.2014.12
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chu S, 2009, IEEE T AUDIO SPEECH, V17, P1142, DOI 10.1109/TASL.2009.2017438
   Chu WT, 2015, IEEE T MULTIMEDIA, V17, P201, DOI 10.1109/TMM.2014.2383616
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Fang YM, 2012, IEEE T MULTIMEDIA, V14, P187, DOI 10.1109/TMM.2011.2169775
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   Goldhor R. S., 1993, ICASSP-93. 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing (Cat. No.92CH3252-4), P149, DOI 10.1109/ICASSP.1993.319077
   Goldman DB, 2006, ACM T GRAPHIC, V25, P862, DOI 10.1145/1141911.1141967
   Goldman DB, 2008, UIST 2008: PROCEEDINGS OF THE 21ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P3, DOI 10.1145/1449715.1449719
   Gribonval R, 1996, PROCEEDINGS OF THE IEEE-SP INTERNATIONAL SYMPOSIUM ON TIME-FREQUENCY AND TIME-SCALE ANALYSIS, P125, DOI 10.1109/TFSA.1996.546702
   Hong R., 2010, Proceedings of the 18th ACM international conference on Multimedia, P421
   Tran HD, 2011, IEEE T AUDIO SPEECH, V19, P1556, DOI 10.1109/TASL.2010.2093519
   Ishihara K, 2004, LECT NOTES ARTIF INT, V3157, P909
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jing GM, 2015, IEEE T MULTIMEDIA, V17, P2122, DOI 10.1109/TMM.2015.2474263
   Kittur A., 2008, Proceedings of the SIGCHI conference on human factors in computing systems, P453
   Lee JC, 2002, ACTA HORTIC, P81, DOI 10.17660/ActaHortic.2002.587.5
   Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410
   Matthews Tara., 2005, Proceedings of the 7th international ACM SIGACCESS conference on Computers and accessibility, Assets '05, P52, DOI DOI 10.1145/1090785.109079
   Nienhaus M, 2005, IEEE COMPUT GRAPH, V25, P40, DOI 10.1109/MCG.2005.53
   Rashid R, 2008, INT J HUM-COMPUT INT, V24, P505, DOI 10.1080/10447310802142342
   Rashid R, 2006, LECT NOTES COMPUT SC, V4061, P24
   Rosten E, 2005, LECT NOTES COMPUT SC, V3804, P294
   SHANNO DF, 1970, MATH COMPUT, V24, P647, DOI 10.2307/2004840
   Shen J., 2016, SMART AMBIENT SOUND, P231
   Smaragdis P, 2003, 2003 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS PROCEEDINGS, P177, DOI 10.1109/ASPAA.2003.1285860
   Stowell D, 2015, IEEE T MULTIMEDIA, V17, P1733, DOI 10.1109/TMM.2015.2428998
   Strapparava C., 2006, P LREC 2006, P1
   Thanedar V., 2004, 200411 U CAL
   Wake SH, 2001, IEICE T INF SYST, VE84D, P1568
   Wang M, 2012, IEEE T MULTIMEDIA, V14, P858, DOI 10.1109/TMM.2012.2187181
   Wold E, 1996, IEEE MULTIMEDIA, V3, P27, DOI 10.1109/93.556537
   Wu TF, 2004, J MACH LEARN RES, V5, P975
   Yamamoto T, 2012, J. Soc. Art. Sci, V11, P1
NR 37
TC 17
Z9 17
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2017
VL 19
IS 2
BP 418
EP 429
DI 10.1109/TMM.2016.2613641
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN1UN
UT WOS:000395795800016
DA 2024-07-18
ER

PT J
AU Hao, YB
   Mu, TT
   Hong, RC
   Wang, M
   An, N
   Goulermas, JY
AF Hao, Yanbin
   Mu, Tingting
   Hong, Richang
   Wang, Meng
   An, Ning
   Goulermas, John Y.
TI Stochastic Multiview Hashing for Large-Scale Near-Duplicate Video
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Divergence; hashing; multiview learning; near-duplicate video retrieval
   (NDVR); semi-supervised learning
ID NONLINEAR DIMENSIONALITY REDUCTION; FEATURE FUSION
AB Near-duplicate video retrieval (NDVR) has been a significant research task in multimedia given its high impact in applications, such as video search, recommendation, and copyright protection. In addition to accurate retrieval performance, the exponential growth of online videos has imposed heavy demands on the efficiency and scalability of the existing systems. Aiming at improving both the retrieval accuracy and speed, we propose a novel stochastic multiview hashing algorithm to facilitate the construction of a large-scale NDVR system. Reliable mapping functions, which convert multiple types of keyframe features, enhanced by auxiliary information such as video-keyframe association and ground truth relevance to binary hash code strings, are learned by maximizing a mixture of the generalized retrieval precision and recall scores. A composite Kullback-Leibler divergence measure is used to approximate the retrieval scores, which aligns stochastically the neighborhood structures between the original feature and the relaxed hash code spaces. The efficiency and effectiveness of the proposed method are examined using two public near-duplicate video collections and are compared against various classical and state-of-the-art NDVR systems.
C1 [Hao, Yanbin; Hong, Richang; Wang, Meng; An, Ning] Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Peoples R China.
   [Mu, Tingting] Univ Manchester, Sch Comp Sci, Manchester M13 9PL, Lancs, England.
   [Goulermas, John Y.] Univ Liverpool, Dept Comp Sci, Liverpool L69 3BX, Merseyside, England.
C3 Hefei University of Technology; University of Manchester; University of
   Liverpool
RP Hao, YB (corresponding author), Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Peoples R China.
EM haoyanbin@mail.hfut.edu.cn; tingtingmu@me.com; hongrc.hfut@gmail.com;
   eric.mengwang@gmail.com; ning.an@hfut.edu.cn;
   j.y.goulermas@liverpool.ac.uk
RI Mu, Tingting/AAV-4795-2020; Hao, Yanbin/AAC-8050-2019; Wang,
   Meng/ITR-8699-2023
OI Mu, Tingting/0000-0001-6315-3432; Hao, Yanbin/0000-0002-0695-1566; An,
   Ning/0000-0003-3317-5299
FU International Research Base for Developing Innovative Gerontechnology;
   national "111" project of China [B14025]
FX This work was supported by "The International Research Base for
   Developing Innovative Gerontechnology", sponsored by the national "111"
   project (No. B14025) of China. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Marco Bertini.
CR [Anonymous], 2013, Book Linear cross-modal hashing for efficient multimedia search, DOI DOI 10.1145/2502081.2502107
   [Anonymous], 2004, ACMMM
   [Anonymous], 1952, Psychometrika
   [Anonymous], 2009, P ACM INT C MULT, DOI DOI 10.1145/1631272.1631295
   Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928
   Cao LJ, 2015, INFORM SCIENCES, V302, P122, DOI 10.1016/j.ins.2014.08.017
   Chen J, 2010, BUILD ENVIRON, V45, P1113, DOI 10.1016/j.buildenv.2009.10.017
   Cherubini Mauro., 2009, Proceedings of the 17th ACM International Conference on Multimedia, number April in MM '09, P35, DOI DOI 10.1145/1631272.1631280
   Chou CL, 2015, IEEE T MULTIMEDIA, V17, P382, DOI 10.1109/TMM.2015.2391674
   Cui Bin., 2010, ACM SIGMOD International Conference on Management of Data, P435
   Dong W., 2008, PROCEEDING ACM MULTI, P179, DOI DOI 10.1145/1459359.1459384
   Douze M, 2010, LECT NOTES COMPUT SC, V6311, P522, DOI 10.1007/978-3-642-15549-9_38
   Fu Y., 2008, P 2008 INT C CONT BA, P127, DOI DOI 10.1145/1386352.1386373
   Gao LL, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P903, DOI 10.1145/2733373.2806360
   Gao LL, 2015, PROC CVPR IEEE, P4371, DOI 10.1109/CVPR.2015.7299066
   Gao XJ, 2016, NEUROCOMPUTING, V171, P901, DOI 10.1016/j.neucom.2015.07.043
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Hampapur A, 2002, P SOC PHOTO-OPT INS, V4676, P194
   Hinton G. E., 2002, Advances in Neural InformationProcessing Systems, P857
   Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325
   Hu Y, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P527, DOI 10.1145/2647868.2654906
   Hua XS, 2004, IEEE IMAGE PROC, P685
   JACOBS RA, 1988, NEURAL NETWORKS, V1, P295, DOI 10.1016/0893-6080(88)90003-2
   Ke Y, 2004, PROC CVPR IEEE, P506
   Law-To J, 2006, INT C PATT RECOG, P232
   Law-To Julien., 2007, P 6 ACM INT C IMAGE, P371
   Liu JJ, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2501654.2501658
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu XL, 2015, IEEE I CONF COMP VIS, P1107, DOI 10.1109/ICCV.2015.132
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ngo C-W., 2006, ACM MULTIMEDIA 06, P845
   Nie X., 2016, CORR
   Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Salakhutdinov R., 2007, PMLR, P412
   Shang L., 2010, Proceedings of the International Conference on Multimedia, P531
   Shen H.T., 2007, VLDB, P1374
   Shen XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P831, DOI 10.1145/2733373.2806342
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399, DOI 10.1145/1101149.1101236
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Song J., 2011, P 19 ACM INT C MULT, P423, DOI DOI 10.1145/2072298.2072354
   Song JK, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P827, DOI 10.1145/2733373.2806341
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Sun SL, 2013, NEURAL COMPUT APPL, V23, P2031, DOI 10.1007/s00521-013-1362-6
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Venna J, 2010, J MACH LEARN RES, V11, P451
   Wang J., 2014, CoRR
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Weiss Yair, 2009, Advances in Neural Information Processing Systems, P1753, DOI DOI 10.5555/2981780.2981999
   Wu A. G., 2007, P ACM MM, P218
   Wu X, 2009, IEEE T MULTIMEDIA, V11, P196, DOI 10.1109/TMM.2008.2009673
   Xu JJ, 2014, IEEE T MULTIMEDIA, V16, P403, DOI 10.1109/TMM.2013.2291218
   Yang J, 2003, PATTERN RECOGN, V36, P1369, DOI 10.1016/S0031-3203(02)00262-5
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Yuan J., 2005, P ADV MULT INF PROC, P479
   Zhang D, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P18
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhou XM, 2009, IEEE T MULTIMEDIA, V11, P879, DOI 10.1109/TMM.2009.2021794
   Zou FH, 2015, IEEE T MULTIMEDIA, V17, P1006, DOI 10.1109/TMM.2015.2425651
NR 63
TC 69
Z9 75
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2017
VL 19
IS 1
BP 1
EP 14
DI 10.1109/TMM.2016.2610324
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EH0SX
UT WOS:000391475200001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jiang, JJ
   Ma, X
   Chen, C
   Lu, T
   Wang, ZY
   Ma, JY
AF Jiang, Junjun
   Ma, Xiang
   Chen, Chen
   Lu, Tao
   Wang, Zhongyuan
   Ma, Jiayi
TI Single Image Super-Resolution via Locally Regularized Anchored
   Neighborhood Regression and Nonlocal Means
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Anchored neighborhood regression; locality geometry; neighbor embedding;
   nonlocal means; super-resolution (SR)
ID SPARSE REPRESENTATION; FACE SUPERRESOLUTION; INTERPOLATION;
   HALLUCINATION; REGISTRATION; RESOLUTION; ALGORITHM; LIMITS
AB The goal of learning-based image super resolution (SR) is to generate a plausible and visually pleasing high-resolution (HR) image from a given low-resolution (LR) input. The SR problem is severely underconstrained, and it has to rely on examples or some strong image priors to reconstruct the missing HR image details. This paper addresses the problem of learning the mapping functions (i.e., projection matrices) between the LR and HR images based on a dictionary of LR and HR examples. Encouraged by recent developments in image prior modeling, where the state-of-the-art algorithms are formed with nonlocal self-similarity and local geometry priors, we seek an SR algorithm of similar nature that will incorporate these two priors into the learning from LR space to HR space. The nonlocal self-similarity prior takes advantage of the redundancy of similar patches in natural images, while the local geometry prior of the data space can be used to regularize the modeling of the nonlinear relationship between LR and HR spaces. Based on the above two considerations, we first apply the local geometry prior to regularize the patch representation, and then utilize the nonlocal means filter to improve the super-resolved outcome. Experimental results verify the effectiveness of the proposed algorithm compared with the state-of-the-art SR methods.
C1 [Jiang, Junjun] China Univ Geosci, Sch Comp Sci, Wuhan 430074, Peoples R China.
   [Jiang, Junjun] China Univ Geosci, Hubei Key Lab Intelligent Geoinformat Proc, Wuhan 430074, Peoples R China.
   [Ma, Xiang] Changan Univ, Sch Informat Engn, Xian 710048, Peoples R China.
   [Chen, Chen] Univ Cent Florida, Ctr Comp Vis Res, Orlando, FL 32816 USA.
   [Lu, Tao] Wuhan Inst Technol, Sch Comp Sci & Engn, Wuhan 430073, Peoples R China.
   [Wang, Zhongyuan] Wuhan Univ, Sch Comp, Natl Engn Res Ctr Multimedia Software, Wuhan 430072, Peoples R China.
   [Ma, Jiayi] Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.
C3 China University of Geosciences; China University of Geosciences;
   Chang'an University; State University System of Florida; University of
   Central Florida; Wuhan Institute of Technology; Wuhan University; Wuhan
   University
RP Ma, X (corresponding author), Changan Univ, Sch Informat Engn, Xian 710048, Peoples R China.
EM junjun0595@163.com; maxiangmail@163.com; chenchen870713@gmail.com;
   lutxyl@gmail.com; wzyhope@163.com; jyma2010@gmail.com
RI Jiang, Junjun/L-7087-2019; , Chen_Chen/A-8825-2015; Ma,
   Jiayi/Y-2470-2019; Wang, Zhongyuan/ABD-2189-2020
OI Jiang, Junjun/0000-0002-5694-505X; , Chen_Chen/0000-0003-3957-7061; Ma,
   Jiayi/0000-0003-3264-3265; 
FU National Natural Science Foundation of China [61501413, 61502354,
   61671332, 61503288]; China Fundamental Research Funds for the Central
   Universities [310824153508]; Shannxi Science Foundation of China
   [2015JM6309]; Fundamental Research Funds for the Central Universities,
   China University of Geosciences (Wuhan) [CUGL160412]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61501413, Grant 61502354, Grant 61671332, and Grant
   61503288, by the China Fundamental Research Funds for the Central
   Universities under Grant 310824153508, by the Shannxi Science Foundation
   of China under Grant 2015JM6309, and by the Fundamental Research Funds
   for the Central Universities, China University of Geosciences (Wuhan)
   under Grant CUGL160412. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Shahram
   Shirani. (Corresponding author: Xiang Ma.)
CR [Anonymous], 2009, P ADV NEUR INF PROC
   Arias P, 2009, LECT NOTES COMPUT SC, V5681, P345, DOI 10.1007/978-3-642-03641-5_26
   Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Chen C, 2012, CONF REC ASILOMAR C, P608, DOI 10.1109/ACSSC.2012.6489079
   Chen CLP, 2015, IEEE T IMAGE PROCESS, V24, P4014, DOI 10.1109/TIP.2015.2456432
   Chen D, 2015, IEEE T PARALL DISTR, V26, P847, DOI 10.1109/TPDS.2014.2311805
   Chen D, 2015, IEEE T COMPUT, V64, P707, DOI 10.1109/TC.2013.2295806
   Dong WS, 2014, IEEE T IMAGE PROCESS, V23, P5249, DOI 10.1109/TIP.2014.2363616
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1382, DOI 10.1109/TIP.2012.2231086
   Dong Weisheng, 2011, IEEE Trans Image Process, V20, P1838, DOI 10.1109/TIP.2011.2108306
   Elad M, 1999, IEEE T IMAGE PROCESS, V8, P387, DOI 10.1109/83.748893
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075
   Gao XB, 2012, IEEE T IMAGE PROCESS, V21, P469, DOI 10.1109/TIP.2011.2161482
   Greenspan H, 2009, COMPUT J, V52, P43, DOI 10.1093/comjnl/bxm075
   Hawe S, 2013, IEEE T IMAGE PROCESS, V22, P2138, DOI 10.1109/TIP.2013.2246175
   Hu YT, 2016, IEEE T IMAGE PROCESS, V25, P4091, DOI 10.1109/TIP.2016.2580942
   IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P27, DOI 10.1109/TMM.2016.2601020
   Jiang JJ, 2016, IEEE T CIRC SYST VID, V26, P1674, DOI 10.1109/TCSVT.2015.2433538
   Jiang JJ, 2016, INFORM SCIENCES, V367, P354, DOI 10.1016/j.ins.2016.05.032
   Jiang JJ, 2015, IEEE PHOTONICS J, V7
   Jiang JJ, 2014, IEEE T IMAGE PROCESS, V23, P4220, DOI 10.1109/TIP.2014.2347201
   Jiang JJ, 2014, IEEE T MULTIMEDIA, V16, P1268, DOI 10.1109/TMM.2014.2311320
   Kim KI, 2010, IEEE T PATTERN ANAL, V32, P1127, DOI 10.1109/TPAMI.2010.25
   Li K, 2016, PATTERN RECOGN, V51, P59, DOI 10.1016/j.patcog.2015.08.008
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Li XY, 2015, IEEE T IMAGE PROCESS, V24, P2874, DOI 10.1109/TIP.2015.2432713
   Lin ZC, 2004, IEEE T PATTERN ANAL, V26, P83, DOI 10.1109/TPAMI.2004.1261081
   Liu LC, 2017, IEEE T CYBERNETICS, V47, P600, DOI 10.1109/TCYB.2016.2521428
   Lu HY, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8060499
   Ma JY, 2016, INFORM FUSION, V31, P100, DOI 10.1016/j.inffus.2016.02.001
   Ma JY, 2016, IEEE T IMAGE PROCESS, V25, P53, DOI 10.1109/TIP.2015.2467217
   Ma JY, 2015, IEEE T GEOSCI REMOTE, V53, P6469, DOI 10.1109/TGRS.2015.2441954
   Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452
   Ren J, 2013, IEEE T IMAGE PROCESS, V22, P1454, DOI 10.1109/TIP.2012.2231690
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Saul LK, 2004, J MACH LEARN RES, V4, P119, DOI 10.1162/153244304322972667
   STARK H, 1989, J OPT SOC AM A, V6, P1715, DOI 10.1364/JOSAA.6.001715
   Sun J, 2008, PROC CVPR IEEE, P2471, DOI 10.1109/CVPR.2008.4587659
   Tikhonov A., 1977, Solution of Ill-Posed Problems
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Tsai R.Y., 1984, Proc. Inst Elect Eng, V1, P317
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang LZ, 2015, IEEE GEOSCI REMOTE S, V12, P736, DOI 10.1109/LGRS.2014.2360457
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZY, 2014, IEEE T CIRC SYST VID, V24, P802, DOI 10.1109/TCSVT.2013.2290574
   Xiong ZW, 2013, IEEE T MULTIMEDIA, V15, P1458, DOI 10.1109/TMM.2013.2264654
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang MC, 2013, IEEE T MULTIMEDIA, V15, P498, DOI 10.1109/TMM.2012.2232646
   Yu GS, 2012, IEEE T IMAGE PROCESS, V21, P2481, DOI 10.1109/TIP.2011.2176743
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang XJ, 2008, IEEE T IMAGE PROCESS, V17, P887, DOI 10.1109/TIP.2008.924279
   Zhang YQ, 2015, IEEE T IMAGE PROCESS, V24, P2797, DOI 10.1109/TIP.2015.2431435
   Zhu YM, 2014, SIGNAL PROCESS-IMAGE, V29, P875, DOI 10.1016/j.image.2014.06.005
   Zhu YF, 2015, IEEE INT CONF MOB, P1, DOI 10.1109/MASS.2015.83
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
NR 62
TC 137
Z9 147
U1 1
U2 98
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2017
VL 19
IS 1
BP 15
EP 26
DI 10.1109/TMM.2016.2599145
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EH0SX
UT WOS:000391475200002
DA 2024-07-18
ER

PT J
AU Guo, J
   Song, B
   Du, XJ
AF Guo, Jie
   Song, Bin
   Du, Xiaojiang
TI Significance Evaluation of Video Data Over Media Cloud Based on
   Compressed Sensing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compressed sensing; media cloud; significance evaluation; video
ID ALGORITHM; QUALITY
AB Given the varying communication environment between the media cloud and users, there is a need to ensure the most significant part of a video will be successfully transmitted. Although there exist some techniques to evaluate the significance of video data in traditional video coding methods, such as H.264, the evaluation algorithms are often simple and inaccurate. This paper presents a novel significance evaluation method for video data based on compressed sensing. Specifically, we propose a method to obtain a trained dictionary directly by using the measurements of the video data, and then keep the sparse components and generate a saliency map. Since the sparse components can reflect the essential parts of videos, we discuss how to analyze the area and distribution of salient regions. At last, we present a computing method that gives the degree of significance of a frame. Experimental results show that the proposed saliency map reflects the focus points of humans. The method can be used in the distribution of video data over "wireless" transmissions and provide good video quality to mobile users.
C1 [Guo, Jie; Song, Bin] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Du, Xiaojiang] Temple Univ, Dept Comp & Informat Sci, Philadelphia, PA 19122 USA.
C3 Xidian University; Pennsylvania Commonwealth System of Higher Education
   (PCSHE); Temple University
RP Guo, J (corresponding author), Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
EM irene2010guojie@163.com; bsong@mail.xidian.edu.cn; dxj@ieee.org
FU National Natural Science Foundation of China [61271173, 61372068];
   Research Fund for the Doctoral Program of Higher Education of China
   [20130203110005]; Fundamental Research Funds for the Central
   Universities [K5051301033]; 111 Project [B08038]; ISN State Key
   Laboratory at Xidian University; U.S. National Science Foundation
   [CNS-1065444]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61271173 and Grant 61372068, by the Research Fund for
   the Doctoral Program of Higher Education of China 20130203110005, by the
   Fundamental Research Funds for the Central Universities K5051301033, by
   the 111 Project B08038, by the ISN State Key Laboratory at Xidian
   University, and by the U.S. National Science Foundation under Grant
   CNS-1065444. The guest editor coordinating the review of this manuscript
   and approving it for publication was Prof. Honggang Wang.
CR 4G Americas, 2013, SUPP WIR VID GROWTH
   Aghagolzadeh M, 2013, IEEE GLOB CONF SIG, P1033, DOI 10.1109/GlobalSIP.2013.6737070
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Anaraki FP, 2013, INT CONF ACOUST SPEE, P5469, DOI 10.1109/ICASSP.2013.6638709
   [Anonymous], 2014, 2014 IEEE INT C MULT, DOI DOI 10.1109/ICME.2014.6890255
   [Anonymous], 2014, CISC VIS NETW IND GL
   [Anonymous], 2014, 14 LTE BROADC BUS CA
   Borji Ali., 2015, CoRR
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Chang SH, 2007, IEEE T BROADCAST, V53, P79, DOI 10.1109/TBC.2006.887170
   Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010
   Díaz-Sánchez D, 2011, IEEE T CONSUM ELECTR, V57, P970, DOI 10.1109/TCE.2011.5955247
   Do TT, 2009, IEEE IMAGE PROC, P1393, DOI 10.1109/ICIP.2009.5414631
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Jaekyung Lee, 2014, 2014 IEEE International Conference on Consumer Electronics (ICCE), P79, DOI 10.1109/ICCE.2014.6775917
   Jin YC, 2015, IEEE T CIRC SYST VID, V25, P1914, DOI 10.1109/TCSVT.2015.2402892
   Lee H, 2012, IEEE T BROADCAST, V58, P47, DOI 10.1109/TBC.2011.2164308
   Lin KCJ, 2013, IEEE T MOBILE COMPUT, V12, P21, DOI 10.1109/TMC.2011.242
   Liu ZR, 2010, IEEE IMAGE PROC, P1649, DOI 10.1109/ICIP.2010.5654000
   Ong D, 2013, C LOCAL COMPUT NETW, P743, DOI 10.1109/LCN.2013.6761325
   Soltani S, 2009, IEEE T MULTIMEDIA, V11, P742, DOI 10.1109/TMM.2009.2017622
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Wei HC, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 2, PROCEEDINGS, P65
   Xue GJ, 2013, IEEE T CIRC SYST VID, V23, P1346, DOI 10.1109/TCSVT.2013.2243053
   Yan JC, 2010, IEEE SIGNAL PROC LET, V17, P739, DOI 10.1109/LSP.2010.2053200
   Zhou L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2438546
NR 26
TC 12
Z9 14
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2016
VL 18
IS 7
BP 1297
EP 1304
DI 10.1109/TMM.2016.2564100
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR2RW
UT WOS:000379752600006
DA 2024-07-18
ER

PT J
AU Choi, MW
   Lee, GJ
   Jin, SG
   Koo, JH
   Kim, BJ
   Choi, SY
AF Choi, Munhwan
   Lee, Gyujin
   Jin, Sunggeun
   Koo, Jonghoe
   Kim, Byoungjin
   Choi, Sunghyun
TI Link Adaptation for High-Quality Uncompressed Video Streaming in 60-GHz
   Wireless Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 60-GHz networks; expected peak signal-to-noise ratio (ePSNR); link
   adaptation; uncompressed video streaming; unequal error protection (UEP)
ID TRANSMISSION
AB The emerging 60-GHz multigigabits per second wireless technology enables the streaming of high-quality "uncompressed" video, which has been impossible with other existing wireless technologies. To support such a resource-hungry uncompressed video streaming service with limited wireless resources, it is necessary to design efficient link adaptation policies selecting suitable transmission rates for the 60-GHz wireless channel environment, thus optimizing video quality and resource management. For proper design of the link adaptation policies, we propose a new metric, called expected peak signal-to-noise ratio (ePSNR), to numerically estimate the video streaming quality. By using the ePSNR as a criterion, we propose two link adaptation policies with different objectives considering unequal error protection (UEP). The proposed link adaptation policies attempt to 1) maximize the video quality for given wireless resources, or 2) minimize the required wireless resources, while meeting the video quality. From the link adaptation policies, we provide a distributed resource management scheme for multiple users to maintain satisfactory video streaming quality. Our extensive simulation results demonstrate that the newly proposed variable, i.e., ePSNR, well represents the level of video quality. It is also shown that the proposed link adaptation policies can enhance the resource efficiency while achieving acceptable quality of the video streaming.
C1 [Choi, Munhwan] Samsung Elect Co Ltd, Suwon 443803, South Korea.
   [Lee, Gyujin; Koo, Jonghoe; Choi, Sunghyun] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 151744, South Korea.
   [Lee, Gyujin; Koo, Jonghoe; Choi, Sunghyun] Seoul Natl Univ, INMC, Seoul 151744, South Korea.
   [Jin, Sunggeun] Daegu Univ, Sch Comp & Informat Technol, Gyongsan 712714, South Korea.
   [Kim, Byoungjin] Seoul Natl Univ, Sch Law, Seoul 151744, South Korea.
C3 Samsung; Samsung Electronics; Seoul National University (SNU); Seoul
   National University (SNU); Daegu University; Seoul National University
   (SNU)
RP Choi, MW (corresponding author), Samsung Elect Co Ltd, Suwon 443803, South Korea.; Lee, GJ; Koo, JH; Choi, SY (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 151744, South Korea.; Lee, GJ; Koo, JH; Choi, SY (corresponding author), Seoul Natl Univ, INMC, Seoul 151744, South Korea.; Jin, SG (corresponding author), Daegu Univ, Sch Comp & Informat Technol, Gyongsan 712714, South Korea.; Kim, BJ (corresponding author), Seoul Natl Univ, Sch Law, Seoul 151744, South Korea.
EM mchoi@mwnl.snu.ac.kr; gjlee@mwnl.snu.ac.kr; sgjin@daegu.ac.kr;
   jhkoo@mwnl.snu.ac.kr; bjkim@mwnl.snu.ac.kr; schoi@snu.ac.kr
RI Sohn, Illsoo/AAE-5244-2020
OI Sohn, Illsoo/0000-0003-3943-4781; Choi, Sunghyun/0000-0003-0279-445X
FU National Research Foundation of Korea (NRF) [NRF-2015R1A2A2A01006750];
   Korean government (MSIP) [NRF-2014R1A1A2058490]; Ministry of Education;
   Basic Science Research Program through the NRF
FX This work was supported by the National Research Foundation of Korea
   (NRF) under Grant NRF-2015R1A2A2A01006750, by the Korean government
   (MSIP) and Basic Science Research Program through the NRF under Grant
   NRF-2014R1A1A2058490, and by the Ministry of Education. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Tommaso Melodia.
CR An XL, 2008, IEEE VTS VEH TECHNOL, P1636
   [Anonymous], EURASIP J APPL S MAR
   [Anonymous], 2012, P80211AD IEEE
   [Anonymous], 2010, 387 ECMA INT
   [Anonymous], 13 INT C MOD TECHN T
   [Anonymous], 2009, HIGH DEFINITION MULT
   Argyriou A, 2008, IEEE T MULTIMEDIA, V10, P691, DOI 10.1109/TMM.2008.922776
   Daji Qiao, 2002, IEEE Transactions on Mobile Computing, V1, P278, DOI 10.1109/TMC.2002.1175541
   De F. Simone, 2008, P SPIE OPT ENG APPL
   Ecma International, 2008, 368 ECMA INT
   Ferre P., 2003, P S IEEE BEN CHAPT C
   He ZF, 2014, MOB COMPUT COMMUN RE, V18, P14, DOI 10.1145/2508478.2508485
   Hsu C.-W., 2009, EAI INT C HET NETW Q
   IEEE, 2013, P80211AC IEEE
   Korte R., 2010, COMMUNICATION
   Pavon JD, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-5, P1108
   Reibman AR, 2004, IEEE T MULTIMEDIA, V6, P327, DOI 10.1109/TMM.2003.822785
   Singh H, 2008, IEEE WCNC, P1939, DOI 10.1109/WCNC.2008.345
   Singh H, 2008, CONSUM COMM NETWORK, P243, DOI 10.1109/ccnc08.2007.61
   Singh H, 2008, IEEE COMMUN MAG, V46, P71, DOI 10.1109/MCOM.2008.4689210
   SUN MT, 2001, SIGNAL PROCESSING CO
   Tang SY, 2014, IEEE T MULTIMEDIA, V16, P2256, DOI 10.1109/TMM.2014.2348947
   Van den Branden Lambrecht C. J., 2001, VISION MODELS APPL I, P209
   Wang Z, 2004, SIGNAL PROCESS-IMAGE, V19, P121, DOI 10.1016/S0923-5965(03)00076-6
   Xu H, 2002, IEEE J SEL AREA COMM, V20, P620, DOI 10.1109/49.995521
   Yong SK, 2007, EURASIP J WIREL COMM, DOI 10.1155/2007/78907
   Zhou Lan, 2008, 2008 International Wireless Communications and Mobile Computing Conference Conference, P689, DOI 10.1109/IWCMC.2008.119
NR 27
TC 9
Z9 11
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2016
VL 18
IS 4
BP 627
EP 642
DI 10.1109/TMM.2016.2525012
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DH4XZ
UT WOS:000372790300007
DA 2024-07-18
ER

PT J
AU Shen, CY
   Huang, X
   Zhao, Q
AF Shen, Chengyao
   Huang, Xun
   Zhao, Qi
TI Predicting Eye Fixations on Webpage With an Ensemble of Early Features
   and High-Level Representations from Deep Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; visual attention; web viewing; webpage saliency
ID VISUAL-ATTENTION; SALIENCY; SHIFTS
AB In recent decades, webpages are becoming an increasingly important visual information source. Compared with natural images, webpages are different in many ways. For example, webpages are usually rich in semantically meaningful visual media (text, pictures, logos, and animations), which make the direct application of some traditional low-level saliency models ineffective. Besides, distinct web-viewing patterns such as top-left bias and banner blindness suggest different ways for predicting attention deployment on a webpage. In this study, we utilize a new scheme of low-level feature extraction pipeline and combine it with high-level representations from deep neural networks. The proposed model is evaluated on a newly published webpage saliency dataset with three popular evaluation metrics. Results show that our model outperforms other existing saliency models by a large margin and both low-and high-level features play an important role in predicting fixations on webpage.
C1 [Shen, Chengyao; Huang, Xun; Zhao, Qi] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117583, Singapore.
   [Huang, Xun] Beihang Univ, Sch Comp, Beijing 100191, Peoples R China.
C3 National University of Singapore; Beihang University
RP Shen, CY (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117583, Singapore.
EM scyscyao@gmail.com; xunhuang1995@gmail.com; eleqiz@nus.edu.sg
RI zhao, qi/KGK-3760-2024
FU Singapore Ministry of Education Academic Research Fund
   [R-263-000-B32-112]; Defense Innovative Research Programme [9014100596]
FX This work was supported by the Singapore Ministry of Education Academic
   Research Fund Tier 2 under Grant R-263-000-B32-112 and by the Defense
   Innovative Research Programme under Grant 9014100596. The guest editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Jiebo Luo.
CR [Anonymous], 2008, Advances in neural information processing systems
   [Anonymous], 2008, NIPS
   Borji A., 2012, CVPR, DOI DOI 10.1109/CVPR.2012.6247706
   Borji A, 2012, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2012.6247711
   Brainard DH, 1997, SPATIAL VISION, V10, P433, DOI 10.1163/156856897X00357
   Bruce NDB, 2009, J VISION, V9, DOI 10.1167/9.3.5
   Buscher G, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P21
   Bylinskii Z., 2015, MIT saliency benchmark
   Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963
   Cho CH, 2004, J ADVERTISING, V33, P89, DOI 10.1080/00913367.2004.10639175
   Cutrell E, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P407
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   DERRINGTON AM, 1984, J PHYSIOL-LONDON, V357, P241, DOI 10.1113/jphysiol.1984.sp015499
   Donahue J., 2013, Decaf: A deep convolutional activation feature for generic visual recognition
   Duggan GB, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1141
   Einhauser W., 2008, J VISION, V8
   Faraday P, 2000, SPRING COMP SCI, P155
   Frey HP, 2008, J VISION, V8, DOI 10.1167/8.14.6
   Garcia-Diaz A, 2012, J VISION, V12, DOI 10.1167/12.6.17
   Garcia-Diaz A, 2012, IMAGE VISION COMPUT, V30, P51, DOI 10.1016/j.imavis.2011.11.007
   Grier R.A., 2007, Human computer interaction research in web design and evaluation, P22
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hervet G, 2011, APPL COGNITIVE PSYCH, V25, P708, DOI 10.1002/acp.1742
   Hou XD, 2012, IEEE T PATTERN ANAL, V34, P194, DOI 10.1109/TPAMI.2011.146
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Itti L, 2000, VISION RES, V40, P1489, DOI 10.1016/S0042-6989(99)00163-7
   Itti L, 2009, Advances in neural information processing systems, V49, P1295, DOI [10.1016/j.visres.2008.09.007, DOI 10.1016/J.VISRES.2008.09.007]
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liang M, 2015, IEEE T IMAGE PROCESS, V24, P1178, DOI 10.1109/TIP.2015.2395713
   Liu N, 2015, PROC CVPR IEEE, P362, DOI 10.1109/CVPR.2015.7298633
   Nielsen J., 2006, F SHAPED PATTERN REA
   Nielsen J., 2007, BANNER BLINDNESS OLD
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Riche N, 2013, SIGNAL PROCESS-IMAGE, V28, P642, DOI 10.1016/j.image.2013.03.009
   Schauerte B., 2012, IEEE WORKSH APPL COM, P137, DOI 10.1109/WACV.2012.6163035
   Shen C., 2012, P NIPS DEEP LEARN UN, V2
   Shen CY, 2014, LECT NOTES COMPUT SC, V8695, P33, DOI 10.1007/978-3-319-10584-0_3
   Shen CY, 2014, NEUROCOMPUTING, V138, P61, DOI 10.1016/j.neucom.2013.09.053
   Still JeremiahD., 2010, 5th International Workshop on Model Driven Development of Advanced User Interfaces, MDDAUI '10, P25
   Stone B., 2007, P 29 COGN SCI SOC C, P665
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Vig E, 2014, PROC CVPR IEEE, P2798, DOI 10.1109/CVPR.2014.358
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhang XL, 2012, NEURON, V73, P183, DOI 10.1016/j.neuron.2011.10.035
   Zhao Q, 2011, J VISION, V11, DOI 10.1167/11.3.9
   Zhaoping L, 2008, J VISION, V8, DOI 10.1167/8.5.1
NR 49
TC 27
Z9 29
U1 1
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2015
VL 17
IS 11
SI SI
BP 2084
EP 2093
DI 10.1109/TMM.2015.2483370
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CV2QV
UT WOS:000364102400020
DA 2024-07-18
ER

PT J
AU Tang, JH
   Jin, L
   Li, ZC
   Gao, SH
AF Tang, Jinhui
   Jin, Lu
   Li, Zechao
   Gao, Shenghua
TI RGB-D Object Recognition via Incorporating Latent Data Structure and
   Prior Knowledge
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; RGB-D object recognition; transfer learning
ID FEATURES
AB For the task of RGB-D object recognition, it is important to identify suitable representations of images, which can boost the performance of object recognition. In this work, we propose a novel representation learning method for RGB-D images by jointly incorporating the underlying data structure and the prior knowledge of the data. Specifically, the convolutional neural networks (CNN) are employed to learn image representation by exploiting the underlying data structure. To handle the problem of the limited RGB and depth images for object recognition, the multi-level hierarchies of features trained on ImageNet from the CNN are transferred to learn rich generic feature representation for RGB and depth images while the labeled images are leveraged. On the other hand, we propose a novel deep auto-encoders (DAE) to exploit the prior knowledge, which can overcome the expensive computational cost of optimization in feature encoding. The expected representations of images are obtained by integrating the two types of image representations. To verify the effectiveness of the proposed method, we thoroughly conduct extensive experiments on two publicly available RGB-D datasets. The encouraging experimental results compared with the state-of-the-art approaches demonstrate the advantages of the proposed method.
C1 [Tang, Jinhui; Jin, Lu; Li, Zechao] Nanjing Univ Sci & Technol, Sch Engn & Comp Sci, Nanjing 210094, Jiangsu, Peoples R China.
   [Gao, Shenghua] ShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai 200031, Peoples R China.
C3 Nanjing University of Science & Technology; ShanghaiTech University
RP Tang, JH (corresponding author), Nanjing Univ Sci & Technol, Sch Engn & Comp Sci, Nanjing 210094, Jiangsu, Peoples R China.
EM jinhuitang@njust.edu.cn; lujin505@gmail.com; zechao.li@njust.edu.cn;
   gaoshh@shanghaitech.edu.cn
RI Tang, Jinhui/KBR-0891-2024
OI Tang, Jinhui/0000-0001-9008-222X
FU 973 Program [2014CB347600]; National Natural Science Foundation of China
   [61522203, 61402228]
FX This work was supported in part by the 973 Program Project 2014CB347600,
   and by the National Natural Science Foundation of China under Grant
   61522203 and Grant 61402228. The guest editor coordinating the review of
   this manuscript and approving it for publication was Dr. Guo-Jun Qi.
CR [Anonymous], 2006, ADV NEURAL INF PROCE
   [Anonymous], 2007, MIR
   [Anonymous], 2000, P 15 IEEE INT PARALL
   [Anonymous], 2015, CORR
   [Anonymous], 2012, IMPROVING NEURAL NET
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], 2014, ACMMM
   [Anonymous], 2014, P IEEE C COMPUTER VI
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bo L., 2013, EXPT ROBOTICS VOLUME, P387, DOI DOI 10.1007/978-3-319-00065-7
   Bo LF, 2011, IEEE INT C INT ROBOT, P821, DOI 10.1109/IROS.2011.6048717
   Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963
   Browatzki B., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1189, DOI 10.1109/ICCVW.2011.6130385
   Bu SH, 2014, IEEE T MULTIMEDIA, V16, P2154, DOI 10.1109/TMM.2014.2351788
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Gao SH, 2010, LECT NOTES COMPUT SC, V6314, P1
   Goldberger J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P487
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Janoch A, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Jin L, 2014, 2014 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P311, DOI 10.1109/ISM.2014.56
   Kavukcuoglu K., 2010, CoRR
   Lawrence S, 1997, IEEE T NEURAL NETWOR, V8, P98, DOI 10.1109/72.554195
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li ZC, 2015, IEEE T PATTERN ANAL, V37, P2085, DOI 10.1109/TPAMI.2015.2400461
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mao QR, 2014, IEEE T MULTIMEDIA, V16, P2203, DOI 10.1109/TMM.2014.2360798
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Qi GJ, 2012, P IEEE, V100, P2688, DOI 10.1109/JPROC.2012.2201909
   Qi GJ, 2012, IEEE T PATTERN ANAL, V34, P850, DOI 10.1109/TPAMI.2011.191
   RIDLEY BK, 1987, SEMICOND SCI TECH, V2, P116, DOI 10.1088/0268-1242/2/2/009
   Wan L, 2013, P 30 INT C MACH LEAR, P1058, DOI DOI 10.5555/3042817.3043055
   Wang AR, 2014, LECT NOTES COMPUT SC, V8693, P453, DOI 10.1007/978-3-319-10602-1_30
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/iccv.2009.5459207
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Yosinski J, 2014, ADV NEUR IN, V27
   Zhang L, 2012, IEEE T SIGNAL PROCES, V60, P1684, DOI 10.1109/TSP.2011.2179539
NR 46
TC 45
Z9 45
U1 1
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2015
VL 17
IS 11
SI SI
BP 1899
EP 1908
DI 10.1109/TMM.2015.2476660
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CV2QV
UT WOS:000364102400004
DA 2024-07-18
ER

PT J
AU Yin, YF
   Yu, Y
   Zimmermann, R
AF Yin, Yifang
   Yu, Yi
   Zimmermann, Roger
TI On Generating Content-Oriented Geo Features for Sensor-Rich Outdoor
   Video Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature fusion; geographic coverage; map segmentation; semantic
   annotation; video search
ID SIMILARITY MEASUREMENT; REPRESENTATION; RELEVANCE
AB Advanced technologies in consumer electronics products have enabled individual users to record, share, and view videos on mobile devices. With the volume of videos increasing tremendously on the Internet, fast and accurate video search has attracted much research attention. A good similarity measure is a key component in a video retrieval system. Most of the existing solutions only rely on either the low-level visual features or the surrounding textual annotations. Those approaches often suffer from low recall as they are highly susceptible to changes in viewpoint, illumination, and noisy tags. By leveraging geo-metadata, more reliable and precise search results can be obtained. However, two issues remain challenging: (1) how to quantify the spatial relevance of videos with the visual similarity to generate a pertinent ranking of results according to users' needs, and (2) how to design a compact video representation that supports efficient indexing for fast video retrieval. In this study, we propose a novel video description which consists of (a) determining the geographic coverage of a video based on the camera's field-of-view and a pre-constructed geo-codebook, and (b) fusing video spatial relevance and region-aware visual similarities to achieve a robust video similarity measure. Toward a better encoding of a video's geo-coverage, we construct a geo-codebook by semantically segmenting a map into a collection of coherent regions. To evaluate the proposed technique we developed a video retrieval prototype. Experiments show that our proposed method improves the mean average precision by 4.6% similar to 10.5%, compared with existing approaches.
C1 [Yin, Yifang; Zimmermann, Roger] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
   [Yu, Yi] Natl Inst Informat, Digital Content & Media Sci Res Div, Tokyo 1018430, Japan.
C3 National University of Singapore; Research Organization of Information &
   Systems (ROIS); National Institute of Informatics (NII) - Japan
RP Yin, YF (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
EM yifang@comp.nus.edu.sg; yiyu@nii.ac.jp; rogerz@comp.nus.edu.sg
RI Zimmermann, Roger/D-7944-2015
OI Zimmermann, Roger/0000-0002-7410-2590; yin, yifang/0000-0002-6525-6133
FU National Natural Science Foundation of China [61472266]; National
   University of Singapore (Suzhou) Research Institute
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61472266, and by the National University
   of Singapore (Suzhou) Research Institute. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Chia-Wen Lin.
CR [Anonymous], P IEEE INT C MULT EX
   [Anonymous], P ACM MULT 2012 WORK
   [Anonymous], 2009, P 18 INT C WORLD WID
   [Anonymous], 2007, PROC INT C MULTIMEDI, DOI DOI 10.1145/1291233.1291447
   [Anonymous], P 4 BCS IRSG C FUT D
   [Anonymous], 2009, ACM INT C IMAGE VIDE
   Ay S., 2008, Proceedings of the 16th ACM international conference on Multimedia (MM '08), P309
   Ay SA, 2010, MULTIMEDIA SYST, V16, P105, DOI 10.1007/s00530-009-0177-x
   Ballan L., 2010, Proceedings 79 of Second ACM SIGMM Workshop on Social Media, P3
   Ballan L, 2013, INT WORK CONTENT MUL, P229, DOI 10.1109/CBMI.2013.6576588
   Ballatore A, 2013, KNOWL INF SYST, V37, P61, DOI 10.1007/s10115-012-0571-0
   Campbell M., 2006, P TRECVID 2006
   Cao Liangliang., 2012, Proceedings of the 20th ACM International Conference on Multimedia, P299
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cheung SCS, 2003, IEEE T CIRC SYST VID, V13, P59, DOI 10.1109/TCSVT.2002.808080
   Cheung SCS, 2000, IEEE IMAGE PROC, P85, DOI 10.1109/ICIP.2000.900898
   De Choudhury M., 2010, Proceedings of the 21st ACM Conference on Hypertext and Hypermedia-HT'10, P35, DOI DOI 10.1145/1810617.1810626
   Hauff C, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P1037
   Hauptmann A.G., 2004, PROC 12 ANN ACM INT, P668
   Hsu WinstonH., 2007, ACM MM
   Intagorn S., 2011, Proceedings of the 19th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, P425
   Jain R., 2010, INT C MULTIMEDIA, P1259, DOI DOI 10.1145/1873951.1874199
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kennedy L.S., 2008, P ACM INT C WORLD WI, P297
   Kim Y., 2012, P ACM INT C KNOWL DI, P1540
   Li LJ, 2014, INT J COMPUT VISION, V107, P20, DOI 10.1007/s11263-013-0660-x
   Li X., 2012, P 2 ACM INT C MULT R, P4
   Li XR, 2009, IEEE T MULTIMEDIA, V11, P1310, DOI 10.1109/TMM.2009.2030598
   Liu Jingjing., 2007, MULTIMEDIA 07, P208
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Newman MEJ, 2004, PHYS REV E, V70, DOI 10.1103/PhysRevE.70.056131
   O'Hare Neil, 2005, 2nd European Workshop on the Integration of Knowledge, Semantics and Digital Media Technology (EWIMT 2005), P323, DOI 10.1049/ic.2005.0750
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Penatti O. A. B., 2012, P ACM ICMR, P53
   Seo HJ, 2009, J VISION, V9, DOI 10.1167/9.12.15
   Shen H.T., 2005, P ACM SIGMOD INT C M, P730
   Shen Z., 2011, MM, P93
   Silva T.H., 2013, Proceedings of the 2Nd ACM SIGKDD International Workshop on Urban Computing, p4:1, DOI DOI 10.1145/2505821.2505836
   STRICKER M, 1995, P SOC PHOTO-OPT INS, V2410, P381, DOI 10.1117/12.205308
   Thomee B, 2013, P 22 INT C WORLD WID, P1285
   Tian X., 2008, ACM INT C MULTIMEDIA, P131, DOI DOI 10.1145/1459359.1459378.ISBN
   Van Leuken Reinier H, 2009, P 18 INT C WORLD WID, P341, DOI 10.1145/1526709.1526756
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Yin Y., 2015, ACM T MULTIM COMPUT, V11, P29
   Yin YF, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700287
   Yuan J, 2011, P 19 ACM INT C MULT, P453
   Zhang B., 2010, P 18 SIGSPATIAL INT, P260
   Zhou DY, 2004, ADV NEUR IN, V16, P321
NR 48
TC 10
Z9 11
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2015
VL 17
IS 10
BP 1760
EP 1772
DI 10.1109/TMM.2015.2458042
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CR9OD
UT WOS:000361685400007
DA 2024-07-18
ER

PT J
AU Pang, JB
   Jia, F
   Zhang, CJ
   Zhang, WG
   Huang, QM
   Yin, BC
AF Pang, Junbiao
   Jia, Fei
   Zhang, Chunjie
   Zhang, Weigang
   Huang, Qingming
   Yin, Baocai
TI Unsupervised Web Topic Detection Using A Ranked Clustering-Like Pattern
   Across Similarity Cascades
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Maximal clique; Poisson deconvolution; similarity cascade (SC);
   unsupervised ranking; web topic detection
ID CLIQUES; TIME
AB Despite the massive growth of social media on the Internet, the process of organizing, understanding, and monitoring user generated content (UGC) has become one of the most pressing problems in today's society. Discovering topics on the web from a huge volume of UGC is one of the promising approaches to achieve this goal. Compared with classical topic detection and tracking in news articles, identifying topics on the web is by no means easy due to the noisy, sparse, and less-constrained data on the Internet. In this paper, we investigate methods from the perspective of similarity diffusion, and propose a clustering-like pattern across similarity cascades (SCs). SCs are a series of subgraphs generated by truncating a similarity graph with a set of thresholds, and then maximal cliques are used to capture topics. Finally, a topic-restricted similarity diffusion process is proposed to efficiently identify real topics from a large number of candidates. Experiments demonstrate that our approach outperforms the state-of-the-art methods on three public data sets.
C1 [Pang, Junbiao; Huang, Qingming; Yin, Baocai] Beijing Univ Technol, Coll Metropolitan Transportat, Beijing Key Lab Multimedia, Beijing 100124, Peoples R China.
   [Pang, Junbiao; Huang, Qingming; Yin, Baocai] Beijing Univ Technol, Coll Metropolitan Transportat, Intelligent Software Technol, Beijing 100124, Peoples R China.
   [Jia, Fei; Zhang, Chunjie] Univ Chinese Acad Sci, Sch Comp & Control Engn, Beijing 100049, Peoples R China.
   [Zhang, Weigang] Harbin Inst Technol Weihai, Sch Comp Sci & Technol, Weihai 264209, Peoples R China.
   [Huang, Qingming] Chinese Acad Sci, Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Huang, Qingming] Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
C3 Beijing University of Technology; Beijing University of Technology;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Harbin Institute of Technology; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS; Chinese Academy of
   Sciences; Institute of Computing Technology, CAS
RP Pang, JB (corresponding author), Beijing Univ Technol, Coll Metropolitan Transportat, Beijing Key Lab Multimedia, Beijing 100124, Peoples R China.
EM junbiao_pang@bjut.edu.cn; jiafei@jdl.ac.cn; cjzhang@jdl.ac.cn;
   wgzhang@jdl.ac.cn; qmhuang@jdl.ac.cn; ybc@bjut.edu.cn
RI Zhang, Weigang/GZA-9095-2022; zhang, chunjie/Z-3035-2019
OI zhang, chunjie/0000-0002-1161-8995; Zhang, Weigang/0000-0003-0042-7074;
   pang, Junbiao/0000-0001-8153-7229
FU National Basic Research Program of China (973 Program) [2012CB316400];
   Natural Science Foundation of China [61332016, 61202234, 61202322,
   61303154, 61390510, 61472387]; Beijing Natural Science Foundation
   [4132010, KZ201310005006]; Funding Project for Academic Human Resources
   Development in Institutions of Higher Learning Under the Jurisdiction of
   Beijing Municipality (PHR)
FX This work was supported in part by National Basic Research Program of
   China (973 Program) under Grant 2012CB316400, in part by Natural Science
   Foundation of China under Grant 61332016, Grant 61202234, Grant
   61202322, Grant 61303154, Grant 61390510, and Grant 61472387, by the
   Beijing Natural Science Foundation under Grant 4132010 and Grant
   KZ201310005006, and by the Funding Project for Academic Human Resources
   Development in Institutions of Higher Learning Under the Jurisdiction of
   Beijing Municipality (PHR). The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Jing-Ming
   Guo.
CR Allan J., 2000, TOPIC DETECTION TRAC
   Allan J., 1998, P DARPA BROADC NEWS
   [Anonymous], 2010, P 16 ACM SIGKDD INT, DOI DOI 10.1145/1835804.1835884
   [Anonymous], 2007, P 15 ACM INT C MULT
   [Anonymous], 2003, P 26 ANN INT ACM SIG, DOI DOI 10.1145/860435.860495
   [Anonymous], 2012, P 2012 IEEE PES INNO, DOI DOI 10.1109/ISGT.2012.6175713
   [Anonymous], P MEDIAEVAL 2012 WOR
   Banerjee A, 2007, PROCEEDINGS OF THE SEVENTH SIAM INTERNATIONAL CONFERENCE ON DATA MINING, P431
   Blei DM, 2007, ANN APPL STAT, V1, P17, DOI 10.1214/07-AOAS114
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   BRON C, 1973, COMMUN ACM, V16, P575, DOI 10.1145/362342.362367
   Cao J, 2011, IEEE T CIRC SYST VID, V21, P1835, DOI 10.1109/TCSVT.2011.2148470
   Chen T., 2012, P 20 ACM INT C MULT, P781
   diaeresis>tze Hinrich Schu<spacing, 2008, INTRO INFORM RETRIEV, V39
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Gevers T, 2004, IEEE T PATTERN ANAL, V26, P113, DOI 10.1109/TPAMI.2004.1261083
   Han Bo., 2012, Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning, P421
   He Q, 2010, IEEE T PATTERN ANAL, V32, P1795, DOI 10.1109/TPAMI.2009.203
   Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649
   Hu Xia., 2009, Proceedings of the 18th ACM Conference on Information and Knowledge Management, Hong Kong, China, P919
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Liu H., 2010, Proceedings of the 27th International Conference on Machine Learning (ICML-10), P671
   Liu Y, 2009, PROCEEDINGS OF INTERNATIONAL FORUM ON TECHNOLOGICAL INNOVATION AND COMPETITIVE TECHNICAL INTELLIGENCE 2008, P338, DOI 10.1109/ICNC.2009.485
   LUCY LB, 1974, ASTRON J, V79, P745, DOI 10.1086/111605
   Aiello LM, 2013, IEEE T MULTIMEDIA, V15, P1268, DOI 10.1109/TMM.2013.2265080
   Newman D, 2011, Advances in Neural Information Processing Systems, P496, DOI DOI 10.5555/2986459.2986515
   Pan C.Mitra., 2011, Proceedings of the 11th annual international ACM/IEEE joint conference on Digital libraries, JCDL '11, P349
   Papadopoulos S, 2011, IEEE MULTIMEDIA, V18, P52, DOI 10.1109/MMUL.2010.68
   PUTTHIVIDHYA D, 2010, PROC CVPR IEEE, P3408, DOI DOI 10.1109/CVPR.2010.5540000
   Qi He, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P207
   RICHARDSON WH, 1972, J OPT SOC AM, V62, P55, DOI 10.1364/JOSA.62.000055
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Shepp L A, 1982, IEEE Trans Med Imaging, V1, P113, DOI 10.1109/TMI.1982.4307558
   Strehl A, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P93, DOI 10.1162/153244303321897735
   Sun AX, 2011, IEEE T SYST MAN CY A, V41, P834, DOI 10.1109/TSMCA.2011.2157129
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Thomas T. M.CoverandJ. A., 1991, ELEMENTS INFORM THEO, DOI 10.1002/0471200611
   Tie-Yan Liu, 2009, Foundations and Trends in Information Retrieval, V3, P225, DOI 10.1561/1500000016
   Tomita E, 2006, THEOR COMPUT SCI, V363, P28, DOI 10.1016/j.tcs.2006.06.015
   Vardi Y., 1996, NONLINEAR PROGRAMMIN
   Varma M., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P1065, DOI DOI 10.1145/1553374.1553510
   Wang C., 2008, Proceedings of the 17th International Conference on World Wide Web, WWW'08, P457, DOI DOI 10.1145/1367497.1367560.
   Wu X, 2007, P INT C ACM MULT, P168
   Xu W., 2003, P 26 ANN INT ACM SIG, P267
   Yang Y., 1998, P 21 ANN INT ACM SIG, P28, DOI DOI 10.1145/290941.290953
   Zhang YD, 2013, MATH PROBL ENG, V2013, DOI 10.1155/2013/705238
   Zhao WNX, 2011, LECT NOTES COMPUT SC, V6611, P338, DOI 10.1007/978-3-642-20161-5_34
NR 47
TC 20
Z9 21
U1 1
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2015
VL 17
IS 6
BP 843
EP 853
DI 10.1109/TMM.2015.2425143
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CI1TM
UT WOS:000354527500007
DA 2024-07-18
ER

PT J
AU Gottlieb, L
   Friedland, G
   Choi, J
   Kelm, P
   Sikora, T
AF Gottlieb, Luke
   Friedland, Gerald
   Choi, Jaeyoung
   Kelm, Pascal
   Sikora, Thomas
TI Creating Experts From the Crowd: Techniques for Finding Workers for
   Difficult Tasks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Annotation; cheat detection; crowdsourcing; mechanical turk; multimodal
   location estimation; qualification
AB Crowdsourcing is currently used for a range of applications, either by exploiting unsolicited user-generated content, such as spontaneously annotated images, or by utilizing explicit crowdsourcing platforms such as Amazon Mechanical Turk to mass-outsource artificial-intelligence-type jobs. However, crowdsourcing is most often seen as the best option for tasks that do not require more of people than their uneducated intuition as a human being. This article describes our methods for identifying workers for crowdsourced tasks that are difficult for both machines and humans. It discusses the challenges we encountered in qualifying annotators and the steps we took to select the individuals most likely to do well at these tasks.
C1 [Gottlieb, Luke; Friedland, Gerald; Choi, Jaeyoung] Int Comp Sci Inst, Audio & Multimedia Res Direct, Berkeley, CA 94704 USA.
   [Kelm, Pascal; Sikora, Thomas] Univ Technol, Berlin, Germany.
C3 Technical University of Berlin
RP Gottlieb, L (corresponding author), Int Comp Sci Inst, Audio & Multimedia Res Direct, Berkeley, CA 94704 USA.
EM luke@icsi.berkeley.edu; fractor@icsi.berkeley.edu;
   jaeyoung@icsi.berkeley.edu
FU NSF EAGER [IIS-1128599]; KFAS
FX This work was supported in part by NSF EAGER under Grant IIS-1128599 and
   KFAS under a Doctoral Study Abroad Fellowship. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Sheng-Wei (Kuan-Ta) Chen.
CR [Anonymous], 2011, P 24 ANN ACM S USER, DOI DOI 10.1145/2047196.2047202
   [Anonymous], TR11001 ICSI
   [Anonymous], 2013, P 21 ACM INT C MULT
   [Anonymous], 2011, P HUM FACT COMP SYST
   [Anonymous], P ACM MULT WORKSH CR
   [Anonymous], P MEDIAEVAL
   [Anonymous], P MEDIAEVAL 2010 WOR
   Doan A, 2011, COMMUN ACM, V54, P86, DOI 10.1145/1924421.1924442
   Downs JS, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2399
   Hays J, 2008, PROC CVPR IEEE, P3436
   Ipeirotis Panagiotis G., 2010, XRDS: Crossroads, V17, P16, DOI [10.1145/1869086.1869094, DOI 10.1145/1869086.1869094]
   Jaeyoung Choi, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P43, DOI 10.1109/ICME.2012.141
   Karger David R., 2011, 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P284
   Kittur A, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P453
   Little G., 2011, Human ocr: Insights from a complex human computation process
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
NR 16
TC 5
Z9 5
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 2075
EP 2079
DI 10.1109/TMM.2014.2347268
PG 5
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300023
DA 2024-07-18
ER

PT J
AU Niu, GL
   Fan, XG
   Li, VOK
   Long, Y
   Xu, K
AF Niu, Guolin
   Fan, Xiaoguang
   Li, Victor O. K.
   Long, Yi
   Xu, Kuang
TI Multi-Source-Driven Asynchronous Diffusion Model for Video-Sharing in
   Online Social Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Asynchronous diffusion process; exponential mixture model; measurement;
   online social network
ID INFORMATION DIFFUSION
AB Characterizing the video diffusion in online social networks (OSNs) is not only instructive for network traffic engineering, but also provides insights into the information diffusion process. A number of continuous-time diffusion models have been proposed to describe video diffusion under the assumption that the activation latency along social links follows a single parametric distribution. However, such assumption has not been empirically verified. Moreover, a user usually has multiple activated neighbors with different activation times, and it is hard to distinguish the different contributions of these multiple potential sources. To fill this gap, we study the multiple-source-driven asynchronous information diffusion problem based on substantial video diffusion traces. Specifically, we first investigate the latency of information propagation along social links and define the single-source (SS) activation latency for an OSN user. We find that the SS activation latency follows the exponential mixture model. Then we develop an analytical framework which incorporates the temporal factor and the influence of multiple sources to describe the influence propagation process. We show that one's activation probability decreases exponentially with time. We also show that the time shift of the exponential function is only determined by the most recent source (MRS) active user, but the total activation probability is the combination of influence exerted by all active neighbors. Based on these discoveries, we develop a multi-source-driven asynchronous diffusion model (MADM). Using maximum likelihood techniques, we develop an algorithm based on expectation maximization (EM) to learn model parameters, and validate our proposed model with real data. The experimental results show that the MADM obtains better prediction accuracy under various evaluation metrics.
C1 [Niu, Guolin; Fan, Xiaoguang; Li, Victor O. K.; Long, Yi; Xu, Kuang] Univ Hong Kong, Dept Elect & Elect Engn, Hong Kong, Hong Kong, Peoples R China.
C3 University of Hong Kong
RP Niu, GL (corresponding author), Univ Hong Kong, Dept Elect & Elect Engn, Hong Kong, Hong Kong, Peoples R China.
EM glniu@eee.hku.hk; xgfan@eee.hku.hk; vli@eee.hku.hk; yilong@eee.hku.hk;
   xukuang@eee.hku.hk
RI Fan, xiao/AEF-7654-2022; Li, Victor On Kwok/C-1858-2009
OI Fan, xiao/0000-0001-5147-6701; 
CR [Anonymous], P 2012 IEEE 20 INT W
   [Anonymous], 2010, IMC 2010 P
   [Anonymous], IMC07 P 2007 ACM
   [Anonymous], P 2012 IEEE GLOBECOM
   [Anonymous], P 2010 2 INT C COMM
   Bakshy E., 2011, P 4 ACM INT C WEB SE, P65
   Barabási AL, 2005, NATURE, V435, P207, DOI 10.1038/nature03459
   BASS FM, 1969, MANAGE SCI, V15, P215, DOI 10.1287/mnsc.15.5.215
   Benevenuto F, 2009, IMC'09: PROCEEDINGS OF THE 2009 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P49
   Biernacki C, 2003, COMPUT STAT DATA AN, V41, P561, DOI 10.1016/S0167-9473(02)00163-9
   Cha M, 2009, WWW 09 P 18 INT WORL, DOI DOI 10.1145/1526709.1526806
   Cheng X, 2008, INT WORKSH QUAL SERV, P249
   Cho E., 2011, P 17 ACM SIGKDD INT, P1082, DOI DOI 10.1145/2020408.2020579
   De Choudhury Munmun., 2010, AAAI Conference on Weblogs and Social Media, P34
   Du N., 2012, ADV NEURAL INFORM PR, P2780
   Gill P, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P15
   Goldenberg J, 2001, MARKET LETT, V12, P211, DOI 10.1023/A:1011122126881
   Gomez Rodriguez M., 2010, SIGKDD, P1019
   Gomez-Rodriguez M., 2011, P 28 INT C MACH LEAR, P561, DOI DOI 10.5555/3104482.3104553
   GomezRodriguez M., 2013, P 6 ACM INT C WEB SE, P23, DOI DOI 10.1145/2433396.2433402
   Goyal A., 2010, P 3 ACM INT C WEB SE, P241, DOI DOI 10.1145/1718487.1718518
   Goyal A, 2011, PROC VLDB ENDOW, V5, P73, DOI 10.14778/2047485.2047492
   GRANOVETTER M, 1978, AM J SOCIOL, V83, P1420, DOI 10.1086/226707
   Gruhl D, 2004, INFORM DIFFUSION BLO, P491, DOI DOI 10.1145/988672.988739
   Guille A, 2013, SIGMOD REC, V42, P17
   HASSELBL.V, 1969, J AM STAT ASSOC, V64, P1459, DOI 10.2307/2286083
   Kempe D., 2005, Automata, Languages and Programming. 32nd International Colloquium, ICALP 2005. Proceedings (Lecture Notes in Computer Science Vol. 3580), P1127, DOI 10.1007/11523468_91
   Kempe D, 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Kiesling E, 2012, CENT EUR J OPER RES, V20, P183, DOI [10.1007/s10100-011-0210-y, DOI 10.1007/s10100-011-0210-y]
   Kimura M, 2010, DATA MIN KNOWL DISC, V20, P70, DOI 10.1007/s10618-009-0150-5
   Labovitz C, 2010, ACM SIGCOMM COMP COM, V40, P75, DOI 10.1145/1851275.1851194
   Leskovec J., 2007, ACM Transactions on the Web, V1
   Li Haitao., 2012, P 22 INT WORKSHOP NE, P83
   McLachlan G., 2004, FINITE MIXTURE MODEL
   Myers S, 2010, Advances in neural information processing systems, V23
   Myers S. A., 2012, P 18 ACM SIGKDD INT, P33, DOI [DOI 10.1145/2339530.2339540, 10.1145/956750.956769]
   Romero Daniel M, 2011, P 20 INT C WORLD WID, P695, DOI [10.1145/1963405.1963503, DOI 10.1145/1963405.1963503]
   Saito K, 2009, LECT NOTES ARTIF INT, V5828, P322, DOI 10.1007/978-3-642-05224-8_25
   Sun E., 2009, Proc AAAI Intl Conf on Weblogs and Social Media, P146
   Wallinga J, 2004, AM J EPIDEMIOL, V160, P509, DOI 10.1093/aje/kwh255
   Wang Z, 2013, IEEE T MULTIMEDIA, V15, P698, DOI 10.1109/TMM.2012.2237022
   Wilson C, 2009, EUROSYS'09: PROCEEDINGS OF THE FOURTH EUROSYS CONFERENCE, P205
   WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060
   Xie L, 2011, P 19 ACM INT C MULT, P53, DOI DOI 10.1145/2072298.2072307
   Zhao ZD, 2011, CHINESE PHYS LETT, V28, DOI 10.1088/0256-307X/28/6/068901
   Zhou Renjie, 2010, P 10 ACM SIGCOMM C I, P404, DOI DOI 10.1145/1879141.1879193
NR 46
TC 19
Z9 20
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 2025
EP 2037
DI 10.1109/TMM.2014.2340133
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300019
DA 2024-07-18
ER

PT J
AU Xu, Z
   Zhang, Y
   Cao, LB
AF Xu, Zhe
   Zhang, Ya
   Cao, Longbing
TI Social Image Analysis From a Non-IID Perspective
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Not independent and identically distributed (non-IID) learning;
   similarity metric; social media; structure mining
ID ALGORITHM
AB An image in social media, termed a social image, exhibits characteristics different from images widely discussed in image processing. They can be described by both content and social related attributes, called social image attributes, including visual contents, users, tags, and timestamps. There are strong coupling relationships between social image attributes, which make social images not independent and identically distributed (non-IID). By analyzing the relationships among these attributes, we can better understand the semantic activities conducted on such non-IID social images, hence enabling new applications including content organization, recommendation, and social activity understanding. In this article, we present a novel algorithm to analyze the coupling relationships between social images, which involves not only intra-coupled similarity within a social image attribute, but also inter-coupled similarity between attributes, in analyzing the non-IIDness of the similarity between social images. In particular, we propose a multi-entry version of the coupled similarity metric to deal with attributes (i.e., tags) which have a many-to-one relationship with respect to images. Experimental results on a Flickr group dataset show that the proposed algorithm captures coupling relationships and therefore achieves promising results in various applications, including image clustering and tagging.
C1 [Xu, Zhe; Zhang, Ya] Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai 200240, Peoples R China.
   [Xu, Zhe; Zhang, Ya] Shanghai Jiao Tong Univ, Shanghai Key Lab Multimedia Proc & Transmiss, Shanghai 200240, Peoples R China.
   [Cao, Longbing] Univ Technol Sydney, Adv Analyt Inst, Sydney, NSW 2007, Australia.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; University
   of Technology Sydney
RP Zhang, Y (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai 200240, Peoples R China.
EM xz3030@sjtu.edu.cn; ya_zhang@sjtu.edu.cn; LongBing.Cao@uts.edu.au
RI Zhang, Ya/Y-8255-2019
OI Zhang, Ya/0000-0002-5390-9053; cao, longbing/0000-0003-1562-9429
FU High Technology Research and Development Program of China
   [2012AA011702]; National Natural Science Foundation of China [61003107]
FX This work was supported in part by the High Technology Research and
   Development Program of China under Grant 2012AA011702 and by the
   National Natural Science Foundation of China under Grant 61003107. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. K. Selcuk Candan. (Corresponding
   author: Ya Zhang.)
CR [Anonymous], 2006, Book Annosearch: Image auto-annotation by search, DOI DOI 10.1109/CVPR.2006.58
   [Anonymous], 2011, PROC 20 ACM C INFORM
   Blei D.M., 2006, P 23 INT C MACHINE L, P113, DOI [10.1145/1143844.1143859, DOI 10.1145/1143844.1143859, 10.1145/1143844.114385]
   Cao LB, 2014, COMPUT J, V57, P1358, DOI 10.1093/comjnl/bxt084
   Cao LB, 2012, IEEE T KNOWL DATA EN, V24, P1378, DOI 10.1109/TKDE.2011.129
   COST S, 1993, MACH LEARN, V10, P57, DOI 10.1007/BF00993481
   Gan G., 2007, SER ASA SIAM SERIES, V20
   Huang ZX, 1998, DATA MIN KNOWL DISC, V2, P283, DOI 10.1023/A:1009769707641
   Li FF, 2013, LECT NOTES COMPUT SC, V8180, P189, DOI 10.1007/978-3-642-41230-1_16
   Lin YR, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2071396.2071400
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Negoescu R.A., 2008, Proc. Content-based Image and Video Retrieval, P417, DOI DOI 10.1145/1386352.1386406
   Rui X., 2007, Proc. 15th ACM intl. conf. on multimedia, P585, DOI DOI 10.1145/1291233.1291378
   Sivic J, 2009, IEEE T PATTERN ANAL, V31, P591, DOI 10.1109/TPAMI.2008.111
   Smadja F, 1996, COMPUT LINGUIST, V22, P1
   Wang C., 2008, ACM SIGIR, P355, DOI DOI 10.1145/1390334.1390396
   WANG L, 1990, PATTERN RECOGN, V23, P905, DOI 10.1016/0031-3203(90)90135-8
   Wu F, 2010, J COMPUT SCI TECH-CH, V25, P750, DOI [10.1007/s11390-010-9362-9, 10.1007/s11390-010-1058-7]
   WU ZB, 1994, 32ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P133
   Yonghong Yu, 2013, Advances in Knowledge Discovery and Data Mining. 17th Pacific-Asia Conference (PAKDD 2013). Proceedings, P365, DOI 10.1007/978-3-642-37456-2_31
NR 21
TC 18
Z9 19
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 1986
EP 1998
DI 10.1109/TMM.2014.2342658
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300016
OA Green Published
DA 2024-07-18
ER

PT J
AU Yang, Y
   Zha, ZJ
   Gao, Y
   Zhu, XF
   Chua, TS
AF Yang, Yang
   Zha, Zheng-Jun
   Gao, Yue
   Zhu, Xiaofeng
   Chua, Tat-Seng
TI Exploiting Web Images for Semantic Video Indexing Via Robust
   Sample-Specific Loss
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Robust; semantic video indexing; transfer learning
ID RELEVANCE; SCALE
AB Semantic video indexing, also known as video annotation or video concept detection in literatures, has been attracting significant attention in recent years. Due to deficiency of labeled training videos, most of the existing approaches can hardly achieve satisfactory performance. In this paper, we propose a novel semantic video indexing approach, which exploits the abundant user-tagged Web images to help learn robust semantic video indexing classifiers. The following two major challenges are well studied: 1) noisy Web images with imprecise and/or incomplete tags; and 2) domain difference between images and videos. Specifically, we first apply a non-parametric approach to estimate the probabilities of images being correctly tagged as confidence scores. We then develop a robust transfer video indexing (RTVI) model to learn reliable classifiers from a limited number of training videos together with the abundance of user-tagged images. The RTVI model is equipped with a novel sample-specific robust loss function, which employs the confidence score of a Web image as prior knowledge to suppress the influence and control the contribution of this image in the learning process. Meanwhile, the RTVI model discovers an optimal kernel space, in which the mismatch between images and videos is minimized for tackling the domain difference problem. Besides, we devise an iterative algorithm to effectively optimize the proposed RTVI model and a theoretical analysis on the convergence of the proposed algorithm is provided as well. Extensive experiments on various real-world multimedia collections demonstrate the effectiveness of the proposed robust semantic video indexing approach.
C1 [Yang, Yang] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Zha, Zheng-Jun] Chinese Acad Sci, Inst Intelligent Machines, Hefei 230031, Peoples R China.
   [Gao, Yue] Tsinghua Univ, Dept Automat, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
   [Zhu, Xiaofeng] Univ N Carolina, Biomed Res Imaging Ctr, Chapel Hill, NC 27599 USA.
   [Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
C3 University of Electronic Science & Technology of China; Chinese Academy
   of Sciences; Hefei Institutes of Physical Science, CAS; Tsinghua
   University; University of North Carolina; University of North Carolina
   Chapel Hill; National University of Singapore
RP Gao, Y (corresponding author), Tsinghua Univ, Dept Automat, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
EM kevin.gaoy@gmail.com
RI Zha, Zheng-Jun/AAF-8667-2020; Zha, Zheng-Jun/AAE-8408-2020; yang,
   yang/GVT-5210-2022; Zhu, Xiaofeng/F-3601-2016; yang, yang/HGT-7999-2022;
   Gao, Yue/B-3376-2012; Lang, Ming/HIK-0758-2022; Zhu,
   Xiaofeng/HII-5291-2022
OI Zha, Zheng-Jun/0000-0003-2510-8993; Zhu, Xiaofeng/0000-0001-6840-0578
FU Singapore National Research Foundation under International Research
   Centre at the Singapore Funding Initiative; Natural Science Foundation
   of China (NSFC) [61263035]
FX This work was supported by the Singapore National Research Foundation
   under its International Research Centre at the Singapore Funding
   Initiative and administered by the IDM Programme Office. The work of X.
   Zhu was supported in part by the Natural Science Foundation of China
   (NSFC) under Grant 61263035. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Cees
   G. M. Snoek.
CR [Anonymous], 2012, OUTLIER ANAL
   [Anonymous], 2011, 25 AAAI C ART INT
   [Anonymous], 2014, P INT C MULT RETR
   [Anonymous], INT J NEUROSCI
   [Anonymous], 2013, P 21 ACM INT C MULT
   [Anonymous], P KAIS
   [Anonymous], P INT WORKSH MULT IN
   [Anonymous], P MMM
   [Anonymous], 2010, P ACM MULTIMEDIA
   [Anonymous], P ACM INT C MULT CAL
   [Anonymous], P ACM MM
   Binder A, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0038897
   Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Daume III Hal, 2007, ACL 2007, P256
   Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114
   Duan LX, 2010, PROC CVPR IEEE, P1959, DOI 10.1109/CVPR.2010.5539870
   Duan LX, 2009, PROC CVPR IEEE, P1375, DOI [10.1109/CVPRW.2009.5206747, 10.1109/CVPR.2009.5206747]
   Gao Y, 2013, IEEE T IMAGE PROCESS, V22, P363, DOI 10.1109/TIP.2012.2202676
   Geng B, 2012, IEEE T MULTIMEDIA, V14, P55, DOI 10.1109/TMM.2011.2174781
   Gönen M, 2011, J MACH LEARN RES, V12, P2211
   Jiang W, 2008, IEEE IMAGE PROC, P161, DOI 10.1109/ICIP.2008.4711716
   Jiang Y.-G., 2009, Proc. ACM MM, P155, DOI DOI 10.1145/1631272.1631296
   Jiang YG, 2009, IEEE I CONF COMP VIS, P1420, DOI 10.1109/ICCV.2009.5459295
   Kloft M, 2011, J MACH LEARN RES, V12, P953
   Li LJ, 2010, INT J COMPUT VISION, V88, P147, DOI 10.1007/s11263-009-0265-6
   Li XR, 2009, IEEE T MULTIMEDIA, V11, P1310, DOI 10.1109/TMM.2009.2030598
   Liu Dong., 2009, P 18 INT C WORLD WID, P351
   Loui Alexander., 2007, MIR 07, P245
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Raina R., 2007, P 24 INT C MACH LEAR, P759
   Smeaton AF, 2009, SIGNALS COMMUN TECHN, P151, DOI 10.1007/978-0-387-76569-3_6
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Tang Jinhui., 2009, Proceedings of ACM international conference on Multimedia, P223, DOI DOI 10.1145/1631272.1631305
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Xu HX, 2009, PROCEEDINGS OF THE 2009 WRI GLOBAL CONGRESS ON INTELLIGENT SYSTEMS, VOL III, P573, DOI 10.1109/GCIS.2009.320
   Yang J., 2007, P 15 ACM INT C MULT, P188
   Yang Y, 2011, PROC CVPR IEEE, P881, DOI 10.1109/CVPR.2011.5995499
   Yang Y, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2457450.2457456
   Yang Y, 2013, PATTERN RECOGN, V46, P1358, DOI 10.1016/j.patcog.2012.10.026
   Zha ZJ, 2012, IEEE T MULTIMEDIA, V14, P17, DOI 10.1109/TMM.2011.2174782
   Zhang HW, 2012, PROC CVPR IEEE, P2464, DOI 10.1109/CVPR.2012.6247961
   Zhu SA, 2012, IEEE T MULTIMEDIA, V14, P1068, DOI 10.1109/TMM.2012.2190387
NR 45
TC 105
Z9 113
U1 2
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1677
EP 1689
DI 10.1109/TMM.2014.2323014
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200016
DA 2024-07-18
ER

PT J
AU Yang, ZJ
   Metallinou, A
   Narayanan, S
AF Yang, Zhaojun
   Metallinou, Angeliki
   Narayanan, Shrikanth
TI Analysis and Predictive Modeling of Body Language Behavior in Dyadic
   Interactions From Multimodal Interlocutor Cues
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Behavior coordination; body language; dyadic interactions; interaction
   goals; motion capture
ID HEAD MOTION; GESTURE; DRIVEN; RECOGNITION
AB During dyadic interactions, participants adjust their behavior and give feedback continuously in response to the behavior of their interlocutors and the interaction context. In this paper, we study how a participant in a dyadic interaction adapts his/her body language to the behavior of the interlocutor, given the interaction goals and context. We apply a variety of psychology-inspired body language features to describe body motion and posture. We first examine the coordination between the dyad's behavior for two interaction stances: friendly and conflictive. The analysis empirically reveals the dyad's behavior coordination, and helps identify informative interlocutor features with respect to the participant's target body language features. The coordination patterns between the dyad's behavior are found to depend on the interaction stances assumed. We apply a Gaussian-Mixture-Model-based (GMM) statistical mapping in combination with a Fisher kernel framework for automatically predicting the body language of an interacting participant from the speech and gesture behavior of an interlocutor. The experimental results show that the Fisher kernel-based approach outperforms methods using only the GMM-based mapping, and using the support vector regression, in terms of correlation coefficient and. These results suggest a significant level of predictability of body language behavior from interlocutor cues.
C1 [Yang, Zhaojun; Metallinou, Angeliki; Narayanan, Shrikanth] Univ So Calif, Dept Elect Engn, Los Angeles, CA 90089 USA.
C3 University of Southern California
RP Yang, ZJ (corresponding author), Univ So Calif, Dept Elect Engn, Los Angeles, CA 90089 USA.
EM zhaojuny@usc.edu; angeliki.met-allinou@pearson.com; shri@sipi.usc.edu
RI Yang, Yufei/JXX-6325-2024; Narayanan, Shrikanth S/D-5676-2012; Yang,
   yufei/KHW-2735-2024
FU NSF; DARPA; Direct For Computer & Info Scie & Enginr; Division of
   Computing and Communication Foundations [1029373] Funding Source:
   National Science Foundation; Direct For Computer & Info Scie & Enginr;
   Div Of Information & Intelligent Systems [0911009] Funding Source:
   National Science Foundation; Division Of Research On Learning; Direct
   For Education and Human Resources [1008372] Funding Source: National
   Science Foundation
FX This work was supported in part by NSF and DARPA. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Jiebo Luo.
CR Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746
   [Anonymous], 2011, P INTERSPEECH
   [Anonymous], 2005, NEW HDB METHODS NONV
   [Anonymous], P ICASSP
   Bach Francis R., 2005, PROBABILISTIC INTERP
   Bartlett MS, 1941, BIOMETRIKA, V32, P29
   Basu S., 2001, LEARNING HUMAN INTER
   Beck A, 2010, 2010 IEEE RO-MAN, P464, DOI 10.1109/ROMAN.2010.5598649
   Bernhardt D, 2007, LECT NOTES COMPUT SC, V4738, P59
   Bulut M., 2004, P 6 INT C MULT INT, P205
   Burgoon JudeeK., 2007, INTERPERSONAL ADAPTA
   Busso C, 2005, COMPUT ANIMAT VIRT W, V16, P283, DOI 10.1002/cav.80
   Busso C, 2007, IEEE T AUDIO SPEECH, V15, P1075, DOI 10.1109/TASL.2006.885910
   Carnicke SharonMarie., 2008, Stanislavsky in Focus: An Acting Master for the Twenty First Century
   Castellano G, 2007, LECT NOTES COMPUT SC, V4738, P71
   Chartrand TL, 1999, J PERS SOC PSYCHOL, V76, P893, DOI 10.1037/0022-3514.76.6.893
   Conty L, 2012, J NEUROSCI, V32, P4531, DOI 10.1523/JNEUROSCI.5636-11.2012
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Delaherche E, 2012, IEEE T AFFECT COMPUT, V3, P349, DOI 10.1109/T-AFFC.2012.12
   Ekman P., 1964, J ABNORMAL SOCIAL PS, V63
   Gunes H, 2007, J NETW COMPUT APPL, V30, P1334, DOI 10.1016/j.jnca.2006.09.007
   Heylen D, 2011, COGN TECHNOL, P321, DOI 10.1007/978-3-642-15184-2_17
   Jaakkola TS, 1999, ADV NEUR IN, V11, P487
   Kapur A, 2005, LECT NOTES COMPUT SC, V3784, P1
   KENDON A, 1970, ACTA PSYCHOL, V32, P100
   Lee C.C., 2009, P INTERSPEECH
   Lee C. C., 2012, COMPUTER SPEECH LANG
   Levine S, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778861
   Levine S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618518
   Mariooryad S, 2012, IEEE T AUDIO SPEECH, V20, P2329, DOI 10.1109/TASL.2012.2201476
   Metallinou A., 2010, P MULT CORP ADV CAPT
   Metallinou A, 2013, IMAGE VISION COMPUT, V31, P137, DOI 10.1016/j.imavis.2012.08.018
   Metallinou A, 2012, INT CONF ACOUST SPEE, P2401, DOI 10.1109/ICASSP.2012.6288399
   Morency LP, 2010, AUTON AGENT MULTI-AG, V20, P70, DOI 10.1007/s10458-009-9092-y
   Moreno PJ, 2000, INT CONF ACOUST SPEE, P2417, DOI 10.1109/ICASSP.2000.859329
   Neumann R, 2000, J PERS SOC PSYCHOL, V79, P211, DOI 10.1037//0022-3514.79.2.211
   Oikonomidis I, 2012, PROC CVPR IEEE, P1862, DOI 10.1109/CVPR.2012.6247885
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Perronnin F., 2007, P IEEE CVPR, P1
   Sargin ME, 2007, INT CONF ACOUST SPEE, P677
   Sheets-Johnstone M., 1999, Journal of Consciousness Studies, V6, P11
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Toda T, 2008, SPEECH COMMUN, V50, P215, DOI 10.1016/j.specom.2007.09.001
   Toda T, 2007, IEEE T AUDIO SPEECH, V15, P2222, DOI 10.1109/TASL.2007.907344
   Ververidis D, 2006, SPEECH COMMUN, V48, P1162, DOI 10.1016/j.specom.2006.04.003
   Wan V, 2005, IEEE T SPEECH AUDI P, V13, P203, DOI 10.1109/TSA.2004.841042
   Xiao B., 2013, P IEEE INT C MULT EX, P1
   Xiao B, 2013, INT CONF ACOUST SPEE, P3766, DOI 10.1109/ICASSP.2013.6638362
   Yang Z., 2013, P 2013 IEEE INT C AC
NR 49
TC 26
Z9 26
U1 1
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1766
EP 1778
DI 10.1109/TMM.2014.2328311
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200023
DA 2024-07-18
ER

PT J
AU Zoidi, O
   Tefas, A
   Nikolaidis, N
   Pitas, I
AF Zoidi, Olga
   Tefas, Anastasios
   Nikolaidis, Nikos
   Pitas, Ioannis
TI Person Identity Label Propagation in Stereo Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Locality preserving projections; label propagation; multiple graphs
ID PRESERVING PROJECTIONS; RECOGNITION
AB In this paper a novel method is introduced for propagating person identity labels on facial images extracted from stereo videos. It operates on image data with multiple representations and calculates a projection matrix that preserves locality information and a priori pairwise information, in the form of must-link and cannot-link constraints between the various data representations. The final data representation is a linear combination of the projections of all data representations. Moreover, the proposed method takes into account information obtained through data clustering. This information is exploited during the data propagation step in two ways: to regulate the similarity strength between the projected data and to indicate which samples should be selected for label propagation initialization. The performance of the proposed Multiple Locality Preserving Projections with Cluster-based Label Propagation (MLPP-CLP) method was evaluated on facial images extracted from stereo movies. Experimental results showed that the proposed method outperforms state of the art methods.
C1 [Zoidi, Olga; Tefas, Anastasios; Nikolaidis, Nikos; Pitas, Ioannis] Aristotle Univ Thessaloniki, Dept Informat, Thessaloniki 54124, Greece.
C3 Aristotle University of Thessaloniki
RP Zoidi, O (corresponding author), Aristotle Univ Thessaloniki, Dept Informat, Thessaloniki 54124, Greece.
EM ozoidi@aiia.csd.auth.gr; tefas@aiia.csd.auth.gr;
   nikolaid@aiia.csd.auth.gr; pitas@aiia.csd.auth.gr
RI Tefas, Anastasios/ABA-2328-2020; Nikolaidis, Nikos/F-1819-2010; Tefas,
   Anastasios/F-1899-2010
OI Nikolaidis, Nikos/0000-0003-1515-7986; Tefas,
   Anastasios/0000-0003-1288-3667
FU European Union Seventh Framework Programme (FP7) [287674]
FX This work was supported by the European Union Seventh Framework
   Programme (FP7/2007-2013) under grant agreement number 287674 (3DTVS).
   This publication reflects only the authors' views. The European Union is
   not liable for any use that may be made of the information contained
   therein. The associate editor coordinating the review of this manuscript
   and approving it for publication was Prof. Ebroul Izquierdo.
CR [Anonymous], 2012, MATRIX COMPUTATIONS
   [Anonymous], 2006, The Matrix Cookbook
   [Anonymous], P INT C COMP VIS ICC
   Argyriou A., 2005, Advances in Neural Information Processing Systems, P67
   Boyd S., 2004, CONVEX OPTIMIZATION
   Cevikalp H, 2008, VISAPP 2008: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P489
   Chapelle O., 2006, SEMISUPERVISED LEARN
   FREEMAN LC, 1977, SOCIOMETRY, V40, P35, DOI 10.2307/3033543
   He XF, 2004, ADV NEUR IN, V16, P153
   Huang G.B., 2008, PROC WORKSHOP FACES
   Kato T, 2009, IEEE T NEURAL NETWOR, V20, P35, DOI 10.1109/TNN.2008.2003354
   Liu JG, 2009, PROC CVPR IEEE, P1996
   Lu GF, 2010, PATTERN RECOGN, V43, P3572, DOI 10.1016/j.patcog.2010.04.007
   Lu JW, 2010, IEEE T SYST MAN CY B, V40, P958, DOI 10.1109/TSMCB.2009.2032926
   Pham PT, 2011, IEEE MULTIMEDIA, V18, P44, DOI 10.1109/MMUL.2011.22
   Qiao LS, 2010, PATTERN RECOGN, V43, P331, DOI 10.1016/j.patcog.2009.05.005
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399, DOI 10.1145/1101149.1101236
   Stamou G., 2005, VISUAL COMMUNICATION, p[59, 602C]
   Viola P., 2001, INT J COMPUT VISION
   Wang Fei, 2006, P 23 INT C MACH LEAR, P985
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Wolf L, 2011, IEEE T PATTERN ANAL, V33, P1978, DOI 10.1109/TPAMI.2010.230
   Yu WW, 2006, IMAGE VISION COMPUT, V24, P239, DOI 10.1016/j.imavis.2005.11.006
   Zhang LM, 2010, PATTERN RECOGN, V43, P1993, DOI 10.1016/j.patcog.2009.12.022
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhou SHK, 2004, IEEE T IMAGE PROCESS, V13, P1491, DOI 10.1109/TIP.2004.836152
   Zhu L, 2007, NEUROCOMPUTING, V70, P1543, DOI 10.1016/j.neucom.2006.12.004
   Zhu X., 2008, Semi-Supervised Learning Literature Survey
   Zoidi O., 2013, P IEEE S SERIES COMP
NR 31
TC 25
Z9 26
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1358
EP 1368
DI 10.1109/TMM.2014.2315595
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Su, Z
   Zeng, K
   Liu, L
   Li, B
   Luo, XN
AF Su, Zhuo
   Zeng, Kun
   Liu, Li
   Li, Bo
   Luo, Xiaonan
TI Corruptive Artifacts Suppression for Example-Based Color Transfer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Color transfer; computational photograph; edge-preserving smoothing;
   image manipulation
ID IMAGE; PHOTOGRAPHY; FLASH; TONE
AB Example-based color transfer is a critical operation in image editing but easily suffers from some corruptive artifacts in the mapping process. In this paper, we propose a novel unified color transfer framework with corruptive artifacts suppression, which performs iterative probabilistic color mapping with self-learning filtering scheme and multiscale detail manipulation scheme in minimizing the normalized Kullback-Leibler distance. First, an iterative probabilistic color mapping is applied to construct the mapping relationship between the reference and target images. Then, a self-learning filtering scheme is applied into the transfer process to prevent from artifacts and extract details. The transferred output and the extracted multi-levels details are integrated by the measurement minimization to yield the final result. Our framework achieves a sound grain suppression, color fidelity and detail appearance seamlessly. For demonstration, a series of objective and subjective measurements are used to evaluate the quality in color transfer. Finally, a few extended applications are implemented to show the applicability of this framework.
C1 [Su, Zhuo; Liu, Li; Luo, Xiaonan] Sun Yat Sen Univ, Shenzhen Digital Home Key Technol Engn Lab,Res In, State Prov Joint Lab Digital Home Interact Applic, Natl Engn Res Ctr Digital Life,Sch Informat Sci &, Shenzhen, Peoples R China.
   [Zeng, Kun] Sun Yat Sen Univ, Sch Software, Guangzhou 510275, Guangdong, Peoples R China.
   [Li, Bo] Nanchang Hangkong Univ, Sch Math & Informat Sci, Nanchang, Peoples R China.
C3 Sun Yat Sen University; Sun Yat Sen University; Nanchang Hangkong
   University
RP Su, Z (corresponding author), Sun Yat Sen Univ, Shenzhen Digital Home Key Technol Engn Lab,Res In, State Prov Joint Lab Digital Home Interact Applic, Natl Engn Res Ctr Digital Life,Sch Informat Sci &, Shenzhen, Peoples R China.
EM zengkun@gmail.com
RI Su, Zhuo/AAO-4506-2020; li, yifang/JVM-8060-2024
OI Su, Zhuo/0000-0002-6090-0110; 
FU National Natural Science Foundation of China [61320106008, 61103163,
   61262050]; NSFC-Guangdong Joint Fund [U1135003]; Ministry of Education
FX This work was supported in part by the National Natural Science
   Foundation of China (No. 61320106008), the NSFC-Guangdong Joint Fund
   (No. U1135003), the National Natural Science Foundation of China (No.
   61103163, 61262050), and the Scholarship Award for Excellent Doctoral
   Student granted by the Ministry of Education 2012. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Adrian Munteanu.
CR Abadpour A, 2007, J VIS COMMUN IMAGE R, V18, P15, DOI 10.1016/j.jvcir.2006.08.001
   [Anonymous], 2011, ACM T GRAPHIC, DOI DOI 10.1145/2010324.1964965
   [Anonymous], ACM T GRAPH
   Bae SM, 2006, ACM T GRAPHIC, V25, P637, DOI 10.1145/1141911.1141935
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Chang Y., 2005, ACM Trans. Appl. Perception, V2, P322
   Chang Y, 2007, IEEE T IMAGE PROCESS, V16, P329, DOI 10.1109/TIP.2006.888347
   Dong W., 2010, ACM SIGGRAPH ASIA 20
   Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778
   Farbman Z, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866171
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Gastal E. S. L., 2011, ACM TOG IN PRESS, V30
   Gastal ESL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185529
   Gonzalez R C, 2008, DIGITAL IMAGE PROCES
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Kass M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778837
   Liu F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899408
   Paris S., P COMP GRAPH VIS 200
   Paris S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964963
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Pitie A., 2007, 4 EUR C VIS MED PROD, DOI DOI 10.1049/CP:20070055
   Pitié F, 2005, IEEE I CONF COMP VIS, P1434
   Pitié F, 2007, COMPUT VIS IMAGE UND, V107, P123, DOI 10.1016/j.cviu.2006.11.011
   Pouli T., 2012, SIGGRAPH 12
   Pouli T, 2011, COMPUT GRAPH-UK, V35, P67, DOI 10.1016/j.cag.2010.11.003
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Su Z, 2013, VISUAL COMPUT, V29, P1011, DOI 10.1007/s00371-012-0753-5
   Su Z, 2013, IEEE T MULTIMEDIA, V15, P535, DOI 10.1109/TMM.2012.2237025
   Tai YW, 2007, IEEE T PATTERN ANAL, V29, P1520, DOI 10.1109/TPAMI.2007.1168
   Wang BY, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866172
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Wong BY, 2012, IEEE T MULTIMEDIA, V14, P760, DOI 10.1109/TMM.2012.2188997
   Wu F., 2011, ACM SIGGRAPH AS 2011
   Xiao CX, 2011, IEEE T VIS COMPUT GR, V17, P1135, DOI 10.1109/TVCG.2010.125
   Xiao XZ, 2009, COMPUT GRAPH FORUM, V28, P1879, DOI 10.1111/j.1467-8659.2009.01566.x
   Xu L, 2011, P SIGGRAPH AS 2011
   Zhu LF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366146
NR 37
TC 34
Z9 37
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2014
VL 16
IS 4
BP 988
EP 999
DI 10.1109/TMM.2014.2305914
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AJ8LN
UT WOS:000337955800009
OA Bronze
DA 2024-07-18
ER

PT J
AU Min, WQ
   Xu, CS
   Xu, M
   Xiao, X
   Bao, BK
AF Min, Weiqing
   Xu, Changsheng
   Xu, Min
   Xiao, Xian
   Bao, Bing-Kun
TI Mobile Landmark Search with 3D Models
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D reconstruction; 3D to 2D matching; content-based image retrieval;
   mobile landmark search
ID IMAGE; COLLECTIONS; RECOGNITION
AB Landmark search is crucial to improve the quality of travel experience. Smart phones make it possible to search landmarks anytime and anywhere. Most of the existing work computes image features on smart phones locally after taking a landmark image. Compared with sending original image to the remote server, sending computed features saves network bandwidth and consequently makes sending process fast. However, this scheme would be restricted by the limitations of phone battery power and computational ability. In this paper, we propose to send compressed (low resolution) images to remote server instead of computing image features locally for landmark recognition and search. To this end, a robust 3D model based method is proposed to recognize query images with corresponding landmarks. Using the proposed method, images with low resolution can be recognized accurately, even though images only contain a small part of the landmark or are taken under various conditions of lighting, zoom, occlusions and different viewpoints. In order to provide an attractive landmark search result, a 3D texture model is generated to respond to a landmark query. The proposed search approach, which opens up a new direction, starts from a 2D compressed image query input and ends with a 3D model search result.
C1 [Min, Weiqing; Xiao, Xian] Chinese Acad Sci, Natl Lab Pattern Recognit Inst Automat, Beijing 100190, Peoples R China.
   [Xu, Changsheng; Bao, Bing-Kun] China Singapore Inst Digital Media, Singapore 139951, Singapore.
   [Xu, Changsheng; Bao, Bing-Kun] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
   [Xu, Min] Univ Technol Sydney, Sch Comp & Commun, INEXT, Sydney, NSW 2007, Australia.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; Institute of Automation, CAS; University of
   Technology Sydney
RP Min, WQ (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit Inst Automat, Beijing 100190, Peoples R China.
EM wqmin@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn; Min.Xu@uts.edu.au;
   bkbao@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023
OI Xu, Min/0000-0001-9581-8849
FU National Basic Research Program of China [2012CB316304]; National
   Natural Science Foundation of China [61225009]; Beijing Natural Science
   Foundation [4131004]; Singapore National Research Foundation under its
   International Research Centre @ Singapore Funding Initiative
FX This work was supported in part by National Basic Research Program of
   China (No. 2012CB316304), National Natural Science Foundation of China
   (No. 61225009), and Beijing Natural Science Foundation (No. 4131004).
   This work is also supported by the Singapore National Research
   Foundation under its International Research Centre @ Singapore Funding
   Initiative and administered by the IDM Programme Office. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Tao Mei.
CR [Anonymous], 2009, P 18 INT C WORLD WID
   [Anonymous], P CVPR
   [Anonymous], P ACM SIGGRAPH
   [Anonymous], 2008, P 16 ACM INT C MULT, DOI DOI 10.1145/1459359.1459410
   Avrithis Yannis, 2010, P 18 ACM INT C MULTI, P153, DOI 10.1145/1873951.1873973Place
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Chandrasekhar Vijay, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2504, DOI 10.1109/CVPRW.2009.5206733
   Chandrasekhar V., 2009, P SPIE VCIP
   Chandrasekhar V, 2012, INT J COMPUT VISION, V96, P384, DOI 10.1007/s11263-011-0453-z
   Chandrasekhar V, 2010, IEEE IMAGE PROC, P3885, DOI 10.1109/ICIP.2010.5649937
   Chen DM, 2011, PROC CVPR IEEE, P737, DOI 10.1109/CVPR.2011.5995610
   Chen DM, 2008, IEEE DATA COMPR CONF, P143, DOI 10.1109/DCC.2009.33
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Frahm J-M, 2006, 2006 IEEE COMP SOC C, P453
   Gavves Efstratios., 2010, Proceedings of the International Conference on Multimedia, MM '10, P1123
   Girod B, 2011, IEEE SIGNAL PROC MAG, V28, P61, DOI 10.1109/MSP.2011.940881
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   Irschara A, 2009, PROC CVPR IEEE, P2591, DOI 10.1109/CVPRW.2009.5206587
   Ji R., 2011, Proceedings of the 19th ACM International Conference on Multimedia, MM'11, P573
   Ji RR, 2012, INT J COMPUT VISION, V96, P290, DOI 10.1007/s11263-011-0472-9
   Kalantidis Y, 2011, MULTIMED TOOLS APPL, V51, P555, DOI 10.1007/s11042-010-0651-7
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Lee J. A., 2008, ADV VIS COMPUT
   Li XW, 2008, LECT NOTES COMPUT SC, V5302, P427
   Liu H., 2012, Proceedings_of_the_20th_ACM_ International_Conference_on_Multimedia, MM'12, P9
   Liu H., ACM T MULTI IN PRESS
   Lourakis MIA, 2009, ACM T MATH SOFTWARE, V36, DOI 10.1145/1486525.1486527
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Makar M, 2009, INT CONF ACOUST SPEE, P821, DOI 10.1109/ICASSP.2009.4959710
   Morel J., 2009, SIAM J IMAG SCI
   Nistér D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17
   Nister D., 2006, P COMP VIS PATT REC, P1000
   Schindler G., 2007, P IEEE C COMP VIS PA, V2007, P1
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   SOODAK RE, 1986, P NATL ACAD SCI USA, V83, P9259, DOI 10.1073/pnas.83.23.9259
   Tsai S., 2009, ICST MOBILEMEDIA
   Wang M., 2013, IEEE T IMAGE PROCESS, V22
   Wang M, 2012, IEEE T IMAGE PROCESS, V21, P4649, DOI 10.1109/TIP.2012.2207397
   Wang M, 2010, IEEE T MULTIMEDIA, V12, P829, DOI 10.1109/TMM.2010.2055045
   Xiao X., 2010, Proceedings of the International Conference on Multimedia, Firenze, Italy, P719
   Xiao X., 2011, P ACM J HGBC, P77
   Xiao XA, 2010, IEEE INT CON MULTI, P1091, DOI 10.1109/ICME.2010.5582982
   Yang Jun, 2009, Proceedings of the 2009 2nd International Congress on Image and Signal Processing (CISP), DOI 10.1109/CISP.2009.5304123
   Yibin Li, 2009, 2009 IEEE International Conference on Automation and Logistics (ICAL), P1957, DOI 10.1109/ICAL.2009.5262626
   Zhang W, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P33, DOI 10.1109/3dpvt.2006.80
   Zheng YT, 2009, PROC CVPR IEEE, P1085, DOI 10.1109/CVPRW.2009.5206749
NR 46
TC 18
Z9 21
U1 2
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 623
EP 636
DI 10.1109/TMM.2014.2302744
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500005
DA 2024-07-18
ER

PT J
AU Yin, WY
   Mei, T
   Chen, CW
   Li, SP
AF Yin, Wenyuan
   Mei, Tao
   Chen, Chang Wen
   Li, Shipeng
TI Socialized Mobile Photography: Learning to Photograph With Social
   Context via Mobile Devices
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Camera parameters; mobile photography; social context; social media;
   view enclosure
ID AESTHETICS; QUALITY; IMAGES
AB The popularity of mobile devices equipped with various cameras has revolutionized modern photography. People are able to take photos and share their experiences anytime and anywhere. However, taking a high quality photograph via mobile device remains a challenge for mobile users. In this paper we investigate a photography model to assist mobile users in capturing high quality photos by using both the rich context available frommobile devices and crowdsourced social media on the Web. The photography model is learned from community-contributed images on the Web, and dependent on user's social context. The context includes user's current geo-location, time (i.e., time of the day), and weather (e. g., clear, cloudy, foggy, etc.). Given a wide view of scene, our socialized mobile photography system is able to suggest the optimal view enclosure (composition) and appropriate camera parameters (aperture, ISO, and exposure time). Extensive experiments have been performed for eight well-known hot spot landmark locations where sufficient context tagged photos can be obtained. Through both objective and subjective evaluations, we show that the proposed socialized mobile photography system can indeed effectively suggest proper composition and camera parameters to help the user capture high quality photos.
C1 [Yin, Wenyuan; Chen, Chang Wen] SUNY Buffalo, Buffalo, NY 14260 USA.
   [Mei, Tao; Li, Shipeng] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Buffalo; Microsoft; Microsoft Research Asia
RP Yin, WY (corresponding author), SUNY Buffalo, Buffalo, NY 14260 USA.
EM tmei@microsoft.com; chencw@buffalo.edu
RI Mei, Tao/GQZ-0596-2022; Li, Shipeng/AAA-3374-2020
OI Mei, Tao/0000-0002-5990-7307; Li, Shipeng/0000-0001-5368-4256
FU NSF [0964797]; Kodak; Direct For Computer & Info Scie & Enginr; Div Of
   Information & Intelligent Systems [0964797] Funding Source: National
   Science Foundation
FX This work was supported by NSF Grant 0964797 and a Gift Funding from
   Kodak. Part of this work was performed when the first author visited
   Microsoft Research Asia as a research intern. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Vasileios Mezaris.
CR [Anonymous], [No title captured]
   [Anonymous], 2009, P 17 ACM INT C MULT
   [Anonymous], 2010, ACM MULTIMEDIA
   Bhattacharya S., 2010, P 18 ACM INT C MULTI, P271
   Bourke S., 2011, Proceedings of the International Conference of Intelligent User Interfaces (IUI), P13, DOI DOI 10.1145/1943403.1943408
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen DM, 2011, PROC CVPR IEEE, P737, DOI 10.1109/CVPR.2011.5995610
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Datta Ritendra., 2010, Proceedings of the international conference on Multimedia information retrieval, P421, DOI DOI 10.1145/1743384.1743457
   De Choudhury M., 2010, Proceedings of the 21st ACM Conference on Hypertext and Hypermedia-HT'10, P35, DOI DOI 10.1145/1810617.1810626
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Goodale Mark, 2007, PHOTOGRAPHERS EYE CO, P1
   Hou X., 2007, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2007.383267
   Ji RR, 2012, INT J COMPUT VISION, V96, P290, DOI 10.1007/s11263-011-0472-9
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Kim JG, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1171, DOI 10.1109/ICME.2000.871569
   Liu LG, 2010, COMPUT GRAPH FORUM, V29, P469, DOI 10.1111/j.1467-8659.2009.01616.x
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Mei T, 2007, IEEE T CIRC SYST VID, V17, P699, DOI 10.1109/TCSVT.2007.896640
   Papadopoulos S, 2011, IEEE MULTIMEDIA, V18, P52, DOI 10.1109/MMUL.2010.68
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Su HH, 2012, IEEE T MULTIMEDIA, V14, P833, DOI 10.1109/TMM.2012.2186123
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Yao L, 2012, INT J COMPUT VISION, V96, P353, DOI 10.1007/s11263-011-0478-3
   Yin W., 2012, P IEEE INT C MULT EX
   Yin W., 2012, P VCIP
   Yin WY, 2011, IEEE T MULTIMEDIA, V13, P432, DOI 10.1109/TMM.2011.2129501
   Zhou W., 2010, P 18 ACM INT C MULTI, P511
   Zhu CJ, 2009, 2009 INTERNATIONAL FORUM ON COMPUTER SCIENCE-TECHNOLOGY AND APPLICATIONS, VOL 1, PROCEEDINGS, P277, DOI 10.1109/IFCSTA.2009.73
   Zhuang JF, 2011, UBICOMP'11: PROCEEDINGS OF THE 2011 ACM INTERNATIONAL CONFERENCE ON UBIQUITOUS COMPUTING, P153
NR 34
TC 34
Z9 38
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 184
EP 200
DI 10.1109/TMM.2013.2283468
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100016
DA 2024-07-18
ER

PT J
AU Diego, F
   Serrat, J
   López, AM
AF Diego, Ferran
   Serrat, Joan
   Lopez, Antonio M.
TI Joint Spatio-Temporal Alignment of Sequences
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Direct-based; feature-based; image registration; Markov random fields;
   synchronization; video alignment; video retrieval
ID VIDEO SYNCHRONIZATION
AB Video alignment is important in different areas of computer vision such as wide baseline matching, action recognition, change detection, video copy detection and frame dropping prevention. Current video alignment methods usually deal with a relatively simple case of fixed or rigidly attached cameras or simultaneous acquisition. Therefore, in this paper we propose a joint video alignment for bringing two video sequences into a spatio-temporal alignment. Specifically, the novelty of the paper is to formulate the video alignment to fold the spatial and temporal alignment into a single alignment framework. This simultaneously satisfies a frame-correspondence and frame-alignment similarity; exploiting the knowledge among neighbor frames by a standard pairwise Markov random field (MRF). This new formulation is able to handle the alignment of sequences recorded at different times by independent moving cameras that follows a similar trajectory, and also generalizes the particular cases that of fixed geometric transformation and/or linear temporal mapping. We conduct experiments on different scenarios such as sequences recorded simultaneously or by moving cameras to validate the robustness of the proposed approach. The proposed method provides the highest video alignment accuracy compared to the state-of-the-art methods on sequences recorded from vehicles driving along the same track at different times.
C1 [Diego, Ferran] Heidelberg Univ, Heidelberg Collaboratory Image Proc HCI, Heidelberg, Germany.
   [Serrat, Joan; Lopez, Antonio M.] Univ Autonoma Barcelona, Comp Vis Ctr, Cerdanyola Del Valls 08193, Spain.
   [Serrat, Joan; Lopez, Antonio M.] Univ Autonoma Barcelona, Dept Comp Sci, Cerdanyola Del Valls 08193, Spain.
C3 Ruprecht Karls University Heidelberg; Autonomous University of
   Barcelona; Centre de Visio per Computador (CVC); Autonomous University
   of Barcelona
RP Diego, F (corresponding author), Heidelberg Univ, Heidelberg Collaboratory Image Proc HCI, Heidelberg, Germany.
EM ferran.diego@iwr.uni-heidelberg.de; joans@cvc.uab.es; antonio@cvc.uab.es
RI Serrat, Joan/L-4735-2014; López, Antonio M/L-5303-2014
OI López, Antonio M/0000-0002-6979-5783; Serrat, Joan/0000-0002-4554-199X
FU Spanish Ministry of Education and Science [TRA2011-29454-C03-01];
   research program Consolider Ingenio: MIPRCV [CSD200700018]; FPU MEC
   grant [AP2007-01558]; Cellular Networks at the University of Heidelberg
FX Manuscript received June 23, 2011; revised June 13, 2012 and September
   21, 2012; accepted November 08, 2012. Date of publication February 14,
   2013; date of current version September 13, 2013. This work was
   supported by the Spanish Ministry of Education and Science under project
   TRA2011-29454-C03-01, and research program Consolider Ingenio 2010:
   MIPRCV (CSD200700018), the FPU MEC grant AP2007-01558 and Cellular
   Networks at the University of Heidelberg. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Christophe De Vleeschouwer.
CR [Anonymous], P ADV CONC INT VIS S
   [Anonymous], 2008, An Open and Portable Library of Computer Vision Algorithms
   Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Caspi Y, 2002, IEEE T PATTERN ANAL, V24, P1409, DOI 10.1109/TPAMI.2002.1046148
   Dai CX, 2006, IEEE SIGNAL PROC LET, V13, P737, DOI 10.1109/LSP.2006.879852
   Davis TA, 2004, ACM T MATH SOFTWARE, V30, P165, DOI 10.1145/992200.992205
   diaeresis>tze Hinrich Schu<spacing, 2008, INTRO INFORM RETRIEV, V39
   Diego F., 2010, IEEE T IMAGE PROCESS
   Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711
   Irani M, 2002, INT J COMPUT VISION, V48, P173, DOI 10.1023/A:1016372015744
   Kelly P, 2008, IEEE T CIRC SYST VID, V18, P1163, DOI 10.1109/TCSVT.2008.928228
   Kong H, 2010, IEEE T IMAGE PROCESS, V19, P2201, DOI 10.1109/TIP.2010.2045714
   Lei C, 2006, IEEE T IMAGE PROCESS, V15, P2473, DOI 10.1109/TIP.2006.877438
   Li S. Z., 2009, Markov random field modeling in image analysis
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Nister D., 2006, IEEE COMP SOC C COMP, P2161, DOI [10.1109/cvpr.2006.264, DOI 10.1109/CVPR.2006.264, 10.1109/CVPR]
   Pádua FLC, 2010, IEEE T PATTERN ANAL, V32, P304, DOI 10.1109/TPAMI.2008.301
   Pundik D, 2010, LECT NOTES COMPUT SC, V6313, P15
   Rao C, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P939
   Ravichandran A, 2008, LECT NOTES COMPUT SC, V5303, P514, DOI 10.1007/978-3-540-88688-4_38
   Russell S., 2009, Artificial intelligence
   Sand P, 2004, ACM T GRAPHIC, V23, P592, DOI 10.1145/1015706.1015765
   Serrat J., 2007, P 3 IB C PATT REC IM
   Singh M., P ECCV
   Sivic J, 2006, LECT NOTES COMPUT SC, V4170, P127
   Stein G., 1999, TRACKING MULTIPLE VI, V1, P1521
   Szeliski R., MSRTR200492
   Tresadern PA, 2009, COMPUT VIS IMAGE UND, V113, P891, DOI 10.1016/j.cviu.2009.03.012
   Tuytelaars T, 2004, PROC CVPR IEEE, P762
   Ukrainitz Y, 2006, LECT NOTES COMPUT SC, V3953, P538, DOI 10.1007/11744078_42
   Wedge D., 2007, P IAPR C MACH VIS AP
   Wolf L, 2006, INT J COMPUT VISION, V68, P43, DOI 10.1007/s11263-005-4841-0
NR 34
TC 21
Z9 23
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2013
VL 15
IS 6
BP 1377
EP 1387
DI 10.1109/TMM.2013.2247390
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222XH
UT WOS:000324765400014
DA 2024-07-18
ER

PT J
AU Sang, JT
   Xu, CS
   Liu, J
AF Sang, Jitao
   Xu, Changsheng
   Liu, Jing
TI User-Aware Image Tag Refinement via Ternary Semantic Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Factor analysis; social media; tag refinement; tensor factorization
ID TENSOR DECOMPOSITIONS
AB Large-scale user contributed images with tags are easily available on photo sharing websites. However, the noisy or incomplete correspondence between the images and tags prohibits them from being leveraged for precise image retrieval and effective management. To tackle the problem of tag refinement, we propose a method of Ranking based Multi-correlation Tensor Factorization (RMTF), to jointly model the ternary relations among user, image, and tag, and further to precisely reconstruct the user-aware image-tag associations as a result. Since the user interest or background can be explored to eliminate the ambiguity of image tags, the proposed RMTF is believed to be superior to the traditional solutions, which only focus on the binary image-tag relations. During the model estimation, we employ a ranking based optimization scheme to interpret the tagging data, in which the pair-wise qualitative difference between positive and negative examples is used, instead of the point-wise 0/1 confidence. Specifically, the positive examples are directly decided by the observed user-image-tag interrelations, while the negative ones are collected with respect to the most semantically and contextually irrelevant tags. Extensive experiments on a benchmark Flickr dataset demonstrate the effectiveness of the proposed solution for tag refinement. We also show attractive performances on two potential applications as the by-products of the ternary relation analysis.
C1 [Sang, Jitao; Xu, Changsheng; Liu, Jing] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
   [Sang, Jitao; Xu, Changsheng; Liu, Jing] China Singapore Inst Digital Media, Singapore 119613, Singapore.
C3 Chinese Academy of Sciences; Institute of Automation, CAS
RP Sang, JT (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
EM jtsang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn; jliu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023
FU National Program on Key Basic Research Project (973 Program)
   [2012CB316304]; National Natural Science Foundation of China [90920303,
   61003161]
FX This work was supported in part by National Program on Key Basic
   Research Project (973 Program, Project No. 2012CB316304) and the
   National Natural Science Foundation of China (Grant No. 90920303,
   61003161). The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Shin'ichi Satoh.
CR Acar E, 2009, IEEE T KNOWL DATA EN, V21, P6, DOI 10.1109/TKDE.2008.112
   ALEX M, 2003, P 2003 IEEE COMP SOC, P93
   [Anonymous], P CIVR
   [Anonymous], 2010, P ACM MULTIMEDIA
   BENGIO Y, 2003, P NIPS
   Chen L, 2010, PROC CVPR IEEE, P3440, DOI 10.1109/CVPR.2010.5539988
   Chi Y., 2010, Proceedings of the 19th ACM international conference on Information and knowledge management, P569
   Franz T, 2009, LECT NOTES COMPUT SC, V5823, P213, DOI 10.1007/978-3-642-04930-9_14
   He X. F., 2005, P NIPS, P1
   Jäschke R, 2007, LECT NOTES ARTIF INT, V4702, P506
   Jäschke R, 2008, AI COMMUN, V21, P231, DOI 10.3233/AIC-2008-0438
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Kolda TG, 2008, IEEE DATA MINING, P363, DOI 10.1109/ICDM.2008.89
   Kolda TG, 2005, FIFTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P242, DOI 10.1109/ICDM.2005.77
   Li WJ, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1126
   Li X, 2007, IEEE I CONF COMP VIS, P960
   Li Zheng, 2010, Proceedings of the SICE 2010 - 49th Annual Conference of the Society of Instrument and Control Engineers of Japan, P1187, DOI 10.1145/1873951.1874183
   Lin YR, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P527
   Liu D., 2010, Proceedings of ACM International Conference on Multimedea, P25
   Liu D, 2011, MULTIMED TOOLS APPL, V51, P723, DOI 10.1007/s11042-010-0647-3
   Liu Dong., 2010, Proceedings of the International Conference on Multimedia, P491
   Liu Dong., 2009, P 18 INT C WORLD WID, P351
   Liu JJ, 2007, PROCEEDINGS OF THE 2007 CHINESE CONTROL AND DECISION CONFERENCE, P605, DOI 10.1145/1291233.1291380
   Lu D., 2011, P 20 INT C COMP WORL
   Rendle S, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P727
   Rendle Steffen, 2010, P 3 ACM INT C WEB SE, P81, DOI DOI 10.1145/1718487.1718498
   Rohini U, 2005, LECT NOTES COMPUT SC, V3815, P194
   Sutskever I., 2009, P 22 INT C NEURAL IN, V22, P1821
   Symeonidis P., 2008, Proceedings of the 9th International Conference on Music Information Retrieval (ISMIR 2008), P219
   Wang C, 2007, P CVPR
   Wang CB, 2006, LECT NOTES COMPUT SC, V4035, P647, DOI 10.1145/1180639.1180774
   Wei C, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P1023
   Wen J, 2010, NEUROCOMPUTING, V73, P827, DOI 10.1016/j.neucom.2009.10.013
   Xie L., 2010, P CIVR, P58
   Xu HX, 2009, PROCEEDINGS OF THE 2009 WRI GLOBAL CONGRESS ON INTELLIGENT SYSTEMS, VOL III, P573, DOI 10.1109/GCIS.2009.320
   Yeung C. M. A., 2008, P SWKM
   Yohan Jin, 2005, 13th Annual ACM International Conference on Multimedia, P706
   Zhang T, 2001, SIAM J MATRIX ANAL A, V23, P534, DOI 10.1137/S0895479899352045
   Zhou YH, 2008, LECT NOTES COMPUT SC, V5034, P337, DOI 10.1007/978-3-540-68880-8_32
NR 39
TC 93
Z9 103
U1 0
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 883
EP 895
DI 10.1109/TMM.2012.2188782
PN 2
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700019
DA 2024-07-18
ER

PT J
AU Sohel, FA
   Karmakar, GC
   Dooley, LS
   Bennamoun, M
AF Sohel, Ferdous A.
   Karmakar, Gour C.
   Dooley, Laurence S.
   Bennamoun, Mohammed
TI Sliding-Window Designs for Vertex-Based Shape Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image processing; shape coding; sliding window
ID MPEG-4
AB Traditionally the sliding window (SW) has been employed in vertex-based operational rate distortion (ORD) optimal shape coding algorithms to ensure consistent distortion (quality) measurement and improve computational efficiency. It also regulates the memory requirements for an encoder design enabling regular, symmetrical hardware implementations. This paper presents a series of new enhancements to existing techniques for determining the best SW-length within a rate-distortion (RD) framework, and analyses the nexus between SW-length and storage for ORD hardware realizations. In addition, it presents an efficient bit-allocation strategy for managing multiple shapes together with a generalized adaptive SW scheme which integrates localized curvature information (cornerity) on contour points with a bi-directional spatial distance, to afford a superior and more pragmatic SW design compared with existing adaptive SW solutions which are based on only cornerity values. Experimental results consistently corroborate the effectiveness of these new strategies.
C1 [Sohel, Ferdous A.; Bennamoun, Mohammed] Univ Western Australia, Sch Comp Sci & Software Engn, Crawley, Australia.
   [Karmakar, Gour C.] Monash Univ, Gippsland Sch Informat Technol, Churchill, Australia.
   [Dooley, Laurence S.] Open Univ, Dept Commun & Syst, Milton Keynes MK7 6AA, Bucks, England.
C3 University of Western Australia; Federation University Australia; Monash
   University; Open University - UK
RP Sohel, FA (corresponding author), Univ Western Australia, Sch Comp Sci & Software Engn, Crawley, Australia.
EM Ferdous.Sohel@csse.uwa.edu.au; Gour.Karmakar@in-fotech.monash.edu.au;
   L.S.Dooley@open.ac.uk; m.bennamoun@csse.uwa.edu.au
RI Sohel, Ferdous/C-2428-2013; Bennamoun, Mohammed/C-2789-2013
OI Sohel, Ferdous/0000-0003-1557-4907; Bennamoun,
   Mohammed/0000-0002-6603-3257; Karmakar, Gour/0000-0002-1308-7315
FU University of Western Australia (UWA); ARC [DP0771294]; Australian
   Research Council [DP0771294] Funding Source: Australian Research Council
FX This work was supported in part by a University of Western Australia
   (UWA) Post-doctoral Fellowship and an ARC discovery grant (DP0771294).
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Ketan Mayer-Patel.
CR Aghito SM, 2006, IEEE T IMAGE PROCESS, V15, P2120, DOI 10.1109/TIP.2006.875168
   Aghito SM, 2004, IEEE DATA COMPR CONF, P399
   [Anonymous], 2003, H.264 and MPEG-4 video compression: video coding for next generation multimedia
   Brady N., 1997, Proceedings. International Conference on Image Processing (Cat. No.97CB36144), P29, DOI 10.1109/ICIP.1997.647376
   Chen ZZ, 2004, IEEE T CIRC SYST VID, V14, P869, DOI 10.1109/TCSVT.2004.828331
   Freeman H., 1961, IRE T ELECTRON COMPU, V10, P260, DOI [DOI 10.1109/TEC.1961.5219197, 10.1109/TEC.1961.5219197]
   Graham R. L, 1994, CONCRETE MATH FDN CO, V2nd
   Katsaggelos AK, 1998, P IEEE, V86, P1126, DOI 10.1109/5.687833
   Kondi LP, 2004, IEEE T CIRC SYST VID, V14, P528, DOI 10.1109/TCSVT.2004.825569
   Lai Z., 2009, P DAT COMPR C DCC
   Meier FW, 2000, SIGNAL PROCESS-IMAGE, V15, P685, DOI 10.1016/S0923-5965(99)00045-4
   MELKMAN AA, 1987, INFORM PROCESS LETT, V25, P11, DOI 10.1016/0020-0190(87)90086-X
   PHILLIPS TY, 1987, PATTERN RECOGN LETT, V5, P285, DOI 10.1016/0167-8655(87)90059-6
   Scarborough J.B., 1958, NUMERICAL MATH ANAL, V4th
   Schuster G., 1997, RATE DISTORTION BASE
   Sohel F. A., 2010, ACM COMPUT SURV
   SOHEL F. A., 2006, P INT C SIGN PROC IC
   Sohel F. A., 2007, THESIS MONASH U CHUR
   Sohel FA, 2006, PATTERN RECOGN LETT, V27, P133, DOI 10.1016/j.patrec.2005.07.006
   Sohel FA, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1978802.1978808
   Sohel FA, 2007, IEEE T CIRC SYST VID, V17, P1408, DOI 10.1109/TCSVT.2007.903788
   Venkatraman D., 2009, P IEEE INT C AC SPEE
NR 22
TC 7
Z9 8
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 683
EP 692
DI 10.1109/TMM.2011.2182507
PN 2
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700003
DA 2024-07-18
ER

PT J
AU Tang, NC
   Hsu, CT
   Su, CW
   Shih, TK
   Liao, HYM
AF Tang, Nick C.
   Hsu, Chiou-Ting
   Su, Chih-Wen
   Shih, Timothy K.
   Liao, Hong-Yuan Mark
TI Video Inpainting on Digitized Vintage Films via Maintaining
   Spatiotemporal Continuity
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Frame completion; motion completion; motion estimation; video inpainting
ID COMPLETION; REMOVAL
AB Video inpainting is an important video enhancement technique used to facilitate the repair or editing of digital videos. It has been employed worldwide to transform cultural artifacts such as vintage videos/films into digital formats. However, the quality of such videos is usually very poor and often contain unstable luminance and damaged content. In this paper, we propose a video inpainting algorithm for repairing damaged content in digitized vintage films, focusing on maintaining good spatiotemporal continuity. The proposed algorithm utilizes two key techniques. Motion completion recovers missing motion information in damaged areas to maintain good temporal continuity. Frame completion repairs damaged frames to produce a visually pleasing video with good spatial continuity and stabilized luminance. We demonstrate the efficacy of the algorithm on different types of video clips.
C1 [Tang, Nick C.; Liao, Hong-Yuan Mark] Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
   [Hsu, Chiou-Ting] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30043, Taiwan.
   [Su, Chih-Wen] Chung Yuan Christian Univ, Dept Informat & Comp Engn, Taipei, Taiwan.
   [Shih, Timothy K.] Natl Cent Univ, Dept Comp Sci & Informat Engn, Chungli 32054, Taiwan.
   [Liao, Hong-Yuan Mark] Natl Chiao Tung Univ, Taipei, Taiwan.
   [Liao, Hong-Yuan Mark] Acad Sinica, Inst Appl Sci & Engn, Taipei 115, Taiwan.
C3 Academia Sinica - Taiwan; National Tsing Hua University; Chung Yuan
   Christian University; National Central University; National Yang Ming
   Chiao Tung University; Academia Sinica - Taiwan
RP Tang, NC (corresponding author), Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
EM nickctang@iis.sinica.edu.tw; cthsu@cs.nthu.edu.tw; lucas@cycu.edu.tw;
   timothykshih@gmail.com; liao@iis.sinica.edu.tw
RI Liao, Hong-Yuan Mark/AAQ-5514-2021
OI Hsu, Chiou-Ting/0000-0001-8857-2481
FU National Science Council of Taiwan under NSC [NSC99-2631-H-001-020]
FX This work was supported in part by Taiwan E-learning and Digital
   Archives Programs (TELDAP) sponsored by the National Science Council of
   Taiwan under NSC Grant: NSC99-2631-H-001-020. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Pascal Frossard.
CR [Anonymous], P IEEE COMP SOC C CO
   BERGEN JR, 1992, P 2 EUR C COMP VIS, P237
   Bouguet J-Y, 1999, Pyramidal implementation of the Lucas Kanade feature tracker
   Brown M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1218
   BURT PJ, 1983, ACM T GRAPHIC, V2, P217, DOI 10.1145/245.247
   Capel D, 1998, PROC CVPR IEEE, P885, DOI 10.1109/CVPR.1998.698709
   CENCI A, 1999, P 10 INT C IM AN PRO, P665
   Cham TJ, 1998, PROC CVPR IEEE, P442, DOI 10.1109/CVPR.1998.698643
   Cheung CH, 2005, IEEE T MULTIMEDIA, V7, P16, DOI 10.1109/TMM.2004.840609
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   CUEVAS E, 2005, B0512 FREIE U
   CUEVAS E, B0513 FREIE U
   Drori I, 2003, ACM T GRAPHIC, V22, P303, DOI 10.1145/882262.882267
   Huang AM, 2009, IEEE T IMAGE PROCESS, V18, P740, DOI 10.1109/TIP.2008.2010206
   Jia JY, 2006, IEEE T PATTERN ANAL, V28, P832, DOI 10.1109/TPAMI.2006.108
   Jia JY, 2004, PROC CVPR IEEE, P364
   JOYEUX L, 1999, P IEEE C COMP VIS PA
   KOKARAM AC, 1997, P INT C IM PROC OCT, V2, P191
   Lucas B., 1981, P INT JOINT C ART IN, P674, DOI DOI 10.1364/J0SAA.19.002142
   Machì A, 2003, 12TH INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND PROCESSING, PROCEEDINGS, P454, DOI 10.1109/ICIAP.2003.1234092
   Matsushita Y, 2005, PROC CVPR IEEE, P50
   Patwardhan KA, 2007, IEEE T IMAGE PROCESS, V16, P545, DOI 10.1109/TIP.2006.888343
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Po LM, 1996, IEEE T CIRC SYST VID, V6, P313, DOI 10.1109/76.499840
   SELENSNICK IW, 2003, P SPIE WAV 10 SAN DI
   Shen YP, 2006, INT C PATT RECOG, P63
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Shih TK, 2009, IEEE T CIRC SYST VID, V19, P347, DOI 10.1109/TCSVT.2009.2013519
   Sun J, 2005, ACM T GRAPHIC, V24, P861, DOI 10.1145/1073204.1073274
   TANG NC, 2009, P INT C KNOWL BAS 2, P421
   TURAGA D, 1999, CORRELATION BASED SE
   Güllü MK, 2006, IEEE INT SYMP CIRC S, P4591
   Venkatesh MV, 2009, PATTERN RECOGN LETT, V30, P168, DOI 10.1016/j.patrec.2008.03.011
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Zhang YJ, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P516
   Zhu Z., 1998, IEEE INT C INTELLIGE, P329
   Zoghlami I, 1997, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.1997.609359
NR 38
TC 33
Z9 38
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2011
VL 13
IS 4
BP 602
EP 614
DI 10.1109/TMM.2011.2112642
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 805EX
UT WOS:000293710300002
DA 2024-07-18
ER

PT J
AU Yeh, MC
   Cheng, KT
AF Yeh, Mei-Chen
   Cheng, Kwang-Ting
TI Fast Visual Retrieval Using Accelerated Sequence Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image classification; similarity measure; string matching; video
   retrieval
ID OBJECT CATEGORIES; CLASSIFICATION; DISTANCE; FEATURES
AB We present an approach to represent, match, and index various types of visual data, with the primary goal of enabling effective and computationally efficient searches. In this approach, an image/video is represented by an ordered list of feature descriptors. Similarities between such representations are then measured by the approximate string matching technique. This approach unifies visual appearance and the ordering information in a holistic manner with joint consideration of visual-order consistency between the query and the reference instances, and can be used for automatically identifying local alignments between two pieces of visual data. This capability is essential for tasks such as video copy detection where only small portions of the query and the reference videos are similar. To deal with large volumes of data, we further show that this approach can be significantly accelerated along with a dedicated indexing structure. Extensive experiments on various visual retrieval and classification tasks demonstrate the superior performance of the proposed techniques compared to existing solutions.
C1 [Yeh, Mei-Chen] Natl Taiwan Normal Univ, Dept Comp Sci & Informat Engn, Taipei, Taiwan.
   [Cheng, Kwang-Ting] Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA.
C3 National Taiwan Normal University; University of California System;
   University of California Santa Barbara
RP Yeh, MC (corresponding author), Natl Taiwan Normal Univ, Dept Comp Sci & Informat Engn, Taipei, Taiwan.
EM myeh@cs.ntnu.edu.tw; timcheng@ece.ucsb.edu
OI Yeh, Mei-Chen/0000-0001-8665-7860; Cheng, Kwang-Ting
   Tim/0000-0002-3885-4912
FU National Science Council of the Republic of China [NSC
   99-2218-E-003-001]
FX This work was supported in part by the National Science Council of the
   Republic of China, under Grant NSC 99-2218-E-003-001. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Daniel Gatica-Perez.
CR Adjeroh DA, 1998, INTERNATIONAL WORKSHOP ON MULTI-MEDIA DATABASE MANAGEMENT SYSTEMS- PROCEEDINGS, P72, DOI 10.1109/MMDBMS.1998.709503
   [Anonymous], P ACMMM
   Barrow H. G., 1977, P IMAGE UNDERSTANDIN, P659
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Berg AC, 2005, PROC CVPR IEEE, P26
   Bertini M, 2006, LECT NOTES COMPUT SC, V4071, P133
   Bosch A., 2007, P 6 ACM INT C IMAGE, P401, DOI DOI 10.1145/1282280.1282340
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   CHUM O, 2007, P 6 ACM INT C IM VID, P549
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Duda R., 1973, Pattern Classification and Scene Analysis
   FELZENSZWALB P, 2007, P IEEE C COMP VIS PA, P1
   Fergus R, 2005, IEEE I CONF COMP VIS, P1816
   Gonzalez RC, 2002, DIGITAL IMAGE PROCES
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   Grauman K, 2004, PROC CVPR IEEE, P220
   Joly A, 2007, IEEE T MULTIMEDIA, V9, P293, DOI 10.1109/TMM.2006.886278
   Ke Y., 2004, Proceedings of the ACM International Conference on Multimedia, P1150
   Kim YT, 2005, 11TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P68
   Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850
   Law-To J., 2006, P ACM INT C MULTIMED, P835
   Law-To J., 2007, Muscle-VCD-2007: a live benchmark for video copy detection
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Leibe Bastian., 2006, CATEGORY LEVEL OBJEC
   Li HF, 2005, J COMPUT BIOL, V12, P702, DOI 10.1089/cmb.2005.12.702
   Ling H., 2006, Proceedings of the European Conference on Computer Vision, P330
   Ling HB, 2005, PROC CVPR IEEE, P719
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mori G, 2001, PROC CVPR IEEE, P723
   Nister D., 2006, IEEE COMP SOC C COMP, P2161, DOI [10.1109/cvpr.2006.264, DOI 10.1109/CVPR.2006.264, 10.1109/CVPR]
   PEARSON WR, 1988, P NATL ACAD SCI USA, V85, P2444, DOI 10.1073/pnas.85.8.2444
   Quelhas P, 2007, IEEE T PATTERN ANAL, V29, P1575, DOI 10.1109/TPAMI.2007.1155
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Scott C, 2006, IEEE T IMAGE PROCESS, V15, P1831, DOI 10.1109/TIP.2006.877038
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   SMITH TF, 1981, J MOL BIOL, V147, P195, DOI 10.1016/0022-2836(81)90087-5
   Thayananthan A, 2003, PROC CVPR IEEE, P127
   Wu A. G., 2007, P ACM MM, P218
   YEH M, 2009, P ACM INT C VID IM R
   Yeh M.-C., 2008, Multimedia Information Retrieval, P52
   YEH MC, 2009, P ACM MM OCT, P633
   Yeh T, 2007, IEEE I CONF COMP VIS, P1759
   ZHANG D.-Q., 2004, PROC ACM INT C MULTI, P877, DOI DOI 10.1145/1027527.1027730
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
   ZHOU X, 2009, P IEEE INT C COMP VI
NR 47
TC 11
Z9 12
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2011
VL 13
IS 2
BP 320
EP 329
DI 10.1109/TMM.2010.2094999
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 738SG
UT WOS:000288661800013
DA 2024-07-18
ER

PT J
AU Liu, D
   Wang, M
   Hua, XS
   Zhang, HJ
AF Liu, Dong
   Wang, Meng
   Hua, Xian-Sheng
   Zhang, Hong-Jiang
TI Semi-Automatic Tagging of Photo Albums via Exemplar Selection and Tag
   Inference
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Exemplar selection; photo album; semi-automatic tagging; tag propagation
ID ANNOTATION; EVENT
AB As one of the emerging Web 2.0 activities, tagging becomes a popular approach to manage personal media data, such as photo albums. A dilemma in tagging behavior is the users' manual efforts and the tagging accuracy: exhaustively tagging all photos in an album is labor-intensive and time-consuming, and simply entering tags for the whole album leads to unsatisfying results. In this paper, we propose a semi-automatic tagging scheme that aims to facilitate users in photo album tagging. The scheme is able to achieve a good trade-off between manual efforts and tagging accuracy as well as to adjust tagging performance according to the user's customization. For a given album, it first selects a set of representative exemplars for manual tagging via a temporally consistent affinity propagation algorithm, and the tags of the rest of the photos are automatically inferred. Then a constrained affinity propagation algorithm is applied to select a new set of exemplars for manual tagging in an incremental manner, based on which the performance of the tag inference in the previous round can be estimated. If the results are not satisfying enough, a further round of exemplar selection and tag inference will be implemented. This process repeats until satisfactory tagging results are achieved, and users can also stop the process at any time. Experimental results on real-world Flickr photo albums have demonstrated the effectiveness and usefulness of the proposed scheme.
C1 [Liu, Dong] Harbin Inst Technol, Harbin 150001, Peoples R China.
   [Wang, Meng; Hua, Xian-Sheng] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Zhang, Hong-Jiang] Microsoft Adv Technol Ctr, Beijing 100080, Peoples R China.
C3 Harbin Institute of Technology; Microsoft Research Asia; Microsoft;
   Microsoft
RP Liu, D (corresponding author), Harbin Inst Technol, Harbin 150001, Peoples R China.
EM dongliu.hit@gmail.com; mengwang@microsoft.com; xshua@microsoft.com;
   hjzhang@microsoft.com
RI Liu, Dong/AAL-8559-2021
CR Ames M, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P971
   [Anonymous], P IEEE INT C IM PROC
   [Anonymous], 2008, P 2008 IEEE POW EN S
   Cao L., 2008, Proc. ACM Multimedia, P121
   Cao LL, 2009, IEEE T MULTIMEDIA, V11, P208, DOI 10.1109/TMM.2008.2009693
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen H.M., 2008, MULTIMEDIA 2008, P737
   Chu Wei-Ta., 2008, P 16 ACM INT C MULTI, P829, DOI DOI 10.1145/1459359.1459498
   Cui JY, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P367
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Givoni IE, 2009, NEURAL COMPUT, V21, P1589, DOI 10.1162/neco.2009.05-08-785
   Jeon J., 2003, Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P119, DOI DOI 10.1145/860435.860459
   Jia J., 2008, ACM International Conference on Multimedia, P459
   Jia Y., 2008, ACM International Conference on Multimedia (MM), P639
   Kennedy L.S., 2006, Proc. ACM Multimedia Information Retrieval, P249, DOI DOI 10.1145/1178677.1178712
   Li J, 2008, IEEE T PATTERN ANAL, V30, P985, DOI 10.1109/TPAMI.2007.70847
   LIU D, 2009, P 15 INT MULT MOD C, P239
   LIU D, 2009, P 18 INT C WORLD WID, P351, DOI DOI 10.1145/1526709.1526757
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Sigurbjornsson B., 2008, Proceeding of the 17th International Conference on World Wide Web
   STRICKER M, 1995, P SOC PHOTO-OPT INS, V2410, P381, DOI 10.1117/12.205308
   Suh B, 2007, INTERACT COMPUT, V19, P524, DOI 10.1016/j.intcom.2007.02.002
   Tong S, 2001, P 9 ACM INT C MULT, P107, DOI DOI 10.1145/500141.500159
   Vailaya A, 1998, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES - PROCEEDINGS, P3, DOI 10.1109/IVL.1998.694464
   ZHU, 2003, WORKSH CONT LAB UNL
NR 26
TC 19
Z9 21
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2011
VL 13
IS 1
BP 82
EP 91
DI 10.1109/TMM.2010.2087744
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 708TJ
UT WOS:000286386900009
DA 2024-07-18
ER

PT J
AU Battiato, S
   Bruna, AR
   Puglisi, G
AF Battiato, Sebastiano
   Bruna, Arcangelo Ranieri
   Puglisi, Giovanni
TI A Robust Block-Based Image/Video Registration Approach for Mobile
   Imaging Devices
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Block matching; motion estimation; video stabilization
ID VIDEO STABILIZATION; MOTION
AB Digital video stabilization enables to acquire video sequences without disturbing jerkiness by compensating unwanted camera movements. In this paper, we propose a novel fast image registration algorithm based on block matching. Unreliable motion vectors (i.e., not related with jitter movements) are properly filtered out by making use of ad-hoc rules taking into account local similarity, local "activity," and matching effectiveness. Moreover, a temporal analysis of the relative error computed at each frame has been performed. Reliable information is then used to retrieve inter-frame transformation parameters. Experiments on real cases confirm the effectiveness of the proposed approach even in critical conditions.
C1 [Battiato, Sebastiano; Puglisi, Giovanni] Catania Univ, Dept Math & Comp Sci, I-95015 Catania, Italy.
   [Bruna, Arcangelo Ranieri] STMicroelectronics, AST, Imaging Team, Catania Lab, I-95121 Catania, Italy.
C3 University of Catania; STMicroelectronics
RP Battiato, S (corresponding author), Catania Univ, Dept Math & Comp Sci, I-95015 Catania, Italy.
EM battiato@dmi.unict.it; arcangelo.bruna@st.com; puglisi@dmi.unict.it
RI Bruna, Arcangelo Ranieri/I-7842-2019; Battiato,
   Sebastiano/ABI-1584-2020; Battiato, Sebastiano/O-7799-2019
OI Bruna, Arcangelo Ranieri/0000-0002-9147-4978; Battiato,
   Sebastiano/0000-0001-6127-2470; 
CR Adams A, 2008, COMPUT GRAPH FORUM, V27, P597, DOI 10.1111/j.1467-8659.2008.01157.x
   Battiato S, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P373, DOI 10.1109/ICME.2008.4607449
   Battiato S., 2008, Encyclopedia of Multimedia, P941, DOI [DOI 10.1007/978-0-387-78414-4_76, 10.1007/978-0-387-78414-4_76]
   Battiato S, 2008, 19 INT C PATT REC IC, P1
   Battiato S, 2007, 14TH INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND PROCESSING, PROCEEDINGS, P825, DOI 10.1109/ICIAP.2007.4362878
   Batur AU, 2006, IEEE IMAGE PROC, P465, DOI 10.1109/ICIP.2006.312494
   Bosco A, 2008, IEEE T CONSUM ELECTR, V54, P220, DOI 10.1109/TCE.2008.4560078
   BROWN N, 2003, P INT C COMP VIS ECC, P1218
   Chen HH, 2007, IEEE T CIRC SYST VID, V17, P801, DOI 10.1109/TCSVT.2007.897113
   Cho WH, 2007, IEEE T CONSUM ELECTR, V53, P833, DOI 10.1109/TCE.2007.4341553
   Del-Blanco CR, 2008, LECT NOTES COMPUT SC, V5259, P356, DOI 10.1007/978-3-540-88458-3_32
   Dorkó G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P634
   Ertürk S, 2000, IEE P-VIS IMAGE SIGN, V147, P95, DOI 10.1049/ip-vis:20000222
   FERRARI V, 2004, P EUR C COMP VIS, P40
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Irani M., 1999, P WORKSH VIS ALG THE, P267
   Jang SW, 2005, IMAGE VISION COMPUT, V23, P1250, DOI 10.1016/j.imavis.2005.09.003
   Jiménez H, 2008, LECT NOTES COMPUT SC, V5259, P290, DOI 10.1007/978-3-540-88458-3_26
   JODOIN PM, 2008, P INT C IM PROC ICIP
   KOGA T, 1981, P NAT TEL C, V4
   Kwon O, 2005, LECT NOTES COMPUT SC, V3656, P141, DOI 10.1007/11559573_18
   Lee KY, 2009, IEEE I CONF COMP VIS, P1397
   Liu F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531350
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lukac R, 2009, IMAGE PROCESS SER, P1
   Matsushita Y, 2006, IEEE T PATTERN ANAL, V28, P1150, DOI 10.1109/TPAMI.2006.141
   Mercenaro L., 2001, P INT C IM PROC ICIP
   Mikolajczyk K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P525, DOI 10.1109/ICCV.2001.937561
   Puglisi G, 2009, LECT NOTES ARTIF INT, V5571, P271, DOI 10.1007/978-3-642-02282-1_34
   Ramachandran M, 2006, IEEE IMAGE PROC, P345, DOI 10.1109/ICIP.2006.313164
   Ratakonda K., 1998, ISCAS '98. Proceedings of the 1998 IEEE International Symposium on Circuits and Systems (Cat. No.98CH36187), P69, DOI 10.1109/ISCAS.1998.698760
   Rovati FS, 2000, IEEE T CONSUM ELECTR, V46, P697, DOI 10.1109/30.883434
   SANJI P, 2007, P EUR SIGN PROC C EU, P1823
   Schaffalitzky F, 2002, LECT NOTES COMPUT SC, V2350, P414
   Se S, 2002, 2002 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-3, PROCEEDINGS, P226, DOI 10.1109/IRDS.2002.1041393
   SOLDATOV S, 2006, P GRAPHICOM NOV AK R
   Szeliski R, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000009
   TICO M, 2006, P INT C AC SPEECH SI
   Torr P H S, 1999, Vision Algorithms: Theory and Practice, P278, DOI DOI 10.1007/3-540-44480-7_19
   Vella F, 2002, IEEE T CONSUM ELECTR, V48, P796, DOI 10.1109/TCE.2002.1037077
   Yang JL, 2006, IEEE IMAGE PROC, P1545, DOI 10.1109/ICIP.2006.312645
NR 43
TC 43
Z9 49
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2010
VL 12
IS 7
BP 622
EP 635
DI 10.1109/TMM.2010.2060474
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 670SV
UT WOS:000283448500002
DA 2024-07-18
ER

PT J
AU Kholaif, AM
   Todd, TD
   Koutsakis, P
   Lazaris, A
AF Kholaif, Ahmad M.
   Todd, Terence D.
   Koutsakis, Polychronis
   Lazaris, Aggelos
TI Energy Efficient H.263 Video Transmission in Power Saving Wireless LAN
   Infrastructure
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE H.263; media access control; power saving; video transmission; wireless
   LAN
ID ACCESS; MPEG-4
AB Wireless local area networks (WLANs) are now being used in places where access point (AP) power saving would be very desirable. Although this is not currently possible, modifications to the IEEE 802.11 protocol have recently been proposed which would permit this functionality. A difficult problem that arises is to maintain good client power saving when AP power saving is introduced. This is especially true when carrying multiple real-time traffic flows such as H. 263 video, where the payload size varies randomly over short time periods. In this paper, we first present protocol modifications for power saving quality-of-service (QoS) enabled access points (PSQAP). We then focus on the transmission of bursty real-time H. 263 video over a PSQAP. A variety of mechanisms are proposed for this purpose, which result in various trade-offs between station and PSQAP power saving performance. Analytical models and simulation experiments are used to assess the performance of the video scheduling algorithms. The best overall performance is obtained using a mechanism that combines a novel variant of the Power Save Multi-Poll (PSMP) protocol with a scheduling mechanism based on a discrete autoregressive video prediction model.
C1 [Kholaif, Ahmad M.] Res Mot Inc, Waterloo, ON N2L 3W8, Canada.
   [Todd, Terence D.] McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8S 4K1, Canada.
   [Koutsakis, Polychronis; Lazaris, Aggelos] Tech Univ Crete, Dept Elect & Comp Engn, Khania, Greece.
C3 McMaster University; Technical University of Crete
RP Kholaif, AM (corresponding author), Res Mot Inc, Waterloo, ON N2L 3W8, Canada.
EM akholaif@rim.com; todd@mc-master.ca; polk@telecom.tuc.gr;
   alazaris@telecom.tuc.gr
OI Koutsakis, Polychronis/0000-0002-4168-0888
CR Anand M, 2005, WIREL NETW, V11, P451, DOI 10.1007/s11276-005-1768-x
   [Anonymous], 2002, MODEL SELECTION MULT
   [Anonymous], 2012, 802112012 IEEE
   [Anonymous], 1991, SIMULATION MODELING
   Ansel P, 2006, MOBILE NETW APPL, V11, P391, DOI 10.1007/s11036-006-5191-z
   *ANSI IEEE, 1999, 80211 ANSIIEEE
   Chandra S, 2003, MULTIMEDIA SYST, V9, P185, DOI 10.1007/s00530-003-0089-0
   Chandra S, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P329
   Chatziperis S, 2008, IEEE T MOBILE COMPUT, V7, P95, DOI 10.1109/TMC.2007.70706
   Chen JC, 1999, WIREL NETW, V5, P445, DOI 10.1023/A:1019136102972
   Chen Y, 2004, IEEE WCNC, P1648, DOI 10.1109/WCNC.2004.1311800
   Fan WF, 2004, WRKS LOC METRO AREA, P61
   Farbod A, 2007, IEEE T MOBILE COMPUT, V6, P960, DOI 10.1109/TMC.2007.1079
   Fitzek FHP, 2001, IEEE NETWORK, V15, P40, DOI 10.1109/65.967596
   Grilo A, 2003, IEEE WIREL COMMUN, V10, P36, DOI 10.1109/MWC.2003.1209594
   He Y, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, P154, DOI 10.1109/ICNP.2007.4375846
   *IEEE, 2005, 802LIE IEEE
   JACOBS PA, 1983, J TIME SER ANAL, V4, P19, DOI [DOI 10.1111/j.1467-9892.1983.tb00354.x, 10.1111/j.1467-9892.1983.tb00354.x]
   KHOLAIF AM, 2008, P IEEE WCNC LAS VEG
   Koutsakis P., 2006, P 49 IEEE GLOBECOM 2
   Krashinsky R, 2005, WIREL NETW, V11, P135, DOI 10.1007/s11276-004-4751-z
   Lazaris A, 2008, PERFORM EVALUATION, V65, P51, DOI 10.1016/j.peva.2007.02.004
   Lazaris A, 2008, GLOB TELECOMM CONF, DOI 10.1109/GLOCOM.2008.ECP.289
   LI Y, 2005, P IEEE QSHINE
   *MCMAST U, 2008, SOLARMESH
   Monks JP, 2001, IEEE INFOCOM SER, P219, DOI 10.1109/INFCOM.2001.916704
   Nath S., 2004, Proc. of the Second International Workshop on Mobility Management Wireless Access Protocols (MobiWac), P43
   NAVARO EAV, 2006, P IEEE CONS COMM NET, V2, P1023
   Ng SX, 2006, IEEE T CIRC SYST VID, V16, P363, DOI 10.1109/TCSVT.2006.869968
   Qiao D, 2005, IEEE INFOCOM SER, P1573
   RAMOS N, 2004, P INT WORKSH BROADB
   Tseng YC, 2003, COMPUT NETW, V43, P317, DOI 10.1016/S1389-1286(03)00284-6
   UNION IT, 1996, H263 ITUTN
   van der Schaar M, 2006, IEEE T MOBILE COMPUT, V5, P755, DOI 10.1109/TMC.2006.81
   VERBIEST W, 1988, IEEE J SEL AREA COMM, V6, P1623, DOI 10.1109/49.12890
   Zhang F, 2006, IEEE T MOBILE COMPUT, V5, P144, DOI 10.1109/TMC.2006.25
NR 36
TC 12
Z9 12
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2010
VL 12
IS 2
BP 142
EP 153
DI 10.1109/TMM.2009.2037380
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 573OA
UT WOS:000275922000005
DA 2024-07-18
ER

PT J
AU Jin, X
   Cheng, KL
   Chan, SHG
AF Jin, Xing
   Cheng, Kan-Leung
   Chan, S. -H. Gary
TI Island Multicast: Combining IP Multicast With Overlay Data Distribution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Application-level multicast; IP multicast; island multicast; overlay
   data distribution; overlay multicast
AB Traditional overlay protocols use unicast connections to form delivery trees. While it can achieve global multicast across the Internet, it is not as efficient as IP multicast. In this paper, we integrate IP multicast into overlay data distribution to improve delivery efficiency. We investigate island multicast where unicast connections are used to connect multicast domains and IP multicast is used within multicast domains. We first explore a centralized island multicast protocol (termed CIM), which relies on a central server to construct a delivery tree. We then study a distributed protocol (termed DIM), where hosts can distributedly join islands and form a delivery tree. We study the key issues in both protocols. We also discuss how to apply these protocols to media streaming applications.
   We have evaluated both protocols on Internet-like topologies. We have also implemented a prototype for CIM and tested it on PlanetLab. The results show that our approaches can significantly improve network performance as compared to pure overlay protocols. Our study shows that it is important to consider local multicast capability when designing overlay protocols.
C1 [Jin, Xing] Oracle USA Inc, Syst Technol Grp, Redwood Shores, CA 94065 USA.
   [Cheng, Kan-Leung] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
   [Chan, S. -H. Gary] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Kowloon, Hong Kong, Peoples R China.
C3 Oracle; University System of Maryland; University of Maryland College
   Park; Hong Kong University of Science & Technology
RP Jin, X (corresponding author), Oracle USA Inc, Syst Technol Grp, Redwood Shores, CA 94065 USA.
EM xing.jin@oracle.com; klcheng@cs.umd.edu; gchan@cse.ust.hk
OI Chan, Gary Shueng Han/0000-0003-4207-764X
FU Hong Kong Special Administrative Region, China [611107]; Hong Kong
   Innovation Technology Fund [ITS/013/08]
FX This work was supported in part by the General Research Fund from the
   Research Grant Council of the Hong Kong Special Administrative Region,
   China (611107), and in part by the Hong Kong Innovation Technology Fund
   (ITS/013/08).
CR [Anonymous], P ACM SIGCOMM
   Banerjee S, 2002, ACM SIGCOMM COMP COM, V32, P205, DOI 10.1145/964725.633045
   Cheng KL, 2005, IEEE ICC, P1299
   Cheuk KWR, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P1441, DOI 10.1109/ICC.2004.1312750
   Chu YH, 2002, IEEE J SEL AREA COMM, V20, P1456, DOI 10.1109/JSAC.2002.803066
   DABEK F, 2004, P 2004 C APPL TECHN, P15
   DEERING SE, 1988, COMPUT COMMUN REV, V18, P55
   Diot C, 2000, IEEE NETWORK, V14, P78, DOI 10.1109/65.819174
   ERIKSSON H, 1994, COMMUN ACM, V37, P54, DOI 10.1145/179606.179627
   GANJAM A, 2004, P ACM INT WORKSH NET
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   JIN X, 2007, HINDAWI J ADV MULTIM
   Jin X, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P913, DOI 10.1109/ICME.2006.262668
   Li B, 2003, IEEE NETWORK, V17, P24
   Liebeherr J, 2002, IEEE J SEL AREA COMM, V20, P1472, DOI 10.1109/JSAC.2002.803067
   Magharei N., 2007, P IEEE INFOCOM 07 MA
   Ng ISE, 2002, IEEE INFOCOM SER, P170, DOI 10.1109/INFCOM.2002.1019258
   Park CH, 2001, IEEE COMMUN LETT, V5, P4, DOI 10.1109/4234.901807
   Pendarakis D, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX SYMPOSIUM ON INTERNET TECHNOLOGIES AND SYSTEMS, P49
   Rosenberg J., 2003, 3489 RFC
   Savage S, 1999, COMP COMM R, V29, P289, DOI 10.1145/316194.316233
   SHI S, 2001, P 11 INT WORKSH NETW, P83
   THALER D, 2004, IPV4 AUTOMA IN PRESS
   Wang WJ, 2005, IEEE INFOCOM SER, P2124
   Yiu WPK, 2005, IEEE ICC, P1304
   Zegura EW, 1996, IEEE INFOCOM SER, P594, DOI 10.1109/INFCOM.1996.493353
   Zhang BC, 2006, COMPUT NETW, V50, P781, DOI 10.1016/j.comnet.2005.07.016
   Zhang BC, 2002, IEEE INFOCOM SER, P1366, DOI 10.1109/INFCOM.2002.1019387
NR 28
TC 12
Z9 15
U1 1
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2009
VL 11
IS 5
BP 1024
EP 1036
DI 10.1109/TMM.2009.2021804
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 474KN
UT WOS:000268282300019
DA 2024-07-18
ER

PT J
AU Gilbert, E
   Karahalios, K
AF Gilbert, Eric
   Karahalios, Karrie
TI Using Social Visualization to Motivate Social Production
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Open source; remote collaboration; social visualization; visualization
AB In this paper we argue that social visualization can motivate contributors to social production projects, such as Wikipedia and open source development. As evidence, we present CodeSaw, a social visualization of open source software development that we studied with real open source communities. CodeSaw mines open source archives to visualize group dynamics that currently lie buried in textual databases. Furthermore. CodeSaw becomes an active social space itself by supporting comments; directly inside the visualization. To demonstrate CodeSaw, We apply it to a popular open source project, showing how the visualization reveals group dynamics and individual roles. The paper concludes by presenting evidence that CodeSaw, and social visualization more generally, can motivate contributors to social production projects if the visualization leaves the laboratory and makes it to the community visualized.
C1 [Gilbert, Eric; Karahalios, Karrie] Univ Illinois, Siebel Ctr Comp Sci, Urbana, IL 61801 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign
RP Gilbert, E (corresponding author), Univ Illinois, Siebel Ctr Comp Sci, Urbana, IL 61801 USA.
EM egilber2@cs.uiuc.edu; kkarahal@cs.uiuc.edu
FU National Science Foundation [NSF 0643502]
FX Manuscript received June 01, 2008; revised November 22, 2008. Current
   version published March 18, 2009. This work was supported in pail by the
   National Science Foundation under NSF 0643502. The associate editor
   coordinating the review of this manuscript and approving, it for
   publication was Dr. Lexing Xie.
CR [Anonymous], 1999, 1 MONDAY
   [Anonymous], 2004, P ACM C HUMAN FACTOR, DOI [DOI 10.1145/985692.985765, 10.1145/985692.985765]
   [Anonymous], REDISCOVERING CTR
   BAKER MJ, 1995, J VIS LANG COMPUT
   BEDERSON BB, 1994, 7 ANN ACM S US INT S, P17
   Benkler Y, 2002, YALE LAW J, V112, P369, DOI 10.2307/1562247
   Benkler Yochai, 2006, WEALTH NETWORKS SOCI
   Biehl JT, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1313
   Deci EL, 1999, PSYCHOL BULL, V125, P627, DOI 10.1037/0033-2909.125.6.627
   DONATH JS, 1995, 3 ACM INT C MULT, P99
   Ducheneaut N., 2005, Computer Supported Cooperative Work: The Journal of Collaborative Computing, V14, P323, DOI 10.1007/s10606-005-9000-1
   EICK SG, 1992, IEEE T SOFTWARE ENG, V18, P957, DOI 10.1109/32.177365
   Erickson T., 2000, ACM Transactions on Computer-Human Interaction, V7, P59, DOI 10.1145/344949.345004
   FISHER D, 2004, SIGCHI C HUM FACT CO, P551
   Froehlich J, 2004, PROC INT CONF SOFTW, P387, DOI 10.1109/ICSE.2004.1317461
   GILBERT E, 2007, INTERACT 2007, P303
   Gutwin C., 2004, Computer Supported Cooperative Work Conference Proceedings, P72, DOI 10.1145/1031607.1031621
   Heer J, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1029
   Herbsleb J. D., 1999, Proceedings of the 1999 International Conference on Software Engineering (IEEE Cat. No.99CB37002), P85, DOI 10.1109/ICSE.1999.840998
   KARABALIOS K, 2006, P ACM COMP HUM INT, P1667
   Kelty C., 2001, First Monday, V6
   Kerr Bernard., 2006, CHI EXTENDED ABSTRAC, P93, DOI DOI 10.1145/1125451.1125476
   KRAUSZ MM, 1995, SHOCK, V3, P69, DOI 10.1097/00024382-199501000-00012
   LaToza T. D., 2006, 28th International Conference on Software Engineering Proceedings, P492, DOI 10.1145/1134285.1134355
   Levine Donald., 1971, Georg Simmel: On Individuality and Social Forms
   LUDFORD PJ, 2004, SIGCHI C HUM FACT CO, P631
   MEDYNSKIY YE, 2006, SIGCHI C HUM FACT CO, P513
   Mockus A, 2002, ACM T SOFTW ENG METH, V11, P309, DOI 10.1145/567793.567795
   NAKAKOJI K, 2002, INT WORKSH PRINC SOF
   NARDI BA, 2001, COMMUN ACM, P89
   *NASA, 2007, CLICKW RES
   ORGELL E, 2007, MORE 1 1 MILLION DEV
   Osterloh M, 2000, ORGAN SCI, V11, P538, DOI 10.1287/orsc.11.5.538.15204
   REEVES S, 2005, SIGCHI C HUM FACT CO, P741
   SACK W, 2000, 33 HAW INT C SYST SC, V3
   Singer J, 1998, PROC IEEE INT CONF S, P139, DOI 10.1109/ICSM.1998.738502
   Smith M., 1999, COMMUNITIES CYBERSPA
   SMITH MA, 2001, SIGCHI C HUM FACT CO, P136
   VIEGAS F, 2007, P 2007 IEEE S INF VI, P1121
   VIEGAS FB, 2006, SIGCHI C HUM FACT CO, P979
   VIEGAS FB, 2007, HAW INT C SYST SCI
   VIEGAS FB, 2004, HAW INT C SYST SCI
   Wattenberg M, 2005, INFOVIS 05: IEEE Symposium on Information Visualization, Proceedings, P1, DOI 10.1109/INFVIS.2005.1532122
   YAMAUCHI Y, 2000, 2000 ACM C COMP SUPP, P329
   2005, BRAZIL ADOPTS OPEN S
NR 46
TC 17
Z9 17
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2009
VL 11
IS 3
BP 413
EP 421
DI 10.1109/TMM.2009.2012916
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 425JH
UT WOS:000264632300007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lin, CK
   Lu, MT
   Yeh, SC
   Chen, HH
AF Lin, Chang-Kuan
   Lu, Meng-Ting
   Yeh, Shiann-Chang
   Chen, Homer H.
TI Video Streaming Over In-Home Power Line Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT 11th IEEE International Symposium on Power Line Communications and its
   Applications (ISPLC 2007)
CY MAR 26-28, 2007
CL Pisa, ITALY
SP IEEE, intel, InTellon, Power Plus Commun, Siemens, IEEE Commun Soc, AEIT
DE Bandwidth estimation; power line network; scalable video; video
   streaming
ID TCP; BANDWIDTH; NOISE
AB The deployment of power line communication technology for broadband video streaming remains a challenge because power lines are not originally designed for signal transmission. Scalable video is a viable approach that can cope with the bandwidth fluctuation of power line communication networks provided that the bandwidth information is available. In this paper we first investigate how the interference caused by electrical appliances or power supplies affects the power line channel bandwidth and packet transmission. Then we take the obtained characteristics of in-home power line network into account in the design of a simple but effective heuristic-based application-layer bandwidth estimation scheme, for which the cutoff rate is estimated from the packet size and the physical-layer data rates. Experimental results show that the proposed approach can effectively combat the noise interference and deliver robust video streaming over power line.
C1 [Chen, Homer H.] Natl Taiwan Univ, Grad Inst Commun Engn, Dept Elect Engn, Taipei 10617, Taiwan.
   [Chen, Homer H.] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10617, Taiwan.
C3 National Taiwan University; National Taiwan University
RP Lin, CK (corresponding author), Natl Taiwan Univ, Grad Inst Commun Engn, Dept Elect Engn, Taipei 10617, Taiwan.
EM homer@cc.ee.ntu.edu.tw
OI Chen, Homer/0000-0002-8795-1911
CR BOGGS DR, 1995, P ACM SIGCOMM JAN, P123
   Capone A, 2004, IEEE T MOBILE COMPUT, V3, P129, DOI 10.1109/TMC.2004.5
   Casetti C, 2002, WIREL NETW, V8, P467, DOI 10.1023/A:1016590112381
   Dovrolis C, 2004, IEEE ACM T NETWORK, V12, P963, DOI 10.1109/TNET.2004.838606
   Götz M, 2004, IEEE COMMUN MAG, V42, P78, DOI 10.1109/MCOM.2004.1284933
   Hooijen OG, 1998, IEEE T ELECTROMAGN C, V40, P331, DOI 10.1109/15.736218
   Huang Hsiu-Wen, 2007, Taiwanese Journal of Agricultural Chemistry and Food Science, V45, P1
   Jain M, 2003, IEEE ACM T NETWORK, V11, P537, DOI 10.1109/TNET.2003.815304
   JAIN M, 2005, P 2005 ACM SIGMETRIC, P265
   Kapoor R, 2004, ACM SIGCOMM COMP COM, V34, P67, DOI 10.1145/1030194.1015476
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   Lin CK, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON POWER LINE COMMUNICATIONS AND ITS APPLICATIONS, P412
   Luby M, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON POWER LINE COMMUNICATIONS AND ITS APPLICATIONS, P430
   Meng H, 2005, IEEE T POWER DELIVER, V20, P630, DOI 10.1109/TPWRD.2005.844349
   Ohmi S, 2004, CCNC 2004: 1ST IEEE CONSUMER COMMUNICATIONS AND NETWORKING CONFERENCE, PROCEEDINGS, P295, DOI 10.1109/CCNC.2004.1286875
   REICHEL J, 2006, JTC1SC29WG11 ISOIEC
   Ribeiro V.J., 2003, PASSIVE ACTIVE MEASU
   Schierl T, 2007, IEEE T CIRC SYST VID, V17, P1204, DOI 10.1109/TCSVT.2007.905528
   SHOCH JF, 1980, COMMUN ACM, V23, P711, DOI 10.1145/359038.359044
   Strauss Jacob., 2003, IMC 03, P39, DOI [DOI 10.1145/948205.948211, 10.1145/948205.948211]
   Tsuzuki S, 2006, IEICE T FUND ELECTR, VE89A, P3006, DOI 10.1093/ietfec/e89-a.11.3006
   WIEGAND T, 2006, JTC1SC29WG11 ISOIEC
   Yang GH, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P707
   Zimmermann M, 2002, IEEE T ELECTROMAGN C, V44, P249, DOI 10.1109/15.990732
NR 24
TC 3
Z9 4
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2009
VL 11
IS 3
BP 523
EP 534
DI 10.1109/TMM.2009.2012933
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 425JH
UT WOS:000264632300017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Marolt, M
AF Marolt, Matija
TI A Mid-Level Representation for Melody-Based Retrieval in Audio
   Collections
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio collections; information retrieval; melody; music
ID MUSIC; SYSTEM
AB Searching audio collections using high-level musical descriptors is a difficult problem, due to the lack of reliable methods for extracting melody, harmony, rhythm, and other such descriptors from unstructured audio signals. In this paper, we present a novel approach to melody-based retrieval in audio collections. Our approach supports audio, as well as symbolic queries and ranks results according to melodic similarity to the query. We introduce a beat-synchronous melodic representation consisting of salient melodic lines, which are extracted from the analyzed audio signal. We propose the use of a 2-D shift-invariant transform to extract shift-invariant melodic fragments from the melodic representation and demonstrate how such fragments can be indexed and stored in a song database. An efficient search algorithm based on locality-sensitive hashing is used to perform retrieval according to similarity of melodic fragments. On the cover song detection task, good results are achieved for audio, as well as for symbolic queries, while fast retrieval performance makes the proposed system suitable for retrieval in large databases.
C1 Univ Ljubljana, Fac Comp & Informat Sci, Ljubljana 1000, Slovenia.
C3 University of Ljubljana
RP Marolt, M (corresponding author), Univ Ljubljana, Fac Comp & Informat Sci, Trzaska 25, Ljubljana 1000, Slovenia.
EM matija.marolt@fri.uni-lj.si
FU Slovenian Government-Founded R&D projects EthnoMuse: multimedia digital
   archive of Slovenian folk music and folk dance culture; EthnoCatalogue:
   creating semantic descriptions of Slovene folk song and music based on
   melodic and metro-rhythmic analysis
FX Manuscript received November 13, 2007: revised July 29, 2008. Current
   version published December 10, 2008. This work was supported in part by
   the Slovenian Government-Founded R&D projects EthnoMuse: multimedia
   digital archive of Slovenian folk music and folk dance culture, and by
   EthnoCatalogue: creating semantic descriptions of Slovene folk song and
   music based on melodic and metro-rhythmic analysis. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Bangalore S. Manjunath.
CR [Anonymous], MUSIC INFORM RETRIEV
   [Anonymous], 2002, P 34 ACM S THEOR COM
   [Anonymous], 1990, STUDYING POPULAR MUS
   Bello J.P., 2007, ISMIR, P239
   Berenzweig A, 2004, COMPUT MUSIC J, V28, P63, DOI 10.1162/014892604323112257
   CASEY M, 2006, P 7 INT C MUS INF RE
   Cohen D, 2000, ENG OPTIMIZ, V33, P1, DOI 10.1080/03052150008940909
   COOPER M, 2002, P 3 INT C MUS INF RE
   Dixon S, 2001, J NEW MUSIC RES, V30, P39, DOI 10.1076/jnmr.30.1.39.7119
   Downie J.Stephen., 1999, EVALUATING SIMPLE AP
   Ellis DPW, 2007, INT CONF ACOUST SPEE, P1429
   Haitsma J, 2003, J NEW MUSIC RES, V32, P211, DOI 10.1076/jnmr.32.2.211.16746
   Heikkilä J, 2004, IEEE SIGNAL PROC LET, V11, P545, DOI 10.1109/LSP.2004.827915
   Klapuri A, 2006, SIGNAL PROCESSING ME
   Klapuri A., 2006, P 7 INT C MUS INF RE
   Levitin D. J., 1999, MUSIC COGNITION COMP, P105, DOI [10.7551/mitpress/4808.003.0019, DOI 10.7551/MITPRESS/4808.003.0019]
   Marolt Matija, 2006, ISMIR, P280
   Muller M, 2005, P 6 INT C MUSIC INFO, P288
   Pampalk E, 2004, COMPUT MUSIC J, V28, P49, DOI 10.1162/014892604323112248
   PARNCUTT R, 1994, MUSIC PERCEPT, V11, P55
   Poliner GE, 2007, IEEE T AUDIO SPEECH, V15, P1247, DOI 10.1109/TASL.2006.889797
   SELFRIDGEFIELD E, 1998, MELODIC SIMULARITY C
   Serra J., 2007, Music similarity based on sequences of descriptors: tonal features applied to audio cover song identification
   SERRA J, 2007, P 8 INT C MUS INF RE
   SERRA X, 1990, COMPUT MUSIC J, V14, P12, DOI 10.2307/3680788
   Shalev-Shwartz S., 2002, Proceedings of SIGIR 2002. Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P331
   Timoney J., 2004, P 7 INT C DIG AUD EF, P177
   Typke Rainer., 2007, Music Retrieval based on Melodic Similarity
   Uitdenbogerd AL, 2004, J AM SOC INF SCI TEC, V55, P1053, DOI 10.1002/asi.20057
NR 29
TC 26
Z9 34
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2008
VL 10
IS 8
BP 1617
EP 1625
DI 10.1109/TMM.2008.2007293
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 390KS
UT WOS:000262163600016
DA 2024-07-18
ER

PT J
AU Wang, K
   Lavoué, G
   Denis, F
   Baskurt, A
AF Wang, Kai
   Lavoue, Guillaume
   Denis, Florence
   Baskurt, Atilla
TI A Comprehensive Survey on Three-Dimensional Mesh Watermarking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D mesh; attack; authentication; copyright protection; digital
   watermarking; robustness
ID MULTIRESOLUTION ANALYSIS; FRAGILE WATERMARKING; 3D MESH; OBJECTS; SCHEME
AB Three-dimensional (3-D) meshes have been used more and more in industrial, medical and entertainment applications during the last decade. Many researchers, from both the academic and the industrial sectors, have become aware of their intellectual property protection and authentication problems arising with their increasing use. This paper gives a comprehensive survey on 3-D mesh watermarking, which is considered an effective solution to the above two emerging problems. Our survey covers an introduction to the relevant state of the art, an attack-centric investigation, and a list of existing problems and potential solutions. First, the particular difficulties encountered while applying watermarking on 3-D meshes are discussed. Then we give a presentation and an analysis of the existing algorithms by distinguishing them between fragile techniques and robust techniques. Since attacks play an important role in the design of 3-D mesh watermarking algorithms, we also provide an attack-centric viewpoint of this state of the art. Finally, some future working directions are pointed out especially on the ways of devising robust and blind algorithms and on some new probably promising watermarking feature spaces.
C1 [Wang, Kai; Lavoue, Guillaume; Baskurt, Atilla] Inst Natl Sci Appl, LIRIS, CNRS, UMR 5205, F-69621 Villeurbanne, France.
   [Denis, Florence] Univ Lyon 1, CNRS, UMR 5205, LIRIS, F-69622 Villeurbanne, France.
C3 Institut National des Sciences Appliquees de Lyon - INSA Lyon; Centre
   National de la Recherche Scientifique (CNRS); Centre National de la
   Recherche Scientifique (CNRS); Universite Claude Bernard Lyon 1;
   Institut National des Sciences Appliquees de Lyon - INSA Lyon
RP Wang, K (corresponding author), Inst Natl Sci Appl, LIRIS, CNRS, UMR 5205, F-69621 Villeurbanne, France.
EM kwang@liris.cnrs.fr; glavoue@liris.cnrs.fr; fdenis@liris.cnrs.fr;
   abaskurt@liris.cnrs.fr
FU China Scholarship Council of the Chinese government and the region of
   Rhone-Alpes, France
FX Manuscript received May 21, 2007; revised July 25, 2008. Current version
   published December 10, 2008. This work was supported in part by the
   China Scholarship Council of the Chinese government and the region of
   Rhone-Alpes, France, under the contract of ISLE cluster's SYSECUR
   project. The associate editor coordinating the review of this manuscript
   and approving it for publication was Dr. Ishfaq Ahmad.
CR Alface PR, 2007, IEEE IMAGE PROC, P2717
   Alface PR, 2005, IEEE IMAGE PROC, P633
   Alface PR, 2005, PROC SPIE, V5681, P230, DOI 10.1117/12.589129
   Alghoniemy M, 2004, IEEE T IMAGE PROCESS, V13, P145, DOI 10.1109/TIP.2004.823831
   Alliez P, 2008, MATH VIS, P53, DOI 10.1007/978-3-540-33265-7_2
   [Anonymous], 2000, Digital Watermarking
   ASHOURIAN M, 2003, P IEEE REG 10 ANN IN, V1, P428
   Aspert N, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P705, DOI 10.1109/ICME.2002.1035879
   Attene M, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P14
   Barni M., 2004, WATERMARKING SYSTEMS, V1st
   Benedens O, 1999, IEEE COMPUT GRAPH, V19, P46, DOI 10.1109/38.736468
   Benedens O, 2003, PROC SPIE, V5020, P337, DOI 10.1117/12.477299
   Benedens O, 2000, COMPUT GRAPH FORUM, V19, pC199, DOI 10.1111/1467-8659.00412
   Benedens O., 1999, P MULT SEC WORKSH AC, V99, P95
   Bennour J, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1113, DOI 10.1109/ICME.2006.262730
   Bors AG, 2006, IEEE T IMAGE PROCESS, V15, P687, DOI 10.1109/TIP.2005.863116
   Botsch M, 2007, P ACM SIGGRAPH COURS
   Cayre F, 2003, IEEE T SIGNAL PROCES, V51, P939, DOI 10.1109/TSP.2003.809380
   Cayre F, 2003, SIGNAL PROCESS-IMAGE, V18, P309, DOI 10.1016/S0923-5965(02)00147-9
   Cheng YM, 2006, VISUAL COMPUT, V22, P845, DOI 10.1007/s00371-006-0069-4
   Cho JW, 2007, IEEE T SIGNAL PROCES, V55, P142, DOI 10.1109/TSP.2006.882111
   Cho WH, 2005, LECT NOTES COMPUT SC, V3304, P259
   Chou CM, 2006, COMPUT AIDED DESIGN, V38, P1154, DOI 10.1016/j.cad.2006.06.009
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Corsini M, 2007, IEEE T MULTIMEDIA, V9, P247, DOI 10.1109/TMM.2006.886261
   Cox I., 2001, Digital Watermarking
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Dodgson Neil., 2005, Advances in Multiresolution for Geometric Modelling
   Floater MS, 2005, MATH VIS, P157, DOI 10.1007/3-540-26808-1_9
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   Guskov I, 1999, COMP GRAPH, P325, DOI 10.1145/311535.311577
   Han Sae Song, 2004, Proceedings of 2004 International Symposium on Intelligent Signal Processing And Communication Systems ISPACS 2004 (IEEE Cat. No.04EX910), P272
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   Jin Jian-Qiu, 2004, J Zhejiang Univ Sci, V5, P251, DOI 10.1631/jzus.2004.0251
   Kalivas A, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P637
   KANAI S, 1998, P 6 IFIP WG 5 2 GEO, P296
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   Kim HS, 2003, IEEE T CIRC SYST VID, V13, P766, DOI 10.1109/TCSVT.2003.815955
   Kim MS, 2005, LECT NOTES COMPUT SC, V3710, P313
   KWON K, 2003, IEEE INT C IMAGE PRO, V2, P499
   LAVOUE G, 2007, P ACM SIGGRAPH S APP, P57
   LAVOUE G, 2006, P SPIE IS T ELECT IM, V6312
   Lavoué G, 2007, COMPUT GRAPH-UK, V31, P480, DOI 10.1016/j.cag.2007.01.022
   Lee GW, 2005, IEICE T FUND ELECTR, VE88A, P1512, DOI 10.1093/ietfec/e88-a.6.1512
   Li L, 2004, COMPUT GRAPH-UK, V28, P981, DOI 10.1016/j.cag.2004.08.002
   Lin HYS, 2005, IEEE T MULTIMEDIA, V7, P997, DOI 10.1109/TMM.2005.858412
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Lounsbery M, 1997, ACM T GRAPHIC, V16, P34, DOI 10.1145/237748.237750
   Maret Y, 2004, P MULT SEC WORKSH, P68
   MUROTANI K, 2003, P IMA INT C MATH SUR, P85
   Nielson G. M., 1989, Mathematical Methods in Computer Aided Geometric Design, P445
   Novotni M, 2004, COMPUT AIDED DESIGN, V36, P1047, DOI 10.1016/j.cad.2004.01.005
   Ohbuchi R, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P261, DOI 10.1145/266180.266377
   Ohbuchi R, 2002, COMPUT GRAPH FORUM, V21, P373, DOI 10.1111/1467-8659.t01-1-00597
   Ohbuchi R, 1998, COMPUT COMMUN, V21, P1344, DOI 10.1016/S0140-3664(98)00202-3
   Ohbuchi R., 2001, Graphics Interface, P9
   Praun E, 1999, COMP GRAPH, P49, DOI 10.1145/311535.311540
   Ricard J, 2005, PATTERN RECOGN LETT, V26, P2174, DOI 10.1016/j.patrec.2005.03.030
   RICARD J, 2005, THESIS I NAT SCI APP
   Saupe D., 2001, Pattern Recognition. 23rd DAGM Symposium. Proceedings (Lecture Notes in Computer Science Vol.2191), P392
   Schruder P., 1995, Proc. 22nd Ann. Conf. Comput. Graphics Interactive Techniques (SIGGRAPH'95), P161
   Tuzikov AV, 2003, PATTERN RECOGN, V36, P2521, DOI 10.1016/S0031-3203(03)00127-4
   UCCHEDDU F, 2004, P 2004 MULT SEC WORK, P143
   Uccheddu F., 2004, P 10 INTERNALTIONAL, P934
   Valette S, 2004, IEEE T VIS COMPUT GR, V10, P113, DOI 10.1109/TVCG.2004.1260763
   Vranic DV, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P293, DOI 10.1109/MMSP.2001.962749
   Wagner M. G., 2000, Proceedings Geometric Modeling and Processing 2000. Theory and Applications, P201, DOI 10.1109/GMAP.2000.838252
   Wang K., 2008, EUROGRAPHICS SHORT P, P5
   Wang K, 2007, LECT NOTES COMPUT SC, V4567, P50
   Wang K, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1235
   Wu HH, 2005, Computer Graphics, Imaging and Vision: New Trends, P117
   Wu JH, 2005, VISUAL COMPUT, V21, P848, DOI 10.1007/s00371-005-0311-5
   Yeo BL, 1999, IEEE COMPUT GRAPH, V19, P36, DOI 10.1109/38.736467
   Yin KK, 2001, COMPUT GRAPH-UK, V25, P409, DOI 10.1016/S0097-8493(01)00065-6
   Yu ZQ, 2003, PATTERN RECOGN, V36, P2603, DOI 10.1016/S0031-3203(03)00086-4
   Zafeiriou S, 2005, IEEE T VIS COMPUT GR, V11, P596, DOI 10.1109/TVCG.2005.71
   ZHANG H., 2007, P EUROGRAPHICS STATE, P1, DOI DOI 10.1109/IPDPS.2007.370248
NR 78
TC 109
Z9 123
U1 0
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2008
VL 10
IS 8
BP 1513
EP 1527
DI 10.1109/TMM.2008.2007350
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 390KS
UT WOS:000262163600008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yu, HM
   Tsai, WH
   Wang, HM
AF Yu, Hung-Ming
   Tsai, Wei-Ho
   Wang, Hsin-Min
TI A Query-by-Singing System for Retrieving Karaoke Music
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bayesian information criterion; dynamic time warping; karaoke; music
   information retrieval; query-by-singing
ID AUDIO
AB This paper investigates the problem of retrieving karaoke music using query-by-singing techniques. Unlike regular CD music, where the stereo sound involves two audio channels that usually sound the same, karaoke music encompasses two distinct channels in each track: one is a mixture of the lead vocals and background accompaniment, and the other consists of accompaniment only. Although the two audio channels are distinct, the accompaniments in the two channels often resemble each other. We exploit this characteristic to: i) infer the background accompaniment for the lead vocals from the accompaniment-only channel, so that the main melody underlying the lead vocals can be extracted more effectively; and ii) detect phrase onsets based on the Bayesian information criterion (BIC) to predict the onset points of a song where a user's sung query may begin, so that the similarity between the melodies of the query and the song can be examined more efficiently. To further refine extraction of the main melody, we propose correcting potential errors in the estimated sung notes by exploiting a composition characteristic of popular songs whereby the sung notes within a verse or chorus section usually vary no more than two octaves. In addition, to facilitate an efficient and accurate search of a large music database, we employ multiple-pass dynamic time warping (DTW) combined with multiple-level data abstraction (MLDA) to compare the similarities of melodies. The results of experiments conducted on a karaoke database comprised of 1071 popular songs demonstrate the feasibility of query-by-singing retrieval for karaoke music.
C1 [Yu, Hung-Ming] Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
   [Tsai, Wei-Ho] Natl Taipei Univ Technol, Dept Elect Engn, Taipei, Taiwan.
   [Tsai, Wei-Ho] Natl Taipei Univ Technol, Grad Inst Comp & Commun Engn, Taipei, Taiwan.
C3 Academia Sinica - Taiwan; National Taipei University of Technology;
   National Taipei University of Technology
RP Yu, HM (corresponding author), Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
EM donny@iis.sinica.edu.tw; whtsai@ntut.edu.tw; whm@iis.sinica.edu.tw
RI Wang, Hsin-Min/ABA-8747-2020
OI Wang, Hsin-Min/0000-0003-3599-5071
FU National Digital Archives Program (NDAP, Taiwan); National Science
   Council, Taiwan [NSC94-2422-H-001-007, NSC95-2422-H-001 008,
   NSC95-2422-H-001-031]
FX Manuscript received January 09, 2008; revised July 25, 2008. Current
   version published December 10, 2008. This work was supported in part by
   the National Digital Archives Program (NDAP, Taiwan) sponsored by the
   National Science Council, Taiwan, under Grants NSC94-2422-H-001-007,
   NSC95-2422-H-001 008, and NSC95-2422-H-001-031. The associate editor
   coordinating the review of this paper and approving it for publication
   was Prof. Alan Hanjalic.
CR [Anonymous], 2004, KDD WORKSHOP MINING
   [Anonymous], P INT C MUS INF RETR
   Dannenberg R.B., 2003, Proceedings of the International Conference on Music Information Retrieval, P41
   Dannenberg RB, 2007, J AM SOC INF SCI TEC, V58, P687, DOI 10.1002/asi.20532
   De Mulder T, 2006, IEEE T MULTIMEDIA, V8, P728, DOI 10.1109/TMM.2006.876291
   Doraisamy S, 2003, J INTELL INF SYST, V21, P53, DOI 10.1023/A:1023553801115
   DORAISAMY S, 2001, P INT S MUS INF RETR
   Duda A., 2007, P INT C MUS INF RETR
   EGGINK J, 2004, P INT C MUS INF RETR
   FENG Y, 2003, P ACM C RES DEV INF
   FOOTE J, 2000, P INT S MUS INF RETR
   FOOTE J, 1999, P ACM MULT
   GHIAS A, 1995, P ACM INT C MULT
   GOTO M, 2001, P IEEE INT C AC SPEE
   HAYKIN S., 2002, ADAPTIVE FILTER THEO
   HU N, 2002, P ACM IEEE JOINT C D
   Jang J.S., 2001, P ACM INT C MULT
   Keogh E. J., 2000, P ACM SIGKDD
   KOSUGI N, 2000, P ACM INT C MULT
   LIU CC, 1999, P IEEE INT C MULT CO
   Meek C, 2003, J INTELL INF SYST, V21, P9, DOI 10.1023/A:1023549700206
   MO JS, 1999, P WORKSH KNOWL DAT E
   NISHIMURA T, 2001, P INT S MUS INF RETR
   Pardo B, 2006, IEEE SIGNAL PROC MAG, V23, P126, DOI 10.1109/MSP.2006.1628889
   Pardo B, 2004, J AM SOC INF SCI TEC, V55, P283, DOI 10.1002/asi.10373
   PAUWS S, 2003, P INT C MUS INF RETR
   Pickens J, 2003, J NEW MUSIC RES, V32, P223, DOI 10.1076/jnmr.32.2.223.16742
   PISZCZALSKI M, 1979, J ACOUST SOC AM, V66, P710, DOI 10.1121/1.383221
   SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136
   Shifrin J., 2003, P INT C MUS INF RETR
   Song J., 2002, P INT C MUS INF RETR
   TSAI WH, 2004, P IEEE INT C MULT EX
   TYPKE R, 2004, P ACM C MULT
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   WIDROW B, 1975, P IEEE, V63, P12
   YU HM, 2005, P AS INF RETR S
   ZHU Y, 2005, P IEEE INT C MULT EX
NR 37
TC 31
Z9 44
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2008
VL 10
IS 8
BP 1626
EP 1637
DI 10.1109/TMM.2008.2007345
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 390KS
UT WOS:000262163600017
DA 2024-07-18
ER

PT J
AU Park, J
   Ko, H
AF Park, Junho
   Ko, Hanseok
TI Real-Time Continuous Phoneme Recognition System Using Class-Dependent
   Tied-Mixture HMM With HBT Structure for Speech-Driven Lip-Sync
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Head-body-tail HMM; phoneme recognition; real-time lip-sync
AB This work describes a real-time lip-sync method using which an avatar's lip shape is synchronized with the corresponding speech signal. Phoneme recognition is generally regarded as an important task in the operation of a real-time lip-sync system. In this work, the use of the Head-Body-Tail (HBT) model is proposed for the purpose of more efficiently recognizing phonemes which are variously uttered due to co-articulation effects. The HBT model effectively deals with the transition parts of context-dependent models for small-sized vocabulary tasks. These models provide better recognition performance than general context-dependent or context-independent models for the task of digit or vowel recognition. Moreover, each phoneme is categorized into one among four classes and the class-dependent codebook is generated to further improve the performance. Additionally, for the clear representation of the context dependency information in the transient parts, some Gaussians are excluded from class-dependent codebook. The proposed method leads to a lip-sync system that performs at a level that is similar to previous designs based on HBT and continuous hidden Markov models (CHMMs). However, our method reduces the number of model parameters by one-third and enables real-time operation.
C1 [Park, Junho; Ko, Hanseok] Korea Univ, Sch Elect Engn, Seoul 136713, South Korea.
C3 Korea University
RP Park, J (corresponding author), Univ Cambridge, Dept Engn, Speech Grp, Cambridge CB2 1PZ, England.
EM jhp33@eng.cam.ac.uk; hsko@korea.ac.kr
OI Ko, Hanseok/0000-0002-8744-4514
FU Ministry of Science and Technology [R01-2006000-11162-0(2008)]
FX Manuscript received June 25, 2007: revised July 07, 2008. Current
   version published November 17, 2008. This work was supported by Grant
   R01-2006000-11162-0(2008) from the Basic Research Program Korea Science
   and Engineering Foundation of the Ministry of Science and Technology.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Hanseok Ko.
CR ANDREOBRECHT R, 1988, IEEE T ACOUST SPEECH, V36, P29, DOI 10.1109/29.1486
   [Anonymous], 2002, CAMBRIDGE U ENG DEP
   BAILLY G, 2002, P ICSLP, P1453
   BELLEGARDA JR, 1990, IEEE T ACOUST SPEECH, V38, P2033, DOI 10.1109/29.61531
   Brand M, 1999, COMP GRAPH, P21, DOI 10.1145/311535.311537
   Brandt A. V., 1983, Proceedings of ICASSP 83. IEEE International Conference on Acoustics, Speech and Signal Processing, P1017
   Chen T, 1998, P IEEE, V86, P837, DOI 10.1109/5.664274
   CHIO K, 2001, J VLSI SIGNAL PROC, V29, P51
   CHOU W, 1994, P ICSLP 94, P439
   Cohen M. M., 1993, Models and Techniques in Computer Animation, P139
   Cosatto E, 2003, P IEEE, V91, P1406, DOI 10.1109/JPROC.2003.817141
   CURINGA S, 1996, P EUSIPCO 96 SYST CO, P36
   Ferreiros J, 1999, SPEECH COMMUN, V29, P65, DOI 10.1016/S0167-6393(99)00013-8
   Gandhi MB, 1998, INT CONF ACOUST SPEE, P457, DOI 10.1109/ICASSP.1998.674466
   Huang FJ, 1998, 1998 IEEE SECOND WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P352, DOI 10.1109/MMSP.1998.738959
   Kim T, 2002, FOURTH IEEE INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, PROCEEDINGS, P299, DOI 10.1109/ICMI.2002.1167010
   Ko H, 2006, INT J PATTERN RECOGN, V20, P1029, DOI 10.1142/S0218001406005113
   KOSTER BE, 1994, CONF REC ASILOMAR C, P583, DOI 10.1109/ACSSC.1994.471519
   Lewis J., 1991, Journal of Visualization and Computer Animation, V2, P118, DOI 10.1002/vis.4340020404
   MCALLISTER DV, 1997, P SIGGRAPH 97 LOS AN, P225, DOI DOI 10.1145/259081.259312
   MORISHIMA S, 1998, P AVSP 98, P195
   OHMAN SEG, 2006, J ACOUST SOC AM, V31, P151
   Park J, 2006, SPEECH COMMUN, V48, P737, DOI 10.1016/j.specom.2005.10.001
   Park J, 2006, INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5, P1583
   STRUM J, 2000, P ICSLP, P429
   TAMURA M, 1998, P AUD VIS SPEECH PRO, P221
   Xie L, 2007, IEEE T MULTIMEDIA, V9, P500, DOI 10.1109/TMM.2006.888009
   Yamamoto E, 1998, SPEECH COMMUN, V26, P105, DOI 10.1016/S0167-6393(98)00054-5
   ZORIC G, 2005, P 8 INT C TEL, V2, P353
   Zoric G, 2006, SIGNAL PROCESS, V86, P3644, DOI 10.1016/j.sigpro.2006.02.038
NR 30
TC 11
Z9 13
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2008
VL 10
IS 7
BP 1299
EP 1306
DI 10.1109/TMM.2008.2004908
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 378HB
UT WOS:000261310700007
DA 2024-07-18
ER

PT J
AU Sheinin, V
   Jagmohan, A
   He, DK
AF Sheinin, Vadim
   Jagmohan, Ashish
   He, Da-ke
TI On the Operational Rate-Distortion Performance of Uniform Scalar
   Quantization-Based Wyner-Ziv Coding of Laplace-Markov Sources
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE DPCM; Laplace-Markov; quantization; rate-distortion; Slepian-Wolf
   coding; Wyner-Ziv coding
ID SIDE INFORMATION
AB Wyner-Ziv (WZ) coding has recently been proposed as a low encoding complexity alternative to traditional DPCM coding for compression of sources with memory, in particular, in applications like multimedia compression. The viability of this alternative approach clearly depends on the compression performance of WZ coding compared to that of DPCM coding. In an attempt to understand the performance gap between WZ coding and DPCM coding, this paper studies the operational rate-distortion performance of WZ coding, using uniform scalar quantization followed by perfect Slepian-Wolf coding, for compression of a Laplace-Markov (LM) source. It is shown that at low rates or for weakly correlated LM sources, WZ coding is indeed a competitive alternative to DPCM coding. However, at high rates the performance gap becomes non-negligible for strongly correlated LM sources. In order to reduce the gap at high rates, a hybrid approach that combines DPCM coding and WZ coding is further investigated. It is shown that the hybrid approach is indeed competitive to DPCM coding at all rates even for strongly correlated LM sources.
C1 [Sheinin, Vadim; Jagmohan, Ashish; He, Da-ke] IBM Corp, TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.
C3 International Business Machines (IBM)
RP Sheinin, V (corresponding author), IBM Corp, TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.
EM vadims@us.ibm.com; ashishja@us.ibm.com
RI He, Dake/AAP-3592-2020
CR AARON A, 2003, P ICIP
   BERGER T, 1972, IEEE T INFORM THEORY, V18, P759, DOI 10.1109/TIT.1972.1054906
   CHEN J, 2006, P ISIT 06 SEATTL WA
   Cover T. M., 1991, ELEMENTS INFORM THEO
   ELIAS P, 1955, IRE T INFORM THEOR, V1, P24, DOI 10.1109/TIT.1955.1055116
   ELIAS P, 1955, IRE T INFORM THEOR, V1, P16, DOI 10.1109/TIT.1955.1055126
   FARVARDIN N, 1984, IEEE T INFORM THEORY, V30, P485, DOI 10.1109/TIT.1984.1056920
   FARVARDIN N, 1985, IEEE T INFORM THEORY, V31, P402, DOI 10.1109/TIT.1985.1057040
   Girod B, 2005, P IEEE, V93, P71, DOI 10.1109/JPROC.2004.839619
   He DK, 2008, P SOC PHOTO-OPT INS, V6822, pU8221, DOI 10.1117/12.766699
   HE DK, 2006, P ISIT 06 SEATTL WA
   *ISO IEC JTC, 1SC29WG11 ISOIEC JTC
   Liveris AD, 2002, IEEE COMMUN LETT, V6, P440, DOI 10.1109/LCOMM.2002.804244
   Puri R, 2003, IEEE IMAGE PROC, P617
   PURI R, 2002, ALL C COMM CONTR COM
   REININGER RC, 1983, IEEE T COMMUN, V31, P835, DOI 10.1109/TCOM.1983.1095893
   Rose K, 2001, IEEE T IMAGE PROCESS, V10, P965, DOI 10.1109/83.931091
   SLEPIAN D, 1973, IEEE T INFORM THEORY, V19, P471, DOI 10.1109/TIT.1973.1055037
   Stankovic V, 2006, IEEE T INFORM THEORY, V52, P1495, DOI 10.1109/TIT.2006.871046
   WYNER AD, 1976, IEEE T INFORM THEORY, V22, P1, DOI 10.1109/TIT.1976.1055508
NR 20
TC 4
Z9 5
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2008
VL 10
IS 7
BP 1225
EP 1236
DI 10.1109/TMM.2008.2004902
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 378HB
UT WOS:000261310700001
DA 2024-07-18
ER

PT J
AU Su, SC
   van der Schaar, M
AF Su, Shih-Chung
   van der Schaar, Mihaela
TI On the Application of Game-Theoretic Mechanism Design for Resource
   Allocation in Multimedia Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mechanism design; multimedia systems; multimedia tasks; system resource
   management
AB In this paper, we study the system-level computational resource allocation problem among multiple multimedia tasks. We consider the multimedia tasks to be autonomous, i.e., they are selfish and behave strategically. We propose a resource allocation framework based on mechanism design to prevent the tasks from behaving strategically and manipulating the available system resources. We apply two mechanisms in the framework and assess their advantages over proportional-share resource allocation algorithms, which are often used in multimedia systems. We show in the simulations that the incorporation of mechanism design for system resource allocation is a promising solution that achieves efficient, fair and robust allocation against manipulation from strategic applications.
C1 [Su, Shih-Chung; van der Schaar, Mihaela] Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
C3 University of California System; University of California Los Angeles
RP Su, SC (corresponding author), Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
EM shihchungsu@gmail.com; mihaela@ee.ucla.edu
CR [Anonymous], H 264 AVC JOINT MODE
   BROWNE S, 2000, P 2000 ACM IEEE C SU
   Goyal P, 1996, PROCEEDINGS OF THE SECOND SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '96), P107, DOI 10.1145/248155.238766
   Horowitz M, 2003, IEEE T CIRC SYST VID, V13, P704, DOI 10.1109/TCSVT.2003.814967
   Jackson M. O., 2003, OPTIMIZATION OPERATI
   KWONG RH, 1992, IEEE T SIGNAL PROCES, V40, P1633, DOI 10.1109/78.143435
   LIU CL, 1987, IEEE REAL TIM SYST S
   Mas-Colell A., 1995, MICROECONOMIC THEORY, V1
   Nahrstedt K, 1998, J HIGH SPEED NETW, V7, P229
   Nieh Jason., 1997, P 16 ACM S OPERATING, P184, DOI DOI 10.1145/268998.266677
   Ohm JR, 2005, P IEEE, V93, P42, DOI 10.1109/JPROC.2004.839611
   Ortega A, 1998, IEEE SIGNAL PROC MAG, V15, P23, DOI 10.1109/79.733495
   Ostermann J., 2004, IEEE Circuits and Systems Magazine, V4, P7, DOI 10.1109/MCAS.2004.1286980
   Plagemann T, 2000, COMPUT COMMUN, V23, P267, DOI 10.1016/S0140-3664(99)00180-2
   Pons J., 2000, IASTEDACTA PRESS AI, P302
   Radha HM, 2001, IEEE T MULTIMEDIA, V3, P53, DOI 10.1109/6046.966110
   Rajkumar R, 1997, REAL TIM SYST SYMP P, P298, DOI 10.1109/REAL.1997.641291
   Silberschatz Abraham., 2004, OPERATING SYSTEM CON
   Stoenescu TM, 2006, IEEE DECIS CONTR P, P1270, DOI 10.1109/CDC.2006.377177
   Waldspurger C. A., 1994, Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation (OSDI), P1
   Yuan WH, 2006, IEEE T MOBILE COMPUT, V5, P799, DOI 10.1109/TMC.2006.98
   [No title captured]
NR 22
TC 6
Z9 9
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 1197
EP 1207
DI 10.1109/TMM.2008.2001366
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600021
DA 2024-07-18
ER

PT J
AU Zhuang, YT
   Yang, Y
   Wu, F
AF Zhuang, Yue-Ting
   Yang, Yi
   Wu, Fei
TI Mining semantic correlation of heterogeneous multimedia data for
   cross-media retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE cross-media retrieval; multimedia document; multimedia semantics mining;
   relevance feedback
ID CLASSIFICATION
AB Although multimedia objects such as images, audios and texts are of different modalities, there are a great amount of semantic correlations among them. In this paper, we propose a method of transductive learning to mine the semantic correlations among media objects of different modalities so that to achieve the cross-media retrieval. Cross-media retrieval is a new kind of searching technology by which the query examples and the returned results can be of different modalities, e.g., to query images by an example of audio. First, according to the media objects features and their co-existence information, we construct a uniform cross-media correlation graph, in which media objects of different modalities are represented uniformly. To perform the cross-media retrieval, a positive score is assigned to the query example; the score spreads along the graph and media objects of target modality or MMDs with the highest scores are returned. To boost the retrieval performance, we also propose different approaches of long-term and short-term relevance feedback to mine the information contained in the positive and negative examples.
C1 [Zhuang, Yue-Ting; Yang, Yi; Wu, Fei] Zhejiang Univ, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Zhuang, YT (corresponding author), Zhejiang Univ, Hangzhou 310027, Zhejiang, Peoples R China.
EM yzhuang@cs.zju.edu.cn; yangyi_zju@yahoo.com.cn; wufei@cs.zju.edu.cn
RI yang, yang/HGT-7999-2022; Lang, Ming/HIK-0758-2022; yang,
   yang/GVT-5210-2022; Yang, Yi/B-9273-2017; yang, yang/GWB-9426-2022
OI Yang, Yi/0000-0002-0512-880X; 
CR [Anonymous], 2005, VISUAL COMMUN-US, DOI [DOI 10.1177/1470357205055928, 10.1177/1470357205055928]
   [Anonymous], 2003, P NEUR INF PROC SYST
   Bach JR, 1996, P SOC PHOTO-OPT INS, V2670, P76, DOI 10.1117/12.234785
   Bakker EM, 2002, LECT NOTES COMPUT SC, V2383, P271
   BENITEZ AB, 2000, P IS T SPIE 2000 C I, V4210
   Chang SF, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P531, DOI 10.1109/ICIP.1998.727321
   CHEN Y, 2002, P 11 INT C INF KNOWL
   Cooper Matthew., 2005, ACM T MULTIM COMPUT, V1, P269, DOI [DOI 10.1145/1083314.1083317, 10.1145/1083314.1083317]
   Del Bimbo A, 2006, ACM T MULTIM COMPUT, V2, P20
   Djeraba C, 2003, IEEE T KNOWL DATA EN, V15, P118, DOI 10.1109/TKDE.2003.1161586
   Fan JP, 2004, IEEE T MULTIMEDIA, V6, P70, DOI 10.1109/TMM.2003.819583
   Foote J, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P452, DOI 10.1109/ICME.2000.869637
   Guo GD, 2003, IEEE T NEURAL NETWOR, V14, P209, DOI 10.1109/TNN.2002.806626
   HAAS M, 2004, P 6 ACM SIGMM INT WO
   HE J, 2004, P ACM MULT C NEW YOR
   HE X, 2004, P ACM MULT C NEW YOR
   Jafari-Khouzani K, 2005, IEEE T PATTERN ANAL, V27, P1004, DOI 10.1109/TPAMI.2005.126
   Krishnapuram R, 2004, IEEE T KNOWL DATA EN, V16, P1185, DOI 10.1109/TKDE.2004.53
   Langville A. N., 2003, Internet Math, V1, P335, DOI DOI 10.1080/15427951.2004.10129091
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Liu F, 2003, COMPUT VIS IMAGE UND, V92, P265, DOI 10.1016/j.cviu.2003.06.001
   MADDAGE NC, 2004, P ACM MULT C NEW YOR
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   MULLER M, 2005, P ACM SIGGRAPH C LOS
   Rui Y, 2000, PROC CVPR IEEE, P236, DOI 10.1109/CVPR.2000.855825
   Schmid C, 2000, INT J COMPUT VISION, V37, P151, DOI 10.1023/A:1008199403446
   Sebe N, 2003, IMAGE VISION COMPUT, V21, P1087, DOI 10.1016/j.imavis.2003.08.012
   SHRAGER J, 1987, SCIENCE, V236, P1092, DOI 10.1126/science.236.4805.1092
   Srivastava A, 2005, IEEE T PATTERN ANAL, V27, P590, DOI 10.1109/TPAMI.2005.86
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Tieu K, 2004, INT J COMPUT VISION, V56, P17, DOI 10.1023/B:VISI.0000004830.93820.78
   TONG H, 2005, P ACM MULT C SING
   Veltkamp RC, 2001, ADV PTRN RECOGNIT, P87
   Wold E, 1996, IEEE MULTIMEDIA, V3, P27, DOI 10.1109/93.556537
   WU F, 2006, P 13 INT C IM PROC A
   WU MY, 2003, P 16 IPPR C COMP VIS
   WU P, 2001, INT J IMAGE GRAPH, V1, P547
   Wu Y, 2000, PROC CVPR IEEE, P222, DOI 10.1109/CVPR.2000.855823
   ZHANG H, 2007, P ACM MULT AUG GERM
   Zhang RF, 2007, IEEE T IMAGE PROCESS, V16, P562, DOI 10.1109/TIP.2006.888350
   Zhou DY, 2004, ADV NEUR IN, V16, P169
   ZHOU XS, 2001, P ACM INT C MULT OTT
   Zhuang YT, 2007, J VLSI SIG PROC SYST, V46, P153, DOI 10.1007/s11265-006-0020-y
NR 43
TC 107
Z9 128
U1 0
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2008
VL 10
IS 2
BP 221
EP 229
DI 10.1109/TMM.2007.911822
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 254HC
UT WOS:000252576700006
DA 2024-07-18
ER

PT J
AU Jin, X
   Yiu, WPK
   Chan, SHG
   Wang, Y
AF Jin, Xing
   Yiu, W. -P. Ken
   Chan, S. -H. Gary
   Wang, Yajun
TI On maximizing tree bandwidth for topology-aware peer-to-peer streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE overlay tree; peer-to-peer streaming; topologyaware; tree bandwidth
ID MULTICAST; RECOVERY; INTERNET; NETWORK
AB In recent years, there has been an increasing interest in peer-to-peer (P2P) multimedia streaming. In this paper, we consider constructing a high-bandwidth overlay tree for streaming services. We observe that underlay information such as link connectivity and link bandwidth is important in tree construction, because two seemingly disjoint overlay paths may share common links on the underlay. We hence study how to construct a high-bandwidth overlay tree given the underlay topology. We formulate the problem as building a Maximum Bandwidth Multicast Tree (MBMT) or a Minimum Stress Multicast Tree (MSMT), depending on whether link bandwidth is available or not. We prove that both problems are NP-hard and are not approximable within a factor of (2/3 + epsilon), for any epsilon > 0, unless P = NP. We then present approximation algorithms to address them and analyze the algorithm performance. Furthermore, we discuss some practical issues (e.g., group dynamics, resilience and scalability) in system implementation. We evaluate our algorithms on Internet-like topologies. The results show that our algorithms can achieve high tree bandwidth and low link stress with low penalty in end-to-end delay. Measurement study based on PlanetLab further confirms this. Our study shows that the knowledge of underlay is important for constructing efficient overlay trees.
C1 Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Kowloon, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Jin, X (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Kowloon, Peoples R China.
EM csvenus@cse.ust.hk; kenyiu@cse.ust.hk; gchan@cse.ust.hk;
   yalding@cse.ust.hk
RI Tavares, António JV/A-7115-2008
OI Chan, Gary Shueng Han/0000-0003-4207-764X
CR Albert R, 1999, NATURE, V401, P130, DOI 10.1038/43601
   Andersen DG, 2002, IMW 2002: PROCEEDINGS OF THE SECOND INTERNET MEASUREMENT WORKSHOP, P243, DOI 10.1145/637201.637239
   [Anonymous], P 12 INT WORKSH NETW
   [Anonymous], 1997, APPROXIMATION ALGORI
   Ballardie T., 1993, Computer Communication Review, V23, P85, DOI 10.1145/167954.166246
   Banerjee S, 2002, ACM SIGCOMM COMP COM, V32, P205, DOI 10.1145/964725.633045
   Banerjee S, 2006, IEEE ACM T NETWORK, V14, P237, DOI 10.1109/TNET.2006.872579
   Castro M., 2003, Operating Systems Review, V37, P298, DOI 10.1145/1165389.945474
   Cheuk KWR, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P1441, DOI 10.1109/ICC.2004.1312750
   Chu YH, 2002, IEEE J SEL AREA COMM, V20, P1456, DOI 10.1109/JSAC.2002.803066
   Coates M, 2002, IEEE SIGNAL PROC MAG, V19, P47, DOI 10.1109/79.998081
   COATES M, 2002, P ACM SIGMETRICS 02, P11
   Cohen R, 2001, IEEE INFOCOM SER, P440, DOI 10.1109/INFCOM.2001.916727
   CORMEN TH, 2001, INTROL ALGORITHMS
   Cui Y, 2003, 11TH IEEE INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, PROCEEDINGS, P71
   Deering S, 1996, IEEE ACM T NETWORK, V4, P153, DOI 10.1109/90.490743
   DEERING SE, 1990, ACM T COMPUT SYST, V8, P85, DOI 10.1145/78952.78953
   DEERING SE, 1988, P ACM SIGCOMM CCR AU, V18, P55
   DO T, 2004, P IEEE ICC 04 JUN, P167
   Donnet B., 2005, P ACMSIGMETRICS, P327
   Fei ZM, 2007, IEEE ACM T NETWORK, V15, P173, DOI 10.1109/TNET.2006.890086
   Garey M.R., 1979, COMPUTERS INTRACTABI
   Govindan R., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P1371, DOI 10.1109/INFCOM.2000.832534
   Guo Y., 2003, Proceedings of the 12th International Conference on World Wide Web, P301, DOI DOI 10.1145/775152.775195
   Hu NN, 2003, IEEE J SEL AREA COMM, V21, P879, DOI 10.1109/JSAC.2003.814505
   JAIN M, 2002, P ACM SIGCOMM AUG, P295
   Jannotti J, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE FOURTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P197
   Jin X, 2005, IEEE ICC, P1319
   Jin X, 2006, IEEE J SEL AREA COMM, V24, P2182, DOI 10.1109/JSAC.2006.884016
   Kostic D., 2003, Operating Systems Review, V37, P282, DOI 10.1145/1165389.945473
   Moy J., 1994, MULTICAST EXTENSION
   Oliveira CAS, 2005, COMPUT OPER RES, V32, P1953, DOI 10.1016/j.cor.2003.12.007
   RIBEIRO V, 2003, P PAM 03 APR
   Spring N, 2002, ACM SIGCOMM COMP COM, V32, P133, DOI 10.1145/964725.633039
   Tang Y, 2007, IEEE COMMUN MAG, V45, P100, DOI 10.1109/MCOM.2007.374426
   Waldvogel M, 2003, ACM SIGCOMM COMP COM, V33, P101, DOI 10.1145/774763.774779
   Wang F., 2003, P 3 ACM SIGCOMM C IN, P15, DOI DOI 10.1145/948205.948208
   WINTER R, 2004, P IEEE ICN 04 MAR
   Yao B, 2003, IEEE INFOCOM SER, P353
   Yiu WPK, 2006, IEEE T MULTIMEDIA, V8, P219, DOI 10.1109/TMM.2005.864268
   Zegura EW, 1996, IEEE INFOCOM SER, P594, DOI 10.1109/INFCOM.1996.493353
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
   Zhang XY, 2004, IEEE J SEL AREA COMM, V22, P18, DOI 10.1109/JSAC.2003.818780
NR 43
TC 33
Z9 34
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2007
VL 9
IS 8
BP 1580
EP 1592
DI 10.1109/TMM.2007.907459
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 233SF
UT WOS:000251109900005
DA 2024-07-18
ER

PT J
AU Kordasiewicz, RC
   Gallant, MD
   Shirani, S
AF Kordasiewicz, Roman C.
   Gallant, Michael D.
   Shirani, Shahram
TI Encoding of alffine motion vectors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE terms-Affine motion estimation; affine motion vectors; arithmetic
   coding; CABAC; context determination; video compresion; VLC
ID COMPENSATION
AB An affine motion model provides better motion representation than a translational motion model. Therefore, it is a good candidate for advanced video compression algorithms, requiring higher compression efficiency than current algorithms. One disadvantage of the affine motion model is the increased number of motion vector parameters, therefore increased motion vector bit rate. We develop and analyze several simulation based approaches of entropy coding for orthonormalized affine motion vector (AMV) coefficients, by considering various context-types and coders. In our work we expand the traditional idea of a context type by introducing four new context types. We compare our method of contexts-type and coder selection with context quantization. The best of our contexts-type and coder solutions produces 4% to 15% average AMV bit-rate reductions over the original VLC approach. For more difficult content AMV bit rate reduction up to 26% is reported.
C1 LSI Log Corp, Waterloo, ON N2V 1C5, Canada.
   McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8S 4K1, Canada.
C3 McMaster University
RP Kordasiewicz, RC (corresponding author), LSI Log Corp, Waterloo, ON N2V 1C5, Canada.
EM roman.kordasiewicz@utoronto.ca; mgallant@lsi.com;
   shirani@mail.ece.mcmaster.ca
CR CORBERA JR, 2001, IEEE T CIRCUITS SYST, V11, P497
   GAHLOT A, 2003, TENCON 2003 C CONV T, V4, P1343
   GERSHO A, 1995, VECTOR QUANTIZATION
   Huang JC, 2004, IEEE T CONSUM ELECTR, V50, P911, DOI 10.1109/TCE.2004.1341699
   *ITU T NOK INC, 2000, MVC COD DEC SUBM ITU
   JIANG W, 1997, P ICIP, V2
   Karczewicz M, 1997, SIGNAL PROCESS-IMAGE, V10, P63, DOI 10.1016/S0923-5965(97)00019-2
   Keller Y, 2004, IEEE T IMAGE PROCESS, V13, P1042, DOI 10.1109/TIP.2004.823823
   Kordasiewicz RC, 2007, IEEE T CIRC SYST VID, V17, P86, DOI 10.1109/TCSVT.2006.887080
   Sayed M, 2006, IEEE T CIRC SYST VID, V16, P457, DOI 10.1109/TCSVT.2006.872780
   Servais M, 2005, IEE P-VIS IMAGE SIGN, V152, P415, DOI 10.1049/ip-vis:20045110
   SHEN J, 1999, P C IEEE ICIP 99, V2, P629
   Utgikar A, 2003, SIPS 2003: IEEE WORKSHOP ON SIGNAL PROCESSING SYSTEMS, P159, DOI 10.1109/SIPS.2003.1235662
   VAISEY J, 2002, ICIP, V3
   Wiegand T, 2005, IEEE T CIRC SYST VID, V15, P197, DOI 10.1109/TCSVT.2004.841690
NR 15
TC 6
Z9 6
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2007
VL 9
IS 7
BP 1346
EP 1356
DI 10.1109/TMM.2007.906592
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 224LB
UT WOS:000250447400002
DA 2024-07-18
ER

PT J
AU Siu, WC
   Chan, YL
   Fung, KT
AF Siu, Wan-Chi
   Chan, Yui-Lam
   Fung, Kai-Tat
TI On transcoding a B-Frame to a P-frame in the compressed domain
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE B-picture and P-picture; compressed domain processing; heterogeneous
   transcoding; video coding and transcoding
ID BLOCK MOTION ESTIMATION; SEARCH ALGORITHM; ARCHITECTURES
AB Only a limited number of methods have been proposed to realize heterogeneous transcoding, for example from MPEG-2 to H.263, or from H.264 to H.263. The major difficulties of transcoding a B-picture to a P-picture are that the incoming discrete cosine transform (DCT) coefficients of the B-frame are prediction errors arising from both forward and backward predictions, whilst the prediction errors in the DCT domain arising from the prediction using the previous frame alone are not available. The required new prediction errors need to be re-estimated in the pixel domain. This process involves highly complex computation and introduces re-encoding errors. We propose a new approach to convert a B-picture into a P-picture by making use of some properties of motion compensation in the DCT domain and the direct addition of DCT coefficients. We derive a set of equations and formulate the problem of how to obtain the DCT coefficients. One difficulty is that the last P-frame inside a GOP with an IBBP structure, for example, needs to be transcoded to become the last P-frame in the TPPP structure, and it has to be linked to the previous reconstructed P-frame instead of to the I-frame. We increased the speed of the transcoding process by making use of the motion activity which is expressed in terms of the correlation between pictures. The whole trainscoding process is done in the transform domain, hence re-encoding errors are completely avoided. Results from our experimental work show that the proposed video transcoder not only achieves a speed-up of two to six times that of the conventional video transcoder, but it also substantially improves the quality of the video.
C1 Hong Kong Polytech Univ, Dept Elect & Informat Engn, Ctr Multimedia Signal Proc, Kowloon, Hong Kong, Peoples R China.
C3 Hong Kong Polytechnic University
RP Siu, WC (corresponding author), Hong Kong Polytech Univ, Dept Elect & Informat Engn, Ctr Multimedia Signal Proc, Kowloon, Hong Kong, Peoples R China.
EM enwesiu@polyu.edu.hk; enylchan@polyu.edu.hk;
   enktfung@eieserver.eie.polyu.edu.hk
RI Chan, Yui-Lam/C-3799-2014
OI Chan, Yui-Lam/0000-0002-1473-094X
CR Ahmad I, 2005, IEEE T MULTIMEDIA, V7, P793, DOI 10.1109/TMM.2005.854472
   Assunçao PAA, 1998, IEEE T CIRC SYST VID, V8, P953, DOI 10.1109/76.736724
   Assuncao PAA, 1997, ELECTRON LETT, V33, P284, DOI 10.1049/el:19970188
   Chalidabhongse J, 1997, IEEE T CIRC SYST VID, V7, P477, DOI 10.1109/76.585927
   Chan YL, 1996, IEEE T CIRC SYST VID, V6, P113, DOI 10.1109/76.486426
   Chan YL, 1998, J VIS COMMUN IMAGE R, V9, P139, DOI 10.1006/jvci.1998.0388
   Chan YL, 1997, IEE P-VIS IMAGE SIGN, V144, P136, DOI 10.1049/ip-vis:19971199
   CHANG SF, 1995, IEEE J SEL AREA COMM, V13, P1, DOI 10.1109/49.363151
   Côté G, 1998, IEEE T CIRC SYST VID, V8, P849, DOI 10.1109/76.735381
   Fung KT, 2002, IEEE T IMAGE PROCESS, V11, P886, DOI 10.1109/TIP.2002.800890
   *ISO IEC, INF TECHN GEN COD MO
   Keeman G., 1996, SIGNAL PROCESS-IMAGE, V8, P481
   Koc UV, 1998, IEEE T IMAGE PROCESS, V7, P948, DOI 10.1109/83.701146
   Merhav N, 1997, IEEE T CIRC SYST VID, V7, P468, DOI 10.1109/76.585926
   Ostermann J., 2004, IEEE Circuits and Systems Magazine, V4, P7, DOI 10.1109/MCAS.2004.1286980
   Po LM, 1996, IEEE T CIRC SYST VID, V6, P313, DOI 10.1109/76.499840
   Schweizer P, 2003, EUR J PEDIATR SURG, V13, P7, DOI 10.1055/s-2003-38287
   Shanableh T, 2000, IEEE T MULTIMEDIA, V2, P101, DOI 10.1109/6046.845014
   Shen B, 1999, IEEE T CIRC SYST VID, V9, P929, DOI 10.1109/76.785730
   Song JH, 2000, IEEE T CIRC SYST VID, V10, P767, DOI 10.1109/76.856453
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Sun HF, 1996, IEEE T CIRC SYST VID, V6, P191, DOI 10.1109/76.488826
   Tham JY, 1998, IEEE T CIRC SYST VID, V8, P369, DOI 10.1109/76.709403
   Vetro A, 2003, IEEE SIGNAL PROC MAG, V20, P18, DOI 10.1109/MSP.2003.1184336
   Warabino T, 2000, IEEE COMMUN MAG, V38, P66, DOI 10.1109/35.874971
   Xin J, 2005, P IEEE, V93, P84, DOI 10.1109/JPROC.2004.839620
   Youn J, 1998, IEEE T CONSUM ELECTR, V44, P649, DOI 10.1109/30.713176
   Youngs Gillian., 1999, International Feminist Journal of Politics, V1, P1
   ZENG B, 1994, IEEE T CIRCUITS SYST, V4, P438
NR 29
TC 4
Z9 6
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2007
VL 9
IS 6
BP 1093
EP 1102
DI 10.1109/TMM.2007.902895
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 215XJ
UT WOS:000249842000001
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Ramanathan, P
   Kalman, M
   Girod, B
AF Ramanathan, Prashant
   Kalman, Mark
   Girod, Bernd
TI Rate-distortion optimized interactive light field streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE image-based rendering; image coding; image communication; interactive
   streaming; light field coding; light field streaming; light fields;
   rate-distortion optimized streaming
ID VIDEO TRANSMISSION; DATA-COMPRESSION; CHANNELS
AB High-quality, photorealistic image-based rendering datasets are typically too large to download entirely before viewing, even when compressed. It is more suitable to instead stream the required image data to a remote user who can start interacting with the dataset immediately. This paper presents an interactive light field streaming system and proposes packet scheduling for transmitting the encoded image data in a rate-distortion optimized manner. An interactive light field streaming system must have low user latency. The system presented in this paper predicts the future user viewing trajectory to mitigate the effects of the low-latency constraints. Experimental results show that view prediction can improve performance, and that this improvement is limited by the prediction accuracy. The proposed packet scheduling algorithm considers network conditions and rate-distortion cost, knowledge of sent and received images, and the distortion for a set of images, to optimize the rendered image quality for the remote user. Rate-distortion optimized scheduling can be implemented either at the receiver or the sender. It is shown that this rate-distortion optimized packet scheduling can significantly improve performance over a heuristic scheduling approach. Experimental results also show that the encoding prediction dependency structure affects streaming performance both through the compression efficiency of the encoding and also through any decoding dependencies that may be introduced.
C1 AOL LLC, San Francisco, CA 94140 USA.
   Stanford Univ, Stanford, CA 94305 USA.
C3 Time Warner Inc; Stanford University
RP Ramanathan, P (corresponding author), AOL LLC, San Francisco, CA 94140 USA.
EM pramanat@gmail.com; mkalman@stanford.edu; bgirod@stanford.edu
CR Albanese A, 1996, IEEE T INFORM THEORY, V42, P1737, DOI 10.1109/18.556670
   [Anonymous], P EGWR
   [Anonymous], 2000, PROC VMV
   [Anonymous], 1999, IEEE VISUALIZATION
   AYDINOGLU H, 1995, INT CONF ACOUST SPEE, P2173, DOI 10.1109/ICASSP.1995.479906
   AYDINOGLU H, 1994, IEEE IMAGE PROC, P385, DOI 10.1109/ICIP.1994.413597
   AZUMA R, 1995, P SIGGRAPH 95, P401
   Chang CF, 1999, COMP GRAPH, P291, DOI 10.1145/311535.311571
   CHANG CL, 2003, P IEEE INT C IM PROC
   CHANG CL, 2004, P SPIE VIS COMM IM P
   CHANG CL, 2004, P IEEE INT C MULT EX
   CHANG CL, 2003, P SPIE VIS COMM IM P
   Chen S. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P29, DOI 10.1145/218380.218395
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   CHOU PA, 2002, PACK VID WORKSH PITT
   CRUZ DS, 1999, P IEEE 3 WORKSH MULT, P389
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   Dinstein I., 1988, 9th International Conference on Pattern Recognition (IEEE Cat. No.88CH2614-6), P357, DOI 10.1109/ICPR.1988.28242
   DINSTEIN I, 1989, SIGNAL PROCESS, V17, P373, DOI 10.1016/0165-1684(89)90122-9
   Fujii T., 1994, THESIS U TOKYO TOKYO
   Fujii T, 1996, PICT COD S 96, V2, P447
   GIROD B, 2003, P IEEE INT C AC SPEE, V4, P761
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   GOTZ D, 2004, P ACM MULT ACM, P612
   Gotz D, 2005, INT J DIGIT LIBRARIE, V5, P205, DOI 10.1007/s00799-004-0106-8
   *ITU T, 2002, ISEIEC154441
   *ITU T, 1996, G114 ITU T
   Khansari M, 1996, IEEE T CIRC SYST VID, V6, P1, DOI 10.1109/76.486415
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Levoy M, 2000, COMP GRAPH, P131, DOI 10.1145/344779.344849
   Liu H, 1997, IEEE J SEL AREA COMM, V15, P1775, DOI 10.1109/49.650050
   Lukacs M. E., 1986, ICASSP 86 Proceedings. IEEE-IECEJ-ASJ International Conference on Acoustics, Speech and Signal Processing (Cat. No.86CH2243-4), P521
   Magnor M, 2000, IEEE T CIRC SYST VID, V10, P338, DOI 10.1109/76.836278
   Magnor M, 2003, IEEE T CIRC SYST VID, V13, P1092, DOI 10.1109/TCSVT.2003.817630
   Magnor M, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P919, DOI 10.1109/ICIP.2000.899866
   MCMILLAN L, 1999, THESIS U N CAROLINA
   PERKINS MG, 1992, IEEE T COMMUN, V40, P684, DOI 10.1109/26.141424
   Peter Ingmar., 2001, Proceedings of Eurographics Rendering Workshop, P262
   RAMANATHAN P, 2005, THESIS STANFORD U ST
   RAMANATHAN P, 2003, LIGHT FIELD VIEWER S
   RAMANATHAN P, 2004, P IEEE INT C IM PROC
   RAMANATHAN P, 2003, P IEEE INT C IM PROC, V3, P277
   RAMANATHAN P, 2004, P PICT COD S PCS 200
   RAMANATHAN P, 2001, P IEEE INT C IM PROC
   Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882
   Shum HY, 1999, COMP GRAPH, P299, DOI 10.1145/311535.311573
   SINGHAL SK, 1995, PRESENCE-TELEOP VIRT, V4, P169, DOI 10.1162/pres.1995.4.2.169
   Taubman D, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P765
   Tong X, 2003, IEEE T CIRC SYST VID, V13, P1080, DOI 10.1109/TCSVT.2003.817626
   Wood DN, 2000, COMP GRAPH, P287, DOI 10.1145/344779.344925
   ZHANG C, 2001, P SPIE VIS COMM IM P, P509
   ZHANG C, 2000, P IEEE DAT COMPR C M, P253
   ZHANG C, 2000, P SPIE VIS COMM IM P, P43
NR 53
TC 27
Z9 29
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2007
VL 9
IS 4
BP 813
EP 825
DI 10.1109/TMM.2007.893350
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 170NW
UT WOS:000246671200012
OA Green Published
DA 2024-07-18
ER

PT J
AU Chan, YS
   Modestino, JW
   Qu, Q
   Fan, XZ
AF Chan, Yee Sin
   Modestino, James W.
   Qu, Qi
   Fan, Xingzhe
TI An end-to-end embedded approach for multicast/broadcast of scalable
   video over multiuser CDMA wireless networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptive modulation and coding; code-division multiple access (CDMA);
   cross-layer; drift control; embedded transmission; H.263+; joint
   source-channel coding; joint source coding-power control;
   multicast/broadcast; turbo codes; wireless video
ID ERROR CONTROL; CONVOLUTIONAL-CODES; ADVANCED TELEVISION; POWER-CONTROL;
   TURBO-CODES; TRANSMISSION; PERFORMANCE; MODULATION; ALLOCATION;
   COMMUNICATION
AB We investigate an embedded multicast/broadcast approach for transport of digital video over spread-spectrum code-division multiple access (CDMA) cellular networks. Previous work has shown that the incorporation of a scalable source coding scheme with multiresolution modulation provides a promising design paradigm for the practical realization of the information-theoretic performance predictions originally developed by Cover [1], which demonstrated that optimal multicast/broadcast performance could be achieved by an embedded transmission scheme. Hence, the major technical challenge associated with the design of an end-to-end embedded multicast/broadcast system is how to match an embedded modulation constellation with a scalable source coding scheme. In this work, taking into consideration both the interference-limited and bandwidth-limited characteristics of a CDMA system, we provide a cross-layer approach incorporating adaptive power allocation and channel coding strategies and effectively match a discrete cosine transform based scalable motion-compensated video encoder to an embedded multiresolution modulation scheme to simultaneously deliver a basic quality-of-service (QoS) to less capable receivers while maximizing both the QoS for more capable receivers and the system capacity. We demonstrate the efficacy of this approach using the ITU-T H.263+ video scalable hybrid coder, although the approach is generally extensible to other scalable coding schemes as well.
C1 Univ Miami, Dept Elect & Comp Engn, Coral Gables, FL 33124 USA.
   Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
C3 University of Miami; University of California System; University of
   California San Diego
RP Chan, YS (corresponding author), Univ Miami, Dept Elect & Comp Engn, Coral Gables, FL 33124 USA.
EM chanys@ieee.org; jmodestino@miami.edu; qqu@ucsd.edu; fanx@miami.edu
CR [Anonymous], 1996, WIRELESS COMMUNICATI
   BAHL LR, 1974, IEEE T INFORM THEORY, V20, P284, DOI 10.1109/TIT.1974.1055186
   Bystrom M, 2000, IEEE J SEL AREA COMM, V18, P880, DOI 10.1109/49.848242
   Bystrom M, 2001, IEEE T COMMUN, V49, P1142, DOI 10.1109/26.935152
   CALDERBANK AR, 1993, IEEE T INFORM THEORY, V39, P1234, DOI 10.1109/18.243441
   CHAN Y, 2003, P GLOBECOM 03 C DEC, V6, P3585
   Chan YS, 2006, IEEE T COMMUN, V54, P1705, DOI 10.1109/TCOMM.2006.878811
   Chan YS, 2003, IEEE J SEL AREA COMM, V21, P1516, DOI 10.1109/JSAC.2003.815228
   CHO S, 1998, THESIS RENSSELAER PO
   COVER TM, 1972, IEEE T INFORM THEORY, V18, P2, DOI 10.1109/TIT.1972.1054727
   Divsalar D, 1995, PROCEEDINGS 1995 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, P35, DOI 10.1109/ISIT.1995.531137
   DUAT D, 1983, IEEE T COMMUN, V31, P315
   Eisenberg Y, 2002, IEEE T CIRC SYST VID, V12, P411, DOI 10.1109/TCSVT.2002.800309
   GILHOUSEN KS, 1991, IEEE T VEH TECHNOL, V40, P303, DOI 10.1109/25.289411
   Girod B, 1999, P IEEE, V87, P1707, DOI 10.1109/5.790632
   Goldsmith AJ, 2002, IEEE WIREL COMMUN, V9, P8, DOI 10.1109/MWC.2002.1028874
   HAGENAUER J, 1988, IEEE T COMMUN, V36, P389, DOI 10.1109/26.2763
   Hui J. Y. N., 1984, IEEE Journal on Selected Areas in Communications, VSAC-2, P482, DOI 10.1109/JSAC.1984.1146083
   Jayant N.C., 1984, Digital Coding of Waveforms: Principles and Applications to Speech and Video
   Jung P, 1997, ELECTRON LETT, V33, P2102, DOI 10.1049/el:19971444
   Katsaggelos AK, 2005, P IEEE, V93, P135, DOI 10.1109/JPROC.2004.839621
   Kim BJ, 2000, IEEE T CIRC SYST VID, V10, P1374, DOI 10.1109/76.889025
   Kondi LP, 2002, IEEE T IMAGE PROCESS, V11, P1043, DOI 10.1109/TIP.2002.802507
   Lee JW, 2002, IEEE INFOCOM SER, P1480, DOI 10.1109/INFCOM.2002.1019399
   Lee LN, 2000, IEEE T VEH TECHNOL, V49, P2198, DOI 10.1109/25.901891
   Li J., 1997, P INT S TURB COD BRE, P188
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   Lu YM, 1999, IEEE J SEL AREA COMM, V17, P978, DOI 10.1109/49.768210
   Pei Y, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P1009, DOI 10.1109/ICIP.2001.958667
   Prasad Ramjee, 1998, IEEE Communications Surveys & Tutorials, V1, P2, DOI 10.1109/COMST.1998.5340404
   Pursley MB, 1999, IEEE J SEL AREA COMM, V17, P1999, DOI 10.1109/49.806828
   Pursley MB, 1998, IEEE MILIT COMMUN C, P113, DOI 10.1109/MILCOM.1998.722555
   RAMCHANDRAN K, 1993, IEEE J SEL AREA COMM, V11, P6, DOI 10.1109/49.210540
   Reibman AR, 2003, IEEE T CIRC SYST VID, V13, P131, DOI 10.1109/TCSVT.2002.808435
   Rowitch DN, 2000, IEEE T COMMUN, V48, P948, DOI 10.1109/26.848555
   SABLATASH M, 1994, IEEE T BROADCAST, V40, P102, DOI 10.1109/11.286291
   Sajadieh M, 1998, IEEE T VEH TECHNOL, V47, P900, DOI 10.1109/25.704843
   SCHREIBER WF, 1995, P IEEE, V83, P958, DOI 10.1109/5.387095
   *TIA EIA, 1993, TIAEIAIS95
   Van Dyck RE, 1999, P IEEE, V87, P1734, DOI 10.1109/5.790634
   Viterbi A. J., 1979, IEEE Communications Magazine, V17, P11, DOI 10.1109/MCOM.1979.1089969
   VITERBI AJ, 1990, IEEE J SEL AREA COMM, V8, P641, DOI 10.1109/49.54460
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   WEI LF, 1993, IEEE T COMMUN, V41, P1439, DOI 10.1109/26.237878
   Wu F, 2001, IEEE T CIRC SYST VID, V11, P332, DOI 10.1109/76.911159
   YUN LC, 1995, P ICC SEATTL WA, V2, P713
   ZHAI F, 2003, IEEE J SEL AREA COMM, V21, P1710
   Zhang Q, 2002, IEEE T CIRC SYST VID, V12, P398, DOI 10.1109/TCSVT.2002.800322
   Zhao SJ, 2002, IEEE T CIRC SYST VID, V12, P425, DOI 10.1109/TCSVT.2002.800336
NR 49
TC 12
Z9 18
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2007
VL 9
IS 3
BP 655
EP 667
DI 10.1109/TMM.2006.887993
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148PW
UT WOS:000245088100019
DA 2024-07-18
ER

PT J
AU Pan, ZL
   Ngo, CW
AF Pan, Zailiang
   Ngo, Chong-Wah
TI Moving-object detection, association, and selection in home videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
ID FRAMEWORK
AB Due to the prevalence of digital video camcorders, home videos have become an important part of life-logs of personal experiences. To enable efficient video parsing, a critical step is to automatically extract objects, events and scene characteristics present in videos. This paper addresses the problem of extracting objects from home videos. Automatic detection of objects is a classical yet difficult vision problem, particularly for videos with complex scenes and unrestricted domains. Compared with edited and surveillant videos, home videos captured in uncontrolled environment are usually coupled with several notable features such as shaking artifacts, irregular motions, and arbitrary settings. These characteristics have actually prohibited the effective parsing of semantic video content using conventional vision analysis. In this paper, we propose a new approach to automatically locate multiple objects in home videos, by taking into account of how and when to initialize objects. Previous approaches mostly consider the problem of how but not when due to the efficiency or real-time requirements. In home-video indexing, online processing is optional. By considering when, some difficult problems can be alleviated, and most importantly, enlightens the possibility of parsing semantic video objects. In our proposed approach, the how part is formulated as an object detection and association problem, while the when part is a saliency measurement to determine the best few locations to start multiple object initialization.
C1 City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Pan, ZL (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM alan.zl.pan@gmail.com; cwngo@cs.cityu.edu.hk
OI Ngo, Chong Wah/0000-0003-4182-8261
CR [Anonymous], 2003, P IEEE INT C COMP VI
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Figueiredo MAT, 2002, IEEE T PATTERN ANAL, V24, P381, DOI 10.1109/34.990138
   Gatica-Perez D, 2003, IEEE T CIRC SYST VID, V13, P539, DOI 10.1109/TCSVT.2003.813428
   GATICAPEREZ D, 2002, P INT C MULT EXP
   GATICAPEREZ D, 2004, P INT C CIVR DUB IR
   HUA XA, 2003, ACM MULT C
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   IYENGAR G, 2000, P ICME
   Jain A. K., 1988, Algorithms for Clustering Data, P446
   KENDER JR, 2000, AS C COMP VIS
   Lienhart R., 1999, ACM MULTIMEDIA, P37, DOI DOI 10.1145/319878.319888
   LIENHART R, 2000, SPIE STORAGE RETRIEV
   MA WY, 2000, P EUSIPCO
   Mittal A, 2003, INT J COMPUT VISION, V51, P189, DOI 10.1023/A:1021849801764
   NEUMANN U, 1998, INTEGRATION REGION T, P658
   Ngo CW, 2001, IEEE T CIRC SYST VID, V11, P941, DOI 10.1109/76.937435
   PAN Z, 2004, MIR 04, P69
   Rousseeuw P.J., 1987, ROBUST REGRESSION OU
   Sawhney HS, 1996, IEEE T PATTERN ANAL, V18, P814, DOI 10.1109/34.531801
   SIVIC J, 2004, P IEEE C COMP VIS PA
   WANG JYA, 1994, IEEE T IMAGE PROCESS, V3, P625, DOI 10.1109/83.334981
   Weiss Y, 1996, PROC CVPR IEEE, P321, DOI 10.1109/CVPR.1996.517092
   Yeasin M, 2004, IEEE T MULTIMEDIA, V6, P398, DOI 10.1109/TMM.2004.827514
NR 24
TC 8
Z9 9
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2007
VL 9
IS 2
BP 268
EP 279
DI 10.1109/TMM.2006.887992
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 135CO
UT WOS:000244131900007
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Rykaczewski, R
AF Rykaczewski, Roman
TI Comments on "An SVD-based watermarking scheme for protecting rightful
   ownership"
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE rightful ownership; singular value decomposition; watermarking
AB It is shown that the watermarking algorithm presented in the above paper [1] can be easily discredited and ipso facto cannot be used for protecting rightful ownership.
C1 Gdansk Univ Technol, Fac Elect Telecommun & Comp Sci, PL-80952 Gdansk, Poland.
C3 Fahrenheit Universities; Gdansk University of Technology
RP Rykaczewski, R (corresponding author), Gdansk Univ Technol, Fac Elect Telecommun & Comp Sci, PL-80952 Gdansk, Poland.
EM romryk@eti.pg.gda.pl
CR Liu RZ, 2002, IEEE T MULTIMEDIA, V4, P121, DOI 10.1109/6046.985560
   Tian Y, 2003, PATTERN RECOGN, V36, P649, DOI 10.1016/S0031-3203(02)00105-X
NR 2
TC 73
Z9 78
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2007
VL 9
IS 2
BP 421
EP 423
DI 10.1109/TMM.2006.886297
PG 3
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 135CO
UT WOS:000244131900020
DA 2024-07-18
ER

PT J
AU Coskun, B
   Sankur, B
   Memon, N
AF Coskun, Baris
   Sankur, Bulent
   Memon, Nasir
TI Spatio-temporal transform based video hashing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE broadcast monitoring; multimedia content authentication; robust hash;
   video database indexing; video hash
AB Identification and verification of a video clip via its fingerprint find applications in video browsing, database search and security. For this purpose, the video sequence must be collapsed into a short fingerprint using a robust hash function based on signal processing operations. We propose two robust hash algorithms for video based both on the Discrete Cosine Transform (DCT), one on the classical basis set and the other on a novel randomized basis set (RBT). The robustness and randomness properties of the proposed hash functions are investigated in detail. It is found that these hash functions are resistant to signal processing and transmission impairments, and therefore can be instrumental in building database search, broadcast monitoring and watermarking applications for video. The DCT hash is more robust, but lacks security aspect, as it is easy to find different video clips with the same hash value. The RBT based hash, being secret key based, does not allow this and is more secure at the cost of a slight loss in the receiver operating curves.
C1 Polytech Univ, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
   Bogazici Univ, Dept Elect Engn, Istanbul, Turkey.
C3 New York University; Bogazici University
RP Coskun, B (corresponding author), Polytech Univ, Dept Elect & Comp Engn, 6 Metrotech Ctr, Brooklyn, NY 11201 USA.
EM bcosku01@utopia.poly.edu; bulent.sankur@boun.edu.tr; memon@poly.edu
RI Sankur, Bulent/N-4663-2017
OI Memon, Nasir/0000-0002-0103-9762
CR Bhattacharjee S, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P435, DOI 10.1109/ICIP.1998.723518
   CASPI Y, 2004, ICIP 04 SING OCT
   DAgostino R., 2017, Goodness-of-fit-techniques
   Dubes Richard, 1980, ADV COMPUT, V19, P113
   Fridrich J., 2000, Proceedings International Conference on Information Technology: Coding and Computing (Cat. No.PR00540), P178, DOI 10.1109/ITCC.2000.844203
   FRIDRICH J, 1998, 2 INT HID WORKSH POR
   Johnson M, 2003, P IEEE INT C IM PROC
   LEFEBVRE F, 2003, IEEE ICIP BARC SPAIN
   Lin CY, 2001, IEEE T CIRC SYST VID, V11, P153, DOI 10.1109/76.905982
   MIHCAK MK, 2001, P DIG RIGHTS MAN WOR
   MONGA V, 2005, ICIP 05 GEN IT SEP
   OOSTVEEN JC, 2001, SPIE APPL DIGITAL IM, V24
   Pua KM, 2004, COMPUT VIS IMAGE UND, V93, P310, DOI 10.1016/j.cviu.2003.10.005
   RADHAKRISHNAN R, 2003, P SPIE ELECT IMAGING, V5020
   Seo JS, 2004, SIGNAL PROCESS-IMAGE, V19, P325, DOI 10.1016/j.image.2003.12.001
   Venkatesan R, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P664, DOI 10.1109/ICIP.2000.899541
   Wang YW, 2004, J INORG MATER, V19, P1
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   YANG X, 2004, P 12 ANN ACM INT C M, P276
   YANG Z, 2004, ICIP 04 SING OCT
   Yuan J., 2004, PROC ACM MULTIMEDIA, P61, DOI DOI 10.1145/1026711.1026722
NR 21
TC 140
Z9 169
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2006
VL 8
IS 6
BP 1190
EP 1208
DI 10.1109/TMM.2006.884614
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109MG
UT WOS:000242311700009
DA 2024-07-18
ER

PT J
AU Grecos, C
   Yang, MY
AF Grecos, Christos
   Yang, Mingyuan
TI Fast mode prediction for the baseline and main profiles in the H.264
   video coding standard
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE complexity; H.264; mode prediction; video coding standards
AB We propose a novel fast mode decision scheme for both the baseline and main profiles of the H.264 video coding standard. Our scheme combines a set of conditions for SKIP mode decision, two heuristics that reduce the set of intermodes to be examined, inter/intramode prediction and the monotonicity property of the cost functions. For very similar RD performance, we achieve (33%-90%) reduction in run times as compared to H.264. Compared to other work that was used as input to the standard, our scheme is faster by 9-23% for very similar RD performance. The proposed scheme can be used either in its entirety or as individual components for speeding up the standard. It can be also used in conjunction with fast motion search methods and is applicable in both the rate controlled and non rate controlled cases of H.264.
C1 Univ Loughborough, Dept Elect Elect Engn, Loughborough LE11 3TU, Leics, England.
C3 Loughborough University
RP Grecos, C (corresponding author), Univ Loughborough, Dept Elect Elect Engn, Loughborough LE11 3TU, Leics, England.
EM C.Grecos@lboro.ac.uk; M.Y.Yang@lboro.ac.uk
CR [Anonymous], 144962 ISOIEC
   [Anonymous], 2003, IEEE T CIRCUITS SYST, V13
   Bazen AM, 2002, IEEE T PATTERN ANAL, V24, P905, DOI 10.1109/TPAMI.2002.1017618
   Bjontegaard G., 2001, CALCULATION AVERAGE
   Castleman K. R., 1996, Digital Image Processing
   CHAN MH, 1990, IEE PROC-I, V137, P205, DOI 10.1049/ip-i-2.1990.0029
   CHOI I, 2005, JTC1SC29WG11 ISOIEC
   *ISO IEC, 1994, JTC1 ISOIEC
   Jeon B, JTC1SC29WG11 ISOIEC
   LEE J, 2004, JTC1SC29WG11 ISOIEC
   Lee JY, 2003, LECT NOTES COMPUT SC, V2869, P723
   LIM KP, 2003, JTC1SC29WG11 ISOIEC
   Liu XW, 2001, IEEE IMAGE PROC, P70, DOI 10.1109/ICIP.2001.958955
   PAN F, 2003, JTC1SC29WG11 ISOIEC
   Rhee I, 2000, IEEE T CIRC SYST VID, V10, P42, DOI 10.1109/76.825857
   TU YK, P ICME 2003, P789
   Uchiyama T, 2000, INT C PATT RECOG, P1072, DOI 10.1109/ICPR.2000.903731
   Yin P, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P853
NR 18
TC 33
Z9 38
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2006
VL 8
IS 6
BP 1125
EP 1134
DI 10.1109/TMM.2006.884631
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109MG
UT WOS:000242311700003
DA 2024-07-18
ER

PT J
AU Tao, DC
   Tang, XO
   Li, XL
   Rui, Y
AF Tao, Dacheng
   Tang, Xiaoou
   Li, Xuelong
   Rui, Yong
TI Direct kernel biased discriminant analysis:: A new content-based image
   retrieval <i>relevance feedback</i> algorithm
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE biased discriminant analysis (BDA); content-based image retrieval
   (CBIR); direct kernel biased discriminant analysis (DKBDA); incremental
   direct kernel biased discriminant analysis (IDKBDA); kernel biased
   discriminant analysis (KBDA); relevance feedback (RF)
ID CLASSIFICATION; FEATURES; COLOR
AB In recent years, a variety of relevance feedback (RF) schemes have been developed to improve the performance of content-based image retrieval (CBIR). Given user feedback information, the key to a RF scheme is how to select a subset of image features to construct a suitable dissimilarity measure. Among various RF schemes, biased discriminant analysis (BDA) based RF is one of the most promising. It is based on the observation that all positive samples are alike, while in general each negative sample is negative in its own way. However, to use BDA, the small sample size (SSS) problem is a big challenge, as users tend to give a small number of feedback samples. To explore solutions to this issue, this paper proposes a direct kernel BDA (DKBDA), which is less sensitive to SSS. An incremental DKBDA (IDKBDA) is also developed to speed up the analysis. Experimental results are reported on a real-world image collection to demonstrate that the proposed methods outperform the traditional kernel BDA (KBDA) and the support vector machine (SVM) based RF algorithms.
C1 Univ London, Birkbeck, Sch Comp Sci & Informat Syst, London WC1E 7HX, England.
   Chinese Univ Hong Kong, Dept Informat Engn, Shatin, Hong Kong, Peoples R China.
   Microsoft Corp, Res, Redmond, WA 98052 USA.
C3 University of London; Birkbeck University London; Chinese University of
   Hong Kong; Microsoft
RP Tao, DC (corresponding author), Univ London, Birkbeck, Sch Comp Sci & Informat Syst, London WC1E 7HX, England.
EM dacheng@dcs.bbk.ac.uk; xtang@ie.cuhk.edu.hk; xuelong@dcs.bbk.ac.uk;
   yongrui@microsoft.com
RI li, xiang/GWM-6319-2022; Li, Xuelong/ABF-3381-2020; Li,
   Xuelong/Z-3785-2019; Tang, Xiaoou/G-6509-2012; Tao, Dacheng/A-5449-2012
OI Tao, Dacheng/0000-0001-7225-5449; Li, Xuelong/0000-0002-0019-4197
CR [Anonymous], 1990, STAT PATTERN RECOGNI
   [Anonymous], P SPIE STORAGE RET 6
   Brand M, 2002, LECT NOTES COMPUT SC, V2350, P707
   Chang T, 1993, IEEE T IMAGE PROCESS, V2, P429, DOI 10.1109/83.242353
   Chen Y., 2001, P IEEE INT C IM PROC, P815
   CHENG YQ, 1992, PATTERN RECOGN, V25, P101, DOI 10.1016/0031-3203(92)90010-G
   Guo GD, 2002, IEEE T NEURAL NETWOR, V13, P811, DOI 10.1109/TNN.2002.1021882
   Hong PY, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P750
   Ishikawa Y., 1998, P 24 INT C VER LARG, P433
   Jain AK, 1996, PATTERN RECOGN, V29, P1233, DOI 10.1016/0031-3203(95)00160-3
   Jain AK, 1998, PATTERN RECOGN, V31, P1369, DOI 10.1016/S0031-3203(97)00131-3
   Ke Liu, 1992, International Journal of Pattern Recognition and Artificial Intelligence, V6, P817, DOI 10.1142/S0218001492000412
   Lei Z, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P721, DOI 10.1109/ICIP.2001.958595
   Lu JW, 2003, IEEE T NEURAL NETWOR, V14, P117, DOI 10.1109/TNN.2002.806629
   Ma W.-Y., 1998, HDB MULTIMEDIA COMPU, P227
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   MAO JC, 1992, PATTERN RECOGN, V25, P173, DOI 10.1016/0031-3203(92)90099-5
   Müller KR, 2001, IEEE T NEURAL NETWOR, V12, P181, DOI 10.1109/72.914517
   NIBLACK W, 1993, P SOC PHOTO-OPT INS, V1908, P173
   Pass G., 1996, P 4 ACM INT C MULT, V96, P65, DOI DOI 10.1145/244130.244148
   Roger AHorn Charles R Johnson., 1990, MATRIX ANAL
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Swets DL, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P192, DOI 10.1109/AFGR.1996.557263
   TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999
   Tao DC, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P441
   Vapnik V., 1999, NATURE STAT LEARNING
   Vasconcelos N, 2001, PROC CVPR IEEE, P3
   Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P947, DOI 10.1109/34.955109
   Yu H, 2001, PATTERN RECOGN, V34, P2067, DOI 10.1016/S0031-3203(00)00162-X
   Zhao W, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P336, DOI 10.1109/AFGR.1998.670971
   Zhou X.S., 2001, P ACM INT C MULTIMED, P137
   Zhou XS, 2001, PROC CVPR IEEE, P11
NR 36
TC 130
Z9 139
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2006
VL 8
IS 4
BP 716
EP 727
DI 10.1109/TMM.2005.861375
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 069BS
UT WOS:000239420300007
OA Green Accepted, Green Submitted
DA 2024-07-18
ER

PT J
AU Jiang, MQ
   Ling, N
AF Jiang, MQ
   Ling, N
TI Low-delay rate control for real-time H.264/AVC video coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE AVC; bit allocation; H.264; JVT; low delay; rate control; video coding
ID BIT ALLOCATION
AB This paper presents an efficient rate control scheme for the H.264/AVC video coding in low-delay environments. In our scheme, we propose an enhancement to the buffer-status based H.264/AVC bit allocation method. The enhancement is by using a PSNR-based frame complexity estimation to improve the existing mean absolute difference based (MAD-based) complexity measure. Bit allocation to each frame is not just computed by encoder buffer status but also adjusted by a combined frame complexity measure. To prevent the buffer from undesirable overflow or underflow under small buffer size constraint in low delay environment, the computed quantization parameter (QP) for the current MB is adjusted based on actual encoding results at that point. We also propose to compare the bits produced by each mode with the average target bits per MB to dynamically modify Lagrange multiplier (gimel(MODE)) for mode decision. The objective of QP and gimel(MODE) adjustment is to produce bits as close to the frame target as possible, which is especially important for low delay applications. Simulation results show that the H.264 coder, using our proposed scheme, obtains significant improvement for the mismatch ratio of target bits and actual bits in all testing cases, achieves a visual quality improvement of about 0.6 dB on the average, performs better for buffer overflow and underflow, and achieves a similar or smaller PSNR deviation.
C1 Santa Clara Univ, Dept Comp Engn, Santa Clara, CA 95053 USA.
C3 Santa Clara University
RP Jiang, MQ (corresponding author), Santa Clara Univ, Dept Comp Engn, Santa Clara, CA 95053 USA.
EM mjiang@scu.edu; nling@scu.edu
CR HE Z, 2004, UNPUB IEEE T BROADCA
   He ZH, 2001, IEEE T CIRC SYST VID, V11, P928, DOI 10.1109/76.937431
   JIANG M, 2004, P IEEE INT C MULT EX
   Lee HJ, 2000, IEEE T CIRC SYST VID, V10, P878, DOI 10.1109/76.867926
   Li ZG, 2003, IEEE T CIRC SYST VID, V13, P472, DOI 10.1109/TCSVT.2003.813420
   LI ZG, 2003, JVTG012RL 7 M
   Lin LJ, 1998, IEEE T CIRC SYST VID, V8, P446, DOI 10.1109/76.709411
   MA S, 2003, P INT S CIRC SYST IS, V2, P892
   MA S, 2003, JVTH017
   RAMCHANDRAN K, 1994, IEEE T IMAGE PROCESS, V3, P533, DOI 10.1109/83.334987
   Ribas-Corbera J, 2000, IEEE T CIRC SYST VID, V10, P1154, DOI 10.1109/76.875518
   Ribas-Corbera J, 1999, IEEE T CIRC SYST VID, V9, P172, DOI 10.1109/76.744284
   SULLIVAN GJ, 2003, JVT1049
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P688, DOI 10.1109/TCSVT.2003.815168
   1993, JTC1SC29WG11 ISO IEC
   2003, JVTG050 ISO IEC MPEG
   1997, ITUTSG15
   2001, JTC1SC29WG11 ISO IEC
NR 18
TC 51
Z9 66
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2006
VL 8
IS 3
BP 467
EP 477
DI 10.1109/TMM.2006.870713
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046OX
UT WOS:000237822000004
DA 2024-07-18
ER

PT J
AU Zhang, C
   Li, J
AF Zhang, C
   Li, J
TI On the compression and streaming of concentric mosaic data for free
   wandering in a realistic environment over the Internet
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE cache; concentric mosaic; image based rendering (IBR); interactive
   browsing; just-in-time (JIT) rendering; reference block coder (RBC);
   virtual media (Vmedia) access protocol
AB In this paper, we describe a system for wandering in a realistic environment over the Internet. The environment is captured by the concentric mosaic, compressed via the reference block coder (RBC), and accessed and delivered over the Internet through the virtual media (Vniedia) access protocol. One of the key contributions of the paper is the proposal of the RBC concentric mosaic coder. The RBC coder not only compresses the huge dataset of the concentric mosaic very efficiently, but also produces a well-organized bitstream that can be accessed just-in-time (JIT). To reconstruct a virtual view, only a portion of the RBC bitstream needs to be accessed and decoded. This greatly reduces the memory and computation requirement of the viewer compared with first decoding the entire concentric mosaic data set and then rendering from the decoded data. Our second contribution is the employment or the Vmedia protocol to deliver the compressed concentric mosaic bitstream just-in-time over the Internet. Only the bitstream segments corresponding to the current view are streamed over the Internet. The delivered bitstream segments are managed by a local Vmedia cache, so that frequently used bitstream segments do not need to be streamed over the Internet repeatedly, and a RBC bitstream larger than the memory capacity can be easily handled. Combining RBC and Vmedia, a concentric mosaic interactive browser is developed through which the user can freely wander in a realistic environment, e.g., rotate around, walk forward/backward, and sidestep, even under a tight bandwidth.
C1 Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
   Microsoft Res China, Beijing 100084, Peoples R China.
C3 Tsinghua University; Microsoft
RP Microsoft Res, Redmond, WA 98052 USA.
EM chazhang@microsoft.com
CR Adelson EH, 1991, COMPUTATIONAL MODELS
   Chen S. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P29, DOI 10.1145/218380.218395
   DEERING M, 1995, P 22 ANN C COMP GRAP, P13
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   *ISOIEC DIS, 1997, 147721 ISO IEC DIS
   KANG SB, 1999, P SOC PHOTO-OPT INS, V3641, P2
   KHODAKOVSKY A, 2000, P SIGGRAPH2000 COMPU
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   LI J, 2001, P SPIE MULTIMEDIA CO
   Li JK, 1998, P IEEE, V86, P1052, DOI 10.1109/5.687829
   LIPPMAN A, 1980, P SIGGRAPH 80, P32
   Luo L, 2002, IEEE T IMAGE PROCESS, V11, P802, DOI [10.1109/TIP.2002.801942, 10.1109/TIR2002.801942]
   Magnor M., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P334, DOI 10.1109/ICIP.1999.817130
   Matsuba SN, 1999, IEEE COMPUT GRAPH, V19, P45, DOI 10.1109/38.749122
   McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398
   Miller G., 1992, Journal of Visualization and Computer Animation, V3, P183, DOI 10.1002/vis.4340030305
   RIPLEY GD, 1989, COMMUN ACM, V32, P811, DOI 10.1145/65445.65448
   Shum HY, 1999, COMP GRAPH, P299, DOI 10.1145/311535.311573
   Taubin G, 1998, P IEEE, V86, P1228, DOI 10.1109/5.687837
   WU M, IEEE T MULTIMEDIA
   Wu YN, 2002, IEEE T MULTIMEDIA, V4, P332, DOI 10.1109/TMM.2002.802838
   ZHANG C, 2000, P IEEE DAT COMPR C S
NR 23
TC 7
Z9 7
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2005
VL 7
IS 6
BP 1170
EP 1182
DI 10.1109/TMM.2005.858406
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 986SY
UT WOS:000233471200018
DA 2024-07-18
ER

PT J
AU Farbiz, F
   Cheok, AD
   Wei, L
   ZhiYing, Z
   Ke, X
   Prince, S
   Billinghurst, M
   Kato, H
AF Farbiz, F
   Cheok, AD
   Wei, L
   ZhiYing, Z
   Ke, X
   Prince, S
   Billinghurst, M
   Kato, H
TI Live three-dimensional content for augmented reality
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE augmented reality; shape-from-silhouette; three-dimensional computer
   vision
AB We describe an augmented reality system for superimposing three-dimensional (3-D) live content onto two-dimensional fiducial markers in the scene. In each frame, the Euclidean transformation between the marker and the camera is estimated. The equivalent virtual view of the live model is then generated and rendered into the scene at interactive speeds. The 3-D structure of the model is calculated using a fast shape-from-silhouette algorithm based on the outputs of 15 cameras surrounding the subject. The novel view is generated by projecting rays through each pixel of the desired image and intersecting them with the 3-D structure. Pixel color is estimated by taking a weighted sum of the colors of the projections of this 3-D point in nearby real camera images. Using this system, we capture live human models and present them via the augmented reality interface at a remote location. We can generate 384 x 288 pixel images of the models at 25 fps, with a latency of < 100 ms. The result gives the strong impression that the model is a real 3-D part of the scene.
C1 Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore.
   York Univ, Toronto, ON M3J 2R7, Canada.
   HIT Lab, Christchurch, New Zealand.
   Osaka Univ, Osaka 5650871, Japan.
C3 National University of Singapore; York University - Canada; Osaka
   University
RP Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore.
EM adriancheok@nus.edu.sg
RI Cheok, Adrian David/AAT-6141-2021; Farbiz, Farzam/AAC-4088-2020;
   Billinghurst, Mark/AAJ-4236-2020
OI Cheok, Adrian David/0000-0001-6316-2339; Farbiz,
   Farzam/0000-0001-8387-6507; Billinghurst, Mark/0000-0003-4172-6759;
   Kato, Hirokazu/0000-0003-3921-2871
CR [Anonymous], P 23 ANN C COMP GRAP
   [Anonymous], P IEEE C COMP VIS PA
   Avidan S, 1998, IEEE T VIS COMPUT GR, V4, P293, DOI 10.1109/2945.765324
   Azuma R, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.963459
   Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355
   BILLINGHURST M, 2000, P IEEE INT C MULT EX
   BILLINGHURST M, 1999, CHI 99 C COMP ACM NE
   Cheung GKM, 2000, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2000.854944
   ELGAMMAL A, 1999, P IEEE ICCV FRAM RAT
   Feiner Steven., 1993, Proceedings of the 6th ACM Symposium on User Interface Software and Technology (UIST '93), P145
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Horprasert T., 1999, P IEEE ICCV 99 FRAM
   Intel, OP SOURC COMP VIS LI
   KANADE T, 1995, IROS '95 - 1995 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS: HUMAN ROBOT INTERACTION AND COOPERATIVE ROBOTS, PROCEEDINGS, VOL 3, P95, DOI 10.1109/IROS.1995.525868
   KATO H, 2002, ARTOOLKIT LIBR VERSI
   KATO H, 2000, P INT S AUGM REAL MU
   Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954
   LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735
   Lok B., 2001, P 2001 S INTERACTIVE, P69, DOI DOI 10.1145/364338.364364
   Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951
   MULLIGAN J, 2001, P IEEE ICIP 01 THESS
   MULLIGAN J, 2001, P IEEE ICCV 01 VANC
   Ogi T, 2001, P IEEE VIRT REAL ANN, P45, DOI 10.1109/VR.2001.913769
   Raskar R., 1998, Computer Graphics, P179, DOI 10.1145/280814.280861
   Seitz SM, 1999, INT J COMPUT VISION, V35, P151, DOI 10.1023/A:1008176507526
   SLABAUGH GG, 2002, P IEEE ICIP 02 ROCH
   SNOW D, 2000, P CVPR 00 HILT HEAD
   STURM P, 2000, P C COMP VIS PATT RE, P1010
   SZELISKI R, 1999, P BRIT MACH VIS C, P314
   VEDULA S, 2002, P EUR SAARBR GERM
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 31
TC 19
Z9 22
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2005
VL 7
IS 3
BP 514
EP 523
DI 10.1109/TMM.2005.846787
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 949OO
UT WOS:000230798200014
DA 2024-07-18
ER

PT J
AU Sun, QB
   Chang, SF
AF Sun, QB
   Chang, SF
TI A secure and robust digital signature scheme for JPEG2000 image
   authentication
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE digital signature; error correction coding (ECC); image authentication;
   JPEG2000; watermarking
ID COMPRESSION
AB In this paper, we present a secure and robust content-based digital signature scheme for verifying the authenticity of JPEG2000 images quantitatively, in terms of a unique concept named lowest authenticable bit rates (LABR). Given a LABR, the authenticity of the watermarked JPEG2000 image will be protected as long as its final transcoded bit rate is not less than the LABR. The whole scheme, which is extended from the crypto data-based digital signature scheme, mainly comprises signature generation/verification, error correction coding (ECC) and watermark embedding/extracting. The invariant features, which are generated from fractionalized bit planes during the procedure of embedded block coding with optimized truncation in JPEG2000, are coded and signed by the sender's private key to generate one crypto signature (hundreds of bits only) per image, regardless of the image size. ECC is employed to tame the perturbations of extracted features caused by processes such as transcoding. Watermarking only serves to store the check information of ECC. The proposed solution can be efficiently incorporated into the JPEG2000 codec (Part 1) and is also compatible with Public Key Infrastructure. After detailing the proposed solution, system performance on security as well as robustness will be evaluated.
C1 Inst Infocom Res, Singapore 119613, Singapore.
   Columbia Univ, Dept Elect Engn, New York, NY 10027 USA.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); Columbia University
RP Inst Infocom Res, Singapore 119613, Singapore.
EM qibin@ee.columbia.edu; sfchang@ee.columbia.edu
CR [Anonymous], P IEEE INT C IM PROC
   Christopoulos C, 2000, IEEE SIGNAL PROC LET, V7, P247, DOI 10.1109/97.863146
   Davida GI, 1998, 1998 IEEE SYMPOSIUM ON SECURITY AND PRIVACY - PROCEEDINGS, P148, DOI 10.1109/SECPRI.1998.674831
   FRIDRCIH J, 2000, P IEEE INT C INF TEC
   Johnson D., 1999, The elliptic curve digital signature algorithm
   Lin CY, 2001, IEEE T CIRC SYST VID, V11, P153, DOI 10.1109/76.905982
   Lin S., 1983, Error Control Coding: Fundamentals and Applications
   Lu CS, 2003, IEEE T MULTIMEDIA, V5, P161, DOI 10.1109/TMM.2003.811621
   SCHNEIDER M, 1996, P ICIP 96 LAUS SWITZ
   Schneier B., 2015, APPL CRYPTOGRAPHY, VSecond
   SU PC, 2001, J VLSI SIGNAL PROC, P35
   Taubman D, 2000, IEEE T IMAGE PROCESS, V9, P1158, DOI 10.1109/83.847830
   TUABMAN D, 1998, JTC1SC29WGIN1020R IS
   Wong PW, 2001, IEEE T IMAGE PROCESS, V10, P1593, DOI 10.1109/83.951543
   WU M, 2000, P ICME 00 NY AUG
   Xie LH, 2001, IEEE T MULTIMEDIA, V3, P242, DOI 10.1109/6046.923823
   2000, 154441 ISO IEC
NR 17
TC 47
Z9 55
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2005
VL 7
IS 3
BP 480
EP 494
DI 10.1109/TMM.2005.846776
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 949OO
UT WOS:000230798200011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Viéron, J
   Guillemot, C
AF Viéron, J
   Guillemot, C
TI Real-time constrained TCP-compatible rate control for video over the
   Internet
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Internet; rate control; real-time; video coding; video communication
AB This paper describes a rate control algorithm that captures not only the behavior of TCP's congestion control avoidance mechanism but also the delay constraints of real-time streams. Building upon the TFRC protocol [11, a new protocol has been designed for estimating the bandwidth prediction model parameters. Making use of RTP and RTCP, this protocol allows to better take into account the multimedia flows characteristics (variable packet size, delay...). Given the current channel state estimated by the above protocol, encoder and decoder buffers states as well as delay constraints of the real-time video source are translated into encoder rate constraints. This global rate control model, coupled with an H.263+ loss resilient video compression algorithm, has been extensively experimented with on various Internet links. The experiments clearly demonstrate the benefits of 1/ the new protocol used for estimating the bandwidth prediction model parameters, adapted to multimedia flows characteristics, and of 2/ the global rate control model encompassing source buffers and end-to-end delay characteristics. The overall system leads to reduce significantly the source timeouts, hence to minimize the expected distortion, for a comparable usage of the TCP-compatible predicted bandwidth.
C1 INRIA, IRISA, F-35042 Rennes, France.
C3 Universite de Rennes; Inria
RP INRIA, IRISA, Campus Beaulieu, F-35042 Rennes, France.
EM Jerome.Vieron@irisa.fr; Christine.Guillemot@irisa.fr
CR Agrawal P, 1998, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS, P259, DOI 10.1109/MMCS.1998.693653
   [Anonymous], 1988, ACM SIGCOMM COMPUTER
   BACELLI F, 2000, TR3986
   BANSAL D, 2000, 806 MITLCS TR
   BORMANN C, 1998, 2429 IETF
   Crowcroft J., 1998, Computer Communication Review, V28, P53, DOI 10.1145/293927.293930
   Floyd S, 2000, ACM SIGCOMM COMP COM, V30, P43, DOI 10.1145/347057.347397
   HANDLEY M, 1998, STRAWMAN SPECIFICATI
   HANDLEY M, 2001, INTERNET DRAFT DRAFT
   Hsu CY, 1999, IEEE J SEL AREA COMM, V17, P756, DOI 10.1109/49.768193
   JACOBSON V, 1990, P 18 INTERNET ENG TA
   LEON PD, 1999, P IEEE INT C AC SPEE, P3097
   MAHDAVI J, 1997, TCP FRIENDLY UNICAST
   MATHIS M, 1997, COMPUT COMMUN REV, V27
   OTT T, 1999, ECN PROTOCOLS TCP PA
   Ott TeunisJ., 1996, The Stationary Behavior of Ideal TCP Congestion Avoidance
   PADHYE J, 1998, ACM SIGCOMM S COMM
   Padhye J., 1999, P NOSSDAV 99
   PADHYE J, 1999, IEEE INTERNET COMPUT
   PAXON V, 1997, THESIS U CALTECH BER
   RADHA H, 1999, P PACKET VIDEO 99
   RAMESH S, 1999, TR9915 N CAR STAT U
   RAMJEE R, 1994, IEEE INFOCOM SER, P680, DOI 10.1109/INFCOM.1994.337672
   REJAIE R, 1999, P INFOCOMM 99
   RIBASCORBERA J, 1997, Q15A20 ITU
   SANDVOSS J, 1994, 439401 IBM EUR NETW
   SCHULTZRINNE H, 1996, 1889 IETF NETW WORK
   Sisalem D., 1998, P NOSSDAV 98
   Steinbach E, 2001, IEEE IMAGE PROC, P962, DOI 10.1109/ICIP.2001.959207
   Tan WT, 1999, IEEE T MULTIMEDIA, V1, P172, DOI 10.1109/6046.766738
NR 30
TC 23
Z9 25
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2004
VL 6
IS 4
BP 634
EP 646
DI 10.1109/tmm.2004.830805
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 838RC
UT WOS:000222729800011
DA 2024-07-18
ER

PT J
AU Mundur, P
   Simon, R
   Sood, AK
AF Mundur, P
   Simon, R
   Sood, AK
TI End-to-end analysis of distributed Video-on-Demand systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE distributed Video-on-Demand; end-to-end admission control; performance
   analysis; resource allocation
AB The focus of the research presented in this paper is the end-to-end analysis of a distributed Video-on-Demand (VoD) system. We analyze the distributed architecture of a VoD system to design global request handling and admission control strategies and evaluate them using global metrics. The performance evaluation methodology developed in this paper helps in determining efficient ways of using all resources in the VoD architecture within the constraints of providing guaranteed high quality service to each request. For instance, our simulation results show that request handling policies based on limited redirection of blocked requests to other resources perform better than load sharing policies. We also show that request handling policies based on redirection have simpler connection establishment semantics than load sharing policies and, therefore, are easily incorporated into reservation or signaling protocols.
C1 Univ Maryland, Dept Comp Sci & Elect Engn, Baltimore, MD 21250 USA.
   George Mason Univ, Dept Comp Sci, Fairfax, VA 22030 USA.
C3 University System of Maryland; University of Maryland Baltimore; George
   Mason University
RP Mundur, P (corresponding author), Univ Maryland, Dept Comp Sci & Elect Engn, Baltimore, MD 21250 USA.
EM pmundur@cs.umbc.edu; simon@cs.gmu.edu; asood@cs.gmu.edu
CR Blake S., 1998, IETFRFC2475
   CHANG E, 1994, P SOC PHOTO-OPT INS, V2185, P208, DOI 10.1117/12.171778
   GEORGIADIS L, 1996, ACM SIGCOMM, P106
   KEETON K, 1995, MULTIMEDIA SYST, V3, P43, DOI 10.1007/BF01219800
   MUNDUR PV, 2000, THESIS G MASON U FAI
   Neogi A, 1999, IEEE NETWORK, V13, P56, DOI 10.1109/65.793693
   Parekh AK, 1993, IEEE ACM T NETWORK, V1, P344, DOI 10.1109/90.234856
   PAREKH AK, 1994, IEEE ACM T NETWORK, V2, P137, DOI 10.1109/90.298432
   Reddy A. L. N., 1993, P ACM MULT C, P225
   YU PS, 1992, P 3 INT WORKSH NETW, P38
   Zhang L., 1993, IEEE Network, V7, P8, DOI 10.1109/65.238150
NR 11
TC 15
Z9 18
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2004
VL 6
IS 1
BP 129
EP 141
DI 10.1109/TMM.2003.819757
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765QP
UT WOS:000188295200010
DA 2024-07-18
ER

PT J
AU Bentaleb, A
   Akcay, MN
   Lim, M
   Begen, AC
   Zimmermann, R
AF Bentaleb, Abdelhak
   Akcay, Mehmet N.
   Lim, May
   Begen, Ali C.
   Zimmermann, Roger
TI BoB: Bandwidth Prediction for Real-Time Communications Using Heuristic
   and Reinforcement Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bandwidth; Quality of experience; Prediction algorithms; Optimization;
   Estimation; Bit rate; Streaming media; Bandwidth prediction; real-time
   communications; reinforcement learning; RTC; WebRTC; AlphaRTC
ID CONGESTION CONTROL; VIDEO; ARCHITECTURE
AB Bandwidth prediction is critical in any Real-time Communication (RTC) service or application. This component decides how much media data can be sent in real time. Subsequently, the video and audio encoder dynamically adapts the bitrate to achieve the best quality without congesting the network and causing packets to be lost or delayed. To date, several RTC services have deployed the heuristic-based Google Congestion Control (GCC), which performs well under certain circumstances and falls short in some others. In this paper, we leverage the advancements in reinforcement learning and propose BoB(Bang-on-Bandwidth) - a hybrid bandwidth predictor for RTC. At the beginning of the RTC session, BoBuses a heuristic-based approach. It then switches to a learning-based approach. BoBpredicts the available bandwidth accurately and improves bandwidth utilization under diverse network conditions compared to the two winning solutions of the ACM MMSys'21 grand challenge on bandwidth estimation in RTC. An open-source implementation of BoBis publicly available for further testing and research.
C1 [Bentaleb, Abdelhak] Concordia Univ, Gina Cody Sch Engn & Comp Sci, Montreal, PQ H3H 2R9, Canada.
   [Lim, May; Zimmermann, Roger] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
   [Akcay, Mehmet N.; Begen, Ali C.] Ozyegin Univ, TR-34794 Istanbul, Turkiye.
C3 Concordia University - Canada; National University of Singapore; Ozyegin
   University
RP Bentaleb, A (corresponding author), Concordia Univ, Gina Cody Sch Engn & Comp Sci, Montreal, PQ H3H 2R9, Canada.
EM abdelhak.bentaleb@concordia.ca; necmettin.akcay@ozu.edu.tr;
   maylim@comp.nus.edu.sg; ali.begen@ozyegin.edu.tr; rogerz@comp.nus.edu.sg
RI Zimmermann, Roger/D-7944-2015; Begen, Ali C./R-5897-2016
OI Zimmermann, Roger/0000-0002-7410-2590; Begen, Ali
   C./0000-0002-0835-3017; Bentaleb, Abdelhak/0000-0002-5382-6530; Lim,
   May/0000-0001-5458-2112
FU Singapore Ministry of Education Academic Research Fund Tier 2
FX No Statement Available
CR Abbasloo S, 2020, SIGCOMM '20: PROCEEDINGS OF THE 2020 ANNUAL CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION ON THE APPLICATIONS, TECHNOLOGIES, ARCHITECTURES, AND PROTOCOLS FOR COMPUTER COMMUNICATION, P632, DOI 10.1145/3387514.3405892
   [Anonymous], 2021, RTC evaluation score
   [Anonymous], 2016, A google congestion control algorithm for real-time communication
   [Anonymous], 2021, Review, V42
   [Anonymous], 2021, Grand challenge on bandwidth estimation for real-time communications
   [Anonymous], 2021, OpenNetLab AlphaRTC
   Arun V, 2018, PROCEEDINGS OF THE 15TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI'18), P329
   Balan T., 2017, eLearn. Softw. Educ., V2, P48
   Begen AC, 2016, IEEE INTERNET COMPUT, V20, P42, DOI 10.1109/MIC.2016.49
   Bentaleb A., 2022, BoB Code
   Bentaleb A, 2022, IEEE T MULTIMEDIA, V24, P2300, DOI 10.1109/TMM.2021.3079288
   Bentaleb A, 2021, IEEE T MULTIMEDIA, V23, P2588, DOI 10.1109/TMM.2020.3013387
   Bentaleb A, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P13, DOI 10.1145/3204949.3204961
   Bentaleb A, 2019, IEEE COMMUN SURV TUT, V21, P562, DOI 10.1109/COMST.2018.2862938
   Bentaleb A, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1296, DOI 10.1145/2964284.2964332
   Bouten N, 2014, IEEE T MULTIMEDIA, V16, P2281, DOI 10.1109/TMM.2014.2362856
   Brakmo L. S., 1994, Computer Communication Review, V24, P24, DOI 10.1145/190809.190317
   Buck B., 2008, ABOUT US
   Cardwell N., 2019, Technical report
   Cardwell N, 2017, COMMUN ACM, V60, P58, DOI 10.1145/3009824
   Carlucci G, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P133, DOI 10.1145/2910017.2910605
   Carlucci G, 2017, IEEE ACM T NETWORK, V25, P2629, DOI 10.1109/TNET.2017.2703615
   Cisco. VNI, 2020, Complete forecast highlights
   Dong M., 2015, P 12 USENIX C NETW S, P395
   Dong M, 2018, PROCEEDINGS OF THE 15TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI'18), P343
   Dong Y, 2021, IEEE T BROADCAST, V67, P824, DOI 10.1109/TBC.2021.3077758
   Fang JY, 2019, Arxiv, DOI arXiv:1912.02222
   Federal Communications Commission, 2016, Raw Data-Measuring Broadband America 2016
   Floyd Sally., 2004, The newreno modification to tcp's fast recovery algorithm
   Fouladi S, 2018, PROCEEDINGS OF THE 15TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI'18), P267
   Herrero R, 2017, TELECOMMUN SYST, V64, P211, DOI 10.1007/s11235-016-0169-z
   Hong RY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2637, DOI 10.1145/3343031.3356063
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Huang TC, 2022, IEEE T MULTIMEDIA, V24, P1350, DOI 10.1109/TMM.2021.3063620
   Huang TC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1208, DOI 10.1145/3240508.3240545
   iproute2, 2022, ABOUT US
   Jacobson V., 1988, Computer Communication Review, V18, P314, DOI 10.1145/52325.52356
   Jansen Bart, 2017, ACM SIGMETRICS Performance Evaluation Review, V45, P56, DOI 10.1145/3199524.3199534
   Jay N., 2019, P 36 INT C MACH LEAR, P3050
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Johansson I., 2020, Self-clocked rate adaptation for multimedia
   Li WZ, 2019, IEEE J SEL AREA COMM, V37, P2621, DOI 10.1109/JSAC.2019.2933761
   Li YS, 2022, IEEE T COMPUT, V71, P1049, DOI 10.1109/TC.2021.3070879
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Mao HZ, 2016, PROCEEDINGS OF THE 15TH ACM WORKSHOP ON HOT TOPICS IN NETWORKS (HOTNETS '16), P50, DOI 10.1145/3005745.3005750
   Mei LF, 2019, LECT NOTES COMPUT SC, V11419, P34, DOI 10.1007/978-3-030-15986-3_3
   Paszke A, 2019, ADV NEUR IN, V32
   Polese M, 2019, IEEE COMMUN SURV TUT, V21, P3584, DOI 10.1109/COMST.2019.2932905
   Riiser Haakon, 2013, P 4 ACM MULT SYST C, P114, DOI DOI 10.1145/2483977.2483991
   Riley G. F., 2010, MODELING TOOLS NETWO, P15, DOI [DOI 10.1007/978-3-642-12331-3_2, DOI 10.1007/978-3-642-12331-32, 10.1007/978-3-642-12331-3_2]
   Sangtae Ha, 2008, Operating Systems Review, V42, P64, DOI 10.1145/1400097.1400105
   Schulman J, 2017, Arxiv, DOI arXiv:1707.06347
   Shea R, 2013, IEEE NETWORK, V27, P16, DOI 10.1109/MNET.2013.6574660
   Spiteri K, 2020, IEEE ACM T NETWORK, V28, P1698, DOI 10.1109/TNET.2020.2996964
   Tianrun Y., 2021, P 12 ACM MULT SYST C, P1
   van der Hooft J, 2016, IEEE COMMUN LETT, V20, P2177, DOI 10.1109/LCOMM.2016.2601087
   Wang B, 2021, MMSYS '21: PROCEEDINGS OF THE 2021 MULTIMEDIA SYSTEMS CONFERENCE, P332, DOI 10.1145/3458305.3479970
   Winstein K, 2013, ACM SIGCOMM COMP COM, V43, P123, DOI 10.1145/2534169.2486020
   Xie XF, 2017, CONEXT'17: PROCEEDINGS OF THE 2017 THE 13TH INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES, P336, DOI 10.1145/3143361.3143381
   Xu Qiang., 2013, ACM MOBISYS
   Yadav PK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1130, DOI 10.1145/3123266.3123390
   Yan FY, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P731
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zaki Y, 2015, ACM SIGCOMM COMP COM, V45, P509, DOI 10.1145/2829988.2787498
   Zaveri P., 2021, Remote work boom
   Zhang H., 2020, P 26 ANN INT C MOB C, P1
   Zhang SY, 2020, T EMERG TELECOMMUN T, V31, DOI 10.1002/ett.4131
   Zhang Y, 2020, INFORM SCIENCES, V506, P395, DOI 10.1016/j.ins.2019.07.096
   Zhou AF, 2019, MOBICOM'19: PROCEEDINGS OF THE 25TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, DOI 10.1145/3300061.3345430
   Zhu X., 2020, Network-assisted dynamic adaptation (NADA): A unified congestion control scheme for real-time media
NR 70
TC 4
Z9 4
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6930
EP 6945
DI 10.1109/TMM.2022.3216456
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP5R1
UT WOS:001133278300003
OA hybrid
DA 2024-07-18
ER

PT J
AU Cao, Z
   Xu, LM
   Chen, DZ
   Gao, HH
   Wu, J
AF Cao, Zheng
   Xu, Liming
   Chen, Danny Z.
   Gao, Honghao
   Wu, Jian
TI A Robust Shape-Aware Rib Fracture Detection and Segmentation Framework
   With Contrastive Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Ribs; Computed tomography; Three-dimensional displays; Image
   segmentation; Task analysis; Training; Deep learning; Computer-aided
   diagnosis; rib fracture detection and segmentation; self-supervised
   contrastive learning; shape-aware model
ID ALGORITHM
AB The rib fracture is a common type of thoracic skeletal trauma, and its inspections using computed tomography (CT) scans are critical for clinical evaluation and treatment planning. However, it is often challenging for radiologists to quickly and accurately detect rib fractures due to tiny objects and blurriness in large 3D CT images. Previous diagnoses for automatic rib fracture mostly relied on deep learning (DL)-based object detection, which highly depends on label quality and quantity. Moreover, general object detection methods did not take into consideration the typically elongated and oblique shapes of ribs in 3D volumes. To address these issues, we propose a shape-aware method based on DL called SA-FracNet for rib fracture detection and segmentation. First, we design a pixel-level pretext task founded on contrastive learning on massive unlabeled CT images. Second, we train the fine-tuned rib fracture detection model based on the pre-trained weights. Third, we develop a fracture shape-aware multi-task segmentation network to delineate the fracture based on the detection result. Experiments demonstrate that our proposed SA-FracNet achieves state-of-the-art rib fracture detection and segmentation performance on the public RibFrac dataset, with a detection sensitivity of 0.926 and segmentation Dice of 0.754. Test on a private dataset also validates the robustness and generalization of our SA-FracNet.
C1 [Cao, Zheng; Xu, Liming] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310058, Peoples R China.
   [Chen, Danny Z.] Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 200444 USA.
   [Gao, Honghao] Shanghai Univ, Sch Comp Engn & Sci, Shanghai 200444, Peoples R China.
   [Wu, Jian] Zhejiang Univ, Affiliated Hosp 2, Sch Med, Hangzhou 310058, Peoples R China.
   [Wu, Jian] Zhejiang Univ, Sch Publ Hlth, Hangzhou 310058, Peoples R China.
C3 Zhejiang University; University of Notre Dame; Shanghai University;
   Zhejiang University; Zhejiang University
RP Gao, HH (corresponding author), Shanghai Univ, Sch Comp Engn & Sci, Shanghai 200444, Peoples R China.; Wu, J (corresponding author), Zhejiang Univ, Affiliated Hosp 2, Sch Med, Hangzhou 310058, Peoples R China.; Wu, J (corresponding author), Zhejiang Univ, Sch Publ Hlth, Hangzhou 310058, Peoples R China.
EM z.cao@zju.edu.cn; sunrise@zju.edu.cn; dchen@nd.edu;
   gaohong-hao@shu.edu.cn; wujian2000@zju.edu.cn
RI PENG, CHENG/KCL-2506-2024; Gao, Honghao/AAX-4529-2020; Cao,
   Zheng/AAB-7910-2022
OI Gao, Honghao/0000-0001-6861-9684; Cao, Zheng/0000-0002-4934-0888; Chen,
   Danny/0000-0001-6565-2884
CR Bier G, 2015, EUR J RADIOL, V84, P2173, DOI 10.1016/j.ejrad.2015.07.023
   Cao Z, 2021, IEEE ENG MED BIO, P3213, DOI 10.1109/EMBC46164.2021.9630536
   Cao Z, 2021, NEUROCOMPUTING, V453, P357, DOI 10.1016/j.neucom.2020.08.086
   Cao Z, 2021, PATTERN RECOGN LETT, V142, P58, DOI 10.1016/j.patrec.2020.12.009
   Caron M., 2020, Advances in neural information processing systems, V33, P9912
   Chaitanya K., 2020, Advances in Neural Information Processing Systems, V33, P12546, DOI DOI 10.48550/ARXIV.2006.10511
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen XL, 2020, Arxiv, DOI arXiv:2003.04297
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Chen XL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9620, DOI 10.1109/ICCV48922.2021.00950
   Chuang Ching-Yao, 2020, ADV NEURAL INFORM PR, V33, P8765, DOI DOI 10.48550/ARXIV.2007.00224
   Dosovitskiy Alexey, 2021, 2021 INT C LEARN REP
   Franke I, 2014, CHILD ABUSE NEGLECT, V38, P1267, DOI 10.1016/j.chiabu.2014.01.021
   Grill Jean-Bastien, 2020, ADV NEURAL INFORM PR
   Gunz S, 2019, Arxiv, DOI arXiv:1908.05467
   He K., 2017, IEEE C COMP VIS PATT, P2961, DOI DOI 10.1109/ICCV.2017.322
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huynh T, 2022, IEEE WINT CONF APPL, P986, DOI 10.1109/WACV51458.2022.00106
   Ibanez V, 2022, FORENSIC SCI MED PAT, V18, P20, DOI 10.1007/s12024-021-00431-8
   Jin L, 2020, EBIOMEDICINE, V62, DOI 10.1016/j.ebiom.2020.103106
   Jin QG, 2019, KNOWL-BASED SYST, V178, P149, DOI 10.1016/j.knosys.2019.04.025
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   LEVINE MD, 1985, IEEE T PATTERN ANAL, V7, P155, DOI 10.1109/TPAMI.1985.4767640
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu JM, 2021, LECT NOTES COMPUT SC, V12966, P546, DOI 10.1007/978-3-030-87589-3_56
   MACKERSIE RC, 1991, J TRAUMA, V31, P443, DOI 10.1097/00005373-199104000-00002
   May L, 2016, BJA EDUC, V16, P26, DOI 10.1093/bjaceaccp/mkv011
   Meng XH, 2021, SKELETAL RADIOL, V50, P1821, DOI 10.1007/s00256-021-03709-8
   Mobiny A, 2018, LECT NOTES COMPUT SC, V11071, P741, DOI 10.1007/978-3-030-00934-2_82
   Pinheiro P. O. O., 2020, Advances in Neural Information Processing Systems (NeurIPS), V33, P4489
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ringl H, 2015, EUR RADIOL, V25, P1865, DOI 10.1007/s00330-015-3598-2
   Roh B, 2021, PROC CVPR IEEE, P1144, DOI 10.1109/CVPR46437.2021.00120
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shuailin Li, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12261), P552, DOI 10.1007/978-3-030-59710-8_54
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Urbaneja A, 2019, EUR J RADIOL, V110, P121, DOI 10.1016/j.ejrad.2018.11.011
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   Van Gansbeke W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10032, DOI 10.1109/ICCV48922.2021.00990
   Wang WG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7283, DOI 10.1109/ICCV48922.2021.00721
   Wang YR, 2021, LECT NOTES COMPUT SC, V12729, P599, DOI 10.1007/978-3-030-78191-0_46
   Weikert T, 2020, KOREAN J RADIOL, V21, P891, DOI 10.3348/kjr.2019.0653
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xie ZD, 2021, PROC CVPR IEEE, P16679, DOI 10.1109/CVPR46437.2021.01641
   Yao L., 2021, SCI REP-UK, V11, P1
   Yuan T., 2013, CHINA MED EQUIP, V9, P92
   Zhang B, 2021, BRIT J RADIOL, V94, DOI 10.1259/bjr.20200870
   Zhang HT, 2020, EUR J NUCL MED MOL I, V47, P2525, DOI 10.1007/s00259-020-04953-1
   Zhou QQ, 2021, EUR RADIOL, V31, P3815, DOI 10.1007/s00330-020-07418-z
   ZIEGLER DW, 1994, J TRAUMA, V37, P975, DOI 10.1097/00005373-199412000-00018
NR 51
TC 10
Z9 10
U1 12
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1584
EP 1591
DI 10.1109/TMM.2023.3263074
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA G2GW2
UT WOS:000987415000018
DA 2024-07-18
ER

PT J
AU Chen, SY
   Lai, YK
   Xia, SH
   Rosin, PL
   Gao, L
AF Chen, Shu-Yu
   Lai, Yu-Kun
   Xia, Shihong
   Rosin, Paul L.
   Gao, Lin
TI 3D Face Reconstruction and Gaze Tracking in the HMD for Virtual
   Interaction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Communication; eye tracking; head-mounted display; real-time facial
   performance capture; user interaction; virtual reality
AB With the rapid development of virtual reality (VR) technology, VR headsets, a.k.a. Head-Mounted Displays (HMDs), are widely available, allowing immersive 3D content to be viewed. A natural need for truly immersive VR is to allow bidirectional communication: the user should be able to interact with the virtual world using facial expressions and eye gaze, in addition to traditional means of interaction. The typical application scenario includes VR virtual conferencing and virtual roaming, where ideally users are able to see other users' expressions and have eye contact with them in the virtual world. In addition, eye gaze also provides a natural means of interaction with virtual objects. Despite significant achievements in recent years for reconstruction of 3D faces from RGB or RGB-D images, it remains a challenge to reliably capture and reconstruct 3D facial expressions including eye gaze when the user is wearing an HMD, because the majority of the face is occluded, especially those areas around the eyes which are essential for recognizing facial expressions and eye gaze. In this paper, we introduce a novel real-time system that is able to capture and reconstruct 3D faces wearing HMDs, and robustly recover eye gaze. We further propose a novel method to map eye gaze directions to the 3D virtual world, which provides a novel and useful interactive mode in VR. We compare our method with state-of-the-art techniques both qualitatively and quantitatively, and demonstrate the effectiveness of our system using live capture.
C1 [Chen, Shu-Yu] Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100190, Peoples R China.
   [Xia, Shihong; Gao, Lin] Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100190, Peoples R China.
   [Xia, Shihong; Gao, Lin] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Lai, Yu-Kun; Rosin, Paul L.] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 4AG, Wales.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Cardiff University
RP Gao, L (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100190, Peoples R China.
EM chenshuyu@ict.ac.cn; laiy4@cardiff.ac.uk; xsh@ict.ac.cn;
   rosinpl@cardiff.ac.uk; gaolin@ict.ac.cn
RI Lai, Yu-Kun/D-2343-2010; Gao, Lin/JNF-0375-2023
OI Rosin, Paul/0000-0002-4965-3884; Lai, Yukun/0000-0002-2094-5680
FU National Natural Science Foundation of China [62102403, 61872440,
   62061136007]; Beijing Municipal Natural Science Foundation for
   Distinguished Young Scholars [JQ21013]; Science and Technology Service
   Network Initiative; Chinese Academy of Sciences [KFJ-STS-QYZD-
   2021-11-001]; Youth Innovation Promotion Association CAS; Royal Society
   Newton Advanced Fellowship [NAF\R2\192151]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62102403, 61872440, and 62061136007, in
   part by the Beijing Municipal Natural Science Foundation for
   Distinguished Young Scholars under Grant JQ21013, in part by the Science
   and Technology Service Network Initiative, Chinese Academy of Sciences
   under Grant KFJ-STS-QYZD- 2021-11-001, and in part by the Youth
   Innovation Promotion Association CAS and Royal Society Newton Advanced
   Fellowship under Grant NAF\R2\192151.
CR [Anonymous], 2017, CERES SOLVER
   [Anonymous], 2017, TOB EYEC
   [Anonymous], 2017, 3D PUP TRACK
   Beeler T, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778777
   Beeler T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964970
   Bradley D, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778778
   Cao C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766943
   Cao C, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601204
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Cao C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462012
   Cao XD, 2012, PROC CVPR IEEE, P2887, DOI 10.1109/CVPR.2012.6248015
   Chai XY, 2021, IEEE T MULTIMEDIA, V23, P2998, DOI 10.1109/TMM.2021.3068567
   Chaudhuri Bindita, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P142, DOI 10.1007/978-3-030-58558-7_9
   Chen JX, 2015, IEEE T IMAGE PROCESS, V24, P1076, DOI 10.1109/TIP.2014.2383326
   Cheng YH, 2020, IEEE T IMAGE PROCESS, V29, P5259, DOI 10.1109/TIP.2020.2982828
   Dierkes K, 2018, 2018 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2018), DOI 10.1145/3204493.3204525
   Fan X, 2021, IEEE T MULTIMEDIA, V23, P1252, DOI 10.1109/TMM.2020.2994506
   Feng Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459936
   Garrido P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508380
   Gutierrez Mlot Esteban, 2016, BIOSTEC 2016. 9th International Joint Conference on Biomedical Engineering Systems and Technologies. Proceedings: HealthInf, P125
   Hansen DW, 2010, IEEE T PATTERN ANAL, V32, P478, DOI 10.1109/TPAMI.2009.30
   HARDY RL, 1971, J GEOPHYS RES, V76, P1905, DOI 10.1029/JB076i008p01905
   Hennessey C., 2008, THESIS U BRIT COLUMB
   Hsieh PL, 2015, PROC CVPR IEEE, P1675, DOI 10.1109/CVPR.2015.7298776
   Jörg S, 2018, ACM T APPL PERCEPT, V15, DOI 10.1145/3238302
   Kar A, 2017, Arxiv, DOI arXiv:1708.01817
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Kong SG, 2015, IEEE T IMAGE PROCESS, V24, P1801, DOI 10.1109/TIP.2015.2405483
   Li H, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766939
   Lombardi S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201401
   Lou JW, 2020, IEEE T MULTIMEDIA, V22, P730, DOI 10.1109/TMM.2019.2933338
   Lu F, 2017, IEEE T IMAGE PROCESS, V26, P1543, DOI 10.1109/TIP.2017.2657880
   M. Technology, 2015, FACE
   Morimoto C. H., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P8, DOI 10.1109/AFGR.2000.840605
   Olszewski K, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980252
   Papoutsaki Alexandra, 2016, P 25 INT JOINT C ART, P3839, DOI [DOI 10.5555/3061053.3061156, 10.5555/3061053.3061156, DOI 10.1145/2702613.2702627]
   Pfeuffer K., 2013, P 26 ANN ACM S US IN, P261, DOI [DOI 10.1145/2501988.2501998, 10.1145/2501988.2501998]
   Pupil labs, 2017, US
   Rekimoto J, 2018, AVI'18: PROCEEDINGS OF THE 2018 INTERNATIONAL CONFERENCE ON ADVANCED VISUAL INTERFACES, DOI 10.1145/3206505.3206544
   Ruhland K., 2014, EUROGRAPHICS STATE O, P69
   Saito S, 2016, LECT NOTES COMPUT SC, V9912, P244, DOI 10.1007/978-3-319-46484-8_15
   Seonwook Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P747, DOI 10.1007/978-3-030-58610-2_44
   SIMPSON WE, 1972, PSYCHON SCI, V29, P197
   Song GX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P923, DOI 10.1145/3240508.3240570
   Sugano Y, 2015, UIST'15: PROCEEDINGS OF THE 28TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P363, DOI 10.1145/2807442.2807445
   Sugano Y, 2014, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2014.235
   Svoboda T, 2005, PRESENCE-VIRTUAL AUG, V14, P407, DOI 10.1162/105474605774785325
   Thies J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3182644
   Tripathi S, 2017, IEEE WINT CONF APPL, P862, DOI 10.1109/WACV.2017.101
   Tu XG, 2021, IEEE T MULTIMEDIA, V23, P1160, DOI 10.1109/TMM.2020.2993962
   Valgaerts L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366206
   Vidal M, 2013, UBICOMP'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P439, DOI 10.1145/2493432.2493477
   Wang C., 2016, ACM T GRAPHIC, V35, P1, DOI DOI 10.1145/2897824.2925947
   Wang M, 2020, COMPUT VIS MEDIA, V6, P3, DOI 10.1007/s41095-020-0162-z
   Wang XM, 2015, INT CONF KNOWL SYS, P260, DOI 10.1109/ISKE.2015.12
   Wei SE, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323030
   Wen X, 2020, IEEE T VIS COMPUT GR, V26, P3457, DOI 10.1109/TVCG.2020.3023573
   Wood E., 2014, P S EYE TRACK RES AP, P207, DOI DOI 10.1145/2578153.2578185
   Wood E, 2016, LECT NOTES COMPUT SC, V9905, P297, DOI 10.1007/978-3-319-46448-0_18
   Wu C., 2018, ACM Transactions on Graphics (TOG), V37, P1
   Wu HY, 2007, LECT NOTES COMPUT SC, V4843, P688
   Zhang HW, 2019, IEEE T IMAGE PROCESS, V28, P4526, DOI 10.1109/TIP.2019.2911114
   Zhao Y., 2016, MASK OFF SYNTHESIZIN, Vabs/1610.08481
NR 63
TC 10
Z9 10
U1 6
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3166
EP 3179
DI 10.1109/TMM.2022.3156820
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200017
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Chopin, B
   Tang, H
   Otberdout, N
   Daoudi, M
   Sebe, N
AF Chopin, Baptiste
   Tang, Hao
   Otberdout, Naima
   Daoudi, Mohamed
   Sebe, Nicu
TI Interaction Transformer for Human Reaction Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Skeleton; Transformers; Task analysis; Three-dimensional displays;
   Decoding; Encoding; Computational modeling; Human reaction generation;
   interaction; transformer
AB We address the challenging task of human reaction generation, which aims to generate a corresponding reaction based on an input action. Most of the existing works do not focus on generating and predicting the reaction and cannot generate the motion when only the action is given as input. To address this limitation, we propose a novel interaction Transformer (InterFormer) consisting of a Transformer network with both temporal and spatial attention. Specifically, temporal attention captures the temporal dependencies of the motion of both characters and of their interaction, while spatial attention learns the dependencies between the different body parts of each character and those which are part of the interaction. Moreover, we propose using graphs to increase the performance of spatial attention via an interaction distance module that helps focus on nearby joints from both characters. Extensive experiments on the SBU interaction, K3HI, and DuetDance datasets demonstrate the effectiveness of InterFormer. Our method is general and can be used to generate more complex and long-term interactions.
C1 [Chopin, Baptiste] Univ Lille, CNRS, Cent Lille, UMR 9189,CRIStAL, F-59000 Lille, France.
   [Tang, Hao] Swiss Fed Inst Technol, Dept Informat Technol & Elect Engn, CH-8092 Zurich, Switzerland.
   [Otberdout, Naima] Univ Mohammed VI Polytech, Movement, Ben Guerir 43150, Morocco.
   [Daoudi, Mohamed] Univ Lille, CNRS, UMR 9189, Centrale Lille,IMT Nord Europe,Inst Mines Telecom,, F-59000 Lille, France.
   [Sebe, Nicu] Univ Trento, Dept Informat Engn & Comp Sci, I-38123 Trento, Italy.
C3 Universite de Lille; Centrale Lille; Centre National de la Recherche
   Scientifique (CNRS); Swiss Federal Institutes of Technology Domain; ETH
   Zurich; Mohammed VI Polytechnic University; Universite de Lille;
   Centrale Lille; IMT - Institut Mines-Telecom; IMT Nord Europe; Centre
   National de la Recherche Scientifique (CNRS); University of Trento
RP Chopin, B (corresponding author), Univ Lille, CNRS, Cent Lille, UMR 9189,CRIStAL, F-59000 Lille, France.
EM baptiste.chopin@univ-lille.fr; hao.tang@vision.ee.ethz.ch;
   naimaotberd@gmail.com; mohamed.daoudi@imt-nord-europe.fr;
   niculae.sebe@unitn.it
RI Sebe, Niculae/KEC-2000-2024; Daoudi, Mohammed/H-5935-2013
OI Sebe, Niculae/0000-0002-6597-7248; Otberdout, naima/0000-0002-5694-0128;
   Daoudi, Mohammed/0000-0003-4219-7860; Tang, Hao/0000-0002-2077-1246
FU CNRS
FX No Statement Available
CR Ahuja C, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P1884
   Ahuja C, 2019, ICMI'19: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P74, DOI 10.1145/3340555.3353725
   Aksan E, 2021, INT CONF 3D VISION, P565, DOI 10.1109/3DV53792.2021.00066
   Al-Rfou R., 2019, P 33 AAAI C ART INT
   Aliakbarian S, 2020, PROC CVPR IEEE, P5222, DOI 10.1109/CVPR42600.2020.00527
   Baruah M, 2020, IEEE COMPUT SOC CONF, P4402, DOI 10.1109/CVPRW50498.2020.00519
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen H., 2021, P BRIT MACH VIS C
   Chen HY, 2022, AAAI CONF ARTIF INTE, P258
   Chopin Baptiste, 2023, IEEE Transactions on Biometrics, Behavior, and Identity Science, P321, DOI 10.1109/TBIOM.2022.3215067
   Cui QJ, 2020, PROC CVPR IEEE, P6518, DOI 10.1109/CVPR42600.2020.00655
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Devanne M, 2015, IEEE T CYBERNETICS, V45, P1340, DOI 10.1109/TCYB.2014.2350774
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494
   Guo C, 2022, LECT NOTES COMPUT SC, V13695, P580, DOI 10.1007/978-3-031-19833-5_34
   Gupta A, 2018, PROC CVPR IEEE, P2255, DOI 10.1109/CVPR.2018.00240
   Habibie I, 2021, PROCEEDINGS OF THE 21ST ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA), P101, DOI 10.1145/3472306.3478335
   Hensel M, 2017, ADV NEUR IN, V30
   Hu T., 2014, J. Comput. Inf. Syst., V10, P1965
   Joo H, 2019, PROC CVPR IEEE, P10865, DOI 10.1109/CVPR.2019.01113
   Kacem A, 2020, IEEE T PATTERN ANAL, V42, P1, DOI 10.1109/TPAMI.2018.2872564
   Kingma D. P., 2014, arXiv
   Kong Z., 2023, P AAAI C ART INT
   Kong ZL, 2022, LECT NOTES COMPUT SC, V13671, P620, DOI 10.1007/978-3-031-20083-0_37
   Kundu JN, 2020, IEEE WINT CONF APPL, P2713, DOI 10.1109/WACV45572.2020.9093627
   Lee G, 2019, IEEE I CONF COMP VIS, P763, DOI 10.1109/ICCV.2019.00085
   Lee H.-Y., 2019, P 33 INT C NEUR INF
   Li RL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13381, DOI 10.1109/ICCV48922.2021.01315
   Li WH, 2022, PROC CVPR IEEE, P13137, DOI 10.1109/CVPR52688.2022.01280
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Lucas T, 2022, LECT NOTES COMPUT SC, V13666, P417, DOI 10.1007/978-3-031-20068-7_24
   Ma TZ, 2022, PROC CVPR IEEE, P6427, DOI 10.1109/CVPR52688.2022.00633
   Maghoumi M, 2020, LECT NOTES COMPUT SC, V11844, P16, DOI 10.1007/978-3-030-33720-9_2
   Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497
   Martinez-Gonzalez A, 2021, IEEE INT CONF COMP V, P2276, DOI 10.1109/ICCVW54120.2021.00257
   Norcliffe-Brown W, 2018, ADV NEUR IN, V31
   Parmar N, 2018, PR MACH LEARN RES, V80
   Petrovich M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10965, DOI 10.1109/ICCV48922.2021.01080
   Plizzari C, 2021, COMPUT VIS IMAGE UND, V208, DOI 10.1016/j.cviu.2021.103219
   Qi XJ, 2017, IEEE I CONF COMP VIS, P5209, DOI 10.1109/ICCV.2017.556
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Sofianos T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11189, DOI 10.1109/ICCV48922.2021.01102
   Song ZY, 2022, Arxiv, DOI arXiv:2203.07706
   Unterthiner T, 2019, Arxiv, DOI arXiv:1812.01717
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang ZY, 2020, AAAI CONF ARTIF INTE, V34, P12281
   Yang GL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16249, DOI 10.1109/ICCV48922.2021.01596
   Yang Y., 2020, P ACM SIGGR EUR S CO
   Yin WJ, 2021, IEEE ROMAN, P641, DOI 10.1109/RO-MAN50785.2021.9515316
   Yoon Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417838
   Yun K., 2012, 2012 IEEE COMP SOC C, P28, DOI DOI 10.1109/CVPRW.2012.6239234
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhao MY, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3579359
   Zhao R, 2020, PROC CVPR IEEE, P6224, DOI 10.1109/CVPR42600.2020.00626
   Zhou Y., 2018, P INT C LEARN REPR
   Zhuetal L., 2021, Pattern Recognit., V115
NR 58
TC 2
Z9 2
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8842
EP 8854
DI 10.1109/TMM.2023.3242152
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000025
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Cui, WX
   Liu, SH
   Jiang, F
   Zhao, DB
AF Cui, Wenxue
   Liu, Shaohui
   Jiang, Feng
   Zhao, Debin
TI Image Compressed Sensing Using Non-Local Neural Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image reconstruction; Neural networks; Reconstruction algorithms;
   Training; Noise reduction; Iterative algorithms; Current measurement;
   Convolutional neural networks (CNNs); image compressed sensing;
   non-local neural network; non-local self-similarity prior
ID THRESHOLDING ALGORITHM; RECONSTRUCTION; REGULARIZATION; FRAMEWORK
AB Deep network-based image Compressed Sensing (CS) has attracted much attention in recent years. However, the existing deep network-based CS schemes either reconstruct the target image in a block-by-block manner that leads to serious block artifacts or train the deep network as a black box that brings about limited insights of image prior knowledge. In this paper, a novel image CS framework using non-local neural network (NL-CSNet) is proposed, which utilizes the non-local self-similarity priors with deep network to improve the reconstruction quality. In the proposed NL-CSNet, two non-local subnetworks are constructed for utilizing the non-local self-similarity priors in the measurement domain and the multi-scale feature domain respectively. Specifically, in the subnetwork of measurement domain, the long-distance dependencies between the measurements of different image blocks are established for better initial reconstruction. Analogically, in the subnetwork of multi-scale feature domain, the affinities between the dense feature representations are explored in the multi-scale space for deep reconstruction. Furthermore, a novel loss function is developed to enhance the coupling between the non-local representations, which also enables an end-to-end training of NL-CSNet. Extensive experiments manifest that NL-CSNet outperforms existing state-of-the-art CS methods, while maintaining fast computational speed.
C1 [Zhao, Debin] Harbin Inst Technol, Dept Comp Sci & Technol, Harbin 150001, Peoples R China.
   Peng Cheng Lab, Shenzhen 518055, Peoples R China.
C3 Harbin Institute of Technology; Peng Cheng Laboratory
RP Zhao, DB (corresponding author), Harbin Inst Technol, Dept Comp Sci & Technol, Harbin 150001, Peoples R China.
EM wenxuecui@stu.hit.edu.cn; shliu@hit.edu.cn; fjiang@hit.edu.cn;
   dbzhao@hit.edu.cn
RI Zhao, Debin/JEP-0204-2023; JIANG, Feng/HTP-2862-2023; Liu,
   Shaohui/AAC-3092-2019
OI Liu, Shaohui/0000-0002-1810-5412; Cui, Wenxue/0000-0001-8656-0954;
   Jiang, Feng/0000-0001-8342-1211
FU National Natural Science Foundation of China [61872116]
FX Manuscript received 22 May 2021; revised 25 September 2021; accepted 25
   November 2021. Date of publication 3 December 2021; date of current
   version 9 March 2023. This work was supported by the National Natural
   Science Foundation of China under Grant 61872116. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Andrew D. Bagdanov. (Corresponding author: Debin
   Zhao.)
CR Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Brown TB, 2020, Arxiv, DOI [arXiv:2005.14165, DOI 10.48550/ARXIV.2005.14165]
   Baraniuk RG, 2010, IEEE T INFORM THEORY, V56, P1982, DOI 10.1109/TIT.2010.2040894
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Chen C, 2011, CONF REC ASILOMAR C, P1193, DOI 10.1109/ACSSC.2011.6190204
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   Christopher M., 2017, P NIPS, P1770
   Cui WX, 2018, IEEE IMAGE PROC, P3883, DOI 10.1109/ICIP.2018.8451841
   Cui WX, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1748, DOI 10.1109/ICASSP.2018.8461766
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dong WS, 2019, IEEE T PATTERN ANAL, V41, P2305, DOI 10.1109/TPAMI.2018.2873610
   Dong WS, 2014, IEEE T IMAGE PROCESS, V23, P3618, DOI 10.1109/TIP.2014.2329449
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Donoho DL, 2009, P NATL ACAD SCI USA, V106, P18914, DOI 10.1073/pnas.0909892106
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Elmoataz A, 2008, IEEE T IMAGE PROCESS, V17, P1047, DOI 10.1109/TIP.2008.924284
   Ender JHG, 2010, SIGNAL PROCESS, V90, P1402, DOI 10.1016/j.sigpro.2009.11.009
   Everingham M., 2011, PATTERN ANAL STAT MO, V8, P5
   Fourure D, 2017, Arxiv, DOI arXiv:1707.07958
   Gan L, 2007, PROCEEDINGS OF THE 2007 15TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, P403
   Gao XW, 2015, IEEE DATA COMPR CONF, P133, DOI 10.1109/DCC.2015.47
   Gilton D, 2020, IEEE T COMPUT IMAG, V6, P328, DOI 10.1109/TCI.2019.2948732
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hegde C, 2014, IEEE INT SYMP INFO, P1842, DOI 10.1109/ISIT.2014.6875152
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Jiwei Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P513, DOI 10.1007/978-3-030-58542-6_31
   Jung MY, 2011, IEEE T IMAGE PROCESS, V20, P1583, DOI 10.1109/TIP.2010.2092433
   Kindermann S, 2005, MULTISCALE MODEL SIM, V4, P1091, DOI 10.1137/050622249
   Kulkarni K, 2016, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2016.55
   Li C., TVAL3: TV minimization by Augmented Lagrangian and ALternating direction ALgorithms
   Li CB, 2013, COMPUT OPTIM APPL, V56, P507, DOI 10.1007/s10589-013-9576-1
   Li SC, 2013, IEEE T IND INFORM, V9, P2177, DOI 10.1109/TII.2012.2189222
   Li W, 2019, IEEE ACCESS, V7, P70910, DOI 10.1109/ACCESS.2019.2918593
   Liu JM, 2020, IEEE J-STSP, V14, P1088, DOI 10.1109/JSTSP.2020.2998402
   Lohit S, 2018, IEEE T COMPUT IMAG, V4, P326, DOI 10.1109/TCI.2018.2846413
   Lustig M, 2008, IEEE SIGNAL PROC MAG, V25, P72, DOI 10.1109/MSP.2007.914728
   MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082
   Mousavi A, 2015, ANN ALLERTON CONF, P1336, DOI 10.1109/ALLERTON.2015.7447163
   Romano Y, 2017, SIAM J IMAGING SCI, V10, P1804, DOI 10.1137/16M1102884
   Shi WZ, 2019, PROC CVPR IEEE, P12282, DOI 10.1109/CVPR.2019.01257
   Shi WZ, 2020, IEEE T IMAGE PROCESS, V29, P375, DOI 10.1109/TIP.2019.2928136
   Shi WZ, 2017, IEEE INT CON MULTI, P877, DOI 10.1109/ICME.2017.8019428
   Sun YB, 2020, IEEE T MULTIMEDIA, V22, P3236, DOI 10.1109/TMM.2020.2973862
   Sun YB, 2020, IEEE T IMAGE PROCESS, V29, P9482, DOI 10.1109/TIP.2020.3023629
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984
   Valsesia D, 2020, IEEE T IMAGE PROCESS, V29, P8226, DOI 10.1109/TIP.2020.3013166
   Valsesia D, 2019, IEEE IMAGE PROC, P2399, DOI [10.1109/icip.2019.8803367, 10.1109/ICIP.2019.8803367]
   Vehkaperä M, 2016, IEEE T INFORM THEORY, V62, P2100, DOI 10.1109/TIT.2016.2525824
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wright SJ, 2009, IEEE T SIGNAL PROCES, V57, P2479, DOI 10.1109/TSP.2009.2016892
   Xu K, 2018, LECT NOTES COMPUT SC, V11214, P491, DOI 10.1007/978-3-030-01249-6_30
   Yao HT, 2019, NEUROCOMPUTING, V359, P483, DOI 10.1016/j.neucom.2019.05.006
   Yue KY, 2018, ADV NEUR IN, V31
   Zhang J, 2020, IEEE J-STSP, V14, P765, DOI 10.1109/JSTSP.2020.2977507
   Zhang J, 2018, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR.2018.00196
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang J, 2012, IEEE DATA COMPR CONF, P287, DOI 10.1109/DCC.2012.71
   Zhang K, 2022, IEEE T PATTERN ANAL, V44, P6360, DOI 10.1109/TPAMI.2021.3088914
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang ZH, 2021, IEEE T IMAGE PROCESS, V30, P1487, DOI 10.1109/TIP.2020.3044472
   Zhao C, 2016, IEEE DATA COMPR CONF, P161, DOI 10.1109/DCC.2016.104
   Zhou SC, 2020, Arxiv, DOI arXiv:2006.16673
   Zhou SW, 2021, IEEE T MULTIMEDIA, V23, P2627, DOI 10.1109/TMM.2020.3014561
NR 71
TC 23
Z9 24
U1 15
U2 51
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 816
EP 830
DI 10.1109/TMM.2021.3132489
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Gao, CS
   Liu, D
   Li, L
   Wu, F
AF Gao, Changsheng
   Liu, Dong
   Li, Li
   Wu, Feng
TI Towards Task-Generic Image Compression: A Study of Semantics-Oriented
   Metrics
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Task analysis; Semantics; Measurement; Feature extraction;
   Image reconstruction; Bit rate; Deep feature distance;
   importance-weighted pixel distance; learned image compression; machine
   vision; task generic
AB Instead of being observed by human, multimedia data are now more and more fed into machines to perform different kinds of semantic analysis. One image may be analyzed multiple times by different machine vision algorithms for different purposes. While machine vision-oriented image compression has been studied, the existing methods are usually driven by a specific machine vision task, and may not be applicable for the other tasks. We address the task-generic image compression, in the hope that an image is compressed once but used multiple times for different tasks, all with satisfactory performance. Our study is based on the end-to-end learned image compression. We focus ourselves on the distortion metric, i.e., finding out a task-agnostic metric to estimate the quality of reconstructed images. On the one hand, we study deep feature distance as the metric, which transforms images into a latent space by a pretrained convolutional network-the latent space is believed to be more aligned to semantics-and calculates distance in the latent space. On the other hand, inspired by the saliency mechanism, we study an importance-weighted pixel distance as the metric, where the weights are generated to reflect the importance of the pixels to semantics. Moreover, we combine the two distances into one metric to investigate their complementary nature. An extensive set of experiments are performed to evaluate these metrics. Experimental results show that using the combined metric performs the best, and leads to 20.79%$\sim$42.69% bits saving under the same semantic analysis performance, compared to using signal fidelity metrics. Interestingly, we observe that using the combined metric also improves the visual quality of the reconstructed images.
C1 [Gao, Changsheng; Liu, Dong; Li, Li; Wu, Feng] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Liu, D (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.
EM gcs@mail.ustc.edu.cn; dongeliu@ustc.edu.cn; lil1@ustc.edu.cn;
   fengwu@ustc.edu.cn
RI Wu, Feng/KCY-3017-2024; Gao, Changsheng/ISS-3579-2023; Liu,
   Dong/K-7488-2012
OI Liu, Dong/0000-0001-9100-2906
FU National Key Research and Development Program of China [2018YFA0701603];
   Natural Science Foundation of China [62022075, 62036005, 62021001]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018YFA0701603, and in part by
   the Natural Science Foundation of China under Grants 62022075, 62036005,
   and 62021001.
CR Agustsson E, 2019, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2019.00031
   Akbari M, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102877
   Akbari M, 2019, INT CONF ACOUST SPEE, P2042, DOI [10.1109/icassp.2019.8683541, 10.1109/ICASSP.2019.8683541]
   Ball‚ J, 2017, Arxiv, DOI [arXiv:1611.01704, 10.48550/arXiv.1611.01704]
   Ball‚ J, 2018, Arxiv, DOI arXiv:1802.01436
   Chamain LD, 2020, Arxiv, DOI arXiv:2011.06409
   Chamain LD, 2019, IEEE INT CON MULTI, P338, DOI 10.1109/ICME.2019.00066
   Chen LH, 2020, Arxiv, DOI arXiv:2007.02711
   Chen ZB, 2019, NEUROCOMPUTING, V338, P16, DOI 10.1016/j.neucom.2019.01.086
   Chen Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2414, DOI 10.1145/3343031.3350849
   Chen Z, 2020, IEEE T IMAGE PROCESS, V29, P2230, DOI 10.1109/TIP.2019.2941660
   Choi H, 2018, IEEE IMAGE PROC, P3743, DOI 10.1109/ICIP.2018.8451100
   Choi Y, 2019, IEEE I CONF COMP VIS, P3146, DOI 10.1109/ICCV.2019.00324
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Duan LY, 2012, 2012 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Duan LY, 2020, IEEE T IMAGE PROCESS, V29, P8680, DOI 10.1109/TIP.2020.3016485
   Duan SY, 2022, Arxiv, DOI arXiv:2005.12810
   Eshratifar AE, 2019, I SYMPOS LOW POWER E, DOI 10.1109/islped.2019.8824955
   Guo Tiansheng, 2020, P IEEECVF C COMPUTER, P122
   Guo Z., 2019, PROC IEEE PICTURE CO, P1
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Hu Y., 2020, PROC IEEE INT C MULT
   Jinyoung Choi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P309, DOI 10.1007/978-3-030-58565-5_19
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Johnston N, 2018, PROC CVPR IEEE, P4385, DOI 10.1109/CVPR.2018.00461
   Kingma D. P., 2014, arXiv
   Lee J., 2020, P IEEECVF C COMPUTER, P144
   Lee JY, 2019, Arxiv, DOI arXiv:1809.10452
   Lee JY, 2020, Arxiv, DOI arXiv:1912.12817
   Lei JJ, 2016, IEEE T MULTIMEDIA, V18, P1783, DOI 10.1109/TMM.2016.2592325
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Li M, 2020, Arxiv, DOI arXiv:2005.04661
   Li M, 2018, PROC CVPR IEEE, P3214, DOI 10.1109/CVPR.2018.00339
   Li ZJ, 2020, Arxiv, DOI arXiv:2003.02874
   Lin T.Y., 2014, P 13 EUR C COMP VIS, P740
   Liu D, 2016, SENS IMAGING, V18, DOI 10.1007/s11220-016-0152-5
   Liu JH, 2020, Arxiv, DOI [arXiv:2002.03370, DOI 10.48550/ARXIV.2002.03370]
   Liu Z, 2018, DES AUT CON, DOI 10.1145/3195970.3196022
   L”hdefink J, 2019, Arxiv, DOI [arXiv:1902.04311, DOI arXiv:1902.04311.v1]
   Luo SH, 2018, LECT NOTES COMPUT SC, V11301, P96, DOI 10.1007/978-3-030-04167-0_9
   Luo XY, 2020, Arxiv, DOI arXiv:2008.00605
   Ma HC, 2022, IEEE T PATTERN ANAL, V44, P1247, DOI 10.1109/TPAMI.2020.3026003
   Man Hoang T., 2020, P IEEE C COMP VIS PA, P160
   Mentzer F, 2020, Arxiv, DOI arXiv:2006.09965
   Minnen D, 2018, ADV NEUR IN, V31
   Patwa N, 2020, IEEE IMAGE PROC, P1281, DOI [10.1109/ICIP40778.2020.9191247, 10.1109/icip40778.2020.9191247]
   Pennebaker W. B., 1992, JPEG STILL IMAGE DAT
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rippel O, 2017, Arxiv, DOI arXiv:1705.05823
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shao JW, 2020, IEEE INT CONF COMM
   Shi J., 2020, IEEE INT SYMP CIRC S, P1, DOI DOI 10.1109/iscas45731.2020.9180454
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh S, 2020, IEEE IMAGE PROC, P3349, DOI 10.1109/ICIP40778.2020.9190860
   Skodras A, 2001, IEEE SIGNAL PROC MAG, V18, P36, DOI 10.1109/79.952804
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Suzuki S, 2020, IEEE IMAGE PROC, P3099, DOI 10.1109/ICIP40778.2020.9190933
   Suzuki S, 2019, IEEE IMAGE PROC, P2686, DOI [10.1109/ICIP.2019.8803275, 10.1109/icip.2019.8803275]
   Thomas T. M.CoverandJ. A., 1991, ELEMENTS INFORM THEO, DOI 10.1002/0471200611
   Toderici G, 2016, Arxiv, DOI arXiv:1511.06085
   Toderici G, 2017, PROC CVPR IEEE, P5435, DOI 10.1109/CVPR.2017.577
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang F, 2020, IEEE SIGNAL PROC LET, V27, P331, DOI 10.1109/LSP.2020.2970539
   Yuan X, 2020, IEEE T MULTIMEDIA, V22, P2889, DOI 10.1109/TMM.2020.2967646
   Zamir AR, 2018, PROC CVPR IEEE, P3712, DOI 10.1109/CVPR.2018.00391
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhong ZS, 2020, Arxiv, DOI arXiv:2007.12619
NR 68
TC 7
Z9 7
U1 3
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 721
EP 735
DI 10.1109/TMM.2021.3130754
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900004
DA 2024-07-18
ER

PT J
AU Gao, XH
   Yang, Y
   Zhang, YM
   Li, MS
   Yu, JG
   Du, SY
AF Gao, Xuehao
   Yang, Yang
   Zhang, Yimeng
   Li, Maosen
   Yu, Jin-Gang
   Du, Shaoyi
TI Efficient Spatio-Temporal Contrastive Learning for Skeleton-Based 3-D
   Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Skeleton; Encoding; Three-dimensional displays; Training;
   Feature extraction; Visualization; self-supervised method; observation
   scene; contrastive learning; 3D action recognition
ID LSTM
AB In this paper, we propose a simple yet effective self-supervised method called spatio-temporal contrastive learning (ST-CL) for 3D skeleton-based action recognition. ST-CL acquires action-specific features by regarding the spatio-temporal continuity of motion tendency as the supervisory signal. To yield effective representations, ST-CL first designs some novel contrastive proxy tasks by providing different spatio-temporal observation scenes for the same 3D action and pulling them together in the embedding space. Second, three key components are devised in the action encoding to efficiently extract representations in contrastive tasks: (1) Information Representation introduces the awareness of joint type when analyzing motion dynamics. (2) Non-local GCN learns a data-driven graph topology structure and promotes a spatial message passing among long-range joints in each frame. (3) Multi-Scale TCN makes larger receptive fields for capturing richer longe-range temporal dynamics amomg adjacent frames. In ST-CL, these effective proxy tasks yield useful representations and efficient action encoding further enhances the representation capacity. As validated on four large-scale datasets, ST-CL is a strong baseline with high performance and efficiency for the contrastive learning study of the skeleton data. Compared to previous self-supervised methods, the proposed ST-CL achieves significant improvement consistently with a smaller model size and better training efficiency.
C1 [Gao, Xuehao; Du, Shaoyi] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Peoples R China.
   [Yang, Yang; Zhang, Yimeng] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
   [Li, Maosen] Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai Key Lab Multimedia Proc & Transmiss, Shanghai 200240, Peoples R China.
   [Yu, Jin-Gang] South China Univ Technol, Sch Automation Sci & Engn, Guangzhou 510641, Peoples R China.
   [Yu, Jin-Gang] Pazhou Lab, Ctr Brain Comp Interface, Guangzhou 510335, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; Shanghai Jiao Tong
   University; South China University of Technology; Pazhou Lab
RP Yang, Y (corresponding author), Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
EM gaoxuehao.xjtu@gmail.com; yyang@mail.xjtu.edu.cn;
   yimengzhang@stu.xjtu.edu.cn; maosen_li@sjtu.edu.cn;
   jingangyu@scut.edu.cn; dushaoyi@gmail.com
OI Gao, Xuehao/0000-0003-3168-5770; Yang, Yang/0000-0001-8687-4427; Du,
   Shaoyi/0000-0002-7092-0596
FU National Key Research and Development Program of China [2018AAA0102500]
FX This work was supported by the National Key Research and Development
   Program of China under Grant 2018AAA0102500. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Zhu Liu.
CR [Anonymous], 2018, Advances in Neural Information Processing Systems
   [Anonymous], 2006, PROC IEEE COMPUT SOC
   Azzam M, 2021, IEEE T MULTIMEDIA, V23, P3318, DOI 10.1109/TMM.2020.3023792
   Bachman P, 2019, ADV NEUR IN, V32
   Bai YT, 2020, Arxiv, DOI arXiv:2011.13046
   Baradel F, 2018, PROC CVPR IEEE, P469, DOI 10.1109/CVPR.2018.00056
   Chen T, 2020, PR MACH LEARN RES, V119
   Dosovitskiy A, 2014, ADV NEUR IN, V27
   Du Y, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P579, DOI 10.1109/ACPR.2015.7486569
   Gidaris S., 2018, P 6 INT C LEARNING R
   Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Hu GY, 2020, IEEE T MULTIMEDIA, V22, P2207, DOI 10.1109/TMM.2019.2953325
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kim D, 2019, AAAI CONF ARTIF INTE, P8545
   Kun Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9628, DOI 10.1109/CVPR42600.2020.00965
   Lee HY, 2017, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2017.79
   Lee I, 2021, IEEE T MULTIMEDIA, V23, P415, DOI 10.1109/TMM.2020.2978637
   Li B, 2017, IEEE INT C COMPUT, P187, DOI 10.1109/CSE-EUC.2017.217
   Lin LL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2490, DOI 10.1145/3394171.3413548
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Qian R, 2021, PROC CVPR IEEE, P6960, DOI 10.1109/CVPR46437.2021.00689
   Rahmani H, 2014, LECT NOTES COMPUT SC, V8690, P742, DOI 10.1007/978-3-319-10605-2_48
   Rao HC, 2021, INFORM SCIENCES, V569, P90, DOI 10.1016/j.ins.2021.04.023
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339
   Wang JL, 2019, PROC CVPR IEEE, P4001, DOI 10.1109/CVPR.2019.00413
   Wei P, 2019, IEEE T MULTIMEDIA, V21, P2195, DOI 10.1109/TMM.2019.2897902
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang JY, 2021, IEEE T MULTIMEDIA, V23, P883, DOI 10.1109/TMM.2020.2990082
   Yao Y, 2020, PROC CVPR IEEE, P6547, DOI 10.1109/CVPR42600.2020.00658
   Yin JL, 2021, IEEE T MULTIMEDIA, V23, P1049, DOI 10.1109/TMM.2020.2992962
   Zhang PF, 2019, IEEE T PATTERN ANAL, V41, P1963, DOI 10.1109/TPAMI.2019.2896631
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhang Z., 2020, Hierarchically decoupled spatial-temporal contrast for self-supervised video representation learning
   Zheng NG, 2018, AAAI CONF ARTIF INTE, P2644
   Zhu KJ, 2020, IEEE T MULTIMEDIA, V22, P2977, DOI 10.1109/TMM.2019.2962304
NR 47
TC 11
Z9 11
U1 6
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 405
EP 417
DI 10.1109/TMM.2021.3127040
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800006
DA 2024-07-18
ER

PT J
AU Guo, KH
   Shen, CC
   Hu, B
   Hu, M
   Kui, XY
AF Guo, Kehua
   Shen, Changchun
   Hu, Bin
   Hu, Min
   Kui, Xiaoyan
TI RSNet: Relation Separation Network for Few-Shot Similar Class
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Task analysis; Training; Estimation; Degradation;
   Visualization; Training data; Few-shot learning; image classification;
   similar class recognition; feature separation
AB Although deep learning methods have drastically improved the performance on visual recognition tasks in which large inter-class variances exist, similar-class recognition continues to pose significant challenges, mainly due to the close resemblance between similar classes. The challenge is further compounded in the case of few-shot learning because only a very small amount of training data is available; accordingly, a certain performance degradation has been observed when some few-shot methods are applied for classification tasks. To address the aforementioned issue, we propose a novel Relation Separation Network (RSNet) in this paper, aiming to boost few-shot learning by improving similar-class recognition performance. We assume that image features consist of commonand private features, where thecommonfeatures capture the basic attributes shared among similar classes and their private counterparts capture the unique attributes of each class. Our RSNet learns to decouple the common and private features of an image. As a result, the feature representation of an image is composed of two weakly associated but easily aligned components, and better classification performance is achieved by giving more attention to subtle features. Experimental results on the publicly available datasets miniImageNet, CUB, and CIFAR-FS show that the proposedmodel outperforms existing state-of-the-artmethods. Specifically, compared to PT+MAP, RSNet improves the accuracy of classification on the CUB dataset by approximately 5% and that of similar-class classification by more than 10%.
C1 [Guo, Kehua; Shen, Changchun; Hu, Bin; Hu, Min; Kui, Xiaoyan] Cent South Univ, Sch Comp Sci & Engn, Changsha 410083, Peoples R China.
C3 Central South University
RP Hu, B (corresponding author), Cent South Univ, Sch Comp Sci & Engn, Changsha 410083, Peoples R China.
EM guokehua@csu.edu.cn; shencc@csu.edu.cn; hubincsu@csu.edu.cn;
   mhu1515@csu.edu.cn; xykui@csu.edu.cn
RI chen, xian/KHW-2227-2024
OI Hu, Bin/0000-0002-2974-1166; Min, Hu/0000-0002-2303-7936; Shen,
   Changchun/0000-0001-9024-8664
FU Natural Science Foundation of China [62076255, 62177047]; Open Research
   Projects of Zhejiang Laboratory [2022RC0AB07]; Hunan Provincial Science
   and Technology Plan [2020SK2059]; Key Projects of Hunan Education
   Department [20A88]; National Science Foundation of Hunan Province
   [2021JJ30082, 2019JJ20025, 2019JJ40406]; National Social Science Fund of
   China [20ZD120]; Fundamental Research Funds for the Central Universities
   of Central South University [2021zzts0754]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 62076255 and 62177047, in part by the Open Research
   Projects of Zhejiang Laboratory under Grant 2022RC0AB07, in part by
   Hunan Provincial Science and Technology Plan under Project 2020SK2059,
   in part by the Key Projects of Hunan Education Department under Grant
   20A88, in part by the National Science Foundation of Hunan Province
   under Grants 2021JJ30082, 2019JJ20025, and 2019JJ40406, in part by the
   National Social Science Fund of China under Grant 20&ZD120, and in part
   by the Fundamental Research Funds for the Central Universities of
   Central South University under Grant 2021zzts0754.
CR BERTINETTO L, 2019, P INT C LEARN REPRES
   Chai Y, 2013, IEEE I CONF COMP VIS, P321, DOI 10.1109/ICCV.2013.47
   Chen JY, 2021, ADV ELECTRON MATER, V7, DOI 10.1002/aelm.202000968
   Chen ZT, 2019, IEEE T IMAGE PROCESS, V28, P4594, DOI 10.1109/TIP.2019.2910052
   Cui Y, 2017, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2017.325
   Finn C, 2017, PR MACH LEARN RES, V70
   GOODFELLOW IJ, 2014, ADV NEURAL INFORMAT
   HU Y, 2020, EXPLOITING UNSUPERVI
   Hu YQ, 2021, LECT NOTES COMPUT SC, V12892, P487, DOI 10.1007/978-3-030-86340-1_39
   Huang HX, 2021, IEEE T MULTIMEDIA, V23, P1666, DOI 10.1109/TMM.2020.3001510
   Jinlu Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P741, DOI 10.1007/978-3-030-58452-8_43
   Khosla A., 2011, P CVPR WORKSH FIN GR
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   KYE SM, 2020, TRANSDUCTIVE FEW SHO
   LI Z, 2017, META SGD LEARNING TO
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu CB, 2020, AAAI CONF ARTIF INTE, V34, P11555
   Mangla P, 2020, IEEE WINT CONF APPL, P2207, DOI [10.1109/wacv45572.2020.9093338, 10.1109/WACV45572.2020.9093338]
   Min WD, 2018, IEEE T INTELL TRANSP, V19, P174, DOI 10.1109/TITS.2017.2756989
   MLLER R, 2019, P ADV NEURAL INF PRO, P1
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Simon C, 2020, PROC CVPR IEEE, P4135, DOI 10.1109/CVPR42600.2020.00419
   Snell J., 2017, ADV NEURAL INFORM PR, V30, P4077
   Sun GL, 2020, AAAI CONF ARTIF INTE, V34, P12047
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tang L., 2020, P IEEECVF C COMPUTER, P14352
   Vedaldi A., 2013, Technical report
   Vinyals O., 2016, ADV NEURAL INFORM PR, P3630, DOI DOI 10.48550/ARXIV.1606.04080
   WAH C, 2011, THE CALTECH UCSD BIR
   Xie LX, 2013, IEEE I CONF COMP VIS, P1641, DOI 10.1109/ICCV.2013.206
   Yang SB, 2023, PUBLIC ADMIN, V101, P406, DOI 10.1111/padm.12794
   Yaoyao Liu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P404, DOI 10.1007/978-3-030-58517-4_24
   Yiwei Lu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P125, DOI 10.1007/978-3-030-58558-7_8
   ZHANG H, 2021, SILL NET FEATURE AUG
   Zhang R., 2018, ARXIV180711428
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhu YH, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1090
   Zhu YH, 2021, IEEE T MULTIMEDIA, V23, P1200, DOI 10.1109/TMM.2020.2993952
NR 38
TC 9
Z9 9
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3894
EP 3904
DI 10.1109/TMM.2022.3168146
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500027
DA 2024-07-18
ER

PT J
AU Hang, TK
   Yang, H
   Liu, B
   Fu, JL
   Geng, X
   Guo, BN
AF Hang, Tiankai
   Yang, Huan
   Liu, Bei
   Fu, Jianlong
   Geng, Xin
   Guo, Baining
TI Language-Guided Face Animation by Recurrent StyleGAN-Based Generator
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modality; face animation; video synthesis
AB Recent works on language-guided image manipu- lation have shown great power of language in providing rich semantics, especially for face images. However, the other natural information, motions, in language is less explored. In this article, we leverage the motion information and study a novel task, language-guided face animation, that aims to animate a static face image with the help of languages. To better utilize both semantics and motions from languages, we propose a simple yet effective framework. Specifically, we propose a recurrent motion generator to extract a series of semantic and motion information from the language and feed it along with visual information to a pre-trained StyleGAN to generate high-quality frames. To optimize the proposed framework, three carefully designed loss functions are proposed including a regularization loss to keep the face identity, a path length regularization loss to ensure motion smoothness, and a contrastive loss to enable video synthesis with various language guidance in one single model. Extensive experiments with both qualitative and quantitative evaluations on diverse domains (e.g., human face, anime face, and dog face) demonstrate the superiority of our model in generating high-quality and realistic videos from one still image with the guidance of language.
C1 [Hang, Tiankai; Yang, Huan; Liu, Bei; Fu, Jianlong; Guo, Baining] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Hang, Tiankai; Geng, Xin; Guo, Baining] Southeast Univ, Sch Comp Sci & Engn, Nanjing 211189, Peoples R China.
   [Hang, Tiankai; Geng, Xin; Guo, Baining] Southeast Univ, Key Lab Comp Network & Informat Integrat, Minist Educ, Nanjing 211189, Peoples R China.
C3 Microsoft; Microsoft Research Asia; Southeast University - China;
   Southeast University - China
RP Geng, X; Guo, BN (corresponding author), Southeast Univ, Sch Comp Sci & Engn, Nanjing 211189, Peoples R China.; Geng, X; Guo, BN (corresponding author), Southeast Univ, Key Lab Comp Network & Informat Integrat, Minist Educ, Nanjing 211189, Peoples R China.
EM tkhang@seu.edu.cn; huayan@microsoft.com; bei.liu@microsoft.com;
   jianf@microsoft.com; xgeng@seu.edu.cn; bainguo@microsoft.com
FU National Key Research amp; Development Plan of China
FX No Statement Available
CR Aila T, 2020, P ADV NEUR INF PROC, P12104, DOI DOI 10.48550/ARXIV.2006.06676
   Austin J, 2021, ADV NEUR IN
   Bau D, 2021, Arxiv, DOI arXiv:2103.10951
   Bhat KS, 2004, ACM T GRAPHIC, V23, P360, DOI 10.1145/1015706.1015729
   Branwen G, 2019, Making anime faces with stylegan
   Brock J., 2019, P INT C LEARN REPR
   Buitelaar P, 2018, IEEE T MULTIMEDIA, V20, P2454, DOI 10.1109/TMM.2018.2798287
   Dhariwal P, 2021, ADV NEUR IN, V34
   Dorkenwald M, 2021, PROC CVPR IEEE, P3741, DOI 10.1109/CVPR46437.2021.00374
   EKMAN P, 1976, ENVIRON PSYCH NONVER, V1, P56, DOI 10.1007/BF01115465
   Gal R, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530164
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang ZC, 2020, Arxiv, DOI arXiv:2004.00849
   Jia C, 2021, PR MACH LEARN RES, V139
   Jiang YG, 2018, IEEE T MULTIMEDIA, V20, P3137, DOI 10.1109/TMM.2018.2823900
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Karras Tero, 2020, IEEE C COMP VIS PATT
   Kim W, 2021, PR MACH LEARN RES, V139
   Kingma D. P., 2014, arXiv
   Kwon G, 2022, PROC CVPR IEEE, P18041, DOI 10.1109/CVPR52688.2022.01753
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Li X., 2020, P EUR C COMP VIS, DOI DOI 10.1007/978-3-030-58577-8_8
   Parmar R., 2022, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, P11410
   Patashnik O, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2065, DOI 10.1109/ICCV48922.2021.00209
   Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramesh A., 2022, arXiv, DOI 10.48550/arXiv.2204.06125
   Ramesh P, 2022, 38 INT C MACHINE LEA
   Saharia C., 2022, P ADV NEURAL INFORM
   Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715
   Shen YJ, 2021, PROC CVPR IEEE, P1532, DOI 10.1109/CVPR46437.2021.00158
   Shen YJ, 2022, IEEE T PATTERN ANAL, V44, P2004, DOI 10.1109/TPAMI.2020.3034267
   Shi J., 2022, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, P19730
   Siarohin A, 2019, ADV NEUR IN, V32
   Singh A, 2022, PROC CVPR IEEE, P15617, DOI 10.1109/CVPR52688.2022.01519
   Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24
   Tian Y., 2021, P INT C LEARN REPR
   Tov O, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459838
   Tu XG, 2022, IEEE T CIRC SYST VID, V32, P1805, DOI 10.1109/TCSVT.2021.3083257
   Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C, 2022, PROC CVPR IEEE, P3825, DOI 10.1109/CVPR52688.2022.00381
   Wang T., 2018, ARXIV
   Wei TY, 2022, PROC CVPR IEEE, P18051, DOI 10.1109/CVPR52688.2022.01754
   Xia WH, 2021, PROC CVPR IEEE, P2256, DOI 10.1109/CVPR46437.2021.00229
   Xu B, 2015, Arxiv, DOI [arXiv:1505.00853, DOI 10.48550/ARXIV.1505.00853]
   Xue H, 2022, P IEEECVF C COMPUTER, P5036
   Xue HW, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P291, DOI 10.1145/3474085.3475421
   Ye ZP, 2023, IEEE T MULTIMEDIA, V25, P2033, DOI 10.1109/TMM.2022.3142387
   Yuan L., 2021, arXiv
   Zellers R, 2021, Advances in Neural Information Processing Systems, V34
   Zhao L, 2018, LECT NOTES COMPUT SC, V11219, P403, DOI 10.1007/978-3-030-01267-0_24
NR 53
TC 1
Z9 1
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9216
EP 9227
DI 10.1109/TMM.2023.3248143
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200040
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hu, XY
   Shen, LQ
   Jiang, MX
   Ma, R
   An, P
AF Hu, Xiangyu
   Shen, Liquan
   Jiang, Mingxing
   Ma, Ran
   An, Ping
TI LA-HDR: Light Adaptive HDR Reconstruction Framework for Single LDR Image
   Considering Varied Light Conditions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic range; Noise reduction; Neural networks; Histograms;
   Convolutional neural networks; Cameras; Deep learning; high dynamic
   range (HDR); image enhancement; inverse tone mapping; multi-image fusion
ID DYNAMIC-RANGE EXPANSION; TONE MAPPING OPERATOR; CAMERA RESPONSE; NETWORK
AB The high dynamic range (HDR) image recovery from the low dynamic range (LDR) image aims to estimate HDR image by decompressing luminance range and enhancing details of the LDR input. In practical usages, when faced with the over-exposed, the under-exposed or the low-light images, the state-of-art prediction methods lack the capability for ideally handling them. Aiming for this, a light adaptation HDR recovery framework (LA-HDR) is proposed, which includes the multi-images generation for adaptive details amplification in different light ranges, and the following multi-details fusion. To create the multi-images, first, the designed bit-depth enhancement network (EnhanceNet) produces the high bit-depth result with enhanced contrast. This result can be furtherly processed by user-defined denoising method to refrain the low-light noise. Meanwhile, the proposed exposure bias network (EBNet) estimates the global exposure bias of the input for rectifying the mid-range details. With the enhanced result and the exposure bias, the designed transfer functions adaptively create three multi-images containing the enhanced details in different light ranges, and they are fused by the designed multi-images fusion network (FuseNet) for the final HDR prediction. The amplification and fusion scheme ensures robust HDR recovery under different light conditions, eliminating high-light recovery artifacts from previous methods. The proposed fusion masks generation (FMG) and the global feature embedding (GFE) modules in FuseNet help eliminate the fusion artifacts. Experimental results show that LA-HDR acquires the best average performance under various light conditions, and it receives low influence from the input light conditions among the tested state-of-art HDR recovery methods.
C1 [Hu, Xiangyu; Ma, Ran; An, Ping] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
   [Shen, Liquan] Shanghai Univ, Key Lab Specialty Fiber Opt & Opt Access Networks, Joint Int Res Lab Specialty Fiber Opt & Adv Commun, Shanghai 200444, Peoples R China.
   [Jiang, Mingxing] Hefei Univ Technol, Anhui Prov Key Lab Affect Comp & Adv Intel ligent, Hefei 230602, Peoples R China.
C3 Shanghai University; Shanghai University; Hefei University of Technology
RP Shen, LQ (corresponding author), Shanghai Univ, Key Lab Specialty Fiber Opt & Opt Access Networks, Joint Int Res Lab Specialty Fiber Opt & Adv Commun, Shanghai 200444, Peoples R China.
EM arhu314@shu.edu.cn; jsslq@163.com; mx0551@163.com; maran@shu.edu.cn;
   anping@shu.edu.cn
RI Shen, Liquan/D-4832-2012
OI Shen, Liquan/0000-0002-2148-6279; Jiang, Mingxing/0000-0001-7474-6612
FU National Natural Science Foundation of China [61931022, 61671282]; Open
   Fund of Key Laboratory of Advanced Display and System Applications of
   Ministry of Education (Shanghai University); Shanghai Science and
   Technology Innovation Plan [18010500200]; Shanghai Shuguang Program
   [17SG37]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61931022 and 61671282, in part by the
   Open Fund of Key Laboratory of Advanced Display and System Applications
   of Ministry of Education (Shanghai University), in part by Shanghai
   Science and Technology Innovation Plan under Grant 18010500200, and in
   part by Shanghai Shuguang Program under Grant 17SG37. The Associate
   Editor coordinating the review of this manuscript and approving it for
   publication was Dr. Alexandros Michael Tourapis.
CR Abdelhamed A, 2018, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2018.00182
   Akyüz AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276425
   Anderson E F., 2007, ACM SIGGRAPH 2007 educators program, P7
   [Anonymous], 1999, P 1999 IEEE COMP SOC
   [Anonymous], 1988, Pulse Code Modulation (PCM) of Voice Frequencies
   [Anonymous], 2018, BT. 2100: Image Parameter Values for High Dynamic Range Television for Use in Production and International Programme Exchange
   Avi-Aharon M, 2020, Arxiv, DOI arXiv:2005.03995
   Banterle F., 2006, Computer Graphics and Interactive Techniques in Australasia and Southeast Asia Association for Computing Machinery, P349, DOI 10.1145/1174429.1174489
   Banterle F, 2007, VISUAL COMPUT, V23, P467, DOI 10.1007/s00371-007-0124-9
   Brooks T, 2019, PROC CVPR IEEE, P11028, DOI 10.1109/CVPR.2019.01129
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Chen J, 2007, ACM T GRAPHIC, V26, DOI [10.1109/SARNOF.2007.4567317, 10.1145/1276377.1276506, 10.1145/1239451.1239554]
   Çogalan U, 2020, IEEE T IMAGE PROCESS, V29, P7511, DOI 10.1109/TIP.2020.3004014
   Dang-Nguyen D.T., 2015, P ACM MULT SYST C, P219
   Debattista K, 2015, VISUAL COMPUT, V31, P1089, DOI 10.1007/s00371-015-1121-z
   Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816
   Endo Y, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130834
   Fairchild MD, 2007, FIFTEENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, AND APPLICATIONS, FINAL PROGRAM AND PROCEEDINGS, P233
   Funt B., 2010, PROC HUM VIS ELECT I, P1
   Funt B, 2010, COLOR IMAG CONF, P256
   Grossberg MD, 2004, IEEE T PATTERN ANAL, V26, P1272, DOI 10.1109/TPAMI.2004.88
   Grossberg MD, 2003, IEEE T PATTERN ANAL, V25, P1455, DOI 10.1109/TPAMI.2003.1240119
   Hasinoff SW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980254
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang YF, 2021, IEEE T MULTIMEDIA, V23, P176, DOI 10.1109/TMM.2020.2981994
   Jagatap G, 2020, INT CONF ACOUST SPEE, P9289, DOI [10.1109/ICASSP40776.2020.9054218, 10.1109/icassp40776.2020.9054218]
   Jang H, 2020, IEEE ACCESS, V8, P38554, DOI 10.1109/ACCESS.2020.2975857
   Kalantari NK, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073609
   Kim DE, 2020, IEEE T CIRC SYST VID, V30, P400, DOI 10.1109/TCSVT.2019.2892438
   Kim JH, 2021, AAAI CONF ARTIF INTE, V35, P1780
   Kim SY, 2019, IEEE I CONF COMP VIS, P3116, DOI 10.1109/ICCV.2019.00321
   Kingma D. P., 2014, arXiv
   Kinoshita Y, 2019, EUR SIGNAL PR CONF, DOI 10.23919/eusipco.2019.8902744
   Kovaleski RP, 2014, 2014 27TH SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND IMAGES (SIBGRAPI), P49, DOI 10.1109/SIBGRAPI.2014.29
   Kuo PH, 2012, 2012 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Lee S, 2021, IEEE T MULTIMEDIA, V23, P2561, DOI 10.1109/TMM.2020.3013378
   Lee S, 2018, LECT NOTES COMPUT SC, V11206, P613, DOI 10.1007/978-3-030-01216-8_37
   Lee S, 2018, IEEE ACCESS, V6, P49913, DOI 10.1109/ACCESS.2018.2868246
   Lian J., 2019, P IEEE VIS COMM IM P, P1
   Liu YL, 2020, PROC CVPR IEEE, P1648, DOI 10.1109/CVPR42600.2020.00172
   Liu Z, 2021, IEEE COMPUT SOC CONF, P463, DOI 10.1109/CVPRW53098.2021.00057
   Marnerides D, 2018, COMPUT GRAPH FORUM, V37, P37, DOI 10.1111/cgf.13340
   Masia B, 2017, MULTIMED TOOLS APPL, V76, P631, DOI 10.1007/s11042-015-3036-0
   Masia B, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618506
   Mertens T, 2009, COMPUT GRAPH FORUM, V28, P161, DOI 10.1111/j.1467-8659.2008.01171.x
   Meylan L., 2006, COLOR IMAGING C, V1, P333, DOI [10.2352/CIC.2006.14.1.ART00061, DOI 10.2352/CIC.2006.14.1.ART00061]
   Moriwaki K, 2018, Arxiv, DOI arXiv:1812.07134
   Narwaria M, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.1.010501
   Niehoff P, 2008, J CRANIO MAXILL SURG, V36, P203, DOI 10.1016/j.jcms.2008.01.003
   Ning SY, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1383, DOI 10.1109/ICASSP.2018.8462444
   Panetta K, 2021, IEEE ACCESS, V9, P39500, DOI 10.1109/ACCESS.2021.3064295
   Prabhakar K. R., ser. Lecture Notes in Computer Science
   Quan YH, 2020, PROC CVPR IEEE, P1887, DOI 10.1109/CVPR42600.2020.00196
   Rana A, 2019, IEEE T MULTIMEDIA, V21, P256, DOI 10.1109/TMM.2018.2839885
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Reinhard S. P. E., 2005, High Dynamic Range Imaging: Acquisition, Display, and Image-Based Lighting
   Rempel A.G., 2009, P 6 S APPL PERC GRAP, P45
   Rempel AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239490
   Robertson M. A., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P159, DOI 10.1109/ICIP.1999.817091
   Rosh KSG, 2019, IEEE IMAGE PROC, P4714, DOI [10.1109/ICIP.2019.8803582, 10.1109/icip.2019.8803582]
   Santos MS, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392403
   Schlick C., 1995, Quantization Techniques for Visualization of High Dynamic Range Pictures
   Sharma A., 2020, arXiv
   Wang C, 2019, IEEE ACCESS, V7, P74558, DOI 10.1109/ACCESS.2019.2920951
   Wang TH, 2015, IEEE T MULTIMEDIA, V17, P470, DOI 10.1109/TMM.2015.2403612
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZX, 2020, PROC CVPR IEEE, P1817, DOI 10.1109/CVPR42600.2020.00189
   Xiao F., 2002, The Tenth Color Imaging Conference: Color Science andEngineering Systems, Technologies, Applications, P337
   Xu Y., 2019, PROC IEEE INT C IMAG, P1
   Yan QS, 2020, IEEE T IMAGE PROCESS, V29, P4308, DOI 10.1109/TIP.2020.2971346
   Yan QS, 2019, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2019.00185
   Zamir SW, 2020, PROC CVPR IEEE, P2693, DOI 10.1109/CVPR42600.2020.00277
NR 73
TC 3
Z9 3
U1 5
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4814
EP 4829
DI 10.1109/TMM.2022.3183404
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300015
DA 2024-07-18
ER

PT J
AU Koepke, AS
   Oncescu, AM
   Henriques, JF
   Akata, Z
   Albanie, S
AF Koepke, A. Sophia
   Oncescu, Andreea-Maria
   Henriques, Joao F.
   Akata, Zeynep
   Albanie, Samuel
TI Audio Retrieval With Natural Language Queries: A Benchmark Study
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio retrieval; text-based retrieval; datasets
ID CLASSIFICATION; SEARCH; TEXT
AB The objectives of this work are cross-modal text-audio and audio-text retrieval, in which the goal is to retrieve the audio content from a pool of candidates that best matches a given written description and vice versa. Text-audio retrieval enables users to search large databases through an intuitive interface: they simply issue free-form natural language descriptions of the sound they would like to hear. To study the tasks of text-audio and audio-text retrieval, which have received limited attention in the existing literature, we introduce three challenging new benchmarks. We first construct text-audio and audio-text retrieval benchmarks from the AudioCaps and Clotho audio captioning datasets. Additionally, we introduce the SoundDescs benchmark, which consists of paired audio and natural language descriptions for a diverse collection of sounds that are complementary to those found in AudioCaps and Clotho. We employ these three benchmarks to establish baselines for cross-modal text-audio and audio-text retrieval, where we demonstrate the benefits of pre-training on diverse audio tasks. We hope that our benchmarks will inspire further research into audio retrieval with free-form text queries.
C1 [Koepke, A. Sophia; Akata, Zeynep] Univ Tubingen, Explainable Machine Learning Grp, D-72074 Tubingen, Germany.
   [Akata, Zeynep] Max Planck Inst Intelligent Syst, D-66123 Saarbrucken, Germany.
   [Oncescu, Andreea-Maria; Henriques, Joao F.] Univ Oxford, Visual Geometry Grp, Oxford OX1 2JD, England.
   [Albanie, Samuel] Univ Cambridge, Dept Engn, Cambridge CB21TN, England.
C3 Eberhard Karls University of Tubingen; Max Planck Society; University of
   Oxford; University of Cambridge
RP Koepke, AS (corresponding author), Univ Tubingen, Explainable Machine Learning Grp, D-72074 Tubingen, Germany.; Oncescu, AM (corresponding author), Univ Oxford, Visual Geometry Grp, Oxford OX1 2JD, England.
EM a-sophia.koepke@uni-tuebingen.de; oncescu@robots.ox.ac.uk;
   joao@robots.ox.ac.uk; zeynep.akata@uni-tuebingen.de;
   albanie@robots.ox.ac.uk
FU ERC [853489 -DEXIM]; DFG [2064/1, 390727645]; BMBF [FKZ: 01IS18039A];
   EPSRC DTA Studentship; Royal Academy of Engineering [RF\201819\18\163];
   EPSRC [EP/T028572/1]; EPSRC [2285346] Funding Source: UKRI
FX The work of A. Sophia Koepke and Zeynep Akata was supported in part by
   the ERC under Grant 853489 -DEXIM, in part by the DFG 2064/1 - under
   Project 390727645, and in part by the BMBF under Grant FKZ: 01IS18039A.
   The work of Andreea-Maria Oncescu was supported by the EPSRC DTA
   Studentship. The work of Joao F. Henriques was supported by the Royal
   Academy of Engineering under Grant RF\201819\18\163. The work of Samuel
   Albanie was supported by EPSRC EP/T028572/1 Visual AI.& nbsp;
CR Abu-El-Haija Sami, 2016, arXiv, DOI [DOI 10.48550/ARXIV.1609.08675, DOI 10.48550/-ARXIV.1609.08675]
   [Anonymous], 2020, DCASE2020 CHALL TASK
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   ATREY P.K., 2006, 2006 IEEE INT C AC S, DOI DOI 10.1109/ICASSP.2006.1661400
   Aucouturier JJ, 2007, J ACOUST SOC AM, V122, P881, DOI 10.1121/1.2750160
   Avgoustinakis P, 2021, Arxiv, DOI arXiv:2010.08737
   Aytar Y, 2017, Arxiv, DOI arXiv:1706.00932
   Bain M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1708, DOI 10.1109/ICCV48922.2021.00175
   Banerjee S., 2005, P ACL WORKSH INTR EX, P65
   Chechik G., 2008, P 1 ACM INT C MULT I, P105, DOI DOI 10.1145/1460096.1460115
   Chen HL, 2020, INT CONF ACOUST SPEE, P721, DOI [10.1109/ICASSP40776.2020.9053174, 10.1109/icassp40776.2020.9053174]
   Cong Jin, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12666), P301, DOI 10.1007/978-3-030-68780-9_26
   Croitoru I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11563, DOI 10.1109/ICCV48922.2021.01138
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dong JF, 2016, Arxiv, DOI arXiv:1604.06838
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Drossos K, 2020, INT CONF ACOUST SPEE, P736, DOI [10.1109/icassp40776.2020.9052990, 10.1109/ICASSP40776.2020.9052990]
   Drossos K, 2017, IEEE WORK APPL SIG, P374, DOI 10.1109/WASPAA.2017.8170058
   Elizalde B, 2018, Arxiv, DOI arXiv:1801.05544
   Elizalde B, 2019, INT CONF ACOUST SPEE, P4095, DOI 10.1109/ICASSP.2019.8682632
   Eren AÖ, 2020, IEEE INT SYM MULTIM, P41, DOI 10.1109/ISM.2020.00014
   Fonseca E, 2020, Arxiv, DOI arXiv:1906.02975
   Font F., 2013, P 2013 ACM MULTIMEDI, P411, DOI 10.1145/2502081.2502245
   Foote JT, 1997, P SOC PHOTO-OPT INS, V3229, P138, DOI 10.1117/12.290336
   Ford L, 2019, INTERSPEECH, P2568, DOI 10.21437/Interspeech.2019-2731
   Foster Peter, 2015, 2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). Proceedings, P1, DOI 10.1109/WASPAA.2015.7336899
   Gabeur Valentin, 2020, P EUR C COMP VIS, V12349, P214, DOI [10.48550/arXiv.2007.10639, DOI 10.48550/ARXIV.2007.10639]
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Ghadiyaram D, 2019, PROC CVPR IEEE, P12038, DOI 10.1109/CVPR.2019.01232
   github, RANG OPT
   Google, SPEECH TO TEXT API
   Harwath D, 2018, LECT NOTES COMPUT SC, V11210, P659, DOI 10.1007/978-3-030-01231-1_40
   He KM, 2015, Arxiv, DOI [arXiv:1512.03385, 10.48550/arxiv.1512.03385]
   Helén M, 2007, INT CONF ACOUST SPEE, P225
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hou SJ, 2013, MATH PROBL ENG, V2013, DOI 10.1155/2013/972438
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ikawa S., 2018, DETECTION CLASSIFICA, P59
   Ikawa S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P346, DOI 10.1109/ICASSP.2018.8462034
   Jin Q, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P2083
   Kim CD, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P119
   Koizumi Y., 2020, arXiv
   Kong QQ, 2020, IEEE-ACM T AUDIO SPE, V28, P2880, DOI 10.1109/TASLP.2020.3030497
   Kong QQ, 2019, IEEE-ACM T AUDIO SPE, V27, P1791, DOI 10.1109/TASLP.2019.2930913
   Kong QQ, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P316, DOI 10.1109/ICASSP.2018.8461392
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Lallemand I., 2012, SMC2012, P1
   Liu LY, 2021, Arxiv, DOI arXiv:1908.03265
   Liu X., 2021, ARXIV
   Liu Y., 2019, PROC BRIT MACH VIS C
   Luo HS, 2021, Arxiv, DOI arXiv:2104.08860
   Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12
   Manocha P, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P3136, DOI 10.1109/ICASSP.2018.8461524
   Mei XH, 2021, Arxiv, DOI arXiv:2107.09817
   Mesaros A., 2017, **DATA OBJECT**, DOI [10.5281/zenodo.400515, 10.5281/ZENODO.400515]
   Mesaros A, 2017, DCASE 2017 WORKSHOP
   Miech A, 2020, Arxiv, DOI arXiv:1804.02516
   Mikolov T, 2013, Arxiv, DOI [arXiv:1301.3781, 10.48550/arXiv.1301.3781]
   Mithun NC, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/3206025.3206064
   Nagrani A, 2018, LECT NOTES COMPUT SC, V11217, P73, DOI 10.1007/978-3-030-01261-8_5
   Oncescu AM, 2021, INTERSPEECH, P2411, DOI 10.21437/Interspeech.2021-2227
   Oncescu AM, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2265, DOI 10.1109/ICASSP39728.2021.9414640
   Piczak KJ, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1015, DOI 10.1145/2733373.2806390
   Slaney M, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P345, DOI 10.1109/ICME.2002.1035789
   Slaney M, 2002, INT CONF ACOUST SPEE, P4108
   Socher R., 2014, Trans Assoc Comput Linguist, V2, P207, DOI [DOI 10.1162/TACLA00177, 10.1162/tacl_a_00177, DOI 10.1162/TACL_A_00177]
   Stowell D, 2015, IEEE T MULTIMEDIA, V17, P1733, DOI 10.1109/TMM.2015.2428998
   Surís D, 2019, LECT NOTES COMPUT SC, V11132, P711, DOI 10.1007/978-3-030-11018-5_62
   Tang HY, 2023, Arxiv, DOI arXiv:2106.14136
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Video Description Research and Development Center, 2013, YOUD
   Wold E, 1996, IEEE MULTIMEDIA, V3, P27, DOI 10.1109/93.556537
   Wray M, 2019, IEEE I CONF COMP VIS, P450, DOI 10.1109/ICCV.2019.00054
   Wu MY, 2019, INT CONF ACOUST SPEE, P830, DOI 10.1109/ICASSP.2019.8682377
   www.robots.ox.ac.uk, PROJ PAG
   Xiong ZY, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P401
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu R, 2015, AAAI CONF ARTIF INTE, P2346
   Xu XA, 2021, Arxiv, DOI arXiv:2102.11457
   Xu XN, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P606, DOI 10.1109/ICASSP39728.2021.9414834
   Yasuda M, 2020, INTERSPEECH, P1446, DOI 10.21437/Interspeech.2020-2445
   Yu CS, 2018, Arxiv, DOI arXiv:1803.02353
   Zeng DH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3387164
   Zhang MR, 2019, ADV NEUR IN, V32
   Zhong YJ, 2019, LECT NOTES COMPUT SC, V11362, P35, DOI 10.1007/978-3-030-20890-5_3
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 90
TC 7
Z9 7
U1 4
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2675
EP 2685
DI 10.1109/TMM.2022.3149712
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lebreton, P
   Yamagishi, K
AF Lebreton, Pierre
   Yamagishi, Kazuhisa
TI Quitting Ratio-Based Bitrate Ladder Selection Mechanism for Adaptive
   Bitrate Video Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bit rate; Streaming media; Encoding; Complexity theory; Estimation;
   Throughput; Video coding; Adaptive bitrate video streaming; bitrate
   ladder; quality of experience; engagement; user quitting ratio
ID QUALITY; ACCEPTABILITY
AB To improve users' experience and decrease their likelihood of quitting watching videos, this paper addresses the question of how to encode the videos used in adaptive bitrate (ABR) video streaming. When addressing ABR video streaming, a lot of effort has been put into developing ABR control schemes. However, ways to appropriately encode videos also need to be defined. Unlike previous approaches that focus on coding quality, this paper considers the user quitting ratio. The user quitting ratio is the percentage of users still watching videos at a given time and enables us to address the consequences of quality and stimulus duration on the decision of a user to quit. Considering the value of the user quitting ratio, this paper describes a method that uses content analysis, as well as a network's historical throughput data, to define how video should be encoded to decrease the likelihood of users quitting watching. Unlike previous approaches, the method is independent of the ABR control scheme used by the video player, and the selected ladders perform equivalently across different players with different behaviors. Results of experiments based on real-world network traces demonstrate the usefulness of the proposed method.
C1 [Lebreton, Pierre; Yamagishi, Kazuhisa] NTT Corp, NTT Network Serv Syst Labs, Tokyo 1808585, Japan.
C3 Nippon Telegraph & Telephone Corporation
RP Lebreton, P (corresponding author), NTT Corp, NTT Network Serv Syst Labs, Tokyo 1808585, Japan.
EM lebreton.pierre.mz@hco.ntt.co.jp; kazuhisa.yamagishi.vf@hco.ntt.co.jp
RI Yamagishi, Kazuhisa/JXY-6414-2024
CR Aaron A., 2015, Netflix Technol. Blog
   Akamai, 2012, White Paper
   Apple, HILLS Authoring Specification for Apple Devices
   APTEKER RT, 1995, IEEE MULTIMEDIA, V2, P32, DOI 10.1109/93.410510
   Balachandran A, 2013, ACM SIGCOMM COMP COM, V43, P339, DOI 10.1145/2534169.2486025
   Bhat M, 2021, IEEE IMAGE PROC, P2164, DOI 10.1109/ICIP42928.2021.9506310
   Bhat M, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102934
   Chen C, 2018, IEEE IMAGE PROC, P3269, DOI 10.1109/ICIP.2018.8451307
   Chen JW, 2021, J CHEMINFORMATICS, V13, DOI 10.1186/s13321-021-00488-1
   Chen SX, 2021, PROC CVPR IEEE, P9791, DOI 10.1109/CVPR46437.2021.00967
   Chen YS, 2013, IEEE T MULTIMEDIA, V15, P2087, DOI 10.1109/TMM.2013.2280123
   De Cock J, 2016, IEEE IMAGE PROC, P1484, DOI 10.1109/ICIP.2016.7532605
   de Koning TCM, 2007, PROC SPIE, V6507, DOI 10.1117/12.704159
   Garcia M.-N, 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P141, DOI 10.1109/QoMEX.2014.6982310
   Google, Recommended upload encoding settings
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Institute for Telecommunication Sciences, The consumer digital videolibrary
   ITU Telecommunication Standardization Sector, 2008, ITU T RECOMMENDATION, V247, P18
   ITU-T, 2008, ITU T P910 SUBJECTIV
   Katsenou AV, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954529
   Krishnan SS, 2013, IEEE ACM T NETWORK, V21, P2001, DOI 10.1109/TNET.2013.2281542
   Laming D., 2009, Inside psychology: A science over 50 years, P179
   Lebreton P, 2021, IEEE T MULTIMEDIA, V23, P4526, DOI 10.1109/TMM.2020.3044452
   Lebreton P, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P4205, DOI 10.1109/ICASSP39728.2021.9413558
   Lebreton P, 2020, IEEE INT WORKSH MULT
   Lebreton P, 2019, IEICE T COMMUN, VE102B, P2226, DOI 10.1587/transcom.2019EBP3045
   Li J, 2019, IEEE T MULTIMEDIA, V21, P2589, DOI 10.1109/TMM.2019.2903722
   Li Z., 2022, Netflix Technol. Blog
   Ling S., 2020, P IEEE INT C MULT EX, P1
   Manohara M., Netflix Technology Blog
   Menon VV, 2022, INT CONF ACOUST SPEE, P1865, DOI 10.1109/ICASSP43922.2022.9746745
   Moller S., 2013, Eur. Netw. Qual. Exp. Multimedia Syst. Serv.
   Mux, Better quality through machine learning
   Nam H., 2016, Youslow: What influences user abandonment behavior for internet video?
   Nam H, 2016, IEEE INFOCOM SER
   Reznik YA, 2019, IEEE INT CONF MULTI, P348, DOI 10.1109/ICMEW.2019.00066
   Reznik YA, 2018, PROCEEDINGS OF THE 23TH ACM WORKSHOP ON PACKET VIDEO (PV'18), P43, DOI 10.1145/3210424.3210436
   Robitza W, 2021, INT WORK QUAL MULTIM, P65, DOI 10.1109/QoMEX51781.2021.9465452
   Sasse M., 2006, P 2 ISCA DEGA WORKSH, P11
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Shafiq M. Zubair, 2014, ACM SIGMETRICS Performance Evaluation Review, V42, P367, DOI 10.1145/2591971.2591975
   Song W, 2014, IEEE T MULTIMEDIA, V16, P738, DOI 10.1109/TMM.2014.2298217
   Spachos P, 2015, IEEE INT CONF COMM, P1693, DOI 10.1109/ICCW.2015.7247424
   Szegedy C., 2014, P IEEE CVF C COMP VI, DOI 10.1109/CVPR.2015.7298594
   Takahashi Shoko, 2019, 2019 Eleventh International Conference on Quality of Multimedia Experience (QoMEX), DOI 10.1109/QoMEX.2019.8743191
   Tan X., 2018, Wireless Commun. Mobile Comput., V2018, P1
   Toni L, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700294
   Twitch, Broadcasting Guidelines
   Wolf S., 2002, Video quality measurement techniques
   Wu Siqi, 2018, 12 INT AAAI C WEB SO, P434
   Yamagishi K, 2017, IEEE T MULTIMEDIA, V19, P1545, DOI 10.1109/TMM.2017.2669859
   Yan F. Y., Puffer experimental results
   Yan FY, 2020, PROCEEDINGS OF THE 17TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P495
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zhibin M., 2019, International Telecommunication Union, Study Group 12, ITUI Contribution SG12-C370R2
NR 55
TC 1
Z9 1
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8418
EP 8431
DI 10.1109/TMM.2023.3237168
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000029
OA hybrid
DA 2024-07-18
ER

PT J
AU Li, BR
   Guo, B
   Zhu, YS
   Yin, JF
   Ji, XL
AF Li, Bairong
   Guo, Biao
   Zhu, Yuesheng
   Yin, Jianfeng
   Ji, Xiangli
TI Superframe-Based Temporal Proposals for Weakly Supervised Temporal
   Action Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Temporal action detection; video understanding; weakly supervised
ID RECOGNITION
AB The weakly supervised Temporal Action Detection (TAD) by using the video-level annotations can lighten the burden of labor consumption. However, the current methods for weakly supervised TAD do not take full advantage of the short-term consistency between consecutive frames and the long-term continuity inside an action, resulting in less accurate detecting boundaries of actions in untrimmed videos. In this paper, the SuperFrame-based Temporal Proposal (SFTP) is proposed, in which superframes are formed for representing a series of consecutive frames with high temporal consistency and their features are pooled from the features of frames through the integration function. Then, the temporal proposal is built based on the multiple consecutive superframes and the features of all proposals are generated from a pyramidal feature hierarchy. This hierarchy consists of the designed Structured Outer-Inner Context (SOIC) features formed from superframe features and is able to explicitly characterize the temporal continuity inside a proposal. Furthermore, a novel Scale-Wise Normalization Strategy (SWNS) is proposed to identify proposals, which can effectively detect multiple actions with different duration in one untrimmed video. Extensive experiments are conducted on two public datasets: THUMOS14 and ActivityNet1.2 for performance evaluation. Our experimental results have demonstrated that the proposed approach is able to detect the boundaries of actions more effectively and obtain competitive mAP (mean average precision) compared with other approaches.
C1 [Li, Bairong; Guo, Biao; Zhu, Yuesheng; Yin, Jianfeng; Ji, Xiangli] Peking Univ, Shenzhen Grad Sch, Shenzhen 518000, Peoples R China.
C3 Peking University
RP Zhu, YS (corresponding author), Peking Univ, Shenzhen Grad Sch, Shenzhen 518000, Peoples R China.
EM lbairong@pku.edu.cn; guob@sz.pku.edu.cn; zhuys@pku.edu.cn;
   jfyin@pku.edu.cn; jxiangli@pku.edu.cn
FU National Innovation 2030 Major Samp;T Project of China; Nature Science
   Foundation of China [2020AAA0104203];  [62006007]
FX This work was supported in part by the National Innovation 2030 Major S
   & amp;T Project of China under Grant 2020AAA0104203,and in part by the
   Nature Science Foundation of China under Grant 62006007.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523
   Benesova W., 2014, P C MACH VIS MACH LE
   Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Chen PH, 2020, IEEE T MULTIMEDIA, V22, P2723, DOI 10.1109/TMM.2019.2959977
   Chen YT, 2019, IEEE T MULTIMEDIA, V21, P704, DOI 10.1109/TMM.2018.2865860
   Dai XY, 2017, IEEE I CONF COMP VIS, P5727, DOI 10.1109/ICCV.2017.610
   Duchenne O, 2009, IEEE I CONF COMP VIS, P1491, DOI 10.1109/ICCV.2009.5459279
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872
   Gao JY, 2017, Arxiv, DOI arXiv:1705.01180
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Gong G., 2020, PROC IEEE C COMPUT, P9819
   Idrees H, 2017, COMPUT VIS IMAGE UND, V155, P1, DOI 10.1016/j.cviu.2016.10.018
   Islam A, 2020, IEEE WINT CONF APPL, P536, DOI 10.1109/WACV45572.2020.9093620
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Lee P, 2021, AAAI CONF ARTIF INTE, V35, P1854
   Lee P, 2020, AAAI CONF ARTIF INTE, V34, P11320
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Liu DC, 2019, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2019.00139
   Liu ZY, 2019, IEEE I CONF COMP VIS, P3898, DOI 10.1109/ICCV.2019.00400
   Machairas V, 2014, IEEE IMAGE PROC, P4343, DOI 10.1109/ICIP.2014.7025882
   Montes A, 2017, Arxiv, DOI arXiv:1608.08128
   Moore AP, 2008, PROC CVPR IEEE, P998
   Narayan S, 2019, IEEE I CONF COMP VIS, P8678, DOI 10.1109/ICCV.2019.00877
   Paul S, 2018, LECT NOTES COMPUT SC, V11208, P588, DOI 10.1007/978-3-030-01225-0_35
   Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706
   Nguyen PX, 2019, IEEE I CONF COMP VIS, P5501, DOI 10.1109/ICCV.2019.00560
   Qing ZW, 2021, PROC CVPR IEEE, P485, DOI 10.1109/CVPR46437.2021.00055
   Rashid M, 2020, IEEE WINT CONF APPL, P604, DOI [10.1109/wacv45572.2020.9093404, 10.1109/WACV45572.2020.9093404]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi BF, 2020, PROC CVPR IEEE, P1006, DOI 10.1109/CVPR42600.2020.00109
   Shou Z, 2018, LECT NOTES COMPUT SC, V11220, P162, DOI 10.1007/978-3-030-01270-0_10
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Singh KK, 2017, IEEE I CONF COMP VIS, P3544, DOI 10.1109/ICCV.2017.381
   Sokeh HS, 2018, INT C PATT RECOG, P566, DOI 10.1109/ICPR.2018.8545723
   Stutz D, 2018, COMPUT VIS IMAGE UND, V166, P1, DOI 10.1016/j.cviu.2017.03.007
   Su H., 2018, P AS C COMP VIS, P558
   Su HS, 2021, AAAI CONF ARTIF INTE, V35, P2602
   Su QB, 2019, NEUROCOMPUTING, V339, P202, DOI 10.1016/j.neucom.2019.02.026
   Sultani W, 2018, PROC CVPR IEEE, P6479, DOI 10.1109/CVPR.2018.00678
   Sun C, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P371, DOI 10.1145/2733373.2806226
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Xu YL, 2019, AAAI CONF ARTIF INTE, P9070
   Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293
   Yu T, 2019, IEEE I CONF COMP VIS, P5521, DOI 10.1109/ICCV.2019.00562
   Yuan Y, 2019, Arxiv, DOI arXiv:1905.08586
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zeng RH, 2019, IEEE T IMAGE PROCESS, V28, P5797, DOI 10.1109/TIP.2019.2922108
   Zhang CW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P738, DOI 10.1145/3343031.3351044
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhong JX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P35, DOI 10.1145/3240508.3240511
   Zhu GM, 2019, IEEE T MULTIMEDIA, V21, P1011, DOI 10.1109/TMM.2018.2869278
NR 61
TC 2
Z9 2
U1 1
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3628
EP 3641
DI 10.1109/TMM.2022.3163459
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA S2QI4
UT WOS:001069663600002
DA 2024-07-18
ER

PT J
AU Li, JL
   Jie, ZQ
   Wang, X
   Zhou, Y
   Wei, XL
   Ma, L
AF Li, Jinlong
   Jie, Zequn
   Wang, Xu
   Zhou, Yu
   Wei, Xiaolin
   Ma, Lin
TI Weakly Supervised Semantic Segmentation Via Progressive Patch Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Weakly supervised learning; semantic segmentation; patch learning;
   progressive learning
ID SALIENT OBJECT DETECTION; ATTENTION
AB Most of the existing semantic segmentation approaches with image-level class labels as supervision, highly rely on the initial class activation map (CAM) generated from the standard classification network. In this paper, a novel "Progressive Patch Learning" approach is proposed to improve the local details extraction of the classification, producing the CAM better covering the whole object rather than only the most discriminative regions as in CAMs obtained in conventional classification models. "Patch Learning" destructs the feature maps into patches and independently processes each local patch in parallel before the final aggregation. Such a mechanism enforces the network to find weak information from the scattered discriminative local parts, achieving enhanced local details sensitivity. "Progressive Patch Learning" further extends the feature destruction and patch learning to multi-level granularities in a progressive manner. Cooperating with a multi-stage optimization strategy, such a "Progressive Patch Learning" mechanism implicitly provides the model with the feature extraction ability across different locality-granularities. As an alternative to the implicit multi-granularity progressive fusion approach, we additionally propose an explicit method to simultaneously fuse features from different granularities in a single model, further enhancing the CAM quality on the full object coverage. Our proposed method achieves outstanding performance on the PASCAL VOC 2012 dataset (e.g., with 69.6$\%$ mIoU on the test set), which surpasses most existing weakly supervised semantic segmentation methods.
C1 [Li, Jinlong; Wang, Xu; Zhou, Yu] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
   [Li, Jinlong; Wang, Xu; Zhou, Yu] Shenzhen Univ, Guangdong Lab Artificial Intelligence & Digital Ec, Shenzhen 518060, Peoples R China.
   [Jie, Zequn; Wei, Xiaolin; Ma, Lin] Meituan Inc, Beijing 100000, Peoples R China.
C3 Shenzhen University; Guangming Laboratory; Shenzhen University
RP Wang, X (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
EM jinlong.szu@gmail.com; zequn.nus@gmail.com; wangxu@szu.edu.cn;
   yu.zhou@szu.edu.cn; weixiaolin02@meituan.com; forest.linma@gmail.com
OI Wei, Xiaolin/0000-0002-3983-047X; ZHOU, Yu/0000-0002-3224-0063; JinLong,
   Li/0000-0002-8746-4566
FU National Natural Science Foundation of China [61871270]; Shenzhen
   Natural Science Foundation [JCYJ20200109110410133, 20200812110350001];
   National Engineering Laboratory for Big Data System Computing Technology
   of China
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61871270, in part by the Shenzhen
   Natural Science Foundation under Grants JCYJ20200109110410133 and
   20200812110350001, and in part by the National Engineering Laboratory
   for Big Data System Computing Technology of China.
CR Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231
   Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523
   Araslanov N, 2020, PROC CVPR IEEE, P4252, DOI 10.1109/CVPR42600.2020.00431
   Bearman Amy, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9911, P549, DOI 10.1007/978-3-319-46478-7_34
   Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233
   Chang Y.-T., 2020, PROC BRIT MACH VIS C
   Chen L., 2020, P 16 EUR C COMP VIS, P347
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Liang-Chieh, 2014, Comput Sci, DOI DOI 10.48550/ARXIV.1412.7062
   Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan JS, 2020, AAAI CONF ARTIF INTE, V34, P10762
   Fan JS, 2020, PROC CVPR IEEE, P4282, DOI 10.1109/CVPR42600.2020.00434
   Fan Junsong, 2020, EUR C COMP VIS
   Guolei Sun, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P347, DOI 10.1007/978-3-030-58536-5_21
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou Q., 2018, ADV NEURAL INF PROCE, P549
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Huang ZL, 2018, PROC CVPR IEEE, P7014, DOI 10.1109/CVPR.2018.00733
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jiang PT, 2021, IEEE T IMAGE PROCESS, V30, P5875, DOI 10.1109/TIP.2021.3089943
   Jiang PT, 2019, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2019.00216
   Kaiming He, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1026, DOI 10.1109/ICCV.2015.123
   Ke T.-W., 2021, PROC INT C LEARN REP
   Khoreva A, 2017, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2017.181
   Kolesnikov A, 2016, LECT NOTES COMPUT SC, V9908, P695, DOI 10.1007/978-3-319-46493-0_42
   KRAHENBUHL P, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472
   Lee J., 2021, IEEE CVF C COMP VIS, P4071
   Lee J, 2019, PROC CVPR IEEE, P5262, DOI 10.1109/CVPR.2019.00541
   Li KP, 2018, PROC CVPR IEEE, P9215, DOI 10.1109/CVPR.2018.00960
   Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu WD, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2085, DOI 10.1145/3394171.3413652
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Noroozi M, 2018, PROC CVPR IEEE, P9359, DOI 10.1109/CVPR.2018.00975
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Papandreou G, 2015, IEEE I CONF COMP VIS, P1742, DOI 10.1109/ICCV.2015.203
   Ru Lixiang, 2021, PROC INT JOINT C ART, P982
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shimoda W, 2016, LECT NOTES COMPUT SC, V9908, P218, DOI 10.1007/978-3-319-46493-0_14
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh KK, 2017, IEEE I CONF COMP VIS, P3544, DOI 10.1109/ICCV.2017.381
   Tianyi Zhang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P663, DOI 10.1007/978-3-030-58542-6_40
   Vernaza P, 2017, PROC CVPR IEEE, P2953, DOI 10.1109/CVPR.2017.315
   Wang B, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3663
   Wang HJ, 2020, Cambria Sinophone Wo, P111, DOI 10.1109/CVPRW50498.2020.00020
   Wang X, 2018, PROC CVPR IEEE, P1354, DOI 10.1109/CVPR.2018.00147
   Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687
   Wei YC, 2018, PROC CVPR IEEE, P7268, DOI 10.1109/CVPR.2018.00759
   Wei YC, 2017, IEEE T PATTERN ANAL, V39, P2314, DOI 10.1109/TPAMI.2016.2636150
   Wu T, 2021, PROC CVPR IEEE, P16760, DOI 10.1109/CVPR46437.2021.01649
   Wu ZF, 2019, PATTERN RECOGN, V90, P119, DOI 10.1016/j.patcog.2019.01.006
   Xie EZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8372, DOI 10.1109/ICCV48922.2021.00828
   Yu-Ting Chang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8988, DOI 10.1109/CVPR42600.2020.00901
   Yude Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12272, DOI 10.1109/CVPR42600.2020.01229
   Zhang BF, 2020, AAAI CONF ARTIF INTE, V34, P12765
   ZHANG D, 2020, ADV NEURAL INFORM PR, V33, P655, DOI DOI 10.5555/3495724.3495780
   Zhang JM, 2018, INT J COMPUT VISION, V126, P1084, DOI 10.1007/s11263-017-1059-x
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P470, DOI 10.1109/TMM.2013.2293424
   Zhang QL, 2021, Arxiv, DOI arXiv:2103.13859
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhang XL, 2018, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2018.00144
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou L, 2021, IEEE T MULTIMEDIA, V23, P1035, DOI 10.1109/TMM.2020.2991592
NR 67
TC 4
Z9 4
U1 10
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1686
EP 1699
DI 10.1109/TMM.2022.3152388
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Mao, AH
   Yang, Z
   Lin, K
   Xuan, J
   Liu, YJ
AF Mao, Aihua
   Yang, Zhi
   Lin, Ken
   Xuan, Jun
   Liu, Yong-Jin
TI Positional Attention Guided Transformer-Like Architecture for Visual
   Question Answering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Task analysis; Cognition; Visualization; Feature
   extraction; Semantics; Question answering (information retrieval);
   Visual question answering; positional attention; transformer-like models
ID NETWORKS
AB Transformer architectures have recently been introduced into the field of visual question answering (VQA), due to their powerful capabilities of information extraction and fusion. However, existing Transformer-like models, including models using a single Transformer structure and large-scale pre-training generic visual-linguistic models, do not fully utilize both positional information of words in questions and positional information of objects in images, which are shown in this paper to be crucial in VQA tasks. To address this challenge, we propose a novel positional attention guided Transformer-like architecture, which can adaptively extracts positional information within and across the visual and language modalities, and use this information to guide high-level interactions in inter- and intra-modality information flows. In particular, we design and assemble three positional attention modules into a single Transformer-like model MCAN. We show that the positional information introduced in intra-modality interaction can adaptively modulate inter-modality interaction according to different inputs, which plays an important role for visual reasoning. Experimental results demonstrate that our model outperforms the state-of-the-art models and is particularly good at handling object counting questions. Overall, our model achieves the accuracy of 70.10%, 71.27%, and 71.52% on the datasets of COCO-QA, VQA v1.0 test-std and VQA v2.0 test-std, respectively.
C1 [Mao, Aihua; Yang, Zhi; Lin, Ken; Xuan, Jun] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510640, Peoples R China.
   [Liu, Yong-Jin] Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 100084, Peoples R China.
   [Liu, Yong-Jin] MOE Key Lab Pervas Comp, Beijing 100084, Peoples R China.
C3 South China University of Technology; Tsinghua University
RP Liu, YJ (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 100084, Peoples R China.; Liu, YJ (corresponding author), MOE Key Lab Pervas Comp, Beijing 100084, Peoples R China.
EM ahmao@scut.edu.cn; 202021044575@mail.scut.edu.cn;
   csklin@mail.scut.edu.cn; 202020143921@mail.scut.edu.cn;
   liuyongjin@tsinghua.edu.cn
OI Yang, Zhi/0000-0002-9768-2356; Aihua, Mao/0000-0001-6861-9414
FU NSF of Guangdong Province [2019A1515010833, 2022A1515011573]; NSF of
   China [61725204]; Tsinghua University Initiative Scientific Research
   Program [20211080093]
FX This work was supported in part by the NSF of Guangdong Province under
   Grants 2019A1515010833 and 2022A1515011573, in part by the NSF of China
   under Grant 61725204 and in part by Tsinghua University Initiative
   Scientific Research Program under Grant 20211080093.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12
   [Anonymous], 2016, P ADV NEUR INF PROC
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Dailan He, 2021, MM '21: Proceedings of the 29th ACM International Conference on Multimedia, P2344, DOI 10.1145/3474085.3475397
   Das A, 2017, COMPUT VIS IMAGE UND, V163, P90, DOI 10.1016/j.cviu.2017.10.001
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Gao P, 2019, PROC CVPR IEEE, P6632, DOI 10.1109/CVPR.2019.00680
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Guan WL, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2299, DOI 10.1145/3474085.3475392
   Guo WY, 2020, AAAI CONF ARTIF INTE, V34, P91
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Hu Ronghang, 2020, 2020 IEEE CVF C COMP, P9992, DOI DOI 10.1109/CVPR42600.2020.01001
   Huang Q., 2020, P 58 ANN M ASS COMP, P7166
   Huang X, 2021, IEEE T CYBERNETICS, V51, P5692, DOI 10.1109/TCYB.2019.2956975
   Islam M. A., 2020, P INT C LEARN REPR I
   Ke G., 2020, P INT C LEARN REPR, P1
   Kim JH, 2018, ADV NEUR IN, V31
   Kingma D. P., 2014, arXiv
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li L. H., 2020, P 58 ANN M ASS COMP, P5265, DOI [10.18653/v1/2020.acl-main.469, DOI 10.18653/V1/2020.ACL-MAIN.469]
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Liu F, 2021, IEEE T MULTIMEDIA, V23, P3518, DOI 10.1109/TMM.2020.3026892
   Liu XY, 2022, IEEE T CYBERNETICS, V52, P7852, DOI 10.1109/TCYB.2021.3049537
   Liu Y, 2022, IEEE T CYBERNETICS, V52, P4520, DOI 10.1109/TCYB.2020.3029423
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu JS, 2019, ADV NEUR IN, V32
   Lu JS, 2016, ADV NEUR IN, V29
   Lu P, 2018, AAAI CONF ARTIF INTE, P7218
   Malinowski M, 2014, ADV NEUR IN, V27
   Noh H, 2016, PROC CVPR IEEE, P30, DOI 10.1109/CVPR.2016.11
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Raffel C, 2020, J MACH LEARN RES, V21
   Rahman T, 2021, IEEE COMPUT SOC CONF, P1653, DOI 10.1109/CVPRW53098.2021.00181
   Ren MY, 2015, ADV NEUR IN, V28
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shaw P., 2018, P 2018 NAACL, V2, P464, DOI [DOI 10.18653/V1/N18-2074, 10.18653/v1/N18-2074]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song JK, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P906
   Su W. J., 2020, 8 INT C LEARNING REP
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Teney D, 2018, PROC CVPR IEEE, P4223, DOI 10.1109/CVPR.2018.00444
   Vaswani A, 2017, ADV NEUR IN, V30
   Velickovic Petar, 2018, INT C LEARN REPR
   Wang B., 2020, P INT C LEARN REPR, P1
   Wu CF, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P519, DOI 10.1145/3240508.3240513
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Zhang Y., 2018, 6 INT C LEARN REPR I
   Zhong HS, 2021, IEEE T MULTIMEDIA, V23, P1264, DOI 10.1109/TMM.2020.2995278
NR 51
TC 7
Z9 7
U1 6
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6997
EP 7009
DI 10.1109/TMM.2022.3216770
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000021
DA 2024-07-18
ER

PT J
AU Mao, Q
   Ma, SW
AF Mao, Qi
   Ma, Siwei
TI Enhancing Style-Guided Image-to-Image Translation via Self-Supervised
   Metric Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Visualization; Task analysis; Generators; Self-supervised
   learning; Image reconstruction; Topology; Image-to-image translation;
   style-guided; metric learning; self-supervised
ID GENERATIVE ADVERSARIAL NETWORKS
AB There has been significant success in recent image-to-image translation (I2I) approaches in translating the source image into the style of the target image. Existing techniques rely on the disentanglement of content and style representations, requiring a two-stage style mapping process: Reference images are used to extract style vectors, which are subsequently remapped into the translated images. However, when the target domain contains a variety of styles, such a two-stage style mapping cannot guarantee the translated image be style consistent with its guided reference image. In this work, we propose to explicitly employ metric learning to enhance the two-stage style mapping in style-guided image translation. The distance between deep features Gram matrices is utilized to construct the visual style metric as self-supervised similarity labels, guiding the embedding of style vectors using triplet loss with adaptive margins in the first stage. Furthermore, in the second stage, we consider generated images and their corresponding reference images as positive samples and anchors for each other, while the nearest negative sample is used to construct the triplet loss in the proposed metric space. The proposed learning algorithms can be applied to any I2I framework that uses disentangled representations without modifying the original network architectures. We evaluate the proposed method on three representative I2I translation baselines. Both qualitative and quantitative results demonstrate that the proposed approach enhances style alignment in style-guided translation compared to the baselines.
C1 [Mao, Qi] Commun Univ China, State Key Lab Media Convergence & Commun, Beijing 100024, Peoples R China.
   [Ma, Siwei] Peking Univ, Natl Engn Res Ctr Visual Technol, Sch Comp Sci, Beijing 100871, Peoples R China.
C3 Communication University of China; Peking University
RP Mao, Q (corresponding author), Commun Univ China, State Key Lab Media Convergence & Commun, Beijing 100024, Peoples R China.
EM qimao@cuc.edu.cn; swma@pku.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR An J, 2021, PROC CVPR IEEE, P862, DOI 10.1109/CVPR46437.2021.00092
   Anoosheh A, 2018, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2018.00122
   Baek K., 2021, P IEEE CVF INT C COM, P14154
   Binkowski Mikolaj, 2018, INT C LEARNING REPRE
   Cai Qi, 2020, P NEURIPS, V33, P12638
   Chen HB, 2021, ADV NEUR IN, V34
   Chen HB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14860, DOI 10.1109/ICCV48922.2021.01461
   Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen XL, 2020, Arxiv, DOI arXiv:2003.04297
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Dai GX, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P672, DOI 10.1145/3123266.3123334
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng YY, 2022, PROC CVPR IEEE, P11316, DOI 10.1109/CVPR52688.2022.01104
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Ge SM, 2020, IEEE T CIRC SYST VID, V30, P3387, DOI 10.1109/TCSVT.2020.2967754
   Ge SM, 2019, IEEE T IMAGE PROCESS, V28, P2051, DOI 10.1109/TIP.2018.2883743
   Gong R, 2019, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2019.00258
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Hensel M, 2017, ADV NEUR IN, V30
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Hsin-Yu Chang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P573, DOI 10.1007/978-3-030-58598-3_34
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Huang JL, 2021, IEEE T MULTIMEDIA, V23, P1654, DOI 10.1109/TMM.2020.3001536
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Jung CY, 2022, PROC CVPR IEEE, P18239, DOI 10.1109/CVPR52688.2022.01772
   Karras T., 2018, INT C LEARN REPR, P1, Patent No. 171010196
   Kim T, 2017, PR MACH LEARN RES, V70
   Kotovenko D, 2019, IEEE I CONF COMP VIS, P4421, DOI 10.1109/ICCV.2019.00452
   Kotovenko D, 2019, PROC CVPR IEEE, P10024, DOI 10.1109/CVPR.2019.01027
   Lee HY, 2020, INT J COMPUT VISION, V128, P2402, DOI 10.1007/s11263-019-01284-z
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li B, 2022, IEEE T MULTIMEDIA, V24, P4077, DOI 10.1109/TMM.2021.3113786
   Li Y., 2017, Universal style transfer via feature transforms, P386
   Li YH, 2020, IEEE T MULTIMEDIA, V22, P1285, DOI 10.1109/TMM.2019.2939711
   Li ZC, 2015, IEEE T MULTIMEDIA, V17, P1989, DOI 10.1109/TMM.2015.2477035
   Lin J., 2020, EUROPEAN C COMPUTER, P18
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   Liu MY, 2017, ADV NEUR IN, V30
   Liu Shaohui., 2019, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, P10306
   Liu SH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6629, DOI 10.1109/ICCV48922.2021.00658
   Liu X, 2023, IEEE T KNOWL DATA EN, V35, P857, DOI 10.1109/TKDE.2021.3090866
   Liu YH, 2021, PROC CVPR IEEE, P10780, DOI 10.1109/CVPR46437.2021.01064
   Lu JW, 2017, IEEE SIGNAL PROC MAG, V34, P76, DOI 10.1109/MSP.2017.2732900
   Ma L, 2019, P INT C LEARN REPR
   Mao Q, 2022, INT J COMPUT VISION, V130, P517, DOI 10.1007/s11263-021-01557-6
   Mao Q, 2019, PROC CVPR IEEE, P1429, DOI 10.1109/CVPR.2019.00152
   Misra I, 2020, PROC CVPR IEEE, P6706, DOI 10.1109/CVPR42600.2020.00674
   Pang YX, 2022, IEEE T MULTIMEDIA, V24, P3859, DOI 10.1109/TMM.2021.3109419
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Park Taesung, 2020, Advances in Neural Information Processing Systems, V33, P7198
   Park Taesung, 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Saito Kuniaki, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P382, DOI 10.1007/978-3-030-58580-8_23
   Sanakoyeu A, 2018, LECT NOTES COMPUT SC, V11212, P715, DOI 10.1007/978-3-030-01237-3_43
   Shaham TR, 2019, IEEE I CONF COMP VIS, P4569, DOI 10.1109/ICCV.2019.00467
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Svoboda J., 2020, P IEEECVF C COMPUTER, P13816
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   Wang X, 2019, PROC CVPR IEEE, P5017, DOI 10.1109/CVPR.2019.00516
   Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309
   Wu WY, 2019, PROC CVPR IEEE, P8004, DOI 10.1109/CVPR.2019.00820
   Wu XL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14598, DOI 10.1109/ICCV48922.2021.01435
   Wu ZJ, 2020, AAAI CONF ARTIF INTE, V34, P12305
   Yang Dingdong, 2019, INT C LEARN REPR
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
   Yu X., 2019, P ADV NEUR INF PROC, P1
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang Y., 2022, P ACM SIGGRAPH, P1
   Zheng CX, 2021, PROC CVPR IEEE, P16402, DOI 10.1109/CVPR46437.2021.01614
   Zheng ZQ, 2023, IEEE T MULTIMEDIA, V25, P2474, DOI 10.1109/TMM.2022.3147425
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zieba M., 2017, P INT C LEARN REPR W, P1
NR 82
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8511
EP 8526
DI 10.1109/TMM.2023.3238313
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000020
DA 2024-07-18
ER

PT J
AU Qi, L
   Wang, L
   Shi, YH
   Geng, X
AF Qi, Lei
   Wang, Lei
   Shi, Yinghuan
   Geng, Xin
TI A Novel Mix-Normalization Method for Generalizable Multi-Source Person
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Domain-aware mix-normalization; generalizable multi-source person
   re-identification; mixnorm
ID NETWORK
AB Person re-identification (Re-ID) has achieved great success in the supervised scenario. However, it is difficult to directly transfer the supervised model to arbitrary unseen domains due to the model overfitting to the seen source domains. In this paper, we aim to tackle the generalizable multi-source person Re-ID task (i.e., there are multiple available source domains, and the testing domain is unseen during training) from the data augmentation perspective, thus we put forward a novel method, termed MixNorm. It consists of domain-aware mix-normalization (DMN) and domain-aware center regularization (DCR). Different from the conventional data augmentation, the proposed domain-aware mix-normalization enhances the diversity of features during training from the normalization perspective of the neural network, which can effectively alleviate the model overfitting to the source domains, so as to boost the generalization capability of the model in the unseen domain. To further promote the efficacy of the proposed DMN, we exploit the domain-aware center regularization to better map the diversely generated features into the same space. Extensive experiments on multiple benchmark datasets validate the effectiveness of the proposed method and show that the proposed method can outperform the state-of-the-art methods. Besides, further analysis also reveals the superiority of the proposed method.
C1 [Qi, Lei; Geng, Xin] Southeast Univ, Sch Comp Sci & Engn, Nanjing 211189, Peoples R China.
   [Qi, Lei; Geng, Xin] Southeast Univ, Key Lab Comp Network & Informat Integrat, Minist Educ, Nanjing 211189, Peoples R China.
   [Wang, Lei] Univ Wollongong, Sch Comp & Informat Technol, Wollongong, NSW 2522, Australia.
   [Shi, Yinghuan] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
C3 Southeast University - China; Southeast University - China; University
   of Wollongong; Nanjing University
RP Geng, X (corresponding author), Southeast Univ, Sch Comp Sci & Engn, Nanjing 211189, Peoples R China.; Geng, X (corresponding author), Southeast Univ, Key Lab Comp Network & Informat Integrat, Minist Educ, Nanjing 211189, Peoples R China.
EM qilei@seu.edu.cn; leiw@uow.edu.au; syh@nju.edu.cn; xgeng@seu.edu.cn
RI Wang, Lei/D-9079-2013
OI Wang, Lei/0000-0002-0961-0441; Qi, Lei/0000-0001-7091-0702
FU National Key Research and Development Plan of China [2018AAA0100104];
   Jiangsu Natural Science Foundation [BK20210224]; National Science
   Foundation of China [62125602, 62076063]; NSFC Major Program [62192783];
   CAAI-Huawei MindSpore Project [CAAIXSJLJJ-2021-042A]; China Postdoctoral
   Science Foundation Project [2021M690609]
FX This work was supported in part by the National Key Research and
   Development Plan of China under Grant 2018AAA0100104, in part by Jiangsu
   Natural Science Foundation under Project BK20210224, in part by the
   National Science Foundation of China under Grants 62125602 and 62076063,
   in part by NSFC Major Program under Grant 62192783, in part by
   CAAI-Huawei MindSpore Project under Grant CAAIXSJLJJ-2021-042A, and in
   part by China Postdoctoral Science Foundation Project under Grant
   2021M690609.The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Prof. Ferdous Sohel.
CR Balaji Y, 2018, ADV NEUR IN, V31
   Busto PP, 2017, IEEE I CONF COMP VIS, P754, DOI 10.1109/ICCV.2017.88
   Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233
   Chen PX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11813, DOI 10.1109/ICCV48922.2021.01162
   Choi S, 2021, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR46437.2021.00343
   Cubuk ED, 2019, Arxiv, DOI arXiv:1805.09501
   Dai YX, 2021, PROC CVPR IEEE, P16140, DOI 10.1109/CVPR46437.2021.01588
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Guangyi Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P643, DOI 10.1007/978-3-030-58598-3_38
   Guo JZ, 2020, PROC CVPR IEEE, P6162, DOI 10.1109/CVPR42600.2020.00620
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9
   Hou RB, 2022, IEEE T PATTERN ANAL, V44, P4894, DOI 10.1109/TPAMI.2021.3079910
   Hou RB, 2021, IEEE T NEUR NET LEAR, V32, P4460, DOI 10.1109/TNNLS.2020.3017939
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jia J, 2019, 30 BRIT MACH VIS C 2, P117
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Kaiyang Zhou, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P561, DOI 10.1007/978-3-030-58517-4_33
   Leng QM, 2020, IEEE T CIRC SYST VID, V30, P1092, DOI 10.1109/TCSVT.2019.2898940
   Li D, 2019, IEEE I CONF COMP VIS, P1446, DOI 10.1109/ICCV.2019.00153
   Li D, 2018, AAAI CONF ARTIF INTE, P3490
   Li MX, 2020, IEEE T PATTERN ANAL, V42, P1770, DOI 10.1109/TPAMI.2019.2903058
   Li P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8866, DOI 10.1109/ICCV48922.2021.00876
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li W, 2013, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2013.461
   Li Y, 2018, LECT NOTES COMPUT SC, V11219, P647, DOI 10.1007/978-3-030-01267-0_38
   Liu H, 2019, PROC CVPR IEEE, P2922, DOI 10.1109/CVPR.2019.00304
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Luo Y., 2020, PMLR, P6468
   Nam H, 2021, PROC CVPR IEEE, P8686, DOI 10.1109/CVPR46437.2021.00858
   Nichol A, 2018, Arxiv, DOI [arXiv:1803.02999, DOI 10.48550/ARXIV.1803.02999]
   Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29
   Qi L, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3419439
   Qi L, 2020, IEEE T CIRC SYST VID, V30, P2815, DOI 10.1109/TCSVT.2020.2983600
   Qi L, 2019, IEEE I CONF COMP VIS, P8079, DOI 10.1109/ICCV.2019.00817
   Qiao SY, 2018, PROC CVPR IEEE, P7229, DOI 10.1109/CVPR.2018.00755
   Saito K, 2018, LECT NOTES COMPUT SC, V11209, P156, DOI 10.1007/978-3-030-01228-1_10
   Seonguk Seo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P68, DOI 10.1007/978-3-030-58542-6_5
   Shankar S., 2018, INT C LEARN REPR
   Shengcai Liao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P456, DOI 10.1007/978-3-030-58621-8_27
   Shermin T, 2021, IEEE T MULTIMEDIA, V23, P2732, DOI 10.1109/TMM.2020.3016126
   Song JF, 2019, PROC CVPR IEEE, P719, DOI 10.1109/CVPR.2019.00081
   Sun YF, 2021, IEEE T PATTERN ANAL, V43, P902, DOI 10.1109/TPAMI.2019.2938523
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu AC, 2019, IEEE I CONF COMP VIS, P6921, DOI 10.1109/ICCV.2019.00702
   Wu AC, 2019, PROC CVPR IEEE, P1187, DOI 10.1109/CVPR.2019.00128
   Xiao T., 2016, arXiv
   Xu QW, 2021, PROC CVPR IEEE, P14378, DOI 10.1109/CVPR46437.2021.01415
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13864, DOI 10.1109/CVPR42600.2020.01388
   Yue XY, 2019, IEEE I CONF COMP VIS, P2100, DOI 10.1109/ICCV.2019.00219
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zeyi Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P124, DOI 10.1007/978-3-030-58536-5_8
   Zhang J, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108292
   Zhang X, 2018, Arxiv, DOI arXiv:1711.08184
   Zhao CR, 2020, IEEE T MULTIMEDIA, V22, P3180, DOI 10.1109/TMM.2020.2972125
   Zhao YY, 2021, PROC CVPR IEEE, P6273, DOI 10.1109/CVPR46437.2021.00621
   Zheng L, 2016, Arxiv, DOI arXiv:1610.02984
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou K., 2021, PROC INT C LEARN REP
   Zhou KY, 2022, Arxiv, DOI arXiv:2103.02503
   Zhou KY, 2020, AAAI CONF ARTIF INTE, V34, P13025
NR 74
TC 6
Z9 6
U1 4
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4856
EP 4867
DI 10.1109/TMM.2022.3183393
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300018
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Shen, TY
   Li, DQ
   Wang, FY
   Huang, H
AF Shen, Tianyu
   Li, Deqi
   Wang, Fei-Yue
   Huang, Hua
TI Depth-Aware Multi-Person 3D Pose Estimation With Multi-Scale Waterfall
   Representations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Pose estimation; Feature extraction;
   Location awareness; Cameras; Semantics; Solid modeling; Human depth
   perceiving; multi-person 3d pose estimation; multi-scale representation;
   occlusion handling
AB Estimating absolute 3D poses of multiple people from monocular image is challenging due to the presence of occlusions and the scale variation among different persons. Among the existing methods, the top-down paradigms are highly dependent on human detection which is prone to the influence from inter-person occlusions, while the bottom-up paradigms suffer from the difficulties in keypoint feature extraction caused by scale variation and unreliable joint grouping caused by occlusions. To address these challenges, we introduce a novel multi-person 3D pose estimation framework, aided by multi-scale feature representations and human depth perceiving. Firstly, a waterfall-based architecture is incorporated for multi-scale feature representations to achieve a more accurate estimation of occluded joints with a better detection of human shapes. Then the global and local representations are fused for handling the effects of inter-person occlusion and scale variation in depth perceiving and keypoint feature extraction. Finally, with the guidance of the fused multi-scale representations, a depth-aware model is exploited for better 2D joint grouping and 3D pose recovering. Quantitative and qualitative evaluations on benchmark datasets of MuCo-3DHP and MuPoTS-3D prove the effectiveness of our proposed method. Furthermore, we produce an occluded MuPoTS-3D dataset and the experiments on it validate the superiority of our method for overcoming the occlusions.
C1 [Shen, Tianyu; Li, Deqi; Huang, Hua] Beijing Normal Univ, Sch Artificial Intelligence, Beijing 100875, Peoples R China.
   [Wang, Fei-Yue] Chinese Acad Sci, State Key Lab Management & Control Complex Syst, Beijing 100190, Peoples R China.
   [Wang, Fei-Yue] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
C3 Beijing Normal University; Chinese Academy of Sciences; Institute of
   Automation, CAS; Chinese Academy of Sciences; Institute of Automation,
   CAS
RP Huang, H (corresponding author), Beijing Normal Univ, Sch Artificial Intelligence, Beijing 100875, Peoples R China.
EM tianyu.shen@bnu.edu.cn; dqli@mail.bnu.edu.cn; feiyue.wang@ia.ac.cn;
   huahuang@bnu.edu.cn
RI Shen, Tianyu/JIE-9808-2023; Huang, Hua/M-9684-2013
OI Huang, Hua/0000-0003-2587-1702; Shen, Tianyu/0000-0001-7614-7015
CR Artacho B, 2021, Arxiv, DOI arXiv:2103.10180
   Artacho B, 2020, PROC CVPR IEEE, P7033, DOI 10.1109/CVPR42600.2020.00706
   Benzine A, 2020, PROC CVPR IEEE, P6855, DOI 10.1109/CVPR42600.2020.00689
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   Cheng Y, 2023, IEEE T PATTERN ANAL, V45, P1636, DOI 10.1109/TPAMI.2022.3170353
   Cheng Y, 2021, PROC CVPR IEEE, P7645, DOI 10.1109/CVPR46437.2021.00756
   Choi S, 2021, IEEE COMPUT SOC CONF, P2328, DOI 10.1109/CVPRW53098.2021.00265
   Chu X., 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, P1831, DOI DOI 10.1109/CVPR.2017.601
   Dabral R, 2019, INT CONF 3D VISION, P405, DOI 10.1109/3DV.2019.00052
   Duan HD, 2022, PROC CVPR IEEE, P2959, DOI 10.1109/CVPR52688.2022.00298
   Fabbri M, 2020, PROC CVPR IEEE, P7202, DOI 10.1109/CVPR42600.2020.00723
   Ghafoor M, 2023, IEEE T MULTIMEDIA, V25, P3311, DOI 10.1109/TMM.2022.3158068
   He K., 2017, IEEE C COMP VIS PATT, P2961, DOI DOI 10.1109/ICCV.2017.322
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jafarian Y, 2021, PROC CVPR IEEE, P12748, DOI 10.1109/CVPR46437.2021.01256
   Jiang W, 2020, PROC CVPR IEEE, P5578, DOI 10.1109/CVPR42600.2020.00562
   Khirodkar R., 2021, P IEEECVF INT C COMP, P3122
   Li WB, 2019, Arxiv, DOI arXiv:1901.00148
   Li ZQ, 2021, IEEE T PATTERN ANAL, V43, P4229, DOI 10.1109/TPAMI.2020.2974454
   Lin J., 2020, EUR C COMP VIS, P633
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu HB, 2021, INTERVIROLOGY, V64, P126, DOI [10.1109/TPWRD.2021.3054889, 10.1159/000513687, 10.1109/TMM.2021.3081873]
   Luo ZX, 2021, PROC CVPR IEEE, P13259, DOI 10.1109/CVPR46437.2021.01306
   Mehta D, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392410
   Mehta D, 2018, INT CONF 3D VISION, P120, DOI 10.1109/3DV.2018.00024
   Moon G, 2019, IEEE I CONF COMP VIS, P10132, DOI 10.1109/ICCV.2019.01023
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Nie XC, 2019, IEEE I CONF COMP VIS, P6950, DOI 10.1109/ICCV.2019.00705
   Rogez G, 2020, IEEE T PATTERN ANAL, V42, P1146, DOI 10.1109/TPAMI.2019.2892985
   Rogez G, 2017, PROC CVPR IEEE, P1216, DOI 10.1109/CVPR.2017.134
   Shuai H, 2023, IEEE T PATTERN ANAL, V45, P4122, DOI 10.1109/TPAMI.2022.3188716
   Song LC, 2021, J VIS COMMUN IMAGE R, V76, DOI 10.1016/j.jvcir.2021.103055
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Tan FT, 2020, PROC CVPR IEEE, P647, DOI 10.1109/CVPR42600.2020.00073
   Véges M, 2020, LECT NOTES COMPUT SC, V12396, P258, DOI 10.1007/978-3-030-61609-0_21
   Véges M, 2019, IEEE IJCNN, DOI 10.1109/ijcnn.2019.8852387
   Wang T, 2022, AAAI CONF ARTIF INTE, P2540
   Wang Z., 2022, P IEEE CVF C COMP VI, P13096
   Yuan Y, 2021, PROC CVPR IEEE, P7155, DOI 10.1109/CVPR46437.2021.00708
   Zanfir A, 2018, ADV NEUR IN, V31
   Zanfir A, 2018, PROC CVPR IEEE, P2148, DOI 10.1109/CVPR.2018.00229
   Zhan Y, 2022, PROC CVPR IEEE, P13106, DOI 10.1109/CVPR52688.2022.01277
   Zhang F, 2020, PROC CVPR IEEE, P7091, DOI 10.1109/CVPR42600.2020.00712
   Zhang JF, 2021, PROC CVPR IEEE, P546, DOI 10.1109/CVPR46437.2021.00061
   Zhang XM, 2023, IEEE T MULTIMEDIA, V25, P2601, DOI 10.1109/TMM.2022.3148595
   Zhen Jianan, 2020, EUROPEAN C COMPUTER, DOI 10.1007/978-3030-58555-6
   Zheng C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11636, DOI 10.1109/ICCV48922.2021.01145
NR 49
TC 3
Z9 3
U1 3
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1439
EP 1451
DI 10.1109/TMM.2022.3233251
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA G2GW2
UT WOS:000987415000004
DA 2024-07-18
ER

PT J
AU Tang, W
   He, FZ
   Liu, Y
AF Tang, Wei
   He, Fazhi
   Liu, Yu
TI YDTR: Infrared and Visible Image Fusion via Y-Shape Dynamic Transformer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic transformer; image fusion; infrared image; Y-shape network
ID NETWORK; PERFORMANCE
AB Infrared and visible image fusion is aims to generate a composite image that can simultaneously describe the salient target in the infrared image and texture details in the visible image of the same scene. Since deep learning (DL) exhibits great feature extraction ability in computer vision tasks, it has also been widely employed in handling infrared and visible image fusion issue. However, the existing DL-based methods generally extract complementary information from source images through convolutional operations, which results in limited preservation of global features. To this end, we propose a novel infrared and visible image fusion method, i.e., the Y-shape dynamic Transformer (YDTR). Specifically, a dynamic Transformer module (DTRM) is designed to acquire not only the local features but also the significant context information. Furthermore, the proposed network is devised in a Y-shape to comprehensively maintain the thermal radiation information from the infrared image and scene details from the visible image. Considering the specific information provided by the source images, we design a loss function that consists of two terms to improve fusion quality: a structural similarity (SSIM) term and a spatial frequency (SF) term. Extensive experiments on mainstream datasets illustrate that the proposed method outperforms both classical and state-of-the-art approaches in both qualitative and quantitative assessments. We further extend the YDTR to address other infrared and RGB-visible images and multi-focus images without fine-tuning, and the satisfactory fusion results demonstrate that the proposed method has good generalization capability.
C1 [Tang, Wei; He, Fazhi] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Liu, Yu] Hefei Univ Technol, Dept Biomed Engn, Hefei 230009, Peoples R China.
C3 Wuhan University; Hefei University of Technology
RP He, FZ (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
EM weitang2021@whu.edu.cn; fzhe@whu.edu.cn; yuliu@hfut.edu.cn
OI Tang, Wei/0000-0001-8995-705X
FU National Natural Science Foundation of China [62072348, 62176081];
   Science and Technology Major Project of Hubei Province (Next -Generation
   AI Technologies) [2019AEA170]; National Key R&D Program of China
   [2018AAA0101104]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62072348 and 62176081, in part by the
   Science and Technology Major Project of Hubei Province (Next -Generation
   AI Technologies) under Grant 2019AEA170, and in part by the National Key
   R&D Program of China under Grant 2018AAA0101104.
CR Chen H, 2007, INFORM FUSION, V8, P193, DOI 10.1016/j.inffus.2005.10.001
   Dai ZH, 2019, Arxiv, DOI [arXiv:1901.02860, DOI 10.48550/ARXIV.1901.02860]
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Eskicioglu AM, 1995, IEEE T COMMUN, V43, P2959, DOI 10.1109/26.477498
   Fu Y, 2022, Arxiv, DOI arXiv:2107.13967
   Haibo Zhao, 2021, 2021 International Conference on Information Technology and Biomedical Engineering (ICITBE), P71, DOI 10.1109/ICITBE54178.2021.00025
   Hossny M, 2008, ELECTRON LETT, V44, P1066, DOI 10.1049/el:20081754
   Kingma D. P., 2014, arXiv
   Kong SG, 2007, INT J COMPUT VISION, V71, P215, DOI 10.1007/s11263-006-6655-0
   Kumar M, 2009, IEEE T IMAGE PROCESS, V18, P2137, DOI 10.1109/TIP.2009.2025006
   Li H, 2021, INFORM FUSION, V73, P72, DOI 10.1016/j.inffus.2021.02.023
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li H, 2017, LECT NOTES COMPUT SC, V10666, P675, DOI 10.1007/978-3-319-71607-7_59
   Li H, 2015, OPT COMMUN, V342, P1, DOI 10.1016/j.optcom.2014.12.048
   Li J, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3029360
   Li J, 2021, IEEE T MULTIMEDIA, V23, P1383, DOI 10.1109/TMM.2020.2997127
   Li ST, 2017, INFORM FUSION, V33, P100, DOI 10.1016/j.inffus.2016.05.004
   Li YQ, 2020, INFORM FUSION, V58, P1, DOI 10.1016/j.inffus.2019.12.014
   Li Z, 2021, IEEE T MULTIMEDIA, V23, P306, DOI 10.1109/TMM.2020.2978640
   Liu JY, 2022, IEEE T CIRC SYST VID, V32, P105, DOI 10.1109/TCSVT.2021.3056725
   Liu Y, 2018, INT J WAVELETS MULTI, V16, DOI 10.1142/S0219691318500182
   Liu Y, 2018, INFORM FUSION, V42, P158, DOI 10.1016/j.inffus.2017.10.007
   Liu Y, 2015, INFORM FUSION, V24, P147, DOI 10.1016/j.inffus.2014.09.004
   Liu Z, 2012, IEEE T PATTERN ANAL, V34, P94, DOI 10.1109/TPAMI.2011.109
   Lu K, 2021, IEEE T MULTIMEDIA, V23, P4093, DOI 10.1109/TMM.2020.3037526
   Ma JY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3038013
   Ma JY, 2020, IEEE T IMAGE PROCESS, V29, P4980, DOI 10.1109/TIP.2020.2977573
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Muller AC, 2009, INFORM FUSION, V10, P137, DOI 10.1016/j.inffus.2008.08.008
   Nencini F, 2007, INFORM FUSION, V8, P143, DOI 10.1016/j.inffus.2006.02.001
   Raghavendra R, 2011, PATTERN RECOGN, V44, P401, DOI 10.1016/j.patcog.2010.08.006
   Rao D., 2022, arXiv
   Tang LF, 2022, INFORM FUSION, V83, P79, DOI 10.1016/j.inffus.2022.03.007
   Tang W., 2021, P IEEE INT C MED IM, P1
   Tang W, 2021, IEEE T COMPUT IMAG, V7, P584, DOI 10.1109/TCI.2021.3083965
   Tang W, 2019, COMPUT MATH METHOD M, V2019, DOI 10.1155/2019/5450373
   TOET A, 1989, PATTERN RECOGN LETT, V9, P245, DOI 10.1016/0167-8655(89)90003-2
   Vaswani A, 2017, ADV NEUR IN, V30
   Vibashan V, 2021, Arxiv, DOI [arXiv:2107.09011, DOI 10.48550/ARXIV.2107.09011]
   Wan T, 2009, IEEE T MULTIMEDIA, V11, P624, DOI 10.1109/TMM.2009.2017640
   Wang B, 2021, IEEE T MULTIMEDIA, V23, P3137, DOI 10.1109/TMM.2020.3020695
   Wang Q, 2005, PHYSICA D, V200, P287, DOI 10.1016/j.physd.2004.11.001
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang YQ, 2021, PROC CVPR IEEE, P8737, DOI 10.1109/CVPR46437.2021.00863
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiao T., 2021, P ADV NEUR INF PROC, P1
   Xie Q, 2022, IEEE T PATTERN ANAL, V44, P1457, DOI 10.1109/TPAMI.2020.3015691
   Xu H, 2021, IEEE T COMPUT IMAG, V7, P824, DOI 10.1109/TCI.2021.3100986
   Xu H, 2020, AAAI CONF ARTIF INTE, V34, P12484
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Yang ZL, 2019, ADV NEUR IN, V32
   Yin HT, 2013, INFORM FUSION, V14, P229, DOI 10.1016/j.inffus.2012.01.008
   Yu Q., 2021, arXiv
   Zhang H, 2021, INFORM FUSION, V76, P323, DOI 10.1016/j.inffus.2021.06.008
   Zhang H, 2020, AAAI CONF ARTIF INTE, V34, P12797
   Zhang Q, 2018, INFORM FUSION, V40, P57, DOI 10.1016/j.inffus.2017.05.006
   Zhao F, 2021, IEEE T MULTIMEDIA, V23, P2745, DOI 10.1109/TMM.2020.3016123
   Zhao JY, 2007, INT J INNOV COMPUT I, V3, P1433
   Zhao WD, 2018, IEEE T MULTIMEDIA, V20, P866, DOI 10.1109/TMM.2017.2760100
   Zhao ZX, 2022, IEEE T CIRC SYST VID, V32, P1186, DOI 10.1109/TCSVT.2021.3075745
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhu ZQ, 2018, INFORM SCIENCES, V432, P516, DOI 10.1016/j.ins.2017.09.010
NR 65
TC 80
Z9 80
U1 52
U2 60
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5413
EP 5428
DI 10.1109/TMM.2022.3192661
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300057
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Wang, XH
   Guo, YY
   Song, JK
   Gao, LL
   Shen, HT
AF Wang, Xuanhan
   Guo, Yuyu
   Song, Jingkuan
   Gao, Lianli
   Shen, Heng Tao
TI AMANet: Adaptive Multi-Path Aggregation for Learning Human 2D-3D
   Correspondences
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Task analysis; Annotations; Feature
   extraction; Adaptive systems; Visualization; Pose estimation; 2D-to-3D
   surface estimation; human densepose estimation; human instance-level
   analysis; limited supervision
ID POSE; RECOGNITION; NETWORK
AB Learning human 2D-3D correspondences aims to map all human 2D pixels to a 3D human template, namely human densepose estimation, involving surface patch recognition (i.e., Index-to-Patch (I)) and regression of patch-specific UV coordinates. Despite recent progress, it remains challenging especially under the condition of "in the wild ", where RGB images capture real-world scenes with backgrounds, occlusions, scale variations, and postural diversity. In this paper, we address three vital problems in this task: 1) how to perceive multi-scale visual information for instances "in the wild "; 2) how to design learning objectives to address the precise instance representation harassed by "multiple instances in one bounding box " phenomenon; and 3) how to boost the performance of index-to-patch prediction faced by limited supervision. To tackle problems above, we propose an end-to-end deep Adaptive Multi-path Aggregation network (AMA-net) for Human DensePose Estimation. First, we introduce an adaptive multi-path aggregation algorithm to extract varying-sized instance-level features, which capture multi-scale information of a bounding-box and are then utilized for parsing different instances. Second, we adopt an instance augmentation learning objective to further distinguish the target instance from other interference instances. Third, taking advantage of 2D human parsers that are trained from sufficient annotations, we introduce a task transformer that bridges the "gap " between 2D human parsing and densepose estimation, thus benefiting the performance of densepose estimator. Experimental results on the challenging DensePose-COCO dataset demonstrate that our approach sets a new record, and it significantly outperforms the state-of-the-art methods. Codes and models are publicly available.
C1 [Wang, Xuanhan; Guo, Yuyu; Gao, Lianli; Shen, Heng Tao] Univ Elect Sci & Technol China, Ctr Future Media, Chengdu 611731, Peoples R China.
   [Wang, Xuanhan; Guo, Yuyu; Gao, Lianli; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Song, Jingkuan] Sichuan Prov Peoples Hosp, Inst Neurol, Chengdu 610072, Peoples R China.
   [Song, Jingkuan] Univ Elect Sci & Technol China, Chengdu 610072, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China; Sichuan Provincial People's
   Hospital; University of Electronic Science & Technology of China
RP Song, JK (corresponding author), Sichuan Prov Peoples Hosp, Inst Neurol, Chengdu 610072, Peoples R China.; Song, JK (corresponding author), Univ Elect Sci & Technol China, Chengdu 610072, Peoples R China.
EM wxuanhan@hotmail.com; yuyuguo1994@gmail.com; jingkuan.song@gmail.com;
   lianli.gao@uestc.edu.cn; shenhengtao@hotmail.com
RI Shen, Heng Tao/ABD-5331-2021
OI song, jingkuan/0000-0002-2549-8322
FU National Key Research and Development Program of China [2018AAA0102200];
   National Natural Science Foundation of China [61772116, 61872064,
   62020106008]; Sichuan Science and Technology Program [2019JDTD0005];
   Tianshu AI Platform, Zhejiang Laboratory
FX This work was supported by the National Key Research and Development
   Program of China under Grant 2018AAA0102200, in part by the National
   Natural Science Foundation of China under Grants 61772116, 61872064, and
   62020106008, in part by the Sichuan Science and Technology Program under
   Grant 2019JDTD0005, and in part by the Tianshu AI Platform, Zhejiang
   Laboratory.
CR [Anonymous], 2015, Training deeper convolutional networks with deep supervision
   Bao Q, 2021, IEEE T MULTIMEDIA, V23, P161, DOI 10.1109/TMM.2020.2980194
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Boscaini D., 2016, P 30 INT C NEUR INF, P3197
   Cai Y., 2020, COMPUTER VISION ECCV, P455
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Chen LC, 2018, PROC CVPR IEEE, P4013, DOI 10.1109/CVPR.2018.00422
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Dai Y, 2021, AAAI CONF ARTIF INTE, V35, P1193
   Finn C, 2017, PR MACH LEARN RES, V70
   Gao LL, 2020, IEEE T PATTERN ANAL, V42, P1112, DOI 10.1109/TPAMI.2019.2894139
   Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459
   Girshick R., 2014, P 2014 IEEE C COMPUT, P580, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Güler RA, 2017, PROC CVPR IEEE, P2614, DOI 10.1109/CVPR.2017.280
   Guo Y., 2021, P IEEE CVF INT C COM, P16383
   Guo YY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P356, DOI 10.1145/3343031.3350856
   Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kamel A, 2021, IEEE T MULTIMEDIA, V23, P1330, DOI 10.1109/TMM.2020.2999181
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Li YH, 2019, IEEE I CONF COMP VIS, P6053, DOI 10.1109/ICCV.2019.00615
   Lin T.Y., 2014, P 13 EUR C COMP VIS, P740
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Marino K, 2017, PROC CVPR IEEE, P20, DOI 10.1109/CVPR.2017.10
   Masci J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P832, DOI 10.1109/ICCVW.2015.112
   Neverova N, 2019, Advances in Neural Information Processing Systems (NeurIPS), V32, P920
   Neverova N, 2019, PROC CVPR IEEE, P10907, DOI 10.1109/CVPR.2019.01117
   Newell A, 2017, ADV NEUR IN, V30
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25
   Rakhimov R, 2021, IEEE WINT CONF APPL, P1868, DOI 10.1109/WACV48630.2021.00191
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shen FM, 2015, IEEE T IMAGE PROCESS, V24, P1839, DOI 10.1109/TIP.2015.2405340
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Snell J, 2017, ADV NEUR IN, V30
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang XL, 2018, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR.2018.00717
   Wang XH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P686, DOI 10.1145/3474085.3475233
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Yang L, 2019, PROC CVPR IEEE, P364, DOI 10.1109/CVPR.2019.00045
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhao J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P792, DOI 10.1145/3240508.3240509
   Zhao QJ, 2019, AAAI CONF ARTIF INTE, P9259
   Zhu Tyler, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P225, DOI 10.1007/978-3-030-58526-6_14
   Zhu XF, 2017, IEEE T NEUR NET LEAR, V28, P1263, DOI 10.1109/TNNLS.2016.2521602
NR 60
TC 6
Z9 6
U1 4
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 979
EP 992
DI 10.1109/TMM.2021.3135145
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900023
DA 2024-07-18
ER

PT J
AU Xie, Z
   Chen, JS
   Wu, KW
   Guo, D
   Hong, RC
AF Xie, Zhao
   Chen, Jiansong
   Wu, Kewei
   Guo, Dan
   Hong, Richang
TI Global Temporal Difference Network for Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Convolution; Three-dimensional displays; Correlation;
   Visualization; Kernel; Feature extraction; Action recognition; deep
   neural networks; global temporal difference; temporal modeling
ID MOTION REPRESENTATION
AB Temporal modeling still remains as a challenge for action recognition. Most existing temporal models focus on learning local variation between neighbor frames. There exists obvious deviations between local and global variations, such as subtle and notable motion variations. In this paper, we propose a global temporal difference module for action recognition, which consists of two sub-modules, i.e., a global aggregation module and a global difference module. These two sub-modules cooperate following the idea of using prior knowledge from the global view (i.e., global motion variation) to guide local learning at each moment. In the global aggregation module, the global prior knowledge is learned by aggregating the visual feature sequence of video into a global vector. In the global difference module, we prepare the difference vector sequence of video by subtracting each local vector from the global vector. Our method performs as a contextual guidance with a global view. The sequential dependency between these difference vectors is exploited with a channel-wise self-attention operation. Finally, the difference vectors at each timestamp are further used to enhance the semantics of the original local features. The enhanced features endow the action recognition has less deviation to understand the variation in the video globally. We instantiate the global temporal difference module into the ResNet block to form a global temporal difference network (GTDNet). Exhaustive experiments are conducted and our method achieves competitive performance at small FLOPs on Something-Something V1 & V2 and Kinetics-400.
C1 [Xie, Zhao; Chen, Jiansong; Wu, Kewei; Guo, Dan; Hong, Richang] Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Peoples R China.
C3 Hefei University of Technology
RP Wu, KW (corresponding author), Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Peoples R China.
EM xiezhao@hfut.edu.cn; chenjiansong0202@163.com; wukewei@hfut.edu.cn;
   guodan@hfut.edu.cn; hongrc@hfut.edu.cn
OI Guo, Dan/0000-0003-2594-254X
FU National Natural Science Foundation of China [62272144]; Fundamental
   Research Funds for the Central Universities of China [JZ2021HGQA0219]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62272144 and in part by the Fundamental
   Research Funds for the Central Universities of China under Grant
   JZ2021HGQA0219.
CR Bertasius G, 2021, PR MACH LEARN RES, V139
   Bilen H, 2016, PROC CVPR IEEE, P3034, DOI 10.1109/CVPR.2016.331
   Crasto N, 2019, PROC CVPR IEEE, P7874, DOI 10.1109/CVPR.2019.00807
   Diba A, 2018, LECT NOTES COMPUT SC, V11208, P299, DOI 10.1007/978-3-030-01225-0_18
   Dosovitskiy A., 2021, P INT C LEARN REPR, P1, DOI DOI 10.48550/ARXIV
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2020, PROC CVPR IEEE, P200, DOI 10.1109/CVPR42600.2020.00028
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Fu J, 2022, IEEE T CIRC SYST VID, V32, P5213, DOI 10.1109/TCSVT.2021.3137023
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heeseung Kwon, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P345, DOI 10.1007/978-3-030-58517-4_21
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jiang BY, 2019, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2019.00209
   Junwu Weng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P363, DOI 10.1007/978-3-030-58571-6_22
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kumawat S, 2022, IEEE T PATTERN ANAL, V44, P4839, DOI 10.1109/TPAMI.2021.3076522
   Li KC, 2021, Arxiv, DOI arXiv:2106.01603
   Li XH, 2020, PROC CVPR IEEE, P1089, DOI 10.1109/CVPR42600.2020.00117
   Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13688, DOI 10.1109/ICCV48922.2021.01345
   Liu ZY, 2020, AAAI CONF ARTIF INTE, V34, P11669
   Luo CX, 2019, IEEE I CONF COMP VIS, P5511, DOI 10.1109/ICCV.2019.00561
   Meng LL, 2019, IEEE INT CONF COMP V, P1513, DOI 10.1109/ICCVW.2019.00189
   Meng Y, 2021, Arxiv, DOI arXiv:2102.05775
   Ng JYH, 2018, IEEE WINT CONF APPL, P1606, DOI 10.1109/WACV.2018.00179
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Perez-Rua JM, 2020, Arxiv, DOI arXiv:2004.01278
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen ZW, 2022, IEEE T CIRC SYST VID, V32, P3141, DOI 10.1109/TCSVT.2021.3103677
   Shi XJ, 2015, ADV NEUR IN, V28
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Stergiou A, 2019, IEEE IMAGE PROC, P1830, DOI [10.1109/ICIP.2019.8803153, 10.1109/icip.2019.8803153]
   Stroud JC, 2020, IEEE WINT CONF APPL, P614, DOI 10.1109/wacv45572.2020.9093274
   Sudhakaran S, 2020, PROC CVPR IEEE, P1099, DOI 10.1109/CVPR42600.2020.00118
   Sun SY, 2018, PROC CVPR IEEE, P1390, DOI 10.1109/CVPR.2018.00151
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Wang H, 2020, PROC CVPR IEEE, P349, DOI 10.1109/CVPR42600.2020.00043
   Wang JP, 2022, IEEE T MULTIMEDIA, V24, P2553, DOI 10.1109/TMM.2021.3087023
   Wang LM, 2021, PROC CVPR IEEE, P1895, DOI 10.1109/CVPR46437.2021.00193
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang ZW, 2021, PROC CVPR IEEE, P13209, DOI 10.1109/CVPR46437.2021.01301
   Wu MY, 2021, AAAI CONF ARTIF INTE, V35, P2934
   Wu WH, 2021, AAAI CONF ARTIF INTE, V35, P2943
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Yang CY, 2020, PROC CVPR IEEE, P588, DOI 10.1109/CVPR42600.2020.00067
   Yang XT, 2021, PROC CVPR IEEE, P7563, DOI 10.1109/CVPR46437.2021.00748
   Zhang Y, 2019, P IEEE CVF C COMP VI, P12005
   Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49
   Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43
NR 53
TC 2
Z9 2
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7594
EP 7606
DI 10.1109/TMM.2022.3224327
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000062
DA 2024-07-18
ER

PT J
AU Yao, LX
   Kusakunniran, W
   Zhang, P
   Wu, Q
   Zhang, J
AF Yao, Lingxiang
   Kusakunniran, Worapan
   Zhang, Peng
   Wu, Qiang
   Zhang, Jian
TI Improving Disentangled Representation Learning for Gait Recognition
   Using Group Supervision
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gait recognition; deep learning; disentangled representation learning
ID RECOGNIZING GAITS; IDENTIFICATION; MOTION
AB For decades, gait has been gathering extensive interest due to the advantage that it can be measured from a distance without physical contact. However, for image/video-based gait recognition, its performance can be remarkably influenced by exterior factors, such as viewing angles and clothing changes. Thus, in this paper, a group-supervised disentangled representation learning network is proposed for gait recognition to extract features invariant to these factors. First, sequences are explicitly disentangled into pose, gait, appearance, and view features through a generic encoder-decoder framework. To ensure feature adaptability and independency, a disentanglement swap module is specifically adopted during our encoder-decoder process through a series of swap operations based on the feature attributes. Following the feature disentanglement, a disentanglement aggregation module is also specially proposed for pose, gait, and appearance features to enhance their effectiveness. Finally, the enhanced three features are concatenated together for gait recognition. Relevant experiments certify that compared with other disentangled representation learning-based gait recognition methods, our proposed method enables a more excellent recognition result, despite fewer gait frames being utilized.
C1 [Yao, Lingxiang] Univ Technol Sydney, Sch Elect & Data Engn, Sydney, NSW 2007, Australia.
   [Kusakunniran, Worapan] Mahidol Univ, Fac Informat & Commun Technol, Nakhon Pathom 73170, Thailand.
   [Zhang, Peng] Shandong Univ Sci & Technol, Dept Artificial Intelligence, Qingdao 266590, Peoples R China.
   [Wu, Qiang; Zhang, Jian] Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
C3 University of Technology Sydney; Mahidol University; Shandong University
   of Science & Technology; University of Technology Sydney
RP Kusakunniran, W (corresponding author), Mahidol Univ, Fac Informat & Commun Technol, Nakhon Pathom 73170, Thailand.
EM lingxiang.yao@student.uts.edu.au; worapan.kun@mahidol.edu;
   pengzhang_skd@sdust.edu.cn; qiang.wu@uts.edu.au; jian.zhang@uts.edu.au
OI Kusakunniran, Worapan/0000-0002-2896-611X; Zhang,
   Jian/0000-0002-7240-3541; Wu, Qiang/0000-0001-5641-2483
CR Anusha R, 2020, MULTIMED TOOLS APPL, V79, P2873, DOI 10.1007/s11042-019-08400-8
   Battistone F, 2019, PATTERN RECOGN LETT, V126, P132, DOI 10.1016/j.patrec.2018.05.004
   Chai T., 2021, P IEEE INT C MULT EX, P1
   Chao Fan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14213, DOI 10.1109/CVPR42600.2020.01423
   Chao HQ, 2019, AAAI CONF ARTIF INTE, P8126
   Chen X, 2018, IEEE T PATTERN ANAL, V40, P1697, DOI 10.1109/TPAMI.2017.2726061
   Deng MQ, 2024, IEEE-CAA J AUTOMATIC, V11, P1530, DOI 10.1109/JAS.2018.7511096
   Feng Y, 2016, INT C PATT RECOG, P325, DOI 10.1109/ICPR.2016.7899654
   Ge YH, 2021, Arxiv, DOI arXiv:2009.06586
   Han J, 2006, IEEE T PATTERN ANAL, V28, P316, DOI 10.1109/TPAMI.2006.38
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hossain MA, 2010, PATTERN RECOGN, V43, P2281, DOI 10.1016/j.patcog.2009.12.020
   Hu HF, 2013, IEEE T CIRC SYST VID, V23, P1274, DOI 10.1109/TCSVT.2013.2242640
   Hu MD, 2013, IEEE T INF FOREN SEC, V8, P2034, DOI 10.1109/TIFS.2013.2287605
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Kingma D. P., 2014, arXiv
   Kusakunniran Worapan, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1058, DOI 10.1109/ICCVW.2009.5457587
   Kusakunniran W, 2014, IEEE T INF FOREN SEC, V9, P1416, DOI 10.1109/TIFS.2014.2336379
   Kusakunniran W, 2014, IEEE T IMAGE PROCESS, V23, P696, DOI 10.1109/TIP.2013.2294552
   Kusakunniran W, 2012, IEEE T SYST MAN CY B, V42, P1654, DOI 10.1109/TSMCB.2012.2197823
   Kusakunniran W, 2012, IEEE T CIRC SYST VID, V22, P966, DOI 10.1109/TCSVT.2012.2186744
   Kusakunniran W, 2010, PROC CVPR IEEE, P974, DOI 10.1109/CVPR.2010.5540113
   Lam THW, 2006, INT C PATT RECOG, P996
   Li SQ, 2019, IEEE T MULTIMEDIA, V21, P2361, DOI 10.1109/TMM.2019.2900134
   Lin BB, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3054, DOI 10.1145/3394171.3413861
   Lin BB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14628, DOI 10.1109/ICCV48922.2021.01438
   Makihara Y., 2020, Comput. Vis, A Reference Guide, P1
   Makihara Y., 2012, IPSJ T COMPUT VISION, V4, P53, DOI DOI 10.2197/IPSJTCVA.4.53
   Makihara Y, 2006, LECT NOTES COMPUT SC, V3953, P151, DOI 10.1007/11744078_12
   Makihara Y, 2018, IEEE COMPUT SOC CONF, P674, DOI 10.1109/CVPRW.2018.00098
   Peng X, 2017, IEEE I CONF COMP VIS, P1632, DOI 10.1109/ICCV.2017.180
   Sepas-Moghaddam A, 2023, IEEE T PATTERN ANAL, V45, P264, DOI 10.1109/TPAMI.2022.3151865
   Shiraga K, 2016, INT CONF BIOMETR
   Singh S, 2009, LECT NOTES COMPUT SC, V5909, P446, DOI 10.1007/978-3-642-11164-8_72
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141
   Voynov A., 2020, INT C MACH LEARN, V119, P9786
   Wang YT, 2018, LECT NOTES COMPUT SC, V11219, P764, DOI 10.1007/978-3-030-01267-0_45
   Wolf T, 2016, IEEE IMAGE PROC, P4165, DOI 10.1109/ICIP.2016.7533144
   Wu ZF, 2017, IEEE T PATTERN ANAL, V39, P209, DOI 10.1109/TPAMI.2016.2545669
   Xiang Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13306, DOI 10.1109/CVPR42600.2020.01332
   Xing XL, 2016, PATTERN RECOGN, V50, P107, DOI 10.1016/j.patcog.2015.08.011
   Xu C, 2016, P AS C COMP VIS, P52
   Xu K, 2021, IEEE T MULTIMEDIA, V24, P3265, DOI 10.1109/TMM.2021.3095809
   Yao LX, 2021, INT C PATT RECOG, P2057, DOI 10.1109/ICPR48806.2021.9412714
   Yao LX, 2021, PATTERN RECOGN LETT, V150, P289, DOI 10.1016/j.patrec.2019.05.012
   Ye MX, 2020, IEEE T MULTIMEDIA, V22, P1113, DOI 10.1109/TMM.2019.2942479
   Yoo JS, 2021, 35TH INTERNATIONAL CONFERENCE ON INFORMATION NETWORKING (ICOIN 2021), P687, DOI 10.1109/ICOIN50884.2021.9334007
   Yu SQ, 2019, PATTERN RECOGN, V87, P179, DOI 10.1016/j.patcog.2018.10.019
   Yu SQ, 2017, IEEE COMPUT SOC CONF, P532, DOI 10.1109/CVPRW.2017.80
   Yu SQ, 2017, NEUROCOMPUTING, V239, P81, DOI 10.1016/j.neucom.2017.02.006
   Zhang C, 2016, INT CONF ACOUST SPEE, P2832, DOI 10.1109/ICASSP.2016.7472194
   Zhang KH, 2019, PROC CVPR IEEE, P4695, DOI 10.1109/CVPR.2019.00483
   Zhang P., 2019, P INT JOINT C NEUR N, P1
   Zhang YQ, 2019, PATTERN RECOGN, V93, P228, DOI 10.1016/j.patcog.2019.04.023
   Zhang ZY, 2022, IEEE T PATTERN ANAL, V44, P345, DOI 10.1109/TPAMI.2020.2998790
   Zhang ZY, 2019, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR.2019.00484
   Zheng S, 2011, IEEE IMAGE PROC, DOI 10.1109/ICIP.2011.6115889
NR 59
TC 5
Z9 5
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4187
EP 4198
DI 10.1109/TMM.2022.3171961
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200009
DA 2024-07-18
ER

PT J
AU Zeng, HJ
   Xue, JZ
   Luong, HQ
   Philips, W
AF Zeng, Haijin
   Xue, Jize
   Luong, Hiap Q.
   Philips, Wilfried
TI Multimodal Core Tensor Factorization and its Applications to Low-Rank
   Tensor Completion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Tensors; Matrix decomposition; Correlation; Task analysis; Hyperspectral
   imaging; Data models; Minimization; Low-rankness; nonconvex
   optimization; tensor; tensor factorization
ID NUCLEAR NORM; MATRIX; DECOMPOSITIONS; REPRESENTATION; MINIMIZATION
AB Low-rank tensor completion has been widely used in computer vision and machine learning. This paper develops a novel multimodal core tensor factorization (MCTF) method combined with a tensor low-rankness measure and a better nonconvex relaxation form of this measure (NC-MCTF). The proposed models encode low-rank insights for general tensors provided by Tucker and T-SVD and thus are expected to simultaneously model spectral low-rankness in multiple orientations and accurately restore the data of intrinsic low-rank structure based on few observed entries. Furthermore, we study the MCTF and NC-MCTF regularization minimization problem and design an effective block successive upper-bound minimization (BSUM) algorithm to solve them. Theoretically, we prove that the iterates generated by the proposed models converge to the set of coordinatewise minimizers. This efficient solver can extend MCTF to various tasks such as tensor completion. A series of experiments including hyperspectral image (HSI), video and MRI completion confirm the superior performance of the proposed method.
C1 [Zeng, Haijin; Xue, Jize; Luong, Hiap Q.; Philips, Wilfried] Univ Ghent, Image Proc & Interpretat IMEC Res Grp, B-9000 Ghent, Belgium.
C3 Ghent University
RP Xue, JZ (corresponding author), Univ Ghent, Image Proc & Interpretat IMEC Res Grp, B-9000 Ghent, Belgium.
EM zeng_navy@163.com; xuejize900507@mail.nwpu.edu.cn; Hiep.Luong@UGent.be;
   wilfried.philips@ugent.be
OI Xue, Jize/0000-0003-2451-7445; Zeng, Haijin/0000-0003-0398-3316
FU Flemish Government (AI Research Program) and China Scholarship Council
   [202106300021]
FX This work was supported by Flemish Government (AI Research Program) and
   China Scholarship Council under Grant 202106300021. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Jiebo Luo.
CR Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chen Y., 2021, IEEE Trans. Multimedia, V24, P4054
   Chen YY, 2020, IEEE T MULTIMEDIA, V22, P1985, DOI 10.1109/TMM.2019.2952984
   Chia-Hsiang Lin, 2021, 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS, P2480, DOI 10.1109/IGARSS47720.2021.9554403
   Cichocki A, 2015, IEEE SIGNAL PROC MAG, V32, P145, DOI 10.1109/MSP.2013.2297439
   De Lathauwer L, 1998, INST MATH C, V67, P1
   Eriksson A, 2010, PROC CVPR IEEE, P771, DOI 10.1109/CVPR.2010.5540139
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   HARSHMAN RA, 1994, COMPUT STAT DATA AN, V18, P39, DOI 10.1016/0167-9473(94)90132-5
   HASTAD J, 1990, J ALGORITHMS, V11, P644, DOI 10.1016/0196-6774(90)90014-6
   Heide F, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661260
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Ji TY, 2022, IEEE SIGNAL PROC LET, V29, P1162, DOI 10.1109/LSP.2022.3169044
   Ji TY, 2016, INFORM SCIENCES, V326, P243, DOI 10.1016/j.ins.2015.07.049
   Jiang TX, 2020, IEEE T IMAGE PROCESS, V29, P7233, DOI 10.1109/TIP.2020.3000349
   Jiang TX, 2020, J COMPUT APPL MATH, V372, DOI 10.1016/j.cam.2019.112680
   Kilmer ME, 2013, SIAM J MATRIX ANAL A, V34, P148, DOI 10.1137/110837711
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Lai MJ, 2013, SIAM J NUMER ANAL, V51, P927, DOI 10.1137/110840364
   Lin C.-H., 2021, IEEE Trans. Geosci. Remote Sens., V60
   Lin CH, 2018, IEEE T GEOSCI REMOTE, V56, P1652, DOI 10.1109/TGRS.2017.2766080
   Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39
   Lu CY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2504
   Lu CY, 2020, IEEE T PATTERN ANAL, V42, P925, DOI 10.1109/TPAMI.2019.2891760
   Lu CY, 2016, PROC CVPR IEEE, P5249, DOI 10.1109/CVPR.2016.567
   Oseledets IV, 2011, SIAM J SCI COMPUT, V33, P2295, DOI 10.1137/090752286
   Raychaudhuri S, 2000, Pac Symp Biocomput, P455
   Razaviyayn M, 2013, SIAM J OPTIMIZ, V23, P1126, DOI 10.1137/120891009
   Shashua A, 1997, INT J COMPUT VISION, V21, P99, DOI 10.1023/A:1007975506780
   Song LC, 2018, AAAI CONF ARTIF INTE, P2419
   Teodoro AM, 2016, IEEE IMAGE PROC, P3518, DOI 10.1109/ICIP.2016.7533014
   TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464
   Wald L., 2002, Data Fusion: Definitions and Architectures: Fusion of Images of Different Spatial Resolutions
   Wang Y, 2018, IEEE J-STARS, V11, P1227, DOI 10.1109/JSTARS.2017.2779539
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wen ZW, 2012, MATH PROGRAM COMPUT, V4, P333, DOI 10.1007/s12532-012-0044-1
   Wu YK, 2017, NEUROCOMPUTING, V223, P107, DOI 10.1016/j.neucom.2016.10.030
   Xie Q, 2018, IEEE T PATTERN ANAL, V40, P1888, DOI 10.1109/TPAMI.2017.2734888
   Xie Y, 2018, INT J COMPUT VISION, V126, P1157, DOI 10.1007/s11263-018-1086-2
   Xu Y., 2015, Inverse Problems Imag., V9
   Xue JZ, 2022, IEEE T NEUR NET LEAR, V33, P6916, DOI 10.1109/TNNLS.2021.3083931
   Xue JZ, 2020, IEEE T NEUR NET LEAR, V31, P4567, DOI 10.1109/TNNLS.2019.2956153
   Yuan LH, 2019, AAAI CONF ARTIF INTE, P9151
   Zeng H., 2020, arXiv
   Zeng HJ, 2021, IEEE T COMPUT IMAG, V7, P164, DOI 10.1109/TCI.2021.3053699
   Zeng HJ, 2021, IEEE T GEOSCI REMOTE, V59, P3309, DOI 10.1109/TGRS.2020.3007945
   Zeng HJ, 2021, SIGNAL PROCESS, V178, DOI 10.1016/j.sigpro.2020.107805
   Zeng HJ, 2020, COMPUT VIS IMAGE UND, V197, DOI 10.1016/j.cviu.2020.103004
   Zeng HJ, 2020, IEEE ACCESS, V8, P50190, DOI 10.1109/ACCESS.2020.2979809
   Zhang LF, 2021, IEEE T CYBERNETICS, V51, P673, DOI [10.1109/TCYB.2019.2910151, 10.1109/TCYB.2019.2935066]
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang ZM, 2017, IEEE T SIGNAL PROCES, V65, P1511, DOI 10.1109/TSP.2016.2639466
   Zheng YB, 2019, APPL MATH MODEL, V70, P677, DOI 10.1016/j.apm.2019.02.001
   Zhou P, 2018, IEEE T IMAGE PROCESS, V27, P1152, DOI 10.1109/TIP.2017.2762595
NR 54
TC 13
Z9 13
U1 4
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7010
EP 7024
DI 10.1109/TMM.2022.3216746
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000022
OA Green Published
DA 2024-07-18
ER

PT J
AU Zhang, TY
   El Ali, A
   Hanjalic, A
   Cesar, P
AF Zhang, Tianyi
   El Ali, Abdallah
   Hanjalic, Alan
   Cesar, Pablo
TI Few-Shot Learning for Fine-Grained Emotion Recognition Using
   Physiological Signals
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotion recognition; Training; Physiology; Prediction algorithms; Data
   models; Training data; Annotations; Deep siamese network; emotion
   recognition; physiological signals; small data
ID MULTITASK
AB Fine-grained emotion recognition can model the temporal dynamics of emotions, which is more precise than predicting one emotion retrospectively for an activity (e.g., video clip watching). Previous works require large amounts of continuously annotated data to train an accurate recognition model, however experiments to collect such large amounts of continuously annotated physiological signals are costly and time-consuming. To overcome this challenge, we propose an Emotion recognition algorithm based on Deep Siamese Networks (EmoDSN) which can rapidly converge on a small amount of training data, typically less than 10 samples per class (i.e., <10 shot). EmoDSN recognizes fine-grained valence and arousal (V-A) labels by maximizing the distance metric between signal segments with differentV-Alabels. We tested EmoDSN on three different datasets collected in three different environments: desktop, mobile and HMD-based virtual reality, respectively. The results from our experiments show that EmoDSN achieves promising results for both one-dimension binary (high/low V-A, 1D-2 C) and two-dimensional 5-class (four quadrants of V- A space + neutral, 2D-5 C) classification. We get an averaged accuracy of 76.04, 76.62 and 57.62% for 1D-2 C valence, 1D-2 C arousal, and 2D5 C, respectively, by using only 5 shots of training data. Our experiments show that EmoDSN can achieve better results if we select training samples from the changing points of emotion or the ending moments of video watching.
C1 [Zhang, Tianyi; El Ali, Abdallah; Cesar, Pablo] Ctr Wiskunde & Informat CWI, Distributed & Interact Syst, NL-1098XG Amsterdam, Netherlands.
   [Zhang, Tianyi; Hanjalic, Alan; Cesar, Pablo] Delft Univ Technol, Multimedia Comp Grp, NL-2600AA Delft, Netherlands.
C3 Delft University of Technology
RP Zhang, TY (corresponding author), Ctr Wiskunde & Informat CWI, Distributed & Interact Syst, NL-1098XG Amsterdam, Netherlands.; Zhang, TY (corresponding author), Delft Univ Technol, Multimedia Comp Grp, NL-2600AA Delft, Netherlands.
EM tianyi@cwi.nl; abdallah.el.ali@cwi.nl; a.hanjalic@tudelft.nl;
   p.s.cesar@cwi.nl
RI Zhang, Tianyi/AAG-6220-2021
OI El Ali, Abdallah/0000-0002-9954-4088; Cesar, Pablo/0000-0003-1752-6837;
   Hanjalic, Alan/0000-0002-5771-2549; Zhang, Tianyi/0000-0001-6293-881X
CR [Anonymous], 1988, Social psychophysiology and emotion: Theory and clinical applications
   [Anonymous], IEEE T AFFECT COMPUT
   [Anonymous], 1884, Mind, DOI DOI 10.1093/MIND/OS-IX.34.188
   [Anonymous], 2013, 2013 10 IEEE INT C W, DOI [10.1109/FG.2013.6553805, DOI 10.1109/FG.2013.6553805]
   Awais M, 2021, IEEE INTERNET THINGS, V8, P16863, DOI 10.1109/JIOT.2020.3044031
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Calvo RA, 2010, IEEE T AFFECT COMPUT, V1, P18, DOI 10.1109/T-AFFC.2010.1
   Chao L, 2015, P 5 INT WORKSH AUD V, P65, DOI [DOI 10.1145/2808196.2811634, 10.1145/2808196.2811634]
   Chen G, 2019, P 2019 INT C ARTIFIC, P309, DOI [DOI 10.1145/3349341.3349422, 10.1145/3349341.3349422]
   Chen MT, 2020, AAAI CONF ARTIF INTE, V34, P10559
   Chu YQ, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00279
   Daniels R.W., 1974, Approximation Methods for Electronic Filter Design
   Deng XW, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2391, DOI 10.1145/3474085.3475403
   Le D, 2017, INTERSPEECH, P1108, DOI 10.21437/Interspeech.2017-94
   FENG K, 2020, A SIAMESE NEURAL NET
   Finn C, 2017, PR MACH LEARN RES, V70
   Fleureau J, 2013, INT CONF AFFECT, P73, DOI 10.1109/ACII.2013.19
   FREDRICKSON BL, 1993, J PERS SOC PSYCHOL, V65, P45, DOI 10.1037/0022-3514.65.1.45
   GOLDBLUM M, 2019, ROBUST FEW SHOT LEAR
   GOLKAR S, 2019, CONTINUAL LEARNING V
   Hanjalic A, 2006, IEEE SIGNAL PROC MAG, V23, P90, DOI 10.1109/MSP.2006.1621452
   HAYALE W, IEEE T AFFECT COMPUT
   Huang Z, 2015, P 5 INT WORKSH AUD V, P41, DOI 10.1145/2808196.2811640
   Jarwar MA, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10062181
   Jia ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1047, DOI 10.1145/3474085.3475583
   JIANG S, IEEE INTERNET THINGS
   Jiao YY, 2020, NEUROCOMPUTING, V408, P100, DOI 10.1016/j.neucom.2019.05.108
   Kandemir M, 2014, NEUROCOMPUTING, V139, P97, DOI 10.1016/j.neucom.2014.02.057
   KARTHIKEYAN P, 2012, J PHYS THER SCI, V24, P1341, DOI DOI 10.1589/jpts.24.1341
   KHORRAM S, 2017, CAPTURING LONG TERM
   Khorram S, 2021, IEEE T AFFECT COMPUT, V12, P1069, DOI [10.1109/TAFFC.2019.2917047, 10.1109/taffc.2019.2917047]
   Kumar P, 2017, GEOCARTO INT, V32, P206, DOI 10.1080/10106049.2015.1132483
   LI Y, 2017, APPL SCI, V7
   Loo Yi, 2019, Few-shot regression via learned basis functions.
   Mariooryad S, 2015, IEEE T AFFECT COMPUT, V6, P97, DOI 10.1109/TAFFC.2014.2334294
   Mehmood A, 2020, BRAIN SCI, V10, DOI 10.3390/brainsci10020084
   Meijering E, 2002, P IEEE, V90, P319, DOI 10.1109/5.993400
   Metallinou A, 2013, IEEE INT CONF AUTOMA
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Na J, 2021, PROC CVPR IEEE, P1094, DOI 10.1109/CVPR46437.2021.00115
   Nagel F, 2007, BEHAV RES METHODS, V39, P283, DOI 10.3758/BF03193159
   OORD AVD, 2016, WAVENET A GENERATIVE
   Park CY, 2020, SCI DATA, V7, DOI 10.1038/s41597-020-00630-y
   Patane Andrea, 2019, Machine Learning, Optimization, and Data Science. 4th International Conference, LOD 2018. Revised Selected Papers: Lecture Notes in Computer Science (LNCS 11331), P1, DOI 10.1007/978-3-030-13709-0_1
   Paul, 2007, EMOTIONS REVEALED RE
   Pei EC, 2019, MULTIMED TOOLS APPL, V78, P19387, DOI 10.1007/s11042-019-7313-1
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Romeo L, 2022, IEEE T AFFECT COMPUT, V13, P389, DOI 10.1109/TAFFC.2019.2954118
   SALEHI P, 2020, GENERATIVE ADVERSARI
   Schmidt P, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P400, DOI 10.1145/3242969.3242985
   Sharma K, 2019, SCI DATA, V6, DOI 10.1038/s41597-019-0209-0
   Sharma K, 2020, IEEE T AFFECT COMPUT, V11, P78, DOI 10.1109/TAFFC.2017.2772882
   Shu L, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18072074
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Snell J., 2017, ADV NEURAL INFORM PR, V30, P4077
   Soleymani M, 2016, IEEE T AFFECT COMPUT, V7, P17, DOI 10.1109/TAFFC.2015.2436926
   Song RC, 2021, IEEE J BIOMED HEALTH, V25, P1373, DOI 10.1109/JBHI.2021.3051176
   Srinivasan A, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376782
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tripathi S, 2017, AAAI CONF ARTIF INTE, P4746
   Valstar M, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON AUDIO/VISUAL EMOTION CHALLENGE (AVEC'16), P3, DOI 10.1145/2988257.2988258
   Van Gent P., 2018, P 6 HUMANIST C HAG N, P173
   Van Houdt G, 2020, ARTIF INTELL REV, V53, P5929, DOI 10.1007/s10462-020-09838-1
   Vinyals O., 2016, ADV NEURAL INFORM PR, P3630, DOI DOI 10.48550/ARXIV.1606.04080
   WANG Y, 2021, PATTERN RECOGNIT, V110
   WANG Y, 2019, FEW SHOT LEARNING A
   XUE T, IEEE T MULTIMEDIA
   Xue T, 2021, EXTENDED ABSTRACTS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'21), DOI 10.1145/3411763.3451627
   Zhan C, 2019, IEEE I CONF COMP VIS, P1151, DOI 10.1109/ICCV.2019.00124
   Zhang AT, 2018, IEEE IMAGE PROC, P2660, DOI 10.1109/ICIP.2018.8451219
   Zhang HC, 2011, INT CONF ACOUST SPEE, P2220
   ZHANG T, 2021, SENSORS, V21
   Zhang TC, 2020, IEEE GLOB COMM CONF, DOI 10.1109/GLOBECOM42002.2020.9322560
   ZHANG TY, 2019, P INT C MULTIMODAL I, P404
   Zhong QH, 2020, FRONT HUM NEUROSCI, V14, DOI 10.3389/fnhum.2020.589001
   Zou FY, 2019, PROC CVPR IEEE, P1119, DOI 10.1109/CVPR.2019.01138
NR 77
TC 6
Z9 6
U1 7
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3773
EP 3787
DI 10.1109/TMM.2022.3165715
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500018
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Zhou, YH
   Wang, Y
   Chau, LP
AF Zhou, Yunhao
   Wang, Yi
   Chau, Lap-Pui
TI Moving Towards Centers: Re-Ranking With Attention and Memory for
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlation; Probes; Feature extraction; Transformers; Image
   reconstruction; Benchmark testing; Aggregates; Re-Identification;
   Transformer; attention; re-ranking; contextual memory
ID PERSON REIDENTIFICATION
AB utilizes contextual information to optimize the initial ranking list of person or vehicle re-identification (re-ID), which boosts the retrieval performance at post-processing steps. This paper proposes a re-ranking network to predict the correlations between the probe and top-ranked neighbor samples. Specifically, all the feature embeddings of query and gallery images are expanded and enhanced by a linear combination of their neighbors, with the correlation prediction serving as discriminative combination weights. The combination process is equivalent to moving independent embeddings toward the identity centers, improving cluster compactness. For correlation prediction, we first aggregate the contextual information for probe's k-nearest neighbors via the Transformer encoder. Then, we distill and refine the probe-related features into the Contextual Memory cell via attention mechanism. Like humans that retrieve images by not only considering probe images but also memorizing the retrieved ones, the Contextual Memory produces multi-view descriptions for each instance. Finally, the neighbors are reconstructed with features fetched from the Contextual Memory, and a binary classifier predicts their correlations with the probe. Experiments on six widely-used person and vehicle re-ID benchmarks demonstrate the effectiveness of the proposed method. Especially, our method surpasses the state-of-the-art re-ranking approaches on large-scale datasets by a significant margin, i.e., with an average 4.83% CMC@1 and 14.83% mAP improvements on VERI-Wild, MSMT17, and VehicleID datasets.
C1 [Zhou, Yunhao; Wang, Yi; Chau, Lap-Pui] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Chau, LP (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM zh0022ao@e.ntu.edu.sg; wang1241@e.ntu.edu.sg; elpchau@ntu.edu.sg
OI Wang, Yi/0000-0001-8659-4724; Chau, Lap-Pui/0000-0003-4932-0593
CR Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Bai S, 2016, IEEE T IMAGE PROCESS, V25, P1056, DOI 10.1109/TIP.2016.2514498
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Dosovitskiy Alexey, 2020, ABS201011929 CORR
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LX, 2020, Arxiv, DOI arXiv:2006.02631
   He Shuting, 2021, P IEEE CVF INT C COM
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Khorramshahi Pirazh, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P369, DOI 10.1007/978-3-030-58568-6_22
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liu HM, 2021, NEUROCOMPUTING, V423, P57, DOI 10.1016/j.neucom.2020.10.019
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Liu XC, 2018, IEEE T MULTIMEDIA, V20, P645, DOI 10.1109/TMM.2017.2751966
   Lou YH, 2019, PROC CVPR IEEE, P3230, DOI 10.1109/CVPR.2019.00335
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Mang Ye, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015, January 5-7, 2015, Proceedings: LNCS 8935, P105, DOI 10.1007/978-3-319-14445-0_10
   Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152
   Parmar N, 2018, PR MACH LEARN RES, V80
   Qian XL, 2017, IEEE I CONF COMP VIS, P5409, DOI 10.1109/ICCV.2017.577
   Qin DF, 2011, PROC CVPR IEEE, P777, DOI 10.1109/CVPR.2011.5995373
   Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HB, 2020, IEEE MULTIMEDIA, V27, P112, DOI 10.1109/MMUL.2020.2999464
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y, 2018, PROC CVPR IEEE, P8042, DOI [10.1109/CVPR.2018.00839, 10.1109/CVPR.2018.00736]
   Wang Z, 2019, IEEE T MULTIMEDIA, V21, P2376, DOI 10.1109/TMM.2019.2898753
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Xiong F, 2014, LECT NOTES COMPUT SC, V8695, P1, DOI 10.1007/978-3-319-10584-0_1
   Yang TY, 2021, IEEE T PATTERN ANAL, V43, P360, DOI 10.1109/TPAMI.2019.2929034
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Yu R., 2017, BRIT MACH VIS C 2017
   Yuan Y, 2019, AAAI CONF ARTIF INTE, P9167
   Zhang XM, 2020, Arxiv, DOI arXiv:2012.07620
   Zheng L, 2017, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2017.357
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2021, IEEE T MULTIMEDIA, V23, P2683, DOI 10.1109/TMM.2020.3014488
   Zhong Z., 2017, P IEEE CVF C COMP VI, P1318, DOI [10.1109/CVPR.2017.389, DOI 10.1109/CVPR.2017.389]
   Zhou Y, 2018, IEEE T IMAGE PROCESS, V27, P3275, DOI 10.1109/TIP.2018.2819820
NR 49
TC 5
Z9 5
U1 4
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3456
EP 3468
DI 10.1109/TMM.2022.3161189
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200040
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chang, M
   Feng, HJ
   Xu, ZH
   Li, Q
AF Chang, Meng
   Feng, Huajun
   Xu, Zhihai
   Li, Qi
TI Low-Light Image Restoration With Short- and Long-Exposure Raw Pairs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Imaging; Image color analysis; Colored noise; Task analysis; Pipelines;
   Noise reduction; Mobile handsets; Deblurring; denoising; image fusion;
   image restoration; low-light imaging
AB Low-light imaging with handheld mobile devices is a challenging issue. Limited by the existing models and training data, most existing methods cannot be effectively applied in real scenarios. In this paper, we propose a new low-light image restoration method by using the complementary information of short- and long-exposure images. We first propose a novel data generation method to synthesize realistic short- and long-exposure raw images by simulating the imaging pipeline in low-light environment. Then, we design a new long-short-exposure fusion network (LSFNet) to deal with the problems of low-light image fusion, including high noise, motion blur, color distortion and misalignment. The proposed LSFNet takes pairs of short- and long-exposure raw images as input, and outputs a clear RGB image. Using our data generation method and the proposed LSFNet, we can recover the details and color of the original scene, and improve the low-light image quality effectively. Experiments demonstrate that our method can outperform the state-of-the-art methods.
C1 [Chang, Meng; Feng, Huajun; Xu, Zhihai; Li, Qi] Zhejiang Univ, State Key Lab Modern Opt Instrumentat, Hangzhou 310027, Peoples R China.
C3 Zhejiang University
RP Li, Q (corresponding author), Zhejiang Univ, State Key Lab Modern Opt Instrumentat, Hangzhou 310027, Peoples R China.
EM changm@zju.edu.cn; fenghj@zju.edu.cn; xuzh@zju.edu.cn; liqi@zju.edu.cn
OI Chang, Meng/0000-0002-9387-9338
FU Science and Technology Project of Jiangsu Province [D040301]; 
   [BE2019063]
FX This work was supported in part by by Basic Research on Civil Aerospace
   No. D040301, and in part by the Science and Technology Project of
   Jiangsu Province No. BE2019063.
CR Abdelhamed A, 2019, IEEE I CONF COMP VIS, P3165, DOI 10.1109/ICCV.2019.00326
   Abdelhamed A, 2018, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2018.00182
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Aittala M, 2018, LECT NOTES COMPUT SC, V11212, P748, DOI 10.1007/978-3-030-01237-3_45
   Anwar S, 2019, IEEE I CONF COMP VIS, P3155, DOI 10.1109/ICCV.2019.00325
   Brooks T, 2019, PROC CVPR IEEE, P11028, DOI 10.1109/CVPR.2019.01129
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Chan K. C. K., 2020, ARXIVABS200907265
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Cho S, 2011, IEEE I CONF COMP VIS, P495, DOI 10.1109/ICCV.2011.6126280
   Dabov K, 2007, IEEE IMAGE PROC, P313, DOI 10.1109/icip.2007.4378954
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Du Y, 2021, IEEE T MULTIMEDIA, V23, P2139, DOI 10.1109/TMM.2020.3008057
   Godard C, 2018, LECT NOTES COMPUT SC, V11219, P560, DOI 10.1007/978-3-030-01267-0_33
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   Hasinoff SW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980254
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jin Z, 2020, IEEE T MULTIMEDIA, V22, P1055, DOI 10.1109/TMM.2019.2938340
   Kalantari NK, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073609
   Khashabi D, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2359774
   Kingma D. P., 2014, arXiv
   Köhler R, 2012, LECT NOTES COMPUT SC, V7578, P27, DOI 10.1007/978-3-642-33786-4_3
   Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Malvar HS, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P485
   Mertens T, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P382, DOI 10.1109/PG.2007.17
   Mildenhall B, 2018, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2018.00265
   Mustaniemi Janne, 2018, ARXIV181109485
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Pan JS, 2016, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR.2016.180
   Pan JS, 2016, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2016.306
   Park SH, 2014, PROC CVPR IEEE, P3366, DOI 10.1109/CVPR.2014.430
   Paszke A, 2019, ADV NEUR IN, V32
   Ren C, 2019, IEEE T MULTIMEDIA, V21, P731, DOI 10.1109/TMM.2018.2866362
   Rengarajan Vijay, 2020, P IEEECVF C COMPUTER, P510
   Roth S, 2009, INT J COMPUT VISION, V82, P205, DOI 10.1007/s11263-008-0197-6
   Sindelár O, 2013, J ELECTRON IMAGING, V22, DOI 10.1117/1.JEI.22.1.011003
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Wang W, 2019, IEEE I CONF COMP VIS, P4110, DOI 10.1109/ICCV.2019.00421
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wei KX, 2020, PROC CVPR IEEE, P2755, DOI 10.1109/CVPR42600.2020.00283
   Wieschollek P, 2017, IEEE I CONF COMP VIS, P231, DOI 10.1109/ICCV.2017.34
   Xu J, 2018, LECT NOTES COMPUT SC, V11212, P21, DOI 10.1007/978-3-030-01237-3_2
   Xu J, 2018, IEEE T IMAGE PROCESS, V27, P2996, DOI 10.1109/TIP.2018.2811546
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157
   Yan QS, 2019, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2019.00185
   Yuan L, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239452
   Zhang HG, 2019, PROC CVPR IEEE, P5971, DOI 10.1109/CVPR.2019.00613
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang Shuang, 2019, ARXIV191108541
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
   Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278
NR 56
TC 17
Z9 17
U1 2
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 702
EP 714
DI 10.1109/TMM.2021.3058586
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100015
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, HA
   Jin, Y
   Xu, K
   Chen, YX
   Zhu, CG
AF Chen, Huaian
   Jin, Yi
   Xu, Kai
   Chen, Yuxuan
   Zhu, Changan
TI Multiframe-to-Multiframe Network for Video Denoising
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video denoising; multiframe to multiframe; denoising efficiency;
   temporal consistency; spatiotemporal
ID OBJECT DETECTION; IMAGE; FRAMEWORK; SEGMENTATION; ENHANCEMENT; FILTER;
   NOISE; CNN
AB Most existing studies performed video denoising by using multiple adjacent noisy frames to recover one clean frame; however, despite achieving relatively good quality for each individual frame, these approaches may result in visual flickering when the denoised frames are considered in sequence. In this paper, instead of separately restoring each clean frame, we propose a multiframe-to-multiframe (MM) denoising scheme that simultaneously recovers multiple clean frames from consecutive noisy frames. The proposed MM denoising scheme uses a training strategy that optimizes the denoised video from both the spatial and temporal dimensions, enabling better temporal consistency in the denoised video. Furthermore, we present an MM network (MMNet), which adopts a spatiotemporal convolutional architecture that considers both the interframe similarity and single-frame characteristics. Benefiting from the underlying parallel mechanism of the MM denoising scheme, MMNet achieves a highly competitive denoising efficiency. Extensive analyses and experiments demonstrate that MMNet outperforms the state-of-the-art video denoising methods, yielding temporal consistency improvements of at least 13.3% and running more than 2 times faster than the other methods.
C1 [Chen, Huaian; Jin, Yi; Xu, Kai; Chen, Yuxuan; Zhu, Changan] Univ Sci & Technol China, Sch Engn Sci, Huainan 230022, Anhui, Peoples R China.
   [Jin, Yi; Zhu, Changan] Univ Sci & Technol China, Sch Data Sci, Huainan 230022, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Jin, Y (corresponding author), Univ Sci & Technol China, Sch Engn Sci, Huainan 230022, Anhui, Peoples R China.; Jin, Y (corresponding author), Univ Sci & Technol China, Sch Data Sci, Huainan 230022, Anhui, Peoples R China.
EM anchen@mail.ustc.edu.cn; jinyi08@ustc.edu.cn; xukaikai@mail.ustc.edu.cn;
   cyx0089@mail.ustc.edu.cn; changan@ustc.edu.cn
RI Zhu, ChangAn/KIL-0881-2024; Chen, Yuxuan/IWL-8267-2023; 徐,
   凯/HKO-4837-2023; Jin, Yi/ABG-3022-2022
OI Chen, Yuxuan/0000-0002-3044-4169; Xu, Kai/0000-0003-0191-0375; Chen,
   Huaian/0000-0003-3999-2206
FU National Natural Science Foundation of China [61727809]; Special Fund
   for Key Program of Science, and Technology of Anhui Province
   [201903a05020022, 201903c08020002]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61727809, and in part by the Special
   Fund for Key Program of Science, and Technology of Anhui Province
   201903a05020022, and 201903c08020002. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Han Hu.
CR Arias P, 2018, J MATH IMAGING VIS, V60, P70, DOI 10.1007/s10851-017-0742-4
   Buades A, 2016, IEEE T IMAGE PROCESS, V25, P2573, DOI 10.1109/TIP.2016.2551639
   Cai PD, 2021, IEEE T INTELL VEHICL, V6, P419, DOI 10.1109/TIV.2020.3033878
   Chen C, 2020, IEEE T PATTERN ANAL, V42, P3071, DOI 10.1109/TPAMI.2019.2921548
   Chen C, 2019, IEEE I CONF COMP VIS, P3184, DOI 10.1109/ICCV.2019.00328
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen HA, 2021, IEEE T IND INFORM, V17, P5369, DOI 10.1109/TII.2020.3024187
   Chen HA, 2022, IEEE T NEUR NET LEAR, V33, P4991, DOI 10.1109/TNNLS.2021.3066850
   Chen X., 2016, APPL DIGIT IMAGE PRO, V39
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   Cho SI, 2019, IEEE T MULTIMEDIA, V21, P484, DOI 10.1109/TMM.2018.2859791
   Claus M, 2019, IEEE COMPUT SOC CONF, P1843, DOI 10.1109/CVPRW.2019.00235
   Dabov Kostadin, 2007, 2007 15th European Signal Processing Conference (EUSIPCO), P145
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Davy A, 2021, J MATH IMAGING VIS, V63, P73, DOI 10.1007/s10851-020-00995-0
   Davy A, 2019, IEEE IMAGE PROC, P2409, DOI [10.1109/icip.2019.8803314, 10.1109/ICIP.2019.8803314]
   Du Y, 2021, IEEE T MULTIMEDIA, V23, P2139, DOI 10.1109/TMM.2020.3008057
   Feng WS, 2018, IEEE T CYBERNETICS, V48, P1708, DOI 10.1109/TCYB.2017.2713421
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   Gupta A, 2017, IEEE I CONF COMP VIS, P4087, DOI 10.1109/ICCV.2017.438
   Hazirbas C, 2017, LECT NOTES COMPUT SC, V10111, P213, DOI 10.1007/978-3-319-54181-5_14
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang HZ, 2017, PROC CVPR IEEE, P7044, DOI 10.1109/CVPR.2017.745
   Ioffe S., 2015, P INT C MACH LEARN, VVolume 1, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Jain V., 2009, ADV NEURAL INFORM PR, P769, DOI DOI 10.5555/2981780.2981876
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kingma D. P., 2014, arXiv
   Kumar K, 2021, MULTIMED TOOLS APPL, V80, P11079, DOI 10.1007/s11042-020-10157-4
   Kumar K, 2018, ADV INTELL SYST, V709, P383, DOI 10.1007/978-981-10-8633-5_38
   Kumar K, 2019, IETE TECH REV, V36, P265, DOI 10.1080/02564602.2018.1454347
   Kumar K, 2017, 7TH INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATION TECHNOLOGY (ICCCT - 2017), P100, DOI 10.1145/3154979.3154998
   Kumar K, 2019, J VIS COMMUN IMAGE R, V58, P345, DOI 10.1016/j.jvcir.2018.12.009
   Kumar K, 2018, MULTIMED TOOLS APPL, V77, P26635, DOI 10.1007/s11042-018-5882-z
   Kumar K, 2018, IEEE T MULTIMEDIA, V20, P323, DOI 10.1109/TMM.2017.2741423
   Kumar K, 2017, 2017 INTERNATIONAL CONFERENCE ON INNOVATIONS IN ELECTRONICS, SIGNAL PROCESSING AND COMMUNICATION (IESC), P106, DOI 10.1109/IESPC.2017.8071874
   Li DJ, 2020, MULTIMED TOOLS APPL, V79, P34443, DOI 10.1007/s11042-020-09113-z
   Li S, 2019, PROC CVPR IEEE, P10514, DOI 10.1109/CVPR.2019.01077
   Liu JM, 2019, INT CONF ACOUST SPEE, P7715, DOI 10.1109/ICASSP.2019.8682856
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Maggioni M, 2013, IEEE T IMAGE PROCESS, V22, P119, DOI 10.1109/TIP.2012.2210725
   Maggioni M, 2012, IEEE T IMAGE PROCESS, V21, P3952, DOI 10.1109/TIP.2012.2199324
   Marinc T, 2019, IEEE IMAGE PROC, P2404, DOI [10.1109/ICIP.2019.8803335, 10.1109/icip.2019.8803335]
   Mildenhall B, 2018, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2018.00265
   Roth S, 2005, PROC CVPR IEEE, P860
   Sharma S, 2022, IETE J RES, V68, P3798, DOI 10.1080/03772063.2020.1780164
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Singh M, 2019, IET COMPUT VIS, V13, P578, DOI 10.1049/iet-cvi.2018.5814
   Soundararajan R, 2013, IEEE T CIRC SYST VID, V23, P684, DOI 10.1109/TCSVT.2012.2214933
   Sun YX, 2021, IEEE ROBOT AUTOM LET, V6, P510, DOI 10.1109/LRA.2020.3047783
   Sun YX, 2020, IEEE ROBOT AUTOM LET, V5, P3066, DOI 10.1109/LRA.2020.2975414
   Sun YX, 2019, IEEE ROBOT AUTOM LET, V4, P2576, DOI 10.1109/LRA.2019.2904733
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tassano M, 2020, PROC CVPR IEEE, P1351, DOI 10.1109/CVPR42600.2020.00143
   Tassano M, 2019, IEEE IMAGE PROC, P1805, DOI [10.1109/ICIP.2019.8803136, 10.1109/icip.2019.8803136]
   Wang HL, 2022, IEEE T CYBERNETICS, V52, P10750, DOI 10.1109/TCYB.2021.3064089
   Wang L, 2021, IEEE T MULTIMEDIA, V23, P1287, DOI 10.1109/TMM.2020.2995266
   Wang SK, 2021, IEEE ROBOT AUTOM LET, V6, P3397, DOI 10.1109/LRA.2021.3062016
   Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175
   Xu XY, 2020, IEEE T IMAGE PROCESS, V29, P7153, DOI 10.1109/TIP.2020.2999209
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P15, DOI 10.1109/TMM.2018.2849605
   Yang QS, 2018, IEEE T MED IMAGING, V37, P1348, DOI 10.1109/TMI.2018.2827462
   Yu S, 2020, IEEE COMPUT SOC CONF, P2099, DOI 10.1109/CVPRW50498.2020.00258
   Yue HJ, 2020, PROC CVPR IEEE, P2298, DOI 10.1109/CVPR42600.2020.00237
   Zamir SW, 2020, PROC CVPR IEEE, P2693, DOI 10.1109/CVPR42600.2020.00277
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhong JX, 2019, PROC CVPR IEEE, P1237, DOI 10.1109/CVPR.2019.00133
NR 68
TC 14
Z9 15
U1 1
U2 43
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2164
EP 2178
DI 10.1109/TMM.2021.3077140
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200030
DA 2024-07-18
ER

PT J
AU Guo, R
   Shao, XX
   Zhang, CC
   Qian, XH
AF Guo, Rui
   Shao, Xiangxin
   Zhang, Chencheng
   Qian, Xiaohua
TI Multi-Scale Sparse Graph Convolutional Network For the Assessment of
   Parkinsonian Gait
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Deep learning; Pose estimation; Bones; Legged
   locomotion; Joints; Correlation; Parkinson's disease; gait motor
   disorder; video-based assessment; graph convolutional network;
   model-driven deep learning
ID SPATIOTEMPORAL ATTENTION NETWORKS; ACTION RECOGNITION; DISEASE; FEATURES
AB Automated assessment of patients with Parkinson's disease (PD) is urgently required in clinical practice to improve the diagnostic efficiency and objectivity and to remotely monitor the motor disorder symptoms and general health of these patients, especially in view of the travel restrictions due to the recent coronavirus epidemic. Gait motor disorder is one of the critical manifestations of PD, and automated assessment of gait is vital to realize automated assessment of PD patients. To this end, we propose a novel two-stream spatial-temporal attention graph convolutional network (2s-ST-AGCN) for video assessment of PD gait motor disorder. Specifically, the skeleton sequence of human body is extracted from videos to construct spatial-temporal graphs of joints and bones, and a two-stream spatial-temporal graph convolutional network is then built to simultaneously model the static spatial information and dynamic temporal variations. The multi-scale spatial-temporal attention-aware mechanism is also designed to effectively extract the discriminative spatial-temporal features. The deep supervision strategy is then embedded to minimize classification errors, thereby guiding the weight update process of the hidden layer to promote significant discriminative features. Besides, two model-driven terms are integrated into this deep learning framework to strengthen multi-scale similarity in the deep supervision and realize sparsification of discriminative features. Extensive experiments on the clinical video dataset show that the proposed model exhibits good performance with an accuracy of 65.66% and an acceptable accuracy of 98.90%, which is much better than that of the existing sensor- and vision-based methods for Parkinsonian gait assessment. Thus, the proposed method is potentially useful for assessing PD gait motor disorder in clinical practice.
C1 [Guo, Rui; Qian, Xiaohua] Shanghai Jiao Tong Univ, Sch Biomed Engn, Shanghai 200030, Peoples R China.
   [Shao, Xiangxin] Changchun Univ Technol, Sch Elect & Elect Engn, Changchun 130012, Peoples R China.
   [Zhang, Chencheng] Shanghai Jiao Tong Univ, Ruijin Hosp, Dept Funct Neurosurg, Sch Med, Shanghai 200025, Peoples R China.
C3 Shanghai Jiao Tong University; Changchun University of Technology;
   Shanghai Jiao Tong University
RP Qian, XH (corresponding author), Shanghai Jiao Tong Univ, Sch Biomed Engn, Shanghai 200030, Peoples R China.
EM graymm@sjtu.edu.cn; shaoxiangxin@ccut.edu.cn; i@cczhang.org;
   xiaohua.qian@sjtu.edu.cn
OI Zhang, ChenCheng/0000-0003-4472-4134
CR Asuroglu T, 2018, BIOCYBERN BIOMED ENG, V38, P760, DOI 10.1016/j.bbe.2018.06.002
   Borzi L., 2020, IEEE OPEN J ENG MED
   Burki T, 2020, LANCET INFECT DIS, V20, P292, DOI 10.1016/S1473-3099(20)30076-1
   Chen YY, 2012, EXPERT SYST APPL, V39, P520, DOI 10.1016/j.eswa.2011.07.042
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Giuffrida J. P., 2009, MOV DISORD
   Goetz CG, 2003, MOVEMENT DISORD, V18, P738, DOI 10.1002/mds.10473
   Goetz CG, 2008, MOVEMENT DISORD, V23, P2129, DOI 10.1002/mds.22340
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jankovic J, 2008, J NEUROL NEUROSUR PS, V79, P368, DOI 10.1136/jnnp.2007.131045
   Guo BJ, 2020, MED PHYS, V47, P1775, DOI 10.1002/mp.14066
   Kipf TN, 2016, ARXIV
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   Lei Y, 2020, MED PHYS, V47, P530, DOI 10.1002/mp.13933
   Leo M, 2020, INFORMATION, V11, DOI 10.3390/info11030128
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Li MH, 2018, J NEUROENG REHABIL, V15, DOI 10.1186/s12984-018-0446-z
   Li MH, 2018, PARKINSONISM RELAT D, V53, P42, DOI 10.1016/j.parkreldis.2018.04.036
   Li MH, 2017, IEEE ENG MED BIO, P3377, DOI 10.1109/EMBC.2017.8037580
   Li SQ, 2019, IEEE T MULTIMEDIA, V21, P2361, DOI 10.1109/TMM.2019.2900134
   Liao X., 2020, P IEEECVF C COMPUTER, P9394
   Liu Y, 2019, IEEE T NEUR SYS REH, V27, P1952, DOI 10.1109/TNSRE.2019.2939596
   Morris ME, 1996, HUM MOVEMENT SCI, V15, P649, DOI 10.1016/0167-9457(96)00020-6
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Ni BB, 2016, PROC CVPR IEEE, P1020, DOI 10.1109/CVPR.2016.116
   Ni BB, 2014, PROC CVPR IEEE, P756, DOI 10.1109/CVPR.2014.102
   Parisi F, 2016, IEEE T AFFECT COMPUT, V7, P258, DOI 10.1109/TAFFC.2016.2549533
   Parisi F, 2015, IEEE J BIOMED HEALTH, V19, P1777, DOI 10.1109/JBHI.2015.2472640
   Post B, 2005, MOVEMENT DISORD, V20, P1577, DOI 10.1002/mds.20640
   Sabo A, 2020, J NEUROENG REHABIL, V17, DOI 10.1186/s12984-020-00728-9
   Schlemper J, 2019, MED IMAGE ANAL, V53, P197, DOI 10.1016/j.media.2019.01.012
   Schlemper Jo, 2018, arXiv
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Singh B, 2016, PROC CVPR IEEE, P1961, DOI 10.1109/CVPR.2016.216
   Singh JP, 2018, IEEE ACCESS, V6, P70497, DOI 10.1109/ACCESS.2018.2879896
   Turner TH, 2020, MOVEMENT DISORD, V35, P1488, DOI 10.1002/mds.28108
   Wang W, 2020, IEEE ACCESS, V8, P14579, DOI 10.1109/ACCESS.2019.2961410
   Wen YH, 2019, AAAI CONF ARTIF INTE, P8989
   Xu M, 2017, IEEE INT CON MULTI, P517, DOI 10.1109/ICME.2017.8019351
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Ye MX, 2020, IEEE T MULTIMEDIA, V22, P1113, DOI 10.1109/TMM.2019.2942479
   Yeung S, 2019, NPJ DIGIT MED, V2, DOI 10.1038/s41746-019-0087-z
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhou Y, 2015, PROC CVPR IEEE, P3323, DOI 10.1109/CVPR.2015.7298953
   Zhou Y, 2014, LECT NOTES COMPUT SC, V8692, P481, DOI 10.1007/978-3-319-10593-2_32
   Zhu KJ, 2020, IEEE T MULTIMEDIA, V22, P2977, DOI 10.1109/TMM.2019.2962304
   Zhu YS, 2020, VISUAL COMPUT, V36, P1771, DOI 10.1007/s00371-019-01770-y
NR 48
TC 24
Z9 26
U1 7
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1583
EP 1594
DI 10.1109/TMM.2021.3068609
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200026
DA 2024-07-18
ER

PT J
AU Liu, JD
   Li, ZH
   Jiang, XH
   Zhang, ZZ
AF Liu, Jindou
   Li, Zhaohong
   Jiang, Xinghao
   Zhang, Zhenzhen
TI A High-Performance CNN-Applied HEVC Steganography Based on Diamond-Coded
   PU Partition Modes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video steganography; HEVC; high capacity; PU partition modes; diamond
   coding; CNN
ID ALGORITHM
AB High efficiency video coding (HEVC) is the latest high-performance video coding standard, and HEVC video steganography has become a new way to hide data for covert communication. This paper proposes a novel multilevel steganography algorithm based on diamond-encoded prediction unit (PU) partition modes. The PU modes of smaller 8 x 8 and 16 x 16 CUs are selected as carriers for information hiding. The diamond-coding rules are adopted to enhance the expressive ability of limited PU types, allowing them to carry more information under limited modification. Based on the encoded PU partition modes, three different embedding levels with different capacities are proposed. This paper's most outstanding contribution is the introduction of convolutional neural networks (CNNs) for the first time to improve visual quality and reduce steganographic video bitrate increases. Experimental results show that the embedding capacity of the proposed algorithm is significantly higher than the state-of-the-art work at the same bitrate, whether in high- or low-resolution HEVC videos. At the same time, the visual quality of the steganographic videos is excellent, and the resistance to video steganalysis is strong.
C1 [Liu, Jindou; Li, Zhaohong] Beijing Jiaotong Univ, Sch Elect & Informat Engn, Beijing 100044, Peoples R China.
   [Jiang, Xinghao] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
   [Zhang, Zhenzhen] Beijing Inst Graph Commun, Beijing 102600, Peoples R China.
C3 Beijing Jiaotong University; Shanghai Jiao Tong University; Beijing
   Institute of Graphic Communication
RP Li, ZH (corresponding author), Beijing Jiaotong Univ, Sch Elect & Informat Engn, Beijing 100044, Peoples R China.
EM 18120008@bjtu.edu.cn; zhhli2@bjtu.edu.cn; xhjiang@sjtu.edu.cn;
   zhangzhenzhen@bigc.edu.cn
FU National Natural Science Foundation of China [61771270, 61572321,
   61702034]; National Key R&D Program of China [2018YFC0830700]; Zhejiang
   Provincial Natural Science Foundation of China [LR20F020001]
FX This work was supported in part by National Natural Science Foundation
   of China under Grants 61771270, 61572321, and 61702034, in part by the
   National Key R&D Program of China under Grant 2018YFC0830700, and in
   part by the Zhejiang Provincial Natural Science Foundation of China
   under Grant LR20F020001.
CR [Anonymous], 2017, MULTIMEDIA TOOLS APP
   Bjotegaard G., 2001, VCEGM33
   Borges PVK, 2008, IEEE T MULTIMEDIA, V10, P1479, DOI 10.1109/TMM.2008.2007294
   Chang P. C., 2013, 2013 FUTURE NETWORK, P1
   Chang PC, 2014, J VIS COMMUN IMAGE R, V25, P239, DOI 10.1016/j.jvcir.2013.10.007
   Chao RM, 2009, EURASIP J INF SECUR, DOI 10.1155/2009/658047
   Chen Y., 2017, J COMPUT APPL
   Dai YY, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING (IEEE VCIP)
   Dai YY, 2017, LECT NOTES COMPUT SC, V10132, P28, DOI 10.1007/978-3-319-51811-4_3
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong Y., 2018, INT WORKSHOP DIGIT W, P233
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jia CM, 2019, IEEE T IMAGE PROCESS, V28, P3343, DOI 10.1109/TIP.2019.2896489
   Jin ZP, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1368, DOI 10.1109/ICASSP.2018.8461356
   Kang J, 2017, IEEE IMAGE PROC, P26, DOI 10.1109/ICIP.2017.8296236
   Kar Dulal Chandra, 2018, International Journal of Information and Computer Security, V10, P276
   King DB, 2015, ACS SYM SER, V1214, P1
   Li ZH, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11081015
   Li ZH, 2019, CMC-COMPUT MATER CON, V59, P563, DOI 10.32604/cmc.2019.05565
   Lin WY, 2020, IEEE T MULTIMEDIA, V22, P2749, DOI 10.1109/TMM.2019.2962310
   Liu YX, 2019, MULTIMED TOOLS APPL, V78, P6459, DOI 10.1007/s11042-018-6320-y
   Shanableh T, 2019, IET IMAGE PROCESS, V13, P1909, DOI 10.1049/iet-ipr.2018.5782
   [盛琪 Sheng Qi], 2017, [光电子·激光, Journal of Optoelectronics·Laser], V28, P433
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tew Y, 2014, IEEE IMAGE PROC, P5502, DOI 10.1109/ICIP.2014.7026113
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Wang J, 2015, THESIS NINGBO U NING
   Wang J. J., 2014, GUANGDIANZI JIGUANG, V10, P8933
   [王家骥 Wang Jiaji], 2015, [光电子·激光, Journal of Optoelectronics·Laser], V26, P942
   [王家骥 Wang Jiaji], 2014, [光电子·激光, Journal of Optoelectronics·Laser], V25, P1578
   Wang J, 2019, IEEE ACCESS, V7, P119393, DOI 10.1109/ACCESS.2019.2936614
   Wang YB, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING (IEEE VCIP)
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu ST, 2020, IEEE T MULTIMEDIA, V22, P256, DOI 10.1109/TMM.2019.2920605
   Xie WC, 2018, LECT NOTES COMPUT SC, V11066, P252, DOI 10.1007/978-3-030-00015-8_22
   Xu J., 2015, COMPUT INF SYST, V11, P5587
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Yang WM, 2019, IEEE SIGNAL PROC LET, V26, P538, DOI 10.1109/LSP.2018.2890770
   Yang YY, 2019, MULTIMED TOOLS APPL, V78, P8423, DOI 10.1007/s11042-018-6859-7
   Zhang YB, 2018, IEEE T IMAGE PROCESS, V27, P3827, DOI 10.1109/TIP.2018.2815841
   Zhang YB, 2018, IEEE T IMAGE PROCESS, V27, P3150, DOI 10.1109/TIP.2018.2812081
NR 43
TC 15
Z9 16
U1 3
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2084
EP 2097
DI 10.1109/TMM.2021.3075858
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200024
DA 2024-07-18
ER

PT J
AU Sun, ZR
   Liu, HF
   Wang, Q
   Zhou, TF
   Wu, Q
   Tang, ZN
AF Sun, Zeren
   Liu, Huafeng
   Wang, Qiong
   Zhou, Tianfei
   Wu, Qi
   Tang, Zhenmin
TI Co-LDL: A Co-Training-Based Label Distribution Learning Method for
   Tackling Label Noise
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Noise measurement; Deep learning; Training data; Neural
   networks; Computer vision; Task analysis; Label noise; co-training;
   label distribution learning; self-supervised representation learning
AB Performances of deep neural networks are prone to be degraded by label noise due to their powerful capability in fitting training data. Deeming low-loss instances as clean data is one of the most promising strategies in tackling label noise and has been widely adopted by state-of-the-art methods. However, prior works tend to drop high-loss instances directly, neglecting their valuable information. To address this issue, we propose an end-to-end framework named Co-LDL, which incorporates the low-loss sample selection strategy with label distribution learning. Specifically, we simultaneously train two deep neural networks and let them communicate useful knowledge by selecting low-loss and high-loss samples for each other. Low-loss samples are leveraged conventionally for updating network parameters. On the contrary, high-loss samples are trained in a label distribution learning manner to update network parameters and label distributions concurrently. Moreover, we propose a self-supervised module to further boost the model performance by enhancing the learned representations. Comprehensive experiments on both synthetic and real-world noisy datasets are provided to demonstrate the superiority of our Co-LDL method over state-of-the-art approaches in learning with noisy labels. The source code and models have been made available at https://github.com/NUST-Machine-Intelligence-Laboratory/CoLDL.
C1 [Sun, Zeren; Liu, Huafeng; Wang, Qiong; Tang, Zhenmin] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Zhou, Tianfei] Swiss Fed Inst Technol, Comp Vis Lab, CH-8092 Zurich, Switzerland.
   [Wu, Qi] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia.
C3 Nanjing University of Science & Technology; Swiss Federal Institutes of
   Technology Domain; ETH Zurich; University of Adelaide
RP Wang, Q (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
EM zerens@njust.edu.cn; 312060741@njust.edu.cn; wangq@njust.edu.cn;
   ztfei.debug@gmail.com; qi.wu01@adelaide.edu.au; tzm.cs@njust.edu.cn
RI Zhou, Tianfei/AAC-6115-2022; Wu, Qi/ABD-6304-2021
OI Zhou, Tianfei/0000-0001-5475-1473; Wu, Qi/0000-0003-3631-256X; Sun,
   Zeren/0000-0001-6262-5338; Wang, Qiong/0000-0003-4193-0960
FU National Natural Science Foundation of China [62102182, 61976116];
   Natural Science Foundation of Jiangsu Province [BK20210327]; Fundamental
   Research Funds for the Central Universities [30920021135]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62102182 and 61976116, Natural Science Foundation of
   Jiangsu Province under Grant BK20210327, and the Fundamental Research
   Funds for the Central Universities under Grant 30920021135. The
   Associate Editor coordinating the review of this manuscript and
   approving it for publication was Dr. Jiebo Luo.
CR Anguelov D., 2015, P INT C LEARN REPR W
   [Anonymous], 2013, Tech. rep.
   Arpit D, 2017, PR MACH LEARN RES, V70
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Chen T, 2020, PR MACH LEARN RES, V119
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Gao BB, 2017, IEEE T IMAGE PROCESS, V26, P2825, DOI 10.1109/TIP.2017.2689998
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Goldberger J., 2017, PROC INT C LEARN REP, P1
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Grill Jean-Bastien, 2020, P 34 INT C NEUR INF
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Hendrycks D, 2019, ADV NEUR IN, V32
   Hendrycks D, 2018, ADV NEUR IN, V31
   Horiguchi S, 2018, IEEE T MULTIMEDIA, V20, P2836, DOI 10.1109/TMM.2018.2814339
   Huang L., 2020, P ADV NEUR INF PROC, P1
   Jiang Lu, 2018, PR MACH LEARN RES
   Krizhevsky A, 2009, MSCTR2009 U TOR
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee KH, 2018, PROC CVPR IEEE, P5447, DOI 10.1109/CVPR.2018.00571
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Li Junnan, 2020, ARXIV200207394
   Li YC, 2017, IEEE I CONF COMP VIS, P1928, DOI 10.1109/ICCV.2017.211
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma XJ, 2018, PR MACH LEARN RES, V80
   Malach E, 2017, ADV NEUR IN, V30
   Mandal D, 2020, IEEE IMAGE PROC, P2326, DOI [10.1109/icip40778.2020.9190981, 10.1109/ICIP40778.2020.9190981]
   Mandal D, 2020, IEEE WINT CONF APPL, P1370, DOI 10.1109/WACV45572.2020.9093342
   Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240
   Pawan Kumar M., 2010, NIPS
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278
   Shen FM, 2017, IEEE T MULTIMEDIA, V19, P2022, DOI 10.1109/TMM.2017.2699863
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sukhbaatar S., 2015, Training convolutional networks with noisy labels, P1
   Sun Z, 2020, PATTERN RECOGN, V110
   Sun Z., 2020, ACM MM, P92
   Sun Z., 2021, PROC INT C COMPUT VI, P1
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582
   Tsai T. W., 2019, ARXIV190513305
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
   Vandat A, 2017, ADV NEUR IN, V30
   Veit A, 2017, PROC CVPR IEEE, P6575, DOI 10.1109/CVPR.2017.696
   Vincent P., 2008, INT C MACH LEARN ICM, P1096, DOI [10.1145/1390156.1390294, DOI 10.1145/1390156.1390294]
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wei Hongxin, 2020, P IEEE C COMP VIS PA
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Xiaojiang Peng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P786, DOI 10.1007/978-3-030-58517-4_46
   Yan Y, 2014, MACH LEARN, V95, P291, DOI 10.1007/s10994-013-5412-1
   Yao YZ, 2021, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR46437.2021.00515
   Yao YZ, 2020, IEEE T NEUR NET LEAR, V31, P2348, DOI 10.1109/TNNLS.2020.2966644
   Yao YZ, 2019, IEEE T MULTIMEDIA, V21, P184, DOI 10.1109/TMM.2018.2847248
   Yao YZ, 2017, IEEE T MULTIMEDIA, V19, P1771, DOI 10.1109/TMM.2017.2684626
   Yao Yazhou, 2021, CVPR
   Yi K, 2019, PROC CVPR IEEE, P7010, DOI 10.1109/CVPR.2019.00718
   Yu Xingrui, 2019, PROC INT C MACH LEAR, P7164
   Yu XY, 2018, LECT NOTES COMPUT SC, V11205, P69, DOI 10.1007/978-3-030-01246-5_5
   Zhang C., 2020, ACM MM, P2372
   Zhang C., 2021, PROC ACM INT C MULTI, P1
   Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414
   Zhang CY, 2020, AAAI CONF ARTIF INTE, V34, P12781
   Zhang Hongyi, 2017, ARXIV171009412
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhang ZL, 2018, ADV NEUR IN, V31
   Zhu WW, 2020, IEEE T MULTIMEDIA, V22, P1823, DOI 10.1109/TMM.2020.2969791
NR 70
TC 14
Z9 14
U1 3
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1093
EP 1104
DI 10.1109/TMM.2021.3116430
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ZN1SD
UT WOS:000764821800007
DA 2024-07-18
ER

PT J
AU Wang, L
   Zareapoor, M
   Yang, J
   Zheng, ZL
AF Wang, Lu
   Zareapoor, Masoumeh
   Yang, Jie
   Zheng, Zhonglong
TI Asymmetric Correlation Quantization Hashing for Cross-Modal Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Quantization (signal); Correlation; Binary codes; Databases;
   Optimization; Hash functions; Cross-modal similarity retrieval;
   Asymmetric cross-modal hashing; Real value embedding; Compositional
   quantization
ID CODES
AB In recent years, cross-modal hashing (CMH) has attracted considerable attention due to its ability to learn across different modalities and its high efficiency for similarity retrieval applications. This procedure is computationally inexpensive when dealing with large-scale multi-modalities datasets. However, they do not form the ideal representative model to fully exploit multi-modal data's underlying properties despite their successful performance. We identify that: (i) most CMH models in their current forms transform the real data points into discrete compact binary codes, which can limit their ability to prevent the loss of important information and thereby produce suboptimal results. (ii) the discrete-binary constraint model is hard to implement, and relaxing the binary constraints is a common property in most existing methods, which often leads to significant quantization errors. (iii) handling the CMH in a symmetry domain leads to a complex and inefficient optimization problem. This paper addresses the above challenges and proposes a novel Asymmetric Correlation Quantization Hashing (ACQH) method. ACQH learns a projection matrix for each heterogeneous modality to map the data point into a low-dimensional semantic space and constructs a compositional quantization to generate hash codes, using the pairwise semantic similarity preservation and the pointwise label regression. As a specific instantiation of our model, we use discrete iterative optimization to obtain the unified hash codes across different modalities. Extensive experiments show that ACQH outperforms state-of-the-art methods on several diverse datasets.
C1 [Wang, Lu; Zareapoor, Masoumeh; Yang, Jie] Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Shanghai 201100, Peoples R China.
   [Zheng, Zhonglong] Zhejiang Normal Univ, Jinhua 321000, Zhejiang, Peoples R China.
C3 Shanghai Jiao Tong University; Zhejiang Normal University
RP Yang, J (corresponding author), Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Shanghai 201100, Peoples R China.; Zheng, ZL (corresponding author), Zhejiang Normal Univ, Jinhua 321000, Zhejiang, Peoples R China.
EM luwang_16@sjtu.edu.cn; mzarea222@gmail.com; jieyang@sjtu.edu.cn;
   zhonglong@zjnu.cn
RI Zareapoor, Dr. Masoumeh/AAE-6067-2019; Yang, Jie/JCD-9867-2023
OI Zareapoor, Dr. Masoumeh/0000-0002-3991-0584; Wang,
   Lu/0000-0002-9376-0599; Zareapoor, Masoumeh/0000-0002-7569-9018; Zheng,
   Zhonglong/0000-0002-5271-9215
FU National Key R and D Program of China [2019YFB1311503]; NSFC, China
   [61876107, U1803261]
FX This work was supported in part by the National Key R and D Program of
   China under Grant 2019YFB1311503 and in part by NSFC, China under Grants
   61876107 and U1803261.
CR [Anonymous], 2008, P IEEE 11 INT C INF
   [Anonymous], 2013, NeurIPS
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen ZD, 2018, AAAI CONF ARTIF INTE, P274
   Da C, 2017, PROC CVPR IEEE, P898, DOI 10.1109/CVPR.2017.102
   Ding GG, 2014, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2014.267
   Ding K, 2017, IEEE T MULTIMEDIA, V19, P571, DOI 10.1109/TMM.2016.2625747
   Gordo A, 2014, IEEE T PATTERN ANAL, V36, P33, DOI 10.1109/TPAMI.2013.101
   Gui J, 2018, IEEE T PATTERN ANAL, V40, P490, DOI 10.1109/TPAMI.2017.2678475
   Guillaumin M, 2010, PROC CVPR IEEE, P902, DOI 10.1109/CVPR.2010.5540120
   He R, 2015, IEEE T IMAGE PROCESS, V24, P5543, DOI 10.1109/TIP.2015.2466106
   Hu D, 2019, IEEE T MULTIMEDIA, V21, P973, DOI 10.1109/TMM.2018.2866771
   Hu MQ, 2019, IEEE T IMAGE PROCESS, V28, P2770, DOI 10.1109/TIP.2018.2890144
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   KaiyeWang Qiyue Yin, 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06215
   Kang WC, 2016, AAAI CONF ARTIF INTE, P1230
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Lin GS, 2013, IEEE I CONF COMP VIS, P2552, DOI 10.1109/ICCV.2013.317
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liu H, 2017, PROC CVPR IEEE, P6345, DOI 10.1109/CVPR.2017.672
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Liu XB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1662, DOI 10.1145/3240508.3240683
   Long MS, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P579, DOI 10.1145/2911451.2911493
   Luo X, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P204, DOI 10.1145/3206025.3206034
   Ma DK, 2016, IEEE INT SYM MULTIM, P38, DOI [10.1109/ISM.2016.0017, 10.1109/ISM.2016.73]
   Ma XH, 2020, IEEE T MULTIMEDIA, V22, P3101, DOI 10.1109/TMM.2020.2969792
   Mandal D, 2017, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR.2017.282
   Martinez J, 2014, Arxiv, DOI arXiv:1411.2173
   Masci J, 2014, IEEE T PATTERN ANAL, V36, P824, DOI 10.1109/TPAMI.2013.225
   Ngo DCL, 2006, IEEE T CIRC SYST VID, V16, P771, DOI 10.1109/TCSVT.2006.873780
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen HT, 2021, IEEE T KNOWL DATA EN, V33, P3351, DOI [10.1109/TKDE.2020.2970050, 10.1109/TNNLS.2020.2995708]
   Wan  J., 2018, P 14 IEEE INT C SOL, P1, DOI [10.1109/ICSICT.2018.8565035, DOI 10.1109/ICSICT.2018.8565035]
   Wang CX, 2020, DATA MIN KNOWL DISC, V34, P1744, DOI 10.1007/s10618-020-00699-4
   Wang DX, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2291
   Wang D, 2018, IEEE T CIRC SYST VID, V28, P2703, DOI 10.1109/TCSVT.2017.2723302
   Wang J, 2013, IEEE I CONF COMP VIS, P3032, DOI 10.1109/ICCV.2013.377
   Wu BT, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3946
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Xu PF, 2013, IEEE MULTIMEDIA, V20, P34, DOI 10.1109/MMUL.2013.18
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang T, 2016, PROC CVPR IEEE, P2036, DOI 10.1109/CVPR.2016.224
   Zhou JL, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P415
NR 45
TC 14
Z9 14
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3665
EP 3678
DI 10.1109/TMM.2021.3105824
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xie, PY
   Xu, X
   Wang, Z
   Yamasaki, T
AF Xie, Pengyu
   Xu, Xin
   Wang, Zheng
   Yamasaki, Toshihiko
TI Sampling and Re-Weighting: Towards Diverse Frame Aware Unsupervised
   Video Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise measurement; Target tracking; Cameras; Training; Task analysis;
   Mars; Data models; Clustering; re-identification; unsupervised
AB Video person re-identification (re-ID) methods extract richer features from video tracklets than image-based ones and have received growing attention. However, existing supervised methods require numerous cross-camera identity labels, which is impractical for large-scale data. Although clustering-based unsupervised methods have been exploited to obtain pseudo labels and train the models iteratively for video person re-ID, they remain in their infancy due to the diversity of person images and uncertainty in the image quality of video tracklets. In this work, we employ two strategies of Sampling and Re-weighting for Clustering (SRC) to obtain robust and discriminative person feature representations. This method considers the influence of two kinds of frames in the tracklet: 1) Detection errors or heavy occlusions generate noisy frames in the tracklet. These tracklets with noisy frames may be assigned with unreliable annotations during clustering. 2) Different frames are identified by the model with varying degrees of difficulty, caused by pose changes or partial occlusions. We call them hard frames, which are hard to identify but informative. To alleviate these problems, we propose a dynamic noise trimming module and diverse frame re-weighting module for sampling and re-weighting. The dynamic noise trimming module strengthens the dependability of the tracklet representation by removing noisy frames to enhance the clustering accuracy. The diverse frame re-weighting module focuses on training hard frames to enhance the learning of rich information from tracklet. Experiments on three video datasets, i.e. DukeMTMC-VideoReID, MARS and PRID2011, demonstrate the effectiveness of the proposed SRC under the unsupervised re-ID setting.
C1 [Xie, Pengyu; Xu, Xin] Wuhan Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430065, Peoples R China.
   [Wang, Zheng] Wuhan Univ, Sch Comp Sci, Wuhan 430065, Peoples R China.
   [Yamasaki, Toshihiko] Univ Tokyo, Dept Informat & Commun Engn, Bunkyo Ku, Tokyo 1138654, Japan.
   [Yamasaki, Toshihiko] Univ Tokyo, Res Inst Inclus Soc Engn RIISE, Bunkyo Ku, Tokyo 1138654, Japan.
C3 Wuhan University of Science & Technology; Wuhan University; University
   of Tokyo; University of Tokyo
RP Xu, X (corresponding author), Wuhan Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430065, Peoples R China.
EM xpydgqb@gmail.com; xuxin@wust.edu.cn; wangzwhu@whu.edu.cn;
   yamasaki@cvm.t.u-tokyo.ac.jp
RI Xu, Xin/JRW-5800-2023
OI Xu, Xin/0000-0003-0748-3669
FU National Natural Science Foundation of China [62171325]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62171325.
CR [Anonymous], 2018, P BRIT MACH VIS C
   Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Dehghan A, 2015, PROC CVPR IEEE, P4091, DOI 10.1109/CVPR.2015.7299036
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Ding G., 2019, ARXIV190601308
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9
   Hou RB, 2019, PROC CVPR IEEE, P7176, DOI 10.1109/CVPR.2019.00735
   Kodirov E., 2015, BMVC, DOI DOI 10.5244/C.29.44
   Li MX, 2020, IEEE T PATTERN ANAL, V42, P1770, DOI 10.1109/TPAMI.2019.2903058
   Li MX, 2018, LECT NOTES COMPUT SC, V11208, P772, DOI 10.1007/978-3-030-01225-0_45
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin YT, 2020, IEEE T IMAGE PROCESS, V29, P5481, DOI 10.1109/TIP.2020.2982826
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Lisanti G, 2015, IEEE T PATTERN ANAL, V37, P1629, DOI 10.1109/TPAMI.2014.2369055
   Liu Y, 2017, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2017.499
   Liu ZM, 2017, IEEE I CONF COMP VIS, P2448, DOI 10.1109/ICCV.2017.266
   Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sheng H, 2020, IEEE INTERNET THINGS, V7, P9611, DOI 10.1109/JIOT.2020.2980549
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wan ZJ, 2022, INT J INTELL SYST, V37, P2988, DOI 10.1002/int.22829
   Wang J, 2018, PATTERN RECOGN, V74, P38, DOI 10.1016/j.patcog.2017.09.014
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wang Z, 2020, Arxiv, DOI arXiv:2011.11506
   Wang Z, 2019, IEEE T MULTIMEDIA, V21, P2376, DOI 10.1109/TMM.2019.2898753
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu GL, 2020, AAAI CONF ARTIF INTE, V34, P12362
   Wu JL, 2019, IEEE I CONF COMP VIS, P8320, DOI 10.1109/ICCV.2019.00841
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Wu Y, 2018, PROC CVPR IEEE, P5177, DOI 10.1109/CVPR.2018.00543
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Xie Pengyu, 2021, PROC IEEE INT C MULT, P1, DOI 10.1109/ICME51207.2021.9428200
   Xie QK, 2021, IEEE T MULTIMEDIA, V23, P597, DOI 10.1109/TMM.2020.2985525
   Xu X., 2022, ACMTRANS MULTIMEDIA
   Xu X, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414839
   Xu X, 2021, PATTERN RECOGN, V113, DOI 10.1016/j.patcog.2021.107827
   Yan C, 2022, IEEE T MULTIMEDIA, V24, P1665, DOI 10.1109/TMM.2021.3069562
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522
   Yang QZ, 2019, PROC CVPR IEEE, P3628, DOI 10.1109/CVPR.2019.00375
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2019, IEEE T IMAGE PROCESS, V28, P2976, DOI 10.1109/TIP.2019.2893066
   Ye M, 2018, LECT NOTES COMPUT SC, V11211, P176, DOI 10.1007/978-3-030-01234-2_11
   Ye M, 2017, IEEE I CONF COMP VIS, P5152, DOI 10.1109/ICCV.2017.550
   Zeng ZL, 2023, IEEE T MULTIMEDIA, V25, P2176, DOI 10.1109/TMM.2022.3144066
   Zeng ZL, 2020, IEEE T MULTIMEDIA, V22, P3064, DOI 10.1109/TMM.2020.2969782
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zhong X, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5316, DOI 10.1145/3474085.3475654
   Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11
   Zhong Z, 2021, IEEE T PATTERN ANAL, V43, P2723, DOI 10.1109/TPAMI.2020.2976933
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
NR 62
TC 10
Z9 10
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4250
EP 4261
DI 10.1109/TMM.2022.3186177
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 5C3SZ
UT WOS:000864185400001
DA 2024-07-18
ER

PT J
AU Yu, ZX
   Zhao, YL
   Hong, B
   Jin, ZM
   Huang, JQ
   Cai, D
   Hua, XS
AF Yu, Zhengxu
   Zhao, Yilun
   Hong, Bin
   Jin, Zhongming
   Huang, Jianqiang
   Cai, Deng
   Hua, Xian-Sheng
TI Apparel-Invariant Feature Learning for Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Person re-identification; image synthesis; GAN; transfer learning
AB With the rise of deep learning methods, person Re-Identification (ReID) performance has been improved tremendously in many public datasets. However, most public ReID datasets are collected in a short time window in which persons' appearance rarely changes. In real-world applications such as in a shopping mall, the same person may change their wearings, and different persons may wear similar apparel. It reveals a critical problem that current ReID models heavily rely on a person's apparel, resulting in an inconsistent ReID performance. Therefore, it is crucial to learn an apparel-invariant person representation under clothes changing or several persons wearing similar clothes cases. In this work, we tackle this problem from the viewpoint of invariant feature representation learning. The main contributions of this work are as follows. (1) We propose the semi-supervised Apparel-invariant Feature Learning (AIFL) framework to learn an apparel-invariant pedestrian representation using images of the same person wearing different clothes. (2) To obtain images of the same person wearing different clothes, we propose an unsupervised apparel-simulation GAN (AS-GAN) to synthesize cloth-changing images according to the target cloth embedding. It is worth noting that the images used in ReID tasks were cropped from real-world low-quality CCTV videos, making it more challenging to synthesize cloth-changing images. Extensive experiments demonstrate that our proposal can improve the ReID performance of the baseline models.
C1 [Yu, Zhengxu; Cai, Deng] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Peoples R China.
   [Yu, Zhengxu; Hong, Bin; Jin, Zhongming; Huang, Jianqiang; Hua, Xian-Sheng] DAMO Acad, Alibaba Grp, Hangzhou, Peoples R China.
   [Zhao, Yilun] Zhejiang Univ, Hangzhou, Peoples R China.
C3 Zhejiang University; Alibaba Group; Zhejiang University
RP Cai, D (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Peoples R China.
EM yuzxfred@gmail.com; zhaoyilun@zju.edu.cn; bin_hong@zju.edu.cn;
   zhongming.jinzm@alibaba-inc.com; jianqiang.hjq@alibaba-inc.com;
   dengcai@gmail.com; xiansheng.hxs@alibaba-inc.com
RI Zhao, Yilun/IAR-5845-2023
OI Hua, Xian-Sheng/0000-0002-8232-5049; Yu, Zhengxu/0000-0002-6059-0300
FU National Key Research and Development Program of China [2018AAA0101400];
   National Natural Science Foundation of China [62036009, U1909203,
   61936006]; Innovation Capability Support Program of Shaanxi [2021TD-05]
FX This work was supported in part by The National Key Research and
   Development Program of China under Grant 2018AAA0101400, in part by The
   National Natural Science Foundation of China under Grants 62036009,
   U1909203, and 61936006, in part by Innovation Capability Support Program
   of Shaanxi under Grant 2021TD-05. The Associate Editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Jianguo Zhang. (Zhengxu Yu and Yilun Zhao contributed equally to this
   work.) (Corresponding author: Deng Cai.)
CR Barbosa IB, 2012, LECT NOTES COMPUT SC, V7583, P433, DOI 10.1007/978-3-642-33863-2_43
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   BOURLARD H, 1988, BIOL CYBERN, V59, P291, DOI 10.1007/BF00332918
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen DP, 2016, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2016.142
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Ding GD, 2019, IEEE T MULTIMEDIA, V21, P2891, DOI 10.1109/TMM.2019.2916456
   Donahue J., 2016, arXiv
   Fan HH, 2017, Arxiv, DOI arXiv:1705.10444
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Li YJ, 2021, IEEE WINT CONF APPL, P2431, DOI 10.1109/WACV48630.2021.00248
   Liang XD, 2015, IEEE T PATTERN ANAL, V37, P2402, DOI 10.1109/TPAMI.2015.2408360
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Loy CC, 2009, PROC CVPR IEEE, P1988, DOI 10.1109/CVPRW.2009.5206827
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Lv JM, 2018, Arxiv, DOI arXiv:1803.07293
   Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018
   Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Peng PX, 2016, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2016.146
   Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Wan FB, 2020, IEEE COMPUT SOC CONF, P3620, DOI 10.1109/CVPRW50498.2020.00423
   Wang TQ, 2016, IEEE T PATTERN ANAL, V38, P2501, DOI 10.1109/TPAMI.2016.2522418
   Wang XG, 2013, PATTERN RECOGN LETT, V34, P3, DOI 10.1016/j.patrec.2012.07.005
   Wang Z, 2019, IEEE T MULTIMEDIA, V21, P2376, DOI 10.1109/TMM.2019.2898753
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Wu AC, 2017, IEEE T IMAGE PROCESS, V26, P2588, DOI 10.1109/TIP.2017.2675201
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Xie QK, 2021, IEEE T MULTIMEDIA, V23, P597, DOI 10.1109/TMM.2020.2985525
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P2347, DOI 10.1109/TMM.2020.3009476
   Yang Q., 2019, IEEE transactions on pattern analysis and machine intelligence, V43, P2029
   Ye M, 2017, IEEE I CONF COMP VIS, P5152, DOI 10.1109/ICCV.2017.550
   Yu HX, 2017, IEEE I CONF COMP VIS, P994, DOI 10.1109/ICCV.2017.113
   Zeng ZL, 2020, IEEE T MULTIMEDIA, V22, P3064, DOI 10.1109/TMM.2020.2969782
   Zhang X, 2018, Arxiv, DOI arXiv:1711.08184
   Zhao CR, 2020, IEEE T MULTIMEDIA, V22, P3180, DOI 10.1109/TMM.2020.2972125
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zhao LM, 2017, Arxiv, DOI arXiv:1707.07256
   Zhao R, 2017, IEEE T PATTERN ANAL, V39, P356, DOI 10.1109/TPAMI.2016.2544310
   Zhao R, 2013, IEEE I CONF COMP VIS, P2528, DOI 10.1109/ICCV.2013.314
   Zheng L, 2017, Arxiv, DOI arXiv:1701.07732
   Zheng L, 2016, Arxiv, DOI arXiv:1610.02984
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zhu SZ, 2017, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2017.186
NR 56
TC 11
Z9 13
U1 3
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4482
EP 4492
DI 10.1109/TMM.2021.3119133
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 7B6NT
UT WOS:000899248400003
DA 2024-07-18
ER

PT J
AU Zheng, AH
   Hu, ML
   Jiang, B
   Huang, Y
   Yan, Y
   Luo, B
AF Zheng, Aihua
   Hu, Menglan
   Jiang, Bo
   Huang, Yan
   Yan, Yan
   Luo, Bin
TI Adversarial-Metric Learning for Audio-Visual Cross-Modal Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Task analysis; Measurement; Speech recognition; Videos;
   Location awareness; Image recognition; Adversarial learning;
   audio-visual matching; cross-modal learning; metric learning
ID FACE; IDENTITY; VOICE
AB Audio-visual matching aims to learn the intrinsic correspondence between image and audio clip. Existing works mainly concentrate on learning discriminative features, while ignore the cross-modal heterogeneous issue between audio and visual modalities. To deal with this issue, we propose a novel Adversarial-Metric Learning (AML) model for audio-visual matching. AML aims to generate a modality-independent representation for each person in each modality via adversarial learning, while simultaneously learns a robust similarity measure for cross-modality matching via metric learning. By integrating the discriminative modality-independent representation and robust cross-modality metric learning into an end-to-end trainable deep network, AML can overcome the heterogeneous issue with promising performance for audio-visual matching. Experiments on the various audio-visual learning tasks, including audio-visual matching, audio-visual verification and audio-visual retrieval on benchmark dataset demonstrate the effectiveness of the proposed AML model. The implementation codes are available on https://github.com/MLanHu/AML.
C1 [Zheng, Aihua; Hu, Menglan; Jiang, Bo; Luo, Bin] Minist Educ, Key Lab Intelligent Comp & Signal Proc, Anhui Prov Key Lab Multimodal Cognit Computat, Hefei, Peoples R China.
   [Zheng, Aihua; Hu, Menglan; Jiang, Bo; Luo, Bin] Anhui Univ, Sch Comp Sci & Technol, Hefei 230601, Peoples R China.
   [Jiang, Bo] Anhui Univ, Inst Phys Sci & Informat Technol, Hefei 230601, Peoples R China.
   [Huang, Yan] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Yan, Yan] IIT, Dept Comp Sci, Chicago, IL 60616 USA.
C3 Anhui University; Anhui University; Chinese Academy of Sciences;
   Institute of Automation, CAS; Illinois Institute of Technology
RP Jiang, B (corresponding author), Minist Educ, Key Lab Intelligent Comp & Signal Proc, Anhui Prov Key Lab Multimodal Cognit Computat, Hefei, Peoples R China.; Jiang, B (corresponding author), Anhui Univ, Sch Comp Sci & Technol, Hefei 230601, Peoples R China.; Jiang, B (corresponding author), Anhui Univ, Inst Phys Sci & Informat Technol, Hefei 230601, Peoples R China.
EM ahzheng214@foxmail.com; 2364244962@qq.com; jiangbo@ahu.edu.cn;
   yan.huang@nlpria.ac.cn; tom_yan@txstate.edu; luobin@ahu.edu.cn
RI Huang, Yan/HCH-6526-2022; Zheng, Aihua/ABA-5196-2020; lu,
   bin/HPE-4790-2023; LUO, BIN/Y-1233-2018
OI Huang, Yan/0000-0002-8239-7229; LUO, BIN/0000-0001-5948-5055
FU Major Project for New Generation of AI [2018AAA0100400]; National
   Natural Science Foundation of China [61976002, 62076004]; Natural
   Science Foundation of Anhui Higher Education Institutions of China
   [KJ2019A0033]; Open Project Program of the National Laboratory of
   Pattern Recognition (NLPR) [201900046]; Cooperative Research Project
   Program of Nanjing Artificial Intelligence Chip Research, Institute of
   Automation, Chinese Academy of Sciences
FX This work was supported in part by the Major Project for New Generation
   of AI under Grant 2018AAA0100400, in part by the National Natural
   Science Foundation of China under Grants 61976002 and 62076004, in part
   by the Natural Science Foundation of Anhui Higher Education Institutions
   of China under Grant KJ2019A0033, and in part by the Open Project
   Program of the National Laboratory of Pattern Recognition (NLPR)
   (201900046), and the Cooperative Research Project Program of Nanjing
   Artificial Intelligence Chip Research, Institute of Automation, Chinese
   Academy of Sciences. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Concetto
   Spampinato. (Corresponding author: Bo Jiang.)
CR Afouras T, 2018, INTERSPEECH, P3244
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Chen LL, 2018, LECT NOTES COMPUT SC, V11211, P538, DOI 10.1007/978-3-030-01234-2_32
   Chen LL, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P349, DOI 10.1145/3126686.3126723
   Chen ZD, 2018, AAAI CONF ARTIF INTE, P274
   Chi JZ, 2020, IEEE T CIRC SYST VID, V30, P1173, DOI 10.1109/TCSVT.2019.2900171
   Cui Y, 2016, PROC CVPR IEEE, P1153, DOI 10.1109/CVPR.2016.130
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Davis A, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601119
   Duarte Amanda, 2019, ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Proceedings, P8633, DOI 10.1109/ICASSP.2019.8682970
   Ephrat A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201357
   Fisher JW, 2001, ADV NEUR IN, V13, P772
   Gabbay A, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P3051, DOI 10.1109/ICASSP.2018.8462527
   Gao RH, 2018, LECT NOTES COMPUT SC, V11207, P36, DOI 10.1007/978-3-030-01219-9_3
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Hao WL, 2018, AAAI CONF ARTIF INTE, P6886
   HAO Y, 2019, AAAI, P8385
   Hershey J, 2000, ADV NEUR IN, V12, P813
   Hoover K., 2017, ARXIV170600079
   Jalalifar S. A., 2018, ARXIV180307461
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   JIANG B, IEEE T MULTIMEDIA
   Kamachi M, 2003, CURR BIOL, V13, P1709, DOI 10.1016/j.cub.2003.09.005
   Kim Changil, 2018, P AS C COMP VIS ACCV, P276
   Kingma D. P., 2015, PROC INT C LEARN REP, P1
   Le Cornu T, 2017, IEEE-ACM T AUDIO SPE, V25, P1447, DOI 10.1109/TASLP.2017.2716178
   Le Cornu T, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P3355
   Li C, 2019, AAAI CONF ARTIF INTE, P176
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1234, DOI 10.1109/TMM.2016.2646180
   Liu ZM, 2017, IEEE I CONF COMP VIS, P2448, DOI 10.1109/ICCV.2017.266
   Lu JW, 2017, IEEE T IMAGE PROCESS, V26, P4269, DOI 10.1109/TIP.2017.2717505
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Nagrani A, 2018, LECT NOTES COMPUT SC, V11217, P73, DOI 10.1007/978-3-030-01261-8_5
   Nagrani A, 2018, PROC CVPR IEEE, P8427, DOI 10.1109/CVPR.2018.00879
   Nagrani A, 2017, INTERSPEECH, P2616, DOI 10.21437/Interspeech.2017-950
   Nawaz S, 2019, 2019 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P83, DOI 10.1109/dicta47822.2019.8945863
   Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Qiu Y., 2018, P IEEE C COMP VIS PA, P2510
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Senocak A, 2018, PROC CVPR IEEE, P4358, DOI 10.1109/CVPR.2018.00458
   Shlizerman E, 2018, PROC CVPR IEEE, P7574, DOI 10.1109/CVPR.2018.00790
   Smith HMJ, 2016, ATTEN PERCEPT PSYCHO, V78, P868, DOI 10.3758/s13414-015-1045-8
   Sohn K, 2016, ADV NEUR IN, V29
   Song G, 2019, IEEE T MULTIMEDIA, V21, P1261, DOI 10.1109/TMM.2018.2877122
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Su SP, 2019, IEEE I CONF COMP VIS, P3027, DOI 10.1109/ICCV.2019.00312
   Tang TR, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1598, DOI 10.1145/3240508.3240526
   Tian YP, 2018, LECT NOTES COMPUT SC, V11206, P252, DOI 10.1007/978-3-030-01216-8_16
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wan CH, 2019, INT CONF ACOUST SPEE, P496, DOI [10.1109/ICASSP.2019.8682383, 10.1109/icassp.2019.8682383]
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang XS, 2019, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR.2019.00535
   Wen Y, 2019, P 7 INT C LEARN REPR
   Xu X, 2020, IEEE T CYBERNETICS, V50, P2400, DOI 10.1109/TCYB.2019.2928180
   Xu X, 2019, WORLD WIDE WEB, V22, P657, DOI 10.1007/s11280-018-0541-x
   Yang EK, 2017, AAAI CONF ARTIF INTE, P1618
   Yu E, 2019, IEEE T MULTIMEDIA, V21, P1276, DOI 10.1109/TMM.2018.2877127
   Zhang J, 2020, IEEE T MULTIMEDIA, V22, P174, DOI 10.1109/TMM.2019.2922128
   Zhao X, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3511
   Zhou H, 2019, AAAI CONF ARTIF INTE, P9299
   Zhou YP, 2018, PROC CVPR IEEE, P3550, DOI 10.1109/CVPR.2018.00374
   Zhu B, 2019, PROC CVPR IEEE, P11469, DOI 10.1109/CVPR.2019.01174
   Zhu Hao, 2020, ARXIV200104758
NR 67
TC 28
Z9 28
U1 5
U2 40
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 338
EP 351
DI 10.1109/TMM.2021.3050089
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300026
DA 2024-07-18
ER

PT J
AU Zuo, YK
   Yao, HT
   Zhuang, LS
   Xu, CS
AF Zuo, Yukun
   Yao, Hantao
   Zhuang, Liansheng
   Xu, Changsheng
TI Seek Common Ground While Reserving Differences: A Model-Agnostic Module
   for Noisy Domain Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise measurement; Adaptation models; Predictive models; Reliability;
   Task analysis; Standards; Data models; Noisy domain adaptation; Seek
   common ground component; Reserve differences component
AB Noisy domain adaptation aims to solve the problem that the source dataset contains noisy labels in domain adaptation. Previous methods handle noisy labels by selecting the small-loss samples with inconsistent predictions between two models and discarding the consistent samples, resulting in many noises contained in the selected samples. By jointly considering the consistent and inconsistent samples, we propose a model-agnostic module, named Seek Common Ground While Reserving Differences (SCGWRD), to reduce the impact of noisy samples. The proposed SCGWRD module consists of Seek Common Ground (SCG) component and Reserve Differences (RD) component by utilizing the outputs of two symmetrical domain adaptation models. As the common samples with consistent predictions between two models are more likely to be clean samples, the SCG component applies the small-loss strategy to select the reliable samples with consistent predictions. Unlike SCG, the RD component maintains the divergences between two models with mutual learning and reduces the effect of noisy data using the samples with different predictions and small losses. Evaluations on three benchmarks demonstrate the effectiveness and robustness of the proposed SCGWRD module for noisy domain adaptation.
C1 [Zuo, Yukun; Zhuang, Liansheng] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
   [Yao, Hantao; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; Institute of Automation, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Xu, CS (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
EM zykpy@mail.ustc.edu.cn; hantao.yao@nlpr.ia.ac.cn; lszhuang@ustc.edu.cn;
   csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; Xu, Chang/GQP-7280-2022
OI xu, chang sheng/0000-0001-8343-9665; Yao, Hantao/0000-0001-8125-2864
FU National Key Research and Development Program of China [2018AAA0102205];
   National Natural Science Foundation of China [61902399, 61721004,
   U1836220, U1705262, 61832002, 61720106006, U20B2070, 61976199,
   62036012]; Beijing Natural Science Foundation [L201001]; Key Research
   Program of Frontier Sciences, CAS [QYZDJ-SSW-JSC039]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018AAA0102205, in part by the
   National Natural Science Foundation of China under Grants 61902399,
   61721004, U1836220, U1705262, 61832002, 61720106006, U20B2070, 61976199,
   and 62036012, in part by Beijing Natural Science Foundation under Grant
   L201001, and in part by the Key Research Program of Frontier Sciences,
   CAS under Grant QYZDJ-SSW-JSC039. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Yazhou Yao.
CR [Anonymous], 2018, PR MACH LEARN RES
   Arpit D., 2017, P 34 INT C MACH LEAR, P233, DOI DOI 10.48550/ARXIV.1706.05394
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Bergamo Alessandro, 2010, ADV NEURAL INFORM PR, V23
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng ZJ, 2019, IEEE I CONF COMP VIS, P9943, DOI 10.1109/ICCV.2019.01004
   French G., 2018, INT C LEARN REPR ICL
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Han ZY, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2269
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jiang L., 2018, ICML, P2304
   Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503
   Long M., 2016, Advances in neural information processing systems, V29
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Malach E, 2017, ADV NEUR IN, V30
   Pawan Kumar M., 2010, NIPS
   Rozantsev A, 2019, IEEE T PATTERN ANAL, V41, P801, DOI 10.1109/TPAMI.2018.2814042
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Saito K, 2017, PR MACH LEARN RES, V70
   Shen J, 2018, AAAI CONF ARTIF INTE, P4058
   Shermin T., 2020, IEEE T MULTIMEDIA, P1
   Shu Y, 2019, AAAI CONF ARTIF INTE, V33, P4951
   Shuhao Cui, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12452, DOI 10.1109/CVPR42600.2020.01247
   Song H, 2018, IEEE T MULTIMEDIA, V20, P1088, DOI 10.1109/TMM.2017.2763322
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Yu Xingrui, 2019, PROC INT C MACH LEAR, P7164
   Zeng HQ, 2017, PROC INT CONF RECON
   Zhang Y., 2019, PR MACH LEARN RES, P7404
   Zuo Y, 2020, IEEE INT CON MULTI, P1
   Zuo YK, 2022, IEEE T CIRC SYST VID, V32, P2057, DOI 10.1109/TCSVT.2021.3081729
   Zuo YK, 2021, IEEE T IMAGE PROCESS, V30, P3793, DOI 10.1109/TIP.2021.3065254
NR 39
TC 3
Z9 3
U1 2
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1020
EP 1030
DI 10.1109/TMM.2021.3097495
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ZN1SD
UT WOS:000764821800001
DA 2024-07-18
ER

PT J
AU Sheng, CC
   Zhu, XZ
   Xu, HY
   Pietikainen, M
   Liu, L
AF Sheng, Changchong
   Zhu, Xinzhong
   Xu, Huiying
   Pietikainen, Matti
   Liu, Li
TI Adaptive Semantic-Spatio-Temporal Graph Convolutional Network for Lip
   Reading
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Lips; Visualization; Feature extraction; Mouth; Task analysis;
   Convolution; Hidden Markov models; Lip reading;
   semantic-spatio-temporal; adaptive graph convolution network; two-stream
ID FEATURES
AB The goal of this work is to recognize words, phrases, and sentences being spoken by a talking face without given the audio. Current deep learning approaches for lip reading focus on exploring the appearance and optical flow information of videos. However, these methods do not fully exploit the characteristics of lip motion. In addition to appearance and optical flow, the mouth contour deformation usually conveys significant information that is complementary to others. However, the modeling of dynamic mouth contour has received little attention than that of appearance and optical flow. In this work, we propose a novel model of dynamic mouth contours called Adaptive Semantic-Spatio-Temporal Graph Convolution Network (ASST-GCN), to go beyond previous methods by automatically learning both the spatial and temporal information from videos. To combine the complementary information from appearance and mouth contour, a two-stream visual front-end network is proposed. Experimental results demonstrate that the proposed method significantly outperforms the state-of-the-art lip reading methods on several large-scale lip reading benchmarks.
C1 [Sheng, Changchong] Natl Univ Def Technol, Coll Elect Sci, Changsha 410073, Peoples R China.
   [Sheng, Changchong; Pietikainen, Matti; Liu, Li] Univ Oulu, Ctr Machine Vis & Signal Anal, FI-90014 Oulu, Finland.
   [Zhu, Xinzhong; Xu, Huiying] Res Inst Ningbo Cixing Co Ltd, Ningbo 315336, Peoples R China.
   [Zhu, Xinzhong; Xu, Huiying] Zhejiang Normal Univ, Coll Math Phys & Informat Engn, Jinhua 321004, Zhejiang, Peoples R China.
C3 National University of Defense Technology - China; University of Oulu;
   Zhejiang Normal University
RP Zhu, XZ (corresponding author), Res Inst Ningbo Cixing Co Ltd, Ningbo 315336, Peoples R China.
EM shengcc.nudt@gmail.com; zxz@zjnu.edu.cn; xhy@zjnu.edu.cn;
   matti.pietikainen@oulu.fi; li.liu@oulu.fi
OI Zhu, Xinzhong/0000-0002-0033-5260; Sheng,
   Changchong/0000-0001-6255-754X; Pietikainen, Matti/0000-0003-2263-6731;
   Liu, li/0000-0002-2011-2873
FU Academy of Finland [331883]; Outstanding Talents of "Ten Thousand
   Talents Plan" in Zhejiang Province project [2018R51001]; Natural Science
   Foundation of China [61976196]; Academy of Finland (AKA) [331883]
   Funding Source: Academy of Finland (AKA)
FX This work was supported in part by the Academy of Finland under Grant
   331883, in part by the Outstanding Talents of "Ten Thousand Talents
   Plan" in Zhejiang Province project under Grant 2018R51001, and in part
   by the Natural Science Foundation of China project under Grant 61976196.
CR Afouras T., 2018, ARXIV180606053
   Afouras T, 2022, IEEE T PATTERN ANAL, V44, P8717, DOI 10.1109/TPAMI.2018.2889052
   Afouras T, 2020, INT CONF ACOUST SPEE, P2143, DOI [10.1109/ICASSP40776.2020.9054253, 10.1109/icassp40776.2020.9054253]
   Afouras Triantafyllos, 2018, arXiv preprint arXiv:1809.00496
   Almajai I, 2016, INT CONF ACOUST SPEE, P2722, DOI 10.1109/ICASSP.2016.7472172
   [Anonymous], 2018, Deafness and hearing loss
   Arandjelovic R, 2018, LECT NOTES COMPUT SC, V11205, P451, DOI 10.1007/978-3-030-01246-5_27
   Assael Yannis M, 2016, ARXIV161101599
   Atoum Y, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P319, DOI 10.1109/BTAS.2017.8272713
   Atwood J, 2016, NIPS, P2001
   Bruna Joan, 2014, ICLR
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Çetingül HE, 2006, IEEE T IMAGE PROCESS, V15, P2879, DOI 10.1109/TIP.2006.877528
   Chen LL, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P349, DOI 10.1145/3126686.3126723
   Chen PH, 2020, IEEE T IMAGE PROCESS, V29, P8292, DOI 10.1109/TIP.2020.3009820
   Chuang Gan, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P758, DOI 10.1007/978-3-030-58621-8_44
   Chuang Gan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10475, DOI 10.1109/CVPR42600.2020.01049
   Chung JS, 2017, LECT NOTES COMPUT SC, V10117, P251, DOI 10.1007/978-3-319-54427-4_19
   Chung JS, 2017, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2017.367
   Chung JS, 2017, LECT NOTES COMPUT SC, V10112, P87, DOI 10.1007/978-3-319-54184-6_6
   Chung S.-W., 2020, P INTERSPEECH, P3486
   Chung SW, 2019, INT CONF ACOUST SPEE, P3965, DOI 10.1109/ICASSP.2019.8682524
   Cox S., 2008, The challenge of multispeaker lip-reading
   Defferrard M, 2016, ADV NEUR IN, V29
   Duvenaudt D, 2015, ADV NEUR IN, V28
   Fan LF, 2019, IEEE I CONF COMP VIS, P5723, DOI 10.1109/ICCV.2019.00582
   Fernandez-Lopez A, 2018, IMAGE VISION COMPUT, V78, P53, DOI 10.1016/j.imavis.2018.07.002
   Gilmer J, 2017, PR MACH LEARN RES, V70
   Graves A., 2006, P 23 INT C MACHINE L, P369
   Hamilton WL, 2017, ADV NEUR IN, V30
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henaff M., 2015, ARXIV150605163
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Kingma DP., 2014, ADAM METHOD STOCHAST
   Kipf T, 2018, PR MACH LEARN RES, V80
   Kipf TN, 2016, ARXIV
   Levie R, 2019, IEEE T SIGNAL PROCES, V67, P97, DOI 10.1109/TSP.2018.2879624
   Li RY, 2018, AAAI CONF ARTIF INTE, P3546
   Liu YJ, 2018, PROC CVPR IEEE, P389, DOI 10.1109/CVPR.2018.00048
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Lucey P., 2007, P 8 ANN C INT SPEECH, P650
   Luettin J, 1997, COMPUT VIS IMAGE UND, V65, P163, DOI 10.1006/cviu.1996.0570
   Martinez B, 2020, INT CONF ACOUST SPEE, P6319, DOI [10.1109/ICASSP40776.2020.9053841, 10.1109/icassp40776.2020.9053841]
   Matthews I, 2002, IEEE T PATTERN ANAL, V24, P198, DOI 10.1109/34.982900
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Niepert M, 2016, PR MACH LEARN RES, V48
   Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39
   Pei YR, 2013, IEEE I CONF COMP VIS, P129, DOI 10.1109/ICCV.2013.23
   Petridis S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6548, DOI 10.1109/ICASSP.2018.8461326
   Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25
   Rekik A, 2015, LECT NOTES COMPUT SC, V9386, P566, DOI 10.1007/978-3-319-25903-1_49
   Ristad ES, 1998, IEEE T PATTERN ANAL, V20, P522, DOI 10.1109/34.682181
   Saenko K, 2005, IEEE I CONF COMP VIS, P1424, DOI 10.1109/ICCV.2005.251
   Shi L, 2020, IEEE T IMAGE PROCESS, V29, P9532, DOI 10.1109/TIP.2020.3028207
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Shillingford B., 2018, ARXIV180705162
   Sidorov K., 2019, P BRIT MACH VIS C BM, DOI 10.5244/C.33.2
   Stafylakis T, 2017, INTERSPEECH, P3652, DOI 10.21437/Interspeech.2017-85
   Sutskever I, 2014, ADV NEUR IN, V27
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang WG, 2019, IEEE I CONF COMP VIS, P5702, DOI 10.1109/ICCV.2019.00580
   Wang WG, 2018, PROC CVPR IEEE, P4271, DOI 10.1109/CVPR.2018.00449
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wenguan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8926, DOI 10.1109/CVPR42600.2020.00895
   Wu Y, 2019, INT J COMPUT VISION, V127, P115, DOI 10.1007/s11263-018-1097-z
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang S, 2019, PROC 14 IEEE INT C A, P1
   Yun Fu, 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, P325
   Zhang XX, 2019, IEEE I CONF COMP VIS, P713, DOI 10.1109/ICCV.2019.00080
   Zhao GY, 2009, IEEE T MULTIMEDIA, V11, P1254, DOI 10.1109/TMM.2009.2030637
   Zhao H, 2019, IEEE I CONF COMP VIS, P1735, DOI 10.1109/ICCV.2019.00182
   Zhao Hang, 2018, LECT NOTES COMPUT SC, DOI DOI 10.1007/978-3-030-01246-5_35
NR 73
TC 7
Z9 7
U1 2
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG 16
PY 2021
VL 24
BP 3545
EP 3557
DI 10.1109/TMM.2021.3102433
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NH
UT WOS:000824706600001
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Wang, WN
   Lin, TW
   He, DL
   Li, F
   Wen, SL
   Wang, L
   Liu, J
AF Wang, Weining
   Lin, Tianwei
   He, Dongliang
   Li, Fu
   Wen, Shilei
   Wang, Liang
   Liu, Jing
TI Semi-Supervised Temporal Action Proposal Generation via Exploiting 2-D
   Proposal Map
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Proposals; Data models; Task analysis; Semisupervised learning;
   Training; Supervised learning; Predictive models; Semi-supervised
   learning; proposal map oriented mean-teacher; pseudo label
AB Temporal action proposal generation aims to generate temporal video segments containing human actions in untrimmed videos, which is always a preliminary for such video understanding tasks as action localization and temporally description grounding, etc. Fully-supervised solutions, though proven to be effective, suffer much from heavy data annotation overhead. To address this problem, this paper focuses on a rarely investigated yet practical problem of semi-supervised learning for temporal action proposal generation. Firstly, we propose a Proposal Map oriented Mean-Teacher (PM-MT) model, which can use both labeled and unlabeled data for end-to-end model training. Secondly, a Suppression-and-Re-Generation (SRG) strategy is designed to generate high-quality pseudo labels for unlabeled data, which are then used to finetune the model. Extensive experiments demonstrate the effectiveness of our proposed method, by achieving the state-of-the-art results on two public benchmark datatsets on the task of semi-supervised action proposal generation and outperforming fully-supervised learning methods with only a portion of labeled data.
C1 [Wang, Weining; Wang, Liang; Liu, Jing] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit NLPR, Beijing 100190, Peoples R China.
   [Wang, Weining] Minist Educ, Key Lab Knowledge Automat Ind Proc, Beijing 100083, Peoples R China.
   [Lin, Tianwei; He, Dongliang; Li, Fu; Wen, Shilei] Baidu Inc, Dept Comp Vis Technol VIS, Beijing 100193, Peoples R China.
   [Wang, Liang; Liu, Jing] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Baidu;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Liu, J (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit NLPR, Beijing 100190, Peoples R China.
EM weining.wang@nlpria.ac.cn; lintianwei01@baidu.com;
   hedongliang01@baidu.com; lifu@baidu.com; wenshilei@baidu.com;
   wangliang@nlpria.ac.cn; jliu@nlpria.ac.cn
RI Liu, Jing/A-7644-2016
OI liu, jing/0000-0003-0903-9131; Wang, Weining/0000-0001-7299-6431
FU National Key Research and Development Program of China [2020AAA0106400];
   National Natural Science Foundation of China [61922086, 61872366];
   Beijing Natural Science Foundation [4192059, JQ20022]; Fundamental
   Research Funds for the Central Universities [FRF-BD-20-10A]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020AAA0106400, in part by the
   National Natural Science Foundation of China under Grants 61922086 and
   61872366, in part by Beijing Natural Science Foundation under Grants
   4192059 and JQ20022, and in part by the Fundamental Research Funds for
   the Central Universities under Grant FRF-BD-20-10A.
CR Bachman P, 2014, ADV NEUR IN, V27
   Bachman P, 2019, ADV NEUR IN, V32
   Buch S., 2017, P BRIT MACH VIS C BM
   Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Chen PH, 2020, IEEE T MULTIMEDIA, V22, P2723, DOI 10.1109/TMM.2019.2959977
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47
   Gao JL, 2020, AAAI CONF ARTIF INTE, V34, P10810
   Gao JY, 2018, LECT NOTES COMPUT SC, V11206, P70, DOI 10.1007/978-3-030-01216-8_5
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Gao Jiyang, 2017, BMVC
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heilbron FC, 2016, PROC CVPR IEEE, P1914, DOI 10.1109/CVPR.2016.211
   Ioffe S., 2015, 32 INT C MACHINE LEA
   Iscen A, 2019, PROC CVPR IEEE, P5065, DOI 10.1109/CVPR.2019.00521
   Jackway PT, 1996, IEEE T PATTERN ANAL, V18, P38, DOI 10.1109/34.476009
   Ji JW, 2019, IEEE I CONF COMP VIS, P7072, DOI 10.1109/ICCV.2019.00717
   Ji X, 2019, IEEE I CONF COMP VIS, P9864, DOI 10.1109/ICCV.2019.00996
   Jiang Y.-G., 2014, THUMOS challenge: Action recognition with a large number of classes
   Kay W., 2017, ARXIV170506950
   Kingma D. P., 2014, arXiv
   Laine S., 2017, ICLR POSTER, P1
   Lee D.-H., 2013, WORKSHOP CHALLENGES, V3, P896
   Li S, 2020, P IEEECVF C COMPUTER, P13400
   Li Y, 2021, IEEE T MULTIMEDIA, V23, P1354, DOI 10.1109/TMM.2020.2997185
   Lin CRN, 2020, AAAI CONF ARTIF INTE, V34, P11499
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Liu HJ, 2020, IEEE T MULTIMEDIA, V22, P337, DOI 10.1109/TMM.2019.2929923
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Peisen Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P539, DOI 10.1007/978-3-030-58598-3_32
   POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren S., 2015, NEURAL INFORM PROCES, V28, P91, DOI DOI 10.1109/TPAMI.2016.2577031
   Sajjadi M, 2016, ADV NEUR IN, V29
   Shi WW, 2018, LECT NOTES COMPUT SC, V11209, P311, DOI 10.1007/978-3-030-01228-1_19
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Su HS, 2021, AAAI CONF ARTIF INTE, V35, P2602
   Tarvainen A, 2017, ADV NEUR IN, V30
   Xiong Y., 2016, PROC IEEE C COMPUT V, P1
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhang S, 2019, ARXIV191203612
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
NR 46
TC 3
Z9 4
U1 1
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG 13
PY 2021
VL 24
BP 3624
EP 3635
DI 10.1109/TMM.2021.3104398
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NP
UT WOS:000824707400001
DA 2024-07-18
ER

PT J
AU Zhang, Z
   Wang, XG
   Lu, GM
   Shen, FM
   Zhu, L
AF Zhang, Zheng
   Wang, Xunguang
   Lu, Guangming
   Shen, Fumin
   Zhu, Lei
TI Targeted Attack of Deep Hashing Via Prototype-Supervised Adversarial
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Prototypes; Generators; Optimization; Cats; Binary codes;
   Task analysis; Adversarial example; targeted attack; deep hashing;
   similarity retrieval; generative adversarial network
ID IMAGE RETRIEVAL
AB Due to its powerful capability of representation learning and efficient computation, deep hashing has made significant progress in large-scale image retrieval. It has been recognized that deep neural networks are vulnerable to adversarial examples, which is a practical secure problem but seldom studied in deep hashing-based retrieval field. In this paper, we propose a novel prototype-supervised adversarial network (ProS-GAN), which formulates a flexible generative architecture for efficient and effective targeted hashing attack. To the best of our knowledge, this is one of the first generation-based methods to attack deep hashing networks. Generally, our proposed framework consists of three parts, i.e., a PrototypeNet, a Generator and a Discriminator. Specifically, the designed PrototypeNet embeds the target label into the semantic representation and learns the prototype code as the category-level representative of the target label. Moreover, the semantic representation and the original image are jointly fed into the generator for flexible targeted attack. Particularly, the prototype code is adopted to supervise the generator to construct the targeted adversarial example by minimizing the Hamming distance between the hash code of the adversarial example and the prototype code. Furthermore, the generator fools the discriminator to simultaneously encourage the adversarial examples visually realistic and the semantic representation informative. Extensive experiments demonstrate that the proposed framework can efficiently produce adversarial examples with better targeted attack performance and transferability over state-of-the-art targeted attack methods of deep hashing. The source code is available at https://github.com/xunguangwang/ProS-GAN_Trans.
C1 [Zhang, Zheng] Harbin Inst Technol, Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.
   [Zhang, Zheng] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Wang, Xunguang; Lu, Guangming] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
   [Shen, Fumin] Univ Elect Sci & Technol China, Ctr Future Media, Chengdu 610054, Peoples R China.
   [Shen, Fumin] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
   [Zhu, Lei] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 47856, Shandong, Peoples R China.
C3 Harbin Institute of Technology; Peng Cheng Laboratory; Harbin Institute
   of Technology; University of Electronic Science & Technology of China;
   University of Electronic Science & Technology of China; Shandong Normal
   University
RP Zhang, Z (corresponding author), Harbin Inst Technol, Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.
EM darrenzz219@gmail.com; xunguangwang@gmail.com; luguangm@hit.edu.cn;
   fumin.shen@gmail.com; leizhu0608@gmail.com
RI Zhang, Zhang/JAX-2097-2023; Zhu, Lei/GQQ-1130-2022; Shen,
   Fumin/R-2121-2016
OI Zhu, Lei/0000-0002-5348-7532; Lu, Guangming/0000-0003-1578-2634; Zhang,
   Zheng/0000-0003-1470-6998; Zhu, Lei/0000-0002-2993-7142; Wang,
   Xunguang/0000-0002-5330-2286
FU National Natural Science Foundation of China [62002085]; Guangdong
   Basic, and Applied Basic Research Foundation [2019A1515110475,
   2019Bl515120055]; Shenzhen Fundamental Research Fund
   [JCYJ20180306172023949, GXWD2020123015542700320200824103320001];
   Shenzhen Institute of Artificial Intelligence, and Robotics for Society
   [AC01202005018]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62002085, in part by the Guangdong
   Basic, and Applied Basic Research Foundation under Grants
   2019A1515110475 and 2019Bl515120055, in part by Shenzhen Fundamental
   Research Fund under Grants JCYJ20180306172023949 and
   GXWD2020123015542700320200824103320001, and in part by the Open Project
   Fund under Grant AC01202005018 from the Shenzhen Institute of Artificial
   Intelligence, and Robotics for Society.
CR Akhtar N, 2018, IEEE ACCESS, V6, P14410, DOI 10.1109/ACCESS.2018.2807385
   Andoni A, 2006, ANN IEEE SYMP FOUND, P459
   Baluja S, 2018, AAAI CONF ARTIF INTE, P2687
   Biggio Battista, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P387, DOI 10.1007/978-3-642-40994-3_25
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chen J, 2022, IEEE T CYBERNETICS, V52, P5935, DOI 10.1109/TCYB.2021.3064092
   Chen PY, 2017, P 10 ACM WORKSH ART, P15, DOI [10.1145/3128572.3140448, DOI 10.1145/3128572.3140448]
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Cui H, 2020, IEEE T IMAGE PROCESS, V29, P1271, DOI 10.1109/TIP.2019.2940693
   Deng C, 2020, IEEE T NEUR NET LEAR, V31, P2189, DOI 10.1109/TNNLS.2019.2929068
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Dizaji KG, 2018, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR.2018.00386
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Feng Y, 2020, AAAI CONF ARTIF INTE, V34, P10786
   Goodfellow I. J., 2015, 3 INT C LEARNING REP
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Han JF, 2019, IEEE I CONF COMP VIS, P5157, DOI 10.1109/ICCV.2019.00526
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Jiawang Bai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P618, DOI 10.1007/978-3-030-58452-8_36
   Kingma D, 2014, ICLR P, V2014, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kurakin Alexey, 2017, INT C LEARN REPR
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li J, 2019, IEEE I CONF COMP VIS, P4898, DOI 10.1109/ICCV.2019.00500
   Li WJ, 2016, IJCAI, P1711
   Li Z, 2021, KNOWL-BASED SYST, V212, DOI 10.1016/j.knosys.2020.106618
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Madry A., 2018, ARXIV
   Mopuri KR, 2018, PROC CVPR IEEE, P742, DOI 10.1109/CVPR.2018.00084
   Papernot N, 2017, PROCEEDINGS OF THE 2017 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (ASIA CCS'17), P506, DOI 10.1145/3052973.3053009
   Poursaeed O, 2018, PROC CVPR IEEE, P4422, DOI 10.1109/CVPR.2018.00465
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Simonyan K., 2014, 14091556 ARXIV
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Szegedy C, 2014, INT C LEARN REPR
   Tolias G, 2019, IEEE I CONF COMP VIS, P5036, DOI 10.1109/ICCV.2019.00514
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang XG, 2021, PROC CVPR IEEE, P16352, DOI 10.1109/CVPR46437.2021.01609
   Wang XG, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P2298, DOI 10.1145/3404835.3463233
   Wang ZJ, 2021, IEEE T MULTIMEDIA, V23, P1274, DOI 10.1109/TMM.2020.2995267
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Xiao CW, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3905
   Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153
   Yang EK, 2020, IEEE T CYBERNETICS, V50, P1473, DOI 10.1109/TCYB.2018.2882908
   Yang E, 2018, IEEE T NEUR NET LEAR, V29, P5292, DOI 10.1109/TNNLS.2018.2793863
   Zhang HF, 2021, IEEE T MULTIMEDIA, V23, P3400, DOI 10.1109/TMM.2020.3025000
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
   Zhu JR, 2018, LECT NOTES COMPUT SC, V11219, P682, DOI 10.1007/978-3-030-01267-0_40
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu L, 2020, IEEE T IMAGE PROCESS, V29, P4643, DOI 10.1109/TIP.2020.2974065
NR 57
TC 7
Z9 7
U1 2
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 16
PY 2021
VL 24
BP 3392
EP 3404
DI 10.1109/TMM.2021.3097506
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NK
UT WOS:000824706900002
DA 2024-07-18
ER

PT J
AU Meng, CL
   An, P
   Huang, XP
   Yang, C
   Shen, LQ
   Wang, B
AF Meng, Chunli
   An, Ping
   Huang, Xinpeng
   Yang, Chao
   Shen, Liquan
   Wang, Bin
TI Objective Quality Assessment of Lenslet Light Field Image Based on Focus
   Stack
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distortion; Light fields; Image coding; Indexes; Cameras; Image quality;
   Microoptics; Angular-spatial feature; Focus stack; Image quality
   assessment; Light field; Refocus
ID SIMILARITY; CONTOURLET
AB The large amount of complex scene information recorded by light field imaging has the potential for immersive media applications. Compression and reconstruction algorithms are crucial for the transmission, storage, and display of such massive data. Most of the existing quality evaluation indexes do not effectively account for light field characteristics. To accurately evaluate the distortions caused by compression and reconstruction algorithms, it is necessary to construct an image evaluation index that reflects the angular-spatial characteristics of the light field. This work proposes a full-reference light field image quality evaluation index that attempts to extract less information from the focus stack to accurately evaluate the entire light field quality. The proposed framework includes three specific steps. First, we construct a key refocused image extraction framework by the maximal spatial information contrast and the minimal angular information variation. Specifically, the gradient and phase congruency operators are used in the extraction framework. Second, a novel light field quality evaluation index is built based on the angular-spatial characteristics of the key refocused images. In detail, the features used in the key refocused image extraction framework and the chrominance feature are combined to construct the union feature. Third, the similarity of the union feature is pooled by the relevant visual saliency map to obtain the predicted score. Finally, the overall quality of the light field is measured by applying the proposed index to the key refocused images. The high efficiency and precision of the proposed method are shown by extensive comparison experiments.
C1 [Meng, Chunli; An, Ping; Huang, Xinpeng; Yang, Chao; Shen, Liquan; Wang, Bin] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
   [Meng, Chunli; An, Ping; Huang, Xinpeng; Yang, Chao; Shen, Liquan; Wang, Bin] Minist Educ, Key Lab Adv Display & Syst Applicat, Shanghai 200072, Peoples R China.
C3 Shanghai University
RP An, P (corresponding author), Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
EM windymeng@shu.edu.cn; anping@shu.edu.cn; huangxinpeng@shu.edu.cn;
   yangchaoie@shu.edu.cn; jsslq@shu.edu.cn; brantley.wang@hotmail.com
RI Wang, Bin/KMA-1411-2024; Huang, xp/JRX-2837-2023; Shen,
   Liquan/D-4832-2012
OI Shen, Liquan/0000-0002-2148-6279
FU National Natural Science Foundation of China [62020106011, 62001279,
   62071287, 61901252]; Science and Technology Commission of Shanghai
   Municipality [18XD1423900, 20DZ2290100]
FX This work was supported in part by the National Natural Science
   Foundation of China, under Grants 62020106011, 62001279, 62071287, and
   61901252, and in part by the Science and Technology Commission of
   Shanghai Municipality under Grants 18XD1423900 and 20DZ2290100.
CR Adhikarla VK, 2017, PROC CVPR IEEE, P3720, DOI 10.1109/CVPR.2017.396
   Ahmad W, 2017, IEEE IMAGE PROC, P4557, DOI 10.1109/ICIP.2017.8297145
   [Anonymous], 2006, THESIS STANFORD U CA
   Bakir N., 2019, EUR SIGNAL PR CONF, DOI DOI 10.23919/eusipco.2019.8902614
   Brites C, 2021, IEEE T CIRC SYST VID, V31, P339, DOI 10.1109/TCSVT.2020.2976784
   Bruce NDB, 2009, J VISION, V9, DOI 10.1167/9.3.5
   Cao FY, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2365, DOI 10.1109/ICASSP39728.2021.9413824
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Chen J, 2018, IEEE T IMAGE PROCESS, V27, P314, DOI 10.1109/TIP.2017.2750413
   Chen MJ, 2013, SIGNAL PROCESS-IMAGE, V28, P1143, DOI 10.1016/j.image.2013.05.006
   Chen Y., 2019, P IEEE VIS COMM IM S, P1
   Chen ZB, 2018, IEEE T IMAGE PROCESS, V27, P721, DOI 10.1109/TIP.2017.2766780
   Conti C, 2020, IEEE ACCESS, V8, P49244, DOI 10.1109/ACCESS.2020.2977767
   Dai F, 2015, IEEE IMAGE PROC, P4733, DOI 10.1109/ICIP.2015.7351705
   Damera-Venkata N, 2000, IEEE T IMAGE PROCESS, V9, P636, DOI 10.1109/83.841940
   Fang YM, 2018, 2018 IEEE FOURTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM)
   Gao XB, 2008, NEUROCOMPUTING, V72, P378, DOI 10.1016/j.neucom.2007.12.031
   Gu K, 2017, IEEE T IND ELECTRON, V64, P3903, DOI 10.1109/TIE.2017.2652339
   Hao Z., 2015, EFFICIENT ANTIOCCLUS
   Hu ZX, 2020, EXPERT SYST APPL, V151, DOI 10.1016/j.eswa.2020.113394
   Huang HL, 2020, THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2020), P348, DOI 10.1109/MIPR49039.2020.00077
   Huang XP, 2022, IEEE T MULTIMEDIA, V24, P152, DOI 10.1109/TMM.2020.3046860
   Huang ZJ, 2019, INT SYM NETWO COMP, DOI 10.1109/isncc.2019.8909170
   Kovesi P., 1999, Videre, V1
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Levoy M, 2006, COMPUTER, V39, P46, DOI 10.1109/MC.2006.270
   Liu LX, 2017, SIGNAL PROCESS-IMAGE, V58, P287, DOI 10.1016/j.image.2017.08.011
   Liu X., 2016, PROC 3D IMAGE ACQUIS, P1
   Luo Z., 2019, PROC PICTURE CODING, P1
   Lv HJ, 2015, IEEE INT CONF MULTI
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Meng CL, 2020, IEEE SIGNAL PROC LET, V27, P525, DOI 10.1109/LSP.2020.2982060
   Meng CL, 2019, LECT NOTES COMPUT SC, V11903, P193, DOI 10.1007/978-3-030-34113-8_17
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P6054, DOI 10.1109/TIP.2020.2988148
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3790, DOI 10.1109/TIP.2020.2966081
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   MORRONE MC, 1988, PROC R SOC SER B-BIO, V235, P221, DOI 10.1098/rspb.1988.0073
   Paudyal P, 2019, IEEE T BROADCAST, V65, P152, DOI 10.1109/TBC.2019.2892092
   Paudyal P, 2017, IEEE T BROADCAST, V63, P507, DOI 10.1109/TBC.2017.2704430
   Perra C, 2018, MULTIMED TOOLS APPL, V77, P21771, DOI 10.1007/s11042-018-5615-3
   Ren NgMarc Levoy., 2005, Light field photography with a hand-held plenoptic camera
   Ribeiro FML, 2018, IEEE T MULTIMEDIA, V20, P1, DOI 10.1109/TMM.2017.2714425
   Rizkallah M, 2016, EUR SIGNAL PR CONF, P898, DOI 10.1109/EUSIPCO.2016.7760378
   Sandic-Stankovic D., 2015, P IEEE 7 INT WORKSH, P1
   Sandic-Stankovic D, 2015, 3DTV CONF
   Shan L, 2019, IEEE ACCESS, V7, P127217, DOI 10.1109/ACCESS.2019.2940093
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Shi LK, 2020, IEEE T CIRC SYST VID, V30, P4114, DOI 10.1109/TCSVT.2019.2955011
   Shi LK, 2019, IEEE IMAGE PROC, P3781, DOI [10.1109/icip.2019.8803559, 10.1109/ICIP.2019.8803559]
   Shi LK, 2018, IEEE IMAGE PROC, P41, DOI 10.1109/ICIP.2018.8451077
   Tabus I, 2017, IEEE IMAGE PROC, P4567, DOI 10.1109/ICIP.2017.8297147
   Tao DC, 2009, IEEE T SYST MAN CY B, V39, P1623, DOI 10.1109/TSMCB.2009.2021951
   Tao MW, 2013, IEEE I CONF COMP VIS, P673, DOI 10.1109/ICCV.2013.89
   Tian SS, 2019, IEEE T MULTIMEDIA, V21, P1235, DOI 10.1109/TMM.2018.2875307
   Tian Y, 2021, IEEE T CIRC SYST VID, V31, P2046, DOI 10.1109/TCSVT.2020.2971256
   Tian Y, 2018, J VIS COMMUN IMAGE R, V57, P212, DOI 10.1016/j.jvcir.2018.11.005
   Venkatesh S., 1989, P INT C IMAGE PROCES, P553
   Viola I, 2018, INT WORK QUAL MULTIM, P189
   Viola I, 2016, PICT COD SYMP
   Wang Z, 2005, PROC SPIE, V5666, P149, DOI 10.1117/12.597306
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z., 2006, Modern Image Quality Assessment, DOI 10.2200/S00010ED1V01Y200508IVM003
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wilburn B, 2005, ACM T GRAPHIC, V24, P765, DOI 10.1145/1073204.1073259
   Wu GC, 2017, IEEE J-STSP, V11, P926, DOI 10.1109/JSTSP.2017.2747126
   Wu JD, 2015, 2015 VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP), DOI 10.1109/VCIP.2015.7457904
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhai GT, 2019, DIGIT SIGNAL PROCESS, V91, P11, DOI 10.1016/j.dsp.2019.02.017
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang L, 2010, IEEE IMAGE PROC, P321, DOI 10.1109/ICIP.2010.5649275
   Zhang W., 2018, PROC PACIFIC RIM C M, P1
   Zhao SY, 2017, IEEE IMAGE PROC, P4562, DOI 10.1109/ICIP.2017.8297146
   Zhou W, 2020, IEEE T IMAGE PROCESS, V29, P4070, DOI 10.1109/TIP.2020.2969777
NR 78
TC 28
Z9 28
U1 1
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 13
PY 2021
VL 24
BP 3193
EP 3207
DI 10.1109/TMM.2021.3096071
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NQ
UT WOS:000824707500001
DA 2024-07-18
ER

PT J
AU Chen, YH
   Tan, B
   Wu, J
   Zhang, ZF
   Ren, HQ
AF Chen, Yihao
   Tan, Bin
   Wu, Jun
   Zhang, Zhifeng
   Ren, Haoqi
TI A Deep Image Coding Scheme With Generative Network to Learn From
   Correlated Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Image reconstruction; Inverse problems; Generative
   adversarial networks; Generators; Deep learning; Encoding; Coding
   schemes; correlated images; GAN; inverse problem
AB This paper provides a method to build a deep learning image coding system based on inverse problem, choosing a suitable measurement operator to reduce the amount of information transmitted at the sender, and reconstructing the original image by tackling the inverse problem at the receiver. Unlike most compressed sensing (CS) methods, the proposed coding scheme does not rely on sparsity but uses the structural priors of the generative adversarial networks (GAN) to solve the inverse problem. The proposed model trains the GAN to learn a mapping from the latent space to the sample space formed by correlated images on the cloud. Then the measurements are used to localize the optimal latent variable in the representation space which corresponding to the original image in the sample space. The proposed method encodes and transmits the measurements instead of the original image, which greatly reduces the cost of transmission while ensuring the quality of the reconstructed the image at high compression ratios. To the best of our knowledge, this is the first time to introduce the GAN-based inverse problem in the field of the deep image coding area. The experimental results show that the visual quality of the images generated by the proposed scheme is better than the traditional encoding scheme JPEG2000. Especially in the case of extremely high compression ratios, the proposed scheme can still maintain good performance.
C1 [Chen, Yihao; Zhang, Zhifeng; Ren, Haoqi] Tongji Univ, Coll Elect & Informat Engn, Shanghai 201804, Peoples R China.
   [Chen, Yihao] Tongji Univ, Key Lab Embedded Syst & Serv Comp, Minist Educ, Shanghai 201804, Peoples R China.
   [Tan, Bin] Jinggangshan Univ, Coll Elect & Informat Engn, Jian 343009, Jiangxi, Peoples R China.
   [Wu, Jun] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
   [Wu, Jun] Fudan Univ, Shanghai Key Lab Intelligent Informat Proc, Shanghai 200433, Peoples R China.
C3 Tongji University; Tongji University; Jinggangshan University; Fudan
   University; Fudan University
RP Tan, B (corresponding author), Jinggangshan Univ, Coll Elect & Informat Engn, Jian 343009, Jiangxi, Peoples R China.
EM xxxasdcyh@tongji.edu.cn; tanbin@jgsu.edu.cn; wujun@fudan.edu.cn;
   zhangzf@tongji.edu.cn; renhaoqi@tongji.edu.cn
FU National Key R&D Program of China [2020YFA0711400]; National Natural
   Science Foundation of China under (NSFC) [61901199, 61831018]; Key
   Laboratory of Embedded System and Service Computing (Tongji University),
   Ministry of Education (ESSCKF) [2018-06]; Natural Science Foundation of
   Jiangxi China [20202BAB202002]
FX This work was supported in part by National Key R&D Program of China
   under Grant 2020YFA0711400, in part by the National Natural Science
   Foundation of China under (NSFC) under Grants 61901199, and 61831018,
   Key Laboratory of Embedded System and Service Computing (Tongji
   University), Ministry of Education under Grant ESSCKF 2018-06, and in
   part by the Natural Science Foundation of Jiangxi China under Grant
   20202BAB202002.
CR Agustsson E, 2019, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2019.00031
   Akbari M, 2019, INT CONF ACOUST SPEE, P2042, DOI [10.1109/icassp.2019.8683541, 10.1109/ICASSP.2019.8683541]
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Balle J, 2018, ICLR
   Balle J., 2016, PROC INT C LEARN REP, P1
   Choi Y, 2019, IEEE I CONF COMP VIS, P3146, DOI 10.1109/ICCV.2019.00324
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hsu CC, 2017, INTERSPEECH, P3364, DOI 10.21437/Interspeech.2017-63
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Ledig C., 2017, P IEEE C COMP VIS PA, P4681
   Mentzer F, 2018, PROC CVPR IEEE, P4394, DOI 10.1109/CVPR.2018.00462
   Radford A., 2016, 4 INT C LEARN REPR I
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wu J, 2016, IEEE T MULTIMEDIA, V18, P893, DOI 10.1109/TMM.2016.2535727
   Yue HJ, 2013, IEEE T MULTIMEDIA, V15, P845, DOI 10.1109/TMM.2013.2239629
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 22
TC 1
Z9 1
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2235
EP 2244
DI 10.1109/TMM.2021.3087011
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800006
DA 2024-07-18
ER

PT J
AU Guan, MY
   Wen, CY
AF Guan, Mingyang
   Wen, Changyun
TI Adaptive Multi-Feature Reliability Re-Determinative Correlation Filter
   for Visual Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target tracking; Reliability; Visualization; Adaptation models;
   Numerical models; Computational modeling; Hidden Markov models;
   Correlation filter; machine learning; multiple features; visual tracking
AB Model drift is a challenging issue for visual object tracking. Most available approaches aim to address this issue by constructing of a stronger discriminative prediction model with the integration of multiple features. In this article, we also address the issue of model drift using multiple features, but we consider the fusion problem from a different point of view, namely, strengthening the features that are suitable for the current scenario while weakening the remaining ones. Therefore, an advanced tracker that adaptively redetermines the reliability for each feature during the tracking process is proposed. Furthermore, to correctly evaluate and redetermine these reliabilities, two different solutions, called model evaluation and numerical optimization, are proposed, and two independent trackers corresponding to these two solutions are implemented. Extensive experiments have been designed on five large datasets to validate the following: 1) The proposed tracking framework is superior for making the tracking model more robust, and 2) the two solutions proposed for redetermining the reliability for each feature are effective. As expected, the two implemented trackers do indeed improve the accuracy and robustness compared to state-of-the-art trackers. Especially on VOT2016, the proposed trackers based on model evaluation and numerical optimization achieve outstanding EAO scores (i.e., 0.453 and 0.428, respectively), outperforming the recently developed top trackers by a large margin.
C1 [Guan, Mingyang; Wen, Changyun] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Guan, MY (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM guan0050@e.ntu.edu.sg; ecywen@ntu.edu.sg
RI Wen, Changyun/A-5018-2011
OI Wen, Changyun/0000-0001-9530-360X
CR Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Bhat G, 2018, LECT NOTES COMPUT SC, V11206, P493, DOI 10.1007/978-3-030-01216-8_30
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Choi J, 2018, PROC CVPR IEEE, P479, DOI 10.1109/CVPR.2018.00057
   Dai KN, 2019, PROC CVPR IEEE, P4665, DOI 10.1109/CVPR.2019.00480
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2016.159
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fan H, 2017, IEEE I CONF COMP VIS, P5487, DOI 10.1109/ICCV.2017.585
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Guan MY, 2019, IEEE T IND ELECTRON, V66, P2054, DOI 10.1109/TIE.2018.2835390
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Huang ZY, 2019, IEEE I CONF COMP VIS, P2891, DOI 10.1109/ICCV.2019.00298
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Kristan M, 2015, LECT NOTES COMPUT SC, V8926, P191, DOI 10.1007/978-3-319-16181-5_14
   Lee DY, 2015, PROC CVPR IEEE, P5088, DOI 10.1109/CVPR.2015.7299144
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li JT, 2017, IEEE T IMAGE PROCESS, V26, P2736, DOI 10.1109/TIP.2017.2686601
   Li X, 2019, PROC CVPR IEEE, P1369, DOI 10.1109/CVPR.2019.00146
   Liang NX, 2018, IEEE T MULTIMEDIA, V20, P2289, DOI 10.1109/TMM.2018.2803518
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Lu XK, 2018, LECT NOTES COMPUT SC, V11218, P369, DOI 10.1007/978-3-030-01264-9_22
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Mueller M, 2017, PROC CVPR IEEE, P1387, DOI 10.1109/CVPR.2017.152
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Parrella F., 2007, THESIS
   Pu S., 2018, ADV NEURAL INFORM PR, V31, P1931, DOI DOI 10.1016/J.PATCOG.2018.10.005
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wachter A., 2002, INTERIOR POINT ALGOR
   Wang NY, 2014, PR MACH LEARN RES, V32, P1107
   Wang N, 2019, PROC CVPR IEEE, P1308, DOI 10.1109/CVPR.2019.00140
   Wang N, 2018, PROC CVPR IEEE, P4844, DOI 10.1109/CVPR.2018.00509
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Zhang TZ, 2017, PROC CVPR IEEE, P4819, DOI [10.1109/CVPR.2017.512, 10.1109/ICCV.2017.469]
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
NR 51
TC 5
Z9 5
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3841
EP 3852
DI 10.1109/TMM.2020.3032043
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100032
DA 2024-07-18
ER

PT J
AU Jiang, B
   Zhang, Y
   Luo, B
   Cao, XC
   Tang, J
AF Jiang, Bo
   Zhang, Yuan
   Luo, Bin
   Cao, Xiaochun
   Tang, Jin
TI STGL: Spatial-Temporal Graph Representation and Learning for Visual
   Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target tracking; Computational modeling; Visualization; Noise
   measurement; Semisupervised learning; Shape; Feature extraction; Visual
   tracking; semi-supervised learning; graph representation; graph learning
ID NONNEGATIVE LOW-RANK; THRESHOLDING ALGORITHM; OBJECT TRACKING; SPARSE
   GRAPH
AB Tracking-by-detection framework has been normally adopted in visual tracking methods. It aims to localize the visual target object with a bounding box. However, the bounding box is usually difficult to describe the target object accurately and thus easily introduces noisy background information, which usually degrades the final tracking results. Recently, weighted patch representation of the object has been shown very effectively for suppressing the undesirable background information and thus can obviously improve the tracking results. In this paper, we propose a novel Spatial-Temporal Graph representation and Learning (STGL) model to generate a kind of robust target representation for visual tracking problem. The main aspect of STGL is that it aims to exploit both spatial (within each frame) and temporal (between consecutive frames) structure of patches simultaneously in a unified graph representation and semi-supervised learning model. Comparing with existing works, STGL naturally exploits the learned representation of object in previous frame and thus can obtain the representation of object in current frame more accurately and robustly. A new ADMM algorithm is derived to solve the proposed STGL model. Based on the proposed object representation, we then adapt the structured SVM by introducing scale estimation to achieve object tracking. Extensive experiments show that our method outperforms the state-of-the-art patch based tracking methods on two standard benchmark datasets.
C1 [Jiang, Bo; Zhang, Yuan; Luo, Bin; Tang, Jin] Anhui Univ, Sch Comp Sci & Technol, Key Lab Intelligent Comp & Signal Proc, Minist Educ, Hefei 230039, Peoples R China.
   [Jiang, Bo] Anhui Univ, Inst Phys Sci & Informat Technol, Hefei 230039, Peoples R China.
   [Cao, Xiaochun] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing 100864, Peoples R China.
C3 Anhui University; Anhui University; Chinese Academy of Sciences;
   Institute of Information Engineering, CAS
RP Tang, J (corresponding author), Anhui Univ, Sch Comp Sci & Technol, Key Lab Intelligent Comp & Signal Proc, Minist Educ, Hefei 230039, Peoples R China.
EM zeyiabc@163.com; 270953467@qq.com; ahu_lb@163.com;
   caoxiaochun@iie.ac.cn; ahu_tj@163.com
RI Chen, Bowen/KFB-3986-2024; lu, bin/HPE-4790-2023
FU National Natural Science Foundation of China [61602001, 61872005,
   61671018]; NSFC Key Projects of International (Regional) Cooperation and
   Exchanges [61860206004]; Open fund for Discipline Construction,
   Institute of Physical Science and Information Technology, Anhui
   University; Cooperative Research Project Program of Nanjing Artificial
   Intelligence Chip Research, Institute of Automation, Chinese Academy of
   Sciences
FX This work is supported in part by National Natural Science Foundation of
   China under Grants 61602001, 61872005, and 61671018, in part by NSFC Key
   Projects of International (Regional) Cooperation and Exchanges under
   Grant 61860206004, in part by Open fund for Discipline Construction,
   Institute of Physical Science and Information Technology, Anhui
   University, and in part by Cooperative Research Project Program of
   Nanjing Artificial Intelligence Chip Research, Institute of Automation,
   Chinese Academy of Sciences.
CR Beck A, 2009, INT CONF ACOUST SPEE, P693, DOI 10.1109/ICASSP.2009.4959678
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Choi J, 2018, PROC CVPR IEEE, P479, DOI 10.1109/CVPR.2018.00057
   Choi J, 2017, PROC CVPR IEEE, P4828, DOI 10.1109/CVPR.2017.513
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Dong XP, 2018, PROC CVPR IEEE, P518, DOI 10.1109/CVPR.2018.00061
   Du DW, 2018, IEEE T IMAGE PROCESS, V27, P1809, DOI 10.1109/TIP.2017.2785626
   Du DW, 2016, IEEE T IMAGE PROCESS, V25, P3572, DOI 10.1109/TIP.2016.2570556
   Duffner S, 2017, IEEE T IMAGE PROCESS, V26, P2368, DOI 10.1109/TIP.2017.2676346
   Duffner S, 2013, IEEE I CONF COMP VIS, P2480, DOI 10.1109/ICCV.2013.308
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   He SF, 2013, PROC CVPR IEEE, P2427, DOI 10.1109/CVPR.2013.314
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Jiang B, 2019, NEUROCOMPUTING, V339, P139, DOI 10.1016/j.neucom.2019.01.102
   Jiang B, 2018, INT C PATT RECOG, P2118, DOI 10.1109/ICPR.2018.8546072
   Jiang B, 2018, LECT NOTES COMPUT SC, V11164, P313, DOI 10.1007/978-3-030-00776-8_29
   Kim HU, 2015, IEEE I CONF COMP VIS, P3011, DOI 10.1109/ICCV.2015.345
   Kristan M, 2015, P IEEE INT C COMPUTE, P1
   Kristan M, 2017, IEEE INT CONF COMP V, P1949, DOI 10.1109/ICCVW.2017.230
   Kristan M, 2016, LECT NOTES COMPUT SC, V9914, P777, DOI 10.1007/978-3-319-48881-3_54
   Lafferty J. D., 2003, P INT C MACH LEARN, P912, DOI DOI 10.5555/3041838.3041953
   Li AN, 2016, IEEE T PATTERN ANAL, V38, P335, DOI 10.1109/TPAMI.2015.2417577
   Li CL, 2017, AAAI CONF ARTIF INTE, P4126
   Li CL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P252, DOI 10.1145/3123266.3123288
   Li JT, 2017, IEEE T IMAGE PROCESS, V26, P2736, DOI 10.1109/TIP.2017.2686601
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Lin Z., 2011, ADV NEURAL INFORM PR, V24, P612, DOI DOI 10.1007/S11263-013-0611-6
   Liu FH, 2017, IEEE T MULTIMEDIA, V19, P2680, DOI 10.1109/TMM.2017.2708424
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Mueller M, 2017, PROC CVPR IEEE, P1387, DOI 10.1109/CVPR.2017.152
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Yang F, 2014, IEEE T IMAGE PROCESS, V23, P1639, DOI 10.1109/TIP.2014.2300823
   Yuan Y, 2015, IEEE T MULTIMEDIA, V17, P1125, DOI 10.1109/TMM.2015.2440996
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang YMZ, 2013, PROC CVPR IEEE, P676, DOI 10.1109/CVPR.2013.93
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhuang LS, 2015, IEEE T IMAGE PROCESS, V24, P3717, DOI 10.1109/TIP.2015.2441632
   Zhuang LS, 2012, PROC CVPR IEEE, P2328, DOI 10.1109/CVPR.2012.6247944
NR 42
TC 9
Z9 9
U1 3
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2162
EP 2171
DI 10.1109/TMM.2020.3008035
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100025
DA 2024-07-18
ER

PT J
AU Li, G
   Wang, XC
   Hu, RM
   Zhang, HY
   Ke, SF
AF Li, Gang
   Wang, Xiaochen
   Hu, Ruimin
   Zhang, Huyin
   Ke, Shanfa
TI Intelligibility Enhancement Via Normal-to-Lombard Speech Conversion With
   Long Short-Term Memory Network and Bayesian Gaussian Mixture Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Environmental noise; feature mapping; intelligibility enhancement
   (IENH); lombard effect; speech conversion
ID GENDER RECOGNITION; NOISE; SPEAKING; VOCODER
AB Speech communications and interactions frequently occur in a variety of environments. Noise in the environment significantly degrades speech intelligibility when speaking and listening. Especially in the listening stage, even if the multimedia terminal outputs clean speech, it is still difficult for listeners to obtain information. Intelligibility enhancement (IENH) of speech is a technique for overcoming the environmental noise in the listening stage. It implements a perceptual enhancement of non-noisy speech. This study focuses on IENH via normal-to-Lombard speech conversion, inspired by a well known acoustic mechanism named the Lombard effect. Our method combines the long short-term memory (LSTM) network and Bayesian Gaussian mixture model (BGMM) to build a conversion architecture. Compared with baselines, it has three main advantages: 1) an LSTM network is used for spectral tilt mapping with fully considering short-term correlations and high-dimensional expression abilities; 2) the aperiodicity (AP) is mapped together with the fundamental frequency (F-0) by a BGMM, which considers their relevance constraints and the importance of APs; 3) the gender-dependent mapping is used for F-0 and APs to consider distribution differences between genders. Experiments indicate that our method gets better performance in both objective and subjective tests.
C1 [Li, Gang; Wang, Xiaochen; Hu, Ruimin; Ke, Shanfa] Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Li, Gang; Hu, Ruimin] Wuhan Univ, Hubei Key Lab Multimedia & Network Commun Engn, Wuhan 430072, Peoples R China.
   [Wang, Xiaochen; Zhang, Huyin] Wuhan Univ Shenzhen, Res Inst, Shenzhen 518057, Peoples R China.
C3 Wuhan University; Wuhan University; Wuhan University
RP Hu, RM (corresponding author), Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Sch Comp Sci, Wuhan 430072, Peoples R China.
EM ligang10@whu.edu.cn; clowang@whu.edu.cn; hrm@whu.edu.cn;
   zhy2536@whu.edu.cn; shanfa_ke@whu.edu.cn
OI Hu, Ruimin/0000-0002-0290-5757
FU National Natural Science Foundation of China [61801334]; Basic Research
   Project of Science and Technology Plan of Shenzhen
   [JCYJ20170818143246278]; National Key Research and Development Program
   of China [2017YFB1002803]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61801334, in part by the Basic Research
   Project of Science and Technology Plan of Shenzhen under Grant
   JCYJ20170818143246278, and in part by National Key Research and
   Development Program of China under Grant 2017YFB1002803.
CR Alghamdi N, 2018, J ACOUST SOC AM, V143, pEL523, DOI 10.1121/1.5042758
   ANI, 1997, AM NAT STANDARD I, V3
   [Anonymous], 1996, P 800 METHODS SUBJEC
   [Anonymous], 2014, P INTERSPEECH
   Barker J, 2007, SPEECH COMMUN, V49, P402, DOI 10.1016/j.specom.2006.11.003
   Bishop C, 2007, RECOGNITION PATTERN
   CHILDERS DG, 1991, J ACOUST SOC AM, V90, P1841, DOI 10.1121/1.401664
   Cooke M, 2006, J ACOUST SOC AM, V119, P1562, DOI 10.1121/1.2166600
   Cooke M., 2013, Proceedings of interspeech, P3552, DOI DOI 10.21437/INTERSPEECH.2013-764
   Cooke M, 2014, COMPUT SPEECH LANG, V28, P543, DOI 10.1016/j.csl.2013.08.003
   Cooke M, 2010, J ACOUST SOC AM, V128, P2059, DOI 10.1121/1.3478775
   Ellis D., 2003, DYNAMIC TIMEWARP DTW
   European Telecommunications Standards Institute (ETSI), 2009, EG 202 3961 SPEECH M
   Fukada T., 1992, ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech and Signal Processing (Cat. No.92CH3103-9), P137, DOI 10.1109/ICASSP.1992.225953
   Garnier M, 2014, COMPUT SPEECH LANG, V28, P580, DOI 10.1016/j.csl.2013.07.005
   Gómez AM, 2011, IEEE T MULTIMEDIA, V13, P894, DOI 10.1109/TMM.2011.2156773
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   ITU-T, 2001, G 114 ONE WAY TRANSM
   Jokinen E, 2017, IEEE-ACM T AUDIO SPE, V25, P1985, DOI 10.1109/TASLP.2017.2740004
   Jokinen E, 2017, J ACOUST SOC AM, V141, pEL327, DOI 10.1121/1.4979162
   Junqua JC, 1996, SPEECH COMMUN, V20, P13, DOI 10.1016/S0167-6393(96)00041-6
   Junqua JC, 1999, INT CONF ACOUST SPEE, P2083, DOI 10.1109/ICASSP.1999.758343
   Kawahara H, 1999, SPEECH COMMUN, V27, P187, DOI 10.1016/S0167-6393(98)00085-5
   Kleijn WB, 2015, IEEE SIGNAL PROC MAG, V32, P43, DOI 10.1109/MSP.2014.2365594
   Koutsogiannaki M, 2017, INTERSPEECH, P1973, DOI 10.21437/Interspeech.2017-1157
   Li G, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102857
   Lombard E., 1911, Ann. Maladies Oreille, Larynx, Nez, Pharynx, V37, P101, DOI DOI 10.1145/1168987.1169028
   López AR, 2017, INTERSPEECH, P1363, DOI 10.21437/Interspeech.2017-400
   Lu YY, 2009, SPEECH COMMUN, V51, P1253, DOI 10.1016/j.specom.2009.07.002
   Mäkinen J, 2005, INT CONF ACOUST SPEE, P1109
   Marxer R, 2016, J ACOUST SOC AM, V140, pEL458, DOI 10.1121/1.4967185
   Mohammadi SH, 2017, SPEECH COMMUN, V88, P65, DOI 10.1016/j.specom.2017.01.008
   Morise M, 2017, INTERSPEECH, P2321, DOI 10.21437/Interspeech.2017-68
   Morise M, 2016, SPEECH COMMUN, V84, P57, DOI 10.1016/j.specom.2016.09.001
   Morise M, 2016, IEICE T INF SYST, VE99D, P1877, DOI 10.1587/transinf.2015EDP7457
   Morise M, 2015, SPEECH COMMUN, V67, P1, DOI 10.1016/j.specom.2014.09.003
   Oord A., 2016, ARXIV160903499
   Petkov PN, 2015, IEEE-ACM T AUDIO SPE, V23, P327, DOI 10.1109/TASLP.2014.2384271
   Rabiner LR, 2011, Theory and applications of digital speech processing
   Seshadri S, 2019, INTERSPEECH, P2838, DOI 10.21437/Interspeech.2019-1681
   Seshadri S, 2019, INT CONF ACOUST SPEE, P6835, DOI [10.1109/icassp.2019.8682648, 10.1109/ICASSP.2019.8682648]
   Seshadri S, 2019, IEEE ACCESS, V7, P17230, DOI 10.1109/ACCESS.2019.2895923
   Soloducha M., 2016, PROC IEEE SPEECH COM, P1
   Taal CH, 2014, COMPUT SPEECH LANG, V28, P858, DOI 10.1016/j.csl.2013.11.003
   Van Kuyk S, 2018, IEEE-ACM T AUDIO SPE, V26, P2153, DOI 10.1109/TASLP.2018.2856374
   Van Kuyk S, 2018, IEEE SIGNAL PROC LET, V25, P115, DOI 10.1109/LSP.2017.2774250
   VARGA A, 1993, SPEECH COMMUN, V12, P247, DOI 10.1016/0167-6393(93)90095-3
   WU K, 1991, J ACOUST SOC AM, V90, P1828, DOI 10.1121/1.401663
   Zorila TC, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P634
NR 50
TC 2
Z9 2
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3035
EP 3047
DI 10.1109/TMM.2021.3068565
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000007
DA 2024-07-18
ER

PT J
AU Mesgaran, M
   Ben Hamzae, A
AF Mesgaran, Mahsa
   Ben Hamzae, A.
TI Anisotropic Graph Convolutional Network for Semi-Supervised Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolution; Task analysis; Laplace equations; Smoothing methods;
   Semisupervised learning; Anisotropic magnetoresistance; Geometry;
   Network embedding; graph convolutional networks; anisotropic diffusion;
   classification
ID CLASSIFICATION; RETRIEVAL
AB Graph convolutional networks learn effective node embeddings that have proven to be useful in achieving high-accuracy prediction results in semi-supervised learning tasks, such as node classification. However, these networks suffer from the issue of over-smoothing and shrinking effect of the graph due in large part to the fact that they diffuse features across the edges of the graph using a linear Laplacian flow. This limitation is especially problematic for the task of node classification, where the goal is to predict the label associated with a graph node. To address this issue, we propose an anisotropic graph convolutional network for semi-supervised node classification by introducing a nonlinear function that captures informative features from nodes, while preventing oversmoothing. The proposed framework is largely motivated by the good performance of anisotropic diffusion in image and geometry processing, and learns nonlinear representations based on local graph structure and node features. The effectiveness of our approach is demonstrated on three citation networks and two image datasets, achieving better or comparable classification accuracy results compared to the standard baseline methods.
C1 [Mesgaran, Mahsa; Ben Hamzae, A.] Concordia Univ, Concordia Inst Informat Syst Engn, Montreal, PQ H3G 1M8, Canada.
C3 Concordia University - Canada
RP Ben Hamzae, A (corresponding author), Concordia Univ, Concordia Inst Informat Syst Engn, Montreal, PQ H3G 1M8, Canada.
EM mahsa.mesgaran@gmail.com; hamza@ciise.concordia.ca
RI Hamza, Abdessamad Ben/G-4571-2013
OI Ben Hamza, Abdessamad/0000-0002-3778-8167
FU Natural Sciences and Engineering Research Council of Canada [N00929]
FX This work was supported in part by Natural Sciences and Engineering
   Research Council of Canada through Discovery Grant N00929. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Xin Geng. (Corresponding author: Abdessamad
   BenHamza.)
CR Aggarwal CC, 2011, SOCIAL NETWORK DATA ANALYTICS, P1, DOI 10.1007/978-1-4419-8462-3
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Biasotti S, 2016, VISUAL COMPUT, V32, P217, DOI 10.1007/s00371-015-1146-3
   Black MJ, 1998, IEEE T IMAGE PROCESS, V7, P421, DOI 10.1109/83.661192
   Bresson Xavier, 2018, ARXIV171107553
   Chen YT, 2019, IEEE T MULTIMEDIA, V21, P704, DOI 10.1109/TMM.2018.2865860
   Defferrard M, 2016, ADV NEUR IN, V29
   Gao HY, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1416, DOI 10.1145/3219819.3219947
   Gao JY, 2020, IEEE T MULTIMEDIA, V22, P3088, DOI 10.1109/TMM.2020.2969787
   Goyal P, 2018, KNOWL-BASED SYST, V151, P78, DOI 10.1016/j.knosys.2018.03.022
   Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754
   Guo S., 2013, PROC AAAI C ARTIF IN, P922
   Hamilton WL, 2017, ADV NEUR IN, V30
   Hamilton William L., 2017, IEEE Data Eng. Bull
   Huang FL, 2017, IEEE T MULTIMEDIA, V19, P1881, DOI 10.1109/TMM.2017.2692650
   Huang W., 2018, PROC ADV NEURAL INF, P1
   Kingma D. P., 2015, PROC INT C LEARN REP, P1
   Kipf TN, 2017, INT C LEARN REPR
   Ktena SI, 2018, NEUROIMAGE, V169, P431, DOI 10.1016/j.neuroimage.2017.12.052
   Li QM, 2018, AAAI CONF ARTIF INTE, P3538
   Li Y., 2016, ICLR, P1, DOI DOI 10.48550/ARXIV.1511.05493
   Liao R., 2019, ICLR
   Lu LH, 2020, IEEE T MULTIMEDIA, V22, P524, DOI 10.1109/TMM.2019.2930344
   Masoumi M, 2017, J VIS COMMUN IMAGE R, V43, P198, DOI 10.1016/j.jvcir.2017.01.001
   Masoumi M, 2016, PATTERN RECOGN LETT, V83, P339, DOI 10.1016/j.patrec.2016.04.009
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Ortega A, 2018, P IEEE, V106, P808, DOI 10.1109/JPROC.2018.2820126
   Peng QM, 2019, IEEE T MULTIMEDIA, V21, P3083, DOI 10.1109/TMM.2019.2918730
   Perozzi B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P701, DOI 10.1145/2623330.2623732
   Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Velicckovicet P., 2018, P INT C LEARN REPR
   Velickovic P., 2019, P ICLR, P1
   Wang S., 2013, P INT JOINT C ART IN, P3762
   Weickert J., 1996, ANISOTROPIC DIFFUSIO
   Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34
   Weston J., 2008, P 25 INT C MACH LEAR, P1168
   Wu F, 2019, PR MACH LEARN RES, V97
   Wu X. M., 2012, NIPS, P3077
   Xu B., 2019, P INT C LEARN REPR
   Xu K, 2019, IEEE VTS VEH TECHNOL, DOI [10.1109/vtcfall.2019.8891597, 10.1109/biocas.2019.8918711]
   Xu KYL, 2018, PR MACH LEARN RES, V80
   Xu L, 2019, IEEE T MULTIMEDIA, V21, P591, DOI 10.1109/TMM.2018.2887019
   Yang Z, 2016, PR MACH LEARN RES, V48
   Zhang Y, 2007, IEEE T IMAGE PROCESS, V16, P1036, DOI 10.1109/TIP.2007.891787
   Zhao L., 2020, 8 INT C LEARN REPR I
   Zhuang CY, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P499, DOI 10.1145/3178876.3186116
NR 48
TC 3
Z9 3
U1 3
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3931
EP 3942
DI 10.1109/TMM.2020.3034530
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ouyang, JB
   Zhou, WG
   Wang, M
   Tian, Q
   Li, HQ
AF Ouyang, Jianbo
   Zhou, Wengang
   Wang, Min
   Tian, Qi
   Li, Houqiang
TI Collaborative Image Relevance Learning for Visual Re-Ranking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Image retrieval; Computational modeling; Correlation;
   Feature extraction; Semantics; Deep learning; Image retrieval;
   re-ranking
ID OBJECT RETRIEVAL; ACCURATE; FEATURES; MODEL
AB In content-based image retrieval, the initial retrieval result may be unsatisfactory, which can be refined with visual re-ranking techniques, such as query expansion, geometric verification, etc. In this work, we approach visual re-ranking from a novel perspective. Observing that the contextual similarity of images from a retrieval result list exhibits strong visual relevance, we propose to collaboratively learn the semantic relevance among images for visual re-ranking. In our approach, we represent the image set of a fixed-length retrieval list into a correlation matrix, and learn the relevance of all image pairs simultaneously with a lightweight CNN model. To optimize the CNN model, a weighted MSE loss is defined, which takes into account the sparsity of labels. To find the optimal length of retrieval result list for different queries, we present a query sensitive selection method. We conduct comprehensive experiments on five benchmark datasets, and demonstrate the generality, and effectiveness of the proposed visual re-ranking method.
C1 [Ouyang, Jianbo; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
   [Wang, Min] Huawei Noahs Ark Lab, Hefei 230000, Peoples R China.
   [Tian, Qi] Huawei Cloud, Shenzhen 518000, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Huawei Technologies; Huawei Technologies
RP Zhou, WG; Li, HQ (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
EM ouyjb@mail.ustc.edu.cn; zhwg@ustc.edu.cn; wm123@mail.ustc.edu.cn;
   tian.qi1@huawei.com; lihq@ustc.edu.cn
RI Li, Houqiang Li/B-6259-2013
OI Wang, Min/0000-0003-3048-6980
FU National Key R&D Program of China [2018YFB1402600]; National Natural
   Science Foundation of China [61702009, 61822208, 61632019]; Youth
   Innovation Promotion Association CAS [2018497]; NSFC [61836011]; GPU
   cluster
FX The work of W. Zhou was supported in part by the National Key R&D
   Program of China under Contract 2018YFB1402600, in part by the National
   Natural Science Foundation of China under Contracts 61702009, 61822208,
   and 61632019, and in part by Youth Innovation Promotion Association CAS
   2018497. The work of H. Li was supported by NSFC under Contract
   61836011. This work was supported by GPU cluster built by MCC Laboratory
   of Information Science, and Technology Institution, USTC. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Zhu Liu. (Corresponding authors: Wengang Zhou;
   Houqiang Li.)
CR Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Avrithis Y, 2014, INT J COMPUT VISION, V107, P1, DOI 10.1007/s11263-013-0659-3
   Babenko A, 2015, IEEE I CONF COMP VIS, P1269, DOI 10.1109/ICCV.2015.150
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Bai S, 2019, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2019.00083
   Bai S, 2019, IEEE T PATTERN ANAL, V41, P1213, DOI 10.1109/TPAMI.2018.2828815
   Bai S, 2017, AAAI CONF ARTIF INTE, P3967
   Cao Y, 2016, AAAI CONF ARTIF INTE, P3457
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2011, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2011.5995601
   Donoser M, 2013, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2013.174
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gao ZN, 2016, IEEE T MULTIMEDIA, V18, P1661, DOI 10.1109/TMM.2016.2568748
   González-Díaz I, 2017, IEEE T MULTIMEDIA, V19, P544, DOI 10.1109/TMM.2016.2616298
   Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8
   Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Iscen A, 2017, PROC CVPR IEEE, P926, DOI 10.1109/CVPR.2017.105
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jegou H, 2007, PROC CVPR IEEE, P9
   Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121
   Liu Z, 2015, IEEE T MULTIMEDIA, V17, P538, DOI 10.1109/TMM.2015.2399851
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Min WQ, 2020, IEEE T MULTIMEDIA, V22, P3128, DOI 10.1109/TMM.2020.2974326
   Nister David, 2006, CVPR
   Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374
   Pang SM, 2019, IEEE T MULTIMEDIA, V21, P1513, DOI 10.1109/TMM.2018.2876833
   Pang SM, 2019, IEEE T MULTIMEDIA, V21, P760, DOI 10.1109/TMM.2018.2866230
   Pedronette D. C. G., 2010, Proceedings of the 23rd SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI 2010), P1, DOI 10.1109/SIBGRAPI.2010.9
   Qin DF, 2011, PROC CVPR IEEE, P777, DOI 10.1109/CVPR.2011.5995373
   Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566
   Radenovic F, 2018, PROC CVPR IEEE, P5706, DOI 10.1109/CVPR.2018.00598
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Revaud J, 2019, IEEE I CONF COMP VIS, P5106, DOI 10.1109/ICCV.2019.00521
   Shen XH, 2012, PROC CVPR IEEE, P3013, DOI 10.1109/CVPR.2012.6248031
   Siméoni O, 2019, PROC CVPR IEEE, P11643, DOI 10.1109/CVPR.2019.01192
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Song Bai, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P774, DOI 10.1109/ICCV.2017.90
   Song JK, 2018, AAAI CONF ARTIF INTE, P394
   Sun SY, 2018, IEEE T IMAGE PROCESS, V27, P6124, DOI 10.1109/TIP.2018.2864919
   Tian Xinmei, 2011, P ACM INT C MULT, P363
   Tolias G., 2016, Conference Track Proceedings,
   Xu J, 2018, AAAI CONF ARTIF INTE, P7436
   Yang F, 2019, AAAI CONF ARTIF INTE, P9087
   Yang XW, 2013, IEEE T PATTERN ANAL, V35, P28, DOI 10.1109/TPAMI.2012.60
   Zhang ZZ, 2019, IEEE T MULTIMEDIA, V21, P2878, DOI 10.1109/TMM.2019.2915036
   Zheng L, 2018, IEEE T PATTERN ANAL, V40, P1224, DOI 10.1109/TPAMI.2017.2709749
   Zhou DY, 2004, ADV NEUR IN, V16, P169
   Zhou W., 2010, P 18 ACM INT C MULTI, P511
   Zhou WG, 2018, IEEE T PATTERN ANAL, V40, P1154, DOI 10.1109/TPAMI.2017.2676779
   Zhou WG, 2014, IEEE T MULTIMEDIA, V16, P601, DOI 10.1109/TMM.2014.2301979
NR 53
TC 6
Z9 6
U1 2
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3646
EP 3656
DI 10.1109/TMM.2020.3029886
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100017
DA 2024-07-18
ER

PT J
AU Sang, L
   Xu, M
   Qian, SS
   Martin, M
   Li, P
   Wu, XD
AF Sang, Lei
   Xu, Min
   Qian, Shengsheng
   Martin, Matt
   Li, Peter
   Wu, Xindong
TI Context-Dependent Propagating-Based Video Recommendation in Multimodal
   Heterogeneous Information Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Collaboration; YouTube; Australia; Visualization; Context
   modeling; Video recommendation; context-dependent propagating;
   Heterogeneous Information Network (HIN); Network embedding
AB With the emergence of online social networks (OSNs), video recommendation has come to play a crucial role in mitigating the semantic gap between users and videos. Conventional approaches to video recommendation primarily focus on exploiting content features or simple user-video interactions to model the users' preferences. Although these methods have achieved promising results, they fail to model the complex video context interdependency, which is obscure/hidden in heterogeneous auxiliary data from OSNs. In this paper, we study the problem of video recommendation in Heterogeneous Information Networks (HINs) due to its excellence in characterizing heterogeneous and complex context information. We propose a Context-Dependent Propagating Recommendation network (CDPRec) to obtain accurate video embedding and capture global context cues among videos in HINs. The CDPRec can iteratively propagate the contexts of a video along links in a graph-structured HIN and explore multiple types of dependencies among the surrounding video nodes. Then, each video is represented as the composition of the multimodal content feature and global dependency structure information using an attention network. The learned video embedding with sequential based recommendation are jointly optimized for the final rating prediction. Experimental results on real-world YouTube video recommendation scenarios demonstrate the effectiveness of the proposed methods compared with strong baselines.
C1 [Sang, Lei; Wu, Xindong] Hefei Univ Technol, Key Lab Knowledge Engn Big Data, Minist Educ, Hefei 230009, Peoples R China.
   [Sang, Lei; Xu, Min] Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
   [Qian, Shengsheng] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
   [Martin, Matt; Li, Peter] INTERACT Technol, Sydney, NSW 2000, Australia.
   [Wu, Xindong] Mininglamp Acad Sci, Mininglamp Technol, Beijing 100084, Peoples R China.
C3 Hefei University of Technology; University of Technology Sydney; Chinese
   Academy of Sciences; Institute of Automation, CAS
RP Xu, M (corresponding author), Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
EM lei.sang@student.uts.edu.au; Min.Xu@uts.edu.au;
   shengsheng.qian@nlpr.ia.ac.cn; matt@interact.technology;
   peter@interact.technology; xwu@hfut.edu.cn
RI Wu, Xindong/AAB-6713-2022
OI Wu, Xindong/0000-0003-2396-1704
FU National Key Research and Development Program of China [2016YFB1000901];
   China Scholarship Council (CSC); National Natural Science Foundation of
   China [91746209]; Program for Changjiang Scholars and Innovative
   Research Team in University (PCSIRT) of theMinistry of Education of
   China [IRT17R32]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2016YFB1000901, in part by the
   China Scholarship Council (CSC), in part by the National Natural Science
   Foundation of China under Grant 91746209, and in part by the Program for
   Changjiang Scholars and Innovative Research Team in University (PCSIRT)
   of theMinistry of Education of China under Grant IRT17R32.
CR [Anonymous], 2012, P 20 ACM INT C MULT
   [Anonymous], 2018, INT C LEARN REPR
   [Anonymous], CONVOLUTIONAL NEURAL
   Baluja S, 2008, WORLD WID WEB C, P895
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Cui LZ, 2017, CONCURR COMP-PRACT E, V29, DOI 10.1002/cpe.3900
   Cui P, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P597, DOI 10.1145/2647868.2654946
   Davidson James, 2010, P 4 ACM C REC SYST, P293, DOI [DOI 10.1145/1864708.1864770, 10.1145/1864708]
   Dong YX, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P135, DOI 10.1145/3097983.3098036
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Ferracani A, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P351, DOI 10.1145/2911996.2912066
   Fu TY, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1797, DOI 10.1145/3132847.3132953
   Gao JY, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P127, DOI 10.1145/3123266.3123433
   Hamilton WL, 2017, ADV NEUR IN, V30
   He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569
   Hu BB, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1531, DOI 10.1145/3219819.3219965
   Huang Q., 2014, ACMTRANS MULTIMEDIA, V10, P1
   Huang XW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P548, DOI 10.1145/3343031.3350893
   Huang XW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P447, DOI 10.1145/3240508.3240609
   Huang YX, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P35, DOI 10.1145/2882903.2903743
   Karypis G., 2001, Proceedings of the 2001 ACM CIKM. Tenth International Conference on Information and Knowledge Management, P247, DOI 10.1145/502585.502627
   Kipf TN, 2016, ARXIV
   Koren Y., 2008, P 14 ACM SIGKDD INT, P426
   Kumar BGV, 2016, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2016.581
   Lü LY, 2011, PHYSICA A, V390, P1150, DOI 10.1016/j.physa.2010.11.027
   Ma XQ, 2014, MULTIMEDIA SYST, V20, P675, DOI 10.1007/s00530-013-0309-1
   Ma ZG, 2012, IEEE T MULTIMEDIA, V14, P1662, DOI 10.1109/TMM.2012.2199293
   Mei T, 2011, ACM T INFORM SYST, V29, DOI 10.1145/1961209.1961213
   Mihalcea, 2004, P EMNLP, P401, DOI DOI 10.3115/1219044.1219064
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Perozzi B., 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732
   Qian SS, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P99, DOI 10.1145/2733373.2806234
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Rendle S., 2009, P 25 C UNCERTAINTY A, P452
   Ricci F, 2011, RECOMMENDER SYSTEMS HANDBOOK, P1, DOI 10.1007/978-0-387-85820-3_1
   Roy SD, 2013, IEEE T MULTIMEDIA, V15, P1255, DOI 10.1109/TMM.2013.2265079
   Sang L, 2019, LECT NOTES ARTIF INT, V11441, P3, DOI 10.1007/978-3-030-16142-2_1
   Sang L, 2019, NEUROCOMPUTING, V334, P44, DOI 10.1016/j.neucom.2018.12.067
   Shi C, 2019, IEEE T KNOWL DATA EN, V31, P357, DOI 10.1109/TKDE.2018.2833443
   Simonyan K., 2014, CORR
   Singh A, 2008, INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, P650
   Sun LF, 2017, IEEE T MULTIMEDIA, V19, P609, DOI 10.1109/TMM.2016.2635589
   Sunt YZ, 2011, PROC VLDB ENDOW, V4, P992
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang H, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1235, DOI 10.1145/2783258.2783273
   Wang X, 2017, AAAI CONF ARTIF INTE, P203
   Wang Z, 2013, IEEE T MULTIMEDIA, V15, P698, DOI 10.1109/TMM.2012.2237022
   Wu LX, 2020, IEEE T MULTIMEDIA, V22, P808, DOI 10.1109/TMM.2019.2931815
   Yan M, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P557, DOI 10.1145/2647868.2654920
   Yu X, 2013, P IJCAI HINA WORKSH
   Yu X, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P283, DOI 10.1145/2556195.2556259
   Zang XF, 2015, INT CONF INFRA MILLI
   Zhang YY, 2014, AAAI CONF ARTIF INTE, P1369
   Zhao Xiaojian, 2011, P 19 ACM INT C MULTI, P1521
   Zhao Z, 2018, IEEE T MULTIMEDIA, V20, P430, DOI 10.1109/TMM.2017.2740022
   Zhou Renjie, 2010, P 10 ACM SIGCOMM C I, P404, DOI DOI 10.1145/1879141.1879193
   Zhou Sheng, 2019, ARXIV190201475
NR 57
TC 28
Z9 28
U1 5
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2019
EP 2032
DI 10.1109/TMM.2020.3007330
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100014
DA 2024-07-18
ER

PT J
AU Wang, JY
   Xu, M
   Jiang, L
   Song, YH
AF Wang, Jianyi
   Xu, Mai
   Jiang, Lai
   Song, Yuhang
TI Attention-Based Deep Reinforcement Learning for Virtual Cinematography
   of 360° Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 360 degrees video; attention; deep reinforcement learning
ID SALIENCY PREDICTION; MODEL; IMAGES; HEAD; EYE; 2D
AB Virtual cinematography refers to automatically selecting a natural-looking normal field-of-view (NFOV) from an entire 360 degrees video. In fact, virtual cinematography can be modeled as a deep reinforcement learning (DRL) problem, in which an agent makes actions related to NFOV selection according to the environment of 360 degrees video frames. More importantly, we find from our data analysis that the selected NFOVs attract significantly more attention than other regions, i.e., the NFOVs have high saliency. Therefore, in this paper, we propose an attention based DRL (A-DRL) approach for virtual cinematography in 360 degrees video. Specifically, we develop a new DRL framework for automatic NFOV selection with the input of both the content, and saliency map of each 360 degrees frame. Then, we propose a new reward function for the DRL framework in our approach, which considers the saliency values, ground-truth, and smooth transition for NFOV selection. Subsequently, a simplified DenseNet (called Mini-DenseNet) is designed to learn the optimal policy via maximizing the reward. Based on the learned policy, the actions of NFOV can be made in our A-DRL approach for virtual cinematography of 360 degrees video. Extensive experiments show that our A-DRL approach outperforms other state-of-the-art virtual cinematography methods, over the datasets of Sports-360 video, and Pano2Vid.
C1 [Wang, Jianyi; Xu, Mai; Jiang, Lai] Beihang Univ, Sch Elect & Informat Engn, Beijing 100191, Peoples R China.
   [Song, Yuhang] Univ Oxford, Somerville Coll, Dept Comp Sci, Oxford OX2 6HD, England.
C3 Beihang University; University of Oxford
RP Xu, M (corresponding author), Beihang Univ, Sch Elect & Informat Engn, Beijing 100191, Peoples R China.
EM iceclearwjy@buaa.edu.cn; MaiXu@buaa.edu.cn; jianglai.china@buaa.edu.cn;
   yuhang.song@some.ox.ac.uk
RI Wang, Jianyi/ADL-2553-2022
OI Wang, Jianyi/0000-0001-7025-3626; Song, Yuhang/0000-0002-7999-0291
FU NSFC [61876013, 61922009, 61573037]; Beijing Natural Science Foundation
   [JQ20019]
FX This work was supported in part by the NSFC Projects under Grants
   61876013, 61922009, and 61573037 and in part by Beijing Natural Science
   Foundation under Grant JQ20019. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Xiaoqing Zhu
CR Nguyen A, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1190, DOI 10.1145/3240508.3240669
   [Anonymous], 2017, P 27 NOSSDAV, DOI DOI 10.1145/3083165.3083180
   [Anonymous], 2018, IEEE INT CONF MULTI
   [Anonymous], 2015, P 31 SPRING C COMPUT, DOI DOI 10.1145/2788539.2788549
   Bak C, 2018, IEEE T MULTIMEDIA, V20, P1688, DOI 10.1109/TMM.2017.2777665
   Bazzani L., 2017, INT C LEARN REPR
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Butterworth S., 1930, EXP WIREL WIREL ENG, V7, P536
   Chen JH, 2016, PROC CVPR IEEE, P4688, DOI 10.1109/CVPR.2016.507
   Chen JH, 2015, IEEE WINT CONF APPL, P215, DOI 10.1109/WACV.2015.36
   Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   Ding XY, 2019, IEEE T MULTIMEDIA, V21, P124, DOI 10.1109/TMM.2018.2851389
   Elson David, 2007, P AAAI C ARTIFICIAL, V3, P8
   Fang YM, 2012, IEEE T MULTIMEDIA, V14, P187, DOI 10.1109/TMM.2011.2169775
   Foote J, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1419, DOI 10.1109/ICME.2000.871033
   Hu HN, 2017, PROC CVPR IEEE, P1396, DOI 10.1109/CVPR.2017.153
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38
   Itti L, 2003, PROC SPIE, V5200, P64, DOI 10.1117/12.512618
   Jiang L., 2018, P EUR C COMP VIS ECC, P602
   King DB, 2015, ACS SYM SER, V1214, P1
   Kombrink S, 2011, 12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5, P2888
   Lebreton P, 2018, SIGNAL PROCESS-IMAGE, V69, P69, DOI 10.1016/j.image.2018.03.006
   Lee S, 2018, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2018.00153
   Lee SH, 2014, IEEE IMAGE PROC, P1120, DOI 10.1109/ICIP.2014.7025223
   Li-Wei He, 1996, Computer Graphics Proceedings. SIGGRAPH '96, P217
   Ling J, 2018, SIGNAL PROCESS-IMAGE, V69, P60, DOI 10.1016/j.image.2018.03.007
   Littman ML, 2015, NATURE, V521, P445, DOI 10.1038/nature14540
   Liu YF, 2017, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2017.343
   Monroy R, 2018, SIGNAL PROCESS-IMAGE, V69, P26, DOI 10.1016/j.image.2018.05.005
   Neumann U., 2000, MULTIMEDIA 00 P 8 AC, P493
   Nguyen T.V., 2013, P 21 ACM INT C MULT, P987, DOI DOI 10.1145/2502081.2502128
   Palazzi A, 2019, IEEE T PATTERN ANAL, V41, P1720, DOI 10.1109/TPAMI.2018.2845370
   Pan J., 2017, PROC IEEE C COMPUT V
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren ZX, 2013, IEEE T IMAGE PROCESS, V22, P3120, DOI 10.1109/TIP.2013.2259837
   Schulman J., 2017, ARXIV
   Shi XJ, 2015, ADV NEUR IN, V28
   Snyder J. P., 1997, FLATTENING EARTH 200
   Strasburger H, 2011, J VISION, V11, DOI 10.1167/11.5.13
   Su YC, 2017, LECT NOTES COMPUT SC, V10114, P154, DOI 10.1007/978-3-319-54190-7_10
   Su YC, 2017, PROC CVPR IEEE, P1368, DOI 10.1109/CVPR.2017.150
   Sun XS, 2017, AAAI CONF ARTIF INTE, P274
   Suzuki T, 2018, IEEE SYS MAN CYBERN, P2079, DOI 10.1109/SMC.2018.00358
   Vig E, 2014, PROC CVPR IEEE, P2798, DOI 10.1109/CVPR.2014.358
   Vincenty T., 1975, SURV REV, V23, P88, DOI [10.1179/sre.1975.23.176.88, DOI 10.1179/SRE.1975.23.176.88]
   Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50
   Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941
   Xinding Sun, 2005, IEEE Transactions on Multimedia, V7, P981, DOI 10.1109/TMM.2005.854388
   Xu M, 2019, IEEE T PATTERN ANAL, V41, P2693, DOI 10.1109/TPAMI.2018.2858783
   Xu YY, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3887
   Yu Y, 2018, AAAI CONF ARTIF INTE, P7525
   Zhang ZH, 2018, LECT NOTES COMPUT SC, V11211, P504, DOI 10.1007/978-3-030-01234-2_30
   Zhong S.-h., 2013, AAAI Conference on Artificial Intelligence, P1063
   Zhu YC, 2018, SIGNAL PROCESS-IMAGE, V69, P15, DOI 10.1016/j.image.2018.05.010
NR 56
TC 5
Z9 5
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3227
EP 3238
DI 10.1109/TMM.2020.3021984
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000022
DA 2024-07-18
ER

PT J
AU Xiao, YQ
   Cai, ZCA
   Yuan, XX
AF Xiao, Youqing
   Cai, Zhanchuan
   Yuan, Xixi
TI YuvConv: Multi-Scale Non-Uniform Convolution Structure Based on YUV
   Color Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Tensile stress; Convolution; Machine learning; Spatial resolution;
   Computational modeling; Task analysis; Feature extraction; YuvConv;
   multi-scale convolution; non-uniform; Pelee; WideResnet
ID DEEP; NETWORKS
AB Since digital images are able to be encoded through the luminance-bandwidth-chrominance (YUV) mode, and the contribution of luminance information is greater than that of chrominance information for human visual perception, it can be inferred that the appropriate reduction of chrominance information in convolutional neural network does not disturb image object recognition. In this paper, we propose a new multi-scale non-uniform convolution called YuvConv, wherein the output feature map of the convolutional layer is regarded as an image. First, the output channels in the new convolution are divided into three kinds of components: Y, U, and V tensors. Then, the tensor Y is used to process luminance information, which is high-resolution and occupies more output channels. Next, the tensors U and V are low-resolution and use fewer channels to process chrominance information. Finally, the adjacent tensors (Y-U, Y-U-V, and U-V) are fused as the output of YuvConv. Experimental results indicate that the use of the YuvConv instead of the standard convolution can improve the performance of deep learning tasks, and it can also reduce memory consumption and computation cost.
C1 [Xiao, Youqing; Cai, Zhanchuan; Yuan, Xixi] Macau Univ Sci & Technol, Fac Informat Technol, Taipa 999078, Macao, Peoples R China.
C3 Macau University of Science & Technology
RP Cai, ZCA (corresponding author), Macau Univ Sci & Technol, Fac Informat Technol, Taipa 999078, Macao, Peoples R China.
EM 635196543@qq.com; zccai@must.edu.mo; yuanxixi.ok@gmail.com
OI Yuan, Xixi/0000-0001-7027-2253
FU National Basic Research Program of China (973 Program) [2011CB302400];
   Science and Technology Development Fund of Macau [0025/2019/AKP,
   0038/2020/A, 0012/2018/A1, 0052/2020/AFJ, 0069/2018/A2]; Major
   Scientific Research Project for Universities of Guangdong Province
   [2017KTSCX207]; Open Project Program of State Key Laboratory of Virtual
   Reality Technology and Systems, Beihang University [VRLAB2019C02]; Open
   Fund of State Key Laboratory of Remote Sensing Science [OFSLRSS201901];
   Open Project Program of the State Key Laboratory of CAD; CG at Zhejiang
   University [A1910]
FX This work was supported in part by the National Basic Research Program
   of China (973 Program) under Grant 2011CB302400, in part by the Science
   and Technology Development Fund of Macau under Grants 0025/2019/AKP,
   0038/2020/A, 0012/2018/A1, 0052/2020/AFJ, and 0069/2018/A2, in part by
   the Major Scientific Research Project for Universities of Guangdong
   Province under Grant 2017KTSCX207, in part by the Open Project Program
   of State Key Laboratory of Virtual Reality Technology and Systems,
   Beihang University, under Grant VRLAB2019C02, in part by the Open Fund
   of State Key Laboratory of Remote Sensing Science, under Grant
   OFSLRSS201901, in part by the Open Project Program of the State Key
   Laboratory of CAD, and in part by CG at Zhejiang University under Grant
   A1910.
CR A. C. A. B. B. W. A. Y. N. Yuval Netzer and TaoWang, 2011, READING DIGITS NATUR
   [Anonymous], 2016, LEARNING
   [Anonymous], 2017, Tiny imagenet
   Chen Chun-Fu, 2019, P INT C LEARN REPR
   Chen YP, 2019, IEEE I CONF COMP VIS, P3434, DOI 10.1109/ICCV.2019.00353
   Choi Y, 2020, IEEE J SEL TOPICS SI
   Dean J., 2015, NIPS DEEP LEARNING R
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Graham B., 2017, NEURAL EVOL COMPUT
   Gu K, 2020, IEEE T MULTIMEDIA, V22, P311, DOI 10.1109/TMM.2019.2929009
   Guo YW, 2016, ADV NEUR IN, V29
   Han S., 2016, ARXIV151000149
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   He YH, 2018, LECT NOTES COMPUT SC, V11211, P815, DOI 10.1007/978-3-030-01234-2_48
   He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155
   Howard A. G, 2017, COMPUT VISION PATTER
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu F, 2015, REMOTE SENS-BASEL, V7, P14680, DOI 10.3390/rs71114680
   Huang G, 2022, IEEE T PATTERN ANAL, V44, P8704, DOI 10.1109/TPAMI.2019.2918284
   Iandola F. N., 2016, ARXIV160207360, DOI 10.1007/978-3-319-24553-9
   Jin X., 2016, COMPUT VISION PATTER
   Ke TW, 2017, PROC CVPR IEEE, P4067, DOI 10.1109/CVPR.2017.433
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849
   Maninis KK, 2018, IEEE T PATTERN ANAL, V40, P819, DOI 10.1109/TPAMI.2017.2700300
   Rahman S, 2020, IEEE T MULTIMEDIA, V22, P242, DOI 10.1109/TMM.2019.2924511
   Ren MY, 2018, PROC CVPR IEEE, P8711, DOI 10.1109/CVPR.2018.00908
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sun K, 2019, ABS190404514 CORR, V4514
   Umuroglu Y, 2017, FPGA'17: PROCEEDINGS OF THE 2017 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS, P65, DOI 10.1145/3020078.3021744
   Wang HY, 2019, PROC CVPR IEEE, P2253, DOI 10.1109/CVPR.2019.00236
   Wang RJ, 2018, 32 C NEURAL INFORM P
   Wen W, 2016, ADV NEUR IN, V29
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2603, DOI 10.1109/TMM.2019.2904879
   Yang SF, 2015, IEEE I CONF COMP VIS, P1215, DOI 10.1109/ICCV.2015.144
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Zagoruyko S., 2016, BMVC, P1
   Zhao K, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1191
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
   Zhuang ZW, 2018, ADV NEUR IN, V31
NR 42
TC 0
Z9 0
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2533
EP 2544
DI 10.1109/TMM.2020.3013352
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800029
DA 2024-07-18
ER

PT J
AU Feng, TT
   Sun, HF
   Qi, Q
   Wang, JY
   Liao, JX
AF Feng, Tongtong
   Sun, Haifeng
   Qi, Qi
   Wang, Jingyu
   Liao, Jianxin
TI Vabis: Video Adaptation Bitrate System for Time-Critical Live Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bit rate; Streaming media; Servers; Quality of experience; Delays; Time
   factors; Training; Bitrate adaptation; live streaming; ultra-low
   latency; reinforcement learning
ID QUALITY
AB With the rise of time-critical and interactive scenarios, ultra-low latency has become the most urgent requirement. Adaptive bitrate (ABR) schemes have been widely used in reducing latency for live streaming services. However, the traditional solutions suffer from a key limitation: they only utilize coarsegrained chunk to solve the I-frame misalignment problem in different bitrate switching process at the cost of increasing latency. As a result, existing schemes are difficult to guarantee the timeliness and granularity of control in essence. In this paper, we use a frame-based approach to solve the I-frame misalignment problem and propose a video adaptation bitrate system (Vabis) in units of the frame for time-critical live streaming to obtain the optimal quality of experience (QoE). On the server-side, a Few-Wait ABR algorithm based on Reinforcement Learning (RL) is designed to adaptively select the bitrate of future frames by state information that can be observed, which can subtly solve the problem of I-frame misalignment. A rule-based ABR algorithm is designed to optimize the Vabis system for the weak network. On the client-side, three delay control mechanisms are designed to achieve frame-based fine-grained control. We construct a trace-driven simulator and the real live platform to evaluate the comprehensive live streaming performance. The results show that Vabis is significantly better than the existing methods with decreases in an average delay of 32%-77% and improvements in average QoE of 28-67%.
C1 [Feng, Tongtong; Sun, Haifeng; Qi, Qi; Wang, Jingyu; Liao, Jianxin] Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.
   [Feng, Tongtong; Sun, Haifeng; Qi, Qi; Wang, Jingyu; Liao, Jianxin] EBUPT COM, Beijing 100191, Peoples R China.
C3 Beijing University of Posts & Telecommunications
RP Qi, Q; Wang, JY (corresponding author), Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.; Qi, Q; Wang, JY (corresponding author), EBUPT COM, Beijing 100191, Peoples R China.
EM ftt@bupt.edu.cn; hfsun@bupt.edu.cn; qiqi8266@bupt.edu.cn;
   wangjingyu@bupt.edu.cn; liaojx@bupt.edu.cn
RI Feng, Tongtong/ABD-4886-2020; Li, Jiaai/JCO-0168-2023; Sun,
   Haifeng/Y-7829-2019; wang, jing/HJA-5384-2022; liu,
   jiajia/IUN-0901-2023; Wang, Jingyu/JFK-6346-2023; Tang,
   Wei/IZQ-1283-2023; LI, WEI/ISS-1208-2023
OI Feng, Tongtong/0000-0003-4734-5607; Wang, Jingyu/0000-0002-2182-2228;
   Sun, Haifeng/0000-0003-3072-7422; Qi, Qi/0000-0003-0829-4624
FU National Key R&D Program of China [2018YFB1800502]; National Natural
   Science Foundation of China [61671079, 61771068]; Beijing Municipal
   Natural Science Foundation [4182041]; Ministry of Education and China
   Mobile Joint Fund [MCM20180101]
FX This work was supported in part by the National Key R&D Program of China
   2018YFB1800502, in part by the National Natural Science Foundation of
   China under Grants 61671079 and 61771068, in part by the Beijing
   Municipal Natural Science Foundation under Grant 4182041, and in part by
   the Ministry of Education and China Mobile Joint Fund MCM20180101. The
   co-first author is H. Sun. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Mea Wang.
CR Akhshabi S., 2011, P ACM C MULT SYST
   Akhshabi S., 2013, P ACM WORKSH NETW OP
   [Anonymous], 2014, APPLE HTTP LIVE STRE
   [Anonymous], 2017, 23000192018 ISOIEC
   [Anonymous], 2013, ARXIV13050510
   [Anonymous], 2018, ACMMM, DOI DOI 10.1145/3240508.3240708
   Beben A, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P13, DOI 10.1145/2910017.2910596
   Bentaleb A, 2019, PROCEEDINGS OF THE 29TH ACM WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO (NOSSDAV'19), P7, DOI 10.1145/3304112.3325611
   Cicco L. D., 2011, P ACM SIGMM C MULT S
   Cisco, 2020, FOR METH 2016 2021
   De Cicco L, 2013, 2013 20TH INTERNATIONAL PACKET VIDEO WORKSHOP (PV)
   De Praeter J, 2017, IEEE T MULTIMEDIA, V19, P2252, DOI 10.1109/TMM.2017.2734330
   Dobrian F, 2013, COMMUN ACM, V56, P91, DOI 10.1145/2428556.2428577
   El Essaili A, 2018, IEEE INT SYM BROADB
   F. C. Commission, 2016, raw data-measuring broadband america
   Gao GY, 2018, IEEE T MULTIMEDIA, V20, P3399, DOI 10.1109/TMM.2018.2838330
   He QY, 2016, IEEE T MULTIMEDIA, V18, P916, DOI 10.1109/TMM.2016.2544698
   Hu YC, 2016, IEEE T MULTIMEDIA, V18, P840, DOI 10.1109/TMM.2016.2538721
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Huang TC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1208, DOI 10.1145/3240508.3240545
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Kämäräinen T, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1181, DOI 10.1145/3240508.3240620
   Krishnan SS, 2013, IEEE ACM T NETWORK, V21, P2001, DOI 10.1109/TNET.2013.2281542
   Li B, 2008, GLOB TELECOMM CONF, DOI 10.1109/GLOCOM.2008.ECP.339
   Liu FM, 2012, IEEE T PARALL DISTR, V23, P1227, DOI 10.1109/TPDS.2011.283
   Liu K., 2015, PROC 12 ACM INT C CO, p20:1, DOI [10.1145/2742854.2742875, DOI 10.1145/2742854.2742875]
   Ma XQ, 2017, IEEE T MULTIMEDIA, V19, P1184, DOI 10.1109/TMM.2016.2646182
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Meng SB, 2016, IEEE T MULTIMEDIA, V18, P1124, DOI 10.1109/TMM.2016.2535270
   Mnih V, 2016, PR MACH LEARN RES, V48
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Pang HT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1217, DOI 10.1145/3240508.3240642
   Park YJ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1278, DOI 10.1145/3240508.3241362
   Ravi Netravali, 2015, 2015 USENIX ANN TECH, P417
   Riiser Haakon, 2013, ACM MMSYS
   Schulman J., 2017, PROXIMAL POLICY OPTI
   Schulman J, 2015, PR MACH LEARN RES, V37, P1889
   Sengupta S, 2018, I C NETWORK PROTOCOL, P165, DOI 10.1109/ICNP.2018.00026
   Spiteri K, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P123, DOI 10.1145/3204949.3204953
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Sun Y, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P272, DOI 10.1145/2934872.2934898
   T. University, 2018, GLOB INT NETW TRANSM
   van der Hooft J, 2018, J NETW SYST MANAG, V26, P51, DOI 10.1007/s10922-017-9407-2
   van der Hooft J, 2016, IEEE COMMUN LETT, V20, P2177, DOI 10.1109/LCOMM.2016.2601087
   Wu TT, 2017, IEEE T MULTIMEDIA, V19, P2267, DOI 10.1109/TMM.2017.2736963
   Wu Y., 2017, SCALABLE TRUST REGIO
   Yamagishi K, 2017, IEEE T MULTIMEDIA, V19, P1545, DOI 10.1109/TMM.2017.2669859
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zaki Y, 2015, ACM SIGCOMM COMP COM, V45, P509, DOI 10.1145/2829988.2787498
   Zhang R, 2012, IEEE INT POWER MODUL, P225, DOI 10.1109/IPMHVC.2012.6518720
   Zhang WX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P355, DOI 10.1145/3240508.3240561
NR 51
TC 10
Z9 10
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2020
VL 22
IS 11
BP 2963
EP 2976
DI 10.1109/TMM.2019.2962313
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OJ8YR
UT WOS:000584239900016
DA 2024-07-18
ER

PT J
AU Zhang, T
   Zheng, WM
   Cui, Z
   Zong, Y
   Li, CL
   Zhou, XY
   Yang, J
AF Zhang, Tong
   Zheng, Wenming
   Cui, Zhen
   Zong, Yuan
   Li, Chaolong
   Zhou, Xiaoyan
   Yang, Jian
TI Deep Manifold-to-Manifold Transforming Network for Skeleton-Based Action
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Manifolds; Measurement; Covariance matrices; Feature extraction; Task
   analysis; Convolution; Kernel; Riemannian manifold; SPD matrix; deep
   learning; action recognition
ID DIMENSIONALITY REDUCTION; GEOMETRY
AB In this paper, we will investigate skeleton-based action recognition by employing high-order statistics feature and first-order statistics feature, where the high-order statistics feature is characterized by symmetric positive definite (SPD) matrices. Noting that SPD matrices are theoretically embedded on Riemannian manifolds, we propose an end-to-end deep manifold-to-manifold transforming network (DMT-Net), which can make SPD matrices flow from one Riemannian manifold to another one for facilitating the action recognition task. To learn discriminative SPD features from both spatial and temporal dependencies, we propose a neural network model with three novel layers on manifolds: i.e., (1) the local SPD convolutional layer, (2) the non-linear SPD activation layer, and (3) the Riemannian-preserved recursive layer. The SPD property is preserved through all layers without the singular value decomposition (SVD) operation, which has to be conducted in the existing methods with expensive computation cost. Furthermore, a diagonalizing SPD layer is designed to efficiently calculate the final metric for the classification task. Finally, DMT-Net is further fused with a first order layer to capture temporal evolution information. To evaluate our proposed method, we conduct extensive experiments on the task of action recognition, where the input signals are represented as SPD matrices. The experimental results demonstrate that the proposed method is competitive over state-of-the-art methods.
C1 [Zhang, Tong; Zheng, Wenming; Zong, Yuan; Li, Chaolong] Southeast Univ, Sch Biol Sci & Med Engn, Minist Educ, Key Lab Child Dev & Learning Sci, Nanjing 210096, Peoples R China.
   [Zhang, Tong; Cui, Zhen; Zhou, Xiaoyan; Yang, Jian] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Zhou, Xiaoyan] Nanjing Univ Informat Sci & Engn Technol, Sch Elect & Informat Engn, Nanjing 210044, Peoples R China.
C3 Southeast University - China; Nanjing University of Science &
   Technology; Nanjing University of Information Science & Technology
RP Zheng, WM (corresponding author), Southeast Univ, Sch Biol Sci & Med Engn, Minist Educ, Key Lab Child Dev & Learning Sci, Nanjing 210096, Peoples R China.
EM tong.zhang@njust.edu.cn; wenming_zheng@seu.edu.cn;
   zhen.cui@njust.edu.cn; xhzongyuan@seu.edu.cn; lichaolong@seu.edu.cn;
   xiaoyan_zhou@nuist.edu.cn; csjyang@njust.edu.cn
RI Zheng, Wenming/AAG-6507-2020; Zhang, Tong/IWU-6890-2023
OI Zheng, Wenming/0000-0002-7709-2164; Zhang, Tong/0000-0002-1769-9829
FU National Key Research and Development Program of China [2018YFB1305200];
   National Natural Science Foundation of China [61921004, 61906094,
   61772276, 61902064, 81971282]; Jiangsu Provincial Key Research and
   Development Program [BE2016616]; Natural Science Foundation of Jiangsu
   Province [BK20190452]; Fundamental Research Funds for the Central
   Universities [30919011232, 2242018K3DN01, 2242019K40047]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018YFB1305200, in part by the
   National Natural Science Foundation of China underGrants 61921004,
   61906094, 61772276, 61902064, and 81971282, in part by the Jiangsu
   Provincial Key Research and Development Program under Grant BE2016616,
   in part by the Natural Science Foundation of Jiangsu Province under
   Grant BK20190452, and in part by the Fundamental Research Funds for the
   Central Universities under Grants 30919011232, 2242018K3DN01 and
   2242019K40047. The associate editor coordinating the reviewof this
   manuscript and approving it for publication was Prof. Balakrishnan
   Prabhakaran.
CR Anirudh R, 2015, PROC CVPR IEEE, P3147, DOI 10.1109/CVPR.2015.7298934
   [Anonymous], 2012, 2012 IEEE COMPUTER S, DOI DOI 10.1109/CVPRW.2012.6239231
   Arsigny V, 2006, MAGN RESON MED, V56, P411, DOI 10.1002/mrm.20965
   Baccouche Moez, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P29, DOI 10.1007/978-3-642-25446-8_4
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Ben Amor B, 2016, IEEE T PATTERN ANAL, V38, P1, DOI 10.1109/TPAMI.2015.2439257
   Cai XY, 2016, IEEE T MULTIMEDIA, V18, P141, DOI 10.1109/TMM.2015.2505089
   Chaudhry R, 2013, IEEE COMPUT SOC CONF, P471, DOI 10.1109/CVPRW.2013.153
   Cho K., 2014, ARXIV14061078
   Devanne M, 2015, IEEE T CYBERNETICS, V45, P1340, DOI 10.1109/TCYB.2014.2350774
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954
   Evangelidis G, 2014, INT C PATT RECOG, P4513, DOI 10.1109/ICPR.2014.772
   Goh A, 2008, PROC CVPR IEEE, P626
   Gowayyed M.A., 2013, Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI '13, P1351
   Harandi M, 2018, IEEE T PATTERN ANAL, V40, P48, DOI 10.1109/TPAMI.2017.2655048
   Harandi MT, 2014, LECT NOTES COMPUT SC, V8690, P17, DOI 10.1007/978-3-319-10605-2_2
   Harandi MT, 2012, LECT NOTES COMPUT SC, V7573, P216, DOI 10.1007/978-3-642-33709-3_16
   Huang ZW, 2017, AAAI CONF ARTIF INTE, P2036
   Huang ZW, 2017, PROC CVPR IEEE, P1243, DOI 10.1109/CVPR.2017.137
   Hussein, 2013, INT JOINT C ART INT
   Jayasumana S, 2015, IEEE T PATTERN ANAL, V37, P2464, DOI 10.1109/TPAMI.2015.2414422
   Jayasumana S, 2013, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2013.17
   KOLES Z J, 1990, Brain Topography, V2, P275, DOI 10.1007/BF01129656
   Koniusz P, 2016, LECT NOTES COMPUT SC, V9908, P37, DOI 10.1007/978-3-319-46493-0_3
   Krizhevsky A., 2012, ADV NEURAL INF PROCE, V25, P1097
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu MY, 2018, IEEE T MULTIMEDIA, V20, P1932, DOI 10.1109/TMM.2017.2786868
   Muller M., 2007, Tech. Rep. CG-2007-2
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Pang YW, 2008, IEEE T CIRC SYST VID, V18, P989, DOI 10.1109/TCSVT.2008.924108
   Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z
   Porikli F., 2006, 2006 IEEE COMPUTER S, V1, P728, DOI [10.1109/CVPR.2006.94, DOI 10.1109/CVPR.2006.94]
   Seidenari L, 2013, IEEE COMPUT SOC CONF, P479, DOI 10.1109/CVPRW.2013.77
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Tao LL, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P303, DOI 10.1109/ICCVW.2015.48
   Tosato D, 2010, LECT NOTES COMPUT SC, V6312, P378, DOI 10.1007/978-3-642-15552-9_28
   Tuzel O, 2008, IEEE T PATTERN ANAL, V30, P1713, DOI 10.1109/TPAMI.2008.75
   Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang L, 2015, IEEE I CONF COMP VIS, P4570, DOI 10.1109/ICCV.2015.519
   Wang P, 2016, LECT NOTES COMPUT SC, V9911, P370, DOI 10.1007/978-3-319-46478-7_23
   Wang W, 2015, PROC CVPR IEEE, P3395, DOI 10.1109/CVPR.2015.7298816
   Yang YH, 2017, IEEE T MULTIMEDIA, V19, P519, DOI 10.1109/TMM.2016.2626959
   Zhang J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P101, DOI 10.1109/ICIT.2016.7474733
   Zhang SY, 2017, IEEE WINT CONF APPL, P148, DOI 10.1109/WACV.2017.24
   Zhang T, 2018, IEEE IMAGE PROC, P4098, DOI 10.1109/ICIP.2018.8451626
   Zhang YP, 2012, IEEE VTS VEH TECHNOL
NR 51
TC 17
Z9 17
U1 1
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2020
VL 22
IS 11
BP 2926
EP 2937
DI 10.1109/TMM.2020.2966878
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OJ8YR
UT WOS:000584239900013
DA 2024-07-18
ER

PT J
AU Angelini, F
   Fu, ZY
   Long, Y
   Shao, L
   Naqvi, SM
AF Angelini, Federico
   Fu, Zeyu
   Long, Yang
   Shao, Ling
   Naqvi, Syed Mohsen
TI 2D Pose-Based Real-Time Human Action Recognition With Occlusion-Handling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Three-dimensional displays; Two dimensional
   displays; Target tracking; Detectors; Skeleton; Videos; Pose; LSTM; CNN;
   ISLD; CCTV
ID PARTICLE PHD FILTER; TRACKING
AB Human Action Recognition (HAR) for CCTV-oriented applications is still a challenging problem. Real-world scenarios HAR implementations is difficult because of the gap between Deep Learning data requirements and what the CCTV-based frameworks can offer in terms of data recording equipments. We propose to reduce this gap by exploiting human poses provided by the OpenPose, which has been already proven to be an effective detector in CCTV-like recordings for tracking applications. Therefore, in this work, we first propose ActionXPose: a novel 2D pose-based approach for pose-level HAR. ActionXPose extracts low- and high-level features from body poses which are provided to a Long Short-Term Memory Neural Network and a 1D Convolutional Neural Network for the classification. We also provide a new dataset, named ISLD, for realistic pose-level HAR in a CCTV-like environment, recorded in the Intelligent Sensing Lab. ActionXPose is extensively tested on ISLD under multiple experimental settings, e.g. Dataset Augmentation and Cross-Dataset setting, as well as revising other existing datasets for HAR. ActionXPose achieves state-of-the-art performance in terms of accuracy, very high robustness to occlusions and missing data, and promising results for practical implementation in real-world applications.
C1 [Angelini, Federico; Fu, Zeyu; Naqvi, Syed Mohsen] Newcastle Univ, Intelligent Sensing & Commun Res Grp, Sch Engn, Newcastle Upon Tyne NE1 7RU, Tyne & Wear, England.
   [Long, Yang] Univ Durham, Dept Comp Sci, Durham DH1 3LE, England.
   [Shao, Ling] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.
C3 Newcastle University - UK; Durham University
RP Angelini, F; Naqvi, SM (corresponding author), Newcastle Univ, Intelligent Sensing & Commun Res Grp, Sch Engn, Newcastle Upon Tyne NE1 7RU, Tyne & Wear, England.
EM f.angelini2@ncl.ac.uk; z.fu2@ncl.ac.uk; yang.long@durham.ac.uk;
   ling.shao@ieee.org; mohsen.naqvi@ncl.ac.uk
RI FU, ZEYU/AAP-7322-2020; Shao, Ling/D-3535-2011
OI FU, ZEYU/0000-0002-2076-7597; 
FU EPSRC; Thales in the Industrial Cooperative Awards in Science &
   Technology (iCASE); MRC [MR/S003916/2, MR/S003916/1] Funding Source:
   UKRI
FX This work was supported by EPSRC and Thales in the Industrial
   Cooperative Awards in Science & Technology (iCASE). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Xiaochun Cao.
CR Almeida R, 2017, IEEE IJCNN, P1354, DOI 10.1109/IJCNN.2017.7966010
   Angelini F, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4219, DOI 10.1109/ICASSP.2018.8461472
   [Anonymous], 2016, DEEP LEARNING
   [Anonymous], 2015, INT J SCI ENG RES
   Azary S, 2012, 2012 WESTERN NEW YORK IMAGE PROCESSING WORKSHOP (WNYIPW), P5, DOI 10.1109/WNYIPW.2012.6466646
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Berg AC, 2005, PROC CVPR IEEE, P26
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Castro-Muñoz G, 2015, 2015 INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE AND COMPUTATIONAL INTELLIGENCE (CSCI), P509, DOI 10.1109/CSCI.2015.12
   Chen CP, 2012, PROCEEDINGS OF THE ASME INTERNATIONAL MECHANICAL ENGINEERING CONGRESS AND EXPOSITION, 2011, VOL 7, PTS A AND B, P533
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Feng PM, 2017, IEEE T MULTIMEDIA, V19, P725, DOI 10.1109/TMM.2016.2638206
   Feng PM, 2016, IEEE SIGNAL PROC LET, V23, P1592, DOI 10.1109/LSP.2016.2611138
   Fu ZY, 2019, IEEE T MULTIMEDIA, V21, P2277, DOI 10.1109/TMM.2019.2902480
   Fu ZY, 2018, IEEE ACCESS, V6, P14764, DOI 10.1109/ACCESS.2018.2816805
   Gilbert A, 2011, IEEE T PATTERN ANAL, V33, P883, DOI 10.1109/TPAMI.2010.144
   Gkalelis N, 2009, 2009 CONFERENCE FOR VISUAL MEDIA PRODUCTION: CVMP 2009, P159, DOI 10.1109/CVMP.2009.19
   Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   Herath S, 2017, IMAGE VISION COMPUT, V60, P4, DOI 10.1016/j.imavis.2017.01.010
   Hilsenbeck B, 2016, INT C PATT RECOG, P1911, DOI 10.1109/ICPR.2016.7899916
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hsu WN, 2017, 2017 IEEE AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING WORKSHOP (ASRU), P16, DOI 10.1109/ASRU.2017.8268911
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Iosifidis A, 2013, SIGNAL PROCESS, V93, P1445, DOI 10.1016/j.sigpro.2012.08.015
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jo H, 2017, INT C CONTR AUTOMAT, P1035, DOI 10.23919/ICCAS.2017.8204369
   Karim F, 2019, NEURAL NETWORKS, V116, P237, DOI 10.1016/j.neunet.2019.04.014
   Kay S. M., 1993, FUNDAMENTALS STAT SI
   Kohonen T., 2001, INFORM SCIENCES
   Kovashka A, 2010, PROC CVPR IEEE, P2046, DOI 10.1109/CVPR.2010.5539881
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Kuhn M., 2018, APPL PREDICTIVE MODE
   Langmann Benjamin, 2012, Proceedings of the 1st International Conference on Pattern Recognition Applications and Methods. ICPRAM 2012, P438
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Liang B, 2015, 2015 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P76
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu JG, 2009, PROC CVPR IEEE, P1996
   Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306
   Milan A., 2016, MOT16 BENCHMARK MULT
   Raptis M, 2010, LECT NOTES COMPUT SC, V6311, P577, DOI 10.1007/978-3-642-15549-9_42
   Roussinov D. G., 1998, CC-AI, The Journal for the Integrated Study of Artificial Intelligence, Cognitive Science and Applied Epistemology, V15, P81
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Ur-Rehman A, 2016, IEEE T SIGNAL PROCES, V64, P1320, DOI 10.1109/TSP.2015.2504340
   Veeriah V, 2015, IEEE I CONF COMP VIS, P4041, DOI 10.1109/ICCV.2015.460
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Vrigkas M, 2014, COMPUT VIS IMAGE UND, V119, P27, DOI 10.1016/j.cviu.2013.11.007
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang Y, 2017, IEEE PHOT SPEC CONF, P2000, DOI 10.1109/PVSC.2017.8366636
   Weinland D, 2006, COMPUT VIS IMAGE UND, V104, P249, DOI 10.1016/j.cviu.2006.07.013
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Zhang SG, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/3090343
   Zhang YM, 2012, LECT NOTES COMPUT SC, V7574, P707, DOI 10.1007/978-3-642-33712-3_51
   Zhu WH, 2016, PROC INT CONF ANTI, P1, DOI 10.1109/ICASID.2016.7873885
NR 55
TC 45
Z9 47
U1 2
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2020
VL 22
IS 6
BP 1433
EP 1446
DI 10.1109/TMM.2019.2944745
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU8YA
UT WOS:000538033100005
OA Green Accepted, Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, Y
   Dong, YF
   Guo, ST
   Yang, YY
   Liao, XF
AF Wang, Ying
   Dong, Yifan
   Guo, Songtao
   Yang, Yuanyuan
   Liao, Xiaofeng
TI Latency-Aware Adaptive Video Summarization for Mobile Edge Clouds
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bandwidth; Feature extraction; Wireless communication; Machine learning;
   Dictionaries; Optimization; Image edge detection; Video summarization;
   latency; mobile edge devices; wireless bandwidth
ID KEY-FRAME EXTRACTION
AB With the technological advances in wireless multimedia domain, these videos made by mobile edge devices dominate network traffics. The video summarization technology enables users to understand the storyline of a video before a client requests the complete video content. Summarizing a video on edge devices and transmitting the summary between them requires a user-oriented and adaptive solution due to the limited capability and the dynamic wireless links of edge devices. Therefore, it is beneficial to improve the user's viewing experience and the bandwidth utilization ratio if we generate and transmit a video summary based on network connections and the user's tolerant latency. Unfortunately, previous summarization approaches are incapable of adjusting the summary size adapted to the varying network bandwidth and the user's attitude towards latency. To timely and flexibly deal with mobile videos, we first formulate the video summarization optimization problem with the elastic number of selected representative segments and the outlier detection within a bounded time budget. Furthermore, we develop an online greedy algorithm called the Elastic Video Summarization Algorithm (EVS) to solve the NP hard problem. We analyze the properties associated with EVS and further design an improved EVS-II to reduce computation complexity. Finally, the experimental results demonstrate that our proposed algorithms outperform other existing researches in fitting network bandwidth and detecting outliers.
C1 [Wang, Ying; Dong, Yifan; Guo, Songtao; Liao, Xiaofeng] Southwest Univ, Sch Elect & Informat Engn, Chongqing 400715, Peoples R China.
   [Wang, Ying] Southwest Univ, Sch Comp & Informat Sci, Chongqing 400715, Peoples R China.
   [Guo, Songtao; Liao, Xiaofeng] Chongqing Univ, Coll Comp Sci, Chongqing 400044, Peoples R China.
   [Yang, Yuanyuan] SUNY Stony Brook, Dept Elect & Comp Engn, Stony Brook, NY 11794 USA.
C3 Southwest University - China; Southwest University - China; Chongqing
   University; State University of New York (SUNY) System; State University
   of New York (SUNY) Stony Brook
RP Guo, ST (corresponding author), Chongqing Univ, Coll Comp Sci, Chongqing 400044, Peoples R China.
EM 466176423@qq.com; 78662852@qq.com; songtao_guo@163.com;
   yuanyuan.yang@stonybrook.edu; xfliao@swu.edu.cn
RI Liao, Xiaofeng/HPD-6655-2023; yang, yuanyuan/HPG-2020-2023
OI Yang, Yuanyuan Kara/0000-0001-7296-9222
FU National Key R&D Program of China [2018YFB0803400]; National Natural
   Science Foundation of China [61772432, 61772433]; Technological
   Innovation and Application Demonstration Projects of Chongqing
   [cstc2018jszx-cyztzxX0014]; Fundamental Research Funds for the Central
   Universities of Southwest University [XDJK2016C040]; Fundamental
   Research Funds for the Central Universities [2019CDYGZD004]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018YFB0803400, in part by the National Natural Science
   Foundation of China under Grants 61772432 and 61772433, in part by the
   Technological Innovation and Application Demonstration Projects of
   Chongqing under Grant cstc2018jszx-cyztzxX0014, in part by the
   Fundamental Research Funds for the Central Universities of Southwest
   University under Grant XDJK2016C040, and in part by the Fundamental
   Research Funds for the Central Universities under Grant 2019CDYGZD004.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Xiaoqing Zhu.
CR Abd-Almageed W, 2008, IEEE IMAGE PROC, P3200, DOI 10.1109/ICIP.2008.4712476
   [Anonymous], 2013, Iberoamerican Congress on Pattern Recognition
   [Anonymous], P IEEE CVF C COMP VI
   Boukadida H, 2017, IEEE T CIRC SYST VID, V27, P920, DOI 10.1109/TCSVT.2015.2513678
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Chan P. P. K., 2011, Proceedings of the 2011 International Conference on Machine Learning and Cybernetics (ICMLC 2011), P1637, DOI 10.1109/ICMLC.2011.6017035
   Dang C, 2015, IEEE T IMAGE PROCESS, V24, P3742, DOI 10.1109/TIP.2015.2445572
   Ejaz N, 2013, SIGNAL PROCESS-IMAGE, V28, P34, DOI 10.1016/j.image.2012.10.002
   Elhamifar E, 2017, PROC CVPR IEEE, P1818, DOI 10.1109/CVPR.2017.197
   Elhamifar E, 2016, IEEE T PATTERN ANAL, V38, P2182, DOI 10.1109/TPAMI.2015.2511748
   Elhamifar E, 2012, PROC CVPR IEEE, P1600, DOI 10.1109/CVPR.2012.6247852
   Ferreira L, 2015, SIGNAL PROCESS-IMAGE, V39, P98, DOI 10.1016/j.image.2015.09.005
   de Avila SEF, 2011, PATTERN RECOGN LETT, V32, P56, DOI 10.1016/j.patrec.2010.08.004
   Huayong Liu, 2012, 2012 9th International Conference on Fuzzy Systems and Knowledge Discovery, P1238, DOI 10.1109/FSKD.2012.6233777
   Jeong DJ, 2016, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-016-0122-9
   Kuanar SK, 2015, IEEE T MULTIMEDIA, V17, P1166, DOI 10.1109/TMM.2015.2443558
   Kuanar SK, 2013, J VIS COMMUN IMAGE R, V24, P1212, DOI 10.1016/j.jvcir.2013.08.003
   Lai JL, 2012, J VIS COMMUN IMAGE R, V23, P114, DOI 10.1016/j.jvcir.2011.08.005
   Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432, DOI 10.1109/iccv.2003.1238378
   Li JT, 2017, NEUROCOMPUTING, V266, P66, DOI 10.1016/j.neucom.2017.04.065
   Li XL, 2017, IEEE T IMAGE PROCESS, V26, P3652, DOI 10.1109/TIP.2017.2695887
   Lu XQ, 2017, IEEE T CYBERNETICS, V47, P884, DOI 10.1109/TCYB.2016.2531179
   Mademlis I, 2018, INFORM SCIENCES, V432, P319, DOI 10.1016/j.ins.2017.12.020
   Oh SM, 2011, PROC CVPR IEEE
   Ou SH, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6853799
   Panda R, 2017, IEEE T MULTIMEDIA, V19, P2010, DOI 10.1109/TMM.2017.2708981
   Sah S, 2017, IEEE WINT CONF APPL, P989, DOI 10.1109/WACV.2017.115
   Song XM, 2006, IEEE T CIRC SYST VID, V16, P904, DOI 10.1109/TCSVT.2006.877419
   Song YL, 2015, PROC CVPR IEEE, P5179, DOI 10.1109/CVPR.2015.7299154
   Talavera E, 2015, LECT NOTES COMPUT SC, V9117, P327, DOI 10.1007/978-3-319-19390-8_37
   Thomas SS, 2017, IEEE T CIRC SYST VID, V27, P1790, DOI 10.1109/TCSVT.2016.2556558
   Vázquez-Martín R, 2013, PATTERN RECOGN LETT, V34, P770, DOI 10.1016/j.patrec.2012.12.009
   Vullings HJLM, 1997, LECT NOTES COMPUT SC, V1280, P275
   Wang LZ, 2014, COMPUT SCI ENG, V16, P41, DOI 10.1109/MCSE.2014.52
   Wang WG, 2017, IEEE I CONF COMP VIS, P1680, DOI 10.1109/ICCV.2017.185
   Xu BH, 2016, IEEE MULTIMEDIA, V23, P23, DOI 10.1109/MMUL.2016.18
   Xu YY, 2013, SIAM J IMAGING SCI, V6, P1758, DOI 10.1137/120887795
   Zhang LM, 2015, IEEE T IND ELECTRON, V62, P1309, DOI 10.1109/TIE.2014.2336639
   Zhang S, 2016, IEEE T IMAGE PROCESS, V25, P5469, DOI 10.1109/TIP.2016.2601493
   Zhang Y, 2016, IEEE T MULTIMEDIA, V18, P418, DOI 10.1109/TMM.2016.2520827
   Zhang YZ, 2017, IEEE T CIRC SYST VID, V27, P1340, DOI 10.1109/TCSVT.2016.2539638
   Zhao B, 2014, PROC CVPR IEEE, P2513, DOI 10.1109/CVPR.2014.322
NR 42
TC 13
Z9 14
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2020
VL 22
IS 5
BP 1193
EP 1207
DI 10.1109/TMM.2019.2939753
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LJ3VZ
UT WOS:000530097200007
DA 2024-07-18
ER

PT J
AU Zhang, Z
   Lung, CH
   St-Hilaire, M
   Lambadaris, I
AF Zhang, Zhe
   Lung, Chung-Horng
   St-Hilaire, Marc
   Lambadaris, Ioannis
TI An SDN-Based Caching Decision Policy for Video Caching in
   Information-Centric Networking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Software-defined networking (SDN); information-centric networking (ICN);
   caching decision policy; integer linear programming (ILP)
ID SOFTWARE-DEFINED NETWORKING; PLACEMENT; STRATEGY; ICN
AB The considerable increase of multimedia services, such as video-on-demand (VoD) services, is a significant contributor to the total Internet traffic. Software-defined networking (SDN) and information-centric networking (ICN) are two promising technologies that can be combined to facilitate video delivery and to reduce network delays. In this paper, we first formulate the caching decision problem as a 0-1 integer linear programming (ILP) problem. Second, in contrast to existing approaches that solve the formulated ILP problem by assuming all future video requests are known, we consider the impact of the time scale, which transforms the static 0-1 ILP problem into a dynamic problem. By solving the dynamic 0-1 ILP problem, we find more accurate optimal solutions compared to existing approaches. Third, since the formulated 0-1 dynamic ILP problem is NP-hard, we leverage the in-network caching of ICN and the global view of the SDN controller to propose a novel SDN-based caching decision policy. Finally, extensive evaluations are performed, and the results demonstrate that the proposed SDN-based caching decision policy provides solutions that are close to the optimum in substantially less computation time. The SDN-based caching decision policy also outperforms existing practical ICN caching decision policies in terms of the cache hit ratio and the average number of hops, which are directly related to the video delivery latency. Moreover, the SDN-based caching decision policy can substantially reduce the number of generated and broadcasted interest packets, which is a shortcoming of the current ICN.
C1 [Zhang, Zhe; Lung, Chung-Horng; Lambadaris, Ioannis] Carleton Univ, Dept Syst & Comp Engn, Ottawa, ON K1S 5B6, Canada.
   [St-Hilaire, Marc] Carleton Univ, Sch Informat Technol, Dept Syst & Comp Engn, Ottawa, ON K1S 5B6, Canada.
C3 Carleton University; Carleton University
RP Zhang, Z (corresponding author), Carleton Univ, Dept Syst & Comp Engn, Ottawa, ON K1S 5B6, Canada.
EM zhezhang4@sce.carleton.ca; chlung@sce.carleton.ca;
   marc_st_hilaire@carleton.ca; ioannis@sce.carleton.ca
RI Zhang, Zhe/HMD-5984-2023
OI Zhang, Zhe/0000-0002-6791-9009; Lung, Chung-Horng/0000-0002-5662-490X
FU Natural Sciences and Engineering Research Council of Canada (NSERC)
FX This work was supported by the Natural Sciences and Engineering Research
   Council of Canada (NSERC).
CR [Anonymous], [No title captured]
   [Anonymous], 2017, Cisco Visual Networking Index: Forecast and Trends
   Ao WC, 2018, IEEE T MOBILE COMPUT, V17, P1048, DOI 10.1109/TMC.2017.2750143
   Bernardini C, 2013, IEEE ICC, P3619, DOI 10.1109/ICC.2013.6655114
   Bilal M, 2017, IEEE ACCESS, V5, P1692, DOI 10.1109/ACCESS.2017.2669344
   Cha M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P1
   Chen BQ, 2017, IEEE COMMUN LETT, V21, P1155, DOI 10.1109/LCOMM.2017.2652440
   Chen QX, 2018, IEEE ACM T NETWORK, V26, P274, DOI 10.1109/TNET.2017.2782216
   Chiocchetti R, 2013, IEEE ICC
   Cho K, 2012, IEEE CONF COMPUT, P316, DOI 10.1109/INFCOMW.2012.6193512
   Cui Y, 2018, IEEE T BIG DATA, V4, P356, DOI 10.1109/TBDATA.2017.2651901
   Dehghan M, 2017, IEEE ACM T NETWORK, V25, P1635, DOI 10.1109/TNET.2016.2636843
   IBM, ILOG CPLEX OPT STUD
   Jacobson V, 2012, COMMUN ACM, V55, P117, DOI 10.1145/2063176.2063204
   Jmal R, 2017, COMPUT STAND INTER, V51, P22, DOI 10.1016/j.csi.2016.10.016
   Kalghoum A, 2018, COMPUT ELECTR ENG, V66, P98, DOI 10.1016/j.compeleceng.2017.12.025
   Kreutz D, 2015, P IEEE, V103, P14, DOI 10.1109/JPROC.2014.2371999
   Li CL, 2018, IEEE T MULTIMEDIA, V20, P965, DOI 10.1109/TMM.2017.2757761
   Li XH, 2017, IEEE T WIREL COMMUN, V16, P6926, DOI 10.1109/TWC.2017.2734646
   Liang CC, 2017, IEEE T WIREL COMMUN, V16, P6912, DOI 10.1109/TWC.2017.2734081
   Liu Z, 2016, PROCEEDINGS OF THE 2016 3RD ACM CONFERENCE ON INFORMATION-CENTRIC NETWORKING (ACM-ICN '16), P197, DOI 10.1145/2984356.2985227
   Mahdavi M, 2019, IEEE SYST J, V13, P3129, DOI 10.1109/JSYST.2018.2871793
   Mahmood A, 2018, T EMERG TELECOMMUN T, V29, DOI 10.1002/ett.3271
   Newman MEJ, 2005, CONTEMP PHYS, V46, P323, DOI 10.1080/00107510500052444
   Psaras I., 2012, P 2 ED ICN WORKSH IN, P55, DOI DOI 10.1145/2342488.2342501
   Rossi D., 2011, CACHING PERFORMANCE
   Seo E, 2018, IEEE ACCESS, V6, P66305, DOI 10.1109/ACCESS.2018.2879167
   Siracusano G, 2018, COMPUT NETW, V134, P245, DOI 10.1016/j.comnet.2018.01.026
   Tan B, 2013, IEEE ACM T NETWORK, V21, P566, DOI 10.1109/TNET.2012.2208199
   Wang L, 2017, IEEE ACM T NETWORK, V25, P2686, DOI 10.1109/TNET.2017.2707131
   Wang R, 2016, IEEE COMMUN MAG, V54, P77, DOI 10.1109/MCOM.2016.7537180
   Wang S, 2016, IEEE ACM T NETWORK, V24, P2774, DOI 10.1109/TNET.2015.2480093
   Wang W, 2019, IEEE T WIREL COMMUN, V18, P796, DOI 10.1109/TWC.2018.2883443
   Wang W, 2016, IEEE COMMUN MAG, V54, P33, DOI 10.1109/MCOM.2016.7378423
   Wang YG, 2016, IEEE T COMPUT, V65, P95, DOI 10.1109/TC.2015.2409848
   Yan H, 2017, IEEE ACCESS, V5, P8433, DOI 10.1109/ACCESS.2017.2694045
   Zhang GQ, 2013, COMPUT NETW, V57, P3128, DOI 10.1016/j.comnet.2013.07.007
   Zhang Z, 2018, COMPUT COMMUN, V118, P81, DOI 10.1016/j.comcom.2017.10.002
   Zhang Z, 2017, P INT COMP SOFTW APP, P523, DOI 10.1109/COMPSAC.2017.203
   Zhao H, 2017, IEEE T MULTIMEDIA, V19, P149, DOI 10.1109/TMM.2016.2612123
NR 40
TC 22
Z9 23
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 1069
EP 1083
DI 10.1109/TMM.2019.2935683
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400020
DA 2024-07-18
ER

PT J
AU Xiao, B
   Ou, G
   Tang, H
   Bi, XL
   Li, WS
AF Xiao, Bin
   Ou, Ge
   Tang, Han
   Bi, Xiuli
   Li, Weisheng
TI Multi-Focus Image Fusion by Hessian Matrix Based Decomposition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image fusion; Frequency modulation; Matrix decomposition; Feature
   extraction; Transforms; Image edge detection; Image decomposition;
   Multi-focus image fusion; hessian matrix; focus measure; integral image
AB In this paper, a Hessian matrix based multi-focus image fusion method is proposed. First, the integral map is introduced for fast compute the Hessian matrix of source images at different scales, and the multi-scale Hessian matrix of source image is obtained. Second, the multi-scale Hessian matrix is used to decompose each source image into two kinds of regions: the feature and background regions. In order to improve the fusion performance, two new focus measures based on the multi-scale Hessian matrix and two different fusion strategies for both feature and background regions are utilized to obtain the initial decision maps, respectively. Finally, the final decision map for image fusion is achieved by post-processing on the results of the previous step. The proposed method is a primary attempt to introduce image feature and background regions decomposition strategies in the field of multi-focus image fusion. The experimental results also show that our method outperforms the existing image fusion methods in both visual perception and objective evaluations.
C1 [Xiao, Bin; Ou, Ge; Tang, Han; Bi, Xiuli; Li, Weisheng] Chongqing Univ Posts & Telecommun, Sch Comp Sci & Technol, Chongqing 400065, Peoples R China.
C3 Chongqing University of Posts & Telecommunications
RP Xiao, B (corresponding author), Chongqing Univ Posts & Telecommun, Sch Comp Sci & Technol, Chongqing 400065, Peoples R China.
EM xiaobin@cqupt.edu.cn; s170231062@stu.cqupt.edu.cn;
   hantang.cqupt@gmail.com; bixl@cqupt.edu.cn; liws@cqupt.edu.cn
RI Xiao, Bin/E-2722-2012
OI Bi, Xiuli/0000-0003-3134-217X
FU National Natural Science Foundation of China [61572092, U1401252];
   Chongqing Research Program of Application Foundation and Advanced
   Technology [cstc2018jcyjAX0117]; Scientific and TechnologicalKey
   Research Program of Chongqing Municipal Education Commission
   [KJZD-K201800601]; National Science and Technology Major Project
   [2016YFC10003073]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61572092 and Grant U1401252, in part by
   theNational Science and Technology Major Project 2016YFC10003073, in
   part by the Chongqing Research Program of Application Foundation and
   Advanced Technology (cstc2018jcyjAX0117), and in part by the Scientific
   and TechnologicalKey Research Program of Chongqing Municipal Education
   Commission (KJZD-K201800601). The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Lei
   Zhang.
CR [Anonymous], NEURAL PROCESSING LE
   Bai XZ, 2015, INFORM FUSION, V22, P105, DOI 10.1016/j.inffus.2014.05.003
   Bay H., 2008, COMPUT VIS IMAGE UND, V10, P346, DOI DOI 10.1016/j.cviu.2007.09.014
   Bhatnagar G, 2013, IEEE T MULTIMEDIA, V15, P1014, DOI 10.1109/TMM.2013.2244870
   Eskicioglu AM, 1995, IEEE T COMMUN, V43, P2959, DOI 10.1109/26.477498
   Helmli FS, 2001, ISPA 2001: PROCEEDINGS OF THE 2ND INTERNATIONAL SYMPOSIUM ON IMAGE AND SIGNAL PROCESSING AND ANALYSIS, P188, DOI 10.1109/ISPA.2001.938626
   Wei H, 2007, PATTERN RECOGN LETT, V28, P493, DOI 10.1016/j.patrec.2006.09.005
   Lei JJ, 2017, IEEE T MULTIMEDIA, V19, P1442, DOI 10.1109/TMM.2017.2660440
   Lewis JJ, 2007, INFORM FUSION, V8, P119, DOI 10.1016/j.inffus.2005.09.006
   Li HF, 2016, INFORM SCIENCES, V349, P25, DOI 10.1016/j.ins.2016.02.030
   Li ST, 2017, INFORM FUSION, V33, P100, DOI 10.1016/j.inffus.2016.05.004
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Li S, 2013, INFORM FUSION, V14, P147, DOI 10.1016/j.inffus.2011.07.001
   Li YY, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20070522
   Liu Y, 2018, INFORM FUSION, V42, P158, DOI 10.1016/j.inffus.2017.10.007
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Liu Y, 2015, INFORM FUSION, V24, P147, DOI 10.1016/j.inffus.2014.09.004
   Liu Y, 2015, INFORM FUSION, V23, P139, DOI 10.1016/j.inffus.2014.05.004
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Nayar S. K., 1990, Proceedings 1990 IEEE International Conference on Robotics and Automation (Cat. No.90CH2876-1), P218, DOI 10.1109/ROBOT.1990.125976
   Oliveira PAM, 2017, IEEE T CIRC SYST VID, V27, P1066, DOI 10.1109/TCSVT.2016.2515378
   Oszust M, 2016, IEEE SIGNAL PROC LET, V23, P65, DOI 10.1109/LSP.2015.2500819
   Petrovic VS, 2004, IEEE T IMAGE PROCESS, V13, P228, DOI 10.1109/tip.2004.823821
   Tang H, 2018, INFORM SCIENCES, V433, P125, DOI 10.1016/j.ins.2017.12.043
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wan T, 2009, IEEE T MULTIMEDIA, V11, P624, DOI 10.1109/TMM.2009.2017640
   Wang ZB, 2010, PATTERN RECOGN, V43, P2003, DOI 10.1016/j.patcog.2010.01.011
   Yap PT, 2004, IEE P-VIS IMAGE SIGN, V151, P128, DOI 10.1049/ip-vis:20040395
   Zhang BH, 2016, NEUROCOMPUTING, V174, P733, DOI 10.1016/j.neucom.2015.09.092
   Zhang Q, 2016, IEEE T IMAGE PROCESS, V25, P2045, DOI 10.1109/TIP.2016.2524212
   Zhang Q, 2009, SIGNAL PROCESS, V89, P1334, DOI 10.1016/j.sigpro.2009.01.012
   Zhang Y, 2017, INFORM FUSION, V35, P81, DOI 10.1016/j.inffus.2016.09.006
   Zhang Z, 1999, P IEEE, V87, P1315, DOI 10.1109/5.775414
   Zhao HJ, 2013, PATTERN RECOGN, V46, P1002, DOI 10.1016/j.patcog.2012.09.012
   Zhao WD, 2018, IEEE T MULTIMEDIA, V20, P866, DOI 10.1109/TMM.2017.2760100
   Zhou Z, 2014, INFORM FUSION, V20, P60, DOI 10.1016/j.inffus.2013.11.005
NR 37
TC 53
Z9 55
U1 5
U2 51
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2020
VL 22
IS 2
BP 285
EP 297
DI 10.1109/TMM.2019.2928516
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KH5FZ
UT WOS:000510676300001
DA 2024-07-18
ER

PT J
AU Zhang, SF
   Xie, YL
   Wan, J
   Xia, HS
   Li, SZ
   Guo, GD
AF Zhang, Shifeng
   Xie, Yiliang
   Wan, Jun
   Xia, Hansheng
   Li, Stan Z.
   Guo, Guodong
TI WiderPerson: A Diverse Dataset for Dense Pedestrian Detection in the
   Wild
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Benchmark testing; Detectors; Training; Urban areas; Cameras; Task
   analysis; Deep learning; Pedestrian detection; dataset; rich diversity;
   high density
ID MULTIPLE
AB Pedestrian detection has achieved significant progress with the availability of existing benchmark datasets. However, there is a gap in the diversity and density between real world requirements and current pedestrian detection benchmarks: first, most existing datasets are taken from a vehicle driving through the regular traffic scenario, usually leading to insufficient diversity; second, crowd scenarios with highly occluded pedestrians are still underrepresented, resulting in low density. To narrow this gap and facilitate future pedestrian detection research, we introduce a large and diverse dataset named WiderPerson for dense pedestrian detection in the wild. This dataset involves five types of annotations in a wide range of scenarios, no longer limited to the traffic scenario. There are a total of 13 382 images with 399 786 annotations, that is, 29.87 annotations per image, which means this dataset contains dense pedestrians with various kinds of occlusions. Hence, pedestrians in the proposed dataset are extremely challenging due to large variations in the scenario and occlusion, which is suitable to evaluate pedestrian detectors in the wild. We introduce an improved Faster R-CNN and the vanilla RetinaNet to serve as baselines for the new pedestrian detection benchmark. Several experiments are conducted on previous datasets including Caltech-USA and CityPersons to analyze the generalization capabilities of the proposed dataset, and we achieve state-of-the-art performances on these previous datasets without bells and whistles. Finally, we analyze common failure cases and find the classification ability of pedestrian detector needs to be improved to reduce false alarm and misdetection rates. The proposed dataset is available at http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson.
C1 [Zhang, Shifeng; Wan, Jun; Li, Stan Z.] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Ctr Biometr Secur Res, Beijing 100190, Peoples R China.
   [Zhang, Shifeng; Wan, Jun; Li, Stan Z.] Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
   [Xie, Yiliang] Univ Southern Calif, Los Angeles, CA 90007 USA.
   [Xia, Hansheng] Nanjing Univ Aeronaut & Astronaut, Coll Energy & Power Engn, Nanjing 210016, Peoples R China.
   [Li, Stan Z.] Macau Univ Sci & Technol, Macau 999078, Peoples R China.
   [Guo, Guodong] Baidu Res, Inst Deep Learning, Beijing 100193, Peoples R China.
   [Guo, Guodong] Natl Engn Lab Deep Learning Technol & Applicat, Beijing 100193, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   University of Southern California; Nanjing University of Aeronautics &
   Astronautics; Macau University of Science & Technology; Baidu
RP Wan, J (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Ctr Biometr Secur Res, Beijing 100190, Peoples R China.
EM shifeng.zhang@nlpr.ia.ac.cn; microos316@gmail.com;
   jun.wan@nlpr.ia.ac.cn; hanson_cha@163.com; szli@nlpr.ia.ac.cn;
   guoguodong01@baidu.com
RI Li, SY/JPK-3839-2023; zhang, Shifeng/HPH-0217-2023; wu,
   jd/IST-2336-2023; Guo, Guodong/M-5066-2015
OI Li, SY/0009-0000-9254-7115; Zhang, Shifeng/0000-0003-3109-5770; wan,
   jun/0000-0002-4735-2885; Guo, Guodong/0000-0001-9583-0055
FU National Key Research and Development Plan [2016YFC0801002]; Chinese
   National Natural Science Foundation [61876179, 61872367, 61806203];
   Science and Technology Development Fund of Macau [152/2017/A,
   0025/2018/A1, 008/2019/A1]
FX This work was supported in part by the National Key Research and
   Development Plan under Grant 2016YFC0801002, in part by the Chinese
   National Natural Science Foundation under Grants 61876179, 61872367, and
   61806203, and in part by the Science and Technology Development Fund of
   Macau under Grants 152/2017/A, 0025/2018/A1, and 008/2019/A1. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Wolfgang Hurst.
CR [Anonymous], 2014, VERY DEEP CONVOLUTIO
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], P BMVC
   [Anonymous], P 5 INT C COMP VIS S
   Braun M, 2019, IEEE T PATTERN ANAL, V41, P1844, DOI 10.1109/TPAMI.2019.2897684
   Brazil G, 2017, IEEE I CONF COMP VIS, P4960, DOI 10.1109/ICCV.2017.530
   Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22
   Cai ZW, 2015, IEEE I CONF COMP VIS, P3361, DOI 10.1109/ICCV.2015.384
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dollar D, 2009, DIR DEV, P1
   Dollár P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Du N, 2007, PROCEEDINGS OF THE IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE, P100, DOI 10.1109/WI.2007.36
   Duan GQ, 2010, LECT NOTES COMPUT SC, V6316, P238, DOI 10.1007/978-3-642-15567-3_18
   Enzweiler M, 2009, IEEE T PATTERN ANAL, V31, P2179, DOI 10.1109/TPAMI.2008.260
   Ess A, 2007, IEEE I CONF COMP VIS, P2065
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hwang S, 2015, PROC CVPR IEEE, P1037, DOI 10.1109/CVPR.2015.7298706
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Lan XY, 2018, IEEE T IMAGE PROCESS, V27, P2022, DOI 10.1109/TIP.2017.2777183
   Leibe B, 2005, PROC CVPR IEEE, P878
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li XF, 2016, IEEE INT VEH SYM, P1028
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Mao JY, 2017, PROC CVPR IEEE, P6034, DOI 10.1109/CVPR.2017.639
   Mihcak M. K., 2001, ACM WORKSH DIG RIGHT, P13
   Ouyang WL, 2013, PROC CVPR IEEE, P3198, DOI 10.1109/CVPR.2013.411
   Ouyang WL, 2012, PROC CVPR IEEE, P3258, DOI 10.1109/CVPR.2012.6248062
   Overett G, 2008, IEEE INT VEH SYM, P1038
   Paisitkriangkrai S, 2014, LECT NOTES COMPUT SC, V8692, P546, DOI 10.1007/978-3-319-10593-2_36
   Pepik B, 2013, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2013.422
   Rajaram RN, 2015, IEEE INT C INTELL TR, P2335, DOI 10.1109/ITSC.2015.377
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465
   Shao J, 2015, PROC CVPR IEEE, P4657, DOI 10.1109/CVPR.2015.7299097
   Silberstein S, 2014, IEEE INT VEH SYM, P859
   Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97
   Sundaresan A, 2009, IEEE T IMAGE PROCESS, V18, P2114, DOI 10.1109/TIP.2009.2022290
   Tian YL, 2015, IEEE I CONF COMP VIS, P1904, DOI 10.1109/ICCV.2015.221
   Tian YL, 2015, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR.2015.7299143
   Wang SG, 2018, IEEE T MULTIMEDIA, V20, P3148, DOI 10.1109/TMM.2018.2829602
   Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/iccv.2009.5459207
   Wang XL, 2018, PROC CVPR IEEE, P7774, DOI 10.1109/CVPR.2018.00811
   Wojek C, 2009, PROC CVPR IEEE, P794, DOI 10.1109/CVPRW.2009.5206638
   Yan JJ, 2013, PROC CVPR IEEE, P3033, DOI 10.1109/CVPR.2013.390
   Yang F, 2016, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2016.234
   Yang S, 2016, PROC CVPR IEEE, P5525, DOI 10.1109/CVPR.2016.596
   Zhang LL, 2016, LECT NOTES COMPUT SC, V9906, P443, DOI 10.1007/978-3-319-46475-6_28
   Zhang SS, 2018, PROC CVPR IEEE, P6995, DOI 10.1109/CVPR.2018.00731
   Zhang SS, 2018, IEEE T PATTERN ANAL, V40, P973, DOI 10.1109/TPAMI.2017.2700460
   Zhang SS, 2017, PROC CVPR IEEE, P4457, DOI 10.1109/CVPR.2017.474
   Zhang SS, 2016, PROC CVPR IEEE, P1259, DOI 10.1109/CVPR.2016.141
   Zhang SS, 2015, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2015.7298784
   Zhang SS, 2014, PROC CVPR IEEE, P947, DOI 10.1109/CVPR.2014.126
   Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442
   Zhang SF, 2018, LECT NOTES COMPUT SC, V11207, P657, DOI 10.1007/978-3-030-01219-9_39
   Zhou C., 2016, ASIAN C COMP VIS, P305
   Zhou CL, 2017, IEEE I CONF COMP VIS, P3506, DOI 10.1109/ICCV.2017.377
   Zhou K, 2016, DESTECH TRANS COMP
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 63
TC 79
Z9 96
U1 11
U2 44
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2020
VL 22
IS 2
BP 380
EP 393
DI 10.1109/TMM.2019.2929005
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KH5FZ
UT WOS:000510676300008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yan, CG
   Tu, YB
   Wang, XZ
   Zhang, YB
   Hao, XH
   Zhang, YD
   Dai, QH
AF Yan, Chenggang
   Tu, Yunbin
   Wang, Xingzheng
   Zhang, Yongbing
   Hao, Xinhong
   Zhang, Yongdong
   Dai, Qionghai
TI STAT: Spatial-Temporal Attention Mechanism for Video Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video captioning; spatial-temporal attention mechanism; encoder-decoder
   neural networks
ID TEXT; FRAMEWORK
AB Video captioning refers to automatic generate natural language sentences, which summarize the video contents. Inspired by the visual attention mechanism of human beings, temporal attention mechanism has been widely used in video description to selectively focus on important frames. However, most existing methods based on temporal attention mechanism suffer from the problems of recognition error and detail missing, because temporal attention mechanism cannot further catch significant regions in frames. In order to address above problems, we propose the use of a novel spatial-temporal attention mechanism (STAT) within an encoder-decoder neural network for video captioning. The proposed STAT successfully takes into account both the spatial and temporal structures in a video, so it makes the decoder to automatically select the significant regions in the most relevant temporal segments for word prediction. We evaluate our STAT on two well-known benchmarks: MSVD and MSR-VTT-10K. Experimental results show that our proposed STAT achieves the state-of-the-art performance with several popular evaluation metrics: BLEU-4, METEOR, and CIDEr.
C1 [Yan, Chenggang; Tu, Yunbin] Hangzhou Dianzi Univ, Inst Informat & Control, Hangzhou 310018, Peoples R China.
   [Wang, Xingzheng] Shenzhen Univ, Coll Mechatron & Control Engn, Shenzhen 518060, Peoples R China.
   [Zhang, Yongbing] Tsinghua Univ, Grad Sch Shenzhen, Shenzhen 518055, Peoples R China.
   [Hao, Xinhong] Beijing Inst Technol, Sci & Technol Mechatron Dynam Control Lab, Beijing 100081, Peoples R China.
   [Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
   [Dai, Qionghai] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
C3 Hangzhou Dianzi University; Shenzhen University; Tsinghua Shenzhen
   International Graduate School; Tsinghua University; Beijing Institute of
   Technology; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; Tsinghua University
RP Wang, XZ (corresponding author), Shenzhen Univ, Coll Mechatron & Control Engn, Shenzhen 518060, Peoples R China.
EM cgyan@hdu.edu.cn; tuyunbin1995@foxmail.com; xingzheng.wang@szu.edu.cn;
   zhang.yongbing@sz.tsinghua.edu.cn; haoxinhong@bit.edu.cn;
   zhyd@ict.ac.cn; daiqionghai@tsinghua.edu.cn
RI Dai, Qionghai/ABD-5298-2021
OI Dai, Qionghai/0000-0001-7043-3061; hao, xinhong/0000-0002-6448-4839; Tu,
   Yunbin/0000-0002-9525-9060
FU National Nature Science Foundation of China [61671196, 61525206,
   61701149]; Shenzhen Fundamental Research fund [JCYJ20180306174120445,
   JCYJ20160331185006518]; Zhejiang Province Nature Science Foundation of
   China [LR17F030006]; National Key Research and Development Program of
   China [2017YFC0820600,, 2017YFC0820605, 2017YFC0820604]; 111 Project
   [D17019]; Shenzhen University [2019041]
FX This work was supported in part by the National Nature Science
   Foundation of China under Grant 61671196, Grant 61525206, and Grant
   61701149, in part by the Shenzhen Fundamental Research fund under Grant
   JCYJ20180306174120445 and Grant JCYJ20160331185006518, in part by the
   Zhejiang Province Nature Science Foundation of China under Grant
   LR17F030006, in part by the National Key Research and Development
   Program of China under Grant 2017YFC0820600, Grant 2017YFC0820605, and
   Grant 2017YFC0820604, in part by 111 Project, D17019, and in part by the
   Startup Research fund of Shenzhen University under Grant 2019041. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Zhu Li.
CR [Anonymous], 2016, PROC 24 ACM INT C MU, DOI [DOI 10.1145/2964284.2984065, 10.1145/2964284.2984065]
   [Anonymous], 2016, P 24 ACM INT C MULTI, DOI DOI 10.1145/2964284.2984066
   [Anonymous], P NAACL HLT
   [Anonymous], 2015, ARXIV151202949
   Bescós J, 2005, IEEE T MULTIMEDIA, V7, P293, DOI 10.1109/TMM.2004.840598
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen X, 2015, Microsoft coco captions: Data collection and evaluation server
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Dong JF, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1082, DOI 10.1145/2964284.2984064
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Ferman AM, 2003, IEEE T MULTIMEDIA, V5, P244, DOI 10.1109/TMM.2003.811617
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hori C, 2017, IEEE I CONF COMP VIS, P4203, DOI 10.1109/ICCV.2017.450
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Liao YT, 2011, IEEE T MULTIMEDIA, V13, P132, DOI 10.1109/TMM.2010.2089504
   Liu XQ, 2012, IEEE T MULTIMEDIA, V14, P482, DOI 10.1109/TMM.2011.2177646
   Pan PB, 2016, PROC CVPR IEEE, P1029, DOI 10.1109/CVPR.2016.117
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Peng YX, 2019, IEEE T CIRC SYST VID, V29, P773, DOI 10.1109/TCSVT.2018.2808685
   Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI [DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]
   Qi JW, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2630
   Ramanathan V, 2015, IEEE I CONF COMP VIS, P4471, DOI 10.1109/ICCV.2015.508
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shetty R., 2016, P 24 ACM INT C MULTI, P1073
   Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tu YB, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1014, DOI 10.1145/3123266.3123354
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang XZ, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0162939
   Wang XZ, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0158664
   Wu L, 2015, IEEE T MULTIMEDIA, V17, P1137, DOI 10.1109/TMM.2015.2443556
   Xu CS, 2008, IEEE T MULTIMEDIA, V10, P421, DOI 10.1109/TMM.2008.917346
   Xu CS, 2008, IEEE T MULTIMEDIA, V10, P1342, DOI 10.1109/TMM.2008.2004912
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu YJ, 2018, IEEE T IMAGE PROCESS, V27, P4933, DOI 10.1109/TIP.2018.2846664
   Yan CG, 2018, IEEE T INTELL TRANSP, V19, P220, DOI 10.1109/TITS.2017.2749977
   Yan CG, 2018, IEEE T INTELL TRANSP, V19, P284, DOI 10.1109/TITS.2017.2749965
   Yan CG, 2014, IEEE SIGNAL PROC LET, V21, P573, DOI 10.1109/LSP.2014.2310494
   Yang Y, 2018, IEEE T IMAGE PROCESS, V27, P5600, DOI 10.1109/TIP.2018.2855422
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Zeiler M. D., 2012, CoRR
   Zhang CR, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1128
   Zhang JC, 2019, LECT NOTES COMPUT SC, V11295, P42, DOI 10.1007/978-3-030-05710-7_4
   Zhang XS, 2017, PROC CVPR IEEE, P6250, DOI 10.1109/CVPR.2017.662
   Zhao N, 2017, IEEE T MULTIMEDIA, V19, P2080, DOI 10.1109/TMM.2017.2722687
NR 54
TC 276
Z9 287
U1 17
U2 152
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2020
VL 22
IS 1
BP 229
EP 241
DI 10.1109/TMM.2019.2924576
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KB6AZ
UT WOS:000506577000020
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Zhang, J
   Yang, Y
   Zhuo, L
   Tian, Q
   Liang, X
AF Zhang, Jing
   Yang, Ying
   Zhuo, Li
   Tian, Qi
   Liang, Xi
TI Personalized Recommendation of Social Images by Constructing a User
   Interest Tree With Deep Features and Tag Trees
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; Semantics; Feature extraction; Predictive models;
   Cultural differences; Flickr; Training; Social image; personalized
   recommendation; user-interest tree; deep features; tag trees
ID NETWORKS
AB In view of the great diversity and complexity of social images, it is of great significance to improve the performance of personalized recommendation by learning a user interest from large-scale social images. Deep learning, as the latest research in the field of artificial intelligence, provides a new personalized recommendation solution of social images for learning a users interest. Moreover, social image sharing websites (such as Flickr) allow users to tag uploaded images with tags. As an important image semantic cue, effective tags not only represent the latent image information but also show personalized user interest. Therefore, a personalized recommendation method of social image is proposed by constructing a user-interest tree with deep features and tag trees in this paper. The main contributions of our paper are as follows: first, to efficiently make use of tags, a tag tree of social images is created by the re-ranked tags; second, for compactly representing the image content, deep features are learned by training the AlexNet network; third, a user-interest tree is constructed with deep features and tag trees that include the user-interest tree of social images and the user-interest tree of tags, respectively, and finally, a personalized recommendation system of social images is built based on a user-interest tree. Experiments on the NUS-WIDE dataset have shown that our method outperforms state-of-the-art methods in terms of both precision and recall of personalized recommendations.
C1 [Zhang, Jing; Yang, Ying; Zhuo, Li; Liang, Xi] Beijing Univ Technol, Beijing Key Lab Computat Intelligence & Intellige, Beijing 100124, Peoples R China.
   [Zhuo, Li] Collaborat Innovat Ctr Elect Vehicles, Beijing 100081, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
   [Tian, Qi] Huawei, Noahs Ark Lab, Comp Vis, Shenzhen 518129, Peoples R China.
C3 Beijing University of Technology; University of Texas System; University
   of Texas at San Antonio (UTSA); Huawei Technologies
RP Zhang, J (corresponding author), Beijing Univ Technol, Beijing Key Lab Computat Intelligence & Intellige, Beijing 100124, Peoples R China.
EM zhj@bjut.edu.cn; yying@emails.bjut.edu.cn; zhuoli@bjut.edu.cn;
   qi.tian@utsa.edu; liangxi627@emails.bjut.edu.cn
OI ZHANG, JING/0000-0003-1290-0738
FU Beijing Natural Science Foundation [4163071]; National Natural Science
   Foundation of China [61531006, 61602018, 61701011]; Beijing Municipal
   Natural Science Foundation Cooperation Beijing Education Committee
   [KZ201910005007]
FX This work was supported in part by the Beijing Natural Science
   Foundation under Grant 4163071, in part by the National Natural Science
   Foundation of China under Grant 61531006, Grant 61602018, and Grant
   61701011, and in part by the Beijing Municipal Natural Science
   Foundation Cooperation Beijing Education Committee under Grant
   KZ201910005007. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Jingdong Wang.
CR Cai Y, 2014, NEURAL NETWORKS, V58, P98, DOI 10.1016/j.neunet.2014.05.017
   Cai Y, 2014, IEEE T KNOWL DATA EN, V26, P766, DOI 10.1109/TKDE.2013.7
   Cui HL, 2015, CIT/IUCC/DASC/PICOM 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION TECHNOLOGY - UBIQUITOUS COMPUTING AND COMMUNICATIONS - DEPENDABLE, AUTONOMIC AND SECURE COMPUTING - PERVASIVE INTELLIGENCE AND COMPUTING, P1017, DOI 10.1109/CIT/IUCC/DASC/PICOM.2015.154
   Deng SG, 2017, IEEE T NEUR NET LEAR, V28, P1164, DOI 10.1109/TNNLS.2016.2514368
   Fang Q, 2014, IEEE T MULTIMEDIA, V16, P796, DOI 10.1109/TMM.2014.2298216
   Fu JL, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P344, DOI 10.1145/2736277.2741112
   Gao Y, 2013, IEEE T IMAGE PROCESS, V22, P363, DOI 10.1109/TIP.2012.2202676
   Geng X, 2015, IEEE I CONF COMP VIS, P4274, DOI 10.1109/ICCV.2015.486
   Heymann P., 2006, 201610 STANF U
   Hsu IC, 2013, APPL SOFT COMPUT, V13, P3745, DOI 10.1016/j.asoc.2013.03.004
   Jiang H, 2015, IEEE T SIGNAL PROCES, V63, P2678, DOI 10.1109/TSP.2015.2412919
   Jiang SH, 2015, IEEE T MULTIMEDIA, V17, P907, DOI 10.1109/TMM.2015.2417506
   Le P., 2015, P INT C PARS TECHN J, P87
   Lei CY, 2016, PROC CVPR IEEE, P2545, DOI 10.1109/CVPR.2016.279
   Lei Z, 2015, LECT NOTES ENG COMP, P27
   Liew SS, 2016, TURK J ELECTR ENG CO, V24, P1248, DOI 10.3906/elk-1311-58
   Liu X, 2015, IEEE IMAGE PROC, P3901, DOI 10.1109/ICIP.2015.7351536
   Mei T, 2011, ACM T INFORM SYST, V29, DOI 10.1145/1961209.1961213
   Min WQ, 2015, IEEE T MULTIMEDIA, V17, P1787, DOI 10.1109/TMM.2015.2463226
   Qian XM, 2014, IEEE T CYBERNETICS, V44, P2493, DOI 10.1109/TCYB.2014.2309593
   Qian XM, 2014, IEEE T KNOWL DATA EN, V26, P1763, DOI 10.1109/TKDE.2013.168
   Rawashdeh M, 2016, MULTIMED TOOLS APPL, V75, P13299, DOI 10.1007/s11042-015-2813-0
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Serratosa F, 2014, PATTERN RECOGN LETT, V45, P244, DOI 10.1016/j.patrec.2014.04.015
   Sui L, 2012, CHINESE J ELECTRON, V21, P697
   Van den Oord A., 2013, P NIPS
   Wang Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P79, DOI 10.1145/2733373.2806233
   Wu YC, 2016, IEEE T MULTIMEDIA, V18, P2135, DOI 10.1109/TMM.2016.2614220
   Yang Y, 2017, IEEE IMAGE PROC, P2164, DOI 10.1109/ICIP.2017.8296665
   Yang Y, 2016, 8TH INTERNATIONAL CONFERENCE ON INTERNET MULTIMEDIA COMPUTING AND SERVICE (ICIMCS2016), P172, DOI 10.1145/3007669.3007686
   Yuan ZF, 2016, IEEE VTS VEH TECHNOL, DOI 10.1109/VTCSpring.2016.7504361
   Zhang HW, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1079, DOI 10.1145/2733373.2806286
   Zhang H, 2017, J CHEM-NY, V2017, DOI 10.1155/2017/4513410
   Zhang J, 2017, IEEE T MULTIMEDIA, V19, P2439, DOI 10.1109/TMM.2017.2701641
   Zhang J, 2010, INT J PATTERN RECOGN, V24, P401, DOI 10.1142/S0218001410008019
   Zhu YY, 2017, INFORM SCIENCES, V375, P246, DOI 10.1016/j.ins.2016.09.021
NR 36
TC 30
Z9 31
U1 2
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2019
VL 21
IS 11
BP 2762
EP 2775
DI 10.1109/TMM.2019.2912124
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JJ7VX
UT WOS:000494363000006
DA 2024-07-18
ER

PT J
AU Tsang, SH
   Chan, YL
   Kuang, W
AF Tsang, Sik-Ho
   Chan, Yui-Lam
   Kuang, Wei
TI Mode Skipping for HEVC Screen Content Coding via Random Forest
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE HEVC; machine learning; random forest; screen content coding; video
   coding
ID INTRA BLOCK COPY
AB Screen content coding (SCC) is the extension to high-efficiency video coding (HEVC) for compressing screen content videos. New coding tools, intrablock copy (IBC), and palette (PLT) modes, are introduced to encode screen content (SC) such as texts and graphics. The IBC mode is used for encoding repeating patterns by performing block matching within the same frame, while the PLT mode is designed for SC with few distinct colors by coding the major colors and their corresponding locations using an index map. However, the use of IBC and PLT modes increases the encoder complexity remarkably though coding efficiency can be improved. Therefore, we propose to have a mode skipping approach to reduce the encoder complexity of SCC by making use of SC characteristics, neighbor coding unit (CU) correlations, and intermediate cost information via random forest (RF). Detailed feature analyses and sample preparation are also described. A novel hyperparameter tuning approach with the consideration of coding bitrate and encoding time is proposed for RFs at each CU size to further boost the encoding process. Experimental results show that our proposed approach can obtain 45.06% average encoding time reduction with only a 1.08% increase in Bjontegaard delta bitrate. Average encoding time can even be reduced to 58.57% by regulating the hyperparameters.
C1 [Tsang, Sik-Ho; Chan, Yui-Lam; Kuang, Wei] Hong Kong Polytech Univ, Dept Elect & Informat Engn, Hung Hom, Hong Kong, Peoples R China.
C3 Hong Kong Polytechnic University
RP Chan, YL (corresponding author), Hong Kong Polytech Univ, Dept Elect & Informat Engn, Hung Hom, Hong Kong, Peoples R China.
EM sik-ho.tsang@polyu.edu.hk; enylchan@polyu.edu.hk;
   wei.kuang@connect.polyu.hk
RI Chan, Yui-Lam/C-3799-2014
OI Chan, Yui-Lam/0000-0002-1473-094X; Kuang, Wei/0000-0002-6801-7744;
   TSANG, Sik Ho/0000-0002-8578-0696
FU Hong Kong Research Grants Council [PolyU 152112/17E]
FX The work was supported by the Hong Kong Research Grants Council under
   Research Grant PolyU 152112/17E.
CR [Anonymous], 2014, N14175 ISOIEC JTC1SC
   [Anonymous], 2016, JCTVCX1015, P1
   [Anonymous], 2014, 2014 C LAS EL CLEO L
   Bjontegaard G., 2001, ITUTVCEGM33, P1
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Budagavi M, 2013, PICT COD SYMP, P365, DOI 10.1109/PCS.2013.6737759
   Chen CC, 2017, IEEE T CIRC SYST VID, V27, P1568, DOI 10.1109/TCSVT.2016.2543098
   Du BC, 2015, ASIAPAC SIGN INFO PR, P1085, DOI 10.1109/APSIPA.2015.7415439
   Duanmu F, 2016, IEEE J EM SEL TOP C, V6, P517, DOI 10.1109/JETCAS.2016.2597698
   Duanmu F, 2015, IEEE IMAGE PROC, P4972, DOI 10.1109/ICIP.2015.7351753
   Fang S, 2016, IEEE IMAGE PROC, P4022, DOI 10.1109/ICIP.2016.7533115
   Guo JK, 2017, J HYDRAUL RES, V55, P27, DOI 10.1080/00221686.2016.1212945
   Ho TK, 1998, IEEE T PATTERN ANAL, V20, P832, DOI 10.1109/34.709601
   Huang JJ, 2015, IEEE T IMAGE PROCESS, V24, P3232, DOI 10.1109/TIP.2015.2440751
   Kawakami Y, 2016, 2016 IEEE 12TH INTERNATIONAL COLLOQUIUM ON SIGNAL PROCESSING & ITS APPLICATIONS (CSPA), P46, DOI 10.1109/CSPA.2016.7515801
   Krzywinski M, 2014, NAT METHODS, V11, P119, DOI 10.1038/nmeth.2813
   Kuang W., IEEE T CIRCUITS SYST
   Kuang W, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.5.053029
   Kuang W, 2017, IEEE IMAGE PROC, P2473, DOI 10.1109/ICIP.2017.8296727
   Kwon DK, 2014, IEEE INT SYMP CIRC S, P9, DOI 10.1109/ISCAS.2014.6865052
   Lainema J, 2015, I SYMP CONSUM ELECTR, P333, DOI 10.1109/ICCE.2015.7066434
   Lainema J, 2012, IEEE T CIRC SYST VID, V22, P1792, DOI 10.1109/TCSVT.2012.2221525
   Lei JJ, 2017, IEEE T BROADCAST, V63, P48, DOI 10.1109/TBC.2016.2623241
   Lin T, 2013, IEEE T CIRC SYST VID, V23, P173, DOI 10.1109/TCSVT.2012.2223871
   Liu ZS, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1019, DOI 10.1109/ICIT.2017.7915501
   Louppe G, 2014, THESIS U LIEGE, P1
   Lu Y, 2011, IEEE MULTIMEDIA, V18, P4, DOI 10.1109/MMUL.2011.33
   Peng WH, 2016, IEEE J EM SEL TOP C, V6, P393, DOI 10.1109/JETCAS.2016.2608971
   Pu W, 2016, IEEE J EM SEL TOP C, V6, P420, DOI 10.1109/JETCAS.2016.2605661
   Sullivan GJ, 2013, IEEE J-STSP, V7, P1001, DOI 10.1109/JSTSP.2013.2283657
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun YC, 2016, IEEE IMAGE PROC, P4210, DOI 10.1109/ICIP.2016.7533153
   Tsang S.-H., 2016, P APSIPA ANN SUMM C, P1
   Tsang SH, 2019, IEEE T MULTIMEDIA, V21, P269, DOI 10.1109/TMM.2018.2856078
   Tsang SH, 2017, IEEE IMAGE PROC, P260, DOI 10.1109/ICIP.2017.8296283
   Tsang SH, 2015, INT CONF ACOUST SPEE, P1409, DOI 10.1109/ICASSP.2015.7178202
   Tsang SH, 2015, ASIAPAC SIGN INFO PR, P396, DOI 10.1109/APSIPA.2015.7415302
   Tsang SH, 2014, INT CONF DIGIT SIG, P888, DOI 10.1109/ICDSP.2014.6900796
   Wang S., 2015, IEEE T CIRCUITS SYST, V26, P1595
   Xu JZ, 2016, IEEE T CIRC SYST VID, V26, P50, DOI 10.1109/TCSVT.2015.2478706
   Xu XZ, 2016, IEEE J EM SEL TOP C, V6, P409, DOI 10.1109/JETCAS.2016.2597645
   Yang H, 2017, IEEE IMAGE PROC, P2468, DOI 10.1109/ICIP.2017.8296726
   Zhang H, 2016, INT CONF ACOUST SPEE, P1377, DOI 10.1109/ICASSP.2016.7471902
   Zhang J, 2010, J COSMOL ASTROPART P, DOI 10.1088/1475-7516/2010/05/001
   Zhang K, 2015, IEEE INT SYMP CIRC S, P521, DOI 10.1109/ISCAS.2015.7168685
   Zhang MM, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P390, DOI 10.1109/VCIP.2014.7051588
   Zhang ZD, 2015, IEEE IMAGE PROC, P4102, DOI 10.1109/ICIP.2015.7351577
   Zhao LP, 2018, IEEE T MULTIMEDIA, V20, P796, DOI 10.1109/TMM.2017.2758519
   Zhao LP, 2016, IEEE T MULTIMEDIA, V18, P339, DOI 10.1109/TMM.2015.2512539
   Zhu WJ, 2017, IEEE T BROADCAST, V63, P673, DOI 10.1109/TBC.2017.2711144
   Zhu WJ, 2015, IEEE T MULTIMEDIA, V17, P935, DOI 10.1109/TMM.2015.2428171
NR 52
TC 21
Z9 22
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2019
VL 21
IS 10
BP 2433
EP 2446
DI 10.1109/TMM.2019.2907472
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JD1IM
UT WOS:000489728400001
DA 2024-07-18
ER

PT J
AU Xu, JY
   Xu, M
   Wei, YA
   Wang, ZL
   Guan, ZY
AF Xu, Jingyao
   Xu, Mai
   Wei, Yanan
   Wang, Zulin
   Guan, Zhenyu
TI Fast H.264 to HEVC Transcoding: A Deep Learning Method
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE H.264; HEVC; transcoding; deep learning; LSTM
ID EFFICIENCY; ALLOCATION
AB With the development of video coding technology, high-efficiency video coding (HEVC) has become a promising alternative, compared with the previous coding standards, for example, H.264. In general, H.264 to HEVC transcoding can be accomplished by fully H.264 decoding and fully HEVC encoding, which suffers from considerable time consumption on the brute-force search of the HEVC coding tree unit (CTU) partition for rate-distortion optimization (RDO). In this paper, we propose a deep learning method to predict the HEVC CTU partition, instead of the brute-force RDO search, for H.264 to HEVC transcoding. First, we build a large-scale H.264 to HEVC transcoding database. Second, we investigate the correlation between the HEVC CTU partition and H.264 features, and analyze both temporal and spatial-temporal similarities of the CTU partition across video frames. Third, we propose a deep learning architecture of a hierarchical long short-term memory (H-LSTM) network to predict the CTU partition of HEVC. Then, the brute-force RDO search of the CTU partition is replaced by the H-LSTM prediction such that the computational time can be significantly reduced for fast H.264 to HEVC transcoding. Finally, the experimental results verify that the proposed H-LSTM method can achieve a better tradeoff between coding efficiency and complexity, compared to the state-of-the-art H.264 to HEVC transcoding methods.
C1 [Xu, Jingyao; Xu, Mai; Wei, Yanan; Wang, Zulin; Guan, Zhenyu] Beihang Univ, Sch Elect & Informat Engn, Beijing 100191, Peoples R China.
C3 Beihang University
RP Xu, M (corresponding author), Beihang Univ, Sch Elect & Informat Engn, Beijing 100191, Peoples R China.
EM xxxjy@buaa.edu.cn; maixu@buaa.edu.cn; yananwei@buaa.edu.cn;
   wzulin@buaa.edu.cn; guanzhenyu@buaa.edu.cn
RI Zhang, Kai/KBD-3312-2024; Xu, Jingyao/GQB-0952-2022
OI GUAN, zhenyu/0000-0002-3959-338X
FU National Nature Science Foundation of China [61573037, 61876013 151061]
FX This work was supported in part by the National Nature Science
   Foundation of China under Grants 61573037 and 61876013 151061.
CR [Anonymous], BT 500 11 METH SUBJ
   [Anonymous], 2014, SIGNAL IMAGE PROCESS
   [Anonymous], IEEE INT TEL S ITS
   Chen YC, 2015, IEEE T CIRC SYST VID, V25, P1423, DOI 10.1109/TCSVT.2014.2380231
   Correa G, 2016, IEEE INT SYMP CIRC S, P2539, DOI 10.1109/ISCAS.2016.7539110
   Correa G, 2015, IEEE T CIRC SYST VID, V25, P660, DOI 10.1109/TCSVT.2014.2363753
   Díaz-Honrubia AJ, 2014, IEEE IMAGE PROC, P2497, DOI 10.1109/ICIP.2014.7025505
   Dong Zhang, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P651, DOI 10.1109/ICME.2012.112
   Franche JF, 2015, IEEE IMAGE PROC, P477, DOI 10.1109/ICIP.2015.7350844
   Huangyuan QX, 2015, ASIAPAC SIGN INFO PR, P563, DOI 10.1109/APSIPA.2015.7415333
   Díaz-Honrubia AJ, 2016, IEEE T CIRC SYST VID, V26, P154, DOI 10.1109/TCSVT.2015.2473299
   Jiang W, 2013, ELECTRON LETT, V49, DOI 10.1049/el.2013.0329
   Jiang W, 2014, MULTIMED TOOLS APPL, V73, P2179, DOI 10.1007/s11042-013-1675-6
   King DB, 2015, ACS SYM SER, V1214, P1
   Li SX, 2017, IEEE T CIRC SYST VID, V27, P2409, DOI 10.1109/TCSVT.2016.2589878
   Lin CS, 2016, J VIS COMMUN IMAGE R, V38, P130, DOI 10.1016/j.jvcir.2016.03.003
   Mora EG, 2017, MULTIMED TOOLS APPL, V76, P8991, DOI 10.1007/s11042-016-3498-8
   Nagaraghatta A, 2015, 2015 IEEE 5TH INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS - BERLIN (ICCE-BERLIN), P165, DOI 10.1109/ICCE-Berlin.2015.7391223
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   Peixoto E, 2013, IEEE IMAGE PROC, P1972, DOI 10.1109/ICIP.2013.6738406
   Peixoto E, 2014, IEEE T CIRC SYST VID, V24, P99, DOI 10.1109/TCSVT.2013.2273651
   Peixoto E, 2012, IEEE IMAGE PROC, P737, DOI 10.1109/ICIP.2012.6466965
   Shanableh T, 2010, NEUROCOMPUTING, V73, P1752, DOI 10.1016/j.neucom.2009.11.045
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Shen T, 2013, IEEE DATA COMPR CONF, P241, DOI 10.1109/DCC.2013.32
   Tao XM, 2015, IEEE NETWORK, V29, P14, DOI 10.1109/MNET.2015.7340419
   Vetro A, 2003, IEEE SIGNAL PROC MAG, V20, P18, DOI 10.1109/MSP.2003.1184336
   Wan L, 2013, P 30 INT C MACH LEAR, P1058, DOI DOI 10.5555/3042817.3043055
   Wei Y., 2017, PROC VIS COMMUN IMAG, P1
   Wei YN, 2017, IEEE GEOSCI REMOTE S, V14, P709, DOI 10.1109/LGRS.2017.2672734
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P559, DOI 10.1109/TMM.2013.2291958
   Xu M, 2019, IEEE T PATTERN ANAL, V41, P2693, DOI 10.1109/TPAMI.2018.2858783
   Xu M, 2018, IEEE T IMAGE PROCESS, V27, P5044, DOI 10.1109/TIP.2018.2847035
   Xu M, 2014, IEEE J-STSP, V8, P475, DOI 10.1109/JSTSP.2014.2314864
   Yuan H, 2017, IEEE T MULTIMEDIA, V19, P1416, DOI 10.1109/TMM.2017.2669858
   Zhang DJ, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0151864
   Zhang JL, 2016, IEEE T CIRC SYST VID, V26, P1502, DOI 10.1109/TCSVT.2015.2461991
   Zhang Y, 2015, IEEE T IMAGE PROCESS, V24, P2225, DOI 10.1109/TIP.2015.2417498
   Zheng FY, 2014, 2014 INTERNATIONAL CONFERENCE ON AUDIO, LANGUAGE AND IMAGE PROCESSING (ICALIP), VOLS 1-2, P765, DOI 10.1109/ICALIP.2014.7009898
   Zhu LW, 2016, J VIS COMMUN IMAGE R, V38, P824, DOI 10.1016/j.jvcir.2016.04.020
NR 40
TC 12
Z9 12
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2019
VL 21
IS 7
BP 1633
EP 1645
DI 10.1109/TMM.2018.2885921
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IF6IF
UT WOS:000473183700002
DA 2024-07-18
ER

PT J
AU Jiang, ZB
   Xu, CQ
   Guan, JF
   Liu, Y
   Muntean, GM
AF Jiang, Zhongbai
   Xu, Changqiao
   Guan, Jianfeng
   Liu, Yang
   Muntean, Gabriel-Miro
TI Stochastic Analysis of DASH-Based Video Service in High-Speed Railway
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video Streaming; DASH; Stochastic Analysis; Service Optimization; HSR
   Networks
ID COMMUNICATION; ALGORITHM; MANAGEMENT; SYSTEMS; QOE
AB The latest increasing popularity of high-speed railways (HSR) has stimulated growing demands for wireless Internet services in HSR networks, especially for video streaming. However, due to the high variability and unpredictability of wireless communications in HSR networks, it is still difficult for the existing solutions to provide high-quality video streaming services to HSR passengers. This paper addresses this crucial problem first by reporting on field experiments performed to investigate the characteristics of HSR networks. Then the paper formulates an intractable optimization problem for dynamic adaptive streaming over HTTP (DASH)-enabling service in HSR networks considering various factors, including packet loss, energy consumption, video service quality, etc. By leveraging Lyapunov optimization approaches, the formulated optimization problem is transformed into a queue stability problem which is of high scalability and generality. Moreover, in order to overcome the intractability of the initial optimization problem, the queue stability problem is further decomposed into three subproblems which can be easily solved individually. Finally, a novel joint stochastic DASH optimization (JSDO) mechanism consisting of three algorithms for the derived subproblems is proposed. Rigorous theoretical analyses and realistic dataset-based simulations demonstrate the effectiveness of the proposed JSDO mechanism.
C1 [Jiang, Zhongbai; Xu, Changqiao; Guan, Jianfeng; Liu, Yang] Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.
   [Muntean, Gabriel-Miro] Dublin City Univ, Performance Engn Lab, Sch Elect Engn, Dublin 9, Ireland.
C3 Beijing University of Posts & Telecommunications; Dublin City University
RP Xu, CQ (corresponding author), Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.
EM zbjiang@bupt.edu.cn; cqxu@bupt.edu.cn; jfguan@bupt.edu.cn;
   liu.yang@bupt.edu.cn; gabriel.muntean@dcu.ie
RI Chen, John/GPW-8839-2022; Liu, Yang/AAF-5125-2020; Xu,
   Chang/GQP-7280-2022; Muntean, Gabriel-Miro/U-6783-2019; xu,
   cj/HJZ-3488-2023
OI Muntean, Gabriel-Miro/0000-0002-9332-4770; Guan,
   Jianfeng/0000-0002-4411-0741
FU National Natural Science Foundation of China [61871048, 61522103,
   61872253, 61602038]; EU Horizon 2020 Grant [688503]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61871048, 61522103, 61872253, and
   61602038, and in part by the EU Horizon 2020 Grant 688503. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Christian Timmerer.
CR Aguayo M, 2018, IEEE T MULTIMEDIA, V20, P1224, DOI 10.1109/TMM.2017.2764325
   Ali W, 2017, IEEE ICC
   [Anonymous], 2016, PROC IEEE VTC
   [Anonymous], CHINADAILY AUG
   [Anonymous], 2011, CISCO VISUAL NETWORK
   [Anonymous], P IEEE C COMP COMM
   [Anonymous], IEEE MULTIMEDIA
   Bagci KT, 2017, IEEE T MULTIMEDIA, V19, P2152, DOI 10.1109/TMM.2017.2736638
   Bentaleb A, 2017, IEEE T MULTIMEDIA, V19, P2136, DOI 10.1109/TMM.2017.2733344
   Chang HB, 2017, IEEE T VEH TECHNOL, V66, P8398, DOI 10.1109/TVT.2017.2676813
   Chang YJ, 2014, IEEE WCNC, P1950, DOI 10.1109/WCNC.2014.6952568
   Chiti F, 2014, IEEE T VEH TECHNOL, V63, P2450, DOI 10.1109/TVT.2013.2291432
   Gao GY, 2015, IEEE ICC, P6880, DOI 10.1109/ICC.2015.7249422
   Ghazzai H, 2017, IEEE T VEH TECHNOL, V66, P175, DOI 10.1109/TVT.2016.2542344
   Hosseini M, 2017, IEEE T MULTIMEDIA, V19, P2307, DOI 10.1109/TMM.2017.2733298
   Hosseini Mohammad., 2013, Proceedings of the 4th ACM Multimedia Systems Conference, MMSys '13, P1
   Hu Y, 2017, IEEE T COMMUN, V65, P3066, DOI 10.1109/TCOMM.2017.2687930
   Jiang Z., 2017, 2017 AUSTR U POW ENG, P1, DOI [10.1007/s1113-017-0617-5, DOI 10.1007/S1113-017-0617-5]
   Jin YC, 2016, IEEE T MULTIMEDIA, V18, P807, DOI 10.1109/TMM.2016.2537199
   Kanai K, 2016, IEEE J SEL AREA COMM, V34, P2102, DOI 10.1109/JSAC.2016.2577238
   Li T, 2017, IEEE ACCESS, V5, P8343, DOI 10.1109/ACCESS.2017.2702616
   Naito Hiromitsu, 2016, Research Bulletin of the Plant Protection Service Japan, P1
   Neely M. J., 2010, STOCHASTIC NETWORK O
   Qadir QM, 2015, IEEE T MULTIMEDIA, V17, P711, DOI 10.1109/TMM.2015.2416637
   Qiao J, 2016, IEEE T WIREL COMMUN, V15, P7187, DOI 10.1109/TWC.2016.2598748
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Song H, 2016, IEEE WIREL COMMUN, V23, P114, DOI 10.1109/MWC.2016.1500255WC
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Sun NX, 2017, IEEE ACCESS, V5, P620, DOI 10.1109/ACCESS.2016.2646461
   Wang JZ, 2012, IEEE J SEL AREA COMM, V30, P675, DOI 10.1109/JSAC.2012.120502
   Wu D, 2017, IEEE T MULTIMEDIA, V19, P2571, DOI 10.1109/TMM.2017.2700621
   Xu CQ, 2017, IEEE WIREL COMMUN, V24, P157, DOI 10.1109/MWC.2017.1600219WC
   Xu CQ, 2017, COMPUT COMMUN, V99, P93, DOI 10.1016/j.comcom.2016.07.014
   Xu CQ, 2015, IEEE T CIRC SYST VID, V25, P1175, DOI 10.1109/TCSVT.2014.2376138
   Xu QS, 2016, IEEE T VEH TECHNOL, V65, P5251, DOI 10.1109/TVT.2015.2458512
   Xu SF, 2014, IEEE ICC, P2855, DOI 10.1109/ICC.2014.6883757
   Xu SH, 2014, MATH PROBL ENG, V2014, DOI 10.1155/2014/480147
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Yuan H, 2018, IEEE T MULTIMEDIA, V20, P183, DOI 10.1109/TMM.2017.2724850
   Zhang WW, 2013, IEEE T MULTIMEDIA, V15, P1431, DOI 10.1109/TMM.2013.2247583
   Zhou YZ, 2014, COMPUT COMMUN, V47, P1, DOI 10.1016/j.comcom.2014.04.005
NR 41
TC 22
Z9 23
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2019
VL 21
IS 6
BP 1577
EP 1592
DI 10.1109/TMM.2018.2881095
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IA1QQ
UT WOS:000469337400019
DA 2024-07-18
ER

PT J
AU Nguyen, TD
   Kim, B
   Hong, MC
AF Tien-Dat Nguyen
   Kim, Beomsu
   Hong, Min-Cheol
TI New Hole-Filling Method Using Extrapolated Spatio-Temporal Background
   Information for a Synthesized Free-View
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hole-filling; temporal background modeling; local background estimation;
   ghost removal; inpainting
ID IMAGE GENERATION; DEPTH; VIDEO; SUBTRACTION
AB This paper introduces a new hole-filling method using extrapolated spatio-temporal background information to obtain a synthesized free-view. New temporal background modeling is proposed, which incorporates stationary temporal information in the hole-filling process and preserves the temporal consistency of synthesized views. A background codebook is distinguished from a non-overlapped patch-based codebook, which contributes to extracting reliable temporal background information. Furthermore, a depth-map driven spatial local background estimation is also addressed to discriminate the background holes in each disocclusion and to define two spatial BG constraints that represent the lower and upper bounds of a background candidate. Holes are filled by comparing the similarities between the temporal background information and the spatial background constraints. In addition, a depth map-based ghost removal filter is described to solve the problem of the non-fit between a color image and the corresponding depth map of a virtual view. Finally, an exemplar-based inpainting is applied to fill in the remaining holes with a priority function that includes a new depth term. The experimental results demonstrated that the proposed method led to results that promised subjective and objective improvement over state-of-the-art methods.
C1 [Tien-Dat Nguyen; Kim, Beomsu; Hong, Min-Cheol] Soongsil Univ, Sch Elect Engn, Seoul 156743, South Korea.
C3 Soongsil University
RP Hong, MC (corresponding author), Soongsil Univ, Sch Elect Engn, Seoul 156743, South Korea.
EM datbkhn2k10@gmail.com; rhand41@ssu.ac.kr; mhong@ssu.ac.kr
RI 辛, 雨菲/JBS-6390-2023
FU National Research Foundation of Korea - Korean government (MIST)
   [2017R1A2B4002205]
FX This work was supported by the National Research Foundation of Korea
   grant funded by Korean government (MIST) (2017R1A2B4002205).
CR [Anonymous], COMP GRAPH
   Choi S, 2013, IEEE T IMAGE PROCESS, V22, P2429, DOI 10.1109/TIP.2013.2251646
   Criminisi A, 2003, PROC CVPR IEEE, P721
   Daribo I, 2011, IEEE T BROADCAST, V57, P533, DOI 10.1109/TBC.2011.2125110
   Doan H.N., 2015, P 2015 RES AD CONV S, P152, DOI [10.1145/2811411.2811497, DOI 10.1145/2811411.2811497]
   Doan H. N., 2014, P RES AD CONV SYST, P100
   Doan HN, 2015, LECT NOTES COMPUT SC, V9315, P598, DOI 10.1007/978-3-319-24078-7_61
   Dodgson NA, 2005, COMPUTER, V38, P31, DOI 10.1109/MC.2005.252
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Fernandez-Sanchez EJ, 2013, SENSORS-BASEL, V13, P8895, DOI 10.3390/s130708895
   Glassner A., 1990, GRAPHICS GEMS I
   Guillemot C, 2014, IEEE SIGNAL PROC MAG, V31, P127, DOI 10.1109/MSP.2013.2273004
   Kauff P, 2007, SIGNAL PROCESS-IMAGE, V22, P217, DOI 10.1016/j.image.2006.11.013
   Kim K, 2004, IEEE IMAGE PROC, P3061
   Köppel M, 2012, IEEE INT WORKSH MULT, P25, DOI 10.1109/MMSP.2012.6343410
   Lee PJ, 2011, IEEE T MULTIMEDIA, V13, P246, DOI 10.1109/TMM.2010.2100372
   Macchiavello B, 2014, IEEE T MULTIMEDIA, V16, P711, DOI 10.1109/TMM.2014.2299768
   Masnou S, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P259, DOI 10.1109/ICIP.1998.999016
   Mori Y, 2009, SIGNAL PROCESS-IMAGE, V24, P65, DOI 10.1016/j.image.2008.10.013
   Muddala SM, 2016, J VIS COMMUN IMAGE R, V38, P351, DOI 10.1016/j.jvcir.2016.02.017
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   Oh KJ, 2009, PCS: 2009 PICTURE CODING SYMPOSIUM, P233
   Smolic A, 2011, PATTERN RECOGN, V44, P1958, DOI 10.1016/j.patcog.2010.09.005
   Smolic A, 2011, P IEEE, V99, P607, DOI 10.1109/JPROC.2010.2098350
   Solh M, 2012, IEEE J-STSP, V6, P495, DOI 10.1109/JSTSP.2012.2204723
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xi M, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-9
   Yao C, 2014, IEEE T BROADCAST, V60, P394, DOI 10.1109/TBC.2014.2321671
   Zhang L, 2005, IEEE T BROADCAST, V51, P191, DOI 10.1109/TBC.2005.846190
   Zhu C, 2016, IEEE T BROADCAST, V62, P82, DOI 10.1109/TBC.2015.2475697
   Zinger S, 2010, J VIS COMMUN IMAGE R, V21, P533, DOI 10.1016/j.jvcir.2010.01.004
   Zinger S, 2012, IEEE T CIRC SYST VID, V22, P128, DOI 10.1109/TCSVT.2011.2158362
   Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766
NR 33
TC 8
Z9 8
U1 1
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2019
VL 21
IS 6
BP 1345
EP 1358
DI 10.1109/TMM.2018.2880954
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IA1QQ
UT WOS:000469337400001
DA 2024-07-18
ER

PT J
AU Wang, M
   Xu, CQ
   Chen, XY
   Hao, H
   Zhong, LJ
   Yu, S
AF Wang, Mu
   Xu, Changqiao
   Chen, Xingyan
   Hao, Hao
   Zhong, Lujie
   Yu, Shui
TI Differential Privacy Oriented Distributed Online Learning for Mobile
   Social Video Prefetching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mobile video; social network; content prefetching; differential privacy;
   distributed online learning
ID RECOMMENDATION; INFORMATION; SERVICES
AB The ever fast growing mobile social video traffic has motivated the urgent requirement of alleviating backbone pressures while ensuring the user-quality experience. Mobile video prefetching previously caches the future accessed videos at the edge, which has become a promising solution for traffic offloading and delay reduction. However, providing high performance prefetching still remains problematic in the presence of high dynamic mobile users' viewing behaviors and consecutive generated video content. Besides, given the fact that making prefetching decision requires viewing history that is sensitive, the increasing privacy issues should also be considered. In this paper, we propose a differential privacy oriented distributed online learning method for mobile social video prefetching (DPDL-SVP). Through a large-scale data analysis based on one of the most popular online social network sites, WeiBo.cn, we reveal that users' viewing behaviors have strong a relation with video preference, content popularity, and social interactions. We then formulate the prefetching problem as an online convex optimization based on these three factors. Furthermore, the problem is divided into two subproblems, and we implement a distributed algorithm separately to solve them with differential privacy. The performance bound of the proposed online algorithms is also theoretically proved. We conduct a series simulation based on real viewing traces to evaluate the performance of DPDL-SVP. Evaluation results show how our proposed algorithms achieve superior performance in terms of the prediction accuracy, delay reduction, and scalability.
C1 [Wang, Mu; Xu, Changqiao; Chen, Xingyan; Hao, Hao; Zhong, Lujie] Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.
   [Zhong, Lujie] Capital Normal Univ, Informat Engn Coll, Beijing 100048, Peoples R China.
   [Yu, Shui] Guangzhou Univ, Sch Comp Sci, Guangzhou 510006, Guangdong, Peoples R China.
   [Yu, Shui] Univ Technol Sydney, Sch Software, Sydney, NSW 2007, Australia.
C3 Beijing University of Posts & Telecommunications; Capital Normal
   University; Guangzhou University; University of Technology Sydney
RP Yu, S (corresponding author), Guangzhou Univ, Sch Comp Sci, Guangzhou 510006, Guangdong, Peoples R China.; Yu, S (corresponding author), Univ Technol Sydney, Sch Software, Sydney, NSW 2007, Australia.
EM wangmu@bupt.edu.cn; cqxu@bupt.edu.cn; chenxingyan@bupt.edu.cn;
   hao_hao@bupt.edu.cn; zhonglj@cnu.edu.cn; shui.yu@uts.edu.au
RI Chen, Xingyan/AFL-4355-2022; Yu, Shui/AFL-2699-2022; Chen,
   John/GPW-8839-2022; 郝, 昊/IUO-2107-2023; xu, cj/HJZ-3488-2023; Xu,
   Chang/GQP-7280-2022
OI Chen, Xingyan/0000-0003-0861-2100; Yu, Shui/0000-0003-4485-6743; 郝,
   昊/0000-0003-2765-3303; Zhong, Lujie/0000-0002-2111-0896
FU National Natural Science Foundation of China [61871048, 61872253,
   61728201, 61522103]; Open Foundation of State Key Laboratory of
   Networking and Switching Technology (Beijing University of Posts and
   Telecommunications) [SKLNST-2018-1-05]; Australia ARC [DP 180102828];
   BUPT Excellent Ph.D.; Students Foundation [CX2017312]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61871048, 61872253, 61728201, and
   61522103; in part by the Open Foundation of State Key Laboratory of
   Networking and Switching Technology (Beijing University of Posts and
   Telecommunications) under Grant SKLNST-2018-1-05; in part by the
   Australia ARC DP 180102828; and in part by the BUPT Excellent Ph.D.
   Students Foundation CX2017312. The associate editor coordinating the
   review of this manuscript and approving it for publication was Qiang Ye.
   (Corresponding author: Shui Yu.)
CR Acs  G., 2018, IEEE T DEPEND SECURE, P1
   [Anonymous], 2018, NETWORK SIMULATOR 3
   Bertsekas D. P., 1997, Journal of the Operational Research Society, V48, P334, DOI 10.1057/palgrave.jors.2600425
   Boyd S.P., 2004, Convex optimization, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441]
   Chen XL, 2018, IEEE J SEL AREA COMM, V36, P1593, DOI 10.1109/JSAC.2018.2825658
   Dwork  C., 2006, P INT C AUT LANG PRO, P1
   Dwork C, 2008, LECT NOTES COMPUT SC, V4890, P1
   Hu H, 2018, IEEE T MULTIMEDIA, V20, P1864, DOI 10.1109/TMM.2017.2779041
   Hu H, 2018, IEEE T CIRC SYST VID, V28, P759, DOI 10.1109/TCSVT.2016.2620152
   Hu H, 2017, IEEE J SEL AREA COMM, V35, P935, DOI 10.1109/JSAC.2017.2676598
   Hu WB, 2017, ADV POLYM SCI, V276, P1, DOI 10.1007/12_2016_349
   Kilanioti I, 2015, IEEE T MULTIMEDIA, V17, P1460, DOI 10.1109/TMM.2015.2459658
   Liu D, 2018, IEEE GLOB COMM CONF
   Ma G, 2017, IEEE INT CON MULTI, P7, DOI 10.1109/ICME.2017.8019404
   Mauri G, 2017, IEEE T VEH TECHNOL, V66, P2513, DOI 10.1109/TVT.2016.2580586
   McSherry F, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P627
   Shen HY, 2014, IEEE T PARALL DISTR, V25, P2428, DOI 10.1109/TPDS.2013.139
   Su Z, 2018, IEEE T IND INFORM, V14, P4579, DOI 10.1109/TII.2018.2849984
   Su Z, 2018, IEEE J SEL AREA COMM, V36, P2175, DOI 10.1109/JSAC.2018.2869948
   Sweeney L, 2002, INT J UNCERTAIN FUZZ, V10, P557, DOI 10.1142/S0218488502001648
   Wang X, 2013, IEEE WIREL COMMUN, V20, P72, DOI 10.1109/MWC.2013.6549285
   Wang XF, 2013, IEEE T MULTIMEDIA, V15, P811, DOI 10.1109/TMM.2013.2239630
   Wang Z, 2013, IEEE T MULTIMEDIA, V15, P698, DOI 10.1109/TMM.2012.2237022
   Wu C, 2017, IEEE ACM T NETWORK, V25, P2320, DOI 10.1109/TNET.2017.2681121
   Xu CQ, 2018, IEEE T MOBILE COMPUT, V17, P2114, DOI 10.1109/TMC.2018.2794970
   Xu CQ, 2015, IEEE COMMUN MAG, V53, P150, DOI 10.1109/MCOM.2015.7295477
   Xu CQ, 2013, IEEE T VEH TECHNOL, V62, P2273, DOI 10.1109/TVT.2012.2228682
   Xu DW, 2014, IEEE T INF FOREN SEC, V9, P596, DOI 10.1109/TIFS.2014.2302899
   Xu QC, 2019, IEEE INTERNET THINGS, V6, P4536, DOI 10.1109/JIOT.2018.2876417
   Zhou P, 2016, IEEE T MULTIMEDIA, V18, P1217, DOI 10.1109/TMM.2016.2537216
NR 30
TC 19
Z9 20
U1 5
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2019
VL 21
IS 3
BP 636
EP 651
DI 10.1109/TMM.2019.2892561
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HN6YJ
UT WOS:000460333800010
DA 2024-07-18
ER

PT J
AU Li, D
   Yao, T
   Duan, LY
   Mei, T
   Rui, Y
AF Li, Dong
   Yao, Ting
   Duan, Ling-Yu
   Mei, Tao
   Rui, Yong
TI Unified Spatio-Temporal Attention Networks for Action Recognition in
   Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action recognition; spatio-temporal attention; deep convolutional
   networks
ID MODEL
AB Recognizing actions in videos is not a trivial task because video is an information-intensive media and includes multiple modalities. Moreover, on each modality, an action may only appear at some spatial regions, or only part of the temporal video segments may contain the action. A valid question is how to locate the attended spatial areas and selective video segments for action recognition. In this paper, we devise a general attention neural cell, called AttCell, that estimates the attention probability not only at each spatial location but also for each video segment in a temporal sequence. With AttCell, a unified Spatio-Temporal Attention Networks (STAN) is proposed in the context of multiple modalities. Specifically, STAN extracts the feature map of one convolutional layer as the local descriptors on each modality and pools the extracted descriptors with the spatial attention measured by AttCell as a representation of each segment. Then, we concatenate the representation on each modality to seek a consensus on the temporal attention, a priori, to holistically fuse the combined representation of video segments to the video representation for recognition. Our model differs from conventional deep networks, which focus on the attention mechanism, because our temporal attention provides a principled and global guidance across different modalities and video segments. Extensive experiments are conducted on four public datasets; UCF101, CCV, THUMOS14, and Sports-1M; our STAN consistently achieves superior results over several state-of-the-art techniques. More remarkably, we validate and demonstrate the effectiveness of our proposal when capitalizing on the different number of modalities.
C1 [Li, Dong] Univ Sci & Technol China, Hefei 230000, Anhui, Peoples R China.
   [Li, Dong] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230000, Anhui, Peoples R China.
   [Yao, Ting] Microsoft Res, Multimedia Search & Mining Grp, Beijing 100080, Peoples R China.
   [Duan, Ling-Yu] Peking Univ, Natl Engn Lab Video Technol, Sch Elect Engn & Comp Sci, Beijing 100080, Peoples R China.
   [Mei, Tao] JD AI Res, Beijing 100101, Peoples R China.
   [Mei, Tao] JD AI Res, Comp Vis & Multimedia Lab, Beijing 100101, Peoples R China.
   [Rui, Yong] Lenovo, Beijing 100085, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; Peking University; Legend Holdings; Lenovo
RP Mei, T (corresponding author), JD AI Res, Beijing 100101, Peoples R China.; Mei, T (corresponding author), JD AI Res, Comp Vis & Multimedia Lab, Beijing 100101, Peoples R China.
EM dongli1995.ustc@gmail.com; tingyao.ustc@gmail.com; lingyu@pku.edu.cn;
   tmei@live.com; yongrui@lenovo.com
RI Mei, Tao/GQZ-0596-2022
OI Mei, Tao/0000-0002-5990-7307; Li, Dong/0000-0002-9936-1126; Yao,
   Ting/0000-0001-7587-101X
FU PKU-NTU Joint Research Institute (JRI)
FX This work was supported in part by the PKU-NTU Joint Research Institute
   (JRI) sponsored by a donation from the Ng Teng Fong Charitable
   Foundation. The associate editor coordinating the reviewof this
   manuscript and approving it for publication was Dr. Chang-Su Kim.
CR [Anonymous], P THUMOS CHALL WORKS
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2016, P 24 ACM INT C MULT
   [Anonymous], P EUR C COMPUT VIS
   [Anonymous], THUMOS CHALL WORKSH
   [Anonymous], P WORKSH INT C LEARN
   [Anonymous], 2017, CVPR
   [Anonymous], P THUMOS CHALL WORKS
   [Anonymous], 2015, P INT C LEARN REPR
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2016, P INT JOINT C ARTIFI
   [Anonymous], P ACTIVITYNET CHALL
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], P THUMOS CHALL WORKS
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], 2016, ARXIV160706416
   [Anonymous], IEEE C COMP VIS PATT
   [Anonymous], 2017, CVPR
   [Anonymous], P THUMOS CHALL WORKS
   [Anonymous], 2012, CoRR
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Bilen H, 2016, PROC CVPR IEEE, P3034, DOI 10.1109/CVPR.2016.331
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Du WB, 2018, IEEE T IMAGE PROCESS, V27, P1347, DOI 10.1109/TIP.2017.2778563
   Feichtenhofer C., 2016, P INT C NEUR INF PRO, P3468
   Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Gupta A, 2009, IEEE T PATTERN ANAL, V31, P1775, DOI 10.1109/TPAMI.2009.83
   Hasan M, 2015, IEEE T MULTIMEDIA, V17, P1909, DOI 10.1109/TMM.2015.2477242
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Jain M, 2015, PROC CVPR IEEE, P46, DOI 10.1109/CVPR.2015.7298599
   Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang Y., 2011, P ACM INT C MULT RET
   Jiang YG, 2015, IEEE T MULTIMEDIA, V17, P1174, DOI 10.1109/TMM.2015.2436813
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Lan T, 2015, IEEE I CONF COMP VIS, P4552, DOI 10.1109/ICCV.2015.517
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Li Q, 2017, INT J MULTIMED INF R, V6, P85, DOI 10.1007/s13735-016-0117-4
   Li Q, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P159, DOI 10.1145/2911996.2912001
   Li ZY, 2018, COMPUT VIS IMAGE UND, V166, P41, DOI 10.1016/j.cviu.2017.10.011
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Ma AJ, 2014, INT J COMPUT VISION, V109, P233, DOI 10.1007/s11263-014-0723-7
   Mahasseni B, 2016, PROC CVPR IEEE, P3054, DOI 10.1109/CVPR.2016.333
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Scovanner P., 2007, P 15 ACM INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Sun C, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P371, DOI 10.1145/2733373.2806226
   Sun L, 2015, IEEE I CONF COMP VIS, P4597, DOI 10.1109/ICCV.2015.522
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Varol G, 2018, IEEE T PATTERN ANAL, V40, P1510, DOI 10.1109/TPAMI.2017.2712608
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang LM, 2013, PROC CVPR IEEE, P2674, DOI 10.1109/CVPR.2013.345
   Wang XL, 2016, PROC CVPR IEEE, P2658, DOI 10.1109/CVPR.2016.291
   Willems G, 2008, LECT NOTES COMPUT SC, V5303, P650, DOI 10.1007/978-3-540-88688-4_48
   Wu ZX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P167, DOI 10.1145/2647868.2654931
   Xu WR, 2017, IEEE T MULTIMEDIA, V19, P1494, DOI 10.1109/TMM.2017.2674622
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Yan SY, 2018, SIGNAL PROCESS-IMAGE, V61, P73, DOI 10.1016/j.image.2017.11.005
   Ye H, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P435, DOI 10.1145/2671188.2749406
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zhang BW, 2016, PROC CVPR IEEE, P2718, DOI 10.1109/CVPR.2016.297
   Zhou Z, 2015, IEEE T MULTIMEDIA, V17, P512, DOI 10.1109/TMM.2015.2404779
NR 72
TC 97
Z9 100
U1 1
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2019
VL 21
IS 2
BP 416
EP 428
DI 10.1109/TMM.2018.2862341
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HJ7GT
UT WOS:000457364400012
DA 2024-07-18
ER

PT J
AU Tsang, SH
   Chan, YL
   Kuang, W
   Siu, WC
AF Tsang, Sik-Ho
   Chan, Yui-Lam
   Kuang, Wei
   Siu, Wan-Chi
TI Reduced-Complexity Intra Block Copy (IntraBC) Mode With Early CU
   Splitting and Pruning for HEVC Screen Content Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hash search; HEVC; intra block copy; screen content coding; video coding
AB A screen content coding (SCC) extension to high efficiency video coding has been developed to incorporate many new coding tools in order to achieve better coding efficiency for videos mixed with camera-captured content and graphics/text/animation. For instance, the Intra Block Copy (IntraBC) mode helps to encode repeating patterns within the same frame while the Palette mode aims at encoding screen content with a few major colors. However, the IntraBC mode brings along high computational complexity due to the exhaustive block matching within the same frame though there are already some constraints and fast approaches applied to the IntraBC mode to reduce its complexity. Thus, we propose a fast intracoding scheme to reduce the complexity of using the IntraBC mode in SCC. Screen content always contains no sensor noise resulting in the characteristics with pixel exactness along both horizontal and vertical directions. These characteristics pave the way for mode skipping and early coding unit (CU) splitting. Besides, early CU pruning and early termination are proposed based on the rate distortion cost to further reduce encoder complexity. Moreover, we also propose reducing the complexity of the IntraBC mode by checking the hash value of each block candidate and the current block during block matching. With our proposed scheme, the encoding time is reduced compared with the SCC while the coding efficiency can still be maintained with a minor increase in the bjontegaard delta bitrate.
C1 [Tsang, Sik-Ho; Chan, Yui-Lam; Kuang, Wei; Siu, Wan-Chi] Hong Kong Polytech Univ, Ctr Signal Proc, Dept Elect & Informat Engn, Hung Hom, Hong Kong, Peoples R China.
C3 Hong Kong Polytechnic University
RP Chan, YL (corresponding author), Hong Kong Polytech Univ, Ctr Signal Proc, Dept Elect & Informat Engn, Hung Hom, Hong Kong, Peoples R China.
EM sik-ho.tsang@polyu.edu.hk; enylchan@polyu.edu.hk;
   wei.kuang@connect.polyu.hk; enwcsiu@polyu.edu.hk
RI Kuang, Wei/U-3585-2019; Chan, Yui-Lam/C-3799-2014
OI TSANG, Sik Ho/0000-0002-8578-0696; Chan, Yui-Lam/0000-0002-1473-094X;
   Kuang, Wei/0000-0002-6801-7744
FU Research Grants Council of the Hong Kong Special Administrative Region,
   China [PolyU 152016/14E]
FX This work was supported by the Research Grants Council of the Hong Kong
   Special Administrative Region, China, under Grant PolyU 152016/14E. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Leonel Sousa.
CR [Anonymous], 2015, HM 16 7 SCM 6 0 HEVC
   [Anonymous], 2015, SCREEN CONTENT CODIN
   [Anonymous], JTC1SC29WG11 ISOIEC
   [Anonymous], 2015, COMMON TEST CONDITIO
   Bjotegaard G., 2001, VCEGM33
   Blasi SG, 2015, IEEE IMAGE PROC, P1478, DOI 10.1109/ICIP.2015.7351046
   Budagavi M, 2013, PICT COD SYMP, P365, DOI 10.1109/PCS.2013.6737759
   Chen C.-C., 2016, IEEE T CIRCUITS SYST, V27, P1568
   Chen C, 2014, ADV INTEL SYS RES, V101, P1, DOI 10.1109/TSA.2014.10
   Duanmu F, 2016, IEEE J EM SEL TOP C, V6, P517, DOI 10.1109/JETCAS.2016.2597698
   Kim IK, 2012, IEEE T CIRC SYST VID, V22, P1697, DOI 10.1109/TCSVT.2012.2223011
   Kwon DK, 2014, IEEE INT SYMP CIRC S, P9, DOI 10.1109/ISCAS.2014.6865052
   Lainema J, 2015, I SYMP CONSUM ELECTR, P333, DOI 10.1109/ICCE.2015.7066434
   Lainema J, 2012, IEEE T CIRC SYST VID, V22, P1792, DOI 10.1109/TCSVT.2012.2221525
   Lei JJ, 2017, IEEE T BROADCAST, V63, P48, DOI 10.1109/TBC.2016.2623241
   Li B, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P530, DOI 10.1109/VCIP.2014.7051623
   Li B, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P189, DOI 10.1109/VCIP.2014.7051536
   Lin T., 2013, P IEEE INT C MULT EX, P1
   Lin T, 2013, PICT COD SYMP, P369, DOI 10.1109/PCS.2013.6737760
   Lin T, 2013, IEEE T CIRC SYST VID, V23, P173, DOI 10.1109/TCSVT.2012.2223871
   Lu Y, 2011, IEEE MULTIMEDIA, V18, P4, DOI 10.1109/MMUL.2011.33
   Ma Z, 2014, IEEE T IMAGE PROCESS, V23, P4399, DOI 10.1109/TIP.2014.2346995
   Pan ZT, 2013, IEEE T CIRC SYST VID, V23, P949, DOI 10.1109/TCSVT.2013.2243056
   Piao Y., 2010, JCTVCC207
   Shen LQ, 2014, IEEE T IMAGE PROCESS, V23, P4232, DOI 10.1109/TIP.2014.2341927
   Sullivan GJ, 2013, IEEE J-STSP, V7, P1001, DOI 10.1109/JSTSP.2013.2283657
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun YC, 2015, IEEE IMAGE PROC, P2409, DOI 10.1109/ICIP.2015.7351234
   Tsang SH, 2015, INT CONF ACOUST SPEE, P1409, DOI 10.1109/ICASSP.2015.7178202
   Tsang SH, 2015, ASIAPAC SIGN INFO PR, P396, DOI 10.1109/APSIPA.2015.7415302
   Tsang SH, 2014, INT CONF DIGIT SIG, P888, DOI 10.1109/ICDSP.2014.6900796
   Wang S., 2015, IEEE T CIRCUITS SYST, V26, P1595
   Wang SH, 2015, MULTIMED TOOLS APPL, V74, P7753, DOI 10.1007/s11042-014-2021-3
   Xiaozhong Xu, 2015, 2015 Data Compression Conference (DCC), P273, DOI 10.1109/DCC.2015.22
   Xiu XY, 2015, IEEE DATA COMPR CONF, P253, DOI 10.1109/DCC.2015.79
   Xu JZ, 2016, IEEE T CIRC SYST VID, V26, P50, DOI 10.1109/TCSVT.2015.2478706
   Xu X., 2014, DATE, P349, DOI DOI 10.1080/00207543.2014.937013
   Xu XZ, 2016, IEEE J EM SEL TOP C, V6, P409, DOI 10.1109/JETCAS.2016.2597645
   Zhang H, 2016, INT CONF ACOUST SPEE, P1377, DOI 10.1109/ICASSP.2016.7471902
   Zhang K, 2015, IEEE INT SYMP CIRC S, P521, DOI 10.1109/ISCAS.2015.7168685
   Zhang MM, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P390, DOI 10.1109/VCIP.2014.7051588
   Zhang MM, 2014, IEEE IMAGE PROC, P3744, DOI 10.1109/ICIP.2014.7025760
   Zhang XY, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P129, DOI 10.1109/VCIP.2014.7051521
   Zhang ZD, 2015, IEEE IMAGE PROC, P4102, DOI 10.1109/ICIP.2015.7351577
   Zhou KL, 2016, IEEE J EM SEL TOP C, V6, P560, DOI 10.1109/JETCAS.2016.2599876
   Zhu WJ, 2015, IEEE T MULTIMEDIA, V17, P935, DOI 10.1109/TMM.2015.2428171
   Zhu WJ, 2014, IEEE T MULTIMEDIA, V16, P1316, DOI 10.1109/TMM.2014.2315782
   Zhu WJ, 2014, IEEE DATA COMPR CONF, P43, DOI 10.1109/DCC.2014.11
   Zou FH, 2016, MULTIMED TOOLS APPL, V75, P12627, DOI 10.1007/s11042-014-2423-2
NR 49
TC 30
Z9 38
U1 3
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2019
VL 21
IS 2
BP 269
EP 283
DI 10.1109/TMM.2018.2856078
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HJ7GT
UT WOS:000457364400001
DA 2024-07-18
ER

PT J
AU Zhou, XF
   Liu, Z
   Gong, C
   Liu, W
AF Zhou, Xiaofei
   Liu, Zhi
   Gong, Chen
   Liu, Wei
TI Improving Video Saliency Detection via Localized Estimation and
   Spatiotemporal Refinement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video saliency; localized estimation; local temporal window;
   spatiotemporal refinement; saliency update
ID VISUAL-ATTENTION; OBJECT SEGMENTATION; DETECTION MODEL; IMAGE;
   OPTIMIZATION; CONTRAST
AB Video saliency detection aims to pop out the most salient regions in every frame of a video. Up to now, many efforts have been made from various aspects for video saliency detection. Unfortunately, the existing video saliency models are very likely to fail in challenging videos with complicated motions and complex scenes. Therefore, in this paper, we propose a novel framework to improve the saliency detection results generated by existing video saliency models. The proposed framework consists of three key steps including localized estimation, spatiotemporal refinement, and saliency update. Specifically, the initial saliency map of each frame in a video is first generated by using an existing saliency model. Then, by considering the temporal consistency and strong correlation among adjacent frames, the localized estimation models, which are generated by training the random forest regressor within a local temporal window, are employed to generate the temporary saliency map. Finally, by taking the appearance and motion information of salient objects into consideration, the spatiotemporal refinement step is deployed to further improve the temporary saliency map and generate the final saliency map. Furthermore, such an improved saliency map is then utilized to update the initial saliency map and provide reliable cues for saliency detection in the next frame. The experimental results on four challenging datasets demonstrate that the proposed framework is able to consistently and significantly improve the saliency detection performance of various video saliency models, thereby achieving the state-of-the-art performance.
C1 [Zhou, Xiaofei; Liu, Zhi] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.
   [Zhou, Xiaofei; Liu, Zhi] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
   [Gong, Chen] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Key Lab Intelligent Percept & Syst High Dimens In, Minist Educ, Nanjing 210094, Jiangsu, Peoples R China.
   [Liu, Wei] Shanghai Jiao Tong Univ, Minist Educ Syst Control & Informat Proc, Key Lab, Shanghai 200240, Peoples R China.
C3 Shanghai University; Shanghai University; Nanjing University of Science
   & Technology; Shanghai Jiao Tong University
RP Liu, Z (corresponding author), Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.; Liu, Z (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
EM zxforchid@outlook.com; liuzhisjtu@163.com; chen.gong@njust.edu.cn;
   liuwei.1989@sjtu.edu.cn
RI LIU, Zhi/D-4518-2012; Xiaofei, Zhou/AAE-8347-2020; GONG,
   CHEN/JDW-5727-2023
OI LIU, Zhi/0000-0002-8428-1131; Liu, Wei/0000-0001-6351-9019
FU National Natural Science Foundation of China [61771301, 61602246];
   Natural Science Foundation of Jiangsu Province [BK20171430]; Fundamental
   Research Funds for the Central Universities [30918011319]; Summit of the
   Six Top Talents Program [DZXX-027]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61771301 and Grant 61602246, in part by
   the Natural Science Foundation of Jiangsu Province under Grant
   BK20171430, in part by the Fundamental Research Funds for the Central
   Universities under Grant 30918011319, and in part by the Summit of the
   Six Top Talents Program under Grant DZXX-027.
CR Abdollahian G, 2010, IEEE T MULTIMEDIA, V12, P28, DOI 10.1109/TMM.2009.2036286
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2009, ADV NEURAL INFORM PR
   Aytekin C, 2018, IEEE T MULTIMEDIA, V20, P82, DOI 10.1109/TMM.2017.2713982
   Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143
   Chen CZ, 2017, IEEE T IMAGE PROCESS, V26, P3156, DOI 10.1109/TIP.2017.2670143
   Chen L, 2015, IEEE T MULTIMEDIA, V17, P2225, DOI 10.1109/TMM.2015.2481711
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cui Xinyi, 2009, P 17 ACM INT C MULT, P617, DOI DOI 10.1145/1631272.1631370
   Du H, 2013, J VIS COMMUN IMAGE R, V24, P499, DOI 10.1016/j.jvcir.2013.03.003
   Faktor Alon, 2014, BMVC
   Fang YM, 2014, IEEE T IMAGE PROCESS, V23, P3910, DOI 10.1109/TIP.2014.2336549
   Fang YM, 2014, IEEE T CIRC SYST VID, V24, P27, DOI 10.1109/TCSVT.2013.2273613
   Gao D., 2008, Advances in Neural Information Processing Systems 20, P497
   Gopalakrishnan V, 2012, IEEE T CIRC SYST VID, V22, P683, DOI 10.1109/TCSVT.2011.2177177
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Guo J, 2016, IEEE T MULTIMEDIA, V18, P1297, DOI 10.1109/TMM.2016.2564100
   Heikkilä M, 2009, PATTERN RECOGN, V42, P425, DOI 10.1016/j.patcog.2008.08.014
   Huang CR, 2014, IEEE T CIRC SYST VID, V24, P1336, DOI 10.1109/TCSVT.2014.2308652
   Itti L, 2005, PROC CVPR IEEE, P631
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jerripothula KR, 2018, IEEE T MULTIMEDIA, V20, P2466, DOI 10.1109/TMM.2018.2798294
   Jerripothula KR, 2019, IEEE T CIRC SYST VID, V29, P744, DOI 10.1109/TCSVT.2018.2805811
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Kim H, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2425544
   Kim W, 2014, IEEE T CIRC SYST VID, V24, P646, DOI 10.1109/TCSVT.2013.2290579
   Kim W, 2011, IEEE T CIRC SYST VID, V21, P446, DOI 10.1109/TCSVT.2011.2125450
   Kuipers B, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P174
   Le Meur O, 2007, VISION RES, V47, P2483, DOI 10.1016/j.visres.2007.06.015
   Lee WF, 2011, IEEE T IMAGE PROCESS, V20, P3028, DOI 10.1109/TIP.2011.2144610
   Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li J, 2010, INT J COMPUT VISION, V90, P150, DOI 10.1007/s11263-010-0354-6
   Li WT, 2013, IEEE T IMAGE PROCESS, V22, P2600, DOI 10.1109/TIP.2013.2253483
   Li YB, 2009, PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON ANTI-COUNTERFEITING, SECURITY, AND IDENTIFICATION IN COMMUNICATION, P246, DOI 10.1109/ICASID.2009.5276913
   Lin YW, 2013, IEEE T PATTERN ANAL, V35, P314, DOI 10.1109/TPAMI.2012.119
   Liu C, 2009, PATTERN RECOGN, V42, P2897, DOI 10.1016/j.patcog.2009.02.002
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu Z, 2017, IEEE T CIRC SYST VID, V27, P2527, DOI 10.1109/TCSVT.2016.2595324
   Liu Z, 2014, IEEE T CIRC SYST VID, V24, P1522, DOI 10.1109/TCSVT.2014.2308642
   Liu Z, 2014, IEEE T IMAGE PROCESS, V23, P1937, DOI 10.1109/TIP.2014.2307434
   Liu Z, 2012, IEEE T MULTIMEDIA, V14, P1275, DOI 10.1109/TMM.2012.2190385
   Mahadevan V, 2010, IEEE T PATTERN ANAL, V32, P171, DOI 10.1109/TPAMI.2009.112
   Marat S, 2009, INT J COMPUT VISION, V82, P231, DOI 10.1007/s11263-009-0215-3
   Muthuswamy K, 2012, INT CONF ACOUST SPEE, P1465, DOI 10.1109/ICASSP.2012.6288167
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Ren ZX, 2013, IEEE T IMAGE PROCESS, V22, P3120, DOI 10.1109/TIP.2013.2259837
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Seo HJ, 2009, J VISION, V9, DOI 10.1167/9.12.15
   Shi R, 2012, IEEE SIGNAL PROC LET, V19, P215, DOI 10.1109/LSP.2012.2188388
   Tang YB, 2016, LECT NOTES COMPUT SC, V9912, P809, DOI 10.1007/978-3-319-46484-8_49
   Tong N, 2015, PROC CVPR IEEE, P1884, DOI 10.1109/CVPR.2015.7298798
   Vig E, 2012, IEEE T PATTERN ANAL, V34, P1080, DOI 10.1109/TPAMI.2011.198
   Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Wenguan Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3395, DOI 10.1109/CVPR.2015.7298961
   Xi T, 2017, IEEE T IMAGE PROCESS, V26, P3425, DOI 10.1109/TIP.2016.2631900
   Yan B, 2014, IEEE T MULTIMEDIA, V16, P272, DOI 10.1109/TMM.2013.2286112
   Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022
   Yuan Z, 2012, IEEE T CIRC SYST VID, V22, P890, DOI 10.1109/TCSVT.2011.2181230
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhou XF, 2016, IEEE SIGNAL PROC LET, V23, P517, DOI 10.1109/LSP.2016.2536743
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 69
TC 61
Z9 61
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 2993
EP 3007
DI 10.1109/TMM.2018.2829605
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800010
DA 2024-07-18
ER

PT J
AU Qian, SS
   Zhang, TZ
   Xu, CS
AF Qian, Shengsheng
   Zhang, Tianzhu
   Xu, Changsheng
TI Online Multimodal Multiexpert Learning for Social Event Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Social event tracking; topic evolution; multimodality; topic model;
   social media
AB In this paper, we aim to automatically identify and track the interesting social event from vast amounts of social media data. However, there are two existing challenges: 1) how to model multimodal social event data over time and visualize the topic evolution and 2) how to alleviate the tracking drift problem to boost social event tracking accuracy. We propose a novel online multimodal multiexpert learning algorithm for social event tracking. Compared with existing methods, the proposed model has several advantages: First, it has a nonparametric online multimodal tracking module, which is able to not only automatically learn the number of topics from data over time, but also exploit the multimodal property of the social event. Second, it adopts a novel multiexpert minimization restoration scheme and allows the tracked model to evolve backwards to undo undesirable model updates, which helps alleviate the model drift problem of social event tracking. Third, it is able to not only effectively track the multimodal social event, but also automatically exploit the topic evolution of the social event for a deep understanding with multimodal topics. To evaluate the proposed model, we collect a real-world dataset for research on social event tracking with multimodality information. We have conducted extensive experiments, and both qualitative and quantitative evaluation results have demonstrated the effectiveness of the proposed model.
C1 [Qian, Shengsheng; Zhang, Tianzhu; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Qian, Shengsheng; Zhang, Tianzhu; Xu, Changsheng] Univ Chinese Acad Sci, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM shengsheng.qian@nlpr.ia.ac.cn; tzzhang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn
RI ARSLAN, Okan/AAA-3232-2020; Zhang, Tianzhu/AGY-9389-2022; xu,
   cj/HJZ-3488-2023
OI Zhang, Tianzhu/0000-0003-0764-6106; zhang, tian zhu/0000-0003-1856-9564;
   xu, chang sheng/0000-0001-8343-9665
FU National Natural Science Foundation of China [61720106006, 61432019,
   61572498, 61532009, 61572296]; Key Research Program of Frontier
   Sciences, CAS [QYZDJ-SSW-JSC039]; Beijing Natural Science Foundation
   [4172062]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61720106006, Grant 61432019, Grant
   61572498, Grant 61532009, and Grant 61572296; in part by the Key
   Research Program of Frontier Sciences, CAS, under Grant
   QYZDJ-SSW-JSC039; and in part by the Beijing Natural Science Foundation
   under Grant 4172062. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. K. Selcuk
   Candan.
CR Allan J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P37, DOI 10.1145/290941.290954
   Allan J., 2003, P 26 ANN INT ACM SIG, P314, DOI DOI 10.1145/860435.860493
   [Anonymous], 2011, P 14 INT C ART INT S
   [Anonymous], 2008, Proceedings of the 9th International Workshop on Multimedia Data Mining: held in conjunction with the ACM SIGKDD 2008
   [Anonymous], 2013, P 6 ACM INT C WEB SE
   [Anonymous], 2013, PROC THE 21 ACM INT
   [Anonymous], 2008, Advances in Neural Information Processing Systems
   [Anonymous], 2008, P ADV NEURAL INFORM
   [Anonymous], 2012, P 20 ACM INT C MULTI
   [Anonymous], 2009, Proceedings of the Second ACM international Conference on Web Search and Data Mining (Barcelona, Spain, February 09-12, DOI DOI 10.1145/1498759.1498809
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   Becker H., 2009, P 3 ACM INT C WEB SE, P291
   Bian JW, 2015, IEEE T MULTIMEDIA, V17, P216, DOI 10.1109/TMM.2014.2384912
   Blei D.M., 2006, INT C MACHINE LEARNI, DOI [DOI 10.1145/1143844.1143859, 10.1145/1143844.1143859]
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Diakopoulos N., 2010, 2010 Proceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2010), P115, DOI 10.1109/VAST.2010.5652922
   FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360
   Firan C.S., 2010, Proceedings of the 19th ACM international conference on Information and knowledge management, P189
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Grandvalet Y, 2004, Advances in neural information processing systems, V17
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Hoffman M., 2010, P ADV NEUR INF PROC, V23:856-864
   Hofmann T, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P289
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   Kalamaras I, 2014, IEEE T MULTIMEDIA, V16, P1460, DOI 10.1109/TMM.2014.2316473
   Kender JR, 2005, PROC CVPR IEEE, P1174
   Kumaran G., 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P297, DOI 10.1145/1008992.1009044
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Makkonen J, 2004, INFORM RETRIEVAL, V7, P347, DOI 10.1023/B:INRT.0000011210.12953.86
   Matthews I, 2004, IEEE T PATTERN ANAL, V26, P810, DOI 10.1109/TPAMI.2004.16
   Merler M, 2012, IEEE T MULTIMEDIA, V14, P88, DOI 10.1109/TMM.2011.2168948
   Niu ZX, 2011, PROC CVPR IEEE, P1769, DOI 10.1109/CVPR.2011.5995426
   Patel D., 2008, P 2008 ACM SIGMOD IN, P393, DOI DOI 10.1145/1376616.1376658
   Qian S., 2015, ACM T MULTIM COMPUT, V11, P1
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Qian Shengsheng, 2016, P 24 ACM INT C MULTI
   Reuter Timo., 2013, P MEDIAEVAL 2013 MUL, P1
   Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Wang X, 2005, P 3 INT WORKSHOP LIN, P28
   Wang Xuerui., P 12 ACM SIGKDD INT, P424
   Wu X, 2008, IEEE T MULTIMEDIA, V10, P188, DOI 10.1109/TMM.2007.911778
   Xie L, 2004, IEEE IMAGE PROC, P2383
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P346, DOI 10.1109/TMM.2015.2393635
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yang Y., 2002, P 8 ACM SIGKDD INT C, P688, DOI DOI 10.1145/775047.775150
   Yang YM, 1999, IEEE INTELL SYST APP, V14, P32, DOI 10.1109/5254.784083
   Yun Zhai, 2005, 13th Annual ACM International Conference on Multimedia, P2, DOI 10.1145/1101149.1101152
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang TZ, 2015, INT J COMPUT VISION, V111, P171, DOI 10.1007/s11263-014-0738-0
   Zhang TZ, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2602633
   Zhao Y, 2004, MACH LEARN, V55, P311, DOI 10.1023/B:MACH.0000027785.44527.d6
NR 53
TC 5
Z9 5
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2018
VL 20
IS 10
SI SI
BP 2733
EP 2748
DI 10.1109/TMM.2018.2815785
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GT9YH
UT WOS:000444903000016
DA 2024-07-18
ER

PT J
AU Jiang, QP
   Shao, F
   Lin, WS
   Gu, K
   Jiang, GY
   Sun, HF
AF Jiang, Qiuping
   Shao, Feng
   Lin, Weisi
   Gu, Ke
   Jiang, Gangyi
   Sun, Huifang
TI Optimizing Multistage Discriminative Dictionaries for Blind Image
   Quality Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Blind image quality assessment; label consistent K-SVD; multi-stage
   discriminative dictionaries; multi-stage feature encoding;
   reconstruction residual
ID STEREOSCOPIC IMAGES; GRADIENT MAGNITUDE; K-SVD; STATISTICS
AB State-of-the-art algorithms for blind image quality assessment (BIQA) typically have two categories. The first category approaches extract natural scene statistics (NSS) as features based on the statistical regularity of natural images. The second category approaches extract features by feature encoding with respect to a learned codebook. However, several problems need to he addressed in existing codebook-based BIQA methods. First, the high-dimensional codebook-based features are memory-consuming and have the risk of over-fitting. Second, there is a semantic gap between the constructed codebook by unsupervised learning and image quality. To address these problems, we propose a novel codebook-based BIQA method by optimizing multistage discriminative dictionaries (MSDDs). To be specific, MSDDs are learned by performing the label consistent K-SVD (LC-KSVD) algorithm in a stage-by-stage manner. For each stage, a new quality consistency constraint called "quality-discriminative regularization" term is introduced and incorporated into the reconstruction error term to form a unified objective function, which can be effectively solved by LC-KSVD for discriminative dictionary learning. Then, the latter stage takes the reconstruction residual data in the former stage as input based on which LC-KSVD is repeatedly performed until the final stage is reached. Once the MSDDs are learned, multistage feature encoding is performed to extract feature codes. Finally, the feature codes are concatenated across all stages and aggregated over the entire image for quality prediction via regression. The proposed method has been evaluated on five databases and experimental results well confirm its superiority over existing relevant BIQA methods.
C1 [Jiang, Qiuping; Shao, Feng; Jiang, Gangyi] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Zhejiang, Peoples R China.
   [Jiang, Qiuping; Lin, Weisi] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Gu, Ke] Beijing Univ Technol, Fac Informat Technol, Beijing Key Lab Computat Intelligence & Intellige, Beijing 100124, Peoples R China.
   [Sun, Huifang] Mitsubishi Elect Res Labs, Cambridge, MA 02139 USA.
C3 Ningbo University; Nanyang Technological University; Beijing University
   of Technology
RP Shao, F (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Zhejiang, Peoples R China.
EM jqp910707@126.com; shaofeng@nbu.edu.cn; wslin@ntu.edu.sg;
   guke@bjut.edu.cn; jianggangyi@nbu.edu.cn; hsun@merl.com
RI Gu, Ke/AAJ-9684-2021; Jiang, Qiuping/AAL-8273-2020; Lin,
   Weisi/A-8011-2012; Lin, Weisi/A-3696-2011; Qiuping, Jiang/AAO-2830-2021;
   jiang, gang/KII-8233-2024
OI Lin, Weisi/0000-0001-9866-1947; Qiuping, Jiang/0000-0002-6025-9343; 
FU Natural Science Foundation of China [61622109]; Zhejiang Natural Science
   Foundation [R18F010008]; China Scholarship Council [201708330302]; K.C.
   Wong Magna Fund in Ningbo University
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61622109, in part by the Zhejiang Natural Science
   Foundation under Grant R18F010008, in part by the China Scholarship
   Council under Grant 201708330302, and in part by the K.C. Wong Magna
   Fund in Ningbo University. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Judith Redi.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Anegekuh L, 2015, IEEE T MULTIMEDIA, V17, P1323, DOI 10.1109/TMM.2015.2444098
   Avila S, 2013, COMPUT VIS IMAGE UND, V117, P453, DOI 10.1016/j.cviu.2012.09.007
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chang HW, 2013, IEEE T IMAGE PROCESS, V22, P4007, DOI 10.1109/TIP.2013.2266579
   Engelke U, 2009, SIGNAL PROCESS-IMAGE, V24, P525, DOI 10.1016/j.image.2009.06.005
   Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760
   Gao W, 2016, IEEE T MULTIMEDIA, V18, P988, DOI 10.1109/TMM.2016.2535254
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Hyvärinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Jiang QP, 2018, IEEE T CYBERNETICS, V48, P1276, DOI 10.1109/TCYB.2017.2690452
   Jiang QP, 2015, J VIS COMMUN IMAGE R, V33, P123, DOI 10.1016/j.jvcir.2015.09.009
   Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88
   Kim J, 2017, IEEE J-STSP, V11, P206, DOI 10.1109/JSTSP.2016.2639328
   Kiran I, 2016, TENTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS AND IMAGE PROCESSING (ICVGIP 2016), DOI 10.1145/3009977.3010014
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lazebnik S, 2009, IEEE T PATTERN ANAL, V31, P1294, DOI 10.1109/TPAMI.2008.138
   Li SN, 2011, IEEE T MULTIMEDIA, V13, P935, DOI 10.1109/TMM.2011.2152382
   Liang HY, 2016, IEEE T IMAGE PROCESS, V25, P5118, DOI 10.1109/TIP.2016.2601783
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Liu DL, 2014, SIGNAL PROCESS-IMAGE, V29, P844, DOI 10.1016/j.image.2014.06.007
   Liu HT, 2010, IEEE T CIRC SYST VID, V20, P529, DOI 10.1109/TCSVT.2009.2035848
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Ma KD, 2015, IEEE T IMAGE PROCESS, V24, P3086, DOI [10.1109/TIP.2015.2436340, 10.1109/TIP.2015.2456638]
   Ma L, 2011, IEEE T MULTIMEDIA, V13, P824, DOI 10.1109/TMM.2011.2109701
   Mairal J, 2012, IEEE T PATTERN ANAL, V34, P791, DOI 10.1109/TPAMI.2011.156
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Redi JA, 2010, IEEE T CIRC SYST VID, V20, P1757, DOI 10.1109/TCSVT.2010.2087456
   Rubinstein R, 2008, Tech. rep.
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Shao F, 2015, IEEE T IMAGE PROCESS, V24, P2971, DOI 10.1109/TIP.2015.2436332
   Shao F, 2013, IEEE T IMAGE PROCESS, V22, P1940, DOI 10.1109/TIP.2013.2240003
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Shen J, 2011, IEEE T IMAGE PROCESS, V20, P2089, DOI 10.1109/TIP.2011.2108661
   Sheskin D.J., 2003, HDB PARAMETRIC NONPA, DOI [DOI 10.4324/9780203489536, 10.4324/9780203489536]
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Video Quality Experts Group, 2000, Final report from the video quality experts group on the validation of objective models of video quality assessment march 2000
   Wang SQ, 2013, IEEE T IMAGE PROCESS, V22, P1418, DOI 10.1109/TIP.2012.2231090
   Wang SQ, 2012, IEEE T CIRC SYST VID, V22, P516, DOI 10.1109/TCSVT.2011.2168269
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P981, DOI 10.1109/ICIP.2000.899622
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Wu QB, 2016, IEEE T CIRC SYST VID, V26, P425, DOI 10.1109/TCSVT.2015.2412773
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yang H, 2015, IEEE T IMAGE PROCESS, V24, P4408, DOI 10.1109/TIP.2015.2465145
   Yang Jun, 2009, Proceedings of the 2009 2nd International Congress on Image and Signal Processing (CISP), DOI 10.1109/CISP.2009.5304123
   Yang M, 2011, IEEE I CONF COMP VIS, P543, DOI 10.1109/ICCV.2011.6126286
   Ye P, 2013, PROC CVPR IEEE, P987, DOI 10.1109/CVPR.2013.132
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Ye P, 2012, IEEE T IMAGE PROCESS, V21, P3129, DOI 10.1109/TIP.2012.2190086
   Yue GH, 2018, IEEE T IND ELECTRON, V65, P2525, DOI 10.1109/TIE.2017.2739708
   Zhai GT, 2008, IEEE T MULTIMEDIA, V10, P1316, DOI 10.1109/TMM.2008.2004910
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2014, IEEE MULTIMEDIA, V21, P67, DOI 10.1109/MMUL.2014.50
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
NR 65
TC 198
Z9 203
U1 4
U2 45
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2018
VL 20
IS 8
BP 2035
EP 2048
DI 10.1109/TMM.2017.2763321
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GN8BR
UT WOS:000439378600010
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Li, JA
   Liang, XD
   Li, JS
   Wei, YC
   Xu, TF
   Feng, JS
   Yan, SC
AF Li, Jianan
   Liang, Xiaodan
   Li, Jianshu
   Wei, Yunchao
   Xu, Tingfa
   Feng, Jiashi
   Yan, Shuicheng
TI Multistage Object Detection With Group Recursive Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image segmentation; neural networks; object detection
AB Most existing detection pipelines treat object proposals independently and predict bounding box locations and classification scores over them separately. However, the important semantic and spatial layout correlations among proposals are often ignored, which are actually useful for more accurate object detection. In this paper, we propose a new EM-like group recursive learning approach to iteratively refine object proposals by incorporating such context of surrounding proposals and provide an optimal spatial configuration of object detections. In addition, we propose to incorporate the weakly supervised object segmentation cues and region-based object detection into a multistage architecture in order to fully exploit the learned segmentation features for better object detection in an end-toend way. The proposed architecture consists of three cascaded networks that, respectively, learn to perform weakly supervised object segmentation, object proposal generation, and recursive detection refinement. Combining the group recursive learning and the multistage architecture provides competitive mAPs of 78.7% and 74.9% on the PASCAL VOC2007 and VOC2012 datasets, respectively, which outperform many well-established baselines significantly.
C1 [Li, Jianan; Xu, Tingfa] Beijing Inst Technol, Sch Opt Engn, Beijing 100081, Peoples R China.
   [Liang, Xiaodan] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Li, Jianshu; Wei, Yunchao; Feng, Jiashi; Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
C3 Beijing Institute of Technology; Carnegie Mellon University; National
   University of Singapore
RP Xu, TF (corresponding author), Beijing Inst Technol, Sch Opt Engn, Beijing 100081, Peoples R China.
EM 20090964@bit.edu.cn; xdliang328@gmail.com; jianshu@u.nus.edu;
   wychao1987@gmail.com; 15210538723@163.com; jshfeng@gmail.com;
   eleyans@nus.edu.sg
RI Yan, Shuicheng/HCI-1431-2022; Feng, Jiashi/AGX-6209-2022
OI Li, Jianshu/0000-0001-8554-6886
FU China Scholarship Council [201506030045]
FX This work was supported in part by the China Scholarship Council under
   Grant 201506030045.
CR [Anonymous], 2012, P ADV NEUR INF PROC
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], ADV NEURAL INFORM PR
   Arbeláez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Carreira J, 2012, LECT NOTES COMPUT SC, V7578, P430, DOI 10.1007/978-3-642-33786-4_32
   Chen XZ, 2015, ADV NEUR IN, V28
   Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343
   Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
   Dai JF, 2015, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2015.7299025
   Dai QY, 2012, PROC CVPR IEEE, P3322, DOI 10.1109/CVPR.2012.6248070
   Dong J, 2014, LECT NOTES COMPUT SC, V8693, P299, DOI 10.1007/978-3-319-10602-1_20
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fidler S., 2012, NEURIPS, P611
   Fidler S, 2013, PROC CVPR IEEE, P3294, DOI 10.1109/CVPR.2013.423
   Gidaris S, 2015, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2015.135
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gonzalez-Garcia A, 2015, PROC CVPR IEEE, P3022, DOI 10.1109/CVPR.2015.7298921
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hoffman J., 2014, NIPS (News Physiol. Sci.), P3536
   Hoiem D, 2012, LECT NOTES COMPUT SC, V7574, P340, DOI 10.1007/978-3-642-33712-3_25
   Hosang J, 2016, IEEE T PATTERN ANAL, V38, P814, DOI 10.1109/TPAMI.2015.2465908
   Huang C, 2016, IEEE T MULTIMEDIA, V18, P2372, DOI 10.1109/TMM.2016.2602060
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kang K, 2017, ARXIV170206355
   Kong T, 2016, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2016.98
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu W., 2015, ARXIV150604579
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Najibi M, 2016, PROC CVPR IEEE, P2369, DOI 10.1109/CVPR.2016.260
   Jose MJ, 2016, COMPUT VIS IMAGE UND, V152, P118, DOI 10.1016/j.cviu.2016.08.007
   Ouyang WL, 2017, IEEE T PATTERN ANAL, V39, P1320, DOI 10.1109/TPAMI.2016.2587642
   Papandreou G, 2015, IEEE I CONF COMP VIS, P1742, DOI 10.1109/ICCV.2015.203
   Parkhi OM, 2011, IEEE I CONF COMP VIS, P1427, DOI 10.1109/ICCV.2011.6126398
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C., 2013, Advances in Neural Information Processing Systems, V26, P2553
   Tang JH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2998574
   Tang JH, 2015, IEEE T MULTIMEDIA, V17, P1899, DOI 10.1109/TMM.2015.2476660
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   van de Sande KEA, 2011, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2011.6126456
   Wang AR, 2015, IEEE T MULTIMEDIA, V17, P1887, DOI 10.1109/TMM.2015.2476655
   Wang XY, 2015, IEEE T PATTERN ANAL, V37, P2071, DOI 10.1109/TPAMI.2015.2389830
   Zhu YK, 2014, IEEE T MULTIMEDIA, V16, P1585, DOI 10.1109/TMM.2014.2321534
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 52
TC 41
Z9 42
U1 1
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1645
EP 1655
DI 10.1109/TMM.2017.2772796
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Min, WQ
   Bao, BK
   Mei, SH
   Zhu, YH
   Rui, Y
   Jiang, SQ
AF Min, Weiqing
   Bao, Bing-Kun
   Mei, Shuhuan
   Zhu, Yaohui
   Rui, Yong
   Jiang, Shuqiang
TI You Are What You Eat: hxploring Rich Recipe Information for Cross-Region
   Food Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-region food analysis; cuisine recommendation; cuisine
   summarization; culinary culture analysis; topic model
AB Cuisine is a style of cooking and usually associated with a specific geographic region. Recipes from different cuisines shared on the web are an indicator of culinary cultures in different countries. Therefore, analysis of these recipes can lead to deep understanding of food from the cultural perspective. In this paper, we perform the first cross-region recipe analysis by jointly using the recipe ingredients, food images, and attributes such as the cuisine and course (e.g., main dish and dessert). For that solution, we propose a culinary culture analysis framework to discover the topics of ingredient bases and visualize them to enable various applications. We first propose a probabilistic topic model to discover cuisine-course specific topics. The manilbld ranking method is then utilized to incorporate deep visual features to retrieve food images for topic visualization. At last, we applied the topic modeling and visualization method for three applications: 1) multimodal cuisine summarization with both recipe ingredients and images, 2) cuisine-course pattern analysis including topic specific cuisine distribution and cuisine-specific course distribution of topics, and 3) cuisine recommendation for both cuisine-oriented and ingredient-oriented queries. Through these three applications, we can analyze the culinary cultures at both macro and micro levels. We conduct the experiment on a recipe database Yummly-66K with 66,615 recipes from 10 cuisines in Yummly. Qualitative and quantitative evaluation results have validated the effectiveness of topic modeling and visualization, and demonstrated the advantage of the framework in utilizing rich recipe information to analyze and interpret the culinary cultures from different regions.
C1 [Min, Weiqing; Mei, Shuhuan; Zhu, Yaohui; Rui, Yong; Jiang, Shuqiang] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Bao, Bing-Kun] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Bao, Bing-Kun; Jiang, Shuqiang] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Mei, Shuhuan] Shandong Univ Sci & Technol, Qingdao 266590, Shandong, Peoples R China.
   [Rui, Yong] Lenovo Grp Ltd, Lenovo Corp Res, Beijing 100085, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Shandong University of Science & Technology; Legend Holdings; Lenovo
RP Min, WQ (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM weiqing.min@vipl.ict.ac.cn; bkbao@nlpr.ia.ac.cn;
   shuhuan.mei@vipl.ict.ac.cn; yaohui.zhu@vipl.ict.ac.cn;
   yongrui@lenovo.com; sqjiang@ict.ac.cn
OI Zhu, Yaohui/0009-0009-4841-1195; Zhu, Yaohui/0000-0003-4091-4782
FU National Natural Science Foundation of China [61532018, 61602437,
   61672497, 61572503, 61620106003]; Beijing Municipal Commission of
   Science and Technology [D161100001816001]; Beijing Natural Science
   Foundation [4174106, 4152053]; Lenovo Outstanding Young Scientists
   Program; National Program for Special Support of Eminent Professionals
   and National Program for Support of Top-notch Young Professionals; China
   Postdoctoral Science Foundation [2016M590135, 2017T100110]
FX This work was supported in part by the National Natural Science
   Foundation of China (61532018, 61602437, 61672497, 61572503, and
   61620106003), in part by Beijing Municipal Commission of Science and
   Technology (D161100001816001), in part by the Beijing Natural Science
   Foundation (4174106 and 4152053), in part by the Lenovo Outstanding
   Young Scientists Program, in part by the National Program for Special
   Support of Eminent Professionals and National Program for Support of
   Top-notch Young Professionals, and in part by the China Postdoctoral
   Science Foundation (2016M590135 and 2017T100110). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Yi-Hsuan Yang.
CR Ahmad Z, 2016, MADIMA'16: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P53, DOI 10.1145/2986035.2986038
   Ahn YY, 2011, SCI REP-UK, V1, DOI 10.1038/srep00196
   [Anonymous], 2016, P 2016 ACM MULT C
   [Anonymous], P COMP VIS PATT REC
   [Anonymous], ARXIV150203815
   [Anonymous], P 6 INT C DIG HLTH C
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2011, P 5 ACM C REC SYST, DOI 10.1145/2043932.2043979
   [Anonymous], 2017, P 26 INT C WORLD WID
   [Anonymous], 2012, Proceedings of the 2nd ACM international workshop on Interactive multimedia on mobile and portable devices
   [Anonymous], INT AAAI C WEBL SOC
   Bauer S, 2012, PROCEEDINGS OF 2012 ASE/IEEE INTERNATIONAL CONFERENCE ON PRIVACY, SECURITY, RISK AND TRUST AND 2012 ASE/IEEE INTERNATIONAL CONFERENCE ON SOCIAL COMPUTING (SOCIALCOM/PASSAT 2012), P348, DOI 10.1109/SocialCom-PASSAT.2012.107
   Beijbom O, 2015, IEEE WINT CONF APPL, P844, DOI 10.1109/WACV.2015.117
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Bettadapura V, 2015, IEEE WINT CONF APPL, P580, DOI 10.1109/WACV.2015.83
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Boscarino C, 2014, PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP'14 ADJUNCT), P559, DOI 10.1145/2638728.2641334
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Chang X, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P581, DOI 10.1145/2733373.2806218
   Chen JJ, 2017, LECT NOTES COMPUT SC, V10132, P588, DOI 10.1007/978-3-319-51811-4_48
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Chia-Jen Lin, 2014, Advances in Knowledge Discovery and Data Mining. 18th Pacific-Asia Conference, PAKDD 2014. Proceedings: LNCS 8444, P560, DOI 10.1007/978-3-319-06605-9_46
   Wang C, 2009, PROC CVPR IEEE, P1903, DOI [10.1109/CVPR.2009.5206800, 10.1109/CVPRW.2009.5206800]
   Christodoulidis S, 2015, LECT NOTES COMPUT SC, V9281, P458, DOI 10.1007/978-3-319-23222-5_56
   Cordeiro F, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1159, DOI 10.1145/2702123.2702155
   Dehais J, 2015, LECT NOTES COMPUT SC, V9281, P433, DOI 10.1007/978-3-319-23222-5_53
   Farinella GM, 2014, IEEE IMAGE PROC, P5212, DOI 10.1109/ICIP.2014.7026055
   Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872
   Ge M., 2015, P 5 INT C DIGITAL HL, P105, DOI DOI 10.1145/2750511.2750528
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Hassannejad H, 2016, MADIMA'16: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P41, DOI 10.1145/2986035.2986042
   He J., P 12 ANN ACM INT C M, P9, DOI 10.1145/1027527.1027531
   Herranz L, 2015, IEEE INT CON MULTI
   Herranz L, 2017, IEEE T MULTIMEDIA, V19, P430, DOI 10.1109/TMM.2016.2614861
   Hessel Jack., 2015, CoRR
   Hoffman M., 2008, Proc. ISMIR, P349
   Howell PD, 2016, DH'16: PROCEEDINGS OF THE 2016 DIGITAL HEALTH CONFERENCE, P131, DOI 10.1145/2896338.2896358
   Jiang XY, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P69, DOI 10.1145/2733373.2806240
   Kagaya H, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1085, DOI 10.1145/2647868.2654970
   Kawano Y, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P761, DOI 10.1145/2647868.2654869
   Kennedy L.S., 2008, P ACM INT C WORLD WI, P297
   Kusmierczyk T, 2016, CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P2013, DOI 10.1145/2983323.2983897
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Mejova Y., 2015, DH 15, P51, DOI [10.1145/2750511.2750524, DOI 10.1145/2750511.2750524]
   Mimno D., 2012, ARXIV12063278
   Min WQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P402, DOI 10.1145/3123266.3123272
   Min WQ, 2017, IEEE T MULTIMEDIA, V19, P1100, DOI 10.1109/TMM.2016.2639382
   Min WQ, 2015, IEEE T MULTIMEDIA, V17, P1787, DOI 10.1109/TMM.2015.2463226
   Min WQ, 2014, IEEE MULTIMEDIA, V21, P20, DOI 10.1109/MMUL.2014.1
   Nedovic V., 2013, Proceedings of the 2nd Workshop on Cooking with Computers (CwC), P13
   Okamoto K, 2016, MADIMA'16: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P63, DOI 10.1145/2986035.2986040
   Oliveira L, 2014, PATTERN RECOGN, V47, P1941, DOI 10.1016/j.patcog.2013.12.006
   Qian Shengsheng, 2016, P 24 ACM INT C MULTI
   Rudinac S, 2013, IEEE T MULTIMEDIA, V15, P1231, DOI 10.1109/TMM.2013.2261481
   Rudinac S, 2013, IEEE T MULTIMEDIA, V15, P921, DOI 10.1109/TMM.2013.2237896
   Salvador A, 2017, PROC CVPR IEEE, P3068, DOI 10.1109/CVPR.2017.327
   Singla A, 2016, MADIMA'16: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P3, DOI 10.1145/2986035.2986039
   Su H, 2014, PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP'14 ADJUNCT), P565, DOI 10.1145/2638728.2641335
   Tanno R, 2016, MADIMA'16: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P89, DOI 10.1145/2986035.2986044
   Wang X, 2015, IEEE INT CONF MULTI
   Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15
   Xu RH, 2015, IEEE T MULTIMEDIA, V17, P1187, DOI 10.1109/TMM.2015.2438717
   Yang Longqi., 2015, Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, P183, DOI [DOI 10.1145/2806416, 10.1145/2806416]
   Yin HZ, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P221
   Yu Q., 2012, P 20 ACM INT C MULT, P1073
   Zepeda L, 2008, INT J CONSUM STUD, V32, P692, DOI 10.1111/j.1470-6431.2008.00725.x
   Zhang FZ, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P725, DOI 10.1145/2872427.2882995
NR 67
TC 55
Z9 62
U1 1
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2018
VL 20
IS 4
BP 950
EP 964
DI 10.1109/TMM.2017.2759499
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FZ5HM
UT WOS:000427623000015
DA 2024-07-18
ER

PT J
AU Ge, XH
   Pan, LH
   Li, Q
   Mao, GQ
   Tu, S
AF Ge, Xiaohu
   Pan, Linghui
   Li, Qiang
   Mao, Guoqiang
   Tu, Song
TI Multipath Cooperative Communications Networks for Augmented and Virtual
   Reality Transmission
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimedia communications; cooperative communications; multi-path
   transmissions; network latency; energy consumption
ID WAVE CELLULAR NETWORKS; MOBILE; COMPENSATION; 5G
AB Augmented and/or virtual reality (AR/VR) are emerging as one of the main applications in future fifth-generation (5G) networks. To meet the requirements of lower latency and massive data transmission in AR/VR applications, a solution with software-defined networking architecture is proposed for 5G small cell networks. On this basis, a multipath cooperative route (MCR) scheme is proposed to facilitate the AR/VR wireless transmissions in 5G small cell networks, in which the delay of the MCR scheme is analytically studied. Furthermore, a service effective energy (SEE) optimization algorithm is developed for AR/VR wireless transmission in 5G small cell networks. Simulation results indicate that both the delay and SEE of the proposed MCR scheme outperform the delay and SEE of the conventional single-path route scheme in 5G small cell networks.
C1 [Ge, Xiaohu; Pan, Linghui; Li, Qiang; Tu, Song] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Hubei, Peoples R China.
   [Mao, Guoqiang] Univ Technol Sydney, Sch Comp & Commun, Ultimo, NSW 2007, Australia.
   [Mao, Guoqiang] CSIRO, Data61, Canberra, ACT 2601, Australia.
C3 Huazhong University of Science & Technology; University of Technology
   Sydney; Commonwealth Scientific & Industrial Research Organisation
   (CSIRO)
RP Li, Q (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Hubei, Peoples R China.
EM xhge@hust.edu.cn; lhpan@hust.edu.cn; qli_patrick@hust.edu.cn;
   g.mao@ieee.org; songtu@hust.edu.cn
RI Mao, Guoqiang/Q-4196-2019; Mao, Guoqiang/G-2585-2017; Ge,
   Xiaohu/N-8504-2015
OI Mao, Guoqiang/0000-0002-3598-4949; Mao, Guoqiang/0000-0002-3598-4949;
   Ge, Xiaohu/0000-0002-3204-5241
FU National Natural Science Foundation of China (NSFC) [61271224,
   61461136004]; NFSC Major International Joint Research Project
   [61210002]; Fundamental Research Funds for the Central Universities
   [2015XJGH011]; EU FP7-PEOPLE-IRSES, project acronym CROWN [610524];
   China International Joint Research Center of Green Communications and
   Networking [2015B01008]; Hubei Provincial Science and Technology
   Department [2016AHB006]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant 61271224 and Grant 61461136004,
   in part by the NFSC Major International Joint Research Project under
   Grant 61210002, in part by the Fundamental Research Funds for the
   Central Universities under Grant 2015XJGH011, in part by the EU
   FP7-PEOPLE-IRSES, project acronym CROWN, under Grant 610524, in part by
   the China International Joint Research Center of Green Communications
   and Networking under Grant 2015B01008, and in part by the Hubei
   Provincial Science and Technology Department under Grant 2016AHB006. The
   guest editor coordinating the review of this manuscript and approving it
   for publication was Dr. Shiwen Mao. (Corresponding author: Qiang Li.)
CR [Anonymous], 2016, WHIT VR OR BEAR NETW
   [Anonymous], 2016, WHIT 5G OP NEW BUS O
   Bai TY, 2015, IEEE T WIREL COMMUN, V14, P1100, DOI 10.1109/TWC.2014.2364267
   Banerjee I, 2012, PROCEEDINGS OF THE 2012 INTERNATIONAL CONFERENCE ON COMMUNICATIONS, DEVICES AND INTELLIGENT SYSTEMS (CODLS), P262, DOI 10.1109/CODIS.2012.6422188
   Bhushan N, 2014, IEEE COMMUN MAG, V52, P82, DOI 10.1109/MCOM.2014.6736747
   Bobrovs V., 2014, P SOC PHOTO-OPT INS, V9008, P117
   Choi SW, 2016, INT SOC DESIGN CONF, P9, DOI 10.1109/ISOCC.2016.7799715
   Ge XH, 2016, IEEE T VEH TECHNOL, V65, P7882, DOI 10.1109/TVT.2016.2539285
   Ge XH, 2015, IEEE T COMMUN, V63, P1019, DOI 10.1109/TCOMM.2015.2394386
   Hartl AD, 2016, IEEE T VIS COMPUT GR, V22, P1843, DOI 10.1109/TVCG.2015.2498612
   Hu DL, 2016, IEEE T VEH TECHNOL, V65, P6487, DOI 10.1109/TVT.2015.2475183
   Langlotz T, 2016, IEEE T VIS COMPUT GR, V22, P2385, DOI 10.1109/TVCG.2016.2593781
   Laoutaris N, 2013, IEEE ACM T NETWORK, V21, P1852, DOI 10.1109/TNET.2012.2237555
   Larsson EG, 2014, IEEE COMMUN MAG, V52, P186, DOI 10.1109/MCOM.2014.6736761
   LEE J, 2015, P IEEE ISCAS, P1790
   Li J, 2016, IEEE ACCESS, V4, P7519, DOI 10.1109/ACCESS.2016.2582836
   Liu JJ, 2016, IEEE J SEL AREA COMM, V34, P163, DOI 10.1109/JSAC.2015.2452492
   Qualcomm, 2016, WHIT MAK IMM VIRT RE
   Rappaport TS, 2013, IEEE ACCESS, V1, P335, DOI 10.1109/ACCESS.2013.2260813
   Razavi R, 2008, IET IMAGE PROCESS, V2, P150, DOI 10.1049/iet-ipr:20070183
   Sekhavat YA, 2017, IEEE T MULTIMEDIA, V19, P1041, DOI 10.1109/TMM.2016.2639380
   Singh S, 2015, IEEE J SEL AREA COMM, V33, P2196, DOI 10.1109/JSAC.2015.2435357
   Ta XY, 2009, IEEE T VEH TECHNOL, V58, P5152, DOI 10.1109/TVT.2009.2026480
   Tsai JC, 2005, IEEE T CIRC SYST VID, V15, P133, DOI 10.1109/TCSVT.2004.837015
   Vastardis N, 2013, IEEE COMMUN SURV TUT, V15, P1355, DOI 10.1109/SURV.2012.060912.00108
   Walravens C, 2008, IEEE POTENTIALS, V27, P12, DOI 10.1109/MPOT.2007.913659
   Wang G, 2016, IEEE T IMAGE PROCESS, V25, P5104, DOI 10.1109/TIP.2016.2603602
   Xiang W, 2016, IEEE NETWORK, V30, P30, DOI 10.1109/MNET.2016.7474341
   Yang K, 2016, IEEE ACCESS, V4, P713, DOI 10.1109/ACCESS.2016.2526622
   Zhang GZ, 2016, IEEE T COMMUN, V64, P876, DOI 10.1109/TCOMM.2016.2515596
NR 30
TC 80
Z9 87
U1 1
U2 45
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2017
VL 19
IS 10
BP 2345
EP 2358
DI 10.1109/TMM.2017.2733461
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5YG
UT WOS:000411247600017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhu, L
   Huang, Z
   Liu, XB
   He, XN
   Sun, JD
   Zhou, XF
AF Zhu, Lei
   Huang, Zi
   Liu, Xiaobai
   He, Xiangnan
   Sun, Jiande
   Zhou, Xiaofang
TI Discrete Multimodal Hashing With Canonical Views for Robust Mobile
   Landmark Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Binary embedding; canonical view-based discrete multi-modal hashing
   (CV-DMH); discrete optimization; intermediate representation; mobile
   landmark search (MLS); submodular function
ID BINARY-CODES; IMAGE; ALGORITHM
AB Mobile landmark search (MLS) recently receives increasing attention for its great practical values. However, it still remains unsolved due to two important challenges. One is high bandwidth consumption of query transmission, and the other is the huge visual variations of query images sent from mobile devices. In this paper, we propose a novel hashing scheme, named as canonical view based discrete multimodal hashing (CVDMH), to handle these problems. First, a submodular function is designed to measure visual representativeness and redundancy of a view set. With it, canonical views, which capture key visual appearances of landmark with limited redundancy, are efficiently discovered with an iterative mining strategy. Second, multimodal sparse coding is applied to transform visual features from multiple modalities into an intermediate representation. It can robustly and adaptively characterize visual contents of varied landmark images with certain canonical views. Finally, compact binary codes are learned on intermediate representation within a tailored discrete binary embedding model which preserves visual relations of images measured with canonical views and removes the involved noises. In this part, we develop a new augmented Lagrangian multiplier (ALM) based optimization method to directly solve the discrete binary codes. We can not only explicitly deal with the discrete constraint, but also consider the bit-uncorrelated constraint and balance constraint together. The proposed solution can desirably avoid accumulated quantization errors in conventional optimization method which simply adopts two-step "relaxing+rounding" framework. Experiments on real world landmark datasets demonstrate the superior performance of CV-DMH over several state-of-the-art methods.
C1 [Zhu, Lei; Huang, Zi; Zhou, Xiaofang] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
   [Liu, Xiaobai] San Diego State Univ, Dept Comp Sci, San Diego, CA 92182 USA.
   [He, Xiangnan] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
   [Sun, Jiande] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250014, Shandong, Peoples R China.
   [Sun, Jiande] Shandong Normal Univ, Inst Data Sci & Technol, Jinan 250014, Shandong, Peoples R China.
C3 University of Queensland; California State University System; San Diego
   State University; National University of Singapore; Shandong Normal
   University; Shandong Normal University
RP Zhu, L (corresponding author), Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
EM leizhu0608@gmail.com; huang@itee.uq.edu.au; xiaobai.liu@mail.sdsu.edu;
   xiangnanhe@gmail.com; jiandesun@hotmail.com; zxf@itee.uq.edu.au
RI Zhu, Lei/AAC-6810-2019; Sun, Jiande/B-4681-2018; Zhou,
   Xiangfeng/KDO-8724-2024; Zhou, Xiaofang/C-6169-2013; liu,
   xiaobai/J-4120-2014; Zhu, Lei/GQQ-1130-2022
OI Zhu, Lei/0000-0002-2993-7142; Zhou, Xiaofang/0000-0001-6343-1455; Zhu,
   Lei/0000-0002-5348-7532; HUANG, ZI/0000-0002-9738-4949
FU ARC [DP150103008, FT130101530]
FX This work was supported by ARC Project under Grant DP150103008 and Grant
   FT130101530. The guest editor coordinating the review of this manuscript
   and approving it for publication was Mr. Jingkuan Song. (Corresponding
   author: Lei Zhu.)
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], IEEE T MULT IN PRESS
   [Anonymous], 2000, Em: Proceedings of the 2000 ACM workshops on Multimedia, DOI DOI 10.1145/357744.357758
   [Anonymous], ADV MULTIMEDIA MODEL
   [Anonymous], IEEE T IMAG IN PRESS
   [Anonymous], CORR
   [Anonymous], 2016, P IJCAI
   [Anonymous], IEEE T CYBE IN PRESS
   [Anonymous], PATTERN RECOG
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231
   Chang XJ, 2017, IEEE T IMAGE PROCESS, V26, P3911, DOI 10.1109/TIP.2017.2708506
   Chang XJ, 2017, IEEE T CYBERNETICS, V47, P1180, DOI 10.1109/TCYB.2016.2539546
   Chang XJ, 2016, PROC CVPR IEEE, P1884, DOI 10.1109/CVPR.2016.208
   Chang XJ, 2017, IEEE T PATTERN ANAL, V39, P1617, DOI 10.1109/TPAMI.2016.2608901
   Chen T, 2014, IEEE T MULTIMEDIA, V16, P612, DOI 10.1109/TMM.2014.2301978
   Cheng ZY, 2016, MULTIMEDIA SYST, V22, P509, DOI 10.1007/s00530-014-0432-7
   Cheng ZY, 2016, SIGNAL PROCESS, V124, P13, DOI 10.1016/j.sigpro.2015.10.037
   Chum O., 2008, PROC IEEE INT C COMP, P1
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Duan LY, 2013, AI MAG, V34, P67, DOI 10.1609/aimag.v34i2.2469
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Gao LL, 2016, AAAI CONF ARTIF INTE, P1188
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Ji RR, 2012, INT J COMPUT VISION, V96, P290, DOI 10.1007/s11263-011-0472-9
   Jiang QY, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2248
   Kang WC, 2016, AAAI CONF ARTIF INTE, P1230
   Kim S, 2013, INT CONF ACOUST SPEE, P3123, DOI 10.1109/ICASSP.2013.6638233
   Liu L, 2015, IEEE T IMAGE PROCESS, V24, P956, DOI 10.1109/TIP.2015.2390975
   Liu W., 2014, P NEURAL INF PROCESS, P3419
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Liu XL, 2014, PATTERN RECOGN, V47, P748, DOI 10.1016/j.patcog.2013.08.022
   Luo YD, 2018, PATTERN RECOGN, V75, P128, DOI 10.1016/j.patcog.2017.02.034
   Min WQ, 2014, IEEE T MULTIMEDIA, V16, P623, DOI 10.1109/TMM.2014.2302744
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Shen FM, 2016, IEEE T IMAGE PROCESS, V25, P5610, DOI 10.1109/TIP.2016.2612883
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P831, DOI 10.1145/2733373.2806342
   Shi XS, 2016, LECT NOTES COMPUT SC, V9911, P419, DOI 10.1007/978-3-319-46478-7_26
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Song JK, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P831, DOI 10.1145/2964284.2964295
   Song J, 2016, IEEE T IMAGE PROCESS, V25, P4999, DOI 10.1109/TIP.2016.2601260
   Song JK, 2016, IEEE T MULTIMEDIA, V18, P484, DOI 10.1109/TMM.2016.2515990
   Song JK, 2014, IEEE T CYBERNETICS, V44, P1225, DOI 10.1109/TCYB.2013.2289351
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/iccv.2009.5459207
   Weyand T, 2013, IEEE I CONF COMP VIS, P3479, DOI 10.1109/ICCV.2013.432
   Xie L, 2016, AAAI CONF ARTIF INTE, P294
   Xie L, 2016, MULTIMED TOOLS APPL, V75, P9185, DOI 10.1007/s11042-016-3432-0
   Xie L, 2016, SIGNAL PROCESS, V124, P81, DOI 10.1016/j.sigpro.2015.10.010
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Yang Y, 2014, IEEE T MULTIMEDIA, V16, P1677, DOI 10.1109/TMM.2014.2323014
   Yu H, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P929, DOI 10.1109/ICIP.2002.1039125
   Zhang D, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P225
   Zhou WG, 2014, IEEE T MULTIMEDIA, V16, P601, DOI 10.1109/TMM.2014.2301979
   Zhu L., 2016, IJCAI, P3959
   Zhu L, 2017, IEEE T KNOWL DATA EN, V29, P472, DOI 10.1109/TKDE.2016.2562624
   Zhu L, 2015, IEEE T CYBERNETICS, V45, P2756, DOI 10.1109/TCYB.2014.2383389
   Zhu L, 2015, IEEE T MULTIMEDIA, V17, P981, DOI 10.1109/TMM.2015.2431496
NR 62
TC 114
Z9 114
U1 1
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2017
VL 19
IS 9
BP 2066
EP 2079
DI 10.1109/TMM.2017.2729025
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5XC
UT WOS:000411244200010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhou, Y
   Han, JH
   Yuan, XH
   Wei, ZC
   Hong, RC
AF Zhou, Yun
   Han, Jianghong
   Yuan, Xiaohui
   Wei, Zhenchun
   Hong, Richang
TI Inverse Sparse Group Lasso Model for Robust Object Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computer vision; hash distance; sparse coding; sparse group lasso;
   visual tracking
ID VISUAL TRACKING
AB Sparse representation has been applied to visual tracking. The visual tracking models based on sparse representation use a template set as dictionary atoms to reconstruct candidate samples without considering similarity among atoms. In this paper, we present a robust tracking method based on the inverse sparse group lasso model. Our method exploits both the group structure of similar candidate samples and the local structure between templates and samples. Unlike the conventional sparse representation, the templates are encoded by the candidate samples, and similar samples are selected to reconstruct the template at the group level, which facilitates inter-group sparsity. Every sample group achieves the intra-group sparsity so that the information between the related dictionary atoms is taken into account. Moreover, the local structure between templates and samples is considered to build the reconstruction model, which ensures that the computed coefficients similarity is consistent with the similarity between templates and samples. A gradient descent-based optimization method is employed and a sparse mapping table is obtained using the coefficient matrix and hash-distance weight matrix. Experiments were conducted with publicly available datasets and a comparison study was performed against 20 state-of-the-art methods. Both qualitative and quantitative results are reported. The proposed method demonstrated improved robustness and accuracy and exhibited comparable computational complexity.
C1 [Zhou, Yun; Han, Jianghong; Wei, Zhenchun; Hong, Richang] Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Anhui, Peoples R China.
   [Yuan, Xiaohui] Univ North Texas, Dept Comp Sci & Engn, Denton, TX 76203 USA.
   [Yuan, Xiaohui] China Univ Geosci, Coll Informat Engn, Wuhan 430074, Hubei, Peoples R China.
C3 Hefei University of Technology; University of North Texas System;
   University of North Texas Denton; China University of Geosciences
RP Wei, ZC (corresponding author), Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Anhui, Peoples R China.
EM zhouyun_hfut@163.com; hanjh@hfut.edu.cn; xiaohui.yuan@unt.edu;
   weizc@hfut.edu.cn; hongrc.hfut@gmail.com
RI Yuan, Xiaohui/AAQ-1172-2020
OI , Yun/0009-0000-3594-2151; Yuan, Xiaohui/0000-0001-6897-4563
FU International SAMP;T Cooperation Program of China [2014DFB10060];
   National Science Foundation of China [61472116]; Anhui Fund for
   Distinguished Young Scholars [1508085J04]
FX This work was supported in part by the International S&T Cooperation
   Program of China under Grant 2014DFB10060, in part by the National
   Science Foundation of China under Grant 61472116, and in part by the
   Anhui Fund for Distinguished Young Scholars under Grant 1508085J04. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Shu-Ching Chen. (Corresponding
   author: Zhenchun Wei.)
CR [Anonymous], 2010, Statistics
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Bao CL, 2012, PROC CVPR IEEE, P1830, DOI 10.1109/CVPR.2012.6247881
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Bengio S., 2009, Advances in Neural Information Processing Systems, V22, P82
   Chi YT, 2013, IEEE I CONF COMP VIS, P681, DOI 10.1109/ICCV.2013.90
   Di Lascio R, 2013, COMPUT VIS IMAGE UND, V117, P892, DOI 10.1016/j.cviu.2013.04.004
   Dinh TB, 2011, PROC CVPR IEEE, P1177, DOI 10.1109/CVPR.2011.5995733
   Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gao SH, 2014, IEEE T MULTIMEDIA, V16, P762, DOI 10.1109/TMM.2014.2299516
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Huaping Liu, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1702, DOI 10.1109/ICPR.2010.421
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kiliç V, 2015, IEEE T MULTIMEDIA, V17, P186, DOI 10.1109/TMM.2014.2377515
   Li F, 2016, IEEE T CIRC SYST VID, V26, P1697, DOI 10.1109/TCSVT.2015.2469171
   Li K, 2015, IEEE T CYBERNETICS, V45, P1401, DOI 10.1109/TCYB.2014.2351831
   Li X., 2007, P IEEE 11 INT C COMP, P14
   Ma B, 2015, IEEE T MULTIMEDIA, V17, P1818, DOI 10.1109/TMM.2015.2463221
   Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292
   Mihcak M. K., 2002, Security and Privacy in Digital Rights Management. ACM CCS-8 Workshop DRM 2001. Revised Papers (Lecture Notes in Computer Science Vol.2320), P13
   Oron S, 2012, PROC CVPR IEEE, P1940, DOI 10.1109/CVPR.2012.6247895
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891
   Shao F, 2016, IEEE T MULTIMEDIA, V18, P2104, DOI 10.1109/TMM.2016.2594142
   Simon N, 2013, J COMPUT GRAPH STAT, V22, P231, DOI 10.1080/10618600.2012.681250
   Wang D, 2015, IEEE T CYBERNETICS, V45, P1838, DOI 10.1109/TCYB.2014.2360924
   Wang D, 2015, IEEE T IMAGE PROCESS, V24, P2646, DOI 10.1109/TIP.2015.2427518
   Wang D, 2013, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR.2013.307
   Wang N, 2013, P ADV NEURAL INFORM
   Wang NY, 2014, PR MACH LEARN RES, V32, P1107
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu Y., 2011, VISUAL TRACKING BASE, P738
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yang F, 2014, IEEE T IMAGE PROCESS, V23, P1639, DOI 10.1109/TIP.2014.2300823
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Yang Y, 2016, IEEE T NEUR NET LEAR, V27, P952, DOI 10.1109/TNNLS.2015.2430821
   Yuan M., 2006, Journal of the Royal Statistical Society, Series B, V70, P53
   Yuan XH, 2017, IEEE-CAA J AUTOMATIC, V4, P677, DOI 10.1109/JAS.2017.7510625
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908
   Zhao L, 2015, IEEE T MULTIMEDIA, V17, P1936, DOI 10.1109/TMM.2015.2477058
   Zhong W, 2014, IEEE T IMAGE PROCESS, V23, P2356, DOI 10.1109/TIP.2014.2313227
   Zhou XZ, 2015, IEEE T MULTIMEDIA, V17, P145, DOI 10.1109/TMM.2014.2380914
   Zhu XF, 2013, IEEE T MULTIMEDIA, V15, P633, DOI 10.1109/TMM.2012.2233723
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
   Zhuang BH, 2014, IEEE T IMAGE PROCESS, V23, P1872, DOI 10.1109/TIP.2014.2308414
NR 51
TC 31
Z9 32
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2017
VL 19
IS 8
BP 1798
EP 1810
DI 10.1109/TMM.2017.2689918
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5VV
UT WOS:000411240400009
DA 2024-07-18
ER

PT J
AU He, QY
   Zhang, C
   Liu, JC
AF He, Qiyun
   Zhang, Cong
   Liu, Jiangchuan
TI CrowdTranscoding: Online Video Transcoding With Massive Viewers
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video transcoding; livecast; crowdsourcing
AB Driven by the advances in personal computing devices and the prevalence of high-speed network accesses, crowdsourced livecast platforms have emerged in recent years, through which numerous broadcasters lively stream their video content to fellow viewers. Compared to professional video producers and broadcasters, these new generation broadcasters are highly heterogeneous in terms of the network/system configurations and, therefore, the generated video quality, which calls for massive encoding and transcoding in order to unify the video sources and serve multiple quality versions to viewers with different configurations. On the other hand, with the rapid evolution in the hardware industry, high-performance processors become mainstream in personal computer market. More end devices can easily transcode high-quality videos in realtime. We witness huge computational resource among the massive fellow viewers that could potentially be used for transcoding. In this paper, we propose CrowdTranscoding, a novel framework for crowdsourced livecast systems that offloads the transcoding assignment to the massive viewers. We identify that the key challenges in CrowdTranscoding are to detect qualified stable viewers and to properly assign them to the source channels. We put forward a viewer crowdsourcing transcode scheduler to smartly schedule the workload assignment. Our solution has been evaluated under diverse viewer/channel conditions as well as different parameter settings. The trace-driven simulation confirms the superiority of CrowdTranscoder, while our PlanetLab-based and real world end-viewer experiments show the practical performance of our approach, which also give hint to the further enhancement.
C1 [He, Qiyun; Zhang, Cong; Liu, Jiangchuan] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
C3 Simon Fraser University
RP He, QY (corresponding author), Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
EM qiyunh@cs.sfu.ca; congz@cs.sfu.ca; jcliu@cs.sfu.ca
FU NPRP from the Qatar National Research Fund (a member of Qatar
   Foundation) [8-519-1-108]
FX This work was supported by the NPRP under Grant 8-519-1-108 from the
   Qatar National Research Fund (a member of Qatar Foundation). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Lingfen Sun.
CR [Anonymous], 2010, CHI EA
   [Anonymous], 2014, P 5 ACM MULT SYST C, DOI DOI 10.1145/2557642.2563671
   Bonomi Flavio, 2012, P 1 MCC WORKSH MOB C, P13, DOI 10.1145/2342509.2342513
   Chen F, 2015, IEEE T MULTIMEDIA, V17, P1471, DOI 10.1109/TMM.2015.2460193
   Cheng X, 2013, IEEE T MULTIMEDIA, V15, P1184, DOI 10.1109/TMM.2013.2265531
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Gill P, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P15
   Hamilton WA, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P1315, DOI 10.1145/2556288.2557048
   He QY, 2016, IEEE T MULTIMEDIA, V18, P916, DOI 10.1109/TMM.2016.2544698
   Hossfeld T, 2014, IEEE T MULTIMEDIA, V16, P541, DOI 10.1109/TMM.2013.2291663
   Kaytoue M., 2012, Proceedings of the 21st International Conference Companion on World Wide Web-WWW'12 Companion, P1181, DOI [DOI 10.1145/2187980.2188259, 10.1145/2187980.2188259]
   Krishnan SS, 2013, IEEE ACM T NETWORK, V21, P2001, DOI 10.1109/TNET.2013.2281542
   Li BC, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2505805
   Mu M, 2012, IEEE T MULTIMEDIA, V14, P1515, DOI 10.1109/TMM.2012.2217119
   Sripanidkulchai K., 2004, Proceedings of the 4th ACM SIGCOMM conference on Internet measurement, P41
   Traverso S, 2015, IEEE T MULTIMEDIA, V17, P1839, DOI 10.1109/TMM.2015.2458043
   Wang F, 2012, IEEE INFOCOM SER, P199, DOI 10.1109/INFCOM.2012.6195578
   Wang SX, 2013, IEEE T MULTIMEDIA, V15, P870, DOI 10.1109/TMM.2013.2240674
   Wang XF, 2013, IEEE T MULTIMEDIA, V15, P811, DOI 10.1109/TMM.2013.2239630
   Zhang C., 2015, ACM Workshop on Network and Operating Systems Support for Digital Audio and Video, P55, DOI DOI 10.1145/2736084.2736091
   Zhang G, 2015, IEEE T MULTIMEDIA, V17, P229, DOI 10.1109/TMM.2014.2383617
NR 21
TC 21
Z9 22
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2017
VL 19
IS 6
BP 1365
EP 1375
DI 10.1109/TMM.2017.2652061
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5YS
UT WOS:000404059400020
DA 2024-07-18
ER

PT J
AU Du, B
   Zhang, MF
   Zhang, LF
   Hu, RM
   Tao, DC
AF Du, Bo
   Zhang, Mengfei
   Zhang, Lefei
   Hu, Ruimin
   Tao, Dacheng
TI PLTD: Patch-Based Low-Rank Tensor Decomposition for Hyperspectral Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compression; hyperspectral image (HSI); low-rank decomposition;
   reconstruction; tensor representation
ID FACE RECOGNITION; COMPRESSION; SPARSE; REDUCTION; ALGORITHM; FEATURES
AB Recent years has witnessed growing interest in hyperspectral image (HSI) processing. In practice, however, HSIs always suffer from huge data size and mass of redundant information, which hinder their application in many cases. HSI compression is a straightforward way of relieving these problems. However, most of the conventional image encoding algorithms mainly focus on the spatial dimensions, and they need not consider the redundancy in the spectral dimension. In this paper, we propose a novel HSI compression and reconstruction algorithm via patch-based low-rank tensor decomposition (PLTD). Instead of processing the HSI separately by spectral channel or by pixel, we represent each local patch of the HSI as a third-order tensor. Then, the similar tensor patches are grouped by clustering to form a fourth-order tensor per cluster. Since the grouped tensor is assumed to be redundant, each cluster can be approximately decomposed to a coefficient tensor and three dictionary matrices, which leads to a low-rank tensor representation of both the spatial and spectral modes. The reconstructed HSI can then be simply obtained by the product of the coefficient tensor and dictionary matrices per cluster. In this way, the proposed PLTD algorithm simultaneously removes the redundancy in both the spatial and spectral domains in a unified framework. The extensive experimental results on various public HSI datasets demonstrate that the proposed method outperforms the traditional image compression approaches and other tensor-based methods.
C1 [Du, Bo; Zhang, Mengfei; Zhang, Lefei] Wuhan Univ, State Key Lab Software Engn, Wuhan 430072, Peoples R China.
   [Du, Bo; Zhang, Mengfei; Zhang, Lefei] Wuhan Univ, Sch Comp, Minist Educ, Key Lab Aerosp Informat Secur & Trusted Comp, Wuhan 430072, Peoples R China.
   [Hu, Ruimin] Wuhan Univ, Sch Comp, Natl Engn Res Ctr Multimedia Software, Wuhan 430072, Peoples R China.
   [Hu, Ruimin] Collaborat Innovat Ctr Geospatial Technol, Wuhan 430079, Peoples R China.
   [Tao, Dacheng] Univ Technol Sydney, Ctr Quantum Computat & Intelligent Syst, Ultimo, NSW 2007, Australia.
   [Tao, Dacheng] Univ Technol Sydney, Fac Engn & Informat Technol, Ultimo, NSW 2007, Australia.
C3 Wuhan University; Wuhan University; Wuhan University; University of
   Technology Sydney; University of Technology Sydney
RP Du, B (corresponding author), Wuhan Univ, State Key Lab Software Engn, Wuhan 430072, Peoples R China.; Du, B (corresponding author), Wuhan Univ, Sch Comp, Minist Educ, Key Lab Aerosp Informat Secur & Trusted Comp, Wuhan 430072, Peoples R China.
EM remoteking@whu.edu.cn; mzhang@whu.edu.cn; zhanglefei@whu.edu.cn;
   hrm1964@163.com; dacheng.tao@uts.edu.au
RI Zhang, Lefei/HHM-8850-2022; Tao, Dacheng/A-5449-2012; Zhang,
   Lefei/AAJ-5223-2020
OI Zhang, Lefei/0000-0003-0542-2280; Tao, Dacheng/0000-0001-7225-5449; 
FU National Natural Science Foundation of China [61471274, 91338111,
   61302111, 61671336, 61231015]; Natural Science Foundation of Hubei
   Province [2014CFB193, 2014CFB711]; Fundamental Research Funds for the
   Central Universities and National High Technology Research; Development
   Program of China (863 Program) [2015AA016306]; Australian Research
   Council [FT-130101457, DP-140102164, LE-140100061]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61471274, Grant 91338111, Grant
   61302111, Grant 61671336, and Grant 61231015, in part by the Natural
   Science Foundation of Hubei Province under Grant 2014CFB193 and Grant
   2014CFB711, in part by the Fundamental Research Funds for the Central
   Universities and National High Technology Research, in part by the
   Development Program of China (863 Program) under Grant 2015AA016306, and
   in part by the Australian Research Council Projects under Grant
   FT-130101457, Grant DP-140102164, and Grant LE-140100061. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Balakrishnan Prabhakaran.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2002, DATA FUSION DEFINITI
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Camps-Valls G, 2014, IEEE SIGNAL PROC MAG, V31, P45, DOI 10.1109/MSP.2013.2279179
   Cao XY, 2016, IEEE T IMAGE PROCESS, V25, P4677, DOI 10.1109/TIP.2016.2593343
   Chakrabarti A, 2011, PROC CVPR IEEE, P193, DOI 10.1109/CVPR.2011.5995660
   Chen B.-H., 2014, IEEE T MULTIMEDIA, V16, P1520
   Chen R., 2012, PATTERN RECOGN LETT, V33, P123
   Christophe E, 2008, IEEE T IMAGE PROCESS, V17, P2334, DOI 10.1109/TIP.2008.2005824
   Dong WS, 2015, IEEE I CONF COMP VIS, P442, DOI 10.1109/ICCV.2015.58
   Du B. X, 2014, P 2014 IEEE INT C MU, P1, DOI 10.1109/ICDL.2014.6893127
   Du Q, 2007, IEEE GEOSCI REMOTE S, V4, P201, DOI 10.1109/LGRS.2006.888109
   Du Q, 2014, IEEE J-STARS, V7, P2237, DOI 10.1109/JSTARS.2013.2274527
   Fu Y, 2013, IEEE I CONF COMP VIS, P457, DOI 10.1109/ICCV.2013.63
   Gao Y, 2015, IEEE T NEUR NET LEAR, V26, P1582, DOI 10.1109/TNNLS.2014.2339222
   GRASEDYCK L., 2013, GAMM-Mitteilungen36, V36, P53, DOI 10.1002/gamm.201310004
   Gu L, 2011, IEEE I CONF COMP VIS, P1987, DOI 10.1109/ICCV.2011.6126470
   Guo X, 2016, IEEE T GEOSCI REMOTE, V54, P3248, DOI 10.1109/TGRS.2016.2514404
   Guo X, 2013, ISPRS J PHOTOGRAMM, V83, P50, DOI 10.1016/j.isprsjprs.2013.06.001
   Guo ZH, 2012, IEEE T INF FOREN SEC, V7, P1094, DOI 10.1109/TIFS.2012.2189206
   Hasinoff SW, 2011, IEEE I CONF COMP VIS, P185, DOI 10.1109/ICCV.2011.6126241
   Holloway J, 2014, IEEE IMAGE PROC, P655, DOI 10.1109/ICIP.2014.7025131
   Jia XP, 2013, P IEEE, V101, P676, DOI 10.1109/JPROC.2012.2229082
   Jolliffe I. T., 2002, PRINCIPAL COMPONENT
   Karami A, 2012, IEEE J-STARS, V5, P444, DOI 10.1109/JSTARS.2012.2189200
   Khan Z, 2011, IEEE I CONF COMP VIS, P1935, DOI 10.1109/ICCV.2011.6126463
   Khelifi F, 2008, IEEE T MULTIMEDIA, V10, P316, DOI 10.1109/TMM.2008.917357
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Lathauwer L.D., 1997, SIGNAL PROCESSING BA
   Lee SH, 2012, IEEE T IMAGE PROCESS, V21, P2347, DOI 10.1109/TIP.2011.2181526
   Li J, 2016, IEEE T GEOSCI REMOTE, V54, P5425, DOI 10.1109/TGRS.2016.2564639
   Liangpei Zhang, 2013, IEEE Transactions on Geoscience and Remote Sensing, V51, P242, DOI 10.1109/TGRS.2012.2197860
   Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899
   Lu HP, 2008, IEEE T NEURAL NETWOR, V19, P18, DOI 10.1109/TNN.2007.901277
   Lu HP, 2011, PATTERN RECOGN, V44, P1540, DOI 10.1016/j.patcog.2011.01.004
   Luo Y, 2015, IEEE T KNOWL DATA EN, V27, P3111, DOI 10.1109/TKDE.2015.2445757
   Pan ZH, 2003, IEEE T PATTERN ANAL, V25, P1552, DOI 10.1109/TPAMI.2003.1251148
   Peng Y, 2014, PROC CVPR IEEE, P2949, DOI 10.1109/CVPR.2014.377
   Plaza A, 2009, REMOTE SENS ENVIRON, V113, pS110, DOI 10.1016/j.rse.2007.07.028
   Qian ZX, 2014, IEEE T MULTIMEDIA, V16, P1486, DOI 10.1109/TMM.2014.2316154
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Shah P., 2015, ADV NEURAL INFORM PR, P2539
   SHAPIRO JM, 1993, IEEE T SIGNAL PROCES, V41, P3445, DOI 10.1109/78.258085
   Shen HF, 2016, IEEE T CYBERNETICS, V46, P1388, DOI 10.1109/TCYB.2015.2446755
   Shen HF, 2015, IEEE GEOSC REM SEN M, V3, P61, DOI 10.1109/MGRS.2015.2441912
   Tao DC, 2007, IEEE T PATTERN ANAL, V29, P1700, DOI 10.1109/TPAMI.2007.1096
   Tao DP, 2016, IEEE T IMAGE PROCESS, V25, P2726, DOI 10.1109/TIP.2016.2553446
   Uzair M, 2015, IEEE T IMAGE PROCESS, V24, P1127, DOI 10.1109/TIP.2015.2393057
   Velasco-Forero S, 2013, PATTERN RECOGN, V46, P566, DOI 10.1016/j.patcog.2012.08.011
   Wang L, 2010, ELECTRON LETT, V46, P1601, DOI 10.1049/el.2010.1788
   Wang SF, 2010, IEEE T MULTIMEDIA, V12, P682, DOI 10.1109/TMM.2010.2060716
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie Q, 2016, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2016.187
   Xu C, 2016, IEEE T IMAGE PROCESS, V25, P1495, DOI 10.1109/TIP.2016.2524207
   Xu D, 2008, IEEE T CIRC SYST VID, V18, P36, DOI 10.1109/TCSVT.2007.903317
   Yuan QQ, 2012, IEEE T GEOSCI REMOTE, V50, P3660, DOI 10.1109/TGRS.2012.2185054
   Zhang LF, 2015, PATTERN RECOGN, V48, P3102, DOI 10.1016/j.patcog.2014.12.016
   Zhang LF, 2015, SIGNAL PROCESS, V106, P245, DOI 10.1016/j.sigpro.2014.08.005
   Zhang LF, 2015, NEUROCOMPUTING, V147, P358, DOI 10.1016/j.neucom.2014.06.052
   Zhang LF, 2014, IEEE T GEOSCI REMOTE, V52, P1030, DOI 10.1109/TGRS.2013.2246837
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhao B, 2016, ISPRS J PHOTOGRAMM, V116, P73, DOI 10.1016/j.isprsjprs.2016.03.004
   Zhao J, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2577886
   Zhong YF, 2015, IEEE T GEOSCI REMOTE, V53, P1411, DOI 10.1109/TGRS.2014.2340734
   Zhou Y, 2014, PROC CVPR IEEE, P3081, DOI 10.1109/CVPR.2014.394
   Zhu QQ, 2016, IEEE GEOSCI REMOTE S, V13, P747, DOI 10.1109/LGRS.2015.2513443
NR 66
TC 138
Z9 144
U1 1
U2 86
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2017
VL 19
IS 1
BP 67
EP 79
DI 10.1109/TMM.2016.2608780
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EH0SX
UT WOS:000391475200006
DA 2024-07-18
ER

PT J
AU Cantareira, GD
   Nonato, LG
   Paulovich, FV
AF Cantareira, Gabriel Dias
   Nonato, Luis Gustavo
   Paulovich, Fernando V.
TI MoshViz: A Detail plus Overview Approach to Visualize Music Elements
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Music visualization; overview representation
AB Amusic piece contains a large amount of information represented as a series of instructions corresponding to notes that must be played at specific times. These simple notes are combined to form complex harmonic structures that can be difficult to identify and analyze. Due to its simplicity and straightforward interpretation, music sheets and piano rolls have been the visual metaphor employed by most music visualization tools to support interpretation. Albeit it can represent all necessary elements to perform a music piece, these metaphors do not explicitly show many of the patterns and structures inherent to music arrangements, such as rhythm progression and harmonic interactions, needing users to create a mental model of them. Moreover, comparing different pieces and visualizing how a particular instrument track relates to the others is an issue not only for music sheet-based techniques, but also for most existing music visualization methods. In this paper, we present a novel visualization framework, called Music Overview, Stability, and Harmony Visualization (MoshViz), which facilitates the visualization and understanding of music renditions, focusing mainly on the visual analysis of specific musical instruments. Our approach creates a high-level model of music data and highlights structures of interest, enabling a detail+overview visualization to assist users in the task of identifying harmonic and melodic patterns. The usefulness and representativeness of MoshViz are confirmed by a set of user tests which demonstrate that the proposed visual metaphor matches, with a high degree of accuracy, the mental model of different users regarding the recognizable patterns of sounds.
C1 [Cantareira, Gabriel Dias; Nonato, Luis Gustavo; Paulovich, Fernando V.] Univ Sao Paulo, Inst Math & Comp Sci, BR-13566590 Sao Carlos, SP, Brazil.
C3 Universidade de Sao Paulo
RP Cantareira, GD (corresponding author), Univ Sao Paulo, Inst Math & Comp Sci, BR-13566590 Sao Carlos, SP, Brazil.
EM gabrieldc@icmc.usp.br; gnonato@icmc.usp.br; paulovic@icmc.usp.br
RI Nonato, Luis Gustavo/D-5782-2011; Paulovich, Fernando/G-1329-2010
OI Paulovich, Fernando/0000-0002-2316-760X
FU Sao Paulo Research Foundation (FAPESP) [2013/02455-5, 2011/22749-8];
   CNPq-Brazil; Fundacao de Amparo a Pesquisa do Estado de Sao Paulo
   (FAPESP) [13/02455-5] Funding Source: FAPESP
FX This work was supported in part by the Sao Paulo Research Foundation
   (FAPESP) under Grant #2013/02455-5 and Grant #2011/22749-8, and in part
   by CNPq-Brazil. The guest editor coordinating the review of this
   manuscript and approving it for publication was Dr. David Gotz.
CR Bergstrom Tony, 2007, Proceedings Graphics Interface 2007, P297, DOI 10.1145/1268517.1268565
   Berry Wallace., 1976, STRUCTURAL FUNCTIONS
   Chan WY, 2010, IEEE T VIS COMPUT GR, V16, P161, DOI 10.1109/TVCG.2009.63
   Ciuha P., 2010, P INT C MULT, P1677
   Hayashi A, 2011, IEEE INT CONF INF VI, P420, DOI 10.1109/IV.2011.19
   Hewitt M., 2008, MUSIC THEORY COMPUTE
   Keim DA, 2007, IEEE CONF VIS ANAL, P115, DOI 10.1109/VAST.2007.4389004
   Malinowski S., 2014, MUSIC ANIMATION MACH
   Mardirossian Arpi, 2007, ISMIR PP
   Miyazaki R., 2004, P INT COMP MUS C, P157
   Robertson G. G., 1991, Human Factors in Computing Systems. Reaching Through Technology. CHI '91. Conference Proceedings, P189, DOI 10.1145/108844.108883
   Rothstein William., 1989, PHRASE RHYTHM TONAL
   Smith SM, 1997, VISUALIZATION '97 - PROCEEDINGS, P499, DOI 10.1109/VISUAL.1997.663931
   Snydal J., 2005, CHI 05 EXTENDED ABST, P1805
   Watanabe F., 2003, P ICMC ICMA, P107
   Wattenberg M, 2002, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2002, P110, DOI 10.1109/INFVIS.2002.1173155
   Wolkowicz J., 2009, P 2009 INT COMP MUS, P53
NR 17
TC 6
Z9 7
U1 2
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2016
VL 18
IS 11
BP 2238
EP 2246
DI 10.1109/TMM.2016.2614226
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EA9BX
UT WOS:000386936900011
DA 2024-07-18
ER

PT J
AU Jerripothula, KR
   Cai, JF
   Yuan, JS
AF Jerripothula, Koteswar Rao
   Cai, Jianfei
   Yuan, Junsong
TI Image Co-segmentation via Saliency Co-fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Co-fusion; co-saliency; co-segmentation; fusion; saliency; segmentation
ID OBJECT DETECTION; COSEGMENTATION
AB Most existing high-performance co-segmentation algorithms are usually complex due to the way of co-labeling a set of images as well as the common need of fine-tuning few parameters for effective co-segmentation. In this paper, instead of following the conventional way of co-labeling multiple images, we propose to first exploit inter-image information through co-saliency, and then perform single-image segmentation on each individual image. To make the system robust and to avoid heavy dependence on one single saliency extraction method, we propose to apply multiple existing saliency extraction methods on each image to obtain diverse salient maps. Our major contribution lies in the proposed method that fuses the obtained diverse saliency maps by exploiting the inter-image information, which we call saliency co-fusion. Experiments on five benchmark datasets with eight saliency extraction methods show that our saliency co-fusion-based approach achieves competitive performance even without parameter fine-tuning when compared with the state-of-the-art methods.
C1 [Jerripothula, Koteswar Rao; Cai, Jianfei; Yuan, Junsong] Nanyang Technol Univ, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Jerripothula, KR (corresponding author), Nanyang Technol Univ, Singapore 639798, Singapore.
EM koteswar001@e.ntu.edu.sg; asjfcai@ntu.edu.sg; jsyuan@ntu.edu.sg
RI Yuan, Junsong/A-5171-2011; Jerripothula, Koteswar Rao/N-6980-2015; Yuan,
   Junsong/R-4352-2019; Cai, Jianfei/A-3691-2011
OI Jerripothula, Koteswar Rao/0000-0002-3507-3731; Cai,
   Jianfei/0000-0002-9444-3763; Yuan, Junsong/0000-0002-7901-8793
FU National Research Foundation, Prime Minister's Office, Singapore, under
   its IDM Futures Funding Initiative
FX This research was carried out at the Rapid-Rich Object Search (ROSE)
   Laboratory, Nanyang Technological University, Singapore. The ROSE
   Laboratory is supported by the National Research Foundation, Prime
   Minister's Office, Singapore, under its IDM Futures Funding Initiative
   and administered by the Interactive and Digital Media Programe Office.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   [Anonymous], P VIS COMM IM PROC V
   Batra D, 2009, IEEE IMAGE PROC, P2393, DOI 10.1109/ICIP.2009.5414482
   Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Chen HT, 2010, IEEE IMAGE PROC, P1117, DOI 10.1109/ICIP.2010.5650014
   Cheng MM, 2015, COMPUT GRAPH FORUM, V34, P193, DOI 10.1111/cgf.12758
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Collins MD, 2012, PROC CVPR IEEE, P1656, DOI 10.1109/CVPR.2012.6247859
   Dai JF, 2013, IEEE I CONF COMP VIS, P1305, DOI 10.1109/ICCV.2013.165
   Faktor A, 2013, IEEE I CONF COMP VIS, P1297, DOI 10.1109/ICCV.2013.164
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Guillaumin M, 2014, INT J COMPUT VISION, V110, P328, DOI 10.1007/s11263-014-0713-9
   Hochbaum DS, 2009, IEEE I CONF COMP VIS, P269, DOI 10.1109/ICCV.2009.5459261
   Ion A, 2011, IEEE I CONF COMP VIS, P2110, DOI 10.1109/ICCV.2011.6126486
   Jaccard P., 1912, New Phytologist, V11, P37, DOI [10.1111/j.1469-8137.1912.tb05611.x, DOI 10.1111/J.1469-8137.1912.TB05611.X]
   Jacobs D.E., 2010, ACM S USER INTERFACE, P219
   Jerripothula KR, 2015, IEEE IMAGE PROC, P4639, DOI 10.1109/ICIP.2015.7351686
   Jerripothula KR, 2014, IEEE IMAGE PROC, P3277, DOI 10.1109/ICIP.2014.7025663
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Joulin A, 2012, PROC CVPR IEEE, P542, DOI 10.1109/CVPR.2012.6247719
   Joulin A, 2010, PROC CVPR IEEE, P1943, DOI 10.1109/CVPR.2010.5539868
   Kai-Yueh Chang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2129, DOI 10.1109/CVPR.2011.5995415
   Kim G, 2012, PROC CVPR IEEE, P837, DOI 10.1109/CVPR.2012.6247756
   Kim G, 2011, IEEE I CONF COMP VIS, P169, DOI 10.1109/ICCV.2011.6126239
   Kolmogorov V., 2006, P IEEE CVPR, V1, P993, DOI DOI 10.1109/CVPR.2006.91
   Li HL, 2013, IEEE T MULTIMEDIA, V15, P1896, DOI 10.1109/TMM.2013.2271476
   Li HL, 2011, IEEE T IMAGE PROCESS, V20, P3365, DOI 10.1109/TIP.2011.2156803
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu Z, 2014, NEUROCOMPUTING, V135, P107, DOI 10.1016/j.neucom.2013.12.050
   Ma TY, 2013, PROC CVPR IEEE, P1955, DOI 10.1109/CVPR.2013.255
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   Meng F., 2014, P AS C COMP VIS, P258
   Meng FM, 2012, IEEE T MULTIMEDIA, V14, P1429, DOI 10.1109/TMM.2012.2197741
   Mukherjee Lopamudra, 2011, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, P1881, DOI 10.1109/CVPR.2011.5995420
   Mukherjee L, 2009, PROC CVPR IEEE, P2028, DOI 10.1109/CVPRW.2009.5206652
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Rubinstein M, 2013, PROC CVPR IEEE, P1939, DOI 10.1109/CVPR.2013.253
   Rubio JC, 2012, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2012.6247745
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1
   Tang K, 2014, PROC CVPR IEEE, P1464, DOI 10.1109/CVPR.2014.190
   Vedaldi A, 2008, LECT NOTES COMPUT SC, V5305, P705, DOI 10.1007/978-3-540-88693-8_52
   Vicente S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2217, DOI 10.1109/CVPR.2011.5995530
   Vicente S, 2010, LECT NOTES COMPUT SC, V6312, P465, DOI 10.1007/978-3-642-15552-9_34
   Wang F, 2013, IEEE I CONF COMP VIS, P849, DOI 10.1109/ICCV.2013.110
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yuan JS, 2012, IEEE T IMAGE PROCESS, V21, P2207, DOI 10.1109/TIP.2011.2181952
NR 50
TC 114
Z9 118
U1 2
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2016
VL 18
IS 9
BP 1896
EP 1909
DI 10.1109/TMM.2016.2576283
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DT4GI
UT WOS:000381437800018
DA 2024-07-18
ER

PT J
AU Kumar, N
   Sethi, A
AF Kumar, Neeraj
   Sethi, Amit
TI Fast Learning-Based Single Image Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Approximations; function; group method of data handling; polynomial
   neural networks; super-resolution (SR)
ID INTERPOLATION
AB We present a learning-based single image super-resolution (SISR) method to obtain a high resolution (HR) image from a single given low resolution (LR) image. Our method gives more accurate results while also testing (runs) and training faster with a smaller number of training samples compared to other methods. We posed SISR as a problem of estimating a function to predict the pixels of an HR patch using its corresponding LR pixels and their spatial neighborhood. We studied the impact of varying the input LR and output HR patch sizes and gained the following insights: reconstruction accuracy for a given output HR patch size improves when input LR patch size is increased, but the improvement saturates after including a few extra layers of LR pixels. Moreover, HR reconstruction accuracy is the highest when the output HR patch is restricted to only that which corresponds to one LR pixel. We used zero component analysis as a preprocessing step to enhance the estimation optimization energy on perceptually salient features such as edges. We tapped into the ability of polynomial neural networks to hierarchically learn refinements of a function that maps LR to HR patches. Accurate HR reconstruction with small input and output patch sizes not only makes learning more efficient, it also indicates that SISR is a highly local problem. In contrast, a recently proposed and related technique using convolutional neural networks needs much larger training set and longer training time because of larger input-output patch sizes and a computationally expensive learning algorithm.
C1 [Kumar, Neeraj; Sethi, Amit] Indian Inst Technol Guwahati, Dept Elect & Elect Engn, Gauhati 781039, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Guwahati
RP Kumar, N (corresponding author), Indian Inst Technol Guwahati, Dept Elect & Elect Engn, Gauhati 781039, India.
EM neeraj.kuma@iitg.ac.in; amitsethi@iitg.ac.in
RI Kumar, Neeraj/AAD-4748-2020
CR Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   CROWLEY JL, 1987, IEEE T PATTERN ANAL, V9, P113, DOI 10.1109/TPAMI.1987.4767876
   Dai SY, 2009, IEEE T IMAGE PROCESS, V18, P969, DOI 10.1109/TIP.2009.2012908
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Foo C. Y., 2010, UNSUPERVISED FEATURE
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Freeman WT., 2011, Advances in Markov Random Fields for Vision and Image Processing
   Gilboa G., 2004, THESIS
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Kang LW, 2015, IEEE T MULTIMEDIA, V17, P921, DOI 10.1109/TMM.2015.2434216
   Kim KI, 2010, IEEE T PATTERN ANAL, V32, P1127, DOI 10.1109/TPAMI.2010.25
   Kim SS, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1723
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Kumar N, 2012, INT C PATT RECOG, P3468
   Mallat S, 2010, IEEE T IMAGE PROCESS, V19, P2889, DOI 10.1109/TIP.2010.2049927
   Massopust P. R., 2010, Interpolation and Approximation with Splines and Fractals
   Nikolaev NY, 2003, IEEE T NEURAL NETWOR, V14, P337, DOI 10.1109/TNN.2003.809405
   Peleg T, 2014, IEEE T IMAGE PROCESS, V23, P2569, DOI 10.1109/TIP.2014.2305844
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Pollak I, 2002, IEEE SIGNAL PROC MAG, V19, P26, DOI 10.1109/MSP.2002.1028350
   Schetzen M., 1980, The Volterra and Wiener Theories of Nonlinear Systems
   Sethi A, 2012, ANNU IEEE IND CONF, P790
   Sun J, 2008, PROC CVPR IEEE, P2471, DOI 10.1109/CVPR.2008.4587659
   Tian J, 2011, EXPERT SYST APPL, V38, P12514, DOI 10.1016/j.eswa.2011.04.037
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Wang SL, 2012, PROC CVPR IEEE, P2216, DOI 10.1109/CVPR.2012.6247930
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang CY, 2013, IEEE I CONF COMP VIS, P561, DOI 10.1109/ICCV.2013.75
   Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang MC, 2013, IEEE T MULTIMEDIA, V15, P498, DOI 10.1109/TMM.2012.2232646
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang XJ, 2008, IEEE T IMAGE PROCESS, V17, P887, DOI 10.1109/TIP.2008.924279
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
   ZONTAK M, 2011, PROC CVPR IEEE, P977, DOI DOI 10.1109/CVPR.2011.5995401
NR 38
TC 34
Z9 36
U1 0
U2 39
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2016
VL 18
IS 8
BP 1504
EP 1515
DI 10.1109/TMM.2016.2571625
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR5YJ
UT WOS:000379978000005
DA 2024-07-18
ER

PT J
AU Zhang, L
   Li, LD
   Li, HY
   Yang, M
AF Zhang, Lin
   Li, Lida
   Li, Hongyu
   Yang, Meng
TI 3D Ear Identification Using Block-Wise Statistics-Based Features and
   LC-KSVD
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D ear; dictionary learning; label consistent K-SVD (LC-KSVD); sparse
   coding; surface types
ID SPARSE REPRESENTATION; FACE RECOGNITION; DICTIONARY; UNIVERSAL
AB Biometrics authentication has been corroborated to be an effective method for recognizing a person's identity with high confidence. In this field, the use of three-dimensional (3D) ear shape is a recent trend. As a biometric identifier, the ear has several inherent merits. However, although a great deal of efforts have been devoted, there is still large room for improvement in developing a highly effective and efficient 3D ear identification approach. In this paper, we attempt to fill this gap to some extent by proposing a novel 3D ear classification scheme that makes use of the label consistent K-SVD (LC-KSVD) framework. As an effective supervised dictionary learning algorithm, LC-KSVD learns a single compact discriminative dictionary for sparse coding and a multi-class linear classifier simultaneously. To use the LC-KSVD framework, one key issue is how to extract feature vectors from 3D ear scans. To this end, we propose a block-wise statistics-based feature extraction scheme. Specifically, we divide a 3D ear region of interest into uniform blocks and extract a histogram of surface types from each block; histograms from all blocks are then concatenated to form the desired feature vector. Feature vectors extracted in this way are highly discriminative and are robust to mere misalignment between samples. Experiments demonstrate that our approach can achieve better recognition accuracy than the other state-of-the-art methods. More importantly, its computational complexity is extremely low, making it quite suitable for the large-scale identification applications. MATLAB source codes are publicly online available at http://sse.tongji.edu.cn/linzhang/LCKSVDEar/LCKSVDEar.htm.
C1 [Zhang, Lin; Li, Lida; Li, Hongyu] Tongji Univ, Sch Software Engn, Shanghai 201804, Peoples R China.
   [Zhang, Lin] Shenzhen Inst Future Media Technol, Shenzhen 518055, Peoples R China.
   [Zhang, Lin; Yang, Meng] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
   [Li, Hongyu] Xiamen Macrovis Technol Co Ltd, Xiamen 361008, Peoples R China.
C3 Tongji University; Shenzhen University
RP Zhang, L (corresponding author), Tongji Univ, Sch Software Engn, Shanghai 201804, Peoples R China.; Zhang, L (corresponding author), Shenzhen Inst Future Media Technol, Shenzhen 518055, Peoples R China.
EM cslinzhang@tongji.edu.cn; lld533@hotmail.com; hyli@tongji.edu.cn;
   yang.meng@szu.edu.cn
RI Yang, Michael Ying/AAC-6698-2019
OI Yang, Michael Ying/0000-0002-0649-9987; Zhang, Lin/0000-0002-4360-5523
FU Natural Science Foundation of China [61201394, 61402289]; Shanghai
   Pujiang Program [14PJ1408100]; CCF-Tencent Open Research Fund under
   Grant CCF-Tencent [RAGR20150112]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61201394 and Grant 61402289, in part by the Shanghai
   Pujiang Program under Grant 14PJ1408100, and in part by the CCF-Tencent
   Open Research Fund under Grant CCF-Tencent RAGR20150112. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Cha Zhang.
CR Abaza A, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2431211.2431221
   [Anonymous], 2008, 2008 IEEE C COMP VIS, DOI DOI 10.1109/CVPR.2008.4587652
   [Anonymous], 2003, Handbook of fingerprint recognition
   [Anonymous], 2007, HDB BIOMETRICS HDB B
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   BESL PJ, 1988, IEEE T PATTERN ANAL, V10, P167, DOI 10.1109/34.3881
   Bowyer K. W., 2007, ND COLLECTION J2
   Bowyer KW, 2008, COMPUT VIS IMAGE UND, V110, P281, DOI 10.1016/j.cviu.2007.08.005
   Bustard JD, 2010, IEEE T SYST MAN CY A, V40, P486, DOI 10.1109/TSMCA.2010.2041652
   Chang K, 2003, IEEE T PATTERN ANAL, V25, P1160, DOI 10.1109/TPAMI.2003.1227990
   Chen H, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P123
   Chen H, 2007, IEEE T PATTERN ANAL, V29, P718, DOI 10.1109/TPAMI.2007.1005
   Chen Hui., 2005, COMPUTER VISION PATT, P122
   Choras M., 2005, Electronic Letters on Computer Vision and Image Analysis (Journal ELCVIA), V5, P84, DOI DOI 10.5565/REV/ELCVIA.108
   Daugman J, 2004, IEEE T CIRC SYST VID, V14, P21, DOI 10.1109/TCSVT.2003.818350
   Do Carmo M., 1976, Differential Geometry of Curves and Surfaces
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Flusser J., 2009, Moments and Moment Invariants in Pattern Recognition
   Fulkerson B, 2008, LECT NOTES COMPUT SC, V5302, P179, DOI 10.1007/978-3-540-88682-2_15
   Golub GH, 1999, SIAM J MATRIX ANAL A, V21, P185, DOI 10.1137/S0895479897326432
   He LH, 2012, PROC CVPR IEEE, P1146, DOI 10.1109/CVPR.2012.6247795
   Hurley DJ, 2005, COMPUT VIS IMAGE UND, V98, P491, DOI 10.1016/j.cviu.2004.11.001
   Iannarelli A., 1989, FORENSIC IDENTIFICAT
   Islam SMS, 2008, LECT NOTES COMPUT SC, V5259, P1081, DOI 10.1007/978-3-540-88458-3_98
   Islam SMS, 2011, INT J COMPUT VISION, V95, P52, DOI 10.1007/s11263-011-0436-0
   Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88
   Kakadiaris I, 2007, SIGNALS COMMUN TECHN, P139, DOI 10.1007/978-3-540-49346-4_10
   Kong AWK, 2004, INT C PATT RECOG, P520, DOI 10.1109/ICPR.2004.1334184
   Kumar A, 2013, PATTERN RECOGN, V46, P73, DOI 10.1016/j.patcog.2012.06.020
   Lazebnik S, 2009, IEEE T PATTERN ANAL, V31, P1294, DOI 10.1109/TPAMI.2008.138
   Lee CT, 2012, IEEE T MULTIMEDIA, V14, P608, DOI 10.1109/TMM.2012.2191398
   Liu F, 2015, NEUROCOMPUTING, V168, P599, DOI 10.1016/j.neucom.2015.05.065
   Mairal J, 2012, IEEE T PATTERN ANAL, V34, P791, DOI 10.1109/TPAMI.2011.156
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   Nixon M., 2006, Human Identification Based on Gait
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Passalis G, 2007, 2007 IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P39, DOI 10.1109/AVSS.2007.4425283
   Peng YG, 2012, IEEE T PATTERN ANAL, V34, P2233, DOI 10.1109/TPAMI.2011.282
   Perronnin F, 2008, IEEE T PATTERN ANAL, V30, P1243, DOI 10.1109/TPAMI.2007.70755
   Pham D. H., 2013, P IEEE C COMP VIS PA, P1
   Ramirez I, 2010, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR.2010.5539964
   Sanchez-Reillo R, 2000, IEEE T PATTERN ANAL, V22, P1168, DOI 10.1109/34.879796
   Shi XS, 2014, PATTERN RECOGN, V47, P2447, DOI 10.1016/j.patcog.2014.01.007
   Slonim N, 2000, ADV NEUR IN, V12, P617
   Theoharis T., 2007, BMVA S VIS BAS BIOM
   Theoharis T, 2008, PATTERN RECOGN, V41, P796, DOI 10.1016/j.patcog.2007.06.024
   Tropp J., 2012, IEEE T PATTERN ANAL, V34, P2233
   Wagner A, 2012, IEEE T PATTERN ANAL, V34, P372, DOI 10.1109/TPAMI.2011.112
   Wechsler H., 2006, Reliable face recognition methods-system design, implementation and evaluation
   Winn J, 2005, IEEE I CONF COMP VIS, P1800
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yan P, 2005, PROC SPIE, V5779, P282, DOI 10.1117/12.603154
   Yan P, 2005, LECT NOTES COMPUT SC, V3546, P503
   Yan P., P 2005 IEEE COMPUTER, DOI [DOI 10.1109/CVPR.2005.450, 10.1109/CVPR.2005.450]
   Yan P., 2005, Advanced 3D Ima in for Safet and Securit, VIII, P121
   Yan P, 2007, IEEE T PATTERN ANAL, V29, P1297, DOI 10.1109/TPAMI.2007.1067
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Yang JF, 2011, SIAM J SCI COMPUT, V33, P250, DOI 10.1137/090777761
   Yang M, 2015, NEUROCOMPUTING, V168, P70, DOI 10.1016/j.neucom.2015.06.013
   Yang M, 2011, IEEE I CONF COMP VIS, P543, DOI 10.1109/ICCV.2011.6126286
   Yuan L, 2012, PATTERN RECOGN LETT, V33, P182, DOI 10.1016/j.patrec.2011.09.041
   Zhang D, 2012, ACM COMPUT SURV, V44, DOI 10.1145/2071389.2071391
   Zhang D, 2009, IEEE T SYST MAN CY C, V39, P505, DOI 10.1109/TSMCC.2009.2020790
   Zhang L, 2012, PATTERN RECOGN, V45, P2522, DOI 10.1016/j.patcog.2012.01.017
   Zhang L, 2010, PATTERN RECOGN, V43, P2560, DOI 10.1016/j.patcog.2010.01.020
   Zhang L, 2014, QUOTIENT SPACE BASED PROBLEM SOLVING: A THEORETICAL FOUNDATION OF GRANULAR COMPUTING, P1, DOI 10.1016/B978-0-12-410387-0.00001-9
   Zhang W., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P1241
   Zhou N, 2012, PROC CVPR IEEE, P3490, DOI 10.1109/CVPR.2012.6248091
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
NR 69
TC 10
Z9 10
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2016
VL 18
IS 8
BP 1531
EP 1541
DI 10.1109/TMM.2016.2566578
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR5YJ
UT WOS:000379978000007
DA 2024-07-18
ER

PT J
AU Zhang, WM
   Wang, H
   Hou, DD
   Yu, NH
AF Zhang, Weiming
   Wang, Hui
   Hou, Dongdong
   Yu, Nenghai
TI Reversible Data Hiding in Encrypted Images by Reversible Image
   Transformation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image encryption; outsoured storage in cloud; privacy protection;
   reversible data hiding (RDH); reversible image transformation (RIT)
ID PREDICTION; EXPANSION
AB With the popularity of outsourcing data to the cloud, it is vital to protect the privacy of data and enable the cloud server to easily manage the data at the same time. Under such demands, reversible data hiding in encrypted images (RDH-EI) attracts more and more researchers' attention. In this paper, we propose a novel framework for RDH-EI based on reversible image transformation (RIT). Different from all previous encryption-based frameworks, in which the ciphertexts may attract the notation of the curious cloud, RIT-based framework allows the user to transform the content of original image into the content of another target image with the same size. The transformed image, that looks like the target image, is used as the "encrypted image," and is outsourced to the cloud. Therefore, the cloud server can easily embed data into the "encrypted image" by any RDH methods for plaintext images. And thus a client-free scheme for RDH-EI can be realized, that is, the data-embedding process executed by the cloud server is irrelevant with the processes of both encryption and decryption. Two RDH methods, including traditional RDH scheme and unified embedding and scrambling scheme, are adopted to embed watermark in the encrypted image, which can satisfy different needs on image quality and large embedding capacity, respectively.
C1 [Zhang, Weiming; Wang, Hui; Hou, Dongdong; Yu, Nenghai] Univ Sci & Technol China, CAS Key Lab Electromagnet Space Informat, Hefei 230026, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhang, WM (corresponding author), Univ Sci & Technol China, CAS Key Lab Electromagnet Space Informat, Hefei 230026, Peoples R China.
EM zhangwm@ustc.edu.cn; whlory@mail.ustc.edu.cn; houdd@mail.ustc.edu.cn;
   ynh@ustc.edu.cn
RI wang, huimin/HDM-8421-2022
OI Zhang, Weiming/0000-0001-5576-6108
FU Natural Science Foundation of China [61572452, 61502007]; China
   Postdoctoral Science Foundation [2015M582015]; Strategic Priority
   Research Program of the Chinese Academy of Sciences [XDA06030601]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61572452 and Grant 61502007, in part by the China
   Postdoctoral Science Foundation under Grant 2015M582015, and in part by
   the Strategic Priority Research Program of the Chinese Academy of
   Sciences under Grant XDA06030601. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Alessandro Piva.
CR [Anonymous], P 42 ANN ALL C COMM
   Bao F, 2005, IEEE T INF TECHNOL B, V9, P554, DOI 10.1109/TITB.2005.855556
   Cao XC, 2016, IEEE T CYBERNETICS, V46, P1132, DOI 10.1109/TCYB.2015.2423678
   Dragoi IC, 2014, IEEE T IMAGE PROCESS, V23, P1779, DOI 10.1109/TIP.2014.2307482
   Hong W, 2012, IEEE SIGNAL PROC LET, V19, P199, DOI 10.1109/LSP.2012.2187334
   Hu XC, 2015, IEEE T INF FOREN SEC, V10, P653, DOI 10.1109/TIFS.2015.2392556
   Hu XC, 2013, IEEE T INF FOREN SEC, V8, P779, DOI 10.1109/TIFS.2013.2256131
   Hwang K, 2010, IEEE INTERNET COMPUT, V14, P14, DOI 10.1109/MIC.2010.86
   Johnson M, 2004, IEEE T SIGNAL PROCES, V52, P2992, DOI 10.1109/TSP.2004.833860
   Lai IJ, 2011, IEEE T INF FOREN SEC, V6, P936, DOI 10.1109/TIFS.2011.2135853
   Lee YL, 2014, IEEE T CIRC SYST VID, V24, P695, DOI 10.1109/TCSVT.2013.2283431
   Liu W, 2010, IEEE T IMAGE PROCESS, V19, P1097, DOI 10.1109/TIP.2009.2038773
   Ma KD, 2013, IEEE T INF FOREN SEC, V8, P553, DOI 10.1109/TIFS.2013.2248725
   Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964
   Ou B, 2013, IEEE T IMAGE PROCESS, V22, P5010, DOI 10.1109/TIP.2013.2281422
   Qian ZX, 2016, IEEE T CIRC SYST VID, V26, P636, DOI 10.1109/TCSVT.2015.2418611
   Rad Reza Moradi, 2014, IEEE Trans Image Process, V23, P1463, DOI 10.1109/TIP.2014.2302681
   Sachnev V, 2009, IEEE T CIRC SYST VID, V19, P989, DOI 10.1109/TCSVT.2009.2020257
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yu SC, 2010, IEEE INFOCOM SER, DOI 10.1109/INFCOM.2010.5462174
   Zhang WM, 2015, IEEE T IMAGE PROCESS, V24, P294, DOI 10.1109/TIP.2014.2358881
   Zhang WM, 2014, SIGNAL PROCESS, V94, P118, DOI 10.1016/j.sigpro.2013.06.023
   Zhang WM, 2013, IEEE T IMAGE PROCESS, V22, P2775, DOI 10.1109/TIP.2013.2257814
   Zhang XP, 2012, IEEE T INF FOREN SEC, V7, P826, DOI 10.1109/TIFS.2011.2176120
   Zhang XP, 2011, IEEE SIGNAL PROC LET, V18, P255, DOI 10.1109/LSP.2011.2114651
   Zhou JT, 2016, IEEE T CIRC SYST VID, V26, P441, DOI 10.1109/TCSVT.2015.2416591
NR 27
TC 98
Z9 106
U1 0
U2 63
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2016
VL 18
IS 8
BP 1469
EP 1479
DI 10.1109/TMM.2016.2569497
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR5YJ
UT WOS:000379978000002
DA 2024-07-18
ER

PT J
AU Xu, JX
   Wah, BW
AF Xu, Jingxi
   Wah, Benjamin W.
TI Optimality of Greedy Algorithm for Generating Just-Noticeable Difference
   Surfaces
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Greedy algorithm; interactive multimedia; just-noticeable difference
   (JND); perceptual quality; subjective tests
ID QUALITY
AB Tuning multimedia applications at run time to achieve high perceptual quality entails the search of nonlinear mappings that determine how control inputs should be set in order to lead to high user-perceived quality. Offline subjective tests are often used for this purpose but they are expensive to conduct because each can only evaluate one mapping at a time and there can be infinitely many such mappings to be evaluated. In this paper, we present a greedy algorithm that uses a small number of subjective test results to accurately approximate this space of mappings. Based on an axiom on monotonicity and the property of just-noticeable differences, we prove its optimality in minimizing the average absolute error between the approximate and the original mappings. We further demonstrate the results using numerical simulations and the application of the mappings found to tune the control of the multimedia game BZFlag.
C1 [Xu, Jingxi; Wah, Benjamin W.] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
RP Xu, JX (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
EM jxxu@cse.cuhk.edu.hk; bwah@cuhk.edu.hk
CR Adler H.E, 1966, Elemente der Psychophysik Elements of psychophysics, V1
   Chou CH, 1995, IEEE T CIRC SYST VID, V5, P467, DOI 10.1109/76.475889
   JAYANT N, 1992, IEEE J SEL AREA COMM, V10, P796, DOI 10.1109/49.138986
   Lee JS, 2014, IEEE T MULTIMEDIA, V16, P564, DOI 10.1109/TMM.2013.2292590
   Ma L, 2011, SIGNAL PROCESS-IMAGE, V26, P162, DOI 10.1016/j.image.2011.02.002
   Murray G., 2013, Rotation about an arbitrary axis in 3 dimensions
   Romero VJ, 2004, STRUCT SAF, V26, P201, DOI 10.1016/j.strusafe.2003.03.001
   Sat B, 2009, IEEE T MULTIMEDIA, V11, P1114, DOI 10.1109/TMM.2009.2026097
   TAYLOR MM, 1967, J ACOUST SOC AM, V41, P782, DOI 10.1121/1.1910407
   WATSON AB, 1983, PERCEPT PSYCHOPHYS, V33, P113, DOI 10.3758/BF03202828
   Xu J., 2013, P IEEE CUST INT CIRC, P1
   Xu JX, 2015, IEEE MULTIMEDIA, V22, P14, DOI 10.1109/MMUL.2015.70
   Yang XK, 2005, SIGNAL PROCESS-IMAGE, V20, P662, DOI 10.1016/j.image.2005.04.001
NR 13
TC 4
Z9 5
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2016
VL 18
IS 7
BP 1330
EP 1337
DI 10.1109/TMM.2016.2557728
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR2RW
UT WOS:000379752600009
DA 2024-07-18
ER

PT J
AU Kordelas, GA
   Alexiadis, DS
   Daras, P
   Izquierdo, E
AF Kordelas, Georgios A.
   Alexiadis, Dimitrios S.
   Daras, Petros
   Izquierdo, Ebroul
TI Content-Based Guided Image Filtering, Weighted Semi-Global Optimization,
   and Efficient Disparity Refinement for Fast and Accurate Disparity
   Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Disparity estimation; disparity refinement; guided image filter;
   outliers handling; semi-global optimization; stereo matching; stereo
   vision
ID BELIEF PROPAGATION; STEREO; CENSUS
AB This paper presents a novel approach, which relies on content-based guided image filtering and weighted semi-global optimization for fast and accurate disparity estimation. The approach uses a pixel-based cost term that combines gradient, Gabor-Feature, and color information. The pixel-based matching costs are filtered by applying guided image filtering, which relies on rectangular support windows of two different sizes. In this way, two filtered costs are estimated for each pixel. Among the two filtered costs, the one that will be finally assigned to each pixel depends on the local image content around this pixel. The filtered cost volume is further refined by exploiting weighted semi-global optimization, which improves the disparity estimation accuracy. Finally, the disparity refinement in outlier regions relies on a straightforward and time-efficient outliers handling scheme and on a simple approach which deals with the disparity outliers at depth discontinuities. Experimental results on the Middlebury online stereo evaluation benchmark and 27 additional Middlebury stereo pairs prove that our method is able to generate disparity maps with high accuracy while keeping the computational cost low.
C1 [Kordelas, Georgios A.; Alexiadis, Dimitrios S.; Daras, Petros] Ctr Res & Technol Hellas CERTH, Inst Informat Technol, GR-57001 Thessaloniki, Greece.
   [Kordelas, Georgios A.; Izquierdo, Ebroul] Queen Mary Univ London, Sch Elect Engn & Comp Sci, London E1 4NS, England.
C3 Centre for Research & Technology Hellas; University of London; Queen
   Mary University London
RP Kordelas, GA; Alexiadis, DS; Daras, P (corresponding author), Ctr Res & Technol Hellas CERTH, Inst Informat Technol, GR-57001 Thessaloniki, Greece.; Kordelas, GA; Izquierdo, E (corresponding author), Queen Mary Univ London, Sch Elect Engn & Comp Sci, London E1 4NS, England.
EM kordelas@iti.gr; dalexiad@iti.gr; daras@iti.gr;
   ebroul.izquierdo@eecs.qmul.ac.uk
RI Daras, Petros/F-5284-2012
OI Daras, Petros/0000-0003-3814-6710
FU LASIE EU [607480]
FX This work was supported by the LASIE EU-funded IP project under Contract
   607480. The associate editor coordinating the review of this manuscript
   and approving it for publication was Prof. Jing-Ming Guo.
CR Akin A, 2014, INTEGRATION, V47, P365, DOI 10.1016/j.vlsi.2013.11.002
   [Anonymous], P SPIE
   [Anonymous], P ASCI IPA SIKS TRAC
   [Anonymous], 2012, AS C COMP VIS
   [Anonymous], 2010, 2010 25 INT C IM VIS
   [Anonymous], T HIGH PERFORM EMBED
   [Anonymous], P INT C 3D IM
   [Anonymous], IEEE T CIRC IN PRESS
   [Anonymous], P INT S 3D DAT PROC
   Banz C., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P514, DOI 10.1109/ICCVW.2011.6130286
   Birchfield S, 1999, INT J COMPUT VISION, V35, P269, DOI 10.1023/A:1008160311296
   Cigla C, 2013, SIGNAL PROCESS-IMAGE, V28, P1072, DOI 10.1016/j.image.2013.04.001
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Fife WS, 2013, IEEE T CIRC SYST VID, V23, P60, DOI 10.1109/TCSVT.2012.2203197
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Hirschmüller H, 2007, PROC CVPR IEEE, P2134
   Hong L, 2004, PROC CVPR IEEE, P74
   Hosni A, 2013, IEEE T PATTERN ANAL, V35, P504, DOI 10.1109/TPAMI.2012.156
   Hosni A, 2009, IEEE IMAGE PROC, P2093, DOI 10.1109/ICIP.2009.5414478
   Izquierdo E, 1998, COMPUT VIS IMAGE UND, V71, P231, DOI 10.1006/cviu.1998.0706
   Izquierdo E, 2000, SIGNAL PROCESS-IMAGE, V15, P817, DOI 10.1016/S0923-5965(99)00014-4
   Izquierdo ME, 1998, SIGNAL PROCESS-IMAGE, V11, P231, DOI 10.1016/S0923-5965(97)00031-3
   Jung IL, 2013, IEEE T MULTIMEDIA, V15, P56, DOI 10.1109/TMM.2012.2225041
   Klaus A, 2006, INT C PATT RECOG, P15
   Kolmogorov V, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P508, DOI 10.1109/ICCV.2001.937668
   Kordelas GA, 2015, IMAGE VISION COMPUT, V35, P31, DOI 10.1016/j.imavis.2014.12.001
   Kordelas GA, 2014, IEEE IMAGE PROC, P3803, DOI 10.1109/ICIP.2014.7025772
   Kowalczuk J, 2013, IEEE T CIRC SYST VID, V23, P94, DOI 10.1109/TCSVT.2012.2203200
   Lee Z, 2013, IEEE T MULTIMEDIA, V15, P1855, DOI 10.1109/TMM.2013.2270456
   Liu H., 2013, IEEE Power and Energy Society General Meeting (PES), P1, DOI DOI 10.1109/PTC.2013.6652443
   Liu J, 2015, VISUAL COMPUT, V31, P1253, DOI 10.1007/s00371-014-1009-3
   Liu TL, 2009, LECT NOTES COMPUT SC, V5414, P449
   Ma ZY, 2013, IEEE I CONF COMP VIS, P49, DOI 10.1109/ICCV.2013.13
   Mattoccia S, 2007, LECT NOTES COMPUT SC, V4844, P517
   Mei X, 2011, PROC CVPR IEEE, P1257
   Mozerov MG, 2015, IEEE T IMAGE PROCESS, V24, P1153, DOI 10.1109/TIP.2015.2395820
   Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Tombari F., 2008, Computer Vision and Pattern Recognition, P1
   Tombari F, 2007, LECT NOTES COMPUT SC, V4872, P427
   Wang YC, 2013, IEEE T CIRC SYST VID, V23, P784, DOI 10.1109/TCSVT.2012.2223633
   Wang ZF, 2008, PROC CVPR IEEE, P887
   Xun Sun, 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P132, DOI 10.1109/3DIMPVT.2011.24
   Yang QQ, 2014, IMAGE VISION COMPUT, V32, P202, DOI 10.1016/j.imavis.2014.01.001
   Yang QX, 2012, LECT NOTES COMPUT SC, V7572, P399, DOI 10.1007/978-3-642-33718-5_29
   Yang QX, 2009, IEEE T PATTERN ANAL, V31, P492, DOI 10.1109/TPAMI.2008.99
   Yoon KJ, 2006, IEEE T PATTERN ANAL, V28, P650, DOI 10.1109/TPAMI.2006.70
   Zhang K, 2009, IEEE T CIRC SYST VID, V19, P1073, DOI 10.1109/TCSVT.2009.2020478
   Zhang Z, 2012, 2012 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV), P99, DOI 10.1109/IVS.2012.6232234
   Zhu XZ, 2013, CHIN CONTR CONF, P5785
NR 51
TC 26
Z9 28
U1 1
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2016
VL 18
IS 2
BP 155
EP 170
DI 10.1109/TMM.2015.2505905
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DB3HX
UT WOS:000368402400002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Phamduy, P
   DeBellis, M
   Porfiri, M
AF Phamduy, Paul
   DeBellis, Mauro
   Porfiri, Maurizio
TI Controlling a Robotic Fish Via a Natural User Interface for Informal
   Science Education
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Biologically-inspired robots; educational robotics; Kinect; natural user
   interfaces (NUIs)
ID KINECT; REHABILITATION; SMOOTHNESS; DESIGN; SYSTEM
AB Informal science education is a key contributing factor to scientific literacy, which determines our capacity as a society to technologically progress and make cognizant decisions on pressing issues of global scale. Both the fields of robotics and natural user interfaces have been separately proposed as effective means to aid informal science education, by increasing users' engagement through multiple interactive features. Here, we demonstrate the integration of these two fields of investigation toward a novel educational platform, revolving around the control of a robotic fish via a natural user interface. Users control the robotic fish through upper limb gestures that are captured by the Kinect. The robotic fish incorporates a temperature sensor, which collects data in a tank instrumented with heating and cooling sources. Participants observe the measurements they are recording in real-time to map the environment. Self-reported post-activity surveys and behavioral coding data on young users were collected to assess their level of engagement in the activity and their perception of the system. Our results indicate that the robotic fish is intuitive to drive with the natural user interface, the activity of collecting water temperature is interesting, and robotics may be a viable and accessible career option.
C1 [Phamduy, Paul; DeBellis, Mauro; Porfiri, Maurizio] NYU, Tandon Sch Engn, Dept Mech & Aerosp Engn, Brooklyn, NY 11201 USA.
   [DeBellis, Mauro] Univ Parma, Dipartimento Ingn Ind, I-43100 Parma, Italy.
C3 New York University; New York University Tandon School of Engineering;
   University of Parma
RP Phamduy, P (corresponding author), NYU, New York, NY 11201 USA.
EM pp1247@nyu.edu; mauro.debellis@studenti.unipr.it; mporfiri@nyu.edu
RI Porfiri, Maurizio/A-1712-2009
OI Porfiri, Maurizio/0000-0002-1480-3539
FU National Science Foundation [CMMI-0745753, DGE-0741714, DRL-1200911];
   Division Of Research On Learning; Direct For Education and Human
   Resources [1200911] Funding Source: National Science Foundation
FX This work was supported by the National Science Foundation under Grant
   CMMI-0745753, Grant DGE-0741714, and Grant DRL-1200911. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Yonggang Wen.
CR Abaid N, 2013, MECHATRONICS, V23, P491, DOI 10.1016/j.mechatronics.2013.03.010
   Abaid N, 2013, IEEE ROBOT AUTOM MAG, V20, P31, DOI 10.1109/MRA.2012.2184672
   Ainsworth H. L., 2010, FORMAL NONFORMAL INF
   Alisi TM, 2005, IEEE MULTIMEDIA, V12, P80, DOI 10.1109/MMUL.2005.52
   [Anonymous], BEH COD TOOL
   [Anonymous], P ASEE N CENTR SEC C
   [Anonymous], 2013, ISRN ARTIFICIAL INTE, DOI DOI 10.1155/2013/514641
   [Anonymous], ROB ED YOUTH OC RES
   Birk M., 2013, P SIGCHI C HUMAN FAC, P685, DOI DOI 10.1145/2470654.2470752
   Borun M., 1996, CURATOR, V39, P123, DOI DOI 10.1111/J.2151-6952.1996.TB01084.X
   Bruder G, 2009, 3DUI : IEEE SYMPOSIUM ON 3D USER INTERFACES 2009, PROCEEDINGS, P75, DOI 10.1109/3DUI.2009.4811208
   Chang YJ, 2011, RES DEV DISABIL, V32, P2566, DOI 10.1016/j.ridd.2011.07.002
   Cho S, 2014, IEEE COMPUT GRAPH, V34, P49
   Dierking LD, 2003, J RES SCI TEACH, V40, P108, DOI 10.1002/tea.10066
   Doriot N, 2006, ERGONOMICS, V49, P269, DOI 10.1080/00140130500489873
   Gibbons JD., 2020, NONPARAMETRIC STAT I, DOI DOI 10.1201/9781439896129
   HRELJAC A, 1993, BIOL CYBERN, V68, P375, DOI 10.1007/BF00201862
   Hsu H.M. J., 2011, INT J INFORM ED TECH, V1, P365, DOI [10.7763/IJIET.2011.V1.59, DOI 10.7763/IJIET.2011.V1.59]
   Innes T., 2012, GLOBAL J ENG ED, V14, P225
   Jagodzinski P, 2015, J SCI EDUC TECHNOL, V24, P16, DOI 10.1007/s10956-014-9517-5
   Johnson K, 2013, COMPUTER, V46, P101, DOI 10.1109/MC.2013.363
   Johnson-Glenberg M.C., 2011, Proceedings of the 7th International Conference on Games + Learning + Society Conference, GLS'11, P129
   Kanda T, 2004, HUM-COMPUT INTERACT, V19, P61, DOI 10.1207/s15327051hci1901&2_4
   Kopman V, 2013, IEEE-ASME T MECH, V18, P471, DOI 10.1109/TMECH.2012.2222431
   Korn R., 2003, SUMMATIVE EVALUATION
   Lange B, 2011, IEEE ENG MED BIO, P1831, DOI 10.1109/IEMBS.2011.6090521
   Laut J, 2015, IEEE T EDUC, V58, P48, DOI 10.1109/TE.2014.2324533
   Laut J, 2014, IEEE CONTR SYST MAG, V34, P60, DOI 10.1109/MCS.2013.2287386
   Liu JN, 2011, IEEE CONTR SYST MAG, V31, P105, DOI 10.1109/MCS.2011.941835
   Mead RA, 2012, ROBOTS IN K-12 EDUCATION: A NEW TECHNOLOGY FOR LEARNING, P302, DOI 10.4018/978-1-4666-0182-6.ch015
   Navidi W., 2008, Statistics for Engineers and Scientists, V2
   Nourbakhsh I, 2006, INT J ENG EDUC, V22, P777
   Patla AE, 1999, EXP BRAIN RES, V129, P629, DOI 10.1007/s002210050932
   Phamduy P, 2015, IEEE ROBOT AUTOM MAG, V22, P86, DOI 10.1109/MRA.2014.2381367
   Reale MJ, 2011, IEEE T MULTIMEDIA, V13, P474, DOI 10.1109/TMM.2011.2120600
   Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148
   Rohrer B, 2002, J NEUROSCI, V22, P8297
   Rouanet P, 2013, IEEE T ROBOT, V29, P525, DOI 10.1109/TRO.2012.2228134
   Sanna A, 2013, ENTERTAIN COMPUT, V4, P179, DOI 10.1016/j.entcom.2013.01.001
   Sato E, 2007, IEEE T IND ELECTRON, V54, P1105, DOI 10.1109/TIE.2007.892728
   Sell J, 2014, IEEE MICRO, V34, P44, DOI 10.1109/MM.2014.9
   Siegel S., 1956, Nonparametric statistics for the behavioral sciences
   Tan XB, 2007, PROC SPIE, V6524, DOI 10.1117/12.715724
   Uribe Alvaro, 2011, Proceedings of the Latin American IEEE Colombian Conf. Automat. Control Ind. Appl., Bogot, P1
   Villaroman N., 2011, Proceedings of the conference on Information technology education, P227
   Webster D, 2014, J NEUROENG REHABIL, V11, DOI 10.1186/1743-0003-11-108
   Yu JZ, 2011, IEEE ROBOT AUTOM MAG, V18, P47, DOI 10.1109/MRA.2011.942998
   Yu JZ, 2004, IEEE T SYST MAN CY B, V34, P1798, DOI 10.1109/TSMCB.2004.831151
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
NR 49
TC 8
Z9 11
U1 3
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2015
VL 17
IS 12
BP 2328
EP 2337
DI 10.1109/TMM.2015.2480226
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA CW9JR
UT WOS:000365315500018
DA 2024-07-18
ER

PT J
AU Wu, XM
   Yang, J
   Ran, YY
   Xi, HS
AF Wu, Xiaomin
   Yang, Jian
   Ran, Yongyi
   Xi, Hongsheng
TI Adaptive Scalable Video Transmission Strategy in Energy Harvesting
   Communication System
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Energy harvesting; energy-efficient wireless communication; Lyapunov
   optimization; scalable video coding (SVC)
ID PERFORMANCE ANALYSIS; RESOURCE-ALLOCATION; NETWORKS; OPTIMIZATION;
   TRANSMITTER; DEVICES
AB In this paper, we consider the adaptive transmission problem of scalable video in an energy harvesting communication system. The stochastic nature of the harvested energy puts a new challenge on the video transmission. Against this challenge, we formulate the adaptive scalable video transmission problem as maximizing the time average quality of the transmitted video subject to the energy constraint for reducing the playback interruption and the video quality smoothness constraint. In order to solve this problem, the Lyapunov optimization method is applied to derive an online dynamic layer transmission algorithm (DLTA). The simulation results show that the proposed DLTA can achieve better performance in terms of the received video quality and the convergence rate than a conventional reinforcement learning algorithm like the Q-learning method. It is also illustrated that the energy and smoothness constraints are beneficial for controlling the behavior of DLTA.
C1 [Wu, Xiaomin; Yang, Jian; Ran, Yongyi; Xi, Hongsheng] Univ Sci & Technol China, Sch Informat & Technol, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Yang, J (corresponding author), Univ Sci & Technol China, Sch Informat & Technol, Hefei 230027, Peoples R China.
EM xmwu@mail.ustc.edu.cn; jianyang@ustc.edu.cn; yyran@ustc.edu.cn;
   xihs@ustc.edu.cn
FU National High Technology Research and Development Program of China (863
   Program) [2015AA016201]; National NSF of China [61573329]; State Key
   Program of National NSF of China [61233003]; Fundamental Research Funds
   for the Central Universities
FX This work was supported by the National High Technology Research and
   Development Program of China (863 Program) under Grant 2015AA016201, by
   the National NSF of China under Grant 61573329, by the State Key Program
   of National NSF of China under Grant 61233003, and by the Fundamental
   Research Funds for the Central Universities. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Tommaso Melodia. (Corresponding author: Jian
   Yang.)
CR Blasco P, 2013, IEEE T WIREL COMMUN, V12, P1872, DOI 10.1109/TWC.2013.030413.121120
   Cicalò S, 2014, IEEE T MULTIMEDIA, V16, P848, DOI 10.1109/TMM.2014.2300442
   GARDNER ES, 1985, J FORECASTING, V4, P1, DOI 10.1002/for.3980040103
   Gorlatova M, 2013, IEEE T MOBILE COMPUT, V12, P1853, DOI 10.1109/TMC.2012.154
   Gupta R, 2012, IEEE T BROADCAST, V58, P428, DOI 10.1109/TBC.2012.2191702
   Ho CK, 2012, IEEE T SIGNAL PROCES, V60, P4808, DOI 10.1109/TSP.2012.2199984
   Huang C, 2013, IEEE J SEL AREA COMM, V31, P1469, DOI 10.1109/JSAC.2013.130811
   Huang LB, 2013, IEEE ACM T NETWORK, V21, P1117, DOI 10.1109/TNET.2012.2230336
   Kuan CC, 2014, IEEE T VEH TECHNOL, V63, P1813, DOI 10.1109/TVT.2013.2288941
   Li MD, 2013, IEEE T MULTIMEDIA, V15, P1519, DOI 10.1109/TMM.2013.2267207
   Mansour H, 2009, IEEE T MULTIMEDIA, V11, P1478, DOI 10.1109/TMM.2009.2032682
   Mastronarde N, 2013, IEEE T MOBILE COMPUT, V12, P694, DOI 10.1109/TMC.2012.36
   Neely, 2010, STOCHASTIC NETWORK O
   Niyato D, 2014, IEEE T WIREL COMMUN, V13, P4205, DOI 10.1109/TWC.2014.2314098
   Niyato D, 2014, IEEE T VEH TECHNOL, V63, P1870, DOI 10.1109/TVT.2013.2285922
   Ozel O, 2011, IEEE J SEL AREA COMM, V29, P1732, DOI 10.1109/JSAC.2011.110921
   Prasad RV, 2014, IEEE COMMUN SURV TUT, V16, P195, DOI 10.1109/SURV.2013.062613.00235
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Wang Z, 2014, IEEE T COMMUN, V62, P4080, DOI 10.1109/TCOMM.2014.2357430
   Yang J, 2012, J COMMUN NETW-S KOR, V14, P140, DOI 10.1109/JCN.2012.6253062
   Yang J, 2012, IEEE T WIREL COMMUN, V11, P571, DOI 10.1109/TWC.2011.120911.101813
   Yang J, 2012, IEEE T COMMUN, V60, P220, DOI 10.1109/TCOMM.2011.112811.100349
NR 22
TC 13
Z9 13
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2015
VL 17
IS 12
BP 2345
EP 2353
DI 10.1109/TMM.2015.2476662
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CW9JR
UT WOS:000365315500020
DA 2024-07-18
ER

PT J
AU Wang, Y
   Li, S
   Kot, AC
AF Wang, Yan
   Li, Sheng
   Kot, Alex C.
TI DeepBag: Recognizing Handbag Models
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural networks; feature selection; handbag recognition;
   soft label
ID FEATURES; RETRIEVAL; GRADIENTS; SCALE
AB In this paper, we address the problem of branded handbag recognition. It is a challenging problem due to the non-rigid deformation, illumination changes, and inter-class similarity. We propose a novel framework based on deep convolutional neural network (CNN). Concretely, we propose a new CNN model, called feature selective joint classification-regression CNN (FSCR-CNN). Its advantages lie in two folds: 1) it alleviates the illumination changes by a feature selection strategy to focus on the color-nondiscriminative features in the network learning, and 2) rather than only targeting on the hard label (i.e., the handbag model), it also incorporates a soft label (i.e., a distribution measuring the similarity between the ground truth model and all the models to be trained) to construct the loss function for training CNN, which leads to a better classifier for handbags with large inter-class similarity. We evaluate the performance of our framework on a newly built branded handbag dataset. The results show that it performs favorably for recognizing handbags with 94.48% in accuracy. We also apply the proposed FSCR-CNN model in recognizing other fine-grained objects with state-of-the-art CNN architectures, which is able to achieve over 5% improvement in accuracy.
C1 [Wang, Yan; Li, Sheng] Nanyang Technol Univ, Sch Elect & Elect Engn, Rapid Rich Object Search ROSE Lab, Singapore 637553, Singapore.
   [Kot, Alex C.] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University; Nanyang Technological University
RP Wang, Y (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Rapid Rich Object Search ROSE Lab, Singapore 637553, Singapore.
EM wang0696@e.ntu.edu.sg; lisheng@ntu.edu.sg; eackot@ntu.edu.sg
OI Li, Sheng/0000-0002-7932-9831
FU National Research Foundation, Singapore, under its Interactive Digital
   Media (IDM) Strategic Research Programme
FX This research was carried out at the Rapid-Rich Object Search (ROSE)
   Laboratory, Nanyang Technological University, Singapore. The ROSE
   Laboratory is supported by the National Research Foundation, Singapore,
   under its Interactive Digital Media (IDM) Strategic Research Programme.
   The authors gratefully acknowledge the support of the NVIDIA Corporation
   for their donation of a Tesla K40 GPU used for research at the ROSE
   Laboratory.
CR Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   Angelova A, 2013, PROC CVPR IEEE, P811, DOI 10.1109/CVPR.2013.110
   [Anonymous], 2014, P BRIT MACH VIS C, DOI 10.5244/C.28.87
   [Anonymous], 2014, P INT C LEARN REPR A
   [Anonymous], 2012, P ACM INT C MULT
   [Anonymous], 2014, CORR
   [Anonymous], CORR
   [Anonymous], 2013, CORR
   [Anonymous], 2014, CORR
   Azizpour H., P IEEE C CO IN PRESS
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Chai Y, 2013, IEEE I CONF COMP VIS, P321, DOI 10.1109/ICCV.2013.47
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen T, 2014, IEEE T MULTIMEDIA, V16, P612, DOI 10.1109/TMM.2014.2301978
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Gavves E, 2013, IEEE I CONF COMP VIS, P1713, DOI 10.1109/ICCV.2013.215
   Geng X, 2013, INT CONF DAT MIN WOR, P377, DOI 10.1109/ICDMW.2013.19
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Godbole Shantanu., 2002, Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, P513
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Ji RR, 2013, IEEE T MULTIMEDIA, V15, P153, DOI 10.1109/TMM.2012.2225035
   Jia D, 2013, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2013.81
   Khosla A., 2011, CVPR
   Krause J, 2014, INT C PATT RECOG, P26, DOI 10.1109/ICPR.2014.15
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuo YH, 2012, IEEE T MULTIMEDIA, V14, P1079, DOI 10.1109/TMM.2012.2190386
   Lee CY, 2015, INT CONF ADV ROBOT
   Li ZC, 2015, IEEE T PATTERN ANAL, V37, P2085, DOI 10.1109/TPAMI.2015.2400461
   Li ZC, 2014, IEEE T KNOWL DATA EN, V26, P2138, DOI 10.1109/TKDE.2013.65
   Liu Luoqi, 2014, ACM T MULTIM COMPUT, V11, P1, DOI DOI 10.1145/2659234
   Liu S, 2014, IEEE T MULTIMEDIA, V16, P253, DOI 10.1109/TMM.2013.2285526
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mao QR, 2014, IEEE T MULTIMEDIA, V16, P2203, DOI 10.1109/TMM.2014.2360798
   Miller G.A., 1990, Int. J. Lexicogr, V3, P235, DOI [DOI 10.1093/IJL/3.4.235, 10.1093/ijl/3.4.235]
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pu J, 2014, LECT NOTES COMPUT SC, V8691, P425, DOI 10.1007/978-3-319-10578-9_28
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Safadi Bahjat, 2014, ICMR 2014 P ACM INT, P265
   Sfar AR, 2015, INT J COMPUT VISION, V111, P255, DOI 10.1007/s11263-014-0743-3
   Shen W., P IEEE C CO IN PRESS
   Shen W, 2014, IEEE T CYBERNETICS, V44, P1053, DOI 10.1109/TCYB.2013.2279071
   Sironi A, 2014, PROC CVPR IEEE, P2697, DOI 10.1109/CVPR.2014.351
   Szegedy C., P IEEE C CO IN PRESS
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Uijlings JRR, 2010, IEEE T MULTIMEDIA, V12, P665, DOI 10.1109/TMM.2010.2052027
   van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809
   Wah Catherine, 2011, Technical report
   Wang Y, 2014, IEEE IMAGE PROC, P5896, DOI 10.1109/ICIP.2014.7026191
   Wang Y, 2014, 2014 6TH INTERNATIONAL SYMPOSIUM ON COMMUNICATIONS, CONTROL AND SIGNAL PROCESSING (ISCCSP), P61, DOI 10.1109/ISCCSP.2014.6877816
   Xiao T., P IEEE C CO IN PRESS
   Yang Shulin., 2012, NIPS, P3131
   Yao BP, 2011, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR.2011.5995368
   Yin WY, 2014, IEEE T MULTIMEDIA, V16, P184, DOI 10.1109/TMM.2013.2283468
   You QZ, 2015, IEEE INT C SEMANT CO, P173, DOI 10.1109/ICOSC.2015.7050803
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
   Zuo Z, 2015, PATTERN RECOGN, V48, P3004, DOI 10.1016/j.patcog.2015.02.003
   Zuo Z, 2014, LECT NOTES COMPUT SC, V8689, P552, DOI 10.1007/978-3-319-10590-1_36
NR 63
TC 7
Z9 7
U1 2
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2015
VL 17
IS 11
SI SI
BP 2072
EP 2083
DI 10.1109/TMM.2015.2480228
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CV2QV
UT WOS:000364102400019
DA 2024-07-18
ER

PT J
AU Yan, M
   Sang, JT
   Xu, CS
   Hossain, MS
AF Yan, Ming
   Sang, Jitao
   Xu, Changsheng
   Hossain, M. Shamim
TI YouTube Video Promotion by Cross-Network Association: @<i>Britney</i> to
   Advertise <i>Gangnam Style</i>
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-network analysis; social media; video promotion
AB The emergence and rapid proliferation of various social media networks have reshaped the way how video contents are generated, distributed, and consumed in traditional video sharing portals. Nowadays, online videos can be accessed from far beyond the internal mechanisms of the video sharing portals, such as internal search and front page highlight. Recent studies have found that external referrers, such as external search engines and other social media websites, arise to be the new and important portals to lead users to online videos. In this paper, we introduce a novel cross-network collaborative application to help drive the online traffic for given videos in the traditional video portal YouTube by leveraging the high propagation efficiency of the popular Twitter followees. Since YouTube videos and Twitter followees distribute on heterogeneous spaces, we present a cross-network association-based solution framework. In this framework, we first represent YouTube videos and Twitter followees in the corresponding topic spaces separately by employing generative topic models. Then, the cross-network topic spaces are associated from both semantic-based and network-based perspectives through the collective intelligence of the observed overlapped users. Based on the derived cross-network association, we finally match the query YouTube videos and candidate Twitter followees in the same topic space with a unified ranking method. The experiments on a real-world large-scale dataset of more than 2.2 million YouTube videos and 31.8 million tweets from 38,540 YouTube users and 39,400 Twitter users demonstrate the effectiveness and superiority of our solution in which network-based and semantic-based association are integrated.
C1 [Yan, Ming; Sang, Jitao; Xu, Changsheng] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
   [Hossain, M. Shamim] King Saud Univ, Coll Comp & Informat Sci, SWE Dept, Riyadh 11543, Saudi Arabia.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; King Saud
   University
RP Yan, M (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
EM ming.yan@nlpr.ia.ac.cn; jtsang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn;
   mshossain@ksu.edu.sa
RI Hossain, M. Shamim/K-1362-2014; xu, cj/HJZ-3488-2023; Guizani,
   Mohsen/AAX-4534-2021
OI Hossain, M. Shamim/0000-0001-5906-9422; Guizani,
   Mohsen/0000-0002-8972-8094
FU National Basic Research Program of China [2012CB316304]; National
   Natural Science Foundation of China [61225009, 61373122, 61303176,
   61402479, 61328205, 61432019]; Beijing Natural Science Foundation
   [4131004]; Singapore National Research Foundation under its
   International Research Centre@Singapore Funding Initiative; Deanship of
   Scientific Research, King Saud University, through the International
   Research Group [IRG 14-18]
FX This work was supported in part by the National Basic Research Program
   of China under Grant 2012CB316304, in part by the National Natural
   Science Foundation of China under Grant 61225009, Grant 61373122, Grant
   61303176, Grant 61402479, Grant 61328205, and Grant 61432019, in part by
   the Beijing Natural Science Foundation under Grant 4131004, by the
   Singapore National Research Foundation under its International Research
   Centre@Singapore Funding Initiative and administered by the IDM
   Programme Office, and by the Deanship of Scientific Research, King Saud
   University, through the International Research Group under Program IRG
   14-18. The associate editor coordinating the review of this manuscript
   and approving it for publication was Prof. K. Selcuk Candan.
CR Abel F, 2011, LECT NOTES COMPUT SC, V6757, P28, DOI 10.1007/978-3-642-22233-7_3
   [Anonymous], 2012, ACM MM'12'
   [Anonymous], INT C MULT
   Bhagat S, 2012, P 5 ACM INT C WEB SE, P603, DOI DOI 10.1145/2124295.2124368
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Cha  M., 2010, ICWSM, P10
   Cha M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P1
   Chen W., 2010, ACM SIGKDD INT C KNO, P1029, DOI DOI 10.1145/1835804.1835934
   Cheng X, 2008, INT WORKSH QUAL SERV, P249
   Cheng X, 2013, IEEE INFOCOM SER, P45
   Deng Zhengyu., 2013, IEEE INT C MULTIMEDI, P1
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Figueiredo F., 2014, CORR
   Ghosh S, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P575, DOI 10.1145/2348283.2348361
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830
   Järvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Joachims T, 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]
   Kempe D, 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Klare BF, 2011, IEEE T PATTERN ANAL, V33, P639, DOI 10.1109/TPAMI.2010.180
   Kwak H., WWW'10, DOI DOI 10.1145/1772690.1772751
   Lei Z, 2014, IEEE T PATTERN ANAL, V36, P289, DOI 10.1109/TPAMI.2013.112
   Li HT, 2013, IEEE INFOCOM SER, P50
   Liu Jing, P 6 ACM INT C WEB SE, P495
   Ng, 2007, ADV NEURAL INF PROCE, P801
   Osborne M., 2012, P TAIA, V12, P5
   Rao D., 2010, P 2 INT WORKSHOP SEA, P37, DOI DOI 10.1145/1871985.1871993
   Roy SD, 2014, 2014 IEEE 15TH INTERNATIONAL CONFERENCE ON INFORMATION REUSE AND INTEGRATION (IRI), P811, DOI 10.1109/IRI.2014.7051972
   Roy SD, 2013, IEEE T MULTIMEDIA, V15, P1255, DOI 10.1109/TMM.2013.2265079
   Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739
   Weng J., 2010, P 3 ACM INT C WEB SE, P261, DOI [10.1145/1718487.1718520, DOI 10.1145/1718487.1718520]
   Xu C., 2007, CORR
   Yan M, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P557, DOI 10.1145/2647868.2654920
   Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127
   Yang M, 2010, IEEE IMAGE PROC, P1601, DOI 10.1109/ICIP.2010.5652363
   Zhiheng Xu, 2011, 2011 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies, P422, DOI 10.1109/WI-IAT.2011.47
NR 37
TC 11
Z9 11
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1248
EP 1261
DI 10.1109/TMM.2015.2446949
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000011
DA 2024-07-18
ER

PT J
AU Chang, ZY
   Chan, SHG
AF Chang, Zhangyu
   Chan, S. -H. Gary
TI Bucket-Filling: An Asymptotically Optimal Video-on-Demand Network With
   Source Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distributed video-on-demand (VoD) cloud; linear programming (LP);
   optimization; source coding
ID STORAGE; VOD; QUALITY; REPLICATION
AB There has recently been growing interest for content providers to provide video-on-demand (VoD) as a cloud service. In such a network, the content provider may rent heterogeneous resources (such as streaming and storage capacities) from geographically distributed data centers deployed close to user pools. These data centers (or proxy servers) collaboratively share content with each other to serve their local users. A critical challenge is to optimize movie storage and retrieval to minimize the deployment cost consisting of streaming, storage, and network transmission between data centers. We propose a novel and effective movie storage and retrieval using linear source coding. All the movies are source-encoded once at the repository, by taking every source symbols of movie to generate coded symbols. These coded symbols are then distributed to the servers in the cloud. Based on a general and comprehensive cost model, we optimize and the number of symbols to retrieve from remote servers for a local movie request. The optimal solution can be efficiently computed with a linear programming (LP) formulation. Our solution is proved to asymptotically approach the global minimum cost as increases. Even when is low, near optimality can be achieved. To accommodate large movie pool and system parameter changes, we propose algorithms for movie grouping and on-line re-optimization which significantly reduce the computational complexity with little compromise on optimality. Through extensive simulation, our algorithm is shown to achieve the lowest cost, outperforming traditional and state-of-the-art heuristics with a substantially wide margin.
C1 [Chang, Zhangyu; Chan, S. -H. Gary] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Chang, ZY (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
EM zchang@cse.ust.hk; gchan@cse.ust.hk
OI Chan, Gary Shueng Han/0000-0003-4207-764X
FU Hong Kong Research Grant Council (RGC) [610713]; HKUST [FSGRF13EG15];
   Hong Kong Innovation and Technology Fund [UIM/246]
FX Manuscript received January 08, 2014; revised May 18, 2014, October 15,
   2014, and February 19, 2015; accepted March 09, 2015. Date of
   publication March 25, 2015; date of current version April 15, 2015. This
   work was supported in part by the Hong Kong Research Grant Council (RGC)
   General Research Fund under Grant 610713, by HKUST under Grant
   FSGRF13EG15, and the Hong Kong Innovation and Technology Fund under
   Grant UIM/246. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Tommaso Melodia.
CR Adhikari VK, 2012, IEEE INFOCOM SER, P1620, DOI 10.1109/INFCOM.2012.6195531
   Alasaad A, 2012, IEEE GLOBE WORK, P753, DOI 10.1109/GLOCOMW.2012.6477669
   [Anonymous], IEEE INT C COMM ICC
   [Anonymous], MULT SIGN PROC 2005
   [Anonymous], P INT C MULT EXP SAN
   [Anonymous], P GLOB DEC
   [Anonymous], 1997, THESIS ERASMUS U ROT
   [Anonymous], P IEEE INT C INT MUL
   Applegate David, 2013, Integer Programming and Combinatorial Optimization. 16th International Conference, IPCO 2013. Proceedings, P49, DOI 10.1007/978-3-642-36694-9_5
   Borst S, 2010, IEEE INFOCOM SER, DOI 10.1109/infcom.2010.5461964
   Chan SHG, 2013, IEEE T MULTIMEDIA, V15, P2125, DOI 10.1109/TMM.2013.2280989
   Chan SHG, 2001, IEEE ACM T NETWORK, V9, P125, DOI 10.1109/90.917070
   Choe Y.R., 2007, MULTIMEDIA 07 P 15 I, P117
   Chu YM, 2014, IEEE SYST J, V8, P292, DOI 10.1109/JSYST.2013.2257338
   Dai J, 2012, IEEE INFOCOM SER, P2444, DOI 10.1109/INFCOM.2012.6195634
   Gkantsidis Christos., 2006, Proceedings of the 6th ACM SIGCOMM Conference on Internet Measurement (IMC), P177
   Grant M., 2020, CVX MATLAB SOFTWARE
   He J, 2013, IEEE T CIRC SYST VID, V23, P1717, DOI 10.1109/TCSVT.2013.2255423
   He YF, 2009, IEEE T MULTIMEDIA, V11, P509, DOI 10.1109/TMM.2009.2012921
   He YF, 2009, IEEE T MULTIMEDIA, V11, P138, DOI 10.1109/TMM.2008.2008929
   Hefeeda M, 2010, IEEE T PARALL DISTR, V21, P998, DOI 10.1109/TPDS.2009.130
   Huang C, 2007, ACM SIGCOMM COMP COM, V37, P133, DOI 10.1145/1282427.1282396
   James M., 1967, PROC BERKELEY S MATH, V1, P281, DOI DOI 10.1007/S11665-016-2173-6
   Kangasharju J, 2007, IEEE INFOCOM SER, P1973, DOI 10.1109/INFCOM.2007.229
   Kao YC, 2012, IEEE T PARALL DISTR, V23, P985, DOI 10.1109/TPDS.2011.244
   Liu FM, 2011, IEEE INFOCOM SER, P936, DOI 10.1109/INFCOM.2011.5935320
   Liu Zhong-Hua, 2011, International Journal of Neuropsychopharmacology, V14, P618, DOI 10.1017/S1461145710000520
   Mehyar M, 2007, IEEE ACM T NETWORK, V15, P512, DOI 10.1109/TNET.2007.893226
   Niu D, 2012, IEEE INFOCOM SER, P460, DOI 10.1109/INFCOM.2012.6195785
   Oh HR, 2011, COMPUT NETW, V55, P2746, DOI 10.1016/j.comnet.2011.05.001
   Tan B, 2013, IEEE ACM T NETWORK, V21, P566, DOI 10.1109/TNET.2012.2208199
   Wang H, 2011, PROTECTING PRIVACY IN CHINA: A RESEARCH ON CHINAS PRIVACY STANDARDS AND THE POSSIBILITY OF ESTABLISHING THE RIGHT TO PRIVACY AND THE INFORMATION PRIVACY PROTECTION LEGISLATION IN MODERN CHINA, P1, DOI 10.1007/978-3-642-21750-0_1
   Wang M, 2007, IEEE J SEL AREA COMM, V25, P1655, DOI 10.1109/JSAC.2007.071205
   Wu WJ, 2011, IEEE INFOCOM SER, P1206, DOI 10.1109/INFCOM.2011.5934900
   Yiu WPK, 2007, IEEE J SEL AREA COMM, V25, P1717, DOI 10.1109/JSAC.2007.071210
   Zaman S, 2011, IEEE T PARALL DISTR, V22, P1455, DOI 10.1109/TPDS.2011.27
   Zhou YP, 2012, IEEE INFOCOM SER, P1530, DOI 10.1109/INFCOM.2012.6195520
   Zhou Y, 2013, J OPTIMIZ THEORY APP, V156, P1, DOI 10.1007/s10957-013-0271-2
NR 38
TC 2
Z9 2
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2015
VL 17
IS 5
BP 723
EP 735
DI 10.1109/TMM.2015.2416636
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CG3AH
UT WOS:000353148300013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Gu, K
   Zhai, GT
   Yang, XK
   Zhang, WJ
AF Gu, Ke
   Zhai, Guangtao
   Yang, Xiaokang
   Zhang, Wenjun
TI Using Free Energy Principle For Blind Image Quality Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Free energy; human visual system; image quality assessment (IQA);
   no-reference (NR); structural degradation
ID NATURAL SCENE STATISTICS; STRUCTURAL SIMILARITY; PREDICTION; BRAIN
AB In this paper we propose a new no-reference (NR) image quality assessment (IQA) metric using the recently revealed free-energy-based brain theory and classical human visual system (HVS)-inspired features. The features used can be divided into three groups. The first involves the features inspired by the free energy principle and the structural degradation model. Furthermore, the free energy theory also reveals that the HVS always tries to infer the meaningful part from the visual stimuli. In terms of this finding, we first predict an image that the HVS perceives from a distorted image based on the free energy theory, then the second group of features is composed of some HVS-inspired features (such as structural information and gradient magnitude) computed using the distorted and predicted images. The third group of features quantifies the possible losses of "naturalness" in the distorted image by fitting the generalized Gaussian distribution to mean subtracted contrast normalized coefficients. After feature extraction, our algorithm utilizes the support vector machine based regression module to derive the overall quality score. Experiments on LIVE, TID2008, CSIQ, IVC, and Toyama databases confirm the effectiveness of our introduced NR IQA metric compared to the state-of-the-art.
C1 [Gu, Ke; Zhai, Guangtao; Yang, Xiaokang; Zhang, Wenjun] Shanghai Jiao Tong Univ, Shanghai Key Lab Digital Media Proc & Transmiss, Inst Image Commun & Informat Proc, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Zhai, GT (corresponding author), Shanghai Jiao Tong Univ, Shanghai Key Lab Digital Media Proc & Transmiss, Inst Image Commun & Informat Proc, Shanghai 200240, Peoples R China.
EM guke.doctor@gmail.com; zhaiguangtao@sjtu.edu.cn; xkyang@sjtu.edu.cn;
   zhangwenjun@sjtu.edu.cn
RI Gu, Ke/AAJ-9684-2021; Yang, Xiaokang/C-6137-2009; Zhang,
   Wenjun/GNH-2095-2022; Zhai, Guangtao/X-5949-2019
OI Yang, Xiaokang/0000-0003-4029-3322; Zhang, Wenjun/0000-0002-5282-3725;
   Zhai, Guangtao/0000-0001-8165-9322
FU National Science Foundation of China [61025005, 61371146, 61221001,
   61390514]; Foundation for the Author of National Excellent Doctoral
   Dissertation of PR China [201339]
FX This work was supported in part by the National Science Foundation of
   China under Grant 61025005, Grant 61371146, Grant 61221001, and Grant
   61390514, and by the Foundation for the Author of National Excellent
   Doctoral Dissertation of PR China under Grant 201339. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Klara Nahrstedt. (Corresponding Author: G. Zhai).
CR Attias H, 2000, ADV NEUR IN, V12, P209
   Barlow H., 1961, COGNITIVE PSYCHOL
   Bovik AC, 2013, P IEEE, V101, P2008, DOI 10.1109/JPROC.2013.2257632
   Carandini M, 1997, J NEUROSCI, V17, P8621
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760
   Friston KJ, 2006, J PHYSIOL-PARIS, V100, P70, DOI 10.1016/j.jphysparis.2006.10.001
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   Gao XB, 2013, IEEE T NEUR NET LEAR, V24, P2013, DOI 10.1109/TNNLS.2013.2271356
   Gu K., 2012, J ELECT COMPUT ENG, V2012, P1
   Gu K, 2013, IEEE IMAGE PROC, P383, DOI 10.1109/ICIP.2013.6738079
   Gu K, 2014, IEEE INT SYMP CIRC S, P518, DOI 10.1109/ISCAS.2014.6865186
   Gu K, 2014, IEEE T BROADCAST, V60, P555, DOI 10.1109/TBC.2014.2344471
   Gu K, 2013, IEEE INT CON MULTI
   Gu K, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING (IEEE VCIP 2013)
   Gu K, 2013, IEEE INT SYMP CIRC S, P2365, DOI 10.1109/ISCAS.2013.6572353
   Gu K, 2013, IEEE INT SYMP CIRC S, P1095, DOI 10.1109/ISCAS.2013.6572041
   Gu K, 2013, SIGNAL IMAGE VIDEO P, V7, P423, DOI 10.1007/s11760-013-0445-2
   Horita Y., 2008, MICT Image Quality Evaluation Database
   Johne B., 1999, HDB COMPUTER VISION
   Knill DC, 2004, TRENDS NEUROSCI, V27, P712, DOI 10.1016/j.tins.2004.10.007
   Kovesi P., 1999, Videre, V1
   Larson E C, 2009, Categorical Image Quality (CSIQ) database [EB/OL]
   LINSKER R, 1990, ANNU REV NEUROSCI, V13, P257, DOI 10.1146/annurev.ne.13.030190.001353
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Ma L, 2011, IEEE T MULTIMEDIA, V13, P824, DOI 10.1109/TMM.2011.2109701
   Mackay D., 1995, P ADV NEUR INF PROC
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Marziliano P, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P57, DOI 10.1109/ICIP.2002.1038902
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2009, IEEE J-STSP, V3, P193, DOI 10.1109/JSTSP.2009.2015374
   MORRONE MC, 1986, NATURE, V324, P250, DOI 10.1038/324250a0
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Ninassi A., 2008, SUBJECTIVE QUALITY A
   Ponomarenko N., 2009, ADV MODERN RADIOELEC, V10, P30, DOI 10.1109/TIP.2015.2439035
   Rehman A, 2012, IEEE T IMAGE PROCESS, V21, P3378, DOI 10.1109/TIP.2012.2197011
   RISSANEN J, 1981, IEEE T INFORM THEORY, V27, P12, DOI 10.1109/TIT.1981.1056282
   Roberts SJ, 2002, IEEE T SIGNAL PROCES, V50, P2245, DOI 10.1109/TSP.2002.801921
   RUDERMAN DL, 1994, NETWORK-COMP NEURAL, V5, P517, DOI 10.1088/0954-898X/5/4/006
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Schölkopf B, 2000, NEURAL COMPUT, V12, P1207, DOI 10.1162/089976600300015565
   SHARIFI K, 1995, IEEE T CIRC SYST VID, V5, P52, DOI 10.1109/76.350779
   Sheikh H.R., 2006, LIVE image quality assessment database release 2
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P1918, DOI 10.1109/TIP.2005.854492
   Sheskin David, 2011, Handbook of Parametric and Nonparametric Statistical Procedures
   Video Quality Experts Group, 2000, Final report from the video quality experts group on the validation of objective models of video quality assessment march 2000
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE IMAGE PROC, P477
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Wu JJ, 2013, IEEE T IMAGE PROCESS, V22, P43, DOI 10.1109/TIP.2012.2214048
   Wu XL, 2011, IEEE T IMAGE PROCESS, V20, P36, DOI 10.1109/TIP.2010.2061860
   Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133
   Zhai GT, 2008, IEEE T BROADCAST, V54, P719, DOI 10.1109/TBC.2008.2001720
   Zhai GT, 2011, IEEE IMAGE PROC, P1857, DOI 10.1109/ICIP.2011.6115828
   Zhai GT, 2012, IEEE T IMAGE PROCESS, V21, P41, DOI 10.1109/TIP.2011.2161092
   Zhai GT, 2008, IEEE T MULTIMEDIA, V10, P1316, DOI 10.1109/TMM.2008.2004910
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zoran D, 2009, IEEE I CONF COMP VIS, P2209, DOI 10.1109/ICCV.2009.5459476
NR 62
TC 483
Z9 509
U1 6
U2 126
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2015
VL 17
IS 1
BP 50
EP 63
DI 10.1109/TMM.2014.2373812
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AX6QW
UT WOS:000347047400006
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Li, CX
   Zhang, BX
   Chen, CJ
   Chiu, DM
AF Li, Chunxi
   Zhang, Baoxian
   Chen, Changjia
   Chiu, Dah Ming
TI Relevant Window-Based Bitmap Compression in P2P Systems: Framework and
   Solution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Buffer-map; compression; P2P; relevant window
ID CAPACITY
AB P2P systems require neighbor peers to frequently exchange buffer-map (BM) messages for efficient content sharing and distribution, which, however, can result in considerable communication overhead. A big problem in the BMs exchanged between neighbor peers is that a lot of information in them is redundant. To reduce the redundancy, some P2P systems have adopted certain block-level compression schemes (e. g., Huffman encoding) to compress each BM in isolation. However, these schemes simply treat each BM separately and as a single block of data, which largely affects their compression efficiency. In this paper, we propose a novel relevant-window-based (RW) compression framework, which takes advantage of the correlation between sequentially exchanged BMs between neighbor peers and thus can greatly remove the redundancy in them. We accordingly design a RW-based distributed compression scheme, which can work alone or co-work well with an existing block-level compression scheme for higher compression efficiency. We prove the correctness of our scheme and derive tight upper bound on average length of compressed bitmaps by our scheme via mathematical modeling. Numerical results demonstrate that our scheme alone can achieve compression efficiency of 96.6%, which can be further increased to up to 97.1% when jointly working with a block-level compression scheme.
C1 [Li, Chunxi; Chen, Changjia] Beijing Jiaotong Univ, Sch Elect & Informat Engn, Beijing 100044, Peoples R China.
   [Zhang, Baoxian] Univ Chinese Acad Sci, Res Ctr Ubiquitous Sensor Networks, Beijing 100049, Peoples R China.
   [Chiu, Dah Ming] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Beijing Jiaotong University; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Chinese University of Hong Kong
RP Li, CX (corresponding author), Beijing Jiaotong Univ, Sch Elect & Informat Engn, Beijing 100044, Peoples R China.
EM chxli1@bjtu.edu.cn; bxzhang@ucas.ac.cn; changjiachen@sina.com;
   dmchiu@ie.cuhk.edu.hk
RI Chiu, Dah Ming/F-1885-2011
FU National Science Foundation of China [61271199, 61173158, 61301082];
   China Scholarship Fund
FX This work was supported in part by the National Science Foundation of
   China under Grant 61271199, Grant 61173158, and Grant 61301082, and by
   the China Scholarship Fund. The associate editor coordinating the review
   of this paper and approving it for publication was Prof. Pal. Halvorsen.
CR Afify Heba, 2011, International Journal of Computer Science & Information Technology, V3, P145, DOI 10.5121/ijcsit.2011.3412
   [Anonymous], 2006, Elements of Information Theory
   [Anonymous], 2010, PROC 19 INT C COMPUT
   [Anonymous], INTERNET STUDY 2008
   Baccelli F, 2013, IEEE INFOCOM SER, P1753
   Barbosa DMJ, 2010, ACM SIGPLAN NOTICES, V45, P193, DOI 10.1145/1932681.1863572
   Cisco Systems Inc, 2014, ZETT ER TRENDS AN
   Feng C, 2009, IEEE INFOCOM SER, P891, DOI 10.1109/INFCOM.2009.5061999
   Giese H, 2009, SOFTW SYST MODEL, V8, P21, DOI 10.1007/s10270-008-0089-9
   GOLOMB SW, 1966, IEEE T INFORM THEORY, V12, P399, DOI 10.1109/TIT.1966.1053907
   Hegde N., 2010, P ICC 10 CAP TOWN S, P1
   HUFFMAN DA, 1952, P IRE, V40, P1098, DOI 10.1109/JRPROC.1952.273898
   LANGDON GG, 1984, IBM J RES DEV, V28, P135, DOI 10.1147/rd.282.0135
   Li C., 2013, RELEVANT WINDOW BASE
   Liu Y., 2007, MULTIMEDIA '07: Proceedings of the 15th international conference on Multimedia, P127
   Liu Y, 2008, PEER PEER NETW APPL, V1, P18, DOI 10.1007/s12083-007-0006-y
   Magharei N, 2014, IEEE ACM T NETWORK, V22, P244, DOI 10.1109/TNET.2013.2257840
   Marciniak P., 2008, P IPTPS 08 TAMP FL U, P9
   Meng Zhang, 2005, 13th Annual ACM International Conference on Multimedia, P287, DOI 10.1145/1101149.1101206
   Mokhtarian K, 2013, IEEE T MULTIMEDIA, V15, P181, DOI 10.1109/TMM.2012.2225042
   Pan W. D., 2010, P IIPS 10 ATL GA US, P21
   RISSANEN J, 1979, IBM J RES DEV, V23, P149, DOI 10.1147/rd.232.0149
   Sanghavi S, 2007, IEEE T INFORM THEORY, V53, P4640, DOI 10.1109/TIT.2007.909171
   Seibert J, 2012, IEEE ACM T NETWORK, V20, P1910, DOI 10.1109/TNET.2012.2190093
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   Wang HY, 2012, IEEE T PARALL DISTR, V23, P1216, DOI 10.1109/TPDS.2011.253
   Wang M, 2011, ACM SIGPLAN NOTICES, V46, P392, DOI 10.1145/2034574.2034825
   Wu C, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1386109.1386112
   Wu WJ, 2013, COMPUT NETW, V57, P1674, DOI 10.1016/j.comnet.2013.02.016
   Zhang M, 2007, IEEE J SEL AREA COMM, V25, P1678, DOI 10.1109/JSAC.2007.071207
   Zhihui Lu, 2012, Journal of Communications, V7, P232, DOI 10.4304/jcm.7.3.232-245
   Zhou YP, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, P226, DOI 10.1109/ICNP.2007.4375853
   ZIV J, 1977, IEEE T INFORM THEORY, V23, P337, DOI 10.1109/TIT.1977.1055714
   ZIV J, 1978, IEEE T INFORM THEORY, V24, P530, DOI 10.1109/TIT.1978.1055934
NR 34
TC 1
Z9 1
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 1821
EP 1833
DI 10.1109/TMM.2014.2340795
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300003
DA 2024-07-18
ER

PT J
AU Tang, JH
   Tay, WP
   Wen, YG
AF Tang, Jianhua
   Tay, Wee Peng
   Wen, Yonggang
TI Dynamic Request Redirection and Elastic Service Scaling in Cloud-Centric
   Media Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cloud-centric content network; cost-aware provisioning; quickest
   detection; resource allocation; service capacity scaling; user request
   redirection
AB We consider the problem of optimally redirecting user requests in a cloud-centric media network (CCMN) to multiple destination Virtual Machines (VMs), which elastically scale their service capacities in order to minimize a cost function that includes service response times, computing costs, and routing costs. We also allow the request arrival process to switch between normal and flash crowd modes to model user requests to a CCMN. We quantify the trade-offs in flash crowd detection delay and false alarm frequency, request allocation rates, and service capacities at the VMs. We show that under each request arrival mode (normal or flash crowd), the optimal redirection policy can be found in terms of a price for each VM, which is a function of the VM's service cost, with requests redirected to VMs in order of nondecreasing prices, and no redirection to VMs with prices above a threshold price. Applying our proposed strategy to a YouTube request trace data set shows that our strategy outperforms various benchmark strategies. We also present simulation results when various arrival traffic characteristics are varied, which again suggest that our proposed strategy performs well under these conditions.
C1 [Tang, Jianhua; Tay, Wee Peng] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Wen, Yonggang] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University; Nanyang Technological University
RP Tang, JH (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM jtang4@ntu.edu.sg; wptay@ntu.edu.sg; ygwen@ntu.edu.sg
RI Wen, Yonggang/P-9406-2017; Tang, Jianhua/A-2720-2016; Tay, Wee
   Peng/A-5110-2011; Wen, Yonggang/B-8848-2011
OI Wen, Yonggang/0000-0002-2751-5114; Tay, Wee Peng/0000-0002-1543-195X; 
FU NTU Start-Up Grant [RG11/31]; Microsoft Research Asia; Cisco Systems,
   Inc.
FX This work was supported in part by NTU Start-Up Grant (RG11/31), a
   research grant from Microsoft Research Asia, and a research grant from
   Cisco Systems, Inc. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Selcuk K. Candan.
CR [Anonymous], 1992, Data networks
   Ari I, 2003, PROCEEDINGS OF THE 11TH IEEE/ACM INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS AND SIMULATION OF COMPUTER TELECOMMUNICATIONS SYSTEMS, P246
   Bertsekas D.P., 2003, ATHENA SCI OPTIMIZAT
   Björkqvist M, 2011, IEEE INFOCOM SER, P1080, DOI 10.1109/INFCOM.2011.5934883
   Buyya R, 2008, LECT NOTES ELECTR EN, V9, P1, DOI 10.1007/978-3-540-77887-5
   Carter RL, 1997, IEEE INFOCOM SER, P1014, DOI 10.1109/INFCOM.1997.631117
   Chen CM, 2005, 19th International Conference on Advanced Information Networking and Applications, Vol 1, Proceedings, P441
   Chen FF, 2012, IEEE INFOCOM SER, P433, DOI 10.1109/INFCOM.2012.6195782
   Chen L., 2011, P ITA WORKSH
   Chen YS, 2011, IEEE ICC, DOI 10.1109/icc.2011.5962881
   Chen Yishuai, 2009, 2009 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery. CyberC 2009, P180, DOI 10.1109/CYBERC.2009.5342142
   Devine KD, 2005, APPL NUMER MATH, V52, P133, DOI 10.1016/j.apnum.2004.08.028
   Gilly K, 2011, WORLD WIDE WEB, V14, P105, DOI 10.1007/s11280-010-0101-5
   Gray J., 1985, Digest of Papers COMPCON Spring 85. Thirtieth IEEE Computer Society International Conference. Technological Leverage: A Competitive Necessity (Cat. No. 85CH2135-2), P96
   Hosanagar K., 2004, P HICSS
   Jaseemuddin M, 2006, IEEE ICC, P323
   Jin Y., 2013, P 2012 IEEE INT C MU
   Kasbekar M., 2010, EFFICIENT DELIVERY W
   Kaxiras S., 2008, COMPUTER ARCHITECTUR, V1st
   Kingsle L., 2008, BROADBAND PROPERTIES, P24
   Li DC, 2007, COMPUT J, V50, P555, DOI 10.1093/comjnl/bxm020
   Lo Presti F, 2007, I S MOD ANAL SIM COM, P366
   LORDEN G, 1971, ANN MATH STAT, V42, P1897, DOI 10.1214/aoms/1177693055
   Mahapatra NR, 1996, 10TH INTERNATIONAL PARALLEL PROCESSING SYMPOSIUM - PROCEEDINGS OF IPPS '96, P881, DOI 10.1109/IPPS.1996.508195
   Minyard T., 1996, COMPUT METH APPL MEC
   Mitzenmacher M, 2001, IEEE T PARALL DISTR, V12, P1094, DOI 10.1109/71.963420
   MOUSTAKIDES GV, 1986, ANN STAT, V14, P1379, DOI 10.1214/aos/1176350164
   Nan XM, 2011, IEEE INT WORKSH MULT
   Poor H., 2008, Quickest Detection
   Poor H. Vincent, 1994, An introduction to signal detection and estimation
   Qiu XJ, 2012, IEEE INFOCOM SER, P2571, DOI 10.1109/INFCOM.2012.6195655
   Qureshi A, 2009, ACM SIGCOMM COMP COM, V39, P123, DOI 10.1145/1594977.1592584
   Ranjan S, 2008, IEEE T PARALL DISTR, V19, P1186, DOI 10.1109/TPDS.2007.70810
   Sharma U, 2011, INT CON DISTR COMP S, P559, DOI 10.1109/ICDCS.2011.59
   Su AJ, 2009, IEEE ACM T NETWORK, V17, P1752, DOI 10.1109/TNET.2009.2022157
   Systems Cisco, 2010, CISC VIS NETW IND FO
   Teresco JD, 2000, COMPUT METHOD APPL M, V184, P269, DOI 10.1016/S0045-7825(99)00231-5
   Walshaw C, 2001, FUTURE GENER COMP SY, V17, P601, DOI 10.1016/S0167-739X(00)00107-2
   Wen YG, 2011, PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON FUTURE INTERNET TECHNOLOGIES (CFI11)
   Wendell P., 2011, Proceedings of the 2011 ACM SIGCOMM conference on Internet measurement conference, P549
   Yin H, 2009, IMC'09: PROCEEDINGS OF THE 2009 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P442
   Zhang WW, 2013, IEEE T MULTIMEDIA, V15, P1431, DOI 10.1109/TMM.2013.2247583
NR 42
TC 48
Z9 51
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1434
EP 1445
DI 10.1109/TMM.2014.2308726
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600023
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Aulí-Llinàs, F
   Marcellin, MW
AF Auli-Llinas, Francesc
   Marcellin, Michael W.
TI Stationary Probability Model for Microscopic Parallelism in JPEG2000
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bitplane image coding; JPEG2000; parallel architectures; probability
   models
ID DISCRETE WAVELET TRANSFORM; IMAGE COMPRESSION; EFFICIENT
AB Parallel processing is key to augmenting the throughput of image codecs. Despite numerous efforts to parallelize wavelet-based image coding systems, most attempts fail at the parallelization of the bitplane coding engine, which is the most computationally intensive stage of the coding pipeline. The main reason for this failure is the causality with which current coding strategies are devised, which assumes that one coefficient is coded after another. This work analyzes the mechanisms employed in bitplane coding and proposes alternatives to enhance opportunities for parallelism. We describe a stationary probability model that, without sacrificing the advantages of current approaches, removes the main obstacle to the parallelization of most coding strategies. Experimental tests evaluate the coding performance achieved by the proposed method in the framework of JPEG2000 when coding different types of images. Results indicate that the stationary probability model achieves similar coding performance, with slight increments or decrements depending on the image type and the desired level of parallelism.
C1 [Auli-Llinas, Francesc] Univ Autonoma Barcelona, Dept Informat & Commun Engn, Bellaterra 08193, Spain.
   [Marcellin, Michael W.] Univ Arizona, Dept Elect & Comp Engn, Tucson, AZ 85721 USA.
C3 Autonomous University of Barcelona; University of Arizona
RP Aulí-Llinàs, F (corresponding author), Univ Autonoma Barcelona, Dept Informat & Commun Engn, Bellaterra 08193, Spain.
EM fauli@deic.uab.cat; marcellin@ece.arizona.edu
RI Auli-Llinas, Francesc/K-4395-2013
OI Auli-Llinas, Francesc/0000-0002-3208-9957
FU Spanish Government (MINECO); FEDER; Catalan Government [RYC-2010-05671,
   TIN2012-38102-C03-03, 2009-SGR-1224]
FX This work was supported in part by the Spanish Government (MINECO), by
   FEDER, and by the Catalan Government, under Grants RYC-2010-05671,
   TIN2012-38102-C03-03, and 2009-SGR-1224. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Feng Wu.
CR [Anonymous], 2005, H264 INT TEL UN
   [Anonymous], 2002, JPEG2000: Image Compression Fundamentals, Standards, and Practice
   [Anonymous], 2000, 15 ISOIEC, P444
   [Anonymous], 2013, H265 INT TEL UN
   [Anonymous], 2008, Digital cinema system specification Version 1.2
   [Anonymous], 2001, 14492 ISOIEC
   Aulí-Llinàs F, 2013, INFORM SCIENCES, V239, P266, DOI 10.1016/j.ins.2013.03.027
   Aulí-Llinàs F, 2012, IEEE T IMAGE PROCESS, V21, P1920, DOI 10.1109/TIP.2011.2176953
   Aulí-Llinàs F, 2011, IEEE T IMAGE PROCESS, V20, P2153, DOI 10.1109/TIP.2011.2114892
   Buccigrossi RW, 1999, IEEE T IMAGE PROCESS, V8, P1688, DOI 10.1109/83.806616
   Chen SG, 2010, IEEE T CIRC SYST VID, V20, P920, DOI 10.1109/TCSVT.2010.2045831
   Chen Y.-K., 2010, IEEE SIGNAL PROCESS, V27, P1
   Chen YK, 2009, IEEE SIGNAL PROC MAG, V26, P24, DOI 10.1109/MSP.2009.934556
   Danyali H., 2003, J TELECOMMUNICATIONS, V2, P92
   Hu WD, 2012, IEEE T COMMUN, V60, P289, DOI 10.1109/TCOMM.2011.101011.110071
   Leung R, 2005, IEEE T IMAGE PROCESS, V14, P1632, DOI 10.1109/TIP.2005.851707
   Liu Z, 2005, IEEE T IMAGE PROCESS, V14, P411, DOI 10.1109/TIP.2004.841199
   LoPresto SM, 1997, IEEE DATA COMPR CONF, P221, DOI 10.1109/DCC.1997.582045
   Matela J., 2009, ANN DOCT WORKSH MATH, P136
   Mehrseresht N, 2006, IEEE T IMAGE PROCESS, V15, P740, DOI 10.1109/TIP.2005.860619
   Pearlman WA, 2004, IEEE T CIRC SYST VID, V14, P1219, DOI 10.1109/TCSVT.2004.835150
   Peng XL, 2012, IEEE T IMAGE PROCESS, V21, P196, DOI 10.1109/TIP.2011.2159986
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Santos L, 2013, IEEE J-STARS, V6, P670, DOI 10.1109/JSTARS.2013.2247975
   Slattery MJ, 1998, IBM J RES DEV, V42, P767, DOI 10.1147/rd.426.0767
   Sole J, 2012, IEEE T CIRC SYST VID, V22, P1765, DOI 10.1109/TCSVT.2012.2223055
   Taubman D, 2000, IEEE T IMAGE PROCESS, V9, P1158, DOI 10.1109/83.847830
   Taubman D, 2002, IEEE IMAGE PROC, P229
   Tenllado C, 2008, IEEE T PARALL DISTR, V19, P299, DOI 10.1109/TPDS.2007.70716
   van der Laan WJ, 2011, IEEE T PARALL DISTR, V22, P132, DOI 10.1109/TPDS.2010.143
   Weinberger MJ, 1996, IEEE T IMAGE PROCESS, V5, P575, DOI 10.1109/83.491334
   Wong TT, 2007, IEEE T MULTIMEDIA, V9, P668, DOI 10.1109/TMM.2006.887994
   Yoo Y, 1999, IEEE T IMAGE PROCESS, V8, P1702, DOI 10.1109/83.806617
NR 33
TC 16
Z9 16
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2014
VL 16
IS 4
BP 960
EP 970
DI 10.1109/TMM.2014.2307553
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AJ8LN
UT WOS:000337955800006
OA Green Published
DA 2024-07-18
ER

PT J
AU Gallea, R
   Ardizzone, E
   Pirrone, R
AF Gallea, Roberto
   Ardizzone, Edoardo
   Pirrone, Roberto
TI Physical Metaphor for Streaming Media Retargeting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Physical simulation; retargeting; visual saliency
ID MODEL
AB We here introduce an image/video retargeting method that operates arbitrary aspect ratios resizing achieved in real-time performances. Most of the literature retargeting approaches sacrifice real-time performances in behalf of quality. On the other hand, existing fast methods provide arguable results. We can obtain a valuable trade-off between effectiveness and efficiency. The method named Spring Simulation Retargeting (SSR) is mainly based on a physical springs-based simulation. The media are assumed as flexible objects composed of particles and springs with different local stiffness properties, related to the visual importance of the content. The variation of the object size generates elastic forces which determine a new arrangement of the particles, according to the elongation of their connected springs. The deformations are mostly introduced in regions where low importance content is present, while high saliency regions are preserved as desired. The proposed method is evaluated and compared both for images and videos, against several state of the art methods and a user study is taken to assess the results, showing the value of the approach.
C1 [Gallea, Roberto; Ardizzone, Edoardo; Pirrone, Roberto] Univ Palermo, Dept Chem Management Informat Technol & Mech Engn, I-90128 Palermo, Italy.
C3 University of Palermo
RP Gallea, R (corresponding author), Univ Palermo, Dept Chem Management Informat Technol & Mech Engn, I-90128 Palermo, Italy.
EM roberto.gallea@unipa.it; edoardo.ardizzone@unipa.it;
   roberto.pirrone@unipa.it
RI Pirrone, Roberto/Z-4841-2019
OI Pirrone, Roberto/0000-0001-9453-510X; ARDIZZONE,
   Edoardo/0000-0002-3096-8253
CR [Anonymous], GRIFFINS STAT MONOGR
   [Anonymous], P 11 IEEE INT C COMP
   [Anonymous], P CVPR
   [Anonymous], P 1 ACM SIGGRAPH C E
   Avidan S., 2007, P ACM SIGGRAPH 2007
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Butman Moshe, 2008, P CVPR, P1
   Gal R., 2006, P 17 EUROGRAPHICS C, P297, DOI DOI 10.2312/EGWR/EGSR06/297-303
   Gallea R., 2011, P ICIP 2011 2011 IEE
   Gallea R., 2010, P ICIP 2010 2010 IEE
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Karni Z., 2009, P EUR S GEOM PROC
   Kaufmann P., 2013, COMPUT GRAP IN PRESS, V32
   Krahenbuhl P., 2009, P SIGGRAPH AS 09
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Panozzo D, 2012, COMPUT GRAPH FORUM, V31, P229, DOI 10.1111/j.1467-8659.2012.03001.x
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   Rasheed Z, 2003, PROC CVPR IEEE, P343
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Runge C., 1895, MATH ANN
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang Y., 2008, P SIGGRAPH AS 08, P118
   Wang YS, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618473
   Wang Z, 2004, SIGNAL PROCESS-IMAGE, V19, P121, DOI 10.1016/S0923-5965(03)00076-6
   Wang Z, 2007, J OPT SOC AM A, V24, pB61, DOI 10.1364/JOSAA.24.000B61
   Wu HS, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866185
   Zhang YF, 2008, COMPUT GRAPH FORUM, V27, P1797, DOI 10.1111/j.1467-8659.2008.01325.x
NR 29
TC 9
Z9 10
U1 3
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2014
VL 16
IS 4
BP 971
EP 979
DI 10.1109/TMM.2014.2305917
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AJ8LN
UT WOS:000337955800007
DA 2024-07-18
ER

PT J
AU Ma, SW
   Wang, SQ
   Gao, W
AF Ma, Siwei
   Wang, Shiqi
   Gao, Wen
TI Low Complexity Adaptive View Synthesis Optimization in HEVC Based 3D
   Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D video coding; computational complexity; view synthesis optimization;
   zero synthesized view difference
ID COMPRESSION; REPRESENTATION
AB In this correspondence, we explore a low-complexity adaptive view synthesis optimization (VSO) scheme in the upcoming high-efficiency video coding (HEVC)-based 3-D video coding standard. We first devise a novel zero-synthesized view difference (ZSVD) model which jointly accounts for the distortion of the synthesized view induced by the compound impact of depth-disparity mapping, texture adaptation, and occlusion in the view synthesis process. This model can efficiently estimate the maximum allowable depth distortion in synthesizing a virtual view without introducing any geometry distortion. Then, an adaptive ZSVD-aware VSO scheme is proposed by incorporating the ZSVD model into the rate-distortion optimization process, which is developed by pruning the conventional view synthesis algorithm. Extensive experimental results confirm that the proposed model is capable of accurately predicting the zero distortion of the synthesized view and exhibit that the proposed ZSVD-aware VSO scheme can remarkably reduce the coding computational complexity with negligible performance loss.
C1 [Ma, Siwei; Wang, Shiqi; Gao, Wen] Peking Univ, Inst Digital Media, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
C3 Peking University
RP Ma, SW (corresponding author), Peking Univ, Inst Digital Media, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
EM swma@pku.edu.cn; sqwang@pku.edu.cn; wgao@pku.edu.cn
OI Wang, Shiqi/0000-0002-3583-959X
FU National High-tech R&D Program of China (863 Program) [SS2012AA010805];
   National Science Foundation [61210005, 61121002]
FX This work was supported in part by the National High-tech R&D Program of
   China (863 Program) under Grant SS2012AA010805 and the National Science
   Foundation under Grant 61210005 and Grant 61121002. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Shahram Shirani.
CR [Anonymous], 2012, M22697 ISOIEC JTC1SC
   [Anonymous], 2011, JTC1SC29WG11 ISOIEC
   Bjontegaard G., 2008, SG16 ITUT
   Chen CC, 2009, 2009 11TH IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM 2009), P171, DOI 10.1109/ISM.2009.68
   De Silva DVSX, 2010, IEEE T CONSUM ELECTR, V56, P2735, DOI 10.1109/TCE.2010.5681163
   Domanski M., 2011, M22697 ISOIEC JTC1SC
   Han WJ, 2010, IEEE T CIRC SYST VID, V20, P1709, DOI 10.1109/TCSVT.2010.2092612
   Jung J., 2012, M23856 ISOIEC JTC1SC
   Kauff P, 2007, SIGNAL PROCESS-IMAGE, V22, P217, DOI 10.1016/j.image.2006.11.013
   Kim W.-S., 2010, IS T SPIE ELECT IMAG, P7543
   Kim WS, 2009, IEEE IMAGE PROC, P721, DOI 10.1109/ICIP.2009.5414304
   Merkle P., 2008, 2008 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video, P245, DOI 10.1109/3DTV.2008.4547854
   Merkle P, 2009, SIGNAL PROCESS-IMAGE, V24, P73, DOI 10.1016/j.image.2008.10.010
   Merkle P, 2007, IEEE IMAGE PROC, P201
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   Oh B., 2012, JCT2A0093 ISOIEC JTC
   Oh B., 2012, JCT2A0033 ISOIEC JTC
   Oh B., 2012, JTC1SC29WG11 ISOIEC
   Oh BT, 2011, IEEE J-STSP, V5, P1344, DOI 10.1109/JSTSP.2011.2164893
   Rusanovskyy D., 2012, JTC1SC29WG11 ISOIEC
   Schwarz H., 2012, N12560 ISOIEC JTC1SC
   Schwarz H., M22570 ISOIEC JTC1SC
   Schwarz H., 2012, JTC1SC29WG11 ISOIEC
   Schwarz H, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P1, DOI 10.1109/PCS.2012.6213271
   Secker A, 2004, IEEE T IMAGE PROCESS, V13, P1029, DOI 10.1109/TIP.2004.826089
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Tech G., 2013, JTC1SC29WG11 ISOIEC
   Tech G, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P25, DOI 10.1109/PCS.2012.6213277
   Wang Q., 2012, IEEE T CIRCUITS SYST, V22
   Yuan H, 2011, IEEE T CIRC SYST VID, V21, P485, DOI 10.1109/TCSVT.2011.2125610
   Zhao Y, 2011, IEEE T IMAGE PROCESS, V20, P2221, DOI 10.1109/TIP.2011.2118218
NR 31
TC 33
Z9 36
U1 1
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 266
EP 271
DI 10.1109/TMM.2013.2284751
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100022
DA 2024-07-18
ER

PT J
AU Laganà, MM
   Preti, MG
   Forzoni, L
   D'Onofrio, S
   De Beni, S
   Barberio, A
   Cecconi, P
   Baselli, G
AF Lagana, Maria Marcella
   Preti, Maria Giulia
   Forzoni, Leonardo
   D'Onofrio, Sara
   De Beni, Stefano
   Barberio, Antonello
   Cecconi, Pietro
   Baselli, Giuseppe
TI Transcranial Ultrasound and Magnetic Resonance Image Fusion With Virtual
   Navigator
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fusion imaging; image registration; magnetic resonance imaging;
   transcranial ultrasound; virtual navigator
ID CEREBROSPINAL VENOUS INSUFFICIENCY; MULTIPLE-SCLEROSIS;
   COMPUTED-TOMOGRAPHY; SYSTEM; REGISTRATION; MRI; CT
AB The Virtual Navigator (VN) technology was used for the fusion of transcranial Ultrasound (US) and brain Magnetic Resonance Images (MRI), with a repeatability error under 0.1 cm. The superimposition of US to the previously acquired MRI volume consisted of external point-based registration, that was subsequently refined with image-based registration of internal brain structures. The common registration procedure with the usage of external fiducial markers acquired with the two modalities was improved using facial anatomical landmarks, with a reduction of the internal targeted structure residual shift (maximum 0.7 cm in the cranio-caudal direction). This allowed the investigation of Deep Cerebral Veins and dural sinuses insonated from the condyloid process of the mandible, a recently introduced US window. The fusion of these vessels to the MRI volume provided their anatomical position and helped excluding false Doppler signal sources.
C1 [Lagana, Maria Marcella; Barberio, Antonello; Cecconi, Pietro] Fdn Don Carlo Gnocchi ONLUS, I-20148 Milan, Italy.
   [Preti, Maria Giulia; Baselli, Giuseppe] Politecn Milan, Dipartimento Bioingn, I-20133 Milan, Italy.
   [Preti, Maria Giulia] IRCCS S Maria Nascente, Fdn Don Carlo Gnocchi ONLUS, I-20148 Milan, Italy.
   [Forzoni, Leonardo; D'Onofrio, Sara; De Beni, Stefano] ESAOTE SpA, I-16153 Genoa, Italy.
C3 IRCCS Fondazione Don Carlo Gnocchi Onlus; Polytechnic University of
   Milan; IRCCS Fondazione Don Carlo Gnocchi Onlus
RP Laganà, MM (corresponding author), Fdn Don Carlo Gnocchi ONLUS, I-20148 Milan, Italy.
EM mlagana@dongnocchi.it; gpreti@dongnocchi.it;
   leonardo.forzoni@esaote.com; sara.donofrio@esaote.com;
   stefano.debeni@esaote.com; giuseppe.baselli@polimi.it
RI Lagana, Maria Marcella/L-8050-2016; Preti, Maria Giulia/X-9430-2019
OI Lagana, Maria Marcella/0000-0001-7848-1711; Preti, Maria
   Giulia/0000-0002-5323-5327
CR Centonze D, 2011, ANN NEUROL, V70, P51, DOI 10.1002/ana.22436
   Ciccone M. M., 2012, CURR NEUROVASC RES
   Clevert DA, 2012, RADIOLOGE, V52, P63, DOI 10.1007/s00117-011-2252-5
   Coupe P, 2012, INT J BIOMED IMAGING, V2012, DOI 10.1155/2012/531319
   Crocetti L, 2008, INVEST RADIOL, V43, P33, DOI 10.1097/RLI.0b013e31815597dc
   De Beni S., 2004, P IEEE SICE 2 INT S
   Forzoni L., 2012, Biomedical Engineering-Biomedizinische Technik, V57, DOI 10.1515/bmt-2012-4282
   Ge Y, 2008, ARCH NEUROL-CHICAGO, V65, P812, DOI 10.1001/archneur.65.6.812
   Ge YL, 2009, J MAGN RESON IMAGING, V29, P1190, DOI 10.1002/jmri.21758
   Haacke EM, 2004, MAGN RESON MED, V52, P612, DOI 10.1002/mrm.20198
   Iagnocco A, 2011, CLIN EXP RHEUMATOL, V29, P757
   Kunishi Y, 2012, AM J ROENTGENOL, V198, P106, DOI 10.2214/AJR.10.6039
   Laganà MM, 2011, IEEE ENG MED BIO, P579, DOI 10.1109/IEMBS.2011.6090108
   Lu XS, 2012, COMPUT AIDED SURG, V17, P1, DOI 10.3109/10929088.2011.637235
   Malferrari G., 2011, P 16 M EUR SOC NEUR, P67
   Menegatti E, 2010, INT ANGIOL, V29, P121
   Mercier L, 2011, INT J COMPUT ASS RAD, V6, P507, DOI 10.1007/s11548-010-0535-3
   MILLER DH, 1991, J NEUROL NEUROSUR PS, V54, P683, DOI 10.1136/jnnp.54.8.683
   Mittal S, 2009, AM J NEURORADIOL, V30, P232, DOI 10.3174/ajnr.A1461
   Nikas Dimitrios C, 2003, Neurosurg Focus, V14, pe6
   Nosal' V., 2010, NEUROLOGIA, V5, P163
   Rennert J, 2011, CLIN HEMORHEOL MICRO, V49, P67, DOI 10.3233/CH-2011-1458
   Schlachetzki F, 2012, CEREBROVASC DIS, V33, P262, DOI 10.1159/000334667
   Schreiber S J, 2002, Eur J Ultrasound, V16, P59, DOI 10.1016/S0929-8266(02)00051-4
   Stolz E, 1999, STROKE, V30, P70, DOI 10.1161/01.STR.30.1.70
   Stolz E, 2001, J NEUROIMAGING, V11, P385, DOI 10.1111/j.1552-6569.2001.tb00067.x
   Stolz E, 2006, FRONT NEUROL NEUROSC, V21, P182, DOI 10.1159/000092400
   Stolz Erwin P, 2008, Front Neurol Neurosci, V23, P112
   Tortoli P., 2010, P IEEE ULTR S, P1190
   Ukimura O, 2012, J UROLOGY, V187, P1080, DOI 10.1016/j.juro.2011.10.124
   Weir B, 2010, CAN J NEUROL SCI, V37, P745, DOI 10.1017/S0317167100051404
   Zamboni P, 2010, PHLEBOLOGY, V25, P269, DOI 10.1258/phleb.2010.009083
   Zamboni P, 2009, J NEUROL NEUROSUR PS, V80, P392, DOI 10.1136/jnnp.2008.157164
   Zamboni P., 2011, PHLEBOLOGY
   Zamboni P, 2012, J APPL PHYSIOL, V112, P904, DOI 10.1152/japplphysiol.00712.2011
   Zedde M., 2011, P 16 M EUR SOC NEUR, P22
   Zedde M., 2012, PERSPECT MED, DOI [10.1016/j.premed.2012.02.008, DOI 10.1016/J.PREMED.2012.02.008]
NR 37
TC 10
Z9 10
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2013
VL 15
IS 5
BP 1039
EP 1048
DI 10.1109/TMM.2013.2244871
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222WT
UT WOS:000324763600008
DA 2024-07-18
ER

PT J
AU Li, XR
   Snoek, CGM
   Worring, M
   Koelma, D
   Smeulders, AWM
AF Li, Xirong
   Snoek, Cees G. M.
   Worring, Marcel
   Koelma, Dennis
   Smeulders, Arnold W. M.
TI Bootstrapping Visual Categorization With Relevant Negatives
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Model compression; negative bootstrap; relevant negative examples;
   visual categorization
AB Learning classifiers for many visual concepts are important for image categorization and retrieval. As a classifier tends to misclassify negative examples which are visually similar to positive ones, inclusion of such misclassified and thus relevant negatives should be stressed during learning. User-tagged images are abundant online, but which images are the relevant negatives remains unclear. Sampling negatives at random is the de facto standard in the literature. In this paper, we go beyond random sampling by proposing Negative Bootstrap. Given a visual concept and a few positive examples, the new algorithm iteratively finds relevant negatives. Per iteration, we learn from a small proportion of many user-tagged images, yielding an ensemble of meta classifiers. For efficient classification, we introduce Model Compression such that the classification time is independent of the ensemble size. Compared with the state of the art, we obtain relative gains of 14% and 18% on two present-day benchmarks in terms of mean average precision. For concept search in one million images, model compression reduces the search time from over 20 h to approximately 6 min. The effectiveness and efficiency, without the need of manually labeling any negatives, make negative bootstrap appealing for learning better visual concept classifiers.
C1 [Li, Xirong] Renmin Univ China, MOE Key Lab Data Engn & Knowledge Engn, Sch Informat, Beijing 100872, Peoples R China.
   [Li, Xirong] State Key Lab Software Dev Environm, Beijing 100191, Peoples R China.
   [Snoek, Cees G. M.; Worring, Marcel; Koelma, Dennis; Smeulders, Arnold W. M.] Univ Amsterdam, Intelligent Syst Lab Amsterdam, NL-1098 XH Amsterdam, Netherlands.
   [Smeulders, Arnold W. M.] Ctr Wiskunde & Informat, NL-1098 XG Amsterdam, Netherlands.
C3 Renmin University of China; University of Amsterdam
RP Li, XR (corresponding author), Renmin Univ China, MOE Key Lab Data Engn & Knowledge Engn, Sch Informat, Beijing 100872, Peoples R China.
EM xirong@ruc.edu.cn
RI Li, Xirong/AAD-3347-2019; Worring, Marcel/JRW-7059-2023
OI Li, Xirong/0000-0002-0220-8310; Worring, Marcel/0000-0003-4097-4136
FU Dutch national program COMMIT; STW SEARCHER Project; SKL-SDE "Research
   on core techniques of unstructured data management"; NSFC [61103062]
FX This work was supported in part by the Dutch national program COMMIT,
   the STW SEARCHER Project, the SKL-SDE Project "Research on core
   techniques of unstructured data management," and NSFC 61103062. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Samson Cheung.
CR [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], P ACM MULT
   [Anonymous], P CIVR
   [Anonymous], P ACM MULT
   [Anonymous], 2004, P 2004WORKSHOP STAT
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], P ICMR
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Bing L, 2003, THIRD IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P179, DOI 10.1109/icdm.2003.1250918
   Breiman L., 2001, Mach. Learn., V45, P5
   Chawla NV, 2004, J MACH LEARN RES, V5, P421
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Cilibrasi RL, 2007, IEEE T KNOWL DATA EN, V19, P370, DOI 10.1109/TKDE.2007.48
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fellbaum C., 1998, WORDNET ELECT LEXICA, DOI DOI 10.7551/MITPRESS/7287.001.0001
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235
   Kennedy L.S., 2006, Proc. ACM Multimedia Information Retrieval, P249, DOI DOI 10.1145/1178677.1178712
   Li XR, 2009, IEEE T MULTIMEDIA, V11, P1310, DOI 10.1109/TMM.2009.2030598
   Liu XY, 2006, IEEE DATA MINING, P965
   Liu YM, 2011, IEEE T PATTERN ANAL, V33, P1022, DOI 10.1109/TPAMI.2010.142
   Maji S., 2008, P CVPR, P1
   Maji S, 2009, IEEE I CONF COMP VIS, P40, DOI 10.1109/ICCV.2009.5459203
   Natsev A., 2005, 13th Annual ACM International Conference on Multimedia, P598, DOI 10.1145/1101149.1101288
   Rokach L, 2010, ARTIF INTELL REV, V33, P1, DOI 10.1007/s10462-009-9124-7
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Schroff F, 2011, IEEE T PATTERN ANAL, V33, P754, DOI 10.1109/TPAMI.2010.133
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Tang JH, 2012, IEEE T IMAGE PROCESS, V21, P2354, DOI 10.1109/TIP.2011.2180916
   Tao DC, 2007, IEEE T KNOWL DATA EN, V19, P568, DOI 10.1109/TKDE.2007.1003
   Tao DC, 2006, IEEE T PATTERN ANAL, V28, P1088, DOI 10.1109/TPAMI.2006.134
   Tax D., 2001, One-class classification: Concept-learning in the absence of counter-examples
   Tian XM, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2240136.2240139
   Tian XM, 2010, IEEE T IMAGE PROCESS, V19, P805, DOI 10.1109/TIP.2009.2035866
   Tong S, 2001, P 9 ACM INT C MULT, P107, DOI DOI 10.1145/500141.500159
   Uijlings JRR, 2010, IEEE T MULTIMEDIA, V12, P665, DOI 10.1109/TMM.2010.2052027
   Ulges A, 2010, COMPUT VIS IMAGE UND, V114, P429, DOI 10.1016/j.cviu.2009.08.002
   Vapnik VN, 2000, NATURE STAT LEARNING, DOI DOI 10.1007/978-1-4757-3264-1
   von Ahn Luis., 2004, CHI, DOI DOI 10.1145/985692.985733
   Wang G, 2012, IEEE T PATTERN ANAL, V34, P2177, DOI 10.1109/TPAMI.2012.29
   Wang M, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1899412.1899414
   Williams C.K.I., PASCAL VISUAL OBJECT
   Wu JX, 2010, LECT NOTES COMPUT SC, V6312, P552
   Yanai K., 2005, Proceedings of the 7th ACM SIGMM international workshop on Multimedia information retrieval, P57
   Yuan Jinhui., 2006, P 14 ANN ACM INT C M, P441
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
NR 47
TC 22
Z9 24
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2013
VL 15
IS 4
BP 933
EP 945
DI 10.1109/TMM.2013.2238523
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148FK
UT WOS:000319228500020
OA Green Published
DA 2024-07-18
ER

PT J
AU Zhang, Z
   Seah, HS
   Quah, CK
   Sun, JX
AF Zhang, Zheng
   Seah, Hock Soon
   Quah, Chee Kwang
   Sun, Jixiang
TI GPU-Accelerated Real-Time Tracking of Full-Body Motion With Multi-Layer
   Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Markerless motion capture; multi-layer search; niching swarm filtering;
   real-time tracking; GPU; CUDA
ID CAPTURE
AB Compared to monocular pose tracking, 3D articulated body pose tracking from multiple cameras can better deal with self-occlusions and meet less ambiguities. Though considerable advances have been made, pose tracking from multiple images has not been extensively studied: very seldom existing work can produce a solution comparable to that of a marker-based system which generally can recover accurate 3D full-body motion in real-time. In this paper, we present a multi-view approach to 3D body pose tracking. We propose a pose search method by introducing a new generative sampling algorithm with a refinement step of local optimization. This multi-layer search method does not rely on strong motion priors and generalizes well to general human motions. Physical constraints are incorporated in a novel way and 3D distance transform is employed for speedup. A voxel subject-specific 3D body model is created automatically at the initial frame to fit the subject to be tracked. We design and develop the optimized parallel implementations of time-consuming algorithms on GPU (Graphics Processing Unit) using CUDA (Compute Unified Device Architecture), which significantly accelerates the pose tracking process, making our method capable of tracking full body movements with a maximum speed of 9 fps. Experiments on various 8-camera datasets and benchmark datasets (HumanEva-II) captured by 4 cameras demonstrate the robustness and accuracy of our method.
C1 [Zhang, Zheng; Seah, Hock Soon; Quah, Chee Kwang] Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
   [Sun, Jixiang] Natl Univ Def Technol, Coll Elect Sci & Engn, Changsha, Hunan, Peoples R China.
C3 Nanyang Technological University; National University of Defense
   Technology - China
RP Zhang, Z (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
EM zhan0236@ntu.edu.sg; ashsseah@ntu.edu.sg; quah_chee_kwang@ntu.edu.sg
RI Quah, Ching Kheng/A-5525-2009; Seah, Hock Soon/AAK-9900-2020
OI Seah, Hock Soon/0000-0003-2699-7147
FU China Scholarship Council (CSC)
FX This work was supported in part by the China Scholarship Council (CSC).
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Weisi Lin.
CR Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21
   [Anonymous], 2010, NVIDIA CUDA PROGR GU
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2006, HUMANEVA SYNCHRONIZ
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2009, P IEEE C COMP VIS PA
   Balan A. O., 2005, P INT WORKSH PERF EV
   Brand M., 1999, P IEEE INT C COMP VI
   Bray M, 2007, COMPUT VIS IMAGE UND, V106, P116, DOI 10.1016/j.cviu.2005.09.013
   Caillette F., 2005, P BRIT MACH VIS C
   Carranza J., 2003, P ACM SIGGRAPH
   CHAM T, 1999, P IEEE C COMP VIS PA
   CHEUNG G, 2003, P IEEE C COMP VIS PA
   CHEUNG GKM, 2000, P IEEE C COMP VIS PA
   CHOO K, 2001, P IEEE INT C COMP VI
   Clerc M, 2002, IEEE T EVOLUT COMPUT, V6, P58, DOI 10.1109/4235.985692
   Deutscher J, 2005, INT J COMPUT VISION, V61, P185, DOI 10.1023/B:VISI.0000043757.18370.9c
   ELGAMMAL A, 2004, P IEEE C COMP VIS PA
   Gall J, 2010, INT J COMPUT VISION, V87, P75, DOI 10.1007/s11263-008-0173-1
   Ganapathi V., 2010, P IEEE C COMP VIS PA
   GAVRILA DM, 1996, P IEEE C COMP VIS PA
   Horaud R, 2009, IEEE T PATTERN ANAL, V31, P158, DOI 10.1109/TPAMI.2008.108
   HOU SB, 2007, P IEEE INT C COMP VI
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   John V., 2010, IMAGE VIS COMPUT, V28
   Kehl R., 2005, P IEEE C COMP VIS PA
   Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968
   Kennedy J., 2003, P IEEE SWARM INT S
   Li XD, 2010, IEEE T EVOLUT COMPUT, V14, P150, DOI 10.1109/TEVC.2009.2026270
   MACCORMICK JP, 2000, P EUR C COMP VIS
   Maurer CR, 2003, IEEE T PATTERN ANAL, V25, P265, DOI 10.1109/TPAMI.2003.1177156
   Michoud B., 2007, P IEEE INT C COMP VI
   Mikic I, 2003, INT J COMPUT VISION, V53, P199, DOI 10.1023/A:1023012723347
   Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002
   Poppe R, 2007, COMPUT VIS IMAGE UND, V108, P4, DOI 10.1016/j.cviu.2006.10.016
   Ramannan D., 2003, P IEEE C COMP VIS PA
   Rosenhahn B., 2007, P IEEE C COMP VIS PA
   Secrest B. R., 2003, P IEEE SWARM INT S
   Shan CF, 2007, PATTERN RECOGN, V40, P1958, DOI 10.1016/j.patcog.2006.12.012
   Sminchisescu C, 2003, INT J ROBOT RES, V22, P371, DOI 10.1177/0278364903022006003
   Sminchisescu Cristian, 2001, P IEEE C COMP VIS PA
   Stenger B, 2006, IEEE T PATTERN ANAL, V28, P1372, DOI 10.1109/TPAMI.2006.189
   Storn R, 1997, J GLOBAL OPTIM, V11, P341, DOI 10.1023/A:1008202821328
   Urtasun R., 2006, P IEEE C COMP VIS PA
   Zhang X., 2010, P IEEE C COMP VIS PA
   Zhang Z., 2011, P IEEE C COMP VIS PA
   Zhang Z., 2011, P IEEE C IM PROC
NR 47
TC 31
Z9 34
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2013
VL 15
IS 1
BP 106
EP 119
DI 10.1109/TMM.2012.2225040
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 058PS
UT WOS:000312646600009
DA 2024-07-18
ER

PT J
AU Feng, JS
   Ni, YZ
   Dong, J
   Wang, ZL
   Yan, SC
AF Feng, Jiashi
   Ni, Yuzhao
   Dong, Jian
   Wang, Zilei
   Yan, Shuicheng
TI Purposive Hidden-Object-Game: Embedding Human Computation in Popular
   Game
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Games with a purpose (GWAP); human computing; image processing;
   multimedia computing
ID IMAGE
AB Having sufficient training images with fully annotated object locations is undoubtedly critical for modern learning-based image annotation, retrieval, and object detection methods. Typically, collecting such annotations for large-scale datasets is notoriously tedious because the process involves amount of manual cropping and hand labeling operations. In this work, following the principle of games with a purpose (GWAP), we design a so-called purposive hidden-object-game (P-HOG), which imperceptibly embeds localizing objects into enjoyable playing game process and thus attracts many people to make voluntary contribution to annotating images. In particular, besides preserving the interestingness as popular HOG games, P-HOG is able to automatically generate satisfactory game images (i.e., "hide" certain items into target images) by integrating several semantic and visual processing techniques. P-HOG is also built in an effective mechanism to prevent the players from cheating. The mechanism inherits the merit of Recaptcha and identifies potential cheating behavior based on the annotation accuracy of some known items. Moreover, P-HOG will filter noisy annotations effectively based on a weighted majority method and improve the accuracy of the raw annotations from the players. Most importantly, players only play P-HOG for entertainment purpose and they are unaware of the background data collection procedure. The collected data are used towards constructing a large database, which may benefit general learning-based algorithms for multimedia tasks. To the best of our knowledge, this is the first work dedicated to such a specific and important task under the GWAP framework. We conduct a pilot study of the game prototype and the comprehensive experiments show that the P-HOG appeals to general players, and is effective for collecting massive object locations with satisfactory accuracy, which further boosts the algorithmic performances for both tag refinement and image annotation tasks.
C1 [Feng, Jiashi; Dong, Jian; Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore.
   [Ni, Yuzhao] MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA.
   [Wang, Zilei] Univ Sci & Technol China, Dept Automat, Hefei 230027, Peoples R China.
C3 National University of Singapore; Massachusetts Institute of Technology
   (MIT); Chinese Academy of Sciences; University of Science & Technology
   of China, CAS
RP Feng, JS (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore.
EM a0066331@nus.edu.sg; nyzstar@mit.edu; a0068947@nus.edu.sg;
   zlwang@ustc.edu.cn; eleyans@nus.edu.sg
RI Feng, Jiashi/AGX-6209-2022; Yan, Shuicheng/HCI-1431-2022; wang,
   jie/GRS-0942-2022; Dong, Jin Song/B-9396-2013; Wang, Zhong
   Lin/E-2176-2011
OI Wang, Zhong Lin/0000-0002-5530-0380
FU Singapore Ministry of Education [MOE2010-T2-1-087]
FX This work was supported by Singapore Ministry of Education under
   research Grant MOE2010-T2-1-087.
CR [Anonymous], 2010, P ACM MULTIMEDIA
   Cheng B, 2010, IEEE T IMAGE PROCESS, V19, P858, DOI 10.1109/TIP.2009.2038764
   Cilibrasi RL, 2007, IEEE T KNOWL DATA EN, V19, P370, DOI 10.1109/TKDE.2007.48
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Everingham Mark, 2007, PASCAL VISUAL OBJECT
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Ho C.-J., 2010, ACM SIGKDD EXPLORATI, V12, P21, DOI [10.1145/1882471.1882475, DOI 10.1145/1882471.1882475]
   Hoiem D, 2007, INT J COMPUT VISION, V75, P151, DOI 10.1007/s11263-006-0031-y
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Kang F., 2006, CVPR, V2, P1719
   Lalonde JF, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276381, 10.1145/1239451.1239454]
   Leung T, 2001, INT J COMPUT VISION, V43, P29, DOI 10.1023/A:1011126920638
   Liu Dong., 2009, P 18 INT C WORLD WID, P351
   Man-Ching Yuen, 2009, 2009 International Conference on Computational Science and Engineering (CSE), P723, DOI 10.1109/CSE.2009.395
   Mei T., 2008, P 16 ACM INT C MULT, P439, DOI [https://doi.org/10.1145/1459359.1459418, DOI 10.1145/1459359.1459418]
   Mori G, 2004, PROC CVPR IEEE, P326
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Ostromoukhov V, 2004, ACM T GRAPHIC, V23, P488, DOI 10.1145/1015706.1015750
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   von Ahn L, 2004, COMMUN ACM, V47, P57, DOI 10.1145/966389.966390
   von Ahn Luis., 2004, CHI, DOI DOI 10.1145/985692.985733
   VONAHN L, 2006, P SIGCHI C HUM FACT, P55, DOI DOI 10.1145/1124772.1124782
   Walsh G, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2079
   Wang CB, 2006, LECT NOTES COMPUT SC, V4035, P647, DOI 10.1145/1180639.1180774
   Wang J, 2007, FOUND TRENDS COMPUT, V3, P97, DOI 10.1561/0600000019
   Xu HX, 2009, PROCEEDINGS OF THE 2009 WRI GLOBAL CONGRESS ON INTELLIGENT SYSTEMS, VOL III, P573, DOI 10.1109/GCIS.2009.320
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
NR 29
TC 1
Z9 1
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2012
VL 14
IS 5
BP 1496
EP 1507
DI 10.1109/TMM.2012.2198801
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 008XT
UT WOS:000308990600010
DA 2024-07-18
ER

PT J
AU Chuah, SP
   Chen, ZZ
   Tan, YP
AF Chuah, Seong-Ping
   Chen, Zhenzhong
   Tan, Yap-Peng
TI Energy-Efficient Resource Allocation and Scheduling for Multicast of
   Scalable Video Over Wireless Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Energy efficient transmission; modulation and coding scheme; resource
   allocation; scalable video coding; scheduling; wireless multicast
ID TRANSMISSION; COMMUNICATION; OPTIMIZATION; ADAPTATION
AB In this paper, we investigate optimal resource allocation and scheduling for scalable video multicast over wireless networks. The wireless video multicasting is a best-effort service which has limited transmission energy and channel access time. To cater for multi-resolution videos to heterogeneous clients and for channel adaptation, we adopt scalable video coding (SVC) with spatial, temporal and quality scalabilities. Our scalable video multicast system consists of a channel probing stage to gather the channel state information and a transmission stage to multicast videos to clients. We formulate the optimal resource allocation problem by maximizing the video quality of the clients subject to transmission energy and channel access constraints. We show that the problem is a joint optimization of the selection of modulation and coding scheme (MCS), and the transmission power allocation. By imposing a quality-of-service (QoS) constraint on the packet loss rate, we simplify the original problem to a binary knapsack problem which can be solved by a dynamic programming approach. Specifically, we first propose a multicast scheduling scheme based on the quality impact of each SVC layer. Guided by the content-aware multicast scheduling, we optimize the resource allocation for each SVC layer sequentially. Solution at each step takes into account of the channel condition, remaining resources, and client requirements. The proposed scheme is of linear complexity and leads to the maximized video quality for the admitted clients, while satisfying the energy budget and channel access constraints. Experiment results demonstrate that our scheme achieves notable video quality improvements for multicast clients, when compared to the state-of-the-art video multicast method.
C1 [Chuah, Seong-Ping; Chen, Zhenzhong; Tan, Yap-Peng] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore, Singapore.
C3 Nanyang Technological University
RP Chuah, SP (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore, Singapore.
EM chua0536@ntu.edu.sg; zzchen@ntu.edu.sg; eyptan@ntu.edu.sg
RI 陈, 震中/C-6857-2014; Tan, Yap-Peng/A-5158-2011; Chen,
   Zhenzhong/J-8017-2012; Chen, Zhenzhong/C-2529-2015
FU Agency for Science, Technology and Research (A*STAR), Singapore under
   the Science and Engineering Research Councils
FX This work was supported in part by a research grant awarded by the
   Agency for Science, Technology and Research (A*STAR), Singapore, under
   the Science and Engineering Research Councils Mobile Media Thematic
   Strategic Research Programme. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Zhihai (Henry) He.
CR [Anonymous], 1998, INTEGER PROGRAMMING
   [Anonymous], 80211A1999 IEEE
   Bandyopadhyay SK, 2008, IEEE T IMAGE PROCESS, V17, P1020, DOI 10.1109/TIP.2008.921312
   Bertsekas D., 2007, Dynamic programming and optimal control, V2
   Goldsmith A., 2005, WIRELESS COMMUNICATI
   He ZH, 2005, IEEE T CIRC SYST VID, V15, P645, DOI 10.1109/TCSVT.2005.846433
   He ZH, 2008, IEEE T CIRC SYST VID, V18, P596, DOI 10.1109/TCSVT.2008.918802
   Hefeeda M, 2010, IEEE ACM T NETWORK, V18, P610, DOI 10.1109/TNET.2009.2030326
   Hua S, 2011, IEEE T MULTIMEDIA, V13, P402, DOI 10.1109/TMM.2010.2103929
   Huang JW, 2008, IEEE T CIRC SYST VID, V18, P582, DOI 10.1109/TCSVT.2008.919109
   HUANG TY, 2008, P ICESS S JUL, P431, DOI DOI 10.1109/ICESS.SYMPOSIA.2008.81
   Jakubczak S., 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P530, DOI 10.1109/ALLERTON.2010.5706952
   Kholaif AM, 2010, IEEE T MULTIMEDIA, V12, P142, DOI 10.1109/TMM.2009.2037380
   Krishnaswamy D., 2002, P 3 GEN WIR MAY
   Li L., 2009, PROCEEDING 17 IEEEAC, P1, DOI DOI 10.1109/BMEI.2009.5305169
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   Liang WF, 2006, IEEE T MOBILE COMPUT, V5, P377, DOI 10.1109/TMC.2006.1599406
   Lu XA, 2003, IEEE J SEL AREA COMM, V21, P1738, DOI 10.1109/JSAC.2003.815682
   Luna CE, 2003, IEEE J SEL AREA COMM, V21, P1710, DOI 10.1109/JSAC.2003.815394
   Maani E, 2008, IEEE T IMAGE PROCESS, V17, P1663, DOI 10.1109/TIP.2008.2001402
   MacLeod H, 2005, Proceedings of the 3rd Annual Communication Networks and Services Research Conference, P63
   Mansour H, 2009, IEEE T MULTIMEDIA, V11, P1478, DOI 10.1109/TMM.2009.2032682
   Pahalawatta P, 2007, IEEE J SEL AREA COMM, V25, P749, DOI 10.1109/JSAC.2007.070511
   Pollin S, 2008, IEEE T WIREL COMMUN, V7, P98, DOI 10.1109/TWC.2008.05356
   Qiao D., 2002, P INFOCOM, V2
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Sharangi S, 2011, IEEE T MULTIMEDIA, V13, P102, DOI 10.1109/TMM.2010.2076799
   Su GM, 2007, IEEE J-STSP, V1, P280, DOI 10.1109/JSTSP.2007.901518
   Villalón J, 2007, IEEE J SEL AREA COMM, V25, P699, DOI 10.1109/JSAC.2007.070507
   Zhai F, 2005, SIGNAL PROCESS-IMAGE, V20, P371, DOI 10.1016/j.image.2005.02.002
   Zhang Q, 2004, IEEE T CIRC SYST VID, V14, P1049, DOI 10.1109/TCSVT.2004.831966
   Zhang Y, 2010, IEEE T SIGNAL PROCES, V58, P3108, DOI 10.1109/TSP.2010.2046040
NR 32
TC 31
Z9 33
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
BP 1324
EP 1336
DI 10.1109/TMM.2012.2193560
PN 2
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QM
UT WOS:000306599400017
DA 2024-07-18
ER

PT J
AU Su, HH
   Chen, TW
   Kao, CC
   Hsu, WH
   Chien, SY
AF Su, Hsiao-Hang
   Chen, Tse-Wei
   Kao, Chieh-Chi
   Hsu, Winston H.
   Chien, Shao-Yi
TI Preference-Aware View Recommendation System for Scenic Photos Based on
   Bag-of-Aesthetics-Preserving Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Aesthetic modeling; aesthetic view finding; photo aesthetic quality
   classification; view recommendation
AB In this paper, the framework for a real-time view recommendation system is proposed. The proposed system comprises two parts: offline aesthetic modeling stage and efficient online aesthetic view finding process. A preference-aware aesthetic model is proposed to suggest views according to varied user-favorite photographic styles, where a bottom-up approach is developed to construct an aesthetic feature library with bag-of-aesthetics-preserving features instead of top-down methods that implement the heuristic guidelines (rule-specific features) listed in photography literatures, which is employed in previous works. A collection of scenic photos is used as the test set; however, the proposed method can be employed to other types of photo collection according to different application scenarios. The proposed model can cover both implicit and explicit aesthetic features and can adapt to users' preferences with a learning process. In the second part, the learned model is employed in a view finder to help the user to locate the most aesthetic view while taking a photograph. The experimental results show that the proposed features in the library (92.06% in accuracy) outperform the state-of-the-art rule-specific features (83.63% in accuracy) significantly in the photo aesthetic quality classification task, and the rule-specific features are also proved to be encompassed by the proposed features. Meanwhile, it is observed from experiments that the features extracted for contrast information are more effective than those for absolute information, which is consistent with the properties of human visual systems. Furthermore, the user studies for the view recommendation task confirm that the suggested views are consistent with users' preferences (81.25% agreements).
C1 [Su, Hsiao-Hang; Chen, Tse-Wei; Kao, Chieh-Chi; Hsu, Winston H.; Chien, Shao-Yi] Natl Taiwan Univ, Media IC & Syst Lab, Grad Inst Elect Engn, Taipei 106, Taiwan.
   [Su, Hsiao-Hang; Chen, Tse-Wei; Kao, Chieh-Chi; Hsu, Winston H.; Chien, Shao-Yi] Natl Taiwan Univ, Dept Elect Engn, Taipei 106, Taiwan.
C3 National Taiwan University; National Taiwan University
RP Su, HH (corresponding author), Natl Taiwan Univ, Media IC & Syst Lab, Grad Inst Elect Engn, Taipei 106, Taiwan.
EM sychien@cc.ee.ntu.edu.tw
RI chen, shen-ming/F-1633-2014
OI Chien, Shao-Yi/0000-0002-0634-6294
CR [Anonymous], 2009, P 17 ACM INT C MULT
   [Anonymous], 2006, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
   [Anonymous], 2010, ACM MULTIMEDIA
   Banerjee S, 2007, IEEE T IMAGE PROCESS, V16, P1807, DOI 10.1109/TIP.2007.898992
   Barrett B. T., 2003, INVESTIGAT OPHTHALMO
   Bhattacharya S., 2010, P 18 ACM INT C MULTI, P271
   Chang YY, 2009, IEEE I CONF COMP VIS, P2225, DOI 10.1109/ICCV.2009.5459470
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Darrell T., 1988, Proceedings CVPR '88: The Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.88CH2605-4), P504, DOI 10.1109/CVPR.1988.196282
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Jiang W, 2010, IEEE INT CON MULTI, P920, DOI 10.1109/ICME.2010.5582588
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Khattab K, 2009, EURASIP J EMBED SYST, DOI 10.1155/2009/235032
   Li CC, 2009, IEEE J-STSP, V3, P236, DOI 10.1109/JSTSP.2009.2015077
   Liu LG, 2010, COMPUT GRAPH FORUM, V29, P469, DOI 10.1111/j.1467-8659.2009.01616.x
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   McNamara A, 2001, COMPUT GRAPH FORUM, V20, P211, DOI 10.1111/1467-8659.00550
   Nishiyama M., 2009, P 17 ACM INT C MULTI, P669
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Opelt A, 2006, IEEE T PATTERN ANAL, V28, P416, DOI 10.1109/TPAMI.2006.54
   Oza NC, 2005, IEEE SYS MAN CYBERN, P2340
   Porikli F, 2005, PROC CVPR IEEE, P829, DOI 10.1109/CVPR.2005.188
   Theocharides T, 2004, 17TH INTERNATIONAL CONFERENCE ON VLSI DESIGN, PROCEEDINGS, P133, DOI 10.1109/ICVD.2004.1260915
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Yeh C.-H., 2010, Proceedings of the international conference on Multimedia - MM'10, page, P211
NR 27
TC 46
Z9 52
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 833
EP 843
DI 10.1109/TMM.2012.2186123
PN 2
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700015
DA 2024-07-18
ER

PT J
AU Cheng, XG
   Chia, LT
AF Cheng, Xiangang
   Chia, Liang-Tien
TI Stratification-Based Keyframe Cliques for Effective and Efficient Video
   Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Near-duplicate; quasi-clique; stratification-based method; web video
ID DISTANCE; TEXTURE
AB As there is an exponential increase of web videos, it is time-consuming to get a query result from the tremendous data. An effective and efficient video management system is in urgent need. To increase the efficiency of video retrieval and storage, the most widely used methods are indexing schemes, such as locality sensitive hashing (LSH). However, it is more essential to represent the video itself compactly. In this paper, we propose a strategy to generate stratification-based keyframe cliques (SKCs) for video description, which are more compact and informative than frames or keyframes. The new representations are scalable for different retrieval tasks due to the ranking of SKCs. To further accelerate the retrieval speed, only top SKCs will be used; meanwhile, the searching results are still satisfactory. Experiments are conducted on TRECVID dataset as well as web video dataset. Results show that our proposed SKCs are more succinct and informative for video retrieval and management.
C1 [Cheng, Xiangang; Chia, Liang-Tien] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Cheng, XG (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM xiangang@pmail.ntu.edu.sg
CR Ahanger G, 1996, J VIS COMMUN IMAGE R, V7, P28, DOI 10.1006/jvci.1996.0004
   [Anonymous], P SUMM 1991 US C
   [Anonymous], P 2004 IEEE COMP SOC
   [Anonymous], P 15 ANN ACM INT C M
   [Anonymous], VIDEO RETRIEVAL USIN
   [Anonymous], P INT C MULT INF RET
   [Anonymous], P ACM MULT OCT
   [Anonymous], P 16 ANN ACM INT C M
   [Anonymous], P C STOR RETR MED DA
   [Anonymous], P 16 ANN ACM INT C M
   [Anonymous], INT J COMPUT VIS NOV
   [Anonymous], P IEEE ICCV OCT
   [Anonymous], P IEEE COMP SOC C CV
   [Anonymous], P ACM MULT OCT
   Chua TS, 2002, MULTIMED TOOLS APPL, V16, P79, DOI 10.1023/A:1013293702591
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Han J, 2002, IEEE T IMAGE PROCESS, V11, P944, DOI 10.1109/TIP.2002.801585
   Hsu W., 1995, P 3 ACM INT C MULTIM, P305
   Lazebnik S, 2005, IEEE T PATTERN ANAL, V27, P1265, DOI 10.1109/TPAMI.2005.151
   Li Y., 2005, Digital Image Computing: Techniques and Applications, P39
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Naphade MR, 2000, P SOC PHOTO-OPT INS, V3972, P564
   Qamra A, 2005, IEEE T PATTERN ANAL, V27, P379, DOI 10.1109/TPAMI.2005.54
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Wu XB, 2007, PROCEEDINGS OF THE 2007 INTERNATIONAL CONFERENCE ON AGRICULTURE ENGINEERING, P162
   Zhang HJ, 1997, PATTERN RECOGN, V30, P643, DOI 10.1016/S0031-3203(96)00109-4
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
   Zhao WL, 2007, IEEE T MULTIMEDIA, V9, P1037, DOI 10.1109/TMM.2007.898928
NR 29
TC 4
Z9 5
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2011
VL 13
IS 6
BP 1333
EP 1342
DI 10.1109/TMM.2011.2167222
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 852GD
UT WOS:000297343400013
DA 2024-07-18
ER

PT J
AU Lee, D
   Song, H
AF Lee, Dongju
   Song, Hwangjun
TI A Robust Luby Transform Encoding Pattern-Aware Symbol Packetization
   Algorithm for Video Streaming Over Wireless Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE LT encoding pattern; Luby transform codes; packetization algorithm;
   video streaming; wireless network
ID RATELESS CODES
AB In this paper, we propose a Luby transform encoding pattern-aware symbol packetization algorithm to minimize the quality degradation of video streaming service caused by packet losses over wireless network. To achieve this goal, the relationship among Luby transform encoded symbols is analyzed based on Luby transform encoding pattern, and the proposed packetization algorithm is designed to minimize packet loss effects by reducing the dependency among packets conveying Luby transform encoded symbols. Finally, experimental results are provided to show the performance of the proposed algorithm
C1 [Lee, Dongju; Song, Hwangjun] Pohang Univ Sci & Technol POSTECH, Dept Comp Sci & Engn, Gyungbuk 790784, South Korea.
C3 Pohang University of Science & Technology (POSTECH)
RP Lee, D (corresponding author), Pohang Univ Sci & Technol POSTECH, Dept Comp Sci & Engn, Gyungbuk 790784, South Korea.
EM ldjblaze@postech.ac.kr; hwangjun@postech.ac.kr
FU MKE (The Ministry of Knowledge Economy), Korea, under the ITRC
   (Information Technology Research Center) [NIPA-2011-C1090-1111-0004];
   Ministry of Education, Science and Technology [R31-2010-000-10100-0]
FX This work was supported by the MKE (The Ministry of Knowledge Economy),
   Korea, under the ITRC (Information Technology Research Center) support
   program supervised by the NIPA (National IT Industry Promotion Agency)
   (NIPA-2011-(C1090-1111-0004)) and WCU (World Class University) program
   through the National Research Foundation of Korea funded by the Ministry
   of Education, Science and Technology (R31-2010-000-10100-0). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Zhu Liu.
CR Afzal J., 2006, Journal of Multimedia, V1, DOI 10.4304/jmm.1.5.25-35
   Ahmad S, 2010, IEEE T CIRC SYST VID, V20, P275, DOI 10.1109/TCSVT.2009.2031545
   [Anonymous], H 264 AVC REFERENCE
   [Anonymous], 26346 3GPP TS
   [Anonymous], LTSREPORT2009011 EPF
   [Anonymous], 2002, HPL2002260
   Bogino MCO, 2007, IEEE INT SYMP CIRC S, P3467, DOI 10.1109/ISCAS.2007.378373
   *ETSI EN, 2004, 302304 ETSI EN
   Gasiba T, 2006, IEEE ICC, P5444
   HAN S, 2009, P IEEE CONS COMM NET
   International Telecommunication Union Telecommunication Standardization Sector (ITU-T), 2000, VID COD LOW BIT RAT, V263
   *ISO IEC, 1994, 13818 ISOIEC
   *ISO IEC JTC 1 SC, 2001, N4030 ISOIEC JTC 1SC
   ITU-T and ISO/IEC JTC 1, 2007, ADV VID COD GEN AUD
   Konrad A, 2003, WIREL NETW, V9, P189, DOI 10.1023/A:1022869025953
   Luby M, 2002, ANN IEEE SYMP FOUND, P271, DOI 10.1109/SFCS.2002.1181950
   Luby M., 1998, P 9 ANN ACM SIAM S D, P364
   MacKay DJC, 2005, IEE P-COMMUN, V152, P1062, DOI 10.1049/ip-com:20050237
   Maymounkov P, 2002, Tech. Rep., TR2002-833
   Nguyen TD, 2007, IEEE WRK SIG PRO SYS, P67, DOI 10.1109/SIPS.2007.4387519
   Rahnavard N, 2007, IEEE T INFORM THEORY, V53, P1521, DOI 10.1109/TIT.2007.892814
   Sanghavi S, 2007, 2007 IEEE INFORMATION THEORY WORKSHOP, VOLS 1 AND 2, P478, DOI 10.1109/ITW.2007.4313121
   Sanneck H. A., 2000, P SPIE ACM SIGMM MUL
   Shokrollahi A, 2006, IEEE T INFORM THEORY, V52, P2551, DOI 10.1109/TIT.2006.874390
   Vukobratovic D, 2009, IEEE T MULTIMEDIA, V11, P1094, DOI 10.1109/TMM.2009.2026087
   WOO SS, P 42 ANN C INF SCI S, P568
NR 26
TC 9
Z9 10
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2011
VL 13
IS 4
BP 788
EP 796
DI 10.1109/TMM.2011.2124448
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 805EX
UT WOS:000293710300017
DA 2024-07-18
ER

PT J
AU Li, ZR
   Wong, KH
   Gong, YB
   Chang, MY
AF Li, Zhaorong
   Wong, Kin-Hong
   Gong, Yibo
   Chang, Ming-Yuen
TI An Effective Method for Movable Projector Keystone Correction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Keystone correction; mobile projection; particle filter
AB Keystone correction is an essential operation for projector-based applications, especially in mobile scenarios. In this paper, we propose a handheld movable projection method that can freely project keystone-free content on a general flat surface without adding any markings or boundary on it. Such a projection system can give the user greater freedom of display control (such as viewing angle, distance, etc.), without suffering from keystone distortion. To achieve this, we attach a camera to the projector to form a camera-projector pair. A green frame with the same resolution as the projector screen is projected onto the screen. Particle filter is employed to track the green frame and the correction of the display content is then achieved by rectifying the projection region of interest into a rectangular area. We built a prototype system to validate the effectiveness of the method. Experimental results show that our method can continuously project distortion free content in real time with good performance.
C1 [Li, Zhaorong; Wong, Kin-Hong; Gong, Yibo] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
   [Chang, Ming-Yuen] Chinese Univ Hong Kong, Dept Informat Engn, Shatin, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong; Chinese University of Hong Kong
RP Li, ZR (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
EM zrli@cse.cuhk.edu.hk; khwong@cse.cuhk.edu.hk; ybgong@cse.cuhk.edu.hk;
   mchang@ie.cuhk.edu.hk
FU Faculty of Engineering of the Chinese University of Hong Kong [2050455]
FX Manuscript received August 05, 2010; revised September 30, 2010;
   accepted October 27, 2010. Date of publication November 15, 2010; date
   of current version January 19, 2011. This work was supported by a direct
   grant (Project Code: 2050455) from the Faculty of Engineering of the
   Chinese University of Hong Kong. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Abdulmotaleb El Saddik.
CR Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   HARTLEY, 2004, MULTIPLE VIEW GEOMET
   Leung MC, 2009, PROC CVPR IEEE, P1109, DOI 10.1109/CVPRW.2009.5206658
   Li BX, 2004, IEEE IMAGE PROC, P2829
   Raskar R, 2001, PROC CVPR IEEE, P504
   SUKTHANKAR R, 2001, P IEEE INT C COMP VI
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   LEVMAR LEVMAR LEVENB
NR 8
TC 11
Z9 11
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2011
VL 13
IS 1
BP 155
EP 160
DI 10.1109/TMM.2010.2092421
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 708TJ
UT WOS:000286386900016
DA 2024-07-18
ER

PT J
AU Magnetto, A
   Gaeta, R
   Grangetto, M
   Sereno, M
AF Magnetto, Andrea
   Gaeta, Rossano
   Grangetto, Marco
   Sereno, Matteo
TI TURINstream: A Totally pUsh, Robust, and effIcieNt P2P Video Streaming
   Architecture
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multiple description coding; peer-to-peer; Planet-Lab testbed; push
   architecture; video streaming
AB This paper presents TURINstream, a novel P2P video streaming architecture designed to jointly achieve low delay, robustness to peer churning, limited protocol overhead, and quality-of-service differentiation based on peers cooperation. Separate control and video overlays are maintained by peers organized in clusters that represent sets of collaborating peers. Clusters are created by means of a distributed algorithm and permit the exploitation of the participant nodes upload capacity. The video is conveyed with a push mechanism by exploiting the advantages of multiple description coding. TURINstream design has been optimized through an event driven overlay simulator able to scale up to tens of thousands of peers. A complete prototype of TURINstream has been developed, deployed, and tested on PlanetLab. We tested our prototype under varying degree of peer churn, flash crowd arrivals, sudden massive departures, and limited upload bandwidth resources. TURINstream fulfills our initial design goals, showing low average connection, startup, and playback delays, high continuity index, low control overhead, and effective quality-of-service differentiation in all tested scenarios.
C1 [Magnetto, Andrea; Gaeta, Rossano; Grangetto, Marco; Sereno, Matteo] Univ Turin, Dipartimento Informat, I-10149 Turin, Italy.
   [Gaeta, Rossano; Grangetto, Marco; Sereno, Matteo] Univ Turin, Dept Comp Sci, Turin, Italy.
C3 University of Turin; University of Turin
RP Magnetto, A (corresponding author), Univ Turin, Dipartimento Informat, I-10149 Turin, Italy.
EM magnetto@di.unito.it; gaeta@di.unito.it; grangetto@di.unito.it;
   sereno@di.unito.it
RI Sereno, Matteo/E-3906-2010; Grangetto, Marco/AFM-8024-2022; GAETA,
   Rossano/C-6256-2011
OI Sereno, Matteo/0000-0002-5339-3456; Grangetto,
   Marco/0000-0002-2709-7864; GAETA, Rossano/0000-0002-6521-403X
FU European Community [FP7-ICT-248036]
FX Manuscript received March 10, 2010; revised July 09, 2010; accepted
   September 06, 2010. Date of publication September 20, 2010; date of
   current version November 17, 2010. This work was supported in part by
   the FP7 COAST (FP7-ICT-248036) project, funded by the European
   Community. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Paal Halvorsen.
CR Akyol E, 2007, IEEE J-STSP, V1, P231, DOI 10.1109/JSTSP.2007.901527
   Banerjee S, 2002, ACM SIGCOMM COMP COM, V32, P205, DOI 10.1145/964725.633045
   Bernardini R, 2004, IEEE IMAGE PROC, P3213
   Bonald T, 2008, PERF E R SI, V36, P325, DOI 10.1145/1384529.1375494
   Castro M., 2003, Operating Systems Review, V37, P298, DOI 10.1145/1165389.945474
   Guo Y., 2008, CIVR '08: Proceedings of the 2008 international conference on Content-based image and video retrieval, P655
   Hei XJ, 2008, IEEE COMMUN MAG, V46, P86, DOI 10.1109/MCOM.2008.4473088
   *ISO IEC MPEG JOIN, 2003, H264 ITUT ISOIEC MPE
   Liu Z, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P48
   Magharei N., 2007, P IEEE INFOCOM
   Magharei N, 2009, IEEE ACM T NETWORK, V17, P1052, DOI 10.1109/TNET.2008.2007434
   OOI W, 2005, P ACM SPIE MMCN JAN, P77
   Padmanabhan V.N., 2002, P 12 INT WORKSHOP NE, P177
   Padmanabhan VN, 2003, 11TH IEEE INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, PROCEEDINGS, P16, DOI 10.1109/ICNP.2003.1249753
   *RFC, 2009, 5583 RFC
   *RFC, 2005, 3984 RFC
   SRIPANIDKULCHAI K, 2004, P ACM SIGCOMM, P107, DOI DOI 10.1145/1030194.1015480
   Tillo T, 2008, IEEE T CIRC SYST VID, V18, P59, DOI 10.1109/TCSVT.2007.913751
   Tran DA, 2003, IEEE INFOCOM SER, P1283
   VAISHAMPAYAN VA, 1993, IEEE T INFORM THEORY, V39, P821, DOI 10.1109/18.256491
   Zhang M, 2007, IEEE J SEL AREA COMM, V25, P1678, DOI 10.1109/JSAC.2007.071207
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
   Zhu C, 2009, IEEE T CIRC SYST VID, V19, P511, DOI 10.1109/TCSVT.2009.2013521
   [No title captured]
NR 24
TC 21
Z9 22
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2010
VL 12
IS 8
BP 901
EP 914
DI 10.1109/TMM.2010.2077623
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 681ZW
UT WOS:000284365100010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU de Rooij, O
   Worring, M
AF de Rooij, Ork
   Worring, Marcel
TI Browsing Video Along Multiple Threads
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Conceptual similarity; information visualization; interactive search;
   multidimensional browsing; semantic threads
ID RETRIEVAL; FUSION
AB This paper describes a novel method for browsing a large video collection. It links various forms of related video fragments together as threads. These threads are based on query results, the timeline as well as visual and semantic similarity. We design two interfaces which use threads as the basis for browsing. One interface shows a minimal set of threads, and the other as many as fit on the screen. To evaluate both interfaces we perform a regular user study, a study based on user simulation, and we participated in the interactive video retrieval task of the TRECVID benchmark. The results indicate that the use of threads in interactive video retrieval is beneficial. Furthermore, we found that in general the query result and the timeline are the most important threads, but having several additional threads improves the performance as it encourages people to explore new dimensions.
C1 [de Rooij, Ork; Worring, Marcel] Univ Amsterdam, Intelligent Syst Lab Amsterdam, NL-1098 XG Amsterdam, Netherlands.
C3 University of Amsterdam
RP de Rooij, O (corresponding author), Univ Amsterdam, Intelligent Syst Lab Amsterdam, NL-1098 XG Amsterdam, Netherlands.
EM orooij@uva.nl; worring@uva.nl
RI Worring, Marcel/JRW-7059-2023
OI Worring, Marcel/0000-0003-4097-4136
CR Adcock J, 2005, LECT NOTES COMPUT SC, V3568, P205
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], P INT WORKSH MULT IN
   [Anonymous], P 13 ACM INT C MULT
   BARECKE T, 2006, P 2006 INT C CONT BA, P340
   Cheng PCH, 1998, TOTAL QUAL MANAGE, V9, P3, DOI 10.1080/0954412989234
   Christel MG, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P1032
   HAUPTMANN A, 2004, P ACM MULT NEW YORK, P668
   Hauptmann A.G., 2006, Proceedings of the 14th annual ACM international conference on Multimedia, P385, DOI DOI 10.1145/1180639.1180721
   Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150
   HEESCH D, 2004, P 15 INT C MULT NEW
   Hsu WH, 2007, IEEE MULTIMEDIA, V14, P14, DOI 10.1109/MMUL.2007.61
   KENDER JR, 2005, P IEEE ICME AMST NET
   Kennedy L, 2008, P IEEE, V96, P567, DOI 10.1109/JPROC.2008.916345
   Liu Jingjing., 2007, MULTIMEDIA 07, P208
   LUAN HB, 2007, MULTIMEDIA, P293
   Luo HZ, 2007, IEEE CONF VIS ANAL, P107
   McDonald K, 2005, LECT NOTES COMPUT SC, V3568, P61
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Naphade MR, 2001, IEEE T MULTIMEDIA, V3, P141, DOI 10.1109/6046.909601
   Neo SY, 2006, LECT NOTES COMPUT SC, V4071, P143
   NGUYEN GP, 2008, P ACM TOMCCAP, V4
   PHILBIN J, 2007, P NIST TRECVID WORKS
   RAUTIANEN M, 2004, P IEEE ICME TAIP TAI
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Snoek CGM, 2007, IEEE T MULTIMEDIA, V9, P280, DOI 10.1109/TMM.2006.886275
   Snoek CGM, 2006, IEEE T PATTERN ANAL, V28, P1678, DOI 10.1109/TPAMI.2006.212
   VANGEMERT JC, 2006, P SLAM WORKSH CONJ C
   Zavesky E., Proceedings of the 2008 International Conference on Content-based Image and Video Retrieval, ser. CIVR '08. New York, NY, USA: ACM, P617, DOI [10.1145/1386352.1386442, DOI 10.1145/1386352.1386442]
NR 29
TC 13
Z9 13
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2010
VL 12
IS 2
BP 121
EP 130
DI 10.1109/TMM.2009.2037388
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 573OA
UT WOS:000275922000003
DA 2024-07-18
ER

PT J
AU Ahmad, S
   Hamzaoui, R
   Al-Akaidi, M
AF Ahmad, Shakeel
   Hamzaoui, Raouf
   Al-Akaidi, Marwan
TI Optimal Packet Loss Protection of Progressively Compressed 3-D Meshes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D graphics; multimedia communication; progressive transmission;
   unequal error protectio
ID ERROR PROTECTION; TRANSMISSION; MODELS
AB We consider a state-of-the-art system that uses layered source coding and forward error correction with Reed-Solomon codes to efficiently transmit 3-D meshes over lossy packet networks. Given a transmission bit budget, the performance of this system can be optimized by determining how many layers should be sent, how each layer should be packetized, and how many parity bits should be allocated to each layer such that the expected distortion at the receiver is minimum. The previous solution for this optimization problem uses exhaustive search, which is not feasible when the transmission bit budget is large. We propose instead an exact algorithm that solves this optimization problem in linear time and space. We illustrate the advantages of our approach by providing experimental results for the compressed progressive meshes (CPM) mesh compression technique.
C1 [Ahmad, Shakeel; Hamzaoui, Raouf; Al-Akaidi, Marwan] De Montfort Univ, Dept Engn, Fac Technol, Leicester LE1 9BH, Leics, England.
C3 De Montfort University
RP Ahmad, S (corresponding author), De Montfort Univ, Dept Engn, Fac Technol, Gateway, Leicester LE1 9BH, Leics, England.
EM sahmad@dmu.ac.uk; rhamzaoui@dmu.ac.uk; mma@dmu.ac.uk
FU DFG Research Training Group [GK-1042]
FX Manuscript received July 02, 2007; January 16, 2009. First published
   August 18, 2009; current version published October 16, 2009. This paper
   was presented in part at ICME-06, IEEE International Conference on
   Multimedia and Expo, Toronto, ON, Canada, Jul. 2006. This work was
   supported by the DFG Research Training Group GK-1042. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Xiaolin Wu.
CR AHMAD S, 222 U KONST
   Al-Regib G, 2005, IEEE T CIRC SYST VID, V15, P256, DOI 10.1109/TCSVT.2004.841638
   AlRegib G, 2005, IEEE T MULTIMEDIA, V7, P766, DOI 10.1109/TMM.2005.850981
   Alregib G, 2005, ACM T GRAPHIC, V24, P182, DOI 10.1145/1061347.1061349
   BICI MO, 2007, P IEEE ICIP 07 SAN A, V5, P121
   BISCHOFF S, 2002, P IEEE ICME 2002 LAU
   Chen Z., 2003, Proc. Web3D, P161
   Chen ZH, 2005, MULTIMEDIA SYST, V10, P230, DOI 10.1007/s00530-004-0154-3
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   DUMITRESCU S, 2008, P IEEE ICME 2005 AMS, P900
   Garland M., 1997, PROC 24 C COMPUTER G, P209, DOI DOI 10.1145/258734.258849
   Hamzaoui R, 2005, IEEE SIGNAL PROC MAG, V22, P91, DOI 10.1109/MSP.2005.1550192
   Horn U, 1999, SIGNAL PROCESS-IMAGE, V15, P77, DOI 10.1016/S0923-5965(99)00025-9
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   Lopes H, 2003, COMPUT GRAPH-UK, V27, P553, DOI 10.1016/S0097-8493(03)00102-X
   Mohr AE, 2000, IEEE J SEL AREA COMM, V18, P819, DOI 10.1109/49.848236
   Pajarola R, 2000, IEEE T VIS COMPUT GR, V6, P79, DOI 10.1109/2945.841122
   Thie J, 2004, EURASIP J APPL SIG P, V2004, P207, DOI 10.1155/S1110865704308024
   Tian DT, 2006, SIGNAL PROCESS-IMAGE, V21, P396, DOI 10.1016/j.image.2006.01.003
   Yan ZD, 2005, IEEE T CIRC SYST VID, V15, P138, DOI 10.1109/TCSVT.2004.837023
   Yan ZD, 2001, IEEE T CIRC SYST VID, V11, P860, DOI 10.1109/76.931112
NR 21
TC 7
Z9 8
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2009
VL 11
IS 7
BP 1381
EP 1387
DI 10.1109/TMM.2009.2030546
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 506GB
UT WOS:000270761300016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Su, YF
   Yang, YH
   Lu, MT
   Chen, HH
AF Su, Ya-Fan
   Yang, Yi-Hsuan
   Lu, Meng-Ting
   Chen, Homer H.
TI Smooth Control of Adaptive Media Playout for Video Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive media playout; error-prone channels; media buffering; video
   streaming
ID DESIGN; AUDIO; VOICE
AB Client-side data buffering is a common technique to deal with media playout interruptions of streaming video caused by network jitters and packet losses of best-effort networks. However, stronger playout interruption protection inevitably amounts to larger data buffering and results in more memory requirements and longer playout delay. Adaptive media playout (AMP), also a client-side technique, can reduce the buffer requirement and avoid buffer outage but at the expense of visual quality degradation because of the fluctuation of playout speed. In this paper, we propose a novel AMP scheme to keep the video playout as smooth as possible while adapting to the channel condition. The triggering of the playout control is based on buffer variation rather than buffer fullness. Experimental results show that our AMP scheme surpasses conventional schemes in unfriendly network conditions. Unlike previous schemes that are tuned for a specific range of packet loss and network instability, the proposed AMP scheme maintains consistent performance across a wide range of network conditions.
C1 [Su, Ya-Fan; Chen, Homer H.] Natl Taiwan Univ, Grad Inst Commun Engn, Dept Elect Engn, Taipei 10617, Taiwan.
   [Chen, Homer H.] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10617, Taiwan.
C3 National Taiwan University; National Taiwan University
RP Su, YF (corresponding author), Natl Taiwan Univ, Grad Inst Commun Engn, Dept Elect Engn, Taipei 10617, Taiwan.
EM b92901017@ntu.edu.tw; affige@gmail.com; b88901124@ntu.edu.tw;
   homer@cc.ee.ntu.edu.tw
OI CHEN, HSIN-HSI/0000-0001-9757-9423; Chen, Homer/0000-0002-8795-1911
FU National Science Council of Taiwan [NSC 96-2219-E-002-003, NSC
   95-2219-E-002-012]
FX Manuscript received July 26, 2008; revised February 20, 2009. First
   published August 18, 2009; current version published October 16, 2009.
   This work was supported in part by grants from the National Science
   Council of Taiwan under contracts NSC 96-2219-E-002-003 and NSC
   95-2219-E-002-012. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Thinh Nguyen.
CR CHUAN HC, 2005, P IEEE ISCAS MAY
   CHUAN HC, 2004, P VEH TECHN C
   Chuang HC, 2007, IEEE T MULTIMEDIA, V9, P1273, DOI 10.1109/TMM.2007.902884
   CLARK A, P IPTEL 2001 WORKSH
   CLAYPOOL M, 1999, P ACM INT C MULT
   DESHPANDE S, 2007, P SPIE JAN
   Deshpande S.G., 2008, Proceedings of ACM Multimedia, P777
   DUA A, 2007, P IEEE GLOBECOM
   GILBERT EN, 1960, BELL SYST TECH J, V39, P1253, DOI 10.1002/j.1538-7305.1960.tb03959.x
   Howard R.A., 1971, SEMIMARKOV DECISION, V1
   Janssen J, 2002, MOBILE NETW APPL, V7, P79, DOI 10.1023/A:1013237731120
   Kalman M, 2004, IEEE T CIRC SYST VID, V14, P841, DOI 10.1109/TCSVT.2004.828335
   LAOUTARIS N, 2001, P IEEE ICC 2001 HELS
   Li Y, 2008, IEEE T MULTIMEDIA, V10, P885, DOI 10.1109/TMM.2008.922860
   LIANG YJ, 2001, P ICASSP SALT LAK CI
   Markopoulou AP, 2003, IEEE ACM T NETWORK, V11, P747, DOI 10.1109/TNET.2003.818179
   MODIANO E, 1999, WIRELESS NETW SPRING, V14, P279
   MOON SB, 1998, P ACM MULT SYST
   Narbutt M, 2005, IEEE INTERNET COMPUT, V9, P28, DOI 10.1109/MIC.2005.72
   Ozden B, 1996, MULTIMEDIA SYST, V4, P40, DOI 10.1007/s005300050011
   Park S, 2008, J VIS COMMUN IMAGE R, V19, P106, DOI 10.1016/j.jvcir.2007.09.002
   Podolsky MG, 2001, J VLSI SIG PROC SYST, V27, P81, DOI 10.1023/A:1008123631453
   Roccetti M, 2001, MULTIMED TOOLS APPL, V14, P23, DOI 10.1023/A:1011303506685
   Ross S.M, 1983, STOCHASTIC PROCESSES
   Roychoudhuri L, 2006, COMPUT COMMUN, V29, P1578, DOI 10.1016/j.comcom.2006.04.004
   STEINBACH E, 2001, P IEEE INT C IM PROC
   Stockhammer T, 2004, IEEE T MULTIMEDIA, V6, P268, DOI 10.1109/TMM.2003.822795
   Tao DY, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1798
   van der Wal K, 1997, P IEEE, V85, P1947, DOI 10.1109/5.650177
   Yang YH, 2006, IEEE INT SYM MULTIM, P415
   Yuang MC, 1997, IEEE J SEL AREA COMM, V15, P136, DOI 10.1109/49.552064
   Yuang MC, 1996, 1996 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS - CONVERGING TECHNOLOGIES FOR TOMORROW'S APPLICATIONS, VOLS. 1-3, P1365, DOI 10.1109/ICC.1996.533632
NR 32
TC 46
Z9 49
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2009
VL 11
IS 7
BP 1331
EP 1339
DI 10.1109/TMM.2009.2030543
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 506GB
UT WOS:000270761300011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Dai, M
   Zhang, YP
   Loguinov, D
AF Dai, Min
   Zhang, Yueping
   Loguinov, Dmitri
TI A Unified Traffic Model for MPEG-4 and H.264 Video Traces
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Intra-GOP correlation; inter-GOP correlation; long-range dependency;
   short-range dependency; video traffic modeling; wavelet
ID GAMMA-BASED FRAMEWORK
AB This paper presents a frame-level hybrid framework for modeling MPEG-4 and H. 264 multi-layer variable bit rate (VBR) video traffic. To accurately capture long-range-dependent and short-range-dependent properties of VBR sequences, we use wavelets to model the distribution of I-frame sizes and a simple time-domain model for P/B frame sizes. However, unlike previous studies, we analyze and successfully model both inter-GOP (group of pictures) and intra-GOP correlation in VBR video and build an enhancement-layer model using cross-layer correlation. Simulation results demonstrate that our model effectively preserves the temporal burstiness and captures important statistical features (e. g., the autocorrelation function and the frame-size distribution) of original traffic. We also show that our model possesses lower complexity and has better performance than the previous methods in both single-and multi-layer sequences.
C1 [Dai, Min] Qualcomm Inc, San Diego, CA 92121 USA.
   [Zhang, Yueping] NEC Labs Amer Inc, Princeton, NJ 08540 USA.
   [Loguinov, Dmitri] Texas A&M Univ, College Stn, TX 77843 USA.
C3 Qualcomm; NEC Corporation; Texas A&M University System; Texas A&M
   University College Station
RP Dai, M (corresponding author), Qualcomm Inc, San Diego, CA 92121 USA.
EM mdai@qualcomm.com; yueping@nec-labs.com; dmitri@cs.tamu.edu
RI zhang, yue/GQP-5565-2022; he, shun/JJD-0182-2023
CR [Anonymous], 2003, HDB HEAVY TAILED DIS
   [Anonymous], 2000, SELF SIMILAR NETWORK, DOI DOI 10.1002/047120644X
   [Anonymous], FUNDAMENTALS WAVELET
   CAPPE O, 2005, STATISTICS
   CHANG MK, 1999, GUOCE ZHUANKAN, V7, P3
   CHEN TPC, 2002, PACKET VIDEO     APR
   CORTE AL, 1991, SIGNAL PROCESS-IMAGE, V3, P167
   DAI M, 2005, P IEEE INFOCOM MAR
   DAI M, 2008, UNIFIED TRAFFIC MODE
   Embrechts P., 2002, CORRELATION DEPENDEN
   Fitzek F., MPEG 4 H 263 VIDEO T
   Frey M, 2000, IEEE ACM T NETWORK, V8, P710, DOI 10.1109/90.893868
   GARRETT MW, 1994, P ACM SIGCOMM AUG
   Heyman DP, 1992, IEEE T CIRC SYST VID, V2, P49, DOI 10.1109/76.134371
   Heyman DP, 1997, IEEE ACM T NETWORK, V5, P554, DOI 10.1109/90.649513
   HUANG C, 1995, P ACM SIGCOMM AUG
   ISMAIL MR, 1995, P IEEE INFOCOM APR
   KRUNZ M, 1997, P ACM SIGMETRICS JUN, V25
   Krunz MM, 1998, IEEE J SEL AREA COMM, V16, P733, DOI 10.1109/49.700909
   Liu DR, 2001, IEEE T CIRC SYST VID, V11, P169, DOI 10.1109/76.905983
   Lombardo A, 2001, EUR T TELECOMMUN, V12, P127, DOI 10.1002/ett.4460120207
   LOMBARDO A, 1998, P IEEE INFOCOM MAR
   Ma S, 2001, IEEE ACM T NETWORK, V9, P634, DOI 10.1109/90.958331
   MAGLARIS B, 1988, IEEE T COMMUN, V36, P834, DOI 10.1109/26.2812
   Melamed B, 1998, IEEE J SEL AREA COMM, V16, P600, DOI 10.1109/49.700899
   Radha HM, 2001, IEEE T MULTIMEDIA, V3, P53, DOI 10.1109/6046.966110
   RAMAMURTHY G, 1992, P IEEE INFOCOM FLOR
   Reisslein M., VIDEO TRACES NETWORK
   RIBEIRO VJ, 2000, P IEEE INFOCOM MAR
   Rose O, 1997, PERFORM EVALUATION, V30, P69, DOI 10.1016/S0166-5316(96)00054-5
   ROSE O, 1995, P 20 ANN C LOC COMP
   Sarkar UK, 2003, IEEE ACM T NETWORK, V11, P638, DOI 10.1109/TNET.2003.815292
   Wang Y, 2005, P IEEE, V93, P57, DOI 10.1109/JPROC.2004.839618
   Wang Y., 2001, VIDEO PROCESSING COM
   WORNELL GW, 1996, SIGNAL PROCESSING WI
   ZHAO JA, 2003, PACKET VIDEO     APR
NR 36
TC 44
Z9 48
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2009
VL 11
IS 5
BP 1010
EP 1023
DI 10.1109/TMM.2009.2021802
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 474KN
UT WOS:000268282300018
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Guo, YW
   Liu, F
   Shi, J
   Zhou, ZH
   Gleicher, M
AF Guo, Yanwen
   Liu, Feng
   Shi, Jian
   Zhou, Zhi-Hua
   Gleicher, Michael
TI Image Retargeting Using Mesh Parametrization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention model; image retargeting; mesh parametrization
ID VISUAL-ATTENTION; VIDEO; MODEL
AB Image retargeting aims to adapt images to displays of small sizes and different aspect ratios. Effective retargeting requires emphasizing the important content while retaining surrounding context with minimal visual distortion. In this paper, we present such an effective image retargeting method using saliency-based mesh parametrization. Our method first constructs a mesh image representation that is consistent with the underlying image structures. Such a mesh representation enables easy preservation of image structures during retargeting since it captures underlying image structures. Based on this mesh representation, we formulate the problem of retargeting an image to a desired size as a constrained image mesh parametrization problem that aims at finding a homomorphous target mesh with desired size. Specifically, to emphasize salient objects and minimize visual distortion, we associate image saliency into the image mesh and regard image structure as constraints for mesh parametrization. Through a stretch-based mesh parametrization process we obtain the homomorphous target mesh, which is then used to render the target image by texture mapping. The effectiveness of our algorithm is demonstrated by experiments.
C1 [Guo, Yanwen; Shi, Jian; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210093, Peoples R China.
   [Liu, Feng; Gleicher, Michael] Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA.
C3 Nanjing University; University of Wisconsin System; University of
   Wisconsin Madison
RP Guo, YW (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210093, Peoples R China.
EM ywguo@nju.edu.cn; fliu@cs.wisc.edu; shilenjian@gmail.com;
   zhouzh@nju.edu.cn; gleicher@cs.wisc.edu
FU National Natural Science Foundation of China [60703084, 60723003,
   60635030, 60721002]; NSF [IIS-0416284]; Jiangsu Science Foundation
   [BK2008018]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 60703084, 60723003, 60635030, and
   60721002; in part by the NSF under grant IIS-0416284; in part by the
   Jiangsu Science Foundation (BK2008018); and in part by the Jiangsu 333
   High-Level Talent Cultivation Program.
CR Altunbasak Y, 1997, IEEE T IMAGE PROCESS, V6, P1255, DOI 10.1109/83.623189
   [Anonymous], 2005, P 18 ANN ACM S US IN
   [Anonymous], 2007, ACM T GRAPH
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], 2003, P 11 ACM INT C MULTI, DOI DOI 10.1145/957013.957094
   Chen LQ, 2003, MULTIMEDIA SYST, V9, P353, DOI 10.1007/s00530-003-0105-4
   Cheng WH, 2007, IEEE T CIRC SYST VID, V17, P43, DOI 10.1109/TCSVT.2006.885717
   Fan Xin., 2003, P ACM MULTIMEDIA, P247
   Gal R., 2006, P 17 EUROGRAPHICS C, P297, DOI DOI 10.2312/EGWR/EGSR06/297-303
   Hearn Donald, 2004, Computer Graphics with Open GL
   Hu Y., 2005, Proceedings of the ACM International Conference on Multimedia, P716
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Li Y, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P269
   Liu Feng., 2006, ACM MULTIMEDIA 2006, P241, DOI DOI 10.1145/1180639.1180702
   Liu Hao., 2003, P ACM INT C MULTIMED, P148
   MAILLOT J, 1993, P SIGGRAPH 93, P27, DOI DOI 10.1145/166117.166120
   Press W. H., 2002, Numerical Recipes in C: the Art of Scientific Computing, V2nd ed., DOI DOI 10.1119/1.14981
   Rui Y., 2001, P 9 ACM INT C MULTIM, P2, DOI DOI 10.1145/500141.500145
   SANTELLA A, 2006, P CHI
   SEIDEL R, 1988, 260 I INF PROC
   Setlur V., 2005, 4 INT C MOB UB MULT, P59
   Setlur V, 2007, IEEE COMPUT GRAPH, V27, P80, DOI 10.1109/MCG.2007.133
   Suh Bongwon, 2003, P ACM S US INT SOFTW, P95, DOI DOI 10.1145/964696.964707
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang J, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P1711
   Wang J, 2006, LECT NOTES COMPUT SC, V4035, P385
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Wolf L., 2007, Computer Vision and Pattern Recognition, P1
   Zhou K., 2004, P EUR ICS ACM SIGGRA, P45
NR 30
TC 153
Z9 178
U1 0
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2009
VL 11
IS 5
BP 856
EP 867
DI 10.1109/TMM.2009.2021781
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 474KN
UT WOS:000268282300006
DA 2024-07-18
ER

PT J
AU Arifin, S
   Cheung, PYK
AF Arifin, Sutjipto
   Cheung, Peter Y. K.
TI Affective Level Video Segmentation by Utilizing the
   Pleasure-Arousal-Dominance Information
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Affective video content analysis; Bayesian networks; emotion
   recognition; video segmentation
ID EMOTION
AB In this paper, we offer an entirely new view to the problem of high level video parsing. We developed a novel computation method for affective level video segmentation. Its function was to extract emotional segments from videos. Its design was based on the pleasure-arousal-dominance (P-A-D) model of affect representation [1], which in principle can represent a large number of emotions. Our method had two stages. The first P-A-D estimation stage was defined within framework of the dynamic bayesian networks (DBNs). A spectral clustering algorithm was applied in the final stage to determine the emotional segments of the video. The performance of our method was compared with the time adaptive clustering (TAC) algorithm and an accelerated version of it which we had developed. According to Vendrig et al. [2], the TAC algorithm was the best segmentation method. Experiment results will show the feasibility of our method.
C1 [Arifin, Sutjipto; Cheung, Peter Y. K.] Univ London Imperial Coll Sci Technol & Med, Dept Elect & Elect Engn, Circuits & Syst Grp, London SW7 2BT, England.
C3 Imperial College London
RP Arifin, S (corresponding author), Univ London Imperial Coll Sci Technol & Med, Dept Elect & Elect Engn, Circuits & Syst Grp, London SW7 2BT, England.
EM sutjipto.airifin@imperial.ac.uk; p.cheung@imperial.ac.uk
CR [Anonymous], INT C DAT MIN
   [Anonymous], ACM MULTIMEDIA
   [Anonymous], P COGN TECHN C
   [Anonymous], 1991, Grammar of the film language
   BOLLE RM, 2002, IBM J RES DEV, V42
   Breazeal C, 2003, INT J HUM-COMPUT ST, V59, P119, DOI 10.1016/S1071-5819(03)00018-1
   Chang SF, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P531, DOI 10.1109/ICIP.1998.727321
   Chiu P, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1329, DOI 10.1109/ICME.2000.871011
   COWIE R, IEEE SIGNAL PROCESS, V18
   Davitz J.R., 1969, The language of emotion
   FERMAN AM, 1999, IEEE INT C IM PROC O
   GROSS JJ, 1995, COGNITION EMOTION, V9, P87, DOI 10.1080/02699939508408966
   HAGEN L, 1992, IEEE T COMPUT AID D, V11, P1074, DOI 10.1109/43.159993
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P580, DOI 10.1109/76.767124
   HUANG Q, 2005, IEEE INT C AC SPEECH
   JAIMES A, 2001, IEEE INT C IM PROC, V1
   Jaimes Alejandro., 2005, 2005 IEEE INT C MULT, P1412
   Kang H.-B., 2003, Proceedings of the 11th ACM International Conference on Multimedia, P259
   KANG HB, ACM INT C MULT
   KENDER J, IEEE C COMPUT VISION
   KWON YM, 2000, IEEE INT C MULT EXP, V2, P773
   LANG A, 1938, J BROADCAST ELECT ME, V38
   LANG A, J BROADCAST ELECT ME, V40
   LANG P, 1995, NETWORK MODEL EMOTIO
   LIENHART R, INT C MULT SYST
   MONCRIEFF S, 2001, ACM INT C MULT, V9
   Murphy Kevin Patrick, 2002, THESIS U C BERKELEY
   Naphade MR, 2001, IEEE T MULTIMEDIA, V3, P141, DOI 10.1109/6046.909601
   Naphade MR, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P536, DOI 10.1109/ICIP.1998.999041
   ORTONY A, PSYCHOL REV, V97
   Osgood C. E., 1967, MEASUREMENT MEANING
   Paskin MarkA., SHORT COURSE GRAPHIC
   Picard R. W., 1997, AFFECTIVE COMPUTING
   Qian R., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P200, DOI 10.1109/CVPR.1999.786939
   RUI Y, MULTIMEDIA SYST, V7
   RUSSELL JA, 1977, J RES PERS, V11, P273, DOI 10.1016/0092-6566(77)90037-X
   SCHLOSBERG H, PSYCHOL REV, V61
   SHAVER P, J PERSONALITY SOCIAL, V52
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   SUNDARAM H, 2000, ACM INT C MULT
   Vendrig J, 2002, IEEE T MULTIMEDIA, V4, P492, DOI 10.1109/TMM.2002.802021
   VONLUXBURG U, 2006, TR149 M PLANCK I BIO, P1
   WOLF W, IEEE INT C AC SPEECH
   XU M, 2005, IEEE INT C MULT EXP
   Yeung M, 1998, COMPUT VIS IMAGE UND, V71, P94, DOI 10.1006/cviu.1997.0628
   Zhang T, 2000, PROC SPIE, V3972, P506
   2001, CLASSIFICATION ETHNI
NR 48
TC 35
Z9 40
U1 1
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2008
VL 10
IS 7
BP 1325
EP 1341
DI 10.1109/TMM.2008.2004911
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 378HB
UT WOS:000261310700010
DA 2024-07-18
ER

PT J
AU Tang, H
   Fu, Y
   Tu, JL
   Hasegawa-Johnson, M
   Huang, TS
AF Tang, Hao
   Fu, Yun
   Tu, Jilin
   Hasegawa-Johnson, Mark
   Huang, Thomas S.
TI Humanoid Audio-Visual Avatar With Emotive Text-to-Speech Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio-visual avatar; emotive speech synthesis; human-computer
   interaction; multimodal system; 3-D face modeling and animation; TTS
ID DRIVEN; FACE
AB Emotive audio-visual avatars are virtual computer agents which have the potential of improving the quality of human-machine interaction and human-human communication significantly. However, the understanding of human communication has not yet advanced to the point where it is possible to make realistic avatars that demonstrate interactions with natural-sounding emotive speech and realistic-looking emotional facial expressions. In this paper, We propose the various technical approaches of a novel multimodal framework leading to a text-driven emotive audio-visual avatar. Our primary work is focused on emotive speech synthesis, realistic emotional facial expression animation, and the co-articulation between speech gestures (i.e., lip movements) and facial expressions. A general framework of emotive text-to-speech (TTS) synthesis using a diphone synthesizer is designed and integrated into a generic 3-D avatar face model. Under the guidance of this framework, we therefore developed a realistic 3-D avatar prototype. A rule-based emotive TTS synthesis system module based on the Festival-MBROLA architecture has been designed to demonstrate the effectiveness of the framework design. Subjective listening experiments were carried out to evaluate the expressiveness of the synthetic talking avatar.
C1 [Tang, Hao; Fu, Yun; Tu, Jilin; Hasegawa-Johnson, Mark; Huang, Thomas S.] Univ Illinois, Beckman Inst Adv Sci & Technol, Urbana, IL 61801 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign
RP Tang, H (corresponding author), Univ Illinois, Beckman Inst Adv Sci & Technol, Urbana, IL 61801 USA.
EM haotang2@uiuc.edu; yunfu2@ifp.uiuc.edu; jilintu@uiuc.edu;
   jhasegaw@uiuc.edu; t-huang1@uiuc.edu
RI yan, shuicheng/A-8531-2014; yan, shuicheng/HCH-9860-2022
OI yan, shuicheng/0000-0001-8906-3777; yan, shuicheng/0000-0003-4527-1018
CR [Anonymous], 3D FACE PROCESSING M
   [Anonymous], Environmental Psychology & Nonverbal Behavior
   [Anonymous], P INT C PHON SCI ICP
   [Anonymous], NSF PLANN WORKSH FAC
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   BURKHARDT F, 2005, P INTERSPEECH 2005 L, P509
   BURKHARDT F, 2000, THESIS TU BERLIN GER
   CAHN JE, 1989, THESIS MIT MEDIA LAB
   CAO Y, 2005, ACM T GRAPH, V24
   Choi KH, 2004, IEEE T SIGNAL PROCES, V52, P1783, DOI 10.1109/TSP.2004.827153
   Dutoit Thierry., 1997, INTRO TEXT TO SPEECH
   FU Y, 2008, IEEE T CIRC IN PRESS
   Fu Y, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P991
   Fu Y, 2006, IEEE T CIRC SYST VID, V16, P830, DOI 10.1109/TCSVT.2006.877398
   HOFER G, 2004, THESIS U EDINBURGH E
   Hong P., 2001, INT J IMAGE GRAPH, V1, P1
   Hong PY, 2002, IEEE T NEURAL NETWOR, V13, P916, DOI 10.1109/TNN.2002.1021892
   IIDA A, 2002, THESIS U KEIO TOKYO
   Ishii H., 1999, CHI'99 Proceedings, P394, DOI [10.1145/ 302979.303115, DOI 10.1145/302979.303115]
   Jaimes A., 2006, ACM INT C MULTIMEDIA, P855
   Kshirsagar S, 2001, COMPUTER GRAPHICS INTERNATIONAL 2001, PROCEEDINGS, P38, DOI 10.1109/CGI.2001.934656
   Ma JY, 2006, IEEE T VIS COMPUT GR, V12, P266, DOI 10.1109/TVCG.2006.18
   Mancini M, 2007, IEEE T AUDIO SPEECH, V15, P1833, DOI 10.1109/TASL.2007.899256
   MEHRABIAN A, 1968, PSYCHOL TODAY, V2, P53
   MONTERO JM, 1999, P EUR 99 BUD HUNG
   MURRAY R, 1989, THESIS U DUNDEE DUND
   Parke FrederickI., 1972, Proceedings of the ACM annual conference, V1, P451
   Pitrelli JF, 2006, IEEE T AUDIO SPEECH, V14, P1099, DOI 10.1109/TASL.2006.876123
   SCHRODER M, 2004, PHONUS, V4, P37
   SCHRODER M, 2004, PHONUS, V7
   TANG H, 2008, 2008 IEEE WORKSH APP
   Tao H., 1999, IEEE C COMPUTER VISI, V1, P611
   FESTIVAL PROJECT
   MBROLA PROJECT
   2002, MPEG 4 FACIAL ANIMAT
NR 35
TC 18
Z9 24
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 969
EP 981
DI 10.1109/TMM.2008.2001355
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600003
DA 2024-07-18
ER

PT J
AU Ortiz, JPG
   Ruiz, VG
   Lopez, MF
   García, I
AF Garcia Ortiz, Juan Pablo
   Gonzalez Ruiz, Vicente
   Francisco Lopez, Manuel
   Garcia, Inmaculada
TI Interactive transmission of JPEG2000 images using web proxy caching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE JPEG2000; JPIP; progressive image transmission; remote browsing of
   images; web caching
ID COMPRESSION; PERFORMANCE; INTERNET; STANDARD
AB This paper describes and analyzes JPIP-W, an innovative proposal for the interactive transmission of JPEG2000 images on the Internet. JPIP-W is an extension of JPIP, the current JPEG protocol proposed for interactive JPEG2000 image browsing. One of the JPIP characteristics of greatest interest is its ability to use the Web for retrieving images. However, JPIP is unable to exploit the large infrastructure of today's Web caching systems (proxies), used to reduce response time and network traffic. To overcome this drawback, JPIP-W defines a new server-client interaction consisting of splitting JPEG2000 images into data blocks that can be cached by the proxies. These blocks can be shared among several clients, allowing fast recovery of some portions of the images. Experimental results demonstrate that JPIP-W significantly reduces,JPIP retrieving times.
C1 [Garcia Ortiz, Juan Pablo; Gonzalez Ruiz, Vicente; Francisco Lopez, Manuel; Garcia, Inmaculada] Univ Almeria, Comp Architecture & Elect Dept, Almeria 04120, Spain.
C3 Universidad de Almeria
RP Ortiz, JPG (corresponding author), Univ Almeria, Comp Architecture & Elect Dept, Almeria 04120, Spain.
EM jportiz@ace.ual.es; vruiz@ual.es; mflopez@ace.ual.es; inma@ace.ual.es
RI Ruiz, Vicente Gonzalez/G-9269-2015; Fernandez, Inmaculada
   Garcia/B-8432-2011
OI Ruiz, Vicente Gonzalez/0000-0001-6495-4856; Garcia Fernandez,
   Inmaculada/0000-0002-1138-2118
FU Spanish Ministry of Education and Science [TIN2005-00447]; Consejeria de
   Innovacion, Ciencia y Empresa, Junta de Andalucia [P06-TIC-01426]
FX This work was supported in part by Grants TIN2005-00447 (Spanish
   Ministry of Education and Science) and P06-TIC-01426 (Consejeria de
   Innovacion, Ciencia y Empresa, Junta de Andalucia).
CR [Anonymous], 1544412004 ISOIEC
   [Anonymous], 2002, JPEG2000: Image Compression Fundamentals, Standards, and Practice
   DESHPANDE S, 2001, ACM MULTIMEDIA, P372
   Fielding R., 1999, 2616 RFC
   ISO/IEC, 2005, 1544492005 ISOIEC
   *ISO IEC, 2004, 1264022004 ISOIEC
   *KAK, COMPR FRAM JPEG2000
   LIANG ST, 2002, IEEE INT C MULT EXP
   Luotonen Ari., 1997, WEB PROXY SERVERS, Vfirst
   Politou EA, 2004, IEEE T IMAGE PROCESS, V13, P293, DOI 10.1109/TIP.2003.821348
   Taubman D, 2000, IEEE T IMAGE PROCESS, V9, P1158, DOI 10.1109/83.847830
   Taubman D, 2003, PROC SPIE, V5150, P791, DOI 10.1117/12.502889
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang JZ, 2007, IEEE T MULTIMEDIA, V9, P147, DOI 10.1109/TMM.2006.886379
   Zeng D, 2004, IEEE T SYST MAN CY C, V34, P270, DOI 10.1109/TSMCC.2004.829261
NR 15
TC 10
Z9 13
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2008
VL 10
IS 4
BP 629
EP 636
DI 10.1109/TMM.2008.921738
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EK
UT WOS:000258767200008
DA 2024-07-18
ER

PT J
AU Zeng, ZH
   Tu, JL
   Pianfetti, BM
   Huang, TS
AF Zeng, Zhihong
   Tu, Jilin
   Pianfetti, Brian M., Jr.
   Huang, Thomas S.
TI Audio-visual affective expression recognition through multistream fused
   HMM
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE affective computing; affect recognition; emotion recognition;
   human-computer interaction; human computing; multimodal fusion
ID FACIAL EXPRESSIONS
AB Advances in computer processing power and emerging algorithms are allowing new ways of envisioning Human-Computer Interaction. Although the benefit of audio-visual fusion is expected for affect recognition from the psychological and engineering perspectives, most of existing approaches to automatic human affect analysis are unimodal: information processed by computer system is limited to either face images or the speech signals. This paper focuses on the development of a computing algorithm that uses both audio and visual sensors to detect and track a user's affective state to aid computer decision making. Using our Multistream Fused Hidden Markov Model (MFHMM), we analyzed coupled audio and visual streams to detect four cognitive states (interest, boredom, frustration and puzzlement) and seven prototypical emotions (neural, happiness, sadness, anger, disgust, fear and surprise). The MFHMM allows the building of an optimal connection among multiple streams according to the maximum entropy principle and the maximum mutual information criterion. Person-independent experimental results from 20 subjects in 660 sequences show that the MFHMM approach outperforms face-only HMM, pitch-only HMM, energy-only HMM, and independent HMM fusion, tinder clean and varying audio channel noise condition.
C1 [Zeng, Zhihong; Tu, Jilin; Huang, Thomas S.] Univ Illinois, Beckman Inst, Urbana, IL 61801 USA.
   [Pianfetti, Brian M., Jr.] Univ Illinois, Coll Educ, Champaign, IL 61820 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign;
   University of Illinois System; University of Illinois Urbana-Champaign
RP Zeng, ZH (corresponding author), Univ Illinois, Beckman Inst, Urbana, IL 61801 USA.
EM bpianfet@uiuc.edu; huang@ifp.uiuc.edu
RI yan, shuicheng/A-8531-2014; yan, shuicheng/HCH-9860-2022
OI yan, shuicheng/0000-0001-8906-3777; yan, shuicheng/0000-0003-4527-1018
FU Beckman Postdoctoral Fellowship; NSF [CCF 04-26627]
FX This work was supported in part by Beckman Postdoctoral Fellowship and
   NSF CCF 04-26627.
CR AMBADY N, 1992, PSYCHOL BULL, V111, P256, DOI 10.1037/0033-2909.111.2.256
   [Anonymous], 1995, SPEECH CODING SYNTHE
   [Anonymous], 2005, P 13 ANN ACM INT C M
   [Anonymous], INT C MULT EXP AMSTR
   Brand M, 1997, P COMP VIS PATT REC, P201
   Bulut M., 2004, P 6 INT C MULT INT, P205
   CARIDAKIS G, 2006, INT C MULT INT, P146
   CHEN L, 1998, INT C AUT FAC GEST R, P396
   Chen LS, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P423, DOI 10.1109/ICME.2000.869630
   CHEN LS, 2000, THESIS UIUC
   COHN JF, 2006, INT C MULT INT, P233
   Cowie R, 2001, IEEE SIGNAL PROC MAG, V18, P32, DOI 10.1109/79.911197
   De Silva L. C., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P332, DOI 10.1109/AFGR.2000.840655
   EKMAN P, 2005, WHAT FACE REVEALS, P429
   Ekman P., 1971, Nebraska symposium on motivation, V19, P207
   Fragopanagos N, 2005, NEURAL NETWORKS, V18, P389, DOI 10.1016/j.neunet.2005.03.006
   GO HJ, 2003, INT C SOC INSTR CONT, P2890
   HOCH S, 2005, ICASSP, V2, P1085
   Kanade T., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P46, DOI 10.1109/AFGR.2000.840611
   Kwon Oh-Wook., 2003, EUROSPEECH 2003
   LUTTRELL SP, 1989, FUND THEOR, V36, P363
   MEHRABIAN A, 1968, PSYCHOL TODAY, V2, P53
   Pan H, 2004, IEEE T SIGNAL PROCES, V52, P573, DOI 10.1109/TSP.2003.822353
   Pan H, 2001, PATTERN RECOGN LETT, V22, P1431, DOI 10.1016/S0167-8655(01)00080-0
   Pantic M, 2003, P IEEE, V91, P1370, DOI 10.1109/JPROC.2003.817122
   Pantic M, 2000, IEEE T PATTERN ANAL, V22, P1424, DOI 10.1109/34.895976
   Pantic M., 2006, PROC MULTIMODAL INTE, P239
   PANTIC M, 2007, FACE RECOGNITION, P327
   Picard R. W., 1997, AFFECTIVE COMPUTING
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   SAMAL A, 1992, PATTERN RECOGN, V25, P65, DOI 10.1016/0031-3203(92)90007-6
   Saul LK, 1999, MACH LEARN, V37, P75, DOI 10.1023/A:1007649326333
   Song ML, 2004, PROC CVPR IEEE, P1020
   Stein Barry E., 1993, The Merging of the Senses. The Merging of the Senses. Cognitive Neuroscience
   Tao H., 1999, PROC IEEE COMPUT VIS, P611
   Tian YL, 2005, HANDBOOK OF FACE RECOGNITION, P247, DOI 10.1007/0-387-27257-7_12
   Tu JL, 2004, PROC CVPR IEEE, P719
   WANG Y, 2005, ICASSP, V2, P1125
   Yoshitomi Y, 2000, IEEE RO-MAN 2000: 9TH IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, PROCEEDINGS, P178, DOI 10.1109/ROMAN.2000.892491
   Zeng Z., 2006, INT C MULTIMODAL INT, P139
   ZENG Z, 2005, INT C AFF COMP INT I, P946
   ZENG Z, IEEE T PATT IN PRESS
   ZENG Z, 2004, INT C MULT INT, P137
   Zeng ZH, 2005, PROC CVPR IEEE, P967
   Zeng ZH, 2007, IEEE T MULTIMEDIA, V9, P424, DOI 10.1109/TMM.2006.886310
NR 45
TC 86
Z9 96
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2008
VL 10
IS 4
BP 570
EP 577
DI 10.1109/TMM.2008.921737
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EK
UT WOS:000258767200002
DA 2024-07-18
ER

PT J
AU Khelifi, F
   Bouridane, A
   Kurugollu, F
AF Khelifi, Fouad
   Bouridane, Ahmed
   Kurugollu, Fatih
TI Joined spectral trees for scalable SPIHT-based multispectral image
   compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE joined spectral trees; multispectral images; rate scalability; set
   partitioning in hierarchical trees (SPIHT) algorithm
ID VECTOR QUANTIZATION; TRANSFORM; EFFICIENT
AB In this paper, the compression of multispectral images is addressed. Such 3-D data are characterized by a high correlation across the spectral components. The efficiency of the state-of-the-art wavelet-based coder 3-D SPIHT is considered. Although the 3-D SPIHT algorithm provides the obvious way to process a multispectral image as a volumetric block and, consequently, maintain the attractive properties exhibited in 2-D (excellent performance, low complexity, and embeddedness of the bit-stream), its 3-D trees structure is shown to be not adequately suited for 3-D wavelet transformed (DWT) multispectral images. The fact that each parent has eight children in the 3-D structure considerably increases the list of insignificant sets (LIS) and the list of insignificant pixels (LIP) since the partitioning of any set produces eight subsets which will be processed similarly during the sorting pass. Thus, a significant portion from the overall bit-budget is wastedly spent to sort insignificant information. Through an investigation based on results analysis, we demonstrate that a straightforward 2-D SPIHT technique, when suitably adjusted to maintain the rate scalability and carried out in the 3-D DWT domain, overcomes this weakness. In addition, a new SPIHT-based scalable multispectral image compression algorithm is used in the initial iterations to exploit the redundancies within each group of two consecutive spectral bands. Numerical experiments on a number of multispectral images have shown that the proposed scheme provides significant improvements over related works.
C1 [Khelifi, Fouad] Univ Jijel, Jijel, Algeria.
   [Khelifi, Fouad; Bouridane, Ahmed; Kurugollu, Fatih] Queens Univ Belfast, Inst Elect Commun & Informat Technol, Sch Comp Sci, Belfast BT3 9DT, Antrim, North Ireland.
C3 Universite de Jijel; Queens University Belfast
RP Khelifi, F (corresponding author), Queens Univ Belfast, Inst Elect Commun & Informat Technol, Sch Comp Sci, Belfast BT3 9DT, Antrim, North Ireland.
EM f.khelifi01@qub.ac.uk; a.bouridane@qub.ac.uk; f.kurugollu@qub.ac.uk
OI Kurugollu, Fatih/0000-0002-2508-4496
CR Antonini M, 1992, IEEE T IMAGE PROCESS, V1, P205, DOI 10.1109/83.136597
   Baker R. L., 1988, Proceedings of the SPIE - The International Society for Optical Engineering, V974, P255, DOI 10.1117/12.948466
   Boussakta, 2004, P IEEE INT C AC SPEE, V3, P689
   Canta GR, 1998, IEEE T IMAGE PROCESS, V7, P668, DOI 10.1109/83.668024
   Dragotti PL, 2000, IEEE T GEOSCI REMOTE, V38, P416, DOI 10.1109/36.823937
   EPSTEIN BR, 1992, P DAT COMPR C SNOWB, P200
   Fry TW, 2005, IEEE T CIRC SYST VID, V15, P1138, DOI 10.1109/TCSVT.2005.852625
   Ginesu G, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P693
   GUPTA S, 1992, IEEE T GEOSCI REMOTE, V30, P491, DOI 10.1109/36.142927
   Kassim AA, 2003, IEEE T CIRC SYST VID, V13, P203, DOI 10.1109/TCSVT.2002.808427
   KHAN E, 2002, P IEEE INT C IM PROC, V3, P241
   KHELIFI F, 2006, P IEEE INT S SIGN PR
   KHELIFI F, 2006, P EUR C SIGN PROC EU
   KHELIFI F, 2005, P EUR C SIGN PROC EU
   Kim BJ, 2000, IEEE T CIRC SYST VID, V10, P1374, DOI 10.1109/76.889025
   Marcellin M. W., 2000, Proceedings DCC 2000. Data Compression Conference, P523, DOI 10.1109/DCC.2000.838192
   Pearlman WA, 2004, IEEE T CIRC SYST VID, V14, P1219, DOI 10.1109/TCSVT.2004.835150
   RAAD M, 2003, P IEEE INT C AC SPEE, V5, P624
   Rucker J., 2005, P IEEE INT GEOSC REM
   SAGHRI JA, 1995, IEEE SIGNAL PROC MAG, V12, P32, DOI 10.1109/79.363506
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Schelkens P, 2003, IEEE T MED IMAGING, V22, P441, DOI 10.1109/TMI.2003.809582
   SHAPIRO JM, 1993, IEEE T SIGNAL PROCES, V41, P3445, DOI 10.1109/78.258085
   Tai SC, 2005, IEEE T BIO-MED ENG, V52, P999, DOI 10.1109/TBME.2005.846727
   Tang X., 2005, HYPERSPECTRAL DATA C
   Tang XL, 2005, PROC SPIE, V5817, P270, DOI 10.1117/12.604634
   Taubman D, 2000, IEEE T IMAGE PROCESS, V9, P1158, DOI 10.1109/83.847830
   Taubman D.S., KAKADU SOFTWARE
   TZANNES A, 2005, P INT C DIG IM COMM
   Xiong ZX, 2003, IEEE T MED IMAGING, V22, P459, DOI 10.1109/TMI.2003.809585
   [No title captured]
   AVIRIS DATA
NR 32
TC 27
Z9 29
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 316
EP 329
DI 10.1109/TMM.2008.917357
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100002
DA 2024-07-18
ER

PT J
AU Luo, C
   Wang, W
   Tang, J
   Sun, J
   Li, J
AF Luo, Chong
   Wang, Wei
   Tang, Jian
   Sun, Jun
   Li, Jiang
TI A multiparty videoconferencing system over an application-level
   multicast protocol
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE application-level multicast; multiparty videoconferencing; peer-to-peer
   networking
AB Increased speeds of PCs and networks have made media communications possible on the Internet. Today, the need for desktop videoconferencing is experiencing robust growth in both business and consumer markets. However, the synchronous delivery of high-volume media content is still a big challenge under a current heterogeneous Internet environment. In this paper, we present a multiparty videoconferencing system based on a peer-to-peer (P2P) solution. The contribution of our paper is twofold. On the one hand, we design an application-level multicast scheme which intends to tolerate the heterogeneity in videoconferencing applications. Design tradeoffs are analyzed and our decisions are made based on extensive experimentation. On the other, we design a five-layer architecture for implementing a multiparty videoconferencing system. This architecture makes a clear-cut distinction between different functional modules and therefore provides rich flexibility in feature adaptation. We believe that our work can be a helpful reference in other efforts on building desktop videoconferencing systems.
C1 Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Dept Elect Engn, Shanghai 200030, Peoples R China.
   Shanghai Jiao Tong Univ, Shanghai Key Lab Digital Media Proc & Transmiss, Shanghai 200030, Peoples R China.
   Microsoft Res Asia, Beijing, Peoples R China.
   Sichuan Univ, Sch Elect & Informat Engn, Chengdu 610064, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; Microsoft
   Research Asia; Microsoft; Sichuan University
RP Luo, C (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Dept Elect Engn, Shanghai 200030, Peoples R China.
EM chong.luo@microsoft.com; weiwang@scu.edu.cn; jtang@microsoft.com;
   junsun@sjtu.edu.cn; jiangli@microsoft.com
CR [Anonymous], 2003, STUN SIMPLE TRAVERSA
   *CAIDA, CAIDA SKITT TOOL
   Chu YH, 2001, ACM SIGCOMM COMP COM, V31, P55, DOI 10.1145/964723.383064
   Chu YH, 2000, PERF E R SI, V28, P1, DOI 10.1145/345063.339337
   Ganjam A., 2004, NOSSDAV 04, P54
   Hosseini M., 2003, Proceedings of the eleventh ACM international conference on Multimedia, MULTIMEDIA '03, P480
   *ICUII, LIV VID CHAT PROGR
   Luo C, 2004, GLOB TELECOMM CONF, P982
   *OECD, OECD BROADB STAT
   PENDARAKIS D, 2001, P 3 US S INT TECHN S
   ROSENBERG J, 2004, INTERACTIVE CONNECTI
   WEI L, 1994, INT C COMP COMM NET
   ZHANG M, 2006, P IEEE INT C COMM IC
   INTERNET TOPOLOGY SI
   SKYPE INTERNET CALLS
   KAZAA FILE SHARING
NR 16
TC 16
Z9 20
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2007
VL 9
IS 8
BP 1621
EP 1632
DI 10.1109/TMM.2007.907467
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 233SF
UT WOS:000251109900008
DA 2024-07-18
ER

PT J
AU Liu, ZC
   Cohen, M
   Bhatnagar, D
   Cutler, R
   Zhang, ZY
AF Liu, Zicheng
   Cohen, Michael
   Bhatnagar, Deepti
   Cutler, Ross
   Zhang, Zhengyou
TI Head-size equalization for improved visual perception in video
   conferencing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE head-size equalization; video conferencing; visual; perception
AB In a video conferencing setting, people often use an elongated meeting table with the major axis along the camera direction. A standard wide-angle perspective image of this setting creates significant foreshortening, thus the people sitting at the far end of the table appear very small relative to those nearer the camera. This has two consequences. First, it is difficult for the remote participants to see the faces of those at the far end, thus affecting the experience of the video conferencing. Second, it is a waste of the screen space and network bandwidth because most of the pixels are used on the background instead of on the faces of the meeting participants. In this paper, we present a novel technique, called Spatially-Varying-Uniform scaling functions, to warp the images to equalize the head sizes of the meeting participants without causing undue distortion. This technique works for both the 180-degree views where the camera is placed at one end of the table and the 360-degree views where the camera is placed at the center of the table. We have implemented this algorithm on two types of camera arrays: one with 180-degree view, and the other with 360-degree view. On both hardware devices, image capturing, stitching, and head-size equalization are run in real time. In addition, we have conducted user study showing that people clearly prefer head-size equalized images.
C1 Microsoft Res, Redmond, WA 98052 USA.
   Indian Inst Technol, New Delhi, India.
C3 Microsoft; Indian Institute of Technology System (IIT System); Indian
   Institute of Technology (IIT) - Delhi
RP Liu, ZC (corresponding author), Microsoft Res, Redmond, WA 98052 USA.
EM zliu@microsoft.com; mcohen@microsoft.com; rcutler@microsoft.com;
   zhang@rmcrosoft.com
RI Zhang, Zhang/JAX-2097-2023; Cutler, Ross/GWM-5862-2022; zhang,
   zheng/HCH-9684-2022
CR Aggarwal M, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P2, DOI 10.1109/ICCV.2001.937492
   [Anonymous], P IEEE C COMP VIS PA
   CHANG Y, 2005, P IEEE INT C MULT EX
   Coorg S, 1998, PROC CVPR IEEE, P872, DOI 10.1109/CVPR.1998.698707
   Cutler R., 2002, MULTIMEDIA 02, P503, DOI DOI 10.1145/641007.641112
   Hicks RA, 2000, IEEE WORKSHOP ON OMNIDIRECTIONAL VISION, PROCEEDINGS, P97, DOI 10.1109/OMNVIS.2000.853813
   IRANI M, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P605, DOI 10.1109/ICCV.1995.466883
   MAGANTI DGP, 2006, IDIAPRR0624 IDIAP
   MANN S, 1994, P 1 IEEE INT C IM PR, P1
   NAYAR S, 1997, DARPA IM UND WORKSH, P235
   Nayar SK, 2000, PROC CVPR IEEE, P388
   Szeliski R., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P251, DOI 10.1145/258734.258861
   Tamai Y, 2003, IEEE SENSOR, P1100, DOI 10.1109/ICSENS.2003.1279114
   VALLESPI MVT, 2006, P IEEE INT C IM PROC
   Yang J, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P159, DOI 10.1145/319463.319484
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 16
TC 5
Z9 7
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2007
VL 9
IS 7
BP 1520
EP 1527
DI 10.1109/TMM.2007.906571
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 224LB
UT WOS:000250447400016
DA 2024-07-18
ER

PT J
AU Huang, CM
   Lin, CW
AF Huang, Chung-Ming
   Lin, Chung-Wei
TI A novel 4-D perceptual quantization modeling for H.264 bit-rate control
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 4-D perceptual quantization modeling; bit-rate control; H264;
   just-noticeable-difference PSNR; weighted least-square estimation
ID MPEG-4 RATE CONTROL; RATE CONTROL SCHEME; VIDEO
AB Bit-rate control plays a major role in video coding and multimedia streaming. A well-designed bit-rate control mechanism can achieve fine visual qualities and avoid network congestion over a time-varying channel. This paper proposes an H.264 bit-rate control using a 4-D perceptual quantization modeling (PQrc), including two major encoding modules: the perceptual frame-level bit-allocation using a I-D temporal pattern and the macroblock-level quantizer decision using a 3-D rate pattern. The temporal pattern is used to predict frame complexity and determine proper budget bits further. The rate pattern is depicted as a bit-complexity-quantization (B.C.Q.) model, in which a tangent slope of a B.C.Q. curve is a piece of unique information to find a proper quantizer. For newly generated video clips, the B.C.Q. model is updated continuously using a weighted least-square estimation. In comparison with the latest H.264 JM10.2, our experiment results show that the proposed PQrc can: 1) keep stable buffer fullness and 2) improve the SNR quality and control accuracy effectively.
C1 Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 70101, Taiwan.
C3 National Cheng Kung University
RP Huang, CM (corresponding author), Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 70101, Taiwan.
EM huangcm@locust.csie.ncku.edu.tw
CR Chen C.-T., 1999, LINEAR SYSTEM THEORY
   Chiang TH, 1997, IEEE T CIRC SYST VID, V7, P246, DOI 10.1109/76.554439
   He ZH, 2001, IEEE T CIRC SYST VID, V11, P928, DOI 10.1109/76.937431
   *ISO IEC, 2003, 1449610 ISOIEC FDIS
   *ISO IEC, 2001, ISOIECJTCSC29WG11
   JIANG M, 2005, P IEEE INT S CIRCUIT, V2, P1501
   *JVT AVC, JVT AVC REF SOFTW
   LAI KC, 2002, P INT C SIGN PROC AU, V1, P656
   Lambert P, 2006, IEEE T CIRC SYST VID, V16, P134, DOI 10.1109/TCSVT.2005.857783
   Lee HJ, 2000, IEEE T CIRC SYST VID, V10, P878, DOI 10.1109/76.867926
   LI Z, 2003, P 8 M JVT H017 GEN S
   Lin LJ, 1998, IEEE T CIRC SYST VID, V8, P446, DOI 10.1109/76.709411
   Ma SW, 2005, IEEE T CIRC SYST VID, V15, P1533, DOI 10.1109/TCSVT.2005.857300
   MIYAJI S, 2005, P IEEE INT C IM PROC, V2, P309
   Ngan KN, 2003, IEEE T CIRC SYST VID, V13, P385, DOI 10.1109/TCSVT.2003.811609
   Pan F, 2003, IEEE T CIRC SYST VID, V13, P440, DOI 10.1109/TCSVT.2003.811603
   Pao IM, 2001, IEEE T CIRC SYST VID, V11, P199, DOI 10.1109/76.905985
   Park JH, 2005, IEEE ICCE, P229, DOI 10.1109/ICCE.2005.1429801
   Reed EC, 2001, IEEE T CIRC SYST VID, V11, P882, DOI 10.1109/76.931115
   Ribas-Corbera J, 2000, IEEE T CIRC SYST VID, V10, P1154, DOI 10.1109/76.875518
   Song HJ, 2001, IEEE T CIRC SYST VID, V11, P512, DOI 10.1109/76.915357
   Vetro A, 1999, IEEE T CIRC SYST VID, V9, P186, DOI 10.1109/76.744285
   *VID COD EXP GROUP, 1997, ITUTSG16
   WEBB JLH, 1997, P INT C IM PROC SANT, V2, P13
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   YANG KH, 1997, P INT C IM PROC SANT, V2, P41
   Zhang ZW, 2005, IEEE T CIRC SYST VID, V15, P1354, DOI 10.1109/TCSVT.2005.856904
NR 27
TC 6
Z9 6
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2007
VL 9
IS 6
BP 1113
EP 1124
DI 10.1109/TMM.2007.902840
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 215XJ
UT WOS:000249842000003
DA 2024-07-18
ER

PT J
AU Shin, HC
   Park, JH
   Kim, SD
AF Shin, Ho-Chul
   Park, Jae Hee
   Kim, Seong-Dae
TI Combination of warping robust elastic graph matching and kernel-based
   projection discriminant analysis for face recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE face recognition; feature extraction
ID LDA; AUTHENTICATION; PCA
AB In this paper, a robust face recognition algorithm is proposed, which is based on the elastic graph matching (EGM) and discriminative feature analysis algorithm. We introduce a cost function for the EGM taking account of variations in face pose and facial expressions, and propose its optimization procedure. Our proposed cost function uses a set of Gabor-wavelet-based features, called robust jet, which are robust against the variations. The robust jet is defined in terms of discrete Fourier transform coefficients of Gabor coefficients. To cope with the difference between face poses of test face and reference faces, 2 x 2 warping matrix is incorporated in the proposed cost function. For the discriminative feature analysis, linear projection discriminant analysis and kernel-based projection discriminant analysis are introduced. These methods are motivated to solve the small-size problem of training samples. The basic idea of PDA is that a class is represented by a subspace spanned by some training samples of the class instead of using sample mean vector, that the distance from a pattern to a class is defined by using the error vector between the pattern and its projection to the subspace representing the class, and that an optimum feature selection rule is developed using the distance concept in a similar way as in the conventional linear discrimnant analysis. In order to evaluate the performance of our face recognition algorithm, we carried out some experiments using the well-known FERET face database, and compared the performance with recently developed approaches. We observed that our algorithm outperformed the compared approaches.
C1 Korea Adv Inst Sci & Technol, Dept Elect Engn & Comp Sci, Div Elect Engn, Visual Commun Lab, Taejon 305701, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Shin, HC (corresponding author), Korea Adv Inst Sci & Technol, Dept Elect Engn & Comp Sci, Div Elect Engn, Visual Commun Lab, Taejon 305701, South Korea.
EM shc@kaist.ac.kr; yscclo@sdvisioil.kaist.ac.kr; sdkim@ee.kaist.ac.kr
RI Kim, Seong-Dae/C-1892-2011
CR Abe S., 2005, ADV PTRN RECOGNIT
   [Anonymous], P 5 IEEE INT C AUT F
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Chen LF, 2000, PATTERN RECOGN, V33, P1713, DOI 10.1016/S0031-3203(99)00139-9
   Chien JT, 2002, IEEE T PATTERN ANAL, V24, P1644, DOI 10.1109/TPAMI.2002.1114855
   Cho JH, 2005, SIGNAL PROCESS-IMAGE, V20, P233, DOI 10.1016/j.image.2004.12.001
   Duc B, 1999, IEEE T IMAGE PROCESS, V8, P504, DOI 10.1109/83.753738
   Duda R., 1973, Pattern Classification and Scene Analysis
   Fukunaga K., 1990, Introduction to StatisticalPattern Recognition
   Göberk B, 2003, IEEE IMAGE PROC, P677
   Haykin S., 1999, NEURAL NETWORKS COMP, DOI [10.1017/S0269888998214044, DOI 10.1017/S0269888998214044]
   Kotropoulos C, 2000, IEEE T IMAGE PROCESS, V9, P555, DOI 10.1109/83.841933
   LADES M, 1993, IEEE T COMPUT, V42, P300, DOI 10.1109/12.210173
   Lawrence BS, 1997, ORGAN SCI, V8, P1, DOI 10.1287/orsc.8.1.1
   Li S.Z., 2005, Handbook of Face Recognition
   Liu CJ, 2004, IEEE T PATTERN ANAL, V26, P572, DOI 10.1109/TPAMI.2004.1273927
   Liu QS, 2004, IEEE T CIRC SYST VID, V14, P42, DOI 10.1109/TCSVT.2003.818352
   Lu JW, 2003, IEEE T NEURAL NETWOR, V14, P117, DOI 10.1109/TNN.2002.806629
   Lyons MJ, 1999, IEEE T PATTERN ANAL, V21, P1357, DOI 10.1109/34.817413
   Martìnez AM, 2001, IEEE T PATTERN ANAL, V23, P228, DOI 10.1109/34.908974
   PARK JH, 2005, P INT C IM PROC 2005, P346
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Press W. H, 1992, NUMERICAL RECIPES C
   Samaria F. S., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P138, DOI 10.1109/ACV.1994.341300
   Shen LL, 2004, INT C PATT RECOG, P284, DOI 10.1109/ICPR.2004.1334108
   Tefas A, 2001, IEEE T PATTERN ANAL, V23, P735, DOI 10.1109/34.935847
   Türk L, 1999, AEROSP SCI TECHNOL, V3, P71, DOI 10.1016/S1270-9638(99)80031-5
   Wiskott L, 1997, IEEE T PATTERN ANAL, V19, P775, DOI 10.1109/34.598235
   Wu AW, 1997, QUAL LIFE RES, V6, P3, DOI 10.1023/A:1026471020698
   Yang J, 2005, IEEE T PATTERN ANAL, V27, P230, DOI 10.1109/TPAMI.2005.33
   Yu H, 2001, PATTERN RECOGN, V34, P2067, DOI 10.1016/S0031-3203(00)00162-X
NR 31
TC 15
Z9 16
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2007
VL 9
IS 6
BP 1125
EP 1136
DI 10.1109/TMM.2007.898933
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 215XJ
UT WOS:000249842000004
DA 2024-07-18
ER

PT J
AU Po, LM
   Ting, CW
   Wong, KM
   Ng, KH
AF Po, Lai-Man
   Ting, Chi-Wang
   Wong, Ka-Man
   Ng, Ka-Ho
TI Novel point-oriented inner searches for fast block motion estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE diamond search; fast motion estimation; hexagon search; inner search
ID HEXAGONAL SEARCH; ALGORITHM; PATTERN
AB Recently, an enhanced hexagon-based (EHS) search algorithm was proposed to speedup the original hexagon-based search (HS) using a 6-side-based fast inner search. However, this 6-side-based method is quite irregular by inspecting the distance between the inner search points and the coarse search points that would lower prediction accuracy. In this paper, a new point-oriented grouping strategy is proposed to develop fast inner search techniques for speeding up the HS and diamond search (DS) algorithms. Experimental results show that the new HS and DS using point-oriented inner searches are faster than their original algorithms up to 30% with negligible peak signal-to-noise ratio degradation.
C1 City Univ Hong Kong, Dept Elect Engn, Kowloon, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Po, LM (corresponding author), City Univ Hong Kong, Dept Elect Engn, Kowloon, Hong Kong, Peoples R China.
EM eelmpo@cityu.edu.hk; cwting@ee.cityu.edu.hk; kmwong@ee.cityu.edu.hk;
   kahomike@gmail.com
OI Po, Lai Man/0000-0002-5185-1492
CR [Anonymous], 1981, P NAT TEL C NEW ORL
   CHEN Z, 2003, P PICT COD S SAINT M, P17
   Cheung CH, 2005, IEEE T MULTIMEDIA, V7, P16, DOI 10.1109/TMM.2004.840609
   Cheung CH, 2002, IEEE T CIRC SYST VID, V12, P1168, DOI 10.1109/TCSVT.2002.806815
   JAIN JR, 1981, IEEE T COMMUN, V29, P1799, DOI 10.1109/TCOM.1981.1094950
   Jing X, 2004, IEEE T MULTIMEDIA, V6, P435, DOI 10.1109/TMM.2004.827517
   *JVT, H264AVC REF SOFTW JO
   LI RX, 1994, IEEE T CIRC SYST VID, V4, P438, DOI 10.1109/76.313138
   Liu LK, 1996, IEEE T CIRC SYST VID, V6, P419, DOI 10.1109/76.510936
   Nie Y, 2002, IEEE T IMAGE PROCESS, V11, P1442, DOI 10.1109/TIP.2002.806251
   Po LM, 1996, IEEE T CIRC SYST VID, V6, P313, DOI 10.1109/76.499840
   SRINIVASAN R, 1985, IEEE T COMMUN, V33, P888, DOI 10.1109/TCOM.1985.1096398
   Tham JY, 1998, IEEE T CIRC SYST VID, V8, P369, DOI 10.1109/76.709403
   Tourapis AM, 2002, IEEE T CIRC SYST VID, V12, P934, DOI 10.1109/TCSVT.2002.804894
   Zhu C, 2002, IEEE T CIRC SYST VID, V12, P349, DOI 10.1109/TCSVT.2002.1003474
   Zhu C, 2004, IEEE T CIRC SYST VID, V14, P1210, DOI 10.1109/TCSVT.2004.833166
   Zhu S, 2000, IEEE T IMAGE PROCESS, V9, P287, DOI 10.1109/83.821744
NR 17
TC 25
Z9 30
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2007
VL 9
IS 1
BP 9
EP 15
DI 10.1109/TMM.2006.886330
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 119VE
UT WOS:000243041500002
DA 2024-07-18
ER

PT J
AU Li, Y
   Markopoulou, A
   Bambos, N
   Apostolopoulos, J
AF Li, Yan
   Markopoulou, Athina
   Bambos, Nick
   Apostolopoulos, John
TI Joint power-playout control for media streaming over wireless links
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptive playout; cross-layer optimization; dynamic programming;
   multimedia streaming; network control; quality-of-service; wireless
   multimedia
AB Media streaming applications over wireless links face various challenges, due to both the nature of the wireless channel and the stringent delivery requirements of media traffic. In this paper, we seek to improve the performance of media streaming over an interference-limited wireless link, by using appropriate transmission and playout control. In particular, we choose both the power at the transmitter and the playout scheduling at the receiver, so as to minimize the power consumption and maximize the media playout quality. We formulate the problem using a dynamic programming approach, and study the structural properties of the optimal solution. We further develop a justified, low-complexity heuristic that achieves significant performance gain over benchmark systems. In particular, our joint power-playout heuristic outperforms: 1) the optimal power control policy in the regime where power is most important and 2) the optimal playout control policy in the regime where media (playout) quality is most important; furthermore, this heuristic has only a slight performance loss as compared to the optimal joint power-playout control policy over the entire range of the investigation.
C1 Stanford Univ, Dept Elect Engn, Stanford, CA 94304 USA.
   Hewlett Packard Labs, Palo Alto, CA 94304 USA.
C3 Stanford University; Hewlett-Packard
RP Li, Y (corresponding author), Qualcomm, Campbell, CA 95008 USA.
EM liyan@stanfordalumni.org; amarko@stanfordalumni.org;
   bambos@stanford.edu; japos@hpl.hp.com
OI Bambos, Nicholas/0000-0001-9250-4553; Markopoulou,
   Athina/0000-0003-1803-8675
CR [Anonymous], 2016, IEEE Standard 802.11-2020
   [Anonymous], 2005, IEEE Std 802.11
   Bambos N., 2000, P IEEE INFOCOM 2000, P368
   BERTSEKAS DP, 1995, DYNAMIC PROGRAMMING, V2
   Cabrera J, 2002, IEEE T CIRC SYST VID, V12, P496, DOI 10.1109/TCSVT.2002.800306
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   Farber N., 2000, Compressed Video over Networks
   Girod B, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P1
   Kalman M, 2004, IEEE T CIRC SYST VID, V14, P841, DOI 10.1109/TCSVT.2004.828335
   Kalman M, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P189, DOI 10.1109/ICIP.2002.1038937
   Kandukuri S, 2001, IEEE INFOCOM SER, P199, DOI 10.1109/INFCOM.2001.916702
   krishnaswamy D., 2002, PROC 3G WIRELESS C, P165
   Li Y, 2004, 2004 WIRELESS TELECOMMUNICATIONS SYMPOSIUM, PROCEEDINGS, P102
   LI Y, 2005, P IEEE MMSP 2005 SPE
   LI Y, 2004, P IEEE PACK VID WORK
   Li YM, 2004, Proceedings of 2004 Chinese Control and Decision Conference, P560
   Liang YJ, 2003, IEEE T MULTIMEDIA, V5, P532, DOI 10.1109/TMM.2003.819095
   Moon SB, 1998, MULTIMEDIA SYST, V6, P17, DOI 10.1007/s005300050073
   RAMJEE R, 1994, IEEE INFOCOM SER, P680, DOI 10.1109/INFCOM.1994.337672
   Shankar S., 2004, P IEEE PACK VID 2004
   SONG KB, 2003, P IEEE VTC ORL FL SE, V1, P572
   Verhelst W., 1993, PROC IEEE INT C ACOU, V2, P554
   Vetro A, 2003, IEEE SIGNAL PROC MAG, V20, P18, DOI 10.1109/MSP.2003.1184336
NR 23
TC 31
Z9 38
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2006
VL 8
IS 4
BP 830
EP 843
DI 10.1109/TMM.2006.876236
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 069BS
UT WOS:000239420300016
DA 2024-07-18
ER

PT J
AU Eleftheriadis, A
   Batra, P
AF Eleftheriadis, A
   Batra, P
TI Dynamic rate shaping of compressed digital video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE data partitioning; dynamic rate shaping; matrix perturbation theory;
   rate-distortion theory; video transcoding
ID MOTION; REQUANTIZATION; TRANSMISSION; OPTIMIZATION; SCALABILITY;
   ALLOCATION; MULTIPLIER; TRANSFORM; DRIFT
AB We discuss new theoretical and experimental results on the dynamic rate shaping (DRS) approach for transcoding compressed video bitstreams (MPEG-1, MPEG-2, MPEG-4, H.261, as well as JPEG). We analyze the behavior of DRS assuming a first order autoregressive source. We propose a set of low complexity algorithms for both constrained and unconstrained DRS and substantiate the almost-optimal experimental performance of the memoryless algorithm by assuming a first order autoregressive source. By deriving the statistical and rate-distortion characteristics of different components of the interframe rate shaping problem, we offer an explanation as to why the set of optimal breakpoint values for any frame is somewhat invariant to the accumulated motion compensated shaping error from past frames. We also present an extensive experimental study on the various DRS algorithms (causally optimal, memoryless, and rate-based) both in their constrained and generalized forms. The study proves the computational viability of the DRS approach to transcoding and identifies a range of rate shaping ratios for which it is better than requantization, both complexity-wise as well as in performance. This result is significant in that it opens up the way to construct much simpler memoryless algorithms that give minimal penalty in achieved quality, not Just for this but possibly other types of algorithms. This is also the very first use of matrix perturbation theory for tracking the spectral behavior of the autocorrelation matrix of the source signal and the motion residual it yields.
C1 Columbia Univ, Dept Elect Engn, New York, NY 10027 USA.
C3 Columbia University
RP Columbia Univ, Dept Elect Engn, New York, NY 10027 USA.
EM eleft@ee.columbia.edu; pbatra@hotmail.com
CR [Anonymous], 1990, LINEAR PROGRAMMING N
   Arnold JE, 2000, IEEE T CIRC SYST VID, V10, P70, DOI 10.1109/76.825862
   ASSUNCAO P, 1997, P IEEE INT C IM PROC, V1, P739
   Assunçao PAA, 1998, IEEE T CIRC SYST VID, V8, P953, DOI 10.1109/76.736724
   Batra P, 2000, IEEE T IMAGE PROCESS, V9, P1677, DOI 10.1109/83.869179
   BATRA P, 2003, THESIS COLUMBIA U NE
   BATRA P, UNPUB IEEE T IMAGE P
   BATRA P, 2002, P ICASSP
   BATRA P, 2003, P ICIP SEP
   Bellifemine F., 1992, Signal Processing: Image Communication, V4, P477, DOI 10.1016/0923-5965(92)90032-B
   Bellman R., 1962, Applied Dynamic Programming
   Berger Toby, 1971, RATE DISTORTION THEO
   CHEN CF, 1993, IEEE T CIRCUITS-II, V40, P393, DOI 10.1109/82.277884
   CHEN CF, 1992, MULTIDIM SYST SIGN P, V3, P241, DOI 10.1007/BF01942044
   CHEN TPC, 2003, IEEE T ACOUST SPEECH, V5, P688
   Effros M, 2004, IEEE T INFORM THEORY, V50, P1605, DOI 10.1109/TIT.2004.831787
   Eleftheriadis A, 2004, IEEE T CIRC SYST VID, V14, P1195, DOI 10.1109/TCSVT.2004.835149
   ELEFTHERIADIS A, 1994, IEEE IMAGE PROC, P273, DOI 10.1109/ICIP.1994.413318
   ELEFTHERIADIS A, 1994, MULTIMEDIA SYST, V2, P89
   ELEFTHERIADIS A, 1995, M ARBITRARY QOS CONS, P96
   ELEFTHERIADIS A, 1995, THESIS COLUMBIA U NE
   EVERETT H, 1963, OPER RES, V11, P399, DOI 10.1287/opre.11.3.399
   FISHER M, 1973, P IEEE, V61, P268
   FISHER ML, 1981, MANAGE SCI, V27, P1, DOI 10.1287/mnsc.27.1.1
   Fox B., 1966, Management science, V13, P210, DOI [DOI 10.1287/MNSC.13.3.210, 10.1287/mnsc.13.3.210]
   FOX BL, 1970, OPER RES, V18, P253, DOI 10.1287/opre.18.2.253
   GOYAL VK, 2002, P DIMACS WORKSH SOUR
   Hsu CY, 1997, IEEE J SEL AREA COMM, V15, P1016, DOI 10.1109/49.611156
   *ISO IEC, 1994, 138181MPEG2H2220 ISI
   *ISO IEC, 1994, 138182MPEG2H262 ISOI
   Jacobsen JL, 1998, ENVIRONMETRICS, V9, P3, DOI 10.1002/(SICI)1099-095X(199801/02)9:1<3::AID-ENV281>3.0.CO;2-2
   Jayant N.C., 1984, Digital Coding of Waveforms: Principles and Applications to Speech and Video
   KANAKIA H, 1993, P ACM SIGCOMM 93, P20
   Keesman G, 1996, SIGNAL PROCESS-IMAGE, V8, P481, DOI 10.1016/0923-5965(95)00067-4
   Lam EY, 2000, IEEE T IMAGE PROCESS, V9, P1661, DOI 10.1109/83.869177
   MOKRY R, 1994, IEEE T CIRC SYST VID, V4, P392, DOI 10.1109/76.313134
   MORRISON DG, 1994, P PACK VID WORKSH
   Nakajima Y., 1995, Proceedings. International Conference on Image Processing (Cat. No.95CB35819), P408, DOI 10.1109/ICIP.1995.537658
   Nemhauser G., 1988, INTEGER COMBINATORIA, DOI DOI 10.1002/9781118627372
   Netravali A.N., 1995, DIGITAL PICTURES REP, V2nd
   Niehsen T, 1999, IEEE T CIRC SYST VID, V9, P536, DOI 10.1109/76.767119
   ORTEGA A, 1994, IEEE T IMAGE PROCESS, V3, P26, DOI 10.1109/83.265978
   RAMCHANDRAN K, 1993, THESIS COLUMBIA U NE
   Rao K.R, 2014, DISCRETE COSINE TRAN
   Rejaie R, 2000, IEEE J SEL AREA COMM, V18, P2530, DOI 10.1109/49.898735
   Roh KC, 2002, SIGNAL PROCESS-IMAGE, V17, P573, DOI 10.1016/S0923-5965(02)00045-0
   Schuster G., 1997, RATE DISTORTION BASE
   SEO D, 2002, NETW MAG, V17, P10
   Shanableh T, 2000, IEEE T MULTIMEDIA, V2, P101, DOI 10.1109/6046.845014
   SHOHAM Y, 1988, IEEE T ACOUST SPEECH, V36, P1445, DOI 10.1109/29.90373
   STOCKER H, 1998, HDB MATH COMPUTATION
   Sun HF, 1996, IEEE T CIRC SYST VID, V6, P191, DOI 10.1109/76.488826
   Tajime J, 2002, IEEE IMAGE PROC, P705
   Taubman D, 1996, IEEE T CIRC SYST VID, V6, P329, DOI 10.1109/76.510928
   Trushkin A. V., 1981, Problems of Information Transmission, V17, P156
   VAN CF, 1996, MATRIX COMPUTATIONS
   VETRO A, 2002, P ICIP, V1, P29
   Werner O, 1999, IEEE T IMAGE PROCESS, V8, P179, DOI 10.1109/83.743853
   YONG M, 1994, P PACK VID WORKSH
   Youn J, 2000, J VIS COMMUN IMAGE R, V11, P385, DOI 10.1006/jvci.2000.0449
   ZENG W, 1996, P ACM MULT C
NR 61
TC 16
Z9 31
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2006
VL 8
IS 2
BP 297
EP 314
DI 10.1109/TMM.2005.864346
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 028GY
UT WOS:000236476300011
DA 2024-07-18
ER

PT J
AU Yan, JY
   Katrinis, K
   May, M
   Plattner, B
AF Yan, JY
   Katrinis, K
   May, M
   Plattner, B
TI Media- and TCP-friendly congestion control for scalable video streams
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE algorithms; internet; multimedia communication; optimal control
ID INTERNET
AB This paper presents a media- and TCP-friendly rate-based congestion control algorithm (MTFRCC) for scalable video streaming in the Internet. The algorithm integrates two new techniques: i) a utility-based model using the rate-distortion function as the application utility measure for optimizing the overall video quality; and ii) a two-timescale approach of rate averages (long-term and short-term) to satisfy both media and TCP-friendliness. We evaluate our algorithm through simulation and compare the results against the TCP-friendly rate control (TFRC) algorithm. For assessment, we consider five criteria: TCP fairness, responsiveness, aggressiveness, overall video quality, and smoothness of the resulting bit rate. Our simulation results manifest that MTFRCC performs better than TFRC for various congestion levels, including an improvement of the overall video quality.
C1 ETH, Comp Engn & Networks Lab, Commun Syst Grp, CH-8092 Zurich, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; ETH Zurich
RP Commun Univ China, Network Ctr, Beijing 100024, Peoples R China.
EM jyan@cuc.edu.cn; katrinis@tik.ee.ethz.ch; maym@tik.ee.ethz.ch;
   plattner@tik.ee.ethz.ch
RI yan, jy/ISS-1790-2023
OI Katrinis, Kostas/0000-0002-1136-6062
CR [Anonymous], P IEEE ICIP GEN IT S
   Bansal D, 2001, IEEE INFOCOM SER, P631, DOI 10.1109/INFCOM.2001.916251
   BANSAL D, 2001, P ACM SIGCOMM AUG, P263
   Bertsekas D. P., 1995, NONLINEAR PROGRAMMIN
   CUETOS PD, 2003, RR03078 I EUR TECH
   CUETOS PD, 2004, INFOCOM 2004, V3, P1479
   CUETOS PD, 2002, P NOSSDAV 02 MAY, P3
   DAI M, 2003, NOSSDAV 03, P60
   Floyd S, 2000, ACM SIGCOMM COMP COM, V30, P43, DOI 10.1145/347057.347397
   Floyd S, 1999, IEEE ACM T NETWORK, V7, P458, DOI 10.1109/90.793002
   Kelly FP, 1998, J OPER RES SOC, V49, P237, DOI 10.2307/3010473
   Kim T, 2003, IEEE INFOCOM SER, P641
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   Low SH, 2002, IEEE CONTR SYST MAG, V22, P28, DOI 10.1109/37.980245
   Low SH, 1999, IEEE ACM T NETWORK, V7, P861, DOI 10.1109/90.811451
   MASSOULIE L, 2000, 2000111 TR MICR RES
   PADHYE J, 1998, SIGCOMM 98, P303
   RHEE I, 2000, TEAR TCP EMULATION R
   Schmitz CH, 2002, REV SCI INSTRUM, V73, P429, DOI 10.1063/1.1427768
   Srikant R., 2003, MATH INTERNET CONGES, V1st
   Tan WT, 1999, IEEE T MULTIMEDIA, V1, P172, DOI 10.1109/6046.766738
   Tang A, 2005, TELECOMMUN SYST, V30, P417, DOI 10.1007/s11235-005-5499-1
   VINNICOMBE G, 2000, CUEDFINFENGTR398 CAM
   WANG Z, 2004, NOSSDAV 03, P82
   Wu DP, 2000, P IEEE, V88, P1855, DOI 10.1109/5.899055
   Wu F, 2001, IEEE T CIRC SYST VID, V11, P332, DOI 10.1109/76.911159
   XU LS, 2004, INFOCOM 2004
   YAN J, IEEE ICNP 2004
   YAN J, 2005, 234 TIK SWISS FED I
   YAN JY, 2005, LECT NOTES COMPUTER, V3462
   Yang YR, 2001, IEEE INFOCOM SER, P1716, DOI 10.1109/INFCOM.2001.916669
   Zhang XM, 2003, IEEE T CIRC SYST VID, V13, P121, DOI 10.1109/TCSVT.2002.808437
   2003, JOINT VID TEAM JVT I
NR 33
TC 58
Z9 64
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2006
VL 8
IS 2
BP 196
EP 206
DI 10.1109/TMM.2005.864265
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 028GY
UT WOS:000236476300002
DA 2024-07-18
ER

PT J
AU Ahmad, I
   Wei, X
   Sun, Y
   Zhang, YQ
AF Ahmad, I
   Wei, X
   Sun, Y
   Zhang, YQ
TI Video transcoding: An overview of various techniques and research issues
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE frequency domain; heterogeneous video systems; H.26X; MPEG-X; motion
   vector refinement; spatial domain; video transcoding
ID MPEG VIDEO; ARCHITECTURES; ALGORITHM
AB One of the fundamental challenges in deploying multimedia systems, such as telemedicine, education, space endeavors, marketing, crisis management, transportation, and military, is to deliver smooth and uninterruptible flow of audio-visual information, anytime and anywhere. A multimedia system may consist of various devices (PCs, laptops, PDAs, smart phones, etc.) interconnected via heterogeneous wireline and wireless networks. In such systems, multimedia content originally authored and compressed with a certain format may need bit rate adjustment and format conversion in order to allow access by receiving devices with diverse capabilities (display, memory, processing, decoder). Thus, a transcoding mechanism is required to make the content adaptive to the capabilities of diverse networks and client devices. A video transcoder can perform several additional functions. For example, if the bandwidth required for a particular video is fluctuating due to congestion or other causes, a transcoder can provide fine and dynamic adjustments in the bit rate of the video bitstream in the compressed domain without imposing additional functional requirements in the decoder. In addition, a video transcoder can change the coding parameters of the compressed video, adjust spatial and temporal resolution, and modify the video content and/or the coding standard used. This paper provides an overview of several video transcoding techniques and some of the related research issues. We introduce some of the basic concepts of video transcoding, and then review and contrast various approaches while highlighting critical research issues. We propose solutions to some of these research issues, and identify possible research directions.
C1 Univ Texas, Dept Comp Sci & Engn, Arlington, TX 76019 USA.
   Microsoft Corp, Mobile & Embedded Devices Div, Redmond, WA 98052 USA.
C3 University of Texas System; University of Texas Arlington; Microsoft
RP Univ Texas, Dept Comp Sci & Engn, Arlington, TX 76019 USA.
EM iahmad@cse.uta.edu; xhwei@cse.uta.edu; yusun@mail.uca.edu;
   yzhang@microsoft.com
CR [Anonymous], IEEE J SEL AREAS COM
   Assunçao PAA, 1998, IEEE T CIRC SYST VID, V8, P953, DOI 10.1109/76.736724
   Assuncao PAA, 1997, IEE P-VIS IMAGE SIGN, V144, P377, DOI 10.1049/ip-vis:19971558
   Bjork N, 1998, IEEE T CONSUM ELECTR, V44, P88, DOI 10.1109/30.663734
   CHANG SF, 1995, IEEE J SEL AREA COMM, V13, P1, DOI 10.1109/49.363151
   Chen MJ, 2002, IEEE T CIRC SYST VID, V12, P269, DOI 10.1109/76.999204
   Dogan S, 2002, IEEE T CIRC SYST VID, V12, P453, DOI 10.1109/TCSVT.2002.800308
   DOGAN S, COMPRESSED VIDEO COM, P215
   ELEFTHERIADIS A, 1995, P IEEE INT C IM PROC, V3, P3963
   FEAMSTER N, 1999, SPIE INT S VOIC VID
   Fung KT, 2002, IEEE T IMAGE PROCESS, V11, P886, DOI 10.1109/TIP.2002.800890
   Huang KL, 2002, IEEE T CONSUM ELECTR, V48, P522, DOI 10.1109/TCE.2002.1037037
   Hwang JN, 1998, CONF REC ASILOMAR C, P1606, DOI 10.1109/ACSSC.1998.751597
   Keesman G, 1996, SIGNAL PROCESS-IMAGE, V8, P481, DOI 10.1016/0923-5965(95)00067-4
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   LIANG YQ, 2002, P IEEE INT S CIRC SY, V4, P719
   Lin YC, 2002, 2002 INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS, DIGEST OF TECHNICAL PAPERS, P134, DOI 10.1109/ICCE.2002.1013961
   Liu SH, 2002, IEEE T CIRC SYST VID, V12, P309, DOI 10.1109/TCSVT.2002.1003470
   Mohan R, 1999, IEEE T MULTIMEDIA, V1, P104, DOI 10.1109/6046.748175
   Morel A, 2001, INT CONF ACOUST SPEE, P1885, DOI 10.1109/ICASSP.2001.941312
   Nakajima Y., 1995, Proceedings. International Conference on Image Processing (Cat. No.95CB35819), P408, DOI 10.1109/ICIP.1995.537658
   PANUSOPONE K, 2001, P IEEE INT C AC SPEE, V2, P981
   ROMA N, 2002, P 14 INT C DIG SIGN, V1, P125
   Shanableh T, 2001, IEEE IMAGE PROC, P433, DOI 10.1109/ICIP.2001.959046
   Shanableh T, 2000, IEEE T MULTIMEDIA, V2, P101, DOI 10.1109/6046.845014
   Shen B, 1999, IEEE T CIRC SYST VID, V9, P929, DOI 10.1109/76.785730
   SHEN G, 2001, P IEEE INT S CIRC SY, V5, P25
   Sun HF, 1996, IEEE T CIRC SYST VID, V6, P191, DOI 10.1109/76.488826
   Sun MT, 1998, IEEE T CIRCUITS-II, V45, P644, DOI 10.1109/82.673649
   SUN Y, 2003, P IEEE INT S CIRC SY, V2
   Takahashi K, 2001, P SOC PHOTO-OPT INS, V4310, P872
   TAN KH, 1995, IEEE T IMAGE PROCESS, V4, P512, DOI 10.1109/83.370682
   Tudor PN, 1997, IEE CONF PUBL, P296, DOI 10.1049/cp:19971286
   Vetro A, 1999, IEEE T CIRC SYST VID, V9, P186, DOI 10.1109/76.744285
   Vetro A, 2001, IEEE T CIRC SYST VID, V11, P387, DOI 10.1109/76.911163
   WANG X, 2001, P WORKSH EXH MPEG 4, P83
   Werner O, 1999, IEEE T IMAGE PROCESS, V8, P179, DOI 10.1109/83.743853
   Xiao S, 2002, PROC SPIE, V4671, P172, DOI 10.1117/12.453055
   Xie R, 2002, PROC SPIE, V4671, P192, DOI 10.1117/12.453058
   Xin J, 2002, 2002 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL IV, PROCEEDINGS, P715
   Yin P, 2000, IEEE IMAGE PROC, P972, DOI 10.1109/ICIP.2000.901123
   YIN P, 2002, P SPIE VIS COMM IM P, V4671
   Youngs Gillian., 1999, International Feminist Journal of Politics, V1, P1
NR 43
TC 194
Z9 249
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2005
VL 7
IS 5
BP 793
EP 804
DI 10.1109/TMM.2005.854472
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 967IM
UT WOS:000232084900001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Vetro, A
   Timmerer, C
AF Vetro, A
   Timmerer, C
TI Digital Item Adaptation: Overview of standardization and research
   activities
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptation; Digital Item; MPEG; multimedia; quality of service;
   universal multimedia access
ID BITSTREAM SYNTAX DESCRIPTION; MULTIMEDIA; NETWORK; MPEG-21
AB MPEG-21 Digital Item Adaptation (DIA) has recently been finalized as part of the MPEG-21 Multimedia Framework. DIA specifies metadata for assisting the adaptation of Digital Items according to constraints on the storage, transmission and consumption, thereby enabling various types of quality of service management. This paper provides an overview of DIA, describes its use in multimedia applications, and reports on some of the ongoing activities in MPEG on extending DIA for use in rights governed environments.
C1 Mitsubishi Elect Res Labs, Cambridge, MA 02139 USA.
   Klagenfurt Univ, Dept Informat Technol ITEC, A-9020 Klagenfurt, Austria.
C3 University of Klagenfurt
RP Mitsubishi Elect Res Labs, Cambridge, MA 02139 USA.
EM avetro@merl.com; christian.timmerer@itec.uni-klu.ac.at
OI Timmerer, Christian/0000-0002-0031-5243
CR ADAMI N, 2002, P MULT SIGN PROC WOR
   ANG X, 2005, IEEE T MULTIMEDIA, V7, P408
   [Anonymous], 2002, Introduction to MPEG- 7: Multimedia content description interface
   Burnett IS, 2005, IEEE T MULTIMEDIA, V7, P400, DOI 10.1109/TMM.2005.846789
   Chang SF, 2005, P IEEE, V93, P148, DOI 10.1109/JPROC.2004.839600
   Chen LQ, 2003, MULTIMEDIA SYST, V9, P353, DOI 10.1007/s00530-003-0105-4
   CIMPRICH P, 2003, STREAMING TRANSORMAT
   Devillers S, 2005, IEEE T MULTIMEDIA, V7, P463, DOI 10.1109/TMM.2005.846794
   Feiten B, 2005, IEEE T MULTIMEDIA, V7, P446, DOI 10.1109/TMM.2005.846793
   Hamblin C.L., 1957, AUST J SCI, V20, P135
   HUH Y, 2001, ILLUMINATION ENV DES
   *ISO IEC, 2005, 210007 ISO IEC
   *ISO IEC, 1593812003 ISO IEC
   LIN CY, 2003, P IEEE INT C IM PROC
   Mohan R, 1999, IEEE T MULTIMEDIA, V1, P104, DOI 10.1109/6046.748175
   Mukherjee D, 2005, IEEE T MULTIMEDIA, V7, P454, DOI 10.1109/TMM.2005.846798
   NISHIKAWA H, 2003, P 5 AS PAC S INF TEL
   Panis G, 2003, SIGNAL PROCESS-IMAGE, V18, P721, DOI 10.1016/S0923-5965(03)00061-4
   Pereira F, 2003, IEEE SIGNAL PROC MAG, V20, P63, DOI 10.1109/MSP.2003.1184340
   Smith JR, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P131, DOI 10.1145/319463.319480
   SONG J, 2003, P SPIE C HUM VIS EL
   THANG TC, 2004, P 5 INT WORKSH IM AN
   Tseng BL, 2004, IEEE MULTIMEDIA, V11, P42, DOI 10.1109/MMUL.2004.1261105
   van Beek P, 2003, IEEE SIGNAL PROC MAG, V20, P40, DOI 10.1109/MSP.2003.1184338
   van der Schaar M, 2005, IEEE T MULTIMEDIA, V7, P471, DOI 10.1109/TMM.2005.846790
   VETRO A, 2003, IEEE SIGNAL PROCESS, V20
   VETRO A, IN PRESS MPEG 21 BOO
   WEE S, 2003, P INT C IM PROC BARC
   2003, 159385 ISO IEC
   2004, W3C RECOMMENDATION C
   2004, 210007 ISO IEC
NR 31
TC 66
Z9 76
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2005
VL 7
IS 3
BP 418
EP 426
DI 10.1109/TMM.2005.846795
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 949OO
UT WOS:000230798200004
DA 2024-07-18
ER

PT J
AU Tang, WK
   Wong, TT
   Heng, PA
AF Tang, WK
   Wong, TT
   Heng, PA
TI A system for real-time panorama generation and display in tele-immersive
   applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE augmented reality; image mosaicing; immersive display; real-time
   panorama; video streaming
AB Wide field-of-view (FOV) is necessary for many industrial applications, such as air traffic control, large vehicle driving and navigation. Unfortunately, the supporting structure/frame in most systems usually blocks part of the view, results in "blind spot" and raises the risk to the pilot. In this paper, we introduce a video-based tele-immersive system, called the immersive cockpit. It captures live videos from the working site and recreates an immersive environment at the remote site where the pilot situates. It immerses the pilot at the remote site with a panoramic view of the environment, and hence improves interactivity and safety. The design goals of our system are real-time, live, low-cost, and scalable. We stitch multiple video streams captured from ordinary charged couple device cameras to generate a panoramic video. To avoid being blocked by the supporting frame, we allow a flexible placement of cameras. This approach trades the accuracy of the generated panoramic image for a larger FOV. To reduce the computation, parameters for stitching are determined once during the system initialization. The panoramic video is presented on an immersive display which covers the FOV of the viewer. We discuss how to correctly present the panoramic video on this nonplanar immersive display screen by sweet spot relocation. We also present the result and the performance evaluation of the system.
C1 Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
RP Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
EM wktang@cse.cuhk.edu.hk; ttwong@acm.org; pheng@cse.cuhk.edu.hk
OI Heng, Pheng Ann/0000-0003-3055-5034
CR Anderson D. P., 1990, Proceedings. The 10th International Conference on Distributed Computing Systems (Cat. No.90CH2878-7), P54, DOI 10.1109/ICDCS.1990.89284
   BALDWIN J, 1999, P 1999 IEEE INT C RO
   BROWNING D, 1993, P 1 ANN INT C VIRT R
   BURT PJ, 1983, ACM T GRAPHIC, V2, P217, DOI 10.1145/245.247
   DANI P, 1995, PATTERN RECOGN, V28, P431, DOI 10.1016/0031-3203(94)00106-V
   Davis J., 1998, P IEEE C COMP VIS PA
   DeFanti TA, 1993, Proceedings of the 20th annual conference on Computer graphics and interactive techniques, P135, DOI 10.1145/166117.166134.
   Faugeras O., 1993, Three-dimensional computer vision: a geometric viewpoint
   FERRARI D, 1991, P 2 INT C NETW OP SY
   Foote J, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1419, DOI 10.1109/ICME.2000.871033
   GLUCKMAN J, 1998, P DARPA IM UND WORKS
   GONZALES MG, 1998, P BRIT MACH VIS C SO
   Gumustekin S, 1996, THIRD IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV '96, PROCEEDINGS, P50, DOI 10.1109/ACV.1996.571998
   Heikkila J, 1997, P C COMP VIS PATT RE
   HELIG MH, 1992, PRESENCE, V1, P279
   Hirose M, 1999, IEEE MULTIMEDIA, V6, P14, DOI 10.1109/93.790608
   Kuglin C. D., 1975, Proceedings of the 1975 International Conference on Cybernetics and Society, P163
   Majumder A, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P169, DOI 10.1145/319463.319485
   MILGRAM DL, 1977, IEEE T COMPUT, V26, P1175, DOI 10.1109/TC.1977.1674772
   MULLIGAN J, 2000, P INT S AUGM REAL OC
   Nayar S.K., 1997, P 1997 DARPA IM UND
   *NETW WORK GROUP, 1998, 2326 NETW WORK GROUP
   Park KS, 1999, P IEEE VIRT REAL ANN, P104, DOI 10.1109/VR.1999.756940
   PARTRIDGE C, 1991, 1257 REQ FOR COMM
   Peleg S, 1997, PROC CVPR IEEE, P338, DOI 10.1109/CVPR.1997.609346
   Peri V., 1997, P DARPA IM UND WORKS, P243
   *POW TEAM, POW
   Raskar R., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P161, DOI 10.1109/VISUAL.1999.809883
   RASKAR R, 2000, P IEEE VIRT REAL 200
   Raskar R., 1998, Computer Graphics, P179, DOI 10.1145/280814.280861
   Reddy BS, 1996, IEEE T IMAGE PROCESS, V5, P1266, DOI 10.1109/83.506761
   SAWHNEY HS, 1998, P EUR C COMP VIS
   Schikore DR, 2000, IEEE COMPUT GRAPH, V20, P38, DOI 10.1109/38.851748
   Shenoy PJ, 1995, ACM COMPUT SURV, V27, P636, DOI 10.1145/234782.234810
   Shum HY, 1999, COMP GRAPH, P299, DOI 10.1145/311535.311573
   SHUM HY, 1997, MSTR9723
   SWAMINATHAN R, 1999, CUCS01299
   SZELISKI R, 1994, 942 CRL DEC
   SZELISKI R, 1996, VIRTUAL REAL     MAR, P22
   Szeliski Richard., 1997, P SIGGRAPH 97 COMPUT, P251, DOI DOI 10.1145/258734.258861
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109
   XIONG Y, 1997, P CVPR SAN JUAN PR J
   FAKESPACE SYSTEMS
NR 43
TC 40
Z9 43
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2005
VL 7
IS 2
BP 280
EP 292
DI 10.1109/TMM.2005.843811
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 909OO
UT WOS:000227869400010
DA 2024-07-18
ER

PT J
AU Umapathy, K
   Krishnan, S
   Jimaa, S
AF Umapathy, K
   Krishnan, S
   Jimaa, S
TI Multigroup classification of audio signals using time-frequency
   parameters
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content-based retrieval; linear discriminant analysis; matching pursuit;
   music classification; time-frequency
AB The ongoing advancements in the multimedia technologies drive the need for efficient classification of the audio Signals to make the content-based retrieval process more accurate and much easier from huge databases. The challenge of this task lies in an accurate extraction of signal characteristics so as to derive a strong discriminatory feature suitable for classification. In this paper, a time-frequency (TF) approach for audio classification is proposed. Audio signals are nonstationary in nature and TF approach is the best way to analyze them. The audio signals were decomposed using an adaptive TF decomposition algorithm, and the signal decomposition parameter based on octave (scaling) was used to generate a set of 42 features over three frequency bands within the auditory range. These features were analyzed using linear discriminant functions and classified into six music groups (rock, classical, country, jazz, folk and pop). Overall classification accuracies as high as 97.6% was achieved by linear discriminant analysis of 170 audio signals.
C1 Univ Western Ontario, Dept Elect & Comp Engn, London, ON N6A 5B9, Canada.
   Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
   Univ Hertfordshire, Dept Elect Commun & Elect Engn, Hatfield AL10 9AB, Herts, England.
C3 Western University (University of Western Ontario); Toronto Metropolitan
   University; University of Hertfordshire
RP Univ Western Ontario, Dept Elect & Comp Engn, London, ON N6A 5B9, Canada.
EM kumapath@uwo.ca; krishnan@ee.ryerson.ca; S.A.Jimaa@herts.ac.uk
RI Krishnan, Sridhar/AAA-2542-2019
OI Krishnan, Sridhar/0000-0002-4659-564X; Jimaa, Shihab/0000-0001-9140-9108
CR Allamanche E., 2001, P INT S MUS INF RETR, P197
   COHEN L, 1989, P IEEE, V77, P941, DOI 10.1109/5.30749
   Foote JT, 1997, P SOC PHOTO-OPT INS, V3229, P138, DOI 10.1117/12.290336
   Fukunaga K., 1990, Introduction to StatisticalPattern Recognition
   Gribonval R, 2001, IEEE T SIGNAL PROCES, V49, P994, DOI 10.1109/78.917803
   Lu L, 2002, IEEE T SPEECH AUDI P, V10, P504, DOI 10.1109/TSA.2002.804546
   LU L, 2001, P IEEE INT C MULT EX, P749
   MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082
   Moore BCJ, 1992, INTRO PSYCHOL HEARIN
   SPSS Inc, 1990, US MAN
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   Umapathy K, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, pA249
   WETPHAL M, 1998, P IEEE INT C AC SPEE, P1137
   Wold E, 1996, IEEE MULTIMEDIA, V3, P27, DOI 10.1109/93.556537
   YANG C, 2001, 200114 STAND U DAT G
   2001, OVERVIEW MPEG 7 STAN
NR 16
TC 46
Z9 54
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2005
VL 7
IS 2
BP 308
EP 315
DI 10.1109/TMM.2005.843363
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 909OO
UT WOS:000227869400012
DA 2024-07-18
ER

PT J
AU Rogge, B
   Bekaert, J
   Van de Walle, R
AF Rogge, B
   Bekaert, J
   Van de Walle, R
TI Timing issues in multimedia formats: Review of the principles and
   comparison of existing formats
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE document; format; multimedia; reference model; temporal synchronization
ID SYNCHRONIZATION
AB In recent years. a large number of new multimedia (MM) formats have been created. It is widely recognized that the life cycle for a MM document consists of three areas. First of all, a document model is needed to model the MM scenario. Secondly, this document model is to be translated into a computer description. Finally, a set of synchronization primitives must be defined in order to be able to present the computer description to the end-user. The contribution of this paper is the definition of an integrated reference model that covers all three areas. For each of the three areas a model is put forward, based on the current state-of-the-art in that area. From these models, a reference model is created consisting of ten rules and a document model. This model is then applied to a number of real-world MM formats (SMIL, QuickTime, RealVideo, Advanced Streaming Format, Shockwave, and MPEG-4). Finally, a comparison is presented showing the results obtained from applying the reference model to the MM formats.
C1 State Univ Ghent, Dept Elect & Informat Syst, B-9000 Ghent, Belgium.
C3 Ghent University
RP Rogge, B (corresponding author), State Univ Ghent, Dept Elect & Informat Syst, B-9000 Ghent, Belgium.
EM Boris.Rogge@metanous.be
CR ALLEN JF, 1983, COMMUN ACM, V26, P11
   Ayars Jeff., 2001, Synchronized Multimedia Integration Language (SMIL 2.0)
   Bertino E, 1998, IEEE T KNOWL DATA EN, V10, P612, DOI 10.1109/69.706060
   Boll S, 2001, IEEE T KNOWL DATA EN, V13, P361, DOI 10.1109/69.929895
   BOLL S, 1999, 9901 U ULM DEP COMP
   BORMANS J, 2002, JTC1SC29WG11 ISOIEC
   CLASS C, 2000, SYNCHRONIZATION DIST
   DECARMO L, 1999, CORE JAVA MULTIMEDIA
   Fleischman E, 1998, ADV STREAMING FORMAT
   *ISO IEC, 1996, 135226 ISOIEC
   *ISO IEC, 1992, 10744 ISOIEC
   *ISO IEC, 1995, 135226 ISOIEC
   KIM M, 2000, MPEG4 AUTH US SMIL
   KIM M, 2000, FLEXT MOD
   KLAUSER W, 1999, REALAUDIO EVALUATION
   KOENEN R, 2001, OV MPEG4 STAND MOT P
   *MPEG, 2002, TEXT ISO IEC 210001
   PEREIRA F, 2002, MPEG4 BOOK
   PerezLuque MJ, 1996, IEEE J SEL AREA COMM, V14, P36, DOI 10.1109/49.481692
   REALN, 2001, SYNCHR MULT
   REALN, 2000, REALPR SOFTW DEV KIT
   ROUSSEAU G, 1998, P 7 INT WORLD WID WE
   SIGNES J, 1999, IS T SPIE C VIS COMM, V3653, P1506
   STEINKRAUS DC, 1993, BIOL CONTROL, V3, P93, DOI 10.1006/bcon.1993.1015
   STEINMETZ R, 1990, IEEE J SEL AREA COMM, V8, P401, DOI 10.1109/49.53016
   *SWF, 2000, MACR FLASH FIL FORM
   *SWF, 2001, SWF FIL FORM OP SWF
   WAHL T, 1993, P IEEE INT C MULT CO, P538
   YU J, 1998, P AS PAC WEB C 1998, P209
   2001, QUICKTIME STREAMING
   2001, QUICKTIME FILE FORMA
   1999, HTML 4 01 SPEC DEC
   2001, WHATS NEW QUICKTIME
   2001, QUICKTIME WIRED MOVI
NR 34
TC 14
Z9 16
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2004
VL 6
IS 6
BP 910
EP 924
DI 10.1109/TMM.2004.835213
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 872QU
UT WOS:000225224200013
DA 2024-07-18
ER

PT J
AU Liu, JC
   Li, B
   Zhang, YQ
AF Liu, JC
   Li, B
   Zhang, YQ
TI An end-to-end adaptation protocol for layered video multicast using
   optimal rate allocation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE rate allocation; scalable coding; TCP-friendliness; video multicast
ID INTERNET
AB Layered transmission is a promising solution to video multicast over the heterogeneous Internet. However, since the number of layers is practically limited, noticeable mismatches would occur between the coarse-grained layer subscription levels and the heterogeneous and dynamic rate requirements from the receivers. In this paper, we show that such mismatch can be effectively reduced using a dynamic and fine-grained layer rate allocation on the sender's side. Specifically, we study the optimization criteria for rate allocation, and propose a metric called Application-aware Fairness Index. This metric takes into consideration 1) the nonlinear relation between the perceived video quality and the delivered rate and 2) the degree of satisfaction for receivers with heterogeneous bandwidth requirements. We formulate the rate allocation into an optimization problem with the objective of maximizing the expected fairness index for all receivers in a multicast session. We then derive an efficient and scalable solution, and demonstrate that it can be seamlessly integrated into an end-to-end adaptation protocol, called Hybrid Adaptation Layered Multicast (HALM). This protocol takes advantage of the emerging fine-grained layered coding, and is fully compatible with the best-effort Internet infrastructure.
   Simulation and numerical results show that HALM noticeably improves the degree of fairness, and interacts with TCP traffic better than static allocation based protocols. More important, increasing the number of layers in HALM generally improves the degree of fairness; it is sufficient to obtain satisfactory performance with a small number of layers (three to five layers).
C1 Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
   Hong Kong Univ Sci & Technol, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   Microsoft Res, Beijing, Peoples R China.
C3 Chinese University of Hong Kong; Hong Kong University of Science &
   Technology; Microsoft
RP Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
EM csljc@ieee.org; bli@cs.ust.hk; yzhang@microsoft.com
CR BAJAJ S, 1998, P ACM SIGCOMM 98 VAN, P131
   BOLOT JC, 1994, P ACM SIGCOMM 94, V24, P58
   Floyd S, 2000, ACM SIGCOMM COMP COM, V30, P43, DOI 10.1145/347057.347397
   Francis P, 2001, IEEE ACM T NETWORK, V9, P525, DOI 10.1109/90.958323
   GORINSKY S, 2000, TR200031 U TEX AUST
   Hang HM, 1997, IEEE T CIRC SYST VID, V7, P287
   Li B, 2003, IEEE NETWORK, V17, P24
   LI S, 2000, EXPT RESULTS PROGRES
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   Li X, 1999, IEEE NETWORK, V13, P46, DOI 10.1109/65.768488
   MCCANNE S, LBNL NETWORK SIMULAT
   MCCANNE S, 1996, P ACM SIGCOMM 96 STA, P117
   Montgomery D. C., 2010, Applied Statistics and Probability for Engineers
   PADHYE J, 1998, P ACM SIGCOMM 98 SEP
   Rubenstein D, 1999, COMP COMM R, V29, P27, DOI 10.1145/316194.316203
   SARKAR S, 2000, P IEEE INFOCOM 2000
   SCHULZRINNE H, 1996, 1889 RFC
   SCHUSTER G, 1997, RATE DISTORATION BAS
   SHACHAM N, 1992, IEEE INFOCOM SER, P2107, DOI 10.1109/INFCOM.1992.263483
   SISALEM D, 2000, P 8 INT WORKSH QUAL
   SPEER M, 1996, RTP USAGE LAYERED MU
   STEVENS WR, 1997, TCP IP ILLUSTRATED, V1
   Titterington D. M., 1985, Statistical Analysis of Finite Mixture Distributions, V198
   TURLETTI T, 1997, N3296 INRIA
   Vicisano L, 1998, IEEE INFOCOM SER, P996, DOI 10.1109/INFCOM.1998.662909
   Vickers BJ, 2000, IEEE ACM T NETWORK, V8, P720, DOI 10.1109/90.893869
   WAXMAN BM, 1988, IEEE J SEL AREA COMM, V6, P1617, DOI 10.1109/49.12889
   Wu DP, 2001, IEEE T CIRC SYST VID, V11, P282, DOI 10.1109/76.911156
   YANG Y, 2000, P IEEE ICNP 00 NOV
   ZEGURA EW, 1996, P IEEE INFOCOM 96 AP
NR 30
TC 45
Z9 59
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2004
VL 6
IS 1
BP 87
EP 102
DI 10.1109/TMM.2003.819753
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765QP
UT WOS:000188295200007
DA 2024-07-18
ER

PT J
AU Tryfonas, C
   Varma, A
AF Tryfonas, C
   Varma, A
TI Efficient algorithms for computation of the loss curve of video sources
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT Packet Video Workshop
CY APR, 1999
CL NEW YORK, NEW YORK
DE MPEG-2; quality-of-service; traffic shaping; video characterization
ID NETWORK DELAY; CALCULUS
AB The loss curve of a video source characterizes the loss rate of the video stream generated by the source as a function of the allocated buffer size for a given transmission rate. The loss curve is useful in the optimal allocation of resources when the video stream is transmitted over a packet network, so that the desired tradeoff can be reached among the loss rate, bandwidth and the buffer space to be allocated in the network. We present an algorithm for computation of the entire loss curve of an elementary video stream. In contrast to earlier algorithms which employ statistical approaches, our algorithm is deterministic and computes the exact loss curve of the video stream. The algorithm exploits the piecewise linearity of the loss curve and computes only the points at which the slope of the loss curve changes. We also present an extension of the algorithm to MPEG-2 transport streams. The efficiency of the algorithm is demonstrated by results from several example video streams. For example, the algorithm was able to compute the entire loss curve of a 2-h elementary video stream in approximately 11 s on a Sun Ultra-2 workstation. The efficiency of the algorithm makes it suitable for both off- and online QoS provisioning in networked video environments.
C1 Sprint Adv Technol Labs, Burlingame, CA 94010 USA.
   Univ Calif Santa Cruz, Dept Comp Engn, Santa Cruz, CA 95064 USA.
C3 University of California System; University of California Santa Cruz
RP Tryfonas, C (corresponding author), Sprint Adv Technol Labs, Burlingame, CA 94010 USA.
CR BOTVICH DD, 1995, QUEUEING SYST, V20, P293, DOI 10.1007/BF01245322
   CHOE J, 1998, P IEEE INFOCOM 98 SA, V1, P364
   CHOE J, 1997, P IEEE INFOCOM 97 KO, V2, P549
   Cormen ThomasH., 1994, INTRO ALGORITHMS
   CRUZ RL, 1991, IEEE T INFORM THEORY, V37, P132, DOI 10.1109/18.61110
   CRUZ RL, 1991, IEEE T INFORM THEORY, V37, P114, DOI 10.1109/18.61109
   GARRETT M, 1993, THESIS COLUMBIA U NE
   *ISO IEC, 1996, 138181 ISOIEC
   LOW S, 1991, P GLOBECOM 91 DEC, V3, P1633
   SHROFF NB, 1996, P IEEE INF SAN FRANC, V2, P561
   STRALEY JC, 2001, P INT C ADV INFR EL
   TRYFONAS C, 1996, THESIS U CALIFORNIA
   TRYFONAS C, 1999, P INT TEL C ITC 99 J
   WONG MK, 1993, P IEEE INFOCOM 93 SA, V2, P395
NR 14
TC 0
Z9 0
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2003
VL 5
IS 3
BP 416
EP 428
DI 10.1109/TMM.2003.811626
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 714BM
UT WOS:000184892500013
DA 2024-07-18
ER

PT J
AU Kang, HG
   Kim, HK
   Cox, RV
AF Kang, HG
   Kim, HK
   Cox, RV
TI Improving the transcoding capability of speech coders
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE G.729; interoperability; IS-641; speech coding; tandem; transcoding;
   VoIP
ID CS-ACELP
AB With the trend of merging various communication networks, a need arises to provide transcoding between different speech coding formats. Presently this means cross tandeming the two coders in each case. This results in both quality loss and extra delay. A possible alternative is using a bitstream mapping approach that directly converts parameter values. For several standard coders having a similar coding structure, it should bib possible to generate comparable or better quality without adding much delay or complexity.
   This paper proposes a bitstream mapping method between ITU-T Recommendation G.729 and TIA IS-641. informal listening tests and the perceptual subjective quality measure (PSQM) scores show that the proposed method has better quality than the cross tandeming method, while it has at least 5 ms less delay and six times less computation.
C1 Yonsei Univ, Seoul 120749, South Korea.
   AT&T Labs Res, Florham Pk, NJ 07932 USA.
C3 Yonsei University; AT&T
RP Kang, HG (corresponding author), Yonsei Univ, Seoul 120749, South Korea.
RI Kang, Hong-Goo/G-8545-2012
CR Austin M, 1999, IEEE PERS COMMUN, V6, P20, DOI 10.1109/98.772975
   AYANOGLU E, 1992, IEEE T COMMUN, V40, P397, DOI 10.1109/26.129201
   CHEN GM, 1992, HIGH ENERG PHYS NUCL, V16, P1
   Cox RV, 1996, IEEE COMMUN MAG, V34, P34, DOI 10.1109/35.556484
   Ekudden E, 1999, P IEEE WORKSH SPEECH, P117
   GNEDENKO B., 1999, STAT RELIABILITY ENG
   Honkanen T., 1997, P INT C AC SPEECH SI, P731
   JARVINEN K, 1997, P INT C AC SPEECH SI, P771
   Minoli D., 1998, DELIVERING VOICE IP
   NETO AFC, 1999, INT J SPEECH TECHNOL, V2, P259
   NETO AFC, 1999, P ICASSP 99, P177
   Paliwal KK, 1993, IEEE T SPEECH AUDI P, V1, P3, DOI 10.1109/89.221363
   Recchione M. C., 1999, International Journal of Speech Technology, V2, P305, DOI 10.1007/BF02108646
   Salami R, 1997, IEEE COMMUN MAG, V35, P56, DOI 10.1109/35.620526
   Salami R, 1998, IEEE T SPEECH AUDI P, V6, P116, DOI 10.1109/89.661471
   ZADISSA MR, 1997, P IEEE WORKSH SPEECH, P1
NR 16
TC 8
Z9 8
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017-2394 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2003
VL 5
IS 1
BP 24
EP 33
DI 10.1109/TMM.2003.808823
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 675HP
UT WOS:000182688200003
DA 2024-07-18
ER

PT J
AU Assefa, M
   Jiang, W
   Gedamu, K
   Yilma, G
   Kumeda, B
   Ayalew, M
AF Assefa, Maregu
   Jiang, Wei
   Gedamu, Kumie
   Yilma, Getinet
   Kumeda, Bulbula
   Ayalew, Melese
TI Self-Supervised Scene-Debiasing for Video Representation Learning via
   Background Patching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action recognition; background patching; label smoothing;
   scene-debiasing; self-supervised learning; video representation
ID NETWORKS
AB Self-supervised learning has considerably improved video representation learning by discovering supervisory signals automatically from unlabeled videos. However, due to the scene-biased nature of existing video datasets, the current methods are biased to the dominant scene context during action inference. Hence, this paper proposes Background Patching (BP), a scene-debiasing augmentation strategy to alleviate the model reliance on the video background in a self-supervised contrastive manner. The BP reduces the negative influence of the video background by mixing a randomly patched frame to the video background. BP randomly crops four frames from four different videos and patches them to construct a new frame for each video separately. The patched frame is mixed with all frames of the target video to produce a spatially distorted video sample. Then, we use existing self-supervised contrastive frameworks to pull representations of the distorted and original videos closer together. Moreover, BP mixes the semantic labels of patches with the target video's label, resulting in the regularization of the contrastive model to soften the decision boundaries in the embedding space. Therefore, the model is explicitly constrained to suppress the background influence by emphasizing more on the motion changes. The extensive experimental results show that our BP significantly improved the performance of various video understanding downstream tasks including action recognition, action detection, and video retrieval.
C1 [Assefa, Maregu; Jiang, Wei; Yilma, Getinet; Kumeda, Bulbula; Ayalew, Melese] Univ Elect Sci & Technol China, Sch Informat & Software Engn, Chengdu 610054, Peoples R China.
   [Gedamu, Kumie] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China
RP Jiang, W (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Software Engn, Chengdu 610054, Peoples R China.
EM maregu2006@gmail.com; weijiang@uestc.edu.cn; alemugedamu@gmail.com;
   getinetyilma@gmail.com; bekumeda@gmail.com; meleawima@gmail.com
RI A, Getinet Yilma/GPP-2884-2022; Ayalew, Melese/HTS-6138-2023
OI A, Getinet Yilma/0000-0001-5577-3201; Ayalew,
   Melese/0000-0002-6398-6060; Assefa, Maregu/0000-0003-2815-7993; Jiang,
   Wei/0000-0001-6181-3900
FU National Natural Science Foundation of China [62072076]; Natural Science
   Foundation of Sichuan, China [2022NSFSC0500]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62072076, and in part by the Natural
   Science Foundation of Sichuan, China under Grant 2022NSFSC0500.
CR Ahsan U, 2019, IEEE WINT CONF APPL, P179, DOI 10.1109/WACV.2019.00025
   Assefa M, 2022, J CIRCUIT SYST COMP, V31, DOI 10.1142/S0218126622501596
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Choi J., 2019, Proceedings of the 33rd International Conference on Neural Information Processing Systems, P853
   Dave I, 2022, COMPUT VIS IMAGE UND, V219, DOI 10.1016/j.cviu.2022.103406
   Diba A., 2021, P IEEE CVF INT C COM, P1502
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Feichtenhofer C, 2021, PROC CVPR IEEE, P3298, DOI 10.1109/CVPR46437.2021.00331
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Gao XH, 2023, IEEE T MULTIMEDIA, V25, P405, DOI 10.1109/TMM.2021.3127040
   Gidaris S., 2018, P 6 INT C LEARNING R
   Grill Jean-Bastien, 2020, ADV NEURAL INFORM PR
   Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633
   Gutmann Michael, 2010, P MACHINE LEARNING R, P297, DOI DOI 10.1145/3292500.3330651
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   Hara K, 2021, IEEE COMPUT SOC CONF, P3344, DOI 10.1109/CVPRW53098.2021.00373
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Huang LH, 2021, PROC CVPR IEEE, P13881, DOI 10.1109/CVPR46437.2021.01367
   Jenni Simon, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P425, DOI 10.1007/978-3-030-58604-1_26
   Jing LL, 2019, Arxiv, DOI arXiv:1811.11387
   Jing LL, 2021, IEEE T PATTERN ANAL, V43, P4037, DOI 10.1109/TPAMI.2020.2992393
   Kim D, 2019, AAAI CONF ARTIF INTE, P8545
   Kong YQ, 2022, IEEE T MULTIMEDIA, V24, P1515, DOI 10.1109/TMM.2021.3066775
   Kuang H., 2021, P IEEE CVF INT C COM, P3195
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Larsson G, 2017, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2017.96
   Li C.-L., 2020, Advances in neural information processing systems, V33, P596
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099
   Li Y, 2019, PROC CVPR IEEE, P9564, DOI 10.1109/CVPR.2019.00980
   Li YW, 2018, LECT NOTES COMPUT SC, V11210, P520, DOI 10.1007/978-3-030-01231-1_32
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Luo DZ, 2020, AAAI CONF ARTIF INTE, V34, P11701
   Min K, 2019, IEEE I CONF COMP VIS, P2394, DOI 10.1109/ICCV.2019.00248
   Müller R, 2019, ADV NEUR IN, V32
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Pan T, 2021, PROC CVPR IEEE, P11200, DOI 10.1109/CVPR46437.2021.01105
   Patrick M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9557, DOI 10.1109/ICCV48922.2021.00944
   Qian R, 2021, PROC CVPR IEEE, P6960, DOI 10.1109/CVPR46437.2021.00689
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rossetto L, 2021, IEEE T MULTIMEDIA, V23, P243, DOI 10.1109/TMM.2020.2980944
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shen ZQ, 2022, AAAI CONF ARTIF INTE, P2216
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Takahashi R, 2020, IEEE T CIRC SYST VID, V30, P2917, DOI 10.1109/TCSVT.2019.2935128
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang JL, 2022, IEEE T PATTERN ANAL, V44, P3791, DOI 10.1109/TPAMI.2021.3057833
   Wang Jiangliu, 2020, ECCV, P504, DOI DOI 10.1007/978-3-030-58520-430
   Wang JP, 2021, PROC CVPR IEEE, P11799, DOI 10.1109/CVPR46437.2021.01163
   Wang ZW, 2021, PROC CVPR IEEE, P13209, DOI 10.1109/CVPR46437.2021.01301
   Xiao J, 2021, IEEE T MULTIMEDIA, V23, P3454, DOI 10.1109/TMM.2020.3025661
   Xu DJ, 2019, PROC CVPR IEEE, P10326, DOI 10.1109/CVPR.2019.01058
   Yang CY, 2020, Arxiv, DOI arXiv:2006.15489
   Yao T, 2021, AAAI CONF ARTIF INTE, V35, P10656
   Yao Y, 2020, PROC CVPR IEEE, P6547, DOI 10.1109/CVPR42600.2020.00658
   Zhang L., 2021, P BRIT MACH VIS C, P1
NR 63
TC 9
Z9 9
U1 2
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5500
EP 5515
DI 10.1109/TMM.2022.3193559
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300062
DA 2024-07-18
ER

PT J
AU Guo, J
   Wang, MT
   Zhou, Y
   Song, B
   Chi, YH
   Fan, W
   Chang, JL
AF Guo, Jie
   Wang, Meiting
   Zhou, Yan
   Song, Bin
   Chi, Yuhao
   Fan, Wei
   Chang, Jianglong
TI HGAN: Hierarchical Graph Alignment Network for Image-Text Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image-text retrieval; feature aggregation; graph convolution network;
   hierarchical alignment
ID ATTENTION
AB Image-text retrieval (ITR) is a challenging task in the field of multimodal information processing due to the semantic gap between different modalities. In recent years, researchers have made great progress in exploring the accurate alignment between image and text. However, existing works mainly focus on the fine-grained alignment between image regions and sentence fragments, which ignores the guiding significance of context background information. Actually, integrating the local fine-grained information and global context background information can provide more semantic clues for retrieval. In this paper, we propose a novel Hierarchical Graph Alignment Network (HGAN) for image-text retrieval. First, to capture the comprehensive multimodal features, we construct the feature graphs for the image and text modality respectively. Then, a multi-granularity shared space is established with a designed Multi-granularity Feature Aggregation and Rearrangement (MFAR) module, which enhances the semantic corresponding relations between the local and global information, and obtains more accurate feature representations for the image and text modalities. Finally, the ultimate image and text features are further refined through three-level similarity functions to achieve the hierarchical alignment. To justify the proposed model, we perform extensive experiments on MS-COCO and Flickr30K datasets. Experimental results show that the proposed HGAN outperforms the state-of-the-art methods on both datasets, which demonstrates the effectiveness and superiority of our model.
C1 [Guo, Jie; Wang, Meiting; Zhou, Yan; Song, Bin; Chi, Yuhao] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Fan, Wei; Chang, Jianglong] Guangdong OPPO Mobile Telecommun Corp Ltd, Dong Guan 523860, Peoples R China.
C3 Xidian University
RP Song, B; Chi, YH (corresponding author), Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
EM jguo@xidian.edu.cn; mtw@stu.xidian.edu.cn;
   zhouyan123y@stu.xidian.edu.cn; bsong@mail.xidian.edu.cn;
   yhchi@xidian.edu.cn; richard.fan@oppo.com; changjianglong@oppo.com
OI Song, Bin/0000-0002-8096-3370
FU National Natural Science Foundation of China
FX No Statement Available
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Chen JC, 2021, PROC CVPR IEEE, P15784, DOI 10.1109/CVPR46437.2021.01553
   Chen TL, 2020, AAAI CONF ARTIF INTE, V34, P10583
   Cho K., 2014, ARXIV14061078
   Deng CR, 2018, PROC CVPR IEEE, P7746, DOI 10.1109/CVPR.2018.00808
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Diao HW, 2021, AAAI CONF ARTIF INTE, V35, P1218
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Ge XR, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5185, DOI 10.1145/3474085.3475634
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1865, DOI 10.1145/3404835.3463031
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu ZB, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P789
   Huang FR, 2019, IEEE T IMAGE PROCESS, V28, P2008, DOI 10.1109/TIP.2018.2882225
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Ji Z., 2021, INT JOINT C ART INT, P765, DOI DOI 10.24963/IJCAI.2021/106
   Jiang H., 2020, IEEE CVF C COMP VIS, DOI 10.1109/CVPR42600.2020.01028
   Jin L, 2019, IEEE T IMAGE PROCESS, V28, P2173, DOI 10.1109/TIP.2018.2883522
   Jing Wang, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P4337, DOI 10.1145/3394171.3413753
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma D. P., 2015, P INT C LEARN REPR, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Kiros Ryan., 2015, Transactions of the Association for Computational Linguistics (TACL)
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lai HJ, 2016, IEEE T IMAGE PROCESS, V25, P2469, DOI 10.1109/TIP.2016.2545300
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu C., 2020, P IEEE CVF C COMP VI, P10921, DOI DOI 10.1109/CVPR42600.2020.01093
   Liu F, 2021, IEEE T MULTIMEDIA, V23, P3518, DOI 10.1109/TMM.2020.3026892
   Olszewska J. I., 2011, Proceedings of the 2011 15th IEEE International Conference on Intelligent Engineering Systems (INES), P369, DOI 10.1109/INES.2011.5954775
   Olszewska JI, 2008, INT CONF ACOUST SPEE, P721, DOI 10.1109/ICASSP.2008.4517711
   Olszewska JI, 2022, ICAART, P996, DOI 10.5220/0010993000003116
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Qi JW, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P892
   Qian SS, 2021, IEEE T MULTIMEDIA, V24, P3520, DOI 10.1109/TMM.2021.3101642
   Qu LG, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1047, DOI 10.1145/3394171.3413961
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ruoyi Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P153, DOI 10.1007/978-3-030-58565-5_10
   Shizhe Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9959, DOI 10.1109/CVPR42600.2020.00998
   Song JK, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P906
   Vaswani A, 2017, ADV NEUR IN, V30
   Velickovic Petar, 2018, INT C LEARN REPR
   Wang H, 2021, NEUROCOMPUTING, V443, P247, DOI 10.1016/j.neucom.2021.03.010
   Wang P, 2019, PROC CVPR IEEE, P1960, DOI 10.1109/CVPR.2019.00206
   Wang W, 2021, IEEE T MULTIMEDIA, V23, P2386, DOI 10.1109/TMM.2020.3011288
   Wang W, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1705
   Wang YX, 2021, IEEE T MULTIMEDIA, V23, P3362, DOI 10.1109/TMM.2020.3024822
   Wang ZH, 2019, IEEE I CONF COMP VIS, P5763, DOI 10.1109/ICCV.2019.00586
   Wehrmann P, 2020, AAAI CONF ARTIF INTE, V34, P12313
   Wei J, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3835, DOI 10.1145/3474085.3475451
   Wen KY, 2021, IEEE T CIRC SYST VID, V31, P2866, DOI 10.1109/TCSVT.2020.3030656
   Xi Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10938, DOI 10.1109/CVPR42600.2020.01095
   Yang W., 2018, P INT C LEARN REPR, P1
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yu J, 2018, LECT NOTES COMPUT SC, V11164, P223, DOI 10.1007/978-3-030-00776-8_21
   Yu LT, 2022, IEEE T MULTIMEDIA, V24, P1775, DOI 10.1109/TMM.2021.3072479
   Zeng PP, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2205, DOI 10.1145/3474085.3475380
   Zeng S, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2022, P239, DOI 10.1145/3512527.3531358
   Zhang PF, 2022, IEEE T MULTIMEDIA, V24, P466, DOI 10.1109/TMM.2021.3053766
   Zhang Q, 2020, PROC CVPR IEEE, P3533, DOI 10.1109/CVPR42600.2020.00359
NR 63
TC 7
Z9 7
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9189
EP 9202
DI 10.1109/TMM.2023.3248160
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Huang, CQ
   Xu, QW
   Wang, YF
   Wang, Y
   Zhang, Y
AF Huang, Chaoqin
   Xu, Qinwei
   Wang, Yanfeng
   Wang, Yu
   Zhang, Ya
TI Self-Supervised Masking for Unsupervised Anomaly Detection and
   Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image reconstruction; Anomaly detection; Location awareness; Image
   restoration; Training; Shape; Task analysis; anomaly localization;
   self-supervised learning; progressive mask refinement
AB Recently, anomaly detection and localization in multimedia data have received significant attention among the machine learning community. In real-world applications such as medical diagnosis and industrial defect detection, anomalies only present in a fraction of the images. To extend the reconstruction-based anomaly detection architecture to the localized anomalies, we propose a self-supervised learning approach through random masking and then restoring, named Self-Supervised Masking (SSM) for unsupervised anomaly detection and localization. SSM not only enhances the training of the inpainting network but also leads to great improvement in the efficiency of mask prediction at inference. Through random masking, each image is augmented into a diverse set of training triplets, thus enabling the autoencoder to learn to reconstruct with masks of various sizes and shapes during training. To improve the efficiency and effectiveness of anomaly detection and localization at inference, we propose a novel progressive mask refinement approach that progressively uncovers the normal regions and finally locates the anomalous regions. The proposed SSM method outperforms several state-of-the-arts for both anomaly detection and anomaly localization, achieving 98.3% AUC on Retinal-OCT and 93.9% AUC on MVTec AD, respectively.
C1 [Huang, Chaoqin; Xu, Qinwei; Wang, Yanfeng; Wang, Yu; Zhang, Ya] Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
   [Huang, Chaoqin; Xu, Qinwei; Wang, Yanfeng; Wang, Yu; Zhang, Ya] Shanghai AI Lab, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Artificial Intelligence
   Laboratory
RP Wang, YF; Zhang, Y (corresponding author), Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.; Wang, YF; Zhang, Y (corresponding author), Shanghai AI Lab, Shanghai 200240, Peoples R China.
EM huangchaoqin@sjtu.edu.cn; qinweixu@sjtu.edu.cn; wangyanfeng@sjtu.edu.cn;
   yuwangsjtu@sjtu.edu.cn; ya_zhang@sjtu.edu.cn
RI Wang, Yan-Feng/F-3288-2016; Huang, Chaoqin/HNI-0155-2023; Huang,
   Chaoqin/JCF-0974-2023
OI Wang, Yan-Feng/0000-0002-5646-4475; Huang, Chaoqin/0000-0001-6314-4472;
   Zhang, Ya/0000-0002-5390-9053
FU National Key Research and Development Program of China [2020YFB1406801];
   111 plan [BP0719010]; STCSM [18DZ2270700]; State Key Laboratory of UHD
   Video and Audio Production and Presentation
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020YFB1406801, in part by 111
   plan under Grant BP0719010, in part by STCSM under Grant 18DZ2270700,
   and in part by the State Key Laboratory of UHD Video and Audio
   Production and Presentation.
CR Akçay S, 2019, IEEE IJCNN, DOI 10.1109/ijcnn.2019.8851808
   An J., 2015, Special Lecture on IE, V2, P1
   Bansal A, 2018, LECT NOTES COMPUT SC, V11205, P397, DOI 10.1007/978-3-030-01246-5_24
   Baur C, 2019, LECT NOTES COMPUT SC, V11383, P161, DOI 10.1007/978-3-030-11723-8_16
   Bergmann P, 2020, PROC CVPR IEEE, P4182, DOI 10.1109/CVPR42600.2020.00424
   Bergmann P, 2019, PROC CVPR IEEE, P9584, DOI 10.1109/CVPR.2019.00982
   Cao V, 2016, LECT NOTES COMPUT SC, V9921, P717, DOI 10.1007/978-3-319-45823-6_67
   Chu WQ, 2019, IEEE T MULTIMEDIA, V21, P246, DOI 10.1109/TMM.2018.2846411
   Deecke L, 2019, LECT NOTES ARTIF INT, V11051, P3, DOI 10.1007/978-3-030-10925-7_1
   Dehaene D., 2020, P INT C LEARN REPR
   Denton E, 2016, Arxiv, DOI arXiv:1611.06430
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Eskin E., 2000, P 17 INT C MACH LEAR
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Eykholt K, 2018, PROC CVPR IEEE, P1625, DOI 10.1109/CVPR.2018.00175
   Fayek HM, 2020, NEURAL NETWORKS, V128, P345, DOI 10.1016/j.neunet.2020.05.011
   Gidaris S., 2019, P INT C LEARN REPR
   Golan I, 2018, 32 C NEURAL INFORM P, V31
   Gong D, 2019, IEEE I CONF COMP VIS, P1705, DOI 10.1109/ICCV.2019.00179
   Han X, 2021, AAAI CONF ARTIF INTE, V35, P4090
   Huang X, 2019, IEEE T MULTIMEDIA, V21, P2850, DOI 10.1109/TMM.2019.2911456
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jenni S, 2018, PROC CVPR IEEE, P2733, DOI 10.1109/CVPR.2018.00289
   Kang Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P360, DOI 10.1007/978-3-030-58565-5_22
   Kermany DS, 2018, CELL, V172, P1122, DOI 10.1016/j.cell.2018.02.010
   Kingma D. P., 2014, arXiv
   Kiran BR, 2018, J IMAGING, V4, DOI 10.3390/jimaging4020036
   Lee KH, 2018, ECO-EFFIC IND SCI, V33, P1, DOI 10.1007/978-3-319-70899-7_1
   Li CL, 2021, PROC CVPR IEEE, P9659, DOI 10.1109/CVPR46437.2021.00954
   Li Zhenyu, 2020, P 31 BRIT MACH VIS C
   Liu QY, 2020, AAAI CONF ARTIF INTE, V34, P11612
   Matsubara T, 2018, IEEE IJCNN
   Napoletano P, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18010209
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Ouardini K, 2019, LECT NOTES COMPUT SC, V11795, P225, DOI 10.1007/978-3-030-33391-1_26
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Perera P, 2019, PROC CVPR IEEE, P2893, DOI 10.1109/CVPR.2019.00301
   Phua Clifton, 2010, A Comprehensive Survey of Data Mining-based Fraud Detection Research, DOI [DOI 10.1016/J.CHB.2012.01.002, 10.1016/j.chb.2012.01.002]
   Rahmani M, 2017, IEEE T SIGNAL PROCES, V65, P6260, DOI 10.1109/TSP.2017.2749215
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruff L, 2018, PR MACH LEARN RES, V80
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sabokrou M, 2016, ELECTRON LETT, V52, P1122, DOI 10.1049/el.2016.0440
   Sabokrou M, 2021, IEEE T NEUR NET LEAR, V32, P675, DOI 10.1109/TNNLS.2020.2979049
   Sabokrou M, 2019, LECT NOTES COMPUT SC, V11366, P488, DOI 10.1007/978-3-030-20876-9_31
   Sabokrou M, 2018, PROC CVPR IEEE, P3379, DOI 10.1109/CVPR.2018.00356
   Sabokrou M, 2018, COMPUT VIS IMAGE UND, V172, P88, DOI 10.1016/j.cviu.2018.02.006
   Sabokrou M, 2017, IEEE T IMAGE PROCESS, V26, P1992, DOI 10.1109/TIP.2017.2670780
   Sakurada M., 2014, P MLSDA 2014 2 WORKS, P4, DOI DOI 10.1145/2689746.2689747
   Salehi M, 2021, PROC CVPR IEEE, P14897, DOI 10.1109/CVPR46437.2021.01466
   Schlegl T, 2019, MED IMAGE ANAL, V54, P30, DOI 10.1016/j.media.2019.01.010
   Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12
   Schölkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965
   Shahsavari Ali, 2021, Informatics in Medicine Unlocked, V24, DOI 10.1016/j.imu.2021.100628
   Shvetsova N, 2021, IEEE ACCESS, V9, P118571, DOI 10.1109/ACCESS.2021.3107163
   Cao VL, 2016, LECT NOTES COMPUT SC, V9594, P3, DOI 10.1007/978-3-319-30668-1_1
   Wang S., 2019, P ADV NERUAL INF PRO, P5960
   Wang SZ, 2021, PROC CVPR IEEE, P254, DOI 10.1109/CVPR46437.2021.00032
   Wang XH, 2020, KNOWL-BASED SYST, V190, DOI 10.1016/j.knosys.2019.105187
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xia Y, 2015, IEEE I CONF COMP VIS, P1511, DOI 10.1109/ICCV.2015.177
   Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156
   Xu K, 2020, IEEE T MULTIMEDIA, V22, P394, DOI 10.1109/TMM.2019.2929931
   Xu K, 2018, IEEE T MULTIMEDIA, V20, P1062, DOI 10.1109/TMM.2018.2818942
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yamanishi K, 2004, DATA MIN KNOWL DISC, V8, P275, DOI 10.1023/B:DAMI.0000023676.72185.7c
   Yan CX, 2024, IEEE T PATTERN ANAL, V46, P1530, DOI 10.1109/TPAMI.2021.3140070
   Yan XD, 2021, AAAI CONF ARTIF INTE, V35, P3110
   Ye F, 2022, IEEE T MULTIMEDIA, V24, P116, DOI 10.1109/TMM.2020.3046884
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu QH, 2021, PROC CVPR IEEE, P1154, DOI 10.1109/CVPR46437.2021.00121
   Yuan D, 2021, IEEE T IMAGE PROCESS, V30, P976, DOI 10.1109/TIP.2020.3037518
   Zavrtanik V, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107706
   Zenati H, 2018, ARXIV
   Zhang JP, 2021, IEEE T MED IMAGING, V40, P879, DOI 10.1109/TMI.2020.3040950
   Zhang Q, 2022, IEEE T IMAGE PROCESS, V31, P352, DOI 10.1109/TIP.2021.3128330
   Zhang ZW, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3237
   Zhou C, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P665, DOI 10.1145/3097983.3098052
   Zhou K, 2022, IEEE T NEUR NET LEAR, V33, P2335, DOI 10.1109/TNNLS.2021.3101403
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu XN, 2020, MATH PROBL ENG, V2020, DOI 10.1155/2020/5217429
   Zong B., 2018, P 6 INT C LEARN REPR
NR 82
TC 17
Z9 17
U1 18
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4426
EP 4438
DI 10.1109/TMM.2022.3175611
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200025
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jiang, JJ
   Liu, ZY
   Zheng, NN
AF Jiang, Jingjing
   Liu, Ziyi
   Zheng, Nanning
TI LiVLR: A Lightweight Visual-Linguistic Reasoning Framework for Video
   Question Answering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video question answering; relational reasoning; graph convolutional
   network; representation integration
ID GRAPH CONVOLUTIONAL NETWORKS
AB Video Question Answering (VideoQA), aiming to correctly answer a given question based on understanding multimodal video content, is challenging due to the richness of the video content. From the perspective of video understanding, a complete VideoQA framework needs to understand the video content at different semantic levels and flexibly integrate diverse video content to distill question-related content. To this end, we propose a Lightweight Visual-Linguistic Reasoning framework named LiVLR. Specifically, LiVLR first utilizes graph-based visual and linguistic encoders to obtain multi-grained visual and linguistic representations, respectively. Subsequently, the obtained representations are integrated with the devised Diversity-aware Visual-Linguistic Reasoning module (DaVL). DaVL distinguishes different types of representations with the learnable index embedding in graph embedding. Therefore, DaVL can flexibly adjust the importance of different representations when generating the question-related joint representation. The proposed LiVLR is lightweight and shows its performance advantage on three VideoQA benchmarks, MRSVTT-QA, KnowIT VQA, and TVQA. Extensive ablation studies demonstrate the effectiveness of the key components of LiVLR.
C1 [Jiang, Jingjing; Liu, Ziyi; Zheng, Nanning] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shannxi, Peoples R China.
C3 Xi'an Jiaotong University
RP Zheng, NN (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shannxi, Peoples R China.
EM jingjingjiang2017@gmail.com; liuziyi@stu.xjtu.edu.cn;
   nnzheng@mail.xjtu.edu.cn
RI Ziyi, Liu/IZE-3373-2023
OI Ziyi, Liu/0000-0003-2198-7708; Liu, Ziyi/0000-0002-2407-9599; Jiang,
   jingjing/0000-0002-8241-4877
FU National Natural Science Foundation of China [62088102, 61773312]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 62088102 and 61773312.
CR Amizadeh S., 2020, arXiv
   Amrani E, 2021, AAAI CONF ARTIF INTE, V35, P6644
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2015, Adv Neural Inf Proces Syst
   Bai L, 2022, IEEE T PATTERN ANAL, V44, P783, DOI 10.1109/TPAMI.2020.3011866
   Bruna J., 2014, ABS13126203 CORR, P1, DOI [10.48550/arXiv.1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Cadene R, 2019, PROC CVPR IEEE, P1989, DOI 10.1109/CVPR.2019.00209
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen C, 2020, IEEE WINT CONF APPL, P1526, DOI 10.1109/WACV45572.2020.9093592
   Chen S., 2020, P IEEE CVF C COMP VI, P10638
   Chen YP, 2019, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2019.00052
   Defferrard M, 2016, ADV NEUR IN, V29
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan CY, 2019, PROC CVPR IEEE, P1999, DOI 10.1109/CVPR.2019.00210
   Gao Difei, 2020, CVPR, P12746
   Gao JY, 2018, PROC CVPR IEEE, P6576, DOI 10.1109/CVPR.2018.00688
   Garcez A. d., 2019, NEURAL SYMBOLIC COMP
   Garcia N., 2020, arXiv
   Garcia N., 2020, P AAAI C ART INT, P10106
   Hamilton WL, 2017, ADV NEUR IN, V30
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henaff M., 2015, ARXIV150605163
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang D, 2020, AAAI CONF ARTIF INTE, V34, P11021
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jang Y, 2017, PROC CVPR IEEE, P1359, DOI 10.1109/CVPR.2017.149
   Jiang JW, 2020, AAAI CONF ARTIF INTE, V34, P11101
   Jiang JJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P199, DOI 10.1145/3474085.3475350
   Jiang P, 2020, AAAI CONF ARTIF INTE, V34, P11109
   Jin WK, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1193, DOI 10.1145/3343031.3351065
   Jingli Wang, 2021, Arabian Journal of Geosciences, V14, DOI 10.1007/s12517-021-06893-y
   Junyeong Kim, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10103, DOI 10.1109/CVPR42600.2020.01012
   Kim H., 2020, P 58 ANN M ACL, P4812
   Kim J, 2019, PROC CVPR IEEE, P8329, DOI 10.1109/CVPR.2019.00853
   Kim KM, 2018, LECT NOTES COMPUT SC, V11219, P698, DOI 10.1007/978-3-030-01267-0_41
   Kipf TN, 2017, INT C LEARN REPR
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lei J., 2019, P ASS COMP LING, P8211
   Lei J, 2021, PROC CVPR IEEE, P7327, DOI 10.1109/CVPR46437.2021.00725
   Lei J, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P1369
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Li XP, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1166, DOI 10.1145/3343031.3350971
   Loshchilov I., 2019, DECOUPLED WEIGHT DEC
   Marcheggiani D., 2017, Encoding sentences with graph convolutional networks for semantic role labeling
   Norcliffe-Brown W, 2018, ADV NEUR IN, V31
   Park J, 2021, PROC CVPR IEEE, P15521, DOI 10.1109/CVPR46437.2021.01527
   Parkhi OM, 2015, Proceedings of the British Machine Vision Conference, DOI DOI 10.5244/C.29.41
   Pei H., 2020, P INT JOINT C NEUR N, P1
   Ren S., 2015, P ADV NEUR INF PROC, P91
   Santoro A, 2017, ADV NEUR IN, V30
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Seo A., 2021, P ANN M ASS COMP LIN, P6167, DOI 10.18653/v1/2021.acllong.481
   Seo PH, 2021, PROC CVPR IEEE, P16872, DOI 10.1109/CVPR46437.2021.01660
   Shi P., 2019, Simple bert models for relation extraction and semantic role labeling
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song X, 2022, IEEE T MULTIMEDIA, V24, P2914, DOI 10.1109/TMM.2021.3090595
   Thao Minh Le, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9969, DOI 10.1109/CVPR42600.2020.00999
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam Ramakrishna., 2019, arXiv
   Velickovie P., 2017, arXiv
   Wang W, 2021, IEEE T MULTIMEDIA, V23, P2386, DOI 10.1109/TMM.2020.3011288
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xu DJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1645, DOI 10.1145/3123266.3123427
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1666, DOI 10.1109/ICCV48922.2021.00171
   Yang TH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1184, DOI 10.1145/3343031.3350969
   Yang ZQ, 2020, IEEE IMAGE PROC, P1411, DOI 10.1109/ICIP40778.2020.9190771
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Zha J., 2019, ACM Trans. MultimediaComput., Commun., Appl., V15, P1
   Zhang MH, 2018, AAAI CONF ARTIF INTE, P4438
   Zhang WF, 2020, INFORM FUSION, V55, P116, DOI 10.1016/j.inffus.2019.08.009
   Zhu Z., 2020, arXiv
NR 77
TC 4
Z9 4
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5002
EP 5013
DI 10.1109/TMM.2022.3185900
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300029
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, SJ
   Lin, MB
   Wang, Y
   Chao, F
   Shao, L
   Ji, RR
AF Li, Shaojie
   Lin, Mingbao
   Wang, Yan
   Chao, Fei
   Shao, Ling
   Ji, Rongrong
TI Learning Efficient GANs for Image Translation via Differentiable Masks
   and Co-Attention Distillation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generative adversarial networks; GAN compression; image translation;
   knowledge distillation; network pruning
AB Generative Adversarial Networks (GANs) have been widely-used in image translation, but their high computation and storage costs impede the deployment on mobile devices. Prevalent methods for CNN compression cannot be directly applied to GANs due to the peculiarties of GAN tasks and the unstable adversarial training. To solve these, in this paper, we introduce a novel GAN compression method, termed DMAD, by proposing a Differentiable Mask and a co-attention Distillation. The former searches for a light-weight generator architecture in a training-adaptive manner. To overcome channel inconsistency when pruning the residual connections, an adaptive cross-block group sparsity is further incorporated. The latter simultaneously distills informative attention maps from both the generator and discriminator of a pre-trained model to the searched generator, effectively stabilizing the adversarial training of our light-weight model. Experiments show that DMAD can reduce the Multiply Accumulate Operations (MACs) of CycleGAN by 13x and that of Pix2Pix by 4x while retaining a comparable performance against the full model.
C1 [Li, Shaojie; Lin, Mingbao; Chao, Fei; Ji, Rongrong] Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Media Analyt & Comp Lab, Xiamen 361005, Peoples R China.
   [Lin, Mingbao] Tencent, Youtu Lab, Shanghai 200233, Peoples R China.
   [Wang, Yan] Pinterest, Seattle, WA 98101 USA.
   [Shao, Ling] Mohamed bin Zayed Univ Artificial Intelligence, Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.
   [Shao, Ling] Mohamed bin Zayed Univ Artificial Intelligence, Abu Dhabi, U Arab Emirates.
   [Ji, Rongrong] Xiamen Univ, Inst Artificial Intelligence, Xiamen 361005, Peoples R China.
C3 Xiamen University; Tencent; Mohamed Bin Zayed University of Artificial
   Intelligence; Mohamed Bin Zayed University of Artificial Intelligence;
   Xiamen University
RP Ji, RR (corresponding author), Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Media Analyt & Comp Lab, Xiamen 361005, Peoples R China.
EM shaojieli@stu.xmu.edu.cn; lmbxmu@stu.xmu.edu.cn; yanw@pinterest.com;
   fchao@xmu.edu.cn; ling.shao@ieee.org; rrji@xmu.edu.cn
RI Shao, Ling/D-3535-2011
OI Wang, Yan/0000-0003-4309-3166; Li, Shaojie/0000-0002-7203-2488
FU National Science Fund for Distinguished Young Scholars [62025603];
   National Natural Science Foundation of China [U1705262, 62176222,
   62176223, 62176226, 62072386, 62072387, 62072389, 62002305, 61772443,
   61802324, 61702136]; Guangdong Basic and Applied Basic Research
   Foundation [2019B1515120049]; Natural Science Foundation of Fujian
   Province of China [2021J01002]; Fundamental Research Funds for the
   central universities [20720200077, 20720200090, 20720200091]
FX This work was supported by the National Science Fund for Distinguished
   Young Scholars under Grant 62025603, the National Natural Science
   Foundation of China under Grants U1705262, 62176222, 62176223, 62176226,
   62072386, 62072387, 62072389, 62002305, 61772443, 61802324 and 61702136,
   Guangdong Basic and Applied Basic Research Foundation under Grant
   2019B1515120049, the Natural Science Foundation of Fujian Province of
   China under Grant 2021J01002, and the Fundamental Research Funds for the
   central universities under Grants 20720200077, 20720200090 and
   20720200091.~
CR Aguinaldo A, 2019, Arxiv, DOI arXiv:1902.00159
   [Anonymous], 1990, Adv Neural Inform Process Syst.
   [Anonymous], 2017, P IEEE C COMP VIS PA
   Azadil S, 2018, PROC CVPR IEEE, P7564, DOI 10.1109/CVPR.2018.00789
   Brock A, 2019, PROC INT C LEARN REP
   Chen HT, 2020, AAAI CONF ARTIF INTE, V34, P3585
   Chen T., 2021, P ADV NEUR INF PROC, V34, P25294
   Chen X., 2021, PROC INT C LEARN REP
   Chung I, 2020, PR MACH LEARN RES, V119
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Denil M., 2013, P 26 INT C NEUR INF, P2148
   Ding X., 2019, Advances in Neural Information Processing Systems (NeurIPS), P6382
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Frankle J., 2018, ARXIV180303635
   Fu Y., 2020, PMLR, V119, P3292
   Gong LQ, 2021, LECT NOTES COMPUT SC, V12892, P637, DOI 10.1007/978-3-030-86340-1_51
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352
   Han S, 2015, ADV NEUR IN, V28
   Hayashi K., 2019, Advances in Neural Information Processing Systems, P5552
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Hensel M, 2017, ADV NEUR IN, V30
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Huang ZH, 2018, LECT NOTES COMPUT SC, V11220, P317, DOI 10.1007/978-3-030-01270-0_19
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jin Qing, 2021, P IEEE C COMP VIS PA, P13600
   Kang M., 2020, PR MACH LEARN RES, P5122
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Komodakis N, 2017, P ICLR
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Ngo LM, 2022, IEEE T MULTIMEDIA, V24, P377, DOI 10.1109/TMM.2021.3050672
   Leng C, 2018, AAAI CONF ARTIF INTE, P3466
   Li MY, 2020, PROC CVPR IEEE, P5283, DOI 10.1109/CVPR42600.2020.00533
   Lin MB, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P673
   Lin MB, 2020, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR42600.2020.00160
   Liu Z., 2018, PROC INT C LEARN REP, P1
   Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Iandola FN, 2016, Arxiv, DOI arXiv:1602.07360
   Peng F, 2020, IEEE T MULTIMEDIA, V22, P2511, DOI 10.1109/TMM.2019.2959443
   Romero A, 2015, PROC INT C LEARN REP
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shu H, 2019, IEEE I CONF COMP VIS, P3234, DOI 10.1109/ICCV.2019.00333
   Tsunashima H, 2021, INT C PATT RECOG, P10636, DOI 10.1109/ICPR48806.2021.9413150
   Wang Huiyu, 2020, EUR C COMP VIS
   Wang YL, 2020, AAAI CONF ARTIF INTE, V34, P12273
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   Yu A, 2014, PROC CVPR IEEE, P192, DOI 10.1109/CVPR.2014.32
   Yu F., 2017, PROC CVPR IEEE, P472, DOI [DOI 10.1109/CVPR.2017.75, 10.1109/CVPR.2017.75]
   Zeqi Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P648, DOI 10.1007/978-3-030-58574-7_39
   Zhang H, 2019, 36 INT C MACHINE LEA, V97
   Zhang ML, 2021, IEEE T MULTIMEDIA, V23, P1938, DOI 10.1109/TMM.2020.3006414
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhou YR, 2018, AAAI CONF ARTIF INTE, P4596
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhuang ZW, 2018, ADV NEUR IN, V31
NR 57
TC 4
Z9 4
U1 3
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3180
EP 3189
DI 10.1109/TMM.2022.3156699
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200018
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, WJ
   Wang, HX
   Chen, YJ
   Abdullahi, SM
   Luo, J
AF Li, Wanjie
   Wang, Hongxia
   Chen, Yijing
   Abdullahi, Sani M.
   Luo, Jie
TI Constructing Immunized Stego-Image for Secure Steganography via
   Artificial Immune System
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image steganography; steganalysis; artificial immune system; antibody
   affinity
ID LEARNING FRAMEWORK; STEGANALYSIS; OPTIMIZATION; DISTORTION
AB Adaptive image steganography is the process of embedding secret messages into undetectable regions of a cover image through the design of a distortion function by a steganographer. Since the state-of-the-art steganalyzers are mainly based on image residual analysis, it is reasonable to modify stego image for withstanding steganalysis by reducing or eliminating the image residual distance between cover and stego image. However, simply modifying stego images may lead to message extraction failure and the introduction of additional detectable artifacts. In this paper, we propose a novel secure steganography strategy by constructing immunized stego-image via an artificial immune system, called ISteg, which ensures the accurate extraction of hidden data while enhancing the security against steganalyzers. Inspired by the biological immune system, we use an artificial immune system (AIS) to build ISteg. Specifically, ISteg generates the immunized stego-image by automatically modifying the stego to maximize the affinity of the antibody. The affinity is developed to evaluate antibody quality according to the Euclidean distance between the residual co-occurrence matrix features of the cover image and the modified stego image. In this manner, the so-called immunized stego-image is generated. Extensive experimental results demonstrate that the proposed ISteg strategy can effectively improve the security performance of existing steganography.
C1 [Li, Wanjie; Wang, Hongxia; Chen, Yijing; Luo, Jie] Sichuan Univ, Sch Cyber Sci & Engn, Chengdu 610065, Peoples R China.
   [Wang, Hongxia] Zhengzhou Xinda Inst Adv Technol, Zhengzhou 450001, Peoples R China.
   [Abdullahi, Sani M.] China Three Gorges Univ, Coll Comp & Informat Technol, Yichang 443002, Peoples R China.
C3 Sichuan University; China Three Gorges University
RP Wang, HX (corresponding author), Sichuan Univ, Sch Cyber Sci & Engn, Chengdu 610065, Peoples R China.; Wang, HX (corresponding author), Zhengzhou Xinda Inst Adv Technol, Zhengzhou 450001, Peoples R China.
EM li_wanjie1014@stu.scu.edu.cn; hxwang@scu.edu.cn;
   chenyijing@stu.scu.edu.cn; sani@my.swjtu.edu.cn; luojie1@stu.scu.edu.cn
RI Wang, Hongxia/AAE-2135-2022; Chen, Yi-Jing/AAK-8059-2020; Abdullahi,
   Sani/HLH-2485-2023; Li, Wanjie/JRW-9759-2023
OI Abdullahi, Sani/0000-0003-4962-2794; Li, Wanjie/0000-0001-7175-7287
FU National Natural Science Foundation of China
FX No Statement Available
CR Ahmadi SD, 2017, 2017 ARTIFICIAL INTELLIGENCE AND ROBOTICS (IRANOPEN), P45, DOI 10.1109/RIOS.2017.7956442
   [Anonymous], 2011, J. Inform. Hid. Multimed. Signal Process.
   [Anonymous], 2015, P 3 ACM WORKSH INF H
   Bas Patrick, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P59, DOI 10.1007/978-3-642-24178-9_5
   Bas P., 2007, Bows-2
   Chen BL, 2019, IH&MMSEC '19: PROCEEDINGS OF THE ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, P63, DOI 10.1145/3335203.3335716
   Chen KJ, 2019, IEEE T INF FOREN SEC, V14, P1052, DOI 10.1109/TIFS.2018.2869353
   Chen YL, 2021, IEEE T DEPEND SECURE, V18, P1320, DOI 10.1109/TDSC.2019.2932983
   Chen Y, 2022, IEEE T DEPEND SECURE, V19, P2405, DOI 10.1109/TDSC.2021.3058134
   Cogranne R, 2019, IH&MMSEC '19: PROCEEDINGS OF THE ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, P125
   Dasgupta D, 1997, IEEE SYS MAN CYBERN, P873, DOI 10.1109/ICSMC.1997.626212
   Dasgupta D, 2011, APPL SOFT COMPUT, V11, P1574, DOI 10.1016/j.asoc.2010.08.024
   de Castro LN, 2002, IEEE T EVOLUT COMPUT, V6, P239, DOI 10.1109/TEVC.2002.1011539
   Denemark T., 2016, Electron. Imag., V28, P1
   Dietterich TG, 1998, NEURAL COMPUT, V10, P1895, DOI 10.1162/089976698300017197
   Fernandes DAB, 2017, J INF SECUR APPL, V35, P138, DOI 10.1016/j.jisa.2017.06.007
   Filler T, 2011, IEEE T INF FOREN SEC, V6, P920, DOI 10.1109/TIFS.2011.2134094
   Fridrich J., 2009, Steganography in Digital Media: Principles, Algorithms, and Applications
   Fridrich J, 2013, INT CONF ACOUST SPEE, P2949, DOI 10.1109/ICASSP.2013.6638198
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Holub V., 2013, P 1 ACM WORKSH INF H, P59, DOI DOI 10.1145/2482513.2482514
   Holub V, 2012, IEEE INT WORKS INFOR, P234, DOI 10.1109/WIFS.2012.6412655
   Hongxia Wang, 2021, Digital Forensics and Watermarking 19th International Workshop, IWDW 2020. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12617), P53, DOI 10.1007/978-3-030-69449-4_5
   Li B, 2018, IEEE T INF FOREN SEC, V13, P1242, DOI 10.1109/TIFS.2017.2780805
   Li B, 2014, IEEE IMAGE PROC, P4206, DOI 10.1109/ICIP.2014.7025854
   Li B, 2015, IEEE T INF FOREN SEC, V10, P1905, DOI 10.1109/TIFS.2015.2434600
   Li JQ, 2021, IEEE T FUZZY SYST, V29, P3234, DOI 10.1109/TFUZZ.2020.3016225
   Li L, 2020, IEEE T MULTIMEDIA, V22, P2526, DOI 10.1109/TMM.2019.2959909
   Lie WN, 2005, IEEE T MULTIMEDIA, V7, P1007, DOI 10.1109/TMM.2005.858377
   Mo XB, 2021, IEEE T INF FOREN SEC, V16, P4306, DOI 10.1109/TIFS.2021.3104140
   Pevny T, 2010, LECT NOTES COMPUT SC, V6387, P161, DOI 10.1007/978-3-642-16435-4_13
   Pevny T, 2010, IEEE T INF FOREN SEC, V5, P215, DOI 10.1109/TIFS.2010.2045842
   Qian SQ, 2016, IEEE T CYBERNETICS, V46, P2056, DOI 10.1109/TCYB.2015.2461651
   Sallee P, 2004, LECT NOTES COMPUT SC, V2939, P154
   Sedighi V, 2016, IEEE T INF FOREN SEC, V11, P221, DOI 10.1109/TIFS.2015.2486744
   Sedighi V, 2015, PROC SPIE, V9409, DOI 10.1117/12.2080272
   Su WK, 2021, IEEE T CIRC SYST VID, V31, P1001, DOI 10.1109/TCSVT.2020.3001122
   Tang WX, 2021, IEEE T INF FOREN SEC, V16, P952, DOI 10.1109/TIFS.2020.3025438
   Tang WX, 2019, IEEE T INF FOREN SEC, V14, P2074, DOI 10.1109/TIFS.2019.2891237
   Tang WX, 2017, IEEE SIGNAL PROC LET, V24, P1547, DOI 10.1109/LSP.2017.2745572
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu GS, 2016, IEEE SIGNAL PROC LET, V23, P708, DOI 10.1109/LSP.2016.2548421
   Yang JH, 2020, IEEE T INF FOREN SEC, V15, P839, DOI 10.1109/TIFS.2019.2922229
   Yichen Tong, 2020, Digital Forensics and Watermarking. 18th International Workshop, IWDW 2019. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12022), P247, DOI 10.1007/978-3-030-43575-2_21
   Zha HY, 2019, IEEE IMAGE PROC, P2284, DOI [10.1109/icip.2019.8804415, 10.1109/ICIP.2019.8804415]
   Zhang JS, 2022, IEEE T MULTIMEDIA, V24, P4538, DOI 10.1109/TMM.2021.3119994
   Zhang R, 2020, IEEE T INF FOREN SEC, V15, P1138, DOI 10.1109/TIFS.2019.2936913
   Zhang WM, 2017, IEEE T CIRC SYST VID, V27, P2274, DOI 10.1109/TCSVT.2016.2587388
   Zhang WW, 2014, IEEE T CYBERNETICS, V44, P185, DOI 10.1109/TCYB.2013.2250956
   Zhou WB, 2017, IEEE T INF FOREN SEC, V12, P2654, DOI 10.1109/TIFS.2017.2718480
NR 50
TC 7
Z9 7
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8320
EP 8333
DI 10.1109/TMM.2023.3234812
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000068
DA 2024-07-18
ER

PT J
AU Liu, CL
   Wu, ZH
   Wen, J
   Xu, Y
   Huang, C
AF Liu, Chengliang
   Wu, Zhihao
   Wen, Jie
   Xu, Yong
   Huang, Chao
TI Localized Sparse Incomplete Multi-View Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Common latent representation; graph embedding; incomplete multi-view
   clustering; matrix factorization
AB Incomplete multi-view clustering, which aims to solve the clustering problem on the incomplete multi-view data with partial view missing, has received more and more attention in recent years. Although numerous methods have been developed, most of the methods either cannot flexibly handle the incomplete multi-view data with arbitrary missing views or do not consider the negative factor of information imbalance among views. Moreover, some methods do not fully explore the local structure of all incomplete views. To tackle these problems, this paper proposes a simple but effective method, named localized sparse incomplete multi-view clustering (LSIMVC). Different from the existing methods, LSIMVC intends to learn a sparse and structured consensus latent representation from the incomplete multi-view data by optimizing a sparse regularized and novel graph embedded multi-view matrix factorization model. Specifically, in such a novel model based on the matrix factorization, a norm based sparse constraint is introduced to obtain the sparse low-dimensional individual representations and the sparse consensus representation. Moreover, a novel local graph embedding term is introduced to learn the structured consensus representation. Different from the existing works, our local graph embedding term aggregates the graph embedding task and consensus representation learning task into a concise term. Furthermore, to reduce the imbalance factor of incomplete multi-view learning, an adaptive weighted learning scheme is introduced to LSIMVC. Comprehensive experimental results performed on six incomplete multi-view databases verify that the performance of our LSIMVC is superior to the state-of-the-art IMC approaches.
C1 [Liu, Chengliang; Wu, Zhihao; Wen, Jie; Huang, Chao] Harbin Inst Technol, Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.
   [Xu, Yong] Harbin Inst Technol, Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.
   [Xu, Yong] Pengcheng Lab, Shenzhen 518055, Peoples R China.
C3 Harbin Institute of Technology; Harbin Institute of Technology
RP Wen, J (corresponding author), Harbin Inst Technol, Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.; Xu, Y (corresponding author), Harbin Inst Technol, Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.; Xu, Y (corresponding author), Pengcheng Lab, Shenzhen 518055, Peoples R China.
EM liucl1996@163.com; horatio_ng@163.com; jiewen_pr@126.com;
   yongxu@ymail.com; huangchao_08@126.com
RI Liu, Chengliang/CAF-8032-2022; Wen, Jie/AAH-8083-2020; Wen,
   Jie/G-7235-2015
OI Liu, Chengliang/0000-0001-5983-8981; Wen, Jie/0000-0001-9554-2379; Chao,
   Huang/0000-0003-1490-2171
FU Shenzhen Science and Technology Program [RCBS20210609103709020];
   Shenzhen Science and Technology Innovation Committee
   [GJHZ20210705141812038]
FX This work was supported in part by the Shenzhen Science andTechnology
   Program under Grant RCBS20210609103709020 and in part by the Shenzhen
   Science and Technology Innovation Committee under Grant
   GJHZ20210705141812038. The Associate Editor coordinating the review
   ofthis manuscript and approving it for publication was Prof. Andrew D.
   Bagdanov.
CR [Anonymous], 2014, P ADV NEURAL INFORM
   Asuncion A., 2007, Uci machine learning repository
   Blaschko MB, 2008, LECT NOTES ARTIF INT, V5211, P133, DOI 10.1007/978-3-540-87479-9_27
   Cai X., 2013, 23 INT JOINT C ART I, P2598
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Cao XC, 2015, IEEE T IMAGE PROCESS, V24, P4381, DOI 10.1109/TIP.2015.2463223
   Chao G., 2022, Appl. Intell., P1
   Chao Guoqing, 2021, IEEE Trans Artif Intell, V2, P146, DOI 10.1109/tai.2021.3065894
   Chao GQ, 2016, INFORM SCIENCES, V367, P296, DOI 10.1016/j.ins.2016.06.004
   Felzenszwalb P, 2008, PROC CVPR IEEE, P1984
   Gao HC, 2015, IEEE I CONF COMP VIS, P4238, DOI 10.1109/ICCV.2015.482
   Greene D., 2006, P 23 INT C MACH LEAR, V148, P377
   Guo J, 2018, AAAI CONF ARTIF INTE, P298
   Hardoon D. R., 2003, P INT WORKSH CONT BA, P22
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hu ML, 2019, AAAI CONF ARTIF INTE, P3838
   Hu ML, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2262
   Huang C, 2023, IEEE T NEUR NET LEAR, V34, P9389, DOI 10.1109/TNNLS.2022.3159538
   Huang C, 2022, IEEE T IND INFORM, V18, P5171, DOI 10.1109/TII.2021.3122801
   Huang C, 2022, IEEE T CYBERNETICS, V52, P13834, DOI 10.1109/TCYB.2021.3127716
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li SY, 2014, AAAI CONF ARTIF INTE, P1968
   Li T, 2009, IEEE T MULTIMEDIA, V11, P477, DOI 10.1109/TMM.2009.2012942
   Li YQ, 2015, AAAI CONF ARTIF INTE, P2750
   Li ZL, 2022, IEEE T MULTIMEDIA, V24, P2461, DOI 10.1109/TMM.2021.3081930
   Lin YJ, 2021, PROC CVPR IEEE, P11169, DOI 10.1109/CVPR46437.2021.01102
   Liu GC, 2017, IEEE T PATTERN ANAL, V39, P47, DOI 10.1109/TPAMI.2016.2539946
   Liu XW, 2021, IEEE T PATTERN ANAL, V43, P2634, DOI 10.1109/TPAMI.2020.2974828
   Liu XW, 2019, AAAI CONF ARTIF INTE, P4392
   Liu XW, 2020, IEEE T PATTERN ANAL, V42, P1191, DOI 10.1109/TPAMI.2019.2892416
   Luo XL, 2021, PATTERN RECOGN, V120, DOI 10.1016/j.patcog.2021.108104
   Ngiam J., 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.5555/3104482.3104569
   Nie F., 2016, IJCAI, P1881
   Nie FP, 2017, AAAI CONF ARTIF INTE, P2408
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Peng YG, 2012, IEEE T PATTERN ANAL, V34, P2233, DOI 10.1109/TPAMI.2011.282
   Rai N, 2016, INT C PATT RECOG, P2192, DOI 10.1109/ICPR.2016.7899961
   Rai P., 2010, P NEUR INF PROC SYST, P1
   Shao WX, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P1012, DOI 10.1109/BigData.2016.7840701
   Shao WX, 2015, LECT NOTES ARTIF INT, V9284, P318, DOI 10.1007/978-3-319-23528-8_20
   Shawe-Taylor John, 2004, KERNEL METHODS PATTE
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang H, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3677
   Wang QQ, 2018, IEEE DATA MINING, P1290, DOI 10.1109/ICDM.2018.00174
   Wang Y, 2015, IEEE T IMAGE PROCESS, V24, P3939, DOI 10.1109/TIP.2015.2457339
   Wen J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3753, DOI 10.1145/3394171.3413807
   Wen J, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3230
   Wen J, 2021, IEEE T CYBERNETICS, V51, P101, DOI 10.1109/TCYB.2020.2987164
   Wen J, 2019, AAAI CONF ARTIF INTE, P5393
   Wen J, 2018, NEURAL NETWORKS, V102, P36, DOI 10.1016/j.neunet.2018.02.002
   Wu BY, 2013, PROC CVPR IEEE, P3507, DOI 10.1109/CVPR.2013.450
   Xu N, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1794, DOI 10.1145/3240508.3240679
   Yang MX, 2023, IEEE T PATTERN ANAL, V45, P1055, DOI 10.1109/TPAMI.2022.3155499
   Yang Y, 2017, IEEE T KNOWL DATA EN, V29, P1834, DOI 10.1109/TKDE.2017.2701825
   Zhang CQ, 2019, ADV NEUR IN, V32
   Zhang Z, 2019, IEEE T PATTERN ANAL, V41, P1774, DOI 10.1109/TPAMI.2018.2847335
   Zhao H., 2016, IJCAI, P2392
   Zhuang LS, 2012, PROC CVPR IEEE, P2328, DOI 10.1109/CVPR.2012.6247944
   Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430
NR 61
TC 37
Z9 37
U1 14
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5539
EP 5551
DI 10.1109/TMM.2022.3194332
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300065
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Liu, YH
   Chen, YJ
   Bao, LC
   Sebe, N
   Lepri, B
   De Nadai, M
AF Liu, Yahui
   Chen, Yajing
   Bao, Linchao
   Sebe, Nicu
   Lepri, Bruno
   De Nadai, Marco
TI ISF-GAN: An Implicit Style Function for High-Resolution Image-to-Image
   Translation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face editing; generative adversarial networks (GANs); unsupervised
   image-to-image translation
ID GENERATIVE ADVERSARIAL NETWORKS
AB Recently, there has been an increasing interest in image editing methods that employ pre-trained unconditional image generators (e.g., StyleGAN). However, applying these methods to translate images to multiple visual domains remains challenging. Existing works do not often preserve the domain-invariant part of the image (e.g., the identity in human face translations), or they do not usually handle multiple domains or allow for multi-modal translations. This work proposes an implicit style function (ISF) to straightforwardly achieve multi-modal and multi-domain image-to-image translation from pre-trained unconditional generators. The ISF manipulates the semantics of a latent code to ensure that the image generated from the manipulated code lies in the desired visual domain. Our human faces and animal image manipulations show significantly improved results over the baselines. Our model enables cost-effective multi-modal unsupervised image-to-image translations at high resolution using pre-trained unconditional GANs. The code and data are available at: https://github.com/yhlleo/stylegan-mmuit.
C1 [Liu, Yahui; Sebe, Nicu] Univ Trento, Dept Informat Engn & Comp Sci, I-38123 Trento, Italy.
   [Chen, Yajing; Bao, Linchao] Tencent AI Lab, Shenzhen 518063, Peoples R China.
   [Lepri, Bruno; De Nadai, Marco] Fdn Bruno Kessler, I-38123 Povo, Italy.
C3 University of Trento; Tencent; Fondazione Bruno Kessler
RP De Nadai, M (corresponding author), Fdn Bruno Kessler, I-38123 Povo, Italy.
EM yahui.liu@unitn.it; jadeyjchen@tencent.com; linchaobao@tencent.com;
   niculae.sebe@unitn.it; lepri@fbk.eu; work@marcodena.it
RI Bao, Linchao/AAG-9148-2020; Sebe, Niculae/KEC-2000-2024
OI Bao, Linchao/0000-0001-9543-3754; Sebe, Niculae/0000-0002-6597-7248
FU EU H2020 AI4Media Project [951911]
FX This work was supported by the EU H2020 AI4Media Project under Grant
   951911.
CR Abdal R., 2020, P IEEECVF C COMPUTER, P8296
   Abdal R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13950, DOI 10.1109/ICCV48922.2021.01371
   Abdal R, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3447648
   Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   [Anonymous], 2017, P IEEE C COMP VIS PA
   Chai L., 2021, PROC INT C LEARN REP
   Chen YC, 2019, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2019.00251
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Collins E, 2020, PROC CVPR IEEE, P5770, DOI 10.1109/CVPR42600.2020.00581
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Goodfellow I., 2014, P 27 INT C NEURAL IN, P2672
   Guan SY, 2020, Arxiv, DOI arXiv:2007.01758
   Harkonen E., 2020, Advances in Neural Information Processing Systems, V33, P9841
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jo Y, 2019, IEEE I CONF COMP VIS, P1745, DOI 10.1109/ICCV.2019.00183
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2018, P INT C LEARN REPR I
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kingma Diederik P., 2015, P 3 INT C LEARN
   Lee D., 2018, ADV NEUR IN, V31
   Lee HY, 2020, INT J COMPUT VISION, V128, P2402, DOI 10.1007/s11263-019-01284-z
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Lei Ba J., 2016, arXiv
   Li Bowen, 2019, Adv. Neural Inf. Process. Syst., V32
   LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116
   Liu MY, 2017, ADV NEUR IN, V30
   Liu YH, 2020, Arxiv, DOI arXiv:2003.06788
   Liu YH, 2021, PROC CVPR IEEE, P10780, DOI 10.1109/CVPR46437.2021.01064
   Liu YH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1357, DOI 10.1145/3394171.3413505
   Ma JY, 2020, INFORM FUSION, V62, P110, DOI 10.1016/j.inffus.2020.04.006
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Mao Q, 2019, PROC CVPR IEEE, P1429, DOI 10.1109/CVPR.2019.00152
   Menon S, 2020, PROC CVPR IEEE, P2434, DOI 10.1109/CVPR42600.2020.00251
   Mescheder L, 2018, PR MACH LEARN RES, V80
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Pan X., 2021, PROC INT C LEARN REP
   Paszke Adam, 2017, NIPS 2017 WORKSH AUT
   Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Shen YJ, 2022, IEEE T PATTERN ANAL, V44, P2004, DOI 10.1109/TPAMI.2020.3034267
   Shen Yujun, 2020, P IEEE CVF C COMP VI, P9243, DOI DOI 10.1109/CVPR42600.2020.00926
   Siarohin A, 2019, ADV NEUR IN, V32
   Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tian Y., 2021, PROC INT C LEARN REP
   Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   Wang Y., 2020, NEURAL INF PROCESS S
   Wu ZZ, 2021, PROC CVPR IEEE, P12858, DOI 10.1109/CVPR46437.2021.01267
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang Y., 2021, PROC INT C LEARN REP
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 60
TC 4
Z9 4
U1 3
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3343
EP 3353
DI 10.1109/TMM.2022.3159115
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200030
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Narahara, T
   Yamasaki, T
AF Narahara, Taro
   Yamasaki, Toshihiko
TI Subjective Functionality and Comfort Prediction for Apartment Floor
   Plans and Its Application to Intuitive Online Property Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attractiveness prediction; crowdsourcing; graph analysis; real estate
   floor plans
ID QUALITY ASSESSMENT; SENTIMENT PREDICTION; EMOTION RECOGNITION;
   NEURAL-NETWORKS; IMAGES
AB This paper presents a new user experience for online apartment search using functionality and comfort as query items. Specifically, it has three technical contributions. First, we present a new dataset on the perceived functionality and comfort scores of residential floor plans using nine question statements about the level of comfort, openness, privacy, etc. Second, we propose an algorithm to predict the scores from the floor plan images. Lastly, we implement a new apartment search system and conduct a large-scale usability study using crowdsourcing. The experimental results show that our apartment search system can provide a better user experience. To the best of our knowledge, this is the first work to propose a highly accurate machine learning model for predicting the subjective functionality and comfort of apartments.
C1 [Narahara, Taro] New Jersey Inst Technol NJIT, Hillier Coll Architecture & Design HCAD, Newark, NJ 07102 USA.
   [Yamasaki, Toshihiko] Univ Tokyo, Grad Sch Informat Sci & Technol, Dept Informat & Commun Engn, Bunkyo Ku, Tokyo 1138656, Japan.
C3 New Jersey Institute of Technology; University of Tokyo
RP Narahara, T (corresponding author), New Jersey Inst Technol NJIT, Hillier Coll Architecture & Design HCAD, Newark, NJ 07102 USA.
EM narahara@njit.edu; yamasaki@cvm.t.u-tokyo.ac.jp
OI Narahara, Taro/0000-0002-2200-1253
FU CREI Research Project of The University of Tokyo; Institutional Review
   Board (IRB) of New Jersey Institute of Technology [F466-20]; University
   of Tokyo [UT-IST-RE190927-1]
FX This work was supported by the CREI Research Project of The University
   of Tokyo. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Prof. Abderrahim
   Benslimane.(Corresponding author: Taro Narahara.)This work involved
   human subjects or animals in its research. Approval of all ethical and
   experimental procedures and protocols was granted by the Institutional
   Review Board (IRB) of New Jersey Institute of Technology (F466-20) and
   The University of Tokyo (UT-IST-RE190927-1).
CR Bai YQ, 2021, IEEE T MULTIMEDIA, V23, P4259, DOI 10.1109/TMM.2020.3039382
   Bappy JH, 2017, IEEE WINT CONF APPL, P373, DOI 10.1109/WACV.2017.48
   Chen FH, 2018, IEEE T MULTIMEDIA, V20, P997, DOI 10.1109/TMM.2017.2757769
   Chen JC, 2019, IEEE I CONF COMP VIS, P2661, DOI 10.1109/ICCV.2019.00275
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Cui Y, 2019, IEEE J-STARS, V12, P3117, DOI 10.1109/JSTARS.2019.2918937
   Deng YB, 2017, IEEE SIGNAL PROC MAG, V34, P80, DOI 10.1109/MSP.2017.2696576
   Dong G., 1999, P 5 ACM SIGKDD INT C, P43, DOI [10.1145/312129., DOI 10.1145/312129, DOI 10.1145/312129.312191]
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan ZW, 2017, IEEE T MULTIMEDIA, V19, P2720, DOI 10.1109/TMM.2017.2711860
   Fang Q, 2015, IEEE T MULTIMEDIA, V17, P2281, DOI 10.1109/TMM.2015.2491019
   Guo GJ, 2018, IEEE T MULTIMEDIA, V20, P2073, DOI 10.1109/TMM.2018.2794262
   Guo WY, 2021, IEEE T MULTIMEDIA, V23, P1785, DOI 10.1109/TMM.2020.3003648
   Gutiérrez J, 2022, IEEE T MULTIMEDIA, V24, P3087, DOI 10.1109/TMM.2021.3093717
   Hanazato T., 2005, Journal of Architecture and Planning (Transactions of AIJ), P9, DOI DOI 10.3130/AIJA.70.9_5
   Hattori R., 2020, P JOINT 11 INT C SOF, P1
   Hattori R., 2021, J. Jpn. Soc. Fuzzy Theory Intell. Informat., V33, P640
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heidari M, 2021, 2021 IEEE INTERNATIONAL IOT, ELECTRONICS AND MECHATRONICS CONFERENCE (IEMTRONICS), P1013, DOI 10.1109/IEMTRONICS52119.2021.9422649
   Hofbauer H, 2021, IEEE T MULTIMEDIA, V24, P3595, DOI 10.1109/TMM.2021.3103394
   Hosu V, 2019, PROC CVPR IEEE, P9367, DOI 10.1109/CVPR.2019.00960
   Hu RZ, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392391
   Ikeda J., 2021, P 25 INT C PATT REC, P2995
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Irie G, 2010, IEEE T MULTIMEDIA, V12, P523, DOI 10.1109/TMM.2010.2051871
   James V., 2005, The Journal of Real Estate Research, V27, P105, DOI [DOI 10.1080/10835547.2005.12091148, 10.1080/10835547.2005.12091148]
   Ji RR, 2019, IEEE T MULTIMEDIA, V21, P1062, DOI 10.1109/TMM.2018.2867718
   Kato N, 2020, IEICE T INF SYST, VE103D, P398, DOI 10.1587/transinf.2019EDP7146
   Kato N, 2018, RETECH'18: PROCEEDINGS OF THE 2018 ACM WORKSHOP ON MULTIMEDIA FOR REAL ESTATE TECH, P7, DOI 10.1145/3210499.3210525
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Law S, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3342240
   Lebreton P, 2021, IEEE T MULTIMEDIA, V23, P4526, DOI 10.1109/TMM.2020.3044452
   Lei XJ, 2016, IEEE T MULTIMEDIA, V18, P1910, DOI 10.1109/TMM.2016.2575738
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Li T, 2006, IEEE T MULTIMEDIA, V8, P564, DOI 10.1109/TMM.2006.870730
   Li T, 2015, IEEE T CYBERNETICS, V45, P2336, DOI 10.1109/TCYB.2015.2392156
   Li Y, 2023, IEEE T MULTIMEDIA, V25, P154, DOI 10.1109/TMM.2021.3122347
   Lin C, 2019, IEEE I CONF COMP VIS, P5673, DOI 10.1109/ICCV.2019.00577
   Liu C, 2018, LECT NOTES COMPUT SC, V11210, P203, DOI 10.1007/978-3-030-01231-1_13
   Liu C, 2017, IEEE I CONF COMP VIS, P2214, DOI 10.1109/ICCV.2017.241
   Liu DC, 2021, PROC CVPR IEEE, P9517, DOI 10.1109/CVPR46437.2021.00940
   Liu TL, 2020, IEEE T MULTIMEDIA, V22, P1098, DOI 10.1109/TMM.2019.2936805
   Liu YX, 2022, IEEE T MULTIMEDIA, V24, P3754, DOI 10.1109/TMM.2021.3107148
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Lv P, 2023, IEEE T MULTIMEDIA, V25, P736, DOI 10.1109/TMM.2021.3130752
   Maas A.L., 2013, P 30 INT C MACH LEAR, V30, P3
   MCGREGOR JJ, 1982, SOFTWARE PRACT EXPER, V12, P23, DOI 10.1002/spe.4380120103
   Min XK, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3470970
   Mura C, 2021, COMPUT GRAPH FORUM, V40, P375, DOI 10.1111/cgf.142640
   Nauata N., 2020, Lect. Notes Comput. Sci, DOI [DOI 10.1007/978-3-030-58452-8_10, 10.1007/978-3-030-58452-8, DOI 10.1007/978-3-030-58452-8]
   Nie WZ, 2022, IEEE T MULTIMEDIA, V24, P4471, DOI 10.1109/TMM.2021.3118881
   Ohara K., 2016, P 78 NAT CONV IPSJ, V2016, P311
   Oyama S, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P421, DOI [10.1109/BigMM.2019.00025, 10.1109/BigMM.2019.00073]
   P. ITU-T, 2020, Methods, metrics and procedures for statistical evaluation, qualification and comparison of objective quality prediction models, P1401, DOI [11.1002/1000/14159, DOI 11.1002/1000/14159]
   Phalak A, 2020, Arxiv, DOI arXiv:2003.07356
   Poursaeed O, 2018, MACH VISION APPL, V29, P667, DOI 10.1007/s00138-018-0922-2
   Redi M, 2014, PROC CVPR IEEE, P4272, DOI 10.1109/CVPR.2014.544
   Ruan SL, 2024, IEEE T MULTIMEDIA, V26, P4097, DOI 10.1109/TMM.2021.3118208
   Seya H, 2022, GEOGR ANAL, V54, P239, DOI 10.1111/gean.12283
   Sirmans G. S., 2005, J REAL ESTATE LIT, V13, P3, DOI DOI 10.1080/10835547.2005.12090154
   Soleymani M, 2012, IEEE T AFFECT COMPUT, V3, P211, DOI 10.1109/T-AFFC.2011.37
   Solovev K, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P2838, DOI 10.1145/3442381.3449967
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Su YT, 2023, IEEE T MULTIMEDIA, V25, P1243, DOI 10.1109/TMM.2022.3140892
   Takada Y, 2018, IEEE ICCE
   Takizawa A., 2008, Journal of Environmental Engineering (Transactions of AIJ), V73, P139, DOI DOI 10.3130/AIJE.73.139
   Teixeira RMA, 2012, MULTIMED TOOLS APPL, V61, P21, DOI 10.1007/s11042-010-0702-0
   Trinh V, 2019, PROCEEDINGS WEB3D 2019: THE 24TH INTERNATIONAL ACM CONFERENCE ON 3D WEB TECHNOLOGY, DOI 10.1145/3329714.3338141
   Wang K, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322941
   Wang XT, 2019, IEEE INT CONF MULTI, P84, DOI 10.1109/ICMEW.2019.0-106
   Wu QB, 2017, IEEE T MULTIMEDIA, V19, P2490, DOI 10.1109/TMM.2017.2700206
   Xia BH, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P162, DOI [10.1109/BigMM.2019.00033, 10.1109/BigMM.2019.00-29]
   Yamada M, 2021, IEEE ICCE, DOI 10.1109/ICCE50685.2021.9427580
   Yamasaki T, 2018, RETECH'18: PROCEEDINGS OF THE 2018 ACM WORKSHOP ON MULTIMEDIA FOR REAL ESTATE TECH, P1, DOI 10.1145/3210499.3210528
   Yan XF, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P721, DOI 10.1109/ICDM.2002.1184038
   Yang JF, 2018, IEEE T MULTIMEDIA, V20, P2513, DOI 10.1109/TMM.2018.2803520
   Yang ST, 2019, PROC CVPR IEEE, P3358, DOI 10.1109/CVPR.2019.00348
   Yao XX, 2021, IEEE T MULTIMEDIA, V23, P1640, DOI 10.1109/TMM.2020.3001527
   Yi SZ, 2022, IEICE T INF SYST, VE105D, P587, DOI 10.1587/transinf.2021HCK0001
   Zeng ZL, 2019, IEEE I CONF COMP VIS, P9095, DOI 10.1109/ICCV.2019.00919
   Zeppelzauer M, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P126, DOI 10.1145/3206025.3206060
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang HM, 2023, IEEE T MULTIMEDIA, V25, P2203, DOI 10.1109/TMM.2022.3144804
   Zhang SQ, 2018, IEEE T MULTIMEDIA, V20, P1576, DOI 10.1109/TMM.2017.2766843
   Zhou W, 2022, J VIS COMMUN IMAGE R, V86, DOI 10.1016/j.jvcir.2022.103526
   Zhu HC, 2023, IEEE T MULTIMEDIA, V25, P179, DOI 10.1109/TMM.2021.3123468
NR 88
TC 2
Z9 2
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6729
EP 6742
DI 10.1109/TMM.2022.3214072
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000003
OA hybrid
DA 2024-07-18
ER

PT J
AU Nie, XX
   Hu, B
   Gao, XB
AF Nie, Xixi
   Hu, Bo
   Gao, Xinbo
TI MLNet: A Multi-Domain Lightweight Network for Multi-Focus Image Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; multi-focus image fusion; discrete cosine transform;
   local binary battern
ID PERFORMANCE; TRANSFORM; GAN
AB Existing multi-focus image fusion (MFIF) methods are difficult to achieve satisfactory results in both fusion performance and rate simultaneously. The spatial domain methods are hard to determine the focus/defocus boundary (FDB), and the transform domain methods are likely to damage the content information of the source images. Moreover, the deep learning-based MFIF methods are usually confronted with low rate due to complex models and enormous learnable parameters. To address these issues, we propose a multi-domain lightweight network (MLNet) for MFIF, which can achieve competitive results in both performance and rate. The proposed MLNet mainly includes three modules, namely focus extraction (FE), focus measure (FM) and image fusion (IF). In the interpretable FE module, the image features extracted by discrete cosine transform-based convolution (DCTConv) and local binary pattern-based convolution (LBPConv) are concatenated and fed into the FM module. DCTConv based on transform domain takes DCT coefficients to construct a fixed convolution kernel without parameter learning, which can effectively capture the high/low frequency content of the image. LBPConv based on spatial domain can achieve structure features and gradient information from source images. In the FM module, a 3-layer 1 x 1 convolution with a few learnable parameters is employed to generate the initial decision map, which has the properties of flexible input. The fused image is obtained by the IF module according to the final decision map. In terms of quantitative and qualitative evaluations, extensive experiments validate that the proposed method outperforms existing state-of-the-art methods on three public datasets. In addition, the proposed MLNet contains only 0.01 M parameters, which is 0.2% of the first CNN-based MFIF method [25].
C1 [Nie, Xixi; Hu, Bo; Gao, Xinbo] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
C3 Chongqing University of Posts & Telecommunications
RP Gao, XB (corresponding author), Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
EM s170201012@cqupt.edu.cn; hubo90@cqupt.edu.cn; gaoxb@cqupt.edu.cn
FU National Natural Science Foundation of China [62036007, 62101084];
   Science and Technology Research Program of Chongqing Municipal Education
   Commission [62101084]; Special Project on Technological Innovation and
   Application Development [62176195]; Chongqing Excellent Scientist
   Project [62176195]; Chongqing University of Posts and Telecommunications
   Ph.D. Innovative Talents Project [KJQN202100628]; National Natural
   Science Foundation of China; Science and Technology Research Program of
   Chongqing Municipal Education Commission; Special Project on
   Technological Innovation and Application Development; Chongqing
   Excellent Scientist Project; Chongqing University of Posts and
   Telecommunications Ph.D. Innovative Talents Project
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62036007, 62101084, and 62176195, in
   part by the Science and Technology Research Program of Chongqing
   Municipal Education Commission under Grant KJQN202100628, in part by
   Special Project on Technological Innovation and Application Development
   under Grant cstc2020jscx-dxwtB0032, in part by Chongqing Excellent
   Scientist Project under Grant cstc2021ycjh-bgzxm0339, and in part by the
   Chongqing University of Posts and Telecommunications Ph.D. Innovative
   Talents Project under Grant BYJS202112.
CR Amin-Naji M., 2018, J AI DATA MINING, V6, P233, DOI DOI 10.22044/JADM.2017.5169.1624
   An K, 2020, IEEE ELECTR DEVICE L, V41, P167, DOI 10.1109/LED.2019.2955089
   Bhatnagar G, 2013, IEEE T MULTIMEDIA, V15, P1014, DOI 10.1109/TMM.2013.2244870
   Cao L, 2015, IEEE SIGNAL PROC LET, V22, P220, DOI 10.1109/LSP.2014.2354534
   Chen Y, 2009, IMAGE VISION COMPUT, V27, P1421, DOI 10.1016/j.imavis.2007.12.002
   Easley G, 2008, APPL COMPUT HARMON A, V25, P25, DOI 10.1016/j.acha.2007.09.003
   Guo XP, 2019, IEEE T MULTIMEDIA, V21, P1982, DOI 10.1109/TMM.2019.2895292
   Haghighat MBA, 2011, COMPUT ELECTR ENG, V37, P789, DOI 10.1016/j.compeleceng.2011.04.016
   He KJ, 2018, NEUROCOMPUTING, V320, P157, DOI 10.1016/j.neucom.2018.09.018
   Hong X., 2019, P IEEE INT C WAV AN, P1
   Li HF, 2016, SIGNAL PROCESS, V128, P474, DOI 10.1016/j.sigpro.2016.05.015
   Li JX, 2020, IEEE T IMAGE PROCESS, V29, P4816, DOI 10.1109/TIP.2020.2976190
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Li S, 2013, INFORM FUSION, V14, P147, DOI 10.1016/j.inffus.2011.07.001
   Li XH, 2021, ISPRS J PHOTOGRAMM, V179, P14, DOI 10.1016/j.isprsjprs.2021.07.007
   Lin W, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0222729
   Liu Y, 2020, INFORM FUSION, V64, P71, DOI 10.1016/j.inffus.2020.06.013
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Liu Y, 2015, INFORM FUSION, V23, P139, DOI 10.1016/j.inffus.2014.05.004
   Lu K, 2021, IEEE T MULTIMEDIA, V23, P4093, DOI 10.1109/TMM.2020.3037526
   Ma HY, 2020, IEEE T IMAGE PROCESS, V29, P8668, DOI 10.1109/TIP.2020.3018261
   Ma JY, 2021, IEEE T COMPUT IMAG, V7, P309, DOI 10.1109/TCI.2021.3063872
   Ma JY, 2020, IEEE T IMAGE PROCESS, V29, P4980, DOI 10.1109/TIP.2020.2977573
   Nejati M, 2015, INFORM FUSION, V25, P72, DOI 10.1016/j.inffus.2014.10.004
   Nie XX, 2021, NEUROCOMPUTING, V465, P93, DOI 10.1016/j.neucom.2021.08.109
   Piella G, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P173
   Qiang Zhang, 2021, Pattern Recognition, V113, DOI 10.1016/j.patcog.2020.107752
   Qu GH, 2002, ELECTRON LETT, V38, P313, DOI 10.1049/el:20020212
   Szegedy C., 2014, P IEEE CVF C COMP VI, DOI 10.1109/CVPR.2015.7298594
   Tang H, 2018, INFORM SCIENCES, V433, P125, DOI 10.1016/j.ins.2017.12.043
   Wang ZB, 2010, PATTERN RECOGN, V43, P2003, DOI 10.1016/j.patcog.2010.01.011
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xiao B, 2021, IEEE T IMAGE PROCESS, V30, P163, DOI 10.1109/TIP.2020.3033158
   Xiao B, 2020, IEEE T MULTIMEDIA, V22, P285, DOI 10.1109/TMM.2019.2928516
   Xiaoming Jiang, 2021, 2021 5th International Conference on Communication and Information Systems (ICCIS), P134, DOI 10.1109/ICCIS53528.2021.9646074
   Xu H, 2020, AAAI CONF ARTIF INTE, V34, P12484
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Xydeas CS, 2000, ELECTRON LETT, V36, P308, DOI 10.1049/el:20000267
   Yang C, 2008, INFORM FUSION, V9, P156, DOI 10.1016/j.inffus.2006.09.001
   Yap PT, 2004, IEE P-VIS IMAGE SIGN, V151, P128, DOI 10.1049/ip-vis:20040395
   Zhang H, 2021, INFORM FUSION, V76, P323, DOI 10.1016/j.inffus.2021.06.008
   Zhang H, 2021, INT J COMPUT VISION, V129, P2761, DOI 10.1007/s11263-021-01501-8
   Zhang H, 2021, INFORM FUSION, V66, P40, DOI 10.1016/j.inffus.2020.08.022
   Zhang Q, 2009, SIGNAL PROCESS, V89, P1334, DOI 10.1016/j.sigpro.2009.01.012
   Zhang XC, 2022, IEEE T PATTERN ANAL, V44, P4819, DOI 10.1109/TPAMI.2021.3078906
   Zhang Y, 2017, INFORM FUSION, V35, P81, DOI 10.1016/j.inffus.2016.09.006
   Zhao JY, 2007, INT J INNOV COMPUT I, V3, P1433
   Zhao WD, 2018, IEEE T MULTIMEDIA, V20, P866, DOI 10.1109/TMM.2017.2760100
NR 48
TC 2
Z9 2
U1 15
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5565
EP 5579
DI 10.1109/TMM.2022.3194991
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300067
DA 2024-07-18
ER

PT J
AU Qin, HD
   Li, J
   Jiang, YQ
   Dai, YR
   Hong, SK
   Zhou, F
   Wang, ZJ
   Yang, T
AF Qin, Haidong
   Li, Jing
   Jiang, Yuqi
   Dai, Yanran
   Hong, Shikuan
   Zhou, Feng
   Wang, Zhijun
   Yang, Tao
TI Bullet-Time Video Synthesis Based on Virtual Dynamic Target Axis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cameras; Calibration; Heuristic algorithms; Optical switches; Visual
   effects; Software; Motion pictures; Camera array; bullet-time;
   multi-camera cali- bration; multi-view
AB Bullet-time videos have been widely used in movies, TV advertisements, and computer games, and can produce an immersive and smooth orbital free-viewpoint of frozen action. However, existing bullet-time video synthesis methods remain challenging in practical applications, especially in complex situations with poor camera calibration and a variety of camera array structures. This paper proposes a novel bullet-time video synthesis method based on a virtual dynamic target axis. We adopt an image similarity transformation strategy to eliminate image distortion in the bullet-time video. We use a high-order polynomial curve fitting strategy to reserve more bullet-time video frame content. The proposed dynamic target axis strategy can support various camera array structures, including camera arrays with and without a common field of view. In addition, this strategy can also tolerate poor camera calibration situations with unevenly distributed reprojection errors to some extent and synthesize smooth bullet-time videos without high-precision camera calibration. Qualitative and quantitative experiments in real environments and on simulation platforms demonstrate the high performance of our bullet-time video synthesis method. Compared with the state-of-the-art methods, the proposed method shows superiority.
C1 [Qin, Haidong; Zhou, Feng; Wang, Zhijun; Yang, Tao] Northwestern Polytech Univ, Sch Comp Sci, Natl Engn Lab Integrated AeroSp Ground Ocean Big D, Xian 710072, Peoples R China.
   [Li, Jing; Jiang, Yuqi; Dai, Yanran; Hong, Shikuan] Xidian Univ, Sch Telecommun Engn, Xidian 710071, Peoples R China.
C3 Northwestern Polytechnical University; Xidian University
RP Yang, T (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Natl Engn Lab Integrated AeroSp Ground Ocean Big D, Xian 710072, Peoples R China.
EM qhd@mail.nwpu.edu.cn; jinglixd@mail.xidian.edu.cn;
   imjiangyq@stu.xidian.edu.cn; yrdai@stu.xidian.edu.cn;
   sk_hong@stu.xidian.edu.cn; fengzhou_npu@mail.nwpu.edu.cn;
   zjwang@mail.nwpu.edu.cn; tyang@nwpu.edu.cn
OI Qin, Haidong/0000-0002-3301-9539; Jiang, Yuqi/0000-0003-2779-0594
FU National Natural Science Foundation of China [62172315, 62073262,
   61672429]; Huawei
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62172315, 62073262, and 61672429 and in
   part by funding of Huawei.
CR Akechi N., 2014, PROC ACM SIGGRAPH AS
   Bradski G, 2000, DR DOBBS J, V25, P120
   Carballeira P, 2022, IEEE T MULTIMEDIA, V24, P2378, DOI 10.1109/TMM.2021.3079711
   Chen D., 2014, PROC ACM SIGGRAPH AS
   Collet A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766945
   Collins Randall., 2001, Passionate Politics: Emotions and Social Movements, P27
   Fridovich-Keil S, 2022, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR52688.2022.00542
   Fujihashi T, 2019, IEEE T MULTIMEDIA, V21, P1000, DOI 10.1109/TMM.2018.2870074
   Garbin SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14326, DOI 10.1109/ICCV48922.2021.01408
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hedman P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5855, DOI 10.1109/ICCV48922.2021.00582
   Hsu CH, 2017, MULTIMED TOOLS APPL, V76, P24961, DOI 10.1007/s11042-017-4852-1
   Ikeya K., 2016, ITE Trans. Media Technol. Appl., V4, P349
   Jeong Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5826, DOI 10.1109/ICCV48922.2021.00579
   Jian-Guang Lou, 2005, 13th Annual ACM International Conference on Multimedia, P161
   Kanade T, 1997, IEEE MULTIMEDIA, V4, P34, DOI 10.1109/93.580394
   Lin CH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5721, DOI 10.1109/ICCV48922.2021.00569
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Muller T., Tiny CUDA neural network framework
   Nagai T, 2018, PROCEEDINGS OF THE 1ST INTERNATIONAL WORKSHOP ON MULTIMEDIA CONTENT ANALYSIS IN SPORTS (MMSPORTS'18), P39, DOI 10.1145/3265845.3265853
   Romero-Ramirez FJ, 2018, IMAGE VISION COMPUT, V76, P38, DOI 10.1016/j.imavis.2018.05.004
   Ruckert Darius, 2021, arXiv
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Schönberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31
   Shah S, 2018, FIELD SERVICE ROBOTI, P621, DOI [10.1007/978-3-319-67361-5_40, DOI 10.1007/978-3-319-67361-5_40]
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Stankiewicz O, 2018, IEEE T MULTIMEDIA, V20, P2182, DOI 10.1109/TMM.2018.2790162
   Takeuchi O, 2020, VISAPP: PROCEEDINGS OF THE 15TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 4: VISAPP, P299, DOI 10.5220/0008959802990306
   Takeuchi O, 2018, AVSU'18: PROCEEDINGS OF THE 2018 WORKSHOP ON AUDIO-VISUAL SCENE UNDERSTANDING FOR IMMERSIVE MULTIMEDIA, P19, DOI 10.1145/3264869.3264872
   Tomiyama K., 2006, IEICE Tech. Rep., V106, P43
   Wang Y, 2015, Arxiv, DOI arXiv:1507.01148
   Xinyi Qiu, 2020, 2020 IEEE 9th Global Conference on Consumer Electronics (GCCE), P813, DOI 10.1109/GCCE50665.2020.9291752
   Yan YZ, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.3.033014
   Yen-Chen L, 2021, IEEE INT C INT ROBOT, P1323, DOI 10.1109/IROS51168.2021.9636708
   Yi Q. X., 2018, P AS PAC WORKSH MIX
   Yu A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5732, DOI 10.1109/ICCV48922.2021.00570
   Yu JY, 2019, PROC CVPR IEEE, P3795, DOI 10.1109/CVPR.2019.00392
   Yu JY, 2018, LECT NOTES COMPUT SC, V11209, P569, DOI 10.1007/978-3-030-01228-1_34
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhao MD, 2020, IEEE T IMAGE PROCESS, V29, P3582, DOI 10.1109/TIP.2019.2963380
NR 41
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5178
EP 5191
DI 10.1109/TMM.2022.3189252
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300041
DA 2024-07-18
ER

PT J
AU Tashtarian, F
   Bentaleb, A
   Erfanian, A
   Hellwagner, H
   Timmerer, C
   Zimmermann, R
AF Tashtarian, Farzad
   Bentaleb, Abdelhak
   Erfanian, Alireza
   Hellwagner, Hermann
   Timmerer, Christian
   Zimmermann, Roger
TI $\mathsf{HxL3}$: Optimized Delivery Architecture for HTTP Low-Latency
   Live Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE CMAF; DASH; edge computing; HAS; HLS; Live streaming; low latency;
   caching; prefetching; transcoding
ID VIDEO; EDGE
AB While most of the HTTP adaptive streaming (HAS) traffic continues to be video-on-demand (VoD), more users have started generating and delivering live streams with high quality through popular online streaming platforms. Typically, the video contents are generated by streamers and being watched by large audiences which are geographically distributed far away from the streamers' locations. The locations of streamers and audiences create a significant challenge in delivering HAS-based live streams with low latency and high quality. Any problem in the delivery paths will result in a reduced viewer experience. In this paper, we propose HxL3, a novel architecture for low-latency live streaming. HxL3 is agnostic to the protocol and codecs that can work equally with existing HAS-based approaches. By holding the minimum number of live media segments through efficient caching and prefetching policies at the edge, improved transmissions, as well as transcoding capabilities, HxL3 is able to achieve high viewer experiences across the Internet by alleviating rebuffering and substantially reducing initial startup delay and live stream latency. HxL3 can be easily deployed and used. Its performance has been evaluated using real live stream sources and entities that are distributed worldwide. Experimental results show the superiority of the proposed architecture and give good insights into how low latency live streaming is working.
C1 [Tashtarian, Farzad; Erfanian, Alireza; Hellwagner, Hermann; Timmerer, Christian] Alpen Adria Univ Klagenfurt, Inst Informat Technol, Christian Doppler Lab ATHENA, A-9020 Klagenfurt, Austria.
   [Bentaleb, Abdelhak; Zimmermann, Roger] Natl Univ Singapore NUS, Dept Comp Sci, Singapore 117417, Singapore.
   [Bentaleb, Abdelhak; Zimmermann, Roger] Natl Univ Singapore NUS, Sch Comp, Singapore 117417, Singapore.
   [Timmerer, Christian] Bitmovin, A-9020 Klagenfurt, Austria.
C3 University of Klagenfurt; National University of Singapore; National
   University of Singapore
RP Tashtarian, F (corresponding author), Alpen Adria Univ Klagenfurt, Inst Informat Technol, Christian Doppler Lab ATHENA, A-9020 Klagenfurt, Austria.
EM farzad.tashtarian@aau.at; bentaleb@comp.nus.edu.sg;
   alireza.erfanian@aau.at; hermann.hellwagner@aau.at;
   christian.timmerer@aau.at; rogerz@comp.nus.edu.sg
RI Zimmermann, Roger/D-7944-2015
OI Zimmermann, Roger/0000-0002-7410-2590; Hellwagner,
   Hermann/0000-0003-1114-2584; Bentaleb, Abdelhak/0000-0002-5382-6530;
   Erfanian, Alireza/0000-0002-8096-8702
FU Ministry of Education -Singapore Academic Research Fund Tier 2
   [MOE2018-T2-1-103]; Austrian Federal Ministry for Digital and Economic
   Affairs; National Foundation for Research, Technology and Development,;
   Christian Doppler Research Association
FX This work was supported in part by the Ministry of Education -Singapore
   Academic Research Fund Tier 2 under MOE's official under Grant
   MOE2018-T2-1-103, in part by the Austrian Federal Ministry for Digital
   and Economic Affairs, in part by the National Foundation for Research,
   Technology and Development, and in part by the Christian Doppler
   Research Association. Christian Doppler Laboratory ATHENA:
   https://athena.itec.aau.at/. The Associate Editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Chonggang Wang. (Farzad Tashtarian and Abdelhak Ben-taleb contributed
   equally to this work.)
CR Bagci KT, 2017, IEEE T MULTIMEDIA, V19, P2152, DOI 10.1109/TMM.2017.2736638
   Bentaleb A, 2021, IEEE T MULTIMEDIA, V23, P2588, DOI 10.1109/TMM.2020.3013387
   Bentaleb A, 2019, PROCEEDINGS OF THE 29TH ACM WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO (NOSSDAV'19), P7, DOI 10.1145/3304112.3325611
   Bentaleb A, 2019, IEEE COMMUN SURV TUT, V21, P562, DOI 10.1109/COMST.2018.2862938
   Bentaleb A, 2018, IEEE T BROADCAST, V64, P575, DOI 10.1109/TBC.2018.2816789
   Bentaleb A, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1296, DOI 10.1145/2964284.2964332
   Bhat D, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P62, DOI 10.1145/3083187.3083196
   Cisco, 2020, VIS NETW IND GLOB MO
   Conviva, RES REP
   CTA, WEB APPL VID EC COMM
   Drago M, 2018, INT CONF COMPUT NETW, P508, DOI 10.1109/ICCNC.2018.8390387
   Erfanian A, 2020, PROCEEDINGS OF THE 2020 6TH IEEE CONFERENCE ON NETWORK SOFTWARIZATION (NETSOFT 2020): BRIDGING THE GAP BETWEEN AI AND NETWORK SOFTWARIZATION, P301, DOI 10.1109/NetSoft48620.2020.9165450
   Federico L., 2015, FACEBOOK CODE
   Ge C, 2018, IEEE J SEL AREA COMM, V36, P1816, DOI 10.1109/JSAC.2018.2845000
   Ge C, 2017, IEEE T MULTIMEDIA, V19, P2222, DOI 10.1109/TMM.2017.2735301
   ISO/IEC, 23000 192020 INF 19
   Jiang JC, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P393
   Jiang JC, 2016, 13TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI '16), P137
   Law W., ULTRALOW LATENCY STR
   Lim M, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P321, DOI 10.1145/3339825.3397043
   Liu F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531350
   Mehrabi A, 2019, IEEE T MOBILE COMPUT, V18, P787, DOI 10.1109/TMC.2018.2850026
   Pantos R., 2020, HTTP LIVE STREAMING
   Robert P., 2019, P NAB BROADC ENG INF
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Sun Y, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P272, DOI 10.1145/2934872.2934898
   Sundaresan S, 2017, PROCEEDINGS OF THE 2017 INTERNET MEASUREMENT CONFERENCE (IMC'17), P64, DOI 10.1145/3131365.3131381
   Tuysuz MF, 2020, IEEE T IND INFORM, V16, P7115, DOI 10.1109/TII.2020.2972931
   Twitch, 2020, P ACM MULT SYST C
   Xu XD, 2017, IEEE ACCESS, V5, P16406, DOI 10.1109/ACCESS.2017.2739343
   Yang SR, 2019, IEEE T VEH TECHNOL, V68, P1888, DOI 10.1109/TVT.2018.2889196
NR 32
TC 5
Z9 5
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2585
EP 2600
DI 10.1109/TMM.2022.3148587
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600010
OA hybrid
DA 2024-07-18
ER

PT J
AU Wang, PY
   Zhao, ZC
   Su, F
   Meng, HY
AF Wang, Pingyu
   Zhao, Zhicheng
   Su, Fei
   Meng, Hongying
TI LTReID: Factorizable Feature Generation With Independent Components for
   Long-Tailed Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Tail; Head; Feature extraction; Training; Indexes; Task analysis;
   Representation learning; Person re-identification; long-tailed
   distribution; feature factorization; feature generation
ID DATA SET; ALIGNMENT
AB With the rapid increase of large-scale and real-world person datasets, it is crucial to address the problem of long-tailed data distributions, i.e., head classes have large number of images while tail classes occupy extremely few samples. We observe that the imbalanced data distribution is likely to distort the overall feature space and impair the generalization capability of trained models. Nevertheless, this long-tailed problem has been rarely investigated in previous person Re-Identification (ReID) works. In this paper, we propose a novel Long-Tailed Re-Identification (LTReID) framework to simultaneously alleviate class-imbalance and hard-imbalance problems. Specifically, each real feature is decomposed into multiple independent components with two decorrelation losses. Then these components are randomly aggregated to generate more fake features for tail classes than head ones, resulting in the class-balance between head and tail classes. For the hard-balance between easy and hard samples, we utilize adversarial learning to generate more hard features than easy ones. The proposed framework can be trained in an end-to-end manner and avoids increasing the space and time complexity of inference models. Moreover, comprehensive experiments are conducted on the four ReID datasets so as to validate the effectiveness of the overall framework and the advantage of each module. Our results show that when trained with either balanced or imbalanced datasets, the LTReID achieves superior performance over the state-of-the-art methods.
C1 [Wang, Pingyu; Zhao, Zhicheng; Su, Fei] Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing Key Lab Network Syst & Network Culture, Beijing 100876, Peoples R China.
   [Meng, Hongying] Brunel Univ London, Coll Engn Design & Phys Sci, Uxbridge UB8 3PH, England.
C3 Beijing University of Posts & Telecommunications; Brunel University
RP Zhao, ZC (corresponding author), Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing Key Lab Network Syst & Network Culture, Beijing 100876, Peoples R China.
EM applewangpingyu@gmail.com; zhaozc@bupt.edu.cn; sufei@bupt.edu.cn;
   hongying.meng@brunel.ac.uk
RI Meng, Hongying/O-5192-2014
OI Meng, Hongying/0000-0002-8836-1382; Wang, Pingyu/0000-0001-9769-8035;
   Zhao, Zhicheng/0000-0001-6506-7298
FU Chinese National Natural Science Foundation [62076033, U1931202]
FX This work was supported by Chinese National Natural Science Foundation
   under Grants 62076033 and U1931202.
CR Boyan Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9716, DOI 10.1109/CVPR42600.2020.00974
   Cao D, 2020, PROC CVPR IEEE, P5670, DOI 10.1109/CVPR42600.2020.00571
   Cao KD, 2019, ADV NEUR IN, V32
   Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Chen GY, 2021, IEEE T IMAGE PROCESS, V30, P7663, DOI 10.1109/TIP.2021.3107211
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Duan YQ, 2019, PROC CVPR IEEE, P3410, DOI 10.1109/CVPR.2019.00353
   Eom C., 2019, ADV NEURAL INFORM PR, P5297
   Fu Y, 2019, AAAI CONF ARTIF INTE, P8295
   Ge Y., 2018, P NIPS, P1230
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Han H, 2005, LECT NOTES COMPUT SC, V3644, P878, DOI 10.1007/11538059_91
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans Alexander, 2017, ARXIV170307737
   Holte R. C., 2003, Workshop on learning from imbalanced datasets II, V11, P1
   Hou RB, 2022, IEEE T PATTERN ANAL, V44, P4894, DOI 10.1109/TPAMI.2021.3079910
   Huang C, 2020, IEEE T PATTERN ANAL, V42, P2781, DOI 10.1109/TPAMI.2019.2914680
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jin X, 2020, AAAI CONF ARTIF INTE, V34, P11173
   Kaiwei Zeng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13654, DOI 10.1109/CVPR42600.2020.01367
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li BY, 2019, AAAI CONF ARTIF INTE, P8577
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu J., 2020, MEMORY BASED JITTER
   Liu JL, 2020, PROC CVPR IEEE, P2967, DOI 10.1109/CVPR42600.2020.00304
   Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Matsukawa T, 2020, IEEE T PATTERN ANAL, V42, P2179, DOI 10.1109/TPAMI.2019.2914686
   Miao JX, 2022, IEEE T NEUR NET LEAR, V33, P4624, DOI 10.1109/TNNLS.2021.3059515
   Mikolov Tomas, 2013, Advances in Neural Information Processing Systems, P3111, DOI DOI 10.48550/ARXIV.1310.4546
   Paszke A, 2019, ADV NEUR IN, V32
   Peng Chu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P694, DOI 10.1007/978-3-030-58526-6_41
   Peng ML, 2019, AAAI CONF ARTIF INTE, P4707
   Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shizhen Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P647, DOI 10.1007/978-3-030-58539-6_39
   Shu J, 2019, ADV NEUR IN, V32
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2021, IEEE T PATTERN ANAL, V43, P902, DOI 10.1109/TPAMI.2019.2938523
   Tong Wu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P162, DOI 10.1007/978-3-030-58548-8_10
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wan CQ, 2020, IEEE T MULTIMEDIA, V22, P1605, DOI 10.1109/TMM.2019.2946486
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang PY, 2021, IEEE T MULTIMEDIA, V23, P1474, DOI 10.1109/TMM.2020.2999180
   Wang PY, 2021, IEEE T IMAGE PROCESS, V30, P2908, DOI 10.1109/TIP.2021.3055952
   Wang PY, 2020, PATTERN RECOGN LETT, V133, P195, DOI 10.1016/j.patrec.2020.03.012
   Wang PY, 2019, NEUROCOMPUTING, V363, P35, DOI 10.1016/j.neucom.2019.04.085
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Yang WJ, 2019, PROC CVPR IEEE, P1389, DOI 10.1109/CVPR.2019.00148
   Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35
   Yin X, 2019, PROC CVPR IEEE, P5697, DOI 10.1109/CVPR.2019.00585
   Yu Q, 2019, AAAI CONF ARTIF INTE, P411
   Yu R, 2018, LECT NOTES COMPUT SC, V11220, P196, DOI 10.1007/978-3-030-01270-0_12
   Zhang X, 2017, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2017.578
   Zhang YY, 2019, AAAI CONF ARTIF INTE, P9243
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zhang ZZ, 2019, PROC CVPR IEEE, P667, DOI 10.1109/CVPR.2019.00076
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng M, 2019, PROC CVPR IEEE, P5728, DOI 10.1109/CVPR.2019.00588
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou QQ, 2022, IEEE T NEUR NET LEAR, V33, P6627, DOI 10.1109/TNNLS.2021.3082701
   Zhu K., 2020, ECCV, P346, DOI [10.1007/978-3-030-58580-8_21, DOI 10.1007/978-3-030-58580-8_21]
NR 80
TC 3
Z9 3
U1 3
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4610
EP 4622
DI 10.1109/TMM.2022.3179902
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300001
OA Green Published
DA 2024-07-18
ER

PT J
AU Wang, QF
   Wei, YW
   Yin, JH
   Wu, JL
   Song, XM
   Nie, LQ
AF Wang, Qifan
   Wei, Yinwei
   Yin, Jianhua
   Wu, Jianlong
   Song, Xuemeng
   Nie, Liqiang
TI DualGNN: Dual Graph Neural Network for Multimedia Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Micro-video recommender systems; graph neural network; multi-modal
   fusion; representation learning
AB One of the important factors affecting micro-video recommender systems is to model the multi-modal user preference on the micro-video. Despite the remarkable performance of prior arts, they are still limited by fusing the user preference derived from different modalities in a unified manner, ignoring the users tend to place different emphasis on different modalities. Furthermore, modality-missing is ubiquity and unavoidable in the micro-video recommendation, some modalities information of micro-videos are lacked in many cases, which negatively affects the multi-modal fusion operations. To overcome these disadvantages, we propose a novel framework for the micro-video recommendation, dubbed Dual Graph Neural Network (DualGNN), upon the user-microvideo bipartite and user co-occurrence graphs, which leverages the correlation between users to collaboratively mine the particular fusion pattern for each user. Specifically, we first introduce a single-modal representation learning module, which performs graph operations on the user-microvideo graph in each modality to capture single-modal user preferences on different modalities. And then, we devise a multi-modal representation learning module to explicitly model the user's attentions over different modalities and inductively learn the multi-modal user preference. Finally, we propose a prediction module to rank the potential micro-videos for users. Extensive experiments on two public datasets demonstrate the significant superiority of our DualGNN over state-of-the-arts methods.
C1 [Wang, Qifan; Yin, Jianhua; Wu, Jianlong; Song, Xuemeng; Nie, Liqiang] Shandong Univ, Coll Comp Sci & Technol, Qingdao 266237, Peoples R China.
   [Wei, Yinwei] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
C3 Shandong University; National University of Singapore
RP Yin, JH (corresponding author), Shandong Univ, Coll Comp Sci & Technol, Qingdao 266237, Peoples R China.; Wei, YW (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
EM wqf@mail.sdu.edu.cn; weiyinwei@hotmail.com; jhyin@sdu.edu.cn;
   jlwu1992@sdu.edu.cn; sxmustc@gmail.com; nieliqiang@gmail.com
RI wang, qifan/GMW-7005-2022; Yin, Jianhua/HMD-6684-2023; Wei,
   Yinwei/JHX-9398-2023
OI Yin, Jianhua/0000-0002-4611-2986; Wei, Yinwei/0000-0003-1791-3159
FU National Natural Science Foundation of China [62172261, 61802231];
   Shandong Provincial Natural Science Foundation [ZR2019QF001]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62172261 and 61802231 and in part by
   the Shandong Provincial Natural Science Foundation under Grant
   ZR2019QF001
CR Arora S, 2019, 5 INT C LEARN REPR I
   Chen HJ, 2019, PROC CVPR IEEE, P10034, DOI 10.1109/CVPR.2019.01028
   Chen L, 2020, AAAI CONF ARTIF INTE, V34, P27
   Cheng WH, 2021, ACM COMPUT SURV, V54, DOI [10.1145/3447239, 10.1145/3552468.3554360]
   Couprie C, 2013, Arxiv, DOI arXiv:1301.3572
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He RN, 2016, AAAI CONF ARTIF INTE, P144
   He XN, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P639, DOI 10.1145/3397271.3401063
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hidayati SC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P438, DOI 10.1145/3240508.3240546
   Hu XX, 2019, IEEE IMAGE PROC, P1440, DOI [10.1109/ICIP.2019.8803025, 10.1109/icip.2019.8803025]
   Ji W, 2020, IEEE T CIRC SYST VID, V30, P4837, DOI 10.1109/TCSVT.2019.2962216
   Ji W, 2020, IEEE T IMAGE PROCESS, V29, P8177, DOI 10.1109/TIP.2020.3002083
   Jiang H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3487, DOI 10.1145/3394171.3413653
   Kingma D. P., 2014, arXiv
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Li YQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1464, DOI 10.1145/3343031.3350950
   Ling Lo, 2021, 2021 IEEE International Conference on Multimedia and Expo (ICME), DOI 10.1109/ICME51207.2021.9428120
   Liu F, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P1296, DOI 10.1145/3442381.3449986
   Liu H, 2022, Arxiv, DOI arXiv:2106.04155
   Liu S, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P3020, DOI 10.1145/3308558.3313513
   Rendle S., 2009, P 25 C UNCERTAINTY A, P452
   Tao ZL, 2020, INFORM PROCESS MANAG, V57, DOI 10.1016/j.ipm.2020.102277
   van der Merwe R, 2019, ARCH REC, V40, P239, DOI 10.1080/23257962.2017.1388224
   Wang X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P165, DOI 10.1145/3331184.3331267
   Wang Y., 2021, PROC IEEE INT C MULT, P1
   Wei YW, 2022, IEEE T MULTIMEDIA, V24, P2701, DOI 10.1109/TMM.2021.3088307
   Wei YW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3541, DOI 10.1145/3394171.3413556
   Wei YW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1437, DOI 10.1145/3343031.3351034
   Wei YW, 2020, IEEE T IMAGE PROCESS, V29, P1, DOI 10.1109/TIP.2019.2923608
   Xie HX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2871, DOI 10.1145/3394171.3414012
   Zhang J., 2016, P ACM INT C MULT, P1415
   Zhang JN, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4264
   Zhang WK, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10010052
NR 35
TC 22
Z9 23
U1 10
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1074
EP 1084
DI 10.1109/TMM.2021.3138298
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100004
DA 2024-07-18
ER

PT J
AU Wu, JP
   Ji, RR
   Wang, Q
   Zhang, SC
   Sun, XS
   Wang, Y
   Xu, ML
   Huang, FY
AF Wu, Jipeng
   Ji, Rongrong
   Wang, Qiang
   Zhang, Shengchuan
   Sun, Xiaoshuai
   Wang, Yan
   Xu, Mingliang
   Huang, Feiyue
TI Fast Monocular Depth Estimation via Side Prediction Aggregation with
   Continuous Spatial Refinement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Estimation; Predictive models; Task analysis; Spatial resolution;
   Real-time systems; Generators; Generative adversarial networks;
   Adversarial network; depth estimation; side output; spatial refinement
   constraint
AB Recent works have validated the benefit of integrating spatial information into deep networks to improve pixel-level prediction tasks such as monocular depth estimation. However, how to efficiently and robustly integrate spatial cues retains as an open problem. In this paper, we introduce the Side Prediction Aggregation (termed SPA) method to enhance the embedding of scene structural information from low-level to high-level layers. To improve the estimation accuracy, the proposed method is further equipped with continuous Spatial Refinement Loss (termed SRL) at multiple resolutions with negligible extra computation. Besides, the proposed sequential network can further perform adversarial learning at multiple resolutions. Such an adversarial refinement strategy greatly improves the accuracy of estimated depth with a little extra computation. Without using any pre-trained models, our network achieves the the-state-of-art accuracy on KITTI, NYUD V2, and Cityscapes datasets, which has achieved real-time depth estimation online.
C1 [Wu, Jipeng; Ji, Rongrong; Wang, Qiang; Zhang, Shengchuan; Sun, Xiaoshuai] Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Media Analyt & Comp Lab, Xiamen 361005, Peoples R China.
   [Wu, Jipeng] Inst Adv Technol, Shenzhen 518055, Peoples R China.
   [Wang, Yan] Pinterest, Seattle, WA 90876 USA.
   [Xu, Mingliang] Zhengzhou Univ, Ctr Interdisciplinary Informat Sci Res, Zhengzhou 450000, Peoples R China.
   [Huang, Feiyue] Tencent Youtu Lab, Shanghai 200000, Peoples R China.
C3 Xiamen University; Chinese Academy of Sciences; Shenzhen Institute of
   Advanced Technology, CAS; Zhengzhou University; Tencent
RP Ji, RR (corresponding author), Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Media Analyt & Comp Lab, Xiamen 361005, Peoples R China.
EM jipengwu@stu.xmu.edu.cn; rrji@xmu.edu.cn; qianwang@stu.xmu.edu.cn;
   zsc_2007@163.com; xssun@xmu.edu.cn; yanw@pinterest.com;
   iexumingliang@zzu.edu.cn; garyhuang@tencent.com
RI Zhou, heng/JCN-6493-2023; Wu, Jipeng/AIE-3794-2022
OI Wang, Yan/0000-0003-4309-3166
FU National Science Fund for Distinguished Young Scholars [62025603];
   Shenzhen Science and Technology for International Cooperation Research
   Program [GJHZ20190821155201661]; National Natural Science Foundation of
   China [U1705262, 62072386, 62072387, 62072389, 62002305, 61772443,
   61802324, 61702136]; Guangdong Basic and Applied Basic Research
   Foundation [2019B1515120049]; Fundamental Research Funds for the Central
   Universities [20720200077, 20720200090, 20720200091]
FX This work was supported in part by the National Science Fund for
   Distinguished Young Scholars under Grant 62025603,in part by the
   Shenzhen Science and Technology for International Cooperation Research
   Program under Grant GJHZ20190821155201661, in part by the National
   Natural Science Foundation of China under Grants U1705262, 62072386,
   62072387, 62072389, 62002305, 61772443, 61802324, and 61702136, in part
   by Guangdong Basic and Applied Basic Research Foundation under Grant
   2019B1515120049, and in part by the Fundamental Research Funds for the
   Central Universities under Grants 20720200077, 20720200090, and
   20720200091.
CR Casser V, 2019, AAAI CONF ARTIF INTE, P8001
   Cheng XJ, 2018, LECT NOTES COMPUT SC, V11220, P108, DOI 10.1007/978-3-030-01270-0_7
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Díaz R, 2019, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2019.00487
   Eigen D, 2014, ADV NEUR IN, V27
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Gan YK, 2018, LECT NOTES COMPUT SC, V11207, P232, DOI 10.1007/978-3-030-01219-9_14
   Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo XY, 2018, LECT NOTES COMPUT SC, V11215, P506, DOI 10.1007/978-3-030-01252-6_30
   Heo M, 2018, LECT NOTES COMPUT SC, V11208, P39, DOI 10.1007/978-3-030-01225-0_3
   Hoiem D, 2005, ACM T GRAPHIC, V24, P577, DOI 10.1145/1073204.1073232
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Jafari Omid Hosseini, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4620, DOI 10.1109/ICRA.2017.7989537
   Jiang HZ, 2018, LECT NOTES COMPUT SC, V11215, P20, DOI 10.1007/978-3-030-01252-6_2
   Karsch K, 2014, IEEE T PATTERN ANAL, V36, P2144, DOI 10.1109/TPAMI.2014.2316835
   Kline J, 2020, PROCEEDINGS OF THE 2020 32ND INTERNATIONAL TELETRAFFIC CONGRESS (ITC 32), P1, DOI [10.1109/ITC3249928.2020.00009, 10.1007/978-3-030-58565-5_35]
   Kundu JN, 2018, PROC CVPR IEEE, P2656, DOI 10.1109/CVPR.2018.00281
   Kuznietsov Y, 2017, PROC CVPR IEEE, P2215, DOI 10.1109/CVPR.2017.238
   Ladicky L, 2014, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2014.19
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li B, 2018, PATTERN RECOGN, V83, P328, DOI 10.1016/j.patcog.2018.05.029
   Li B, 2015, PROC CVPR IEEE, P1119, DOI 10.1109/CVPR.2015.7298715
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li J, 2017, IEEE I CONF COMP VIS, P3392, DOI 10.1109/ICCV.2017.365
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu FY, 2015, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2015.7299152
   Long FC, 2020, IEEE T MULTIMEDIA, V22, P1577, DOI 10.1109/TMM.2019.2943204
   Luo HC, 2019, IEEE T MULTIMEDIA, V21, P470, DOI 10.1109/TMM.2018.2859034
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Ochs M, 2019, LECT NOTES COMPUT SC, V11824, P288, DOI 10.1007/978-3-030-33676-9_20
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy A, 2016, PROC CVPR IEEE, P5506, DOI 10.1109/CVPR.2016.594
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Saxena A., 2006, NIPS, P1161, DOI DOI 10.1109/TPAMI.2015.2505283A
   Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song WF, 2020, IEEE T MULTIMEDIA, V22, P1220, DOI 10.1109/TMM.2019.2941776
   Sun L, 2015, IEEE T GEOSCI REMOTE, V53, P1490, DOI 10.1109/TGRS.2014.2344442
   Tan MX, 2019, PR MACH LEARN RES, V97
   Wang P, 2015, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2015.7298897
   Xu D, 2019, IEEE T PATTERN ANAL, V41, P1426, DOI 10.1109/TPAMI.2018.2839602
   Xu D, 2018, PROC CVPR IEEE, P675, DOI 10.1109/CVPR.2018.00077
   Xu D, 2018, PROC CVPR IEEE, P3917, DOI 10.1109/CVPR.2018.00412
   Xu D, 2017, PROC CVPR IEEE, P161, DOI 10.1109/CVPR.2017.25
   Xu HF, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5685
   Yan B, 2020, IEEE T MULTIMEDIA, V22, P676, DOI 10.1109/TMM.2019.2932566
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P2701, DOI 10.1109/TMM.2019.2912121
   Zhang ZY, 2018, LECT NOTES COMPUT SC, V11214, P238, DOI 10.1007/978-3-030-01249-6_15
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
   Zhuo W, 2015, PROC CVPR IEEE, P614, DOI 10.1109/CVPR.2015.7298660
NR 55
TC 2
Z9 2
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1204
EP 1216
DI 10.1109/TMM.2021.3140001
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100014
DA 2024-07-18
ER

PT J
AU Xu, JT
   Li, YL
   Wang, SJ
AF Xu, Jingtao
   Li, Ya-Li
   Wang, Shengjin
TI AdaZoom: Towards Scale-Aware Large Scene Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object detection; large scenes; adaptive zooming; scale-aware; deep
   reinforcement learning
AB Detection in large scenes is a challenging issue due to small objects and extreme scale variation. It is difficult for the deep-learning-based detector to extract features of small objects with only a few pixels. Most existing methods employ image pyramid and feature pyramid for multi-scale inference to alleviate this issue. However, they lack scale awareness to adapt to objects with different scales. In this paper, we propose a novel Adaptive Zoom (AdaZoom) network for scale-aware large scene object detection. There are three main contributions. First, an Adaptive Zoom network is proposed to actively focus and adaptively zoom the focused regions for high-performance object detection in large scenes. Second, to tackle the problem of missing annotations for focused regions, we train AdaZoom with the reward which measures the quality of generated regions, based on the paradigm of deep reinforcement learning. At last, we propose collaborative training to iteratively promote the joint performance of AdaZoom and the detector. To validate the effectiveness, we conduct extensive experiments on VisDrone2019, UAVDT and DOTA datasets. The experiments show AdaZoom brings consistent and significant improvement over different detection networks, achieving state-of-the-art performance on these datasets, especially outperforming the existing methods by AP of 4.64% on VisDrone2019.
C1 [Xu, Jingtao; Li, Ya-Li; Wang, Shengjin] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Li, YL (corresponding author), Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
EM xjd19@mails.tsinghua.edu.cn; liyali13@tsinghua.edu.cn;
   wgsgj@tsinghua.edu.cn
FU State Key Development Program in 14th Five-Year [2021YFF0602103,
   2021YFF0602102, 2021QY1702]; Institute for Guo Qiang, Tsinghua
   University [2019GQG0001]
FX This work was supported in part by the State Key Development Program in
   14th Five-Year under Grants 2021YFF0602103, 2021YFF0602102,and
   2021QY1702, and in part by the Institute for Guo Qiang, Tsinghua
   University under Grant 2019GQG0001.
CR ALOIMONOS J, 1987, INT J COMPUT VISION, V1, P333
   BELLMAN R, 1957, J MATH MECH, V6, P679, DOI 10.1512/iumj.1957.6.56038
   Bueno M. B., 2017, Deep Learn. Image Process. Appl., V31, P164
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22
   Caicedo JC, 2015, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2015.286
   Chaoxu Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12592, DOI 10.1109/CVPR42600.2020.01261
   Chen CR, 2019, IEEE INT CONF COMP V, P100, DOI 10.1109/ICCVW.2019.00018
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Dai JF, 2016, ADV NEUR IN, V29
   Deng CF, 2022, IEEE T MULTIMEDIA, V24, P1968, DOI 10.1109/TMM.2021.3074273
   Deng ST, 2021, IEEE T IMAGE PROCESS, V30, P1556, DOI 10.1109/TIP.2020.3045636
   Du DW, 2018, LECT NOTES COMPUT SC, V11214, P375, DOI 10.1007/978-3-030-01249-6_23
   Gao MF, 2018, PROC CVPR IEEE, P6926, DOI 10.1109/CVPR.2018.00724
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gonzalez-Garcia A, 2015, PROC CVPR IEEE, P3022, DOI 10.1109/CVPR.2015.7298921
   Hara K., 2017, Attentional network for visual object detection
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Kienzle W, 2009, J VISION, V9, DOI 10.1167/9.5.7
   Kong XY, 2017, PROC CVPR IEEE, P7072, DOI 10.1109/CVPR.2017.748
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li CL, 2020, IEEE COMPUT SOC CONF, P737, DOI 10.1109/CVPRW50498.2020.00103
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Li Y., 2017, P GCAI 2017 3 GLOB C, P20, DOI DOI 10.29007/XTGM
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu YJ, 2020, IEEE ACCESS, V8, P145740, DOI 10.1109/ACCESS.2020.3014910
   Lu Y, 2016, PROC CVPR IEEE, P2351, DOI 10.1109/CVPR.2016.258
   Massa F., 2018, maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch
   Mathe S, 2016, PROC CVPR IEEE, P2894, DOI 10.1109/CVPR.2016.316
   MCCONKIE GW, 1975, PERCEPT PSYCHOPHYS, V17, P578, DOI 10.3758/BF03203972
   Najibi M, 2019, IEEE I CONF COMP VIS, P9744, DOI 10.1109/ICCV.2019.00984
   Pirinen A, 2018, PROC CVPR IEEE, P6945, DOI 10.1109/CVPR.2018.00726
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1476, DOI 10.1109/TPAMI.2016.2601099
   Singh B, 2018, 32 C NEURAL INFORM P
   Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377
   Strasburger H, 2011, J VISION, V11, DOI 10.1167/11.5.13
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   Uzkent B., 2020, P IEEE CVF C COMP VI, P12345
   Wang HR, 2019, IEEE INT CONF COMP V, P64, DOI 10.1109/ICCVW.2019.00014
   Xia GS, 2018, PROC CVPR IEEE, P3974, DOI 10.1109/CVPR.2018.00418
   Yang F, 2019, IEEE I CONF COMP VIS, P8310, DOI 10.1109/ICCV.2019.00840
   Yi Wang, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12538), P651, DOI 10.1007/978-3-030-66823-5_39
   Yoo D, 2015, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2015.305
   Zhang JY, 2019, IEEE INT CONF COMP V, P1, DOI 10.1109/ICCVW.2019.00007
   Zhang XD, 2019, IEEE INT CONF COMP V, P118, DOI 10.1109/ICCVW.2019.00020
   Zhou JK, 2019, NEUROCOMPUTING, V366, P305, DOI 10.1016/j.neucom.2019.07.073
   Zhu P., 2018, VISION MEETS DRONES
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 55
TC 6
Z9 6
U1 15
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4598
EP 4609
DI 10.1109/TMM.2022.3178871
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200038
DA 2024-07-18
ER

PT J
AU Xu, JZ
   Yuan, MK
   Yan, DM
   Wu, TR
AF Xu, Jingzhao
   Yuan, Mengke
   Yan, Dong-Ming
   Wu, Tieru
TI Illumination Guided Attentive Wavelet Network for Low-Light Image
   Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Lighting; Wavelet transforms; Image enhancement; Frequency modulation;
   Wavelet coefficients; Noise reduction; Discrete wavelet transforms;
   Attention mechanism; illumination guidance; low-light image enhancement;
   wavelet transform
ID HISTOGRAM EQUALIZATION; QUALITY ASSESSMENT; RETINEX
AB Deep convolutional neural networks have recently been applied to improve the quality of low-light images and have achieved promising results. However, most existing methods cannot suppress noise during the enhancement process effectively, resulting in unknown artifacts and color distortions. In addition, these methods do not fully utilize illumination information and perform poorly under extremely low-light condition. To alleviate these problems, we propose the illumination guided attentive wavelet network (IGAWN) for low-light image enhancement (LLIE). Considering that the wavelet transform can separate high-frequency noise and desired low-frequency content effectively, we enhance low-light images in the frequency domain. By integrating attention mechanisms with wavelet transform, we develop the attentive wavelet transform to capture more important wavelet features, which enables the desired content to be enhanced and the redundant noise to be suppressed. To improve the image enhancement performance under extremely low-light environment, we extract illumination information from the input images and exploit it as the guidance for image enhancement through the frequency feature transform(FFT) layer. The proposed FFT layer generates frequency-aware affine transformation from the estimated illumination information, which can adaptively modulate the image features of different frequencies. Extensive experiments on synthetic and real-world datasets demonstrate that our IGAWNperforms favorably against state-of-the-art LLIE methods.
C1 [Xu, Jingzhao; Wu, Tieru] Jilin Univ, Sch Math, Changchun 130012, Peoples R China.
   [Yuan, Mengke; Yan, Dong-Ming] Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.
   [Yuan, Mengke; Yan, Dong-Ming] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
C3 Jilin University; Chinese Academy of Sciences; Institute of Automation,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS
RP Wu, TR (corresponding author), Jilin Univ, Sch Math, Changchun 130012, Peoples R China.; Yuan, MK (corresponding author), Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.
EM xujz19@mails.jlu.edu.cn; mengke.yuan@nlpr.ia.ac.cn;
   yandongming@gmail.com; wutr@jlu.edu.cn
OI Yuan, Mengke/0000-0001-9277-2654; Yan, Dong-Ming/0000-0003-2209-2404
FU National Key Research and Development Program of China [2020YFA0714101];
   National Nature Science Foundation of China [61872162, 62102414,
   62172415, 52175493]; Open Research Fund Program of State key Laboratory
   of Hydroscience and Engineering [sklhse-2022-D-04]; Alibaba Group
   through Alibaba Innovative Research Program
FX Thisworkwas supported in part by the National Key Research and
   Development Program of China under Grant 2020YFA0714101, in part by the
   National Nature Science Foundation of China under Grants 61872162,
   62102414, 62172415, and 52175493, in part by the Open Research Fund
   Program of State key Laboratory of Hydroscience and Engineering under
   Grant sklhse-2022-D-04, and in part by Alibaba Group through Alibaba
   Innovative Research Program.
CR Abdullah-Al-Wadud M, 2007, IEEE T CONSUM ELECTR, V53, P593, DOI 10.1109/TCE.2007.381734
   Celik T, 2011, IEEE T IMAGE PROCESS, V20, P3431, DOI 10.1109/TIP.2011.2157513
   Chen QA, 2010, SIGNAL PROCESS, V90, P2778, DOI 10.1016/j.sigpro.2010.03.016
   Choi Y, 2018, IEEE T INTELL TRANSP, V19, P934, DOI 10.1109/TITS.2018.2791533
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang HB, 2021, INT J COMPUT VISION, V129, P1282, DOI 10.1007/s11263-020-01421-z
   Huang HB, 2017, IEEE I CONF COMP VIS, P1698, DOI 10.1109/ICCV.2017.187
   Ji Z, 2021, IEEE IMAGE PROC, P1669, DOI 10.1109/ICIP42928.2021.9506063
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Kingma D, 2014, ICLR P, V2014, P1
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lee C, 2012, IEEE IMAGE PROC, P965, DOI 10.1109/ICIP.2012.6467022
   Li JQ, 2021, IEEE T MULTIMEDIA, V23, P3153, DOI 10.1109/TMM.2020.3021243
   Li L, 2020, IEEE T IMAGE PROCESS, V29, P5273, DOI 10.1109/TIP.2020.2980173
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Li QF, 2021, IEEE T IMAGE PROCESS, V30, P7074, DOI 10.1109/TIP.2021.3101395
   Linhui Dai, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P185, DOI 10.1007/978-3-030-67070-2_11
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu PJ, 2018, IEEE COMPUT SOC CONF, P886, DOI 10.1109/CVPRW.2018.00121
   Liu Y, 2022, IEEE T MULTIMEDIA, V24, P2890, DOI 10.1109/TMM.2021.3090206
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Ma C, 2017, COMPUT VIS IMAGE UND, V158, P1, DOI 10.1016/j.cviu.2016.12.009
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mou C, 2022, IEEE T MULTIMEDIA, V24, P1366, DOI 10.1109/TMM.2021.3063916
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Ni ZK, 2020, IEEE T IMAGE PROCESS, V29, P9140, DOI 10.1109/TIP.2020.3023615
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Paszke A, 2019, ADV NEUR IN, V32
   Pizer S. M., 1990, Proceedings of the First Conference on Visualization in Biomedical Computing (Cat. No.90TH0311-1), P337, DOI 10.1109/VBC.1990.109340
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Ren XT, 2020, IEEE T IMAGE PROCESS, V29, P5862, DOI 10.1109/TIP.2020.2984098
   Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sasagawa Yukihiro, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P345, DOI 10.1007/978-3-030-58589-1_21
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu HT, 2014, IEEE T MULTIMEDIA, V16, P68, DOI 10.1109/TMM.2013.2283453
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28
   Xu JZ, 2023, COMPUT VIS MEDIA, V9, P335, DOI 10.1007/s41095-022-0277-5
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xue SK, 2020, NEUROCOMPUTING, V382, P116, DOI 10.1016/j.neucom.2019.11.044
   Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P2072, DOI 10.1109/TIP.2021.3050850
   Yang WH, 2020, PROC CVPR IEEE, P3060, DOI 10.1109/CVPR42600.2020.00313
   Yuan MK, 2019, J COMPUT SCI TECH-CH, V34, P550, DOI 10.1007/s11390-019-1926-8
   Zhang CY, 2020, J COMPUT SCI TECH-CH, V35, P889, DOI 10.1007/s11390-020-0272-1
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
NR 62
TC 2
Z9 3
U1 10
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6258
EP 6271
DI 10.1109/TMM.2022.3207330
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500044
DA 2024-07-18
ER

PT J
AU Yang, ZJ
   Chen, JY
   Shi, YK
   Li, H
   Chen, TS
   Lin, L
AF Yang, Zhijing
   Chen, Junyang
   Shi, Yukai
   Li, Hao
   Chen, Tianshui
   Lin, Liang
TI OccluMix: Towards De-Occlusion Virtual Try-on by Semantically-Guided
   Mixup
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clothing; Semantics; Task analysis; Shape; Pipelines; Generators;
   Visualization; Deep learning; data augmentation; occlusion handing;
   virtual try-on
AB Image Virtual try-on aims at replacing the cloth on a personal image with a garment image (in-shop clothes), which has attracted increasing attention from the multimedia and computer vision communities. Prior methods successfully preserve the character of clothing images, however, occlusion remains a pernicious effect for realistic virtual try-on. In this work, we first present a comprehensive analysis of the occlusions and categorize them into two aspects: i) Inherent-Occlusion: the ghost of the former cloth still exists in the try-on image; ii) Acquired-Occlusion: the target cloth warps to the unreasonable body part. Based on the in-depth analysis, we find that the occlusions can be simulated by a novel semantically-guided mixup module, which can generate semantic-specific occluded images that work together with the try-on images to facilitate training a de-occlusion try-on (DOC-VTON) framework. Specifically, DOC-VTON first conducts a sharpened semantic parsing on the try-on person. Aided by semantics guidance and pose prior, various complexities of texture are selectively blending with human parts in a copy-and-paste manner. Then, the Generative Module (GM) is utilized to take charge of synthesizing the final try-on image and learning to de-occlusion jointly. In comparison to the state-of-the-art methods, DOC-VTON achieves better perceptual quality by reducing occlusion effects.
C1 [Yang, Zhijing; Chen, Junyang; Shi, Yukai; Li, Hao; Chen, Tianshui] Guangdong Univ Technol, Sch Informat Technol, Guangzhou 510006, Peoples R China.
   [Lin, Liang] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Peoples R China.
C3 Guangdong University of Technology; Sun Yat Sen University
RP Shi, YK (corresponding author), Guangdong Univ Technol, Sch Informat Technol, Guangzhou 510006, Peoples R China.
EM yzhj@gdut.edu.cn; jychen9811@gmail.com; ykshi@gdut.edu.cn;
   lihao9605@gmail.com; tianshuichen@gmail.com; linliang@ieee.org
RI Lin, Liang/IQR-8601-2023; Chen, Junyang/KFS-0551-2024; Chen,
   Junyang/AAZ-4344-2020
OI Lin, Liang/0000-0003-2248-3755; Chen, Junyang/0000-0002-1139-8654; Shi,
   Yukai/0000-0002-9413-6528; Chen, Tianshui/0000-0002-5848-5624
FU National Natural Science Foundation of China [62002069, 62206060];
   Science and Technology Project of Guangdong Province [2021A1515011341];
   Guangzhou Science and Technology Plan Project [202002030386, 102020369];
   Guangdong Provincial Key Laboratory of Human Digital Twin
   [2022B1212010004]
FX This work was supported in part by National Natural Science Foundation
   of China under Grants 62002069 and 62206060, in part by the Science and
   Technology Project of Guangdong Province under Grant 2021A1515011341, in
   part by Guangzhou Science and Technology Plan Project under Grants
   202002030386 and 102020369, and in part by the Guangdong Provincial Key
   Laboratory of Human Digital Twin under Grant 2022B1212010004.
CR Angelini F, 2020, IEEE T MULTIMEDIA, V22, P1433, DOI 10.1109/TMM.2019.2944745
   [Anonymous], 1977, Construction theory of functions of several variables, DOI [DOI 10.1007/BFB0086566, 10.1007/BFb0086566]
   Brouet R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185532
   Choi S, 2021, PROC CVPR IEEE, P14126, DOI 10.1109/CVPR46437.2021.01391
   Dabouei A, 2021, PROC CVPR IEEE, P13789, DOI 10.1109/CVPR46437.2021.01358
   DeVries T, 2017, Arxiv, DOI [arXiv:1708.04552, DOI 10.48550/ARXIV.1708.04552]
   Dong H, 2019, IEEE I CONF COMP VIS, P9025, DOI 10.1109/ICCV.2019.00912
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Du CH, 2023, IEEE T MULTIMEDIA, V25, P777, DOI 10.1109/TMM.2022.3152367
   Ge CJ, 2021, PROC CVPR IEEE, P16923, DOI 10.1109/CVPR46437.2021.01665
   Ge YY, 2021, PROC CVPR IEEE, P8481, DOI 10.1109/CVPR46437.2021.00838
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Guo HY, 2019, AAAI CONF ARTIF INTE, P3714
   Han XT, 2019, IEEE I CONF COMP VIS, P10470, DOI 10.1109/ICCV.2019.01057
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Han Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7847, DOI 10.1109/CVPR42600.2020.00787
   Hensel M, 2017, ADV NEUR IN, V30
   Hong M., 2021, P IEEECVF C COMPUTER, P14862
   Hu BW, 2022, IEEE T MULTIMEDIA, V24, P1233, DOI 10.1109/TMM.2022.3143712
   Hu YT, 2019, PROC CVPR IEEE, P3100, DOI 10.1109/CVPR.2019.00322
   Hur J, 2019, PROC CVPR IEEE, P5747, DOI 10.1109/CVPR.2019.00590
   Issenhuth Thibaut, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P619, DOI 10.1007/978-3-030-58565-5_37
   Jaejun Yoo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8372, DOI 10.1109/CVPR42600.2020.00840
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li PK, 2022, IEEE T PATTERN ANAL, V44, P3260, DOI 10.1109/TPAMI.2020.3048039
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Minar M. R., 2020, CVPR WORKSH
   Partio M., 2002, P 5 NORD SIGN PROC S
   Qi L, 2019, PROC CVPR IEEE, P3009, DOI 10.1109/CVPR.2019.00313
   Rohmer D, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866183
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rosca M, 2017, Arxiv, DOI arXiv:1706.04987
   Salimans T, 2016, ADV NEUR IN, V29
   Santesteban I, 2019, COMPUT GRAPH FORUM, V38, P355, DOI 10.1111/cgf.13643
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun DQ, 2014, INT J COMPUT VISION, V106, P115, DOI 10.1007/s11263-013-0644-x
   Takahashi R, 2020, IEEE T CIRC SYST VID, V30, P2917, DOI 10.1109/TCSVT.2019.2935128
   Tokozume Y, 2018, PROC CVPR IEEE, P5486, DOI 10.1109/CVPR.2018.00575
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang Q, 2022, IEEE T MULTIMEDIA, V24, P1031, DOI 10.1109/TMM.2021.3104141
   Yang F, 2021, PROC CVPR IEEE, P9894, DOI 10.1109/CVPR46437.2021.00977
   Yang H, 2022, PROC CVPR IEEE, P3450, DOI 10.1109/CVPR52688.2022.00345
   Yu RY, 2019, IEEE I CONF COMP VIS, P10510, DOI 10.1109/ICCV.2019.01061
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhan XH, 2020, PROC CVPR IEEE, P3783, DOI 10.1109/CVPR42600.2020.00384
   Zhang H., 2017, PROC INT C LEARN REP, P1
   Zhang N, 2023, IEEE T MULTIMEDIA, V25, P3217, DOI 10.1109/TMM.2022.3157036
   Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18
NR 50
TC 2
Z9 3
U1 5
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1477
EP 1488
DI 10.1109/TMM.2023.3234399
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA G2GW2
UT WOS:000987415000007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yue, SB
   Tu, YB
   Li, L
   Yang, Y
   Gao, SX
   Yu, ZT
AF Yue, Shengbin
   Tu, Yunbin
   Li, Liang
   Yang, Ying
   Gao, Shengxiang
   Yu, Zhengtao
TI I3N: Intra- and Inter-Representation Interaction Network for Change
   Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Change captioning; geometry-semantic interaction refining; hierarchical
   representation interaction; intra- and inter- representation interaction
ID TRANSFORMER; ATTENTION
AB Change captioning aims to describe the disagreement of image pairs with a linguistic sentence. Compared with single image captioning, change captioning requires not only understanding the fine-grained information of each image, but also determining whether change occurs and further representing the differences of image pairs. Although much progress has been made, it remains a severe challenge of the precise difference representation in the distraction of viewpoint change, especially that of tiny difference. In this paper, we propose a novel Intra- and Inter-representation Interaction Network (I3N) to learn the fine difference representation and be immune to viewpoint change. In the Intra-representation Interaction stage, we design Geometry-Semantic Interaction Refining (GSIR) to explore the positional and semantic interactions of intra-image, which can be a prior knowledge of enduring viewpoint change and reinforce the cognition of semantic change. In the Inter-representation Interaction stage, to endow the model with the capability of pinpointing the latent difference in viewpoint change, Hierarchical Representation Interaction (HRI) models difference from coarse to fine representations through the Semantic Matcher and Change Amplifier module. The proposed approach outperforms the state-of-the-art methods with an encouraging performance on the existing change captioning benchmarks.
C1 [Yue, Shengbin; Yang, Ying; Gao, Shengxiang; Yu, Zhengtao] Kunming Univ Sci & Technol, Fac Informat Engn & Automat, Kunming 650500, Peoples R China.
   [Yue, Shengbin; Gao, Shengxiang; Yu, Zhengtao] Kunming Univ Sci & Technol, Yunnan Prov Key Lab Artificial Intelligence, Kunming 650500, Peoples R China.
   [Tu, Yunbin] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101408, Peoples R China.
   [Li, Liang] Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
C3 Kunming University of Science & Technology; Kunming University of
   Science & Technology; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; Chinese Academy of Sciences; Institute of
   Computing Technology, CAS
RP Gao, SX (corresponding author), Kunming Univ Sci & Technol, Fac Informat Engn & Automat, Kunming 650500, Peoples R China.; Gao, SX (corresponding author), Kunming Univ Sci & Technol, Yunnan Prov Key Lab Artificial Intelligence, Kunming 650500, Peoples R China.
EM yueshengbin@foxmail.com; tuyunbin22@mails.ucas.ac.cn;
   liang.li@ict.ac.cn; yangying98@foxmail.com;
   gaoshengxiang.yn@foxmail.com; ztyu@hotmail.com
RI Gao, Shengxiang/KGM-3987-2024
OI Tu, Yunbin/0000-0002-9525-9060
FU National Natural Science Foundation of China
FX No Statement Available
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   [Anonymous], 2017, NEURAL INFORM PROCES
   Banerjee S., 2005, P ACL WORKSH INTR EX, P65
   Bosc M, 2003, NEUROIMAGE, V20, P643, DOI 10.1016/S1053-8119(03)00406-3
   Collins RT, 2000, IEEE T PATTERN ANAL, V22, P745, DOI 10.1109/TPAMI.2000.868676
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Dai ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2978
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Herdade S, 2019, ADV NEUR IN, V32
   Hosseinzadeh M, 2021, PROC CVPR IEEE, P2724, DOI 10.1109/CVPR46437.2021.00275
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Huang QB, 2022, IEEE T MULTIMEDIA, V24, P2004, DOI 10.1109/TMM.2021.3074803
   Jhamtani H, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4024
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Kim H., 2021, ICCV, P2095
   Kingma D. P., 2014, arXiv
   Li L, 2022, IEEE T IMAGE PROCESS, V31, P2726, DOI 10.1109/TIP.2022.3158546
   Liao B, 2020, IEEE ACCESS, V8, P79754, DOI 10.1109/ACCESS.2020.2990539
   Liao ZM, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5074, DOI 10.1145/3474085.3475712
   Lin Chin-Yew, 2004, Text summarization branches out, P74, DOI DOI 10.2307/3105454
   Liu Z, 2018, IEEE T IMAGE PROCESS, V27, P1822, DOI 10.1109/TIP.2017.2784560
   Longteng Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10324, DOI 10.1109/CVPR42600.2020.01034
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Luo YP, 2021, AAAI CONF ARTIF INTE, V35, P2286
   Oluwasanmi A, 2019, IEEE ACCESS, V7, P175929, DOI 10.1109/ACCESS.2019.2957513
   Oluwasanmi A, 2019, IEEE ACCESS, V7, P106772, DOI 10.1109/ACCESS.2019.2931223
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Park DH, 2019, IEEE I CONF COMP VIS, P4623, DOI 10.1109/ICCV.2019.00472
   Patriarche J, 2004, J DIGIT IMAGING, V17, P158, DOI 10.1007/s10278-004-1010-x
   Qiu Y., 2021, P IEEE CVF INT C COM, P1971
   Qu LG, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1104, DOI 10.1145/3404835.3462829
   Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677
   Subudhi BN, 2020, IEEE T MULTIMEDIA, V22, P912, DOI 10.1109/TMM.2019.2938342
   Sutskever I, 2014, ADV NEUR IN, V27
   Tan H, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1873
   Tu YB, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P63
   Tu YB, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9319
   Tu YB, 2022, IEEE T IMAGE PROCESS, V31, P3565, DOI 10.1109/TIP.2022.3159472
   Tu YB, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107702
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang YX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3792
   Xiangxi Shi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P574, DOI 10.1007/978-3-030-58568-6_34
   Xiao XY, 2019, IEEE T MULTIMEDIA, V21, P2942, DOI 10.1109/TMM.2019.2915033
   Yan CG, 2022, IEEE T CIRC SYST VID, V32, P43, DOI 10.1109/TCSVT.2021.3067449
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yang LY, 2021, IEEE T MULTIMEDIA, V23, P835, DOI 10.1109/TMM.2020.2990074
   Yao LL, 2022, AAAI CONF ARTIF INTE, P3108
   Yu LT, 2022, IEEE T MULTIMEDIA, V24, P1775, DOI 10.1109/TMM.2021.3072479
   Yunjae Jung, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P167, DOI 10.1007/978-3-030-58595-2_11
   Zanetti M, 2016, INT GEOSCI REMOTE SE, P3378, DOI 10.1109/IGARSS.2016.7729873
   Zhang JM, 2018, INT J COMPUT VISION, V126, P1084, DOI 10.1007/s11263-017-1059-x
   Zhang XY, 2021, PROC CVPR IEEE, P15460, DOI 10.1109/CVPR46437.2021.01521
NR 55
TC 2
Z9 2
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8828
EP 8841
DI 10.1109/TMM.2023.3242142
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000048
DA 2024-07-18
ER

PT J
AU Zhang, AR
   Yang, YD
   Xu, J
   Cao, XB
   Zhen, XT
   Shao, L
AF Zhang, Anran
   Yang, Yandan
   Xu, Jun
   Cao, Xianbin
   Zhen, Xiantong
   Shao, Ling
TI Latent Domain Generation for Unsupervised Domain Adaptation Object
   Counting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object counting; domain adaptation; unsupervised learning
AB Unsupervised cross-domain object counting has recently received great attention in computer vision, which generalizes the model from the source domain to the unlabeled target domain. However, it is an extremely challenging task because only unlabeled data is available from the target domain and the domain gap between two domains is implicit in object counting. In this paper, we propose a latent domain generation method to improve the generalization ability of unsupervised domain adaptation object counting by generating a latent domain. To this end, we propose a domain generator with random perturbations to learn a new latent distribution derived from the original source distribution. The latent domain generator can extract target information sampled in its stochastic latent representation, which preserves the original target information and enhances the diverse ability. Meanwhile, to ensure that the generated latent domain is consistent with the source domain in counting performance, we introduce a consistency loss to encourage similar output from latent and source domains. Moreover, to enhance the adaptation ability of the generated latent domain, we apply the adversarial loss to achieve alignment between the latent and target domains. The domain generator with the adversarial loss and consistency loss ensures that the generated domain is aligned to the target while also improving the robustness of the original source domain model. The experiment indicates that our framework can effortlessly extend to scenarios with different objects (crowd, cars). The experiments also demonstrate the effectiveness of our method on unsupervised realistic-to-realistic crowd counting problems.
C1 [Zhang, Anran; Yang, Yandan; Cao, Xianbin] Beihang Univ, Key Lab Adv Technol Near Space Informat Syst, Beijing 100191, Peoples R China.
   [Xu, Jun] Nankai Univ, Sch Stat & Data Sci, Tianjin 300071, Peoples R China.
   [Zhen, Xiantong] Univ Amsterdam, AIM Lab, NL-1012 WX Amsterdam, Netherlands.
   [Shao, Ling] Mohamed bin Zayed Univ Artificial Intelligence, Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.
C3 Beihang University; Nankai University; University of Amsterdam; Mohamed
   Bin Zayed University of Artificial Intelligence
RP Cao, XB (corresponding author), Beihang Univ, Key Lab Adv Technol Near Space Informat Syst, Beijing 100191, Peoples R China.
EM zhanganran@buaa.edu.cn; yangyandan@buaa.edu.cn;
   nankaimathxujun@gmail.com; xbcao@buaa.edu.cn; zhenxt@gmail.com;
   ling.shao@ieee.org
RI zhang, an/JMR-3763-2023; Shao, Ling/D-3535-2011
FU National Key Scientific Instrument and Equipment Development Project
   [61827901]; Fundamental Research Funds for the Central Universities of
   Nankai University [63211099]; Natural Science Foundation of China
   [91738301, 62002176, 62088101]
FX This work was supported in part by the National Key Scientific
   Instrument and Equipment Development Project under Grant 61827901, in
   part by the Fundamental Research Funds for the Central Universities of
   Nankai University 63211099, and in part by the Natural Science
   Foundation of China under Grants 91738301, 62002176, and 62088101.
CR [Anonymous], 2018, PROC EUR C COMPUT VI
   Bai S, 2020, PROC CVPR IEEE, P4593, DOI 10.1109/CVPR42600.2020.00465
   Boominathan L, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P640, DOI 10.1145/2964284.2967300
   Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45
   Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569
   Chen K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.21
   Cholakkal H, 2019, PROC CVPR IEEE, P12389, DOI 10.1109/CVPR.2019.01268
   Coates A., 2011, P 14 INT C ART INT S, V15, P215
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng WX, 2022, IEEE T MULTIMEDIA, V24, P2407, DOI 10.1109/TMM.2021.3080516
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gao JY, 2021, IEEE T CYBERNETICS, V51, P4822, DOI 10.1109/TCYB.2020.3034316
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Gong R, 2019, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2019.00258
   Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344
   Guerrero-Gómez-Olmedo R, 2015, LECT NOTES COMPUT SC, V9117, P423, DOI 10.1007/978-3-319-19390-8_48
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Jégou S, 2017, IEEE COMPUT SOC CONF, P1175, DOI 10.1109/CVPRW.2017.156
   Jiang XH, 2021, IEEE T MULTIMEDIA, V23, P443, DOI 10.1109/TMM.2020.2980945
   Kong D, 2006, INT C PATT RECOG, P1187
   Laine S, 2017, Arxiv, DOI [arXiv:1610.02242, DOI 10.48550/ARXIV.1610.02242]
   Lempitsky V., 2010, P ADV NEUR INF PROC, V23, P1, DOI DOI 10.5555/2997189.2997337
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Liu CC, 2019, PROC CVPR IEEE, P1217, DOI 10.1109/CVPR.2019.00131
   Liu L, 2021, IEEE T MULTIMEDIA, V23, P1060, DOI 10.1109/TMM.2020.2992979
   Liu LB, 2019, IEEE I CONF COMP VIS, P1774, DOI 10.1109/ICCV.2019.00186
   Liu W, 2019, PROC CVPR IEEE, P5182, DOI 10.1109/CVPR.2019.00533
   Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524
   Liu YT, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P129, DOI 10.1145/3394171.3413825
   Liu YT, 2019, PROC CVPR IEEE, P6462, DOI 10.1109/CVPR.2019.00663
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Loy CC, 2013, IEEE I CONF COMP VIS, P2256, DOI 10.1109/ICCV.2013.270
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7
   Myeongjin Kim, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12972, DOI 10.1109/CVPR42600.2020.01299
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Oñoro-Rubio D, 2016, LECT NOTES COMPUT SC, V9911, P615, DOI 10.1007/978-3-319-46478-7_38
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059
   Reddy MKK, 2020, IEEE WINT CONF APPL, P2803, DOI [10.1109/WACV45572.2020.9093409, 10.1109/wacv45572.2020.9093409]
   Ryan D, 2015, COMPUT VIS IMAGE UND, V130, P1, DOI 10.1016/j.cviu.2014.07.008
   Saleh SAM, 2015, ENG APPL ARTIF INTEL, V41, P103, DOI 10.1016/j.engappai.2015.01.007
   Sam DB, 2018, AAAI CONF ARTIF INTE, P7323
   Shen Z, 2018, PROC CVPR IEEE, P5245, DOI 10.1109/CVPR.2018.00550
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sindagi Vishwanath A., 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P212, DOI 10.1007/978-3-030-58621-8_13
   Sindagi VA, 2017, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2017.206
   Tan B, 2011, PATTERN RECOGN, V44, P2297, DOI 10.1016/j.patcog.2010.10.002
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262
   Vincent Pascal, 2008, P 25 INT C MACHINE L, DOI DOI 10.1145/1390156.1390294
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wan J, 2021, PROC CVPR IEEE, P1974, DOI 10.1109/CVPR46437.2021.00201
   Wan J, 2019, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2019.00416
   Wang L, 2019, IEEE INT CON MULTI, P193, DOI 10.1109/ICME.2019.00041
   Wang M, 2018, NEUROCOMPUTING, V312, P135, DOI 10.1016/j.neucom.2018.05.083
   Wang Q, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01365-4
   Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839
   Wang R, 2023, IEEE T MULTIMEDIA, V25, P1665, DOI 10.1109/TMM.2022.3146744
   Xu CF, 2019, IEEE I CONF COMP VIS, P8381, DOI 10.1109/ICCV.2019.00847
   Yan ZY, 2019, IEEE I CONF COMP VIS, P952, DOI 10.1109/ICCV.2019.00104
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522
   Yang GL, 2020, AAAI CONF ARTIF INTE, V34, P6615
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
NR 67
TC 5
Z9 5
U1 2
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1773
EP 1783
DI 10.1109/TMM.2022.3162710
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100014
DA 2024-07-18
ER

PT J
AU Zhang, HM
   Xu, M
AF Zhang, Haimin
   Xu, Min
TI Multiscale Emotion Representation Learning for Affective Image
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Affective image recognition; deep neural networks; multiscale
   representation learning
ID PREDICTION; NETWORKS
AB Recognition of emotions conveyed in images has attracted increasing research attention. Recent studies show that leveraging local affective regions helps to improve the recognition performance. However, these studies do not consider features from the broad context of the local affective regions, which could provide useful information for learning improved emotion representations. In this paper, we present a region-based multiscale network that learns features for the local affective region as well as the broad context for affective image recognition. The proposed network consists of an affective region detection module and a multiscale feature learning module. The class activation mapping method is used to generate pseudo affective regions from a pretrained deep neural network to train the detection module. For the affective region outputted by the detection module, three-scale features are extracted and then encoded by a kernel-based graph attention network for final emotion classification. We show that integrating features from the broad context is effective in improving the recognition performance. We experimentally evaluate the proposed network for both multi-class emotion recognition and binary sentiment classification on different benchmark datasets. The experimental results demonstrate that the proposed network achieves improved or comparable performance as compared to previous state-of-the-art models.
C1 [Zhang, Haimin; Xu, Min] Univ Technol Sydney, Sch Elect & Data Engn, Sydney, NSW 2007, Australia.
   [Zhang, Haimin; Xu, Min] Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
C3 University of Technology Sydney; University of Technology Sydney
RP Xu, M (corresponding author), Univ Technol Sydney, Sch Elect & Data Engn, Sydney, NSW 2007, Australia.; Xu, M (corresponding author), Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
EM haimin.Zhang@uts.edu.au; min.xu@uts.edu.au
OI Zhang, Haimin/0000-0002-0021-3634
CR Bresson X, 2018, Arxiv, DOI [arXiv:1711.07553, DOI 10.48550/ARXIV.1711.07553]
   Chen T, 2014, Arxiv, DOI arXiv:1410.8586
   Ekman P., 2013, Emotion in the Human Face: Guidelines for Research and an Integration of Findings, V11
   Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Herranz L, 2016, PROC CVPR IEEE, P571, DOI 10.1109/CVPR.2016.68
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Ioffe S, 2015, Arxiv, DOI [arXiv:1502.03167, DOI 10.48550/ARXIV.1502.03167]
   Jaderberg M., 2015, ICLR, DOI DOI 10.48550/ARXIV.1506.02025
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lang PeterJ., 1997, NIMH Center for the Study of Emotion and Attention, V1, p3. 6
   Lang PJ, 1998, BIOL PSYCHIAT, V44, P1248, DOI 10.1016/S0006-3223(98)00275-3
   LANG PJ, 1979, PSYCHOPHYSIOLOGY, V16, P495, DOI 10.1111/j.1469-8986.1979.tb01511.x
   Lee J, 2019, IEEE I CONF COMP VIS, P10142, DOI 10.1109/ICCV.2019.01024
   Li HB, 2012, INT C PATT RECOG, P2577
   Li ZH, 2018, INT C PATT RECOG, P3384, DOI 10.1109/ICPR.2018.8545489
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Machajdik J., 2010, P 18 ACM INT C MULT, P83, DOI DOI 10.1145/1873951.1873965
   Mikels JA, 2005, BEHAV RES METHODS, V37, P626, DOI 10.3758/BF03192732
   Panda R, 2018, LECT NOTES COMPUT SC, V11206, P594, DOI 10.1007/978-3-030-01216-8_36
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Paszke A, 2019, ADV NEUR IN, V32
   Peng KC, 2016, IEEE IMAGE PROC, P614, DOI 10.1109/ICIP.2016.7532430
   Perveen N, 2020, IEEE T IMAGE PROCESS, V29, P8316, DOI 10.1109/TIP.2020.3011846
   Qizhe Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10684, DOI 10.1109/CVPR42600.2020.01070
   Truong QT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1274, DOI 10.1145/3123266.3123374
   Rao TR, 2020, NEURAL PROCESS LETT, V51, P2043, DOI 10.1007/s11063-019-10033-9
   Rao TR, 2019, NEUROCOMPUTING, V333, P429, DOI 10.1016/j.neucom.2018.12.053
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salakhutdinov R., 2009, ARTIFICIAL INTELLIGE, V5, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157
   She DY, 2020, IEEE T MULTIMEDIA, V22, P1358, DOI 10.1109/TMM.2019.2939744
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun XD, 2018, NEUROCOMPUTING, V299, P42, DOI 10.1016/j.neucom.2018.03.030
   Vaswani A, 2017, ADV NEUR IN, V30
   Velickovic Petar, 2018, INT C LEARN REPR
   Wu LX, 2021, IEEE T CIRC SYST VID, V31, P3118, DOI 10.1109/TCSVT.2020.3036860
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Yang JF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3266
   Yang JF, 2018, IEEE T MULTIMEDIA, V20, P2513, DOI 10.1109/TMM.2018.2803520
   You QZ, 2016, AAAI CONF ARTIF INTE, P308
   You QZ, 2015, AAAI CONF ARTIF INTE, P381
   Yu FS, 2016, Arxiv, DOI [arXiv:1511.07122, DOI 10.48550/ARXIV.1511.07122]
   Zhang HM, 2021, KNOWL-BASED SYST, V229, DOI 10.1016/j.knosys.2021.107299
   Zhang HM, 2021, IEEE T MULTIMEDIA, V23, P2033, DOI 10.1109/TMM.2020.3007352
   Zhang HM, 2018, IEEE T MULTIMEDIA, V20, P2824, DOI 10.1109/TMM.2018.2808760
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P47, DOI 10.1145/2647868.2654930
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu XG, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3595
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 55
TC 7
Z9 7
U1 6
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2203
EP 2212
DI 10.1109/TMM.2022.3144804
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100047
DA 2024-07-18
ER

PT J
AU Zhang, JQ
   Jian, YR
   Wang, SH
   Jia, CM
   Wang, SS
   Ma, SW
   Gao, W
AF Zhang, Jiaqi
   Jian, Yunrui
   Wang, Suhong
   Jia, Chuanmin
   Wang, Shanshe
   Ma, Siwei
   Gao, Wen
TI Textural and Directional Information Based Offset In-Loop Filtering in
   AVS3
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE AVS3; in-loop filter; TDIO; textural and directional offset
ID VIDEO; PREDICTION; TRANSFORM
AB In this paper, we propose a novel low-complexity in-loop filtering approach named textural and directional information based offset (TDIO) for the video coding standard AVS3. Different from conventional offset-based filtering methods which partially use contextual samples, the key contribution of TDIO is that it fully utilizes the textural and edge directional features of each sample to comprehensively determine which type of texture characteristics each sample belongs to. The corresponding offsets are generated and signaled to decoder such that sample-level distortion is reduced. Specifically, the multi-directionality and sample-intensity pattern based classifiers are first proposed to extract the directional and textural features, respectively. The classification results are obtained by incorporating these features, and the optimal offset values for each class are derived based on rate-distortion optimization. Since sample-level offset signalling may cause heavy burden to the overhead of TDIO, we subsequently propose a filtering offset sharing mechanism based on historical information between available temporal-adjacent compressed frames. In addition, an iteration-based filter adaptation method is designed to improve the local adaptivity of TDIO for better compression efficiency. Experimental results show that the proposed TDIO achieves 0.64%, 1.29%, 1.86%, and 2.20% bit rate savings for all intra, random access, low delay B, and low delay P configurations, respectively. Moreover, TDIO is helpful to improve subjective quality by leveraging the fine-grained local texture characteristics. It can be observed that the blurring and ringing artifacts could be significantly suppressed by using the proposed method, yielding higher subjective quality.
C1 [Zhang, Jiaqi] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Zhang, Jiaqi] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Jian, Yunrui] Peking Univ, Shenzhen Grad Sch, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China.
   [Wang, Suhong; Jia, Chuanmin; Wang, Shanshe; Ma, Siwei; Gao, Wen] Peking Univ, Sch Comp Sci, Natl Engn Res Ctr Visual Technol, Beijing 100871, Peoples R China.
   [Wang, Shanshe; Ma, Siwei; Gao, Wen] Peking Univ, Informat Technol Res & Dev Innovat Ctr, Shaoxing 312000, Peoples R China.
   [Wang, Shanshe; Ma, Siwei; Gao, Wen] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Peking University; Peking University; Peking University; Peng Cheng
   Laboratory
RP Jia, CM (corresponding author), Peking Univ, Sch Comp Sci, Natl Engn Res Ctr Visual Technol, Beijing 100871, Peoples R China.
EM zhangjiaqi17@mails.ucas.ac.cn; yunruijian@pku.edu.cn;
   suhong.wang@pku.edu.cn; cmjia@pku.edu.cn; sswang@pku.edu.cn;
   swma@pku.edu.cn; wgao@pku.edu.cn
RI Zhang, Jiaqi/JCO-6818-2023
OI Zhang, Jiaqi/0000-0003-4555-3875; Jia, Chuanmin/0000-0002-7418-6245
FU National Natural Science Foundation of China [62031013, 62088102,
   62101007]; High Performance Computing Platform of Peking University
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62031013, 62088102, and 62101007, and
   in part by the High Performance Computing Platform of Peking University,
   which are gratefully acknowledged.
CR [Anonymous], 2002, Recommendation ITU-R BT.500-11 Methodology for the subjective assessment of the quality of television pictures
   [Anonymous], 2020, AVS3 software repository
   Bjotegaard G., 2001, VCEGM33
   Bordes P., 2021, P PICT COD S, P1
   Bross B, 2021, IEEE T CIRC SYST VID, V31, P3736, DOI 10.1109/TCSVT.2021.3101953
   Bross B, 2020, IEEE T CIRC SYST VID, V30, P1226, DOI 10.1109/TCSVT.2019.2949619
   Chen J., 2013, AVS-Document, M3263
   Chen J., 2020, AVS-Document, N2813
   Chen JL, 2020, IEEE T CIRC SYST VID, V30, P1208, DOI 10.1109/TCSVT.2019.2945830
   Chen J, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P69, DOI 10.1109/VCIP.2014.7051506
   Chi CC, 2015, IEEE T CIRC SYST VID, V25, P841, DOI 10.1109/TCSVT.2014.2364413
   Chien WJ, 2020, IEEE T CIRC SYST VID, V30, P1346, DOI 10.1109/TCSVT.2019.2956455
   Choi K, 2020, IEEE T CIRC SYST VID, V30, P1326, DOI 10.1109/TCSVT.2020.2971268
   Dai YY, 2017, LECT NOTES COMPUT SC, V10132, P28, DOI 10.1007/978-3-319-51811-4_3
   Fan K, 2019, IEEE T CIRC SYST VID, V29, P3716, DOI 10.1109/TCSVT.2018.2885002
   Fu CM, 2012, IEEE T CIRC SYST VID, V22, P1755, DOI 10.1109/TCSVT.2012.2221529
   Gao W., 2014, Advanced Video Coding Systems, P35
   Jia CM, 2021, IEEE T MULTIMEDIA, V23, P39, DOI 10.1109/TMM.2020.2981185
   Jian Y., 2020, AVS-Document, M5373
   Li JR, 2021, IEEE T IMAGE PROCESS, V30, P7305, DOI 10.1109/TIP.2021.3104191
   Lim WQ, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954544
   Lin K, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954561
   Lin WY, 2020, IEEE T MULTIMEDIA, V22, P2749, DOI 10.1109/TMM.2019.2962310
   List P, 2003, IEEE T CIRC SYST VID, V13, P614, DOI 10.1109/TCSVT.2003.815175
   Lu TR, 2020, IEEE DATA COMPR CONF, P193, DOI 10.1109/DCC47342.2020.00027
   Luo B, 2013, PICT COD SYMP, P49, DOI 10.1109/PCS.2013.6737680
   Ma D, 2021, IEEE J-STSP, V15, P378, DOI 10.1109/JSTSP.2020.3043064
   Ma HC, 2020, IEEE T MULTIMEDIA, V22, P1667, DOI 10.1109/TMM.2019.2957990
   Ma SW, 2016, IEEE MULTIMEDIA, V23, P16, DOI 10.1109/MMUL.2016.16
   Ma SW, 2015, IEEE SIGNAL PROC MAG, V32, P172, DOI 10.1109/MSP.2014.2371951
   PANG KK, 1994, IEEE T CIRC SYST VID, V4, P158, DOI 10.1109/76.285622
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun Y., 2021, P INT C MULT EXP, P1
   Taki Y., 1959, NHK Tech. Rep., V11, P117
   Tang CW, 2006, IEEE T MULTIMEDIA, V8, P11, DOI 10.1109/TMM.2005.861295
   Tsai CY, 2013, IEEE J-STSP, V7, P934, DOI 10.1109/JSTSP.2013.2271974
   Wang M, 2020, IEEE T IMAGE PROCESS, V29, P2931, DOI 10.1109/TIP.2019.2955238
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yoo SB, 2014, IEEE T MULTIMEDIA, V16, P1536, DOI 10.1109/TMM.2014.2327563
   Yoshino Tomonobu, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3477, DOI 10.1109/ICIP.2011.6116462
   Yuen M, 1998, SIGNAL PROCESS, V70, P247, DOI 10.1016/S0165-1684(98)00128-5
   Zhang J, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954503
   Zhou ML, 2021, IEEE T MULTIMEDIA, V23, P1106, DOI 10.1109/TMM.2020.2992968
   Zhou QY, 2021, IEEE T MULTIMEDIA, V23, P3867, DOI 10.1109/TMM.2020.3033092
NR 44
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5957
EP 5971
DI 10.1109/TMM.2022.3201747
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500023
DA 2024-07-18
ER

PT J
AU Zhang, ZH
   Yang, XQ
   Xu, C
AF Zhang, Zhihao
   Yang, Xianqiang
   Xu, Chao
TI Natural Image Stitching With Layered Warping Constraint
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image stitching; camera pose estimation; outlier rejection; flatness
   preserving; layered warping; mesh deformation
ID PANORAMAS
AB Stitching images with parallax for naturalness remains a challenging problem. This paper proposes an image stitching method which preserves the flatness of planes in the scene for a natural look. Our method formulates the alignment of images as the camera parameters and the normal vectors of planes. Given a set of feature point matches, a process of grouping points into different layers and rejecting outliers is introduced. According to the epipolar constraint of corresponding points in two images, the focal length and the pose change of the camera are recovered simultaneously. Then, the normal vectors are estimated from the point pairs. To achieve good alignment and guide the warping of images, the model is combined with the mesh deformation as a global similarity constraint. In addition, bundle adjustment is adopted to maintain the consistency for stitching multiple images. Experiment shows that the proposed approach outperforms some state-of-the-art warps on real-world scenes.
C1 [Zhang, Zhihao] Harbin Inst Technol, Res Inst Intelligent Control & Syst, Harbin 150001, Peoples R China.
   [Yang, Xianqiang] Ningbo Inst Intelligent Equipment Technol Co Ltd, Ningbo 315200, Peoples R China.
C3 Harbin Institute of Technology
RP Yang, XQ (corresponding author), Ningbo Inst Intelligent Equipment Technol Co Ltd, Ningbo 315200, Peoples R China.
EM zhihaozhang@hit.edu.cn; xianqiangyang@hit.edu.cn; chaoxu@hit.edu.cn
OI Zhang, Zhihao/0000-0001-7814-6913; yang, xianqiang/0000-0002-0036-6921
FU Joint Funds of the National Natural Science Foundation of China
   [U20A20188]
FX This work was supported by Joint Funds of the National Natural Science
   Foundation of China under Grant U20A20188.
CR Anderson R, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980257
   [Anonymous], 1989, THESIS U CALIFORNIA
   [Anonymous], 2004, MSRTR200448
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Chang CH, 2014, PROC CVPR IEEE, P3254, DOI 10.1109/CVPR.2014.422
   Chen YS, 2016, LECT NOTES COMPUT SC, V9909, P186, DOI 10.1007/978-3-319-46454-1_12
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gaddam VR, 2016, IEEE T MULTIMEDIA, V18, P1819, DOI 10.1109/TMM.2016.2586304
   Gallego G, 2015, J MATH IMAGING VIS, V51, P378, DOI 10.1007/s10851-014-0528-x
   Gao J., 2013, Eurographics (Short Papers), P45, DOI DOI 10.2312/CONF/EG2013/SHORT/045-048
   Gao JH, 2011, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2011.5995433
   He BT, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16010007
   Kaehler A., 2016, Learning OpenCV 3
   Li J, 2018, IEEE T MULTIMEDIA, V20, P1672, DOI 10.1109/TMM.2017.2777461
   Li N, 2018, SIGNAL IMAGE VIDEO P, V12, P967, DOI 10.1007/s11760-018-1241-9
   Li N, 2018, IEEE T MULTIMEDIA, V20, P1365, DOI 10.1109/TMM.2017.2771566
   Li XH, 2015, ISPRS J PHOTOGRAMM, V109, P108, DOI 10.1016/j.isprsjprs.2015.09.009
   Li YL, 2019, IEEE T IMAGE PROCESS, V28, P4730, DOI 10.1109/TIP.2019.2909800
   Lin CC, 2015, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2015.7298719
   Lin KM, 2016, LECT NOTES COMPUT SC, V9907, P370, DOI 10.1007/978-3-319-46487-9_23
   Lin WY, 2011, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2011.5995314
   Liu F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531350
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Nistér D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17
   Pham TT, 2014, IEEE T PATTERN ANAL, V36, P1658, DOI 10.1109/TPAMI.2013.2296310
   Shum HY, 2001, MG COMP SCI, P227
   Szeliski R, 2006, HANDBOOK OF MATHEMATICAL MODELS IN COMPUTER VISION, P273, DOI 10.1007/0-387-28831-7_17
   Tang MH, 2019, IEEE T MULTIMEDIA, V21, P957, DOI 10.1109/TMM.2018.2867266
   Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298
   Vedaldi A., 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiang TZ, 2018, PATTERN RECOGN, V83, P481, DOI 10.1016/j.patcog.2018.06.013
   Zaragoza J, 2014, IEEE T PATTERN ANAL, V36, P1285, DOI 10.1109/TPAMI.2013.247
   Zaragoza J, 2013, PROC CVPR IEEE, P2339, DOI 10.1109/CVPR.2013.303
   Zhang F, 2014, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2014.423
   Zhang GF, 2016, IEEE T IMAGE PROCESS, V25, P3099, DOI 10.1109/TIP.2016.2535225
   Zhang ZY, 1997, IMAGE VISION COMPUT, V15, P59, DOI 10.1016/S0262-8856(96)01112-2
   Zhao Q, 2013, IEEE T MULTIMEDIA, V15, P1745, DOI 10.1109/TMM.2013.2280249
NR 38
TC 5
Z9 5
U1 4
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 329
EP 338
DI 10.1109/TMM.2021.3126157
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9E8KM
UT WOS:000937028400023
DA 2024-07-18
ER

PT J
AU Zhou, HB
   Wu, W
   Zhang, YD
   Ma, JY
   Ling, HB
AF Zhou, Huabing
   Wu, Wei
   Zhang, Yanduo
   Ma, Jiayi
   Ling, Haibin
TI Semantic-Supervised Infrared and Visible Image Fusion Via a
   Dual-Discriminator Generative Adversarial Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image fusion; Semantics; Feature extraction; Generators; Transforms;
   Generative adversarial networks; Games; infrared image; visible image;
   semantic supervised; dual-discriminator
ID QUALITY ASSESSMENT; MULTI-FOCUS; GRADIENT; ENHANCEMENT
AB Image fusion synthesizes a new image from multiple images of the same scene. The synthesized image should be suitable for human visual perception and follow-up high-level image-processing tasks. However, existing methods focus on fusing low-level features, ignoring high-level semantic perception information. We propose a new end-to-end model to obtain a more semantically consistent image in infrared and visible image fusion, termed semantic-supervised dual-discriminator generative adversarial network (SDDGAN). In particular, we design an information quantity discrimination (IQD) block to guide fusion progress. For each source image, the block determines the weight for preserving each semantic object's feature. By this way, the generator learns to fuse various semantic objects via different weights to preserve their characteristics. Moreover, the dual discriminator is employed to identify the distribution of infrared and visible information in the fused image. Each discriminator acts on a certain modality (infrared/visible) of different semantic objects in the fused image to preserve and enhance their modality features. Thus, our fused image is more informative. Both the thermal radiation in the infrared image and the visible image texture details can be well preserved. Qualitative and quantitative experiments demonstrate the superiority of our SDDGAN over state-of-the-art methods in terms of visual effects, efficiency, and quantitative metrics.
C1 [Zhou, Huabing; Wu, Wei; Zhang, Yanduo] Wuhan Inst Technol, Hubei Key Lab Intelligent Robot, Wuhan 430205, Peoples R China.
   [Ma, Jiayi] Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.
   [Ling, Haibin] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 Wuhan Institute of Technology; Wuhan University; State University of New
   York (SUNY) System; State University of New York (SUNY) Stony Brook
RP Zhang, YD (corresponding author), Wuhan Inst Technol, Hubei Key Lab Intelligent Robot, Wuhan 430205, Peoples R China.
EM zhouhuabing@gmail.com; wuwei.wit@qq.com; zhangyanduo@hotmail.com;
   jyma2010@gmail.com; hling@cs.stonybrook.edu
RI Li, YiXue/JRW-6306-2023; Lu, Lu/JPE-5187-2023; Chen, Fang/JZE-4446-2024;
   zhang, jt/JVE-1333-2024; Lin, Xiaoqi/KFS-5750-2024; Zhang,
   Chi/JSK-0744-2023; ZHU, JIALI/JNE-3065-2023; cheng, chen/JHS-9462-2023;
   Ma, Jiayi/Y-2470-2019
OI Ma, Jiayi/0000-0003-3264-3265; Zhou, Huabing/0000-0001-5007-7303; Ling,
   Haibin/0000-0003-4094-8413
FU National Natural Science Foundation of China [62171327, 62171328,
   62072350, 61773295]; Hubei Technology Innovation Project [2019AAA045];
   Key Scientific, and Technological Research Project of Hubei Provincial
   Education Department [D20201507]; Nuclear Energy Development Project
   (Sub-Project: Artificial Intelligence in Nuclear Reactors) in State
   Administration of Science, Technology, and Industry for National
   Defence, PRC [B210610]; first batch of Application Basic Technology, and
   Science Research Foundation in Hubei Nuclear Power Operation Engineering
   Technology Research Center [ZX200302]; Graduate Innovative Fund of Wuhan
   Institute of Technology [CX2020225]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62171327, 62171328, 62072350, and
   61773295, in part by the Hubei Technology Innovation Project under Grant
   2019AAA045, in part by the Key Scientific, and Technological Research
   Project of Hubei Provincial Education Department under Grant D20201507,
   in part by the first batch of Application Basic Technology, and Science
   Research Foundation in Hubei Nuclear Power Operation Engineering
   Technology Research Center under Grant B210610, in part by the Nuclear
   Energy Development Project (Sub-Project: Artificial Intelligence in
   Nuclear Reactors) in State Administration of Science, Technology, and
   Industry for National Defence, PRC under Grant ZX200302, and in part by
   Graduate Innovative Fund of Wuhan Institute of Technology under Grant
   CX2020225. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Dr. Mai Xu.
CR Aslantas V, 2015, AEU-INT J ELECTRON C, V69, P160, DOI 10.1016/j.aeue.2015.09.004
   Bavirisetti D. P., 2017, PROC INT C INF FUSIO, P1
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Eskicioglu AM, 1995, IEEE T COMMUN, V43, P2959, DOI 10.1109/26.477498
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352
   Han JG, 2013, NEUROCOMPUTING, V111, P70, DOI 10.1016/j.neucom.2012.12.015
   Hou JL, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23030376
   Hu HM, 2017, IEEE T MULTIMEDIA, V19, P2706, DOI 10.1109/TMM.2017.2711422
   Hu JW, 2012, INFORM FUSION, V13, P196, DOI 10.1016/j.inffus.2011.01.002
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jin X, 2017, INFRARED PHYS TECHN, V85, P478, DOI 10.1016/j.infrared.2017.07.010
   Kong WW, 2019, IEEE T INSTRUM MEAS, V68, P938, DOI 10.1109/TIM.2018.2865046
   Lewis JJ, 2007, INFORM FUSION, V8, P119, DOI 10.1016/j.inffus.2005.09.006
   Li H, 2018, INT C PATT RECOG, P2705, DOI 10.1109/ICPR.2018.8546006
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li J, 2021, IEEE T MULTIMEDIA, V23, P1383, DOI 10.1109/TMM.2020.2997127
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Liu Y, 2015, IET IMAGE PROCESS, V9, P347, DOI 10.1049/iet-ipr.2014.0311
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Liu Y, 2016, IEEE SIGNAL PROC LET, V23, P1882, DOI 10.1109/LSP.2016.2618776
   Liu Z, 2001, PATTERN RECOGN LETT, V22, P929, DOI 10.1016/S0167-8655(01)00047-2
   Liu ZM, 2022, IEEE T MULTIMEDIA, V24, P451, DOI 10.1109/TMM.2021.3053401
   Ma JY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3075747
   Ma JY, 2020, IEEE T IMAGE PROCESS, V29, P4980, DOI 10.1109/TIP.2020.2977573
   Ma JY, 2020, INFORM FUSION, V54, P85, DOI 10.1016/j.inffus.2019.07.005
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Ma JY, 2016, INFORM FUSION, V31, P100, DOI 10.1016/j.inffus.2016.02.001
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Madheswari K, 2017, QUANT INFR THERM J, V14, P24, DOI 10.1080/17686733.2016.1229328
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Petrovic V, 2005, IEEE I CONF COMP VIS, P1866
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Shibata T, 2015, PROC SPIE, V9404, DOI 10.1117/12.2077050
   Wang R, 2014, INT J REMOTE SENS, V35, P1640, DOI 10.1080/01431161.2014.880819
   Xu H, 2020, AAAI CONF ARTIF INTE, V34, P12484
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Xu H, 2020, IEEE T IMAGE PROCESS, V29, P7203, DOI 10.1109/TIP.2020.2999855
   Yang B, 2012, INFORM FUSION, V13, P10, DOI 10.1016/j.inffus.2010.04.001
   Yang B, 2010, IEEE T INSTRUM MEAS, V59, P884, DOI 10.1109/TIM.2009.2026612
   Yang Y, 2017, IEEE T INSTRUM MEAS, V66, P691, DOI 10.1109/TIM.2017.2658098
   Yin JL, 2022, IEEE T MULTIMEDIA, V24, P2841, DOI 10.1109/TMM.2021.3089324
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhang H, 2020, AAAI CONF ARTIF INTE, V34, P12797
   Zhang Q, 2018, INFORM FUSION, V40, P57, DOI 10.1016/j.inffus.2017.05.006
   Zhang Y, 2017, INFRARED PHYS TECHN, V83, P227, DOI 10.1016/j.infrared.2017.05.007
   Zhao JF, 2016, INFRARED PHYS TECHN, V76, P295, DOI 10.1016/j.infrared.2016.01.020
   Zhao JF, 2014, INFRARED PHYS TECHN, V62, P86, DOI 10.1016/j.infrared.2013.11.008
   Zhao WD, 2018, IEEE T MULTIMEDIA, V20, P866, DOI 10.1109/TMM.2017.2760100
   Zhou HB, 2020, IEEE J-STARS, V13, P4564, DOI 10.1109/JSTARS.2020.3015350
   Zhou HB, 2020, IEEE T IMAGE PROCESS, V29, P5216, DOI 10.1109/TIP.2020.2980210
   Zhou HB, 2016, IEEE GEOSCI REMOTE S, V13, P374, DOI 10.1109/LGRS.2016.2514521
NR 55
TC 45
Z9 46
U1 37
U2 97
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 635
EP 648
DI 10.1109/TMM.2021.3129609
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800023
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Chen, JP
   Ying, PG
   Fu, XL
   Luo, XP
   Guan, H
   Wei, KM
AF Chen, Jinpeng
   Ying, Pinguang
   Fu, Xiangling
   Luo, Xiaopeng
   Guan, Hao
   Wei, Kaimin
TI Automatic Tagging by Leveraging Visual and Annotated Features in Social
   Media
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Annotations; Image annotation; Visualization; Semantics; Hidden Markov
   models; Social networking (online); Multimedia Web sites; Image
   annotation; social media; annotation embedding; social embedding;
   cooperative training
ID IMAGE ANNOTATION; REPRESENTATION; MODEL
AB Automatic image annotation is one of the research fields helping to extract the meaning of images, which aims at the production of a set of semantic annotations for an image to help better present the concept. Over the past few decades, researchers have developed many approaches for automatic image annotation. Nevertheless, previous studies have not fully accounted for visual features and annotated features. Therefore, it is still possible to achieve a better annotation performance by combining visual and annotated information. In this study, we aim to associate multiple semantic tags with a given image. In particular, we detect how to obtain the image annotation by utilizing visual and annotated information. To take advantage of visual information, we first designed a modified neural network method to acquire the features of the image content. In addition, to obtain the annotated features, we exploit an aggregated network embedding approach that consists of annotation embedding, social embedding, profile embedding, and semantic embedding. Finally, to produce an accurate image annotation, we integrate the two aforementioned methods, that is, combining the visual and annotated information, to build a unified cooperative training framework. The experimental results on three real-world datasets clarify that our presented method is superior to the currently popular image annotation approaches.
C1 [Chen, Jinpeng; Fu, Xiangling] Beijing Univ Posts & Telecommun, Natl Pilot Software Engn Sch, Sch Comp Sci, Beijing 100876, Peoples R China.
   [Ying, Pinguang] Shanghai Univ Int Business & Econ, Sch Trade Negotiat, Shanghai 200336, Peoples R China.
   [Luo, Xiaopeng] Beijing Univ Posts & Telecommun, Sch Econ & Management, Beijing 100876, Peoples R China.
   [Guan, Hao] Beijing Univ Posts & Telecommun, Sch Software Engn, Beijing 100876, Peoples R China.
   [Wei, Kaimin] Jinan Univ, Coll Informat Sci & Technol, Guangzhou 510632, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Shanghai University of
   International Business & Economics; Beijing University of Posts &
   Telecommunications; Beijing University of Posts & Telecommunications;
   Jinan University
RP Chen, JP (corresponding author), Beijing Univ Posts & Telecommun, Natl Pilot Software Engn Sch, Sch Comp Sci, Beijing 100876, Peoples R China.; Wei, KM (corresponding author), Jinan Univ, Coll Informat Sci & Technol, Guangzhou 510632, Peoples R China.
EM jpchen@bupt.edu.cn; yingpinguang@suibe.edu.cn; fuxiangling@bupt.edu.cn;
   luoxiaopeng1992@bupt.edu.cn; guanhao@bupt.edu.cn; cswei@jnu.edu.cn
RI fu, xiang/HJH-8919-2023; fu, xiang/JCE-2186-2023
OI Chen, Jinpeng/0000-0003-4157-5110; Wei, Kaimin/0000-0002-8925-6453; Fu,
   Xiangling/0000-0002-1492-2829
FU National Key R&D Program of China [2018YFB1402600]; National Natural
   Science Foundation of China [61702043, 61972178]; Key-Area Research and
   Development Program of Guangdong Province [2019B010137005]; Guangdong
   Basic and Applied Basic Research Foundation [2019A1515011753];
   Fundamental Research Funds for the Central Universities [21620432];
   Beijing Natural Science Foundation [4194086]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018YFB1402600, in part by the National Natural Science
   Foundation of China under Grants 61702043, and 61972178, in part by the
   Key-Area Research and Development Program of Guangdong Province under
   Grant 2019B010137005, in part by the Guangdong Basic and Applied Basic
   Research Foundation under Grant 2019A1515011753, in part by the
   Fundamental Research Funds for the Central Universities under Grant
   21620432, and in part by the Beijing Natural Science Foundation under
   Grant 4194086. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Guo-Jun Qi.
CR Canales L, 2016, PROCEEDINGS OF 3RD IEEE/ACM INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED ANALYTICS, (DSAA 2016), P726, DOI 10.1109/DSAA.2016.78
   Cao XC, 2016, IEEE T NEUR NET LEAR, V27, P1253, DOI 10.1109/TNNLS.2015.2488637
   Cao Z, 2018, AAAI CONF ARTIF INTE, P2803
   Chang SY, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P119, DOI 10.1145/2783258.2783296
   Chaudhary C, 2020, IEEE T MULTIMEDIA, V22, P897, DOI 10.1109/TMM.2019.2937181
   Chen Muhao, 2018, P 2018 SIAM INT C DA, P315
   Cheng QM, 2018, PATTERN RECOGN, V79, P242, DOI 10.1016/j.patcog.2018.02.017
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Cui CR, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P957
   Dong X, 2017, AAAI CONF ARTIF INTE, P1309
   Donnat C, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1320, DOI 10.1145/3219819.3220025
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97
   Fei Wu, 2015, IEEE Transactions on Big Data, V1, P109, DOI 10.1109/TBDATA.2015.2497270
   Feng LN, 2016, IEEE T PATTERN ANAL, V38, P785, DOI 10.1109/TPAMI.2015.2469281
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P309, DOI 10.1109/ICCV.2009.5459266
   Hao ZG, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0206971
   Hu HX, 2016, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2016.323
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiancheng Li, 2016, Advances in Multimedia Information Processing - PCM 2016. 17th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 9916, P345, DOI 10.1007/978-3-319-48890-5_34
   Jin JR, 2016, INT C PATT RECOG, P2452, DOI 10.1109/ICPR.2016.7900004
   Jing XY, 2016, IEEE T IMAGE PROCESS, V25, P2712, DOI 10.1109/TIP.2016.2549459
   Johnson J, 2015, IEEE I CONF COMP VIS, P4624, DOI 10.1109/ICCV.2015.525
   Kalayeh MM, 2014, PROC CVPR IEEE, P184, DOI 10.1109/CVPR.2014.31
   Kennedy L.S., 2006, Proc. ACM Multimedia Information Retrieval, P249, DOI DOI 10.1145/1178677.1178712
   Li XR, 2016, ACM COMPUT SURV, V49, DOI 10.1145/2906152
   Li YQ, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P2163, DOI 10.1145/3132847.3133060
   Liang SS, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1764, DOI 10.1145/3219819.3220043
   Liu Dong., 2010, Proceedings of the International Conference on Multimedia, P491
   Ma YC, 2019, MULTIMED TOOLS APPL, V78, P3767, DOI 10.1007/s11042-018-6038-x
   Maihami V, 2018, PHYSICA A, V507, P123, DOI 10.1016/j.physa.2018.05.028
   Maihami V, 2017, ARTIF INTELL REV, V48, P331, DOI 10.1007/s10462-016-9502-x
   Mayhew MB, 2016, IEEE IMAGE PROC, P2266, DOI 10.1109/ICIP.2016.7532762
   Moran S, 2014, INT J MULTIMED INF R, V3, P209, DOI 10.1007/s13735-014-0063-y
   Murthy V.N., 2014, ICMR 2014 P ACM INT, P369, DOI DOI 10.1145/2578726.2578774
   Murthy VN, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P603, DOI 10.1145/2671188.2749391
   Murthy VN, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P299, DOI 10.1145/2911996.2912055
   Niu YL, 2019, IEEE T IMAGE PROCESS, V28, P1720, DOI 10.1109/TIP.2018.2881928
   Okura S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1933, DOI 10.1145/3097983.3098108
   Ordonez V, 2013, IEEE I CONF COMP VIS, P2768, DOI 10.1109/ICCV.2013.344
   Parra E, 2018, INT C PROGRAM COMPRE, P222, DOI 10.1145/3196321.3196351
   Perozzi B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P701, DOI 10.1145/2623330.2623732
   Pobar M, 2016, 2016 39TH INTERNATIONAL CONVENTION ON INFORMATION AND COMMUNICATION TECHNOLOGY, ELECTRONICS AND MICROELECTRONICS (MIPRO), P1324, DOI 10.1109/MIPRO.2016.7522345
   Qi G.-J., 2012, Proc. of the 5th ACM Intl. Conf. on Web Search and Data Mining (WSDM), P553, DOI DOI 10.1145/2124295.2124363
   Qi G.-J., 2009, P ACM INT C MULT, P243
   Qi GJ, 2017, IEEE T PATTERN ANAL, V39, P1360, DOI 10.1109/TPAMI.2016.2587643
   Qi GJ, 2012, IEEE T PATTERN ANAL, V34, P850, DOI 10.1109/TPAMI.2011.191
   Rahman S, 2020, IEEE T MULTIMEDIA, V22, P242, DOI 10.1109/TMM.2019.2924511
   Runge N, 2016, PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON HUMAN-COMPUTER INTERACTION WITH MOBILE DEVICES AND SERVICES (MOBILEHCI 2016), P846, DOI 10.1145/2957265.2961836
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Seyler D, 2018, ACM/SIGIR PROCEEDINGS 2018, P953, DOI 10.1145/3209978.3210103
   Shi Yu, 2018, Proc SIAM Int Conf Data Min, V2018, P144, DOI 10.1137/1.9781611975321.16
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Sun FM, 2014, IEEE T IMAGE PROCESS, V23, P1028, DOI 10.1109/TIP.2014.2298978
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Tang J., ACM T MULTIM COMPUT, V12
   Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093
   Tang JH, 2019, IEEE T PATTERN ANAL, V41, P2027, DOI 10.1109/TPAMI.2019.2906603
   Tang JH, 2017, IEEE T PATTERN ANAL, V39, P1662, DOI 10.1109/TPAMI.2016.2608882
   Tariq A, 2018, IMAGE VISION COMPUT, V69, P33, DOI 10.1016/j.imavis.2017.11.002
   Tuarob S, 2013, ACM-IEEE J CONF DIG, P239
   Uricchio T, 2017, PATTERN RECOGN, V71, P144, DOI 10.1016/j.patcog.2017.05.019
   Verma Y, 2017, INT J COMPUT VISION, V121, P126, DOI 10.1007/s11263-016-0927-0
   Verma Y, 2012, LECT NOTES COMPUT SC, V7574, P836, DOI 10.1007/978-3-642-33712-3_60
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   Wang DX, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1225, DOI 10.1145/2939672.2939753
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang RG, 2017, J VIS COMMUN IMAGE R, V49, P213, DOI 10.1016/j.jvcir.2017.07.004
   Wei YW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1446, DOI 10.1145/3343031.3350858
   Wu BY, 2018, PROC CVPR IEEE, P7967, DOI 10.1109/CVPR.2018.00831
   Yin YF, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1304, DOI 10.1145/3343031.3351090
   Zhang B, 2016, IFIP ADV INF COMM TE, V486, P211, DOI 10.1007/978-3-319-48390-0_22
   Zhang C, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P595, DOI 10.1145/3097983.3098027
   Zhang R., 2011, ACM Multimedia, P1513
   Zhang WF, 2018, MULTIMED TOOLS APPL, V77, P22385, DOI 10.1007/s11042-018-5973-x
   Zhang Y, 2016, PROC CVPR IEEE, P5985, DOI 10.1109/CVPR.2016.644
   Zhao YF, 2009, EXPERT SYST APPL, V36, P9813, DOI 10.1016/j.eswa.2009.02.050
NR 77
TC 3
Z9 3
U1 2
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2218
EP 2229
DI 10.1109/TMM.2021.3055037
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600001
DA 2024-07-18
ER

PT J
AU Huang, WL
   Hung, CY
   Lin, IC
AF Huang, Wei-Lun
   Hung, Chun-Yi
   Lin, I-Chen
TI Confidence-Based 6D Object Pose Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Pose estimation; Three-dimensional displays; Feature extraction; Image
   segmentation; Detectors; Training; Real-time systems; 6-D pose
   estimation; prediction confidence formulation; deep neural network
AB The aim of this paper is to estimate the six-degree-of-freedom (6-DOF) poses of objects from a single RGB image in which the target objects are partially occluded. Most recent studies have formulated methods for predicting the projected 2-D locations of 3-D keypoints through a deep neural network and then used a PnP algorithm to compute the 6-DOF poses. Several researchers have pointed out the uncertainty of the predicted locations and modelled it according to predefined rules or functions, but the performance of such approaches may still be degraded if occlusion is present. To address this problem, we formulated 2-D keypoint locations as probabilistic distributions in our novel loss function and developed a confidence-based pose estimation network. This network not only predicts the 2-D keypoint locations from each visible patch of a target object but also provides the corresponding confidence values in an unsupervised fashion. Through the proper fusion of the most reliable local predictions, the proposed method can improve the accuracy of pose estimation when target objects are partially occluded. Experiments demonstrated that our method outperforms state-of-the-art methods on a main occlusion data set used for estimating 6-D object poses. Moreover, this framework is efficient and feasible for realtime multimedia applications.
C1 [Huang, Wei-Lun; Hung, Chun-Yi; Lin, I-Chen] Natl Yang Ming Chiao Tung Univ, Inst Multimedia Engn, Coll Comp Sci, Natl Chiao Tung Univ, Hsinchu 30010, Taiwan.
C3 National Yang Ming Chiao Tung University
RP Lin, IC (corresponding author), Natl Yang Ming Chiao Tung Univ, Inst Multimedia Engn, Coll Comp Sci, Natl Chiao Tung Univ, Hsinchu 30010, Taiwan.
EM alen0216056@gmail.com; bally41234.eecs03@nctu.edu.tw;
   ichen-lin@cs.nctu.edu.tw
OI Huang, Wei-Lun/0000-0003-4845-766X; Lin, I-Chen/0000-0001-9924-4723
FU Ministry of Science and Technology, Taiwan [MOST 109-2221-E-009-122-MY3]
FX This work was supported by the Ministry of Science and Technology,
   Taiwan under Grant MOST 109-2221-E-009-122-MY3.
CR Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bochkovskiy A., 2020, PREPRINT
   Brachmann E, 2014, LECT NOTES COMPUT SC, V8690, P536, DOI 10.1007/978-3-319-10605-2_35
   Chen Wang, 2020, 2020 IEEE International Conference on Robotics and Automation (ICRA), P10059, DOI 10.1109/ICRA40945.2020.9196679
   Dwibedi D, 2017, IEEE I CONF COMP VIS, P1310, DOI 10.1109/ICCV.2017.146
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Fu C.-Y., 2017, DSSD: Deconvolutional Single Shot Detector
   Fu KR, 2019, IEEE T MULTIMEDIA, V21, P457, DOI 10.1109/TMM.2018.2859746
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   He YH, 2019, PROC CVPR IEEE, P2883, DOI 10.1109/CVPR.2019.00300
   Hinterstoisser S, 2012, IEEE T PATTERN ANAL, V34, P876, DOI 10.1109/TPAMI.2011.206
   Hinterstoisser S, 2011, IEEE I CONF COMP VIS, P858, DOI 10.1109/ICCV.2011.6126326
   Hinterstoisser S, 2010, PROC CVPR IEEE, P2257, DOI 10.1109/CVPR.2010.5539908
   Hinterstoisser V., 2012, P COMP VIS ACCV 2012, P548
   Hu YL, 2020, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR42600.2020.00300
   Hu YL, 2019, PROC CVPR IEEE, P3380, DOI 10.1109/CVPR.2019.00350
   Jafari O. H., 2018, Asian Conference on Computer Vision, P477
   Kehl W, 2017, IEEE I CONF COMP VIS, P1530, DOI 10.1109/ICCV.2017.169
   Lepetit Vincent, 2005, Foundations and Trends in Computer Graphics and Vision, V1, P1, DOI 10.1561/0600000001
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Li Y, 2018, LECT NOTES COMPUT SC, V11210, P695, DOI [10.1007/s11263-019-01250-9, 10.1007/978-3-030-01231-1_42]
   Li Y, 2019, IEEE T MULTIMEDIA, V21, P875, DOI 10.1109/TMM.2018.2867720
   Li ZG, 2019, IEEE I CONF COMP VIS, P7677, DOI 10.1109/ICCV.2019.00777
   Liebelt J, 2010, PROC CVPR IEEE, P1688, DOI 10.1109/CVPR.2010.5539836
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu YP, 2019, IEEE T MULTIMEDIA, V21, P2776, DOI 10.1109/TMM.2019.2913321
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Manhardt F, 2018, LECT NOTES COMPUT SC, V11218, P833, DOI 10.1007/978-3-030-01264-9_49
   Oberweger M, 2018, LECT NOTES COMPUT SC, V11219, P125, DOI 10.1007/978-3-030-01267-0_8
   Park K, 2019, IEEE I CONF COMP VIS, P7667, DOI 10.1109/ICCV.2019.00776
   Payet N, 2011, IEEE I CONF COMP VIS, P983, DOI 10.1109/ICCV.2011.6126342
   Peng SD, 2019, PROC CVPR IEEE, P4556, DOI 10.1109/CVPR.2019.00469
   Rad M, 2017, IEEE I CONF COMP VIS, P3848, DOI 10.1109/ICCV.2017.413
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rothganger F, 2006, INT J COMPUT VISION, V66, P231, DOI 10.1007/s11263-005-3674-1
   Song C, 2020, PROC CVPR IEEE, P428, DOI 10.1109/CVPR42600.2020.00051
   Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308
   Sundermeyer M, 2018, LECT NOTES COMPUT SC, V11210, P712, DOI 10.1007/978-3-030-01231-1_43
   Tekin B, 2018, PROC CVPR IEEE, P292, DOI 10.1109/CVPR.2018.00038
   Tulsiani S, 2015, PROC CVPR IEEE, P1510, DOI 10.1109/CVPR.2015.7298758
   Wagner D, 2008, INT SYM MIX AUGMENT, P125, DOI 10.1109/ISMAR.2008.4637338
   Wang C, 2019, PROC CVPR IEEE, P3338, DOI 10.1109/CVPR.2019.00346
   Wang H, 2019, PROC CVPR IEEE, P2637, DOI 10.1109/CVPR.2019.00275
   Wu LC, 2016, PROCEEDINGS I3D 2016: 20TH ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, P95, DOI 10.1145/2856400.2856416
   Xiang Y, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Zakharov S, 2019, IEEE I CONF COMP VIS, P1941, DOI 10.1109/ICCV.2019.00203
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P470, DOI 10.1109/TMM.2013.2293424
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou GL, 2021, IEEE T MULTIMEDIA, V23, P1630, DOI 10.1109/TMM.2020.3001533
   Zhou K, 2016, DESTECH TRANS COMP
NR 60
TC 8
Z9 10
U1 2
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3025
EP 3035
DI 10.1109/TMM.2021.3092149
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000027
DA 2024-07-18
ER

PT J
AU Lo, L
   Xie, HX
   Shuai, HH
   Cheng, WH
AF Lo, Ling
   Xie, Hongxia
   Shuai, Hong-Han
   Cheng, Wen-Huang
TI Facial Chirality: From Visual Self-Reflection to Robust Facial Feature
   Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Faces; Feature extraction; Transformers; Reflection; Robustness; Facial
   features; Face recognition; Facial expression; visual chirality; feature
   disentanglement; deep learning; vision transformer
ID NETWORK
AB As a fundamental vision task, facial expression recognition has made substantial progress recently. However, the recognition performance often degrades significantly in real-world scenarios due to the lack of robust facial features. In this paper, we propose an effective facial feature learning method that takes the advantage of facial chirality to discover the discriminative features for facial expression recognition. Most previous studies implicitly assume that human faces are symmetric. However, our work reveals that the facial asymmetric effect can be a crucial clue. Given a face image and its reflection without additional labels, we decouple the emotion-invariant facial features from the input image pair to better capture the emotion-related facial features. Moreover, as our model aligns emotion-related features of the image pair to enhance the recognition performance, the value of precise facial landmark alignment as a pre-processing step is reconsidered in this paper. Experiments demonstrate that the learned emotion-related features outperform the state of the art methods on several facial expression recognition benchmarks as well as real-world occlusion datasets, which manifests the effectiveness and robustness of the proposed model.
C1 [Lo, Ling; Xie, Hongxia; Cheng, Wen-Huang] Natl Yang Ming Chiao Tung Univ, Inst Elect, Hsinchu, Taiwan.
   [Shuai, Hong-Han] Natl Yang Ming Chiao Tung Univ, Dept Elect & Comp Engn, Hsinchu, Taiwan.
   [Cheng, Wen-Huang] Natl Chung Hsing Univ, Artificial Intelligence & Data Sci Program, Taichung, Taiwan.
C3 National Yang Ming Chiao Tung University; National Yang Ming Chiao Tung
   University; National Chung Hsing University
RP Cheng, WH (corresponding author), Natl Yang Ming Chiao Tung Univ, Inst Elect, Hsinchu, Taiwan.
EM linglo.ee08@nycu.edu.tw; hongxiaxie.ee08@nycu.edu.tw;
   hhshuai@nycu.edu.tw; whcheng@nycu.edu.tw
OI Lo, Ling/0000-0002-9471-8528; Shuai, Hong-Han/0000-0003-2216-077X;
   HONG-XIA, XIE/0000-0002-5652-4327
FU Ministry of Science and Technology of Taiwan
   [MOST-109-2223-E-009-002-MY3, MOST-110-2218-E-A49-018,
   MOST-111-2634-F-007-002, MOST-109-2221-E-009-114-MY3]
FX This work was supported by the Ministry of Science and Technology of
   Taiwan under Grants MOST-109-2223-E-009-002-MY3,
   MOST-110-2218-E-A49-018, MOST-111-2634-F-007-002, and
   MOST-109-2221-E-009-114-MY3. A preliminary version of this article was
   accepted by 2021 IEEE International Conference on Multimedia and Expo
   (ICME) and received the Best Paper Award [20]. The guest editors
   coordinating the review of this manuscript and approving it for
   publication are Prof. J. Liu, Prof. W. Li, Dr G.-M. Su, Prof. A. Tefas,
   Prof. Y. Wen, and Dr. C. Wang. (Corresponding author: Wen-Huang Cheng.)
CR Abnar S, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P4190
   Abousaleh FS, 2016, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-016-0151-4
   Barros P, 2020, IEEE INT CONF AUTOMA, P652, DOI 10.1109/FG47880.2020.00070
   Chen C.-Y., 2021, ICCV, P13809
   Cheng WH, 2021, ACM COMPUT SURV, V54, DOI [10.1145/3447239, 10.1145/3552468.3554360]
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Dosovitskiy Alexey, 2020, ABS201011929 CORR
   Farzaneh AH, 2021, IEEE WINT CONF APPL, P2401, DOI 10.1109/WACV48630.2021.00245
   Farzaneh AH, 2020, IEEE COMPUT SOC CONF, P1631, DOI 10.1109/CVPRW50498.2020.00211
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Hu ZH, 2022, ACM T INTEL SYST TEC, V13, DOI 10.1145/3501814
   Huang S, 2021, IEEE INT CONF COMP V, P842, DOI 10.1109/ICCVW54120.2021.00099
   Li JM, 2023, IEEE T KNOWL DATA EN, V35, P12167, DOI 10.1109/TKDE.2021.3117003
   Li S, 2022, IEEE T AFFECT COMPUT, V13, P1195, DOI 10.1109/TAFFC.2020.2981446
   Li XM, 2020, IEEE T IMAGE PROCESS, V29, P7641, DOI 10.1109/TIP.2020.3005241
   Li ZY, 2019, IEEE INT CON MULTI, P1108, DOI 10.1109/ICME.2019.00194
   Lin SS, 2021, IEEE T MULTIMEDIA, V23, P1581, DOI 10.1109/TMM.2020.3001497
   Lin Zhiqiu, 2020, P IEEECVF C COMPUTER, P12295
   Ling Lo, 2021, 2021 IEEE International Conference on Multimedia and Expo (ICME), DOI 10.1109/ICME51207.2021.9428120
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lo L, 2020, THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2020), P79, DOI 10.1109/MIPR49039.2020.00023
   Lou JW, 2020, IEEE T MULTIMEDIA, V22, P730, DOI 10.1109/TMM.2019.2933338
   Lu HH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P496, DOI 10.1145/3474085.3475198
   Lu L., 2020, PROC BRIT MACH VIS C, P1
   Lung KY, 2021, PATTERN RECOGN LETT, V144, P82, DOI 10.1016/j.patrec.2021.01.011
   Ma FY, 2023, IEEE T AFFECT COMPUT, V14, P1236, DOI 10.1109/TAFFC.2021.3122146
   Powell WR, 2009, LATERALITY, V14, P545, DOI 10.1080/13576500802680336
   Ruan DL, 2021, PROC CVPR IEEE, P7656, DOI 10.1109/CVPR46437.2021.00757
   Saber E, 1998, PATTERN RECOGN LETT, V19, P669, DOI 10.1016/S0167-8655(98)00044-0
   Sanchez-Riera J, 2016, PATTERN RECOGN LETT, V73, P1, DOI 10.1016/j.patrec.2015.12.006
   She JH, 2021, PROC CVPR IEEE, P6244, DOI 10.1109/CVPR46437.2021.00618
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Terhorst P, 2020, PROC CVPR IEEE, P5650, DOI 10.1109/CVPR42600.2020.00569
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Wen GH, 2020, IEEE T MULTIMEDIA, V22, P2914, DOI 10.1109/TMM.2020.2966858
   Wu SZ, 2020, PROC CVPR IEEE, P1, DOI 10.1109/CVPR42600.2020.00008
   Xie HX, 2023, IEEE T AFFECT COMPUT, V14, P1857, DOI 10.1109/TAFFC.2022.3143100
   Xie HX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2871, DOI 10.1145/3394171.3414012
   Xie SY, 2019, IEEE T MULTIMEDIA, V21, P211, DOI 10.1109/TMM.2018.2844085
   Xue F., 2021, P IEEE CVF INT C COM, P3581
   Yang CW, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472623
   Yang JY, 2021, PROC CVPR IEEE, P4235, DOI 10.1109/CVPR46437.2021.00422
   Zhang FF, 2022, IEEE T MULTIMEDIA, V24, P1800, DOI 10.1109/TMM.2021.3072786
   Zhang HF, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1040
   Zhang Q., 2021, Group-cam: Group score-weighted visual explanations for deep convolutional networks, V2021
   Zhang Y, 2018, PROC CVPR IEEE, P2314, DOI 10.1109/CVPR.2018.00246
   Zhao ZQ, 2021, AAAI CONF ARTIF INTE, V35, P3510
NR 50
TC 7
Z9 7
U1 4
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4275
EP 4284
DI 10.1109/TMM.2022.3197365
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J3WX2
UT WOS:001008961300001
DA 2024-07-18
ER

PT J
AU Ma, XH
   Yang, XS
   Gao, JY
   Xu, CS
AF Ma, Xinhong
   Yang, Xiaoshan
   Gao, Junyu
   Xu, Changsheng
TI The Model May Fit You: User-Generalized Cross-Modal Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data models; Task analysis; Adaptation models; Training; Benchmark
   testing; Pediatrics; Bridges; cross-modal retrieval; domain
   generalization; meta-learning
ID HASH CODES; RANK
AB In real-world applications, a cross-model retrieval model trained on multimodal instances without considering differences in data distributions among users, termed as user domain shift, usually cannot generalize well to unknown user domains. In this paper, we define a new task of user-generalized cross-modal retrieval, and propose a novel Meta-Learning Multimodal User Generalization (MLMUG) method to solve it. MLMUG simulates the user domain shift with meta-optimization, which aims to embed multimodal data effectively and generalize the cross-modal retrieval model to any unknown user domains. We design a cross-modal embedding network with a learnable meta covariant attention module to encode transferable knowledge among different user domains. A user-adaptive meta-optimization scheme is proposed to adaptively aggregate gradients and meta-gradients for fast and stable meta-optimization. We build two benchmarks for user-generalized cross-modal retrieval evaluation. Experiments on the proposed benchmarks validate the generalization of our method compared with several state-of-the-art methods.
C1 [Ma, Xinhong; Yang, Xiaoshan; Gao, Junyu; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Ma, Xinhong; Yang, Xiaoshan; Gao, Junyu; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Yang, Xiaoshan; Xu, Changsheng] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Peng Cheng Laboratory
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Xu, CS (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.; Xu, CS (corresponding author), Peng Cheng Lab, Shenzhen 518066, Peoples R China.
EM xinhong.ma@nlpr.ia.ac.cn; xiaoshan.yang@nlpr.ia.ac.cn;
   gaojunyu2015@ia.ac.cn; csxu@nlpr.ia.ac.cn
RI Gao, Junyu/HDO-5516-2022; xu, cj/HJZ-3488-2023; Xinhong,
   Ma/ACH-5160-2022; Xu, Chang/GQP-7280-2022
OI xu, chang sheng/0000-0001-8343-9665; Ma, Xinhong/0000-0003-1200-2268;
   Yang, Xiaoshan/0000-0001-5453-9755
FU National Key Research and Development Plan of China [2020AAA0106200];
   National Natural Science Foundation of China [62036012, 61721004,
   62072286, 61720106006, 61832002, 62072455, 62002355, U1836220,
   U1705262]; Key Research Program of Frontier Sciences of CAS
   [QYZDJSSWJSC039]; Beijing Natural Science Foundation [L201001]
FX This work was supported in part by the National Key Research and
   Development Plan of China under Grant 2020AAA0106200, in part by the
   National Natural Science Foundation of China under Grants 62036012,
   61721004, 62072286, 61720106006, 61832002, 62072455, 62002355, U1836220,
   and U1705262, in part by the Key Research Program of Frontier Sciences
   of CAS under Grant QYZDJSSWJSC039, and in part by the Beijing Natural
   Science Foundation under Grant L201001.
CR [Anonymous], 2012, P INT C NEUR INF PRO
   [Anonymous], 2011, P ICML
   [Anonymous], 2016, ARXIV160206697
   Bai B, 2010, INFORM RETRIEVAL, V13, P291, DOI 10.1007/s10791-009-9117-9
   Balaji Y., 2018, Advances in Neural Information Processing Systems, P998
   Blanchard P., 2017, Advances in Neural Information Processing Systems, P118
   Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928
   Cao Z., 2017, ARXIV171204616
   Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233
   Carmel D., 2009, P CIKM, P1227
   Chen TL, 2020, ANN OPER RES, V290, P813, DOI 10.1007/s10479-018-2969-x
   Da Li, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12535), P603, DOI 10.1007/978-3-030-66415-2_39
   Damaskinos Georgios, 2019, C SYST MACH LEARN SY
   Defossez A., 2017, ARXIV171101761
   Deng C, 2016, IEEE T MULTIMEDIA, V18, P208, DOI 10.1109/TMM.2015.2508146
   Diao H., 2021, ARXIV210101368
   Dimitriadis D., 2020, ARXIV200802452
   Dou Q, 2019, ADV NEUR IN, V32
   Du Y., 2021, PROC INT C LEARN REP, P1
   Faghri Fartash, 2017, ARXIV170705612
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Finn C, 2017, PR MACH LEARN RES, V70
   Gao JY, 2022, IEEE T CIRC SYST VID, V32, P1646, DOI 10.1109/TCSVT.2021.3075470
   Gao JY, 2021, IEEE T PATTERN ANAL, V43, P3476, DOI 10.1109/TPAMI.2020.2985708
   Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459
   Grangier D, 2008, IEEE T PATTERN ANAL, V30, P1371, DOI 10.1109/TPAMI.2007.70791
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Guo JZ, 2020, PROC CVPR IEEE, P6162, DOI 10.1109/CVPR42600.2020.00620
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hidayati SC, 2021, IEEE T MULTIMEDIA, V23, P365, DOI 10.1109/TMM.2020.2980195
   Hsieh CW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P275, DOI 10.1145/3343031.3351075
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Ji JL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2614
   Ji Z, 2022, IEEE T CYBERNETICS, V52, P1086, DOI 10.1109/TCYB.2020.2985716
   Jia XW, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2735, DOI 10.1145/3394486.3403324
   Jiang JY, 2020, CAAI T INTELL TECHNO, V5, P230, DOI 10.1049/trit.2020.0082
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Khosla A, 2012, LECT NOTES COMPUT SC, V7572, P158, DOI 10.1007/978-3-642-33718-5_12
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li D, 2018, AAAI CONF ARTIF INTE, P3490
   Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li YJ, 2019, PR MACH LEARN RES, V97
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liu R., 2021, ARXIV210207976
   Liu XL, 2017, IEEE T IMAGE PROCESS, V26, P5324, DOI 10.1109/TIP.2017.2729896
   Liu XL, 2016, IEEE T CYBERNETICS, V46, P2252, DOI 10.1109/TCYB.2015.2474742
   Liu XL, 2016, IEEE T IMAGE PROCESS, V25, P4514, DOI 10.1109/TIP.2016.2593344
   Liu XL, 2016, IEEE T IMAGE PROCESS, V25, P907, DOI 10.1109/TIP.2015.2505180
   Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264
   Lu YJ, 2011, MULTIMED TOOLS APPL, V51, P247, DOI 10.1007/s11042-010-0621-0
   Makwana K., 2017, P INT C INF COMM TEC, P120
   Mancini M., 2020, P EUR C COMP VIS, P466, DOI DOI 10.1007/978-3-030-58592
   Mandal D, 2017, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR.2017.282
   Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609
   Muandet Krikamol, 2013, INT C MACH LEARN, P10
   Murrugarra-Llerena N, 2019, PROC CVPR IEEE, P6422, DOI 10.1109/CVPR.2019.00659
   Nichol A., 2018, ARXIV180302999
   Park CC, 2019, IEEE T PATTERN ANAL, V41, P999, DOI 10.1109/TPAMI.2018.2824816
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Qiao F., 2021, CVPR, P6790
   Qiao F., P IEEE C COMP VIS PA, P12556
   Qiao SY, 2018, PROC CVPR IEEE, P7229, DOI 10.1109/CVPR.2018.00755
   Rajeswaran A, 2019, ADV NEUR IN, V32
   Ramakrishnan R., 2020, Domain Adaptation in Computer Vision with Deep Learning, P57, DOI DOI 10.1007/978-3-030-45529-3_4
   Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466
   Reisizadeh A., 2019, ARXIV190201981
   Ruder S., 2016, ARXIV
   Scardovi L., 2009, Proc ROBOCOMM, P1
   Shankar S., 2018, ARXIV180410745
   Shao R, 2019, PROC CVPR IEEE, P10015, DOI 10.1109/CVPR.2019.01026
   Shuqing L., 2009, INF STUDIES THEORY A, V5, P107
   Snell J, 2017, ADV NEUR IN, V30
   Song G, 2021, IEEE T MULTIMEDIA, V23, P1708, DOI 10.1109/TMM.2020.3002177
   Song J., 2011, P 19 ACM INT C MULT, P423, DOI DOI 10.1145/2072298.2072354
   Song Y, 2019, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2019.00208
   Sun QR, 2019, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.2019.00049
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Torbati GH, 2020, CHIIR'20: PROCEEDINGS OF THE 2020 CONFERENCE ON HUMAN INFORMATION INTERACTION AND RETRIEVAL, P427, DOI 10.1145/3343413.3378011
   TSITSIKLIS JN, 1986, IEEE T AUTOMAT CONTR, V31, P803, DOI 10.1109/TAC.1986.1104412
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vanschoren J., 2018, ARXIV181003548
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Volpi R, 2018, ADV NEUR IN, V31
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976
   Wang J, 2013, IEEE I CONF COMP VIS, P3032, DOI 10.1109/ICCV.2013.377
   Wang K., 2016, A comprehensive survey on cross-modal retrieval
   Wang KY, 2016, IEEE T PATTERN ANAL, V38, P2010, DOI 10.1109/TPAMI.2015.2505311
   Wang S., 2018, P INT C NEUR INF PRO, P4243
   Wehrmann J, 2019, IEEE I CONF COMP VIS, P5803, DOI 10.1109/ICCV.2019.00590
   Wu B, 2016, AAAI CONF ARTIF INTE, P272
   Wu B, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1336, DOI 10.1145/2964284.2964335
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Wu GS, 2019, IEEE T IND ELECTRON, V66, P9868, DOI 10.1109/TIE.2018.2873547
   Wu YL, 2021, IEEE T MULTIMEDIA, V23, P559, DOI 10.1109/TMM.2020.2985540
   Xie HX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2871, DOI 10.1145/3394171.3414012
   Xu XS, 2019, INT J PROD RES, V57, P4594, DOI 10.1080/00207543.2019.1579935
   Yingjun Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P200, DOI 10.1007/978-3-030-58607-2_12
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yu M., 2018, Proc. of Neural Information Processing Systems, V31, P5123
   Zhang D, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P225
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang L, 2017, IEEE T MULTIMEDIA, V19, P1220, DOI 10.1109/TMM.2016.2646219
   Zhen LL, 2019, PROC CVPR IEEE, P10386, DOI 10.1109/CVPR.2019.01064
   Zhu XF, 2017, IEEE T MULTIMEDIA, V19, P2033, DOI 10.1109/TMM.2017.2703636
NR 113
TC 3
Z9 3
U1 4
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2998
EP 3012
DI 10.1109/TMM.2021.3091888
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000025
DA 2024-07-18
ER

PT J
AU Tang, SG
   Guo, D
   Hong, RC
   Wang, M
AF Tang, Shengeng
   Guo, Dan
   Hong, Richang
   Wang, Meng
TI Graph-Based Multimodal Sequential Embedding for Sign Language
   Translation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Continuous sign language translation; graph convolutional network;
   multimodal sequential embedding; multimodal sequential fusion
ID RECOGNITION; FRAMEWORK
AB Sign language translation (SLT) is a challenging weakly supervised task without word-level annotations. An effective method of SLT is to leverage multimodal complementarity and to explore implicit temporal cues. In this work, we propose a graph-based multimodal sequential embedding network (MSeqGraph), in which multiple sequential modalities are densely correlated. Specifically, we build a graph structure to realize the intra-modal and inter-modal correlations. First, we design a graph embedding unit (GEU), which embeds a parallel convolution with channel-wise and temporal-wise learning into the graph convolution to learn the temporal cues in each modal sequence and cross-modal complementarity. Then, a hierarchical GEU stacker with a pooling-based skip connection is proposed. Unlike the state-of-the-art methods, to obtain a compact and informative representation of multimodal sequences, the GEU stacker gradually compresses the channel d with multi-modalities m rather than the temporal dimension t. Finally, we adopt the connectionist temporal decoding strategy to explore the entire video's temporal transition and translate the sentence. Extensive experiments on the USTC-CSL and BOSTON-104 datasets demonstrate the effectiveness of the proposed method.
C1 [Tang, Shengeng; Guo, Dan; Hong, Richang; Wang, Meng] Minist Educ, Key Lab Knowledge Engn Big Data HFUT, Hefei 230601, Peoples R China.
   [Tang, Shengeng; Guo, Dan; Hong, Richang; Wang, Meng] Intelligent Interconnected Syst Lab Anhui Prov HF, Hefei 230601, Peoples R China.
   [Tang, Shengeng; Guo, Dan; Hong, Richang; Wang, Meng] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230601, Peoples R China.
C3 Hefei University of Technology
RP Guo, D; Hong, RC (corresponding author), Minist Educ, Key Lab Knowledge Engn Big Data HFUT, Hefei 230601, Peoples R China.; Guo, D; Hong, RC (corresponding author), Intelligent Interconnected Syst Lab Anhui Prov HF, Hefei 230601, Peoples R China.
EM tsg1995@mail.hfut.edu.cn; dan@hfut.edu.cn; hongrc.hfut@gmail.com;
   eric.mengwang@gmail.com
RI Tang, Shengeng/AEW-8098-2022; Wang, Meng/ITR-8699-2023
OI Tang, Shengeng/0000-0001-6313-2543; Guo, Dan/0000-0003-2594-254X
FU National Natural Science Foundation of China (NSFC) [61876058, 61932009,
   U20A20183, 62020106007]; Fundamental Research Funds for the Central
   Universities [JZ2020HGTB0020]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grants 61876058, 61932009, U20A20183,
   and 62020106007 and in part by Fundamental Research Funds for the
   Central Universities under Grant JZ2020HGTB0020.
CR Beck D, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P273
   Camgoz Necati Cihan, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12538), P301, DOI 10.1007/978-3-030-66823-5_18
   Camgoz Necati Cihan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10020, DOI 10.1109/CVPR42600.2020.01004
   Camgoz NC, 2018, PROC CVPR IEEE, P7784, DOI 10.1109/CVPR.2018.00812
   Chen X, 2014, INT C PATT RECOG, P411, DOI 10.1109/ICPR.2014.79
   Cui RP, 2019, IEEE T MULTIMEDIA, V21, P1880, DOI 10.1109/TMM.2018.2889563
   Cui RP, 2017, PROC CVPR IEEE, P1610, DOI 10.1109/CVPR.2017.175
   de Amorim CC, 2019, LECT NOTES COMPUT SC, V11731, P646, DOI 10.1007/978-3-030-30493-5_59
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Dreuw P., 2008, PROC ITG C VOICE COM, P1
   Dreuw P, 2008, SIXTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, LREC 2008, P1115
   Dreuw P, 2007, INTERSPEECH 2007: 8TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION, VOLS 1-4, P705
   Dreuw P, 2008, IEEE INT CONF AUTOMA, P736
   Dreuw P, 2009, LECT NOTES COMPUT SC, V5524, P24, DOI 10.1007/978-3-642-02172-5_5
   Du Y, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P579, DOI 10.1109/ACPR.2015.7486569
   Du Y, 2016, IEEE T IMAGE PROCESS, V25, P3010, DOI 10.1109/TIP.2016.2552404
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Fan WQ, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P417, DOI 10.1145/3308558.3313488
   Graves  A., 2006, P 23 INT C MACH LEAR, P369, DOI DOI 10.1145/1143844.1143891
   Guo D, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P751
   Guo D, 2020, IEEE T IMAGE PROCESS, V29, P1575, DOI 10.1109/TIP.2019.2941267
   Guo D, 2018, AAAI CONF ARTIF INTE, P6845
   Guo D, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152121
   Hao YB, 2020, IEEE T MULTIMEDIA, V22, P188, DOI 10.1109/TMM.2019.2923121
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hou JC, 2018, IEEE TETCI, V2, P117, DOI 10.1109/TETCI.2017.2784878
   Hua XY, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P591
   Huang J, 2018, AAAI CONF ARTIF INTE, P2257
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Ji JZ, 2020, IEEE T IMAGE PROCESS, V29, P7615, DOI 10.1109/TIP.2020.3004729
   Ka Leong Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P697, DOI 10.1007/978-3-030-58586-0_41
   Kampman O, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P606
   Ke QH, 2018, IEEE T IMAGE PROCESS, V27, P2842, DOI 10.1109/TIP.2018.2812099
   Kingma D. P., 2015, INT C LEARNING REPRE
   Kipf T. N., 2017, 8 INT C LEARN REPR, P1
   Koller O, 2020, IEEE T PATTERN ANAL, V42, P2306, DOI 10.1109/TPAMI.2019.2911077
   Koller O, 2018, INT J COMPUT VISION, V126, P1311, DOI 10.1007/s11263-018-1121-3
   Koller O, 2017, PROC CVPR IEEE, P3416, DOI 10.1109/CVPR.2017.364
   Li D., 2020, NeurIPS, V33, P12034
   Li HB, 2020, INT CONF ACOUST SPEE, P2348, DOI [10.1109/icassp40776.2020.9054316, 10.1109/ICASSP40776.2020.9054316]
   Li MS, 2020, PROC CVPR IEEE, P211, DOI 10.1109/CVPR42600.2020.00029
   Liang XD, 2016, LECT NOTES COMPUT SC, V9905, P125, DOI 10.1007/978-3-319-46448-0_8
   Liu AA, 2019, IEEE T IMAGE PROCESS, V28, P853, DOI 10.1109/TIP.2018.2872879
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539
   Pan PB, 2016, PROC CVPR IEEE, P1029, DOI 10.1109/CVPR.2016.117
   Pérez-Rúa JM, 2019, PROC CVPR IEEE, P6959, DOI 10.1109/CVPR.2019.00713
   Pu JF, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P885
   Song PP, 2019, IEEE IMAGE PROC, P1915, DOI [10.1109/icip.2019.8803123, 10.1109/ICIP.2019.8803123]
   Sun L, 2019, IEEE INT CON MULTI, P1300, DOI 10.1109/ICME.2019.00226
   Tung Phan-Minh, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14062, DOI 10.1109/CVPR42600.2020.01408
   Tunga Anirudh, 2020, PROC IEEECVFWINTER C, P31
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wan J, 2016, IEEE T PATTERN ANAL, V38, P1626, DOI 10.1109/TPAMI.2015.2513479
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang CC, 2020, INT CONF ACOUST SPEE, P1568, DOI [10.1109/icassp40776.2020.9054076, 10.1109/ICASSP40776.2020.9054076]
   Wang HJ, 2019, IEEE T MULTIMEDIA, V21, P2806, DOI 10.1109/TMM.2019.2915032
   Wang HH, 2017, IEEE INT CON MULTI, P949, DOI 10.1109/ICME.2017.8019301
   Wang LC, 2014, IEEE T MULTIMEDIA, V16, P751, DOI 10.1109/TMM.2014.2298382
   Wang S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1483, DOI 10.1145/3240508.3240671
   Wei CC, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P435, DOI [10.1109/BigMM.2019.00075, 10.1109/BigMM.2019.00027]
   Xie R., 2020, PROC IEEE C COMPUT V, p13 686
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang ZW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1877, DOI 10.1145/3123266.3127904
   Yin K., 2020, COLING, P5975, DOI 10.18653/v1/2020.coling-main.525
   Yin YJ, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P3025
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Zadeh A., 2017, P 2017 C EMPIRICAL M, P1114
   Zhang L., 2020, ACM T INTEL SYST TEC, V11, P1
   Zhe Niu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P172, DOI 10.1007/978-3-030-58517-4_11
   Zhu WT, 2016, AAAI CONF ARTIF INTE, P3697
NR 74
TC 18
Z9 18
U1 2
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4433
EP 4445
DI 10.1109/TMM.2021.3117124
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 6A4RT
UT WOS:000880645000005
DA 2024-07-18
ER

PT J
AU Yao, ZJ
   Wang, LP
AF Yao, Zhaojian
   Wang, Luping
TI Boundary Information Progressive Guidance Network for Salient Object
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Codes; Semantics; Object detection; Benchmark testing;
   Feature extraction; Saliency detection; Salient object detection;
   convolutional neural networks; saliency detection unit; boundary
   information guidance
ID REGION
AB In recent years, the use of boundary information in saliency detection has been receiving increasing attention. In some cases, existing methods can output saliency maps with clear object boundaries by learning boundary information. However, their boundary prediction structures are generally separated from the prediction branches of the salient regions, and the resulting boundary features may not match the salient objects. We propose a simple saliency detection unit (SDU) to learn more accurate boundary features, and apply multiple such units to construct a boundary information progressive guidance network (BIPGNet). The SDU cascades the salient region and boundary detections, where the boundary features are directly extracted from the salient regions. In the BIPGNet, semantic and boundary features are progressively merged to produce complementary features. We use the complementary features of each stage in one SDU for detecting the salient objects. In addition, a novel boundary information guidance (BIG) module is designed that focuses on the boundary information in a feature layer. We apply multiple BIG modules to the complementary features at different stages. The quality of output saliency map is improved by modifying the complementary features. Experimental results demonstrate that our method can achieve better performance on five benchmark datasets, consistently surpassing 15 state-of-the-art methods. Our source code is publicly available at https://github.com/CKYiu/BIPG.
C1 [Yao, Zhaojian; Wang, Luping] Sun Yat Sen Univ, Sch Elect & Commun Engn, Guangzhou 510006, Peoples R China.
C3 Sun Yat Sen University
RP Wang, LP (corresponding author), Sun Yat Sen Univ, Sch Elect & Commun Engn, Guangzhou 510006, Peoples R China.
EM yaozhj6@mail2.sysu.edu.cn; wanglp27@mail.sysu.edu.cn
OI Wang, Luping/0000-0003-4119-4799; Yao, Zhaojian/0000-0001-5259-5286
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2011, ADV NEURAL INF PROCE
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9
   Chen ZY, 2020, AAAI CONF ARTIF INTE, V34, P10599
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Deng C, 2020, IEEE T MULTIMEDIA, V22, P885, DOI 10.1109/TMM.2019.2934833
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Fu KR, 2019, IEEE T MULTIMEDIA, V21, P457, DOI 10.1109/TMM.2018.2859746
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Guolei Sun, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P347, DOI 10.1007/978-3-030-58536-5_21
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He T, 2019, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2019.00065
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hu XW, 2018, AAAI CONF ARTIF INTE, P6943
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Jiang P, 2013, IEEE I CONF COMP VIS, P1976, DOI 10.1109/ICCV.2013.248
   Jun Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13022, DOI 10.1109/CVPR42600.2020.01304
   Lei JJ, 2016, IEEE T MULTIMEDIA, V18, P1783, DOI 10.1109/TMM.2016.2592325
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lin X, 2019, IEEE T MULTIMEDIA, V21, P1646, DOI 10.1109/TMM.2018.2884474
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Paszke A, 2019, ADV NEUR IN, V32
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Shang-Hua Gao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P702, DOI 10.1007/978-3-030-58539-6_42
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su JM, 2019, IEEE I CONF COMP VIS, P3798, DOI 10.1109/ICCV.2019.00390
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P220, DOI 10.1109/TPAMI.2019.2924417
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wang WG, 2019, PROC CVPR IEEE, P5961, DOI 10.1109/CVPR.2019.00612
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang WG, 2018, PROC CVPR IEEE, P1711, DOI 10.1109/CVPR.2018.00184
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Wu RM, 2019, PROC CVPR IEEE, P8142, DOI 10.1109/CVPR.2019.00834
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1109/CSTIC.2018.8369274, 10.1007/s11263-019-01198-w]
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xiao HX, 2018, IEEE T MULTIMEDIA, V20, P3239, DOI 10.1109/TMM.2018.2830098
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xie YL, 2013, IEEE T IMAGE PROCESS, V22, P1689, DOI 10.1109/TIP.2012.2216276
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang S, 2020, IEEE T MULTIMEDIA, V22, P2163, DOI 10.1109/TMM.2019.2947352
   Yao ZJ, 2021, NEUROCOMPUTING, V448, P152, DOI 10.1016/j.neucom.2021.03.094
   Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao X., 2020, P EUR C COMP VIS, P35, DOI 10.1007/ 978-3-030-58536-5_3
   Zhou H., 2020, P IEEE C COMP VIS PA, P9141, DOI 10.1109/CVPR42600.2020.00916
   Zhou Y, 2019, IEEE T MULTIMEDIA, V21, P74, DOI 10.1109/TMM.2018.2845667
NR 68
TC 7
Z9 8
U1 11
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4236
EP 4249
DI 10.1109/TMM.2021.3115344
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 4K8TP
UT WOS:000852215000007
DA 2024-07-18
ER

PT J
AU Yin, JL
   Chen, BH
   Peng, YT
AF Yin, Jia-Li
   Chen, Bo-Hao
   Peng, Yan-Tsung
TI Two Exposure Fusion Using Prior-Aware Generative Adversarial Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Decoding; Generative adversarial networks; Dynamic range;
   Quantization (signal); Calibration; Image fusion; High dynamic range
   image; exposure fusion; deep learning
ID QUALITY ASSESSMENT
AB Producing a high dynamic range (HDR) image from two low dynamic range (LDR) images with extreme exposures is challenging due to the lack of well-exposed contents. Existing works either use pixel fusion based on weighted quantization or conduct feature fusion using deep learning techniques. In contrast to these methods, our core idea is to progressively incorporate the pixel domain knowledge of LDR images into the feature fusion process. Specifically, we propose a novel Prior-Aware Generative Adversarial Network (PA-GAN), along with a new dual-level loss for two exposure fusion. The proposed PA-GAN is composed of a content-prior-guided encoder and a detail-prior-guided decoder, respectively in charge of content fusion and detail calibration. We further train the network using a dual-level loss that combines the semantic-level loss and pixel-level loss. Extensive qualitative and quantitative evaluations on diverse image datasets demonstrate that our proposed PA-GAN has superior performance than state-of-the-art methods.
C1 [Yin, Jia-Li] Fuzhou Univ, Coll Math & Comp Sci, Fuzhou 350108, Peoples R China.
   [Chen, Bo-Hao] Yuan Ze Univ, Dept Comp Sci & Engn, Taoyuan 32003, Taiwan.
   [Peng, Yan-Tsung] Natl Chengchi Univ, Dept Comp Sci, Taipei 116, Taiwan.
C3 Fuzhou University; Yuan Ze University; National Chengchi University
RP Chen, BH (corresponding author), Yuan Ze Univ, Dept Comp Sci & Engn, Taoyuan 32003, Taiwan.
EM jlyin@fzu.edu.cn; bhchen@saturn.yzu.edu.tw; ytpeng@cs.nccu.edu.tw
RI Peng, Yan-Tsung/AGW-3513-2022
OI Peng, Yan-Tsung/0000-0002-3802-1670
FU Fujian Provincial Youth Education, and Scientific Research Project
   [JAT200055]; Ministry of Science and Technology, Taiwan [MOST
   108-2221-E-155-034MY3, MOST 107-2221-E-155-052-MY2, MOST
   109-2634-F-019-001, MOST 109-2634-F-004-001, MOST 109-2622-E-004-002,
   MOST 109-2221-E004-014, MOST 110-2221-E-004-010]
FX This work was supported in part by Fujian Provincial Youth Education,
   and Scientific Research Project under Grant JAT200055 and in part by
   Ministry of Science and Technology, Taiwan, under Grants MOST
   108-2221-E-155-034MY3, MOST 107-2221-E-155-052-MY2, MOST
   109-2634-F-019-001, MOST 109-2634-F-004-001, MOST 109-2622-E-004-002,
   and MOST 109-2221-E004-014, and MOST 110-2221-E-004-010.
CR Banterle F, 2009, COMPUT GRAPH FORUM, V28, P2343, DOI 10.1111/j.1467-8659.2009.01541.x
   Burt P. J., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P173, DOI 10.1109/ICCV.1993.378222
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Chen YY, 2019, IEEE IMAGE PROC, P3502, DOI [10.1109/icip.2019.8803656, 10.1109/ICIP.2019.8803656]
   Cheng Ma, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7766, DOI 10.1109/CVPR42600.2020.00779
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Funt B., 2010, PROC SPIE HUM VIS EL, V7527, P282
   Funt B, 2010, COLOR IMAG CONF, P256
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352
   Hessel C, 2020, IEEE WINT CONF APPL, P137, DOI 10.1109/WACV45572.2020.9093643
   Ignatov A, 2017, IEEE I CONF COMP VIS, P3297, DOI 10.1109/ICCV.2017.355
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim JH, 2021, AAAI CONF ARTIF INTE, V35, P1780
   Kou F, 2018, IEEE T MULTIMEDIA, V20, P484, DOI 10.1109/TMM.2017.2743988
   Kou F, 2017, IEEE INT CON MULTI, P1105, DOI 10.1109/ICME.2017.8019529
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee S, 2020, INDIAN J LABOUR ECON, V63, P11, DOI 10.1007/s41027-020-00249-y
   Lee S, 2018, LECT NOTES COMPUT SC, V11206, P613, DOI 10.1007/978-3-030-01216-8_37
   Li ZG, 2017, IEEE T IMAGE PROCESS, V26, P1243, DOI 10.1109/TIP.2017.2651366
   Li ZG, 2014, IEEE T IND ELECTRON, V61, P7076, DOI 10.1109/TIE.2014.2314066
   Liu YL, 2020, PROC CVPR IEEE, P1648, DOI 10.1109/CVPR42600.2020.00172
   Ma KD, 2020, IEEE T IMAGE PROCESS, V29, P2808, DOI 10.1109/TIP.2019.2952716
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Mertens T, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P382, DOI 10.1109/PG.2007.17
   Moriwaki K., 2018, ARXIV181207134
   Prabhakar K. R., 2019, 2019 IEEE INT C COMP, P1
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Rad MS, 2019, IEEE I CONF COMP VIS, P2710, DOI 10.1109/ICCV.2019.00280
   Rana A, 2020, IEEE T IMAGE PROCESS, V29, P1285, DOI 10.1109/TIP.2019.2936649
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Wang TH, 2015, IEEE T MULTIMEDIA, V17, P470, DOI 10.1109/TMM.2015.2403612
   Yan QS, 2019, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2019.00185
   Yang Y, 2018, IEEE SIGNAL PROC LET, V25, P1885, DOI 10.1109/LSP.2018.2877893
   Yeganeh H, 2013, IEEE T IMAGE PROCESS, V22, P657, DOI 10.1109/TIP.2012.2221725
   Yin JL, 2021, IEEE T MULTIMEDIA, V23, P1049, DOI 10.1109/TMM.2020.2992962
   Yin JF, 2021, PLANT SOIL, V459, P249, DOI 10.1007/s11104-020-04756-1
   Zhang W, 2012, IEEE T IMAGE PROCESS, V21, P2318, DOI 10.1109/TIP.2011.2170079
NR 43
TC 6
Z9 7
U1 1
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2841
EP 2851
DI 10.1109/TMM.2021.3089324
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000012
DA 2024-07-18
ER

PT J
AU Zhang, L
   Xu, JS
   Gong, YS
   Yu, LT
   Zhang, J
   Shen, JL
AF Zhang, Lu
   Xu, Jingsong
   Gong, Yongshun
   Yu, Litao
   Zhang, Jian
   Shen, Jialie
TI Unsupervised Image and Text Fusion for Travel Information Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Kernel; Semantics; Multimedia Web sites; Correlation; Fuses; Social
   networking (online); Feature extraction; Information enhancement;
   feature engineering; multiple kernel learning; unsupervised learning
ID CANONICAL CORRELATION-ANALYSIS
AB With the explosive growth of the shared information on social media platforms, people are increasingly interested in sharing and making their travel plans by referring to others' travel experiences. However, different social media sources render the heterogeneity of these valuable data, bringing difficulties for data collection and fusion. Thus, facing massive information online, one of the biggest challenges to enhance travel information is how to integrate and match these multi-source data without clear labels. In this paper, we propose an unsupervised method to fuse and match images and travelogues. We first use the three textual components (title, tag, and description) of the descriptive texts of images as three criteria to embed travelogues and the descriptive texts of images, and further introduce images into our method by joint embedding texts and images. Finally, a multiple kernel clustering approach is adopted for matching travelogues and images. Extensive experiments conducted on the real dataset crawled from two websites (Flickr and TripAdvisor) demonstrate the effectiveness and robustness of our proposed method.
C1 [Zhang, Lu; Xu, Jingsong; Yu, Litao; Zhang, Jian] Univ Technol Sydney, Global Big Data Technol Ctr, Sydney, NSW 2007, Australia.
   [Gong, Yongshun] Shandong Univ, Sch Software, Jinan, Peoples R China.
   [Shen, Jialie] Queens Univ Belfast, Sch Elect Elect Engn & Comp Sci, Belfast BT7 1NN, Antrim, North Ireland.
C3 University of Technology Sydney; Shandong University; Queens University
   Belfast
RP Zhang, J (corresponding author), Univ Technol Sydney, Global Big Data Technol Ctr, Sydney, NSW 2007, Australia.
EM lu.zhang-5@student.uts.edu.au; jingsong.xu@uts.edu.au;
   yongshun.gong@student.uts.edu.au; litao.yu@uts.edu.au;
   Jian.Zhang@uts.edu.au; jialie@gmail.com
RI Gong, Yongshun/HKW-1980-2023
OI Zhang, Jian/0000-0002-7240-3541; Gong, Yongshun/0000-0003-3948-4471;
   Zhang, Lu/0000-0002-2225-9772
CR [Anonymous], 2010, P INT WORKSH SEARCH
   [Anonymous], 2014, ABS14053531 CORR
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   Chechik G, 2010, J MACH LEARN RES, V11, P1109
   Chen FH, 2018, IEEE T MULTIMEDIA, V20, P997, DOI 10.1109/TMM.2017.2757769
   Chen ZW, 2018, IEEE T IND ELECTRON, V65, P1559, DOI 10.1109/TIE.2017.2733501
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Chun IY, 2018, IEEE T IMAGE PROCESS, V27, P1697, DOI 10.1109/TIP.2017.2761545
   Dumais ST, 2004, ANNU REV INFORM SCI, V38, P189
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Gupta A, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P3577, DOI 10.1145/3394486.3406485
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Hui PY, 2014, IEEE-ACM T AUDIO SPE, V22, P417, DOI 10.1109/TASLP.2013.2294586
   Jhamtani H, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P217, DOI 10.1145/2964284.2967214
   Jiang YG, 2018, IEEE T MULTIMEDIA, V20, P3137, DOI 10.1109/TMM.2018.2823900
   Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Lee M. D., 2005, P ANN M COGN SCI SOC, V27
   Li ZB, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2848, DOI 10.1145/3292500.3330731
   Liu XW, 2017, AAAI CONF ARTIF INTE, P2259
   Mao J., 2016, P 30 INT C NEURAL IN, P442
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Ntalianis K, 2015, IEEE IMAGE PROC, P222, DOI 10.1109/ICIP.2015.7350792
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Putthividhya D, 2010, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2010.5540000
   Rehurek R., 2010, P LREC 2010 WORKSH N, P45, DOI DOI 10.13140/2.1.2393.1847
   Ren Z., 2016, ACM Multimedia
   Sharma A, 2011, PROC CVPR IEEE, P593, DOI 10.1109/CVPR.2011.5995350
   Simonyan K., 2014, CORR
   Sun LA, 2011, IEEE T PATTERN ANAL, V33, P194, DOI 10.1109/TPAMI.2010.160
   Turtle Howard, 2017, ACM SIGIR Forum, V51, P124, DOI 10.1145/3130348.3130361
   Ustinova E, 2016, ADV NEUR IN, V29
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Yahav I, 2019, IEEE T KNOWL DATA EN, V31, P437, DOI 10.1109/TKDE.2018.2840127
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Yao JC, 2018, IEEE T MULTIMEDIA, V20, P224, DOI 10.1109/TMM.2017.2716829
   Zhang L., 2018, Digital Image Computing: Techniques and Applications (DICTA), P1
   Zhang L, 2018, IEEE T MULTIMEDIA, V20, P128, DOI 10.1109/TMM.2017.2723841
NR 39
TC 2
Z9 2
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1415
EP 1425
DI 10.1109/TMM.2021.3064408
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200014
DA 2024-07-18
ER

PT J
AU Robinson, JP
   Khan, Z
   Yin, Y
   Shao, M
   Fu, Y
AF Robinson, Joseph P.
   Khan, Zaid
   Yin, Yu
   Shao, Ming
   Fu, Yun
TI Families in Wild Multimedia: A Multimodal Database for Recognizing
   Kinship
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face recognition; Visualization; Task analysis; Media; Streaming media;
   Support vector machines; Speech recognition; Kinship verification; face
   recognition; talking faces; visual information; audio; multimodal;
   feature fusion; deep learning; template adaptation; biometrics;
   multi-task; support vector machines; large-scale; dataset; convolutional
   neural network
ID VERIFICATION
AB Kinship, a soft biometric detectable in media, is fundamental for a myriad of use-cases. Despite the difficulty of detecting kinship, annual data challenges using still-images have consistently improved performances and attracted new researchers. Now, systems reach performance levels unforeseeable a decade ago, closing in on performances acceptable to deploy in practice. Like other biometric tasks, we expect systems can receive help from other modalities. We hypothesize that adding modalities to Families In the Wild (FIW), which has only still-images, will improve performance. Thus, to narrow the gap between research and reality and enhance the power of kinship recognition systems, we extend FIW with multimedia (MM) data (i.e., video, audio, and text captions). Specifically, we introduce the first publicly available multi-task MM kinship dataset. To build FIW in Multimedia (FIW MM), we developed machinery to automatically collect, annotate, and prepare the data, requiring minimal human input and no financial cost. The proposed MM corpus allows the problem statements to be more realistic template-based protocols. We show significant improvements in all benchmarks with the added modalities. The results highlight edge cases to inspire future research with different areas of improvement. FIW MM supplies the data needed to increase the potential of automated systems to detect kinship in MM. It also allows experts from diverse fields to collaborate in novel ways.
C1 [Robinson, Joseph P.; Khan, Zaid; Yin, Yu; Fu, Yun] Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA.
   [Shao, Ming] Univ Massachusetts, Dartmouth, MA 02747 USA.
C3 Northeastern University; University of Massachusetts System; University
   Massachusetts Dartmouth
RP Robinson, JP (corresponding author), Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA.
EM robinson.jo@northeastern.edu; khan.za@northeastern.edu;
   yin.yu1@northeastern.edu; mshao@umassd.edu; yunfu@ece.neu.edu
RI Robinson, Joseph/Y-9207-2019
OI Robinson, Joseph/0000-0001-7699-2104; Shao, Ming/0000-0002-7686-8784;
   Fu, Yun/0000-0002-5098-2853
CR Afouras T, 2018, INTERSPEECH, P3514, DOI 10.21437/Interspeech.2018-1943
   Afouras T, 2018, INTERSPEECH, P3244
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Albanie S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P292, DOI 10.1145/3240508.3240578
   [Anonymous], 2011, Biometrics (IJCB), 2011 International Joint Conference on
   [Anonymous], 2016, ARXIV160207360
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chung J. S., 2017, P BRIT MACH VIS C
   Chung J. S, 2017, PROC BRIT MACH VIS C
   Chung JS, 2020, INTERSPEECH, P2977, DOI 10.21437/Interspeech.2020-1064
   Chung JS, 2018, INTERSPEECH, P1086
   Chung JS, 2017, LECT NOTES COMPUT SC, V10117, P251, DOI 10.1007/978-3-319-54427-4_19
   Chung JS, 2017, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2017.367
   Chung JS, 2017, LECT NOTES COMPUT SC, V10112, P87, DOI 10.1007/978-3-319-54184-6_6
   DeCann B, 2013, 2013 IEEE SIXTH INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS (BTAS)
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Duan Qingyan, 2017, P ACM MM WORKSH RFIW, P21, DOI [DOI 10.1145/3134421.3134422, 10.1145/3134421.3134422]
   Ephrat A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201357
   Fang RG, 2013, IEEE IMAGE PROC, P2983, DOI 10.1109/ICIP.2013.6738614
   Fang RG, 2010, IEEE IMAGE PROC, P1577, DOI 10.1109/ICIP.2010.5652590
   Gao P., 2021, ICME
   Georgopoulos M, 2020, IMAGE VISION COMPUT, V102, DOI 10.1016/j.imavis.2020.103954
   Ghatas FS, 2020, SN APPL SCI, V2, DOI 10.1007/s42452-020-1949-3
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Hao M, 2020, NEUROCOMPUTING, V391, P42, DOI 10.1016/j.neucom.2020.01.048
   Haq al I. U., 2019, INT J DISTRIB SENSOR, V15
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HEPPER PG, 1994, BEHAV PROCESS, V33, P3, DOI 10.1016/0376-6357(94)90056-6
   Huang G.B., 2008, PROC WORKSHOP FACES
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Kingma D. P., 2014, arXiv
   Kumar C, 2020, AAAI CONF ARTIF INTE, V34, P11304
   Li XS, 2015, 2015 INTERNATIONAL CONFERENCE ON INDUSTRIAL INFORMATICS - COMPUTING TECHNOLOGY, INTELLIGENT TECHNOLOGY, INDUSTRIAL INFORMATION INTEGRATION (ICIICII), P1, DOI 10.1109/ICIICII.2015.88
   Li YY, 2017, ADV SOC SCI EDUC HUM, V159, P13
   Li YT, 2017, ADV NEUR IN, V30
   Li Zhang, 2014, Image and Vision Computing, V32, P771, DOI 10.1016/j.imavis.2013.12.005
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Lu JW, 2014, 2014 IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2014)
   Lu JW, 2014, IEEE T PATTERN ANAL, V36, P331, DOI 10.1109/TPAMI.2013.134
   Masi I, 2018, SIBGRAPI, P471, DOI 10.1109/SIBGRAPI.2018.00067
   Maze B, 2018, INT CONF BIOMETR, P158, DOI 10.1109/ICB2018.2018.00033
   Nagrani A, 2018, PROC CVPR IEEE, P8427, DOI 10.1109/CVPR.2018.00879
   Nagrani A, 2017, INTERSPEECH, P2616, DOI 10.21437/Interspeech.2017-950
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Peters J, 2017, ADAPT COMPUT MACH LE
   Petridis S, 2017, 14 INT C AUD VIS SPE, P36
   Poindron P, 2007, HORM BEHAV, V52, P99, DOI 10.1016/j.yhbeh.2007.03.023
   Poindron P, 2007, DEV PSYCHOBIOL, V49, P54, DOI 10.1002/dev.20192
   Qin XQ, 2020, NEUROCOMPUTING, V377, P213, DOI 10.1016/j.neucom.2019.09.089
   Qin XQ, 2015, IEEE T MULTIMEDIA, V17, P1855, DOI 10.1109/TMM.2015.2461462
   Robinson J. P., 2019, IEEE T PATTERN ANAL
   Robinson J. P., 2016, P 24 ACM INT C MULT, P242, DOI DOI 10.1145/2964284.2967219
   Robinson J. P., 2017, PROC RFIW WORKSHOP A
   Robinson JP, 2020, IEEE INT CONF AUTOMA, P857, DOI [10.1109/fg47880.2020.00138, 10.1109/FG47880.2020.00138]
   Robinson JP, 2020, IEEE COMPUT SOC CONF, P1, DOI 10.1109/CVPRW50498.2020.00008
   Robinson JP, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P2096, DOI 10.1145/3240508.3241471
   Robinson JP, 2018, IEEE T PATTERN ANAL, V40, P2624, DOI 10.1109/TPAMI.2018.2826549
   Sánchez-Nielsen E, 2017, MULTIMED TOOLS APPL, V76, P6281, DOI 10.1007/s11042-016-3306-5
   Snell J, 2017, ADV NEUR IN, V30
   Song X., 2019, J PHYS C SER, V1237
   Sun Y., 2018, PROC IEEE VIS COMMUN, P1
   Wan L, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4879, DOI 10.1109/ICASSP.2018.8462665
   Wang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1041, DOI 10.1145/3123266.3123359
   Wang J, 2017, IEEE I CONF COMP VIS, P2612, DOI [10.1109/ICCV.2017.283, 10.1109/ICCV.2017.65]
   Wang SY, 2017, IEEE INT CONF AUTOMA, P216, DOI [10.1109/FG.2017.35, 10.1109/ICEMI.2017.8265769]
   Wei Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P613, DOI 10.1007/978-3-030-58542-6_37
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Whitelam C, 2017, IEEE COMPUT SOC CONF, P592, DOI 10.1109/CVPRW.2017.87
   Wiles O., 2018, BRIT MACH VIS C
   Wiles O, 2018, LECT NOTES COMPUT SC, V11217, P690, DOI 10.1007/978-3-030-01261-8_41
   Wu X., 2016, P IEEE C SIGN PROC C, P1
   Wu XT, 2019, INT CONF BIOMETR
   Wu Y, 2018, IEEE INT CONF AUTOMA, P143, DOI 10.1109/FG.2018.00030
   Xia S., 2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence, P2539, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-422
   Xia SY, 2012, IEEE T MULTIMEDIA, V14, P1046, DOI 10.1109/TMM.2012.2187436
   Xiong L., 2017, CoRR, Vabs/1704.00438
   Zhang L, 2021, IEEE T CYBERNETICS, V51, P5883, DOI 10.1109/TCYB.2019.2959403
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhou X., 2011, ACM Multimedia, P953
   Zhu Hao, 2020, ARXIV200104758
NR 80
TC 6
Z9 8
U1 1
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG 10
PY 2021
VL 24
BP 3582
EP 3594
DI 10.1109/TMM.2021.3103074
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NI
UT WOS:000824706700001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, YC
   Deng, JJ
   Zhou, WG
   Li, HQ
AF Wang, Yuechen
   Deng, Jiajun
   Zhou, Wengang
   Li, Houqiang
TI Weakly Supervised Temporal Adjacent Network for Language Grounding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Grounding; Semantics; Feature extraction; Visualization; Task analysis;
   Annotations; Training; Temporal language grounding; weakly supervised
   learning; multi-model understanding; multiple instance learning
ID LOCALIZATION; ATTENTION; VIDEO
AB Temporal language grounding (TLG) is a fundamental and challenging problem for vision and language understanding. Existing methods mainly focus on fully supervised setting with temporal boundary labels for training, which, however, suffers expensive cost of annotation. In this work, we are dedicated to weakly supervised TLG, where multiple description sentences are given to an untrimmed video without temporal boundary labels. In this task, it is critical to learn a strong cross-modal semantic alignment between sentence semantics and visual content. To this end, we introduce a novel weakly supervised temporal adjacent network (WSTAN) for temporal language grounding. Specifically, WSTAN learns cross-modal semantic alignment by exploiting temporal adjacent network in a multiple instance learning (MIL) paradigm, with a whole description paragraph as input. Moreover, we integrate a complementary branch into the framework, which explicitly refines the predictions with pseudo supervision from the MIL stage. An additional self-discriminating loss is devised on both the MIL branch and the complementary branch, aiming to enhance semantic discrimination by self-supervising. Extensive experiments are conducted on three widely used benchmark datasets, i.e., ActivityNet-Captions, Charades-STA, and DiDeMo, and the results demonstrate the effectiveness of our approach.
C1 [Wang, Yuechen; Deng, Jiajun; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhou, WG; Li, HQ (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
EM wyc9725@mail.ustc.edu.cn; dengjj@mail.ustc.edu.cn; zhwg@ustc.edu.cn;
   lihq@ustc.edu.cn
RI Deng, Jiajun/KIK-3592-2024; Li, Houqiang Li/B-6259-2013
OI Deng, Jiajun/0000-0001-9624-7451; Wang, Yuechen/0000-0003-0965-584X
FU National Key R&D Program of China [2017YFB1002202]; National Natural
   Science Foundation of China [U20A20183, 62021001]; Youth Innovation
   Promotion Association CAS [2018497]; GPU cluster built by MCC Laboratory
   of Information Science and Technology Institution, USTC
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2017YFB1002202, in part by the National Natural Science
   Foundation of China under Grants U20A20183 and 62021001, in part by the
   Youth Innovation Promotion Association CAS under Grant 2018497, and in
   part by the GPU cluster built by MCC Laboratory of Information Science
   and Technology Institution, USTC.
CR [Anonymous], 2018, P C EMP METH NAT LAN
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Bilen H, 2016, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR.2016.311
   Carion N, 2020, ARXIVABS 200512872
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Chen JY, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P162
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8199
   Deng J., 2021, ICCV
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan JS, 2020, AAAI CONF ARTIF INTE, V34, P10762
   Gaidon A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3201, DOI 10.1109/CVPR.2011.5995646
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Gao MF, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P1481
   Ge RZ, 2019, IEEE WINT CONF APPL, P245, DOI 10.1109/WACV.2019.00032
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   He DL, 2019, AAAI CONF ARTIF INTE, P8393
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang T.-H., 2016, NAACL HLT, P1233
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Ke Yang, 2019, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Proceedings, P8371, DOI 10.1109/ICCV.2019.00846
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Li Y, 2018, IEEE T IMAGE PROCESS, V27, P1561, DOI 10.1109/TIP.2017.2779270
   Lin CH, 2020, AAAI CONF ARTIF INTE, V34, P11482
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin ZJ, 2020, AAAI CONF ARTIF INTE, V34, P11539
   Liu M, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P843, DOI 10.1145/3240508.3240549
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mithun NC, 2019, PROC CVPR IEEE, P11584, DOI 10.1109/CVPR.2019.01186
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706
   Nguyen PX, 2019, IEEE I CONF COMP VIS, P5501, DOI 10.1109/ICCV.2019.00560
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shou Z, 2018, LECT NOTES COMPUT SC, V11220, P162, DOI 10.1007/978-3-030-01270-0_10
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh KK, 2017, IEEE I CONF COMP VIS, P3544, DOI 10.1109/ICCV.2017.381
   Tang P, 2020, IEEE T PATTERN ANAL, V42, P176, DOI 10.1109/TPAMI.2018.2876304
   Tang P, 2017, PROC CVPR IEEE, P3059, DOI 10.1109/CVPR.2017.326
   Tapaswi M, 2016, PROC CVPR IEEE, P4631, DOI 10.1109/CVPR.2016.501
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Wang WN, 2019, PROC CVPR IEEE, P334, DOI 10.1109/CVPR.2019.00042
   Wu AM, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1029
   Xiong Y, 2017, ARXIVABS170302716
   Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062
   Yang GR, 2018, LECT NOTES COMPUT SC, V11214, P729, DOI 10.1007/978-3-030-01249-6_44
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yu Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10988, DOI 10.1109/CVPR42600.2020.01100
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Zeng ZY, 2019, IEEE I CONF COMP VIS, P8291, DOI 10.1109/ICCV.2019.00838
   Zhang D, 2019, PROC CVPR IEEE, P1247, DOI 10.1109/CVPR.2019.00134
   Zhang SY, 2020, AAAI CONF ARTIF INTE, V34, P12870
   Zhang WQ, 2020, IEEE T MULTIMEDIA, V22, P1032, DOI 10.1109/TMM.2019.2935678
   Zhang ZW, 2021, IEEE T MULTIMEDIA, V23, P1799, DOI 10.1109/TMM.2020.3003592
   Zhang Z, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P655, DOI 10.1145/3331184.3331235
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhongzheng Ren, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10595, DOI 10.1109/CVPR42600.2020.01061
NR 66
TC 19
Z9 19
U1 1
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 14
PY 2021
VL 24
BP 3276
EP 3286
DI 10.1109/TMM.2021.3096087
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NU
UT WOS:000824707900002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Song, XL
   Jin, Z
AF Song, Xulin
   Jin, Zhong
TI Robust Label Rectifying With Consistent Contrastive-Learning for Domain
   Adaptive Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise measurement; Training; Feature extraction; Uncertainty;
   Reliability; Estimation; Clustering algorithms; Contrastive learning;
   domain adaptation; label rectifying; person Re-ID
AB Domain adaptive person re-identification (Re-ID) is challenging due to the domain gap between the source and target domains. Existing methods have recently shown great promise by training models with contrastive learning and assigning pseudo labels by clustering, in which a memory bank is utilized to keep features for contrast. However, two main problems lead to sub-optimal generalization ability for existing methods. First, there is no constraint on the updating for memory kept features in existing methods, resulting in inaccurate contrastive learning. Second, the inevitable noisy labels during clustering are usually ignored. To alleviate these problems, we propose a Label Rectifying with Consistent Contrastive-learning (LRCC) framework with two strategies. (1) The consistent contrastive-learning (CC) strategy works with a memory bank which stores the source domain class centroids and all the target domain image features. With the CC strategy, the contrast is conducted across the source and target domains simultaneously. More specifically, we design and maintain consistent clustering during model iteration, thus the classes of memory kept target features are invariable in one epoch. (2) The label rectifying (LR) strategy introduces an auxiliary classifier into the LRCC framework. Thus the pseudo labels are rectified by minimizing the prediction variance between the primary classifier and the auxiliary classifier. To verify the effectiveness of LRCC, we conduct experiments on three public person Re-ID datasets under the domain adaptive setting, DukeMTMC-reID, Market-1501, and MSMT17. The experimental results demonstrate that the proposed LRCC can obtain reliable pseudo labels and achieves state-of-the-art adaptation performance.
C1 [Song, Xulin; Jin, Zhong] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Minist Educ, Key Lab Intelligent Percept & Syst High Dimens In, Nanjing 210094, Peoples R China.
C3 Nanjing University of Science & Technology
RP Jin, Z (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Minist Educ, Key Lab Intelligent Percept & Syst High Dimens In, Nanjing 210094, Peoples R China.
EM xulinsong@njust.edu.cn; zhongjin@njust.edu.cn
FU National Natural Science Foundation of China [61872188, U1713208,
   61972204, 61861136011, 61773215]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61872188, U1713208, 61972204, 61861136011, and
   61773215.
CR Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Chen T, 2020, PR MACH LEARN RES, V119
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding YH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3369393
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   Estévez PA, 2009, IEEE T NEURAL NETWOR, V20, P189, DOI 10.1109/TNN.2008.2005601
   Fox NA, 2007, IEEE T MULTIMEDIA, V9, P701, DOI 10.1109/TMM.2007.893339
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ge Y., 2020, P NIPS, V33, P11309
   Ge Yixiao, 2020, ARXIV200101526
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   Han JF, 2019, IEEE I CONF COMP VIS, P5137, DOI 10.1109/ICCV.2019.00524
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hjelm RD., 2018, Statistics, P1
   Hsieh JW, 2008, IEEE T MULTIMEDIA, V10, P372, DOI 10.1109/TMM.2008.917403
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Krishna K, 1999, IEEE T SYST MAN CY B, V29, P433, DOI 10.1109/3477.764879
   Lee KH, 2018, PROC CVPR IEEE, P5447, DOI 10.1109/CVPR.2018.00571
   Li X, 2018, IEEE T MULTIMEDIA, V20, P1169, DOI 10.1109/TMM.2017.2761985
   Li YH, 2020, IEEE T MULTIMEDIA, V22, P1285, DOI 10.1109/TMM.2019.2939711
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Liu Z, 2018, IEEE T MULTIMEDIA, V20, P1321, DOI 10.1109/TMM.2017.2767781
   Naphade M, 2020, IEEE COMPUT SOC CONF, P2665, DOI 10.1109/CVPRW50498.2020.00321
   Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Shi YX, 2021, IEEE T MULTIMEDIA, V23, P3264, DOI 10.1109/TMM.2020.3023272
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang Z, 2019, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2019.00900
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   Wang D., 2020, P IEEE COMP SOC C CO, p10 981
   Wang XW, 2013, IEEE T MULTIMEDIA, V15, P2035, DOI 10.1109/TMM.2013.2279658
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Yadati K, 2018, IEEE T MULTIMEDIA, V20, P2526, DOI 10.1109/TMM.2018.2801719
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522
   Yang FX, 2020, AAAI CONF ARTIF INTE, V34, P12597
   Yang Zou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P87, DOI 10.1007/978-3-030-58536-5_6
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P924, DOI 10.1109/TPAMI.2020.3013379
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2020, IEEE T IMAGE PROCESS, V29, P9387, DOI 10.1109/TIP.2020.2998275
   Ye M, 2020, IEEE T INF FOREN SEC, V15, P2655, DOI 10.1109/TIFS.2020.2970590
   Ye M, 2019, IEEE T IMAGE PROCESS, V28, P2976, DOI 10.1109/TIP.2019.2893066
   Yu TY, 2019, IEEE I CONF COMP VIS, P552, DOI 10.1109/ICCV.2019.00064
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zhang XY, 2019, IEEE I CONF COMP VIS, P8221, DOI 10.1109/ICCV.2019.00831
   Zhang ZL, 2018, ADV NEUR IN, V31
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Z., 2020, PROC INT JOINT C ART, P1
   Zheng ZD, 2021, INT J COMPUT VISION, V129, P1106, DOI 10.1007/s11263-020-01395-y
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhong Z, 2021, IEEE T PATTERN ANAL, V43, P2723, DOI 10.1109/TPAMI.2020.2976933
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
NR 57
TC 15
Z9 15
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 13
PY 2021
VL 24
BP 3229
EP 3239
DI 10.1109/TMM.2021.3096014
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NQ
UT WOS:000824707500002
DA 2024-07-18
ER

PT J
AU Javaheri, A
   Brites, C
   Pereira, F
   Ascenso, J
AF Javaheri, Alireza
   Brites, Catarina
   Pereira, Fernando
   Ascenso, Joao
TI Point Cloud Rendering After Coding: Impacts on Subjective and Objective
   Quality
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Encoding; Rendering (computer graphics);
   Measurement; Geometry; Quality assessment; Codecs; Point cloud coding;
   quality assessment; subjective quality assessment; rendering
ID REPRESENTATION; COMPRESSION; SHAPE
AB Recently, point clouds have shown to be a promising way to represent 3D visual data for a wide range of immersive applications, from augmented reality to autonomous cars. Emerging imaging sensors have made easier to perform richer and denser point cloud acquisition, notably with millions of points, thus raising the need for efficient point cloud coding solutions. In such scenario, it is important to evaluate the impact and performance of several processing steps in a point cloud communication system, notably the degradations associated to point cloud coding solutions. Moreover, since point clouds are not directly visualized but rather processed with a rendering algorithm before shown on any display, the perceived quality of point cloud data highly depends on the rendering solution. In this context, the main objective of this paper is to study the impact of several coding and rendering solutions on the perceived user quality and in the performance of available objective assessment metrics. Another contribution regards the assessment of recent MPEG point cloud coding solutions for several popular rendering methods, which was never presented before. The conclusions regard the visibility of three types of coding artifacts for the three considered rendering approaches as well as the strengths and weaknesses of objective metrics when point clouds are rendered after coding.
C1 [Javaheri, Alireza; Brites, Catarina; Pereira, Fernando; Ascenso, Joao] Inst Super Tecn, P-1049001 Lisbon, Portugal.
   [Javaheri, Alireza; Brites, Catarina; Pereira, Fernando; Ascenso, Joao] Inst Telecomunicacoes, P-1049001 Lisbon, Portugal.
C3 Universidade de Lisboa; Instituto de Telecomunicacoes
RP Ascenso, J (corresponding author), Inst Super Tecn, P-1049001 Lisbon, Portugal.; Ascenso, J (corresponding author), Inst Telecomunicacoes, P-1049001 Lisbon, Portugal.
EM alireza.javaheri@lx.it.pt; catarina.brites@lx.it.pt; fp@lx.it.pt;
   joao.ascenso@lx.it.pt
RI Javaheri, Alireza/X-9046-2019; Pereira, Fernando/K-4046-2012; Pereira,
   Fernando/HNR-7786-2023; Brites, Catarina/L-6191-2013
OI Javaheri, Alireza/0000-0002-9209-1688; 
FU Fundacao para a Ciencia e a Tecnologia (FCT) through the Project
   entitled Progressive Point Cloud Representation
   [PTDC/EEI-PRO/7237/2014]; Fundação para a Ciência e a Tecnologia
   [PTDC/EEI-PRO/7237/2014] Funding Source: FCT
FX This work was supported by the Fundacao para a Ciencia e a Tecnologia
   (FCT) through the Project entitled Progressive Point Cloud
   Representation under Grant PTDC/EEI-PRO/7237/2014.
CR Alexiou E., 2017, P 9 INT C QUAL MULT, P1
   Alexiou E, 2018, IEEE INT CON MULTI
   Alexiou E, 2018, PROC SPIE, V10752, DOI 10.1117/12.2321518
   Alexiou E, 2018, PICT COD SYMP, P51, DOI 10.1109/PCS.2018.8456252
   Alexiou E, 2017, IEEE INT WORKSH MULT
   Alexiou E, 2017, PROC SPIE, V10396, DOI 10.1117/12.2275142
   [Anonymous], 2018, Real-Time Rendering, Fourth Edition, DOI DOI 10.1201/B22086
   [Anonymous], 2016, PHYS BASED RENDERING
   [Anonymous], 2012, ITU-R Recommendation P.530-14.
   Athar S, 2019, IEEE ACCESS, V7, P140030, DOI 10.1109/ACCESS.2019.2943319
   Chen JY, 2014, IEEE T MULTIMEDIA, V16, P337, DOI 10.1109/TMM.2013.2286580
   Christaki K, 2019, LECT NOTES COMPUT SC, V11295, P80, DOI 10.1007/978-3-030-05710-7_7
   Dumic E, 2018, 2018 FIRST INTERNATIONAL COLLOQUIUM ON SMART GRID METROLOGY (SMAGRIMET)
   Ebrahimi T, 2016, IEEE MULTIMEDIA, V23, P14, DOI 10.1109/MMUL.2016.64
   Fouhey DF, 2019, IEEE T PATTERN ANAL, V41, P93, DOI 10.1109/TPAMI.2017.2782810
   Girardeau-Montaut D.C., CLOUD COMPARE 3D POI
   GRAHAM RL, 1985, ANN HIST COMPUT, V7, P43
   ITU-T Tutorial, 2005, OBJ PERC ASS VID QUA
   Javaheri Alireza, 2017, 2017 IEEE International Conference on Multimedia and Expo: Workshops (ICMEW), P1, DOI 10.1109/ICMEW.2017.8026263
   Javaheri A, 2019, IST RENDERING POINT
   Javaheri A, 2017, IEEE INT WORKSH MULT
   Jung H, 2020, IEEE T MULTIMEDIA, V22, P980, DOI 10.1109/TMM.2019.2934819
   Kammerl J, 2012, IEEE INT CONF ROBOT, P778, DOI 10.1109/ICRA.2012.6224647
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Khatamian A, 2016, J INF PROCESS SYST, V12, P338, DOI 10.3745/JIPS.01.0010
   Kim JH, 2021, ABACUS, V57, P27, DOI 10.1111/abac.12172
   Mammou K., 2019, ISO/IEC JTC1/SC29/WG11 N18189
   Mantiuk RK, 2012, COMPUT GRAPH FORUM, V31, P2478, DOI 10.1111/j.1467-8659.2012.03188.x
   Martin G., 1979, VIDEO DATA RECORDING, P24
   Mekuria R., 2016, JTC1SC29WG11N16330 I
   Mekuria R, 2017, IEEE T CIRC SYST VID, V27, P828, DOI 10.1109/TCSVT.2016.2543039
   MPEG 3DG and Requirements Subgroups, 2017, JTC1SC29WG11N16763 I
   MPEG 3DG Subgroup, 2018, JTC1SC29WG11N18030 I
   MPEG 3DG Subgroup, 2017, JTC1SC29WG11N16716 I
   MPEG 3DG Subgroup, 2019, JTC1SC29WG11N18478 I
   MPEG3DG Subgroup, 2019, JTC1SC29WG11N18474 I
   Nehme Y., 2019, PROC ACM S APPL PERC, P1
   Park SB, 2009, IEEE T MULTIMEDIA, V11, P177, DOI 10.1109/TMM.2008.2008868
   RAMACHANDRAN VS, 1988, NATURE, V331, P163, DOI 10.1038/331163a0
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Rosenthal Paul., 2008, P COMPUTER GRAPHICS, P136
   Schütz M, 2015, 2015 DIGITAL HERITAGE INTERNATIONAL CONGRESS, VOL 1: DIGITIZATION & ACQUISITION, COMPUTER GRAPHICS & INTERACTION, P369, DOI 10.1109/DigitalHeritage.2015.7413904
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Sim JY, 2008, IEEE T MULTIMEDIA, V10, P305, DOI 10.1109/TMM.2008.917349
   Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tian D, 2017, IEEE IMAGE PROC, P3460, DOI 10.1109/ICIP.2017.8296925
   Viola I., 2019, INT WORK QUAL MULTIM, P1
   Zakharchenko V., 2018, JTC1SC29WG11N18017 I
   Zhang J, 2014, 2014 INTERNATIONAL CONFERENCE ON AUDIO, LANGUAGE AND IMAGE PROCESSING (ICALIP), VOLS 1-2, P827, DOI 10.1109/ICALIP.2014.7009910
   Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284
NR 51
TC 31
Z9 32
U1 5
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4049
EP 4064
DI 10.1109/TMM.2020.3037481
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ren, Z
   Kong, QQ
   Han, J
   Plumbley, MD
   Schuller, BW
AF Ren, Zhao
   Kong, Qiuqiang
   Han, Jing
   Plumbley, Mark D.
   Schuller, Bjoern W.
TI CAA-Net: Conditional Atrous CNNs With Attention for Explainable
   Device-Robust Acoustic Scene Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Acoustics; Time-frequency analysis; Spectrogram; Data models;
   Computational modeling; Convolution; Acoustic scene classification;
   attention; conditional atrous convolutional neural networks;
   multi-device data; visualisation
ID NETWORKS; AUGMENTATION
AB Acoustic Scene Classification (ASC) aims to classify the environment in which the audio signals are recorded. Recently, Convolutional Neural Networks (CNNs) have been successfully applied to ASC. However, the data distributions of the audio signals recorded with multiple devices are different. There has been little research on the training of robust neural networks on acoustic scene datasets recorded with multiple devices, and on explaining the operation of the internal layers of the neural networks. In this article, we focus on training and explaining device-robust CNNs on multi-device acoustic scene data. We propose conditional atrous CNNs with attention for multi-device ASC. Our proposed system contains an ASC branch and a device classification branch, both modelled by CNNs. We visualise and analyse the intermediate layers of the atrous CNNs. A time-frequency attention mechanism is employed to analyse the contribution of each time-frequency bin of the feature maps in the CNNs. On the Detection and Classification of Acoustic Scenes and Events (DCASE) 2018 ASC dataset, recorded with three devices, our proposed model performs significantly better than CNNs trained on single-device data.
C1 [Ren, Zhao; Han, Jing; Schuller, Bjoern W.] Univ Augsburg, Embedded Intelligence Hlth Care & Wellbeing, D-86159 Augsburg, Germany.
   [Kong, Qiuqiang; Plumbley, Mark D.] Univ Surrey, Ctr Vis Speech & Signal Proc, Surrey GU2 7XH, England.
   [Kong, Qiuqiang] ByteDance AI Lab, Shanghai 200233, Peoples R China.
   [Han, Jing] Univ Cambridge, Dept Comp Sci & Technol, Cambridge CB3 0FD, England.
   [Schuller, Bjoern W.] Imperial Coll London, GLAM Grp Language Audio & Mus, London SW7 2AZ, England.
C3 University of Augsburg; University of Surrey; University of Cambridge;
   Imperial College London
RP Ren, Z (corresponding author), Univ Augsburg, Embedded Intelligence Hlth Care & Wellbeing, D-86159 Augsburg, Germany.
EM zhao.ren@informatik.uni-augsburg.de; q.kong@surrey.ac.uk;
   jh2298@cam.ac.uk; m.plumbley@surrey.ac.uk; schuller@ieee.org
RI Plumbley, Mark D/A-7298-2008; Schuller, Björn Wolfgang/D-3241-2011; Han,
   Jing/AAB-3944-2020
OI Plumbley, Mark D/0000-0002-9708-1075; Schuller, Björn
   Wolfgang/0000-0002-6478-8699; Han, Jing/0000-0001-5776-6849; Ren,
   Zhao/0000-0003-0707-5016
FU European Union's Horizon H2020 research and innovation programme under
   Marie Sklodowska-Curie grant [766287]; EPSRC [EP/N014111/1]; China
   Scholarship Council (CSC) [201406150082]; EPSRC [EP/N014111/1] Funding
   Source: UKRI
FX This work was supported by European Union's Horizon H2020 research and
   innovation programme under Marie Sklodowska-Curie grant agreement No.
   766287 (TAPAS), EPSRC grant EP/N014111/1 "Making Sense of Sounds," and a
   Research Scholarship from the China Scholarship Council (CSC) No.
   201406150082.
CR Amores J, 2013, ARTIF INTELL, V201, P81, DOI 10.1016/j.artint.2013.06.003
   [Anonymous], 2017, P DCASE 2017
   [Anonymous], 2016, DETECTION CLASSIFICA
   Arik SÖ, 2017, ADV NEUR IN, V30
   Barchiesi D, 2015, IEEE SIGNAL PROC MAG, V32, P16, DOI 10.1109/MSP.2014.2326181
   Boureau Y. L., 2010, P 27 INT C MACH LEAR, P111
   Brezina P., AD ALTA J INTERDISCI, V8, P13
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Chorowski J, 2015, ADV NEUR IN, V28
   Fourure D, 2017, NEUROCOMPUTING, V251, P68, DOI 10.1016/j.neucom.2017.04.014
   Frid-Adar M, 2018, NEUROCOMPUTING, V321, P321, DOI 10.1016/j.neucom.2018.09.013
   Gao Y, 2015, IEEE T MULTIMEDIA, V17, P359, DOI 10.1109/TMM.2015.2389616
   Geiger J., 2013, 2013 IEEE WORKSHOP A, P1, DOI [DOI 10.1109/WASPAA.2013.6701857, 10.1109/WASPAA.2013.6701857]
   Goyal A, 2016, ADV NEUR IN, V29
   Goyal A, 2019, FORENSIC SCI INT, V298, P332, DOI 10.1016/j.forsciint.2019.02.031
   Harma A., 2003, AUDIO ENG SOC CONVEN, P1
   Hawthorne C., 2018, P 19 INT SOC MUS INF, P50
   He KM, 2015, PROC CVPR IEEE, P5353, DOI 10.1109/CVPR.2015.7299173
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hojo N, 2018, IEICE T INF SYST, VE101D, P462, DOI 10.1587/transinf.2017EDP7165
   Hu QC, 2017, IEEE T INTELL TRANSP, V18, P3147, DOI 10.1109/TITS.2017.2679114
   Phan H, 2017, INTERSPEECH, P3043, DOI 10.21437/Interspeech.2017-101
   Phan H, 2018, IEEE ENG MED BIO, P453, DOI 10.1109/EMBC.2018.8512286
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Koluguri NR, 2017, IEEE-ACM T AUDIO SPE, V25, P1183, DOI 10.1109/TASLP.2017.2690562
   Kumar A, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P326, DOI 10.1109/ICASSP.2018.8462200
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Martinson E, 2007, IEEE INT CONF ROBOT, P435, DOI 10.1109/ROBOT.2007.363825
   Mesaros A., 2018, P DET CLASS AC SCEN, P9
   Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609
   Perera C, 2014, IEEE COMMUN SURV TUT, V16, P414, DOI 10.1109/SURV.2013.042313.00197
   Primus P., 2019, P DCASE, P204
   Qi JW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P528, DOI 10.1145/3240508.3240558
   Qian K, 2018, ARCH ACOUST, V43, P465, DOI 10.24425/123918
   Ren Z., 2018, P DCASE SURR UK, P39
   Ren Z., 2017, P WORKSH DET CLASS A, P113
   Ren Z., 2017, P DCASE WORKSH MUN G, P108
   Ren Z, 2019, INT CONF ACOUST SPEE, P56, DOI 10.1109/ICASSP.2019.8683434
   Ren Z, 2018, DH '18: PROCEEDINGS OF THE 2018 INTERNATIONAL CONFERENCE ON DIGITAL HEALTH, P143, DOI 10.1145/3194658.3194671
   Ren Z, 2018, IEEE-CAA J AUTOMATIC, V5, P662, DOI 10.1109/JAS.2018.7511066
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Salamon J, 2017, IEEE SIGNAL PROC LET, V24, P279, DOI 10.1109/LSP.2017.2657381
   Shi ZY, 2015, IEEE T PATTERN ANAL, V37, P1959, DOI 10.1109/TPAMI.2015.2392769
   Sindagi VA, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Snyder D, 2017, INTERSPEECH, P999, DOI 10.21437/Interspeech.2017-620
   Snyder D, 2016, IEEE W SP LANG TECH, P165, DOI 10.1109/SLT.2016.7846260
   Stowell D, 2015, IEEE T MULTIMEDIA, V17, P1733, DOI 10.1109/TMM.2015.2428998
   Su TW, 2017, INT CONF ACOUST SPEE, P791, DOI 10.1109/ICASSP.2017.7952264
   Tolias G., 2015, ARXIV151105879
   Nguyen T, 2019, IEEE INT CON MULTI, P1666, DOI 10.1109/ICME.2019.00287
   Valenti M, 2017, IEEE IJCNN, P1547, DOI 10.1109/IJCNN.2017.7966035
   van den Oord A., 2016, ICLR
   Vukotic V, 2017, LECT NOTES COMPUT SC, V10484, P140, DOI 10.1007/978-3-319-68560-1_13
   Xiong C, 2015, IEEE I CONF COMP VIS, P3667, DOI 10.1109/ICCV.2015.418
   Xu X., 2003, Ph.D. thesis
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Zhang ZX, 2019, IEEE T MULTIMEDIA, V21, P1289, DOI 10.1109/TMM.2018.2871949
   Zhao ZP, 2019, IEEE ACCESS, V7, P97515, DOI 10.1109/ACCESS.2019.2928625
   Zijiang Yang, 2019, Proceedings of the 7th Conference on Sound and Music Technology (CSMT). Revised Selected Papers. Lecture Notes in Electrical Engineering (LNEE 635), P133, DOI 10.1007/978-981-15-2756-2_11
NR 60
TC 15
Z9 15
U1 2
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4131
EP 4142
DI 10.1109/TMM.2020.3037534
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Sultana, M
   Mahmood, A
   Jung, SK
AF Sultana, Maryam
   Mahmood, Arif
   Jung, Soon Ki
TI Unsupervised Moving Object Detection in Complex Scenes Using Adversarial
   Regularizations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generative adversarial networks; Gallium nitride; Lighting; Heuristic
   algorithms; Object detection; Minimization; Optical losses; Moving
   object detection; background subtra-ction; generative adversarial
   networks
ID BACKGROUND SUBTRACTION; VIDEO SURVEILLANCE; LOW-RANK; SEPARATION; MODEL
AB Moving object detection (MOD) is a fundamental step in many high-level vision-based applications, such as human activity analysis, visual object tracking, autonomous vehicles, surveillance, and security. Most of the existing MOD algorithms observe performance degradation in the presence of complex scenes containing camouflage objects, shadows, dynamic backgrounds, and varying illumination conditions, and captured by static cameras. To appropriately handle these challenges, we propose a Generative Adversarial Network (GAN) based on a moving object detection algorithm, called MOD_GAN. In the proposed algorithm, scene-specific GANs are trained in an unsupervised MOD setting, thereby enabling the algorithm to learn generating background sequences using input from uniformly distributed random noise samples. In addition to adversarial loss, during training, norm-based loss in the image space and discriminator feature-space is also minimized between the generated images and the training data. The additional losses enable the generator to learn subtle background details, resulting in a more realistic complex scene generation. During testing, a novel back-propagation based algorithm is used to generate images with statistics similar to the test images. More appropriate random noise samples are searched by directly minimizing the loss function between the test and generated images both in the image and discriminator feature-spaces. The network is not updated in this step; only the input noise samples are iteratively modified to minimize the loss function. Moreover, motion information is used to ensure that this loss is only computed on small-motion pixels. A novel dataset containing outdoor time-lapsed images from dawn to dusk with a full illumination variation cycle is also proposed to better compare the MOD algorithms in outdoor scenes. Accordingly, extensive experiments on five benchmark datasets and comparison with 30 existing methods demonstrate the strength of the proposed algorithm.
C1 [Sultana, Maryam; Jung, Soon Ki] Kyungpook Natl Univ, Sch Comp Sci & Engn, Virtual Real Lab, Daegu 41566, South Korea.
   [Mahmood, Arif] Informat Technol Univ, Dept Comp Sci, Lahore 54000, Pakistan.
C3 Kyungpook National University
RP Jung, SK (corresponding author), Kyungpook Natl Univ, Sch Comp Sci & Engn, Virtual Real Lab, Daegu 41566, South Korea.
EM maryam@knu.ac.kr; arif.mahmood@itu.edu.pk; skjung@knu.ac.kr
RI Mahmood, Arif/R-7949-2019; Jung, Soon Ki/P-7687-2018
OI Mahmood, Arif/0000-0001-5986-9876; Jung, Soon Ki/0000-0003-0239-6785
FU Basic Science Research Program through the National Research Foundation
   of Korea (NRF) - Ministry of Education, Science and Technology
   [NRF-2019R1A2C1010786]
FX This work was supported by Basic Science Research Program through the
   National Research Foundation of Korea (NRF) funded by the Ministry of
   Education, Science and Technology under Grant NRF-2019R1A2C1010786.
CR [Anonymous], 2018, ACTA AUTOMATICASINIC
   [Anonymous], 2019, MULTIMEDIA TOOLS APP
   Bakkay MC, 2018, IEEE IMAGE PROC, P4018, DOI 10.1109/ICIP.2018.8451603
   Baradel F, 2018, PROC CVPR IEEE, P469, DOI 10.1109/CVPR.2018.00056
   Berjón D, 2018, PATTERN RECOGN, V74, P156, DOI 10.1016/j.patcog.2017.09.009
   Bianco S, 2017, LECT NOTES COMPUT SC, V10484, P96, DOI 10.1007/978-3-319-68560-1_9
   Bouwmans T., 2008, Recent Patents Comput. Sci., V1, P219
   Bouwmans T, 2019, NEURAL NETWORKS, V117, P8, DOI 10.1016/j.neunet.2019.04.024
   Bouwmans T, 2017, PATTERN RECOGN LETT, V96, P3, DOI 10.1016/j.patrec.2016.12.024
   Bouwmans T, 2014, COMPUT VIS IMAGE UND, V122, P22, DOI 10.1016/j.cviu.2013.11.009
   Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21
   Brutzer S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1937, DOI 10.1109/CVPR.2011.5995508
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Cao XC, 2016, IEEE T CYBERNETICS, V46, P1014, DOI 10.1109/TCYB.2015.2419737
   Chen ML, 2018, IEEE T PATTERN ANAL, V40, P1518, DOI 10.1109/TPAMI.2017.2717828
   Chen Y, 2017, IEEE CUST INTEGR CIR
   Dosovitskiy A, 2016, PROC CVPR IEEE, P4829, DOI 10.1109/CVPR.2016.522
   Elgammal A, 2000, EUR C COMP VIS, P751, DOI DOI 10.1007/3-540-45053-X_48
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He J, 2012, PROC CVPR IEEE, P1568, DOI 10.1109/CVPR.2012.6247848
   Hu WM, 2004, IEEE T SYST MAN CY C, V34, P334, DOI 10.1109/TSMCC.2004.829274
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Javed S, 2017, LECT NOTES COMPUT SC, V10590, P230, DOI 10.1007/978-3-319-70742-6_22
   Javed S, 2018, IEEE T CIRC SYST VID, V28, P1315, DOI 10.1109/TCSVT.2016.2632302
   Khalifa S, 2018, IEEE T MOBILE COMPUT, V17, P1353, DOI 10.1109/TMC.2017.2761744
   Lezki H., 2018, P EUR C COMP VIS, P100
   Li LY, 2004, IEEE T IMAGE PROCESS, V13, P1459, DOI 10.1109/TIP.2004.836169
   Liang D, 2015, PATTERN RECOGN, V48, P1374, DOI 10.1016/j.patcog.2014.10.020
   Liu C., 2009, Beyond pixels: Exploring new representations and applications for motion analysis
   Liu PP, 2019, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2019.00470
   Liu X, 2015, IEEE T IMAGE PROCESS, V24, P2502, DOI 10.1109/TIP.2015.2419084
   Lu XK, 2019, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2019.00374
   Lu XQ, 2014, IEEE IMAGE PROC, P3268, DOI 10.1109/ICIP.2014.7025661
   Maddalena L, 2015, LECT NOTES COMPUT SC, V9281, P469, DOI 10.1007/978-3-319-23222-5_57
   Muhammad K, 2018, IEEE T IND INFORM, V14, P3679, DOI 10.1109/TII.2018.2791944
   Oreifej O, 2013, IEEE T PATTERN ANAL, V35, P450, DOI 10.1109/TPAMI.2012.97
   Ortego D, 2016, COMPUT VIS IMAGE UND, V147, P23, DOI 10.1016/j.cviu.2016.03.012
   Ou X, 2019, IEEE ACCESS, V7, P108
   Pang YW, 2018, IEEE T CIRC SYST VID, V28, P640, DOI 10.1109/TCSVT.2016.2630731
   Pei SL, 2020, IEEE ACCESS, V8, P88259, DOI 10.1109/ACCESS.2020.2992494
   Radford A., 2016, INT C LEARN REPR
   Shah S, 2018, FIELD SERVICE ROBOTI, P621, DOI [10.1007/978-3-319-67361-5_40, DOI 10.1007/978-3-319-67361-5_40]
   Shimada A, 2013, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2013.258
   Song HM, 2018, LECT NOTES COMPUT SC, V11215, P744, DOI 10.1007/978-3-030-01252-6_44
   Staglianò A, 2015, IEEE T IMAGE PROCESS, V24, P2415, DOI 10.1109/TIP.2015.2421435
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Sultana M., 2018, P AS C COMP VIS WORK
   Sultana M, 2020, COMM COM INF SC, V1212, P31, DOI 10.1007/978-981-15-4818-5_3
   Sultana M, 2019, IEEE INT CONF COMP V, P661, DOI 10.1109/ICCVW.2019.00080
   Sultana M, 2019, MACH VISION APPL, V30, P375, DOI 10.1007/s00138-018-0993-0
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Tom AJ, 2021, IEEE T CYBERNETICS, V51, P1004, DOI 10.1109/TCYB.2019.2921827
   Toyama K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P255, DOI 10.1109/ICCV.1999.791228
   Varadarajan S, 2013, 2013 10TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2013), P63, DOI 10.1109/AVSS.2013.6636617
   Wang M, 2018, IEEE T MULTIMEDIA, V20, P620, DOI 10.1109/TMM.2017.2748459
   Wang WG, 2019, IEEE I CONF COMP VIS, P9235, DOI 10.1109/ICCV.2019.00933
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P2413, DOI 10.1109/TPAMI.2020.2966453
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang Y, 2017, PATTERN RECOGN LETT, V96, P66, DOI 10.1016/j.patrec.2016.09.014
   Wang Y, 2014, IEEE COMPUT SOC CONF, P393, DOI 10.1109/CVPRW.2014.126
   Wang YP, 2019, IEEE T IMAGE PROCESS, V28, P2813, DOI 10.1109/TIP.2019.2891055
   Xu J, 2013, IEEE I CONF COMP VIS, P3376, DOI 10.1109/ICCV.2013.419
   Yang YC, 2019, PROC CVPR IEEE, P879, DOI 10.1109/CVPR.2019.00097
   Yang YQ, 2020, IEEE ACCESS, V8, P42169, DOI 10.1109/ACCESS.2020.2977007
   Ye XC, 2015, IEEE T CIRC SYST VID, V25, P1721, DOI 10.1109/TCSVT.2015.2392491
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Zhang LF, 2021, IEEE T CYBERNETICS, V51, P673, DOI [10.1109/TCYB.2019.2910151, 10.1109/TCYB.2019.2935066]
   Zhang TZ, 2013, IEEE T IND INFORM, V9, P149, DOI 10.1109/TII.2012.2218251
   Zhao CH, 2019, IEEE T IND ELECTRON, V66, P4749, DOI 10.1109/TIE.2018.2864703
   Zhou T., 2011, P ICML, P33
   Zhou XW, 2013, IEEE T PATTERN ANAL, V35, P597, DOI 10.1109/TPAMI.2012.132
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
   Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992
NR 75
TC 20
Z9 20
U1 0
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2005
EP 2018
DI 10.1109/TMM.2020.3006419
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100013
DA 2024-07-18
ER

PT J
AU Sun, GF
   Wong, YK
   Cheng, ZY
   Kankanhalli, MS
   Geng, WD
   Li, XD
AF Sun, Guofei
   Wong, Yongkang
   Cheng, Zhiyong
   Kankanhalli, Mohan S.
   Geng, Weidong
   Li, Xiangdong
TI DeepDance: Music-to-Dance Motion Choreography With Adversarial Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generators; Feature extraction; Task analysis; Correlation;
   Three-dimensional displays; Music; Deep learning; Music-driven dance
   choreography; adversarial learning; cross-modal association
AB The creation of improvised dancing choreographies is an important research field of cross-modal analysis. A key point of this task is how to effectively create and correlate music and dance with a probabilistic one-to-many mapping, which is essential to create realistic dances of various genres. To address this issue, we propose a GAN-based cross-modal association framework, DeepDance, which correlates two different modalities (dance motion and music) together, aiming at creating the desired dance sequence in terms of the input music. Its generator is to predictively produce the dance movements best-fit to current music piece by learning from examples. In another hand, its discriminator acts as an external evaluation from the audience and judges the whole performance. The generated dance movements and the corresponding input music are considered to be well-matched if the discriminator cannot distinguish the generated movements from the training samples according to the estimated probability. By adding motion consistency constraints in our loss function, the proposed framework is able to create long realistic dance sequences. To alleviate the problem of expensive and inefficient data collection, we propose an effective approach to create a large-scale dataset, YouTube-Dance3D, from open data source. Extensive experiments on currently available music-dance datasets and our YouTube-Dance3D dataset demonstrate that our approach effectively captures the correlation between music and dance and can be used to choreograph appropriate dance sequences.
C1 [Sun, Guofei; Geng, Weidong; Li, Xiangdong] Zhejiang Univ, Coll Comp Sci & Technol, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
   [Wong, Yongkang; Kankanhalli, Mohan S.] Natl Univ Singapore, Sch Comp, Singapore 119613, Singapore.
   [Cheng, Zhiyong] Qilu Univ Technol, Shandong Comp Sci Ctr, Natl Supercomp Ctr Jinan, Shandong Acad Sci, Jinan 250353, Peoples R China.
C3 Zhejiang University; National University of Singapore; Qilu University
   of Technology
RP Geng, WD; Li, XD (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
EM guofeisun@zju.edu.cn; yongkang.wong@nus.edu.sg;
   jason.zy.cheng@gmail.com; mohan@comp.nus.edu.sg; gengwd@zju.edu.cn;
   axli@zju.edu.cn
RI Kankanhalli, Mohan/Q-9284-2019; sun, guofei/ITT-6545-2023
OI Kankanhalli, Mohan/0000-0002-4846-2015; sun, guofei/0000-0002-0739-0844;
   Wong, Yongkang/0000-0002-1239-4428
FU National Natural Science Foundation of China [61379067]; National
   Research Foundation, Singapore under its Strategic Capability Research
   Centres Funding Initiative
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61379067, and in part by the National
   Research Foundation, Singapore under its Strategic Capability Research
   Centres Funding Initiative. Any opinions, findings and conclusions or
   recommendations expressed in thismaterial are those of the author(s) and
   do not reflect the views of National Research Foundation, Singapore. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Chi-Chun Lee.
CR Alemi O., 2017, NETWORKS, V8, P1
   [Anonymous], 2016, arXiv
   [Anonymous], 2017, P IEEE C COMPUTER VI
   Aristidou A, 2015, COMPUT GRAPH FORUM, V34, P262, DOI 10.1111/cgf.12598
   Aristidou A, 2015, ACM J COMPUT CULT HE, V8, DOI 10.1145/2755566
   Augello A, 2016, ROBOT AUTON SYST, V86, P128, DOI 10.1016/j.robot.2016.09.012
   CALVERT TW, 1993, HUMAN FACTORS IN COMPUTING SYSTEMS, P115
   Cardle M, 2002, 20TH EUROGRAPHICS UK CONFERENCE, PROCEEDINGS, P38, DOI 10.1109/EGUK.2002.1011270
   Chen X., 2014, Advances in neural information processing systems, P1736, DOI DOI 10.1109/CVPR.2018.00742
   Clements L., 2018, FRONTIERS PSYCHOL, V9, P1
   Crnkovic-Friis L., 2016, P 7 INT C COMP CREAT, P272, DOI DOI 10.48550/ARXIV.1605.06921
   Du Y, 2016, LECT NOTES COMPUT SC, V9908, P20, DOI 10.1007/978-3-319-46493-0_2
   Fan RK, 2012, IEEE T VIS COMPUT GR, V18, P501, DOI 10.1109/TVCG.2011.73
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494
   Fukayama Satoru, 2015, P SMC, P177
   Ganin Y, 2016, J MACH LEARN RES, V17
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Grassia F. S., 1998, J. Graph. Tools, V6, DOI [10.1080/10867651.1998.10487493, DOI 10.1080/10867651.1998.10487493]
   Han TT, 2017, IEEE T MULTIMEDIA, V19, P712, DOI 10.1109/TMM.2016.2631881
   Hearst MA, 1998, IEEE INTELL SYST APP, V13, P18, DOI 10.1109/5254.708428
   Heusel M., 2017, ADV NEURAL INFORM PR, P6626
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   Kadu H, 2014, IEEE T MULTIMEDIA, V16, P2191, DOI 10.1109/TMM.2014.2360793
   Ke QH, 2017, IEEE SIGNAL PROC LET, V24, P731, DOI 10.1109/LSP.2017.2690339
   Kim JW, 2009, COMPUT ANIMAT VIRT W, V20, P375, DOI 10.1002/cav.314
   Kim TH, 2003, ACM T GRAPHIC, V22, P392, DOI 10.1145/882262.882283
   Kingma D. P., 2015, P INT C LEARN REPR I, P1
   Kirsh D., 2011, Proceedings of the 2nd International Conference on Computational Creativity, P141
   Krüger B, 2017, IEEE T MULTIMEDIA, V19, P797, DOI 10.1109/TMM.2016.2635030
   LaViers A, 2012, P AMER CONTR CONF, P4327
   Lee H. Y., 2019, P ADV NEUR INF PROC, P3581
   Lee Juheon, 2018, ARXIV181100818
   Lee M, 2013, MULTIMED TOOLS APPL, V62, P895, DOI 10.1007/s11042-012-1288-5
   Liu Ming Yu, 2016, ADV NEURAL INF PROCE, P469
   Malloch Stephen., 2005, THINKING 4 DIMENSION, P14
   Manfré A, 2016, BIOL INSPIR COGN ARC, V17, P12, DOI 10.1016/j.bica.2016.07.004
   Manfrè A, 2016, BIOL INSPIR COGN ARC, V15, P1, DOI 10.1016/j.bica.2015.09.009
   Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497
   McFee B., 2015, P PYTH SCI C AUST TX, P18, DOI 10.25080/Majora-7b98e3ed-003
   Nahrstedt K., 2008, AAAI SPRING S CREATI, P53
   Nopparit Suthasinee, 2013, 2013 International Conference on Information Technology and Electrical Engineering (ICITEE), P8, DOI 10.1109/ICITEED.2013.6676202
   Ofli F, 2012, IEEE T MULTIMEDIA, V14, P747, DOI 10.1109/TMM.2011.2181492
   Oore S, 2006, WSCG 2006: FULL PAPERS PROCEEDINGS, P201
   Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794
   Pavllo Dario, 2018, BRIT MACH VIS C, DOI [10.1109/HUMANOIDS.2018.8624922, DOI 10.1109/HUMANOIDS.2018.8624922]
   Pfister T, 2015, IEEE I CONF COMP VIS, P1913, DOI 10.1109/ICCV.2015.222
   Qin R., 2018, INT J HUMANOID ROBOT, V15, P1
   Radford A, 2016, 4 INT C LEARNING REP
   Risner Doug., 2000, RES DANC EDUC, V1, P155
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Shiratori T, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P857, DOI 10.1109/AFGR.2004.1301641
   Shiratori T., 2008, Information and Media Technologies, V3, P834
   Solomon ELS, 2017, MINERAL MET MAT SER, P349, DOI 10.1007/978-3-319-52392-7_50
   Sonnhammer E L, 1998, Proc Int Conf Intell Syst Mol Biol, V6, P175
   Tang TR, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1598, DOI 10.1145/3240508.3240526
   Taylor GW, 2007, ADV NEURAL INFORM PR, P1345, DOI DOI 10.7551/MITPRESS/7503.003.0173
   Todd NPM, 2002, PSYCHOL RES-PSYCH FO, V66, P26, DOI 10.1007/s004260100071
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wittmann M., 2000, Music Sci, V3, P13, DOI DOI 10.1177/10298649000030S103
   Xia G., 2012, P 11 INT C AUTONOMOU, V1, P205
   Xu JF, 2011, ELECTRON J QUAL THEO, P1
   Yalta N., 2016, TECH REP, P43
   Yalta Nelson, 2019, 2019 International Joint Conference on Neural Networks (IJCNN), P1
   Yan YC, 2019, IEEE T MULTIMEDIA, V21, P1799, DOI 10.1109/TMM.2018.2885235
   Yang B, 2019, IEEE T PATTERN ANAL, V41, P2820, DOI 10.1109/TPAMI.2018.2868195
   Yang W, 2018, PROC CVPR IEEE, P5255, DOI 10.1109/CVPR.2018.00551
   Yue N., 2017, P INT C HUM SCI MAN
   Zhang XH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P926, DOI 10.1145/3343031.3351052
   Zhu J.-Y., 2017, IEEE I CONF COMP VIS, P2223, DOI DOI 10.1109/ICCV.2017.244
NR 72
TC 42
Z9 46
U1 3
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 497
EP 509
DI 10.1109/TMM.2020.2981989
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600039
DA 2024-07-18
ER

PT J
AU Nguyen, TT
   Nguyen, TP
   Bouchara, F
AF Thanh Tuan Nguyen
   Thanh Phuong Nguyen
   Bouchara, Frederic
TI Prominent Local Representation for Dynamic Textures Based on High-Order
   Gaussian-Gradients
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic texture; pattern recognition; Gaussian-based filter; Gaussian
   derivative; CLBP; LBP; video representation
ID BINARY PATTERNS; VIDEO; CLASSIFICATION; SPACE; COUNT; SCALE
AB Understanding dynamic textures (DTs) is a challenge in various computer vision applications due to the negative impacts of noise, changes of environment, illumination, and scales on capturing turbulent characteristics. In this work, we propose an efficient shallow framework for DT representation by addressing the following novel concepts. First, it is the first time in DT analysis that 2D/3D Gaussian-gradient filterings are taken into account as a pre-processing step to point out robust components against those influences in effect. Second, high-order partial derivatives of the Gaussian kernels and their informative magnitudes are exploited to forcefully capture multi-order Gaussian-gradient features. Third, these gradient kernels are investigated in multiscale analysis of different orders and standard deviations in order to enrich more useful scale-gradient information. Finally, the obtained complementary components are shallowly encoded using a simple local operator to construct robust descriptors of Highorder 2D/3D Gaussian-gradient-based Features (HoGF2D/3D) against the well-known issues of DT description. Experiments for DTclassification on various benchmarks have validated the interest of our approach since its performance is comparable to state-of-theart results, including that of deep-learning methods, while it only has a small dimension.
C1 [Thanh Tuan Nguyen] HCMC Univ Technol & Educ, Fac IT, Ho Chi Minh City, Vietnam.
   [Thanh Tuan Nguyen; Thanh Phuong Nguyen; Bouchara, Frederic] Univ Toulon & Var, Aix Marseille Univ, CNRS, LIS, F-13007 Marseille, France.
C3 HCMC University of Technology & Education (HCMUTE); Centre National de
   la Recherche Scientifique (CNRS); Universite de Toulon; Aix-Marseille
   Universite
RP Nguyen, TP (corresponding author), Univ Toulon & Var, Aix Marseille Univ, CNRS, LIS, F-13007 Marseille, France.
EM tuannt@hcmute.edu.vn; tpnguyen@univ-tln.fr; bouchara@univ-tln.fr
OI NGUYEN, Thanh Tuan/0000-0002-5210-6152; Nguyen, Thanh
   Phuong/0000-0002-5646-8505
CR Andrearczyk V, 2018, PATTERN RECOGN, V76, P36, DOI 10.1016/j.patcog.2017.10.030
   Arashloo SR, 2017, J VIS COMMUN IMAGE R, V43, P89, DOI 10.1016/j.jvcir.2016.12.015
   Arashloo SR, 2014, IEEE T MULTIMEDIA, V16, P2099, DOI 10.1109/TMM.2014.2362855
   Baktashmotlagh M, 2014, IEEE T PATTERN ANAL, V36, P2353, DOI 10.1109/TPAMI.2014.2339851
   Barmpoutis P, 2014, EUR SIGNAL PR CONF, P1078
   Chan AB, 2007, PROC CVPR IEEE, P208
   Chan AB, 2008, IEEE T PATTERN ANAL, V30, P909, DOI 10.1109/TPAMI.2007.70738
   Dubois S, 2015, SIGNAL IMAGE VIDEO P, V9, P819, DOI 10.1007/s11760-013-0532-4
   Fan KC, 2014, IEEE T IMAGE PROCESS, V23, P2877, DOI 10.1109/TIP.2014.2321495
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Ghanem B., 2010, LECT NOTES COMPUT SC, P223
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Hong S, 2018, NEUROCOMPUTING, V273, P611, DOI 10.1016/j.neucom.2017.08.046
   Ji H, 2013, IEEE T IMAGE PROCESS, V22, P286, DOI 10.1109/TIP.2012.2214040
   Lu Z, 2005, IEEE IJCNN, P2417
   Mettes P, 2017, COMPUT VIS IMAGE UND, V154, P182, DOI 10.1016/j.cviu.2016.04.003
   Mumtaz A, 2013, IEEE T PATTERN ANAL, V35, P1606, DOI 10.1109/TPAMI.2012.236
   Nguyen TT, 2020, IET COMPUT VIS, V14, P162, DOI 10.1049/iet-cvi.2019.0455
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Peh CH, 2002, IEEE T IMAGE PROCESS, V11, P1179, DOI 10.1109/TIP.2002.804265
   Péteri R, 2005, LECT NOTES COMPUT SC, V3523, P223
   Péteri R, 2010, PATTERN RECOGN LETT, V31, P1627, DOI 10.1016/j.patrec.2010.05.009
   Phan BTN, 2017, APPL NANOSCI, V7, P1, DOI 10.1007/s13204-016-0541-z
   Qi XB, 2016, NEUROCOMPUTING, V171, P1230, DOI 10.1016/j.neucom.2015.07.071
   Quan YH, 2017, COMPUT VIS IMAGE UND, V165, P85, DOI 10.1016/j.cviu.2017.10.008
   Quan YH, 2016, PROC CVPR IEEE, P308, DOI 10.1109/CVPR.2016.40
   Quan YH, 2015, IEEE I CONF COMP VIS, P73, DOI 10.1109/ICCV.2015.17
   Rivera AR, 2015, IEEE T PATTERN ANAL, V37, P2146, DOI 10.1109/TPAMI.2015.2392774
   Ravichandran A, 2009, PROC CVPR IEEE, P1651, DOI 10.1109/CVPRW.2009.5206847
   Ren JF, 2014, IEEE SIGNAL PROC LET, V21, P1346, DOI 10.1109/LSP.2014.2336252
   Ren JF, 2013, INT CONF ACOUST SPEE, P2400, DOI 10.1109/ICASSP.2013.6638085
   Saisan P, 2001, PROC CVPR IEEE, P58
   Song TC, 2019, IEEE IMAGE PROC, P4405, DOI [10.1109/ICIP.2019.8803518, 10.1109/icip.2019.8803518]
   Nguyen TP, 2016, PATTERN RECOGN LETT, V80, P91, DOI 10.1016/j.patrec.2016.06.003
   Nguyen TP, 2016, NEUROCOMPUTING, V173, P1565, DOI 10.1016/j.neucom.2015.09.029
   Nguyen TP, 2014, INT J PATTERN RECOGN, V28, DOI 10.1142/S0218001414600118
   Nguyen TT, 2020, PATTERN RECOGN LETT, V135, P180, DOI 10.1016/j.patrec.2020.04.007
   Nguyen TT, 2019, LECT NOTES COMPUT SC, V11678, P155, DOI 10.1007/978-3-030-29888-3_13
   Nguyen TT, 2020, COMPUT VIS IMAGE UND, V194, DOI 10.1016/j.cviu.2019.102882
   Thanh Tuan Nguyen, 2020, Advanced Concepts for Intelligent Vision Systems. 20th International Conference, ACIVS 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12002), P277, DOI 10.1007/978-3-030-40605-9_24
   Nguyen TT, 2019, IEEE IMAGE PROC, P4400, DOI [10.1109/icip.2019.8803449, 10.1109/ICIP.2019.8803449]
   Nguyen TT, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.5.053044
   Nguyen TT, 2018, LECT NOTES COMPUT SC, V11182, P74, DOI 10.1007/978-3-030-01449-0_7
   Tiwari D, 2017, MULTIMED TOOLS APPL, V76, P6623, DOI 10.1007/s11042-016-3362-x
   Tiwari D, 2017, COMPUT ELECTR ENG, V62, P485, DOI 10.1016/j.compeleceng.2016.11.008
   Tiwari D, 2016, COMPUT VIS IMAGE UND, V150, P58, DOI 10.1016/j.cviu.2016.04.010
   Tiwari D, 2016, MULTIDIM SYST SIGN P, V27, P563, DOI 10.1007/s11045-015-0319-6
   Wang Y, 2016, SOFT COMPUT, V20, P1977, DOI 10.1007/s00500-015-1618-4
   Xu Y, 2015, PATTERN RECOGN, V48, P3239, DOI 10.1016/j.patcog.2015.04.015
   Xu Y, 2012, COMPUT VIS IMAGE UND, V116, P999, DOI 10.1016/j.cviu.2012.05.003
   Xu Y, 2011, IEEE I CONF COMP VIS, P1219, DOI 10.1109/ICCV.2011.6126372
   Nguyen XS, 2018, MULTIMED TOOLS APPL, V77, P8531, DOI 10.1007/s11042-017-4749-z
   Zhang BC, 2010, IEEE T IMAGE PROCESS, V19, P533, DOI 10.1109/TIP.2009.2035882
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao XC, 2019, IEEE T MULTIMEDIA, V21, P1694, DOI 10.1109/TMM.2018.2890362
   Zhao XC, 2018, IEEE T MULTIMEDIA, V20, P552, DOI 10.1109/TMM.2017.2750415
   Zhao Y, 2012, IEEE T IMAGE PROCESS, V21, P4492, DOI 10.1109/TIP.2012.2204271
NR 57
TC 9
Z9 9
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1367
EP 1382
DI 10.1109/TMM.2020.2997202
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RU3SE
UT WOS:000645068200015
DA 2024-07-18
ER

PT J
AU Valsesia, D
   Fracastoro, G
   Magli, E
AF Valsesia, Diego
   Fracastoro, Giulia
   Magli, Enrico
TI Learning Localized Representations of Point Clouds With
   Graph-Convolutional Generative Adversarial Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Convolution; Generators; Gallium nitride;
   Generative adversarial networks; Feature extraction; Data models;
   Generative adversarial networks; graph convolution; point clouds
AB Point clouds are an important type of geometric data generated by 3D acquisition devices, and have widespread use in computer graphics and vision. However, learning representations for point clouds is particularly challenging due to their nature as being an unordered collection of points irregularly distributed in 3D space. Recently, supervised and semisupervised problems for point clouds leveraged graph convolution, a generalization of the convolution operation for data defined over graphs. This operation has been shown to be very successful at extracting localized features from point clouds. In this paper, we study the unsupervised problem of a generative model exploiting graph convolution. Employing graph convolution operations in generative models is not straightforward and it poses some unique challenges. In particular, we focus on the generator of a GAN, where the graph is not known in advance as it is the very output of the generator. We show that the proposed architecture can learn to generate the graph and the features simultaneously. We also study the problem of defining an upsampling layer in the graph-convolutional generator, proposing two methods that respectively learn to exploit a multi-resolution or self-similarity prior to sample the data distribution.
C1 [Valsesia, Diego; Fracastoro, Giulia; Magli, Enrico] Politecn Torino, Dept Elect & Telecommun, I-10129 Turin, Italy.
C3 Polytechnic University of Turin
RP Valsesia, D (corresponding author), Politecn Torino, Dept Elect & Telecommun, I-10129 Turin, Italy.
EM diego.valsesia@polito.it; giulia.fracastoro@polito.it;
   enrico.magli@polito.it
RI VALSESIA, DIEGO/U-3135-2019
OI VALSESIA, DIEGO/0000-0003-1997-2910; MAGLI, ENRICO/0000-0002-0901-0251;
   fracastoro, giulia/0000-0001-8495-1097
FU Smart-Data@PoliTO center for Big Data and Machine Learning technologies
FX This work was supported by the Smart-Data@PoliTO center for Big Data and
   Machine Learning technologies.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Achlioptas P., 2018, P INT C LEARN REPR
   Alipanahi B, 2015, NAT BIOTECHNOL, V33, P831, DOI 10.1038/nbt.3300
   [Anonymous], 2015, DEEP CONVOLUTIONAL N
   [Anonymous], 2018, ICLR
   [Anonymous], 2017, BEGAN BOUNDARY EQUIL
   [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   [Anonymous], 2015, 2015 IEEE INT C COMP, DOI DOI 10.1109/ICCVW.2015.112
   [Anonymous], 2017, 31 INT CONFNEURAL IN
   [Anonymous], 2014, AUTOENCODING VARIATI
   [Anonymous], 2017, ARXIV170400028CSLG
   Arjovsky M, 2017, arXiv preprint arXiv:1701.07875
   Arjovsky Martin, 2017, P INT C LEARN REPR
   Boscaini D, 2016, ADV NEUR IN, V29
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Chang JHR, 2017, IEEE I CONF COMP VIS, P5889, DOI 10.1109/ICCV.2017.627
   Chen JY, 2014, IEEE T MULTIMEDIA, V16, P337, DOI 10.1109/TMM.2013.2286580
   Defferrard M., 2016, P 30 INT C NEURAL IN, V29, P3844
   Denton E.L., 2015, CoRR, P1486
   Duvenaudt D, 2015, ADV NEUR IN, V28
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Funkhouser T.A., 2015, P COMP VIS PATT REC
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Goyal P, 2018, KNOWL-BASED SYST, V151, P78, DOI 10.1016/j.knosys.2018.03.022
   Grover A, 2019, PR MACH LEARN RES, V97
   Kipf T. N., 2017, P INT C LEAR REPR
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Liang XD, 2017, IEEE I CONF COMP VIS, P1762, DOI 10.1109/ICCV.2017.194
   Litany O, 2018, PROC CVPR IEEE, P1886, DOI 10.1109/CVPR.2018.00202
   Magli E, 2019, P INT C LEARN REPR
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Nash C, 2017, COMPUT GRAPH FORUM, V36, P1, DOI 10.1111/cgf.13240
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Radford A, 2016, 4 INT C LEARNING REP
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Schlichtkrull Michael, 2018, PROC EUR SEMANTIC WE, P593
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Simonovsky M, 2017, PROC CVPR IEEE, P29, DOI 10.1109/CVPR.2017.11
   Song G, 2019, IEEE T MULTIMEDIA, V21, P1261, DOI 10.1109/TMM.2018.2877122
   Wang H., 2017, ARXIV171108267
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu PF, 2017, IEEE T MULTIMEDIA, V19, P266, DOI 10.1109/TMM.2016.2612761
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Zhang H, 2019, IEEE T IMAGE PROCESS, V28, P4486, DOI 10.1109/TIP.2019.2910398
   Zhang X., 2018, 2018 IEEE 4 INT C MU, P1, DOI DOI 10.1109/BIGMM.2018.8499105
NR 48
TC 21
Z9 23
U1 3
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 402
EP 414
DI 10.1109/TMM.2020.2976627
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600032
DA 2024-07-18
ER

PT J
AU Wang, YX
   Xie, HT
   Zha, ZJ
   Tian, YL
   Fu, ZL
   Zhang, YD
AF Wang, Yuxin
   Xie, Hongtao
   Zha, Zhengjun
   Tian, Youliang
   Fu, Zilong
   Zhang, Yongdong
TI R-Net: A Relationship Network for Efficient and Accurate Scene Text
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Bidirectional control; Semantics; Convolutional
   codes; Task analysis; Visualization; Decoding; Scene text detection;
   large-variance scale; convolutional neural network (CNN)
ID TRACKING
AB This paper introduces a novel bi-directional con-volutional framework to cope with the large-variance scale problem in scene text detection. Due to the lack of scale normalization in recent CNN-based methods, text instances with large-variance scale are activated inconsistently in feature maps, which makes it hard for CNN-based methods to accurately locate multi-size text instances. Thus, we propose the relationship network (R-Net) that maps multi-scale convolutional features to a scale-invariant space to obtain consistent activation of multi-size text instances. Firstly, we implement an FPN-like backbone with a Spatial Relationship Module (SPM) to extract multi-scale features with powerful spatial semantics. Then, a Scale Relationship Module (SRM) constructed on feature pyramid propagates contextual scale information in sequential features through a bi-directional convolutional operation. SRM supplements the multi-scale information in different feature maps to obtain consistent activation of multi-size text instances. Compared with previous approaches, R-Net effectively handles the large-variance scale problem without complicated post processing and complex hand-crafted hyperparameter setting. Extensive experiments conducted on several benchmarks verify that our R-Net obtains state-of-the-art performance on both accuracy and efficiency. More specifically, R-Net achieves an F-measure of 85.6% at 21.4 frames/s and an F-measure of 81.7% at 11.8 frames/s for ICDAR 2015 and MSRA-TD500 datasets respectively, which is the latest SOTA. The code is available on https://github.com/wangyuxin87/R-Net.
C1 [Wang, Yuxin; Xie, Hongtao; Zha, Zhengjun; Fu, Zilong; Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
   [Tian, Youliang] Guizhou Univ, Guiyang 550025, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Guizhou University
RP Xie, HT; Zhang, YD (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
EM wangyx58@mail.ustc.edu.cn; htxie@ustc.edu.cn; zhazj@ustc.edu.cn;
   yltian@gzu.edu.cn; jeromef@mail.ustc.edu.cn; zhyd73@ustc.edu.cn
RI Zha, Zheng-Jun/AAF-8667-2020
OI Tian, Youliang/0000-0002-5974-1570; Wang, Yuxin/0000-0002-0228-6220
FU Nationa lKey Research and Development Program of China [2017YFC0820600];
   National Nature Science Foundation of China [61525206, 61771468]; Youth
   Innovation Promotion Association Chinese Academy of Sciences [2017209]
FX This work was supported in part by the Nationa lKey Research and
   Development Program of China under Grant 2017YFC0820600, in part by the
   National Nature Science Foundation of China under Grants 61525206 and
   61771468, and in part by the Youth Innovation Promotion Association
   Chinese Academy of Sciences under Grant 2017209. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Lamberto Ballan.
CR [Anonymous], 2014, P INT C LEARN REPR
   Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959
   Bouman KL, 2011, IEEE T MULTIMEDIA, V13, P922, DOI 10.1109/TMM.2011.2154317
   Chen CL, 2019, IEEE T MULTIMEDIA, V21, P3205, DOI 10.1109/TMM.2019.2916104
   Deng D, 2018, AAAI CONF ARTIF INTE, P6773
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He P, 2017, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2017.331
   He T, 2018, PROC CVPR IEEE, P5020, DOI 10.1109/CVPR.2018.00527
   He T, 2016, IEEE T IMAGE PROCESS, V25, P2529, DOI 10.1109/TIP.2016.2547588
   He WH, 2017, IEEE I CONF COMP VIS, P745, DOI 10.1109/ICCV.2017.87
   Karaoglu S, 2017, IEEE T MULTIMEDIA, V19, P1063, DOI 10.1109/TMM.2016.2638622
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Liao MH, 2021, IEEE T PATTERN ANAL, V43, P532, DOI 10.1109/TPAMI.2019.2937086
   Liao MH, 2018, PROC CVPR IEEE, P5909, DOI 10.1109/CVPR.2018.00619
   Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107
   Liao MH, 2017, AAAI CONF ARTIF INTE, P4161
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu AA, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P821
   Liu AA, 2017, IEEE T PATTERN ANAL, V39, P102, DOI 10.1109/TPAMI.2016.2537337
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu XQ, 2012, IEEE T MULTIMEDIA, V14, P482, DOI 10.1109/TMM.2011.2177646
   Liu XB, 2018, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2018.00595
   Liu YL, 2019, PROC CVPR IEEE, P9604, DOI 10.1109/CVPR.2019.00984
   Liu YL, 2019, PATTERN RECOGN, V90, P337, DOI 10.1016/j.patcog.2019.02.002
   Liu ZC, 2019, PROC CVPR IEEE, P7261, DOI 10.1109/CVPR.2019.00744
   Liu ZC, 2018, PROC CVPR IEEE, P6936, DOI 10.1109/CVPR.2018.00725
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Lyu PY, 2018, PROC CVPR IEEE, P7553, DOI 10.1109/CVPR.2018.00788
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Nayef N, 2017, PROC INT CONF DOC, P1454, DOI 10.1109/ICDAR.2017.237
   Pang YW, 2019, PROC CVPR IEEE, P7328, DOI 10.1109/CVPR.2019.00751
   Ren XH, 2017, IEEE T MULTIMEDIA, V19, P506, DOI 10.1109/TMM.2016.2625259
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Shi BG, 2017, PROC INT CONF DOC, P1429, DOI 10.1109/ICDAR.2017.233
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Tang H, 2019, PROC CVPR IEEE, P2412, DOI 10.1109/CVPR.2019.00252
   Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4
   Tian ZT, 2019, PROC CVPR IEEE, P4229, DOI 10.1109/CVPR.2019.00436
   Wang FF, 2018, PROC CVPR IEEE, P1381, DOI 10.1109/CVPR.2018.00150
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956
   Wang XB, 2019, PROC CVPR IEEE, P6442, DOI 10.1109/CVPR.2019.00661
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang YX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P947
   Wu L, 2015, IEEE T MULTIMEDIA, V17, P1137, DOI 10.1109/TMM.2015.2443556
   Xie HT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231737
   Xue CH, 2018, LECT NOTES COMPUT SC, V11220, P370, DOI 10.1007/978-3-030-01270-0_22
   Yao C, 2014, IEEE T IMAGE PROCESS, V23, P4737, DOI 10.1109/TIP.2014.2353813
   Yin XC, 2016, IEEE T IMAGE PROCESS, V25, P2752, DOI 10.1109/TIP.2016.2554321
   Zeng XY, 2016, LECT NOTES COMPUT SC, V9911, P354, DOI 10.1007/978-3-319-46478-7_22
   Zhang Z, 2016, PROC CVPR IEEE, P4159, DOI 10.1109/CVPR.2016.451
   Zhou P, 2018, PROC CVPR IEEE, P528, DOI 10.1109/CVPR.2018.00062
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
   Zhou YZ, 2017, PROC CVPR IEEE, P4961, DOI 10.1109/CVPR.2017.527
NR 58
TC 24
Z9 26
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1316
EP 1329
DI 10.1109/TMM.2020.2995290
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RU3SE
UT WOS:000645068200011
DA 2024-07-18
ER

PT J
AU Wei, DX
   Xu, XW
   Shen, HB
   Huang, KJ
AF Wei, Dongxu
   Xu, Xiaowei
   Shen, Haibin
   Huang, Kejie
TI GAC-GAN: A General Method for Appearance-Controllable Human Video Motion
   Transfer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gallium nitride; Layout; Generative adversarial networks; Training;
   Three-dimensional displays; Computational modeling; Solid modeling;
   Motion transfer; video generation; image synthesis; generative
   adversarial networks (GANs)
AB Human video motion transfer has a wide range of applications in multimedia, computer vision, and graphics. Recently, due to the rapid development of Generative Adversarial Networks (GANs), there has been significant progress in the field. However, almost all existing GAN-based works are prone to address the mapping from human motions to video scenes, with scene appearances encoded individually in the trained models. Therefore, each trained model can only generate videos with a specific scene appearance, and new models are required to be trained to generate new appearances. Besides, existing works lack the capability of appearance control. For example, users have to provide video records of wearing new clothes or performing in new backgrounds to enable clothes or background changing in their synthetic videos, which greatly limits the application flexibility. In this paper, we propose General Appearance-Controllable GAN (GAC-GAN), a general method for appearance-controllable human video motion transfer. To enable general-purpose appearance synthesis, we propose to include appearance information in the conditioning inputs. Thus, once trained, our model can generate new appearances by altering the input appearance information. To achieve appearance control, we first obtain the appearance-controllable conditioning inputs, and then utilize a two-stage GAC-GAN to generate the corresponding appearance-controllable outputs, where we utilize an Appearance-Consistency GAN (ACGAN) loss, and a shadow extraction module for output foreground, and background appearance control respectively. We further build a solo dance dataset containing a large number of dance videos for training, and evaluation. Experimental results on our solo dance dataset, and iPER dataset show that our proposed GAC-GAN can not only support appearance-controllable human video motion transfer but also achieve higher video quality than state-of-art methods.
C1 [Wei, Dongxu; Shen, Haibin; Huang, Kejie] Zhejiang Univ, Dept Informat Sci & Elect Engn, Hangzhou 310027, Peoples R China.
   [Xu, Xiaowei] Guangdong Acad Med Sci, Guangdong Prov Peoples Hosp, Guangzhou 510080, Peoples R China.
C3 Zhejiang University; Guangdong Academy of Medical Sciences & Guangdong
   General Hospital; Southern Medical University - China
RP Huang, KJ (corresponding author), Zhejiang Univ, Dept Informat Sci & Elect Engn, Hangzhou 310027, Peoples R China.
EM tracywei@zju.edu.cn; xiao.wei.xu@foxmail.com; shen_hb@zju.edu.cn;
   huangkejie@zju.edu.cn
RI , 黄科杰/J-5919-2019; Huang, Kejie/E-7511-2018
OI Huang, Kejie/0000-0003-3722-9979
FU National Natural Science Foundation of China [U19B2043]
FX This work was supported by the National Natural Science Foundation of
   China under Grant U19B2043.
CR Aberman K, 2019, COMPUT GRAPH FORUM, V38, P219, DOI 10.1111/cgf.13632
   Balakrishnan G, 2018, PROC CVPR IEEE, P8340, DOI 10.1109/CVPR.2018.00870
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chan C, 2019, IEEE I CONF COMP VIS, P5932, DOI 10.1109/ICCV.2019.00603
   Chen XT, 2020, IEEE T MULTIMEDIA, V22, P1591, DOI 10.1109/TMM.2019.2946475
   Cheung GKM, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P373, DOI 10.1109/TDPVT.2004.1335262
   Denton Emily, 2015, Advances in Neural Information Processing Systems
   Dosovitskiy A., 2017, P 1 ANN C ROB LEARN, P1, DOI DOI 10.48550/ARXIV.1711.03938
   Dosovitskiy A., 2016, Advances in Neural Information Processing Systems, P658
   Efros AA, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P726
   Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352
   Hecker C, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360626
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D. P., 2014, arXiv
   Lee J, 1999, COMP GRAPH, P39
   Leroy V, 2017, IEEE I CONF COMP VIS, P3113, DOI 10.1109/ICCV.2017.336
   Liang D, 2019, AAAI CONF ARTIF INTE, P8698
   Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063
   Liang XD, 2017, IEEE I CONF COMP VIS, P1762, DOI 10.1109/ICCV.2017.194
   Liu LJ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3333002
   Liu M.-Y., 2017, NIPS
   Liu W, 2019, IEEE I CONF COMP VIS, P5903, DOI 10.1109/ICCV.2019.00600
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mathieu Michael, 2016, ICLR, DOI DOI 10.48550/ARXIV.1511.05440
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Miyato T., 2018, Proceedings of the 6th International Conference on Learning Representations, P1
   Neverova N, 2018, LECT NOTES COMPUT SC, V11207, P128, DOI 10.1007/978-3-030-01219-9_8
   Odena A, 2017, PR MACH LEARN RES, V70
   PONSMOLL G, 2017, ACM T GRAPHIC, V36, P1, DOI DOI 10.1145/3072959.3073711
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Saito M, 2017, IEEE I CONF COMP VIS, P2849, DOI 10.1109/ICCV.2017.308
   Schödl A, 2000, COMP GRAPH, P489, DOI 10.1145/344779.345012
   Schodl A., 2002, SCA 02 P 2002 ACM SI, P121, DOI DOI 10.1145/545261.545281
   Shysheya A, 2019, PROC CVPR IEEE, P2382, DOI 10.1109/CVPR.2019.00249
   Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359
   Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165
   Wang M, 2019, PROC CVPR IEEE, P1495, DOI 10.1109/CVPR.2019.00159
   Wang T., 2018, ARXIV
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang W., 2020, IEEE T MULTIMEDIA
   Xu WP, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3181973
   Yan YC, 2019, IEEE T MULTIMEDIA, V21, P1799, DOI 10.1109/TMM.2018.2885235
   Zanfir M, 2018, PROC CVPR IEEE, P5391, DOI 10.1109/CVPR.2018.00565
   Zhang C, 2017, PROC CVPR IEEE, P5484, DOI 10.1109/CVPR.2017.582
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou YN, 2019, IEEE INT CONF COMP V, P388, DOI 10.1109/ICCVW.2019.00050
NR 51
TC 13
Z9 13
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2457
EP 2470
DI 10.1109/TMM.2020.3011290
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800023
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zheng, ZD
   Ruan, T
   Wei, YC
   Yang, Y
   Mei, T
AF Zheng, Zhedong
   Ruan, Tao
   Wei, Yunchao
   Yang, Yi
   Mei, Tao
TI VehicleNet: Learning Robust Visual Representation for Vehicle
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Robustness; Adaptation models; Data models; Automobiles;
   Cameras; Feature extraction; Vehicle re-identification; image
   representation; convolutional neural networks
ID PERSON REIDENTIFICATION
AB One fundamental challenge of vehicle re-identification (re-id) is to learn robust and discriminative visual representation, given the significant intra-class vehicle variations across different camera views. As the existing vehicle datasets are limited in terms of training images and viewpoints, we propose to build a unique large-scale vehicle dataset (called VehicleNet) by harnessing four public vehicle datasets, and design a simple yet effective two-stage progressive approach to learning more robust visual representation from VehicleNet. The first stage of our approach is to learn the generic representation for all domains (i.e., source vehicle datasets) by training with the conventional classification loss. This stage relaxes the full alignment between the training and testing domains, as it is agnostic to the target vehicle domain. The second stage is to fine-tune the trained model purely based on the target vehicle set, by minimizing the distribution discrepancy between our VehicleNet and any target domain. We discuss our proposed multi-source dataset VehicleNet and evaluate the effectiveness of the two-stage progressive representation learning through extensive experiments. We achieve the state-of-art accuracy of 86.07% mAP on the private test set of AICity Challenge, and competitive results on two other public vehicle re-id datasets, i.e., VeRi-776 and VehicleID. We hope this new VehicleNet dataset and the learned robust representations can pave the way for vehicle re-id in the real-world environments.
C1 [Zheng, Zhedong; Wei, Yunchao; Yang, Yi] Univ Technol Sydney, Australian Artificial Intelligence Inst, Sydney, NSW 2007, Australia.
   [Ruan, Tao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Ruan, Tao] Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
   [Mei, Tao] AI Res JD COM, Beijing 100105, Peoples R China.
C3 University of Technology Sydney; Beijing Jiaotong University; Beijing
   Jiaotong University
RP Yang, Y (corresponding author), Univ Technol Sydney, Australian Artificial Intelligence Inst, Sydney, NSW 2007, Australia.
EM zhedong.zheng@student.uts.edu.au; 16112064@bjtu.edu.cn;
   yunchao.wei@uts.edu.au; yi.yang@uts.edu.au; tmei@live.com
RI Zheng, Zhedong/R-5314-2019; yang, yang/GWB-9426-2022; Mei,
   Tao/GQZ-0596-2022; yang, yang/GVT-5210-2022; Yang, Yi/B-9273-2017; yang,
   yang/HGT-7999-2022; Lang, Ming/HIK-0758-2022
OI Zheng, Zhedong/0000-0002-2434-9050; Mei, Tao/0000-0002-5990-7307; Yang,
   Yi/0000-0002-0512-880X; Ruan, Tao/0000-0002-6718-7223
CR [Anonymous], 1996, COMPREHENSIVE CHEMOM, DOI DOI 10.1016/B978-044452701-1.00067-3
   [Anonymous], 2018, 32 AAAI C ART INT
   Bai Y, 2018, IEEE T MULTIMEDIA, V20, P2385, DOI 10.1109/TMM.2018.2796240
   Chen B., 2018, P ADV NEUR INF PROC, P1942
   Chu RH, 2019, IEEE I CONF COMP VIS, P8281, DOI 10.1109/ICCV.2019.00837
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding YH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3369393
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He B, 2019, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2019.00412
   He K., 2017, PROC INT C COMPUT VI
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LX, 2018, PROC CVPR IEEE, P7073, DOI 10.1109/CVPR.2018.00739
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Hu J., 2018, PROC COMPUT VIS PATT
   Huang GL, 2017, IEEE ICC
   Huang T.-W., 2019, P COMP VIS PATT REC
   Khorramshahi P, 2019, IEEE I CONF COMP VIS, P6131, DOI 10.1109/ICCV.2019.00623
   Krause J, 2016, LECT NOTES COMPUT SC, V9907, P301, DOI 10.1007/978-3-319-46487-9_19
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Leibe B., 2017, ARXIV170307737CS
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Liu WY, 2016, PR MACH LEARN RES, V48
   Liu XC, 2019, J COMPUT SCI TECH-CH, V34, P634, DOI 10.1007/s11390-019-1932-x
   Liu XC, 2018, IEEE T MULTIMEDIA, V20, P645, DOI 10.1109/TMM.2017.2751966
   Liu XC, 2016, LECT NOTES COMPUT SC, V9906, P869, DOI 10.1007/978-3-319-46475-6_53
   Lou YH, 2019, PROC CVPR IEEE, P3230, DOI 10.1109/CVPR.2019.00335
   Luo YW, 2019, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2019.00261
   Lv K, 2019, P COMP VIS PATT REC
   Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12
   Meng JK, 2019, PROC CVPR IEEE, P760, DOI 10.1109/CVPR.2019.00085
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Qian Jingjing, 2019, ARXIV191005549
   Qian XL, 2017, IEEE I CONF COMP VIS, P5409, DOI 10.1109/ICCV.2017.577
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen YT, 2017, IEEE I CONF COMP VIS, P1918, DOI 10.1109/ICCV.2017.210
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Tang Z, 2019, IEEE I CONF COMP VIS, P211, DOI 10.1109/ICCV.2019.00030
   Tsai YH, 2019, IEEE I CONF COMP VIS, P1456, DOI 10.1109/ICCV.2019.00154
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wang ZD, 2017, IEEE I CONF COMP VIS, P379, DOI 10.1109/ICCV.2017.49
   Wu ZX, 2019, IEEE I CONF COMP VIS, P2121, DOI 10.1109/ICCV.2019.00221
   Yan K, 2017, IEEE I CONF COMP VIS, P562, DOI 10.1109/ICCV.2017.68
   Yang LJ, 2015, PROC CVPR IEEE, P3973, DOI 10.1109/CVPR.2015.7299023
   Yao Yue, 2019, ARXIV191208855
   Yue XY, 2019, IEEE I CONF COMP VIS, P2100, DOI 10.1109/ICCV.2019.00219
   Zapletal D, 2016, IEEE COMPUT SOC CONF, P1568, DOI 10.1109/CVPRW.2016.195
   Zhang D., 2018, IEEE Transactions on Pattern Analysis and Machine Intelligence
   Zhang DW, 2019, INT J COMPUT VISION, V127, P363, DOI 10.1007/s11263-018-1112-4
   Zhang YH, 2017, IEEE INT CON MULTI, P1386, DOI 10.1109/ICME.2017.8019491
   Zhedong Z., 2019, P COMP VIS PATT REC
   Zheng T., 2019, P COMP VIS PATT REC
   Zheng Z., 2020, ACM MULTIMEDIA
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zheng Zhedong, 2019, CVPR
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou Y., 2017, 1 AS AUSTR C PREC PA, P1, DOI DOI 10.5244/C.31.186
   Zhou Y, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00679
   Zhu J, 2019, IEEE T INTELL TRANSP
   Zhu J.-Y, 2017, PROC NEURIPS
   Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI [10.1007/978-3-030-01219-9_, 10.1007/978-3-030-01219-9_18]
NR 69
TC 81
Z9 83
U1 8
U2 42
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2683
EP 2693
DI 10.1109/TMM.2020.3014488
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Delmotte, A
   Tanaka, K
   Kubo, H
   Funatomi, T
   Mukaigawa, Y
AF Delmotte, Arnaud
   Tanaka, Kenichiro
   Kubo, Hiroyuki
   Funatomi, Takuya
   Mukaigawa, Yasuhiro
TI Blind Watermarking for 3-D Printed Objects by Locally Modifying Layer
   Thickness
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Watermarking; Printers; Three-dimensional displays; Resistance;
   Three-dimensional printing; Strain; Blind watermarking; layer thickness;
   metadata; paper scanner; 3D printing
ID LIQUEFIER DYNAMICS; ROBUST
AB We propose a new blind watermarking algorithm for 3D printed objects that has applications in metadata embedding, robotic grasping, counterfeit prevention, and crime investigation. Our method can be used on fused deposition modeling (FDM) 3D printers and works by modifying the printed layer thickness on small patches of the surface of an object. These patches can be applied to multiple regions of the object, thereby making it resistant to various attacks such as cropping, local deformation, local surface degradation, or printing errors. The novelties of our method are the use of the thickness of printed layers as a one-dimensional carrier signal to embed data, the minimization of distortion by only modifying the layers locally, and one-shot detection using a common paper scanner. To correct encoding or decoding errors, our method combines multiple patches and uses a 2D parity check to estimate the error probability of each bit to obtain a higher correction rate than a naive majority vote. The parity bits included in the patches have a double purpose because, in addition to error detection, they are also used to identify the orientation of the patches. In our experiments, we successfully embedded a watermark into flat surfaces of 3D objects with various filament colors using a standard FDM 3D printer, extracted it using a common 2D paper scanner and evaluated the sensitivity to surface degradation and signal amplitude.
C1 [Delmotte, Arnaud; Tanaka, Kenichiro; Kubo, Hiroyuki; Funatomi, Takuya; Mukaigawa, Yasuhiro] Nara Inst Sci & Technol, Opt Media Interface Lab, Nara 6300192, Japan.
C3 Nara Institute of Science & Technology
RP Delmotte, A (corresponding author), Nara Inst Sci & Technol, Opt Media Interface Lab, Nara 6300192, Japan.
EM arnaud.delmotte.zr3@is.naist.jp; ktanaka@is.naist.jp; hkubo@is.naist.jp;
   funatomi@is.naist.jp; mukaigawa@is.naist.jp
RI delmotte, arnaud/AAJ-4011-2020; Kubo, Hiroyuki/AAS-1487-2021; Funatomi,
   Takuya/K-5919-2018
OI delmotte, arnaud/0000-0002-2536-1887; Kubo,
   Hiroyuki/0000-0002-7061-7941; Funatomi, Takuya/0000-0001-5588-5932;
   Mukaigawa, Yasuhiro/0000-0001-8689-3724
FU JSPS KAKEN [JP17K19979]
FX This work was supported by JSPS KAKEN underGrant JP17K19979.
CR 3dHubs, 2018, ONL MAN TRENDS Q4 20
   [Anonymous], 2004, P DETC
   [Anonymous], 1977, THEORY ERROR CORRECT
   Banerjee R., 2012, J GLOBAL RES COMPUT, V3, P17
   Baraniuk C., 2017, WHY PRINTERS ADD SEC
   Bellini A, 2004, J MANUF SCI E-T ASME, V126, P237, DOI 10.1115/1.1688377
   Chiang PJ, 2009, IEEE SIGNAL PROC MAG, V26, P72, DOI 10.1109/MSP.2008.931082
   Delmotte A, 2018, 2018 JOINT 7TH INTERNATIONAL CONFERENCE ON INFORMATICS, ELECTRONICS & VISION (ICIEV) AND 2018 2ND INTERNATIONAL CONFERENCE ON IMAGING, VISION & PATTERN RECOGNITION (ICIVPR), P282, DOI 10.1109/ICIEV.2018.8640986
   EFF, 2005, INV MACH ID COD TECH
   Greeff GP, 2017, ADDIT MANUF, V14, P31, DOI 10.1016/j.addma.2016.12.005
   Hamming R. W., 1986, Coding and Information Theory
   Harrison C, 2012, UIST'12: PROCEEDINGS OF THE 25TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P563
   Hodgson G., 2011, SLIC3RMANUAL FLOWMAT
   Hou J.U., 2015, P 3 ACM WORKSHOP INF, P115
   Hou JU, 2018, IEEE ACCESS, V6, P44082, DOI 10.1109/ACCESS.2018.2864331
   Hou JU, 2017, IEEE T INF FOREN SEC, V12, P2712, DOI 10.1109/TIFS.2017.2718482
   Jiang RQ, 2018, IEEE T MULTIMEDIA, V20, P55, DOI 10.1109/TMM.2017.2723244
   Katiyar A., 2015, Advances in Computer Science and Information Technology, V2, P441
   Kazemi R, 2016, IEEE T MULTIMEDIA, V18, P2345, DOI 10.1109/TMM.2016.2599149
   Konstantinides JM, 2009, IEEE T MULTIMEDIA, V11, P23, DOI 10.1109/TMM.2008.2008913
   Kumar A., 2016, uS Patent, Patent No. [9,400,910, 9400910]
   Li DZY, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P449, DOI 10.1145/3126594.3126635
   Li ZX, 2018, PROCEEDINGS OF THE 2018 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'18), P1306, DOI 10.1145/3243734.3243735
   MacWilliams F. J., 1978, The Theory of Error-Correcting Codes
   Maia HT, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322960
   Medimegh N, 2015, INT J MULTIMEDIA, V1, P1, DOI DOI 10.16966/IJM.102
   Molitch-Hou M., 2016, FUTURE HPS MULTIJET
   Okada A, 2015, PROC SPIE, V9599, DOI 10.1117/12.2189486
   Pandey A, 2018, MATER TODAY-PROC, V5, P12940, DOI 10.1016/j.matpr.2018.02.279
   Reprap, REPR G COD
   Stütz T, 2014, IEEE T MULTIMEDIA, V16, P1337, DOI 10.1109/TMM.2014.2310595
   Suzuki M., 2017, Electronic Imaging, V2017, P6
   Suzuki M., 2016, P 8 INT C ADV MULT, P56
   Suzuki M, 2017, PROCEEDINGS OF 2017 INTERNATIONAL CONFERENCE ON VIDEO AND IMAGE PROCESSING (ICVIP 2017), P146, DOI 10.1145/3177404.3177455
   Tentori M, 2015, IEEE PERVAS COMPUT, V14, P42, DOI 10.1109/MPRV.2015.22
   Voris J., 2017, U.S. Patent, Patent No. [9,656,428, 9656428]
   Wang K, 2008, IEEE T MULTIMEDIA, V10, P1513, DOI 10.1109/TMM.2008.2007350
   Wee J. Y. S., 2015, U.S. Patent App., Patent No. [14/250,533, 14250533]
   Wee Joseph Ying Sen, 2015, US Patent App, Patent No. [14/485,880, 14485880]
   Willis KDD, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461936
   Xia HX, 2018, RAPID PROTOTYPING J, V24, P463, DOI 10.1108/RPJ-12-2016-0217
   Yamamoto H, 2018, PROCEEDINGS OF 2018 INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY AND ITS APPLICATIONS (ISITA2018), P321, DOI 10.23919/ISITA.2018.8664290
   Yamazaki S, 2014, INT C PATT RECOG, P4576, DOI 10.1109/ICPR.2014.783
   Yusuf B., 2018, DIGITALLY AUGMENTED
   Zha ZY, 2018, VISUAL COMPUT, V34, P117, DOI 10.1007/s00371-016-1318-9
   Zhang WM, 2016, IEEE T MULTIMEDIA, V18, P1469, DOI 10.1109/TMM.2016.2569497
   Zhang YM, 2016, IEEE ACM T NETWORK, V24, P1632, DOI 10.1109/TNET.2015.2425146
   Zhu XS, 2014, IEEE T MULTIMEDIA, V16, P1888, DOI 10.1109/TMM.2014.2340695
NR 48
TC 18
Z9 19
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2020
VL 22
IS 11
BP 2780
EP 2791
DI 10.1109/TMM.2019.2962306
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OJ8YR
UT WOS:000584239900003
OA Green Submitted, hybrid
DA 2024-07-18
ER

PT J
AU Sun, K
   Tao, WB
   Qian, YH
AF Sun, Kun
   Tao, Wenbing
   Qian, Yuhua
TI Guide to Match: Multi-Layer Feature Matching With a Hybrid Gaussian
   Mixture Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Gaussian mixture model; Task analysis; Information
   processing; Sun; Image color analysis; Feature matching; multi-layer;
   hybrid gaussian mixture model
ID POINT SET REGISTRATION; OPTIMIZATION; ALGORITHM; LOCALITY; IMAGES
AB As a fundamental yet challenging task in computer vision, finding correspondences between two sets of feature points has received extensive attention. Among all the proposed methods, the Gaussian Mixture Model (GMM) based algorithms show their great power in formulating such problems. However, they are vulnerable to large portion of outliers in the extracted feature points. In this paper, a new Hybrid Gaussian Mixture Model (HGMM) combined with a multi-layer matching framework is proposed. Different from existing GMM based methods, HGMM uses a set of seed correspondences to guide the matching procedure. To automatically find seed correspondences, the feature points are divided into multiple layers according to their matching potential. With the help of Locality Sensitive Hashing, this can be done economically and efficiently. Correspondences found in lower layers which contain few outliers will be used as hard constraint when matching features in higher layers where a large portion of outliers exist. Extensive experiments show that the proposed method is efficient and more robust to outliers when images have large viewpoint difference or small scene overlap.
C1 [Sun, Kun] China Univ Geosci, Hubei Key Lab Intelligent Geoinformat Proc, Sch Comp Sci, Wuhan 430074, Peoples R China.
   [Tao, Wenbing] Huazhong Univ Sci & Technol, Sch Automat, Natl Key Lab Sci & Technol Multispectral Informat, Wuhan 430074, Peoples R China.
   [Tao, Wenbing] Shenzhen Huazhong Univ Sci & Technol, Res Inst, Shenzhen 518057, Peoples R China.
   [Qian, Yuhua] Shanxi Univ, Inst Big Data Sci & Ind, Minist Educ, Taiyuan 030006, Peoples R China.
   [Qian, Yuhua] Shanxi Univ, Key Lab Computat Intelligence & Chinese Informat, Minist Educ, Taiyuan 030006, Peoples R China.
C3 China University of Geosciences; Huazhong University of Science &
   Technology; Huazhong University of Science & Technology; Shenzhen
   Huazhong University of Science & Technology Research Institute; Shanxi
   University; Shanxi University
RP Tao, WB (corresponding author), Huazhong Univ Sci & Technol, Sch Automat, Natl Key Lab Sci & Technol Multispectral Informat, Wuhan 430074, Peoples R China.
EM sunkun@cug.edu.cn; wenbingtao@hust.edu.cn; jinchengqyh@126.com
RI Sun, Kun/HGD-9691-2022
OI Sun, Kun/0000-0002-9503-3969; Tao, Wenbing/0000-0003-3284-864X
FU National Natural Science Foundation of China [61802356, 91748204,
   61772213]; Fundamental Research Funds for the Central Universities,
   China University of Geosciences (Wuhan) [CUG170675]; Open Project
   Foundation of Intelligent Information Processing Key Laboratory of
   Shanxi Province [CICIP2018003]; Science, Technology and Innovation
   Commission of Shenzhen Municipality [JCYJ20170818165917438]; Wuhan
   Science and Technology Plan [2017010201010121]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61802356, Grant 91748204, and Grant
   61772213, in part by the Fundamental Research Funds for the Central
   Universities, China University of Geosciences (Wuhan) (CUG170675), in
   part by the Open Project Foundation of Intelligent Information
   Processing Key Laboratory of Shanxi Province (CICIP2018003), in part by
   Fund from the Science, Technology and Innovation Commission of Shenzhen
   Municipality under Grant JCYJ20170818165917438, and in part by the Wuhan
   Science and Technology Plan under Grant 2017010201010121.
CR [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.626
   Bai LF, 2018, IEEE T CYBERNETICS, V48, P826, DOI 10.1109/TCYB.2017.2657548
   Bay H., 2008, COMPUT VIS IMAGE UND, V10, P346, DOI DOI 10.1016/j.cviu.2007.09.014
   Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302
   Brandt J, 2010, PROC CVPR IEEE, P1815, DOI 10.1109/CVPR.2010.5539852
   Cao Y, 2010, PROC CVPR IEEE, P3352, DOI 10.1109/CVPR.2010.5540021
   Chang CH, 2014, PROC CVPR IEEE, P3254, DOI 10.1109/CVPR.2014.422
   Chao JS, 2016, IEEE T MULTIMEDIA, V18, P25, DOI 10.1109/TMM.2015.2502552
   Cheng J, 2014, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2014.8
   Cho M, 2012, PROC CVPR IEEE, P398, DOI 10.1109/CVPR.2012.6247701
   Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2
   Cui HN, 2017, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2017.257
   Evangelidis GD, 2014, LECT NOTES COMPUT SC, V8695, P109, DOI 10.1007/978-3-319-10584-0_8
   Evangelidis GD, 2018, IEEE T PATTERN ANAL, V40, P1397, DOI 10.1109/TPAMI.2017.2717829
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Hamid R, 2013, PROC CVPR IEEE, P2914, DOI 10.1109/CVPR.2013.375
   Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948
   Hartmann W, 2014, PROC CVPR IEEE, P9, DOI 10.1109/CVPR.2014.9
   Hauagge DC, 2012, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2012.6247677
   Havlena M, 2014, LECT NOTES COMPUT SC, V8691, P46, DOI 10.1007/978-3-319-10578-9_4
   Hu YT, 2016, PROC CVPR IEEE, P346, DOI 10.1109/CVPR.2016.44
   Hu YT, 2015, IEEE T IMAGE PROCESS, V24, P5995, DOI 10.1109/TIP.2015.2496305
   Jian B, 2011, IEEE T PATTERN ANAL, V33, P1633, DOI 10.1109/TPAMI.2010.223
   Jiang B, 2017, PROC CVPR IEEE, P550, DOI 10.1109/CVPR.2017.66
   Jiang XY, 2019, IEEE T GEOSCI REMOTE, V57, P6462, DOI 10.1109/TGRS.2019.2906183
   Jiang XY, 2020, IEEE T IMAGE PROCESS, V29, P736, DOI 10.1109/TIP.2019.2934572
   Karpushin M, 2016, IEEE T MULTIMEDIA, V18, P1762, DOI 10.1109/TMM.2016.2590305
   Lebeda K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.95
   Li J, 2018, IEEE T MULTIMEDIA, V20, P1672, DOI 10.1109/TMM.2017.2777461
   Liu Liu T. T., IEEE Trans. Ind. Inform, P1, DOI [10.1109/TII.2019.2930463 10.1109/TII.2019.2930463, DOI 10.1109/TII.2019.2930463, 10.1109/TII.2019.2930463]
   Liu TT, 2020, IEEE T IND INFORM, V16, P544, DOI 10.1109/TII.2019.2934728
   Liu TT, 2019, IEEE-ASME T MECH, V24, P384, DOI 10.1109/TMECH.2018.2870056
   Liu TT, 2018, IEEE T IND INFORM, V14, P5268, DOI 10.1109/TII.2018.2794449
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu GY, 2017, IEEE T MULTIMEDIA, V19, P2117, DOI 10.1109/TMM.2017.2731044
   Ma JY, 2019, IEEE T IMAGE PROCESS, V28, P4045, DOI 10.1109/TIP.2019.2906490
   Ma JY, 2019, INT J COMPUT VISION, V127, P512, DOI [10.1109/TMAG.2017.2763198, 10.1007/s11263-018-1117-z]
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2019, IEEE T NEUR NET LEAR, V30, P3584, DOI 10.1109/TNNLS.2018.2872528
   Ma JY, 2018, IEEE T GEOSCI REMOTE, V56, P4435, DOI 10.1109/TGRS.2018.2820040
   Ma JY, 2016, IEEE T IMAGE PROCESS, V25, P53, DOI 10.1109/TIP.2015.2467217
   Ma JY, 2015, IEEE T SIGNAL PROCES, V63, P1115, DOI 10.1109/TSP.2014.2388434
   Ma JY, 2014, IEEE T IMAGE PROCESS, V23, P1706, DOI 10.1109/TIP.2014.2307478
   Magerand L., 2018, T PATTERN ANAL MACHI, P1, DOI 10.1109/TPAMI.2018.2849973
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Ono Y, 2018, ADV NEUR IN, V31
   Qin XM, 2015, IEEE T MULTIMEDIA, V17, P295, DOI 10.1109/TMM.2015.2395078
   Rana A, 2019, IEEE T MULTIMEDIA, V21, P256, DOI 10.1109/TMM.2018.2839885
   Ravikumar N, 2018, MED IMAGE ANAL, V44, P156, DOI 10.1016/j.media.2017.11.012
   Salzmann M, 2007, IEEE I CONF COMP VIS, P1578
   Schönberger JL, 2017, PROC CVPR IEEE, P6959, DOI 10.1109/CVPR.2017.736
   Sun K, 2018, IEEE SIGNAL PROC LET, V25, P1089, DOI 10.1109/LSP.2018.2839022
   Sun K, 2017, IEEE GEOSCI REMOTE S, V14, P289, DOI 10.1109/LGRS.2016.2631165
   Sun K, 2016, INFORM SCIENCES, V367, P848, DOI 10.1016/j.ins.2016.07.020
   Sun K, 2015, INFORM SCIENCES, V295, P323, DOI 10.1016/j.ins.2014.10.029
   Tao WB, 2015, IEEE T IMAGE PROCESS, V24, P3754, DOI 10.1109/TIP.2015.2449559
   Nguyen TM, 2016, IEEE T MED IMAGING, V35, P1381, DOI 10.1109/TMI.2015.2511063
   Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77
   Torki M, 2010, PROC CVPR IEEE, P3058, DOI 10.1109/CVPR.2010.5540059
   Tsin Y, 2004, LECT NOTES COMPUT SC, V3023, P558
   Varol A, 2012, PROC CVPR IEEE, P2248, DOI 10.1109/CVPR.2012.6247934
   Wang G, 2017, IEEE T IMAGE PROCESS, V26, P1759, DOI 10.1109/TIP.2017.2658947
   Wang T, 2018, IEEE T PATTERN ANAL, V40, P2853, DOI 10.1109/TPAMI.2017.2767591
   Yan JC, 2018, IEEE T CYBERNETICS, V48, P765, DOI 10.1109/TCYB.2017.2655538
   Yan JC, 2016, IEEE T PATTERN ANAL, V38, P1228, DOI 10.1109/TPAMI.2015.2477832
   Yan JC, 2015, IEEE T IMAGE PROCESS, V24, P994, DOI 10.1109/TIP.2014.2387386
   Yang K, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9060581
   Yang Y, 2015, PATTERN RECOGN, V48, P156, DOI 10.1016/j.patcog.2014.06.017
   Yang ZQ, 2019, IEEE T IMAGE PROCESS, V28, P2584, DOI 10.1109/TIP.2018.2887204
   Yi BL, 2019, IEEE T IND INFORM, V15, P4591, DOI 10.1109/TII.2019.2893714
   Yi KM, 2018, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2018.00282
   Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28
   YUILLE AL, 1989, INT J COMPUT VISION, V3, P155, DOI 10.1007/BF00126430
   Zhang F, 2014, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2014.423
   Zhang S, 2018, PATTERN RECOGN, V80, P183, DOI 10.1016/j.patcog.2018.03.004
   Zhang S, 2018, IEEE GEOSCI REMOTE S, V15, P592, DOI 10.1109/LGRS.2018.2796136
   Zhou F, 2016, IEEE T PATTERN ANAL, V38, P1774, DOI 10.1109/TPAMI.2015.2501802
   Zhou L, 2017, IEEE I CONF COMP VIS, P2381, DOI 10.1109/ICCV.2017.259
NR 79
TC 27
Z9 28
U1 1
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2246
EP 2261
DI 10.1109/TMM.2019.2957984
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NE0TL
UT WOS:000562310200005
DA 2024-07-18
ER

PT J
AU Zhang, JY
   Zhang, YF
   Shen, MR
AF Zhang, Jinyu
   Zhang, Yifan
   Shen, Mengru
TI A Distance-Driven Alliance for a P2P Live Video System
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Peer-to-peer computing; Network topology; Streaming media; Topology;
   Bandwidth; Media; Synchronization; Media stream; P2P; network ranging;
   alliance
ID SCHEME
AB In peer-to-peer (P2P) networks, free-riders and redundant streams including overlapped and folded streams dramatically degrade playback quality and network performance, respectively. Although a locality-aware P2P live video can reduce the topological complexity, it cannot effectively avoid redundant streams while denying free-riders. In this paper, we first model free-rider, redundant streams and a distance-driven P2P system. Based on that model, a distance-driven alliance algorithm is proposed to construct not only an alliance that directly prevents any utility gains of free-riders through inter-user constraints but also a small-world network or a multicast tree that effectively reduces redundant streams. Finally, simulations confirm its advantages in functionality and performance over several existing strategies and distance-driven P2P live video systems.
C1 [Zhang, Jinyu] Beijing Jiaotong Univ, Sch Comp Sci & Technol, Beijing 100044, Peoples R China.
   [Zhang, Jinyu] Univ Lancaster, Sch Comp & Commun, Lancaster LA1 4YW, England.
   [Zhang, Yifan] Beijing Univ Aeronaut & Astronaut, Sch Econ & Management, Beijing 100191, Peoples R China.
   [Shen, Mengru] Beijing Jiaotong Univ, Sch Traff & Transportat, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University; Lancaster University; Beihang University;
   Beijing Jiaotong University
RP Zhang, JY (corresponding author), Beijing Jiaotong Univ, Sch Comp Sci & Technol, Beijing 100044, Peoples R China.
EM zjy@bjtu.edu.cn; zhangjinyu_ll@126.com; smr@bjtu.edu.cn
FU National Key R&D Program of China [2017YFC0805403, 2016YEE0204800];
   National Natural Science Foundation of China [61071077]; Research and
   Develop Fund of China Rail Corporation [2017J006-D]
FX This work was supported in part by the National Key R&D Program of China
   under Grants 2017YFC0805403 and 2016YEE0204800, in part by the National
   Natural Science Foundation of China under Grant 61071077, and in part by
   the Research and Develop Fund of China Rail Corporation under Grant
   2017J006-D. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Shiwen Mao.
CR [Anonymous], 2016, 2016 IF NETW C IF
   [Anonymous], 2017, CASE REP GASTROINTES
   [Anonymous], 2016, PEER PEER NETW APPL, DOI DOI 10.1007/S12083-014-0307-X
   [Anonymous], 2015, 2015 3 INT C ADV, DOI DOI 10.1109/CBD.2015.52
   [Anonymous], 2016, INT CON DISTR COMP S, DOI DOI 10.1109/ICDCS.2016.17
   [Anonymous], 2018, MULTIMED TOOLS APPL, DOI DOI 10.1007/S11042-017-4705-Y
   [Anonymous], 2009, 2009 IEEE INT S PAR, DOI DOI 10.1109/ISPA.2009.104
   [Anonymous], 2006, P 2006 IEEE INF
   [Anonymous], 2017, MULTIMED TOOLS APPL, DOI DOI 10.1007/S11042-016-4092-9
   [Anonymous], 2009, 2009 INT C ADV INF
   [Anonymous], 2011, WOODHEAD PUBL MATER
   [Anonymous], 2009, TER MID INFR RAD
   Asioli S, 2012, IEEE IMAGE PROC, P2257, DOI 10.1109/ICIP.2012.6467345
   Bhakuni A, 2014, IEEE INT ADV COMPUT, P155, DOI 10.1109/IAdCC.2014.6779311
   Chen JG, 2009, NSWCTC 2009: INTERNATIONAL CONFERENCE ON NETWORKS SECURITY, WIRELESS COMMUNICATIONS AND TRUSTED COMPUTING, VOL 2, PROCEEDINGS, P547, DOI 10.1109/NSWCTC.2009.51
   Chen SQ, 2012, PROCEEDINGS OF 2012 IEEE 14TH INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY, P82, DOI 10.1109/ICCT.2012.6511193
   Dorji D, 2016, INT JOINT CONF COMP, P487
   Endo R, 2012, 2012 9TH INTERNATIONAL CONFERENCE ON UBIQUITOUS INTELLIGENCE & COMPUTING AND 9TH INTERNATIONAL CONFERENCE ON AUTONOMIC & TRUSTED COMPUTING (UIC/ATC), P264, DOI 10.1109/UIC-ATC.2012.121
   Francis P., 2001, IEEE ACM T NETW, V9, P82
   He ZF, 2016, IEEE T MULTIMEDIA, V18, P1401, DOI 10.1109/TMM.2016.2564104
   He ZF, 2016, IEEE T WIREL COMMUN, V15, P728, DOI 10.1109/TWC.2015.2477509
   Hu YC, 2016, IEEE T MULTIMEDIA, V18, P840, DOI 10.1109/TMM.2016.2538721
   Kim Y, 2017, MULTIMED TOOLS APPL, V76, P17193, DOI 10.1007/s11042-016-3794-3
   Kumbhare A, 2015, INT CON DISTR COMP S, P328, DOI 10.1109/ICDCS.2015.41
   Liu X, 2006, J MULTIMEDIA JMM, V1, P38
   Megías D, 2017, EXPERT SYST APPL, V71, P147, DOI 10.1016/j.eswa.2016.11.015
   Naghizadeh A, 2015, IEEE INT C NETW SENS, P128, DOI 10.1109/ICNSC.2015.7116022
   Pan H., 2010, P INT C ED INF TECHN, P156
   Park H, 2008, IEEE T BROADCAST, V54, P557, DOI 10.1109/TBC.2008.2001148
   Purandare D, 2007, IEEE T MULTIMEDIA, V9, P1633, DOI 10.1109/TMM.2007.907453
   Shahriar I., 2017, P INT C COMP NETW CO, P729
   Takayama K., 2012, P 26 INT C ADV INF N, P105
   Tian XH, 2009, IEEE T MULTIMEDIA, V11, P1160, DOI 10.1109/TMM.2009.2026104
   Ullah I., 2012, P 8 INT C NETW SERV, P126
   Ullah S, 2017, MULTIMED TOOLS APPL, V76, P21519, DOI 10.1007/s11042-016-4008-8
   Zhao F, 2017, 2017 IEEE SECOND INTERNATIONAL CONFERENCE ON DATA SCIENCE IN CYBERSPACE (DSC), P140, DOI 10.1109/DSC.2017.94
   Zou S, 2018, IEEE T CIRC SYST VID, V28, P158, DOI 10.1109/TCSVT.2016.2601962
NR 37
TC 4
Z9 4
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2409
EP 2419
DI 10.1109/TMM.2019.2957953
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NE0TL
UT WOS:000562310200015
DA 2024-07-18
ER

PT J
AU Gao, P
   Paul, M
AF Gao, Pan
   Paul, Manoranjan
TI Rate-Distortion Optimal Joint Texture and Depth Map Coding for 3-D Video
   Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Error resilience; joint texture and depth map coding; rate-distortion
   optimization; variable-block-size prediction; 3-D video transmission
ID VIEW SYNTHESIS DISTORTION; MULTIVIEW-VIDEO; COMPRESSION; ALLOCATION;
   H.264/AVC; TRANSMISSION
AB For high compression efficiency, 3-D video coding usually employs a multimode methodology to exploit the dependencies between multiple views as well as between texture and depth. However, different coding modes will posses differentiating error propagation behaviour when the compressed 3-D video bit stream is transmitted over packet-switched networks, and thus lead to different amount of visual distortions. Further, the texture and depth distortions are combined in a highly complex fashion to produce the overall view synthesis distortion. To minimize the expected view synthesis distortion, this paper proposes an efficient rate-distortion optimized algorithm for joint selection of texture and depth modes. Firstly, a statistical model is developed to estimate the overall view synthesis distortion, in which the channel distortions caused by error propagation under different coding modes are analyzed. Then, joint optimization of texture and depth modes is derived within an operational rate-distortion framework using the Lagrange multiplier method. The adjacent block dependency caused by warping operation is explicitly considered in optimization, for which we develop a dynamic programming method to find the optimal solution. Finally, we extend the Lagrange minimization method to the more general variable-block-size prediction case, where the optimal quadtree tree structure and the combined coding modes are jointly determined using a multi-level dual trellis. Experimental results are presented for a wide range of packet loss rates to illustrate the effectiveness of the proposed algorithm.
C1 [Gao, Pan] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
   [Gao, Pan] Nanjing Univ Aeronaut & Astronaut, MIIT Key Lab Pattern Anal & Machine Intelligence, Nanjing 211106, Peoples R China.
   [Paul, Manoranjan] Charles Sturt Univ, Sch Comp & Math, Bathurst, NSW 2795, Australia.
C3 Nanjing University of Aeronautics & Astronautics; Nanjing University of
   Aeronautics & Astronautics; Charles Sturt University
RP Gao, P (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
EM gaopan.1005@gmail.com; mpaul@csu.edu.au
RI Paul, Manoranjan/AAD-4047-2021
OI Paul, Manoranjan/0000-0001-6870-5056
FU Natural Science Foundation of China [61701227]; Natural Science
   Foundation of Jiangsu Province of China [BK20170806]; Australian
   Research Council [DP190102574]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61701227, in part by the Natural Science Foundation of
   Jiangsu Province of China under Grant BK20170806, and in part by
   Australian Research Council Discovery under Grant DP190102574. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Sen-Ching Samson Cheung.
CR [Anonymous], 2014, JCT3VG1100
   [Anonymous], 2010, M18356 ISOIEC JTC1SC
   [Anonymous], P PCS
   Bjotegaard G., 2001, VCEGM33
   Chakareski J, 2015, IEEE J-STSP, V9, P474, DOI 10.1109/JSTSP.2015.2402633
   Chang YL, 2014, CONF REC ASILOMAR C, P678, DOI 10.1109/ACSSC.2014.7094533
   Chen YL, 2016, PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON ADVANCED MATERIALS FOR SCIENCE AND ENGINEERING (IEEE-ICAMSE 2016), P9, DOI 10.1109/ICAMSE.2016.7840217
   Chen Y, 2014, IEEE T CIRC SYST VID, V24, P2090, DOI 10.1109/TCSVT.2014.2352571
   Chen ZF, 2012, IEEE T IMAGE PROCESS, V21, P1123, DOI 10.1109/TIP.2011.2168411
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   De Abreu A, 2018, SIGNAL PROCESS-IMAGE, V63, P113, DOI 10.1016/j.image.2018.02.003
   Ekmekcioglu E, 2017, IEEE T CIRC SYST VID, V27, P1313, DOI 10.1109/TCSVT.2016.2527318
   Fang L, 2016, IEEE T IMAGE PROCESS, V25, P1961, DOI 10.1109/TIP.2016.2535345
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Gao P., IEEE T CIRCUITS SYST
   Gao P, 2017, IEEE IMAGE PROC, P1905, DOI 10.1109/ICIP.2017.8296613
   Gao P, 2017, IEEE T IMAGE PROCESS, V26, P2781, DOI 10.1109/TIP.2017.2690058
   Gao P, 2015, INT CONF ACOUST SPEE, P1448, DOI 10.1109/ICASSP.2015.7178210
   Gao P, 2014, IEEE T MULTIMEDIA, V16, P1797, DOI 10.1109/TMM.2014.2331013
   Hannuksela MM, 2013, IEEE T IMAGE PROCESS, V22, P3449, DOI 10.1109/TIP.2013.2269274
   Huang PC, 2017, IEEE T MULTIMEDIA, V19, P2625, DOI 10.1109/TMM.2017.2694218
   Jin J, 2019, IEEE T CIRC SYST VID, V29, P1754, DOI 10.1109/TCSVT.2018.2844743
   Lee JY, 2015, IEEE T CIRC SYST VID, V25, P1347, DOI 10.1109/TCSVT.2014.2380191
   Li Z., 2003, JVTH014
   Lin CY, 2018, IEEE T MULTIMEDIA, V20, P1209, DOI 10.1109/TMM.2017.2766043
   Lin CY, 2011, IEEE T CIRC SYST VID, V21, P589, DOI 10.1109/TCSVT.2011.2129270
   Macchiavello B., 2012, P SPIE VIS INF PROCE
   Macchiavello B, 2014, IEEE T MULTIMEDIA, V16, P711, DOI 10.1109/TMM.2014.2299768
   Merkle P, 2007, IEEE T CIRC SYST VID, V17, P1461, DOI 10.1109/TCSVT.2007.903665
   Merkle P, 2016, IEEE T CIRC SYST VID, V26, P570, DOI 10.1109/TCSVT.2015.2407791
   Müller K, 2011, P IEEE, V99, P643, DOI 10.1109/JPROC.2010.2091090
   ORTEGA A, 1994, IEEE T IMAGE PROCESS, V3, P26, DOI 10.1109/83.265978
   Secker A, 2004, IEEE T IMAGE PROCESS, V13, P1029, DOI 10.1109/TIP.2004.826089
   SHOHAM Y, 1988, IEEE T ACOUST SPEECH, V36, P1445, DOI 10.1109/29.90373
   Tech G, 2018, IEEE T CIRC SYST VID, V28, P1273, DOI 10.1109/TCSVT.2016.2631568
   Tech G, 2016, IEEE T CIRC SYST VID, V26, P35, DOI 10.1109/TCSVT.2015.2477935
   Tian D., 2013, 16WP3 ITUT SG
   Wenger S, 2003, IEEE T CIRC SYST VID, V13, P645, DOI 10.1109/TCSVT.2003.814966
   Wenger S., 1999, Q15I16R1 ITUT VCEG
   Yan B, 2013, J VIS COMMUN IMAGE R, V24, P669, DOI 10.1016/j.jvcir.2012.04.006
   Yang H, 2010, IEEE T IMAGE PROCESS, V19, P108, DOI 10.1109/TIP.2009.2032895
   Zhang D, 2015, IEEE T CIRC SYST VID, V25, P827, DOI 10.1109/TCSVT.2014.2363746
   Zhang R, 2000, IEEE J SEL AREA COMM, V18, P966, DOI 10.1109/49.848250
   Zhao Y, 2011, IEEE T IMAGE PROCESS, V20, P2221, DOI 10.1109/TIP.2011.2118218
   Zhu CB, 2009, IEEE T CIRC SYST VID, V19, P3, DOI 10.1109/TCSVT.2008.2005802
NR 45
TC 2
Z9 2
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2020
VL 22
IS 3
BP 610
EP 625
DI 10.1109/TMM.2019.2933336
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KU2ZL
UT WOS:000519576700004
DA 2024-07-18
ER

PT J
AU Tang, F
   Dong, WM
   Meng, YP
   Ma, CY
   Wu, FZ
   Li, XR
   Lee, TY
AF Tang, Fan
   Dong, Weiming
   Meng, Yiping
   Ma, Chongyang
   Wu, Fuzhang
   Li, Xinrui
   Lee, Tong-Yee
TI Image Retargetability
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Task analysis; Distortion; Measurement; Image resolution;
   Convolutional neural networks; Semantics; Image retargetability; visual
   attributes; multi-task learning; deep convolutional neural network
ID OBJECTIVE QUALITY ASSESSMENT; MODEL
AB Real-world applications could benefit from the ability to automatically retarget an image to different aspect ratios and resolutions while preserving its visually and semantically important content. However, not all images can be equally processed. This study introduces the notion of image retargetability to describe how well a particular image can be handled by content-aware image retargeting. We propose to learn a deep convolutional neural network to rank photo retargetability, in which the relative ranking of photo retargetability is directly modeled in the loss function. Our model incorporates the joint learning of meaningful photographic attributes and image content information, which can facilitate the regularization of the complicated retargetability rating problem. To train and analyze this model, we collect a dataset that contains retargetability scores and meaningful image attributes assigned by six expert raters. The experiments demonstrate that our unified model can generate retargetability rankings that are highly consistent with human labels. To further validate our model, we show the applications of image retargetability in retargeting method selection, retargeting method assessment and generating a photo collage.
C1 [Tang, Fan; Dong, Weiming] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100864, Peoples R China.
   [Tang, Fan] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Meng, Yiping] Didi Chuxing, Beijing 100094, Peoples R China.
   [Ma, Chongyang] Kuaishou Technol, Beijing 100085, Peoples R China.
   [Wu, Fuzhang] Chinese Acad Sci, Inst Software, Beijing 100864, Peoples R China.
   [Li, Xinrui] North China Elect Power Univ, Dept Math & Phys, Beijing 102206, Peoples R China.
   [Lee, Tong-Yee] Natl Cheng Kung Univ, Tainan 701, Taiwan.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Chinese Academy of Sciences; Institute of Software, CAS; North China
   Electric Power University; National Cheng Kung University
RP Dong, WM (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100864, Peoples R China.
EM tangfan2013@ia.ac.cn; weiming.dong@ia.ac.cn;
   mengyipingkitty@didichtuting.com; chongyangm@gmail.com;
   fuzhang@iscas.ac.cn; szyclxr@163.com; tonylee@mail.ncku.edu.tw
RI DONG, Weiming/AAG-7678-2020; Tang, Fan/O-3923-2018
OI DONG, Weiming/0000-0001-6502-145X; Tang, Fan/0000-0002-3975-2483; Li,
   Xinrui/0000-0003-4816-2626
FU National Key R&D Program of China [2018YFC0807500]; National Natural
   Science Foundation of China [61832016, 61672520, 61702488]; Ministry of
   Science and Technology, Taiwan [108-2221-E-006-038-MY3]; CASIA-Tencent
   Youtu joint research project
FX This work was supported in part by National Key R&D Program of China
   under 2018YFC0807500, and in part by National Natural Science Foundation
   of China under Grants 61832016, 61672520, and 61702488, and in part by
   Ministry of Science and Technology under 108-2221-E-006-038-MY3, Taiwan,
   and in part by CASIA-Tencent Youtu joint research project. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Xavier Giro-i-Nieto.
CR Abdulnabi AH, 2015, IEEE T MULTIMEDIA, V17, P1949, DOI 10.1109/TMM.2015.2477680
   [Anonymous], 2015, CENGAGE LEARNING
   [Anonymous], 2014, P WORKSHOP COMPUTATI, DOI DOI 10.1145/2630099.2630801
   [Anonymous], 2015, IEEE PHOTON J
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   [Anonymous], 2014, 2014 INT C LEARNING
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Bare B, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P925, DOI 10.1145/2647868.2654957
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598
   Castillo S., 2011, ACM SIGGRAPH S APPLP, P7
   Chen HR, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P81, DOI 10.1145/3240508.3240517
   Chen YL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P37, DOI 10.1145/3123266.3123274
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Dai DX, 2014, PROC CVPR IEEE, P3027, DOI 10.1109/CVPR.2014.387
   Dong WM, 2012, J COMPUT SCI TECH-CH, V27, P121, DOI 10.1007/s11390-012-1211-6
   Dong WM, 2016, IEEE T VIS COMPUT GR, V22, P1088, DOI 10.1109/TVCG.2015.2440255
   Dong WM, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618471
   Fang YM, 2014, IEEE J EM SEL TOP C, V4, P95, DOI 10.1109/JETCAS.2014.2298919
   Guo GJ, 2018, IEEE T MULTIMEDIA, V20, P2073, DOI 10.1109/TMM.2018.2794262
   Gygli M, 2013, IEEE I CONF COMP VIS, P1633, DOI 10.1109/ICCV.2013.205
   Hsu CC, 2014, IEEE J-STSP, V8, P377, DOI 10.1109/JSTSP.2014.2311884
   Hwang SJ, 2011, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2011.5995543
   Isola P, 2014, IEEE T PATTERN ANAL, V36, P1469, DOI 10.1109/TPAMI.2013.200
   Jas M, 2015, PROC CVPR IEEE, P2727, DOI 10.1109/CVPR.2015.7298889
   Kaufmann P, 2013, COMPUT GRAPH FORUM, V32, P31, DOI 10.1111/cgf.12023
   Kendall MG, 1939, ANN MATH STAT, V10, P275, DOI 10.1214/aoms/1177732186
   Khosla A, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P867, DOI 10.1145/2566486.2567996
   Krähenbühl P, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1616452.1618472, 10.1145/1618452.1618472]
   Li DB, 2018, PROC CVPR IEEE, P8193, DOI 10.1109/CVPR.2018.00855
   Liang Y, 2017, IEEE T VIS COMPUT GR, V23, P1099, DOI 10.1109/TVCG.2016.2517641
   Lin SS, 2013, IEEE T MULTIMEDIA, V15, P359, DOI 10.1109/TMM.2012.2228475
   Liu J., 2009, P 25 C UNCERTAINTY A, P339, DOI DOI 10.5555/1795114.1795154
   Liu YJ, 2011, COMPUT GRAPH FORUM, V30, P583, DOI 10.1111/j.1467-8659.2011.01881.x
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Ma L, 2012, IEEE J-STSP, V6, P626, DOI 10.1109/JSTSP.2012.2211996
   Panozzo D, 2012, COMPUT GRAPH FORUM, V31, P229, DOI 10.1111/j.1467-8659.2012.03001.x
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   Radenovic F, 2018, PROC CVPR IEEE, P5706, DOI 10.1109/CVPR.2018.00598
   Rawat YS, 2018, IEEE T MULTIMEDIA, V20, P754, DOI 10.1109/TMM.2017.2750420
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rosenholtz R, 2007, J VISION, V7, DOI 10.1167/7.2.17
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saito S, 2016, LECT NOTES COMPUT SC, V9912, P244, DOI 10.1007/978-3-319-46484-8_15
   Song Y, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1047, DOI 10.1145/3240508.3240623
   Sun J, 2013, INT J COMPUT VISION, V104, P135, DOI 10.1007/s11263-013-0618-z
   Tan WM, 2016, IEEE T MULTIMEDIA, V18, P128, DOI 10.1109/TMM.2015.2500727
   Wang WG, 2017, IEEE I CONF COMP VIS, P2205, DOI 10.1109/ICCV.2017.240
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Wolf L, 2007, IEEE I CONF COMP VIS, P1418
   Wu HS, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866185
   Yan JZ, 2013, PROC CVPR IEEE, P971, DOI 10.1109/CVPR.2013.130
   Yang H, 2015, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2015.7299100
   Yang XS, 2016, IEEE T MULTIMEDIA, V18, P1832, DOI 10.1109/TMM.2016.2582379
   Yao L, 2012, INT J COMPUT VISION, V96, P353, DOI 10.1007/s11263-011-0478-3
   Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064
   Zhang JY, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P257, DOI 10.1145/2647868.2654922
   Zhang LM, 2015, IEEE T MULTIMEDIA, V17, P1538, DOI 10.1109/TMM.2015.2451954
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P94, DOI 10.1109/TMM.2013.2286817
   Zhang YB, 2016, IEEE T IMAGE PROCESS, V25, P4286, DOI 10.1109/TIP.2016.2585884
   Zhang YF, 2008, COMPUT GRAPH FORUM, V27, P1797, DOI 10.1111/j.1467-8659.2008.01325.x
NR 65
TC 5
Z9 6
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2020
VL 22
IS 3
BP 641
EP 654
DI 10.1109/TMM.2019.2932620
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KU2ZL
UT WOS:000519576700006
DA 2024-07-18
ER

PT J
AU Chen, L
   Wu, L
   Hu, ZZ
   Wang, M
AF Chen, Lei
   Wu, Le
   Hu, Zhenzhen
   Wang, Meng
TI Quality-Aware Unpaired Image-to-Image Translation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image generation; image processing; image quality; neural networks
ID NEURAL-NETWORKS; STATISTICS
AB Generative adversarial networks (GANs) have been widely used for the image-to-image translation task. While these models rely heavily on the labeled image pairs, recently some GAN variants have been proposed to tackle the unpaired image translation task. These models exploited supervision at the domain level with a reconstruction process for unpaired image translation. On the other hand, parallel works have shown that leveraging perceptual loss functions based on high-level deep features could enhance the generated image quality. Nevertheless, as these GAN-based models either depended on the pretrained deep network structure or relied on the labeled image pairs, they could not be directly applied to the unpaired image translation task. Moreover, despite the improvement of the introduced perceptual losses from deep neural networks, few researchers have explored the possibility of improving the generated image quality from classical image quality measures. To tackle the above two challenges, in this paper, we propose a unified quality-aware GAN-based framework for unpaired image-to-image translation, where a quality-aware loss is explicitly incorporated by comparing each source image and the reconstructed image at the domain level. Specifically, we design two detailed implementations of the quality loss. The first method is based on a classical image quality assessment measure by defining a classical quality-aware loss to ensure similar quality score between an original image and the reconstructed image at the domain level. The second method proposes an adaptive deep network based loss that compares the high level content structure between each original image and its reconstructed image from the generator. Finally, extensive experimental results on many real-world datasets clearly show the quality improvement of our proposed framework, and the superiority of leveraging classical image quality measures for unpaired image translation compared to the deep network based model.
C1 [Chen, Lei; Wu, Le; Hu, Zhenzhen; Wang, Meng] Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Anhui, Peoples R China.
C3 Hefei University of Technology
RP Chen, L (corresponding author), Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Anhui, Peoples R China.
EM chenlei.hfut@gmail.com; lewu.ustc@gmail.com; huzhen.ice@gmail.com;
   eric.mengwang@gmail.com
RI Wang, Meng/ITR-8699-2023
OI Hu, Zhenzhen/0000-0003-1042-8361; Chen, Lei/0000-0002-3193-7256
FU National Natural Science Foundation for Distinguished Young Scholars of
   China [61725203]; National Key Research and Development Program of China
   [2017YFB0803301]; National Natural Science Foundation of China
   [61602147, 61732008, 61802104]
FX This work was supported in part by the National Natural Science
   Foundation for Distinguished Young Scholars of China under Grant
   61725203, in part by the National Key Research and Development Program
   of China under Grant 2017YFB0803301, and in part by the National Natural
   Science Foundation of China under Grants 61602147, 61732008, 61802104.
CR [Anonymous], 2016, P IEEE C COMPUTER VI
   [Anonymous], NEURAL NETW MACH LEA
   [Anonymous], 2015, NATURE, DOI [DOI 10.1038/NATURE14539, 10.1038/nature14539]
   [Anonymous], 2017, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2017.632
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2017, ARXIV170400028CSLG
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bosse S, 2016, IEEE IMAGE PROC, P3773, DOI 10.1109/ICIP.2016.7533065
   Chen ZY, 2014, PROC CVPR IEEE, P3003, DOI 10.1109/CVPR.2014.384
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Elgammal A., 2017, ARXIV170607068
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Guo GJ, 2018, IEEE T MULTIMEDIA, V20, P2073, DOI 10.1109/TMM.2018.2794262
   He D, 2016, ADV NEUR IN, V29
   Hou WL, 2015, IEEE T NEUR NET LEAR, V26, P1275, DOI 10.1109/TNNLS.2014.2336852
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kim T, 2017, PR MACH LEARN RES, V70
   Kovesi P, 2000, PSYCHOL RES-PSYCH FO, V64, P136, DOI 10.1007/s004260000024
   Ledig C., 2017, P IEEE C COMP VIS PA, P4681
   Li RH, 2018, IEEE INT CONF ROBOT, P7286, DOI 10.1109/ICRA.2018.8461251
   Liu D, 2016, IEEE T IMAGE PROCESS, V25, P3194, DOI 10.1109/TIP.2016.2564643
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Luo CZ, 2016, IEEE T MULTIMEDIA, V18, P40, DOI 10.1109/TMM.2015.2495248
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   MORRONE MC, 1988, PROC R SOC SER B-BIO, V235, P221, DOI 10.1098/rspb.1988.0073
   Saad MA, 2010, IEEE SIGNAL PROC LET, V17, P583, DOI 10.1109/LSP.2010.2045550
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P1918, DOI 10.1109/TIP.2005.854492
   Srinivasan PP, 2017, IEEE I CONF COMP VIS, P2262, DOI 10.1109/ICCV.2017.246
   Wang CY, 2018, IEEE T IMAGE PROCESS, V27, P4066, DOI 10.1109/TIP.2018.2836316
   Wang M, 2012, IEEE T MULTIMEDIA, V14, P858, DOI 10.1109/TMM.2012.2187181
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XL, 2016, LECT NOTES COMPUT SC, V9908, P318, DOI 10.1007/978-3-319-46493-0_20
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu QB, 2017, IEEE T MULTIMEDIA, V19, P2490, DOI 10.1109/TMM.2017.2700206
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212
   Zhang K, 2018, IEEE DATA MINING, P747, DOI 10.1109/ICDM.2018.00090
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
   Zhu J.-Y., 2017, IEEE I CONF COMP VIS, P2223, DOI DOI 10.1109/ICCV.2017.244
NR 49
TC 41
Z9 44
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2019
VL 21
IS 10
BP 2664
EP 2674
DI 10.1109/TMM.2019.2907052
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JD1IM
UT WOS:000489728400019
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zheng, LR
   Shen, LQ
   Chen, JA
   An, P
   Luo, J
AF Zheng, Linru
   Shen, Liquan
   Chen, Jianan
   An, Ping
   Luo, Jun
TI No-Reference Quality Assessment for Screen Content Images Based on
   Hybrid Region Features Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Screen content image; image quality assessment; no-reference; image
   segmentation; hybrid-region-based features
ID GRADIENT MAGNITUDE; STATISTICS; INFORMATION; SIMILARITY; SEGMENTATION;
   PHASE
AB Research on screen content images (SCIs) attracts more attention as they are highly applied to image-and video-centric applications on mobile and other devices. It is important to develop an efficient image-quality assessment (IQA) method for SCIs because IQA can guide and optimize various image-processing methods for SCIs and improve user experience. In this paper, we propose a no-reference objective assessment model for SCIs including SCIs segmentation and the analysis of local and global perceptual feature representations. Since the human visual system is highly sensitive to sharp edges that are commonly encountered in SCIs, we utilize the variance of local standard deviation, which is a noise robust index to distinguish the sharp edge patches (SEPes) and non-SEPes of SCIs. For SEPes, we perform two kinds of feature extractions. First, the entropy and contrast features are extracted with a gray-level co-occurrence matrix, which are highly perceptive of microstructural change. Second, the local phase coherence is utilized to capture the loss in sharpness. Then, average pooling is adopted to fuse features obtained from all of the SEPes to represent the local features. We further combine local features with global features that are derived using the BRISQUE method as the hybrid region (HR)-based features. Finally, a regression module is learned using support vector regression to train the mapping function that maps HR-based features to subjective quality scores. Experimental results on the screen image-quality assessment database show that the proposed method can achieve better performance in visual-quality prediction for SCIs than the performance achieved by state-of-the-art methods.
C1 [Zheng, Linru; Chen, Jianan; An, Ping] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.
   [Shen, Liquan] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Joint Int Res Lab Specialty Fiber Opt & Adv Commu, Key Lab Specialty Fiber Opt & Opt Access Networks, Shanghai 200444, Peoples R China.
   [Luo, Jun] Shanghai Univ, Sch Mechatron Engn & Automat, Shanghai 200444, Peoples R China.
C3 Shanghai University; Shanghai University; Shanghai University
RP Shen, LQ (corresponding author), Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Joint Int Res Lab Specialty Fiber Opt & Adv Commu, Key Lab Specialty Fiber Opt & Opt Access Networks, Shanghai 200444, Peoples R China.
EM zhengyongde369@163.com; jsslq@163.com; chenjn2010@hotmail.com;
   anping@shu.edu.cn; luojun@shu.edu.cn
RI Luo, Jun/IQU-6231-2023; Shen, Liquan/D-4832-2012
OI Luo, Jun/0000-0002-6998-4453; Shen, Liquan/0000-0002-2148-6279
FU National Natural Science Foundation of China [61671282, 61373151,
   61601278, 61525305]; Shanghai Pujiang Program [15pjd015]; Shanghai
   Science and Technology Innovation Plan [18010500200]; Shanghai Shuguang
   Program [17SG37]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61671282, 61373151, 61601278, and
   61525305; in part by the Shanghai Pujiang Program under Grant 15pjd015;
   in part by the Shanghai Science and Technology Innovation Plan under
   Grant 18010500200; and in part by the Shanghai Shuguang Program under
   Grant 17SG37. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Abdulmotaleb El
   Saddik. (Corresponding author: Liquan Shen.)
CR [Anonymous], P 33 AAAI C ART INT
   [Anonymous], 2012, BT50011 ITUR
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], IEEE T MULTIMED
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   BARALDI A, 1995, IEEE T GEOSCI REMOTE, V33, P293, DOI 10.1109/36.377929
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Fang YM, 2018, IEEE T IMAGE PROCESS, V27, P1600, DOI 10.1109/TIP.2017.2781307
   Fang YM, 2017, IEEE T IMAGE PROCESS, V26, P2016, DOI 10.1109/TIP.2017.2669840
   Fang YM, 2015, IEEE SIGNAL PROC LET, V22, P838, DOI 10.1109/LSP.2014.2372333
   Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760
   GONG P, 1992, REMOTE SENS ENVIRON, V40, P137, DOI 10.1016/0034-4257(92)90011-8
   Gu K, 2017, IEEE T IMAGE PROCESS, V26, P4005, DOI 10.1109/TIP.2017.2711279
   Gu K, 2017, IEEE T CYBERNETICS, V47, P4559, DOI 10.1109/TCYB.2016.2575544
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Gu K, 2016, NEUROCOMPUTING, V196, P140, DOI 10.1016/j.neucom.2015.11.101
   Gu K, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2439035
   Han L, 2017, IEEE INT CON MULTI, P139, DOI 10.1109/ICME.2017.8019479
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Hassen R, 2013, IEEE T IMAGE PROCESS, V22, P2798, DOI 10.1109/TIP.2013.2251643
   Hassen R, 2010, INT CONF ACOUST SPEE, P2434, DOI 10.1109/ICASSP.2010.5496297
   Huan Yang, 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P257, DOI 10.1109/QoMEX.2014.6982328
   Jiang QP, 2018, IEEE T MULTIMEDIA, V20, P2035, DOI 10.1109/TMM.2017.2763321
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li LD, 2014, IEEE SIGNAL PROC LET, V21, P918, DOI 10.1109/LSP.2014.2320743
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Li YM, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING (DSP), P685, DOI 10.1109/ICDSP.2016.7868646
   Lin T, 2013, IEEE T CIRC SYST VID, V23, P173, DOI 10.1109/TCSVT.2012.2223871
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Ma L, 2012, IEEE T CIRC SYST VID, V22, P1441, DOI 10.1109/TCSVT.2012.2202049
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Min XK, 2016, IEEE INT CON MULTI
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Motoyoshi I, 2007, NATURE, V447, P206, DOI 10.1038/nature05724
   Ni ZK, 2017, IEEE T IMAGE PROCESS, V26, P4818, DOI 10.1109/TIP.2017.2718185
   OPPENHEIM AV, 1981, P IEEE, V69, P529, DOI 10.1109/PROC.1981.12022
   Peuwnuan K, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2017), P798, DOI 10.1109/ICIVC.2017.7984664
   RUDERMAN DL, 1994, NETWORK-COMP NEURAL, V5, P517, DOI 10.1088/0954-898X/5/4/006
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Shao F, 2018, IEEE T SYST MAN CY-S, V48, P1521, DOI 10.1109/TSMC.2017.2676180
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shen LQ, 2018, IEEE T IMAGE PROCESS, V27, P4195, DOI 10.1109/TIP.2018.2837379
   Tolstaya EV, 2010, PROC SPIE, V7537, DOI 10.1117/12.838447
   ULABY FT, 1986, IEEE T GEOSCI REMOTE, V24, P235, DOI 10.1109/TGRS.1986.289643
   Wang CA, 2017, IEEE T IMAGE PROCESS, V26, P1833, DOI 10.1109/TIP.2017.2666742
   Wang SQ, 2018, IEEE T CIRC SYST VID, V28, P1, DOI 10.1109/TCSVT.2016.2602764
   Wang SQ, 2018, IEEE COMPUT GRAPH, V38, P47, DOI 10.1109/MCG.2016.46
   Wang SQ, 2016, IEEE T IMAGE PROCESS, V25, P3838, DOI 10.1109/TIP.2016.2573597
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE IMAGE PROC, P477
   Wang Z., 2004, ADV NEURAL INFORM PR, P786
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wu JJ, 2015, 2015 IEEE CHINA SUMMIT & INTERNATIONAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING, P663, DOI 10.1109/ChinaSIP.2015.7230487
   Wu JJ, 2013, IEEE T MULTIMEDIA, V15, P1700, DOI 10.1109/TMM.2013.2266093
   Wu XL, 2011, IEEE T IMAGE PROCESS, V20, P1262, DOI 10.1109/TIP.2010.2092438
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yang H, 2015, IEEE T IMAGE PROCESS, V24, P4408, DOI 10.1109/TIP.2015.2465145
   Yang H, 2015, IEEE T CYBERNETICS, V45, P533, DOI 10.1109/TCYB.2014.2330657
   Zhan TM, 2013, MAGN RESON IMAGING, V31, P439, DOI 10.1016/j.mri.2012.08.002
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang KH, 2016, IEEE T CYBERNETICS, V46, P546, DOI 10.1109/TCYB.2015.2409119
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
NR 65
TC 35
Z9 36
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2019
VL 21
IS 8
BP 2057
EP 2070
DI 10.1109/TMM.2019.2894939
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IK7YM
UT WOS:000476809700013
DA 2024-07-18
ER

PT J
AU Lin, YB
   Yang, MT
   Lin, YW
AF Lin, Yi-Bing
   Yang, Ming-Ta
   Lin, Yun-Wei
TI Low-Cost Four-Dimensional Experience Theater Using Home Appliances
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 4D effect; 4D video; Internet of Things (IoT); multi-sensorial effect;
   synchronization
ID PERCEIVED SYNCHRONIZATION
AB Multi-sensorial effect or four-dimensional (4-D) effect is considered in the niche arena of film playing but has become a growing business quickly. Installation of 4-D effects are typically expensive, and 4-D films are most often presented in custom-built theaters at special venues. This paper proposes a novel integration of existing video service platform and commercial smart home solution to generate 4-D effects for any non-4-D film (i.e., a 3-D or a 2-D film). Specifically, we describe how the designer can use home appliances to create the effects for a 4-D experience theater at home. Instead of using an expensive and special 4-D movie effect system, we use a low-cost off-the-shelf IoT platform for a smart home to create the 4-D movie effects for any non-4-D film. Therefore, the viewer can enjoy 4-D movies at home without special equipment other than home appliances. In our solution, an IoT device-management system called IoTtalk is utilized to integrate the video service system and the smart home system. Since these systems may be located in distant places, the delays for video streaming and triggering of 4-D effects must be synchronized. We have conducted measurements to indicate that in our design, the synchronization issue can be ignored without affecting the user experience.
C1 [Lin, Yi-Bing; Yang, Ming-Ta; Lin, Yun-Wei] Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
C3 National Yang Ming Chiao Tung University
RP Yang, MT (corresponding author), Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
EM liny@cs.nctu.edu.tw; mingta@itri.org.tw; jyneda@gmail.com
RI Lin, YI-Bing/AAM-5772-2021
OI Lin, YI-Bing/0000-0001-6841-4718
FU Ministry of Science and Technology [106-2221-E-009 -049-MY2,
   107-2221-E-009-039]; Center for Open Intelligent Connectivity of
   National Chiao Tung University; Ministry of Education, Taiwan, R.O.C.;
   Academia Sinica [AS-105-TP-A07, III 106A5041]; Ministry of Economic
   Affairs [107-EC-17-A-24-1530]
FX This work was supported in part by Ministry of Science and Technology
   106-2221-E-009 -049-MY2, 107-2221-E-009-039, in part by the Center for
   Open Intelligent Connectivity of National Chiao Tung University and
   Ministry of Education, Taiwan, R.O.C., in part by Academia Sinica
   AS-105-TP-A07, III 106A5041, and in part by the Ministry of Economic
   Affairs 107-EC-17-A-24-1530.
CR [Anonymous], DISNEY PARKS BLOG
   [Anonymous], 2300532013 ISOIEC
   [Anonymous], VID MULT AD PLAYL
   [Anonymous], 2015, HOLLYWOOD REPORTER
   [Anonymous], P SIGGRAPH EM TECHN
   [Anonymous], CINEMA NEW 4DX SCREE
   [Anonymous], 10282231 ETSI TS
   [Anonymous], VID AD SERV TEMPL
   Ghinea G, 2010, IEEE T SYST MAN CY A, V40, P657, DOI 10.1109/TSMCA.2010.2041224
   Lee J, 2016, IEEE T VIS COMPUT GR, V22, P2300, DOI 10.1109/TVCG.2015.2507591
   Lin Y.W., 2017, IEEE SYST J, P1
   Lin YB, 2018, IEEE ACCESS, V6, P26036, DOI 10.1109/ACCESS.2018.2832222
   Lin YB, 2017, IEEE INTERNET THINGS, V4, P1552, DOI 10.1109/JIOT.2017.2682100
   Lin YB, 2015, IEEE INTERNET THINGS, V2, P551, DOI 10.1109/JIOT.2015.2423286
   Lin YW, 2017, IEEE INTERNET THINGS, V4, P1104, DOI 10.1109/JIOT.2017.2715859
   Sulema Y., 2016, PROC INT C SYSTEMS S, P1
   Yuan ZH, 2015, IEEE T MULTIMEDIA, V17, P957, DOI 10.1109/TMM.2015.2431915
   Yuan ZH, 2015, IEEE T MULTIMEDIA, V17, P104, DOI 10.1109/TMM.2014.2371240
NR 18
TC 5
Z9 5
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2019
VL 21
IS 5
BP 1161
EP 1168
DI 10.1109/TMM.2018.2876043
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HV8HY
UT WOS:000466223600007
DA 2024-07-18
ER

PT J
AU Agarwal, S
   Mukherjee, DP
AF Agarwal, Swapna
   Mukherjee, Dipti Prasad
TI Synthesis of Realistic Facial Expressions Using Expression Map
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Expression map; mixed expression synthesis; facial structure; realistic
   facial expression synthesis
ID DATA-DRIVEN APPROACH; EMOTION; MUSIC; MODEL
AB For synthesis of realistic facial expressions displaying emotions, we need an efficient representation of pure (e.g., surprise) as well as mixed (e.g., happily surprised) emotional expressions. In this paper, we train an expression map (XM) that efficiently represents the emotional expressions. We propose an algorithm that utilizes the XM to synthesize emotional expressions, tailor-made for the facial structure of the target person. The proposed method can also control the proportions of different basic emotional expressions of those, when mixed together, to generate realistic emotional facial expressions. Unlike many existing methods, our expression synthesis model requires only one expression-neutral face image of the target person. Both qualitative and quantitative tests on four data sets show promising results. On average, we have achieved 92.4% correct validation of the expressions synthesized by our method. We also show that for both basic and mixed emotional expressions, our method generates finer expression details compared to existing state-of-the-art works.
C1 [Agarwal, Swapna; Mukherjee, Dipti Prasad] Indian Stat Inst, Elect & Commun Sci Unit, Kolkata 700108, India.
C3 Indian Statistical Institute; Indian Statistical Institute Kolkata
RP Agarwal, S (corresponding author), Indian Stat Inst, Elect & Commun Sci Unit, Kolkata 700108, India.
EM swapna_r@isical.ac.in; dipti@isical.ac.in
OI Agarwal, Swapna/0000-0002-2909-8342
FU DST, Government of India [SR/WOS-A/ET-53/2012(G)]
FX The work of Swapna Agarwal was supported by DST, Government of India
   project no. SR/WOS-A/ET-53/2012(G). The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Qi Tian. (Corresponding author: Swapna Agarwal.)
CR Agarwal S, 2012, P 8 IND C COMP VIS G, V12, P1, DOI 10.1145/2425333.2425361
   Aifanti N., 2010, P 11 INT WORKSH IM A, DOI DOI 10.1371/JOURNAL.PONE.0009715
   [Anonymous], 10 IEEE INT C WORKSH
   Asthana A, 2012, IEEE T VIS COMPUT GR, V18, P1511, DOI 10.1109/TVCG.2011.157
   Bhaskar H., 2016, P ADV FAC DET FAC IM, P101
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Ekman P., 1978, Facial action coding system
   Haykin S.S., 2009, NEURAL NETWORKS COMP
   Huang D, 2010, LECT NOTES COMPUT SC, V6312, P364, DOI 10.1007/978-3-642-15552-9_27
   Kanade T., 2000, P 4 IEEE INT C AUT F, P46, DOI [10.1109/AFGR.2000.840611, DOI 10.1109/AFGR.2000.840611]
   Lei Xiong, 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, P473
   Li K, 2014, IEEE T MULTIMEDIA, V16, P299, DOI 10.1109/TMM.2013.2293064
   Li K, 2012, PROC CVPR IEEE, P57, DOI 10.1109/CVPR.2012.6247658
   Liu WF, 2008, CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOL 2, PROCEEDINGS, P680, DOI 10.1109/CISP.2008.216
   Liu ZC, 2001, COMP GRAPH, P271
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Malleson C, 2015, IEEE I CONF COMP VIS, P3979, DOI 10.1109/ICCV.2015.453
   Mao HY, 2009, IEEE SYS MAN CYBERN, P4842, DOI 10.1109/ICSMC.2009.5346057
   Martinez A, 2012, J MACH LEARN RES, V13, P1589
   Martínez HP, 2014, IEEE T AFFECT COMPUT, V5, P314, DOI 10.1109/TAFFC.2014.2352268
   Mollahosseini A, 2016, IEEE WINT CONF APPL
   Olszewski K, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980252
   Pantic M, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P317, DOI 10.1109/ICME.2005.1521424
   Saito S, 2016, LECT NOTES COMPUT SC, V9912, P244, DOI 10.1007/978-3-319-46484-8_15
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Song ML, 2007, IEEE T MULTIMEDIA, V9, P1384, DOI 10.1109/TMM.2007.906591
   Susskind Joshua M., 2008, Generating facial expressions with deep belief nets
   Valstar M., 2010, Proceedings of 3rd intern. workshop on EMOTION (satellite of LREC): Corpora for research on emotion and affect, P65
   Wang JC, 2015, IEEE T AFFECT COMPUT, V6, P261, DOI 10.1109/TAFFC.2015.2415212
   Wang JC, 2015, IEEE T AFFECT COMPUT, V6, P56, DOI 10.1109/TAFFC.2015.2397457
   Xie WC, 2017, IEEE T MULTIMEDIA, V19, P279, DOI 10.1109/TMM.2016.2614429
   Yin WT, 2005, LECT NOTES COMPUT SC, V3752, P73
   Yu H, 2012, COMPUT GRAPH-UK, V36, P152, DOI 10.1016/j.cag.2011.12.002
   Yuwen Wu, 2005, 2005 IEEE/RSJ International Conference on Intelligent Robots and Systems, P1968, DOI 10.1109/IROS.2005.1545532
   Zeiler M. D., 2011, Advances in Neural Information Processing Systems, P1629
   Zhang QS, 2006, IEEE T VIS COMPUT GR, V12, P48, DOI 10.1109/TVCG.2006.9
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhao SC, 2017, IEEE T MULTIMEDIA, V19, P632, DOI 10.1109/TMM.2016.2617741
NR 39
TC 14
Z9 14
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2019
VL 21
IS 4
BP 902
EP 914
DI 10.1109/TMM.2018.2871417
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HQ4XD
UT WOS:000462413700008
DA 2024-07-18
ER

PT J
AU Yang, X
   Shyu, ML
   Yu, HQ
   Sun, SM
   Yin, NS
   Chen, W
AF Yang, Xue
   Shyu, Mei-Ling
   Yu, Han-Qi
   Sun, Shi-Ming
   Yin, Nian-Sheng
   Chen, Wei
TI Integrating Image and Textual Information in Human-Robot Interactions
   for Children With Autism Spectrum Disorder
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Autism spectrum disorder (ASD); robot; picture book recommendation;
   human-robot interaction; near-duplicated keyframe (NDK); multiple
   correspondence analysis (MCA); integration; multi-modality
ID RECOMMENDATION SYSTEM; DUPLICATE; IMPROVE; CUES; NAO
AB Talking and literary reading are important activities for children, especially for children with autism spectrum disorder (ASD). We try to integrate the activities with NAO robots to excite their communication willingness. In this paper, a novel multimodal picture book recommendation framework that combines textual information and image information to calculate the similarity between the picture books and the conversation topics is proposed and evaluated using a testing dataset. In the proposed framework, an image neighbor discovery method to get more relative terms and an near-duplicated keyframes (NDK) friend detection method to get more relative NDKs are proposed. Finally, the booklist generated from the experiment is evaluated by six performance indicators and the experimental results demonstrate that our proposed framework achieves satisfactory and promising performance. With the help of the proposed recommendation framework, an autistic child can talk to the NAO robot in a relaxed and enjoyable environment. Please note that the proposed framework is not evaluated for its performance with the ASD children but for its performance at recommending books based on visual and textual features. Therefore, no tests were performed with either professionals nor diagnosed individuals.
C1 [Yang, Xue; Yu, Han-Qi; Yin, Nian-Sheng; Chen, Wei] Nanjing Inst Technol, Ind Ctr, Nanjing 21167, Jiangsu, Peoples R China.
   [Shyu, Mei-Ling] Univ Miami, Dept Elect & Comp Engn, Coral Gables, FL 33124 USA.
   [Sun, Shi-Ming] NARI Technol Co Ltd, Dept Basic Res & Dev, Nanjing Power Grid Dispatching & Control Branch, Nanjing 211106, Jiangsu, Peoples R China.
C3 Nanjing Institute of Technology; University of Miami
RP Yang, X (corresponding author), Nanjing Inst Technol, Ind Ctr, Nanjing 21167, Jiangsu, Peoples R China.
EM yangxue@njit.edu.cn; shyu@miami.edu; yuhq@njit.edu.cn;
   sunshiming@sgepri.sgcc.com.cn; yinns@njit.edu.cn; chenwei@njit.edu.cn
FU Nanjing Industry-University-Research Collaboration Project [221722072]
FX The research work is supported by Nanjing Industry-University-Research
   Collaboration Project (Project #: 221722072). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Benoit Huet. (Corresponding author: Xue Yang.)
CR Abdi H., 2007, Encyclopedia of measurement and statistics, DOI DOI 10.4135/9781412952644.N299
   AlSumait L, 2008, IEEE DATA MINING, P3, DOI 10.1109/ICDM.2008.140
   [Anonymous], 2004, ACMMM
   Bekele ET, 2013, IEEE T NEUR SYS REH, V21, P289, DOI 10.1109/TNSRE.2012.2230188
   Bonarini A., 2016, P 7 INT C SOFTWARE D, P223, DOI DOI 10.1145/3019943.3019976
   Chapman  T., 2013, ENG SCH USES ROBOT H
   Chen C, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508042
   Chen C, 2012, INT J SEMANT COMPUT, V6, P135, DOI 10.1142/S1793351X12400053
   Chen CH, 2016, COMPUT HUM BEHAV, V55, P477, DOI 10.1016/j.chb.2015.09.033
   Chen HH, 2002, KLUW S INF, V12, P243
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Chen KY, 2007, IEEE T KNOWL DATA EN, V19, P1016, DOI 10.1109/TKDE.2007.1040
   Dianting Liu, 2010, 2010 IEEE International Conference on Information Reuse & Integration (IRI 2010), P171, DOI 10.1109/IRI.2010.5558944
   Drumm  E., 2015, P INT M AUT RES
   El Zein F, 2016, J DEV PHYS DISABIL, V28, P195, DOI 10.1007/s10882-015-9458-9
   Feng HH, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON HEALTHCARE INFORMATICS (ICHI 2013), P484, DOI 10.1109/ICHI.2013.72
   Forman G., 2003, Journal of Machine Learning Research, V3, P1289, DOI 10.1162/153244303322753670
   Gheyas IA, 2010, PATTERN RECOGN, V43, P5, DOI 10.1016/j.patcog.2009.06.009
   Crespo RG, 2011, COMPUT HUM BEHAV, V27, P1445, DOI 10.1016/j.chb.2010.09.012
   González-Díaz I, 2014, ADV COMPUT VIS PATT, P79, DOI 10.1007/978-3-319-05696-8_4
   Li XR, 2009, IEEE T MULTIMEDIA, V11, P1310, DOI 10.1109/TMM.2009.2030598
   Li YD, 2016, MULTIMED TOOLS APPL, V75, P11683, DOI 10.1007/s11042-015-2676-4
   Lin Lin, 2012, International Journal of Information and Decision Sciences, V4, P199, DOI 10.1504/IJIDS.2012.047073
   Liu L., 2008, P 17 INT C WORLD WID, P1009
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolov Tomas, 2013, EFFICIENT ESTIMATION
   Miskam MA, 2014, PROCEDIA COMPUT SCI, V42, P93, DOI 10.1016/j.procs.2014.11.038
   Mohanaiah P., 2013, INT J SCI RES PUB, V3, P1, DOI 10.5772/58692
   Ngo C-W., 2006, ACM MULTIMEDIA 06, P845
   Petrovic I, 2015, 2015 8TH INTERNATIONAL CONVENTION ON INFORMATION AND COMMUNICATION TECHNOLOGY, ELECTRONICS AND MICROELECTRONICS (MIPRO), P631, DOI 10.1109/MIPRO.2015.7160349
   Qian  H., 2013, MACH TOOL HYDRAUL, V23, P105
   Qian  H., 2014, MACH TOOL HYDRAUL, V15, P36
   Qian  H., 2011, MACH TOOL HYDRAUL, V39, P60
   Sargent G, 2016, MULTIMED TOOLS APPL, V75, P9073, DOI 10.1007/s11042-015-2863-3
   Shamsuddin  S., 2016, P IEEE 8 INT C SIGN, P188
   Shamsuddin S, 2012, PROCEDIA ENGINEER, V41, P1533, DOI 10.1016/j.proeng.2012.07.346
   Shyu ML, 2015, IEEE MULTIMEDIA, V22, P11, DOI 10.1109/MMUL.2015.84
   Tan S., 2010, MM, P1095
   Tapus A, 2012, INTERACT STUD, V13, P315, DOI 10.1075/is.13.3.01tap
   Wang ZB, 2015, IEEE T MOBILE COMPUT, V14, P538, DOI 10.1109/TMC.2014.2322373
   Wei Yan-hui, 2015, Control and Decision, V30, P1785, DOI 10.13195/j.kzyjc.2014.1109
   Westerveld M.F., 2016, Journal of Clinical Practice in Speech-Language Pathology, V18, P80, DOI [10.3109/17549507.2016.1159332, DOI 10.3109/17549507.2016.1159332]
   Wu XB, 2007, PROCEEDINGS OF THE 2007 INTERNATIONAL CONFERENCE ON AGRICULTURE ENGINEERING, P162
   Wu X, 2008, IEEE T MULTIMEDIA, V10, P188, DOI 10.1109/TMM.2007.911778
   Xiao Wu, 2012, Rough Sets and Current Trends in Computing. Proceedings 8th International Conference, RSCTC 2012, P219, DOI 10.1007/978-3-642-32115-3_26
   [杨颖 Yang Ying], 2014, [计算机应用与软件, Computer Applications and Software], V31, P109
   [虞欣 YU Xin], 2007, [测绘学报, Acta Geodetica et Cartographica Sinica], V36, P67
   Zhang CD, 2013, J COMPUT SCI TECH-CH, V28, P788, DOI 10.1007/s11390-013-1377-6
   Zhang CD, 2016, IEEE T HUM-MACH SYST, V46, P124, DOI 10.1109/THMS.2015.2489681
   Zhang CD, 2016, SIGNAL PROCESS, V120, P26, DOI 10.1016/j.sigpro.2015.08.002
   Zhang  T., 2013, SYST SIMUL TECHNOL, V9, P327
   Zhao WL, 2007, IEEE T MULTIMEDIA, V9, P1037, DOI 10.1109/TMM.2007.898928
   Zhao WL, 2010, IEEE T MULTIMEDIA, V12, P448, DOI 10.1109/TMM.2010.2050651
   Zhu  F.-Y., 2017, PICTURE BOOKS GAME E
   Zhu Jianke., 2008, P 16 ACM INT C MULTI, P41
   Zhu Q, 2014, INT J MULTIMED DATA, V5, P1, DOI 10.4018/ijmdem.2014100101
   Zhu QS, 2011, INT J MULTIMED DATA, V2, P34, DOI 10.4018/jmdem.2011070103
NR 57
TC 14
Z9 14
U1 4
U2 45
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2019
VL 21
IS 3
BP 746
EP 759
DI 10.1109/TMM.2018.2865828
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA HN6YJ
UT WOS:000460333800018
DA 2024-07-18
ER

PT J
AU Chen, JH
   Luo, ZJ
   Zhang, ZH
   Huang, FL
   Ye, ZL
   Takiguchi, T
   Hancock, ER
AF Chen, Jinhui
   Luo, Zhaojie
   Zhang, Zhihong
   Huang, Faliang
   Ye, Zhiling
   Takiguchi, Tetsuya
   Hancock, Edwin R.
TI Polar Transformation on Image Features for Orientation-Invariant
   Representations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Rotation-invariant and reversal-invariant representation; HOG; CNN
ID LOCAL BINARY PATTERNS; ROTATION-INVARIANT; FISHER VECTOR; DESCRIPTORS;
   RECOGNITION; MODEL
AB The choice of image feature representation plays a crucial role in the analysis of visual information. Although vast numbers of alternative robust feature representation models have been proposed to improve the performance of different visual tasks, most existing feature representations [e.g., handcrafted features or convolutional neural networks (CNNs)] have a relatively limited capacity to capture the highly orientation-invariant (rotation/reversal) features. The net consequence is suboptimal visual performance. To address these problems, this study adopts a novel transformational approach, which investigates the potential of using polar feature representations. Our low level consists of a histogram of oriented gradient, which is then binned using annular spatial bin-type cells applied to the polar gradient. This gives gradient binning invariance for feature extraction. In this way, the descriptors have significantly enhanced orientation-invariant capabilities. The proposed feature representation, called orientation-invariant histograms of oriented gradients, is capable of accurately processing visual tasks (e.g., facial expression recognition). In the context of the CNN architecture, we propose two polar convolution operations, referred to as full polar convolution and local polar convolution, and use these to develop polar architectures for the CNN orientation-invariant representation. Experimental results show that the proposed orientation-invariant image representation, based on polar models for both handcrafted features and deep learning features, is both competitive with state-of-the-art methods and maintains compact representation on a set of challenging benchmark image datasets.
C1 [Chen, Jinhui] Kobe Univ, Ctr Computat Social Sci, Kobe, Hyogo 6578501, Japan.
   [Luo, Zhaojie; Takiguchi, Tetsuya] Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo 6578501, Japan.
   [Zhang, Zhihong] Xiamen Univ, Sch Software, Xiamen 361005, Peoples R China.
   [Ye, Zhiling] Xiamen Univ, Sch Math Sci, Xiamen 361005, Peoples R China.
   [Huang, Faliang] Fujian Normal Univ, Coll Math & Informat, Fuzhou 350108, Peoples R China.
   [Hancock, Edwin R.] Univ York, Dept Comp Sci, York YO10 5DD, N Yorkshire, England.
C3 Kobe University; Kobe University; Xiamen University; Xiamen University;
   Fujian Normal University; University of York - UK
RP Zhang, ZH (corresponding author), Xiamen Univ, Sch Software, Xiamen 361005, Peoples R China.
EM chen@rieb.kobe-u.ac.jp; zhihong@xmu.edu.cn
RI Luo, Zhaojie/U-2472-2019; Chen, Jinhui/AFN-2369-2022; Hancock,
   Edwin/N-7548-2019; luo, zhaojie/AAA-5177-2021
OI Luo, Zhaojie/0000-0002-4173-6319; Chen, Jinhui/0000-0002-3701-9026;
   Hancock, Edwin/0000-0003-4496-2028; luo, zhaojie/0000-0002-4173-6319
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Angelova A, 2013, PROC CVPR IEEE, P811, DOI 10.1109/CVPR.2013.110
   [Anonymous], 2013, Tech. rep.
   [Anonymous], 2008, IEEE C COMP VIS PATT, DOI [10.1109/CVPR.2008.4587799, DOI 10.1109/CVPR.2008.4587799]
   [Anonymous], 2008, PROC BRIT MACH VIS C
   [Anonymous], 2013, 21 ACM INT C MULT, DOI DOI 10.1145/2502081.2502173
   [Anonymous], 2010, PROC 3 INT WORKSHOP
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2011, TECH REP CNS T 2011
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], 2008, P IEEE INT C COMP VI
   [Anonymous], 2015, PROC 28 INT C NEURAL
   Bay H., 2008, COMPUT VIS IMAGE UND, V10, P346, DOI DOI 10.1016/j.cviu.2007.09.014
   Bicego M., 2006, COMP VIS PATT REC WO, P35, DOI DOI 10.1109/CVPRW.2006.149
   Bourdev L, 2005, PROC CVPR IEEE, P236, DOI 10.1109/cvpr.2005.310
   Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P394, DOI 10.1109/TPAMI.2007.61
   Chai Y, 2013, IEEE I CONF COMP VIS, P321, DOI 10.1109/ICCV.2013.47
   Chen D, 2014, LECT NOTES COMPUT SC, V8694, P109, DOI 10.1007/978-3-319-10599-4_8
   Chen J., 2016, EURASIP Journal on Information Security, V1, P1, DOI DOI 10.18632/0NC0TARGET.9236
   Chen J, 2010, IEEE T PATTERN ANAL, V32, P1705, DOI 10.1109/TPAMI.2009.155
   Chen JH, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P443, DOI 10.1145/2671188.2749287
   Chew Sien W., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P915, DOI 10.1109/FG.2011.5771373
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Desai C, 2011, INT J COMPUT VISION, V95, P1, DOI 10.1007/s11263-011-0439-x
   Dhall A, 2012, IEEE MULTIMEDIA, V19, P34, DOI 10.1109/MMUL.2012.26
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Goesele M, 2007, IEEE I CONF COMP VIS, P825, DOI 10.1109/iccv.2007.4408933
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gosselin PH, 2014, PATTERN RECOGN LETT, V49, P92, DOI 10.1016/j.patrec.2014.06.011
   Haley GM, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pA262
   Han J, 2007, IMAGE VISION COMPUT, V25, P1474, DOI 10.1016/j.imavis.2006.12.015
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jhuang H., 2007, PROC IEEE INT C COMP
   Khan FS, 2012, PROC CVPR IEEE, P3306, DOI 10.1109/CVPR.2012.6248068
   Laptev D, 2016, PROC CVPR IEEE, P289, DOI 10.1109/CVPR.2016.38
   Le Quoc V, 2011, Advances in neural information processing systems, P1017
   Lin J, 2017, IEEE T MULTIMEDIA, V19, P1968, DOI 10.1109/TMM.2017.2713410
   Lin M., 2013, P 2 INT C LEARNING R
   Liu K, 2014, INT J COMPUT VISION, V106, P342, DOI 10.1007/s11263-013-0634-z
   Liu MY, 2015, LECT NOTES COMPUT SC, V9006, P143, DOI 10.1007/978-3-319-16817-3_10
   Liu MY, 2014, PROC CVPR IEEE, P1749, DOI 10.1109/CVPR.2014.226
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Müller H, 2017, IEEE T MULTIMEDIA, V19, P2093, DOI 10.1109/TMM.2017.2729400
   Murray N, 2014, PROC CVPR IEEE, P2473, DOI 10.1109/CVPR.2014.317
   Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P1487, DOI 10.1109/TIP.2017.2774041
   Radford A., 2015, ARXIV
   Rudovic O, 2012, PROC CVPR IEEE, P2634, DOI 10.1109/CVPR.2012.6247983
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Scovanner P., 2007, P 15 ACM INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Simonyan K., 2014, 14091556 ARXIV
   Sun L, 2014, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2014.336
   Takacs G, 2013, IEEE T IMAGE PROCESS, V22, P2970, DOI 10.1109/TIP.2012.2230011
   Trzcinski T, 2015, IEEE T PATTERN ANAL, V37, P597, DOI 10.1109/TPAMI.2014.2343961
   van Dyk DA, 2001, J COMPUT GRAPH STAT, V10, P1, DOI 10.1198/10618600152418584
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Vedaldi A, 2014, PROC CVPR IEEE, P3622, DOI 10.1109/CVPR.2014.463
   Voravuthikunchai W, 2014, PROC CVPR IEEE, P224, DOI 10.1109/CVPR.2014.36
   Wang LM, 2013, PROC CVPR IEEE, P2674, DOI 10.1109/CVPR.2013.345
   Wang XL, 2013, PROC CVPR IEEE, P3334, DOI 10.1109/CVPR.2013.428
   Wang ZL, 2015, INT J COMPUT VISION, V114, P322, DOI 10.1007/s11263-014-0739-z
   Xie LX, 2017, INT J COMPUT VISION, V123, P226, DOI 10.1007/s11263-016-0970-x
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zhang N, 2013, IEEE I CONF COMP VIS, P729, DOI 10.1109/ICCV.2013.96
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao WL, 2016, IEEE T MULTIMEDIA, V18, P1843, DOI 10.1109/TMM.2016.2585023
   Zhou YZ, 2017, PROC CVPR IEEE, P4961, DOI 10.1109/CVPR.2017.527
   Zhu L, 2010, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2010.5540096
NR 73
TC 7
Z9 8
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2019
VL 21
IS 2
BP 300
EP 313
DI 10.1109/TMM.2018.2856121
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HJ7GT
UT WOS:000457364400003
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Zhang, JJ
   Wu, Q
   Shen, CH
   Zhang, J
   Lu, JF
AF Zhang, Junjie
   Wu, Qi
   Shen, Chunhua
   Zhang, Jian
   Lu, Jianfeng
TI Multilabel Image Classification With Regional Latent Semantic
   Dependencies
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multilabel image classification; semantic dependence; deep neural
   network
ID ANNOTATION; GRADIENTS
AB Deep convolution neural networks (CNNs) have demonstrated advanced performance on single-label image classification, and various progress also has been made to apply CNN methods on multilabel image classification, which requires annotating objects, attributes, scene categories, etc., in a single shot. Recent state-of-the-art approaches to the multilabel image classification exploit the label dependencies in an image, at the global level, largely improving the labeling capacity. However, predicting small objects and visual concepts is still challenging due to the limited discrimination of the global visual features. In this paper, we propose a regional latent semantic dependencies model (RLSD) to address this problem. The utilized model includes a fully convolutional localization architecture to localize the regions that may contain multiple highly dependent labels. The localized regions are further sent to the recurrent neural networks to characterize the latent semantic dependencies at the regional level. Experimental results on several benchmark datasets show that our proposed model achieves the best performance compared to the state-of-the-art models, especially for predicting small objects occurring in the images. Also, we set up an upper bound model (RLSD+ft-RPN) using bounding-box coordinates during training, and the experimental results also show that our RLSD can approach the upper bound without using the bounding-box annotations, which is more realistic in the real world.
C1 [Zhang, Junjie; Lu, Jianfeng] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China.
   [Zhang, Junjie; Zhang, Jian] Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
   [Wu, Qi; Shen, Chunhua] Univ Adelaide, Australia Ctr Robot Vis, Adelaide, SA 5005, Australia.
   [Wu, Qi; Shen, Chunhua] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia.
C3 Nanjing University of Science & Technology; University of Technology
   Sydney; University of Adelaide; University of Adelaide
RP Lu, JF (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China.
EM Junjie.Zhang@student.uts.edu.au; qi.wu01@adelaide.edu.au;
   chunhua.shen@adelaide.edu.au; Jian.Zhang@uts.edu.au; lujf@njust.edu.cn
RI Zhang, Junjie/IZD-9295-2023; Wu, Qi/ABD-6304-2021
OI Wu, Qi/0000-0003-3631-256X; Zhang, Jian/0000-0002-7240-3541
FU National Key Research and Development Plan of China [2017YFB1300205]; 
   [CKCY2016082919273553]
FX This work was supported in part by CKCY2016082919273553 and in part by
   National Key Research and Development Plan of China under Grant
   2017YFB1300205. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. I. V. Bajic.
CR Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   [Anonymous], 2011, Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-220
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2010, P ACM SIGKDD
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2005, Proceedings of the 14th ACM International Conference on Information and Knowledge Management, CIKM'05, DOI [DOI 10.1145/1099554.1099591, 10.1145/1099554.1099591]
   [Anonymous], 2011, Advances in Neural Information Processing Systems
   [Anonymous], P ADV NEUR INF PROC
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   [Anonymous], 2015, PROC 28 INT C NEURAL
   Bradley J.K., 2010, Proceedings of the 27th International Conference on Machine Learning ICML-10, P127
   Breiman L., 2001, Mach. Learn., V45, P5
   Chen Q, 2015, IEEE T PATTERN ANAL, V37, P13, DOI 10.1109/TPAMI.2014.2343217
   Chen Q, 2012, PROC CVPR IEEE, P3426, DOI 10.1109/CVPR.2012.6248083
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dong J, 2013, PROC CVPR IEEE, P827, DOI 10.1109/CVPR.2013.112
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Gao SH, 2014, IEEE T MULTIMEDIA, V16, P762, DOI 10.1109/TMM.2014.2299516
   Gong Y., 2013, The characteristies of the petrology, geochronology and themetallogenetic of iron ore deposits in Tangquan-Gaoxing of southwestern Fujian province, P1
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P309, DOI 10.1109/ICCV.2009.5459266
   Han YH, 2014, IEEE T MULTIMEDIA, V16, P1115, DOI 10.1109/TMM.2014.2306092
   Harzallah H, 2009, IEEE I CONF COMP VIS, P237, DOI 10.1109/ICCV.2009.5459257
   Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li J, 2015, IEEE T NEUR NET LEAR, V26, P2111, DOI 10.1109/TNNLS.2014.2377211
   Li X, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P430
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu D., 2010, Proceedings of ACM International Conference on Multimedea, P25
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Oquab M., 2014, P NIPS, P1
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Pont-Tuset J, 2017, IEEE T PATTERN ANAL, V39, P128, DOI 10.1109/TPAMI.2016.2537320
   Ren M., 2015, Proc Adv Neural Inf Process Syst, V1, P5
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2014, 14091556 ARXIV
   Sundermeyer M, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P194
   Sutskever I, 2014, ADV NEUR IN, V27
   Tan MK, 2015, PROC CVPR IEEE, P4100, DOI 10.1109/CVPR.2015.7299037
   Tang JH, 2010, IEEE T MULTIMEDIA, V12, P131, DOI 10.1109/TMM.2009.2037373
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Uricchio T, 2017, PATTERN RECOGN, V71, P144, DOI 10.1016/j.patcog.2017.05.019
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929
   Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29
   Wu Q, 2016, PROC CVPR IEEE, P4622, DOI 10.1109/CVPR.2016.500
   Xu JJ, 2014, IEEE T MULTIMEDIA, V16, P403, DOI 10.1109/TMM.2013.2291218
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 57
TC 75
Z9 76
U1 1
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2018
VL 20
IS 10
SI SI
BP 2801
EP 2813
DI 10.1109/TMM.2018.2812605
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GT9YH
UT WOS:000444903000021
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chu, WT
   Wu, YL
AF Chu, Wei-Ta
   Wu, Yi-Ling
TI Image Style Classification Based on Learnt Deep Correlation Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Painting images; convolutional neural network; Gram matrix; deep
   correlation features; learnt correlation features
AB This paper presents a comprehensive study of deep correlation features on image style classification. Inspired by that, correlation between feature maps can effectively describe image texture, and we design various correlations and transform them into style vectors, and investigate classification performance brought by different variants. In addition to intralayer correlation, interlayer correlation is proposed as well, and its effectiveness is verified. After showing the effectiveness of deep correlation features, we further propose a learning framework to automatically learn correlations between feature maps. Through extensive experiments on image style classification and artist classification, we demonstrate that the proposed learnt deep correlation features outperform several variants of convolutional neural network features by a large margin, and achieve the state-of-the-art performance.
C1 [Chu, Wei-Ta; Wu, Yi-Ling] Natl Chung Cheng Univ, Min Hsiung 621, Taiwan.
C3 National Chung Cheng University
RP Chu, WT (corresponding author), Natl Chung Cheng Univ, Min Hsiung 621, Taiwan.
EM wtchu@cs.ccu.edu.tw; amtommy6@gmail.com
RI Chu, Wei-Ta/AAE-8471-2022
OI Chu, Wei-Ta/0000-0001-5722-7239
FU Ministry of Science and Technology of Taiwan [MOST
   105-2628-E-194-001-MY2, MOST 106-3114-E-002-009]
FX This work was supported in part by the Ministry of Science and
   Technology of Taiwan under Grants MOST 105-2628-E-194-001-MY2 and MOST
   106-3114-E-002-009.
CR [Anonymous], 2016, P 24 ACM INT C MULT
   Borth D., 2013, P 21 ACM INT C MULTI, P223, DOI 10.1145/2502081.2502282
   Champandard AJ, ARXIV160301768
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Dumoulin V.., 2017, P INT C LEARN REPR I
   Folego G, 2016, IEEE IMAGE PROC, P141, DOI 10.1109/ICIP.2016.7532335
   Garces E, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601131
   Gatys L., 2015, NIPS
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Hua Y, 2016, IEEE T MULTIMEDIA, V18, P1201, DOI 10.1109/TMM.2016.2535864
   Jain A, 1998, IEEE T IMAGE PROCESS, V7, P124, DOI 10.1109/83.650858
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karayev S., 2014, P BRIT MACH VIS C, P1, DOI [DOI 10.5244/C.28.122, 10.5244/c.28.122, 10.5244%2Fc.28.122, 10.5244/C.28.122]
   Khan FS, 2014, MACH VISION APPL, V25, P1385, DOI 10.1007/s00138-014-0621-6
   Li YH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2230
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Matsuo S, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P309, DOI 10.1145/2911996.2912057
   Peng KC, 2015, IEEE IMAGE PROC, P3057, DOI 10.1109/ICIP.2015.7351365
   PRISMA, 2016, PRISMA
   Ruder M, 2016, LECT NOTES COMPUT SC, V9796, P26, DOI 10.1007/978-3-319-45886-1_3
   Shen IC, 2015, IEEE T MULTIMEDIA, V17, P526, DOI 10.1109/TMM.2015.2405350
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tanno R, 2017, LECT NOTES COMPUT SC, V10133, P446, DOI 10.1007/978-3-319-51814-5_39
   Tarvainen J, 2014, IEEE T MULTIMEDIA, V16, P2085, DOI 10.1109/TMM.2014.2357688
   Tseng TE, 2016, INT CONF ACOUST SPEE, P1561, DOI 10.1109/ICASSP.2016.7471939
   Tyrolabs Blog, 2016, TYROLABS BLOG
   Ulyanov D., 2016, P 33 INT C INT C MAC, V48, P1349
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang Y, 2016, IEEE T MULTIMEDIA, V18, P1869, DOI 10.1109/TMM.2016.2581580
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Zeiler Matthew D, 2012, ARXIV12125701
   Zhang W, 2013, IEEE T MULTIMEDIA, V15, P1594, DOI 10.1109/TMM.2013.2265675
NR 37
TC 57
Z9 61
U1 1
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2018
VL 20
IS 9
BP 2491
EP 2502
DI 10.1109/TMM.2018.2801718
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GR1ZX
UT WOS:000442358200020
DA 2024-07-18
ER

PT J
AU Liu, YW
   Liu, JX
   Argyriou, A
   Ci, S
AF Liu, Yanwei
   Liu, Jinxia
   Argyriou, Antonios
   Ci, Song
TI 3DQoE-Oriented and Energy-Efficient 2D plus Depth Based 3D Video
   Streaming Over Centrally Controlled Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D video streaming; centrally controlled networks; SDN; OpenFlow; 3D
   quality of experience; energy efficiency; rate allocation
ID QUALITY ASSESSMENT; EXPERIENCE; TRANSMISSION; COMPRESSION
AB IP networks have become the dominant platform for video delivery. However, bandwidth-hungry video is pushing networks to their limits: costs are rising for the operators and the viewing experience is not always satisfactory for the users. When considering 3D video delivery, the previous problems are exacerbated because of the higher volume of data that must be communicated, and the difficulty in characterizing the viewing experience of the end user. Consequently, network operators may be reluctant to deliver 3D video due to casts and unclear quality improvements to their users. In this setting, the true immersive experience of 3D video remains elusive. In this paper, we focus on the efficient delivery of 3D video in terms of quality and energy cost over centrally controlled networks. As a representative example of a centrally controlled network, a software-defined network (SDN) is assumed. Our approach is based on a comprehensive network-dependent 3D quality of experience (3DQoE) model and an energy cost model for 3D video streaming. By using the developed models, we formulate the problem of energy-efficient and 3DQoE-optimized 3D video flow path routing. The particular characteristic of video/depth rate allocation presented in 3D video is embedded seamlessly into the selection of the optimal routing paths for multiple 3D video streams. The formulated problem is NP-hard and is solved with a heuristic algorithm based on the branch-and-bound method after significant reduction of the solution search space. Extensive 3D video streaming experiments are conducted over an OpenFlow-based SDN with subjective and objective evaluations and they highlight the significant benefits of the proposed approach.
C1 [Liu, Yanwei] Chinese Acad Sci, State Key Lab Informat Secur, Inst Informat Engn, Beijing 100049, Peoples R China.
   [Liu, Yanwei] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100049, Peoples R China.
   [Liu, Jinxia] Zhejiang Wanli Univ, Ningbo 315100, Zhejiang, Peoples R China.
   [Argyriou, Antonios] Univ Thessaly, Dept Elect & Comp Engn, Volos 38221, Greece.
   [Ci, Song] Univ Nebraska Lincoln, Omaha, NE 68046 USA.
C3 Chinese Academy of Sciences; Institute of Information Engineering, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Zhejiang Wanli University; University of Thessaly; University of
   Nebraska System; University of Nebraska Lincoln
RP Liu, JX (corresponding author), Zhejiang Wanli Univ, Ningbo 315100, Zhejiang, Peoples R China.
EM liuyanwei@iie.ac.cn; liujinxia1969@126.com; anargyr@ieee.org;
   sci@engr.unl.edu
RI Ci, Song/R-8324-2019; Argyriou, Antonios/AAF-9586-2021; Liu,
   Jinxia/H-1794-2011; liu, yanwei/L-2453-2019
OI Argyriou, Antonios/0000-0002-2510-3124; 
FU NSFC [61771469, 61472388]; Zhejiang Provincial Natural Science
   Foundation of China [LY17F010001]
FX This work was supported in part by NSFC under Grants 61771469 and
   61472388 and in part by the Zhejiang Provincial Natural Science
   Foundation of China under Grant LY17F010001.
CR Agarwal S, 2013, IEEE INFOCOM SER, P2211
   Amaldi E, 2013, COMPUT NETW, V57, P1503, DOI 10.1016/j.comnet.2013.02.006
   Andreopoulos Y, 2006, IEEE J SEL AREA COMM, V24, P2104, DOI 10.1109/JSAC.2006.881614
   [Anonymous], P OP NETW SUMM MAR
   [Anonymous], BT2021 ITUR
   [Anonymous], 2010, JTC1SC29WG11 ISOIEC
   [Anonymous], 2014, P EN
   [Anonymous], 1992, FUZZY MEASURE THEORY
   [Anonymous], BT50011 ITUR
   [Anonymous], G100P10 ITUT
   [Anonymous], FOR HIGHL 2012 2017
   [Anonymous], 1992, Data networks
   [Anonymous], CS244A INTRO COMPUTE
   [Anonymous], NOW 3D JOIN EXP
   [Anonymous], SOFTW VIEW SYNTH
   [Anonymous], 2012, P SPIE
   [Anonymous], OpenFlow switch specification 1.5.0
   [Anonymous], 2010, Proceedings of the 7th international conference on Autonomic computing, ICAC '10
   [Anonymous], OP SVC DEC
   [Anonymous], INST VIRT NETW YOUR
   Apostolopoulos J, 2002, IEEE INFOCOM SER, P1736, DOI 10.1109/INFCOM.2002.1019427
   Arefin A., 2013, PROC INT C NETW PROT, P1
   Bozakov Zdravko., 2016, Proc. IEEE INFOCOM 2016, P1
   Chabarek J, 2008, IEEE INFOCOM SER, P1130, DOI 10.1109/infocom.2008.93
   Cheng M., 2012, 2012 8th International Symposium on Communication Systems, Networks Digital Signal Processing (CSNDSP), P1
   Dong Zhang, 2010, Proceedings 2010 Third International Conference on Advances in Mesh Networks (MESH 2010), P1, DOI 10.1109/MESH.2010.11
   Egilmez HE, 2014, IEEE T MULTIMEDIA, V16, P1597, DOI 10.1109/TMM.2014.2325791
   Egilmez HE, 2013, IEEE T MULTIMEDIA, V15, P710, DOI 10.1109/TMM.2012.2232645
   Fu B., 2013, P 8 ACM WORKSHOP PER, P173
   Gomez K, 2012, COMPUT NETW, V56, P2506, DOI 10.1016/j.comnet.2012.03.028
   Habachi O, 2012, IEEE J SEL AREA COMM, V30, P1225, DOI 10.1109/JSAC.2012.120808
   Hewage CTER, 2013, IEEE COMMUN MAG, V51, P101, DOI 10.1109/MCOM.2013.6515053
   Hewage CTER, 2012, IEEE J-STSP, V6, P471, DOI 10.1109/JSTSP.2012.2195155
   Hewage CTER, 2011, IEEE T CONSUM ELECTR, V57, P1185, DOI 10.1109/TCE.2011.6018873
   Hewage CTER, 2009, IEEE J-STSP, V3, P304, DOI 10.1109/JSTSP.2009.2014805
   Huynh-Thu Q, 2010, IEEE IMAGE PROC, P4025, DOI 10.1109/ICIP.2010.5650571
   Karlsson LS, 2011, IEEE T CIRC SYST VID, V21, P742, DOI 10.1109/TCSVT.2011.2130350
   Keller E, 2012, PROCEEDINGS OF THE 11TH ACM WORKSHOP ON HOT TOPICS IN NETWORKS (HOTNETS-XI), P109
   Khan A, 2012, IEEE T MULTIMEDIA, V14, P431, DOI 10.1109/TMM.2011.2176324
   Kim T, 2014, IEEE T MULTIMEDIA, V16, P387, DOI 10.1109/TMM.2013.2292592
   Lebreton P, 2012, IEEE J-STSP, V6, P710, DOI 10.1109/JSTSP.2012.2213236
   Lee U, 2011, IEEE NETWORK, V25, P14, DOI 10.1109/MNET.2011.5730523
   Liu Y., 2013, PROC IEEE VISUAL COM, P1
   Liu YW, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2348816.2348821
   Liu YW, 2009, SIGNAL PROCESS-IMAGE, V24, P666, DOI 10.1016/j.image.2009.06.002
   Liu YH, 2016, INT CONF SOFTW ENG, P1, DOI 10.1109/ICSESS.2016.7883004
   Malekmohamadi H, 2012, IEEE INT CONF MULTI, P581, DOI 10.1109/ICMEW.2012.107
   Maratsolas E, 2014, IEEE T MULTIMEDIA, V16, P1446, DOI 10.1109/TMM.2014.2310592
   Martini MG, 2012, IEEE J SEL AREA COMM, V30, P1153, DOI 10.1109/JSAC.2012.120801
   McKeown N, 2008, ACM SIGCOMM COMP COM, V38, P69, DOI 10.1145/1355734.1355746
   Müller K, 2013, IEEE T IMAGE PROCESS, V22, P3366, DOI 10.1109/TIP.2013.2264820
   Politis I, 2012, IEEE GLOBE WORK, P1335, DOI 10.1109/GLOCOMW.2012.6477776
   Tam WJ, 2011, IEEE T BROADCAST, V57, P335, DOI 10.1109/TBC.2011.2125070
   Thatte J., 2016, 2016 IEEE International Conference on Multimedia and Expo (ICME), P1
   Urvoy M, 2013, ANN TELECOMMUN, V68, P641, DOI 10.1007/s12243-013-0394-3
   Vetro A, 2011, IEEE T BROADCAST, V57, P384, DOI 10.1109/TBC.2010.2102950
   Video Quality Experts Group, 2000, Final report from the video quality experts group on the validation of objective models of video quality assessment march 2000
   Wang K, 2012, IEEE T BROADCAST, V58, P544, DOI 10.1109/TBC.2012.2191031
   Wang ZY, 2005, FUZZY SET SYST, V156, P371, DOI 10.1016/j.fss.2005.05.034
   Xue NN, 2015, IEEE T MULTIMEDIA, V17, P1617, DOI 10.1109/TMM.2015.2450014
   Yasakethu SLP, 2008, IEEE T CONSUM ELECTR, V54, P1969, DOI 10.1109/TCE.2008.4711260
   Zhu XQ, 2005, SIGNAL PROCESS-IMAGE, V20, P773, DOI 10.1016/j.image.2005.05.005
   Zukerman M, 2003, IEEE INFOCOM SER, P587
NR 63
TC 5
Z9 5
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2018
VL 20
IS 9
BP 2439
EP 2453
DI 10.1109/TMM.2018.2806221
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GR1ZX
UT WOS:000442358200016
DA 2024-07-18
ER

PT J
AU Lin, CY
   Zhao, Y
   Xiao, JM
   Tillo, T
AF Lin, Chunyu
   Zhao, Yao
   Xiao, Jimin
   Tillo, Tammam
TI Region-Based Multiple Description Coding for Multiview Video Plus Depth
   Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multiple description coding; multiview video plus depth; video coding
ID 3D VIDEO; ALLOCATION; TEXTURE
AB Interframe and interview predictions are widely employed in multiview video coding. This technique improves the coding efficiency, but it also increases the vulnerability of the coded bitstream. Thus, one packet loss will affect many subsequent frames in the same view and probably in other referenced views. To address this problem, a region-based multiple description coding scheme is proposed for robust 3-D video communication in this paper, in which two descriptions are formed by setting the left and right view as dominant in the first and second description, respectively. This approach exploits the fact that most regions in the reference view could be synthesized from the base view. Hence, these regions could be skipped or only coarsely encoded. In our work, the disoccluded regions, illumination-affected regions, and remaining regions are first determined and extracted. By assigning different quantization parameters for these three different regions according to the network status, an efficient multiple description scheme is formed. Experimental results demonstrate that the proposed scheme achieves considerably better performance compared with the traditional approach.
C1 [Lin, Chunyu; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing Key Lab Adv Informat Sci & Network, Beijing 100044, Peoples R China.
   [Xiao, Jimin; Tillo, Tammam] Xian Jiaotong Liverpool Univ, Suzhou 215123, Peoples R China.
   [Tillo, Tammam] Libera Univ Bolzano Bozen Unibz, I-39100 Bolzano, Italy.
C3 Beijing Jiaotong University; Xi'an Jiaotong-Liverpool University; Free
   University of Bozen-Bolzano
RP Lin, CY (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing Key Lab Adv Informat Sci & Network, Beijing 100044, Peoples R China.
EM cylin@bjtu.edu.cn; yzhao@bjtu.edu.cn; jimin.xiao@xjtlu.edu.cn;
   Tammam.Tillo@unibz.it
RI Lin, Chunyu/AAI-5185-2021
OI Lin, Chunyu/0000-0003-2847-0349
FU National Natural Science Foundation of China [61772066, 61210006,
   61501379]; Beijing Natural Science Foundation [KZ201610005007]
FX This work was supported in part by the National Natural Science
   Foundation of China (No. 61772066, No. 61210006 and 61501379) and in
   part by the Beijing Natural Science Foundation (No. KZ201610005007).
CR [Anonymous], 2011, CALL PROP 3D VID COD
   Chen Y., 2014, 16WP3 JCT3V ITUT SG
   Electronics and Telecommunications Research Institute, 2013, 3DV TEST SEQ
   Fraunhofer, 2013, 3DV TEST SEQ
   Gao P, 2014, IEEE T MULTIMEDIA, V16, P1797, DOI 10.1109/TMM.2014.2331013
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P74, DOI 10.1109/79.952806
   Guo JS, 2015, IEEE DATA COMPR CONF, P446, DOI 10.1109/DCC.2015.13
   Gwangju Institute of Science and Technology, 2013, 3DV TEST SEQ
   Jin J, 2016, IEEE T MULTIMEDIA, V18, P953, DOI 10.1109/TMM.2016.2539825
   Karim HA, 2009, IEEE T CONSUM ELECTR, V55, P2048, DOI 10.1109/TCE.2009.5373768
   Kim D.-Y., 2012, 16WP3 ITUT SG JOINT
   Lee JY, 2016, IEEE T CIRC SYST VID, V26, P1107, DOI 10.1109/TCSVT.2015.2441491
   Lee JY, 2015, IEEE T CIRC SYST VID, V25, P1347, DOI 10.1109/TCSVT.2014.2380191
   Lin CY, 2015, IEEE T CIRC SYST VID, V25, P1016, DOI 10.1109/TCSVT.2014.2367391
   Lin CY, 2011, IEEE T CIRC SYST VID, V21, P589, DOI 10.1109/TCSVT.2011.2129270
   Macchiavello B, 2014, IEEE T MULTIMEDIA, V16, P711, DOI 10.1109/TMM.2014.2299768
   Merkle P, 2007, IEEE IMAGE PROC, P201
   Nagoya University, 2013, 3DV TEST SEQ
   Nokia, 2013, 3DV TEST SEQ
   Norkin A, 2006, LECT NOTES COMPUT SC, V4105, P730
   Oh BT, 2014, IEEE T CIRC SYST VID, V24, P1006, DOI 10.1109/TCSVT.2013.2290577
   Rusanovskyy D., 2013, JCT3VF1003M
   Shao F, 2013, IEEE T MULTIMEDIA, V15, P1843, DOI 10.1109/TMM.2013.2269897
   Vetro A, 2011, IEEE T BROADCAST, V57, P384, DOI 10.1109/TBC.2010.2102950
   Vosoughi A, 2013, INT CONF ACOUST SPEE, P2050, DOI 10.1109/ICASSP.2013.6638014
   Wang XL, 2013, IEEE DATA COMPR CONF, P527, DOI 10.1109/DCC.2013.106
   Wang Y, 2005, P IEEE, V93, P57, DOI 10.1109/JPROC.2004.839618
   Wu JJ, 2017, IEEE T IMAGE PROCESS, V26, P2682, DOI 10.1109/TIP.2017.2685682
   Zhu C, 2016, IEEE T BROADCAST, V62, P482, DOI 10.1109/TBC.2016.2550762
NR 29
TC 8
Z9 8
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2018
VL 20
IS 5
BP 1209
EP 1223
DI 10.1109/TMM.2017.2766043
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GD7YD
UT WOS:000430728400015
DA 2024-07-18
ER

PT J
AU Su, LC
   Li, CH
   Lai, YC
   Yang, JM
AF Su, Lichao
   Li, Cuihua
   Lai, Yuecong
   Yang, Jianmei
TI A Fast Forgery Detection Algorithm Based on Exponential-Fourier Moments
   for Video Region Duplication
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video forgery; region duplication; passive forensics; fast algorithm
AB Region duplication is one of the most common methods of video forgery. Existing forgery detection algorithms generally suffer from inefficiency and are not effective for the forged regions with mirroring. To address these problems, we present a fast forgery detection algorithm based on Exponential Fourier moments (EFMs) for detecting region duplication in videos. The algorithm first extracts EFMs features from each block in the current frame, and performs a fast match to find potential matching pairs. Then, a postverification scheme is designed to eliminate falsely matched pairs and locate the altered regions in the current frame. Finally, an adaptive parameter-based fast compression tracking algorithm is used to track the tampered regions in the subsequent frames. The experimental results show that our proposed algorithm has higher detection accuracy and computational efficiency than those of previous algorithms.
C1 [Su, Lichao; Li, Cuihua; Lai, Yuecong; Yang, Jianmei] Xiamen Univ, Sch Informat Sci & Engn, Xiamen 361005, Peoples R China.
C3 Xiamen University
RP Su, LC (corresponding author), Xiamen Univ, Sch Informat Sci & Engn, Xiamen 361005, Peoples R China.
EM 651424071@qq.com; fisher-slc@163.com; sa@11.com; 786313129@qq.com
OI Su, Lichao/0000-0003-2641-5901
FU National Natural Science Foundation of China [61373077]; Specialized
   Research Fund for the Doctoral Program of Higher Education of China
   [20110121110020]; National Defense Basic Scientific Research Program of
   China
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61373077, in part by the Specialized
   Research Fund for the Doctoral Program of Higher Education of China
   under Grant 20110121110020, and in part by the National Defense Basic
   Scientific Research Program of China. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Balakrishnan Prabhakaran.
CR Amerini I, 2011, IEEE T INF FOREN SEC, V6, P1099, DOI 10.1109/TIFS.2011.2129512
   [Anonymous], 2013, FAST COMPRESSIVE TRA
   [Anonymous], 2011, THESIS
   Bestagini P, 2012, EUR SIGNAL PR CONF, P1229
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   Chen W, 2009, LECT NOTES COMPUT SC, V5450, P16, DOI 10.1007/978-3-642-04438-0_2
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   DIACONIS P, 1984, ANN STAT, V12, P793, DOI 10.1214/aos/1176346703
   Feng XY, 2012, IEEE T MULTIMEDIA, V14, P536, DOI 10.1109/TMM.2012.2191946
   Hsu CC, 2008, 2008 IEEE 10TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, VOLS 1 AND 2, P170, DOI 10.1109/MMSP.2008.4665069
   Hu HT, 2014, PATTERN RECOGN, V47, P2596, DOI 10.1016/j.patcog.2014.02.014
   Kobayashi M, 2009, LECT NOTES COMPUT SC, V5414, P306, DOI 10.1007/978-3-540-92957-4_27
   [赖玥聪 Lai Yuecong], 2015, [中国图象图形学报, Journal of Image and Graphics], V20, P1212
   Lee JC, 2015, INFORM SCIENCES, V321, P250, DOI 10.1016/j.ins.2015.03.009
   Li YA, 2013, FORENSIC SCI INT, V224, P59, DOI 10.1016/j.forsciint.2012.10.031
   Liao SY, 2013, 2013 6TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), VOLS 1-3, P864, DOI 10.1109/CISP.2013.6745286
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ng AY, 2002, ADV NEUR IN, V14, P841
   Ping Z., 2014, P SPIE INT SOC OPT E, V9159, P4177
   Pun CM, 2015, IEEE T INF FOREN SEC, V10, P1705, DOI 10.1109/TIFS.2015.2423261
   Qadir G., 2012, IET C IMAGE PROCESSI
   Ren HP, 2003, J OPT SOC AM A, V20, P631, DOI 10.1364/JOSAA.20.000631
   Ryu SJ, 2013, IEEE T INF FOREN SEC, V8, P1355, DOI 10.1109/TIFS.2013.2272377
   Stütz T, 2014, IEEE T MULTIMEDIA, V16, P1337, DOI 10.1109/TMM.2014.2310595
   Subramanyam AV, 2012, IEEE INT WORKSH MULT, P89, DOI 10.1109/MMSP.2012.6343421
   Tabatabaei SAH, 2015, IEEE T MULTIMEDIA, V17, P945, DOI 10.1109/TMM.2015.2432672
   Wang WH, 2009, MM&SEC'09: PROCEEDINGS OF THE 2009 ACM SIGMM MULTIMEDIA AND SECURITY WORKSHOP, P39
   Wang WH, 2007, MM&SEC'07: PROCEEDINGS OF THE MULTIMEDIA & SECURITY WORKSHOP 2007, P35
   Yang JM, 2016, MULTIMED TOOLS APPL, V75, P1793, DOI 10.1007/s11042-014-2374-7
   Yap PT, 2010, IEEE T PATTERN ANAL, V32, P1259, DOI 10.1109/TPAMI.2009.119
   Yin H, 2012, IEEE T MULTIMEDIA, V14, P178, DOI 10.1109/TMM.2011.2170556
   Zhang KH, 2014, IEEE T PATTERN ANAL, V36, P2002, DOI 10.1109/TPAMI.2014.2315808
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhao WL, 2013, IEEE T IMAGE PROCESS, V22, P980, DOI 10.1109/TIP.2012.2226043
NR 34
TC 51
Z9 54
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2018
VL 20
IS 4
BP 825
EP 840
DI 10.1109/TMM.2017.2760098
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FZ5HM
UT WOS:000427623000005
DA 2024-07-18
ER

PT J
AU Zhang, W
   Zhang, WD
   Liu, K
   Gu, J
AF Zhang, Wei
   Zhang, Weidong
   Liu, Kan
   Gu, Jason
TI A Feature Descriptor Based on Local Normalized Difference for Real-World
   Texture Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Normalized difference vector; feature extraction; texture descriptor;
   texture representation
ID IMAGE RETRIEVAL; INVARIANT; SCALE; APPEARANCE; COLOR
AB In this paper, we propose a normalized difference vector (NDV) for texture representation. Compared to local binary-pattern-based descriptors, the proposed NDV takes full advantage of the local difference, and the size can be extended flexibly to cover a large local region. We further employ the bag-of-words model to integrate the local descriptors into a global feature representation of an image. In addition, two strategies are introduced for the proposed NDV to achieve rotation invariance. We test the proposed texture descriptor on benchmark datasets, such as AniTex, VehApp, KTH-TIPS2a, OpenSurface, and Kylberg. Classification results demonstrate the superiority of the proposed descriptor over state-of-the-art methods.
C1 [Zhang, Wei; Zhang, Weidong; Liu, Kan; Gu, Jason] Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Shandong, Peoples R China.
C3 Shandong University
RP Zhang, WD (corresponding author), Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Shandong, Peoples R China.
EM davidzhangsdu@gmail.com; chluzhre@gmail.com; sakuraxiafan@gmail.com;
   jasongusdu@gmail.com
OI Gu, Jason Jianjun/0000-0002-7626-1077
FU National Natural Science Foundation of China [61573222, 61233014]; Major
   Research Program of Shandong Province [2015ZDXX0801A02]; Fundamental
   Research Funds of Shandong University [2016JC014]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61573222 and 61233014, Major Research Program of
   Shandong Province (2015ZDXX0801A02), and The Fundamental Research Funds
   of Shandong University (2016JC014). The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Marco Bertini.
CR Allili MS, 2014, IEEE T MULTIMEDIA, V16, P772, DOI 10.1109/TMM.2014.2298832
   Andrearczyk V, 2016, PATTERN RECOGN LETT, V84, P63, DOI 10.1016/j.patrec.2016.08.016
   [Anonymous], P BRIT MACH VIS C PI
   [Anonymous], VIS COMPUT
   [Anonymous], 2013, 31 INT C MACH LEARN
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2008, REAL LIF IM WORKSH E
   [Anonymous], 2004, P 2004WORKSHOP STAT
   [Anonymous], P 6 ACM SIGMM INT WO
   [Anonymous], 2011, KYLBERG TEXTURE DATA
   [Anonymous], P IEEE C COMP VIS PA
   Bell S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462002
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Caputo B, 2005, IEEE I CONF COMP VIS, P1597, DOI 10.1109/iccv.2005.54
   Chen J, 2010, IEEE T PATTERN ANAL, V32, P1705, DOI 10.1109/TPAMI.2009.155
   Chun YD, 2008, IEEE T MULTIMEDIA, V10, P1073, DOI 10.1109/TMM.2008.2001357
   Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007
   Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gao T, 2017, MULTIDIM SYST SIGN P, V28, P281, DOI 10.1007/s11045-015-0379-7
   Guo YM, 2012, PATTERN RECOGN, V45, P3834, DOI 10.1016/j.patcog.2012.04.003
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Guo ZH, 2010, PATTERN RECOGN, V43, P706, DOI 10.1016/j.patcog.2009.08.017
   Hayman E, 2004, LECT NOTES COMPUT SC, V2034, P253
   Leung T, 2001, INT J COMPUT VISION, V43, P29, DOI 10.1023/A:1011126920638
   Lin TY, 2016, PROC CVPR IEEE, P2791, DOI 10.1109/CVPR.2016.305
   Liu L, 2017, PATTERN RECOGN, V62, P135, DOI 10.1016/j.patcog.2016.08.032
   Liu L, 2011, IEEE I CONF COMP VIS, P391, DOI 10.1109/ICCV.2011.6126267
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maani R, 2013, PATTERN RECOGN, V46, P2103, DOI 10.1016/j.patcog.2013.01.014
   Maani R, 2013, IEEE T IMAGE PROCESS, V22, P2409, DOI 10.1109/TIP.2013.2249081
   Maji S., 2008, PROC IEEE CON COMPUT, P1
   Mao JH, 2014, LECT NOTES COMPUT SC, V8691, P140, DOI 10.1007/978-3-319-10578-9_10
   Margolin R, 2014, LECT NOTES COMPUT SC, V8695, P377, DOI 10.1007/978-3-319-10584-0_25
   Meng XL, 2012, PATTERN RECOGN, V45, P373, DOI 10.1016/j.patcog.2011.06.012
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Qi XB, 2014, IEEE T PATTERN ANAL, V36, P2199, DOI 10.1109/TPAMI.2014.2316826
   Ren JF, 2017, IEEE T MULTIMEDIA, V19, P447, DOI 10.1109/TMM.2016.2618218
   Sharma G, 2016, COMPUT VIS IMAGE UND, V142, P13, DOI 10.1016/j.cviu.2015.09.007
   Sharma G, 2012, LECT NOTES COMPUT SC, V7578, P1, DOI 10.1007/978-3-642-33786-4_1
   Sifre L, 2013, PROC CVPR IEEE, P1233, DOI 10.1109/CVPR.2013.163
   Simo-Serra E, 2015, IEEE I CONF COMP VIS, P118, DOI 10.1109/ICCV.2015.22
   Song ZC, 2017, IEEE T MULTIMEDIA, V19, P702, DOI 10.1109/TMM.2016.2631123
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Wang XY, 2014, MULTIMED TOOLS APPL, V68, P545, DOI 10.1007/s11042-012-1055-7
   Wang XG, 2009, PROC CVPR IEEE, P142, DOI 10.1109/CVPRW.2009.5206736
   Xu Y, 2010, PROC CVPR IEEE, P161, DOI 10.1109/CVPR.2010.5540217
   Zhang N, 2013, IEEE I CONF COMP VIS, P729, DOI 10.1109/ICCV.2013.96
   Zhao GY, 2009, IEEE T MULTIMEDIA, V11, P1254, DOI 10.1109/TMM.2009.2030637
   Zhao Y, 2012, IEEE T IMAGE PROCESS, V21, P4492, DOI 10.1109/TIP.2012.2204271
NR 54
TC 31
Z9 31
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2018
VL 20
IS 4
BP 880
EP 888
DI 10.1109/TMM.2017.2760102
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FZ5HM
UT WOS:000427623000009
DA 2024-07-18
ER

PT J
AU Li, SX
   Xu, M
   Ren, Y
   Wang, ZL
AF Li, Shengxi
   Xu, Mai
   Ren, Yun
   Wang, Zulin
TI Closed-Form Optimization on Saliency-Guided Image Compression for
   HEVC-MSP
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE High efficiency video coding (HEVC); perceptual image compression;
   saliency detection
ID QUALITY ASSESSMENT; VISUAL MASKING; REGION; ALGORITHM; MODEL
AB High efficiency video coding (HEVC) is the latest video coding standard, and it has the best performance among all the existing standards. HEVC main still picture profile (HEVC-MSP) also achieves top performance in image compression. In this paper, we propose a closed-form bit allocation approach to optimize the saliency-guided PSNR (viewed as perceptual distortion) such that the coding efficiency of HEVC-based image compression can be significantly improved from a subjective perspective. Specifically, a bit allocation formulation is established to minimize perceptual distortion with a constraint on bit-rates. Then, this formulation is solved using the proposed recursive Taylor expansion method with a closed-form solution. On the basis of our solution, a bit allocation and re-allocation process is developed in our approach to minimize perceptual distortion, meanwhile accurately controlling bit-rates. In addition, we provide both theoretical and numerical analyses of the computational complexity, verifying the little extra time cost of our approach. The experimental results demonstrate the superior performance of our approach over the state-of-the-art HEVC-MSP, and the BD-rate savings are approximately 40% and 24% for face and generic images, respectively.
C1 [Li, Shengxi; Xu, Mai; Ren, Yun; Wang, Zulin] Beihang Univ, Sch Elect & Informat Engn, Beijing 100191, Peoples R China.
   [Wang, Zulin] Wuhan Univ, Collaborat Innovat Ctr Geospatial Technol, Wuhan 430072, Hubei, Peoples R China.
C3 Beihang University; Wuhan University
RP Xu, M (corresponding author), Beihang Univ, Sch Elect & Informat Engn, Beijing 100191, Peoples R China.
EM ShengxiLi2014@gmail.com; MaiXu@buaa.edu.cn; Yunren@buaa.edu.cn;
   wzulin@buaa.edu.cn
FU National Nature Science Foundation of China [61573037]; Fok Ying Tung
   Education Foundation [151061]
FX This work was supported by the National Nature Science Foundation of
   China under Grant 61573037 and by the Fok Ying Tung Education Foundation
   under Grant 151061. This paper was presented at the Data Compression
   Conference, Snowbird, UT, USA, March-April 2016 [1]. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Balakrishnan Prabhakaran.
CR [Anonymous], 2000, 1SC29WG1 ISOISC JTC
   [Anonymous], 1996, FDN VISION
   Bankoski J, 2013, PROC SPIE, V8666, DOI 10.1117/12.2009777
   Beghdadi A, 2013, SIGNAL PROCESS-IMAGE, V28, P811, DOI 10.1016/j.image.2013.06.003
   Chandler DM, 2005, IEEE T IMAGE PROCESS, V14, P397, DOI 10.1109/TIP.2004.841196
   Channappayya SS, 2008, IEEE T IMAGE PROCESS, V17, P1624, DOI 10.1109/TIP.2008.2001400
   Channappayya SS, 2008, IEEE T IMAGE PROCESS, V17, P857, DOI 10.1109/TIP.2008.921328
   Chen OTC, 2007, IEEE T MULTIMEDIA, V9, P1333, DOI 10.1109/TMM.2007.906572
   Chen Y. T. H. W. Shu-Ching, 2015, IEEE T MULTIMEDIA, V17, P261
   Developers G., 2010, NEW IMAGE FORMAT WEB
   Doulamis N, 1998, IEEE T CIRC SYST VID, V8, P928, DOI 10.1109/76.736718
   Dufaux F, 2009, IEEE SIGNAL PROC MAG, V26, P195, DOI 10.1109/MSP.2009.934187
   Ebrahimi-Moghadam A, 2005, IEEE T MULTIMEDIA, V7, P680, DOI 10.1109/TMM.2005.850967
   Fan S., 1989, NAT SCI J HAINAN TEA, V2, P91
   Gibson J.J., 1950, PERCEPTION VISUAL WO
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Höntsch I, 2002, IEEE T IMAGE PROCESS, V11, P213, DOI 10.1109/83.988955
   Höntsch I, 2000, IEEE T IMAGE PROCESS, V9, P1472, DOI 10.1109/83.862622
   Hua KL, 2015, J INF SCI ENG, V31, P475
   Khanna MT, 2015, PERCEPTION AND MACHINE INTELLIGENCE, 2015, P218, DOI 10.1145/2708463.2709063
   Koch K, 2006, CURR BIOL, V16, P1428, DOI 10.1016/j.cub.2006.05.056
   Lainema J, 2012, IEEE T CIRC SYST VID, V22, P1792, DOI 10.1109/TCSVT.2012.2221525
   Lee JS, 2012, IEEE J-STSP, V6, P684, DOI 10.1109/JSTSP.2012.2215006
   Leung R, 2009, IEEE T CIRC SYST VID, V19, P309, DOI 10.1109/TCSVT.2009.2017078
   Li B, 2014, IEEE T IMAGE PROCESS, V23, P3841, DOI 10.1109/TIP.2014.2336550
   Li J, 2003, IEEE T MULTIMEDIA, V5, P581, DOI [10.1109/TMM.2003.813284, 10.1109/TTM.2003.813284]
   Li SX, 2016, IEEE DATA COMPR CONF, P437, DOI 10.1109/DCC.2016.10
   Li ZC, 2011, IMAGE VISION COMPUT, V29, P1, DOI 10.1016/j.imavis.2010.07.001
   Liu LJ, 2003, IEEE SIGNAL PROC LET, V10, P35, DOI 10.1109/LSP.2002.807867
   Liu Z, 2006, IEEE T IMAGE PROCESS, V15, P1763, DOI 10.1109/TIP.2006.873460
   Ma KD, 2015, IEEE T IMAGE PROCESS, V24, P3086, DOI [10.1109/TIP.2015.2436340, 10.1109/TIP.2015.2456638]
   MANNOS JL, 1974, IEEE T INFORM THEORY, V20, P525, DOI 10.1109/TIT.1974.1055250
   Marta K., 2013, JCTVCM0257
   Masmoudi K, 2013, SIGNAL PROCESS-IMAGE, V28, P856, DOI 10.1016/j.image.2012.07.005
   Müller H, 2004, INT J MED INFORM, V73, P1, DOI 10.1016/j.ijmedinf.2003.11.024
   NILL NB, 1985, IEEE T COMMUN, V33, P551, DOI 10.1109/TCOM.1985.1096337
   Niu Y, 2012, IEEE T IMAGE PROCESS, V21, P1899, DOI 10.1109/TIP.2011.2171352
   Park KH, 2002, IEEE T CIRC SYST VID, V12, P106, DOI 10.1109/76.988657
   Prakash A., 2016, CORR
   Schonberg D., 2009, P SOC PHOTO-OPT INS, V7443
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Strom J, 1997, SIGNAL PROCESS, V59, P155, DOI 10.1016/S0165-1684(97)00044-3
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Tahoces PG, 2008, COMPUT VIS IMAGE UND, V109, P139, DOI 10.1016/j.cviu.2007.07.001
   Tan DM, 2004, IEEE SIGNAL PROC LET, V11, P239, DOI 10.1109/LSP.2003.821730
   Taubman D., 2012, JPEG2000 IMAGE COMPR, V642
   Nguyen T, 2015, IEEE T CIRC SYST VID, V25, P790, DOI 10.1109/TCSVT.2014.2358000
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2016, IEEE DATA COMPR CONF, P636, DOI 10.1109/DCC.2016.107
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Whitepaper F., 2013, FACEBOOK WHITEPAPER
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu D, 2006, IEEE T MED IMAGING, V25, P335, DOI 10.1109/TMI.2006.870483
   Wu HR, 2013, P IEEE, V101, P2025, DOI 10.1109/JPROC.2013.2262911
   Wu JJ, 2013, IEEE T MULTIMEDIA, V15, P1705, DOI 10.1109/TMM.2013.2268053
   Xu M, 2015, IEEE I CONF COMP VIS, P3907, DOI 10.1109/ICCV.2015.445
   Yang H, 2005, IEE P-VIS IMAGE SIGN, V152, P590, DOI 10.1049/ip-vis:20041164
   Yue HJ, 2013, IEEE T MULTIMEDIA, V15, P845, DOI 10.1109/TMM.2013.2239629
   Zeng WJ, 2000, IEEE IMAGE PROC, P657, DOI 10.1109/ICIP.2000.901044
   Zhang F, 2011, IEEE T MULTIMEDIA, V13, P615, DOI 10.1109/TMM.2011.2134079
   Zhang JM, 2013, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2013.26
   Zhang XF, 2017, IEEE SIGNAL PROC LET, V24, P96, DOI 10.1109/LSP.2016.2641456
NR 64
TC 51
Z9 53
U1 0
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2018
VL 20
IS 1
BP 155
EP 170
DI 10.1109/TMM.2017.2721544
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FQ3WS
UT WOS:000418289700013
DA 2024-07-18
ER

PT J
AU He, DL
   Lan, CL
   Luo, C
   Chen, EH
   Wu, F
   Zeng, WJ
AF He, Dongliang
   Lan, Cuiling
   Luo, Chong
   Chen, Enhong
   Wu, Feng
   Zeng, Wenjun
TI Progressive Pseudo-analog Transmission for Mobile Video Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimedia communication; radio communication; streaming media
ID PERFORMANCE
AB We propose a progressive pseudo-analog video transmission scheme that simultaneously handles SNR and bandwidth variations with graceful quality degradation for mobile video streaming. With the inherited SNR-adaptability from pseudo-analog transmission, the proposed progressive solution acquires bandwidth adaptability through an innovative scheduling algorithm with optimal power allocation. The basic idea is to aggressively transmit or retransmit important coefficients so that distortion is minimized at the receiver after each received packet. We derive the closed-form expression of reduced distortion for each packet under given transmission power and known channel conditions, and show that the optimal solution can be obtained with a water-filling algorithm. We also illustrate through analyses and simulations that a near-optimal solution can be found through approximation when only statistical channel information is available. Simulations show that our solution approaches the performance upper bound of pseudo-analog transmission in an additive white Gaussian noise channel and significantly outperforms existing pseudo-analog solutions in a fast Rayleigh fading channel. Trace-driven emulations are also carried out to demonstrate the advantage of the proposed solution over the state-of-the-art digital and pseudo-analog solutions under a real dramatically varying wireless environment.
C1 [He, Dongliang; Lan, Cuiling; Luo, Chong; Zeng, Wenjun] Microsoft Res Asia, Beijing 10080, Peoples R China.
   [He, Dongliang; Chen, Enhong; Wu, Feng] Univ Sci & Technol China, Hefei 230026, Anhui, Peoples R China.
   [Chen, Enhong] Univ Sci & Technol China, Sch Comp Sci, Hefei 230026, Anhui, Peoples R China.
   [Chen, Enhong] Univ Sci & Technol China, Natl Engn Lab Speech & Language Informat Proc, Hefei 230026, Anhui, Peoples R China.
C3 Microsoft Research Asia; Microsoft; Chinese Academy of Sciences;
   University of Science & Technology of China, CAS; Chinese Academy of
   Sciences; University of Science & Technology of China, CAS; Chinese
   Academy of Sciences; University of Science & Technology of China, CAS
RP Luo, C (corresponding author), Microsoft Res Asia, Beijing 10080, Peoples R China.
EM hedl@mail.ustc.edu.cn; culan@microsoft.com; cluo@microsoft.com;
   cheneh@ustc.edu.cn; fengwu@ustc.edu.cn; wezeng@microsoft.com
RI Wu, Feng/KCY-3017-2024; Lan, Cuiling/KCK-5597-2024
FU Natural Science Foundation of China [61631017]
FX This work was supported in part by the Natural Science Foundation of
   China under Contract 61631017. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Christian Timmerer. (Corresponding author: Chong Luo.)
CR [Anonymous], 2014, P IEEE INT C MULT EX
   [Anonymous], 2005, ISO/IEC 8802-11 IEEE Std 802.11 Second edition 2005-08-01 ISO/IEC 8802 11:2005(E) IEEE Std 802.11i-2003 Edition, VSecond, P1
   [Anonymous], 2009, MITCSAILTR2009005
   [Anonymous], 2015, TECH REP
   Astély D, 2009, IEEE COMMUN MAG, V47, P44, DOI 10.1109/MCOM.2009.4907406
   Choi H, 2013, IEEE J-STSP, V7, P1112, DOI 10.1109/JSTSP.2013.2272241
   Cui H, 2014, IEEE INFOCOM SER, P73, DOI 10.1109/INFOCOM.2014.6847926
   Cui H, 2013, IEEE T WIREL COMMUN, V12, P4892, DOI 10.1109/TWC.2013.090413.121308
   Cui Hao, 2013, P ACM INT C MOD AN S, P273
   Cuiling Lan, 2015, 2015 Visual Communications and Image Processing (VCIP), P1, DOI 10.1109/VCIP.2015.7457915
   Fan XP, 2015, IEEE T CIRC SYST VID, V25, P1801, DOI 10.1109/TCSVT.2015.2402831
   He DL, 2015, IEEE T MULTIMEDIA, V17, P1658, DOI 10.1109/TMM.2015.2451956
   Hu SD, 2010, IEEE INT CON MULTI, P226, DOI 10.1109/ICME.2010.5583038
   Jakubczak S., 2011, PROC MOBICOM, P289
   Liu XL, 2014, IEEE T MULTIMEDIA, V16, P2038, DOI 10.1109/TMM.2014.2331616
   Liu XL, 2012, MOBICOM 12: PROCEEDINGS OF THE 18TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P233
   Rowitch DN, 2000, IEEE T COMMUN, V48, P948, DOI 10.1109/26.848555
   Schierl T, 2007, IEEE T CIRC SYST VID, V17, P1204, DOI 10.1109/TCSVT.2007.905528
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tan K., 2009, Proceedings of the 6th USENIX Symposium on Networked Systems Design and Implementation, NSDI'09, P75
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiong RG, 2014, IEEE DATA COMPR CONF, P133, DOI 10.1109/DCC.2014.55
   Xiong RQ, 2013, IEEE INT SYMP CIRC S, P1159, DOI 10.1109/ISCAS.2013.6572057
   Yu L, 2014, IEEE T CIRC SYST VID, V24, P331, DOI 10.1109/TCSVT.2013.2273675
   Zhao X, 2016, IEEE T CIRC SYST VID, V26, P1117, DOI 10.1109/TCSVT.2015.2444753
NR 26
TC 21
Z9 21
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2017
VL 19
IS 8
BP 1894
EP 1907
DI 10.1109/TMM.2017.2686703
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5VV
UT WOS:000411240400017
DA 2024-07-18
ER

PT J
AU Henriquez, P
   Matuszewski, BJ
   Andreu-Cabedo, Y
   Bastiani, L
   Colantonio, S
   Coppini, G
   D'Acunto, M
   Favilla, R
   Germanese, D
   Giorgi, D
   Marraccini, P
   Martinelli, M
   Morales, MA
   Pascali, MA
   Righi, M
   Salvetti, O
   Larsson, M
   Strömberg, T
   Randeberg, L
   Bjorgan, A
   Giannakakis, G
   Pediaditis, M
   Chiarugi, F
   Christinaki, E
   Marias, K
   Tsiknakis, M
AF Henriquez, Pedro
   Matuszewski, Bogdan J.
   Andreu-Cabedo, Yasmina
   Bastiani, Luca
   Colantonio, Sara
   Coppini, Giuseppe
   D'Acunto, Mario
   Favilla, Riccardo
   Germanese, Danila
   Giorgi, Daniela
   Marraccini, Paolo
   Martinelli, Massimo
   Morales, Maria-Aurora
   Pascali, Maria Antonietta
   Righi, Marco
   Salvetti, Ovidio
   Larsson, Marcus
   Stromberg, Tomas
   Randeberg, Lise
   Bjorgan, Asgeir
   Giannakakis, Giorgos
   Pediaditis, Matthew
   Chiarugi, Franco
   Christinaki, Eirini
   Marias, Kostas
   Tsiknakis, Manolis
TI Mirror Mirror on the Wall ... An Unobtrusive Intelligent Multisensory
   Mirror for Well-Being Status Self-Assessment and Visualization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cardio-metabolic risk; unobtrusive well-being monitoring; multimodal
   data integration; 3D face detection and tracking; 3D morphometric
   analysis; psychosomatic status recognition; multispectral imaging;
   breath analysis
ID PHYSICAL-ACTIVITY QUESTIONNAIRE; BREATH ANALYSIS; RISK; RELIABILITY;
   PREDICTOR; SENSORS; STRESS; INDEX
AB A person's well-being status is reflected by their face through a combination of facial expressions and physical signs. The SEMEOTICONS project translates the semeiotic code of the human face into measurements and computational descriptors that are automatically extracted from images, videos, and three-dimensional scans of the face. SEMEOTICONS developed a multisensory platform in the form of a smart mirror to identify signs related to cardio-metabolic risk. The aim was to enable users to self-monitor their well-being status over time and guide them to improve their lifestyle. Significant scientific and technological challenges have been addressed to build the multisensory mirror, from touchless data acquisition, to real-time processing and integration of multimodal data.
C1 [Henriquez, Pedro; Matuszewski, Bogdan J.; Andreu-Cabedo, Yasmina] Univ Cent Lancashire, Preston PR1 2HE, Lancs, England.
   [Bastiani, Luca; Coppini, Giuseppe; Favilla, Riccardo; Marraccini, Paolo; Morales, Maria-Aurora] CNR, IFC, Inst Clin Physiol, I-56124 Pisa, Italy.
   [Colantonio, Sara; Germanese, Danila; Giorgi, Daniela; Martinelli, Massimo; Pascali, Maria Antonietta; Righi, Marco; Salvetti, Ovidio] CNR, ISTI, Inst Informat Sci & Technol, I-56124 Pisa, Italy.
   [D'Acunto, Mario] CNR, ISM, Inst Struct Matter, I-00185 Rome, Italy.
   [Larsson, Marcus; Stromberg, Tomas] Linkoping Univ, S-58183 Linkoping, Sweden.
   [Randeberg, Lise; Bjorgan, Asgeir] Norwegian Univ Sci & Technol, N-7491 Trondheim, Norway.
   [Giannakakis, Giorgos; Pediaditis, Matthew; Chiarugi, Franco; Christinaki, Eirini; Marias, Kostas; Tsiknakis, Manolis] Fdn Res & Technol Hellas, Inst Comp Sci, Iraklion 70013, Greece.
   [Tsiknakis, Manolis] Technol Educ Inst Crete, Dept Informat Engn, Iraklion 71410, Greece.
C3 University of Central Lancashire; Consiglio Nazionale delle Ricerche
   (CNR); Istituto di Fisiologia Clinica (IFC-CNR); Consiglio Nazionale
   delle Ricerche (CNR); Istituto di Scienza e Tecnologie dell'Informazione
   "Alessandro Faedo" (ISTI-CNR); Consiglio Nazionale delle Ricerche (CNR);
   Istituto di Struttura della Materia (ISM-CNR); Linkoping University;
   Norwegian University of Science & Technology (NTNU); Foundation for
   Research & Technology - Hellas (FORTH); Hellenic Mediterranean
   University
RP Henriquez, P (corresponding author), Univ Cent Lancashire, Preston PR1 2HE, Lancs, England.
EM phenriquezcastellano@uclan.ac.uk; bmatuszewski1@uclan.ac.uk;
   yan-dreucabedo@uclan.ac.uk; luca.bastiani@isti.cnr.it;
   sara.colantonio@isti.cnr.it; coppini@ifc.cnr.it;
   mario.dacunto@ism.cnr.it; favillar@ifc.cnr.it;
   danila.germanese@isti.cnr.it; daniela.giorgi@isti.cnr.it;
   paolo-mar@ifc.cnr.it; morales@ifc.cnr.it; morales@ifc.cnr.it;
   marco.righi@isti.cnr.it; o.salvetti@isti.cnr.it; marcus.larsson@liu.se;
   tomas.stromberg@liu.se; lise.randeberg@iet.ntnu.no;
   asgeir.bjorgan@iet.ntnu.no; ggian@ics.forth.gr; mped@ics.forth.gr;
   chiarugi@ics.forth.gr; echrist@ics.forth.gr; kmarias@ics.forth.gr;
   tsiknaki@ics.forth.gr
RI Coppini, Giuseppe/C-4000-2009; Giannakakis, Giorgos/GQP-2829-2022;
   Christinaki, Eirini/ITU-5419-2023; Colantonio, Sara/AFL-2051-2022;
   Randeberg, Lise Lyngsnes/G-8664-2019; Giannakakis, Giorgos/V-5626-2019;
   Pascali, Maria Antonietta/AAU-5020-2020; Tsiknakis, Manolis/Z-2114-2019;
   Righi, Marco/HZK-8326-2023; Righi, Marco/B-1595-2016; Marias,
   Kostas/AAM-2330-2021; Martinelli, Massimo/AAG-9440-2019; Larsson,
   Marcus/J-4123-2012
OI Christinaki, Eirini/0000-0001-5785-1509; Colantonio,
   Sara/0000-0003-2022-0804; Randeberg, Lise Lyngsnes/0000-0003-2608-3759;
   Giannakakis, Giorgos/0000-0002-0958-5346; Pascali, Maria
   Antonietta/0000-0001-7742-8126; Righi, Marco/0000-0002-1448-0960;
   Marias, Kostas/0000-0003-3783-5223; Martinelli,
   Massimo/0000-0001-7419-5099; Bjorgan, Asgeir/0000-0003-4200-3827;
   Pediaditis, Matthew/0000-0003-2263-3020; Germanese,
   Danila/0000-0002-7814-5280; Larsson, Marcus/0000-0001-6385-6760;
   Coppini, Giuseppe/0000-0002-1931-0282; Tsiknakis,
   Manolis/0000-0001-8454-1450; FAVILLA, RICCARDO/0000-0003-3281-7229
FU European Union Seventh Framework Programme (FP7) [611516]
FX This work was supported by the European Union Seventh Framework
   Programme (FP7/2013-2016) under Grant 611516 (SEMEOTICONS). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Qi Tian.
CR Abtahi S., 2014, P 5 ACM MULT SYST C, P24, DOI DOI 10.1145/2557642.2563678
   Alhamid M F., 2012, MeMeA 2012 - 2012 IEEE Symp. Med. Meas. Appl. Proc, P1
   Andreu Y, 2016, COMPUT VIS IMAGE UND, V148, P3, DOI 10.1016/j.cviu.2016.03.018
   Baena CP, 2016, METAB SYNDR RELAT D, V14, P145, DOI 10.1089/met.2015.0083
   Bastien CH, 2001, SLEEP MED, V2, P297, DOI 10.1016/S1389-9457(00)00065-4
   Bedogni G, 2006, BMC GASTROENTEROL, V6, DOI 10.1186/1471-230X-6-33
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Christinaki E, 2014, 2014 EAI 4TH INTERNATIONAL CONFERENCE ON WIRELESS MOBILE COMMUNICATION AND HEALTHCARE (MOBIHEALTH), P339, DOI 10.1109/MOBIHEALTH.2014.7015980
   CLIFFORD PK, 1983, SENSOR ACTUATOR, V3, P233, DOI 10.1016/0250-6874(82)80026-7
   COHEN S, 1983, J HEALTH SOC BEHAV, V24, P385, DOI 10.2307/2136404
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Coppini G., 2014, SUPERHEAL HEALTHINF, P606
   Crinière L, 2011, J PHYS ACT HEALTH, V8, P858, DOI 10.1123/jpah.8.6.858
   DAcunto M., 2014, P HEALTHINF 2014, P577, DOI DOI 10.5220/0004938605770582
   Di Francesco F, 2005, MICROCHEM J, V79, P405, DOI 10.1016/j.microc.2004.10.008
   Di Francesco F., 2008, J BREATH RES, V2
   Fanelli G, 2013, INT J COMPUT VISION, V101, P437, DOI 10.1007/s11263-012-0549-0
   FARKAS LG, 1985, CLEFT PALATE J, V22, P253
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Giannakakis G, 2017, BIOMED SIGNAL PROCES, V31, P89, DOI 10.1016/j.bspc.2016.06.020
   Giorgi D., 2015, P EG 3DOR 2015 EUR 2
   Graaff R, 2005, P SOC PHOTO-OPT INS, V5692, P111, DOI 10.1117/12.588984
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Guo DM, 2010, IEEE T BIO-MED ENG, V57, P2753, DOI 10.1109/TBME.2010.2055864
   Hartley RI, 1999, INT J COMPUT VISION, V35, P115, DOI 10.1023/A:1008115206617
   Henriquez P, 2014, IEEE IMAGE PROC, P1957, DOI 10.1109/ICIP.2014.7025392
   JONES JG, 1967, RESP PHYSIOL, V2, P375, DOI 10.1016/0034-5687(67)90042-4
   Kaplan D., 2000, STRUCTURAL EQUATIONM
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Klemm Soeren, 2015, 10th International Conference on Computer Vision Theory and Applications (VISAPP 2015). Proceedings, P447
   Larsson M., 2016, COMPUT BIOL MED
   Lee B., 2012, J BIOMED BIOTECHNOL, V2012
   Lee B., 2014, BMC COMPLEM ALTERN M, V14
   Lindström J, 2008, DIABETES CARE, V31, P857, DOI 10.2337/dc07-2162
   Lucas Bruce D., ITERATIVE IMAGE REGI, P674, DOI DOI 10.1109/HPDC.2004.1323531
   Mannocci A, 2014, EPIDEMIOL BIOSTAT PU, V11, DOI 10.2427/8860
   Matthews D., 1985, DIABETOLOGIA, V7, P402
   McDuff D, 2014, IEEE ENG MED BIO, P2957, DOI 10.1109/EMBC.2014.6944243
   Miekisch W, 2004, CLIN CHIM ACTA, V347, P25, DOI 10.1016/j.cccn.2004.04.023
   Miekisch W, 2008, J BREATH RES, V2, DOI 10.1088/1752-7155/2/2/026007
   Millar SR, 2013, J DIABETES METAB, V4, DOI 10.4172/2155-6156.S11-004
   Nair P, 2009, IEEE T MULTIMEDIA, V11, P611, DOI 10.1109/TMM.2009.2017629
   National Heart Lung and Blood Institute Bethesda MD USA, DESCR DASH EAT PLAN
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Nickerson CAE, 1997, BIOMETRICS, V53, P1503, DOI 10.2307/2533516
   Pediaditis M, 2015, IEEE ENG MED BIO, P3711, DOI 10.1109/EMBC.2015.7319199
   Perk J, 2012, EUR HEART J, V33, P1635, DOI 10.1093/eurheartj/ehs092
   POMERLEAU CS, 1994, ADDICT BEHAV, V19, P33, DOI 10.1016/0306-4603(94)90049-3
   Public Health England London U. K., ALC LEARN RES
   Quan W, 2010, IEEE IMAGE PROC, P2433, DOI 10.1109/ICIP.2010.5651357
   Rahman A., 2010, P IEEE ACM S DISTR S, P203
   Saragih JM, 2011, INT J COMPUT VISION, V91, P200, DOI 10.1007/s11263-010-0380-4
   Scholz U, 2002, EUR J PSYCHOL ASSESS, V18, P242, DOI 10.1027//1015-5759.18.3.242
   Sharma N, 2012, COMPUT METH PROG BIO, V108, P1287, DOI 10.1016/j.cmpb.2012.07.003
   Shier D., 2007, HOLES HUMAN ANATOMY, V11
   Song XM, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P213, DOI 10.1145/2766462.2767726
   Stein JH, 2008, AM J CARDIOL, V101, P986, DOI 10.1016/j.amjcard.2007.11.044
   Stroop JR, 1935, J EXP PSYCHOL, V18, P643, DOI 10.1037/h0054651
   Sukul P, 2015, J BREATH RES, V9, DOI 10.1088/1752-7155/9/4/047105
   Tashakkor AY, 2013, CAN J CARDIOL, V29, P1477, DOI 10.1016/j.cjca.2013.04.007
   Thejaswi N. S., 2008, 14 NAT C COMM MUMB, P456
   Thekedar B, 2011, J BREATH RES, V5, DOI 10.1088/1752-7155/5/1/016001
   Vezzeti E., 2012, IMAGE VIS COMPUT, V30
   World Health Organization, WHO 5 WELL BEING IND
   Yuan ML, 2013, IEEE T MULTIMEDIA, V15, P1958, DOI 10.1109/TMM.2013.2280560
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 66
TC 19
Z9 19
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2017
VL 19
IS 7
BP 1467
EP 1481
DI 10.1109/TMM.2017.2666545
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5ZN
UT WOS:000404061800006
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Huang, C
   Li, HL
   Xie, YR
   Wu, QB
   Luo, B
AF Huang, Chao
   Li, Hongliang
   Xie, Yurui
   Wu, Qingbo
   Luo, Bing
TI PBC: Polygon-Based Classifier for Fine-Grained Categorization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Polygon-based classifier; fine-grained categorization; greedy algorithm;
   coarse-to-fine method
ID SEGMENTATION
AB Fine-grained categorization is a challenging task mainly due to two factors: first, objects share similar appearances between different categories; second, objects present significant pose variation within the same category. To address these challenges, we propose a method to automatically detect discriminative and pose-invariant regions, which is referred to as a polygon-based classifier (PBC). In the first stage, we generate a set of polygons that are composed of multiple parts. For each polygon, a classifier is trained based on deep features of a convolutional network. Then, a greedy algorithm is employed to select the discriminative and complementary polygon-based classifiers that deliver highest classification accuracy for fine-grained object categories. In the second stage, the confusing classes of the first stage are selected and employed to train the polygon-based classifiers. Then, a greedy algorithm is employed to select discriminative classifiers. For the test images, we use the classifiers trained in the first stage to obtain a coarse result. Then, the classifiers of the second stage are adopted to distinguish the confusing classes of the coarse result. In our experiments, the proposed approach is evaluated on three well-known fine-grained datasets. The experiments show that our approach outperforms the state-of-the-art methods.
C1 [Huang, Chao; Li, Hongliang; Xie, Yurui; Wu, Qingbo; Luo, Bing] Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Huang, C (corresponding author), Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 611731, Peoples R China.
EM huangchao_uestc@aliyun.com; hlli@uestc.edu.cn; gloriousxyr@163.com;
   qbwu@uestc.edu.cn; mathild1987@163.com
RI Huang, Chao/L-1445-2019; Wu, Qingbo/AAF-6872-2019; Huang,
   Chao/JJD-0553-2023
OI Wu, Qingbo/0000-0003-2936-6340; Huang, Chao/0000-0001-8775-3192; Li,
   Hongliang/0000-0002-7481-095X
FU National Natural Science Foundation of China [61525102, 61271289];
   program for Science and Technology Innovative Research Team for Young
   Scholars in Sichuan Province, China [2014TD0006]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61525102 and Grant 61271289, and in part
   by the program for Science and Technology Innovative Research Team for
   Young Scholars in Sichuan Province, China, under Grant 2014TD0006. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Martha Larson.
CR Angelova A, 2013, PROC CVPR IEEE, P811, DOI 10.1109/CVPR.2013.110
   [Anonymous], 2014, P BRIT MACH VIS C, DOI 10.5244/C.28.87
   [Anonymous], 2006, PATTERN RECOGN, DOI DOI 10.1117/1.2819119
   [Anonymous], CORR
   [Anonymous], 2014, P 31 INT C INT C MAC
   [Anonymous], 2013, Caffe: An open source convolutional architecture for fast feature embedding
   [Anonymous], 2015, PROC 28 INT C NEURAL
   Azizpour Hossein, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P36, DOI 10.1109/CVPRW.2015.7301270
   Berg T, 2013, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2013.128
   Chai Y, 2013, IEEE I CONF COMP VIS, P321, DOI 10.1109/ICCV.2013.47
   Chai YN, 2012, LECT NOTES COMPUT SC, V7572, P794, DOI 10.1007/978-3-642-33718-5_57
   Chai YN, 2011, IEEE I CONF COMP VIS, P2579, DOI 10.1109/ICCV.2011.6126546
   Chen G, 2015, IEEE WINT CONF APPL, P860, DOI 10.1109/WACV.2015.119
   Cohen WW, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P671
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Fan JP, 2008, IEEE T MULTIMEDIA, V10, P167, DOI 10.1109/TMM.2007.911775
   Farrell R, 2011, IEEE I CONF COMP VIS, P161, DOI 10.1109/ICCV.2011.6126238
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P148
   Gavves E, 2013, IEEE I CONF COMP VIS, P1713, DOI 10.1109/ICCV.2013.215
   Gavves E, 2015, INT J COMPUT VISION, V111, P191, DOI 10.1007/s11263-014-0741-5
   Göring C, 2014, PROC CVPR IEEE, P2489, DOI 10.1109/CVPR.2014.319
   Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642
   Hearst MA, 1998, IEEE INTELL SYST APP, V13, P18, DOI 10.1109/5254.708428
   Huang C, 2014, J VIS COMMUN IMAGE R, V25, P1299, DOI 10.1016/j.jvcir.2014.05.002
   Jia D, 2013, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2013.81
   Khosla A., 2011, CVPR
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Levinshtein A, 2009, IEEE I CONF COMP VIS, P2162, DOI 10.1109/ICCV.2009.5459472
   Li L.J., 2007, PROC IEEE INT C COMP
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu JX, 2012, LECT NOTES COMPUT SC, V7572, P172, DOI 10.1007/978-3-642-33718-5_13
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Lu YJ, 2009, IEEE T MULTIMEDIA, V11, P1289, DOI 10.1109/TMM.2009.2030632
   Meng FM, 2012, IEEE T MULTIMEDIA, V14, P1429, DOI 10.1109/TMM.2012.2197741
   Murray N, 2014, PROC CVPR IEEE, P2473, DOI 10.1109/CVPR.2014.317
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Perronnin F., 2007, P IEEE CVPR, P1
   Pu J, 2014, LECT NOTES COMPUT SC, V8691, P425, DOI 10.1007/978-3-319-10578-9_28
   Qi GJ, 2010, IEEE T MULTIMEDIA, V12, P278, DOI 10.1109/TMM.2010.2046270
   Qian Q, 2015, PROC CVPR IEEE, P3716, DOI 10.1109/CVPR.2015.7298995
   Ramakrishna V, 2014, LECT NOTES COMPUT SC, V8690, P33, DOI 10.1007/978-3-319-10605-2_3
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Shen L., 2004, Proc. of Image and Vision Computing NewZealand, P77
   Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136
   Simon M, 2015, LECT NOTES COMPUT SC, V9004, P162, DOI 10.1007/978-3-319-16808-1_12
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Vedaldi A, 2009, IEEE I CONF COMP VIS, P606, DOI 10.1109/ICCV.2009.5459183
   Wah C, 2011, CALTECH UCSD BIRDS 2
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Winn J, 2005, IEEE I CONF COMP VIS, P1800
   WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Xie LX, 2013, IEEE I CONF COMP VIS, P1641, DOI 10.1109/ICCV.2013.206
   Yao BP, 2011, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR.2011.5995368
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhou X, 2010, LECT NOTES COMPUT SC, V6315, P141, DOI 10.1007/978-3-642-15555-0_11
NR 62
TC 32
Z9 36
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2017
VL 19
IS 4
BP 673
EP 684
DI 10.1109/TMM.2016.2631122
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EO0NT
UT WOS:000396395500001
DA 2024-07-18
ER

PT J
AU Mao, QR
   Rao, QY
   Yu, YB
   Dong, M
AF Mao, Qirong
   Rao, Qiyu
   Yu, Yongbin
   Dong, Ming
TI Hierarchical Bayesian Theme Models for Multipose Facial Expression
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face expression recognition; hierarchical theme model; intermediate
   features; multipose; supervised latent Dirichlet allocation
ID GAUSSIAN-PROCESSES; MULTIVIEW; EMOTION; AUDIO
AB As an essential way of human emotional behavior understanding, facial expression recognition (FER) has attracted a great deal of attention in multimedia research. Most of studies are conducted in a "lab-controlled" environment, and their real-world performance degenerates greatly due to factors such as head pose variations. In this paper, we propose a pose-based hierarchical Bayesian theme model to address challenging issues in multipose FER. Local appearance features and global geometry information are combined in our model to learn an intermediate face representation before recognizing expressions. By sharing a pool of features with various poses, our model provides a unified solution for multipose FER, bypassing the separate training and parameter tuning for each pose, and thus is scalable to a large number of poses. Experiments on both benchmark facial expression databases and Internet images showthe superior/ highly competitive performance of our system when compared with the current state of the art.
C1 [Mao, Qirong; Rao, Qiyu; Yu, Yongbin] Jiangsu Univ, Dept Comp Sci & Commun Engn, Zhenjiang 212013, Peoples R China.
   [Dong, Ming] Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA.
C3 Jiangsu University; Wayne State University
RP Mao, QR (corresponding author), Jiangsu Univ, Dept Comp Sci & Commun Engn, Zhenjiang 212013, Peoples R China.
EM mao_qr@mail.ujs.edu.cn; raoqiyu@mail.ujs.edu.cn; darcy0511@gmail.com;
   mdong@cs.wayne.edu
FU National Nature Science Foundation of China [61272211, 61672267]; Six
   Talent Peaks Foundation of Jiangsu Province [DZXX-027]; General
   Financial Grant from the China Postdoctoral Science Foundation
   [2015M570413]; U.S. National Science Foundation [CNS-1637312]; Ford
   Motor Company University Research Program [2015-9186R]
FX The work of Q. Mao was supported in part by the National Nature Science
   Foundation of China under Grant 61272211 and Grant 61672267, in part by
   the Six Talent Peaks Foundation of Jiangsu Province under Grant
   DZXX-027, and in part by General Financial Grant from the China
   Postdoctoral Science Foundation 2015M570413. The work of M. Dong was
   supported in part by the U.S. National Science Foundation under Grant
   CNS-1637312, and in part by Ford Motor Company University Research
   Program under Grant 2015-9186R. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Jiebo
   Luo.
CR [Anonymous], 2008, Advances in Neural Information Processing Systems
   [Anonymous], P 3 ACM C INT C MULT
   Asthana A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.127
   Ballano S, 2011, LECT NOTES COMPUT SC, V6949, P600, DOI 10.1007/978-3-642-23768-3_92
   Black MJ, 1997, INT J COMPUT VISION, V25, P23, DOI 10.1023/A:1007977618277
   Calvo RA, 2010, IEEE T AFFECT COMPUT, V1, P18, DOI 10.1109/T-AFFC.2010.1
   Cotter SF, 2010, INT CONF ACOUST SPEE, P838, DOI 10.1109/ICASSP.2010.5494903
   Dhall A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P2106, DOI 10.1109/ICCVW.2011.6130508
   Dhall A., 2012, ASIAN C COMPUTER VIS, P613
   Dhillon B, 2011, LECT NOTES COMPUT SC, V6947, P392, DOI 10.1007/978-3-642-23771-3_29
   Dibeklioglu H, 2015, IEEE T MULTIMEDIA, V17, P279, DOI 10.1109/TMM.2015.2394777
   Eleftheriadis S, 2015, IEEE T IMAGE PROCESS, V24, P189, DOI 10.1109/TIP.2014.2375634
   Fasel B, 2003, PATTERN RECOGN, V36, P259, DOI 10.1016/S0031-3203(02)00052-3
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Goodfellow IJ, 2013, IEEE T PATTERN ANAL, V35, P1902, DOI 10.1109/TPAMI.2012.273
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Hesse N, 2012, INT C PATT RECOG, P3533
   Hu Q, 2014, INT C PATT RECOG, P1782, DOI 10.1109/ICPR.2014.313
   Kumano S, 2009, INT J COMPUT VISION, V83, P178, DOI 10.1007/s11263-008-0185-x
   Lade P., 2015, THESIS
   Langner O, 2010, COGNITION EMOTION, V24, P1377, DOI 10.1080/02699930903485076
   Le HS, 2004, INT C PATT RECOG, P318, DOI 10.1109/ICPR.2004.1334116
   Lee-Johnson CP, 2010, IEEE T SYST MAN CY B, V40, P469, DOI 10.1109/TSMCB.2009.2026826
   Li ZC, 2015, IEEE T MULTIMEDIA, V17, P1989, DOI 10.1109/TMM.2015.2477035
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3
   Moore S, 2011, COMPUT VIS IMAGE UND, V115, P541, DOI 10.1016/j.cviu.2010.12.001
   Ngoc-Son Vu, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1204, DOI 10.1109/ICPR.2010.300
   Ojo J. A., 2010, INT J IMAGE PROCESS, V5, P58
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Park JW, 2010, IFIP ADV INF COMM TE, V332, P223
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Rifai S, 2012, LECT NOTES COMPUT SC, V7577, P808, DOI 10.1007/978-3-642-33783-3_58
   Ronning G., 1989, J STAT COMPUT SIM, V32, P215, DOI DOI 10.1080/00949658908811178
   Rudovic Ognjen, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4121, DOI 10.1109/ICPR.2010.1001
   Rudovic O, 2013, IEEE T PATTERN ANAL, V35, P1357, DOI 10.1109/TPAMI.2012.233
   Saragih JM, 2011, INT J COMPUT VISION, V91, P200, DOI 10.1007/s11263-010-0380-4
   Sariyanidi E, 2015, IEEE T PATTERN ANAL, V37, P1113, DOI 10.1109/TPAMI.2014.2366127
   Shepstone SE, 2014, IEEE T MULTIMEDIA, V16, P1999, DOI 10.1109/TMM.2014.2337845
   Sikka K, 2012, LECT NOTES COMPUT SC, V7584, P250, DOI 10.1007/978-3-642-33868-7_25
   Sung J, 2009, IMAGE VISION COMPUT, V27, P1313, DOI 10.1016/j.imavis.2008.11.010
   Tariq U, 2012, LECT NOTES COMPUT SC, V7585, P578, DOI 10.1007/978-3-642-33885-4_58
   Tariq U, 2012, IEEE T SYST MAN CY B, V42, P1017, DOI 10.1109/TSMCB.2012.2194701
   Tong Y, 2010, IEEE T PATTERN ANAL, V32, P258, DOI 10.1109/TPAMI.2008.293
   Walter S, 2011, LECT NOTES COMPUT SC, V6763, P603, DOI 10.1007/978-3-642-21616-9_68
   Yang SF, 2012, IEEE T SYST MAN CY B, V42, P980, DOI 10.1109/TSMCB.2012.2192269
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   YuLu Hu, 2015, 2015 IEEE International Vacuum Electronics Conference (IVEC), P1, DOI 10.1109/IVEC.2015.7224002
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zheng WM, 2014, IEEE T AFFECT COMPUT, V5, P71, DOI 10.1109/TAFFC.2014.2304712
   Zheng WM, 2009, IEEE I CONF COMP VIS, P1901, DOI 10.1109/ICCV.2009.5459421
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
   Zhu Z., 2006, IEEE Conf. Comput. Vision and Pattern Recogn, P681
NR 53
TC 45
Z9 46
U1 0
U2 63
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2017
VL 19
IS 4
BP 861
EP 873
DI 10.1109/TMM.2016.2629282
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EO0NT
UT WOS:000396395500016
DA 2024-07-18
ER

PT J
AU Herranz, L
   Jiang, SQ
   Xu, RH
AF Herranz, Luis
   Jiang, Shuqiang
   Xu, Ruihan
TI Modeling Restaurant Context for Food Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Food recognition; image recognition; location; mobile applications;
   probabilistic modeling.
ID WEIGHT-LOSS
AB Food photos are widely used in food logs for diet monitoring and in social networks to share social and gastronomic experiences. A large number of these images are taken in restaurants. Dish recognition in general is very challenging, due to different cuisines, cooking styles, and the intrinsic difficulty of modeling food from its visual appearance. However, contextual knowledge can be crucial to improve recognition in such scenario. In particular, geocontext has been widely exploited for outdoor landmark recognition. Similarly, we exploit knowledge about menus and location of restaurants and test images. We first adapt a framework based on discarding unlikely categories located far from the test image. Then, we reformulate the problem using a probabilistic model connecting dishes, restaurants, and locations. We apply that model in three different tasks: dish recognition, restaurant recognition, and location refinement. Experiments on six datasets show that by integrating multiple evidences (visual, location, and external knowledge) our system can boost the performance in all tasks.
C1 [Herranz, Luis; Jiang, Shuqiang; Xu, Ruihan] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS
RP Herranz, L (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM luis.herranz@vipl.ict.ac.cn; shuqiang.jiang@vipl.ict.ac.cn;
   ruihan.xu@vipl.ict.ac.cn
RI Herranz, Luis/B-4573-2016; xu, ruihan/JSK-6518-2023
OI Herranz, Luis/0000-0002-7022-3395; 
FU National Basic Research 973 Program of China [2012CB316400]; National
   Natural Science Foundation of China [61550110505, 61532018, 61322212];
   National High Technology Research and Development 863 Program of China
   [2014AA015202]; pal Commission of Science and Technology
   [D161100001816001]; Lenovo Outstanding Young Scientists Program
FX This work was supported in part by the National Basic Research 973
   Program of China under Grant 2012CB316400, in part by the National
   Natural Science Foundation of China under Grant 61550110505, Grant
   61532018, and Grant 61322212, in part by the National High Technology
   Research and Development 863 Program of China under Grant 2014AA015202,
   in part by the pal Commission of Science and Technology under Grant
   D161100001816001, and in part by the Lenovo Outstanding Young Scientists
   Program.
CR Abbar S, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3197, DOI 10.1145/2702123.2702153
   Aizawa K, 2015, IEEE MULTIMEDIA, V22, P4, DOI 10.1109/MMUL.2015.39
   Aizawa K, 2013, IEEE T MULTIMEDIA, V15, P2176, DOI 10.1109/TMM.2013.2271474
   [Anonymous], P SPIE EL IM
   [Anonymous], 2015, P IEEE INT C MULT EX
   [Anonymous], 2014, P 2014 ACM INT JOINT
   [Anonymous], P WORKSH MULT COOK E
   [Anonymous], 2011, Proceedings of the 24th, DOI [DOI 10.1145/2047196.2047198, 10.1145/2047196]
   [Anonymous], P INT C BEH EC SOC C
   [Anonymous], 2014, P 31 INT C INT C MAC
   Beijbom O, 2015, IEEE WINT CONF APPL, P844, DOI 10.1109/WACV.2015.117
   Bettadapura V, 2015, IEEE WINT CONF APPL, P580, DOI 10.1109/WACV.2015.83
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Burke LE, 2011, J AM DIET ASSOC, V111, P92, DOI 10.1016/j.jada.2010.10.008
   Buykx L., 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P387, DOI 10.1109/ISM.2011.70
   Chen M, 2009, IEEE IMAGE PROC, P289, DOI 10.1109/ICIP.2009.5413511
   Christodoulidis S, 2015, LECT NOTES COMPUT SC, V9281, P458, DOI 10.1007/978-3-319-23222-5_56
   De Choudhury M, 2016, ACM CONFERENCE ON COMPUTER-SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING (CSCW 2016), P1157, DOI 10.1145/2818048.2819956
   Doman Keisuke., 2012, Proceedings of the 20th ACM International Conference on Multimedia, MM '12, P1267, DOI DOI 10.1145/2393347.2396435
   Nguyen DT, 2014, NEUROCOMPUTING, V140, P242, DOI 10.1016/j.neucom.2014.03.017
   Elsweiler David, 2015, P 9 ACM C REC SYST, P313, DOI DOI 10.1145/2792838.2799665
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Fanyu Kong, 2011, 2011 8th International Conference on Body Sensor Networks (BSN), P127, DOI 10.1109/BSN.2011.19
   Girod B, 2011, IEEE MULTIMEDIA, V18, P86, DOI 10.1109/MMUL.2011.48
   Hamada R., 2005, 13th Annual ACM International Conference on Multimedia, P371, DOI 10.1145/1101149.1101228
   He Y, 2014, IEEE IMAGE PROC, P2744, DOI 10.1109/ICIP.2014.7025555
   Helsel DL, 2007, J AM DIET ASSOC, V107, P1807, DOI 10.1016/j.jada.2007.07.014
   Hoashi H., 2010, Proceedings 2010 IEEE International Symposium on Multimedia (ISM 2010), P296, DOI 10.1109/ISM.2010.51
   Ide Ichiro, 2010, Proceedings 2010 IEEE International Symposium on Multimedia (ISM 2010), P242, DOI 10.1109/ISM.2010.42
   Iscen A., 2013, 5 INT WORKSHOP MULTI, P3
   Jain R., 2010, INT C MULTIMEDIA, P1259, DOI DOI 10.1145/1873951.1874199
   Ji Y., 2012, P ACM MULT WORKSH MU, P280
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kawano Yoshiyuki, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8326, P369, DOI 10.1007/978-3-319-04117-9_38
   Kawano Y, 2015, LECT NOTES COMPUT SC, V8927, P3, DOI 10.1007/978-3-319-16199-0_1
   Kawano Y, 2013, IEEE COMPUT SOC CONF, P1, DOI 10.1109/CVPRW.2013.5
   Kitamura K, 2010, IEEE INT CON MULTI, P625, DOI 10.1109/ICME.2010.5583021
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuo FF., 2012, Proceedings of the ACM multimedia 2012 workshop on multimedia for cooking and eating activities, P1
   Kusumoto R, 2013, INT CONF BIOMED, P851, DOI 10.1109/BMEI.2013.6747060
   Li T, 2011, IEEE T CIRC SYST VID, V21, P381, DOI 10.1109/TCSVT.2010.2041828
   Li Z, 2012, IEEE SIGNAL PROC LET, V19, P459, DOI 10.1109/LSP.2012.2203120
   Matsuda Y, 2012, INT C PATT RECOG, P2017
   Miyazaki T., 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P363, DOI 10.1109/ISM.2011.66
   Pouladzadeh P, 2014, IEEE T INSTRUM MEAS, V63, P1947, DOI 10.1109/TIM.2014.2303533
   Seljak B. K., 2006, 2006 INT C INT ENG S, P108, DOI [10.1109/INES.2006.1689351, DOI 10.1109/INES.2006.1689351]
   West R., 2013, WWW, P1399
   Wu W, 2009, IEEE INT CON MULTI, P1210, DOI 10.1109/ICME.2009.5202718
   Xu RH, 2015, IEEE T MULTIMEDIA, V17, P1187, DOI 10.1109/TMM.2015.2438717
   Yajima A, 2009, 2009 IEEE/WIC/ACM INTERNATIONAL JOINT CONFERENCES ON WEB INTELLIGENCE (WI) AND INTELLIGENT AGENT TECHNOLOGIES (IAT), VOL 3, P13
   Yap KH, 2010, IEEE INTELL SYST, V25, P48, DOI 10.1109/MIS.2010.12
NR 51
TC 35
Z9 40
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2017
VL 19
IS 2
BP 430
EP 440
DI 10.1109/TMM.2016.2614861
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN1UN
UT WOS:000395795800017
DA 2024-07-18
ER

PT J
AU Maharjan, S
   Zhang, Y
   Gjessing, S
AF Maharjan, Sabita
   Zhang, Yan
   Gjessing, Stein
TI Optimal Incentive Design for Cloud-Enabled Multimedia Crowdsourcing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Crowdsourcing; energy consumption; multimedia cloud; quality of service;
   reward; utility
ID MOBILE; OPPORTUNITIES; MECHANISMS; NETWORK; STATE
AB Multimedia crowdsourcing possesses a huge potential to actualize many new applications that are expected to yield tremendous benefits in diverse fields including environment monitoring, emergency rescues during natural catastrophes, online education, sports, and entertainment. Nonetheless, multimedia crowdsourcing unfolds new challenges such as big data acquisition and processing, more stringent quality of service requirements, and heterogeneity of crowdsensors. Consequently, incentive mechanisms specifically tailored to multimedia crowdsourcing applications need to be developed to fully utilize the potential of multimedia crowdsourcing. In this paper, we design an optimal incentive mechanism for the smartphone contributors to participate in a cloud-enabled multimedia crowdsourcing scheme. We establish a condition that determines whether the smartphones are eligible to participate, and provide a close form expression for the optimal duration of service from the contributors, for a given reward from the crowdsourcer. Consequently, we derive the conditions for existence of an optimal reward for the contributors from the crowdsourcer, and prove its uniqueness. We numerically illustrate the performance of our model considering logarithmic and linear cost functions for the cloud resources. The similarity of the results for different cost models corroborates the validity of our model and the results, whereas the difference in the magnitudes suggests that the strategy of the crowdsourcer as well as the strategies of the smartphone participants considerably depend on the cloud cost model.
C1 [Maharjan, Sabita; Zhang, Yan; Gjessing, Stein] Simula Res Lab, N-1364 Fornebu, Norway.
   [Zhang, Yan] Univ Oslo, Dept Informat, N-0316 Oslo, Norway.
C3 University of Oslo
RP Maharjan, S (corresponding author), Simula Res Lab, N-1364 Fornebu, Norway.
EM sabita@simula.no; yanzhang@ifi.uio.no; steing@ifi.uio.no
RI Maharjan, Sabita/HDO-4388-2022; Zhang, Yan/AFK-8566-2022
OI Maharjan, Sabita/0000-0002-4616-8488
FU Research Council of Norway [240079/F20]
FX This work was supported by the Research Council of Norway under Project
   240079/F20. The guest editor coordinating the review of this manuscript
   and approving it for publication was Prof. Qian Zhang.
CR [Anonymous], 2013, WHIT PAP UND RED LAT
   [Anonymous], 2011, NIST DEFINITION CLOU
   Bashandy A, 1999, IEEE INFOCOM SER, P559, DOI 10.1109/INFCOM.1999.751390
   Chatzimilioudis G, 2012, IEEE INTERNET COMPUT, V16, P36, DOI 10.1109/MIC.2012.70
   Chen F., 2015, CORR, Vabs/1502.06314
   Estrin D, 2010, IEEE INTERNET COMPUT, V14, P12, DOI 10.1109/MIC.2010.12
   Faggiani A, 2014, IEEE COMMUN MAG, V52, P106, DOI 10.1109/MCOM.2014.6710071
   Feng Z, 2014, INT CON DISTR COMP S, P11, DOI 10.1109/ICDCS.2014.10
   Fu ZJ, 2016, IEEE T PARALL DISTR, V27, P2546, DOI 10.1109/TPDS.2015.2506573
   Ganti RK, 2011, IEEE COMMUN MAG, V49, P32, DOI 10.1109/MCOM.2011.6069707
   Guo B., 2014, CORR
   Guo B, 2015, ACM COMPUT SURV, V48, DOI 10.1145/2794400
   Guo XN, 2014, IEEE INFOCOM SER, P1240, DOI 10.1109/INFOCOM.2014.6848056
   Howe J, 2006, WIRED, V14, P1, DOI DOI 10.1086/599595
   Keimel C., 2012, 2012 Proceedings of the 19th International Packet Video Workshop (PV 2012), P155, DOI 10.1109/PV.2012.6229729
   Li BC, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2505805
   Li J, 2015, IEEE T INF FOREN SEC, V10, P507, DOI 10.1109/TIFS.2014.2381872
   Ma HD, 2014, IEEE COMMUN MAG, V52, P29, DOI 10.1109/MCOM.2014.6871666
   Ma TH, 2015, IEICE T INF SYST, VE98D, P902, DOI 10.1587/transinf.2014EDP7283
   Madan A, 2012, IEEE PERVAS COMPUT, V11, P36, DOI 10.1109/MPRV.2011.79
   Markowsky G, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON TECHNOLOGIES FOR HOMELAND SECURITY (HST), P772, DOI 10.1109/THS.2013.6699101
   Nandan N, 2014, IEEE INT CONF MOB DA, P67, DOI 10.1109/MDM.2014.70
   Pan ZQ, 2015, IEEE T BROADCAST, V61, P166, DOI 10.1109/TBC.2015.2419824
   Ren J, 2015, COMPUT COMMUN, V65, P55, DOI 10.1016/j.comcom.2015.01.022
   Simoens P., 2013, Proceeding of the 11th annual international conference on Mobile systems, applications, and services, P139
   Song C, 2012, 2012 EIGHTH INTERNATIONAL CONFERENCE ON MOBILE AD HOC AND SENSOR NETWORKS (MSN 2012), P147, DOI 10.1109/MSN.2012.10
   Sun Y, 2014, IEEE INT CONF SENS, P239, DOI 10.1109/SAHCN.2014.6990359
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Wen YG, 2012, IEEE INFOCOM SER, P2716, DOI 10.1109/INFCOM.2012.6195685
   Wu CS, 2015, IEEE T MOBILE COMPUT, V14, P444, DOI 10.1109/TMC.2014.2320254
   Wu D, 2013, IEEE T INTELL TRANSP, V14, P837, DOI 10.1109/TITS.2013.2243437
   Yang DJ, 2012, MOBICOM 12: PROCEEDINGS OF THE 18TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P173
   Zhao D, 2014, IEEE INFOCOM SER, P1213, DOI 10.1109/INFOCOM.2014.6848053
   Zhu WW, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940269
NR 34
TC 16
Z9 17
U1 0
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2016
VL 18
IS 12
BP 2470
EP 2481
DI 10.1109/TMM.2016.2604080
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ED5VK
UT WOS:000388920200013
DA 2024-07-18
ER

PT J
AU Zhao, WL
   Ngo, CW
   Wang, HZ
AF Zhao, Wan-Lei
   Ngo, Chong-Wah
   Wang, Hanzi
TI Fast Covariant VLAD for Image Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Circular matching; covariant pooling; covariant vector of locally
   aggregated descriptor (CVLAD); similar image search
ID PRODUCT QUANTIZATION; SCALE; RETRIEVAL; FEATURES
AB Vector of locally aggregated descriptor (VLAD) is a popular image encoding approach for its simplicity and better scalability over conventional bag-of-visual-word approach. In order to enhance its distinctiveness and geometric invariance, covariant VLAD (CVLAD) is proposed to pool local features based on their dominant orientations/characteristic scales, which leads to a geometric-aware representation. This representation achieves rotation/scale invariance when being associated with circular matching. However, the circular matching induces several times of computation overhead, which makes CVLAD hardly suitable for large-scale retrieval tasks. In this paper, the issue of computation overhead is alleviated by performing the circular matching in CVLAD's frequency domain. In addition, by operating PCA on CVLAD in its frequency domain, much better scalability is achieved than when it is undertaken in the original feature space. Furthermore, the high-dimensional CVLAD subvectors are converted to dozens of very low-dimensional subvectors, which is possible when transforming the feature into its frequency domain. Nearest neighbor search is therefore undertaken on very low-dimensional subspaces, which becomes easily tractable. The effectiveness of our approach is demonstrated in the retrieval scenario on popular benchmarks comprising up to 1 million database images.
C1 [Zhao, Wan-Lei; Wang, Hanzi] Xiamen Univ, Fujian Key Lab Sensing & Comp Smart City, Xiamen 361005, Fujian, Peoples R China.
   [Ngo, Chong-Wah] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
C3 Xiamen University; City University of Hong Kong
RP Wang, HZ (corresponding author), Xiamen Univ, Fujian Key Lab Sensing & Comp Smart City, Xiamen 361005, Fujian, Peoples R China.
EM wlzhao@xmu.edu.cn; cscwngo@cityu.edu.hk; hanzi.wang@xmu.edu.cn
RI wang, hao/HSE-7975-2023; Wang, Han/GPW-9809-2022; wang,
   handong/HLH-5739-2023
FU National Natural Science Foundation of China [61572408, 61472334];
   Research Grants Council of the Hong Kong Special Administrative Region,
   China [CityU 118812]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61572408 and Grant 61472334, and by the Research
   Grants Council of the Hong Kong Special Administrative Region, China
   under Grant CityU 118812. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Marco
   Bertini. (Corresponding author: Hanzi Wang.)
CR [Anonymous], 2010, ANN: a library for approximate nearest neighbor searching
   [Anonymous], P 6 ACM INT C IM VID
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2002, ART H SIG PROC LIB
   [Anonymous], 2013, P 21 ACM INT C MULT, DOI 10.1145/2502081.2502171
   [Anonymous], 2004, P 2004WORKSHOP STAT
   [Anonymous], 2014, CORR
   [Anonymous], NEAREST NEIGHBOR MET
   Arandjelovic R, 2013, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR.2013.207
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Bay H., 2008, COMPUT VIS IMAGE UND, V10, P346, DOI DOI 10.1016/j.cviu.2007.09.014
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen DM, 2011, PROC CVPR IEEE, P737, DOI 10.1109/CVPR.2011.5995610
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2009, PROC CVPR IEEE, P17, DOI 10.1109/CVPRW.2009.5206531
   Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137
   Gonzalez R C, 2008, DIGITAL IMAGE PROCES
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jaakkola TS, 1999, ADV NEUR IN, V11, P487
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2012, LECT NOTES COMPUT SC, V7573, P774, DOI 10.1007/978-3-642-33709-3_55
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jégou H, 2009, IEEE I CONF COMP VIS, P2357, DOI 10.1109/ICCV.2009.5459419
   Jégou H, 2010, INT J COMPUT VISION, V87, P316, DOI 10.1007/s11263-009-0285-2
   Koniusz P, 2013, COMPUT VIS IMAGE UND, V117, P479, DOI 10.1016/j.cviu.2012.10.010
   Koniusz P, 2011, IEEE IMAGE PROC, P661
   Kuo YH, 2012, IEEE T MULTIMEDIA, V14, P1079, DOI 10.1109/TMM.2012.2190386
   Lampert CH, 2009, IEEE T PATTERN ANAL, V31, P2129, DOI 10.1109/TPAMI.2009.144
   Li ZY, 2013, IEEE I CONF COMP VIS, P2136, DOI 10.1109/ICCV.2013.454
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Moxley E, 2010, IEEE T MULTIMEDIA, V12, P184, DOI 10.1109/TMM.2010.2041101
   Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331
   Negrel R, 2013, IEEE MULTIMEDIA, V20, P24, DOI 10.1109/MMUL.2013.14
   Nister David, 2006, CVPR
   Perronnin F., 2007, P IEEE CVPR, P1
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Philbin J., 2008, PROC IEEE C COMPUT V, P1
   Revaud J, 2013, PROC CVPR IEEE, P2459, DOI 10.1109/CVPR.2013.318
   Sánchez J, 2012, PATTERN RECOGN LETT, V33, P2216, DOI 10.1016/j.patrec.2012.07.019
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Spyromitros-Xioufis E, 2014, IEEE T MULTIMEDIA, V16, P1713, DOI 10.1109/TMM.2014.2329648
   Tao R, 2014, PROC CVPR IEEE, P2099, DOI 10.1109/CVPR.2014.269
   Tolias G, 2015, COMPUT VIS IMAGE UND, V140, P9, DOI 10.1016/j.cviu.2015.06.007
   Tolias G, 2014, LECT NOTES COMPUT SC, V8694, P382, DOI 10.1007/978-3-319-10599-4_25
   Tolias G, 2013, IEEE I CONF COMP VIS, P1401, DOI 10.1109/ICCV.2013.177
   Tolimieri R., 1997, Algorithms for Discrete Fourier Transforms and Convolution
   Wang XJ, 2010, PROC CVPR IEEE, P2987, DOI 10.1109/CVPR.2010.5540046
   Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566
   Xie HT, 2014, IEEE T MULTIMEDIA, V16, P1104, DOI 10.1109/TMM.2014.2305909
   Zhao WL, 2010, IEEE T MULTIMEDIA, V12, P448, DOI 10.1109/TMM.2010.2050651
   Zheng L, 2015, IEEE T MULTIMEDIA, V17, P648, DOI 10.1109/TMM.2015.2408563
   Zhou W., 2010, P 18 ACM INT C MULTI, P511
NR 56
TC 6
Z9 6
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2016
VL 18
IS 9
BP 1843
EP 1854
DI 10.1109/TMM.2016.2585023
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DT4GI
UT WOS:000381437800014
OA Green Published
DA 2024-07-18
ER

PT J
AU Wang, WG
   Shen, JB
AF Wang, Wenguan
   Shen, Jianbing
TI Higher-Order Image Co-segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Energy optimization; higher order cliques; image cosegmentation;
   likelihood estimation
ID COSEGMENTATION; OPTIMIZATION
AB A novel interactive image cosegmentation algorithm using likelihood estimation and higher order energy optimization is proposed for extracting common foreground objects from a group of related images. Our approach introduces the higher order clique's, energy into the cosegmentation optimization process successfully. Aregion-based likelihood estimation procedure is first performed to provide the prior knowledge for our higher order energy function. Then, a new cosegmentation energy function using higher order cliques is developed, which can efficiently cosegment the foreground objects with large appearance variations from a group of images in complex scenes. Both the quantitative and qualitative experimental results on representative datasets demonstrate that the accuracy of our cosegmentation results is much higher than the state-of-the-art cosegmentation methods.
C1 [Wang, Wenguan; Shen, Jianbing] Beijing Inst Technol, Sch Comp Sci, Beijing Key Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Shen, JB (corresponding author), Beijing Inst Technol, Sch Comp Sci, Beijing Key Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
EM wangwenguan@bit.edu.cn; shenjianbing@bit.edu.cn
RI Wang, Wenguan/AAA-5782-2022; Shen, Jianbing/U-8796-2019
OI Wang, Wenguan/0000-0002-0802-9567; Shen, Jianbing/0000-0002-4109-8353
FU National Basic Research Program of China (973 Program) [2013CB328805];
   National Natural Science Foundation of China [61272359]; Fok Ying Tung
   Education Foundation Specialized Fund for Joint Building Program of
   Beijing Municipal Education Commission [141067]
FX This work was supported in part by the National Basic Research Program
   of China (973 Program) under Grant 2013CB328805, in part by the National
   Natural Science Foundation of China under Grant 61272359, and in part by
   the Fok Ying Tung Education Foundation under Grant 141067 Specialized
   Fund for Joint Building Program of Beijing Municipal Education
   Commission. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Chengcui Zhang.
   (Corresponding author: Jianbing Shen.)
CR Batra D, 2011, INT J COMPUT VISION, V93, P273, DOI 10.1007/s11263-010-0415-x
   Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5
   Cao XC, 2014, IEEE T IMAGE PROCESS, V23, P4175, DOI 10.1109/TIP.2014.2332399
   Chai YN, 2011, IEEE I CONF COMP VIS, P2579, DOI 10.1109/ICCV.2011.6126546
   Cheng B, 2011, IEEE I CONF COMP VIS, P2439, DOI 10.1109/ICCV.2011.6126528
   Collins MD, 2012, PROC CVPR IEEE, P1656, DOI 10.1109/CVPR.2012.6247859
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Couprie C, 2009, IEEE I CONF COMP VIS, P731, DOI 10.1109/ICCV.2009.5459284
   Dong XP, 2016, IEEE T IMAGE PROCESS, V25, P516, DOI 10.1109/TIP.2015.2505184
   Dong XP, 2015, IEEE T IMAGE PROCESS, V24, P3966, DOI 10.1109/TIP.2015.2456636
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Fu HZ, 2015, PROC CVPR IEEE, P4428, DOI 10.1109/CVPR.2015.7299072
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Gallagher A. C., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1857, DOI 10.1109/CVPR.2011.5995452
   Gulshan V, 2010, PROC CVPR IEEE, P3129, DOI 10.1109/CVPR.2010.5540073
   Hochbaum DS, 2009, IEEE I CONF COMP VIS, P269, DOI 10.1109/ICCV.2009.5459261
   Ishikawa Hiroshi, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2993, DOI 10.1109/CVPRW.2009.5206689
   Ishikawa H, 2011, IEEE T PATTERN ANAL, V33, P1234, DOI 10.1109/TPAMI.2010.91
   Ishikawa H, 2009, PROC CVPR IEEE, P2985
   Joulin A, 2012, PROC CVPR IEEE, P542, DOI 10.1109/CVPR.2012.6247719
   Joulin A, 2010, PROC CVPR IEEE, P1943, DOI 10.1109/CVPR.2010.5539868
   Kai-Yueh Chang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2129, DOI 10.1109/CVPR.2011.5995415
   Kim G, 2011, IEEE I CONF COMP VIS, P169, DOI 10.1109/ICCV.2011.6126239
   Kim TH, 2010, PROC CVPR IEEE, P3201, DOI 10.1109/CVPR.2010.5540078
   Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Kolmogorov V., 2006, P IEEE CVPR, V1, P993, DOI DOI 10.1109/CVPR.2006.91
   Lee C, 2015, PROC CVPR IEEE, P3837, DOI 10.1109/CVPR.2015.7299008
   Lou ZY, 2014, IEEE T MULTIMEDIA, V16, P2110, DOI 10.1109/TMM.2014.2363936
   Mukherjee Lopamudra, 2011, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, P1881, DOI 10.1109/CVPR.2011.5995420
   Mukherjee L, 2009, PROC CVPR IEEE, P2028, DOI 10.1109/CVPRW.2009.5206652
   Park K, 2012, LECT NOTES COMPUT SC, V7573, P202, DOI 10.1007/978-3-642-33709-3_15
   Rubinstein M, 2013, PROC CVPR IEEE, P1939, DOI 10.1109/CVPR.2013.253
   Rubio JC, 2012, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2012.6247745
   Shen JB, 2014, IEEE T IMAGE PROCESS, V23, P1451, DOI 10.1109/TIP.2014.2302892
   Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1
   Vicente S, 2010, LECT NOTES COMPUT SC, V6312, P465, DOI 10.1007/978-3-642-15552-9_34
   Wang C, 2014, IEEE T MULTIMEDIA, V16, P903, DOI 10.1109/TMM.2014.2306393
   Wang F, 2014, PROC CVPR IEEE, P3142, DOI 10.1109/CVPR.2014.402
   Wang F, 2013, IEEE I CONF COMP VIS, P849, DOI 10.1109/ICCV.2013.110
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Wang Wenguan, 2015, IEEE Trans Image Process, V24, P3137, DOI 10.1109/TIP.2015.2438550
   Zhu HY, 2014, IEEE WINT CONF APPL, P485, DOI 10.1109/WACV.2014.6836062
NR 44
TC 67
Z9 71
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2016
VL 18
IS 6
BP 1011
EP 1021
DI 10.1109/TMM.2016.2545409
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DM1LP
UT WOS:000376107100006
DA 2024-07-18
ER

PT J
AU Yang, WM
   Tian, YP
   Zhou, F
   Liao, QM
   Chen, H
   Zheng, CL
AF Yang, Wenming
   Tian, Yapeng
   Zhou, Fei
   Liao, Qingmin
   Chen, Hai
   Zheng, Chenglin
TI Consistent Coding Scheme for Single-Image Super-Resolution Via
   Independent Dictionaries
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Collaborative representation; consistent coding scheme; independent
   dictionaries; mapping function; super-resolution
ID RECONSTRUCTION; ALGORITHM
AB In this paper, we present a unified frame based on collaborative representation (CR) for single-image super-resolution (SR), which learns low-resolution (LR) and high-resolution (HR) dictionaries independently in the training stage and adopts a consistent coding scheme (CCS) to guarantee the prediction accuracy of HR coding coefficients during SR reconstruction. The independent LR and HR dictionaries are learned based on CR with l(2)-norm regularization, which can well describe the corresponding LR and HR patch space, respectively. Furthermore, a mapping function is learned to map LR coding coefficients onto the corresponding HR coding coefficients. Propagation filtering can achieve smoothing over an image while preserving image context like edges or textural regions. Moreover, to preserve the edge structures of a super-resolved image and suppress artifacts, a propagation filtering-based constraint and image nonlocal self-similarity regularization are introduced into the SR reconstruction framework. Experimental comparison with state-of-the-art single image SR algorithms validates the effectiveness of proposed approach.
C1 [Yang, Wenming; Tian, Yapeng; Zhou, Fei; Liao, Qingmin] Tsinghua Univ, Dept Elect Engn,Grad Sch Shenzhen, Shenzhen Key Lab Informat Sci & Technol, Shenzhen Engn Lab IS&DRM, Shenzhen 518055, Peoples R China.
   [Chen, Hai; Zheng, Chenglin] Huawei Technol Co Ltd, Shenzhen 518129, Peoples R China.
C3 Tsinghua Shenzhen International Graduate School; Tsinghua University;
   Huawei Technologies
RP Zhou, F (corresponding author), Tsinghua Univ, Dept Elect Engn,Grad Sch Shenzhen, Shenzhen Key Lab Informat Sci & Technol, Shenzhen Engn Lab IS&DRM, Shenzhen 518055, Peoples R China.
EM yangelwm@163.com; typ14@mails.tsinghua.edu.cn; flying.zhou@163.com;
   liaoqm@tsinghua.edu.cn; chenhai@hisilicon.com;
   zhengchenglin@hisilicon.com
RI Tian, Yapeng/GSD-1491-2022
OI Tian, Yapeng/0000-0003-4271-5293; Zhou, Fei/0000-0003-1216-2181
FU National Natural Science Foundation of China [61271393, 61301183,
   61471216]; China Post-Doctoral Science Foundation [2013M540947,
   2014T70083]; Special Foundation for the Development of Strategic
   Emerging Industries of Shenzhen [JCYJ20140417115840272,
   ZDSYS20140509172959974, JCYJ20150331151358138]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61271393, Grant 61301183, and Grant 61471216, by the
   China Post-Doctoral Science Foundation under Grant 2013M540947 and Grant
   2014T70083, and by the Special Foundation for the Development of
   Strategic Emerging Industries of Shenzhen under Grant
   JCYJ20140417115840272, Grant ZDSYS20140509172959974, and Grant
   JCYJ20150331151358138. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Tommaso
   Melodia.
CR Allebach J, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P707, DOI 10.1109/ICIP.1996.560768
   Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Chakrabarti A, 2007, IEEE T MULTIMEDIA, V9, P888, DOI 10.1109/TMM.2007.893346
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Chang JHR, 2015, PROC CVPR IEEE, P10, DOI 10.1109/CVPR.2015.7298595
   Dai SY, 2009, IEEE T IMAGE PROCESS, V18, P969, DOI 10.1109/TIP.2009.2012908
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Dong WS, 2011, IEEE T IMAGE PROCESS, V20, P1838, DOI 10.1109/TIP.2011.2108306
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Jiang JJ, 2014, IEEE T MULTIMEDIA, V16, P1268, DOI 10.1109/TMM.2014.2311320
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207
   Peleg T, 2014, IEEE T IMAGE PROCESS, V23, P2569, DOI 10.1109/TIP.2014.2305844
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Sun J, 2008, PROC CVPR IEEE, P2471, DOI 10.1109/CVPR.2008.4587659
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Tingrong Yuan, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P5794, DOI 10.1109/ICASSP.2014.6854714
   Wang SL, 2012, PROC CVPR IEEE, P2216, DOI 10.1109/CVPR.2012.6247930
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiong ZW, 2013, IEEE T MULTIMEDIA, V15, P1458, DOI 10.1109/TMM.2013.2264654
   Yang JQ, 2008, ITESS: 2008 PROCEEDINGS OF INFORMATION TECHNOLOGY AND ENVIRONMENTAL SYSTEM SCIENCES, PT 1, P1, DOI 10.1109/CVPR.2008.4587647
   Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang SY, 2012, IEEE T IMAGE PROCESS, V21, P4016, DOI 10.1109/TIP.2012.2201491
   Zhang KB, 2015, IEEE T IMAGE PROCESS, V24, P846, DOI 10.1109/TIP.2015.2389629
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhang YL, 2015, LECT NOTES COMPUT SC, V9315, P63, DOI 10.1007/978-3-319-24078-7_7
   Zhou F, 2015, IEEE SIGNAL PROC LET, V22, P336, DOI 10.1109/LSP.2014.2360038
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
NR 33
TC 42
Z9 43
U1 0
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2016
VL 18
IS 3
BP 313
EP 325
DI 10.1109/TMM.2016.2515997
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DG2WP
UT WOS:000371931600001
DA 2024-07-18
ER

PT J
AU Bokani, A
   Hassan, M
   Kanhere, S
   Zhu, XQ
AF Bokani, Ayub
   Hassan, Mahbub
   Kanhere, Salil
   Zhu, Xiaoqing
TI Optimizing HTTP-Based Adaptive Streaming in Vehicular Environment Using
   Markov Decision Process
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic adaptive streaming over HTTP; Markov decision process (MDP);
   video streaming
ID RATE ADAPTATION; VIDEO
AB Hypertext transfer protocol (HTTP) is the fundamental mechanics supporting web browsing on the Internet. An HTTP server stores large volumes of contents and delivers specific pieces to the clients when requested. There is a recent move to use HTTP for video streaming as well, which promises seamless integration of video delivery to existing HTTP-based server platforms. This is achieved by segmenting the video into many small chunks and storing these chunks as separate files on the server. For adaptive streaming, the server stores different quality versions of the same chunk in different files to allow real-time quality adaptation of the video due to network bandwidth variation experienced by a client. For each chunk of the video, which quality version to download, therefore, becomes a major decision-making challenge for the streaming client, especially in vehicular environment with significant uncertainty in mobile bandwidth. In this paper, we demonstrate that for such decision making, the Markov decision process (MDP) is superior to previously proposed non-MDP solutions. Using publicly available video and bandwidth datasets, we show that the MDP achieves up to a 15x reduction in playback deadline miss compared to a well-known non-MDP solution when the MDP has the prior knowledge of the bandwidth model. We also consider a model-free MDP implementation that uses Q-learning to gradually learn the optimal decisions by continuously observing the outcome of its decision making. We find that the MDP with Q-learning significantly outperforms the MDP that uses bandwidth models.
C1 [Bokani, Ayub; Hassan, Mahbub; Kanhere, Salil] Univ New S Wales, Sch Comp Sci & Engn, Sydney, NSW 2033, Australia.
   [Zhu, Xiaoqing] Cisco Syst, Chief Technol & Architecture Off, San Jose, CA 95134 USA.
C3 University of New South Wales Sydney; Cisco Systems Inc
RP Bokani, A (corresponding author), Univ New S Wales, Sch Comp Sci & Engn, Sydney, NSW 2033, Australia.
EM abokani@cse.unsw.edu.au; mahbub@cse.unsw.edu.au; salilk@cse.unsw.edu.au;
   xiaoqzhu@cisco.com
RI Hassan, Mahbub/AAF-2656-2019; kanhere, salil/ABA-2025-2021
OI kanhere, salil/0000-0002-1835-3475; Hassan, Mahbub/0000-0002-3417-8590;
   Bokani, Ayub/0000-0001-5160-7724
FU National ICT Australia (NICTA)
FX The work of A. Bokani was supported by the National ICT Australia
   (NICTA). The associate editor coordinating the review of this manuscript
   and approving it for publication was Prof. Christian Timmerer.
CR [Anonymous], P 3 MULT SYST C MAR
   [Anonymous], P IEEE GLOB C SIGN I
   [Anonymous], 1989, LEARNING DELAYED REW
   [Anonymous], 2011, Proc. second annu. acm conf. multimed. syst.-mmsys'11, DOI DOI 10.1145/1943552.1943574
   [Anonymous], 2004, P 2004 C HUM FACT CO
   [Anonymous], HTTP LIV STREAM OV
   [Anonymous], 2008, Proc. ACM WiNTECH
   [Anonymous], P 10 ACM C INT MEAS
   [Anonymous], 2012, Proceedings of the 22nd international workshop on Network and Operating System Support for Digital Audio and Video
   [Anonymous], P 18 INT C ADV INF N
   [Anonymous], IIS SMOOTH STREAM TE
   [Anonymous], 2011, 2011 12 IFIP IEEE IN
   [Anonymous], CORR
   [Anonymous], 2009, MARKOV DECISION PROC
   [Anonymous], ACM T MULTIM COMPUT
   [Anonymous], 2014, P 2014 WORKSH DES QU, DOI DOI 10.1145/2676652.2683463
   Bokani A, 2013, 2013 20TH INTERNATIONAL PACKET VIDEO WORKSHOP (PV)
   Charvillat V, 2007, J NETW COMPUT APPL, V30, P1034, DOI 10.1016/j.jnca.2005.12.010
   Curcio I.D., 2010, Proc. ACM Workshop on Mobile Video (MoVid), P3
   Dai H., 2011, Acta Horticulturae, P169, DOI 10.1145/1943552.1943575
   de Cuetos P., 2003, MULTIMEDIA '03: Proceedings of the eleventh ACM international conference on Multimedia, P55
   Huang T.Y., 2013, P 2013 ACM SIGCOMM W, P9, DOI [DOI 10.1145/2491172, DOI 10.1145/2491172.2491179]
   Jarnikov D, 2011, SIGNAL PROCESS-IMAGE, V26, P378, DOI 10.1016/j.image.2011.03.003
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Krishnan SS, 2013, IEEE ACM T NETWORK, V21, P2001, DOI 10.1109/TNET.2013.2281542
   Lederer S., 2012, P 3 MULT SYST C, P89
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Mastronarde N, 2013, IEEE T MULTIMEDIA, V15, P268, DOI 10.1109/TMM.2012.2231668
   Miller K., 2012, 2012 Proceedings of the 19th International Packet Video Workshop (PV 2012), P173, DOI 10.1109/PV.2012.6229732
   Murtaza G, 2014, IEEE ICC, P2454, DOI 10.1109/ICC.2014.6883691
   Singh V., 2012, Proc. IEEE World of Wireless, P1
   Sobhani A., 2015, P 25 ACM WORKSHOP NE, P31
   Stockmeier T, 2011, PROC INT SYMP POWER, P324, DOI 10.1109/ISPSD.2011.5890856
   Tian G., 2012, P 8 INT C EM NETW EX, P109
   Van Der Wal J., 1980, Stochastic dynamic programming: successive approximations and nearly optimal strategies for Markov decision processes and Markov games
   Wüst CC, 2004, J SCHEDULING, V7, P105, DOI 10.1023/B:JOSH.0000014067.74332.74
   Xiang S., 2012, ACM MMSys '12, P167
   Xing M, 2012, IEEE GLOB COMM CONF, P5745, DOI 10.1109/GLOCOM.2012.6504037
   Yao J, 2012, IEEE T MOBILE COMPUT, V11, P603, DOI 10.1109/TMC.2011.97
NR 39
TC 41
Z9 42
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2015
VL 17
IS 12
BP 2297
EP 2309
DI 10.1109/TMM.2015.2494458
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CW9JR
UT WOS:000365315500016
DA 2024-07-18
ER

PT J
AU Lu, P
   Sun, QY
   Wu, KY
   Zhu, ZQ
AF Lu, Ping
   Sun, Quanying
   Wu, Kaiyue
   Zhu, Zuqing
TI Distributed Online Hybrid Cloud Management for Profit-Driven Multimedia
   Cloud Computing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Datacenter management; hybrid cloud; Lyapunov optimization; QoS-aware
   requests scheduling
ID RESOURCE-ALLOCATION; SERVICE; ENERGY
AB It is known that with a hybrid cloud, a multimedia cloud service provider (MCSP) can quickly extend its services to multiple geographical locations with quality-of-service (QoS) guarantees. Meanwhile, to maximize its profit, the MCSP needs an online management mechanism to operate the hybrid cloud efficiently. In this paper, we study how to maximize an MCSP's profit from provisioning multimedia services to geographically distributed users with a hybrid cloud. We first design a service provisioning model to manage the resources in the hybrid cloud. Here, in order to make the model practical and address the different situations in private and public clouds, we consider different time granularities for resource reservations. Then, we leverage the Lyapunov optimization technique to maximize the profit of MCSP and propose an online algorithm that can manage the hybrid cloud in the distributed manner. Specifically, the algorithm determines the access control and routing of each multimedia service request, and allocates the resources in the hybrid cloud accordingly. We also apply the epsilon-persistent technique to ensure that the worst-case latency of the provisioned requests is bounded. Finally, the proposed algorithm is evaluated with extensive simulations using both synthetical and real traces. Simulation results indicate that the algorithm can manage the hybrid cloud efficiently and maximize the profit of MCSP.
C1 [Lu, Ping; Sun, Quanying; Wu, Kaiyue; Zhu, Zuqing] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Lu, P (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.
EM lpbest@mail.ustc.edu.cn; sqy0410@mail.ustc.edu.cn;
   wooloo@mail.ustc.edu.cn; zqzhu@ieee.org
RI Zhu, Zuqing/J-8431-2017
OI Zhu, Zuqing/0000-0002-4251-788X; Wu, Kaiyue/0000-0003-4032-3878
FU NSFC [61371117]; Fundamental Research Funds for the Central Universities
   [WK2100060010]; Natural Science Research Project for Universities in
   Anhui [KJ2014ZD38]; Strategic Priority Research Program of the CAS
   [XDA06011202]
FX This work was supported in part by the NSFC Project 61371117, in part by
   the Fundamental Research Funds for the Central Universities under Grant
   WK2100060010, in part by the Natural Science Research Project for
   Universities in Anhui under Grant KJ2014ZD38, and in part by the
   Strategic Priority Research Program of the CAS under Grant XDA06011202.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Ali Begen.
CR Altamimi M., 2012, 2012 IEEE 5th International Conference on Cloud Computing (CLOUD), P764, DOI 10.1109/CLOUD.2012.72
   [Anonymous], 2010, 2010 IEEE 3 INT C CL
   [Anonymous], 2014, 2014 IEEE INT C MULT, DOI DOI 10.1109/ICME.2014.6890255
   Bisio I., 2012, IEEE International Conference on Communications (ICC 2012), P4943, DOI 10.1109/ICC.2012.6363942
   Bisio I, 2014, IEEE GLOB COMM CONF, P2454, DOI 10.1109/GLOCOM.2014.7037176
   Bittencourt LF, 2012, IEEE COMMUN MAG, V50, P42, DOI 10.1109/MCOM.2012.6295710
   Cahn R. S., 1998, MOR KAUF NETW
   Duan RB, 2014, IEEE T CLOUD COMPUT, V2, P29, DOI 10.1109/TCC.2014.2303077
   Gong WQ, 2014, INT CONF UBIQ FUTUR, P249, DOI 10.1109/ICUFN.2014.6876791
   Greenberg A, 2009, ACM SIGCOMM COMP COM, V39, P68, DOI 10.1145/1496091.1496103
   Haitao Li, 2011, Proceedings of the 2011 IEEE 4th International Conference on Cloud Computing (CLOUD 2011), P203, DOI 10.1109/CLOUD.2011.41
   Hajjat M, 2010, ACM SIGCOMM COMP COM, V40, P243, DOI 10.1145/1851275.1851212
   Hu H., 2014, proceeding of The IEEE International Conference on Multimedia and Expo (ICME), P1, DOI 10.1109/ICME.2014.6890134
   Jung I. Y., 2011, Proceedings of the 2011 IEEE Asia-Pacific Services Computing Conference (APSCC), P203, DOI 10.1109/APSCC.2011.32
   Lin CC, 2014, IEEE SYST J, V8, P225, DOI 10.1109/JSYST.2013.2256320
   Liu FM, 2014, IEEE T PARALL DISTR, V25, P2648, DOI 10.1109/TPDS.2013.208
   Nan XM, 2013, IEEE INT SYMP CIRC S, P2872, DOI 10.1109/ISCAS.2013.6572478
   Nan XM, 2012, IEEE INT SYMP CIRC S, P1111
   Neely M. J., 2010, STOCHASTIC NETWORK O
   Neely MJ, 2011, IEEE INFOCOM SER, P1728, DOI 10.1109/INFCOM.2011.5934971
   Qiu X., IEEE T PARA IN PRESS
   Ren SL, 2012, INT CON DISTR COMP S, P22, DOI 10.1109/ICDCS.2012.77
   Shifrin M, 2013, 2013 IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT (IM 2013), P51
   Tang JH, 2014, IEEE T MULTIMEDIA, V16, P1434, DOI 10.1109/TMM.2014.2308726
   Urgaonkar R, 2010, IEEE IFIP NETW OPER, P479, DOI 10.1109/NOMS.2010.5488484
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Wu Y, 2012, IEEE INFOCOM SER, P684, DOI 10.1109/INFCOM.2012.6195813
   Wu Y, 2011, INT CON DISTR COMP S, P268, DOI 10.1109/ICDCS.2011.50
   Xiaoming Nan, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P684, DOI 10.1109/ICASSP.2014.6853683
   Xu H, 2013, IEEE INFOCOM SER, P854
   Zhang H, 2014, IEEE T NETW SERV MAN, V11, P90, DOI 10.1109/TNSM.2013.122313.130448
   Zhang WW, 2014, IEEE T VEH TECHNOL, V63, P2002, DOI 10.1109/TVT.2014.2310394
   Zhu WW, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940269
   Zink M, 2009, COMPUT NETW, V53, P501, DOI 10.1016/j.comnet.2008.09.022
NR 34
TC 51
Z9 54
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1297
EP 1308
DI 10.1109/TMM.2015.2441004
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000015
DA 2024-07-18
ER

PT J
AU Thomos, N
   Kurdoglu, E
   Frossard, P
   van der Schaar, M
AF Thomos, Nikolaos
   Kurdoglu, Eymen
   Frossard, Pascal
   van der Schaar, Mihaela
TI Adaptive Prioritized Random Linear Coding and Scheduling for Layered
   Data Delivery From Multiple Servers
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Layered data; Markov decision processes (MDP); prioritized random linear
   codes (PRLC); Q-learning; rateless codes; virtual experience
ID FOUNTAIN CODES; VIDEO; DELAY
AB In this paper, we deal with the problem of jointly determining the optimal coding strategy and the scheduling decisions when receivers obtain layered data from multiple servers. The layered data is encoded by means of prioritized random linear coding (PRLC) in order to be resilient to channel loss while respecting the unequal levels of importance in the data, and data blocks are transmitted simultaneously in order to reduce decoding delays and improve the delivery performance. We formulate the optimal coding and scheduling decisions problem in our novel framework with the help of Markov decision processes (MDP), which are effective tools for modeling adapting streaming systems. Reinforcement learning approaches are then proposed to derive reduced computational complexity solutions to the adaptive coding and scheduling problems. The novel reinforcement learning approaches and the MDP solution are examined in an illustrative example for scalable video transmission. Our methods offer large performance gains over competing methods that deliver the data blocks sequentially. The experimental evaluation also shows that our novel algorithms offer continuous playback and guarantee small quality variations which is not the case for baseline solutions. Finally, our work highlights the advantages of reinforcement learning algorithms to forecast the temporal evolution of data demands and to decide the optimal coding and scheduling decisions.
C1 [Thomos, Nikolaos] Univ Essex, Comp Sci & Elect Engn Dept, Colchester CO4 3SQ, Essex, England.
   [Kurdoglu, Eymen] NYU, Polytech Inst, Elect & Comp Engn Dept, Brooklyn, NY 11201 USA.
   [Frossard, Pascal] Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4 4, CH-1004 Lausanne, Switzerland.
   [van der Schaar, Mihaela] Univ Calif Los Angeles, Dept Elect Engn, Networks Econ Commun Syst Informat & Multimedia R, Los Angeles, CA 90095 USA.
C3 University of Essex; New York University; New York University Tandon
   School of Engineering; Swiss Federal Institutes of Technology Domain;
   Ecole Polytechnique Federale de Lausanne; University of California
   System; University of California Los Angeles
RP Thomos, N (corresponding author), Univ Essex, Comp Sci & Elect Engn Dept, Colchester CO4 3SQ, Essex, England.
EM nthomos@essex.ac.uk; eymen@vision.poly.edu; pascal.frossard@epfl.ch;
   mihaela@ee.ucla.edu
RI Thomos, Nikolaos/AAU-2328-2020; Frossard, Pascal/AAF-2268-2019
OI van der schaar, Mihaela/0000-0003-3933-6049; Thomos,
   Nikolaos/0000-0001-7266-2642
FU "Adaptive Network Coding for Video Communications" project - Hasler
   Foundation
FX This work was supported in part by the "Adaptive Network Coding for
   Video Communications" project funded by the Hasler Foundation. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Liang Zhou.
CR Ahmad S, 2011, IEEE T MULTIMEDIA, V13, P92, DOI 10.1109/TMM.2010.2093511
   [Anonymous], P 41 ALL ANN C COMM
   [Anonymous], MULT EXP ICME 2011 I
   [Anonymous], P 41 ALL C COMM CONT
   BARTO AG, 1995, ARTIF INTELL, V72, P81, DOI 10.1016/0004-3702(94)00011-O
   Bellman R., 1957, Dynamic programming
   Bertsekas D. P., 1999, Nonlinear Program, V2nd
   Bertsekas D. P., 2011, DYNAMIC PROGRAMMING, Vii
   Bogino MCO, 2007, IEEE INT SYMP CIRC S, P3467, DOI 10.1109/ISCAS.2007.378373
   Cataldi P, 2010, IEEE T IMAGE PROCESS, V19, P1491, DOI 10.1109/TIP.2010.2042985
   Ding Y, 2012, IEEE ACM T NETWORK, V20, P1800, DOI 10.1109/TNET.2012.2188642
   Dong N, 2011, IEEE T VEH TECHNOL, V60, P1086, DOI 10.1109/TVT.2011.2112677
   Drinea E, 2013, PHYS COMMUN-AMST, V6, P100, DOI 10.1016/j.phycom.2012.05.005
   Fu FW, 2012, IEEE T VEH TECHNOL, V61, P3931, DOI 10.1109/TVT.2012.2213850
   Fu FW, 2010, IEEE J SEL AREA COMM, V28, P308, DOI 10.1109/JSAC.2010.100403
   Halloush M, 2011, IEEE T WIREL COMMUN, V10, P466, DOI 10.1109/TWC.2011.120810.090280
   Lin YF, 2009, IEEE T PARALL DISTR, V20, P1653, DOI 10.1109/TPDS.2008.251
   Luby M, 2002, ANN IEEE SYMP FOUND, P271, DOI 10.1109/SFCS.2002.1181950
   Lucani D. E., IEEE J SEL ARE UNPUB
   Mastronarde N, 2013, IEEE T MOBILE COMPUT, V12, P694, DOI 10.1109/TMC.2012.36
   Nistor M, 2011, IEEE J SEL AREA COMM, V29, P1084, DOI 10.1109/JSAC.2011.110518
   Salodkar N, 2008, IEEE J SEL AREA COMM, V26, P732, DOI 10.1109/JSAC.2008.080514
   Schierl T, 2008, J VIS COMMUN IMAGE R, V19, P500, DOI 10.1016/j.jvcir.2008.06.004
   Seferoglu H, 2009, IEEE J SEL AREA COMM, V27, P713, DOI 10.1109/JSAC.2009.090612
   Sejdinovic D, 2009, IEEE T COMMUN, V57, P2510, DOI 10.1109/TCOMM.2009.09.070616
   Shiang HP, 2012, IEEE T MULTIMEDIA, V14, P896, DOI 10.1109/TMM.2012.2187178
   Shiang HP, 2010, IEEE J SEL AREA COMM, V28, P728, DOI 10.1109/JSAC.2010.100610
   Shokrollahi A, 2006, IEEE T INFORM THEORY, V52, P2551, DOI 10.1109/TIT.2006.874390
   Sutton R., 1998, Reinforcement Learning: An Introduction
   Thomos N., IEEE T MULT IN PRESS
   Thomos N, 2012, IEEE COMMUN LETT, V16, P1860, DOI 10.1109/LCOMM.2012.092812.121661
   Thomos N, 2011, IEEE T MULTIMEDIA, V13, P776, DOI 10.1109/TMM.2011.2111364
   Tournoux PU, 2011, IEEE T MULTIMEDIA, V13, P797, DOI 10.1109/TMM.2011.2126564
   Tse D., 2005, Fundementals of Wireless Communications
   Vukobratovic D, 2009, IEEE T MULTIMEDIA, V11, P1094, DOI 10.1109/TMM.2009.2026087
   Wagner JP, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1501, DOI 10.1109/ICME.2006.262827
   Wang M, 2007, IEEE J SEL AREA COMM, V25, P1655, DOI 10.1109/JSAC.2007.071205
   Watkins C. J. C. H., 1998, THESIS CAMBRIDGE U C
   Yang L., 2012, Proceedings of the thirteenth ACM international symposium on Mobile Ad Hoc Networking and Computing, P105, DOI [10.1145/2248371.2248389, DOI 10.1145/2248371.2248389]
NR 39
TC 28
Z9 30
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2015
VL 17
IS 6
BP 893
EP 906
DI 10.1109/TMM.2015.2425228
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CI1TM
UT WOS:000354527500011
OA Green Accepted, Green Submitted
DA 2024-07-18
ER

PT J
AU Pei, SC
   Wang, YY
AF Pei, Soo-Chang
   Wang, Yu-Ying
TI Auxiliary Metadata Delivery in View Synthesis Using Depth No Synthesis
   Error Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Depth no-synthesis-error (D-NOSE); depth-image-based rendering (DIBR);
   human visual system (HVS); unseen visible watermarking (UVW)
ID UNSEEN VISIBLE WATERMARKING
AB In this paper, a novel data hiding scheme, 3D unseen visible watermarking (UVW) based on a depth no-synthesis-error (D-NOSE) model is proposed. The proposed method has the ability to detect and discover which region of a depth image is suitable for watermark embedding and calculate optimal variance in pixel values for watermarking. The watermark is embedded into the suitable region by modifying the depth map with optimal variation pixel values to ensure that the watermarked cover can be perceived the same as the original one with no synthesis error under normal rendering conditions. In the proposed UVW scheme, hidden information can be extracted successfully by the human visual system (HVS) when changing the rendering conditions. Experimental results prove that the proposed scheme has strong robustness and imperceptibility while depth distortions follow the thresholds derived from D-NOSE model. Practical applications and limitations of the proposed 3D UVW for auxiliary information delivery are also discussed in this paper.
C1 [Pei, Soo-Chang] Natl Taiwan Univ, Dept Elect Engn, Taipei 10617, Taiwan.
   [Pei, Soo-Chang; Wang, Yu-Ying] Natl Taiwan Univ, Grad Inst Commun Engn, Taipei 10617, Taiwan.
C3 National Taiwan University; National Taiwan University
RP Pei, SC (corresponding author), Natl Taiwan Univ, Dept Elect Engn, Taipei 10617, Taiwan.
EM pei@cc.ee.ntu.edu.tw; d98942026@ntu.edu.tw
FU Ministry of Science and Technology, Taiwan [103-2221-E-002-120]
FX This work was supported by the Ministry of Science and Technology,
   Taiwan, under Contract 103-2221-E-002-120. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Alessandro Piva.
CR Chuang SC, 2007, IEEE IMAGE PROC, P1389
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Huang CH, 2009, IEEE T INF FOREN SEC, V4, P193, DOI 10.1109/TIFS.2009.2020778
   Kauff P, 2007, SIGNAL PROCESS-IMAGE, V22, P217, DOI 10.1016/j.image.2006.11.013
   Lin YH, 2012, INT CONF ACOUST SPEE, P1801, DOI 10.1109/ICASSP.2012.6288250
   Müller K, 2011, P IEEE, V99, P643, DOI 10.1109/JPROC.2010.2091090
   Redert A, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P313
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Zhang L, 2005, IEEE T BROADCAST, V51, P191, DOI 10.1109/TBC.2005.846190
   Zhao Y, 2011, IEEE T IMAGE PROCESS, V20, P2221, DOI 10.1109/TIP.2011.2118218
NR 10
TC 16
Z9 16
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2015
VL 17
IS 1
BP 128
EP 133
DI 10.1109/TMM.2014.2368255
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AX6QW
UT WOS:000347047400012
DA 2024-07-18
ER

PT J
AU Kang, MK
   Yoon, KJ
AF Kang, Min-Koo
   Yoon, Kuk-Jin
TI Depth-Discrepancy-Compensated Inter-Prediction With Adaptive Segment
   Management for Multiview Depth Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D video; depth coding; inter-prediction; multiview video coding (MVC)
ID MAPS RECOVERY
AB One of the most frequently encountered problems in multiview depth video coding (MDVC) is the relatively low temporal and inter-view correlations when compared to that of multiview color video coding (MCVC). This directly results in degraded performance of the conventional inter-prediction. In this study, we analyze noticeable differences in statistical characteristics between MDVC and MCVC. On the basis of these differences, we propose a new depth-discrepancy compensation (DDC) method with adaptive segment management to alleviate the performance degradation. Experimental results show that the proposed method performs better than the original inter-prediction of JMVC v8.2 in terms of depth bit-savings and synthesized view quality.
C1 [Kang, Min-Koo; Yoon, Kuk-Jin] GIST, Sch Informat & Commun, Kwangju 500712, South Korea.
C3 Gwangju Institute of Science & Technology (GIST)
RP Kang, MK (corresponding author), GIST, Sch Informat & Commun, Kwangju 500712, South Korea.
EM minkoo@gist.ac.kr; kjyoon@gist.ac.kr
RI Yoon, Kuk-Jin/F-4329-2018
OI Kang, Min-Koo/0000-0003-1109-4818
CR [Anonymous], JTC1SC29WG11 ISOIEC
   [Anonymous], 2001, VCEGM33 ITUT Q 616
   [Anonymous], JTC1SC29WG11 ISOIEC
   [Anonymous], 2009, JVTAE207 ISOIEC MPEG
   [Anonymous], 2009, M16923 ISOIEC JTC1SC
   [Anonymous], P IEEE ICIP
   Besl P. J., 1988, Machine Vision and Applications, V1, P127, DOI 10.1007/BF01212277
   Hannuksela MM, 2013, IEEE T IMAGE PROCESS, V22, P3449, DOI 10.1109/TIP.2013.2269274
   Hur JH, 2007, IEEE T CIRC SYST VID, V17, P1496, DOI 10.1109/TCSVT.2007.903774
   Kang MK, 2012, IEEE T MULTIMEDIA, V14, P121, DOI 10.1109/TMM.2011.2169238
   Khattak S, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P97, DOI 10.1109/PCS.2012.6213295
   Kim WS, 2010, PROC SPIE, V7543, DOI 10.1117/12.839030
   Lee JY, 2011, IEEE T CIRC SYST VID, V21, P1859, DOI 10.1109/TCSVT.2011.2154730
   Li R, 2012, IEEE IMAGE PROC, P1329, DOI 10.1109/ICIP.2012.6467113
   Merkle P, 2009, SIGNAL PROCESS-IMAGE, V24, P73, DOI 10.1016/j.image.2008.10.010
   Merkle P, 2007, IEEE IMAGE PROC, P201
   Merkle P, 2007, IEEE T CIRC SYST VID, V17, P1461, DOI 10.1109/TCSVT.2007.903665
   Min DB, 2012, IEEE T IMAGE PROCESS, V21, P1176, DOI 10.1109/TIP.2011.2163164
   Na ST, 2008, IEEE IMAGE PROC, P2468, DOI 10.1109/ICIP.2008.4712293
   Oh H, 2006, LECT NOTES COMPUT SC, V4319, P898
   Oh KJ, 2011, IEEE T CIRC SYST VID, V21, P350, DOI 10.1109/TCSVT.2011.2116590
   OHM JR, 1994, IEEE T IMAGE PROCESS, V3, P559, DOI 10.1109/83.334985
   Rocchini C, 2001, COMPUT GRAPH FORUM, V20, pC299
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Schwarz H, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1929, DOI 10.1109/ICME.2006.262934
   Smolic A, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P2161, DOI 10.1109/ICME.2006.262683
   Vetro A, 2011, P IEEE, V99, P626, DOI 10.1109/JPROC.2010.2098830
   Yang WZ, 2012, PROC CVPR IEEE, P1466, DOI 10.1109/CVPR.2012.6247835
   Zhang GF, 2009, IEEE T PATTERN ANAL, V31, P974, DOI 10.1109/TPAMI.2009.52
   Zhang Y, 2013, IEEE T IMAGE PROCESS, V22, P3497, DOI 10.1109/TIP.2013.2265883
NR 30
TC 3
Z9 3
U1 3
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1563
EP 1573
DI 10.1109/TMM.2014.2323939
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200006
DA 2024-07-18
ER

PT J
AU Kafai, M
   Eshghi, K
   Bhanu, B
AF Kafai, Mehran
   Eshghi, Kave
   Bhanu, Bir
TI Discrete Cosine Transform Locality-Sensitive Hashes for Face Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Discrete Cosine Transform (DCT) hashing; face indexing; image retrieval;
   Local Binary Patterns (LBP); Locality-Sensitive Hashing (LSH)
ID NEAREST-NEIGHBOR; FEATURES
AB Descriptors such as local binary patterns perform well for face recognition. Searching large databases using such descriptors has been problematic due to the cost of the linear search, and the inadequate performance of existing indexing methods. We present Discrete Cosine Transform (DCT) hashing for creating index structures for face descriptors. Hashes play the role of keywords: an index is created, and queried to find the images most similar to the query image. Common hash suppression is used to improve retrieval efficiency and accuracy. Results are shown on a combination of six publicly available face databases (LFW, FERET, FEI, BioID, Multi-PIE, and RaFD). It is shown that DCT hashing has significantly better retrieval accuracy and it is more efficient compared to other popular state-of-the-art hash algorithms.
C1 [Kafai, Mehran] Hewlett Packard Labs, Palo Alto, CA 94304 USA.
   [Eshghi, Kave] Google Inc, Mountain View, CA 94043 USA.
   [Bhanu, Bir] Univ Calif Riverside, Ctr Res Intelligent Syst, Riverside, CA 92521 USA.
C3 Hewlett-Packard; Google Incorporated; University of California System;
   University of California Riverside
RP Kafai, M (corresponding author), Hewlett Packard Labs, Palo Alto, CA 94304 USA.
EM mehran.kafai@hp.com; kave@google.com; bhanu@cris.ucr.edu
OI Bhanu, Bir/0000-0001-8971-6416
FU National Science Foundation [0905671, 0915270]; Div Of Information &
   Intelligent Systems; Direct For Computer & Info Scie & Enginr [0905671,
   0915270] Funding Source: National Science Foundation
FX This work was supported in part by National Science Foundation grants
   0905671 and 0915270. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Jing-Ming
   Guo.
CR Ahonen T., 2008, P ICPR
   Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   [Anonymous], 2008, NIPS
   [Anonymous], 2007, P CVPR
   [Anonymous], P CVPR
   [Anonymous], 2010, P CVPR
   [Anonymous], 2008, P ECCV
   [Anonymous], P CVPR
   [Anonymous], P CVPR
   [Anonymous], P INT C ART INT STAT
   Bengio SamyBengio., 2005, Int. Conf. on Machine Learning, P9
   Bhanu B, 2003, IEEE T PATTERN ANAL, V25, P616, DOI 10.1109/TPAMI.2003.1195995
   Bo L., 2009, P NIPS
   Chen H, 2009, IEEE T PATTERN ANAL, V31, P172, DOI 10.1109/TPAMI.2008.176
   Cheng XG, 2011, IEEE T MULTIMEDIA, V13, P1333, DOI 10.1109/TMM.2011.2167222
   Chum O., 2008, P BMVC
   Chum O., 2012, P CVPR
   Davarzani R, 2013, FORENSIC SCI INT, V231, P61, DOI 10.1016/j.forsciint.2013.04.023
   Déniz O, 2011, PATTERN RECOGN LETT, V32, P1598, DOI 10.1016/j.patrec.2011.01.004
   Dey S, 2012, IEEE T INF FOREN SEC, V7, P1192, DOI 10.1109/TIFS.2012.2196515
   Dragomir S. S., 2003, J. Ineq. Pure Appl. Math., V4, P1
   Eshghi K., 2008, Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P221, DOI [DOI 10.1145/1401890.1401921, 10.1145/1401890.1401921]
   Gionis A., 1999, P ICVLD
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Grother PJ, 1997, PATTERN RECOGN, V30, P459, DOI 10.1016/S0031-3203(96)00098-2
   Gyaourova A, 2012, IEEE T INF FOREN SEC, V7, P518, DOI 10.1109/TIFS.2011.2172429
   He J, 2011, P CVPR
   He KM, 2012, PROC CVPR IEEE, P111, DOI 10.1109/CVPR.2012.6247665
   HOEFFDING W, 1951, ANN MATH STAT, V22, P558, DOI 10.1214/aoms/1177729545
   Huang D, 2011, IEEE T SYST MAN CY C, V41, P765, DOI 10.1109/TSMCC.2011.2118750
   Jain A., 2012, IEEE MultiMedia, V19
   Kaushik V. D., 2012, NEUROCOMPUT
   Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219
   Langner O, 2010, COGNITION EMOTION, V24, P1377, DOI 10.1080/02699930903485076
   Lin Y., 2013, P CVPR
   Lu CS, 2005, MULTIMEDIA SYST, V11, P159, DOI 10.1007/s00530-005-0199-y
   Pauleve L, 2010, PATTERN RECOGN LETT, V31, P1348, DOI 10.1016/j.patrec.2010.04.004
   Phillips P., 1997, P ICPR
   SALTON G, 1988, P INF PROC MAN
   Shakhnarovich G., 2006, NEAREST NEIGHBOR MET
   Thomaz CE, 2010, IMAGE VISION COMPUT, V28, P902, DOI 10.1016/j.imavis.2009.11.005
   Vasicek O.A., 1998, J COMPUT FINANC, V1, P5
   Vikram TN, 2008, CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOL 1, PROCEEDINGS, P543, DOI 10.1109/CISP.2008.740
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wu Z, 2011, IEEE T PATTERN ANAL, V33, P1991, DOI 10.1109/TPAMI.2011.111
   Zhang N., 2007, Tech. Rep. 07-49, P7
NR 46
TC 36
Z9 38
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2014
VL 16
IS 4
BP 1090
EP 1103
DI 10.1109/TMM.2014.2305633
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AJ8LN
UT WOS:000337955800016
OA Green Published
DA 2024-07-18
ER

PT J
AU Katsurai, M
   Ogawa, T
   Haseyama, M
AF Katsurai, Marie
   Ogawa, Takahiro
   Haseyama, Miki
TI A Cross-Modal Approach for Extracting Semantic Relationships Between
   Concepts Using Tagged Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Canonical correlation analysis; concept relationships; flickr; tagged
   images
ID SIMILARITY; DETECTORS; ONTOLOGY; WORDNET; CONTEXT; SEARCH
AB This paper presents a cross-modal approach for extracting semantic relationships between concepts using tagged images. In the proposed method, we first project both text and visual features of the tagged images to a latent space using canonical correlation analysis (CCA). Then, under the probabilistic interpretation of CCA, we calculate a representative distribution of the latent variables for each concept. Based on the representative distributions of the concepts, we derive two types of measures: the semantic relatedness between the concepts and the abstraction level of each concept. Because these measures are derived from a cross-modal scheme that enables the collaborative use of both text and visual features, the semantic relationships can successfully reflect semantic and visual contexts. Experiments conducted on tagged images collected from Flickr show that our measures are more coherent to human cognition than the conventional measures that use either text or visual features, or the WordNet-based measures. In particular, a new measure of semantic relatedness, which satisfies the triangle inequality, obtains the best results among different distance measures in our framework. The applicability of our measures to multimedia-related tasks such as concept clustering, image annotation and tag recommendation is also shown in the experiments.
C1 [Katsurai, Marie; Ogawa, Takahiro; Haseyama, Miki] Hokkaido Univ, Grad Sch Informat Sci & Technol, Sapporo, Hokkaido 0600814, Japan.
C3 Hokkaido University
RP Katsurai, M (corresponding author), Hokkaido Univ, Grad Sch Informat Sci & Technol, Sapporo, Hokkaido 0600814, Japan.
EM marie.katsurai@gmail.com; ogawa@lmd.ist.hokudai.ac.jp;
   miki@ist.hokudai.ac.jp
RI Haseyama, Miki/A-6163-2012; Katsurai, Marie/ABF-1378-2021
OI Ogawa, Takahiro/0000-0001-5332-8112
FU Japan Society for the Promotion of Science (JSPS) [25 . 1688];
   Grants-in-Aid for Scientific Research [13J01688] Funding Source: KAKEN
FX This work was supported in part by the Japan Society for the Promotion
   of Science (JSPS) Grant-in-Aid for JSPS Fellows (25 . 1688). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Chong-Wah Ngo.
CR Abou-Moustafa KarimT., 2012, Asian_Conference_on_Machine_Learning,_JMLR: Workshop_and_Conference_Proceedings, P1
   [Anonymous], 2008, P 17 INT C WORLD WID
   [Anonymous], P CIVR
   [Anonymous], 2010, ACM INT C MULTIMEDIA
   [Anonymous], 2012, P 21 ACM INT C INFOR
   [Anonymous], 2007, PROC INT C MULTIMEDI, DOI DOI 10.1145/1291233.1291447
   [Anonymous], 1997, P 10 RES COMPUTATION
   [Anonymous], P ACM INT C IM VID R
   [Anonymous], 2007, P 16 INT WORLD WID W, DOI DOI 10.1145/1242572.1242675
   [Anonymous], 2002, NETDRAW GRAPH VISUAL
   Bach Francis R., 2005, PROBABILISTIC INTERP
   Banerjee A, 2005, J MACH LEARN RES, V6, P1705
   Bannour H, 2012, LECT NOTES COMPUT SC, V7131, P4
   Biemann C., 2004, P INT C LANG RES EV, P967
   Bo Chen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P773, DOI 10.1109/ICDM.2010.72
   Chakraborti S, 2007, LECT NOTES COMPUT SC, V4626, P61
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Cilibrasi RL, 2007, IEEE T KNOWL DATA EN, V19, P370, DOI 10.1109/TKDE.2007.48
   Davis J. V., 2007, P ADV NEUR INF PROC, P337
   Fan JP, 2008, IEEE T IMAGE PROCESS, V17, P407, DOI 10.1109/TIP.2008.916999
   Fan JP, 2012, IEEE T MULTIMEDIA, V14, P1414, DOI 10.1109/TMM.2012.2197604
   Finkelstein L, 2002, ACM T INFORM SYST, V20, P116, DOI 10.1145/503104.503110
   Golder SA, 2006, J INF SCI, V32, P198, DOI 10.1177/0165551506062337
   González-Castro V, 2013, INFORM SCIENCES, V218, P146, DOI 10.1016/j.ins.2012.05.028
   Hardoon DR, 2006, LECT NOTES ARTIF INT, V4093, P681
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hotho A, 2003, THIRD IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P541
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Katsurai M, 2013, INT CONF ACOUST SPEE, P3617, DOI 10.1109/ICASSP.2013.6638332
   Katsurai M, 2012, INT CONF ACOUST SPEE, P2373, DOI 10.1109/ICASSP.2012.6288392
   Kim M, 2012, PATTERN RECOGN, V45, P1050, DOI 10.1016/j.patcog.2011.08.026
   Leacock C, 1998, LANG SPEECH & COMMUN, P265
   Li Rui., 2007, WWW, P943
   Li X., 2011, P INT C MULT RETR
   Li X., 2012, P 2 ACM INT C MULT R, P4
   Li XR, 2009, INT CONF ACOUST SPEE, P3717, DOI 10.1109/ICASSP.2009.4960434
   Li YH, 2003, IEEE T KNOWL DATA EN, V15, P871, DOI 10.1109/TKDE.2003.1209005
   Lin YQ, 2011, PROC CVPR IEEE, P1689, DOI 10.1109/CVPR.2011.5995477
   Lipeng Ning, 2013, IEEE Signal Processing Letters, V20, P787, DOI 10.1109/LSP.2013.2266273
   Liu Dong., 2009, P 18 INT C WORLD WID, P351
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu YJ, 2010, IEEE T MULTIMEDIA, V12, P288, DOI 10.1109/TMM.2010.2046292
   MacKay D J C, 2002, Information Theory, Inference andLearning Algorithms
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Nakayama H., 2010, P BRIT MACH VIS C
   Pan J-Y, 2004, P 10 ACM SIGKDD INT, P653, DOI DOI 10.1145/1014052.1014135
   Patwardhan S., 2006, P EACL 2006 WORKSHOP, P1
   Peng X, 2005, PROCEEDINGS OF THE 2005 IEEE INTERNATIONAL CONFERENCE ON NATURAL LANGUAGE PROCESSING AND KNOWLEDGE ENGINEERING (IEEE NLP-KE'05), P111
   Pirró G, 2009, DATA KNOWL ENG, V68, P1289, DOI 10.1016/j.datak.2009.06.008
   PORTER MF, 1980, PROGRAM-AUTOM LIBR, V14, P130, DOI 10.1108/eb046814
   Radelaar J, 2011, LECT NOTES COMPUT SC, V6757, P274, DOI 10.1007/978-3-642-22233-7_19
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Resnik P, 1995, INT JOINT CONF ARTIF, P448
   Sanderson M, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P206, DOI 10.1145/312624.312679
   Schmitz Patrick., 2006, COLLABORATIVE WEB TA, P210
   Snoek CGM, 2007, IEEE T MULTIMEDIA, V9, P975, DOI 10.1109/TMM.2007.900156
   Tang JH, 2012, MULTIMED TOOLS APPL, V56, P1, DOI 10.1007/s11042-011-0822-1
   Tao JW, 2012, PATTERN RECOGN, V45, P3962, DOI 10.1016/j.patcog.2012.04.014
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   Wang CB, 2006, LECT NOTES COMPUT SC, V4035, P647, DOI 10.1145/1180639.1180774
   Wang M, 2012, NEUROCOMPUTING, V95, P3, DOI 10.1016/j.neucom.2011.03.059
   Wei XY, 2008, IEEE T MULTIMEDIA, V10, P1085, DOI 10.1109/TMM.2008.2001382
   Wei XY, 2011, IEEE T CIRC SYST VID, V21, P62, DOI 10.1109/TCSVT.2011.2105597
   Wen JR, 2002, ACM T INFORM SYST, V20, P59, DOI 10.1145/503104.503108
   Wu L, 2012, IEEE T PATTERN ANAL, V34, P863, DOI 10.1109/TPAMI.2011.195
   WU ZB, 1994, 32ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P133
   Yanai K., 2005, 13th Annual ACM International Conference on Multimedia, P419, DOI 10.1145/1101149.1101241
NR 67
TC 25
Z9 26
U1 1
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2014
VL 16
IS 4
BP 1059
EP 1074
DI 10.1109/TMM.2014.2306655
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AJ8LN
UT WOS:000337955800014
DA 2024-07-18
ER

PT J
AU Barnard, M
   Koniusz, P
   Wang, WW
   Kittler, J
   Naqvi, SM
   Chambers, J
AF Barnard, Mark
   Koniusz, Peter
   Wang, Wenwu
   Kittler, Josef
   Naqvi, Syed Mohsen
   Chambers, Jonathon
TI Robust Multi-Speaker Tracking via Dictionary Learning and Identity
   Modeling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visual Tracking; Particle Filters; Dictionary Learning
ID PARTICLE FILTER; FEATURES
AB We investigate the problem of visual tracking of multiple human speakers in an office environment. In particular, we propose novel solutions to the following challenges: (1) robust and computationally efficient modeling and classification of the changing appearance of the speakers in a variety of different lighting conditions and camera resolutions; (2) dealing with full or partial occlusions when multiple speakers cross or come into very close proximity; (3) automatic initialization of the trackers, or re-initialization when the trackers have lost lock caused by e. g. the limited camera views. First, we develop new algorithms for appearance modeling of the moving speakers based on dictionary learning (DL), using an off-line training process. In the tracking phase, the histograms (coding coefficients) of the image patches derived from the learned dictionaries are used to generate the likelihood functions based on Support Vector Machine (SVM) classification. This likelihood function is then used in the measurement step of the classical particle filtering (PF) algorithm. To improve the computational efficiency of generating the histograms, a soft voting technique based on approximate Locality-constrained Soft Assignment (LcSA) is proposed to reduce the number of dictionary atoms (codewords) used for histogram encoding. Second, an adaptive identity model is proposed to track multiple speakers whilst dealing with occlusions. This model is updated online using Maximum a Posteriori (MAP) adaptation, where we control the adaptation rate using the spatial relationship between the subjects. Third, to enable automatic initialization of the visual trackers, we exploit audio information, the Direction of Arrival (DOA) angle, derived from microphone array recordings. Such information provides, a priori, the number of speakers and constrains the search space for the speaker's faces. The proposed system is tested on a number of sequences from three publicly available and challenging data corpora (AV16.3, EPFL pedestrian data set and CLEAR) with up to five moving subjects.
C1 [Barnard, Mark; Koniusz, Peter; Wang, Wenwu; Kittler, Josef] Univ Surrey, Ctr Vis Speech & Signal Proc, Surrey GU2 7XH, England.
   [Naqvi, Syed Mohsen; Chambers, Jonathon] Loughborough Univ Technol, Adv Signal Proc Grp, Loughborough LE11 3TU, Leics, England.
C3 University of Surrey; Loughborough University
RP Barnard, M (corresponding author), Univ Surrey, Ctr Vis Speech & Signal Proc, Surrey GU2 7XH, England.
EM mark.barnard@surrey.ac.uk; peter.koniusz@inria.fr; w.wang@surrey.ac.uk;
   j.kittler@surrey.ac.uk; s.m.r.naqvi@lboro.ac.uk;
   j.a.chambers@lboro.ac.uk
RI wang, wenwu/HOF-4371-2023
OI Naqvi, Syed Mohsen/0000-0002-1547-0908
FU Engineering and Physical Sciences Research Council of the UK
   [EP/H050000/1]; EPSRC [EP/H049665/1, EP/H050000/1] Funding Source: UKRI
FX This work was supported by the Engineering and Physical Sciences
   Research Council of the UK (grant no. EP/H050000/1). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Sen-Ching Cheung.
CR [Anonymous], 2010, Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE, DOI [DOI 10.1109/CVPR.2010.5540018, 10.1109/CVPR.2010.5540018]
   [Anonymous], 2006, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2006.256
   [Anonymous], 1998, GENTLE TUTORIAL ALGO, DOI DOI 10.1080/0042098032000136147
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2009, P ADV NEUR INF PROC
   [Anonymous], DR DOBBS J SOFTW TOO
   [Anonymous], P IEEE INT C COMP VI
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Berclaz J, 2011, IEEE T PATTERN ANAL, V33, P1806, DOI 10.1109/TPAMI.2011.21
   Bernardin K., 2007, Proc. of the 15th international conference on Multimedia, P661
   Cai Q., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P68, DOI 10.1109/ICPR.1996.546796
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761
   Cristianini N., 2000, INTRO SUPPORT VECTOR
   Czyz J, 2007, IMAGE VISION COMPUT, V25, P1271, DOI 10.1016/j.imavis.2006.07.027
   DiBiase J. H., 2001, MICROPHONE ARRAYS SI
   Duda R. O., 1973, PATTERN CLASSIFICATI, V3
   Gao SH, 2010, PROC CVPR IEEE, P3555, DOI 10.1109/CVPR.2010.5539943
   Gatica-Perez D, 2007, IEEE T AUDIO SPEECH, V15, P601, DOI 10.1109/TASL.2006.881678
   Gauvain JL, 1994, IEEE T SPEECH AUDI P, V2, P291, DOI 10.1109/89.279278
   Jianhua Ye, 2010, Proceedings 2010 Sixth International Conference on Natural Computation (ICNC 2010), P3550, DOI 10.1109/ICNC.2010.5584200
   Khan M., 2009, INT J ADV SCI TECHNO, V12, P25
   Kong A., 1994, J AM STAT ASS, V89
   Koniusz P., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2413, DOI 10.1109/ICIP.2011.6116129
   Koniusz P, 2013, COMPUT VIS IMAGE UND, V117, P479, DOI 10.1016/j.cviu.2012.10.010
   Lathoud G, 2005, LECT NOTES COMPUT SC, V3361, P182
   Lathoud G, 2005, INT CONF ACOUST SPEE, P265
   Lathoud G., 2006, EURASIP J APPL SIG P, V2006, P169
   Li Y, 2008, IEEE T PATTERN ANAL, V30, P1728, DOI 10.1109/TPAMI.2008.73
   Liu BY, 2011, PROC CVPR IEEE, P1313, DOI 10.1109/CVPR.2011.5995730
   Liu T., 2004, INVESTIGATION PRACTI, P825
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MARIETHOZ J, 2002, P INT C SPOK LANG PR, P581
   Mikolajczyk Krystian, 2004, 2004 12th European Signal Processing Conference (EUSIPCO), P1729
   Naqvi SM, 2010, IEEE J-STSP, V4, P895, DOI 10.1109/JSTSP.2010.2057198
   Ng, 2007, ADV NEURAL INF PROCE, P801
   Ning JF, 2009, INT J PATTERN RECOGN, V23, P1245, DOI 10.1142/S0218001409007624
   Nummiaro K, 2003, IMAGE VISION COMPUT, V21, P99, DOI 10.1016/S0262-8856(02)00129-4
   Özuysal M, 2006, LECT NOTES COMPUT SC, V3953, P592, DOI 10.1007/11744078_46
   Pham NT, 2007, LECT NOTES COMPUT SC, V4843, P875
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647
   Shan C, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P669
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Tahir M. A., 2010, P INT C PATT REC
   Tahir M. A., 2009, P SUBSP WORKSH CONJ
   Tong Y, 2007, PATTERN RECOGN, V40, P3195, DOI 10.1016/j.patcog.2007.02.021
   van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang CX, 2008, ISCSCT 2008: INTERNATIONAL SYMPOSIUM ON COMPUTER SCIENCE AND COMPUTATIONAL TECHNOLOGY, VOL 2, PROCEEDINGS, P657, DOI 10.1109/ISCSCT.2008.206
   Wang JY, 2005, PROC CVPR IEEE, P1037
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Zhou HY, 2009, COMPUT VIS IMAGE UND, V113, P345, DOI 10.1016/j.cviu.2008.08.006
   Zhou X, 2010, LECT NOTES COMPUT SC, V6315, P141, DOI 10.1007/978-3-642-15555-0_11
   Zotkin DN, 2002, EURASIP J APPL SIG P, V2002, P1154, DOI 10.1155/S1110865702206058
NR 54
TC 27
Z9 27
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 864
EP 880
DI 10.1109/TMM.2014.2301977
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500024
DA 2024-07-18
ER

PT J
AU Yan, B
   Yuan, BH
   Yang, B
AF Yan, Bo
   Yuan, Binhang
   Yang, Bo
TI Effective Video Retargeting With Jittery Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Jittery artifact; spatial coherence; temporal coherence; video
   retargeting
ID FRAME CONCEALMENT
AB This paper presents an effective video retargeting method with the assessment of jittery artifact. Our method firstly constructs a new energy function by including both spatial and temporal constrains. Then the retargeting processing can be performed effectively by minimizing our proposed energy function for each frame. We also propose a new objective measurement to assess temporal coherence after video retargeting, the accuracy of which is verified by psycho-visual tests. Experimental results show that our video retargeting method is able to provide comfortable resized videos in terms of subjective and objective measurements.
C1 [Yan, Bo; Yuan, Binhang; Yang, Bo] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
C3 Fudan University
RP Yan, B (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
EM byan@fudan.edu.cn
RI Yan, Bo/AFQ-7025-2022
OI Yan, Bo/0000-0002-7775-1270; Yuan, Binhang/0000-0002-3188-2769
FU NSFC [61073067, 61370158]
FX This work was supported by NSFC (Grant No.: 61073067 and 61370158). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Shahram Shirani.
CR [Anonymous], P ACM INT C MULT
   [Anonymous], 2007, 2007 IEEE 11 INT C C, DOI DOI 10.1109/ICCV.2007.4409010
   ASTOLA J, 1990, P IEEE, V78, P678, DOI 10.1109/5.54807
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Chiang CK, 2009, IEEE T CIRC SYST VID, V19, P1588, DOI 10.1109/TCSVT.2009.2031462
   Ding YY, 2011, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2011.5995445
   Grundmann M, 2010, PROC CVPR IEEE, P569, DOI 10.1109/CVPR.2010.5540165
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Krähenbühl P, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1616452.1618472, 10.1145/1618452.1618472]
   Panozzo D, 2012, COMPUT GRAPH FORUM, V31, P229, DOI 10.1111/j.1467-8659.2012.03001.x
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Shamir Ariel, 2009, ACM SIG-GRAPH ASIA 2009 Courses, P11
   Sun KR, 2012, IEEE IMAGE PROC, P2105, DOI 10.1109/ICIP.2012.6467307
   Wu HS, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866185
   Yan B, 2012, IEEE T MULTIMEDIA, V14, P936, DOI 10.1109/TMM.2012.2184743
   Yan B, 2013, IEEE T CIRC SYST VID, V23, P313, DOI 10.1109/TCSVT.2012.2203740
   Yan B, 2010, IEEE T IMAGE PROCESS, V19, P98, DOI 10.1109/TIP.2009.2032311
   Yen TC, 2011, IEEE T IMAGE PROCESS, V20, P2339, DOI 10.1109/TIP.2011.2114357
   Yuan Z, 2012, IEEE T CIRC SYST VID, V22, P890, DOI 10.1109/TCSVT.2011.2181230
NR 19
TC 14
Z9 14
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 272
EP 277
DI 10.1109/TMM.2013.2286112
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100023
DA 2024-07-18
ER

PT J
AU Huang, YS
   Mao, SW
AF Huang, Yingsong
   Mao, Shiwen
TI Downlink Power Control for Multi-User VBR Video Streaming in Cellular
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-layer optimization; downlink power control; variable-bit-rate
   video; video streaming
ID LONG-RANGE DEPENDENCE; TRAFFIC MODEL; ALLOCATION
AB We investigate the problem of downlink power control for streaming multiple variable bit rate (VBR) videos in a multicell wireless network, where downlink capacities are limited by inter-cell interference. We adopt a deterministic model for VBR video traffic that considers video frame sizes and playout buffers at the mobile users. The problem is to find the optimal transmit powers for the base stations, such that VBR video data can be delivered to mobile users without causing playout buffer underflow or overflow. We formulate a nonlinear nonconvex optimization problem and prove the condition for the existence of feasible solutions. A centralized branch-and-bound algorithm is then developed, which incorporates the Reformulation-Linearization Technique and can produce (1 - is an element of)- ptimal solutions. We also propose a low-complexity distributed algorithm with fast convergence as an alternative to the centralized algorithm. Through simulations with VBR video traces under fading channels, we find the distributed algorithm can achieve a performance very close to that of the centralized algorithm.
C1 [Huang, Yingsong; Mao, Shiwen] Auburn Univ, Dept Elect & Comp Engn, Auburn, AL 36849 USA.
C3 Auburn University System; Auburn University
RP Huang, YS (corresponding author), Auburn Univ, Dept Elect & Comp Engn, Auburn, AL 36849 USA.
EM yzh0002@tigermail.auburn.edu; smao@ieee.org
RI Mao, Shiwen/AAY-4471-2020
FU Division Of Computer and Network Systems; Direct For Computer & Info
   Scie & Enginr [1266036] Funding Source: National Science Foundation;
   Division Of Computer and Network Systems; Direct For Computer & Info
   Scie & Enginr [0953513] Funding Source: National Science Foundation
CR Andersin M., 1995, Sixth IEEE International Symposium on Personal, Indoor and Mobile Radio Communications. PIMRC'95. Wireless: Merging onto the Information Superhighway (Cat. No.95TH8135), P56, DOI 10.1109/PIMRC.1995.476403
   BAMBOS ND, 1995, IEEE INFOCOM SER, P97, DOI 10.1109/INFCOM.1995.515865
   BERAN J, 1995, IEEE T COMMUN, V43, P1566, DOI 10.1109/26.380206
   Chatziperis S, 2008, IEEE T MOBILE COMPUT, V7, P95, DOI 10.1109/TMC.2007.70706
   Chen MH, 2006, IEEE T MULTIMEDIA, V8, P1045, DOI 10.1109/TMM.2006.879837
   Chiang M, 2005, IEEE J SEL AREA COMM, V23, P104, DOI 10.1109/JSAC.2004.837347
   Dai M, 2009, IEEE T MULTIMEDIA, V11, P1010, DOI 10.1109/TMM.2009.2021802
   De Rango F, 2008, IEEE T BROADCAST, V54, P612, DOI 10.1109/TBC.2008.2002716
   Garret M. W., 1994, Computer Communication Review, V24, P269, DOI 10.1145/190809.190339
   Gharavol EA, 2006, IEEE INT CONF FUZZY, P2142, DOI 10.1109/FUZZY.2006.1681997
   Gjendemsjo A, 2008, IEEE T WIREL COMMUN, V7, P3164, DOI 10.1109/TWC.2008.070227
   Grandhi S.A., 1995, Wireless Personal Commun, V1, P257, DOI DOI 10.1007/BF01098870
   Heyman DP, 1996, IEEE ACM T NETWORK, V4, P301, DOI 10.1109/90.502230
   Hu D., 2010, IEEE J SEL AREAS COM, V28
   Hu DL, 2012, IEEE J SEL AREA COMM, V30, P641, DOI 10.1109/JSAC.2012.120413
   Huang YS, 2012, COMPUT COMMUN, V35, P1828, DOI 10.1016/j.comcom.2012.01.020
   Huang YS, 2011, IEEE INFOCOM SER, P2561, DOI 10.1109/INFCOM.2011.5935081
   Kang S, 2010, IEEE T SIGNAL PROCES, V58, P1219, DOI 10.1109/TSP.2009.2035983
   Kompella S, 2009, IEEE ACM T NETWORK, V17, P212, DOI 10.1109/TNET.2008.925942
   Lee JW, 2005, IEEE ACM T NETWORK, V13, P854, DOI 10.1109/TNET.2005.852888
   Liang GF, 2007, IEEE INFOCOM SER, P1406, DOI 10.1109/INFCOM.2007.166
   Liew SC, 1997, IEEE J SEL AREA COMM, V15, P1181, DOI 10.1109/49.611167
   McManus JM, 1997, P SOC PHOTO-OPT INS, V3231, P140, DOI 10.1117/12.290405
   Mitliagkas I, 2011, IEEE T WIREL COMMUN, V10, P4110, DOI 10.1109/TWC.2011.100811.101381
   Rasti M, 2011, IEEE T COMMUN, V59, P833, DOI 10.1109/TCOMM.2011.122110.090711
   Reisslein M., Video trace library
   Salehi JD, 1998, IEEE ACM T NETWORK, V6, P397, DOI [10.1109/90.720873, 10.1142/S0218213097000219]
   Sen S, 2002, IEEE J SEL AREA COMM, V20, P1345, DOI 10.1109/JSAC.2002.802063
   Stockhammer T, 2004, IEEE T MULTIMEDIA, V6, P268, DOI 10.1109/TMM.2003.822795
   Tanwir S., IEEE COMMUN SURVEYS, P1
NR 30
TC 11
Z9 12
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 2137
EP 2148
DI 10.1109/TMM.2013.2270457
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900033
DA 2024-07-18
ER

PT J
AU Yen, KK
   Liao, YC
   Chen, CL
   Zao, JK
   Chang, HC
AF Yen, Kuo-Kuang
   Liao, Yen-Chin
   Chen, Chih-Lung
   Zao, John K.
   Chang, Hsie-Chia
TI Integrating Non-Repetitive LT Encoders With Modified Distribution to
   Achieve Unequal Erasure Protection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE BP decoding; LT code; degree; non-repetitive encoding
ID MULTICAST
AB The performance of LT code is highly related to the code length. A decoder is more likely to deplete degree-1 encoding symbols and terminate during early stage when the code length is short. In this work, we modify the robust Soliton distribution (RSD) and increase the degree-1 proportion. More degree-1 encoding symbols can be generated to relieve early decoding termination. The proportion of low degrees, except for degree-1, is also reduced. Therefore, receivers collect less encoding symbols carrying redundant information. In addition, Non-Repetitive (NR) encoding scheme is proposed to avoid producing repeated degree-1 encoding symbols. To improve video transmission quality, previous studies redesign LT codes to provide Unequal Error Protection (UEP) for different Scalable Video Coding (SVC) layers. Unlike those studies to modify the code structure, we integrate multiple NR encoders to achieve UEP ability. Experimental results show that our UEP scheme outperforms previous studies in terms of the PSNR.
C1 [Yen, Kuo-Kuang; Liao, Yen-Chin; Chen, Chih-Lung; Chang, Hsie-Chia] Natl Chiao Tung Univ, Dept Elect Engn, Hsinchu, Taiwan.
   [Yen, Kuo-Kuang; Liao, Yen-Chin; Chen, Chih-Lung; Chang, Hsie-Chia] Natl Chiao Tung Univ, Inst Elect, Hsinchu 30039, Taiwan.
   [Zao, John K.] Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu, Taiwan.
C3 National Yang Ming Chiao Tung University; National Yang Ming Chiao Tung
   University; National Yang Ming Chiao Tung University
RP Yen, KK (corresponding author), Natl Chiao Tung Univ, Dept Elect Engn, Hsinchu, Taiwan.
EM dannies@oasis.ee.nctu.edu.tw; ycliaoee92g@gmail.com; lung@si2lab.org;
   jkzao@cs.nctu.edu.tw; hcchang@mail.nctu.edu.tw
RI Chang, Hsie-Chia/B-8697-2009
CR Ahmad S, 2011, IEEE T MULTIMEDIA, V13, P92, DOI 10.1109/TMM.2010.2093511
   Byers J. W., 1998, Computer Communication Review, V28, P56, DOI 10.1145/285243.285258
   Cataldi P, 2010, IEEE T IMAGE PROCESS, V19, P1491, DOI 10.1109/TIP.2010.2042985
   de Fez I, 2012, IEEE T MULTIMEDIA, V14, P641, DOI 10.1109/TMM.2012.2190392
   Detti A., 2009, P IEEE MEDIAWIN 2009
   Gómez-Barquero D, 2009, IEEE T BROADCAST, V55, P396, DOI 10.1109/TBC.2008.2012024
   Jaspar X, 2007, P IEEE, V95, P1345, DOI 10.1109/JPROC.2007.896491
   Lee SK, 2011, IEEE T BROADCAST, V57, P319, DOI 10.1109/TBC.2011.2104690
   Luby M, 2002, ANN IEEE SYMP FOUND, P271, DOI 10.1109/SFCS.2002.1181950
   Luby M, 2007, IEEE T BROADCAST, V53, P235, DOI 10.1109/TBC.2007.891703
   Maatouk G, 2009, IEEE INT SYMP INFO, P2326, DOI 10.1109/ISIT.2009.5205942
   MacKay DJC, 2005, IEE P-COMMUN, V152, P1062, DOI 10.1049/ip-com:20050237
   Qian LM, 1999, IEEE DATA COMPR CONF, P414, DOI 10.1109/DCC.1999.755691
   Rahnavard N, 2007, IEEE T INFORM THEORY, V53, P1521, DOI 10.1109/TIT.2007.892814
   Raja N, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/49172
   Sejdinovic D, 2007, CONFERENCE RECORD OF THE FORTY-FIRST ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1-5, P1020
   Sejdinovic D, 2009, IEEE T WIREL COMMUN, V8, P5155, DOI 10.1109/TWC.2009.081076
   Shokrollahi A, 2006, IEEE T INFORM THEORY, V52, P2551, DOI 10.1109/TIT.2006.874390
   Wang H, 2008, IEEE INT SYMP CIRC S, P2062, DOI 10.1109/ISCAS.2008.4541854
NR 19
TC 10
Z9 10
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 2162
EP 2175
DI 10.1109/TMM.2013.2269898
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900035
DA 2024-07-18
ER

PT J
AU Zhang, W
   Cao, C
   Chen, SF
   Liu, JZ
   Tang, XO
AF Zhang, Wei
   Cao, Chen
   Chen, Shifeng
   Liu, Jianzhuang
   Tang, Xiaoou
TI Style Transfer Via Image Component Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Example-based stylization; non-photorealistic rendering; video
   stylization and personalization
ID TEXTURE
AB Example-based stylization provides an easy way of making artistic effects for images and videos. However, most existing methods do not consider the content and style separately. In this paper, we propose a style transfer algorithm via a novel component analysis approach, based on various image processing techniques. First, inspired by the steps of drawing a picture, an image is decomposed into three components: draft, paint and edge, which describe the content, main style, and strengthened strokes along the boundaries. Then the style is transferred from the template image to the source image in the paint and edge components. Style transfer is formulated as a global optimization problem by using Markov random fields, and a coarse-to-fine belief propagation algorithm is used to solve the optimization problem. To combine the draft component and the obtained style information, the final artistic result can be achieved via a reconstruction step. Compared to other algorithms, our method not only synthesizes the style, but also preserves the image content well. We also extend our algorithm from single image stylization to video personalization, by maintaining the temporal coherence and identifying faces in video sequences. The results indicate that our approach performs excellently in stylization and personalization for images and videos.
C1 [Zhang, Wei; Liu, Jianzhuang; Tang, Xiaoou] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Hong Kong, Peoples R China.
   [Cao, Chen; Tang, Xiaoou] Chinese Acad Sci, Shenzhen Inst Adv Technol, Beijing 100864, Peoples R China.
   [Chen, Shifeng] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen Key Lab Comp Vis & Pattern Recognit, Beijing 100864, Peoples R China.
   [Liu, Jianzhuang] Huawei Technol Co Ltd, Media Lab, Shenzhen 518129, Peoples R China.
C3 Chinese University of Hong Kong; Chinese Academy of Sciences; Shenzhen
   Institute of Advanced Technology, CAS; Chinese Academy of Sciences;
   Shenzhen Institute of Advanced Technology, CAS; Huawei Technologies
RP Zhang, W (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen Key Lab Comp Vis & Pattern Recognit, Beijing 100864, Peoples R China.
EM zw009@ie.cuhk.edu.hk; chen.cao@siat.ac.cn; shifeng.chen@siat.ac.cn;
   liu.jianzhuang@huawei.com; xtang@ie.cuhk.edu.hk
RI Zhang, Wayne/AAY-7082-2021; Zhang, Wayne/GXM-6869-2022; Zhang,
   Wayne/AAF-3407-2019
OI Zhang, Wayne/0000-0002-8415-1062; Zhang, Wayne/0000-0002-8415-1062; 
FU Guangdong Innovative Research Team Program [201001D0104648280]; Science,
   Industry, Trade, and Information Technology Commission of Shenzhen
   Municipality [60903117]; Science, Industry, Trade, and Information
   Technology Commission of Shenzhen Municipality, China [JC201005270357A]
FX This work was supported in part by Guangdong Innovative Research Team
   Program No. 201001D0104648280); Science, Industry, Trade, and
   Information Technology Commission of Shenzhen Municipality (60903117);
   and Science, Industry, Trade, and Information Technology Commission of
   Shenzhen Municipality, China (JC201005270357A). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Nicu Sebe.
CR [Anonymous], 2003, P GRAPH
   [Anonymous], 2004, P EUR C COMP VIS
   Bae SM, 2006, ACM T GRAPHIC, V25, P637, DOI 10.1145/1141911.1141935
   Bousseau A., 2007, P ACM C COMP GRAPH I
   Cao C., 2011, P ACM INT C MULT
   Chen S., 2008, P ACM INT C MULT
   Cheng L., 2008, P IEEE C COMP VIS PA
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Curtis C., 1997, P ACM C COMP GRAPH I
   DeCarlo D., 2002, P ACM C COMP GRAPH I
   DRORI I, 2003, P IEEE C COMP VIS PA
   Efros A.A., 2001, P ACM C COMP GRAPH I
   Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075
   Gooch B., 2001, Non-photorealistic rendering
   Hertzmann A, 2003, IEEE COMPUT GRAPH, V23, P70, DOI 10.1109/MCG.2003.1210867
   Hertzmann A., 2001, P ACM C COMP GRAPH I
   Huang T., 2007, P ACM INT C MULT
   Kwatra V., 2003, P ACM C COMP GRAPH I
   Ramanarayanan G, 2007, IEEE T VIS COMPUT GR, V13, P167, DOI 10.1109/TVCG.2007.4
   Resales R., 2003, P INT C COMP VIS
   TANG X, 2003, P INT C COMP VIS
   Wang B, 2004, IEEE T VIS COMPUT GR, V10, P266, DOI 10.1109/TVCG.2004.1272726
   Wang J., 2004, P ACM C COMP GRAPH I
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   XIAO J, 2006, P EUR C COMP VIS
   Xiao R., 2007, P INT C COMP VIS
   Zhang W., 2010, P EUR C COMP VIS
   Zhou K, 2005, IEEE T VIS COMPUT GR, V11, P519, DOI 10.1109/TVCG.2005.78
   ZHOU Y, 2003, P IEEE C COMP VIS PA
NR 29
TC 37
Z9 45
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2013
VL 15
IS 7
BP 1594
EP 1601
DI 10.1109/TMM.2013.2265675
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 236PQ
UT WOS:000325811800011
DA 2024-07-18
ER

PT J
AU Cong, FY
   Alluri, V
   Nandi, AK
   Toiviainen, P
   Fa, R
   Abu-Jamous, B
   Gong, LY
   Craenen, BGW
   Poikonen, H
   Huotilainen, M
   Ristaniemi, T
AF Cong, Fengyu
   Alluri, Vinoo
   Nandi, Asoke K.
   Toiviainen, Petri
   Fa, Rui
   Abu-Jamous, Basel
   Gong, Liyun
   Craenen, Bart G. W.
   Poikonen, Hanna
   Huotilainen, Minna
   Ristaniemi, Tapani
TI Linking Brain Responses to Naturalistic Music Through Analysis of
   Ongoing EEG and Stimulus Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Acoustical features; clustering; EEG; independent component analysis;
   natural continuous music; ongoing; oscillation
ID INDEPENDENT COMPONENT ANALYSIS; MISMATCH NEGATIVITY; BLIND SEPARATION;
   SYNCHRONIZATION; OSCILLATIONS; PROJECTION
AB This study proposes a novel approach for the analysis of brain responses in the modality of ongoing EEG elicited by the naturalistic and continuous music stimulus. The 512-second long EEG data (recorded with 64 electrodes) are first decomposed into 64 components by independent component analysis (ICA) for each participant. Then, the spatial maps showing dipolar brain activity are selected in terms of the residual dipole variance through a single dipole model in brain imaging, and clustered into a pre-defined number (estimated by the minimum description length) of clusters. Subsequently, the temporal courses of the EEG theta and alpha oscillations of each component for each cluster are produced and correlated with the temporal courses of tonal and rhythmic features of the music. Using this approach, we found that the extracted temporal courses of the theta and alpha oscillations along central and occipital area of scalp in two of the selected clusters significantly correlated with the musical features representing progressions in the rhythmic content of the stimulus. We suggest that this demonstrates that with the proposed approach, we have managed to discover what kinds of brain responses were elicited when a participant was listening continuously to the long piece of naturalistic music.
C1 [Cong, Fengyu; Alluri, Vinoo; Nandi, Asoke K.; Craenen, Bart G. W.; Ristaniemi, Tapani] Univ Jyvaskyla, Dept Math Informat Technol, SF-40351 Jyvaskyla, Finland.
   [Alluri, Vinoo; Toiviainen, Petri] Univ Jyvaskyla, Finnish Ctr Excellence Interdisciplinary Mus Res, Dept Mus, SF-40351 Jyvaskyla, Finland.
   [Nandi, Asoke K.; Fa, Rui; Abu-Jamous, Basel; Craenen, Bart G. W.] Brunel Univ, Dept Elect & Comp Engn, Uxbridge UB8 3PH, Middx, England.
   [Gong, Liyun] Univ Liverpool, Dept Elect Engn & Elect, Liverpool L69 3BX, Merseyside, England.
   [Poikonen, Hanna] Univ Helsinki, Cognit Brain Res Unit, Inst Behav Sci, FIN-00014 Helsinki, Finland.
   [Huotilainen, Minna] Finnish Inst Occupat Hlth, Brain Work Lab, Helsinki, Finland.
   [Huotilainen, Minna] Univ Helsinki, Cognit Brain Res Unit, Inst Behav, FIN-00014 Helsinki, Finland.
C3 University of Jyvaskyla; University of Jyvaskyla; Brunel University;
   University of Liverpool; University of Helsinki; Finnish Institute of
   Occupational Health; University of Helsinki
RP Cong, FY (corresponding author), Univ Jyvaskyla, Dept Math Informat Technol, SF-40351 Jyvaskyla, Finland.
EM FengyuCong@gmail.com; vinoo.al-luri@jyu.fi; asoke.nandi@brunel.ac.uk;
   petri.toiviainen@jyu.fi; Rui.Fa@brunel.ac.uk;
   Basel.AbuJamous@brunel.ac.uk; l.gong@liverpool.ac.uk;
   bcraenen@xs4all.nl; hapoikon@mappi.helsinki.fi;
   minna.huotilainen@helsinki.fi; tapani.ristaniemi@jyu.fi
OI Alluri, Vinoo/0000-0003-3689-1039; Cong, Fengyu/0000-0003-0058-2429;
   Gong, Liyun/0000-0002-7223-5904; Toiviainen, Petri/0000-0001-6962-2957;
   Fa, Rui/0000-0003-4588-7971; Huotilainen, Minna/0000-0002-7251-6984
FU TEKES (Finland) [40334/10]; TEKES's Finland Distinguished Professorship
FX This work was supported by TEKES (Finland) grant 40334/10 "Machine
   Learning for Future Music and Learning Technologies". The work of A. K.
   Nandi was supported by TEKES's Finland Distinguished Professorship. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Tom Eichele.
CR Abrams DA, 2008, J NEUROSCI, V28, P3958, DOI 10.1523/JNEUROSCI.0187-08.2008
   Aiken SJ, 2008, HEARING RES, V245, P35, DOI 10.1016/j.heares.2008.08.004
   Alluri V, 2012, NEUROIMAGE, V59, P3677, DOI 10.1016/j.neuroimage.2011.11.019
   Basar E, 2001, INT J PSYCHOPHYSIOL, V39, P91, DOI 10.1016/S0167-8760(00)00135-5
   BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129
   Berger H, 1929, ARCH PSYCHIAT NERVEN, V87, P527, DOI 10.1007/BF01797193
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Buzsaki G., 2006, Rhythms of the Brain, DOI DOI 10.1093/ACPROF:OSO/9780195301069.001.0001
   Castellanos NP, 2006, J NEUROSCI METH, V158, P300, DOI 10.1016/j.jneumeth.2006.05.033
   Chen Z, 2008, COGN NEURODYNAMICS, V2, P257, DOI 10.1007/s11571-008-9047-z
   Cichocki A., 2003, Adaptive Blind Signal and Image Processing
   Cong FY, 2013, J NEUROSCI METH, V212, P165, DOI 10.1016/j.jneumeth.2012.09.029
   Cong FY, 2012, J MED BIOL ENG, V32, P205, DOI 10.5405/jmbe.908
   Cong FY, 2011, COGN NEURODYNAMICS, V5, P343, DOI 10.1007/s11571-011-9161-1
   Cong FY, 2011, BIOMED SIGNAL PROCES, V6, P422, DOI 10.1016/j.bspc.2010.05.006
   Cong FY, 2011, J NEUROSCI METH, V201, P269, DOI 10.1016/j.jneumeth.2011.07.015
   Cong FY, 2011, BIOMED TECH, V56, P223, DOI 10.1515/BMT.2011.102
   Cong FY, 2010, INT J NEURAL SYST, V20, P279, DOI 10.1142/S0129065710002413
   Davis H, 1939, J NEUROPHYSIOL, V2, P500, DOI 10.1152/jn.1939.2.6.500
   De Vos M, 2012, NEUROIMAGE, V63, P1196, DOI 10.1016/j.neuroimage.2012.07.055
   Debener S, 2012, PSYCHOPHYSIOLOGY, V49, P1617, DOI 10.1111/j.1469-8986.2012.01471.x
   Delorme A, 2004, J NEUROSCI METH, V134, P9, DOI 10.1016/j.jneumeth.2003.10.009
   Delorme A, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0030135
   Elmer S., 2011, INT J PSYCHOPHYSIOL
   Groppe DM, 2011, PSYCHOPHYSIOLOGY, V48, P1711, DOI 10.1111/j.1469-8986.2011.01273.x
   Gunther W, 1991, Eur Neuropsychopharmacol, V1, P143, DOI 10.1016/0924-977X(91)90716-8
   Hasson U, 2004, SCIENCE, V303, P1634, DOI 10.1126/science.1089506
   Haynes JD, 2006, NAT REV NEUROSCI, V7, P523, DOI 10.1038/nrn1931
   Himberg J, 2004, NEUROIMAGE, V22, P1214, DOI 10.1016/j.neuroimage.2004.03.027
   Hyvärinen A, 2001, INDEPENDENT COMPONENT ANALYSIS: PRINCIPLES AND PRACTICE, P71
   Hyvärinen A, 2010, NEUROIMAGE, V49, P257, DOI 10.1016/j.neuroimage.2009.08.028
   Kaufman L., 2005, Finding groups in data: an introduction to cluster analysis
   Kauppi J.-P., 2010, FRONT NEUROINFORM, V4
   Kay KN, 2008, NATURE, V452, P352, DOI 10.1038/nature06713
   Koskinen M., 2012, Hum. Brain Mapp, DOI 10.1002/hbm.22004
   Lartillot O., 2007, P 10 INT C DIG AUD E, V237, P244, DOI DOI 10.1007/978-3-540-78246-9_31
   Luck S. J., 2005, INTRO EVENT RELATED, DOI DOI 10.1007/S10409-008-0217-3
   Makeig S, 1999, J NEUROSCI, V19, P2665
   Makeig S, 1997, P NATL ACAD SCI USA, V94, P10979, DOI 10.1073/pnas.94.20.10979
   Makeig S, 2009, INT J PSYCHOPHYSIOL, V73, P95, DOI 10.1016/j.ijpsycho.2008.11.008
   Niedermeyer E., 2004, ELECTROEN CLIN NEURO
   Nijholt A, 2010, HUM-COMPUT INT-SPRIN, P1, DOI 10.1007/978-1-84996-272-8
   Onton J, 2006, NEUROSCI BIOBEHAV R, V30, P808, DOI 10.1016/j.neubiorev.2006.06.007
   Pan H, 2011, BRAIN RES REV, V67, P226, DOI 10.1016/j.brainresrev.2011.02.004
   RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5
   Ruiz MH, 2009, HUM BRAIN MAPP, V30, P1207, DOI 10.1002/hbm.20584
   Sammler D, 2007, PSYCHOPHYSIOLOGY, V44, P293, DOI 10.1111/j.1469-8986.2007.00497.x
   Sanei S., 2007, EEG Signal Processing, DOI [10.1002/9780470511923, DOI 10.1002/9780470511923]
   Schaefer RS, 2011, INT J PSYCHOPHYSIOL, V82, P254, DOI 10.1016/j.ijpsycho.2011.09.007
   Shahin AJ, 2010, J NEUROPHYSIOL, V103, P218, DOI 10.1152/jn.00402.2009
   Spiers HJ, 2007, TRENDS COGN SCI, V11, P356, DOI 10.1016/j.tics.2007.06.002
   Vigario Ricardo, 2008, IEEE Rev Biomed Eng, V1, P50, DOI 10.1109/RBME.2008.2008244
   Wang YJ, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0037665
   Yener GG, 2010, COGN NEURODYNAMICS, V4, P263, DOI 10.1007/s11571-010-9138-5
NR 54
TC 49
Z9 52
U1 5
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2013
VL 15
IS 5
BP 1060
EP 1069
DI 10.1109/TMM.2013.2253452
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222WT
UT WOS:000324763600010
DA 2024-07-18
ER

PT J
AU Xiao, Y
   Wan, L
   Leung, CS
   Lai, YK
   Wong, TT
AF Xiao, Yi
   Wan, Liang
   Leung, Chi-Sing
   Lai, Yu-Kun
   Wong, Tien-Tsin
TI Example-Based Color Transfer for Gradient Meshes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gradient mesh; example-based color transfer; linear operator; PCA-based
   color transfer
ID IMAGE; COLORIZATION
AB Editing a photo-realistic gradient mesh is a tough task. Even only editing the colors of an existing gradient mesh can be exhaustive and time-consuming. To facilitate user-friendly color editing, we develop an example-based color transfer method for gradient meshes, which borrows the color characteristics of an example image to a gradient mesh. We start by exploiting the constraints of the gradient mesh, and accordingly propose a linear-operator-based color transfer framework. Our framework operates only on colors and color gradients of the mesh points and preserves the topological structure of the gradient mesh. Bearing the framework in mind, we build our approach on PCA-based color transfer. After relieving the color range problem, we incorporate a fusion-based optimization scheme to improve color similarity between the reference image and the recolored gradient mesh. Finally, a multi-swatch transfer scheme is provided to enable more user control. Our approach is simple, effective, and much faster than color transferring the rastered gradient mesh directly. The experimental results also show that our method can generate pleasing recolored gradient meshes.
C1 [Xiao, Yi; Leung, Chi-Sing] City Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
   [Wan, Liang] Tianjin Univ, Sch Comp Software, Tianjin 300072, Peoples R China.
   [Lai, Yu-Kun] Cardiff Univ, Cardiff CF10 3AT, S Glam, Wales.
   [Wong, Tien-Tsin] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
C3 City University of Hong Kong; Tianjin University; Cardiff University;
   Chinese University of Hong Kong
RP Wan, L (corresponding author), Tianjin Univ, Sch Comp Software, Tianjin 300072, Peoples R China.
EM yixiao1984@gmail.com; lwan@tju.edu.cn; eeleungc@cityu.edu.hk;
   yukun.lai@cs.cardiff.ac.uk; ttwong@cse.cuhk.edu.hk
RI Lai, Yu-Kun/D-2343-2010
OI LEUNG, Chi Sing Andrew/0000-0003-0962-6723
FU General Research Fund, Hong Kong [CityU 116511, CUHK 417411];
   Specialized Research Fund for the Doctoral Program of Higher Education,
   China [SRFDP 20110032120041]
FX This work was mainly supported by a research grant CityU 116511 (from
   General Research Fund, Hong Kong) and supported in part by a research
   grant SRFDP 20110032120041 (from Specialized Research Fund for the
   Doctoral Program of Higher Education, China), and a research grant CUHK
   417411 (from General Research Fund, Hong Kong). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Eckehard Steinbach.
CR Abadpour A, 2007, J VIS COMMUN IMAGE R, V18, P15, DOI 10.1016/j.jvcir.2006.08.001
   An XB, 2010, COMPUT GRAPH FORUM, V29, P263, DOI 10.1111/j.1467-8659.2009.01595.x
   [Anonymous], 2005, Computational Aesthetics in Graphics, Visualization and Imaging, DOI [DOI 10.2312/COMPAESTH/COMPAESTH05/111-122, 10.2312/COMPAESTH/COMPAESTH05/111-122]
   Chang YH, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P176, DOI 10.1109/CGI.2003.1214463
   Chang Y, 2007, IEEE T IMAGE PROCESS, V16, P329, DOI 10.1109/TIP.2006.888347
   Charpiat G, 2008, LECT NOTES COMPUT SC, V5304, P126, DOI 10.1007/978-3-540-88690-7_10
   Chen GH, 2006, IEEE IMAGE PROC, P2929, DOI 10.1109/ICIP.2006.313132
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   FERGUSON J, 1964, J ACM, V11, P221, DOI 10.1145/321217.321225
   Greenfield GR, 2003, WSCG'2003, VOL 11, NO 1, CONFERENCE PROCEEDINGS, P189
   GRUNDLAND M, 2004, P SOC PHOTO-OPT INS, V5667, P610
   Hasler D, 2003, P SOC PHOTO-OPT INS, V5007, P87, DOI 10.1117/12.477378
   Horiuchi T, 2005, THIRTEENTH COLOR IMAGING CONFERENCE, FINAL PROGRAM AND PROCEEDINGS, P245
   Ironi R., 2005, RENDERING TECHNIQUES, P201
   Ji Y, 2004, PROCEEDINGS OF THE 2004 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7, P4057
   Kotera H, 2005, IEEE IMAGE PROC, P2549
   Lai YK, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531391
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li J, 2008, PROCEEDINGS OF THE 27TH INTERNATIONAL CONFERENCE ON OFFSHORE MECHANICS AND ARCHTIC ENGINEERING - 2008, VOL 4, P835
   Luan Q., 2007, P EUR S REND REND TE
   Luan Q, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P465, DOI 10.1109/PG.2007.50
   Morimoto Yuji, 2009, SIGGRAPH 09 POSTERS, P32
   Pitié F, 2007, COMPUT VIS IMAGE UND, V107, P123, DOI 10.1016/j.cviu.2006.11.011
   Pouli T, 2011, COMPUT GRAPH-UK, V35, P67, DOI 10.1016/j.cag.2010.11.003
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Reinhard E, 2011, LECT NOTES COMPUT SC, V6626, P1, DOI 10.1007/978-3-642-20404-3_1
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Sun J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239462
   Tai YW, 2005, PROC CVPR IEEE, P747
   Wang BL, 2011, INT J PRECIS ENG MAN, V12, P1085, DOI 10.1007/s12541-011-0145-1
   Wang CM, 2004, J INF SCI ENG, V20, P1039
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Wen CL, 2008, COMPUT GRAPH FORUM, V27, P1765, DOI 10.1111/j.1467-8659.2008.01321.x
   Wolberg G, 1999, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P188, DOI 10.1109/CGI.1999.777953
   Xiang Y, 2009, PATTERN RECOGN LETT, V30, P682, DOI 10.1016/j.patrec.2009.01.004
   Xiao XZ, 2009, COMPUT GRAPH FORUM, V28, P1879, DOI 10.1111/j.1467-8659.2009.01566.x
   Xiao Xuezhong, 2006, PACM INT C VIRT REAL, P305, DOI DOI 10.1145/1128923.1128974
   Yatziv L, 2006, IEEE T IMAGE PROCESS, V15, P1120, DOI 10.1109/TIP.2005.864231
   Yi-Chin Huang, 2005, 13th Annual ACM International Conference on Multimedia, P351, DOI 10.1145/1101149.1101223
NR 39
TC 17
Z9 19
U1 1
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2013
VL 15
IS 3
BP 549
EP 560
DI 10.1109/TMM.2012.2233725
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109ZM
UT WOS:000316410900007
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Sartori, J
   Kumar, R
AF Sartori, John
   Kumar, Rakesh
TI Branch and Data Herding: Reducing Control and Memory Divergence for
   Error-Tolerant GPU Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Error tolerance; energy efficiency
AB Control and memory divergence between threads within the same execution bundle, or warp, have been shown to cause significant performance bottlenecks for GPU applications. In this paper, we exploit the observation that many GPU applications exhibit error tolerance to propose branch and data herding. Branch herding eliminates control divergence by forcing all threads in a warp to take the same control path. Data herding eliminates memory divergence by forcing each thread in a warp to load from the same memory block. To safely and efficiently support branch and data herding, we propose a static analysis and compiler framework to prevent exceptions when control and data errors are introduced, a profiling framework that aims to maximize performance while maintaining acceptable output quality, and hardware optimizations to improve the performance benefits of exploiting error tolerance through branch and data herding. Our software implementation of branch herding on NVIDIA GeForce GTX 480 improves performance by up to 34% (13%, on average) for a suite of NVIDIA CUDA SDK and Parboil benchmarks. Our hardware implementation of branch herding improves performance by up to 55% (30%, on average). Data herding improves performance by up to 32% (25%, on average). Observed output quality degradation is minimal for several applications that exhibit error tolerance, especially for visual computing applications.
C1 [Sartori, John; Kumar, Rakesh] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign
RP Sartori, J (corresponding author), Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
EM sartori2@illinois.edu; rakeshk@illinois.edu
RI Kumar, Rakesh/ABD-1065-2020
OI Kumar, Rakesh/0000-0001-7664-0803
CR [Anonymous], 2010, NVIDIA CUDA Programming Guide
   [Anonymous], 2010, GPGPU COMP HOR
   [Anonymous], 2009, NVIDIAS NEXT GEN CUD
   [Anonymous], PARB BENCHM SUIT
   [Anonymous], 2011, MAND SET
   [Anonymous], 2010, OPENCL
   Bakhoda A, 2009, INT SYM PERFORM ANAL, P163, DOI 10.1109/ISPASS.2009.4919648
   Cadambi Srihari, 2010, Proceedings of the 3rd Workshop on General-Purpose Computation on Graphics Processing Units, P86
   Che Shuai, 2011, P INT C HIGH PERFORM, P13, DOI 10.1145/2063384.2063401
   Coon B. W., 2008, Patent US, Patent No. 7353369
   Fung WWL, 2007, INT SYMP MICROARCH, P407, DOI 10.1109/MICRO.2007.30
   Fung WWL, 2011, INT S HIGH PERF COMP, P25, DOI 10.1109/HPCA.2011.5749714
   Meng J., 2009, INT PARALL DISTRIB P, P1
   Meng JY, 2010, CONF PROC INT SYMP C, P235, DOI 10.1145/1816038.1815992
   Narasiman V, 2011, INT SYMP MICROARCH, P308, DOI 10.1145/2155620.2155656
   NVIDIA, 2009, NVIDIA COMP PTX PAR
   Tarjan D., 2009, P C HIGH PERF COMP N, P22
   University of Illinois, CLANG A C LANG FAM F
   Varatkar GV, 2006, ISLPED '06: PROCEEDINGS OF THE 2006 INTERNATIONAL SYMPOSIUM ON LOW POWER ELECTRONICS AND DESIGN, P113, DOI 10.1109/LPE.2006.4271817
   Wang N, 2003, 12TH INTERNATIONAL CONFERENCE ON PARALLEL ARCHITECTURES AND COMPILATION TECHNIQUES, PROCEEDINGS, P56
   Wong H, 2010, INT SYM PERFORM ANAL, P235
   Yeh TY, 2007, INT SYMP MICROARCH, P394, DOI 10.1109/MICRO.2007.9
   Zhang EZ, 2011, ACM SIGPLAN NOTICES, V46, P369, DOI [10.1145/1961296.1950408, 10.1145/1961295.1950408]
NR 23
TC 36
Z9 48
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2013
VL 15
IS 2
BP 279
EP 290
DI 10.1109/TMM.2012.2232647
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 075HN
UT WOS:000313875500005
DA 2024-07-18
ER

PT J
AU Jiménez-Rodríguez, L
   Aulí-Llinàs, F
   Marcellin, MW
AF Jimenez-Rodriguez, Leandro
   Auli-Llinas, Francesc
   Marcellin, Michael W.
TI FAST Rate Allocation for JPEG2000 Video Transmission Over Time-Varying
   Channels
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE JPEG2000; rate allocation; time-varying channels; video transmission
ID BIT-RATE VIDEO; VBR VIDEO
AB This work introduces a rate allocation method for the transmission of pre-encoded JPEG2000 video over time-varying channels, which vary their capacity during video transmission due to network congestion, hardware failures, or router saturation. Such variations occur often in networks and are commonly unpredictable in practice. The optimization problem is posed for such networks and a rate allocation method is formulated to handle such variations. The main insight of the proposed method is to extend the complexity scalability features of the FAst rate allocation through STeepest descent (FAST) algorithm. Extensive experimental results suggest that the proposed transmission scheme achieves near-optimal performance while expending few computational resources.
C1 [Jimenez-Rodriguez, Leandro; Auli-Llinas, Francesc] Univ Autonoma Barcelona, Dept Informat & Commun Engn, Bellaterra 08193, Spain.
   [Marcellin, Michael W.] Univ Arizona, Dept Elect & Comp Engn, Tucson, AZ 85721 USA.
C3 Autonomous University of Barcelona; University of Arizona
RP Jiménez-Rodríguez, L (corresponding author), Univ Autonoma Barcelona, Dept Informat & Commun Engn, Bellaterra 08193, Spain.
EM ljimenez@deic.uab.es; fauli@deic.uab.es; marcellin@ece.arizona.edu
RI ; Auli-Llinas, Francesc/K-4395-2013
OI Jimenez-Rodriguez, Leandro/0009-0006-5255-9588; Auli-Llinas,
   Francesc/0000-0002-3208-9957
FU Universitat Autonoma de Barcelona; Spanish Government (MICINN); European
   Union; FEDER; Catalan Government [UAB-472-01-2/09, RYC-2010-05671,
   FP7-PEOPLE-2009-IIF-250420, TIN2009-14426-C02-01, 2009-SGR-1224]
FX This research was carried out when M. W. Marcellin was visiting
   professor and Marie Curie Fellow at the Department of Information and
   Communications Engineering, Universitat Autonoma de Barcelona. This work
   was supported in part by the Universitat Autonoma de Barcelona, by the
   Spanish Government (MICINN), by the European Union, by FEDER, and by the
   Catalan Government, under Grants UAB-472-01-2/09, RYC-2010-05671,
   FP7-PEOPLE-2009-IIF-250420, TIN2009-14426-C02-01, and 2009-SGR-1224. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Feng Wu.
CR [Anonymous], 2002, JPEG2000: Image Compression Fundamentals, Standards, and Practice
   Aulí-Llinàs F, 2011, IEEE T IMAGE PROCESS, V20, P1166, DOI 10.1109/TIP.2010.2077304
   Chen CT, 1993, IEEE T IMAGE PROCESS, V2, P50, DOI 10.1109/83.210865
   CHOI JH, 1994, IEEE T IMAGE PROCESS, V3, P546, DOI 10.1109/83.334986
   Dagher JC, 2003, IEEE T IMAGE PROCESS, V12, P1522, DOI 10.1109/TIP.2003.819228
   EVERETT H, 1963, OPER RES, V11, P399, DOI 10.1287/opre.11.3.399
   Hoang D. T., 1997, THESIS BROWN U PROVI
   Hsu CY, 1997, IEEE J SEL AREA COMM, V15, P1016, DOI 10.1109/49.611156
   Hsu CY, 1999, IEEE J SEL AREA COMM, V17, P756, DOI 10.1109/49.768193
   Huang KL, 2009, IEEE T IMAGE PROCESS, V18, P1004, DOI 10.1109/TIP.2009.2014259
   Jurca D, 2007, IEEE COMMUN MAG, V45, P108, DOI 10.1109/MCOM.2007.374427
   Kurose J.F., 2008, COMPUTER NETWORKING, VForth
   Lakshman TV, 1998, P IEEE, V86, P952, DOI 10.1109/5.664282
   Miao ZR, 2002, IEEE J SEL AREA COMM, V20, P1315, DOI 10.1109/JSAC.2002.802061
   Miao ZR, 2000, CONF REC ASILOMAR C, P1357, DOI 10.1109/ACSSC.2000.911213
   Ortega A, 1998, IEEE SIGNAL PROC MAG, V15, P23, DOI 10.1109/79.733495
   ORTEGA A, 1994, IEEE T IMAGE PROCESS, V3, P26, DOI 10.1109/83.265978
   Ostermann J., 2004, IEEE Circuits and Systems Magazine, V4, P7, DOI 10.1109/MCAS.2004.1286980
   Reibman AR, 1992, IEEE T CIRC SYST VID, V2, P361, DOI 10.1109/76.168912
   Rejaie R, 2000, IEEE J SEL AREA COMM, V18, P2530, DOI 10.1109/49.898735
   Sang-Yong Lee, 2002, Proceedings 2002 IEEE International Conference on Multimedia and Expo (Cat. No.02TH8604), P93, DOI 10.1109/ICME.2002.1035521
   Schuster GM, 1999, IEEE T MULTIMEDIA, V1, P3, DOI 10.1109/6046.748167
   Sermadevi Y, 2004, IEEE DATA COMPR CONF, P232
   Setton E., 2007, PEER PEER VIDEO STRE
   Wu SW, 1991, IEEE T CIRC SYST VID, V1, P100, DOI 10.1109/TCSVT.1991.4519809
NR 25
TC 6
Z9 6
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2013
VL 15
IS 1
BP 15
EP 26
DI 10.1109/TMM.2012.2199973
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 058PS
UT WOS:000312646600002
OA Green Published
DA 2024-07-18
ER

PT J
AU Liu, F
   Niu, YZ
   Jin, HL
AF Liu, Feng
   Niu, Yuzhen
   Jin, Hailin
TI Casual Stereoscopic Photo Authoring
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Stereoscopic photo authoring; stereoscopic photography; image
   rectification
ID RECTIFICATION; WARPS
AB Stereoscopic 3D displays become more and more popular these years. However, authoring high-quality stereoscopic 3D content remains challenging. In this paper, we present a method for easy stereoscopic photo authoring with a regular (monocular) camera. Our method takes two images or video frames using a monocular camera as input and transforms them into a stereoscopic image pair that provides a pleasant viewing experience. The key technique of our method is a perceptual-plausible image rectification algorithm that warps the input image pairs to meet the stereoscopic geometric constraint while avoiding noticeable visual distortion. Our method uses spatially-varying mesh-based image warps. Our warping method encodes a variety of constraints to best meet the stereoscopic geometric constraint and minimize visual distortion. Since each energy term is quadratic, our method eventually formulates the warping problem as a quadratic energy minimization which is solved efficiently using a sparse linear solver. Our method also allows both local and global adjustments of the disparities, an important property for adapting resulting stereoscopic images to different viewing conditions. Our experiments demonstrate that our spatially-varying warping technique can better support casual stereoscopic photo authoring than existing methods and our results and user study show that our method can effectively use casually-taken photos to create high-quality stereoscopic photos that deliver a pleasant 3D viewing experience.
C1 [Liu, Feng; Niu, Yuzhen] Portland State Univ, Dept Comp Sci, Portland, OR 97207 USA.
   [Jin, Hailin] Adobe Syst Inc, Adv Technol Labs, San Jose, CA 95110 USA.
C3 Portland State University; Adobe Systems Inc.
RP Liu, F (corresponding author), Portland State Univ, Dept Comp Sci, Portland, OR 97207 USA.
EM fliu@cs.pdx.edu; yuzhen@cs.pdx.edu; hljin@adobe.com
FU Portland State University Faculty Enhancement Grant
FX This work was supported by the Portland State University Faculty
   Enhancement Grant. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Xian-Sheng Hua.
CR [Anonymous], P BRIT MACH VIS C
   [Anonymous], ACM SIGGRAPH ASIA 20
   [Anonymous], 2007, 2007 IEEE 11 INT C C, DOI DOI 10.1109/ICCV.2007.4409010
   Ayache N., 1989, Rectification of images for binocular and trinocular stereovision, DOI 10.1109/ICPR.1988.28160
   AYACHE N, 1991, IEEE T PATTERN ANAL, V13, P73, DOI 10.1109/34.67633
   Carroll R, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778864
   Chang CH, 2011, IEEE T MULTIMEDIA, V13, P589, DOI 10.1109/TMM.2011.2116775
   Fusiello A, 2000, MACH VISION APPL, V12, P16, DOI 10.1007/s001380050120
   Fusiello A, 2008, INT C PATT RECOG, P1490
   Gluckman J, 2001, PROC CVPR IEEE, P111
   Guttmann M, 2009, IEEE I CONF COMP VIS, P136, DOI 10.1109/ICCV.2009.5459158
   Hartley R, 2000, MULTIPLE VIEW GEOMET
   Hartley RI, 1999, INT J COMPUT VISION, V35, P115, DOI 10.1023/A:1008115206617
   Heckbert P.S., 1989, Fundamentals of texture mapping and image warping
   Igarashi T, 2005, ACM T GRAPHIC, V24, P1134, DOI 10.1145/1073204.1073323
   Knorr S, 2007, IEEE IMAGE PROC, P3368
   Krähenbühl P, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1616452.1618472, 10.1145/1618452.1618472]
   Lang M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778812
   Liu F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531350
   Loop C., 1999, Comput. Vision Pattern Recognit, V1, P1125, DOI [10.1109/CVPR.1999.786928, DOI 10.1109/CVPR.1999.786928]
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mallon J, 2005, IMAGE VISION COMPUT, V23, P643, DOI 10.1016/j.imavis.2005.03.002
   Mendiburu Bernard., 2009, 3D Movie Making: Stereoscopic Digital Cinema From Scrip to Screen
   Moustakas K, 2005, IEEE T CIRC SYST VID, V15, P1065, DOI 10.1109/TCSVT.2005.852401
   Niu YZ, 2012, IEEE T MULTIMEDIA, V14, P783, DOI 10.1109/TMM.2012.2186122
   Oram D., 2001, BMVC, V1, P653
   Pollefeys M., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P496, DOI 10.1109/ICCV.1999.791262
   Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132
   Wang C., 2008, P SOC PHOTO-OPT INS, V6803, pE1
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Zhou J, 2008, J OPT SOC AM A, V25, P2721, DOI 10.1364/JOSAA.25.002721
NR 31
TC 11
Z9 11
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2013
VL 15
IS 1
BP 129
EP 140
DI 10.1109/TMM.2012.2225033
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 058PS
UT WOS:000312646600011
DA 2024-07-18
ER

PT J
AU Cheng, X
   Liu, JC
   Wang, HY
   Wang, CG
AF Cheng, Xu
   Liu, Jiangchuan
   Wang, Haiyang
   Wang, Chonggang
TI Coordinate Live Streaming and Storage Sharing for Social Media Content
   Distribution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Live streaming; social media; storage sharing
AB The recently emerged user-generated contents (UGC) services, social networking services (SNS), as well as the pervasive wireless mobile network services have formed social media which has drastically changed the content distribution landscape. Today such UGC applications as YouTube allow any user to be a content provider, generating enormous amount of video contents that are quickly and extensively propagated on the Internet through such SNSes as Facebook and Twitter.
   Unfortunately, the existing UGC sites are facing critical server bottlenecks and the surges created by the social networking users would make the situation even worse. To better understand the challenges and opportunities therein, we investigate users' social behavior and personal preference of online video sharing from both real-trace measurement study on a popular social networking website and a user questionnaire survey. Our data analysis reveals an interesting coexistence of live streaming and storage sharing, and that the users are generally more interested in watching their friend's videos. It further suggests that even though the traffic is significant, most users are willing to share their resources to assist others, implying user collaboration is a rational choice in this context.
   In this paper, we present Coordinated Live Streaming and Storage Sharing (COOLS), a system for efficient peer-to-peer posting of user-generated videos. Through a novel ID code design that embeds nodes' locations in an overlay, COOLS leverages stable storage users and yet inherently prioritizes living streaming flows. We also present the improvement of the basic overlay design. The evaluation results show that, as compared to other state-of-the-art solutions, COOLS successfully takes advantage of the coexistence of live streaming and storage sharing, providing better scalability, robustness, and streaming quality.
C1 [Cheng, Xu; Liu, Jiangchuan; Wang, Haiyang] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
   [Wang, Chonggang] InterDigital Inc, King Of Prussia, PA 19406 USA.
C3 Simon Fraser University; InterDigital
RP Cheng, X (corresponding author), Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
EM xuc@cs.sfu.ca; jcliu@cs.sfu.ca; hwa17@cs.sfu.ca; cgwang@ieee.org
FU Canada NSERC; NSERC; NSERC DAS; MI-TACS
FX This work was supported by a Canada NSERC Strategic Project Grant, an
   NSERC Discovery Grant, an NSERC DAS Grant, and an MI-TACS Project Grant.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Oscar Bonastre.
CR Albanesius Chloe., 2009, Inauguration: Twitter Reports 5 Times Normal Tweets Per Second
   Benevenuto Fabricio, 2009, P IMC
   Cha M., 2007, P IMC
   Cheng X., 2008, P IWQOS
   Cheng X., 2010, P NOSSDAV
   Gill P., 2007, P IMC
   Huang Y., 2008, P ACM SIGCOMM, P2
   Kaplan AM, 2010, BUS HORIZONS, V53, P59, DOI 10.1016/j.bushor.2009.09.003
   Kwak Haewoon, 2010, P WWW
   Liu JC, 2008, P IEEE, V96, P11, DOI 10.1109/JPROC.2007.909921
   MISLOVE A, 2007, P IMC
   Tartakoff Joseph., 2009, Twitter Search Fails Under Thursday's Celebrity News Rush
   TorrentFreak, 2007, BITTORRENT LAUNCH AD
   Venkataraman V., 2006, P IEEE ICNP
   Zhang X., 2005, P INFOCOM
NR 15
TC 17
Z9 18
U1 1
U2 67
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2012
VL 14
IS 6
BP 1558
EP 1565
DI 10.1109/TMM.2012.2217735
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046YE
UT WOS:000311800400006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lai, JH
   Chen, CL
   Wu, PC
   Kao, CC
   Hu, MC
   Chien, SY
AF Lai, Jui-Hsin
   Chen, Chieh-Li
   Wu, Po-Chen
   Kao, Chieh-Chi
   Hu, Min-Chun
   Chien, Shao-Yi
TI Tennis Real Play
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image-based rendering; sports game; tennis video; video rendering;
   video-based rendering; wiimote control
ID SPORTS VIDEO
AB Tennis Real Play (TRP) is an interactive tennis game system constructed with models extracted from videos of real matches. The key techniques proposed for TRP include player modeling and video-based player/court rendering. For player model creation, we propose the process for database normalization and the behavioral transition model of tennis players, which might be a good alternative for motion capture in the conventional video games. For player/court rendering, we propose the framework for rendering vivid game characters and providing the real-time ability. We can say that image-based rendering leads to a more interactive and realistic rendering. Experiments show that video games with vivid viewing effects and characteristic players can be generated from match videos without much user intervention. Because the player model can adequately record the ability and condition of a player in the real world, it can then be used to roughly predict the results of real tennis matches in the next days. The results of a user study reveal that subjects like the increased interaction, immersive experience, and enjoyment from playing TRP.
C1 [Lai, Jui-Hsin; Chen, Chieh-Li; Wu, Po-Chen; Kao, Chieh-Chi; Chien, Shao-Yi] Natl Taiwan Univ, Grad Inst Elect Engn, Media IC & Syst Lab, Taipei 106, Taiwan.
   [Lai, Jui-Hsin; Chen, Chieh-Li; Wu, Po-Chen; Kao, Chieh-Chi; Chien, Shao-Yi] Natl Taiwan Univ, Dept Elect Engn, Taipei 106, Taiwan.
   [Hu, Min-Chun] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 115, Taiwan.
C3 National Taiwan University; National Taiwan University; Academia Sinica
   - Taiwan
RP Lai, JH (corresponding author), Natl Taiwan Univ, Grad Inst Elect Engn, Media IC & Syst Lab, Taipei 106, Taiwan.
EM juihsin.lai@gmail.com; chiehli.chen@gmail.com; pcwu@media.ee.ntu.edu.tw;
   chiehchi.kao@gmail.com; trimy@citi.sinica.edu.tw;
   sychien@cc.ee.ntu.edu.tw
RI Hu, Min-Chun/AAX-1721-2020
OI Hu, Min-Chun/0000-0003-1917-2155; Chien, Shao-Yi/0000-0002-0634-6294
CR Chang C.C., 2009, LIBSVM LIB SUPPORT V
   Chang CH, 2010, J VIS COMMUN IMAGE R, V21, P595, DOI 10.1016/j.jvcir.2010.03.006
   Colqui G, 2008, 2008 3RD INTERNATIONAL SYMPOSIUM ON COMMUNICATIONS, CONTROL AND SIGNAL PROCESSING, VOLS 1-3, P936, DOI 10.1109/ISCCSP.2008.4537358
   Efros AA, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P726
   FLAGG M, 2009, P 2009 S INT 3D GRAP, P199
   Gao PS, 1998, VISUAL COMPUT, V14, P390, DOI 10.1007/s003710050150
   Han JG, 2008, IEEE T CIRC SYST VID, V18, P1628, DOI 10.1109/TCSVT.2008.2005611
   Han JG, 2011, IEEE MULTIMEDIA, V18, P72, DOI 10.1109/MMUL.2010.24
   Horry Y., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P225, DOI 10.1145/258734.258854
   HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073
   Igarashi T, 2005, ACM T GRAPHIC, V24, P1134, DOI 10.1145/1073204.1073323
   Inamoto N., 2004, Proceedings of ACM ACE, V74, P42
   Jui-Hsin Lai, 2008, 2008 IEEE 10th Workshop on Multimedia Signal Processing (MMSP), P672, DOI 10.1109/MMSP.2008.4665160
   Kalman R.E., 1960, J BASIC ENG-T ASME, V82, P35, DOI [DOI 10.1115/1.3662552, 10.1115/1.3662552]
   Kang HW, 2001, COMPUT GRAPH FORUM, V20, pC132
   Kenner C., 2007, GlovePIE
   Kitamura Y., 2008, P ACM S VIRT REAL SO, P30
   Lai J., 2011, Proceedings of the 19th ACM international conference on Multimedia (MM '11), V3, P483
   Lai J., 2011, P MICR C CHIN JPN JO, P1
   Lai JH, 2011, IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS (ICCE 2011), P275, DOI 10.1109/ICCE.2011.5722580
   Lai JH, 2011, J VIS COMMUN IMAGE R, V22, P271, DOI 10.1016/j.jvcir.2011.01.001
   Lai JH, 2009, IEEE INT CON MULTI, P1306, DOI 10.1109/ICME.2009.5202742
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mahajan D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531348
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Phillips P. M., 2003, P THEOR PRACT COMP G, P726
   Roh M.-C., PATTERN RECOGNIT, V41, P1124
   Schlmer T., 2008, Second International Conference on Tangible and Embedded Interaction, P11, DOI [DOI 10.1145/1347390.1347395, 10.1145/1347390.1347395]
   Schödl A, 2000, COMP GRAPH, P489, DOI 10.1145/344779.345012
   Schodl A., 2002, SCA 02 P 2002 ACM SI, P121, DOI DOI 10.1145/545261.545281
   Seitz S. M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P21, DOI 10.1145/237170.237196
   Wang J., 2004, P 12 ANN ACM INT C M, P32
   Wang JR, 2005, 11TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P102, DOI 10.1109/MMMC.2005.20
   Wang YT, 2009, 2009 SID INTERNATIONAL SYMPOSIUM DIGEST OF TECHNICAL PAPERS, VOL XL, BOOKS I - III, P1223
   WiiLi WiinDows and WiiBrew, 2009, WIIYOURS 2009
   Yu XG, 2009, COMPUT VIS IMAGE UND, V113, P643, DOI 10.1016/j.cviu.2008.01.006
   Zhu G., 2006, Proc. ACM Multimedia, P431, DOI [DOI 10.24963/IJCAI.2018/227, DOI 10.1145/1180639.1180728, 10.1145/1180639.1180728]
NR 39
TC 1
Z9 1
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2012
VL 14
IS 6
BP 1602
EP 1617
DI 10.1109/TMM.2012.2197190
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046YE
UT WOS:000311800400010
DA 2024-07-18
ER

PT J
AU Kim, MJ
   Kim, H
AF Kim, Myung Jong
   Kim, Hoirin
TI Audio-Based Objectionable Content Detection Using Discriminative
   Transforms of Time-Frequency Dynamics
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Discriminative transforms; histograms of oriented gradients;
   objectionable sound detection; segmental two-dimensional Mel-frequency
   cepstral coefficients
ID CLASSIFICATION
AB In this paper, the problem of detecting objectionable sounds, such as sexual screaming or moaning, to classify and block objectionable multimedia content is addressed. Objectionable sounds show distinctive characteristics, such as large temporal variations and fast spectral transitions, which are different from general audio signals, such as speech and music. To represent these characteristics, segment-based two-dimensional Mel-frequency cepstral coefficients and histograms of gradient directions are used as a feature set to characterize the time-frequency dynamics within a long-range segment of the target signal. After extracting the features, they are transformed to features with lower dimensions while preserving discriminative information using linear discriminant analysis based on a combination of global and local Fisher criteria. A Gaussian mixture model is adopted to statistically represent objectionable and non-objectionable sounds, and test sounds are classified by using a likelihood ratio test. Evaluation of the proposed feature extraction method on a database of several hundred objectionable and non-objectionable sound clips yielded precision/recall breakeven point of 91.25%, which is a promising performance which shows that the system can be applied to help an image-based approach to block such multimedia content.
C1 [Kim, Myung Jong; Kim, Hoirin] Korea Adv Inst Sci & Technol, Dept Elect Engn, Taejon 305701, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Kim, MJ (corresponding author), Korea Adv Inst Sci & Technol, Dept Elect Engn, Taejon 305701, South Korea.
EM myungjong@kaist.ac.kr; hrkim@ee.kaist.ac.kr
RI Kim, Hoi Rin/C-1753-2011
FU MEST/NRF [2010-0013288]
FX This work was supported by the MEST/NRF [2010-0013288, Research on
   multiple speech signal indexing robust to environmental distortion].
CR An KH, 2010, IEEE INT CONF ROBOT, P4803, DOI 10.1109/ROBOT.2010.5509146
   [Anonymous], 2006, The 3rd European Conference on Visual Media Production (CVMP 2006)-Part of the 2nd Multimedia Conference 2006, IET, DOI DOI 10.1049/CP:20061978
   Bosson A, 2002, LECT NOTES COMPUT SC, V2383, P50
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Duda R., 1973, Pattern Classification and Scene Analysis
   Eronen AJ, 2006, IEEE T AUDIO SPEECH, V14, P321, DOI 10.1109/TSA.2005.854103
   Joyce RA, 2008, IEEE INTERNET COMPUT, V12, P74, DOI 10.1109/MIC.2008.83
   Kim CY, 2008, 10TH INTERNATIONAL CONFERENCE ON ADVANCED COMMUNICATION TECHNOLOGY, VOLS I-III, P1435
   Kim Myung Jong, 2010, P ACM INT C MULT, P887
   Lee CH, 2008, IEEE T AUDIO SPEECH, V16, P1541, DOI 10.1109/TASL.2008.2005345
   Lim J., 2011, P 37 EUR ROT FOR AID, P1
   Lim J.D., 2009, P 10 INT C COMP COMM, P255
   Lu L, 2003, MULTIMEDIA SYST, V8, P482, DOI 10.1007/s00530-002-0065-0
   Myung Jong Kim, 2011, 2011 9th International Workshop on Content-Based Multimedia Indexing (CBMI), P205, DOI 10.1109/CBMI.2011.5972546
   Shi Z., 2009, P 2 INT C IM SIGN PR, P1
   Shi ZQ, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON NETWORK INFRASTRUCTURE AND DIGITAL CONTENT, PROCEEDINGS, P712, DOI 10.1109/ICNIDC.2009.5360844
   Siegler M.A., 1997, P DARPA SPEECH REC W
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   Xuan Huang, 2021, 2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech), P867, DOI 10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00144
NR 19
TC 10
Z9 11
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2012
VL 14
IS 5
BP 1390
EP 1400
DI 10.1109/TMM.2012.2195481
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 008XT
UT WOS:000308990600002
DA 2024-07-18
ER

PT J
AU Chen, L
   Xu, D
   Tsang, IW
   Luo, JB
AF Chen, Lin
   Xu, Dong
   Tsang, Ivor W.
   Luo, Jiebo
TI Tag-Based Image Retrieval Improved by Augmented Features and Group-Based
   Refinement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Group-based refinement; Laplacian regularized least squares (LapRLS);
   support vector machine (SVM) with augmented features (AFSVM); tag-based
   image retrieval
AB In this paper, we propose a new tag-based image retrieval framework to improve the retrieval performance of a group of related personal images capturedby the same user with in a short period of an event by leveraging millions of training web images and their associated rich textual descriptions. For any given query tag (e.g., "car"), the inverted file method is employed to automatically determine the relevant training web images that are associated with the query tag and the irrelevant training web images that are not associated with the query tag. Using these relevant and irrelevant web images as positive and negative training data respectively, we propose a new classification method called support vector machine (SVM) with augmented features (AFSVM) to learn an adapted classifier by leveraging the prelearned SVM classifiers of popular tags that are associated with a large number of relevant training web images. Treating the decision values of one group of test photos from AFSVM classifiers as the initial relevance scores, in the subsequent group-based refinement process, we propose to use the Laplacian regularized least squares method to further refine the relevance scores of test photos by utilizing the visual similarity of the images within the group. Based on the refined relevance scores, our proposed framework can be readily applied to tag-based image retrieval for a group of raw consumer photos without any textual descriptions or a group of Flickr photos with noisy tags. Moreover, we propose a new method to better calculate the relevance scores for Flickr photos. Extensive experiments on two datasets demonstrate the effectiveness of our framework.
C1 [Chen, Lin; Xu, Dong; Tsang, Ivor W.] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
C3 Nanyang Technological University; University of Rochester
RP Chen, L (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM chen0631@ntu.edu.sg; DongXu@ntu.edu.sg; IvorTsang@ntu.edu.sg;
   jluo@cs.rochester.edu
RI Tsang, Ivor W/E-8653-2011; Xu, Dong/A-3694-2011; Luo,
   Jiebo/AAI-7549-2020
OI Luo, Jiebo/0000-0002-4516-9729; Tsang, Ivor/0000-0001-8095-4637
FU Singapore National Research Foundation under its Interactive and Digital
   Media (IDM) Public Sector R&D Funding Initiative
FX This work was supported by the Singapore National Research Foundation
   under its Interactive and Digital Media (IDM) Public Sector R&D Funding
   Initiative and administered by the IDM Programme Office. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Daniel Gatica-Perez.
CR [Anonymous], 1998, P ADV NEURAL INFORM
   [Anonymous], 2003, ICML
   [Anonymous], 2009, P ACM INT C IM VID R
   [Anonymous], P INT C MACH LEARN
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Cao L., 2008, Proc. ACM Multimedia, P121
   Chang S.-F., 2007, P INT WORKSHOP WORKS, P255, DOI DOI 10.1145/1290082.1290118
   Chen L, 2010, PROC CVPR IEEE, P3440, DOI 10.1109/CVPR.2010.5539988
   Cristianini N, 2002, ADV NEUR IN, V14, P367
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Duan LX, 2010, PROC CVPR IEEE, P1959, DOI 10.1109/CVPR.2010.5539870
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Fellbaum C., 1998, WORDNET ELECT LEXICA, DOI DOI 10.7551/MITPRESS/7287.001.0001
   Grangier D, 2008, IEEE T PATTERN ANAL, V30, P1371, DOI 10.1109/TPAMI.2007.70791
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P309, DOI 10.1109/ICCV.2009.5459266
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li J, 2008, IEEE T PATTERN ANAL, V30, P985, DOI 10.1109/TPAMI.2007.70847
   Li XR, 2009, IEEE T MULTIMEDIA, V11, P1310, DOI 10.1109/TMM.2009.2030598
   Liu Dong., 2009, P 18 INT C WORLD WID, P351
   Liu YM, 2011, IEEE T PATTERN ANAL, V33, P1022, DOI 10.1109/TPAMI.2010.142
   Lu YJ, 2010, IEEE T MULTIMEDIA, V12, P288, DOI 10.1109/TMM.2010.2046292
   Torralba A., 2008, PROC IEEE INT C COMP, P1
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Wang C., 2008, ACM SIGIR, P355, DOI DOI 10.1145/1390334.1390396
   Wang L, 2007, 2007 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN SECURITY AND DEFENSE APPLICATIONS, P1
   Wang XJ, 2008, IEEE T PATTERN ANAL, V30, P1919, DOI 10.1109/TPAMI.2008.127
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
NR 28
TC 34
Z9 35
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
SI SI
BP 1057
EP 1067
DI 10.1109/TMM.2012.2187435
PN 1
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QL
UT WOS:000306599300011
DA 2024-07-18
ER

PT J
AU Ma, CX
   Liu, YJ
   Wang, HA
   Teng, DX
   Dai, GZ
AF Ma, Cui-Xia
   Liu, Yong-Jin
   Wang, Hong-An
   Teng, Dong-Xing
   Dai, Guo-Zhong
TI Sketch-Based Annotation and Visualization in Video Authoring
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Interaction styles; multimedia computing; sketch-based interface; video
   authoring
ID OF-THE-ART; RETRIEVAL; IMAGE
AB Authoring context-aware, interactive video representation is usually a complex process. A user-friendly multimedia authoring environment is thus solicited to explore and express users' design ideas efficiently and naturally. In this paper we present a sketch-based two-layer representation, called scene structure graph (SSG), to facilitate the video authoring process. One layer in SSG uses sketches as a concise form with which the visualization of scene information is easily understood and the other layer uses a graph to represent and edit the narrative structure in the authoring process. With SSG, the authoring process works in two stages. In the first stage, various sketch forms such as symbols and hand-drawing illustrations are used as basic primitives to annotate the video clips and the hyperlinks encoding spatio-temporal relations are established in SSG. In the second stage, sketches in SSGs are modified and new SSG is composed for any particular authoring purpose. Three user studies are elaborated, showing that the SSG is user-friendly and can achieve a good balance between expressiveness of users' intent and ease of use for authoring of interactive video.
C1 [Ma, Cui-Xia; Wang, Hong-An; Teng, Dong-Xing; Dai, Guo-Zhong] Chinese Acad Sci, Inst Software, Beijing, Peoples R China.
   [Liu, Yong-Jin] Tsinghua Univ, Dept Comp Sci & Technol, TNList, Beijing 100084, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Tsinghua
   University
RP Ma, CX (corresponding author), Chinese Acad Sci, Inst Software, Beijing, Peoples R China.
RI Liu, Yong/GWQ-6163-2022
FU National Basic Research Program of China [2011CB302205]; Natural Science
   Foundation of China [61173058, 60970099]; 863 program of China
   [2012AA011801]; Program for NCET; TNList Cross-discipline Foundation
FX This work was supported in part by the National Basic Research Program
   of China (2011CB302205), the Natural Science Foundation of China
   (61173058, 60970099), and the 863 program of China (2012AA011801). The
   work of Y.-J. Liu was supported in part by the Program for NCET and
   TNList Cross-discipline Foundation. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Samson Cheung.
CR Adobe Creative Team, 2008, ADOBE PREMIERE PRO C
   Adomavicius G, 2005, IEEE T KNOWL DATA EN, V17, P734, DOI 10.1109/TKDE.2005.99
   [Anonymous], 2002, Introduction to MPEG- 7: Multimedia content description interface
   [Anonymous], P 21 ANN ACM S US IN
   [Anonymous], 2002, METH SUBJ ASS QUAL T
   [Anonymous], P 2006 IEEE COMP SOC
   [Anonymous], 1998, CHI 98 Cconference Summary on Human Factors in Computing Systems, CHI '98
   Bulterman Dick., 2008, SYNCHRONIZED MULTIME, p3.0
   Bulterman DCA, 2005, ACM T MULTIM COMPUT, V1, P89, DOI 10.1145/1047936.1047943
   Chang SF, 1998, IEEE T CIRC SYST VID, V8, P602, DOI 10.1109/76.718507
   Chen BW, 2009, IEEE T MULTIMEDIA, V11, P295, DOI 10.1109/TMM.2008.2009703
   Collomosse JP, 2009, IEEE I CONF COMP VIS, P245, DOI 10.1109/ICCV.2009.5459258
   Csinger A., 1996, THESIS U BRIT COLUMB
   Feng SL, 2004, PROC CVPR IEEE, P1002
   Goel V., 1995, Sketches of thought
   Goldman DB, 2006, ACM T GRAPHIC, V25, P862, DOI 10.1145/1141911.1141967
   Guimaraes RL, 2010, DOCENG2010: PROCEEDINGS OF THE 2010 ACM SYMPOSIUM ON DOCUMENT ENGINEERING, P27
   Harada K., 1996, Proceedings ACM Multimedia 96, P341, DOI 10.1145/244130.244235
   Herman I, 2000, IEEE T VIS COMPUT GR, V6, P24, DOI 10.1109/2945.841119
   Hoenkamp E., 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P496, DOI 10.1145/1008992.1009088
   Hua X., 2005, P IEEE ICME
   KANG H, 2007, P ACM S NONPH AN REN, P43
   Kanizsa G., 1979, Organization in Vision: Essays on Gestalt Perception
   Laviola J. J., 2007, SIGGRAPH 2007 COURS, V3
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Lin J., 2000, CHI 2000 Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2000. The Future is Here, P510, DOI 10.1145/332040.332486
   Liu YJ, 2011, IEEE COMPUT GRAPH, V31, P49, DOI 10.1109/MCG.2009.147
   Liu YJ, 2011, IEEE T PATTERN ANAL, V33, P1502, DOI 10.1109/TPAMI.2010.221
   Liu YJ, 2010, IEEE T AUTOM SCI ENG, V7, P659, DOI 10.1109/TASE.2009.2039996
   Luo JB, 2009, IEEE T CIRC SYST VID, V19, P289, DOI 10.1109/TCSVT.2008.2009241
   Luo X., 2011, P 2011 AS C DES DIG
   Ma CX, 2011, IEEE T AUTOM SCI ENG, V8, P431, DOI 10.1109/TASE.2010.2086444
   Ma Y.-F., 2002, ACM MULTIMEDIA, P533
   Ma YF, 2005, 11TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P94
   Mei T, 2011, ACM T INFORM SYST, V29, DOI 10.1145/1961209.1961213
   Moxley E, 2010, IEEE T MULTIMEDIA, V12, P184, DOI 10.1109/TMM.2010.2041101
   Mynatt E. D., 1997, P CHI 99 HUM FACT CO, P45
   Shipman F, 2008, ACM T MULTIM COMPUT, V5, DOI 10.1145/1413862.1413868
   Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Sutherland I.E., 1963, P MAY 21 23 1963 SPR, P329, DOI DOI 10.1145/1461551.1461591
   Taniguchi Y, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P427
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   UEDA H, 1993, HUMAN FACTORS IN COMPUTING SYSTEMS, P137
   van Rossum G., 1993, Proceedings ACM Multimedia 93, P183, DOI 10.1145/166266.166287
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Xu CS, 2008, IEEE T MULTIMEDIA, V10, P421, DOI 10.1109/TMM.2008.917346
   Yeung MM, 1997, IEEE T CIRC SYST VID, V7, P771, DOI 10.1109/76.633496
NR 48
TC 20
Z9 22
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
BP 1153
EP 1165
DI 10.1109/TMM.2012.2190389
PN 2
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QM
UT WOS:000306599400004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Sang, JT
   Xu, CS
   Lu, DY
AF Sang, Jitao
   Xu, Changsheng
   Lu, Dongyuan
TI Learn to Personalized Image Search From the Photo Sharing Websites
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Personalized image search; social annotation; tensor factorization;
   topic model
ID TAG RECOMMENDATIONS
AB Increasingly developed social sharing websites like Flickr and Youtube allow users to create, share, annotate, and comment medias. The large-scale user-generated metadata not only facilitate users in sharing and organizing multimedia content, but provide useful information to improve media retrieval and management. Personalized search serves as one of such examples where the web search experience is improved by generating the returned list according to the modified user search intents. In this paper, we exploit the social annotations and propose a novel framework simultaneously considering the user and query relevance to learn to personalized image search. The basic premise is to embed the user preference and query-related search intent into user-specific topic spaces. Since the users' original annotation is too sparse for topic modeling, we need to enrich users' annotation pool before user-specific topic spaces construction. The proposed framework contains two components: 1) a ranking-based multicorrelation tensor factorization model is proposed to perform annotation prediction, which is considered as users' potential annotations for the images; 2) we introduce user-specific topic modeling to map the query relevance and user preference into the same user-specific topic space. For performance evaluation, two resources involved with users' social activities are employed. Experiments on a large-scale Flickr dataset demonstrate the effectiveness of the proposed method.
C1 [Sang, Jitao; Xu, Changsheng] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
   [Sang, Jitao; Xu, Changsheng] China Singapore Inst Digital Media, Singapore 119615, Singapore.
   [Lu, Dongyuan] Chinese Acad Sci, Inst Automat, State Key Lab Intelligent Control & Management Co, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; Institute of Automation, CAS
RP Sang, JT (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
EM jtsang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn; dongyuan.lu@ia.ac.cn
RI xu, cj/HJZ-3488-2023
FU National Program on Key Basic Research Project (973 Program)
   [2012CB316304]; National Natural Science Foundation of China [90920303,
   61003161]
FX This work was supported in part by the National Program on Key Basic
   Research Project (973 Program, Project 2012CB316304) and bye the
   National Natural Science Foundation of China under Grant 90920303 and
   Grant 61003161. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Jinhui Tang.
CR [Anonymous], 2008, Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, DOI [10.1145/1390334.1390363, DOI 10.1145/1390334.1390363]
   [Anonymous], 2011, PROC WWW
   [Anonymous], 2005, P 2005 ACM CIKM INT, DOI DOI 10.1145/1099554
   [Anonymous], 2009, P ACM INT C IM VID R
   [Anonymous], 2010, P ACM MULTIMEDIA
   [Anonymous], 2008, Proceedings of the 1st International Conference on Web Search and Data Mining, DOI [DOI 10.1145/1341531.1341558, 10.1145/1341531.1341558]
   Bao S., 2007, P 16 INT C WORLD WID, P501, DOI DOI 10.1145/1242572.1242640
   BERTSEKAS DP, 1999, NONLINEAR PROGRAMMIN, P2178
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Cai Y., 2010, P CIKM 10 TORONTO CA, P969
   Carman M.J., 2008, P 2008 ACM WORKSHOP, P27, DOI DOI 10.1145/1458583.1458591
   Carmel D., 2009, P CIKM, P1227
   Chirita Paul-Alexandru, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P7, DOI 10.1145/1277741.1277746
   Chirita P.-A., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P178, DOI 10.1145/1076034.1076067
   Dou Zhicheng, 2007, P WWW, P581, DOI DOI 10.1145/1242572.1242651
   Jäschke R, 2007, LECT NOTES ARTIF INT, V4702, P506
   Jäschke R, 2008, AI COMMUN, V21, P231, DOI 10.3233/AIC-2008-0438
   Kiers HAL, 2003, BRIT J MATH STAT PSY, V56, P119, DOI 10.1348/000711003321645386
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Lane ND, 2010, UBICOMP 2010: PROCEEDINGS OF THE 2010 ACM CONFERENCE ON UBIQUITOUS COMPUTING, P109
   Lerman K., 2007, J CORR, Vabs/0704.1676
   Li WJ, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1126
   Liu YA, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P500, DOI 10.1145/1571941.1572027
   Pitkow J, 2002, COMMUN ACM, V45, P50, DOI 10.1145/567498.567526
   Qiu F., 2006, Proceedings of the 15th International Conference on World Wide Web, P727, DOI DOI 10.1145/1135777.1135883
   Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127
   Smyth B, 2007, COMPUTER, V40, P42, DOI 10.1109/MC.2007.259
   Sugiyama Kazunari, 2004, WWW, P675, DOI DOI 10.1145/988672.988764
   Sun Jian-Tao., 2005, PROC WWW 05, P382, DOI DOI 10.1145/1060745.1060803
   Symeonidis P, 2010, IEEE T KNOWL DATA EN, V22, P179, DOI 10.1109/TKDE.2009.85
   Tang JH, 2010, IEEE T MULTIMEDIA, V12, P131, DOI 10.1109/TMM.2009.2037373
   Tang Jinhui., 2009, Proceedings of ACM international conference on Multimedia, P223, DOI DOI 10.1145/1631272.1631305
   Teevan J., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P449, DOI 10.1145/1076034.1076111
   TEEVAN J, 2008, P 31 ANN INT ACM SIG, P163, DOI [10.1145/1390334.1390364, DOI 10.1145/1390334.1390364]
   Teevan Jaime., 2009, P 2 ACM INT C WEB SE, P15, DOI [10.1145/1498759.1498786Mal, DOI 10.1145/1498759.1498786MAL]
   Teh Y., 2004, P ADV NEUR INF PROC, V17, P1
   Zhou D., 2008, Proceedings of the 17th International Conference on World Wide Web, WWW '08, P715, DOI DOI 10.1145/1367497.1367594
NR 37
TC 26
Z9 27
U1 0
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
SI SI
BP 963
EP 974
DI 10.1109/TMM.2011.2181344
PN 1
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QL
UT WOS:000306599300003
DA 2024-07-18
ER

PT J
AU Song, YC
   Zhang, YD
   Cao, J
   Xia, T
   Li, JT
AF Song, Yi-Cheng
   Zhang, Yong-Dong
   Cao, Juan
   Xia, Tian
   Li, Jin-Tao
TI Web Video Geolocation by Geotagged Social Resources
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Geolocation by geotagged relevant videos; geolocation by relevant
   geotagged images; label propagation; V2L; video content analysis; web
   video geolocation
AB This paper considers the problem of web video geolocation: we hope to determine where on the Earth a web video was taken. By analyzing a 6.5-million geotagged web video dataset, we observe that there exist inherent geography intimacies between a video with its relevant videos (related videos and same-author videos). This social relationship supplies a direct and effective cue to locate the video to a particular region on the earth. Based on this observation, we propose an effective web video geolocation algorithm by propagating geotags among the web video social relationship graph. For the video that have no geotagged relevant videos, we aim to collect those geotagged relevant images that are content similar with the video (share some visual or textual information with the video) as the cue to infer the location of the video. The experiments have demonstrated the effectiveness of both methods, with the geolocation accuracy much better than state-of-the-art approaches. Finally, an online web video geolocation system: Video2Locatoin (V2L) is developed to provide public access to our algorithm.
C1 [Song, Yi-Cheng; Zhang, Yong-Dong; Cao, Juan; Xia, Tian; Li, Jin-Tao] Chinese Acad Sci, Adv Comp Res Lab, Beijing Key Lab Mobile Comp & Pervas Device, Inst Comp Technol, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS
RP Song, YC (corresponding author), Chinese Acad Sci, Adv Comp Res Lab, Beijing Key Lab Mobile Comp & Pervas Device, Inst Comp Technol, Beijing 100190, Peoples R China.
EM zhyd@ict.ac.cn
RI XIA, Tian/A-5392-2015
FU Natural Science Foundation of Beijing [4112055]; National Nature Science
   Foundation of China [61172153, 60902090]; Beijing Municipal Education
   Commission; Beijing New Star Project on Science Technology [2007B071]
FX Manuscript received May 19, 2011; revised August 12, 2011; accepted
   October 06, 2011. Date of publication October 19, 2011; date of current
   version March 21, 2012. This work was supported in part by the Natural
   Science Foundation of Beijing under Grant 4112055, the National Nature
   Science Foundation of China under Grant 61172153 and Grant 60902090, the
   Co-building Program of Beijing Municipal Education Commission, Beijing
   New Star Project on Science & Technology, under Grant 2007B071. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Jiangchuan (JC) Liu.
CR Agarwal S, 2009, IEEE I CONF COMP VIS, P72, DOI 10.1109/ICCV.2009.5459148
   Amitay E., 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P273, DOI 10.1145/1008992.1009040
   [Anonymous], 2009, P 18 INT C WORLD WID
   [Anonymous], P CIVR
   [Anonymous], IEEE T MULT IN PRESS
   [Anonymous], THESIS CARNEGIE MELL
   [Anonymous], P MEDIAEVAL PIS IT O
   [Anonymous], 2007, MIR
   [Anonymous], 2009, ACM INT C IMAGE VIDE
   [Anonymous], 2005, SEMISUPERVISED LEARN
   [Anonymous], P ACM MULT
   [Anonymous], 2009, MCG WEBV BENCHMARK D
   [Anonymous], P MEDIAEVAL PIS IT O
   Avrithis Yannis, 2010, P 18 ACM INT C MULTI, P153, DOI 10.1145/1873951.1873973Place
   Backstrom L., 2010, WWW 2010 RAL NC, DOI [10.1145/1772690.1772698, DOI 10.1145/1772690.1772698]
   Backstrom Lars., 2008, P 17 INT C WORLD WID, P357, DOI DOI 10.1145/1367497.1367546
   Boutell M, 2005, PATTERN RECOGN, V38, P935, DOI 10.1016/j.patcog.2004.11.013
   Brockmann D, 2006, NATURE, V439, P462, DOI 10.1038/nature04292
   Cao L., 2008, Proc. ACM Multimedia, P121
   Cao L., 2009, P 17 ACM INT C MULTI, P125
   Cheng X, 2010, STUD COMPUT INTELL, V280, P367
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Friedland Gerald., 2010, P 18 ACM INT C MULTI, P1245, DOI DOI 10.1145/1873951.1874197
   Gao Y., 2010, Proceedings of Descriptional Complexity of Formal Systems 12th Workshop (DCFS 2010), P123
   Hays J, 2008, PROC CVPR IEEE, P3436
   Ji Rongrong., 2009, P 17 ASS COMP MACH I, P105, DOI DOI 10.1145/1631272.1631289
   Jin Xin, 2010, P 18 ACM INT C MULT, P1235, DOI [10.1145/1873951.1874196, DOI 10.1145/1873951.1874196]
   Joshi D., 2008, P INT C CONTENT BASE, P37
   Joshi D, 2011, SOCIAL MEDIA MODELING AND COMPUTING, P239, DOI 10.1007/978-0-85729-436-4_11
   Joshi D, 2010, INT CONF ACOUST SPEE, P5598, DOI 10.1109/ICASSP.2010.5495247
   Kalogerakis E, 2009, IEEE I CONF COMP VIS, P253, DOI 10.1109/ICCV.2009.5459259
   Kennedy L.S., 2008, P ACM INT C WORLD WI, P297
   Leung D, 2010, PROC CVPR IEEE, P2955, DOI 10.1109/CVPR.2010.5540040
   Li YP, 2009, IEEE I CONF COMP VIS, P1957, DOI 10.1109/ICCV.2009.5459432
   Luo JB, 2011, MULTIMED TOOLS APPL, V51, P187, DOI 10.1007/s11042-010-0623-y
   Muir JA, 2009, ACM COMPUT SURV, V42, DOI 10.1145/1592451.1592455
   Sahr K., 2003, CARTOGR GEOGR INF SC, V30, P121, DOI [DOI 10.1559/152304003100011090, 10.1559/152304003100011090]
   Santos R.L. T., 2007, CHARACTERIZING YOUTU
   Serdyukov P, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P484, DOI 10.1145/1571941.1572025
   Zhao WL, 2010, IEEE T MULTIMEDIA, V12, P448, DOI 10.1109/TMM.2010.2050651
   Zheng YT, 2009, PROC CVPR IEEE, P1085, DOI 10.1109/CVPRW.2009.5206749
NR 41
TC 12
Z9 15
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2012
VL 14
IS 2
BP 456
EP 470
DI 10.1109/TMM.2011.2172937
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OZ
UT WOS:000302702500018
DA 2024-07-18
ER

PT J
AU Wu, GL
   Wu, TH
   Chien, SY
AF Wu, Guan-Lin
   Wu, Tung-Hsing
   Chien, Shao-Yi
TI Algorithm and Architecture Design of Perception Engine for Video Coding
   Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Architecture; hardware; human visual system; perceptual; video coding
ID RECOGNITION PROCESSOR; MOTION; MODEL; COMMUNICATION; COMPRESSION;
   ATTENTION; CONTRAST; GOPS
AB In image and video coding field, an effective compression algorithm should remove not only the spatial, temporal, and statistical redundancy but also the perceptual redundancy information from the pictures. Many perceptual models are presented in the literature to cooperate with video coding system to obtain significant bit rate reduction without perceptual distortion. One of the critical issues for those perceptual models is their high computational complexity to apply to real-time applications. To alleviate this problem, this paper aims at hardware architecture design of perception engine for video coding applications. The adopted perceptual models include the structural similarity model, visual attention models, and just-noticeable-distortion model, and contrast sensitivity function. Moreover, those models are further developed and modified to be suitable for hardware implementation. Macroblock-based processing with data reuse scheme is used to save the system bandwidth. The architecture of parallel processing for each visual model with sharing the on-chip memory and buffers is developed to reduce the chip area. Subjective experiment results show that the adopted model achieves about 7%-41% bit-rate saving in the QP range of 24-36 without visual quality degradation. For the hardware implementation of the perception engine, the chip is taped out using 0.18 mu m technology. The chip size is about 3.3 x 3.3 mm(2), and the power consumption is 83.9 mW. The processing capability is HDTV720p.
C1 [Wu, Guan-Lin; Wu, Tung-Hsing; Chien, Shao-Yi] Natl Taiwan Univ, Grad Inst Elect Engn, Taipei 106, Taiwan.
   [Wu, Guan-Lin; Wu, Tung-Hsing; Chien, Shao-Yi] Natl Taiwan Univ, Dept Elect Engn, Taipei 106, Taiwan.
C3 National Taiwan University; National Taiwan University
RP Wu, GL (corresponding author), Natl Taiwan Univ, Grad Inst Elect Engn, Taipei 106, Taiwan.
EM sychien@cc.ee.ntu.edu.tw
OI Chien, Shao-Yi/0000-0002-0634-6294
FU National Science Council [NSC100-2221-E-002-090]; Chip Implementation
   Center (CIC)
FX Manuscript received October 26, 2010; revised March 04, 2011 and June
   23, 2011; accepted August 12, 2011. Date of publication August 30, 2011;
   date of current version November 18, 2011. This work was supported in
   part by the National Science Council under Grant NSC100-2221-E-002-090.
   The EDA tools are supported by Chip Implementation Center (CIC). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Yap-Peng Tan.
CR Abbo AA, 2008, IEEE J SOLID-ST CIRC, V43, P192, DOI 10.1109/JSSC.2007.909328
   Ahumada A. J.  Jr., 1992, Proceedings of the SPIE - The International Society for Optical Engineering, V1666, P365, DOI 10.1117/12.135982
   [Anonymous], 2009, N7779 MPEG
   [Anonymous], 2020, INT TELECOMMUNICATIO
   [Anonymous], 2004, N6383 MPEG
   Chen TC, 2007, 2007 Symposium on VLSI Circuits, Digest of Technical Papers, P222, DOI 10.1109/VLSIC.2007.4342727
   Chou CH, 1996, IEEE T CIRC SYST VID, V6, P143, DOI 10.1109/76.488822
   Daly S. J., 1998, P SOC PHOTO-OPT INS, V3299, P162
   Höntsch I, 2002, IEEE T IMAGE PROCESS, V11, P213, DOI 10.1109/83.988955
   Hsu RL, 2002, IEEE T PATTERN ANAL, V24, P696, DOI 10.1109/34.1000242
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   J. V. Team, 2003, 1449610 ISOIEC JV TE
   Kakumanu P, 2007, PATTERN RECOGN, V40, P1106, DOI 10.1016/j.patcog.2006.06.010
   KELLY DH, 1979, J OPT SOC AM, V69, P1340, DOI 10.1364/JOSA.69.001340
   Kim JY, 2010, IEEE J SOLID-ST CIRC, V45, P32, DOI 10.1109/JSSC.2009.2031768
   Koene AR, 2007, J VISION, V7, DOI 10.1167/7.7.6
   Kyo S, 2003, IEEE J SOLID-ST CIRC, V38, P1992, DOI 10.1109/JSSC.2003.818128
   Lee S, 2001, IEEE T IMAGE PROCESS, V10, P977, DOI 10.1109/83.931092
   Lin C. P., 2006, P IEEE INT SOL STAT, P412
   Liu Y, 2007, IEEE T CIRC SYST VID, V17, P68, DOI 10.1109/TCSVT.2006.887081
   Lu ZK, 2005, IEEE T IMAGE PROCESS, V14, P1928, DOI 10.1109/TIP.2005.854478
   Nothdurft HC, 2000, VISION RES, V40, P1183, DOI 10.1016/S0042-6989(00)00031-6
   ROBSON JG, 1966, J OPT SOC AM, V56, P1141, DOI 10.1364/JOSA.56.001141
   Rohaly AM, 2000, P SOC PHOTO-OPT INS, V4067, P742, DOI 10.1117/12.386632
   Safranek R. J., 1989, ICASSP-89: 1989 International Conference on Acoustics, Speech and Signal Processing (IEEE Cat. No.89CH2673-2), P1945, DOI 10.1109/ICASSP.1989.266837
   Tang CW, 2007, IEEE T MULTIMEDIA, V9, P231, DOI 10.1109/TMM.2006.886328
   Teixeira L, 2009, LECT NOTES COMPUT SC, V5630, P248, DOI 10.1007/978-3-642-02472-6_27
   Treue S, 1999, NATURE, V399, P575, DOI 10.1038/21176
   Tsai AC, 2008, IEEE T CIRC SYST VID, V18, P975, DOI 10.1109/TCSVT.2008.920742
   Wang S., 2010, P SPIE VISUAL COMMUN, V7744
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   WATSON AB, 1993, P SOC PHOTO-OPT INS, V1913, P202, DOI 10.1117/12.152694
   Wei ZY, 2009, IEEE T CIRC SYST VID, V19, P337, DOI 10.1109/TCSVT.2009.2013518
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wolfe JM, 2004, NAT REV NEUROSCI, V5, P495, DOI 10.1038/nrn1411
   Wu H.R., 2005, DIGITAL VIDEO IMAGE
   Yang XK, 2005, SIGNAL PROCESS-IMAGE, V20, P662, DOI 10.1016/j.image.2005.04.001
   Yang XK, 2005, IEEE T CIRC SYST VID, V15, P742, DOI 10.1109/TCSVT.2005.848313
   Yang XK, 2005, IEEE T CIRC SYST VID, V15, P496, DOI 10.1109/TCSVT.2005.844458
   2008, H 264 AVC REFERENCE
NR 41
TC 6
Z9 10
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2011
VL 13
IS 6
BP 1181
EP 1194
DI 10.1109/TMM.2011.2166249
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 852GD
UT WOS:000297343400001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Guo, TT
   Foh, CH
   Cai, JF
   Niyato, D
   Wong, EWM
AF Guo, Tiantian
   Foh, Chuan Heng
   Cai, Jianfei
   Niyato, Dusit
   Wong, Eric W. M.
TI Performance Evaluation of IPTV Over Wireless Home Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE IEEE 80211e wireless home networks; Internet Protocol Television (IPTV);
   performance evaluation
AB The emergence of Internet Protocol Television (IPTV) has brought potentials to revolutionize personal entertainment. Streaming TV content over the highly pervasive wireless networks allows easy access to personalized entertainment. Focusing on wireless home entertainment which is one of the main driving forces of IPTV development, we develop a Markovian framework that investigates several important issues related to network capacity and streaming quality in an IEEE 802.11e enabled wireless home network. The Markovian framework captures not only the IEEE 802.11e MAC protocol performance, but also the statistical characteristics of IPTV media streams. The inclusion of these two key descriptions allows our model to be practically used in wireless home network planning and design. To deal with the complexity in the model, we apply the efficient Matrix Geometric approach to obtain numerical results. We further perform simulations with real IPTV traffic to not only validate our analytical results, but also obtain further insight to the performance.
C1 [Guo, Tiantian; Foh, Chuan Heng; Cai, Jianfei; Niyato, Dusit] Nanyang Technol Univ, Sch Comp Engn, Ctr Multimedia & Network Technol, Singapore, Singapore.
   [Cai, Jianfei; Niyato, Dusit] Nanyang Technol Univ, Sch Comp Engn, Comp Commun Div, Singapore, Singapore.
   [Wong, Eric W. M.] City Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Nanyang Technological University; Nanyang Technological University; City
   University of Hong Kong
RP Guo, TT (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Ctr Multimedia & Network Technol, Singapore, Singapore.
EM guo_tiantian3002@pmail.ntu.edu.sg; aschfoh@ntu.edu.sg;
   asjfcai@ntu.edu.sg; dniyato@ntu.edu.sg; eeewong@cityu.edu.hk
RI Niyato, Dusit/A-3698-2011; Foh, Chuan H/A-3693-2011; Niyato,
   Dusit/Y-2769-2019; Cai, Jianfei/A-3691-2011
OI WONG, Wing Ming Eric/0000-0002-1641-6903; Foh, Chuan
   Heng/0000-0002-5716-1396; Niyato, Dusit/0000-0002-7442-7416; Cai,
   Jianfei/0000-0002-9444-3763
CR Ahmad K, 2009, IEEE COMMUN MAG, V47, P68, DOI 10.1109/MCOM.2009.5350371
   [Anonymous], 2005, 80211E IEEE, V802, P11
   [Anonymous], 80211B IEEE
   [Anonymous], 2009, IEEE Standard for Local and metropolitan area networks- Part 16: Air Inter- face for Broadband Wireless Access Systems Amendment 3: Advanced Air Interface
   [Anonymous], NS 2 NETWORK SIMULAT
   Bianchi G, 2000, IEEE J SEL AREA COMM, V18, P535, DOI 10.1109/49.840210
   Djama I, 2007, IEEE T BROADCAST, V53, P382, DOI 10.1109/TBC.2006.889111
   DU Q, 2009, P INFOCOM, P477
   Foh CH, 2007, IEEE T WIREL COMMUN, V6, P1276, DOI 10.1109/TWC.2007.04371
   Gerhardt I, 2009, INFORMS J COMPUT, V21, P630, DOI 10.1287/ijoc.1080.0316
   Hu J, 2008, IEEE T MULTIMEDIA, V10, P1465, DOI 10.1109/TMM.2008.2007329
   *IEEE, 2007, 80211G IEEE
   Kei CH, 2008, J INF SCI ENG, V24, P425
   Li D, 2010, IEEE T WIREL COMMUN, V9, P338, DOI 10.1109/TWC.2010.01.090556
   Mangold S., 2002, EUROPEAN WIRELESS, V18, P32
   Nasser N, 2009, IEEE T MULTIMEDIA, V11, P786, DOI 10.1109/TMM.2009.2017612
   Neuts MF., 1981, Matrix-Geometric Solutions in Stochastic Models
   REICHEL J, 2007, JSVM 9 8 SOFTWARE, V9212
   SHIHAB E, 2007, P IEEE GLOB NOV DEC, P5341
   Shihab E, 2008, IEEE NETWORK, V22, P52, DOI 10.1109/MNET.2008.4435903
   Symes P., 2004, DIGITAL VIDEO COMPRE
   TANTRA J, 2005, P IEEE INT C COMM, V5
   Xiao Y, 2007, IEEE COMMUN MAG, V45, P126, DOI 10.1109/MCOM.2007.4378332
NR 23
TC 5
Z9 5
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 1116
EP 1126
DI 10.1109/TMM.2011.2150208
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300023
DA 2024-07-18
ER

PT J
AU Zhou, L
   Chen, HH
AF Zhou, Liang
   Chen, Hsiao-Hwa
TI On Distributed Multimedia Scheduling With Constrained Control Channels
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Constrained control channels; control gain; distributed scheduling;
   multimedia communication
ID WIRELESS NETWORKS; COMMUNICATION CONSTRAINTS; TRANSMISSION; DELAY
AB Traditional multimedia scheduling approaches assumed perfect control channels where each node has access to the knowledge of its neighbors. However, in practice the control channels are always constrained and nodes can only exchange limited information with their neighbors. In this paper, we investigate how imperfect neighbor information affects the multimedia scheduling. First, we formulate the optimal multimedia scheduling problem with the constraints on network information. Specifically, a constrained factor is introduced to capture the profile of control channels. Then, we consider two cases of the constrained factor distribution: 1) the class with finite mean and variance, and 2) a general class that does not employ any parametric representation. In each case, we investigate the relationship between the control gain and scheduling performance based on available network and multimedia information. We show that the control gain can be chosen properly such that the optimal distributed multimedia scheduling can be achieved with an exponential convergence rate. In addition, an explicit equation for asymptotic convergence rate is derived for each case. Finally, we use computer simulations to verify the analytical results.
C1 [Zhou, Liang] Nanjing Univ Posts & Telecommun, Minist Educ, Key Lab Broadband Wireless Commun & Sensor Networ, Nanjing 210003, Peoples R China.
   [Chen, Hsiao-Hwa] Natl Cheng Kung Univ, Dept Engn Sci, Tainan 70101, Taiwan.
C3 Nanjing University of Posts & Telecommunications; National Cheng Kung
   University
RP Zhou, L (corresponding author), Nanjing Univ Posts & Telecommun, Minist Educ, Key Lab Broadband Wireless Commun & Sensor Networ, Nanjing 210003, Peoples R China.
EM liang.zhou@ieee.org; hshwchen@mail.ncku.edu.tw
FU Taiwan National Science Council [NSC99-2221-E-006-016-MY3]
FX Manuscript received December 12, 2010; revised May 02, 2011; accepted
   June 19, 2011. Date of publication June 27, 2011; date of current
   version September 16, 2011. This work was supported in part by a Taiwan
   National Science Council Grant (No. NSC99-2221-E-006-016-MY3). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Jiangchuan (JC) Liu.
CR Chakareski J, 2008, IEEE T MULTIMEDIA, V10, P858, DOI 10.1109/TMM.2008.921846
   Chen L., P IEEE INFOCOM 2006
   Cloosterman MBG, 2009, IEEE T AUTOMAT CONTR, V54, P1575, DOI 10.1109/TAC.2009.2015543
   Cui SS, 2010, IEEE T INFORM THEORY, V57, P3793, DOI 10.1109/TIT.2010.2051470
   Dai JG, 1995, ANN APPL PROBAB, V5, P49, DOI 10.1214/aoap/1177004828
   Draves R., P ACM MOBICOM 2004
   Fu FW, 2010, IEEE J SEL AREA COMM, V28, P308, DOI 10.1109/JSAC.2010.100403
   Gao HJ, 2008, AUTOMATICA, V44, P39, DOI 10.1016/j.automatica.2007.04.020
   Hamdaoui B, 2009, WIREL NETW, V15, P875, DOI 10.1007/s11276-007-0080-3
   Heemels WPMH, 2010, IEEE T AUTOMAT CONTR, V55, P1781, DOI 10.1109/TAC.2010.2042352
   Huang YS, 2009, IEEE T MULTIMEDIA, V11, P1072, DOI 10.1109/TMM.2009.2026085
   JURCA D, P IEEE ICME 2007
   Kim J, 2010, IEEE ACM T NETWORK, V18, P515, DOI 10.1109/TNET.2009.2032294
   Kokiopoulou E, 2009, IEEE T SIGNAL PROCES, V57, P342, DOI 10.1109/TSP.2008.2006147
   Kolokoltsov VN., 1997, IDEMPOTENT ANAL ITS
   LI T, IEEE T AUTO IN PRESS
   Li Y, 2009, IEEE T MULTIMEDIA, V11, P1182, DOI 10.1109/TMM.2009.2026102
   Melnyk MA, 2007, IEEE T MULTIMEDIA, V9, P869, DOI 10.1109/TMM.2007.895680
   Oh BJ, 2009, IEEE T MULTIMEDIA, V11, P1052, DOI 10.1109/TMM.2009.2026083
   SHAH D, P ACM SIGMETRICS 201
   Shiang HP, 2010, IEEE T CIRC SYST VID, V20, P505, DOI 10.1109/TCSVT.2009.2035837
   Stolyar AL, 2008, QUEUEING SYST, V59, P1, DOI 10.1007/s11134-008-9072-y
   Tatikonda S, 2004, IEEE T AUTOMAT CONTR, V49, P1056, DOI 10.1109/TAC.2004.831187
   Tu W, 2009, IEEE T CIRC SYST VID, V19, P151, DOI 10.1109/TCSVT.2008.2009240
   van der Schaar M, 2007, IEEE T MULTIMEDIA, V9, P185, DOI 10.1109/TMM.2006.886384
   Wang F, 2010, IEEE T PARALL DISTR, V21, P379, DOI 10.1109/TPDS.2009.77
   Wu DL, 2007, IEEE J SEL AREA COMM, V25, P841, DOI 10.1109/JSAC.2007.070519
   Zhang Q, 2005, P IEEE, V93, P123, DOI 10.1109/JPROC.2004.839603
   Zhou L, 2010, IEEE J SEL AREA COMM, V28, P409, DOI 10.1109/JSAC.2010.100412
   Zhou L, 2009, IEEE COMMUN LETT, V13, P534, DOI 10.1109/LCOMM.2009.090020
   Zhu XQ, 2009, IEEE T MULTIMEDIA, V11, P752, DOI 10.1109/TMM.2009.2017641
NR 31
TC 21
Z9 23
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 1040
EP 1051
DI 10.1109/TMM.2011.2160716
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300017
DA 2024-07-18
ER

PT J
AU Huang, J
   Yang, XK
   Fang, XZ
   Lin, WY
   Zhang, R
AF Huang, Jun
   Yang, Xiaokang
   Fang, Xiangzhong
   Lin, Weiyao
   Zhang, Rui
TI Integrating Visual Saliency and Consistency for Re-Ranking Image Search
   Results
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Random walk; re-ranking; visual consistency; visual saliency
ID ATTENTION
AB In this paper, we propose a new algorithm for image re-ranking in web image search applications. The proposed method focuses on investigating the following two mechanisms: 1) Visual consistency. In most web image search cases, the images that closely related to the search query are visually similar. These visually consistent images which occur most frequently in the first few web pages will be given higher ranks. 2) Visual saliency. From visual aspect, it is obvious that salient images would be easier to catch users' eyes, and it is observed that these visually salient images in the front pages are often relevant to the user's query. By integrating the above two mechanisms, our method can efficiently re-rank the images from search engines and obtain a more satisfactory search result. Experimental results on a real-world web image dataset demonstrate that our approach can effectively improve the performance of image retrieval.
C1 [Huang, Jun; Yang, Xiaokang; Fang, Xiangzhong; Lin, Weiyao; Zhang, Rui] Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai Key Lab Digital Media Proc & Transmiss, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Huang, J (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai Key Lab Digital Media Proc & Transmiss, Shanghai 200240, Peoples R China.
EM jun.huang@sjtu.edu.cn; xkyang@sjtu.edu.cn; xzfang@sjtu.edu.cn;
   wylin@sjtu.edu.cn; zhang_rui@sjtu.edu.cn
RI Yang, Xiaokang/C-6137-2009; lin, yuxi/HKF-6212-2023; zhang,
   ruigang/H-7317-2014
OI Yang, Xiaokang/0000-0003-4029-3322; Lin, Weiyao/0000-0001-8307-7107
FU NSFC [61025005, 60828001, 61001146, 61071155]; 973 Program
   [2010CB731401, 2010CB731406]; 111 Project [B07022]
FX This paper was supported in part by NSFC (61025005, 60828001, 61001146,
   61071155), 973 Program (2010CB731401, 2010CB731406), and the 111 Project
   (B07022). The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Jia Li.
CR [Anonymous], P 8 EUR C COMP VIS E
   [Anonymous], P SPIE STOR RETR IM
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], P INT C IM PROC
   [Anonymous], 2011, ACM T INTEL SYST TEC
   [Anonymous], 2004, P IEEE COMP SOC C CO
   [Anonymous], P 17 ACM INT C MULT
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], P WWW APR
   [Anonymous], P ACM MULT
   [Anonymous], P CVPR WORKSH SLAM
   Bamidele A, 2004, BT TECHNOL J, V22, P151, DOI 10.1023/B:BTTJ.0000047129.83260.79
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   BURT PJ, 1981, COMPUT VISION GRAPH, V16, P20, DOI 10.1016/0146-664X(81)90092-7
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Fergus R., 2005, P IEEE INT C COMP VI
   Fu H, 2006, PATTERN RECOGN, V39, P1604, DOI 10.1016/j.patcog.2005.12.015
   Hsu WH, 2007, IEEE MULTIMEDIA, V14, P14, DOI 10.1109/MMUL.2007.61
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jain AK, 1996, PATTERN RECOGN, V29, P1233, DOI 10.1016/0031-3203(95)00160-3
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Sun YR, 2003, ARTIF INTELL, V146, P77, DOI 10.1016/S0004-3702(02)00399-5
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
NR 24
TC 49
Z9 51
U1 1
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2011
VL 13
IS 4
BP 653
EP 661
DI 10.1109/TMM.2011.2127463
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 805EX
UT WOS:000293710300006
DA 2024-07-18
ER

PT J
AU van de Sande, KEA
   Gevers, T
   Snoek, CGM
AF van de Sande, Koen E. A.
   Gevers, Theo
   Snoek, Cees G. M.
TI Empowering Visual Categorization With the GPU
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bag-of-words; computational efficiency; General-Purpose computation on
   Graphics Processing Units (GPGPU); image classification; image/video
   retrieval; multi-core processing; parallel processing; support vector
   machines
ID FEATURES; KERNEL
AB Visual categorization is important to manage large collections of digital images and video, where textual metadata is often incomplete or simply unavailable. The bag-of-words model has become the most powerful method for visual categorization of images and video. Despite its high accuracy, a severe drawback of this model is its high computational cost. As the trend to increase computational power in newer CPU and GPU architectures is to increase their level of parallelism, exploiting this parallelism becomes an important direction to handle the computational cost of the bag-of-words approach. When optimizing a system based on the bag-of-words approach, the goal is to minimize the time it takes to process batches of images.
   In this paper, we analyze the bag-of-words model for visual categorization in terms of computational cost and identify two major bottlenecks: the quantization step and the classification step. We address these two bottlenecks by proposing two efficient algorithms for quantization and classification by exploiting the GPU hardware and the CUDA parallel programming model. The algorithms are designed to 1) keep categorization accuracy intact, 2) decompose the problem, and 3) give the same numerical results.
   In the experiments on large scale datasets, it is shown that, by using a parallel implementation on the Geforce GTX260GPU, classifying unseen images is 4.8 times faster than a quad-core CPU version on the Core i7 920, while giving the exact same numerical results. In addition, we show how the algorithms can be generalized to other applications, such as text retrieval and video retrieval. Moreover, when the obtained speedup is used to process extra video frames in a video retrieval benchmark, the accuracy of visual categorization is improved by 29%.
C1 [van de Sande, Koen E. A.; Gevers, Theo; Snoek, Cees G. M.] Univ Amsterdam, Inst Informat, Intelligent Syst Lab Amsterdam, NL-1098 XH Amsterdam, Netherlands.
C3 University of Amsterdam
RP van de Sande, KEA (corresponding author), Univ Amsterdam, Inst Informat, Intelligent Syst Lab Amsterdam, NL-1098 XH Amsterdam, Netherlands.
EM ksande@uva.nl; th.gevers@uva.nl; cgmsnoek@uva.nl
OI Snoek, Cees/0000-0001-9092-1556
CR [Anonymous], 2008, 2008 IEEE Hot Chips 20 Symposium (HCS), DOI 10.1109/HOTCHIPS.2008.7476516
   [Anonymous], 2006, P 20 ANN C NEUR INF
   [Anonymous], 2010, NVIDIA CUDA Programming Guide
   [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], 2008, P IASTED INT S COMPU
   [Anonymous], P INT WORKSH MULT IN
   [Anonymous], P VIS REC CHALL WORK
   [Anonymous], 2010, SIGARCH COMPUT ARCHI
   [Anonymous], IEEE C COMP VIS PAT, DOI DOI 10.1109/CVPR.2006.68
   Asuncion A., 2007, Uci machine learning repository
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   BORDAWEKAR R, 2010, IBMRC24982
   Cai D, 2007, IEEE DATA MINING, P427, DOI 10.1109/ICDM.2007.88
   Catanzaro B., 2008, P 25 INT C MACHINE L, P104, DOI DOI 10.1145/1390156.1390170
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chang CC, 2006, PATTERN RECOGN LETT, V27, P1077, DOI 10.1016/j.patrec.2005.12.017
   CHANG SF, 2008, P TRECVID WORKSH
   CORNELIS N, 2008, P IEEE COMP VIS PATT
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Diamos G., 2009, TRANSLATING GPU BINA
   Diamos Gregory., 2009, The design and implementation ocelot's dynamic binary translator from ptx to multi-core x86
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   GAIDON A, 2008, PASCAL VISUAL OBJECT
   Garland M, 2008, IEEE MICRO, V28, P13, DOI 10.1109/MM.2008.57
   Geusebroek JM, 2003, IEEE T IMAGE PROCESS, V12, P938, DOI 10.1109/TIP.2003.812429
   Grauman K, 2007, J MACH LEARN RES, V8, P725
   Hassaballah M, 2008, COMPUT J, V51, P630, DOI 10.1093/comjnl/bxm099
   Huurnink B, 2010, J AM SOC INF SCI TEC, V61, P1180, DOI 10.1002/asi.21327
   Jegou H., 2009, P IEEE INT C COMP VI
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235
   KAHAN W, 1965, COMMUN ACM, V8, P40, DOI 10.1145/363707.363723
   *KHRONOSGROUP, 2010, OPENCL
   Lindholm E, 2008, IEEE MICRO, V28, P39, DOI 10.1109/MM.2008.31
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Owens JD, 2008, P IEEE, V96, P879, DOI 10.1109/JPROC.2008.917757
   Seinstra FJ, 2007, IEEE MULTIMEDIA, V14, P64, DOI 10.1109/MMUL.2007.74
   Sharp T, 2008, LECT NOTES COMPUT SC, V5305, P595, DOI 10.1007/978-3-540-88693-8_44
   Sinha S.N., 2007, Machine Vision and Applications
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Snoek C., 2009, P TRECVID WORKSH
   SNOEK CGM, 2005, P IEEE INT C MULT EX
   STRATTON J, 2008, P WORKSH LANG COMP P
   Do TN, 2008, LECT NOTES ARTIF INT, V5139, P147
   Uijlings J. R. R., 2009, P ACM INT C IM VID R
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132
   Vuduc R., 2010, P USENIX WORKSH HOT
   Webb A. R., 2002, STAT PATTERN RECOGNI
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
NR 51
TC 51
Z9 58
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2011
VL 13
IS 1
BP 60
EP 70
DI 10.1109/TMM.2010.2091400
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 708TJ
UT WOS:000286386900007
DA 2024-07-18
ER

PT J
AU Li, YN
   Tian, YH
   Duan, LY
   Yang, JJ
   Huang, TJ
   Gao, W
AF Li, Yuanning
   Tian, Yonghong
   Duan, Ling-Yu
   Yang, Jingjing
   Huang, Tiejun
   Gao, Wen
TI Sequence Multi-Labeling: A Unified Video Annotation Scheme With Spatial
   and Temporal Context
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Sequence multi-labeling; spatial correlation; temporal correlation;
   video annotation
ID CONCEPT FUSION; FRAMEWORK
AB Automatic video annotation is a challenging yet important problem for content-based video indexing and retrieval. In most existing works, annotation is formulated as a multi-labeling problem over individual shots. However, video is by nature informative in spatial and temporal context of semantic concepts. In this paper, we formulate video annotation as a sequence multi-labeling (SML) problem over a shot sequence. Different from many video annotation paradigms working on individual shots, SML aims to predict a multi-label sequence for consecutive shots in a global optimization manner by incorporating spatial and temporal context into a unified learning framework. A novel discriminative method, called sequence multi-label support vector machine (SVMSML), is accordingly proposed to infer the multi-label sequence for a given shot sequence. In (SVMSML), a joint kernel is employed to model the feature-level and concept-level context relationships (i.e., the dependencies of concepts on the low-level features, spatial and temporal correlations of concepts). A multiple-kernel learning (MKL) algorithm is developed to optimize the kernel weights of the joint kernel as well as the SML score function. To efficiently search the desirable multi-label sequence over the large output space in both training and test phases, we adopt an approximate method to maximize the energy of a binary Markov random field (BMRF). Extensive experiments on TRECVID'05 and TRECVID'07 datasets have shown that our proposed (SVMSML) gains superior performance over the state-of-the-art.
C1 [Li, Yuanning; Yang, Jingjing] Chinese Acad Sci, Inst Comp Technol, Beijing 100080, Peoples R China.
   [Li, Yuanning; Yang, Jingjing] Chinese Acad Sci, Grad Sch, Beijing 100080, Peoples R China.
   [Tian, Yonghong; Duan, Ling-Yu; Huang, Tiejun; Gao, Wen] Peking Univ, Natl Engn Lab Video Technol, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Peking University
RP Li, YN (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing 100080, Peoples R China.
EM ynli@jdl.ac.cn; yhtian@pku.edu.cn; lingyu@pku.edu.cn; jjyang@jdl.ac.cn;
   tjhuang@pku.edu.cn; wgao@pku.edu.cn
RI Huang, Tiejun/D-6161-2011
FU Chinese National Natural Science Foundation [60973055, 90820003,
   60902057]; National Hi-Tech R&D Program (863) of China [2006AA010105];
   National Basic Research Program of China [2009CB320906]
FX (Manuscript received October 26, 2009; revised March 25, 2010 and July
   05, 2010; accepted July 29, 2010. Date of publication August 16, 2010;
   date of current version November 17, 2010. This work was supported in
   part by grants from the Chinese National Natural Science Foundation
   under contract No. 60973055, No. 90820003, and No. 60902057, in part by
   National Hi-Tech R&D Program (863) of China under contract 2006AA010105,
   and in part by National Basic Research Program of China under contract
   No. 2009CB320906. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Christophe De
   Vleeschouwer.
CR Altun Y., 2003, P INT C MACHINE LEAR, P3
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], P ADV MULT MOD
   [Anonymous], THU ICRC TRECVID 200
   [Anonymous], OPER RES SER
   [Anonymous], P ACM MULT
   [Anonymous], DTO CHALL WORKSH LAR
   [Anonymous], P MACH LEARN RES JUL
   [Anonymous], P ADV NEUR INF PROC
   [Anonymous], P IEEE INT C IM VID
   [Anonymous], 2009, P NIPS
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P MULT INF RETR
   [Anonymous], 2007, CVPR, DOI DOI 10.1109/CVPR.2007.383203
   [Anonymous], P ACM MULT
   [Anonymous], P ACM MULT
   [Anonymous], NEUROCOMPUTING
   Bosch A., 2007, P 6 ACM INT C IMAGE, P401, DOI DOI 10.1145/1282280.1282340
   Boyd S., 2004, CONVEX OPTIMIZATION
   Cristianini N., 2000, INTRO SUPPORT VECTOR
   Duan I., 2009, P IEEE INT C COMP VI, P1
   Ebadollahi S, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P881, DOI 10.1109/ICME.2006.262691
   Jiang W, 2007, INT CONF ACOUST SPEE, P949
   Jiang W, 2006, IEEE IMAGE PROC, P2917, DOI 10.1109/ICIP.2006.313129
   Jiang Y., 2009, 2009 International Conference on Sustainable Power Generation and Supply, P1, DOI DOI 10.1109/WCSP.2009.5371739
   Kennedy L.S., 2007, Proceedings of the 6th ACM international conference on Image and video retrieval, P333
   Li HZ, 2007, PROC WRLD ACAD SCI E, V1, P1
   Liu KH, 2008, IEEE T MULTIMEDIA, V10, P240, DOI 10.1109/TMM.2007.911826
   Liu Y, 2008, PACIIA: 2008 PACIFIC-ASIA WORKSHOP ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION, VOLS 1-3, PROCEEDINGS, P91, DOI 10.1145/1459359.1459372
   Loui Alexander., 2007, MIR 07, P245
   Naphade MilindR., 2004, P 12 ANN ACM INT C M, P660, DOI DOI 10.1145/1027527.1027680
   Naphade MR, 2001, IEEE T MULTIMEDIA, V3, P141, DOI 10.1109/6046.909601
   Naphade MR, 2004, J VIS COMMUN IMAGE R, V15, P348, DOI 10.1016/j.jvcir.2004.04.010
   Naphade MR, 2002, IEEE T CIRC SYST VID, V12, P40
   Natsev Apostol., 2007, MULTIMEDIA 07, P991
   Qi G.J., 2007, P 15 ACM INT C MULTI, P17, DOI DOI 10.1145/1291233.1291245
   Qi GJ, 2008, ACM T MULTIM COMPUT, V5, DOI 10.1145/1404880.1404883
   Shechtman E., 2007, PROC IEEE INT C COMP, P1
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Smith JR, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P445
   Tsochantaridis I., 2004, ICML, P104
   Wang M., 2007, ACM Multi- media, P862
   Xie L., 2002, Pattern Recognition Letters, P767
   Yang J., 2006, PROC ACM INT WORKSHO, P33
   Yang J., 2007, P 15 ACM INT C MULT, P188
   Yang M., 2009, WiCom '09. 5th International Conference on Wireless Communications, Networking and Mobile Computing, P1
NR 49
TC 18
Z9 19
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2010
VL 12
IS 8
BP 814
EP 828
DI 10.1109/TMM.2010.2066960
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 681ZW
UT WOS:000284365100004
DA 2024-07-18
ER

PT J
AU Nasser, N
AF Nasser, Nidal
TI Service Adaptability in Multimedia Wireless Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bandwidth adaptation; call admission control; quality of service;
   real-time multimedia traffic; wireless cellular networks
ID ADAPTIVE MULTIMEDIA
AB Next-generation wireless communication systems aim at supporting wireless multimedia services with different quality-of-service (QoS) and bandwidth requirements. Therefore, effective management of the limited radio resources is important to enhance the network performance. In this paper, we propose a QoS adaptive multimedia service framework for controlling the traffic in multimedia wireless networks (MWN) that enhances the current methods used in cellular environments. The proposed framework is designed to take advantage of the adaptive bandwidth allocation (ABA) algorithm with new calls in order to enhance the system utilization and blocking probability of new calls. The performance of our framework is compared to existing framework in the literature. Simulation results show that our QoS adaptive multimedia service framework outperforms the existing framework in terms of new call blocking probability, handoff call dropping probability, and bandwidth utilization.
C1 Univ Guelph, Dept Comp & Informat Sci, Guelph, ON N1G 2W1, Canada.
C3 University of Guelph
RP Nasser, N (corresponding author), Univ Guelph, Dept Comp & Informat Sci, Guelph, ON N1G 2W1, Canada.
EM nasser@cis.uoguelph.ca
RI Nasser, Nidal/C-5450-2017
CR Kesidis G, 1993, IEEE ACM T NETWORK, V1, P424, DOI 10.1109/90.251894
   Kwon T, 2000, ELECTRON LETT, V36, P852, DOI 10.1049/el:20000639
   Kwon T, 2003, WIREL NETW, V9, P51, DOI 10.1023/A:1020877007305
   Kwon T, 2002, WIRELESS PERS COMMUN, V22, P337, DOI 10.1023/A:1020239803918
   Kwon T., 1998, P 1 ACM INT WORKSH W, P111
   KWON T, 1999, P ACM WORKSH WIR MOB, P51
   Lee KJ, 1998, J MICROBIOL BIOTECHN, V8, P1
   Naghshineh M, 1997, IEEE COMMUN MAG, V35, P72, DOI 10.1109/35.634764
   Nasser N, 2004, IEEE IPCCC, P61
   Nasser N, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P4295, DOI 10.1109/ICC.2004.1313358
   NASSER N, 2005, P IEEE IFIP INT C WI, P275
   RAPPAPORT SS, 1991, IEEE T VEH TECHNOL, V40, P546, DOI 10.1109/25.97509
   TALUKDAR AK, 1998, P MOBICOM, P169
   Xiao Y, 2001, IEEE VTS VEH TECHNOL, P2081, DOI 10.1109/VETECS.2001.945063
   Xiao Y, 2001, IEEE IC COMP COM NET, P598, DOI 10.1109/ICCCN.2001.956330
   Xiao Y, 2002, COMPUT COMMUN, V25, P1153, DOI 10.1016/S0140-3664(02)00006-3
   Xiao Y, 2001, NINTH INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS AND SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS, PROCEEDINGS, P81, DOI 10.1109/MASCOT.2001.948856
   Xiao Y, 2000, PROCEEDINGS OF THE IEEE 2000 NATIONAL AEROSPACE AND ELECTRONICS CONFERENCE, P214, DOI 10.1109/NAECON.2000.894913
   Xiao Y, 2000, 8TH INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS AND SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS, PROCEEDINGS, P477, DOI 10.1109/MASCOT.2000.876574
   Yeung KL, 1996, IEEE T VEH TECHNOL, V45, P601, DOI 10.1109/25.543716
   YOON I, 1999, P IEEE INT COMM C IC, P1442
   Yu F, 2004, FIRST INTERNATIONAL CONFERENCE ON BROADBAND NETWORKS, PROCEEDINGS, P579
   YU F, 2004, P IEEE ANN JOINT C C, V3, P2130
   [No title captured]
NR 24
TC 9
Z9 9
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2009
VL 11
IS 4
BP 786
EP 792
DI 10.1109/TMM.2009.2017612
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 449LA
UT WOS:000266330900019
DA 2024-07-18
ER

PT J
AU Kim, HJ
   Yeom, HY
AF Kim, Hyunjoo
   Yeom, Heon Y.
TI Dynamic Scheme Transition Adaptable to Variable Video Popularity in a
   Digital Broadcast Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Periodic broadcast; video-on-demand; video popularity; video streaming
AB To provide on-demand video streaming services through the network, video objects with both high and low client request rates must be served efficiently. In this paper, we propose a dynamic scheme transition to provide on-demand streaming services efficiently regardless of video popularity. This approach can maintain quality-of-service (QoS) by transitioning the service scheme according to the request rate. The server provides services by heuristically broadcasting video segments when the video popularity is low and by a Periodic Broadcast when the video popularity is high. The server identifies the variations in client request rates from the number of service channels and determines transitions to more efficient service schemes autonomously. We evaluated this scheme transition for various parameters and traces from a company providing streaming services. The results show that the performance of scheme transition is very efficient.
C1 [Kim, Hyunjoo] Rutgers State Univ, Dept Elect & Comp Engn, Piscataway, NJ 08854 USA.
   [Yeom, Heon Y.] Seoul Natl Univ, Sch Comp Sci & Engn, Seoul 151742, South Korea.
C3 Rutgers University System; Rutgers University New Brunswick; Seoul
   National University (SNU)
RP Kim, HJ (corresponding author), Rutgers State Univ, Dept Elect & Comp Engn, Piscataway, NJ 08854 USA.
EM mildwind@gmail.com; yeom@snu.ac.kr
CR Aggarwal CC, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P118, DOI 10.1109/MMCS.1996.534963
   Azad SA, 2005, ITCC 2005: INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: CODING AND COMPUTING, VOL 2, P283, DOI 10.1109/ITCC.2005.251
   Carter SR, 2001, INT CON DISTR COMP S, P657
   Chien WD, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P1843, DOI 10.1109/ICME.2004.1394616
   EAGER DL, 1998, P 4 INT WORKSH MULT, P18
   Hua K., 1997, PROC SIGCOMM, P89
   Juhn LS, 1998, IEEE T BROADCAST, V44, P100, DOI 10.1109/11.713059
   Juhn LS, 1997, IEEE T BROADCAST, V43, P268, DOI 10.1109/11.632927
   Paris J.-F., 1999, Proceedings Eight International Conference on Computer Communications and Networks (Cat. No.99EX370), P118, DOI 10.1109/ICCCN.1999.805505
   Paris JF, 1998, SIXTH INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS AND SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS, PROCEEDINGS, P127, DOI 10.1109/MASCOT.1998.693685
   Paris JF, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P49, DOI 10.1109/ICME.2000.869543
   PARIS JF, 1999, P 1999 MULT COMP NET, P317
   SHEU S, 2003, TR0303 IOW STAT U DE
   Tseng YC, 2004, IEEE ACM T NETWORK, V12, P559, DOI 10.1109/TNET.2004.828965
   TSENG YC, 2000, P IEEE INFOCOM 2000, P727
   VISWANATHAN S, 1995, P SOC PHOTO-OPT INS, V2417, P66, DOI 10.1117/12.206080
   YANG ZY, 2000, THESIS NATL CENTRAL
   Zhang GP, 2002, TOP APPL PHYS, V83, P245
NR 18
TC 1
Z9 1
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2009
VL 11
IS 3
BP 486
EP 493
DI 10.1109/TMM.2009.2012935
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 425JH
UT WOS:000264632300014
DA 2024-07-18
ER

PT J
AU Chen, PY
   Lin, YM
   Cho, MY
AF Chen, Pei-Yin
   Lin, Yi-Ming
   Cho, Min-Yi
TI An Efficient Design of Variable Length Decoder for MPEG-1/2/4
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Area-efficient; MPEG-1/2/4; variable length decoder
ID SYSTEM
AB In this paper, a novel and area-efficient variable length decoder (VLD) for MPEG-1/2/4 is presented. Instead of carrying out every variable length coding table with one dedicated lookup table (LUT) directly, we employ an efficient clustering-merging technique to reduce both the size of a single LUT and the total number of LUTs required for MPEG-1/2/4. Synthesis results show that our VLD occupies 10666 gate counts and operates at 125 MHz by using the standard cell from Artisan TSMC's 0.18 mu m process. As demonstrated, the proposed design outperforms other VLDs with less hardware cost. It can decode a symbol of different standards in every cycle and support video resolution of HD1080 at 30 frames/s for MPEG-1/2/4 real-time decoding.
C1 [Chen, Pei-Yin; Lin, Yi-Ming; Cho, Min-Yi] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 701, Taiwan.
C3 National Cheng Kung University
RP Chen, PY (corresponding author), Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 701, Taiwan.
EM pychen@csie.ncku.edu.tw; ymlin@csie.ncku.edu.tw; mycho@csie.ncku.edu
RI Chen, Pei Yin/AFT-4918-2022
OI Chen, Pei-Yin/0000-0002-5104-6055
FU National Science Council of Taiwan [NSC-96-2221-E-006-027-MY3]
FX Manuscript received January 23, 2008 revised May 22, 2008. Current
   version published November 17. 2008. This work was supported in part by
   the National Science Council of Taiwan R.O.C. under Grant
   NSC-96-2221-E-006-027-MY3. This work made use of Shared Facilities
   supported by the Program of Top 100 Universities Advancement, Ministry
   of Education, Taiwan. This paper was recommended by Associate Editor
   Simon R. Jones.
CR [Anonymous], 1996, 138182 ISOIEC
   Chang YC, 2001, 2001 INTERNATIONAL SYMPOSIUM ON VLSI TECHNOLOGY, SYSTEMS, AND APPLICATIONS, PROCEEDINGS OF TECHNICAL PAPERS, P188, DOI 10.1109/VTSA.2001.934516
   Chen PY, 2007, IEEE T CIRCUITS-II, V54, P507, DOI 10.1109/TCSII.2007.891752
   Chien CD, 2006, IEEE T CIRC SYST VID, V16, P1172, DOI 10.1109/TCSVT.2006.881873
   Cho SH, 1999, IEEE T VLSI SYST, V7, P249, DOI 10.1109/92.766752
   *ISO IEC, 2001, N3908 ISOIEC JTC 1SC
   *ISO IEC, 1993, 111722 ISOIEC
   Lee SW, 2003, IEEE T CIRCUITS-II, V50, P73, DOI 10.1109/TCSII.2003.808893
   Lei SM, 1991, IEEE T CIRC SYST VID, V1, P147, DOI 10.1109/76.109154
   Liu CH, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 2, PROCEEDINGS, P337
   MUKHERJEE A, 1991, IEEE T CIRCUITS SYST, V38, P306, DOI 10.1109/31.101323
   Shieh BJ, 2001, IEEE T CIRC SYST VID, V11, P210, DOI 10.1109/76.905986
   WEI BWY, 1995, IEEE T CIRC SYST VID, V5, P175, DOI 10.1109/76.388067
NR 13
TC 1
Z9 1
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2008
VL 10
IS 7
BP 1307
EP 1315
DI 10.1109/TMM.2008.2004909
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 378HB
UT WOS:000261310700008
DA 2024-07-18
ER

PT J
AU Liang, GF
   Liang, B
AF Liang, Guanfeng
   Liang, Ben
TI Effect of Delay and Buffering on Jitter-Free Streaming Over Random VBR
   Channels
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimedia streaming; performance modeling; playout delay; playout
   interruption; receiver buffering
ID VIDEO
AB We study the optimal streaming of variable bit-rate (VBR) video over a random VBR channel. The goal of a streaming application is to enable the successful decoding of each video object before its displaying deadline is violated. Hence, we define the main performance metric of a streaming system as the probability of un-interrupted video presentation, or jitter-free probability. Previous literature has described solutions to estimate the jitter-free probability by assuming either independence in the encoded data process or simplistic channel models. In this work, we present a novel analytical framework, which requires only some basic statistical information of an arbitrary VBR channel, to bound the probability of jitter-free playout under the constraint of initial playout delay and receiver buffer size. Both the infinite and finite buffer cases are considered. This technique is then applied to investigate streaming over a wireless system modeled by an extended Gilbert channel with ARQ transmission control. Experimental results with MPEG-4 VBR encoded video demonstrates that the proposed analysis derives close bounds to the actual system performance. Finally, we show that the proposed analysis provides a theoretical foundation to quantify the tradeoffs between the initial playout delay, the receiver buffer size, and the jitter-free probability for a general class of VBR streaming over random VBR channels.
C1 [Liang, Guanfeng; Liang, Ben] Univ Toronto, Dept Elect & Comp Engn, Toronto, ON M5S 3G4, Canada.
C3 University of Toronto
RP Liang, GF (corresponding author), Univ Toronto, Dept Elect & Comp Engn, Toronto, ON M5S 3G4, Canada.
EM guan-feng@comtn.utoronto.ca; liang@comm.utoronto.ca
RI Liang, Ben/KRO-7641-2024; Liang, Guanfeng/J-9500-2012
OI Liang, Ben/0009-0002-4059-2832; Liang, Ben/0000-0002-1800-1322
CR [Anonymous], MSRTR200135
   CHAKARESKI J, 2002, P IEEE 5 WORKSH MULT
   ELLIOTT EO, 1963, AT&T TECH J, V42, P1977, DOI 10.1002/j.1538-7305.1963.tb00955.x
   Etoh M, 2005, P IEEE, V93, P111, DOI 10.1109/JPROC.2004.839605
   Fitzek FHP, 2001, IEEE NETWORK, V15, P40, DOI 10.1109/65.967596
   GILBERT EN, 1960, BELL SYST TECH J, V39, P1253, DOI 10.1002/j.1538-7305.1960.tb03959.x
   Hassan M, 2004, IEEE T WIREL COMMUN, V3, P821, DOI 10.1109/TWC.2004.827729
   Kalman M, 2004, IEEE T CIRC SYST VID, V14, P841, DOI 10.1109/TCSVT.2004.828335
   KOPKE A, 2003, P IEEE INF MAR
   Lakshman TV, 1998, P IEEE, V86, P952, DOI 10.1109/5.664282
   Sanneck H. A., 2000, P SPIE ACM SIGMM MUL
   SEFEROGLU H, 2005, P IEEE ICC MAY
   Sen S, 2000, IEEE T MULTIMEDIA, V2, P37, DOI 10.1109/6046.825793
   Stockhammer T, 2004, IEEE T MULTIMEDIA, V6, P268, DOI 10.1109/TMM.2003.822795
   THIRAN P, 2001, P IEEE INF APR
   VARSA V, 2003, 26937VI40 3GPP TR
   WANG HS, 1995, IEEE T VEH TECHNOL, V44, P163, DOI 10.1109/25.350282
   WILLIG A, 2000, P 13 IEEE INT S PERS
NR 18
TC 33
Z9 40
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 1128
EP 1141
DI 10.1109/TMM.2008.2001364
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600016
DA 2024-07-18
ER

PT J
AU Nadarajah, S
   Kotz, S
AF Nadarajah, Saralees
   Kotz, Samuel
TI Comments on "Scalable Services via Egress Admission Control"
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE admission control test; Gumbel distribution; sum of two random variables
AB In the recent paper by Cetinkaya et al., admission control tests are derived by approximating the sum of two Gumbel distributed random variables by a Gumbel distributed random variable. In fact, the sum of two Gumbel distributed random variables does not follow a Gumbel distribution. Here, explicit expressions are derived for the probability density function (pdf) and the cumulative distribution function (cdf) of the exact distribution of the sum. The discrepancy between the exact and the approximate distributions is studied numerically.
C1 [Nadarajah, Saralees] Univ Manchester, Manchester M60 1QD, Lancs, England.
   [Kotz, Samuel] George Washington Univ, Washington, DC 20052 USA.
C3 University of Manchester; George Washington University
RP Nadarajah, S (corresponding author), Univ Manchester, Manchester M60 1QD, Lancs, England.
EM saralees.nadarajah@manchester.ac.uk
CR Cetinkaya C, 2001, IEEE T MULTIMEDIA, V3, P69, DOI 10.1109/6046.909595
   Gradshteyn I. S., 2014, Table of Integrals, Series, and Products
   Prudnikov A.P., 1986, Integrals and Series, V2
NR 3
TC 2
Z9 3
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2008
VL 10
IS 1
BP 160
EP 161
DI 10.1109/TMM.2007.911201
PG 2
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 245QZ
UT WOS:000251952200016
DA 2024-07-18
ER

PT J
AU Zhu, JK
   Hoi, SCH
   Lyu, MR
AF Zhu, Jianke
   Hoi, Steven C. H.
   Lyu, Michael R.
TI Face annotation using Transductive Kernel Fisher Discriminant
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE face annotation; image annotation; kernel Fisher discriminant;
   multimedia information retrieval; supervised learning; transductive
   kernel Fisher discriminant; transductive learning
ID IMAGE RETRIEVAL; RECOGNITION
AB Face annotation in images and videos enjoys many potential applications in multimedia information retrieval. Face annotation usually requires many training data labeled by hand in order to build effective classifiers. This is particularly challenging when annotating faces on large-scale collections of media data, in which huge labeling efforts would be very expensive. As a result, traditional supervised face annotation methods often suffer from insufficient training data. To attack this challenge, in this paper, we propose a novel Transductive Kernel Fisher Discriminant (TKFD) scheme for face annotation, which outperforms traditional supervised annotation methods with few training data. I he main idea of our approach is to solve the Fisher's discriminant using deformed kernels incorporating the information of both labeled and unlabeled data. To evaluate the effectiveness of our method, we have conducted extensive experiments on three types of multimedia testbeds: the FRGC benchmark face dataset, the Yahoo! web image collection, and the TRECVID video data collection. The experimental results show that our TKFD algorithm is more effective than traditional supervised approaches, especially when there are very few training data.
C1 [Zhu, Jianke; Lyu, Michael R.] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
   [Hoi, Steven C. H.] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
C3 Chinese University of Hong Kong; Nanyang Technological University
RP Zhu, JK (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
EM jkzhu@cse.cuhk.edu.hk; chhoi@ntu.edu.sg; lyu@cse.cuhk.edu.hk
RI HOI, Steven C. H./A-3736-2011
OI Hoi, Steven/0000-0002-4584-3453
CR AHONEN T, 2004, FACE RECOGNITION LOC, V1, P469
   [Anonymous], TREC VID RETR EV
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], SEMISUPERVISED CLASS
   [Anonymous], 1999, ICML
   [Anonymous], P ACM SIGKDD
   [Anonymous], P ADV NEUR INF PROC
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Berg T. L., 2005, ADV NEURAL INFORM PR
   Berg TL, 2004, PROC CVPR IEEE, P848
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Brunelli R, 2000, IEEE T MULTIMEDIA, V2, P164, DOI 10.1109/6046.865481
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97
   Fukunaga K., 1990, Introduction to StatisticalPattern Recognition
   Goh KS, 2005, IEEE T KNOWL DATA EN, V17, P1333, DOI 10.1109/TKDE.2005.170
   HOI SC, 2004, P ACM INT C MULT MM2
   Hoi SCH, 2006, IEEE T KNOWL DATA EN, V18, P509, DOI 10.1109/TKDE.2006.1599389
   Hoi SCH, 2005, PROC CVPR IEEE, P302
   Hoi Steven C. H., 2006, Proceedings of the International Conference on the World Wide Web (WWW-2006), P633, DOI [10.1145/1135777.1135870, DOI 10.1145/1135777.1135870]
   Houghton R, 1999, IEEE INTELL SYST APP, V14, P45, DOI 10.1109/5254.796089
   Jeon J., 2003, P 26 ANN INT ACM SIG
   Jianhong Z., 2006, P 9 EUR C COMP VIS E, P86
   Jin R., 2004, Proceedings of the 12th annual ACM international conference on Multimedia, P892, DOI DOI 10.1145/1027527.1027732
   Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200
   KANG F, 2005, P 2005 SIAM C DAT MI
   LADES M, 1993, IEEE T COMPUT, V42, P300, DOI 10.1109/12.210173
   Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Liu QS, 2004, IEEE T CIRC SYST VID, V14, P42, DOI 10.1109/TCSVT.2003.818352
   Lu Y, 2003, IEEE T MULTIMEDIA, V5, P339, DOI 10.1109/TMM.2003.813280
   Mika S, 2001, ADV NEUR IN, V13, P591
   Mika S., 1999, Neural Networks for Signal Processing IX: Proceedings of the 1999 IEEE Signal Processing Society Workshop (Cat. No.98TH8468), P41, DOI 10.1109/NNSP.1999.788121
   OVER P, 2005, P TRECVID WORKSH
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   PHILLIPS PJ, 2005, OVERVIEW FACE RECOGN, V1, P947
   Platt J., ADV LARGE MARGIN CLA, P61
   Satoh S, 1999, IEEE MULTIMEDIA, V6, P22, DOI 10.1109/93.752960
   Sindhwani V., 2005, ICML, V2005, P74
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Vasconcelos N, 2005, IEEE T MULTIMEDIA, V7, P127, DOI 10.1109/TMM.2004.840596
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Yang J, 2005, Proceedings of the 2005 International Conference on Active Media Technology (AMT 2005), P28
   Yang J., 2004, P 12 ANN ACM INT C M, P580
   ZHANG L, 2003, P AMC MULT C
   ZHANG T, 2005, P NIPS
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
   Zhu X., 2005, Time-sensitive Dirichlet process mixture models
NR 48
TC 25
Z9 33
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2008
VL 10
IS 1
BP 86
EP 96
DI 10.1109/TMM.2007.911245
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 245QZ
UT WOS:000251952200009
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Wei, J
AF Wei, Jie
TI Shape indexing and recognition based on regional analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE digital library; eigen analysis; shape indexing; shape recognition
ID OBJECT RECOGNITION; RETRIEVAL; REPRESENTATION; CLASSIFICATION
AB Shape indexing and recognition have received great attention in multimedia processing communities due to the wide range of utilities. In achieving shape representation, most influential methods treat shapes as intrinsic curves, which is not in agreement with the way human vision systems achieve the same task. In this paper, a new framework is developed where a shape is treated as a 2-D region. We first perform an eigen analysis to align P in the standard orientation. Three numbers are generated to indicate the global geometrical nature. Next, according to the two eigen vectors, we partition P into four halves and eight quadrants. Five numbers are then produced for each region to signify its geometrical properties and relation with P. The aggregate of these numbers, 63 in total, is the actual index for P. Recognition is effected by weighted L1 distances between shapes. This indexing scheme captures the global geometry of shapes and is resilient to rotations and scales, which are of crucial importance in the perceptive process of human vision systems. It can tolerate occlusions present in most standard shape datasets but is not robust against severe occlusions. Empirical studies conducted on standard synthetic and real-world datasets demonstrate encouraging performances.
C1 CUNY City Coll, Dept Comp Sci, New York, NY 10031 USA.
C3 City University of New York (CUNY) System; City College of New York
   (CUNY)
RP Wei, J (corresponding author), CUNY City Coll, Dept Comp Sci, New York, NY 10031 USA.
EM wei@cs.ccny.cuny.edu
CR Abbasi S, 2001, IEEE T IMAGE PROCESS, V10, P131, DOI 10.1109/83.892449
   Agricola I., 2002, Global analysis: differential forms in analysis, geometry and physics, volume 52 of Graduate Studies in Mathematics
   [Anonymous], 1998, STAT LEARNING THEORY
   [Anonymous], 1999, A comprehensive introduction to differential geometry.
   BASRI R, 1998, VIS RES, V38
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bui TD, 2001, INT J PATTERN RECOGN, V15, P1213, DOI 10.1142/S0218001401001465
   CHEN CC, 1993, PATTERN RECOGN, V26, P683, DOI 10.1016/0031-3203(93)90121-C
   CHERN SS, 1967, MATH ASS AM
   Cyr CM, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P254, DOI 10.1109/ICCV.2001.937526
   Darwin Charles., 1958, ORIGIN SPECIES
   Do Carmo M., 1976, Differential Geometry of Curves and Surfaces
   Drew MS, 1999, PATTERN RECOGN, V32, P1369, DOI 10.1016/S0031-3203(98)00168-X
   Drew MS, 1998, PATTERN RECOGN, V31, P1077, DOI 10.1016/S0031-3203(97)00144-1
   Flickner M., 1995, IEEE COMPUT, V28, P23, DOI DOI 10.1109/2.410146
   Gdalyahu Y, 1999, IEEE T PATTERN ANAL, V21, P1312, DOI 10.1109/34.817410
   Giblin P, 2004, IEEE T PATTERN ANAL, V26, P238, DOI 10.1109/TPAMI.2004.1262192
   Giblin PJ, 2003, IEEE T PATTERN ANAL, V25, P895, DOI 10.1109/TPAMI.2003.1206518
   Hampel FR, 1986, Robust statistics. The approach based on influence functions
   Hopf H., 1989, DIFFERENTIAL GEOMETR, Vsecond
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692
   Jain A. K., 1988, Algorithms for Clustering Data, P446
   Jain R., 1995, MACHINE VISION
   KHOTANZAD A, 1990, IEEE T PATTERN ANAL, V12, P489, DOI 10.1109/34.55109
   KIMIA B, 1995, INT J COMPUT VIS, V15
   Latecki LJ, 2000, IEEE T PATTERN ANAL, V22, P1185, DOI 10.1109/34.879802
   Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850
   Lee HK, 1999, IEEE T PATTERN ANAL, V21, P961, DOI 10.1109/34.799904
   LINDEBERG T, 1998, SCALE SPACE THEORY C
   Malladi R, 1996, IEEE T IMAGE PROCESS, V5, P1554, DOI 10.1109/83.541425
   Marr D., 1982, Vision
   Milios E, 2000, IEEE T IMAGE PROCESS, V9, P141, DOI 10.1109/83.817606
   Millman R. S., 1977, Elements of Differential Geometry
   MOKHTARIAN F, 1992, IEEE T PATTERN ANAL, V14, P789, DOI 10.1109/34.149591
   MOKHTARIAN F, 1995, IEEE T PATTERN ANAL, V17, P539, DOI 10.1109/34.391387
   Mokhtarian F, 2004, IEEE T IMAGE PROCESS, V13, P653, DOI 10.1109/TIP.2004.826126
   Mokhtarian F., 1996, P 1996 BRIT MACH VIS
   Mokhtarian F., 2003, CURVATURE SCALE SPAC
   Mokhtarian F., 1996, Proceedings of International Workshop on Image Databases and Multimedia Search, P35
   Myers R, 2000, IEEE T PATTERN ANAL, V22, P628, DOI 10.1109/34.862201
   Papadimitriou C.H., 1998, COMBINATORIAL OPTIMI
   Pelillo M, 1999, IEEE T PATTERN ANAL, V21, P1105, DOI 10.1109/34.809105
   Picard RW, 1996, IEEE T PATTERN ANAL, V18, P769, DOI 10.1109/TPAMI.1996.531797
   Sebastian TB, 2003, IEEE T PATTERN ANAL, V25, P116, DOI 10.1109/TPAMI.2003.1159951
   Sebastian TB, 2004, IEEE T PATTERN ANAL, V26, P550, DOI 10.1109/TPAMI.2004.1273924
   Sethian JA, 1997, AM SCI, V85, P254
   Sharvit D, 1998, J VIS COMMUN IMAGE R, V9, P366, DOI 10.1006/jvci.1998.0396
   Siddiqi K, 1999, IMAGE VISION COMPUT, V17, P365, DOI 10.1016/S0262-8856(98)00130-9
   TORRE F, 2001, P IEEE CVPR 01
   TSAY YT, 1993, IEEE T PATTERN ANAL, V15, P180, DOI 10.1109/34.192491
   ULLMAN J, 2001, 1 DATABASE SYSTEMS
   Veltkamp RC, 2001, ADV PTRN RECOGNIT, P87
   Vidal R, 2005, IEEE T PATTERN ANAL, V27, P1945, DOI 10.1109/TPAMI.2005.244
   Wei H, 2002, IEEE T IMAGE PROCESS, V11, P912, DOI 10.1109/TIP.2002.801125
   Wertheimer M., 1938, SOURCE BOOK GESTALT, DOI [10.1037/11496-0053, DOI 10.1037/11496-0053, DOI 10.1037/11496-005]
   ZHANG YQ, 1997, IEEE T CIRCUITS SYST, V7
   Zhu SC, 1999, IEEE T PATTERN ANAL, V21, P1170, DOI 10.1109/34.809110
NR 57
TC 8
Z9 8
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2007
VL 9
IS 5
BP 1049
EP 1061
DI 10.1109/TMM.2007.898949
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 193ZX
UT WOS:000248314800014
DA 2024-07-18
ER

PT J
AU Ozcelebi, T
   Tekalp, AM
   Civanlar, MR
AF Ozcelebi, Tanir
   Tekalp, A. Murat
   Civanlar, M. Reha
TI Delay-distortion optimization for content-adaptive video streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content adaptive video coding; GoP rate allocation; pre-roll delay;
   semantic relevance; video streaming
ID BLOCKING ARTIFACTS; BIT ALLOCATION; ALGORITHM; IMAGES; AUDIO
AB We propose a new pre-roll delay-distortion optimization (DDO) framework that allows determination of the minimum pre-roll delay and distortion while ensuring continuous playback for on-demand content-adaptive video streaming over limited bitrate networks. The input video is first divided into temporal segments, which are assigned a relevance weight and a maximum distortion level, called relevance-distortion policy, which may be specified by the user. The system then encodes the input video according to the specified relevance-distortion policy, whereby the optimal spatial and temporal resolutions and quantization parameters, also called encoding parameters, are selected for each temporal segment. The optimal encoding parameters are computed using a novel, multi-objective optimization formulation, where a relevance weighted distortion measure and pre-roll delay are jointly minimized under maximum allowable buffer size, continuous playback, and maximum allowable distortion constraints. The performance of the system has been demonstrated for on-demand streaming of soccer videos with substantial improvement in the weighted distortion without any increase in pre-roll delay over a very low-bitrate network using AVC/H.264 encoding.
C1 Koc Univ, Coll Engn, TR-34450 Istanbul, Turkey.
C3 Koc University
RP Ozcelebi, T (corresponding author), Eindhoven Univ Technol, Dept Math & Comp Sci, NL-5612 AZ Eindhoven, Netherlands.
EM t.ozcelebi@tue.nl; mtekalp@ku.edu.tr; rcivanlar@docomolabs-usa.com
RI Civanlar, Mehmet/E-4656-2010; Civanlar, Mehmet/M-9929-2019; Tekalp,
   Murat/AAW-1060-2020
OI Civanlar, Mehmet/0000-0002-6171-5814; Tekalp, Ahmet
   Murat/0000-0003-1465-8121
CR BERTINI M, 2004, P 6 ACM SIGMM INT WO, P291
   BERTINI M, 2004, P INT WORKSH IM AN M
   Brown M., 1995, P ACM MULT 95 SAN FR, P35
   Chang SF, 2001, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES, PROCEEDINGS, P139, DOI 10.1109/IVL.2001.990868
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   *ITU T ISO IEC, 2003, 1449610 ISO IEC
   Kalman M, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P869, DOI 10.1109/ICME.2002.1035920
   KARMARKAR N, 1984, COMBINATORICA, V4, P373, DOI 10.1007/BF02579150
   KARUNASEKERA SA, 1995, IEEE T IMAGE PROCESS, V4, P713, DOI 10.1109/83.388074
   Lambrecht CJV, 1996, P SOC PHOTO-OPT INS, V2668, P450, DOI 10.1117/12.235440
   Lee JW, 2003, IEEE T CIRC SYST VID, V13, P488, DOI 10.1109/TCSVT.2003.813421
   LIM YI, 1999, P ECCE2 OCT 5 7
   Liu S, 2005, IEEE T CIRC SYST VID, V15, P15, DOI 10.1109/TCSVT.2004.839996
   Ma SW, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P793
   Miyahara M, 1998, IEEE T COMMUN, V46, P1215, DOI 10.1109/26.718563
   Mohan R, 1999, IEEE T MULTIMEDIA, V1, P104, DOI 10.1109/6046.748175
   PAHALAWATTA P, 2005, P SPIE IM VID COMM S
   Pan F, 2004, SIGNAL PROCESS-IMAGE, V19, P499, DOI 10.1016/j.image.2004.04.001
   PAPADIMITRIOU H, 2001, P S PRINC DAT SYST P, P52
   Steinbach E, 2001, IEEE IMAGE PROC, P962, DOI 10.1109/ICIP.2001.959207
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Tovinkere V., 2001, P IEEE INT C MULT EX
   van Beek P, 2003, IEEE SIGNAL PROC MAG, V20, P40, DOI 10.1109/MSP.2003.1184338
   Wang Y, 2000, IEEE SIGNAL PROC MAG, V17, P12, DOI 10.1109/79.888862
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Winkler S., 2001, P INT S WIRELESS PER, P547
   YOW D, 1995, P AS C COMP VIS
   Zhang T, 2001, IEEE T SPEECH AUDI P, V9, P441, DOI 10.1109/89.917689
   ZHAO L, 2003, IEEE INT C AC SPEECH
   2003, IEEE SIGNAL PROCESS, V20
NR 30
TC 12
Z9 15
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2007
VL 9
IS 4
BP 826
EP 836
DI 10.1109/TMM.2007.895670
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 170NW
UT WOS:000246671200013
DA 2024-07-18
ER

PT J
AU Xie, L
   Liu, ZQ
AF Xie, Lei
   Liu, Zhi-Qiang
TI Realistic mouth-synching for speech-driven talking face using
   articulatory modelling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE articulatory model; Baum-Welch DBN inversion (DBNI); dynamic Bayesian
   networks (DBNs); facial animation; mouth-synching; talking face
ID ANIMATION
AB This paper presents an articulatory modelling approach to convert acoustic speech into realistic mouth animation. We directly model the movements of articulators, such as lips, tongue, and teeth, using a dynamic Bayesian network (DBN)-based audio-visual articulatory model (AVAM). A multiple-stream structure with a shared articulator layer is adopted in the model to synchronously associate the two building blocks of speech, i.e., audio and video. This model not only describes the synchronization between visual articulatory movements and audio speech, but also reflects the linguistic fact that different articulators evolve asynchronously. We also present a Baum-Welch DBN inversion (DBNI) algorithm to generate optimal facial parameters from audio given the trained AVAM under maximum likelihood (ML) criterion. Extensive objective and subjective evaluations on the JEWEL audio-visual dataset demonstrate that compared with phonemic HMM approaches, facial parameters estimated by our approach follow the true parameters more accurately, and the synthesized facial animation sequences are so lively that 38% of them are undistinguishable.
C1 City Univ Hong Kong, Sch Creat Media, Hong Kong, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Xie, L (corresponding author), City Univ Hong Kong, Sch Creat Media, Hong Kong, Hong Kong, Peoples R China.
EM xielei21st@gmail.com; zq.liu@cityu.edu.hk
RI Xie, Lei/JWO-8567-2024; Liu, Zhiqiang/GPX-4293-2022
OI Liu, Zhiqiang/0000-0003-2982-8381
CR [Anonymous], 2002, CAMBRIDGE U ENG DEP
   [Anonymous], 1999, AUDITORYVISUAL SPEEC
   BAUM LE, 1968, PAC J MATH, V27, P211, DOI 10.2140/pjm.1968.27.211
   BILMES JA, 2001, JHU 2001 SUMMM WORKS
   Brand M, 1999, COMP GRAPH, P21, DOI 10.1145/311535.311537
   BREGLER C, 1997, P ACM SIGGRAPH 97
   CAO Y, 2004, EUROGRAPH ACM SIGGRA, P347
   Chen TH, 2001, IEEE SIGNAL PROC MAG, V18, P9
   Choi KH, 2001, J VLSI SIG PROC SYST, V29, P51, DOI 10.1023/A:1011171430700
   Cohen M. M., 1993, Models and Techniques in Computer Animation, P139
   Cosatto E, 2003, P IEEE, V91, P1406, DOI 10.1109/JPROC.2003.817141
   Cosatto E, 1998, COMP ANIM CONF PROC, P103, DOI 10.1109/CA.1998.681914
   Cosatto E, 2000, IEEE T MULTIMEDIA, V2, P152, DOI 10.1109/6046.865480
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Eide E, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P708
   Ezzat T, 2002, ACM T GRAPHIC, V21, P388, DOI 10.1145/566570.566594
   Fu SL, 2005, IEEE T MULTIMEDIA, V7, P243, DOI 10.1109/TMM.2005.843341
   Goldsmith JohnA., 1990, AUTOSEGMENTAL METRIC
   Hong PY, 2002, IEEE T NEURAL NETWOR, V13, P916, DOI 10.1109/TNN.2002.1021892
   Jensen F. V., 2007, Bayesian networks and decision graphs
   KAEHLER K, 2002, P 2002 ACM SIGGRAPH, P55
   Kirchhoff K., 1998, P ICSLP, P891
   Kyoung Ho Choi, 1999, 1999 IEEE Third Workshop on Multimedia Signal Processing (Cat. No.99TH8451), P175, DOI 10.1109/MMSP.1999.793816
   MOON SY, 1995, INT CONF ACOUST SPEE, P145, DOI 10.1109/ICASSP.1995.479385
   Murphy Kevin Patrick, 2002, Dynamic bayesian networks: representation, inference and learning
   Ostermann J, 2004, INT C PATT RECOG, P826, DOI 10.1109/ICPR.2004.1334656
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   PIGHIN F, 1998, P ACM SIGGRAPH 98, V3, P75
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   Richardson M, 2003, SPEECH COMMUN, V41, P511, DOI 10.1016/S0167-6393(03)00031-1
   XIE L, 2005, 0511 RCMT
   XIE L, 2004, THESIS NW POLYTECHNI
   Yamamoto E, 1998, SPEECH COMMUN, V26, P105, DOI 10.1016/S0167-6393(98)00054-5
   Zweig G, 2003, COMPUT SPEECH LANG, V17, P173, DOI 10.1016/S0885-2308(03)00007-X
NR 34
TC 54
Z9 65
U1 2
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2007
VL 9
IS 3
BP 500
EP 510
DI 10.1109/TMM.2006.888009
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148PW
UT WOS:000245088100006
DA 2024-07-18
ER

PT J
AU Kao, CF
   Lee, CN
AF Kao, Chi-Feng
   Lee, Chung-Nan
TI Aggregate profit-based caching replacement algorithms for streaming
   media transcoding proxy systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE computer networks; multimedia communication; multimedia streaming;
   prefix caching; proxy caching; streaming media distribution
ID VIDEO DELIVERY
AB This work derives a generalized video object profit function from the extended weighted transcoding graph to calculate the individual cache profit of certain versions of a video object, and the aggregate profit from caching multiple versions of the same video object. This proposed function takes into account the popularity of certain versions of an object, the transcoding delay among versions, and the average duration of access of each version. Based on the profit function, cache-replacement algorithms are proposed to reduce the startup delay and network traffic by efficiently caching video objects with the most profits. Two kinds of simulations were conducted to evaluate the performance of the proposed algorithms. These simulations exploit partial viewing traces and complete viewing traces, separately. The results demonstrate that the proposed algorithms outperform the competing algorithms by 15%-39% in delay saving ratio and 5%-10% in byte-hit ratio.
C1 Natl Sun Yat Sen Univ, Dept Comp Sci & Engn, Kaohsiung, Taiwan.
C3 National Sun Yat Sen University
RP Kao, CF (corresponding author), Natl Sun Yat Sen Univ, Dept Comp Sci & Engn, Kaohsiung, Taiwan.
EM m9034617@student.nsysu.edu.tw; cnlee@mail.cse.nsysu.edu.tw
CR Acharya S, 1999, PROC INT CONF DATA, P40, DOI 10.1109/ICDE.1999.754896
   Cardellini V., 2000, Proceedings of the Ninth International Conference on Information and Knowledge Management. CIKM 2000, P520, DOI 10.1145/354756.354861
   Chandra S, 2000, IEEE J SEL AREA COMM, V18, P2544, DOI 10.1109/49.898736
   Chang CY, 2003, IEEE T PARALL DISTR, V14, P611, DOI 10.1109/TPDS.2003.1206507
   Chang CY, 2002, PROC INT CONF DATA, P383, DOI 10.1109/ICDE.2002.994752
   CHESIRE M, 2001, P USITS 2001 SAN FRA
   Lee SJ, 2002, COMPUT COMMUN, V25, P424, DOI 10.1016/S0140-3664(01)00414-5
   Ma WH, 2004, IEEE T MULTIMEDIA, V6, P599, DOI 10.1109/TMM.2004.830819
   Ma WH, 2002, IEEE T MULTIMEDIA, V4, P539, DOI 10.1109/TMM.2002.806536
   Maheshwari A, 2002, PR GR LAK SYMP VLSI, P50, DOI 10.1109/RIDE.2002.995098
   MAHESHWARI A, 2002, P IEEE RIDE 2002 SAN
   Miao ZR, 2002, IEEE J SEL AREA COMM, V20, P1315, DOI 10.1109/JSAC.2002.802061
   Mohan R, 1999, IEEE T MULTIMEDIA, V1, P104, DOI 10.1109/6046.748175
   Rejaie R., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P980, DOI 10.1109/INFCOM.2000.832273
   Sen S, 1999, IEEE INFOCOM SER, P1310, DOI 10.1109/INFCOM.1999.752149
   Shen B, 2004, IEEE T MULTIMEDIA, V6, P375, DOI 10.1109/TMM.2003.822791
   Tang XY, 2002, PROC INT CONF PARAL, P287, DOI 10.1109/ICPP.2002.1040884
   Verscheure O, 2002, COMPUT COMMUN, V25, P413, DOI 10.1016/S0140-3664(01)00413-3
   Vetro A, 2003, IEEE SIGNAL PROC MAG, V20, P18, DOI 10.1109/MSP.2003.1184336
   Wang YW, 1998, IEEE INFOCOM SER, P660, DOI 10.1109/INFCOM.1998.665087
   Zhang ZL, 2000, IEEE ACM T NETWORK, V8, P429, DOI 10.1109/90.865072
NR 21
TC 23
Z9 28
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2007
VL 9
IS 2
BP 221
EP 230
DI 10.1109/TMM.2006.886259
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 135CO
UT WOS:000244131900002
DA 2024-07-18
ER

PT J
AU Etemadi, F
   Jafarkhani, H
AF Etemadi, Farzad
   Jafarkhani, Hamid
TI An efficient progressive bitstream transmission system for hybrid
   channels with memory
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE finite-state channels; joint source-channel coding; progressive
   transmission; packet erasures; unequal protection
ID ERROR PROTECTION; IMAGE TRANSMISSION; PERFORMANCE
AB We consider progressive transmission over a hybrid channel introducing bit errors and packet erasures. The existing solutions are analyzed and extended to the case of a channel that exhibits memory on both bit errors and packet erasures. We then propose a simple, low-complexity coding scheme that transforms the hybrid channel into a channel with a single impairment for which various optimization techniques exist. Both rate-based and distortion-based optimization problems are investigated. It is shown that our proposed solution has lower channel coding and rate-distortion optimization complexities compared to the known solutions. Simulation results for channels with and without memory show the effectiveness of our proposed solution over a wide range of operating conditions. Numerical results also indicate that the rate-based solution of our proposed algorithm is very close to the corresponding distortion-based solution.
C1 Univ Calif Irvine, Dept Elect Engn & Comp Sci, Irvine, CA 92697 USA.
C3 University of California System; University of California Irvine
RP Etemadi, F (corresponding author), Univ Calif Irvine, Dept Elect Engn & Comp Sci, Irvine, CA 92697 USA.
EM fetemadi@uci.edu; hamidj@uci.edu
OI Jafarkhani, Hamid/0000-0001-6838-8038
CR [Anonymous], 1954, Analysis
   APPADWEDULA A, 1998, P IEEE INT C COMM IC
   Banister BA, 2002, IEEE SIGNAL PROC LET, V9, P117, DOI 10.1109/97.1001646
   Chande V, 2000, IEEE J SEL AREA COMM, V18, P850, DOI 10.1109/49.848239
   CHANDE V, 1999, P IEEE INT C IM PROC
   Cosman PC, 2000, IEEE T IMAGE PROCESS, V9, P982, DOI 10.1109/83.846241
   Dumitrescu S, 2004, IEEE T MULTIMEDIA, V6, P230, DOI 10.1109/TMM.2003.822793
   Etemadi F, 2005, IEEE SIGNAL PROC LET, V12, P365, DOI 10.1109/LSP.2005.843768
   ETEMADI F, 2005, P ICIP 2005 GEN IT S
   Hamzaoui R, 2005, IEEE SIGNAL PROC MAG, V22, P91, DOI 10.1109/MSP.2005.1550192
   HAMZAOUI R, 2002, P DAT COMPR C DCC 20
   Kim BJ, 2000, IEEE T CIRC SYST VID, V10, P1374, DOI 10.1109/76.889025
   LI W, 2001, IEEE T CIRCUITS SYST, V6, P301
   Nosratinia A, 2003, IEEE T COMMUN, V51, P186, DOI 10.1109/TCOMM.2003.809256
   SACHS DG, 2000, P SPIE SAN JOS CA JA
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   SHAPIRO JM, 1993, IEEE T SIGNAL PROCES, V41, P3445, DOI 10.1109/78.258085
   Sherwood PG, 1997, IEEE SIGNAL PROC LET, V4, P189, DOI 10.1109/97.596882
   Sherwood PG, 1998, IEEE T COMMUN, V46, P1555, DOI 10.1109/26.737389
   Simon M. K., 2000, WILEY S TEL
   Stankovic V, 2004, IEEE T MULTIMEDIA, V6, P240, DOI 10.1109/TMM.2003.822789
   Stankovic V, 2003, IEEE T COMMUN, V51, P1788, DOI 10.1109/TCOMM.2003.819235
   STANKOVIC V, 2003, IEEE J SEL AREA COMM, V51, P1526
   Stankovic VM, 2004, IEEE T CIRC SYST VID, V14, P1064, DOI 10.1109/TCSVT.2004.831964
   Stockhammer T., 2001, P 11 PACK VID WORKSH
   Taubman D, 2000, IEEE T IMAGE PROCESS, V9, P1158, DOI 10.1109/83.847830
   Wicker S. B., 1995, Error Control Systems for Digital Communication and Storage, Englewood Cliffs, V1st
   YEE JR, 1995, IEEE T COMMUN, V43, P2316, DOI 10.1109/26.403764
   YOUSEFIZADEH H, 2004, P IEEE DAT COMPR C D
NR 29
TC 6
Z9 8
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2006
VL 8
IS 6
BP 1291
EP 1298
DI 10.1109/TMM.2006.884606
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109MG
UT WOS:000242311700017
OA Green Published
DA 2024-07-18
ER

PT J
AU Bolchini, D
   Paolini, P
AF Bolchini, D
   Paolini, P
TI Interactive dialogue model: A design technique for multichannel
   applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE design methodology; interactive systems; user centered design; user
   interfaces
AB Multichannel applications deliver the same content and a "similar interactive experience" using different devices and different technologies (e.g., web sites, palm held devices, car navigators, or interactive TVs). Various channels imply a number of differences, including screen (size), keyboard (size), pointing devices, output devices, performances, and the context of use (standing, sitting, walking, etc.). In most cases, today, applications for different channels are designed and implemented almost "independently," with ineffectiveness for the developers (high costs) and ineffectiveness for the users (loss of consistency across the different channels and the perception that they are "different applications"). This paper presents an interactive dialogue model (IDM), a novel design model specifically tailored for multichannel applications. The background research, moving from linguistic theories and practices, has led us to the development of a "channel-independent" design model (based on dialogue primitives). Design can start in a "conceptual," channel-independent fashion, and then proceed into a further "logical" design oriented toward specific channels of communication. Designing an interactive application in two steps (channel-independent first, and channel-dependent later) allows a number of advantages without making more cumbersome the overall design process. Beside the emphasis on multichannel, IDM has additional distinctive features: it is lightweight, providing a few set of primitives (and a simple graphic notation) which are easy to learn and teach. Moreover, it is suitable for brainstorming and generating ideas at early stage during design (or during the shift from requirements to design); finally, it is cost-effective (it requires little effort from designers) and modular (designers can take the part they wish, not being forced to "all or nothing").
   IDM has been validated both in the academic and industry environments, providing excellent results so far.
C1 Univ Lugano, Fac Commun SCI, TEC Lab, CH-6900 Lugano TI, Switzerland.
   Politecn Milan, Dept Elect & Informat, HOC, I-20120 Milan, Italy.
C3 Universita della Svizzera Italiana; Polytechnic University of Milan
RP Bolchini, D (corresponding author), Univ Lugano, Fac Commun SCI, TEC Lab, CH-6900 Lugano TI, Switzerland.
EM davide.bolchini@lu.unisi.ch; paolo.paolini@polimi.it
OI Bolchini, Davide/0000-0001-9870-5858
CR [Anonymous], 1997, A Theory of Computer Semiotics: Semiotic Approaches to Construction and Assessment of Computer Systems
   BOLCHINI D, 2004, REQUIREMENTS ENG J, P85
   BOLCHINI D, 2004, 105211102061 FNSRS
   Ceri S., 2003, Designing data-intensive Web applications
   Conallen Jim., 2003, BUILDING WEB APPL UM
   *DELOS, IST2002507618 N
   DETROYER O, 1997, P 7 INT WORLD WID WE
   DIBLAS P, 2004, UI4ALL C US INT ALL
   DIBLAS P, 2004, MUS WEB 2004 C ARL T
   Dix A., 2002, HUMAN COMPUTER INTER
   *EPOCH, IST2002507382 N
   GARZOTTO F, 1993, ACM T INFORM SYST, V11, P1, DOI 10.1145/151480.151483
   GOMEZ J, 2001, IEEE MULTIMEDIA, V8
   HEWETT T, DIALOGUE TECHNIQUES
   LANGE DB, 1996, J ORG COMPUT ELECT C, V6
   *MAIS PROJ, MULT AD INF SYST PRO
   NANARD M, 2003, P HYPERTEXT 03 C
   ROCCI A, 2004, 105211102061 FNSRS
   ROSSI G, 2001, P WWW10          MAY
   *UWA CONS, IST200025131 UWA
   WOUKEU A, 2003, ECSTRIAM03002
   [No title captured]
NR 22
TC 27
Z9 31
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2006
VL 8
IS 3
BP 529
EP 541
DI 10.1109/TMM.2006.870733
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046OX
UT WOS:000237822000010
DA 2024-07-18
ER

PT J
AU Huang, JC
   Liu, Z
   Wang, Y
AF Huang, JC
   Liu, Z
   Wang, Y
TI Joint scene classification and segmentation based on hidden Markov model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE dynamic programming; Hidden Markov model; video analysis; video scene
   classification; video scene segmentation; video understanding
AB Scene classification and segmentation are fundamental steps for efficient accessing, retrieving and browsing large amount of video data. We have developed a scene classification scheme using a Hidden Markov Model (HMM)-based classifier. By utilizing the temporal behaviors of different scene classes, HMM classifier can effectively classify presegmented clips into one of the predefined scene classes. In this paper, we describe three approaches for joint classification and segmentation based on HMM, which search for the most likely class transition path by using the dynamic programming technique. All these approaches utilize audio and visual information simultaneously. The first two approaches search optimal scene class transition based on the likelihood values computed for short video segment belonging to a particular class but with different search constrains. The third approach searches the optimal path in a super HMM by concatenating HMM's for different scene classes.
C1 Lexmark Int Inc, Lexington, KY 40513 USA.
   AT&T Lab Res, Middletown, NJ 07748 USA.
   Polytech Univ, Dept Elect Engn, Brooklyn, NY 11201 USA.
C3 Lexmark International, Inc.; AT&T; New York University
RP Lexmark Int Inc, Lexington, KY 40513 USA.
EM jhuang@lexmark.com; zliu@research.att.com; yao@poly.edu
RI Huang, Junchi/C-2601-2019
OI Wang, Yao/0000-0003-3199-3802
CR Boreczky JS, 1998, INT CONF ACOUST SPEE, P3741, DOI 10.1109/ICASSP.1998.679697
   Chang P, 2002, IEEE IMAGE PROC, P609
   HAUPTMANN AG, 1998, ADV DIG LIBR C ADL 9
   HUANG J, 1999, IEEE WORKSH MULT SIG, P53, DOI DOI 10.1109/MMSP.1999.793797
   Huang JC, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P526, DOI 10.1109/ICIP.1998.727252
   Lienhart R, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P685, DOI 10.1109/MMCS.1999.779282
   Liu Z, 1998, J VLSI SIG PROC SYST, V20, P61, DOI 10.1023/A:1008066223044
   Liu Z, 1998, 1998 IEEE SECOND WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P27, DOI 10.1109/MMSP.1998.738908
   Ney H, 1999, IEEE SIGNAL PROC MAG, V16, P64, DOI 10.1109/79.790984
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Saraceno C, 1997, INT CONF ACOUST SPEE, P2597, DOI 10.1109/ICASSP.1997.595320
   SARACENO C, 1998, P INT C IM PROC CHIC, V1, P363
   Wang Y, 2000, IEEE SIGNAL PROC MAG, V17, P12, DOI 10.1109/79.888862
NR 14
TC 35
Z9 40
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2005
VL 7
IS 3
BP 538
EP 550
DI 10.1109/TMM.2005.843346
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 949OO
UT WOS:000230798200016
DA 2024-07-18
ER

PT J
AU Zhu, BB
   Yuan, C
   Wang, YD
   Li, SP
AF Zhu, BB
   Yuan, C
   Wang, YD
   Li, SP
TI Scalable protection for MPEG-4 fine granularity scalability
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE digital rights management; FGS; fine granularity scalability (FGS);
   layered access control; MPEG-4; multimedia protection; scalable
   protection; scalable video encryption; selective encryption; video
   encryption
ID ENCRYPTION
AB The newly adopted MPEG-4 fine granularity scalability (FGS) video coding standard offers easy and flexible adaptation to varying network bandwidths and different application needs. Encryption for FGS should preserve such adaptation capabilities and enable intermediate stages to process encrypted data directly without decryption. In this paper, we propose two novel encryption algorithms for MPEG-4 FGS that meet these requirements. The first algorithm encrypts an FGS stream (containing both the base and the enhancement layers) into a single access layer and preserves the original fine granularity scalability and error resilience performance in an encrypted stream. The second algorithm encrypts an FGS stream into multiple quality layers divided according to either peak signal-to-noise ratio (PSNR) or bit rates, with lower quality layers being accessible and reusable by a higher quality layer of the same type, but not vice versa. Both PSNR and bit-rate layers are supported simultaneously so a layer of either type can be selected on the fly without decryption. The base layer for the second algorithm may be unencrypted to allow free view of the content at low-quality or content-based search of a video database without decryption. Both algorithms are fast, error-resilient, and have negligible compression overhead. The same approach can be applied to other scalable multimedia formats.
C1 Microsoft Res Asia, Beijing 100080, Peoples R China.
   Tsinghua Univ, Dept Comp Sci, Beijing 100084, Peoples R China.
   Beijing Univ, Dept Comp Sci & Technol, Beijing 100871, Peoples R China.
C3 Microsoft; Microsoft Research Asia; Tsinghua University; Peking
   University
RP Microsoft Res Asia, 3F Sigma, Beijing 100080, Peoples R China.
EM binzhu@microsoft.com; Chun.Yuan@inria.fr; wangyidong@icst.pku.edu.cn;
   spli@microsoft.com
RI Li, Shipeng/AAA-3374-2020
OI Li, Shipeng/0000-0001-5368-4256
CR Agi I., 1996, Proceedings of the Symposium on Network and Distributed System Security, P137, DOI 10.1109/NDSS.1996.492420
   Changgui Shi, 1998, Proceedings ACM Multimedia 98, P81
   Cheng H, 2000, IEEE T SIGNAL PROCES, V48, P2439, DOI 10.1109/78.852023
   Eskicioglu AM, 2003, SIGNAL PROCESS-IMAGE, V18, P237, DOI 10.1016/S0923-5965(02)00143-1
   GROSBOIS R, 2001, P SPIE 46 ANN M APPL, V24
   *ISO IEC, 1449612001AMD3 ISOIE
   *ISO IEC JTC 1SC 2, N5068 ISOIEC JTC 1SC
   *ISO IEC JTC1 SC29, 2000, N3515 ISOIEC JTC1SC2
   JAKUBOWSKI MH, 1998, P EUROCRYPT 98, P281
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   MACQ BM, 1995, P IEEE, V83, P944, DOI 10.1109/5.387094
   Maples T., 1995, P 4 INT C COMP COMM
   Meyer J., 1995, SECURITY MECH MULTIM
   *MICR, ARCH WIND MED RIGHTS
   Qiao L., 1997, Las Vegas : Proceedings of the 1s International Conference on Imaging Science, Systems and Technology, P21
   Qiao L, 1998, INT J COMPUT GRAPH, V22
   Schneier B., 1996, Applied Cryptography: Protocols, Algorithms, and Source Code in C
   Shi CG, 1998, SYM REL DIST SYST, P381, DOI 10.1109/RELDIS.1998.740527
   Tang L., 1996, P 4 ACM INT MULT C A, P219
   Wee SJ, 2001, IEEE IMAGE PROC, P437, DOI 10.1109/ICIP.2001.959047
   Wen JT, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P435, DOI 10.1109/MMSP.2001.962772
   Wen JT, 2002, IEEE T CIRC SYST VID, V12, P545, DOI 10.1109/TCSVT.2002.800321
   WU CP, 2001, P SPIE SEC WAT MUL 3, V4314
   Yuan C, 2003, IEEE IMAGE PROC, P517
   Yuan C, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL II, P620
   ZENG W, 2002, P IEEE INT C IM PROC, V3, P169
   Zeng WJ, 2003, IEEE T MULTIMEDIA, V5, P118, DOI 10.1109/TMM.2003.808817
   Zeng WJ, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P285, DOI 10.1145/319463.319627
   [No title captured]
NR 29
TC 30
Z9 38
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2005
VL 7
IS 2
BP 222
EP 233
DI 10.1109/TMM.2005.843340
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 909OO
UT WOS:000227869400004
DA 2024-07-18
ER

PT J
AU Hanjalic, A
   Xu, LQ
AF Hanjalic, A
   Xu, LQ
TI Affective video content representation and modeling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE affective video content analysis; video abstraction; video content
   modeling; video content representation; video highlights extraction
ID EMOTION
AB This paper looks into a new direction in video content analysis - the representation and modeling of affective video content. The affective content of a given video clip can be defined as the intensity and type of feeling or emotion (both are referred to as affect) that are expected to arise in the user while watching that clip. The availability of methodologies for automatically extracting this type of video content will extend the current scope of possibilities for video indexing and retrieval. For instance, we will be able to search for the funniest or the most thrilling parts of a movie, or the most exciting events of a sport program. Furthermore, as the user may want to select a movie not only based on its genre, cast, director and story content, but also on its prevailing mood, the affective content analysis is also likely to contribute to enhancing the quality of personalizing the video delivery to the user. We propose in this paper a computational framework for affective video content representation and modeling. This framework is based on the dimensional approach to affect that is known from the field of psychophysiology. According to this approach, the affective video content can be represented as a set of points in the two-dimensional (2-D) emotion space that is characterized by the dimensions of arousal (intensity of affect) and valence (type of affect). We map the affective video content onto the 2-D emotion space by using the models that link the arousal and valence dimensions to low-level features extracted from video data. This results in the arousal and valence time curves that, either considered separately or combined into the so-called affect curve, are introduced as reliable representations of expected transitions from one feeling to another along a video, as perceived by a viewer.
C1 Delft Univ Technol, Dept Mediamat, NL-2628 CD Delft, Netherlands.
   Martlesham Hlth, BT Res Venturing, Broadband Appl Res Ctr, Ipswich IP5 3RE, Suffolk, England.
C3 Delft University of Technology
RP Delft Univ Technol, Dept Mediamat, NL-2628 CD Delft, Netherlands.
EM A.Hanjalic@ewi.tudelft.nl; li-qun.xu@bt.com
OI Hanjalic, Alan/0000-0002-5771-2549
CR Adams B, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P283, DOI 10.1109/ICIP.2000.899358
   [Anonymous], IMAGE VIDEO DATABASE
   Arnheim R., 1958, Film As Art
   BABAGUCHI N, 2000, P IEEE ICME, V3, P1519
   BORDWELL D, 2003, FILM ART INTRO FILM
   Bradley M., 1994, EMOTIONS ESSAYS EMOT
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   BRADLEY MM, 1991, INT AFFECTIVE DIGITI
   DELBIMBO A, 1999, VISUAL INFORMATION R
   Detenber B. H., 1997, J BROADCAST ELECTRON, V21, P112
   Dietz R., 1999, P COGN TECHN C SAN F
   FITZGIBBONS L, 1992, PSYCHOPHYSIOLOGY, V29, P613
   GIARNETTI LD, 1976, UNDERSTANDING MOVIES
   Greenwald M. K., 1989, Journal of Psychophysiology, V3, P51
   HOPKINS R, MEASURING PSYCHOL RE, P113
   Jaimes A, 2002, IEEE IMAGE PROC, P133
   Lang A, 1996, J BROADCAST ELECTRON, V40, P460
   Lang A., 1995, J BROADCAST ELECTRON, V38, P1
   Lang P.J., 1988, INT AFFECTIVE PICTUR
   LANG PJ, 1995, ADV SOCIAL COGNITION, V6
   Lew M.S., 2001, PRINCIPLES VISUAL IN
   Li BX, 2002, P SOC PHOTO-OPT INS, V4676, P202
   MURRAY IR, 1993, J ACOUST SOC AM, V93, P1097, DOI 10.1121/1.405558
   Osgood C. E., 1957, The measurement of meaning
   Picard R. W., 1997, AFFECTIVE COMPUTING
   Picard RW, 1997, BT TECHNOL J, V15, P150, DOI 10.1023/A:1018643815520
   ROMER D, 1995, SEM MIT MED LAB CAMB
   RUSSELL JA, 1977, J RES PERS, V11, P273, DOI 10.1016/0092-6566(77)90037-X
   Simons RF, 1999, PSYCHOPHYSIOLOGY, V36, P619, DOI 10.1111/1469-8986.3650619
NR 29
TC 350
Z9 410
U1 3
U2 42
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2005
VL 7
IS 1
BP 143
EP 154
DI 10.1109/TMM.2004.840618
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 893CQ
UT WOS:000226697300014
OA Green Published
DA 2024-07-18
ER

EF