FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Zhang, CJ
   Cheng, J
   Tian, Q
AF Zhang, Chunjie
   Cheng, Jian
   Tian, Qi
TI Unsupervised and Semi-Supervised Image Classification With Weak Semantic
   Consistency
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semi-supervised classification; unsupervised classification; weak
   semantic representation; semantic consistency
ID LABEL PROPAGATION; LOW-RANK; REPRESENTATION
AB Supervised methods have been widely used for image classifications. Although great progress has been made, existing supervised methods rely on well-labeled samples for classification. However, we often have large quantities of images with few or no labels. To cope with this problem, in this paper, we propose a novel weak semantic consistency constrained image classification method. We start from an extreme circumstance by viewing each image as one class. We train exemplar classifiers to separate each image from other images. For each image, we use the learned exemplar classifiers to predict the weak semantic correlations with the exemplar classifiers. When no labeled information is available, we cluster images using the weak semantic correlations and assign images within one cluster to the same mid-level class. When partially labeled images are available, we can use them to constrain the clustering process by assigning images of varied semantics to different mid-level classes. We use the newly assigned images for classifier training and new image representations, which can then be used for similar image assignments. The classifier training, image representation, and assignment processes are repeated until convergence. We conduct both unsupervised and semi-supervised image classification experiments on several datasets. The experimental results show the effectiveness of the proposed unsupervised and semi-supervised weak semantic consistency image classification method.
C1 [Zhang, Chunjie] Chinese Acad Sci, Inst Automat, Res Ctr Brain Inspired Intelligence, Beijing 100190, Peoples R China.
   [Cheng, Jian] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Cheng, Jian] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing 100190, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; Institute of Automation, CAS; Chinese Academy of
   Sciences; University of Texas System; University of Texas at San Antonio
   (UTSA)
RP Zhang, CJ (corresponding author), Chinese Acad Sci, Inst Automat, Res Ctr Brain Inspired Intelligence, Beijing 100190, Peoples R China.
EM chunjie.zhang@ia.ac.cn; jcheng@nlpr.ia.ac.cn; qitian@cs.utsa.edu
RI Gu, Bingxin/JNS-4761-2023; , chengjian/KGL-5551-2024
OI Gu, Bingxin/0009-0005-5667-1430; , chengjian/0000-0003-1289-2758; zhang,
   chunjie/0000-0002-1161-8995
FU National Science Foundation of China [61872362, 61876135]
FX This work was supported by the National Science Foundation of China
   under Grants 61872362 and 61876135.
CR Agrawal P, 2015, IEEE I CONF COMP VIS, P37, DOI 10.1109/ICCV.2015.13
   [Anonymous], ARXIV171106020
   [Anonymous], CNSTR2007001 CALTECH
   [Anonymous], 2007, P ASS COMP MACH MULT
   [Anonymous], 2016, ARXIV160308511
   [Anonymous], ARXIV170106264
   [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], 2018, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2017.2723882
   [Anonymous], 2015, ARXIV150602351
   [Anonymous], P INT C COMP VIS
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], P COMP VIS PATT REC
   [Anonymous], 2014, ABS14053531 CORR
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2014, P 31 INT C INT C MAC
   [Anonymous], ARXIV190104596V1
   [Anonymous], 2009, P 17 ACM INT C MULT
   [Anonymous], 2012, PROC 5 ACM INT C WEB
   [Anonymous], Learning from labeled and unlabeled data with label propagation
   [Anonymous], 2017, ARXIV170400028CSLG
   Arjovsky M., 2017, ARXIV170107875
   Bojanowski P., 2017, INT C MACH LEARN
   Bulò SR, 2014, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2014.18
   Cai X, 2013, IEEE I CONF COMP VIS, P1737, DOI 10.1109/ICCV.2013.218
   Chang SY, 2014, IEEE DATA MINING, P60, DOI 10.1109/ICDM.2014.115
   Chang XJ, 2017, IEEE T CYBERNETICS, V47, P1180, DOI 10.1109/TCYB.2016.2539546
   Dai DX, 2013, IEEE I CONF COMP VIS, P2072, DOI 10.1109/ICCV.2013.259
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Donahue J., 2016, ARXIV160509782
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Gong C, 2019, IEEE T CYBERNETICS, V49, P388, DOI 10.1109/TCYB.2017.2773562
   Gong C, 2018, IEEE T CYBERNETICS, V48, P967, DOI 10.1109/TCYB.2017.2669639
   Gong C, 2017, IEEE T NEUR NET LEAR, V28, P1452, DOI 10.1109/TNNLS.2016.2514360
   Gong C, 2016, IEEE T IMAGE PROCESS, V25, P3249, DOI 10.1109/TIP.2016.2563981
   Gong C, 2015, IEEE T NEUR NET LEAR, V26, P2148, DOI 10.1109/TNNLS.2014.2376963
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Han J, 2015, IEEE T IMAGE PROCESS, V24, P5177, DOI 10.1109/TIP.2015.2447735
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang C, 2017, IEEE T MULTIMEDIA, V19, P673, DOI 10.1109/TMM.2016.2631122
   Hui Z, 2017, IEEE T PATTERN ANAL, V39, P2060, DOI 10.1109/TPAMI.2016.2623613
   Karasuyama M, 2013, IEEE T NEUR NET LEAR, V24, P1999, DOI 10.1109/TNNLS.2013.2271327
   Karpathy A, 2017, IEEE T PATTERN ANAL, V39, P664, DOI 10.1109/TPAMI.2016.2598339
   Kwitt Roland, 2012, Computer Vision - ECCV 2012. Proceedings of the 12th European Conference on Computer Vision, P359, DOI 10.1007/978-3-642-33765-9_26
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li LJ, 2007, LECT NOTES ARTIF INT, V4456, P1
   Li S, 2016, IEEE IJCNN, P1795, DOI 10.1109/IJCNN.2016.7727417
   Li W, 2018, IEEE T PATTERN ANAL, V40, P1114, DOI 10.1109/TPAMI.2017.2704624
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Radford A., 2015, ARXIV151106434
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Shi CJ, 2017, IEEE T CIRC SYST VID, V27, P1947, DOI 10.1109/TCSVT.2016.2576919
   Simonyan K., 2014, 14091556 ARXIV
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang J., 2007, P 15 ACM INT C MULT, P297
   Wang JD, 2012, INFORM RETRIEVAL, V15, P278, DOI 10.1007/s10791-012-9193-0
   Wang M, 2009, COMPUT VIS IMAGE UND, V113, P384, DOI 10.1016/j.cviu.2008.08.003
   Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320
   Wu J, 2017, IEEE T MULTIMEDIA, V19, P1156, DOI 10.1109/TMM.2017.2652065
   Yang HF, 2018, IEEE T PATTERN ANAL, V40, P437, DOI 10.1109/TPAMI.2017.2666812
   Yu XD, 2010, LECT NOTES COMPUT SC, V6315, P127
   Zhang CJ, 2020, IEEE T CYBERNETICS, V50, P2038, DOI 10.1109/TCYB.2018.2875728
   Zhang CJ, 2018, INFORM SCIENCES, V460, P115, DOI 10.1016/j.ins.2018.05.048
   Zhang CJ, 2019, IEEE T CYBERNETICS, V49, P3834, DOI 10.1109/TCYB.2018.2845912
   Zhang CJ, 2018, IEEE T NEUR NET LEAR, V29, P4528, DOI 10.1109/TNNLS.2017.2757497
   Zhang CJ, 2018, IEEE T NEUR NET LEAR, V29, P4479, DOI 10.1109/TNNLS.2017.2748952
   Zhang CJ, 2018, IEEE T MULTIMEDIA, V20, P903, DOI 10.1109/TMM.2017.2759500
   Zhang CJ, 2018, IEEE T CIRC SYST VID, V28, P428, DOI 10.1109/TCSVT.2016.2613125
   Zhang CJ, 2018, IEEE T CIRC SYST VID, V28, P1719, DOI 10.1109/TCSVT.2017.2694060
   Zhang CJ, 2018, IEEE T NEUR NET LEAR, V29, P3442, DOI 10.1109/TNNLS.2017.2728060
   Zhang CJ, 2018, IEEE T CYBERNETICS, V48, P2012, DOI 10.1109/TCYB.2017.2726079
   Zhang CJ, 2018, INFORM SCIENCES, V422, P271, DOI 10.1016/j.ins.2017.09.024
   Zhang CJ, 2017, IEEE T CIRC SYST VID, V27, P1691, DOI 10.1109/TCSVT.2016.2527380
   Zhang CJ, 2017, NEUROCOMPUTING, V257, P88, DOI 10.1016/j.neucom.2016.11.065
   Zhang CJ, 2017, INFORM SCIENCES, V376, P125, DOI 10.1016/j.ins.2016.10.019
   Zhang CJ, 2016, INFORM SCIENCES, V369, P160, DOI 10.1016/j.ins.2016.06.029
   Zhang CJ, 2014, COMPUT VIS IMAGE UND, V123, P14, DOI 10.1016/j.cviu.2014.02.013
   Zhang CJ, 2013, PATTERN RECOGN LETT, V34, P1046, DOI 10.1016/j.patrec.2013.02.013
   Zhang CJ, 2011, PROC CVPR IEEE, P1673, DOI 10.1109/CVPR.2011.5995484
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zhu FD, 2017, IEEE T IMAGE PROCESS, V26, P3542, DOI 10.1109/TIP.2017.2703099
NR 87
TC 40
Z9 42
U1 1
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2019
VL 21
IS 10
BP 2482
EP 2491
DI 10.1109/TMM.2019.2903628
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JD1IM
UT WOS:000489728400005
DA 2024-07-18
ER

PT J
AU Xu, WJ
   Keshmiri, S
   Wang, GH
AF Xu, Wenju
   Keshmiri, Shawn
   Wang, Guanghui
TI Adversarially Approximated Autoencoder for Image Generation and
   Manipulation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Autoencoder; generative adversarial network; adversarial approximation;
   faithful reconstruction; latent manifold structure
AB Regularized autoencoders learn the latent codes, a structure with the regularization under the distribution, which enables them the capability to infer the latent codes given observations and generate new samples given the codes. However, they are sometimes ambiguous as they tend to produce reconstructions that are not necessarily a faithful reproduction of the inputs. The main reason is to enforce the learned latent code distribution to match a prior distribution while the true distribution remains unknown. To improve the reconstruction quality and learn the latent space a manifold structure, this paper presents a novel approach using the adversarially approximated autoencoder (AAAE) to investigate the latent codes with adversarial approximation. Instead of regularizing the latent codes by penalizing on the distance between the distributions of the model and the target, AAAE learns the autoencoder flexibly and approximates the latent space with a simpler generator. The ratio is estimated using a generative adversarial network to enforce the similarity of the distributions. In addition, the image space is regularized with an additional adversarial regularizer. The proposed approach unifies two deep generative models for both latent space inference and diverse generation. The learning scheme is realized without regularization on the latent codes, which also encourages faithful reconstruction. Extensive validation experiments on four real-world datasets demonstrate the superior performance of AAAE. In comparison to the state-of-the-art approaches, AAAE generates samples with better quality and shares the properties of a regularized autoencoder with a nice latent manifold structure.
C1 [Xu, Wenju; Keshmiri, Shawn] Univ Kansas, Dept Aerosp Engn, Lawrence, KS 66045 USA.
   [Wang, Guanghui] Univ Kansas, Dept Elect Engn & Comp Sci, Lawrence, KS 66045 USA.
C3 University of Kansas; University of Kansas
RP Xu, WJ (corresponding author), Univ Kansas, Dept Aerosp Engn, Lawrence, KS 66045 USA.
EM xuwenju@ku.edu; keshmiri@ku.edu; ghwang@ku.edu
RI Wang, Guang/JFS-8374-2023
OI Wang, Guanghui/0000-0003-3182-104X; Xu, Wenju/0000-0003-2740-0357
FU NSF NRI [2019-67021-28996]; USDA NIFA [2019-67021-28996]; NSFC
   [61573351]; Nvidia GPU Grant
FX This work was supported in part by the NSF NRI and USDA NIFA under Award
   2019-67021-28996, in part by the NSFC under Grant 61573351, and in part
   by the Nvidia GPU Grant. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Engin
CR [Anonymous], 2018, P 35 INT C MACH LEAR
   [Anonymous], 2017, PROC NEURAL INF PROC
   [Anonymous], 2017, BEGAN BOUNDARY EQUIL
   [Anonymous], 2017, Advances in neural information processing systems
   [Anonymous], 2017, ARXIV
   [Anonymous], 2017, PROC INT C NEURAL IN
   [Anonymous], 2017, ARXIV170208235
   [Anonymous], 2017, NIPS
   [Anonymous], 2017, P 5 INT C LEARN REPR
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Che T., 2017, P INT C LEARN REPR
   Chen X, 2016, ADV NEUR IN, V29
   Cho CY, 2018, INT SYMP NEXTGEN, P350
   Cui P, 2018, IEEE T MULTIMEDIA, V20, P198, DOI 10.1109/TMM.2017.2724843
   Denton E.L., 2015, CoRR, P1486
   Donahue J, 2017, P 5 INT C LEARNING R
   Dumoulin Vincent, 2016, ARXIV160600704
   Gao JY, 2016, NEUROCOMPUTING, V214, P708, DOI 10.1016/j.neucom.2016.06.055
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gulrajani Ishaan, 2017, P ADV NEUR INF PROC, P5767
   He L, 2018, INT C PATT RECOG, P2504, DOI 10.1109/ICPR.2018.8546170
   He L, 2018, IEEE T IMAGE PROCESS, V27, P4676, DOI 10.1109/TIP.2018.2832296
   Heusel M., 2017, ADV NEURAL INFORM PR, P6626
   Hou XX, 2018, IEEE ACCESS, V6, P49779, DOI 10.1109/ACCESS.2018.2868733
   Hu Z, 2018, P INT C LEARN REPR
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D. P., 2014, arXiv
   Kingma D. P., 2014, P INT C LEARN REPR
   Kingma Diederik P, 2018, P ADV NEUR INF PROC, P10236
   Kingma DP, 2016, 30 C NEURAL INFORM P, V29
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   Ledig C., P CVPR, V2, P4
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Liu LQ, 2016, IEEE T MULTIMEDIA, V18, P64, DOI 10.1109/TMM.2015.2500730
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Ma WC, 2018, INT C PATT RECOG, P2510, DOI 10.1109/ICPR.2018.8545693
   Makhzani A., 2017, P ADV NEURAL INFORM, P1975
   Makhzani A., 2016, P WORKSH TRACK INT C
   Mathieu M. F., 2016, ADV NEUR IN
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Nowozin S., 2017, P 34 INT C MACH LEAR
   Radford A, 2016, 4 INT C LEARNING REP
   Reed S, 2016, P INT C MACH LEARN
   Rezende D., 2015, P INT C MACH LEARN
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Tolstikhin I., 2018, 6 INT C LEARN REPR I
   Tran D, 2017, ADV NEURAL INFORM PR, P5523
   Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Upchurch P, 2017, PROC CVPR IEEE, P6090, DOI 10.1109/CVPR.2017.645
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Voulodimos A, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/7068349
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang L, 2017, IEEE T MULTIMEDIA, V19, P1220, DOI 10.1109/TMM.2016.2646219
   Zhang ZF, 2017, PROC CVPR IEEE, P4352, DOI 10.1109/CVPR.2017.463
   Zhu J.-Y., 2017, IEEE I CONF COMP VIS, P2223, DOI DOI 10.1109/ICCV.2017.244
   Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36
NR 63
TC 49
Z9 53
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2019
VL 21
IS 9
BP 2387
EP 2396
DI 10.1109/TMM.2019.2898777
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IT6VX
UT WOS:000483015200018
OA hybrid, Green Submitted
DA 2024-07-18
ER

PT J
AU Li, Y
   Guo, YQ
   Guo, J
   Ma, Z
   Kong, XW
   Liu, Q
AF Li, Yi
   Guo, Yanqing
   Guo, Jun
   Ma, Zhuang
   Kong, Xiangwei
   Liu, Qian
TI Joint CRF and Locality-Consistent Dictionary Learning for Semantic
   Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image semantic segmentation; conditional random field; dictionary
   learning; locality consistency
ID MULTICLASS OBJECT RECOGNITION; CONDITIONAL RANDOM-FIELDS; SCENE
   CLASSIFICATION; ENERGY MINIMIZATION; K-SVD; IMAGE; REPRESENTATION;
   TEXTONBOOST
AB Semantic image segmentation can be accomplished by assigning a proper object category label to each meaningful region of an image. Beyond the original bottom-up models, the use of top-down categorization information has been applied to semantic segmentation to improve performance. An excellent example of such a top-down scheme is to integrate a Conditional Random Field (CRF) model with sparse dictionary learning. However, the existing solutions merely consider the discrimination of dictionaries to obtain better sparse codes, without considering the inherent data locality characteristics. In this paper, we explore such characteristics and propose a novel semantic segmentation framework based on an innovative CRF model with locality-consistent dictionary learning. In particular, we propose two new locality-consistent dictionary learning strategies by capturing the local consistencies in the feature space and the label space. In addition, we develop a joint dictionary and a CRF model parameter learning algorithm to seamlessly integrate the proposed locality-consistent dictionary learning strategies into the CRF model. Extensive experiments are conducted with two popular data bases of different traits (i.e., Graz-02 and PASCAL-CONTEXT). The simulation results confirm the efficiency of the proposed scheme, especially when training data are limited.
C1 [Li, Yi; Guo, Yanqing; Ma, Zhuang; Kong, Xiangwei] Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Peoples R China.
   [Li, Yi] CASIA, Natl Lab Pattern Recognit, Ctr Res Intelligent Percept & Comp, Beijing 100190, Peoples R China.
   [Guo, Jun] Tsinghua Univ, Tsinghua Berkeley Shenzhen Inst, Shenzhen 518055, Peoples R China.
   [Liu, Qian] Dalian Univ Technol, Dept Comp Sci & Technol, Dalian 116024, Peoples R China.
   [Liu, Qian] Tech Univ Munich, Chair Media Technol, D-80333 Munich, Germany.
   [Liu, Qian] Tech Univ Munich, Chair Commun Networks, D-80333 Munich, Germany.
C3 Dalian University of Technology; Chinese Academy of Sciences; Institute
   of Automation, CAS; Tsinghua University; Tsinghua Shenzhen International
   Graduate School; Dalian University of Technology; Technical University
   of Munich; Technical University of Munich
RP Guo, YQ (corresponding author), Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Peoples R China.
EM liyi@mail.dlut.edu.cn; guoyq@dlut.edu.cn; guoj16@mails.tsinghua.edu.cn;
   mz_dip@mail.dlut.edu.cn; kongxw@dlut.edu.cn; qianliu@dlut.edu.cn
RI Kong, Xiangwei/IWL-9350-2023; Li, Yi/AAC-9201-2019; Liu,
   Qian/AAG-8996-2021
OI Li, Yi/0000-0002-2856-7290; Liu, Qian/0000-0002-0756-4891; Guo,
   Jun/0000-0003-0930-3238
FU National Natural Science Foundation of China (NSFC) [U1636219,
   61379151]; Foundation for Innovative Research Groups of the NSFC
   [71421001]; Open Project Program of the National Laboratory of Pattern
   Recognition; Fundamental Research Funds for the Central Universities
   [DUT18JC06]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grants U1636219 and 61379151, in part
   by the Foundation for Innovative Research Groups of the NSFC under Grant
   71421001, in part by the Open Project Program of the National Laboratory
   of Pattern Recognition, and in part by the Fundamental Research Funds
   for the Central Universities under Grant DUT18JC06. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Wenwu Zhu. This paper was presented in part at the
   3rd IAPR Asian Conference on Pattern Recognition, Kuala Lumpur,
   Malaysia, November 2015. (Corresponding author: Yanqing Guo.)
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], SPRINGER SERIES STAT, DOI DOI 10.1007/978-0-387-84858-7_3
   [Anonymous], 2009, IEEE I CONF COMP VIS, DOI 10.1109/ICCV.2009.5459175
   [Anonymous], 2009, PROC ICML DEEP LEARN
   [Anonymous], 2014, ARXIV14127062
   [Anonymous], 149300 SCH COMP COMM
   [Anonymous], 2001, PROC 18 INT C MACH L
   Bousmalis K, 2013, IEEE T NEUR NET LEAR, V24, P170, DOI 10.1109/TNNLS.2012.2224882
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Caesar H, 2016, LECT NOTES COMPUT SC, V9905, P381, DOI 10.1007/978-3-319-46448-0_23
   Carreira J, 2012, LECT NOTES COMPUT SC, V7578, P430, DOI 10.1007/978-3-642-33786-4_32
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
   Dai JF, 2015, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2015.7299025
   He XM, 2004, PROC CVPR IEEE, P695
   Huang JC, 2005, IEEE T MULTIMEDIA, V7, P538, DOI 10.1109/TMM.2005.843346
   Jain A, 2012, LECT NOTES COMPUT SC, V7576, P718, DOI 10.1007/978-3-642-33715-4_52
   Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88
   Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar S, 2006, INT J COMPUT VISION, V68, P179, DOI 10.1007/s11263-006-7007-9
   Ladicky L'ubor, 2010, Computer Vision - ECCV 2010. Proceedings 11th European Conference on Computer Vision, P424, DOI 10.1007/978-3-642-15561-1_31
   Li Y, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P509, DOI 10.1109/ACPR.2015.7486555
   Liang XD, 2016, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2016.347
   Liang XD, 2016, IEEE T MULTIMEDIA, V18, P1175, DOI 10.1109/TMM.2016.2542983
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu FY, 2015, PATTERN RECOGN, V48, P2983, DOI 10.1016/j.patcog.2015.04.019
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mairal J, 2012, IEEE T PATTERN ANAL, V34, P791, DOI 10.1109/TPAMI.2011.156
   Mostajahi M, 2015, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2015.7298959
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Ohki K, 2005, NATURE, V433, P597, DOI 10.1038/nature03274
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Peng X, 2014, PATTERN RECOGN, V47, P2794, DOI 10.1016/j.patcog.2014.03.013
   Quattoni A, 2007, IEEE T PATTERN ANAL, V29, P1848, DOI 10.1109/TPAMI.2007.1124
   Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1
   Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1
   Singaraju D., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2313, DOI 10.1109/CVPR.2011.5995469
   Tao LL, 2014, LECT NOTES COMPUT SC, V8693, P549, DOI 10.1007/978-3-319-10602-1_36
   Tighe J, 2010, LECT NOTES COMPUT SC, V6315, P352, DOI 10.1007/978-3-642-15555-0_26
   Wang Y, 2009, PROC CVPR IEEE, P872, DOI 10.1109/CVPRW.2009.5206709
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Yang JM, 2013, IEEE I CONF COMP VIS, P857, DOI 10.1109/ICCV.2013.111
   Yang JM, 2012, PROC CVPR IEEE, P2296, DOI 10.1109/CVPR.2012.6247940
   Yang M, 2014, INT J COMPUT VISION, V109, P209, DOI 10.1007/s11263-014-0722-8
   Yao J, 2012, PROC CVPR IEEE, P702, DOI 10.1109/CVPR.2012.6247739
   Zhu HY, 2016, IEEE T MULTIMEDIA, V18, P1516, DOI 10.1109/TMM.2016.2571629
NR 50
TC 20
Z9 20
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2019
VL 21
IS 4
BP 875
EP 886
DI 10.1109/TMM.2018.2867720
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HQ4XD
UT WOS:000462413700006
DA 2024-07-18
ER

PT J
AU Yang, J
   Yao, Z
   Yang, BW
   Tan, XB
   Wang, ZL
   Zheng, Q
AF Yang, Jian
   Yao, Zhen
   Yang, Bowen
   Tan, Xiaobin
   Wang, Zilei
   Zheng, Quan
TI Software-Defined Multimedia Streaming System Aided By Variable-Length
   Interval In-Network Caching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Software defined networking; in-network cache; multimedia streaming
   system; OpenFlow; Video-on-Demand
ID VIDEO; DELIVERY
AB Explosive growth in video traffic volumes incurs a high percentage of redundancy in today's Internet, following the 80-20 rule. Fortunately, the advanced in-network cache is considered as an effective scheme for eliminating the repetitive traffic by caching the popular content in network nodes. Besides, the emerging software-defined networking (SDN) enables centralized control and management, as well as the collaboration between network devices and upper applications. Moreover, the Network Functions Virtualization is also developed to support for customized network functions, including caching and streaming. This inspires us to design an SDN-assisted multimedia streaming Video-on-Demand system, integrating in-network cache, to improve the quality of service. The designed architecture is capable of reducing the redundant traffic via the reusable duplications. In particular, it can achieve greater performance gains by deploying specific scheduling policy. We further propose a variable-length interval cache strategy for RTP streaming, which can realize the self-adaptive adjustment of the size of cached video segments based on their access patterns. Our goal is to efficiently utilize the limited storage resources and increase the cache hit ratio. We present the theoretical analysis to demonstrate the attainable performance of the proposed algorithm; furthermore, the integrated system design is implemented as a prototype to show its feasibility and applicability. Ultimately, emulation experiments are conducted to evaluate the achievable performance improvement more comprehensively.
C1 [Yang, Jian; Yao, Zhen; Yang, Bowen; Tan, Xiaobin; Wang, Zilei; Zheng, Quan] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Yang, J (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Anhui, Peoples R China.
EM jianyang@ustc.edu.cn; yaozhen1@mail.ustc.edu.cn; ybw92@mail.ustc.edu.cn;
   xbtan@ustc.edu.cn; zlwang@ustc.edu.cn; qzheng@ustc.edu.cn
RI Zheng, Quan/HZH-4993-2023
FU Equipment Preliminary RD Project [6141B0801010a]; National Natural
   Science Foundation of China [61573329]; State Key Program of National
   NSF of China [61233003]; Youth Innovation Promotion Association CAS
FX This work was supported in part by the Equipment Preliminary R&D Project
   6141B0801010a, in part by the National Natural Science Foundation of
   China under Grant 61573329, in part by the State Key Program of National
   NSF of China under Grant 61233003, and in part by the Youth Innovation
   Promotion Association CAS. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Christian
   Timmerer.
CR Anand A, 2009, SIGCOMM 2009, P87
   [Anonymous], 2017, 1465272001663118 CIS
   [Anonymous], 2004, CDN CONTENT DISTRIBU
   [Anonymous], 2015, OpenWrt Project
   Balachandran A., 2014, THESIS
   Bartolini N, 2004, LECT NOTES COMPUT SC, V2965, P1
   Chai WK, 2012, LECT NOTES COMPUT SC, V7289, P27, DOI 10.1007/978-3-642-30045-5_3
   CHEN HY, 1995, IEEE T CONSUM ELECTR, V41, P12, DOI 10.1109/30.370305
   Chiosi M., 2012, SDN OPENFLOW WORLD C, P22
   Claeys M, 2016, IEEE T NETW SERV MAN, V13, P308, DOI 10.1109/TNSM.2016.2546459
   Cui Y, 2004, IEEE J SEL AREA COMM, V22, P91, DOI 10.1109/JSAC.2003.818799
   Dan A, 1996, P SOC PHOTO-OPT INS, V2667, P344, DOI 10.1117/12.235887
   DAN A, 1994, 19347 IBM RC
   DEVROYE L, 1981, ANN PROBAB, V9, P860, DOI 10.1214/aop/1176994313
   Egilmez HE, 2014, IEEE T MULTIMEDIA, V16, P1597, DOI 10.1109/TMM.2014.2325791
   Eisley N, 2006, INT SYMP MICROARCH, P321
   Georgopoulos P, 2015, COMPUT COMMUN, V69, P79, DOI 10.1016/j.comcom.2015.06.015
   Herbaut Nicolas, 2016, P IEEE GLOBECOM, P1
   Ibn-Khedher H, 2016, 2016 23RD INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS (ICT), DOI 10.1109/ICT.2016.7500390
   Kanai K, 2016, IEEE J SEL AREA COMM, V34, P2102, DOI 10.1109/JSAC.2016.2577238
   Kim RH, 2015, IEEE INT INTERC TECH, P1, DOI 10.1109/IITC-MAM.2015.7325599
   Kim Y, 2013, COMPUT NETW, V57, P2465, DOI 10.1016/j.comnet.2012.11.026
   Kimmel M.S., 2014, Privilege: A Reader, V3rd, P1
   Li SH, 2016, IEEE T MULTIMEDIA, V18, P2503, DOI 10.1109/TMM.2016.2596042
   Li ZC, 2012, PROCEEDINGS OF THE ASME 6TH INTERNATIONAL CONFERENCE ON ENERGY SUSTAINABILITY - 2012, PTS A AND B, P1
   Liu Y, 2008, PEER PEER NETW APPL, V1, P18, DOI 10.1007/s12083-007-0006-y
   Mangili M, 2014, I C NETWORK PROTOCOL, P344, DOI 10.1109/ICNP.2014.56
   MARSHALL AW, 1985, J AM STAT ASSOC, V80, P332, DOI 10.2307/2287890
   McKeown N, 2008, ACM SIGCOMM COMP COM, V38, P69, DOI 10.1145/1355734.1355746
   Mininet Team, 2017, MIN PROJ
   Mitra S, 2011, ACM T WEB, V5, DOI 10.1145/1961659.1961662
   Open Networking Foundation, 2014, OPENFLOW EN SDN NETW
   Open Networking Lab, 2015, POX WIK
   PALLOTTINO S, 1984, NETWORKS, V14, P257, DOI 10.1002/net.3230140206
   Plass M.F., 2009, P 5 INT C EMERGING N, P1, DOI [10.1145/1658939.1658941, DOI 10.1145/1658939.1658941]
   Psaras I, 2014, IEEE T PARALL DISTR, V25, P2920, DOI 10.1109/TPDS.2013.304
   Schulzrinne H., 2016, RTP TOOLS
   Sezer S, 2013, IEEE COMMUN MAG, V51, P36, DOI 10.1109/MCOM.2013.6553676
   Trajano AFR, 2016, INT CON ADV INFO NET, P532, DOI 10.1109/AINA.2016.103
   Traverso S, 2015, IEEE T MULTIMEDIA, V17, P1839, DOI 10.1109/TMM.2015.2458043
   Ullah I, 2012, IEEE COMMUN SURV TUT, V14, P734, DOI 10.1109/SURV.2011.082611.00134
   Wichtlhuber M, 2015, IEEE T NETW SERV MAN, V12, P48, DOI 10.1109/TNSM.2015.2404792
   Xue NN, 2015, IEEE T MULTIMEDIA, V17, P1617, DOI 10.1109/TMM.2015.2450014
   Yang J, 2017, IEEE T MULTIMEDIA, V19, P619, DOI 10.1109/TMM.2016.2629280
   Zhou YP, 2015, IEEE T MULTIMEDIA, V17, P1273, DOI 10.1109/TMM.2015.2447277
NR 45
TC 8
Z9 8
U1 1
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2019
VL 21
IS 2
BP 494
EP 509
DI 10.1109/TMM.2018.2862349
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HJ7GT
UT WOS:000457364400018
DA 2024-07-18
ER

PT J
AU Hsu, SB
   Lee, CH
   Chang, PC
   Han, CC
   Fan, KC
AF Hsu, Sheng-Bin
   Lee, Chang-Hsing
   Chang, Pei-Chun
   Han, Chin-Chuan
   Fan, Kuo-Chin
TI Local Wavelet Acoustic Pattern: A Novel Time-Frequency Descriptor for
   Birdsong Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Birdsong recognition; discrete wavelet transform (DWT); vector of
   locally aggregated descriptors (VLAD)
ID CLASSIFICATION; SOUNDS; FEATURES
AB Investigating the identity, distribution, and evolution of bird species is important for both biodiversity assessment and environmental conservation. The discrete wavelet transform (DWT) has been widely exploited to extract time-frequency features for acoustic signal analysis. Traditional approaches usually compute statistical measures (e.g., maximum, mean, standard deviation) of the DWT coefficients in each subband independently to yield the feature descriptor, without considering the intersubband correlation. A new acoustic descriptor, called the local wavelet acoustic pattern (LWAP), is proposed to characterize the correlation of the DWT coefficients in different subbands for birdsong recognition. First, we divide a variable-length birdsong segment into a number of fixed-duration texture windows. For each texture window, several LWAP descriptors are extracted. The vector of locally aggregated descriptors (VLAD) is then used to aggregate the set of LWAP descriptors into a single VLAD vector. Finally, principal component analysis (PCA) plus linear discriminant analysis (LDA) are employed to reduce the feature dimensionality for classification purposes. Experiments on two birdsong datasets show that the proposed LWAP descriptor outperforms other local descriptors, including linear predictive coding cepstral coefficients, Mel-frequency cepstral coefficients, perceptual linear prediction cepstral coefficients, chroma features, and prosody features. Furthermore, the proposed LWAP descriptor, followed by VLAD encoding, PCA plus LDA feature extraction, and a simple distance-based classifier, yields promising results that are competitive with those obtained by the state-of-the-art convolutional neural networks.
C1 [Hsu, Sheng-Bin; Fan, Kuo-Chin] Natl Cent Univ, Dept Comp Sci & Informat Engn, Jhongli 32001, Taiwan.
   [Lee, Chang-Hsing] Chung Hua Univ, Dept Comp Sci & Informat Engn, Hsinchu 30012, Taiwan.
   [Chang, Pei-Chun] Natl Chiao Tung Univ, Dept Comp Sci & Informat Engn, Hsinchu 30010, Taiwan.
   [Han, Chin-Chuan] Natl United Univ, Dept Comp Sci & Informat Engn, Miaoli 36003, Taiwan.
C3 National Central University; Chung Hua University; National Yang Ming
   Chiao Tung University; National United University
RP Lee, CH (corresponding author), Chung Hua Univ, Dept Comp Sci & Informat Engn, Hsinchu 30012, Taiwan.
EM smallblight@gmail.com; chlee@chu.edu.tw; maplepig.cs05g@g2.nctu.edu.tw;
   cchan@nuu.edu.tw; kcfan@csie.ncu.edu.tw
RI Fan, K/GXH-3734-2022
OI Chang, Pei-Chun/0000-0002-1181-9340; Lee,
   Chang-Hsing/0000-0002-5761-421X
FU Ministry of Science and Technology of R.O.C. [MOST
   104-2221-E-216-010-MY2, MOST-102-2632-E-216-001-MY3]
FX This work was supported by the Ministry of Science and Technology of
   R.O.C. under Contract MOST 104-2221-E-216-010-MY2 and Contract
   MOST-102-2632-E-216-001-MY3.
CR Adavanne S, 2017, EUR SIGNAL PR CONF, P1729, DOI 10.23919/EUSIPCO.2017.8081505
   Anderson SE, 1996, J ACOUST SOC AM, V100, P1209, DOI 10.1121/1.415968
   [Anonymous], P 13 EUR SIGN PROC C
   [Anonymous], ACM SIGMM RECORDS
   [Anonymous], 2015, P 3 INT C LEARN REPR
   [Anonymous], 1995, CD SOUND MOUNT 4 SON
   [Anonymous], 1996, CD SOUND MOUNT 5 SON, VV
   Aucouturier JJ, 2003, J NEW MUSIC RES, V32, P83, DOI 10.1076/jnmr.32.1.83.16801
   Bastas S, 2012, IEEE INT SYMP CIRC S, P1676
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Briggs F, 2009, IEEE DATA MINING, P51, DOI 10.1109/ICDM.2009.65
   Chen T, 2014, IEEE T MULTIMEDIA, V16, P612, DOI 10.1109/TMM.2014.2301978
   Chen ZX, 2006, J ACOUST SOC AM, V120, P2974, DOI 10.1121/1.2345831
   Connor EF, 2012, J ACOUST SOC AM, V132, P507, DOI 10.1121/1.4726006
   Dubey SR, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2493446
   Duda R., 1973, Pattern Classification and Scene Analysis
   Fagerlund A., 2007, EURASIP J ADV SIG PR, V2007
   Härmä A, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P545
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Juang CF, 2007, NEUROCOMPUTING, V71, P121, DOI 10.1016/j.neucom.2007.08.011
   Kim MJ, 2012, IEEE T MULTIMEDIA, V14, P1390, DOI 10.1109/TMM.2012.2195481
   Kogan JA, 1998, J ACOUST SOC AM, V103, P2185, DOI 10.1121/1.421364
   Lakshminarayanan B, 2009, EIGHTH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS, PROCEEDINGS, P53, DOI 10.1109/ICMLA.2009.79
   Lee C., 2006, Journal of Information Technology and Applications, V1, P17
   Lee CH, 2008, IEEE T AUDIO SPEECH, V16, P1541, DOI 10.1109/TASL.2008.2005345
   Lee CH, 2006, IMECS 2006: INTERNATIONAL MULTICONFERENCE OF ENGINEERS AND COMPUTER SCIENTISTS, P331
   Lee CH, 2013, IEEE T MULTIMEDIA, V15, P454, DOI 10.1109/TMM.2012.2229969
   Manning Christopher D., 2008, INTRO INFORM RETRIEV
   McIlraith AL, 1997, IEEE T SIGNAL PROCES, V45, P2740, DOI 10.1109/78.650100
   McIlraith AL, 1995, IEEE WESCANEX '95 - COMMUNICATIONS, POWER, AND COMPUTING, CONFERENCE PROCEEDINGS, VOLS 1 AND 2, P409, DOI 10.1109/WESCAN.1995.494065
   PARKER TA, 1991, AUK, V108, P443
   Pellegrini T, 2017, EUR SIGNAL PR CONF, P1734, DOI 10.23919/EUSIPCO.2017.8081506
   Priyadarshani N, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0146790
   Qian K, 2015, 2015 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P1317, DOI 10.1109/GlobalSIP.2015.7418412
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   Ren JF, 2017, IEEE T MULTIMEDIA, V19, P447, DOI 10.1109/TMM.2016.2618218
   Reynolds DA, 1994, IEEE T SPEECH AUDI P, V2, P639, DOI 10.1109/89.326623
   Selin A, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/51806
   Sevilla A., 2017, P WORK NOT C LABS EV
   Sha-Sha Chen, 2013, 2013 5th International Conference on Computational Intelligence and Communication Networks (CICN), P262, DOI 10.1109/CICN.2013.62
   Singh NC, 2003, J ACOUST SOC AM, V114, P3394, DOI 10.1121/1.1624067
   Somervuo P, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P825
   Somervuo P, 2006, IEEE T AUDIO SPEECH, V14, P2252, DOI 10.1109/TASL.2006.872624
   Sprengel E., 2016, LIFECLEF
   Spyromitros-Xioufis E, 2014, IEEE T MULTIMEDIA, V16, P1713, DOI 10.1109/TMM.2014.2329648
   Stowell D., 2010, C4DMTR0912 QUEEN MAR
   Stowell D, 2014, PEERJ, V2, DOI 10.7717/peerj.488
   Sun R, 2013, INT C WAVEL ANAL PAT, P306, DOI 10.1109/ICWAPR.2013.6599335
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan WM, 2016, IEEE T MULTIMEDIA, V18, P128, DOI 10.1109/TMM.2015.2500727
   Vaizman Y, 2014, IEEE-ACM T AUDIO SPE, V22, P1483, DOI 10.1109/TASLP.2014.2337842
   Valero X, 2012, IEEE T MULTIMEDIA, V14, P1684, DOI 10.1109/TMM.2012.2199972
   Vallejo EE, 2007, LECT NOTES ARTIF INT, V4828, P212
   Wang H., 2007, 37 ANN M SOC NEUR SA
   Xie J, 2016, ECOL INFORM, V32, P134, DOI 10.1016/j.ecoinf.2016.01.007
   Yang H, 2003, PATTERN RECOGN, V36, P563, DOI 10.1016/S0031-3203(02)00048-1
NR 56
TC 13
Z9 14
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2018
VL 20
IS 12
BP 3187
EP 3199
DI 10.1109/TMM.2018.2834866
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HA4EY
UT WOS:000450212600001
DA 2024-07-18
ER

PT J
AU Salarian, M
   Iliev, N
   Çetin, AE
   Ansari, R
AF Salarian, Mandi
   Iliev, Nick
   Cetin, Ahmet Enis
   Ansari, Rashid
TI Improved Image-Based Localization Using SFM and Modified Coordinate
   System Transfer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image-based localization; BOF; retrieval; GPS uncertainty
ID VISUAL LOCATION RECOGNITION; MOBILE; SEARCH
AB Accurate localization of mobile devices based on camera-acquired visual media information usually requires a search over a very large GPS-referenced image database collected from social sharing websites like Flickr or services such as Google Street View. This paper proposes a new method for reliable estimation of the actual query camera location by optimally utilizing structure from motion (SFM) for three-dimensional (3-D) camera position reconstruction, and introducing a new approach for applying a linear transformation between two different 3-D Cartesian coordinate systems. Since the success of SFM hinges on effectively selecting among the multiple retrieved images, we propose an optimization framework to do this using the criterion of the highest intraclass similarity among images returned from retrieval pipeline to increase SFM convergence rate. The selected images along with the query are then used to reconstruct a 3-D scene and find the relative camera positions by employing SFM. In the last processing step, an effective camera coordinate transformation algorithm is introduced to estimate the query's geo-tag. The influence of the number of images involved in SFM on the ultimate position error is investigated by examining the use of three and four dataset images with different solution for calculating the query world coordinates. We have evaluated our proposed method on query images with known accurate ground truth. Experimental results are presented to demonstrate that our method outperforms other reported methods in terms of average error.
C1 [Salarian, Mandi; Iliev, Nick; Ansari, Rashid] Univ Illinois, Dept Elect & Comp Engn, Chicago, IL 60608 USA.
   [Cetin, Ahmet Enis] Univ Illinois, Chicago, IL 60608 USA.
C3 University of Illinois System; University of Illinois Chicago;
   University of Illinois Chicago Hospital; University of Illinois System;
   University of Illinois Chicago; University of Illinois Chicago Hospital
RP Salarian, M (corresponding author), Univ Illinois, Dept Elect & Comp Engn, Chicago, IL 60608 USA.
EM msalar2@uic.edu; niliev4@uic.edu; aecyy@uic.edu; ransari@uic.edu
RI Iliev, Nick/AAI-9698-2020
OI Iliev, Nick/0000-0001-9645-3810
FU Elizabeth Morse Genius Charitable Trust
FX This research was supported in part by a grant from the Elizabeth Morse
   Genius Charitable Trust.
CR Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Changchang Wu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3057, DOI 10.1109/CVPR.2011.5995552
   Chen DM, 2011, PROC CVPR IEEE, P737, DOI 10.1109/CVPR.2011.5995610
   Chen YZ, 2013, IMAGE VISION COMPUT, V31, P935, DOI 10.1016/j.imavis.2013.09.005
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   FISCHETTI M, 1994, NETWORKS, V24, P11, DOI 10.1002/net.3230240103
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gronát P, 2013, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2013.122
   Guan T, 2013, IEEE T MULTIMEDIA, V15, P1688, DOI 10.1109/TMM.2013.2265674
   HORN BKP, 1988, J OPT SOC AM A, V5, P1127, DOI 10.1364/JOSAA.5.001127
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jing YS, 2013, IEEE T MULTIMEDIA, V15, P2022, DOI 10.1109/TMM.2013.2279663
   KATOH N, 1981, SIAM J COMPUT, V10, P247, DOI 10.1137/0210017
   Knopp J, 2010, LECT NOTES COMPUT SC, V6311, P748, DOI 10.1007/978-3-642-15549-9_54
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Li HQ, 2013, IEEE T MULTIMEDIA, V15, P594, DOI 10.1109/TMM.2012.2234730
   Liu H., 2012, Proceedings_of_the_20th_ACM_ International_Conference_on_Multimedia, MM'12, P9
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Min WQ, 2014, IEEE T MULTIMEDIA, V16, P623, DOI 10.1109/TMM.2014.2302744
   Moisan L, 2012, IMAGE PROCESS ON LIN, V2, P56, DOI 10.5201/ipol.2012.mmm-oh
   Nister David, 2006, CVPR
   Park M, 2009, IEEE T PATTERN ANAL, V31, P1804, DOI 10.1109/TPAMI.2009.73
   Philbin J., 2008, P CVPR, P1
   Reitmayr G., 2007, Mixed and Augmented Reality, P161
   Salarian M, 2015, 2015 SAI INTELLIGENT SYSTEMS CONFERENCE (INTELLISYS), P485, DOI 10.1109/IntelliSys.2015.7361184
   Salarian M, 2016, IEEE INT SYM MULTIM, P181, DOI [10.1109/ISM.2016.87, 10.1109/ISM.2016.0043]
   Salarian M, 2016, IEEE INT SYM MULTIM, P189, DOI [10.1109/ISM.2016.43, 10.1109/ISM.2016.0045]
   Sang JT, 2013, IEEE T MULTIMEDIA, V15, P1665, DOI 10.1109/TMM.2013.2268052
   Sattler T, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.76
   Schroth G, 2011, IEEE SIGNAL PROC MAG, V28, P77, DOI 10.1109/MSP.2011.940882
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Song YF, 2016, IEEE T MULTIMEDIA, V18, P1542, DOI 10.1109/TMM.2016.2568743
   Takacs G, 2012, PROC SPIE, V8499, DOI 10.1117/12.945968
   Torii A, 2013, PROC CVPR IEEE, P883, DOI 10.1109/CVPR.2013.119
   Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832
   Turcot Panu, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P2109, DOI 10.1109/ICCVW.2009.5457541
   Vishal Kumar, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P17, DOI 10.1109/CVPRW.2015.7301390
   Witten I.H., 1999, Managing Gigabytes: Compressing and Indexing Documents and Images
   Xu X., 2012, Proceedings_of_the_4th_International_Conference_on_Internet_Multimedia_Computing_and_ Service, ICIMCS'12, P11
   Yan CG, 2018, IEEE T INTELL TRANSP, V19, P284, DOI 10.1109/TITS.2017.2749965
   Yu F.X., 2011, Proceedings_of_the_19th_ACM_International_Conference_on_Multimedia, MM'11, P3
   Zamir AR, 2014, PROC CVPR IEEE, P4280, DOI 10.1109/CVPR.2014.545
   Zamir AR, 2012, LECT NOTES COMPUT SC, V7573, P343, DOI 10.1007/978-3-642-33709-3_25
   Zamir AR, 2010, LECT NOTES COMPUT SC, V6314, P255, DOI 10.1007/978-3-642-15561-1_19
   Zhang Jerry, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3677, DOI 10.1109/ICIP.2011.6116517
NR 47
TC 12
Z9 13
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2018
VL 20
IS 12
BP 3298
EP 3310
DI 10.1109/TMM.2018.2839893
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HA4EY
UT WOS:000450212600010
DA 2024-07-18
ER

PT J
AU Shen, J
   Yu, L
   Li, L
   Li, HQ
AF Shen, Jian
   Yu, Lei
   Li, Li
   Li, Houqiang
TI Foveation-Based Wireless Soft Image Delivery
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Perceptual coding; human visual system (HVS); foveation; analog
   broadcasting; joint source channel coding (JSCC); power allocation
ID VIDEO; COMPRESSION
AB In image compression and transmission, two basic data redundancies can he identified and exploited: statistical redundancy and perceptual redundancy. Statistical redundancy has been studied for a very long time and has been exploited in most image coding schemes. However, perceptual redundancy has not yet been completely exploited. Furthermore, perceptual redundancy is difficult to exploit by using digital coding techniques because power allocation is performed at the bit level for digital coding. However, as one of one-size-fits-all technique, soft transmission is suitable to exploit the perceptual redundancy because its power allocation is directly applied at the pixel level instead of the bit level. In this paper, we propose a novel image transmission scheme, named FoveaCast, for single-input single-output or multi-input multi-output broadcast systems that effectively utilizes the foveation characteristic of human vision and analog coding techniques to achieve higher visual perceptual quality. Hence, our scheme possesses not only high visual perceptual quality but also graceful quality adaptation. Experimental evaluations show that compared with three existing digital schemes and two existing analog schemes, SoftCast and ParCast, FoveaCast achieves better visual perceptual performance. Meanwhile, it achieves a graceful quality variation along with channel variation, behaving just like SoftCast and ParCast.
C1 [Shen, Jian; Yu, Lei; Li, Li; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Li, HQ (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Anhui, Peoples R China.
EM sjian023@mail.ustc.edu.cn; yulei@ustc.edu.cn; lilimao@mail.ustc.edu.cn;
   lihq@ustc.edu.cn
RI Yu, Lei/AFT-1275-2022; Li, Li/T-2232-2019; Yu, Lei/AAP-3583-2020; Li,
   Houqiang Li/B-6259-2013
OI Yu, Lei/0000-0003-0772-2914; 
FU 973 Program [2015CB351803]; NSFC [61390514, 61631017]
FX This work was supported in part to Prof. Houqiang Li by 973 Program
   under contract no. 2015CB351803 and NSFC under contract no. 61390514,
   and in part to Dr. Lei Yu by NSFC under contract no. 61631017.
CR [Anonymous], IEEE T MULTIMEDIA
   Antonini M, 1992, IEEE T IMAGE PROCESS, V1, P205, DOI 10.1109/83.136597
   Arachchi H. K., 2006, P IEEE CAN C EL COMP, P2033
   Boulier F., 2009, P AS S COMP MATH ASC, P1
   Fan XP, 2013, IEEE T CIRC SYST VID, V23, P1040, DOI 10.1109/TCSVT.2013.2249019
   Girod Bernd, 1993, P207
   Grois D., 2013, ADV MULTIMEDIA, V2013, P6
   He DL, 2017, IEEE T MULTIMEDIA, V19, P1894, DOI 10.1109/TMM.2017.2686703
   Jakubczak S., 2011, PROC MOBICOM, P289
   Jakubczak S, 2010, ACM SIGCOMM COMP COM, V40, P449, DOI 10.1145/1851275.1851257
   Lawson C. L., 1995, SOLVING LEAST SQUARE, V15
   Lee JS, 2012, IEEE J-STSP, V6, P684, DOI 10.1109/JSTSP.2012.2215006
   LEE KH, 1976, IEEE T COMMUN, V24, P1283
   Li SX, 2018, IEEE T MULTIMEDIA, V20, P155, DOI 10.1109/TMM.2017.2721544
   Liang F., 2017, IEEE T CIRCUITS SYST
   Liu XL, 2014, IEEE T MULTIMEDIA, V16, P2038, DOI 10.1109/TMM.2014.2331616
   Liu XL, 2012, MOBICOM 12: PROCEEDINGS OF THE 18TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P233
   LUKAS FXJ, 1982, IEEE T COMMUN, V30, P1679, DOI 10.1109/TCOM.1982.1095616
   Marpe D, 2003, IEEE T CIRC SYST VID, V13, P620, DOI 10.1109/TCSVT.2003.815173
   SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x
   Skoglund M, 2006, IEEE T INFORM THEORY, V52, P3757, DOI 10.1109/TIT.2006.878212
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Taubman D., 2012, JPEG2000 IMAGE COMPR, V642
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang Z, 2003, IEEE T IMAGE PROCESS, V12, P243, DOI 10.1109/TIP.2003.809015
   Wang Z, 2001, IEEE T IMAGE PROCESS, V10, P1397, DOI 10.1109/83.951527
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   WITTEN IH, 1987, COMMUN ACM, V30, P520, DOI 10.1145/214762.214771
   Wu F, 2014, IEEE T IMAGE PROCESS, V23, P1015, DOI 10.1109/TIP.2014.2298972
   Xiong RQ, 2016, IEEE T IMAGE PROCESS, V25, P1820, DOI 10.1109/TIP.2016.2535288
   Yu L, 2014, IEEE T CIRC SYST VID, V24, P331, DOI 10.1109/TCSVT.2013.2273675
   Yu ZH, 2004, 2004 7TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING PROCEEDINGS, VOLS 1-3, P1167
NR 32
TC 21
Z9 21
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2018
VL 20
IS 10
SI SI
BP 2788
EP 2800
DI 10.1109/TMM.2018.2811622
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GT9YH
UT WOS:000444903000020
DA 2024-07-18
ER

PT J
AU Kang, B
   Lee, Y
   Nguyen, TQ
AF Kang, Byeongkeun
   Lee, Yeejin
   Nguyen, Truong Q.
TI Depth-Adaptive Deep Neural Network for Semantic Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantic segmentation; convolutional neural networks; deep learning
AB In this paper, we present the depth-adaptive deep neural network using a depth map for semantic segmentation. Typical deep neural networks receive inputs at the predetermined locations regardless of the distance from the camera. This fixed receptive field presents a challenge to generalize the features of objects at various distances in neural networks. Specifically, the predetermined receptive fields are too small at a short distance, and vice versa. To overcome this challenge, we develop a neural network that is able to adapt the receptive field not only for each layer hut also for each neuron at the spatial location. To adjust the receptive field, we propose the depth-adaptive multiscale (DaM) convolution layer consisting of the adaptive perception neuron and the in-layer multiscale neuron. The adaptive perception neuron is to adjust the receptive field at each spatial location using the corresponding depth information. The in-layer multiscale neuron is to apply the different size of the receptive field at each feature space to learn features at multiple scales. The proposed DaM convolution is applied to two fully convolutional neural networks. We demonstrate the effectiveness of the proposed neural networks on the publicly available RGB-D dataset for semantic segmentation and the novel hand segmentation dataset for hand-object interaction. The experimental results show that the proposed method outperforms the state-of-the-art methods without any additional layers or preprocessing/postprocessing.
C1 [Kang, Byeongkeun; Lee, Yeejin; Nguyen, Truong Q.] Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
C3 University of California System; University of California San Diego
RP Kang, B (corresponding author), Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
EM bkkang@ucsd.edu; yel031@ucsd.edu; tqn001@eng.ucsd.edu
RI Nguyen, Truong/JXN-9786-2024; Kang, Byeongkeun/AAX-9452-2020
OI Kang, Byeongkeun/0000-0003-2537-7720
FU National Science Foundation [IIS-1522125]
FX This work was supported in part by National Science Foundation Grant
   IIS-1522125.
CR Adams MD, 1996, INT J ROBOT RES, V15, P441, DOI 10.1177/027836499601500502
   [Anonymous], P BRIT MACH VIS C YO
   [Anonymous], 2011, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2011.5995316, 10.1109/CVPR.2011.5995316]
   [Anonymous], 2012, LECT NOTES COMPUT SC, DOI [10.1007/978-3-642-33715-4_54, DOI 10.1007/978-3-642-33715-4_54]
   Apostol T. M., 1974, Addison-Wesley series in mathematics, V2nd
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Ge L, 2016, PROC CVPR IEEE, P3593, DOI 10.1109/CVPR.2016.391
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jeon YH, 2017, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2017.200
   Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198
   Kang B., 2017, P IEEE GLOB C SIGN I
   Kang B, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P136, DOI 10.1109/ACPR.2015.7486481
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee Z, 2015, IEEE T MULTIMEDIA, V17, P792, DOI 10.1109/TMM.2015.2425141
   Lee Z, 2014, IEEE T MULTIMEDIA, V16, P2168, DOI 10.1109/TMM.2014.2355131
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Oikonomidis I, 2011, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2011.6126483
   Palmer JA, 2006, LECT NOTES COMPUT SC, V3889, P854
   Ranftl R, 2016, PROC CVPR IEEE, P4058, DOI 10.1109/CVPR.2016.440
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Romero J, 2013, IMAGE VISION COMPUT, V31, P555, DOI 10.1016/j.imavis.2013.04.002
   Romero J, 2010, IEEE INT CONF ROBOT, P458, DOI 10.1109/ROBOT.2010.5509753
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schwarz B., 2010, Nature Photonics, V4, P429, DOI [DOI 10.1038/NPHOTON.2010.148, 10.1038/nphoton.2010.148, 10.1038/nphoton.2010.14]
   Shotton J, 2013, IEEE T PATTERN ANAL, V35, P2821, DOI 10.1109/TPAMI.2012.241
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sinha A, 2016, PROC CVPR IEEE, P4150, DOI 10.1109/CVPR.2016.450
   Strang G., 1996, Wavelets and Filter Banks
   Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500
   Tzionas D, 2015, IEEE I CONF COMP VIS, P729, DOI 10.1109/ICCV.2015.90
   Wang K., 2016, Proceedings of the 24th ACM international conference on Multimedia, P1227
   Wang YG, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462000
   Yu F., 2015, ARXIV
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
NR 45
TC 49
Z9 50
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2018
VL 20
IS 9
BP 2478
EP 2490
DI 10.1109/TMM.2018.2798282
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GR1ZX
UT WOS:000442358200019
OA Green Submitted, hybrid
DA 2024-07-18
ER

PT J
AU Li, PY
   Lo, KT
AF Li, Peiya
   Lo, Kwok-Tung
TI A Content-Adaptive Joint Image Compression and Encryption Scheme
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image encryption; orthogonal transforms; data embedding; security
   analysis
ID ONLY MULTIMEDIA CIPHERS; QUANTITATIVE CRYPTANALYSIS; SECURITY
AB For joint image compression and encryption schemes, the encryption power and compression efficiency are commonly two contradictory things. In this paper, we propose a new joint image compression and encryption scheme based on a lossy JPEG standard, which aims at encryption power's enhancement, on the premise of maintaining the JPEG's compression efficiency. The proposed scheme is image-content-adaptive, since the secret encryption key is generated from the plain image using the BLAKE2 hash algorithm. Three encryption operations are contained in our scheme, alternating new orthogonal transforms transformation, dc coefficients encryption, and ac coefficients encryption. To save the cost for transmitting different encryption keys each time to decoder for decryption when the plain image changes, we propose embedding the encryption key into the entropy-encoded bitstream of some ac coefficients, and the whole embedding procedure is controlled by another secret key called the embedding key. Extensive experiments are conducted to show that our encryption scheme is JPEG friendly, and has good confusion and diffusion properties. Detailed security analysis is also given to illustrate the proposed scheme's persistence to various cryptanalysis strategies.
C1 [Li, Peiya] Jinan Univ, Coll Cyber Secur, Guangzhou 510632, Guangdong, Peoples R China.
   [Li, Peiya; Lo, Kwok-Tung] Hong Kong Polytech Univ, Dept Elect & Informat Engn, Kowloon, Hong Kong, Peoples R China.
C3 Jinan University; Hong Kong Polytechnic University
RP Li, PY (corresponding author), Jinan Univ, Coll Cyber Secur, Guangzhou 510632, Guangdong, Peoples R China.
EM yolanda.peiya@connect.polyu.hk; kwok.tung.lo@polyu.edu.hk
RI Lo, Kwok Tung KT/O-2143-2013
CR [Anonymous], 1996, Applied Cryptography: Protocols, algorithms
   Bahrami S, 2013, OPTIK, V124, P3693, DOI 10.1016/j.ijleo.2012.11.028
   Bose R, 2006, IEEE T CIRCUITS-I, V53, P848, DOI 10.1109/TCSI.2005.859617
   CHAM WK, 1986, IEE PROC-F, V133, P264, DOI 10.1049/ip-f-1.1986.0043
   CHEN WH, 1977, IEEE T COMMUN, V25, P1004, DOI 10.1109/TCOM.1977.1093941
   Chuman T, 2017, INT CONF ACOUST SPEE, P2157, DOI 10.1109/ICASSP.2017.7952538
   Fisher RonaldR., 1938, Statistical tables for biological, agricultural aad medical research
   Jakimoski G, 2008, IEEE T MULTIMEDIA, V10, P330, DOI 10.1109/TMM.2008.917355
   Jolfaei A, 2015, P IM VID TECHN, P344
   Jolfaei A, 2016, IEEE T INF FOREN SEC, V11, P235, DOI 10.1109/TIFS.2015.2489178
   Kim H, 2007, IEEE T SIGNAL PROCES, V55, P2263, DOI 10.1109/TSP.2007.892710
   Li CQ, 2011, SIGNAL PROCESS, V91, P949, DOI 10.1016/j.sigpro.2010.09.014
   Li SJ, 2008, SIGNAL PROCESS-IMAGE, V23, P212, DOI 10.1016/j.image.2008.01.003
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   Massoudi A, 2008, EURASIP J INF SECUR, DOI 10.1155/2008/179290
   Minemura K, 2017, IEEE T CIRC SYST VID, V27, P2309, DOI 10.1109/TCSVT.2016.2589742
   Mitra A., 2006, INT J COMPUTER SCI, V1, P127
   Nag Amitava, 2011, Proceedings 2011 International Conference on Signal Processing, Communication, Computing and Networking Technologies (ICSCCN 2011), P309, DOI 10.1109/ICSCCN.2011.6024565
   Ong SY, 2015, SIGNAL PROCESS-IMAGE, V31, P47, DOI 10.1016/j.image.2014.11.008
   Peiya Li, 2015, 2015 Visual Communications and Image Processing (VCIP), P1, DOI 10.1109/VCIP.2015.7457867
   Qian Z., IEEE T DEPEND UNPUB
   Qian ZX, 2014, IEEE T MULTIMEDIA, V16, P1486, DOI 10.1109/TMM.2014.2316154
   Saarinen M., 2015, The blake2 cryptographic hash and message authentication code (mac)
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   Sesha Pallavi Indrakanti P.S. A., 2011, INT J COMPUT APPL, V28, P45
   Somani H., 2014, INT J COMPUT APPL, V95, P15
   Stinson D. R., 2005, CRYPTOGRAPHY THEORY
   Taneja N, 2012, MULTIMED TOOLS APPL, V59, P775, DOI 10.1007/s11042-011-0775-4
   The JPEG Committee, 2017, 1SC29WG1 ISOIEC JTC
   WANG LM, 1988, IEEE T COMMUN, V36, P75, DOI 10.1109/26.2731
   Wu CP, 2005, IEEE T MULTIMEDIA, V7, P828, DOI 10.1109/TMM.2005.854469
   Wu Y, 2011, Cyber J Multidiscip J Sci Technol J Sel Areas Telecommun (JSAT), V1, P31
   YEUNG SKA, 2011, PROC IEEE INT C ACOU, P2436, DOI DOI 10.1109/ICASSP.2011.5946976
   Younes MAB, 2008, INT J COMPUT SCI NET, V8, P191
   Zhang YS, 2014, COMMUN NONLINEAR SCI, V19, P1366, DOI 10.1016/j.cnsns.2013.09.019
   Zhou JT, 2007, IEEE SIGNAL PROC LET, V14, P201, DOI 10.1109/LSP.2006.884012
NR 36
TC 55
Z9 55
U1 4
U2 76
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2018
VL 20
IS 8
BP 1960
EP 1972
DI 10.1109/TMM.2017.2786860
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GN8BR
UT WOS:000439378600004
DA 2024-07-18
ER

PT J
AU Xu, M
   Ren, Y
   Wang, ZL
   Liu, JX
   Tao, XM
AF Xu, Mai
   Ren, Yun
   Wang, Zulin
   Liu, Jingxian
   Tao, Xiaoming
TI Saliency Detection in Face Videos: A Data-Driven Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face video; visual attention; Gaussian mixture model
ID DETECTION MODEL; VISUAL SENSITIVITY; SEGMENTATION; STRATEGIES;
   ALLOCATION; ATTENTION; GAZE
AB Recently, videoconferencing has been popular in multimedia systems, such as FaceTime and Skype. In videoconferencing, almost every frame contains a human face. Therefore, it is important to predict human visual attention on face videos by saliency detection, as saliency may be used as a guide to the region of interest for the content-based applications of face videos. In this paper, we propose a data-driven approach for saliency detection in face videos. From the data-driven perspective, we first establish an eye-tracking database that contains fixations of 76 face videos viewed by 40 subjects. Upon the analysis of our database, we find that visual attention is significantly attracted by faces in videos. More important, the attention distribution within face regions varies with regard to mouth movement. Since previous works have investigated that it is efficient to model face saliency in still images using a Gaussian mixture model (GMM), the variation of visual attention in videos can be modeled by dynamic GMM (DGMM). Accordingly, we propose adopting the particle filter (PF) in modeling DGMM for saliency detection of face videos, which is called PF-DGMM. Finally, the experimental results show that our PF-DGMM approach significantly outperforms other state-of-the-art approaches in saliency detection of face videos.
C1 [Xu, Mai; Ren, Yun; Wang, Zulin; Liu, Jingxian] Beihang Univ, Sch Elect & Informat Engn, Beijing 100191, Peoples R China.
   [Tao, Xiaoming] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
C3 Beihang University; Tsinghua University
RP Xu, M (corresponding author), Beihang Univ, Sch Elect & Informat Engn, Beijing 100191, Peoples R China.
EM MaiXu@buaa.edu.cn; Yunren@buaa.edu.cn; wzulin@buaa.edu.cn;
   by1302130@buaa.edu.cn; taoxm@mail.tsinghua.edu.cn
RI Tao, XiaoMing/A-9992-2010; Tao, Xiaoming/ABG-9019-2021
OI Tao, Xiaoming/0000-0002-2406-0695
FU NSFC [61573037, 61471022]; Fok Ying-Tong Education Foundation [151061]
FX This work was supported in part by the NSFC under Grant 61573037 and
   Grant 61471022 and in part by the Fok Ying-Tong Education Foundation
   under Grant 151061.
CR ANDERSON J, 1993, J UNIV FILM VIDEO AS, V45, P3
   [Anonymous], 2008, Advances in neural information processing systems
   [Anonymous], 2004, OPT SCI TECHNOL
   [Anonymous], 2011, J VIS
   Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Boccignone G, 2008, INT C PATT RECOG, P1
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Bruce NDB, 2016, PROC CVPR IEEE, P516, DOI 10.1109/CVPR.2016.62
   Chen CZ, 2017, IEEE T IMAGE PROCESS, V26, P3156, DOI 10.1109/TIP.2017.2670143
   Chen CLZ, 2016, PATTERN RECOGN, V52, P410, DOI 10.1016/j.patcog.2015.09.033
   Chen CLZ, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2403232
   Chen L, 2015, IEEE T MULTIMEDIA, V17, P2225, DOI 10.1109/TMM.2015.2481711
   Dong YY, 2016, IEEE T MULTIMEDIA, V18, P549, DOI 10.1109/TMM.2016.2522639
   Doucet A., 2012, The Oxford Handbook of Nonlinear Filtering, P656
   Fang YM, 2014, IEEE T CIRC SYST VID, V24, P27, DOI 10.1109/TCSVT.2013.2273613
   Fang YM, 2012, IEEE T MULTIMEDIA, V14, P187, DOI 10.1109/TMM.2011.2169775
   Grant M., 2014, CVX MATLAB SOFTWARE
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Hadizadeh H, 2012, IEEE T IMAGE PROCESS, V21, P898, DOI 10.1109/TIP.2011.2165292
   Hou WL, 2013, PATTERN RECOGN, V46, P2658, DOI 10.1016/j.patcog.2013.03.008
   Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38
   Huo LN, 2016, PATTERN RECOGN, V49, P162, DOI 10.1016/j.patcog.2015.07.005
   Imamoglu N, 2013, IEEE T MULTIMEDIA, V15, P96, DOI 10.1109/TMM.2012.2225034
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2009, Advances in neural information processing systems, V49, P1295, DOI [10.1016/j.visres.2008.09.007, DOI 10.1016/J.VISRES.2008.09.007]
   Jetley S, 2016, PROC CVPR IEEE, P5753, DOI 10.1109/CVPR.2016.620
   Ji M, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENT RAIL TRANSPORTATION (ICIRT), P32, DOI 10.1109/ICIRT.2013.6696263
   Jiang M, 2014, LECT NOTES COMPUT SC, V8695, P17, DOI 10.1007/978-3-319-10584-0_2
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kavak Y, 2017, SIGNAL PROCESS-IMAGE, V51, P13, DOI 10.1016/j.image.2016.11.003
   KHATOONABADI SH, 2015, PROC CVPR IEEE, P5501
   Kruthiventi SSS, 2016, PROC CVPR IEEE, P5781, DOI 10.1109/CVPR.2016.623
   Kummerer M., 2014, ARXIV14111045
   Leborán V, 2017, IEEE T PATTERN ANAL, V39, P893, DOI 10.1109/TPAMI.2016.2567391
   Li J, 2015, IEEE I CONF COMP VIS, P190, DOI 10.1109/ICCV.2015.30
   Li J, 2010, INT J COMPUT VISION, V90, P150, DOI 10.1007/s11263-010-0354-6
   Lin YW, 2013, IEEE T PATTERN ANAL, V35, P314, DOI 10.1109/TPAMI.2012.119
   Liu Y, 2008, IEEE T CIRC SYST VID, V18, P134, DOI 10.1109/TCSVT.2007.913754
   Liu YF, 2017, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2017.343
   Mathe S, 2012, LECT NOTES COMPUT SC, V7573, P842, DOI 10.1007/978-3-642-33709-3_60
   Mauthner T, 2015, PROC CVPR IEEE, P2494, DOI 10.1109/CVPR.2015.7298864
   Meng FM, 2012, IEEE T MULTIMEDIA, V14, P1429, DOI 10.1109/TMM.2012.2197741
   Mital PK, 2011, COGN COMPUT, V3, P5, DOI 10.1007/s12559-010-9074-z
   Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71
   Rajashekar U, 2008, IEEE T IMAGE PROCESS, V17, P564, DOI 10.1109/TIP.2008.917218
   Ren ZX, 2013, IEEE T IMAGE PROCESS, V22, P3120, DOI 10.1109/TIP.2013.2259837
   Ristic B., 2004, Beyond the Kalman Filter: Particle Filters for Tracking Applications, V685
   Rudoy D, 2013, PROC CVPR IEEE, P1147, DOI 10.1109/CVPR.2013.152
   Saragihand J., 2009, P INT C COMP VIS, P1034, DOI DOI 10.1109/ICCV.2009.5459377
   Shen J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P1003, DOI 10.1109/ICCVW.2015.132
   Tang CW, 2006, IEEE T MULTIMEDIA, V8, P11, DOI 10.1109/TMM.2005.861295
   Vig E, 2014, PROC CVPR IEEE, P2798, DOI 10.1109/CVPR.2014.358
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang W., IEEE T CIRCUITS SYST
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P3137, DOI 10.1109/TIP.2015.2438550
   Wang WG, 2017, IEEE T IMAGE PROCESS, V26, P5645, DOI 10.1109/TIP.2017.2745098
   Wang WG, 2017, IEEE T VIS COMPUT GR, V23, P2014, DOI 10.1109/TVCG.2016.2600594
   Wang WG, 2016, IEEE T IMAGE PROCESS, V25, P5025, DOI 10.1109/TIP.2016.2601784
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Wenguan Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3395, DOI 10.1109/CVPR.2015.7298961
   Xu M, 2016, PATTERN RECOGN, V60, P348, DOI 10.1016/j.patcog.2016.05.023
   Xu M, 2015, IEEE I CONF COMP VIS, P3907, DOI 10.1109/ICCV.2015.445
   Zhang L., 2009, PROC 31 ANN COGNIT S, P2944
   Zhang LM, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P491, DOI 10.1145/2733373.2806255
   Zhao JP, 2015, PROC CVPR IEEE, P3174, DOI 10.1109/CVPR.2015.7298937
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhou F, 2014, PROC CVPR IEEE, P3358, DOI 10.1109/CVPR.2014.429
NR 68
TC 13
Z9 13
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1335
EP 1349
DI 10.1109/TMM.2017.2767784
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400005
DA 2024-07-18
ER

PT J
AU Oro, E
   Pizzuti, C
   Procopio, N
   Ruffolo, M
AF Oro, Ermelinda
   Pizzuti, Clara
   Procopio, Nicola
   Ruffolo, Massimo
TI Detecting Topic Authoritative Social Media Users: A Multilayer Network
   Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Social media; Twitter; multilayer networks; multilinear algebra; tensor
   decomposition; influential users; social network analysis
ID RECOMMENDATION
AB After the impressive diffusion of social media and microblogging websites of the last few years, the identification of users having the capability of influencing other users' choices is an important research topic because of the opportunities it can offer to many business companies. Most of the existing approaches, however, detect influencers by relying on centrality measures computed on networks that connect users having different types of inter-relationships. In this paper, we propose a method capable of finding influential users by exploiting the contents of the messages posted by them to express opinions on items, by modeling these contents with a three-layer network. Layers represent users, items, and keywords, along with intra-layer interactions among the actors of the same layer. Inter-layer connections are triples (u, i, k) expressing the information that a user u comments on an item i by using a keyword k. By exploiting multilinear algebra, we present a method capable of extracting the most active users stating their point of view about dominant items tagged with dominant keywords. We conduct a series of experiments on different real-world datasets collected from Twitter and Yelp Social Networks about different topics. Experimental results show the ability of our approach to find influential users that are both authoritative in the user network and very active in posting opinions about the topic of interest.
C1 [Oro, Ermelinda; Pizzuti, Clara; Ruffolo, Massimo] Natl Res Council Italy, Inst High Performance Comp & Networking, I-87036 Arcavacata Di Rende, Italy.
   [Procopio, Nicola] Integris Data Analyt & Cognit Comp Solut, I-87036 Arcavacata Di Rende, Italy.
C3 Consiglio Nazionale delle Ricerche (CNR); Istituto di Calcolo e Reti ad
   Alte Prestazioni (ICAR-CNR)
RP Pizzuti, C (corresponding author), Natl Res Council Italy, Inst High Performance Comp & Networking, I-87036 Arcavacata Di Rende, Italy.
EM linda.oro@icar.cnr.it; clara.pizzuti@icar.cnr.it;
   nicola.procopio@integris.it; mas-simo.ruffolo@icar.cnr.it
RI Pizzuti, Clara/O-8885-2015
OI Procopio, Nicola/0000-0001-7277-8880
CR Adomavicius G, 2005, IEEE T KNOWL DATA EN, V17, P734, DOI 10.1109/TKDE.2005.99
   Almgren K, 2015, P INT C ADV BIG DAT, P89
   Anger Isabel, 2011, P 11 INT C KNOWL MAN, P1
   Bader B. W., 2015, MATLAB TENSOR TOOLBO
   Beckmann CF, 2005, NEUROIMAGE, V25, P294, DOI 10.1016/j.neuroimage.2004.10.043
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Boccaletti S, 2014, PHYS REP, V544, P1, DOI 10.1016/j.physrep.2014.07.001
   Bonacina F, 2015, QUAL QUANT, V49, P1585, DOI 10.1007/s11135-014-0075-y
   Brin S, 1998, COMPUT NETWORKS ISDN, V30, P107, DOI 10.1016/S0169-7552(98)00110-X
   Cha  M., 2010, ICWSM, P10
   Conover W. J., 1998, PRACTICAL NONPARAMET, V3rd
   Cui P, 2016, IEEE MULTIMEDIA, V23, P92, DOI 10.1109/MMUL.2016.8
   Cui P, 2015, AAAI CONF ARTIF INTE, P65
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696
   Fang Q, 2014, IEEE T MULTIMEDIA, V16, P796, DOI 10.1109/TMM.2014.2298216
   Huang SR, 2016, IEEE T MULTIMEDIA, V18, P287, DOI 10.1109/TMM.2015.2510333
   Jeon I, 2016, VLDB J, V25, P519, DOI 10.1007/s00778-016-0427-4
   Jiang M, 2016, ACM T KNOWL DISCOV D, V10, DOI 10.1145/2746403
   Jiang M, 2015, IEEE T KNOWL DATA EN, V27, P3084, DOI 10.1109/TKDE.2015.2432811
   Jiang SH, 2015, IEEE T MULTIMEDIA, V17, P907, DOI 10.1109/TMM.2015.2417506
   Kivela M, 2014, J COMPLEX NETW, V2, P203, DOI 10.1093/comnet/cnu016
   Kleinberg JM, 1999, J ACM, V46, P604, DOI 10.1145/324133.324140
   Kolda Tamara., 2006, WORKSHOP LINK ANAL C, V7, P26
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Kolda TG, 2005, FIFTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P242, DOI 10.1109/ICDM.2005.77
   Lei XJ, 2016, IEEE T MULTIMEDIA, V18, P1910, DOI 10.1109/TMM.2016.2575738
   Li XB, 2012, PRZ ELEKTROTECHNICZN, V88, P141
   Li XT, 2014, IEEE T KNOWL DATA EN, V26, P929, DOI 10.1109/TKDE.2013.48
   Lin YR, 2011, ACM T KNOWL DISCOV D, V5, DOI 10.1145/1993077.1993081
   Ng M. K.-P., 2011, P 17 ACM SIGKDD INT, P1217
   Page L., 1999, PAGERANK CITATION RA
   Papalexakis EE, 2015, ACM T KNOWL DISCOV D, V10, DOI 10.1145/2729980
   Pawar K K., 2015, International Journal of Scientific Engineering Research, V6/, P957
   Pirie W., 1988, ENCY STAT SCI
   Probst F, 2013, BUS INFORM SYST ENG+, V5, P179, DOI 10.1007/s12599-013-0263-7
   Qian XM, 2014, IEEE T KNOWL DATA EN, V26, P1763, DOI 10.1109/TKDE.2013.168
   Riquelme F, 2016, INFORM PROCESS MANAG, V52, P949, DOI 10.1016/j.ipm.2016.04.003
   Shuhui Jiang, 2016, IEEE Transactions on Big Data, V2, P43, DOI 10.1109/TBDATA.2016.2541160
   Sun B., 2012, Proceedings of the 3rd International Workshop on Modeling Social Media, MSM'12, P1, DOI DOI 10.1145/2310057.2310059
   Sun Jian-Tao., 2005, PROC WWW 05, P382, DOI DOI 10.1145/1060745.1060803
   Tatar A, 2014, J INTERNET SERV APPL, V5, DOI 10.1186/s13174-014-0008-y
   Wang Z, 2013, IEEE T MULTIMEDIA, V15, P698, DOI 10.1109/TMM.2012.2237022
   Weng J., 2010, P 3 ACM INT C WEB SE, P261, DOI [10.1145/1718487.1718520, DOI 10.1145/1718487.1718520]
   Yang Xiwang., 2012, P 18 ACM SIGKDD INT, P1267, DOI [10.1145/2339530.2339728, DOI 10.1145/2339530.2339728]
   Zhao GS, 2016, IEEE T MULTIMEDIA, V18, P496, DOI 10.1109/TMM.2016.2515362
NR 45
TC 19
Z9 19
U1 1
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2018
VL 20
IS 5
BP 1195
EP 1208
DI 10.1109/TMM.2017.2763324
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GD7YD
UT WOS:000430728400014
DA 2024-07-18
ER

PT J
AU Li, CL
   Toni, L
   Zou, JN
   Xiong, HK
   Frossard, P
AF Li, Chenglin
   Toni, Laura
   Zou, Junni
   Xiong, Hongkai
   Frossard, Pascal
TI QoE-Driven Mobile Edge Caching Placement for Adaptive Video Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mobile edge caching; adaptive video streaming; wireless video delivery;
   video-on-demand; submodular function maximization
ID WIRELESS CONTENT DELIVERY; SUBMODULAR SET FUNCTIONS; MEDIA CLOUD;
   NETWORKS; APPROXIMATIONS; TRANSMISSION; STRATEGY; CHANNELS; HELPERS;
   SYSTEMS
AB Caching at mobile edge servers can smooth temporal traffic variability and reduce the service load of base stations in mobile video delivery. However, the assignment of multiple video representations to distributed servers is still a challenging question in the context of adaptive streaming, since any two representations from different videos or even from the same video will compete for the limited caching storage. Therefore, it is important, yet challenging, to optimally select the cached representations for each edge server in order to effectively reduce the service load of base station while maintaining a high quality of experience (QoE) tbr users. To address this, we study a QoE-driven mobile edge caching placement optimization problem for dynamic adaptive video streaming that properly takes into account the different rate-distortion (R-D) characteristics of videos and the coordination among distributed edge servers. Then, by the optimal caching placement of representations for multiple videos, we maximize the aggregate average video distortion reduction of all users while minimizing the additional cost of representation downloading from the base station, subject not only to the storage capacity constraints of the edge servers, but also to the transmission and initial startup delay constraints of the users. We formulate the proposed optimization problem as an integer linear program to provide the performance upper bound, and as a submodular maximization problem with a set of knapsack constraints to develop a practically feasible cost benefit greedy algorithm. The proposed algorithm has polynomial computational complexity and a theoretical lower bound on its performance. Simulation results further show that the proposed algorithm is able to achieve a near-optimal performance with very low time complexity. Therefore, the proposed optimization framework reveals the caching performance upper bound for general adaptive video streaming systems, while the proposed algorithm provides some design guidelines for the edge servers to select the cached representations in practice based on both the video popularity and content information.
C1 [Li, Chenglin; Frossard, Pascal] Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, CH-1015 Lausanne, Switzerland.
   [Toni, Laura] UCL, Elect & Elect Dept, London WC1E 7JE, England.
   [Zou, Junni; Xiong, Hongkai] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne; University of London; University College London;
   Shanghai Jiao Tong University
RP Li, CL (corresponding author), Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, CH-1015 Lausanne, Switzerland.
EM chenglin.li@epfl.ch; l.toni@ucl.ac.uk; zou-jn@cs.sjtu.edu.cn;
   xionghongkai@sjtu.edu.cn; pascal.frossard@epfl.ch
RI Frossard, Pascal/AAF-2268-2019; LI, CHENGLIN/JUF-8254-2023
OI Xiong, Hongkai/0000-0003-4552-0029; Frossard, Pascal/0000-0002-4010-714X
FU National Natural Science Foundation of China [61501293, 61529101,
   61425011, 61622112, 61472234, 61720106001]; Program of Shanghai Academic
   Research Leader [17XD1401900]; China Postdoctoral Science Foundation
   [2016190372, 2015M570365]; China Scholarship Council; SNSF under the
   CIIIST-ERA Project CONCERT [ENS 20CH21_151569]; Swiss National Science
   Foundation (SNF) [20CH21_151569] Funding Source: Swiss National Science
   Foundation (SNF)
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61501293, 61529101, 61425011, 61622112,
   61472234, and 61720106001, in part by the Program of Shanghai Academic
   Research Leader under Grant 17XD1401900, in pail by the China
   Postdoctoral Science Foundation under Grants 2016190372 and 2015M570365,
   in part by the China Scholarship Council, and in part by the SNSF under
   the CIIIST-ERA Project CONCERT, ENS 20CH21_151569. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Shiweu Mao.
CR [Anonymous], 2016, GLOB INT PHEN REP
   [Anonymous], CISCO VISUAL NETWORK
   [Anonymous], 2017, 2017 15 INT S MOD AD
   [Anonymous], 2015, P 6 ACM MULTIMEDIA S
   [Anonymous], P 26 INT WORKSH NETW
   Chen JT, 2011, IEEE T SIGNAL PROCES, V59, P2395, DOI 10.1109/TSP.2011.2106124
   Dash S, 2005, MATH OPER RES, V30, P678, DOI 10.1287/moor.1050.0151
   Duel-Hallen A, 2007, P IEEE, V95, P2299, DOI 10.1109/JPROC.2007.904443
   FISHER ML, 1978, MATH PROGRAM STUD, V8, P73, DOI 10.1007/BFb0121195
   Gao GY, 2015, IEEE T MULTIMEDIA, V17, P1286, DOI 10.1109/TMM.2015.2438713
   Ge C, 2016, PROCEEDINGS OF THE 2016 3RD ACM CONFERENCE ON INFORMATION-CENTRIC NETWORKING (ACM-ICN '16), P237, DOI 10.1145/2984356.2988522
   Golrezaei N, 2012, IEEE INFOCOM SER, P1107, DOI 10.1109/INFCOM.2012.6195469
   Goundan P. R., 2007, Revisiting the greedy approach to submodular set function maximization, P1
   IBM, ILOG CPLEX OPT STUD
   Jin YC, 2015, IEEE T CIRC SYST VID, V25, P1914, DOI 10.1109/TCSVT.2015.2402892
   Joseph V, 2014, IEEE INFOCOM SER, P82, DOI 10.1109/INFOCOM.2014.6847927
   Khreishah A, 2016, IEEE J SEL AREA COMM, V34, P2275, DOI 10.1109/JSAC.2016.2577199
   Krause A., 2012, Tractability: Practical Approaches to Hard Problems, V3, P19
   Krishnamoorthy B, 2008, OPER RES LETT, V36, P19, DOI 10.1016/j.orl.2007.04.011
   Lee D.H., 2014, P NETWORK OPERATING, P31
   Li J, 2015, IEEE T COMMUN, V63, P3553, DOI 10.1109/TCOMM.2015.2455500
   Li SH, 2016, IEEE T MULTIMEDIA, V18, P2503, DOI 10.1109/TMM.2016.2596042
   LI W, 2016, PROC 14 USENIX C, V68, P1
   Li ZG, 2012, PLANT SCI, V185, P185, DOI 10.1016/j.plantsci.2011.10.006
   Liu D, 2016, IEEE COMMUN MAG, V54, P22, DOI 10.1109/MCOM.2016.7565183
   Maddah-Ali MA, 2014, IEEE T INFORM THEORY, V60, P2856, DOI 10.1109/TIT.2014.2306938
   Mao Y., 2017, IEEE COMMUN SURV TUT
   Müller S, 2017, IEEE T WIREL COMMUN, V16, P1024, DOI 10.1109/TWC.2016.2636139
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Poularakis K, 2016, IEEE T COMMUN, V64, P2092, DOI 10.1109/TCOMM.2016.2545655
   Poularakis K, 2014, IEEE T COMMUN, V62, P3665, DOI 10.1109/TCOMM.2014.2351796
   Sengupta Avik, 2016, 2016 Annual Conference on Information Science and Systems (CISS), P320, DOI 10.1109/CISS.2016.7460522
   Shanmugam K, 2013, IEEE T INFORM THEORY, V59, P8402, DOI 10.1109/TIT.2013.2281606
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Sung J, 2016, IEEE T MULTIMEDIA, V18, P1163, DOI 10.1109/TMM.2016.2543658
   Sviridenko M, 2004, OPER RES LETT, V32, P41, DOI 10.1016/S0167-6377(03)00062-2
   Wolsey L. A., 2016, MATH OPER RES, V7, P410
   Zhang QQ, 1999, IEEE T COMMUN, V47, P1688, DOI 10.1109/26.803503
   Zhang WW, 2013, IEEE T MULTIMEDIA, V15, P1431, DOI 10.1109/TMM.2013.2247583
   Zhao H, 2017, IEEE T MULTIMEDIA, V19, P149, DOI 10.1109/TMM.2016.2612123
   Zink M, 2009, COMPUT NETW, V53, P501, DOI 10.1016/j.comnet.2008.09.022
NR 43
TC 113
Z9 123
U1 4
U2 51
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2018
VL 20
IS 4
BP 965
EP 984
DI 10.1109/TMM.2017.2757761
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FZ5HM
UT WOS:000427623000016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, SW
   Gao, CX
   Zhang, J
   Chen, FF
   Sang, N
AF Zhang, Shiwei
   Gao, Changxin
   Zhang, Jing
   Chen, Feifei
   Sang, Nong
TI Discriminative Part Selection for Human Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mid-level; part selection; recursive part elimination (RPE); maximum
   margin model; action recognition
ID HUMAN ACTION CATEGORIES; CLASSIFICATION; CANCER
AB Semantic parts have shown a powerful discriminative capacity for action recognition. However, many existing methods select parts according to predefined heuristic rules, which may cause the correlation among parts to be lost, or do not appropriately consider the cluttered candidate part space, which may result in weak generalizability of the resulting action labels. Therefore, better consideration of the correlation among parts and refinement of the candidate space will lead to a more discriminative action representation. This paper achieves improved performance by more elegantly addressing these two factors. First, considering the cluttered nature of the candidate space, we propose a recursive part elimination strategy for iterative refinement of the candidate parts. In each iteration, we eliminate the parts with the lowest weights, which are deemed to be noise. Second, we measure the discriminative capabilities of the candidates and select the top-ranked parts by applying a maximum margin model, which can alleviate overfitting while simultaneously improving generalizability and correlation extraction. Finally, using the selected parts, we extract mid-level features. We report experiments conducted on four datasets (KTH, Olympic Sports, UCF50, and HIVIDB51). The proposed method can achieve significant improvements compared with other recent methods, including a lower computational cost, a faster speed, and higher accuracy.
C1 [Zhang, Shiwei; Gao, Changxin; Zhang, Jing; Chen, Feifei; Sang, Nong] Huazhong Univ Sci & Technol, Sch Automat, Minist Educ Image Proc & Intelligent Control, Key Lab, Wuhan 430074, Hubei, Peoples R China.
C3 Huazhong University of Science & Technology
RP Sang, N (corresponding author), Huazhong Univ Sci & Technol, Sch Automat, Minist Educ Image Proc & Intelligent Control, Key Lab, Wuhan 430074, Hubei, Peoples R China.
EM swZhang@hust.edu.cn; cgao@hust.edu.cn; celiazjing@gmail.com;
   ffChen@hust.edu.cn; nsang@hust.edu.cn
RI Gao, Changxin/L-4841-2016
OI Gao, Changxin/0000-0003-2736-3920
FU National Natural Science Foundation of China [61271328, 61401170]
FX This work was supported in part by a Project of the National Natural
   Science Foundation of China under Grant 61271328 and 61401170. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Zhu Li.
CR Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   BILEN H, 2016, PROC CVPR IEEE, P3034, DOI [10.1109/CVPR.2016.331, DOI 10.1109/CVPR.2016.331]
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chang Y., 2008, P WORKSHOP CAUSATION, V3, P53
   Chen FF, 2015, J OPT SOC AM A, V32, P173, DOI 10.1364/JOSAA.32.000173
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Cui P, 2012, IEEE T MULTIMEDIA, V14, P102, DOI 10.1109/TMM.2011.2176110
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Fernando B, 2017, IEEE T PATTERN ANAL, V39, P773, DOI 10.1109/TPAMI.2016.2558148
   Gao CX, 2017, IEEE T CIRC SYST VID, V27, P300, DOI 10.1109/TCSVT.2015.2513700
   Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531
   Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797
   Hariharan B, 2012, LECT NOTES COMPUT SC, V7575, P459, DOI 10.1007/978-3-642-33765-9_33
   Hasan M, 2015, IEEE T MULTIMEDIA, V17, P1909, DOI 10.1109/TMM.2015.2477242
   Jain A, 2013, PROC CVPR IEEE, P2571, DOI 10.1109/CVPR.2013.332
   Jingen Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3337, DOI 10.1109/CVPR.2011.5995353
   Joachims Thorsten, 1998, EUROPEAN C MACHINE L, P137, DOI 10.1007/
   Juneja M, 2013, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2013.124
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X
   Kosmopoulos DI, 2012, COMPUT VIS IMAGE UND, V116, P422, DOI 10.1016/j.cviu.2011.09.006
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Liu C., 2016, PATTERN RECOG
   Liu F, 2016, IEEE T IMAGE PROCESS, V25, P949, DOI 10.1109/TIP.2015.2512107
   Makantasis K, 2016, IEEE IMAGE PROC, P1609, DOI 10.1109/ICIP.2016.7532630
   Manzi A., 2016, 3D HUMAN POSTURE APP
   Ni BB, 2015, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2015.7298993
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29
   Peng XJ, 2014, LECT NOTES COMPUT SC, V8693, P581, DOI 10.1007/978-3-319-10602-1_38
   Reddy KK, 2013, MACH VISION APPL, V24, P971, DOI 10.1007/s00138-012-0450-4
   Ryoo MS, 2016, INT J COMPUT VISION, V119, P307, DOI 10.1007/s11263-015-0847-4
   Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806
   Samanta S, 2014, IEEE T MULTIMEDIA, V16, P1525, DOI 10.1109/TMM.2014.2326734
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Segal MR, 2003, J COMPUT BIOL, V10, P961, DOI 10.1089/106652703322756177
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang LM, 2013, PROC CVPR IEEE, P2674, DOI 10.1109/CVPR.2013.345
   Wang S, 2014, IEEE T MULTIMEDIA, V16, P289, DOI 10.1109/TMM.2013.2293060
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Yuan Y., 2016, PATTERN RECOG
   Zhang SW, 2017, IEEE T SYST MAN CY-S, V47, P660, DOI 10.1109/TSMC.2016.2625840
   Zhang SW, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P619, DOI 10.1109/ACPR.2015.7486577
   Zhang XP, 2016, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR.2016.128
   Zhang Y, 2014, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2014.121
   Zhu WJ, 2016, PROC CVPR IEEE, P1991, DOI 10.1109/CVPR.2016.219
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 54
TC 17
Z9 17
U1 1
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2018
VL 20
IS 4
BP 769
EP 780
DI 10.1109/TMM.2017.2758524
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FZ5HM
UT WOS:000427623000001
DA 2024-07-18
ER

PT J
AU Sun, HM
   Zhou, DJ
   Hu, LD
   Kimura, S
   Goto, S
AF Sun, Heming
   Zhou, Dajiang
   Hu, Landan
   Kimura, Shinji
   Goto, Satoshi
TI Fast Algorithm and VLSI Architecture of Rate Distortion Optimization in
   H.265/HEVC
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Encoding; high efficiency video coding (HEVC); rate distortion
   optimization (RDO); video coding
ID VIDEO CODING HEVC; MODE DECISION; COMPLEXITY; H.264/AVC; RDO
AB In H.265/high efficiency video coding (HEVC) encoding, rate distortion optimization (RDO) is an important cost function for mode decision and coding structure decision. Despite being near-optimum in terms of coding efficiency, RDO suffers from a high complexity. To address this problem, this paper presents a fast RDO algorithm and its very large scale implementation (VLSI) for both intra-and inter-frame coding. The proposed algorithm employs a quantization-free framework that significantly reduces the complexity for rate and distortion optimization. Meanwhile, it maintains a low degradation of coding efficiency by taking the syntax element organization and probability model of HEVC into consideration. The algorithm is also designed with hardware architecture in mind to support an efficient VLSI implementation. When implemented in the HEVC test model, the proposed algorithm achieves 62% RDO time reduction with 1.85% coding efficiency loss for the "all-intra" configuration. The hardware implementation achieves 1.6 x higher normalized throughput relative to previous works, and it can support a throughput of 8k@30fps (for four fine-processed modes per prediction unit) with 256 k logic gates when working at 200 MHz.
C1 [Sun, Heming; Zhou, Dajiang; Kimura, Shinji; Goto, Satoshi] Waseda Univ, Grad Sch Informat Prod & Syst, Kitakyushu, Fukuoka 8080135, Japan.
   [Hu, Landan] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
C3 Waseda University; Shanghai Jiao Tong University
RP Zhou, DJ (corresponding author), Waseda Univ, Grad Sch Informat Prod & Syst, Kitakyushu, Fukuoka 8080135, Japan.
EM terrysun1989@akane.waseda.jp; zhou@fuji.waseda.jp;
   nelly12345qq.com@sjtu.edu.cn; shinji_kimura@waseda.jp; goto@waseda.jp
RI Sun, Heming/L-6385-2019
OI Sun, Heming/0000-0001-5583-4895
FU Graduate Program for Embodiment Informatics of the Ministry of
   Education, Culture, Sports, Science and Technology
FX This work was supported by the Graduate Program for Embodiment
   Informatics of the Ministry of Education, Culture, Sports, Science and
   Technology. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Jun Wu.
   (Corresponding author: Dajiang Zhou.)
CR Bjontegaard G., 2001, P 13 VCEG M AUST TX
   Bossen F., 2013, P 12 JCT VC M GEN SW
   Cho S, 2013, IEEE T CIRC SYST VID, V23, P1555, DOI 10.1109/TCSVT.2013.2249017
   Fang XF, 2013, PICT COD SYMP, P273, DOI 10.1109/PCS.2013.6737736
   Heming Sun, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P1085, DOI 10.1109/ICME.2012.4
   Hu L., 2015, International Conference on Renewable Power Generation (RPG 2015), P1
   Hu Q, 2016, IEEE T MULTIMEDIA, V18, P379, DOI 10.1109/TMM.2015.2512799
   Johar S, 2013, 2013 IEEE CONSUMER COMMUNICATIONS AND NETWORKING CONFERENCE (CCNC), P721, DOI 10.1109/CCNC.2013.6488534
   Langlois JAP, 2006, IEEE T CIRCUITS-II, V53, P374, DOI 10.1109/TCSII.2006.873364
   Lee B, 2016, IEEE T IMAGE PROCESS, V25, P3787, DOI 10.1109/TIP.2016.2579559
   Lee B, 2016, IEEE T MULTIMEDIA, V18, P1257, DOI 10.1109/TMM.2016.2557075
   Liu ZY, 2014, IEEE IMAGE PROC, P3676, DOI 10.1109/ICIP.2014.7025746
   Pastuszak G, 2016, IEEE T CIRC SYST VID, V26, P210, DOI 10.1109/TCSVT.2015.2428571
   Sharabayko MP, 2014, 2014 9TH INTERNATIONAL FORUM ON STRATEGIC TECHNOLOGY (IFOST), P56, DOI 10.1109/IFOST.2014.6991071
   Shen LQ, 2014, IEEE T CIRC SYST VID, V24, P1709, DOI 10.1109/TCSVT.2014.2313892
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Shen W., 2014, INT S VLSI DES AUT T, P1
   Shen XL, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P453, DOI 10.1109/PCS.2012.6213252
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sze V., 2014, High Efficiency Video Coding (HEVC): Algorithms and Architectures
   Tsai S. F., 2013, 2013 S VLSI CIRC VLS, pC188
   Tu YK, 2006, IEEE T CIRC SYST VID, V16, P600, DOI 10.1109/TCSVT.2006.873160
   Vanne J, 2014, IEEE T CIRC SYST VID, V24, P1579, DOI 10.1109/TCSVT.2014.2308453
   Wang Q, 2005, IEEE INT SYMP CIRC S, P3467
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P2141, DOI 10.1109/TMM.2014.2356795
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P559, DOI 10.1109/TMM.2013.2291958
   Zhang H, 2014, IEEE T CIRC SYST VID, V24, P660, DOI 10.1109/TCSVT.2013.2290578
   Zhao WJ, 2013, PICT COD SYMP, P109, DOI 10.1109/PCS.2013.6737695
   Zhao X, 2010, IEEE T CIRC SYST VID, V20, P647, DOI 10.1109/TCSVT.2010.2045803
   Zhe Sheng, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8325, P541, DOI 10.1007/978-3-319-04114-8_46
   Zhu J, 2013, IEEE IMAGE PROC, P1977, DOI 10.1109/ICIP.2013.6738407
   Zhu J, 2014, ASIA S PACIF DES AUT, P367, DOI 10.1109/ASPDAC.2014.6742917
NR 32
TC 13
Z9 16
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2017
VL 19
IS 11
BP 2375
EP 2390
DI 10.1109/TMM.2017.2700629
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FJ9CT
UT WOS:000413068200002
DA 2024-07-18
ER

PT J
AU Wu, DP
   Liu, QR
   Wang, HG
   Wu, DL
   Wang, RY
AF Wu, Dapeng
   Liu, Qianru
   Wang, Honggang
   Wu, Dalei
   Wang, Ruyan
TI Socially Aware Energy-Efficient Mobile Edge Collaboration for Video
   Distribution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mobile edge collaboration; scalable video coding; video distribution;
   multicast; energy-efficient; user attributes
ID NETWORKS; COMMUNICATION; ALLOCATION; UNDERLAY
AB To relieve the current overload of cellular networks caused by the continuously growing multimedia service, mobile edge collaboration, which exploits edge users to distribute videos for base station (BS), provides an effective way to share the heavy BS load. With the emergence of mobile edge technologies for Internet-of-Things applications, such as device to device and machine to machine, how to exploit users' social characteristics and mobility to minimize the number of transmissions of BS and how to improve the quality of experience of users have become the key challenges. In this paper, we study two aspects that are critical to these issues. One is the two-step detection mechanism, namely the establishment of virtual communities and collaborative clusters. Specifically, we take into consideration user preference for content and location. First of all, a virtual community is established, which exploits the coalition game based on the user's preference list to dynamically divide users into multiple communities. Then, to take full advantage of the temporary link established between users, a grid-based clustering method is proposed to manage the video requesting users. On the other hand, we propose a scalable video coding sharing scheme based on user's social attributes. This approach makes video distribution more flexible at the edge of mobile network through collaboration among users, and effectively reduces transmission energy consumption of transmitters. Numerical results showthat the proposed mechanism can not only effectively alleviate the BS load, but also dramatically improve the reliability and adaptability of video distribution.
C1 [Wu, Dapeng; Liu, Qianru; Wang, Ruyan] Chongqing Univ Posts & Telecommun, Chongqing 400065, Peoples R China.
   [Wu, Dapeng; Wang, Honggang] Univ Massachusetts Dartmouth, N Dartmouth, MA 02747 USA.
   [Wu, Dalei] Univ Tennessee, Chattanooga, TN 37403 USA.
C3 Chongqing University of Posts & Telecommunications; University of
   Massachusetts System; University Massachusetts Dartmouth; University of
   Tennessee System; University of Tennessee at Chattanooga
RP Wang, HG (corresponding author), Univ Massachusetts Dartmouth, N Dartmouth, MA 02747 USA.
EM wudapengphd@gmail.com; 2370094121@qq.com; hwang1@umassd.edu;
   dalei-wu@utc.edu; wangry@cqupt.edu.cn
RI Wu, Dapeng/IWE-0674-2023; Wu, Dalei/A-6884-2012; Wang,
   Honggang/D-6079-2013
OI Wu, Dapeng/0000-0003-2105-9418; , Dalei/0000-0002-9362-3906; Wang,
   Honggang/0000-0001-9475-2630
FU Natural Science Foundation of China [61371097]; National Science
   Foundation [1429120, 1451629]; Program for Innovation Team Building at
   the Institutions of Higher Education in Chongqing [CXTDX201601020];
   Division Of Computer and Network Systems; Direct For Computer & Info
   Scie & Enginr [1429120, 1451629] Funding Source: National Science
   Foundation
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61371097, in part by the National Science Foundation
   under Grant 1429120 and Grant 1451629, and in part by the Program for
   Innovation Team Building at the Institutions of Higher Education in
   Chongqing under Grant CXTDX201601020. The guest editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Shiwen Mao. (Corresponding author: Honggang Wang.)
CR [Anonymous], 2015, ACM T MULTIMEDIA COM
   [Anonymous], 2015, P IEEE GLOB COMM C
   [Anonymous], 2015, CISC VIS NETW IND FO
   Cao Y, 2016, IEEE WIREL COMMUN, V23, P52, DOI 10.1109/MWC.2016.7553026
   Cheng X, 2012, IEEE T MULTIMEDIA, V14, P1558, DOI 10.1109/TMM.2012.2217735
   Doppler K, 2009, IEEE COMMUN MAG, V47, P42, DOI 10.1109/MCOM.2009.5350367
   Duan LJ, 2014, IEEE T MOBILE COMPUT, V13, P2320, DOI 10.1109/TMC.2014.2307327
   European Telecommunications Standards Institute, MOB EDG COMP KEY TEC
   Golrezaei N, 2014, IEEE T WIREL COMMUN, V13, P3665, DOI 10.1109/TWC.2014.2316817
   Golrezaei N, 2013, IEEE COMMUN MAG, V51, P142, DOI 10.1109/MCOM.2013.6495773
   Guo QH, 2016, IEEE ACCESS, V4, P4310, DOI 10.1109/ACCESS.2016.2597189
   Habbal A, 2017, IEEE ACCESS, V5, P6636, DOI 10.1109/ACCESS.2017.2689725
   Han B, 2010, PROCEEDINGS OF THE 5TH ACM WORKSHOP ON CHALLENGED NETWORKS (CHANTS '10), P31, DOI 10.1145/1859934.1859943
   Helgason Olafur Ragnar, 2010, 2010 European Wireless Conference (EW), P903, DOI 10.1109/EW.2010.5483523
   Hongliang Yu, 2006, Operating Systems Review, V40, P333, DOI 10.1145/1218063.1217968
   Hu DL, 2012, IEEE J SEL AREA COMM, V30, P641, DOI 10.1109/JSAC.2012.120413
   Hu P., IEEE INTERN IN PRESS
   Hua S, 2011, IEEE T MULTIMEDIA, V13, P402, DOI 10.1109/TMM.2010.2103929
   Jiasi Chen, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P1266, DOI 10.1109/INFOCOM.2015.7218502
   Karagiannis T, 2010, IEEE T MOBILE COMPUT, V9, P1377, DOI 10.1109/TMC.2010.99
   Khan AUR, 2014, IEEE COMMUN SURV TUT, V16, P393, DOI 10.1109/SURV.2013.062613.00160
   Luo CQ, 2014, IEEE T PARALL DISTR, V25, P3211, DOI 10.1109/TPDS.2013.2297922
   Malandrino F, 2014, IEEE COMMUN MAG, V52, P94, DOI 10.1109/MCOM.2014.6957148
   Mao SW, 2008, IEEE WIREL COMMUN, V15, P67, DOI 10.1109/MWC.2008.4599223
   Melki L, 2016, DIGIT COMMUN NETW, V2, P225, DOI 10.1016/j.dcan.2016.09.007
   Mumtaz S, 2014, AD HOC NETW, V13, P296, DOI 10.1016/j.adhoc.2013.08.008
   Pan ZQ, 2016, J VIS COMMUN IMAGE R, V40, P516, DOI 10.1016/j.jvcir.2016.07.018
   Pan ZQ, 2016, IEEE T BROADCAST, V62, P675, DOI 10.1109/TBC.2016.2580920
   Pan ZQ, 2015, IEEE T BROADCAST, V61, P166, DOI 10.1109/TBC.2015.2419824
   Perrucci Gian Paolo, 2009, 2009 European Wireless Conference (EW 2009), P255, DOI 10.1109/EW.2009.5357972
   Roy C., 2010, 20100124 AIAA, DOI [10.2514/6.2010-124, DOI 10.2514/6.2010-124]
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Scott James., 2009, CRAWDAD trace cambridge/haggle/imote/infocom2006 (v. 2009-05-29)
   Seeling P., 2001, IEEE NETW
   Seppälä J, 2011, 2011 IEEE WIRELESS COMMUNICATIONS AND NETWORKING CONFERENCE (WCNC), P986, DOI 10.1109/WCNC.2011.5779270
   Shen YY, 2014, 2014 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P108, DOI 10.1109/GlobalSIP.2014.7032088
   Sun LF, 2017, IEEE T MULTIMEDIA, V19, P609, DOI 10.1109/TMM.2016.2635589
   Tran TX, 2017, IEEE COMMUN MAG, V55, P54, DOI 10.1109/MCOM.2017.1600863
   Wu D, 2014, IEEE T VEH TECHNOL, V63, P2093, DOI 10.1109/TVT.2014.2311580
   Wu DP, 2017, IEEE T MULTIMEDIA, V19, P1908, DOI 10.1109/TMM.2017.2692648
   Wu DP, 2016, IEEE T VEH TECHNOL, V65, P7634, DOI 10.1109/TVT.2015.2493516
   Wu DP, 2015, IEEE COMMUN MAG, V53, P92, DOI 10.1109/MCOM.2015.7180514
   Xu C, 2013, IEEE J SEL AREA COMM, V31, P348, DOI 10.1109/JSAC.2013.SUP.0513031
   Xu Y, 2013, IEEE WIREL COMMUN, V20, P46, DOI 10.1109/MWC.2013.6549282
   Zhang XF, 2012, IEEE ACM T COMPUT BI, V9, P740, DOI 10.1109/TCBB.2011.148
   Zhang Y, 2016, CAN J ELECT COMPUT E, V39, P2, DOI 10.1109/CJECE.2015.2469724
   Zhao GS, 2016, IEEE T MULTIMEDIA, V18, P496, DOI 10.1109/TMM.2016.2515362
   Zhou L, 2016, IEEE T MULTIMEDIA, V18, P905, DOI 10.1109/TMM.2016.2537782
NR 48
TC 94
Z9 97
U1 1
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2017
VL 19
IS 10
BP 2197
EP 2209
DI 10.1109/TMM.2017.2733300
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5YG
UT WOS:000411247600006
OA Bronze
DA 2024-07-18
ER

PT J
AU Liong, VE
   Lu, JW
   Tan, YP
   Zhou, J
AF Liong, Venice Erin
   Lu, Jiwen
   Tan, Yap-Peng
   Zhou, Jie
TI Deep Video Hashing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; scalable video search; video hashing
ID QUANTIZATION; SEARCH; RETRIEVAL
AB In this work, we propose a deep video hashing (DVH) method for scalable video search. Unlike most existing video hashing methods that first extract features for each single frame and then use conventional image hashing techniques, our DVH learns binary codes for the entire video with a deep learning framework so that both the temporal and discriminative information can be well exploited. Specifically, we fuse the temporal information across different frames within each video to learn the feature representation under two criteria: the distance between a feature pair obtained at the top layer is small if they are from the same class, and large if they are from different classes; and the quantization loss between the real-valued features and the binary codes is minimized. We exploit different deep architectures to utilize spatial-temporal information in different manners and compare them with single-frame-based deep models and state-of-the-art image hashing methods. Experimental results demonstrate the effectiveness of our proposed method.
C1 [Liong, Venice Erin] Nanyang Technol Univ, Interdisciplinary Grad Sch, Rapid Rich Object Search Lab, Singapore 639798, Singapore.
   [Lu, Jiwen; Zhou, Jie] Tsinghua Univ, Dept Automat, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.
   [Tan, Yap-Peng] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University; Tsinghua University; Nanyang
   Technological University
RP Lu, JW (corresponding author), Tsinghua Univ, Dept Automat, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.
EM veniceer001@e.ntu.edu.sg; lujiwen@tsinghua.edu.cn; eyptan@ntu.edu.sg;
   jzhou@tsinghua.edu.cn
RI Tan, Yap-Peng/A-5158-2011; Lu, Jiwen/C-5291-2009
OI Lu, Jiwen/0000-0002-6121-5529
FU National Key Research and Development Program of China [2016YFB1001001];
   National Natural Science Foundation of China [61672306, 61225008,
   61572271, 61527808, 61373074, 61373090]; National 1000 Young Talents
   Plan Program; MSRA Collaborative Research Program; National Basic
   Research Program of China [2014CB349304]; Ministry of Education of China
   [20120002110033]; Tsinghua University Initiative Scientific Research
   Program
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2016YFB1001001, in part by the
   National Natural Science Foundation of China under Grant 61672306, Grant
   61225008, Grant 61572271, Grant 61527808, Grant 61373074, and Grant
   61373090, in part by the National 1000 Young Talents Plan Program, in
   part by the MSRA Collaborative Research Program, in part by the National
   Basic Research Program of China under Grant 2014CB349304, in part by the
   Ministry of Education of China under Grant 20120002110033, and in part
   by the Tsinghua University Initiative Scientific Research Program. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Jingdong Wang. (Corresponding
   author: Jiwen Lu.)
CR [Anonymous], 2016, SURVEY LEARNING HASH
   Baccouche Moez, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P29, DOI 10.1007/978-3-642-25446-8_4
   Cao Liangliang., 2012, Proceedings of the 20th ACM International Conference on Multimedia, P299
   Cao Y, 2016, AAAI CONF ARTIF INTE, P3457
   Chéron G, 2015, IEEE I CONF COMP VIS, P3218, DOI 10.1109/ICCV.2015.368
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Chou CL, 2015, IEEE T MULTIMEDIA, V17, P382, DOI 10.1109/TMM.2015.2391674
   Coskun B, 2006, IEEE T MULTIMEDIA, V8, P1190, DOI 10.1109/TMM.2006.884614
   Douze M, 2010, IEEE T MULTIMEDIA, V12, P257, DOI 10.1109/TMM.2010.2046265
   Duan LY, 2015, IEEE T MULTIMEDIA, V17, P828, DOI 10.1109/TMM.2015.2419973
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hoi SCH, 2008, IEEE T MULTIMEDIA, V10, P607, DOI 10.1109/TMM.2008.921735
   Hu WM, 2011, IEEE T SYST MAN CY C, V41, P797, DOI 10.1109/TSMCC.2011.2109710
   Irie G, 2014, PROC CVPR IEEE, P2123, DOI 10.1109/CVPR.2014.272
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396
   Ji RR, 2013, IEEE T MULTIMEDIA, V15, P153, DOI 10.1109/TMM.2012.2225035
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiang Y. -G., 2011, P 1 ACM INT C MULT R, P29
   Jiang YG, 2013, IEEE T MULTIMEDIA, V15, P442, DOI 10.1109/TMM.2012.2231061
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kofler C, 2014, IEEE T MULTIMEDIA, V16, P1421, DOI 10.1109/TMM.2014.2315777
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Li P, 2013, IEEE T MULTIMEDIA, V15, P141, DOI 10.1109/TMM.2012.2199970
   Lin GS, 2014, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2014.253
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu W., 2014, P NEURAL INF PROCESS, P3419
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu WJ, 2011, E-POLYMERS
   Liu XL, 2016, PROC CVPR IEEE, P5119, DOI 10.1109/CVPR.2016.553
   Liu XL, 2016, IEEE T IMAGE PROCESS, V25, P4514, DOI 10.1109/TIP.2016.2593344
   Liu XL, 2015, IEEE T CYBERNETICS, V45, P1811, DOI 10.1109/TCYB.2014.2360856
   Masci J, 2014, IEEE T PATTERN ANAL, V36, P824, DOI 10.1109/TPAMI.2013.225
   Mu YD, 2014, PROC CVPR IEEE, P979, DOI 10.1109/CVPR.2014.130
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Peng XJ, 2016, LECT NOTES COMPUT SC, V9908, P744, DOI 10.1007/978-3-319-46493-0_45
   Rumelhart D.E., 2013, Learning internal representations by error propagation, P399, DOI [10.1016/b978-1-4832-1446-7.50035-2, 10.1016/B978-1-4832-1446-7.50035-2]
   Shao J, 2016, PROC CVPR IEEE, P5620, DOI 10.1109/CVPR.2016.606
   Shen FM, 2015, IEEE T IMAGE PROCESS, V24, P1839, DOI 10.1109/TIP.2015.2405340
   Shen FM, 2013, PROC CVPR IEEE, P1562, DOI 10.1109/CVPR.2013.205
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang J, 2012, IEEE IMAGE PROC, P645, DOI 10.1109/ICIP.2012.6466942
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Weiss Y., 2008, PROC ADV NEURAL INFO, V21, P1753
   Wenxing Li Y. L., 2014, INT J ANTENN PROPAG, V2014, P1
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Xu X, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P305, DOI 10.1145/2911996.2912056
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Yan Li, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163089
   Ye GN, 2013, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2013.282
   Ye H, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P435, DOI 10.1145/2671188.2749406
   Yu LT, 2016, IEEE T MULTIMEDIA, V18, P1590, DOI 10.1109/TMM.2016.2557059
   Zeng XY, 2014, LECT NOTES COMPUT SC, V8691, P472, DOI 10.1007/978-3-319-10578-9_31
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zhou JL, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P415
   Zhou WG, 2014, IEEE T MULTIMEDIA, V16, P601, DOI 10.1109/TMM.2014.2301979
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
NR 71
TC 69
Z9 76
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2017
VL 19
IS 6
BP 1209
EP 1219
DI 10.1109/TMM.2016.2645404
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5YS
UT WOS:000404059400008
DA 2024-07-18
ER

PT J
AU Lei, Y
   Yuan, W
   Wang, HP
   You, WH
   Bo, W
AF Lei, You
   Yuan, Wang
   Wang, Hongpeng
   You Wenhu
   Bo, Wu
TI A Skin Segmentation Algorithm Based on Stacked Autoencoders
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Color space; machine learning; skin detection; stacked autoencoders
ID NEURAL-NETWORK
AB A good skin detector that is capable of capturing skin tones under different conditions is important for human-machine interaction applications. In a general situation, skin detectors, such as skin probability maps or Gaussian mixture models, achieve acceptable skin segmentation results. However, the false positive rate increases significantly when the skin tones are in shadow or when skin-like background objects are under similar illumination. In this paper, we propose a novel skin feature learning algorithm based on stacked autoencoders, which are deep neural networks. To overcome the problems encountered in skin segmentation that are caused by different ethnicities and varying illumination conditions, the stacked autoencoders are utilized to learn more discriminative representations of the skin area in both the RGB color space and the HSV color space. Unlike traditional machine learning methods, instead of predicting each pixel individually, our algorithm utilizes blocks to learn the representations and detect the skin areas. The algorithm exploits the learning ability of deep neural networks to learn high-level representations of skin tones. Experiments on test images show that the proposed algorithm achieves acceptable results on several publicly available data sets. To reduce the difficulty of detecting skin pixels in these data sets, the ground truths of these data sets are commonly focused on foreground skin area detection. Our skin detector is also able to detect background areas, as shown in our experiments.
C1 [Lei, You; Yuan, Wang; Wang, Hongpeng] Harbin Inst Technol, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.
   [You Wenhu] Harbin Inst Technol, Sch Astronaut, Harbin 150001, Peoples R China.
   [Bo, Wu] Shenzhen Polytech, Educ Technol & Informat Ctr, Shenzhen 518055, Peoples R China.
C3 Harbin Institute of Technology; Harbin Institute of Technology; Shenzhen
   Polytechnic University
RP Lei, Y (corresponding author), Harbin Inst Technol, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.
EM youlei@hitsz.edu.cn; 592825995@qq.com; wanghp@hit.edu.cn;
   Yourmate_2000@163.com; wubo@szpt.edu.cn
FU Shenzhen IOT Key Technology and Application Systems Integration
   Engineering Laboratory;  [JCYJ20160226201453085]
FX This work was supported in part by the Shenzhen IOT Key Technology and
   Application Systems Integration Engineering Laboratory, and in part by
   the project JCYJ20160226201453085. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Winston Hsu. (Corresponding author: You Lei.)
CR Al-Mohair HK, 2015, APPL SOFT COMPUT, V33, P337, DOI 10.1016/j.asoc.2015.04.046
   [Anonymous], 2010, INT J SIGNAL IMAGE P
   Bengio Y., 2007, Advances in neural information processing systems, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Chakraborty BK, 2015, PROCEEDINGS OF THE 2015 IEEE RECENT ADVANCES IN INTELLIGENT COMPUTATIONAL SYSTEMS (RAICS), P246, DOI 10.1109/RAICS.2015.7488422
   Chan CS, 2007, J INTELL ROBOT SYST, V48, P79, DOI 10.1007/s10846-006-9100-2
   Cheddad A, 2009, SIGNAL PROCESS, V89, P2465, DOI 10.1016/j.sigpro.2009.04.022
   Ciresan D, 2012, NEURAL NETWORKS, V32, P333, DOI 10.1016/j.neunet.2012.02.023
   Erdem CE, 2011, INT CONF ACOUST SPEE, P1497, DOI 10.1109/ICASSP.2011.5946777
   Gasparini F., 2006, P SPIE, V6061, P128
   Getoor, 2011, P 28 INT C MACH LEAR, P833
   Han JW, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P237
   Iraji M.S., 2011, American Journal of Scientific Research, P131, DOI DOI 10.5815/IJIGSP.2012.04.05
   Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198
   Kawulok M, 2013, IEEE INT CONF AUTOMA, DOI 10.1109/FG.2013.6553733
   Kawulok M, 2014, PATTERN RECOGN LETT, V41, P3, DOI 10.1016/j.patrec.2013.08.028
   Li X, 2016, KNOWL-BASED SYST, V113, P88, DOI 10.1016/j.knosys.2016.09.014
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Moon TK, 1996, IEEE SIGNAL PROC MAG, V13, P47, DOI 10.1109/79.543975
   Naji SA, 2012, DIGIT SIGNAL PROCESS, V22, P933, DOI 10.1016/j.dsp.2012.05.004
   Nalepa J, 2014, ADV INTEL SOFT COMPU, V242, P79, DOI 10.1007/978-3-319-02309-0_8
   Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257
   Phung SL, 2005, IEEE T PATTERN ANAL, V27, P148, DOI 10.1109/TPAMI.2005.17
   Qiang Z., 2004, P 12 ANN ACM INT C M
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sohn K., 2015, Neural Information Processing Systems, P3483
   Stöttinger J, 2009, LECT NOTES COMPUT SC, V5876, P303, DOI 10.1007/978-3-642-10520-3_28
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Tan WR, 2012, IEEE T IND INFORM, V8, P138, DOI 10.1109/TII.2011.2172451
   Tao X., 2015, J ELECTRON IMAGING, V24
   Thakur S., 2011, Proceedings of the 2011 World Congress on Information and Communication Technologies (WICT), P53, DOI 10.1109/WICT.2011.6141217
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Yi SY, 2017, PATTERN RECOGN, V61, P524, DOI 10.1016/j.patcog.2016.08.025
   Yogarajah P, 2010, IEEE IMAGE PROC, P2225, DOI 10.1109/ICIP.2010.5652798
   Zaidan AA, 2014, ENG APPL ARTIF INTEL, V32, P136, DOI 10.1016/j.engappai.2014.03.002
   Zaidan AA, 2010, SCI RES ESSAYS, V5, P2931
   Zhang Z, 2009, IEEE IMAGE PROC, P1137, DOI 10.1109/ICIP.2009.5413535
   Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236
   Zuo H., 2010, International Conference on World Wide Web: ACM, P1227
NR 39
TC 33
Z9 36
U1 1
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2017
VL 19
IS 4
BP 740
EP 749
DI 10.1109/TMM.2016.2638204
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EO0NT
UT WOS:000396395500006
DA 2024-07-18
ER

PT J
AU Wang, ZL
   Xiang, D
   Hou, SH
   Wu, F
AF Wang, Zilei
   Xiang, Dao
   Hou, Saihui
   Wu, Feng
TI Background-Driven Salient Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Salient object detection; background prior; saliency estimation;
   graph-based optimization
ID REGION DETECTION; VISUAL SALIENCY; MODEL; ATTENTION
AB The background information is a significant prior for salient object detection, especially when images contain cluttered background and diverse object parts. In this paper, we propose a background-driven salient object detection (BD-SOD) method to more comprehensively exploit the background prior, aiming at generating more accurate and robust salient maps. To be specific, we first exploit the background prior to conduct the saliency estimation, i.e., computing the regional saliency values. In this stage, the background prior is utilized in threefold: restricting the reference regions to only the background regions, weighting the contribution of reference regions, and leveraging the importance of different features. Benefiting from such an explicit utilization, the proposed model can greatly mitigate the negative interference of the cluttered background and diverse object parts. We then embed the background prior into the optimization graph for saliency refinement. Specifically, two virtual supernodes (representing the background and foreground, respectively) are introduced with extra connections, and the nonlocal feature connections between similar regions are also set up. These connections enhance the power of optimization graph to alleviate the perturbations from diverse parts, and thus help to achieve the uniformity of saliency values. Finally, we provide systematical studies to investigate the effectiveness of the proposed BD-SOD in exploiting the valuable background prior. Experimental results on multiple public benchmark datasets, including MSRA-1000, THUS-10000, PASCAL-S, and ECSSD, clearly show that BD-SOD consistently outperforms the well-established baselines and achieves state-ofthe- art performance.
C1 [Wang, Zilei; Xiang, Dao; Hou, Saihui] Univ Sci & Technol China, Dept Automat, Hefei 230027, Peoples R China.
   [Wu, Feng] Univ Sci & Technol China, Sch Informat Sci & Technol, West Campus, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Wang, ZL (corresponding author), Univ Sci & Technol China, Dept Automat, Hefei 230027, Peoples R China.
EM zlwang@ustc.edu.cn; xiangdao@mail.ustc.edu.cn; saihui@mail.ustc.edu.cn;
   fengwu@ustc.edu.cn
RI Wu, Feng/KCY-3017-2024
FU National Natural Science Foundation of China [61673362, 61233003];
   Fundamental Research Funds for the Central Universities
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61673362 and Grant 61233003, and in part
   by the Fundamental Research Funds for the Central Universities. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Jing-Ming Guo.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2016, P BRIT MACH VIS C
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Chang KY, 2011, IEEE I CONF COMP VIS, P914, DOI 10.1109/ICCV.2011.6126333
   Chen  L., 2002, MSRTR2002125
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Ding M, 2010, VISUAL COMPUT, V26, P721, DOI 10.1007/s00371-010-0448-8
   Duan LJ, 2011, PROC CVPR IEEE, P473, DOI 10.1109/CVPR.2011.5995676
   Einhäuser W, 2003, EUR J NEUROSCI, V17, P1089, DOI 10.1046/j.1460-9568.2003.02508.x
   Fang YM, 2012, IEEE T MULTIMEDIA, V14, P187, DOI 10.1109/TMM.2011.2169775
   Gao Y, 2015, IEEE T MULTIMEDIA, V17, P359, DOI 10.1109/TMM.2015.2389616
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Gopalakrishnan V, 2009, IEEE T MULTIMEDIA, V11, P892, DOI 10.1109/TMM.2009.2021726
   Han JW, 2015, IEEE T CIRC SYST VID, V25, P1309, DOI 10.1109/TCSVT.2014.2381471
   He SF, 2015, INT J COMPUT VISION, V115, P330, DOI 10.1007/s11263-015-0822-0
   Hong CQ, 2015, IEEE T IMAGE PROCESS, V24, P5659, DOI 10.1109/TIP.2015.2487860
   Huang J, 2011, IEEE T MULTIMEDIA, V13, P653, DOI 10.1109/TMM.2011.2127463
   Imamoglu N, 2013, IEEE T MULTIMEDIA, V15, P96, DOI 10.1109/TMM.2012.2225034
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li S, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440755
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu Z, 2014, IEEE T IMAGE PROCESS, V23, P1937, DOI 10.1109/TIP.2014.2307434
   Liu Z, 2012, IEEE T MULTIMEDIA, V14, P1275, DOI 10.1109/TMM.2012.2190385
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Shahrian E, 2012, PROC CVPR IEEE, P718, DOI 10.1109/CVPR.2012.6247741
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Sun XS, 2012, PROC CVPR IEEE, P1552, DOI 10.1109/CVPR.2012.6247846
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang ZL, 2013, IEEE T IMAGE PROCESS, V22, P537, DOI 10.1109/TIP.2012.2218826
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhou L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2438546
   Zhu L, 2014, IEEE T IMAGE PROCESS, V23, P5094, DOI 10.1109/TIP.2014.2361024
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 44
TC 42
Z9 44
U1 3
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2017
VL 19
IS 4
BP 750
EP 762
DI 10.1109/TMM.2016.2636739
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EO0NT
UT WOS:000396395500007
DA 2024-07-18
ER

PT J
AU Cheung, M
   She, J
   Park, S
AF Cheung, Ming
   She, James
   Park, Soochang
TI Analytics-Driven Visualization on Digital Directory via Screen-Smart
   Device Interactions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Customized visualization; directory; multimedia; screen-smart device
   interaction
ID SIGNAGE
AB Informative directories have always responded to a fundamental need of humanity: providing available information around people. However, the escalating amount of content to be visualized on directories makes relevant information search extremely time-consuming. Meanwhile, digital displays based on screen-smart device interaction become an emerging interface of smart services to deal with daily-life challenges like information seeking. Also, multimedia content, such as movies, can be understood by multimedia analytics for recommendation, but there is no effective way to visualize the content of a directory. This paper proposes a novel directory visualization framework, called analytics-driven dynamic visualization on digital directory (AVDD): understanding user preferences via smartphone-based interaction and optimizing visualization by visual analytics in terms of high content relevancy and screen utilization for advanced directories. With experiments in laboratory and real-world settings, AVDD is proven to be effective for visualizing directory with screen utilization over 98% and the score for Likert-scale surveys achieving 73% on average in a movie directory.
C1 [Cheung, Ming; She, James; Park, Soochang] Hong Kong Univ Sci & Technol, Elect & Comp Engn Dept, Hong Kong, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Cheung, M (corresponding author), Hong Kong Univ Sci & Technol, Elect & Comp Engn Dept, Hong Kong, Hong Kong, Peoples R China.
EM cpming@ust.hk; eejames@ust.hk; eewinter@ust.hk
OI Cheung, Ming/0000-0003-4646-1980
FU HKUST-NIE Social Media Laboratory HKUST
FX This work was supported by the HKUST-NIE Social Media Laboratory HKUST.
   The guest editor coordinating the review of this manuscript and
   approving it for publication was Prof. Yingcai Wu.
CR Alt Florian, 2013, P SIGCHI C HUM FACT, P1709, DOI [10.1145/2470654.2466226, DOI 10.1145/2470654.2466226]
   [Anonymous], 2011, INT C WORLD WIDE WEB, DOI DOI 10.1145/1963405.1963481
   [Anonymous], SERIES HUMAN COMPUTE
   [Anonymous], P 9 INT C MOB UB MUL
   [Anonymous], P 11 INT C MOB UB MU
   [Anonymous], 1979, COMPUTERS INTRACTABI
   [Anonymous], WI J MOB DIGI COMM N
   Bettman J.R., 1991, HDB CONSUMER BEHAV, P50
   Brignull Harry, 2003, Proc. Interact, V53, P17
   Cardoso J., 2012, P 4 ACM SIGCHI S ENG, P51
   Cheung M, 2015, IEEE T MULTIMEDIA, V17, P1417, DOI 10.1109/TMM.2015.2460192
   Choi Jaz Hee-Jeong, 2011, 2011 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops 2011). PerCom-Workshops 2011: 2011 IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOM Workshops 2011), P508, DOI 10.1109/PERCOMW.2011.5766942
   Clinch S, 2013, IEEE PERVAS COMPUT, V12, P92, DOI 10.1109/MPRV.2013.16
   Dalsgaard P, 2011, LECT NOTES COMPUT SC, V6947, P212, DOI 10.1007/978-3-642-23771-3_17
   Friday A, 2012, COMPUTER, V45, P34, DOI 10.1109/MC.2012.155
   Han SG, 2015, ACM T INFORM SYST, V33, DOI 10.1145/2738036
   Hardy R, 2009, FIRST INTERNATIONAL WORKSHOP ON NEAR FIELD COMMUNICATION, PROCEEDINGS, P36, DOI 10.1109/NFC.2009.10
   Huang EM, 2008, LECT NOTES COMPUT SC, V5013, P228, DOI 10.1007/978-3-540-79576-6_14
   José R, 2008, IEEE PERVAS COMPUT, V7, P52, DOI 10.1109/MPRV.2008.74
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuo Byron YL, 2007, P 16 INT C WORLD WID, P1203, DOI [DOI 10.1145/1242572.1242766, 10.1145/1242572.1242766]
   Lamothe Jean-Loup, 2013, 2013 IEEE International Conference on Green Computing and Communications (GreenCom) and IEEE Internet of Things (iThings) and IEEE Cyber, Physical and Social Computing (CPSCom), P2007, DOI 10.1109/GreenCom-iThings-CPSCom.2013.376
   Lamothe JL, 2014, 2014 IEEE 12TH INTERNATIONAL CONFERENCE ON DEPENDABLE, AUTONOMIC AND SECURE COMPUTING (DASC)/2014 IEEE 12TH INTERNATIONAL CONFERENCE ON EMBEDDED COMPUTING (EMBEDDEDCOM)/2014 IEEE 12TH INTERNATIONAL CONF ON PERVASIVE INTELLIGENCE AND COMPUTING (PICOM), P271, DOI 10.1109/DASC.2014.55
   Lander Christian., 2015, Proceedings of the 4th International Symposium on Pervasive Displays, P163
   Lee J, 2013, INT CONF ADV COMMUN, P978
   Li Y, 2005, EXPERT SYST APPL, V28, P67, DOI 10.1016/j.eswa.2004.08.013
   Likert R., 1932, ARCH PSYCHOL, V140, P55
   Lohmann S, 2009, LECT NOTES COMPUT SC, V5726, P392, DOI 10.1007/978-3-642-03655-2_43
   Mayer JM, 2015, ACM T INFORM SYST, V34, DOI 10.1145/2751557
   Müller J, 2009, LECT NOTES COMPUT SC, V5538, P1, DOI 10.1007/978-3-642-01516-8_1
   Ojala Timo, 2010, Proceedings of the Fifth International Conference on Internet and Web Applications and Services (ICIW 2010), P285, DOI 10.1109/ICIW.2010.49
   Pears N, 2009, IEEE PERVAS COMPUT, V8, P14, DOI 10.1109/MPRV.2009.35
   Peltonen P, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P1285
   Rivadeneira AW, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P995
   Sarwar B., 2001, P 10 INT C WORLD WID, P285, DOI 10.1145/371920.372071
   Seifert C, 2008, IEEE INT CONF INF VI, P17, DOI 10.1109/IV.2008.89
   She J, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2557450
   She J, 2013, IEEE T EMERG TOP COM, V1, P232, DOI 10.1109/TETC.2013.2282618
   Vanderdonckt J., 1994, Proceedings of the Workshop on Advanced Visual Interfaces AVI '94, P95, DOI 10.1145/192309.192334
   Xu RH, 2015, IEEE T MULTIMEDIA, V17, P1187, DOI 10.1109/TMM.2015.2438717
   Yeung KF, 2012, J INTERNET SERV APPL, V3, P195, DOI 10.1007/s13174-012-0061-3
NR 41
TC 3
Z9 3
U1 5
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2016
VL 18
IS 11
BP 2303
EP 2314
DI 10.1109/TMM.2016.2614222
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EA9BX
UT WOS:000386936900017
DA 2024-07-18
ER

PT J
AU Feng, QX
   Zhou, YC
AF Feng, Qingxiang
   Zhou, Yicong
TI Kernel Combined Sparse Representation for Disease Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Collaborative representation; disease recognition; sparse representation
ID ROBUST FACE RECOGNITION; COLLABORATIVE REPRESENTATION;
   LINEAR-REGRESSION; FEATURE DESCRIPTOR; PATTERN; CLASSIFICATION;
   SELECTION; SEGMENTATION; CLASSIFIERS; ENSEMBLE
AB Motivated by the idea that the correlation structure of the entire training set can disclose the relationship between the test sample and the training samples, we propose the combined sparse representation (CSR) classifier for disease recognition. The CSR classifier minimizes the correlation structure of the entire training set multiplied by its transposition and the sparse coefficient together for classification. Including the kernel concept, we propose the kernel combined sparse representation classifier utilizing the high-dimensional nonlinear information instead of the linear information in the CSR classifier. Furthermore, considering the information of the training samples and the class center, we then propose the center-based kernel combined sparse representation (CKCSR) classifier. CKCSR uses the center-based kernel matrix to increase the center-based information that is helpful for classification. The proposed classifiers have been evaluated by extensive experiments on several well-known databases including the EXACT09 database, Emphysema-CT database, mini-MIAS database, Wisconsin breast cancer database, and HD-PECTF database. The experimental results demonstrate that the proposed classifiers achieve better recognition rates than the sparse representation-based classification, collaborative representation based classification, and several state-of-the-art methods.
C1 [Feng, Qingxiang; Zhou, Yicong] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
C3 University of Macau
RP Zhou, YC (corresponding author), Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
EM fengqx1988@gmail.com; yicongzhou@umac.mo
RI Zhou, Yicong/A-8017-2009
OI Zhou, Yicong/0000-0002-4487-6384
FU Macau Science and Technology Development Fund [FDCT/106/2013/A3];
   Research Committee at University of Macau [MYRG2014-00003-FST,
   MYRG2016-00123-FST]
FX This work was supported in part by the Macau Science and Technology
   Development Fund under Grant FDCT/106/2013/A3 and in part by the
   Research Committee at University of Macau under Grant MYRG2014-00003-FST
   and Grant MYRG2016-00123-FST. The guest editor team coordinated the
   review of this manuscript and approved it for publication.
   (Corresponding author: Yicong Zhou.)
CR [Anonymous], 1994, DIGITAL MAMMO, DOI DOI 10.1007/S11999-016-4732-4
   [Anonymous], 2012, MATRIX COMPUTATIONS
   [Anonymous], 2016, P 25 INT JOINT C ART
   [Anonymous], MACHINE LEARNING /
   [Anonymous], 1994, CONTINUOUS UNIVARIAT
   [Anonymous], 2010, CORR
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Cai SJ, 2016, PROC CVPR IEEE, P2950, DOI 10.1109/CVPR.2016.322
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chai XJ, 2007, IEEE T IMAGE PROCESS, V16, P1716, DOI 10.1109/TIP.2007.899195
   Chien JT, 2002, IEEE T PATTERN ANAL, V24, P1644, DOI 10.1109/TPAMI.2002.1114855
   Chou YT, 2016, EURASIP J ADV SIG PR, DOI 10.1186/s13634-016-0328-0
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
   Cristianini N., 2000, INTRO SUPPORT VECTOR
   Deng WH, 2013, PROC CVPR IEEE, P399, DOI 10.1109/CVPR.2013.58
   Depeursinge A, 2012, IEEE T INF TECHNOL B, V16, P665, DOI 10.1109/TITB.2012.2198829
   Dubey SR, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2493446
   Dubey SR, 2015, IEEE SIGNAL PROC LET, V22, P1215, DOI 10.1109/LSP.2015.2392623
   Fang XZ, 2015, IEEE T IMAGE PROCESS, V24, P2760, DOI 10.1109/TIP.2015.2425545
   Feng QX, 2017, IEEE T CYBERNETICS, V47, P378, DOI 10.1109/TCYB.2016.2516239
   Feng QX, 2016, INT CONF ACOUST SPEE, P1566, DOI 10.1109/ICASSP.2016.7471940
   Feng QX, 2015, IEEE IMAGE PROC, P3630, DOI 10.1109/ICIP.2015.7351481
   Feng QX, 2013, IEEE IMAGE PROC, P3216, DOI 10.1109/ICIP.2013.6738662
   Feng QX, 2015, J MOD OPTIC, V62, P288, DOI 10.1080/09500340.2014.975848
   Gao SH, 2013, IEEE T IMAGE PROCESS, V22, P423, DOI 10.1109/TIP.2012.2215620
   Grave G. R., 2011, Adv. Neural Inf. Process.Syst., V24, P2187
   Gu SH, 2014, ADV NEUR IN, V27
   Hale ET, 2008, SIAM J OPTIMIZ, V19, P1107, DOI 10.1137/070698920
   Johnson, 2012, MATRIX ANAL
   Kong H, 2016, NEUROCOMPUTING, V177, P198, DOI 10.1016/j.neucom.2015.11.033
   Kurgan LA, 2001, ARTIF INTELL MED, V23, P149, DOI 10.1016/S0933-3657(01)00082-3
   Lai ZH, 2014, IEEE T NEUR NET LEAR, V25, P1942, DOI 10.1109/TNNLS.2013.2297381
   Lai ZH, 2014, IEEE T NEUR NET LEAR, V25, P1779, DOI 10.1109/TNNLS.2013.2295717
   Lan XY, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2481325
   Li SZ, 2000, IEEE T PATTERN ANAL, V22, P1335, DOI 10.1109/34.888719
   Li SZ, 1999, IEEE T NEURAL NETWOR, V10, P439, DOI 10.1109/72.750575
   Li W, 2015, IEEE GEOSCI REMOTE S, V12, P48, DOI 10.1109/LGRS.2014.2325978
   Liu G., 2010, P INT C MACH LEARN, P663
   Liu SD, 2013, IEEE ENG MED BIO, P5398, DOI 10.1109/EMBC.2013.6610769
   Liu YH, 2016, INT CONF SOFTW ENG, P1, DOI 10.1109/ICSESS.2016.7883004
   Lo P, 2012, IEEE T MED IMAGING, V31, P2093, DOI 10.1109/TMI.2012.2209674
   Lu YW, 2014, NEURAL COMPUT APPL, V24, P1843, DOI 10.1007/s00521-013-1435-6
   Ma L, 2012, PROC CVPR IEEE, P2586, DOI 10.1109/CVPR.2012.6247977
   Martìnez AM, 2001, IEEE T PATTERN ANAL, V23, P228, DOI 10.1109/34.908974
   Naseem I, 2010, IEEE T PATTERN ANAL, V32, P2106, DOI 10.1109/TPAMI.2010.128
   Nefian A. V., 2002, Proceedings 2002 IEEE International Conference on Multimedia and Expo (Cat. No.02TH8604), P133, DOI 10.1109/ICME.2002.1035530
   Pan JS, 2015, IEEE T CIRC SYST VID, V25, P387, DOI 10.1109/TCSVT.2014.2351092
   Parrado-Hernández E, 2014, MED IMAGE ANAL, V18, P435, DOI 10.1016/j.media.2014.01.006
   Shrivastava A, 2014, IEEE T IMAGE PROCESS, V23, P3013, DOI 10.1109/TIP.2014.2324290
   Song Y, 2015, IEEE T MED IMAGING, V34, P1362, DOI 10.1109/TMI.2015.2393954
   Song Y, 2014, I S BIOMED IMAGING, P1023, DOI 10.1109/ISBI.2014.6868047
   Song Y, 2013, IEEE T MED IMAGING, V32, P797, DOI 10.1109/TMI.2013.2241448
   Song Yong, 2012, Exp Diabetes Res, V2012, P254976, DOI 10.1155/2012/254976
   Sorensen L, 2010, IEEE T MED IMAGING, V29, P559, DOI 10.1109/TMI.2009.2038575
   Thomaz CE, 2010, IMAGE VISION COMPUT, V28, P902, DOI 10.1016/j.imavis.2009.11.005
   Tong T, 2013, NEUROIMAGE, V76, P11, DOI 10.1016/j.neuroimage.2013.02.069
   Wang B, 2013, INT CONF ACOUST SPEE, P2877, DOI 10.1109/ICASSP.2013.6638183
   Wang FQ, 2015, IEEE T NEUR NET LEAR, V26, P1950, DOI 10.1109/TNNLS.2014.2361142
   Wang J, 2014, IEEE T CYBERNETICS, V44, P2368, DOI 10.1109/TCYB.2014.2307067
   Wang L, 2014, NEUROIMAGE, V89, P152, DOI 10.1016/j.neuroimage.2013.11.040
   Weiss N, 2013, LECT NOTES COMPUT SC, V8149, P735, DOI 10.1007/978-3-642-40811-3_92
   Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xu Y, 2014, IEEE T CYBERNETICS, V44, P1950, DOI 10.1109/TCYB.2014.2300175
   Xu Y, 2013, INFORM SCIENCES, V238, P138, DOI 10.1016/j.ins.2013.02.051
   Xu Y, 2011, IEEE T CIRC SYST VID, V21, P1255, DOI 10.1109/TCSVT.2011.2138790
   Yang J, 2013, IEEE T NEUR NET LEAR, V24, P1023, DOI 10.1109/TNNLS.2013.2249088
   Yang M, 2014, INT J COMPUT VISION, V109, P209, DOI 10.1007/s11263-014-0722-8
   Yang M, 2013, IEEE T IMAGE PROCESS, V22, P1753, DOI 10.1109/TIP.2012.2235849
   Yang M, 2012, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2012.6247931
   Yaqub M, 2014, IEEE T MED IMAGING, V33, P258, DOI 10.1109/TMI.2013.2284025
   Yu ZW, 2016, IEEE T CYBERNETICS, V46, P1263, DOI 10.1109/TCYB.2015.2443857
   Yu ZW, 2016, IEEE T KNOWL DATA EN, V28, P701, DOI 10.1109/TKDE.2015.2499200
   Yu ZW, 2015, IEEE T KNOWL DATA EN, V27, P3176, DOI 10.1109/TKDE.2015.2453162
   Yu ZW, 2015, IEEE T CYBERNETICS, V45, P177, DOI 10.1109/TCYB.2014.2322195
   Yu ZW, 2014, IEEE ACM T COMPUT BI, V11, P727, DOI 10.1109/TCBB.2014.2315996
   Zhang F, 2014, IEEE T BIO-MED ENG, V61, P1155, DOI 10.1109/TBME.2013.2295593
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhang L, 2015, MULTIMED TOOLS APPL, V74, P123, DOI 10.1007/s11042-013-1457-1
   Zhang L, 2012, IEEE T SIGNAL PROCES, V60, P1684, DOI 10.1109/TSP.2011.2179539
   Zhang ST, 2012, MED IMAGE ANAL, V16, P1385, DOI 10.1016/j.media.2012.07.007
   Zhang S, 2013, LECT NOTES COMPUT SC, V8151, P626, DOI 10.1007/978-3-642-40760-4_78
NR 82
TC 17
Z9 18
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2016
VL 18
IS 10
BP 1956
EP 1968
DI 10.1109/TMM.2016.2602062
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DX8NC
UT WOS:000384644800004
DA 2024-07-18
ER

PT J
AU Gao, Y
   Zhen, Y
   Li, HJ
   Chua, TS
AF Gao, Yue
   Zhen, Yi
   Li, Haojie
   Chua, Tat-Seng
TI Filtering of Brand-Related Microblogs Using Social-Smooth Multiview
   Embedding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Microblog filtering; multiview embedding; social graph
AB In recent years, we have witnessed the boom of social media platforms, through which people have been generating a lot of social media data. This data touches almost every aspect of life and may have significant societal and marketing values for a variety of corporations and organizations. Thus, the development of effective techniques for gathering and analyzing social media content has attracted much research attention. As social media data tend to be heterogeneous, conversational, and fast evolving in content, a recent work reported a multifaceted approach to gather comprehensive brand-related data by crawling data using evolving keywords, key users, similar image content, and known locations. Although such approach has been found to be effective in gathering representative data, it also brings in a lot of noise. This paper aims to develop an accurate classifier to filter out noise by taking into account the multimedia content and social nature of brand-related data. In particular, we develop a microblog filtering method based on a discriminative social-aware multiview embedding. Besides the conventional content-based features, such as textual, low-level visual features, and high-level visual semantic features, that form the three key views of microblogs, we also incorporate the brand and social relations among the microblogs to learn a discriminative and social-aware embedding. With such a learned embedding, an off-the-shelf classifier, such as SVM, can then be trained and applied to microblog filtering. We verify the efficacy of our method on noise filtering in the brand data gathering task on the Brand-Social-Net dataset. Our approach is able to achieve significantly better filtering performance and improve the quality of brand data gathering.
C1 [Gao, Yue] Tsinghua Univ, Sch Software, Key Lab Informat Syst Secur, Minist Educ,TNList, Beijing 100084, Peoples R China.
   [Zhen, Yi] Georgia Inst Technol, Coll Comp, Atlanta, GA 30308 USA.
   [Li, Haojie] Dalian Univ Technol, Sch Software, Dalian 116000, Peoples R China.
   [Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
C3 Tsinghua University; University System of Georgia; Georgia Institute of
   Technology; Dalian University of Technology; National University of
   Singapore
RP Li, HJ (corresponding author), Dalian Univ Technol, Sch Software, Dalian 116000, Peoples R China.
EM hjli@dlut.edu.cn
RI Gao, Yue/B-3376-2012
FU NSFC Program [91218302, 61527812, 61472059]; National Science and
   Technology Major Project [2016ZX01038101]; MIIT IT funds of China;
   National Key Technology RD Program [2015BAG14B01-02]; National Research
   Foundation, Prime Minister's Office, Singapore, under its IRC@SG Funding
   Initiative
FX This work was supported in part by the NSFC Program under Grant
   91218302, Grant 61527812, and Grant 61472059, in part by the National
   Science and Technology Major Project under Grant 2016ZX01038101, in part
   by the MIIT IT funds (research and application of TCN key technologies)
   of China, and in part by The National Key Technology R&D Program under
   Grant 2015BAG14B01-02. This work was also supported by the National
   Research Foundation, Prime Minister's Office, Singapore, under its
   IRC@SG Funding Initiative. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Zhen Wen.
CR Anand S, 2013, IEEE T MULTIMEDIA, V15, P207, DOI 10.1109/TMM.2012.2225031
   [Anonymous], 2010, P INT C WEBL SOC MED
   [Anonymous], P HUM LANG TECHN ANN
   Batool R, 2013, 2013 IEEE/ACIS 12TH INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION SCIENCE (ICIS), P461, DOI 10.1109/ICIS.2013.6607883
   Bian JW, 2015, IEEE T MULTIMEDIA, V17, P216, DOI 10.1109/TMM.2014.2384912
   Blaschko MB, 2008, LECT NOTES ARTIF INT, V5211, P133, DOI 10.1007/978-3-540-87479-9_27
   Chen C., 2011, SIGMOD, P649, DOI DOI 10.1145/1989323.1989391
   Chen N, 2012, IEEE T PATTERN ANAL, V34, P2365, DOI 10.1109/TPAMI.2012.64
   Chunmei Gu, 2012, 2012 International Conference on Business Computing and Global Informatization (BCGIN), P537, DOI 10.1109/BCGIN.2012.146
   Fang Q, 2015, IEEE T MULTIMEDIA, V17, P1031, DOI 10.1109/TMM.2015.2430819
   Fang Q, 2014, IEEE T MULTIMEDIA, V16, P796, DOI 10.1109/TMM.2014.2298216
   Gao Y., 2014, P 4 ACM C MULT RETR
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hu Xia., 2009, Proceedings of the 18th ACM Conference on Information and Knowledge Management, Hong Kong, China, P919
   Huang J., 2010, Proceedings of the 21st ACM conference on Hypertext and hypermedia, HT '10, P173, DOI DOI 10.1145/1810617.1810647
   Jin YC, 2014, IEEE T MULTIMEDIA, V16, P1739, DOI 10.1109/TMM.2014.2329370
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Aiello LM, 2013, IEEE T MULTIMEDIA, V15, P1268, DOI 10.1109/TMM.2013.2265080
   Massoudi K, 2011, LECT NOTES COMPUT SC, V6611, P362, DOI 10.1007/978-3-642-20161-5_36
   Mishne G, 2007, P INT C WEBL SOC MED
   Nagmoti Rinkesh, 2010, Proceedings 2010 IEEE/ACM International Conference on Web Intelligence-Intelligent Agent Technology (WI-IAT), P153, DOI 10.1109/WI-IAT.2010.170
   Naveed Nasir., 2011, Proceedings of the 20th ACM international conference on Information and knowledge management, P183
   Ntalianis K., 2015, MULTIMED TOOLS APPL, P1
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Quadrianto Novi., 2011, ICML, P425
   Rai P., 2009, P NEUR INF PROC SYST
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Reuter T., 2012, PROC ANN ACM INT C M, P22
   Reuter T., 2011, P INT C WEBL SOC MED
   Rowlands T., 2010, P 19 INT C WORLD WID, P1293
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   Sui Yue, 2010, 2010 Second International Conference on Communication Systems, Networks and Applications (ICCSNA 2010), P164, DOI 10.1109/ICCSNA.2010.5588676
   Teevan J., 2011, P 4 ACM INT C WEB SE, P35, DOI DOI 10.1145/1935826.1935842
   Uysal I., 2011, CIKM, P2261, DOI DOI 10.1145/2063576.2063941
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Weerkamp W., 2008, P 46 ANN M ASS COMP
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Zangerle E, 2013, SOC NETW ANAL MIN, V3, P889, DOI 10.1007/s13278-013-0108-x
   Zhou D., 2006, ADV NEURAL INFORM PR, P1601, DOI DOI 10.7551/MITPRESS/7503.003.0205
NR 39
TC 33
Z9 34
U1 3
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2016
VL 18
IS 10
BP 2115
EP 2126
DI 10.1109/TMM.2016.2581483
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DX8NC
UT WOS:000384644800017
DA 2024-07-18
ER

PT J
AU Jang, S
   Lee, JS
AF Jang, Soobeom
   Lee, Jong-Seok
TI On Evaluating Perceptual Quality of Online User-Generated Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Metadata; paired comparison; quality assessment; user-generated video
ID PREDICTION
AB This paper deals with the issue of the perceptual quality evaluation of user-generated videos shared online, which is an important step toward designing video-sharing services that maximize users' satisfaction in terms of quality. We first analyze viewers' quality perception patterns by applying graph analysis techniques to subjective rating data. We then examine the performance of existing state-of-the-art objective metrics for the quality estimation of user-generated videos. In addition, we investigate the feasibility of metadata accompanied with videos in online video-sharing services for quality estimation. Finally, various issues in the quality assessment of online user-generated videos are discussed, including difficulties and opportunities.
C1 [Jang, Soobeom; Lee, Jong-Seok] Yonsei Univ, Sch Integrated Technol, Inchon 21983, South Korea.
   [Jang, Soobeom; Lee, Jong-Seok] Yonsei Univ, Yonsei Inst Convergence Technol, Inchon 21983, South Korea.
C3 Yonsei University; Yonsei University
RP Jang, S (corresponding author), Yonsei Univ, Sch Integrated Technol, Inchon 21983, South Korea.; Jang, S (corresponding author), Yonsei Univ, Yonsei Inst Convergence Technol, Inchon 21983, South Korea.
EM soobeom.jang@yonsei.ac.kr; jong-seok.lee@yonsei.ac.kr
RI Lee, Jong-Seok/AAF-5197-2020
OI Lee, Jong-Seok/0000-0001-5255-4425
FU Ministry of Science, ICT, and Future Planning, South Korea, under the
   "IT Consilience Creative Program" [IITP-2015-R0346-15-1008]
FX This work was supported by the Ministry of Science, ICT, and Future
   Planning, South Korea, under the "IT Consilience Creative Program"
   (IITP-2015-R0346-15-1008) supervised by the Institute for Information
   and Communications Technology Promotion. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Xiaokang Yang.
CR [Anonymous], 2005, 1 INT WORKSHOP VIDEO
   Cha M, 2009, IEEE ACM T NETWORK, V17, P1357, DOI 10.1109/TNET.2008.2011358
   Chul-Hee Han, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1385, DOI 10.1109/ICASSP.2014.6853824
   Davidson James, 2010, P 4 ACM C REC SYST, P293, DOI [DOI 10.1145/1864708.1864770, 10.1145/1864708]
   Drucker H, 1997, ADV NEUR IN, V9, P155
   Duan Dongsheng., 2009, P 1 ACM INT WORKSHOP, P11, DOI DOI 10.1145/1651274.1651278
   Figueiredo F, 2014, ACM T INTERNET TECHN, V14, DOI 10.1145/2665065
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Hossfeld T, 2014, LECT NOTES COMPUT SC, V8376, P136, DOI 10.1007/978-3-319-05359-2_10
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lee JS, 2014, IEEE T MULTIMEDIA, V16, P564, DOI 10.1109/TMM.2013.2292590
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Saad MA, 2014, IEEE T IMAGE PROCESS, V23, P1352, DOI 10.1109/TIP.2014.2299154
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P335, DOI 10.1109/TIP.2009.2034992
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Soundararajan R, 2013, IEEE T CIRC SYST VID, V23, P684, DOI 10.1109/TCSVT.2012.2214933
   Vu P. V., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2505, DOI 10.1109/ICIP.2011.6116171
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wilk S, 2014, IEEE INT CON MULTI
   Xia T, 2010, J VIS COMMUN IMAGE R, V21, P826, DOI 10.1016/j.jvcir.2010.06.005
   Xu Q, 2012, IEEE T MULTIMEDIA, V14, P844, DOI 10.1109/TMM.2012.2190924
   Yang Y, 2014, INFORM SCIENCES, V281, P601, DOI 10.1016/j.ins.2014.03.016
NR 27
TC 6
Z9 6
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2016
VL 18
IS 9
BP 1808
EP 1818
DI 10.1109/TMM.2016.2581582
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DT4GI
UT WOS:000381437800011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU He, QY
   Liu, JC
   Wang, CG
   Li, B
AF He, Qiyun
   Liu, Jiangchuan
   Wang, Chonggang
   Li, Bo
TI Coping With Heterogeneous Video Contributors and Viewers in Crowdsourced
   Live Streaming: A Cloud-Based Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cloud computing; crowdsourced live streaming; video transcoding
AB With the advances in personal computing devices and the prevalence of broadband network and wireless mobile network accesses, end-users are no longer pure content consumers, but contributors, too. In today's crowdsourced streaming systems, numerous broadcasters lively stream their video content, e.g., live events or online game scenes, to fellow viewers. Compared to professional video producers and broadcasters, these new generation broadcasters are geo-distributed globally and highly heterogeneous in terms of the generated video quality and the network/system configurations. The scalability and heterogeneity challenges therefore lie on both broadcasters and the viewers, which call for massive transcoding, and two critical issues: 1) choosing video representation set that maximizes viewer satisfaction and 2) allocating computational resources that minimize operational costs, must be systematically optimized in the global scale. In this paper, we present a generic framework utilizing the powerful and elastic cloud computing services for crowdsourced live streaming with heterogeneous broadcasters and viewers. We jointly consider the viewer satisfaction and the service availability/pricing of geo-distributed cloud resources for transcoding. We develop an optimal scheduler for allocating cloud instances with no regional constraints. We then extend the solution to accommodate regional constraints, and discuss a series of practical enhancements, including popularity forecasting, initialization latency, and viewer feedbacks. Our solutions have been evaluated under diverse networks and cloud system configurations as well as parameter settings. The trace-driven simulation confirms the superiority of our design, while our Planetlab-based experiment offers further practical hints toward real-world migration.
C1 [He, Qiyun; Liu, Jiangchuan] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
   [Wang, Chonggang] InterDigital Commun, King Of Prussia, PA 19404 USA.
   [Li, Bo] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Simon Fraser University; InterDigital; Hong Kong University of Science &
   Technology
RP He, QY; Liu, JC (corresponding author), Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.; Wang, CG (corresponding author), InterDigital Commun, King Of Prussia, PA 19404 USA.; Li, B (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
EM qiyunh@sfu.ca; jcliu@cs.sfu.ca; chonggang.wang@interdigital.com;
   bli@cse.ust.hk
RI Li, Bo/AAA-8968-2020; Li, bo/IWL-9318-2023
OI Li, Bo/0000-0002-7294-6888; 
FU NSERC Discovery Grant; NSERC Strategic Project Grant; RGC (CRF) [615613,
   16211715, C7036-15G]; NSF (China) [U1301253]
FX This work was supported by an NSERC Discovery Grant and an NSERC
   Strategic Project Grant. The work of B. Li was supported in part by the
   RGC under Contract 615613, Contract 16211715, and Contract C7036-15G
   (CRF), and in part by the NSF (China) under Contract U1301253. The guest
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Dapeng Oliver Wu.
CR Adhikari VK, 2012, IEEE INFOCOM SER, P1620, DOI 10.1109/INFCOM.2012.6195531
   Aggarwal V., 2012, COMM SYST NETW COMSN, P1
   [Anonymous], AZ SUBSCR SERV LIM Q
   [Anonymous], AM EC2 FAQ
   [Anonymous], 2011, Proc. second annu. acm conf. multimed. syst.-mmsys'11, DOI DOI 10.1145/1943552.1943574
   [Anonymous], AM EC2 PRIC
   [Anonymous], 2014, WORKSHOP DESIGN QUAL
   [Anonymous], 2014, P 5 ACM MULT SYST C, DOI DOI 10.1145/2557642.2563671
   Aparicio-Pardo Ramon., 2015, Proceedings of the 6th ACM Multimedia Systems Conference, MMSys'15, P49
   Dai H., 2011, Acta Horticulturae, P169, DOI 10.1145/1943552.1943575
   Gill P, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P15
   Huang ZX, 2011, IEEE INFOCOM SER, P201, DOI 10.1109/INFCOM.2011.5935009
   Li BC, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2505805
   Liu JC, 2008, P IEEE, V96, P11, DOI 10.1109/JPROC.2007.909921
   Toni Laura., 2014, Proceedings of_the_5th_ACM_Multimedia_Systems_Conference, P271
   Wang F, 2012, IEEE INFOCOM SER, P199, DOI 10.1109/INFCOM.2012.6195578
   Wang Z, 2014, IEEE INFOCOM SER, P91, DOI 10.1109/INFOCOM.2014.6847928
   Wu Y, 2012, IEEE INFOCOM SER, P684, DOI 10.1109/INFCOM.2012.6195813
   Xi ZY, 2015, 2015 INTERNATIONAL CONFERENCE ON COMPUTERS, COMMUNICATIONS, AND SYSTEMS (ICCCS), P252, DOI 10.1109/CCOMS.2015.7562910
   Zhang C., 2015, ACM Workshop on Network and Operating Systems Support for Digital Audio and Video, P55, DOI DOI 10.1145/2736084.2736091
   Zhang GP, 2003, NEUROCOMPUTING, V50, P159, DOI 10.1016/S0925-2312(01)00702-0
NR 21
TC 51
Z9 53
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2016
VL 18
IS 5
SI SI
BP 916
EP 928
DI 10.1109/TMM.2016.2544698
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DK5YD
UT WOS:000374996200011
DA 2024-07-18
ER

PT J
AU Peng, MG
   Yu, YL
   Xiang, HY
   Poor, HV
AF Peng, Mugen
   Yu, Yuling
   Xiang, Hongyu
   Poor, H. Vincent
TI Energy-Efficient Resource Allocation Optimization for Multimedia
   Heterogeneous Cloud Radio Access Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Heterogeneous cloud radio access networks; Lyapunov optimization;
   multimedia traffic; queue-aware
ID POWER; DOWNLINK; TRADEOFF; SYSTEMS
AB The heterogeneous cloud radio access network (H-CRAN) is a promising paradigm that incorporates cloud computing into heterogeneous networks (HetNets), thereby taking full advantage of cloud radio access networks (C-RANs) and HetNets. Characterizing cooperative beamforming with fronthaul capacity and queue stability constraints is critical for multimedia applications to improve the energy efficiency (EE) in H-CRANs. An energy-efficient optimization objective function with individual fronthaul capacity and intertier interference constraints is presented in this paper for queue-aware multimedia H-CRANs. To solve this nonconvex objective function, a stochastic optimization problem is reformulated by introducing the general Lyapunov optimization framework. Under the Lyapunov framework, this optimization problem is equivalent to an optimal network-wide cooperative beamformer design algorithm with instantaneous power, average power, and intertier interference constraints, which can be regarded as a weighted sum EE maximization problem and solved by a generalized weighted minimum mean-square error approach. The mathematical analysis and simulation results demonstrate that a tradeoff between EE and queuing delay can be achieved, and this tradeoff strictly depends on the fronthaul constraint.
C1 [Peng, Mugen; Yu, Yuling; Xiang, Hongyu] Beijing Univ Posts & Telecommun, Minist Educ, Key Lab Universal Wireless Commun, Beijing 100876, Peoples R China.
   [Poor, H. Vincent] Princeton Univ, Sch Engn & Appl Sci, Princeton, NJ 08544 USA.
C3 Beijing University of Posts & Telecommunications; Princeton University
RP Peng, MG; Yu, YL; Xiang, HY (corresponding author), Beijing Univ Posts & Telecommun, Minist Educ, Key Lab Universal Wireless Commun, Beijing 100876, Peoples R China.; Poor, HV (corresponding author), Princeton Univ, Sch Engn & Appl Sci, Princeton, NJ 08544 USA.
EM pmg@bupt.edu.cn; aliceyu1215@gmail.com; xianghongyu88@163.com;
   poor@princeton.edu
RI peng, mugen/D-3023-2012; Poor, H. Vincent/S-5027-2016
OI peng, mugen/0000-0002-4755-7231; Poor, H. Vincent/0000-0002-2062-131X
FU National Natural Science Foundation of China [61361166005]; National
   High Technology Research and Development Program of China
   [2014AA01A701]; National Basic Research Program of China (973 Program)
   [2013CB336600]; U.S. National Science Foundation [ECCS-1343210]; Div Of
   Electrical, Commun & Cyber Sys; Directorate For Engineering [1343210]
   Funding Source: National Science Foundation
FX The work of M. Peng was supported in part by the National Natural
   Science Foundation of China under Grant 61361166005, the National High
   Technology Research and Development Program of China under Grant
   2014AA01A701, and the National Basic Research Program of China (973
   Program) under Grant 2013CB336600. The work of H. V. Poor was supported
   in part by the U.S. National Science Foundation under Grant
   ECCS-1343210. The guest editor coordinating the review of this
   manuscript and approving it for publication was Prof. Dapeng Oliver Wu.
CR Candès EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x
   Cao DX, 2013, IEEE T WIREL COMMUN, V12, P4129, DOI 10.1109/TWC.2013.052213.121751
   Chen JT, 2013, IEEE T SIGNAL PROCES, V61, P4067, DOI 10.1109/TSP.2013.2265681
   Christensen SS, 2008, IEEE T WIREL COMMUN, V7, P4792, DOI 10.1109/T-WC.2008.070851
   Dai BB, 2013, IEEE GLOB COMM CONF, P1962, DOI 10.1109/GLOCOM.2013.6831362
   Dai BB, 2014, IEEE ACCESS, V2, P1326, DOI 10.1109/ACCESS.2014.2362860
   He CL, 2013, IEEE J SEL AREA COMM, V31, P894, DOI 10.1109/JSAC.2013.130508
   Joham M, 2005, IEEE T SIGNAL PROCES, V53, P2700, DOI 10.1109/TSP.2005.850331
   Ju HH, 2013, IEEE T WIREL COMMUN, V12, P5668, DOI 10.1109/TWC.2013.092513.121922
   Kaviani S, 2012, IEEE T VEH TECHNOL, V61, P2083, DOI 10.1109/TVT.2012.2187710
   Lau VKN, 2010, IEEE T WIREL COMMUN, V9, P227, DOI 10.1109/TWC.2010.01.090031
   Liu A, 2014, IEEE T SIGNAL PROCES, V62, P1319, DOI 10.1109/TSP.2014.2298367
   Meshkati F, 2009, IEEE T COMMUN, V57, P3406, DOI 10.1109/TCOMM.2009.11.050638
   Meshkati F, 2009, IEEE T INFORM THEORY, V55, P3220, DOI 10.1109/TIT.2009.2021374
   Neely M. J., 2010, STOCHASTIC NETWORK O
   Park SH, 2013, IEEE T SIGNAL PROCES, V61, P5646, DOI 10.1109/TSP.2013.2280111
   Peng M, 2015, IEEE ACCESS, V3, P2441, DOI 10.1109/ACCESS.2015.2497268
   Peng MG, 2015, IEEE WIREL COMMUN, V22, P152, DOI 10.1109/MWC.2015.7096298
   Peng MG, 2015, IEEE NETWORK, V29, P6, DOI 10.1109/MNET.2015.7064897
   Peng MG, 2014, IEEE WIREL COMMUN, V21, P126, DOI 10.1109/MWC.2014.7000980
   Shi QJ, 2011, IEEE T SIGNAL PROCES, V59, P4331, DOI 10.1109/TSP.2011.2147784
   Shi YM, 2014, IEEE T WIREL COMMUN, V13, P2809, DOI 10.1109/TWC.2014.040214.131770
   Stai E, 2014, IEEE COMMUN LETT, V18, P1999, DOI 10.1109/LCOMM.2014.2358217
   Urgaonkar R, 2012, IEEE J SEL AREA COMM, V30, P607, DOI 10.1109/JSAC.2012.120410
   Xie RC, 2012, IEEE T WIREL COMMUN, V11, P3910, DOI 10.1109/TWC.2012.092112.111510
   Yang J, 2011, IEEE J SEL AREA COMM, V29, P988, DOI 10.1109/JSAC.2011.110509
   Zhou ZY, 2014, IEEE WIREL COMMUN LE, V3, P485, DOI 10.1109/LWC.2014.2337295
NR 27
TC 93
Z9 95
U1 1
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2016
VL 18
IS 5
SI SI
BP 879
EP 892
DI 10.1109/TMM.2016.2535722
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DK5YD
UT WOS:000374996200008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Song, JR
   Yang, FZ
   Zhou, YC
   Wan, S
   Wu, HR
AF Song, Jiarun
   Yang, Fuzheng
   Zhou, Yicong
   Wan, Shuai
   Wu, Hong Ren
TI QoE Evaluation of Multimedia Services Based on Audiovisual Quality and
   User Interest
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audiovisual quality; audiovisual services; quality of experience (QoE);
   user interest; viewing behavior
ID VISUAL-ATTENTION; EXPERIENCE; VIDEO; MODEL
AB Quality of experience (QoE) has significant influence on whether or not a user will choose a service or product in the competitive era. For multimedia services, there are various factors in a communication ecosystem working together on users, which stimulate their different senses inducing multidimensional perceptions of the services, and inevitably increase the difficulty in measurement and estimation of the user's QoE. In this paper, a user-centric objective QoE evaluation model (QAVIC model for short) is proposed to estimate the user's overall QoE for audiovisual services, which takes account of perceptual audiovisual quality (QAV) and user interest in audiovisual content (IC) amongst influencing factors on QoE such as technology, content, context, and user in the communication ecosystem. To predict the user interest, a number of general viewing behaviors are considered to formulate the IC evaluation model. Subjective tests have been conducted for training and validation of the QAVIC model. The experimental results show that the proposed QAVIC model can estimate the user's QoE reasonably accurately using a 5-point scale absolute category rating scheme.
C1 [Song, Jiarun; Yang, Fuzheng] Xidian Univ, Collaborat Innovat Ctr Informat Sensing & Underst, Xian 710071, Peoples R China.
   [Song, Jiarun; Yang, Fuzheng] Xidian Univ, ISN, Xian 710071, Peoples R China.
   [Zhou, Yicong] Univ Macau, Dept Comp & Informat Sci, Macau, Peoples R China.
   [Wan, Shuai] Northwestern Polytech Univ, Sch Elect & Informat, Xian 710129, Peoples R China.
   [Wu, Hong Ren] RMIT Univ, Sch Elect & Comp Engn, Melbourne, Vic 3001, Australia.
C3 Xidian University; Xidian University; University of Macau; Northwestern
   Polytechnical University; Royal Melbourne Institute of Technology (RMIT)
RP Song, JR; Yang, FZ (corresponding author), Xidian Univ, Collaborat Innovat Ctr Informat Sensing & Underst, Xian 710071, Peoples R China.; Song, JR; Yang, FZ (corresponding author), Xidian Univ, ISN, Xian 710071, Peoples R China.; Zhou, YC (corresponding author), Univ Macau, Dept Comp & Informat Sci, Macau, Peoples R China.; Wan, S (corresponding author), Northwestern Polytech Univ, Sch Elect & Informat, Xian 710129, Peoples R China.; Wu, HR (corresponding author), RMIT Univ, Sch Elect & Comp Engn, Melbourne, Vic 3001, Australia.
EM jrsong@stu.xidian.edu.cn; fzhyang@mail.xidian.edu.cn;
   yicongzhou@umac.mo; swan@nwpu.edu.cn; henry.wu@rmit.edu.au
RI Wu, Hong Ren/C-6068-2014; Zhou, Yicong/A-8017-2009; Wan,
   Shuai/AAA-8777-2022
OI Wu, Hong Ren/0000-0002-7086-1629; Zhou, Yicong/0000-0002-4487-6384; Wan,
   Shuai/0000-0001-8617-149X
FU NSF of China [61371089, 61571337]; 111 Project [B08038]
FX This work was supported in part by the NSF of China under Grant 61371089
   and Grant 61571337, and in part by the 111 Project B08038. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Christian Timmerer.
CR [Anonymous], 2001, 6th International Conference on Intelligent User Interfaces (IUI 2001), DOI DOI 10.1145/359784.359836
   [Anonymous], 2001, 1449632001 INT ORG S
   [Anonymous], P 21 INT C RAD BRN C
   [Anonymous], 2007, OP MOD VID TEL APPL
   [Anonymous], P 4 INT C MEAS SPEEC
   [Anonymous], 2012, PAR NON ASS AUD MED
   [Anonymous], 2012, document Rec. ITU-R BT.500-11
   [Anonymous], 2013, White Paper
   [Anonymous], 2007, DEF QUAL EXP QOE
   [Anonymous], 2014, METH SUBJ ASS VID QU
   [Anonymous], 2011, DOING DATA ANAL SPSS
   Argyle Michael, 1975, BODILY COMMUNICATION
   BALLON P., 2005, P 16 EUR REG C INT T, P4
   Bradley AP, 2003, J VIS COMMUN IMAGE R, V14, P232, DOI 10.1016/S1047-3203(03)00037-3
   Brooks P, 2010, IEEE NETWORK, V24, P8, DOI 10.1109/MNET.2010.5430138
   Chen ZZ, 2010, IEEE T CIRC SYST VID, V20, P806, DOI 10.1109/TCSVT.2010.2045912
   De Moor K., 2014, P SPIE IS T ELECT IM, P2458
   De Pessemier T, 2013, IEEE T BROADCAST, V59, P47, DOI 10.1109/TBC.2012.2220231
   Dibeklioglu H, 2011, J MULTIMODAL USER IN, V4, P81, DOI 10.1007/s12193-011-0057-5
   Fiedler M, 2010, IEEE NETWORK, V24, P36, DOI 10.1109/MNET.2010.5430142
   Frischen A, 2007, PSYCHOL BULL, V133, P694, DOI 10.1037/0033-2909.133.4.694
   Geerts David, 2010, Proceedings of the 2010 Second International Workshop on Quality of Multimedia Experience (QoMEX 2010), P158, DOI 10.1109/QOMEX.2010.5516292
   Hands DS, 2004, IEEE T MULTIMEDIA, V6, P806, DOI 10.1109/TMM.2004.837233
   Hansen DW, 2010, IEEE T PATTERN ANAL, V32, P478, DOI 10.1109/TPAMI.2009.30
   Hassenzahl M, 2004, HUM-COMPUT INTERACT, V19, P319, DOI 10.1207/s15327051hci1904_2
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Jain R, 2004, IEEE MULTIMEDIA, V11, P96, DOI 10.1109/MMUL.2004.1261114
   Jeffrey S. N., 2009, PSYCHOL CONCEPTS APP
   Kilkki K, 2008, J UNIVERS COMPUT SCI, V14, P615
   Kim HL, 2010, INT CONF ADV COMMUN, P1377
   Kim HJ, 2008, NCM 2008: 4TH INTERNATIONAL CONFERENCE ON NETWORKED COMPUTING AND ADVANCED INFORMATION MANAGEMENT, VOL 2, PROCEEDINGS, P719, DOI 10.1109/NCM.2008.202
   Laghari KUR, 2012, IEEE COMMUN MAG, V50, P58, DOI 10.1109/MCOM.2012.6178834
   Lassalle J., 2012, IRGSC WORKING PAPERS, P1
   Le Meur O, 2006, IEEE T PATTERN ANAL, V28, P802, DOI 10.1109/TPAMI.2006.86
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Moller S, 2014, T-LAB SER TELECOMMUN, P1, DOI 10.1007/978-3-319-02681-7
   Nakano T, 2009, P ROY SOC B-BIOL SCI, V276, P3635, DOI 10.1098/rspb.2009.0828
   Peng WT, 2011, IEEE T MULTIMEDIA, V13, P539, DOI 10.1109/TMM.2011.2131638
   Pinson MH, 2011, IEEE SIGNAL PROC MAG, V28, P60, DOI 10.1109/MSP.2011.942470
   Quan HT, 2011, IEEE T BROADCAST, V57, P1, DOI 10.1109/TBC.2010.2086750
   Salmon John K., 2011, Cases on Online Interview Research, P1, DOI DOI 10.1145/2063384.2063405
   Scott M., 2002, Applied Logistic Regression Analysis, V2nd
   Song J., 2014, P 2014 OCEANS ST JOH, DOI [10.1109/OCEANS.2014.7003082, DOI 10.1109/OCEANS.2014.7003082]
   Staelens N, 2010, IEEE T BROADCAST, V56, P458, DOI 10.1109/TBC.2010.2067710
   Takahashi M, 2013, 2013 INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY & INTERNET-BASED SYSTEMS (SITIS), P6, DOI 10.1109/SITIS.2013.13
   Tao J, 2007, COMPUT IND ENG, V53, P270, DOI 10.1016/j.cie.2007.06.020
   ur Rehman Laghari K., 2011, Proceedings 2011 25th IEEE International Conference on Advanced Information Networking and Applications Workshops (WAINA 2011), P837, DOI 10.1109/WAINA.2011.58
   van Eijk RLJ, 2008, PERCEPT PSYCHOPHYS, V70, P955, DOI 10.3758/PP.70.6.955
   Winkler S, 2006, IEEE T MULTIMEDIA, V8, P973, DOI 10.1109/TMM.2006.879871
   You JY, 2010, SIGNAL PROCESS-IMAGE, V25, P482, DOI 10.1016/j.image.2010.02.002
   Zhou ZH, 2004, PATTERN RECOGN, V37, P1049, DOI 10.1016/j.patcog.2003.09.006
NR 51
TC 39
Z9 41
U1 1
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2016
VL 18
IS 3
BP 444
EP 457
DI 10.1109/TMM.2016.2520090
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DG2WP
UT WOS:000371931600011
DA 2024-07-18
ER

PT J
AU Jhou, WC
   Cheng, WH
AF Jhou, Wei-Cih
   Cheng, Wen-Huang
TI Animating Still Landscape Photographs Through Cloud Motion Creation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Animation; clouds; dynamic imagery; landscape photographs; motion
   creation; multimedia enrichment; user experience
ID IMAGE
AB Animating landscape photographs can create an engaging viewing experience and has long been an active area of multimedia research. Images can be used to create an illusion of movement, thereby providing more vivid and interesting visuals. Among numerous objects in visual scenes, clouds appear as a common element in landscape photos but are challenging to manipulate because clouds are ill-posed structures without concrete forms and present motion with very high degrees of freedom and arbitrary shape deformation. This paper addresses the generation of dynamic imagery from a still photo through automatic motion creation. First, a cloud appearance model is acquired by representing each pixel in the sky region as a combination of two computable image properties, i.e., Cloudiness estimates cloud density and Cloud Structure gives the corresponding cloud shape and texture. Then, based on the obtained Cloudiness and Cloud Structure, a content-aware wind field is generated to synthesize the motion of cloud flow. Experimental results demonstrate that our approach can work well on various types of cloud images and produce visually convincing dynamic imagery of landscape photos. Furthermore, we showcased several extended multimedia applications to validate that the proposed framework is generic and widely extensible to other vapor-like objects like mist and smoke. Please see http://mclab.citi.sinica.edu.tw/demo/dynamicimagery.htm for demos.
C1 [Jhou, Wei-Cih; Cheng, Wen-Huang] Acad Sinica, Res Ctr Informat Technol Innovat CITI, Taipei 115, Taiwan.
C3 Academia Sinica - Taiwan
RP Cheng, WH (corresponding author), Acad Sinica, Res Ctr Informat Technol Innovat CITI, Taipei 115, Taiwan.
EM stsin@citi.sinica.edu.tw; whcheng@citi.sinica.edu.tw
RI Cheng, Wen-Huang/AAK-2774-2020
CR [Anonymous], NOK CIN
   [Anonymous], P 8 ANN S COMP AESTH
   [Anonymous], 2013, P IEEE INT C MULT EX
   [Anonymous], P 43 AM I AER ASTR A
   [Anonymous], PARALLAX EFFECT TRAV
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bai JM, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185562
   Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2
   Chen Jun-Cheng, 2006, P 14 ACM INT C MULTI, P25
   Chen QF, 2013, IEEE T PATTERN ANAL, V35, P2175, DOI 10.1109/TPAMI.2013.18
   Chorin A., 1993, Texts in Applied Mathematics, V4
   Chuang YY, 2005, ACM T GRAPHIC, V24, P853, DOI 10.1145/1073204.1073273
   Dobashi Y, 2000, COMP GRAPH, P19, DOI 10.1145/344779.344795
   Dobashi Y, 2010, COMPUT GRAPH FORUM, V29, P2083, DOI 10.1111/j.1467-8659.2010.01795.x
   Doretto G, 2003, PROC CVPR IEEE, P137
   Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711
   HORN BKP, 1977, ARTIF INTELL, V8, P201, DOI 10.1016/0004-3702(77)90020-0
   Horry Y., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P225, DOI 10.1145/258734.258854
   Joshi N, 2012, UIST'12: PROCEEDINGS OF THE 25TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P251
   Kagaya M, 2011, IEEE T VIS COMPUT GR, V17, P74, DOI 10.1109/TVCG.2010.25
   Kazi RH, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P351, DOI 10.1145/2556288.2556987
   Kelvin L., 1871, LONDON, V42, P362, DOI [10.1080/14786447108640585, DOI 10.1080/14786447108640585]
   Ling CH, 2011, IEEE T IMAGE PROCESS, V20, P3124, DOI 10.1109/TIP.2011.2158228
   Ling CH, 2011, IEEE T MULTIMEDIA, V13, P292, DOI 10.1109/TMM.2010.2095000
   Liu BY, 2010, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2010.5539823
   Liu C, 2009, PROC CVPR IEEE, P1972, DOI 10.1109/CVPRW.2009.5206536
   McComb W., 1992, PHYS FLUID TURBULENC
   Peng KC, 2013, IEEE I CONF COMP VIS, P2152, DOI 10.1109/ICCV.2013.267
   Rogers R.R., 1989, SHORT COURSE CLOUD P
   Rubinstein M, 2011, PROC CVPR IEEE, P313, DOI 10.1109/CVPR.2011.5995374
   Sakaino H, 2009, IEEE INT CON MULTI, P986, DOI 10.1109/ICME.2009.5202662
   Schödl A, 2000, COMP GRAPH, P489, DOI 10.1145/344779.345012
   Stam J., 2001, Journal of Graphics Tools, V6, P43, DOI 10.1080/10867651.2001.10487540
   Stam J., 1993, Computer Graphics Proceedings, P369, DOI 10.1145/166117.166163
   Tang NC, 2014, IEEE T MULTIMEDIA, V16, P47, DOI 10.1109/TMM.2013.2283844
   Tompkin J., 2011, 2011 Conference for Visual Media Production, P87, DOI 10.1109/CVMP.2011.16
   Wang YZ, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P213, DOI 10.1109/ICCV.2003.1238343
   Xu XM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409070
   Yeh Mei-Chen., 2012, P 20 ACM INT C MULTI, P1153
   Zhang Q, 2014, IEEE T GEOSCI REMOTE, V52, P7264, DOI 10.1109/TGRS.2014.2310240
   Zhang XY, 2014, IEEE T MULTIMEDIA, V16, P653, DOI 10.1109/TMM.2014.2299511
   Zheng Ke Colin, 2009, P GRAPHICS INTERFACE, P111
NR 42
TC 13
Z9 14
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2016
VL 18
IS 1
BP 4
EP 13
DI 10.1109/TMM.2015.2500031
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CZ5JW
UT WOS:000367139700002
DA 2024-07-18
ER

PT J
AU Wu, L
   Shivakumara, P
   Lu, T
   Tan, CL
AF Wu, Liang
   Shivakumara, Palaiahnakote
   Lu, Tong
   Tan, Chew Lim
TI A New Technique for Multi-Oriented Scene Text Line Detection and
   Tracking in Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Delaunay triangulation; multi-oriented video text detection; multi-sized
   text detection; text detection; text tracking
ID LOCALIZATION
AB Text detection and tracking in video is challenging due to contrast, resolution and background variations, and different orientations and text movements. In addition, the presence of both caption and scene texts in video aggravates the problem because these two text types differ in characteristics significantly. This paper proposes a new technique for detecting and tracking video texts of any orientation by using spatial and temporal information, respectively. The technique explores gradient directional symmetry at component level for smoothing edge components before text detection. Spatial information is preserved by forming Delaunay triangulation in a novel way at this level, which results in text candidates. Text characteristics are then proposed in a different way for eliminating false text candidates, which results in potential text candidates. Then grouping is proposed for combining potential text candidates regardless of orientation based on the nearest neighbor criterion. To tackle the problems of multi-font and multi-sized texts, we propose multi-scale integration by a pyramid structure, which helps in extracting full text lines. Then, the detected text lines are tracked in video by matching the subgraphs of triangulation. Experimental results for text detection and tracking on our video dataset, the benchmark video datasets, and the natural scene image benchmark datasets show that the proposed method is superior to the state-of-the-art methods in terms of recall, precision, and F-measure.
C1 [Wu, Liang; Lu, Tong] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
   [Shivakumara, Palaiahnakote] Univ Malaya, Fac Comp Sci & Informat Technol, Kuala Lumpur 50603, Malaysia.
   [Tan, Chew Lim] Natl Univ Singapore, Sch Comp, Dept Comp Sci, Singapore 117548, Singapore.
C3 Nanjing University; Universiti Malaya; National University of Singapore
RP Lu, T (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
EM wuliang0301@hotmail.com; hudempsk@yahoo.com; lutong@nju.edu.cn;
   tancl@comp.nus.edu.sg
RI Palaiahnakote, Shivakumara/ITU-6488-2023; Palaiahnakote,
   Shivakumara/B-6261-2013
OI Wu, Liang/0000-0001-5214-7715
FU Natural Science Foundation of China [61272218, 61321491]; Program for
   New Century Excellent Talents [NCET-11-0232]; University of Malaya HIR
   [UM.C/625/1/HIR/MOHE/ENG/42]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61272218 and Grant 61321491, by the Program for New
   Century Excellent Talents under Grant NCET-11-0232, and by the
   University of Malaya HIR under Grant UM.C/625/1/HIR/MOHE/ENG/42. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Jiebo Luo. (Corresponding author:
   Tong Lu.)
CR [Anonymous], 2013, P 12 INT C DOC AN RE
   [Anonymous], 2001, PYRAMIDAL IMPLEMENTA
   [Anonymous], MULTIORIENTED SCENE
   Bouman KL, 2011, IEEE T MULTIMEDIA, V13, P922, DOI 10.1109/TMM.2011.2154317
   Chen DT, 2005, PATTERN RECOGN LETT, V26, P1386, DOI 10.1016/j.patrec.2004.11.019
   Crandall D., 2003, International Journal on Document Analysis and Recognition, V5, P138, DOI 10.1007/s10032-002-0091-7
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Huang W., 2008, P ICPR
   Huang XD, 2008, LECT NOTES COMPUT SC, V5353, P525, DOI 10.1007/978-3-540-89796-5_54
   Jung K, 2004, PATTERN RECOGN, V37, P977, DOI 10.1016/j.patcog.2003.10.012
   Li HP, 2000, IEEE T IMAGE PROCESS, V9, P147, DOI 10.1109/83.817607
   Li Y, 2014, IEEE T IMAGE PROCESS, V23, P1666, DOI 10.1109/TIP.2014.2302896
   Liang J., 2005, International Journal on Document Analysis and Recognition, V7, P84, DOI 10.1007/s10032-004-0138-z
   Lijie Li, 2010, Proceedings of the 2010 International Conference on Computer and Information Application (ICCIA 2010), P434, DOI 10.1109/ICCIA.2010.6141629
   Liu XQ, 2012, IEEE T MULTIMEDIA, V14, P482, DOI 10.1109/TMM.2011.2177646
   Liu Y., 2013, P ICDAR, P1387
   Lyu MR, 2005, IEEE T CIRC SYST VID, V15, P243, DOI 10.1109/TCSVT.2004.841653
   Mao J., 2013, Proceedings of ACM MM'13, ACM, P1007
   Mi Y., 2005, INT C INFORM COMMUNI, P678, DOI DOI 10.1109/ICICS.2005.1689133
   Miao GY, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P569, DOI 10.1109/ICME.2008.4607498
   Minetto R, 2014, COMPUT VIS IMAGE UND, V122, P92, DOI 10.1016/j.cviu.2013.10.004
   Mosleh A, 2013, IEEE T IMAGE PROCESS, V22, P4460, DOI 10.1109/TIP.2013.2273672
   Pan YF, 2011, IEEE T IMAGE PROCESS, V20, P800, DOI 10.1109/TIP.2010.2070803
   Phan T.Q., 2012, Proceedings of ACM MM'12, ACM, P765
   Nguyen PX, 2014, IEEE WINT CONF APPL, P776, DOI 10.1109/WACV.2014.6836024
   Shivakuamra P, 2014, IEEE IMAGE PROC, P1668, DOI 10.1109/ICIP.2014.7025334
   Shivakumara P, 2013, IEEE T CIRC SYST VID, V23, P1729, DOI 10.1109/TCSVT.2013.2255396
   Shivakumara P, 2011, IEEE T PATTERN ANAL, V33, P412, DOI 10.1109/TPAMI.2010.166
   Shivakumara P, 2009, IEEE INT CON MULTI, P514, DOI 10.1109/ICME.2009.5202546
   Sugimura D, 2009, IEEE I CONF COMP VIS, P1467, DOI 10.1109/ICCV.2009.5459286
   Phan TQ, 2013, IEEE I CONF COMP VIS, P569, DOI 10.1109/ICCV.2013.76
   Wang YK, 2006, INT C PATT RECOG, P754
   Wu L, 2014, 2014 11TH IAPR INTERNATIONAL WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS (DAS 2014), P41, DOI 10.1109/DAS.2014.28
   Xiaodong Huang, 2011, 2011 4th International Congress on Image and Signal Processing (CISP 2011), P469, DOI 10.1109/CISP.2011.6099945
   Xu CS, 2008, IEEE T MULTIMEDIA, V10, P1342, DOI 10.1109/TMM.2008.2004912
   Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787
   Ye QX, 2015, IEEE T PATTERN ANAL, V37, P1480, DOI 10.1109/TPAMI.2014.2366765
   Yin XC, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P1091
   Yin XC, 2014, IEEE T PATTERN ANAL, V36, P970, DOI 10.1109/TPAMI.2013.182
   Yusufu T., 2013, P ISMM, P533
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P4187, DOI 10.1109/TIP.2014.2341935
   Zhang J, 2008, PROCEEDINGS OF THE 8TH IAPR INTERNATIONAL WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS, P5, DOI 10.1109/DAS.2008.49
   Zhao X, 2011, IEEE T IMAGE PROCESS, V20, P790, DOI 10.1109/TIP.2010.2068553
   Zhou JC, 2007, INTERNATIONAL CONFERENCE ON MACHINE VISION 2007, PROCEEDINGS, P119, DOI 10.1109/ICMV.2007.4469284
NR 44
TC 44
Z9 49
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1137
EP 1152
DI 10.1109/TMM.2015.2443556
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000002
DA 2024-07-18
ER

PT J
AU Yuan, ZH
   Bi, T
   Muntean, GM
   Ghinea, G
AF Yuan, Zhenhui
   Bi, Ting
   Muntean, Gabriel-Miro
   Ghinea, Gheorghita
TI Perceived Synchronization of Mulsemedia Services
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Air-flow; haptic; mulsemedia; multimedia; olfaction; quality of
   experience (QoE); synchronization
ID MULTIMEDIA; EXPERIENCE; PERCEPTION; QUALITY; SCHEME; TIME
AB Multimedia synchronization involves a temporal relationship between audio and visual media components. The presentation of "in-sync" data streams is essential to achieve a natural impression, as "out-of-sync" effects are often associated with user quality of experience (QoE) decrease. Recently, multi-sensory media (mulsemedia) has been demonstrated to provide a highly immersive experience for its users. Unlike traditional multimedia, mulsemedia consists of other media types (i.e., haptic, olfaction, taste, etc.) in addition to audio and visual content. Therefore, the goal of achieving high quality mulsemedia transmission is to present no or little synchronization errors between the multiple media components. In order to achieve this ideal synchronization, there is a need for comprehensive knowledge of the synchronization requirements at the user interface. This paper presents the results of a subjective study carried out to explore the temporal boundaries within which haptic and air-flow media objects can be successfully synchronized with video media. Results show that skews between sensorial media and multimedia might still give the effect that the mulsemedia sequence is "in-sync" and provide certain constraints under which synchronization errors might be tolerated. The outcomes of the paper are used to provide recommendations for mulsemedia service providers in order for their services to be associated with acceptable user experience levels, e.g. haptic media could be presented with a delay of up to 1 s behind video content, while air-flow media could be released either 5 s ahead of or 3 s behind video content.
C1 [Yuan, Zhenhui] Huawei Technol Co Ltd, Shenzhen 518129, Peoples R China.
   [Bi, Ting; Muntean, Gabriel-Miro] Dublin City Univ, Sch Elect Engn, Network Innovat Ctr, Performance Engn Lab, Dublin 9, Ireland.
   [Ghinea, Gheorghita] Brunel Univ, Dept Comp Sci, Uxbridge UB8 3PH, Middx, England.
C3 Huawei Technologies; Dublin City University; Brunel University
RP Bi, T (corresponding author), Dublin City Univ, Sch Elect Engn, Network Innovat Ctr, Performance Engn Lab, Dublin 9, Ireland.
EM yuanzhenhui@outlook.com; ting.bi2@mail.dcu.ie; gabriel.muntean@dcu.ie;
   george.ghinea@brunel.ac.uk
RI Bi, Ting/AAV-9483-2021; Ghinea, Gheorghita/AAG-6770-2020; Muntean,
   Gabriel-Miro/U-6783-2019
OI Bi, Ting/0000-0001-6196-5613; Ghinea, Gheorghita/0000-0003-2578-5580;
   Muntean, Gabriel-Miro/0000-0002-9332-4770
FU Enterprise Ireland Innovation Partnership Program; Ericsson
   [IP/2011/0135]; Science Foundation Ireland [10/CE/I1855]
FX This work was supported in part by the Enterprise Ireland Innovation
   Partnership Program with Ericsson under Grant IP/2011/0135 and in part
   by the Science Foundation Ireland under Grant 10/CE/I1855 to Lero, the
   Irish Software Engineering Research Centre. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Eckehard Steinbach. (Corresponding author: Ting Bi.)
CR Ademoye OA, 2009, IEEE T MULTIMEDIA, V11, P561, DOI 10.1109/TMM.2009.2012927
   [Anonymous], P IEEE IFIP NETW OP
   [Anonymous], 1998, SUBJ AUD QUAL ASS ME
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], 2011, INT ENCY STAT SCI
   Apostolopoulos JG, 2012, P IEEE, V100, P974, DOI 10.1109/JPROC.2011.2182069
   Blakowski G, 1996, IEEE J SEL AREA COMM, V14, P5, DOI 10.1109/49.481691
   Bodnar A, 2004, P 6 INT C MULTIMODAL, P183, DOI [10.1145/1027933.1027965, DOI 10.1145/1027933.1027965]
   Boronat F, 2012, IEEE COMMUN MAG, V50, P150, DOI 10.1109/MCOM.2012.6353695
   Carbon CC, 2005, APPL COGNITIVE PSYCH, V19, P587, DOI 10.1002/acp.1098
   Concolato C., 2012, P ACM MULT SYST C NE, P227
   Cronbach LJ, 1951, PSYCHOMETRIKA, V16, P297
   Din S. Ud, 2012, P 4 INT C ADV MULT A, P1
   Dinh HQ, 1999, P IEEE VIRT REAL ANN, P222, DOI 10.1109/VR.1999.756955
   Ghinea G., 2012, ACM T MULTIM COMPUT, V8, P52
   Ghinea G, 2010, IEEE T SYST MAN CY A, V40, P657, DOI 10.1109/TSMCA.2010.2041224
   Huang ZX, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2490821
   International Telecommunication Union, 2014, METH SUBJ ASS VID QU
   Ishibashi Y., 2004, P 12 ANN ACM INT C M, P604
   Ishibashi Y, 2014, MULTIMEDIA SYST, V20, P621, DOI 10.1007/s00530-014-0382-0
   Isomura E, 2011, 2011 IEEE REGION 10 CONFERENCE TENCON 2011, P1085, DOI 10.1109/TENCON.2011.6129278
   Jakesch M, 2011, RES ENG DES, V22, P143, DOI 10.1007/s00163-010-0102-5
   Jimenez Rodriguez Erick, 2012, 2012 IEEE Workshops of International Conference on Advanced Information Networking and Applications (WAINA), P91, DOI 10.1109/WAINA.2012.47
   Kameyama S, 2006, PROC SPIE, V6391, DOI 10.1117/12.685686
   Kennedy M, 2013, IEEE COMMUN SURV TUT, V15, P768, DOI 10.1109/SURV.2012.072412.00115
   Kim H, 2011, 2011 11TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS), P1329
   LAING DG, 1983, PERCEPTION, V12, P99, DOI 10.1068/p120099
   Mekuria R., 2012, P 10 EUR C INT TV VI, P71
   Moldovan AN, 2014, IEEE COMMUN SURV TUT, V16, P234, DOI 10.1109/SURV.2013.071913.00194
   Molnar A, 2013, IEEE T BROADCAST, V59, P484, DOI 10.1109/TBC.2013.2244786
   Montagud M., 2013, P 21 ACM INT C MULT, P323
   Murray N, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2637293
   Murray N, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2540994
   Murray Niall., 2013, Proceedings of the 4th ACM Multimedia Systems Conference, MMSys'13, P162, DOI [https://doi.org/10.1145/2483977.2483999, DOI 10.1145/2483977.2483999]
   Nagle HT, 1998, IEEE SPECTRUM, V35, P22, DOI 10.1109/6.715180
   Nakano T., 2014, P 7 INT C ADV COMP H, P365
   Nordstokke D.W., 2007, J ED RES POLICY STUD, V7, P1
   Nordstokke D. W., 2011, PRACT ASSESS RES EVA, V16, P2
   P ITU-T RECOMMENDATION, 1999, SUBJ VID QUAL ASS ME
   Pingguo Huang, 2013, 2013 IEEE 2nd Global Conference on Consumer Electronics (GCCE), P456, DOI 10.1109/GCCE.2013.6664889
   Qi Zeng, 2013, 2013 IEEE 2nd Global Conference on Consumer Electronics (GCCE), P466, DOI 10.1109/GCCE.2013.6664891
   Rowe LA, 2005, ACM T MULTIM COMPUT, V1, P3, DOI 10.1145/1047936.1047938
   Silva JM, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2457450.2457451
   Steinbach E, 2012, P IEEE, V100, P937, DOI 10.1109/JPROC.2011.2182100
   Steinmetz R, 1996, IEEE J SEL AREA COMM, V14, P61, DOI 10.1109/49.481694
   Streiner DL, 2003, J PERS ASSESS, V80, P217, DOI 10.1207/S15327752JPA8003_01
   Tatematsu A., 2010, Communications Quality and Reliability (CQR), 2010 IEEE International Workshop Technical Committee on, P1, DOI [DOI 10.1109/CQR.2010.5619913, 10.1109/CQR.2010.5619913]
   Timmerer C, 2014, T-LAB SER TELECOMMUN, P351, DOI 10.1007/978-3-319-02681-7_24
   Washburn D. A., 2003, MODELING SIMULATION, V2, P19
   Xiao JM, 2013, IEEE T BROADCAST, V59, P432, DOI 10.1109/TBC.2013.2258111
   Yuan ZH, 2014, INT WIREL COMMUN, P1142, DOI 10.1109/IWCMC.2014.6906515
   Yuan ZH, 2015, IEEE T MULTIMEDIA, V17, P104, DOI 10.1109/TMM.2014.2371240
   Yuan ZH, 2013, IEEE T NETW SERV MAN, V10, P340, DOI 10.1109/TNSM.2013.110513.130490
   Yuan ZH, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2661329
NR 54
TC 62
Z9 64
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2015
VL 17
IS 7
BP 957
EP 966
DI 10.1109/TMM.2015.2431915
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CK8XC
UT WOS:000356522300004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, XY
   Zhao, YL
   Nie, LQ
   Gao, Y
   Nie, WZ
   Zha, ZJ
   Chua, TS
AF Wang, Xiangyu
   Zhao, Yi-Liang
   Nie, Liqiang
   Gao, Yue
   Nie, Weizhi
   Zha, Zheng-Jun
   Chua, Tat-Seng
TI Semantic-Based Location Recommendation With Multimodal Venue Semantics
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Location recommendation; location representation; multi-dimensional
   profile; venue semantics
AB In recent years, we have witnessed a flourishing of location-based social networks. A well-formed representation of location knowledge is desired to cater to the need of location sensing, browsing, navigation and querying. In this paper, we aim to study the semantics of point-of-interest (POI) by exploiting the abundant heterogeneous user generated content (UGC) from different social networks. Our idea is to explore the text descriptions, photos, user check-in patterns, and venue context for location semantic similarity measurement. We argue that the venue semantics play an important role in user check-in behavior. Based on this argument, a unified POI recommendation algorithm is proposed by incorporating venue semantics as a regularizer. In addition to deriving user preference based on user-venue check-in information, we place special emphasis on location semantic similarity. Finally, we conduct a comprehensive performance evaluation of location semantic similarity and location recommendation over a real world dataset collected from Foursquare and Instagram. Experimental results show that the UGC information can well characterize the venue semantics, which help to improve the recommendation performance.
C1 [Wang, Xiangyu; Nie, Liqiang; Gao, Yue; Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
   [Zhao, Yi-Liang] DigiPen Inst Technol Singapore, Singapore 138649, Singapore.
   [Nie, Weizhi] Tianjin Univ, Tianjin 300072, Peoples R China.
   [Zha, Zheng-Jun] Chinese Acad Sci, Inst Intelligent Machines, Hefei 230031, Peoples R China.
C3 National University of Singapore; Tianjin University; Chinese Academy of
   Sciences; Hefei Institutes of Physical Science, CAS
RP Gao, Y (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
EM kevin.gaoy@gmail.com
RI Zha, Zheng-Jun/AAE-8408-2020; Nie, Weizhi/ABF-5316-2021; Zha,
   Zheng-Jun/AAF-8667-2020
OI Zha, Zheng-Jun/0000-0003-2510-8993; nie, weizhi/0000-0002-0578-8138
FU Singapore National Research Foundation under its International Research
   Centre@Singapore Funding Initiative
FX This work was supported by the Singapore National Research Foundation
   under its International Research Centre@Singapore Funding Initiative and
   administered by the IDM Programme Office. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Vasileios Mezaris.
CR Agarwal S., 2006, P 23 INT C MACH LEAR, P17, DOI DOI 10.1145/1143844.1143847
   [Anonymous], 2010, P 18 SIGSPATIAL INT
   [Anonymous], 2013, HT, DOI [DOI 10.1145/2481492.2481505, 10.1145/2481492.2481505]
   [Anonymous], 2010, ACM Sigkdd Explorations Newsletter
   Bao J., 2012, P 20 INT C ADV GEOGR, P199, DOI [DOI 10.1145/2424321.2424348, 10.1145/2424321.2424348]
   Berjani B., 2011, SNS, P4
   Bisio I, 2013, IEEE T MULTIMEDIA, V15, P858, DOI 10.1109/TMM.2013.2239631
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bottou L, 2004, LECT NOTES ARTIF INT, V3176, P146
   Cao X, 2010, PROC VLDB ENDOW, V3, P1009, DOI 10.14778/1920841.1920968
   Chen YY, 2013, IEEE T MULTIMEDIA, V15, P1283, DOI 10.1109/TMM.2013.2265077
   Cheng C., 2012, P AAAI C ART INT, P17
   Cheng Z., 2011, P 20 ACM INT C INF K, P805, DOI DOI 10.1145/2063576.2063693
   Cho E., 2011, P 17 ACM SIGKDD INT, P1082, DOI DOI 10.1145/2020408.2020579
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Hong RC, 2014, IEEE T CYBERNETICS, V44, P669, DOI 10.1109/TCYB.2013.2265601
   Hu HB, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MOBILE DATA MANAGEMENT, P52
   Huang A., 2008, NZCSRSC 2008
   Huang Y., 2010, THESIS RUTGERS U NEW
   Järvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Kurashima Takeshi., 2013, P 6 ACM INT C WEB SE, P375
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lee MJ, 2011, LECT NOTES COMPUT SC, V6587, P38, DOI 10.1007/978-3-642-20149-3_5
   Li J, 2013, IEEE T MULTIMEDIA, V15, P2058, DOI 10.1109/TMM.2013.2280127
   Ma H., 2011, Proceedings of the 4th ACM International Conference on Web Search and Data Mining, P287
   Mnih A., 2007, ADV NEURAL INFORM PR, V20
   Noulas A, 2012, PROCEEDINGS OF 2012 ASE/IEEE INTERNATIONAL CONFERENCE ON PRIVACY, SECURITY, RISK AND TRUST AND 2012 ASE/IEEE INTERNATIONAL CONFERENCE ON SOCIAL COMPUTING (SOCIALCOM/PASSAT 2012), P144, DOI 10.1109/SocialCom-PASSAT.2012.70
   Rennie Jasson D. M., 2005, P 22 INT C MACH LEAR, P713, DOI DOI 10.1145/1102351.1102441
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang XY, 2013, IEEE T MULTIMEDIA, V15, P120, DOI 10.1109/TMM.2012.2225027
   Xu B., 2012, P 21 INT C WORLD WID, P21
   Ye M., 2011, P 17 ACM SIGKDD INT, P520
   Ye M, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P325
   Ying Josh Jia-Ching, 2012, P ACM SIGKDD INT WOR, P63, DOI DOI 10.1145/2346496.2346507
   Zhao YL, 2014, ACM T INTEL SYST TEC, V5, DOI 10.1145/2532439
   Zheng VW, 2010, P 19 INT C WORLD WID, P1029
   Zheng Y., 2009, WWW, P791, DOI [10.1145/1526709.1526816, DOI 10.1145/1526709.1526816]
   Zheng Y, 2011, ACM T WEB, V5, DOI 10.1145/1921591.1921596
   Zhou D., 2006, ADV NEURAL INFORM PR, P1601, DOI DOI 10.7551/MITPRESS/7503.003.0205
NR 40
TC 54
Z9 59
U1 2
U2 42
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2015
VL 17
IS 3
BP 409
EP 419
DI 10.1109/TMM.2014.2385473
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CE1QB
UT WOS:000351585700012
DA 2024-07-18
ER

PT J
AU Schramm, R
   Jung, CR
   Miranda, ER
AF Schramm, Rodrigo
   Jung, Claudio Rosito
   Miranda, Eduardo Reck
TI Dynamic Time Warping for Music Conducting Gestures Evaluation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distance learning; dynamic time warping (DTW); meter-mimicking; musical
   conducting; pattern recognition
ID RECOGNITION; SYNCHRONIZATION; ENSEMBLE; HAND
AB Musical performance by an ensemble of performers often requires a conductor. This paper presents a tool to aid the study of basic conducting gestures, also known as meter-mimicking gestures, performed by beginners. It is based on the automatic detection of musical metrics and their subdivisions by analysis of hand gestures. Musical metrics are represented by visual conducting patterns performed by hands, which are tracked using an RGB-D camera. These patterns are recognized and evaluated using a probabilistic framework based on dynamic time warping (DTW). There are two main contributions in this work. Firstly, a new metric is proposed for the DTW, allowing better alignment between two gesture movements without the use of explicit maxima local points. Secondly, the time precision of the conducting gesture is extracted directly from the warping path and its accuracy is evaluated by a confidence measure. Experimental results indicate that the classification scheme represents an improvement over other existing related approaches.
C1 [Schramm, Rodrigo; Jung, Claudio Rosito] Univ Fed Rio Grande do Sul, Inst Informat, BR-90040060 Porto Alegre, RS, Brazil.
   [Miranda, Eduardo Reck] Univ Plymouth, ICCMR, Plymouth PL4 8AA, Devon, England.
C3 Universidade Federal do Rio Grande do Sul; University of Plymouth
RP Schramm, R (corresponding author), Univ Fed Rio Grande do Sul, Inst Informat, BR-90040060 Porto Alegre, RS, Brazil.
EM rschramm@inf.ufrgs.br; crjung@inf.ufrgs.br;
   eduardo.miranda@plymouth.ac.uk
OI Miranda, Eduardo/0000-0002-8306-9585
FU Plymouth University
FX The authors thank Plymouth University for the support and encouragement.
CR Adistambha K, 2008, 2008 IEEE 10TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, VOLS 1 AND 2, P626
   Akl A, 2011, IEEE T SIGNAL PROCES, V59, P6197, DOI 10.1109/TSP.2011.2165707
   [Anonymous], 2011, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2011.5995316, 10.1109/CVPR.2011.5995316]
   Argueta CR, 2009, LECT NOTES COMPUT SC, V5611, P654, DOI 10.1007/978-3-642-02577-8_72
   Bahlmann C, 2004, IEEE T PATTERN ANAL, V26, P299, DOI 10.1109/TPAMI.2004.1262308
   Balakrishnan N, 2013, IEEE T RELIAB, V62, P679, DOI 10.1109/TR.2013.2273039
   Bevilacqua F, 2010, LECT NOTES ARTIF INT, V5934, P73, DOI 10.1007/978-3-642-12553-9_7
   Bianne-Bernard AL, 2011, IEEE T PATTERN ANAL, V33, P2066, DOI 10.1109/TPAMI.2011.22
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Borchers JO, 2002, SECOND INTERNATIONAL CONFERENCE ON WEB DELIVERING OF MUSIC, PROCEEDINGS, P93, DOI 10.1109/WDM.2002.1176198
   Camurri A, 2005, IEEE MULTIMEDIA, V12, P43, DOI 10.1109/MMUL.2005.2
   Dahl L., 2014, P INT C NEW INTERFAC, P201
   Duda R. O., 2001, PATTERN CLASSIFICATI, P517
   Fenza D., 2005, P SOUND MUS COMP, P1
   Giles D. E. A., 2009, ECONOMETRICS WORKING
   Harding PRG, 2004, INT C PATT RECOG, P286, DOI 10.1109/ICPR.2004.1334523
   Hussain SMA, 2012, 2012 INTERNATIONAL CONFERENCE ON INFORMATICS, ELECTRONICS & VISION (ICIEV), P1033, DOI 10.1109/ICIEV.2012.6317364
   Ilmonen Tommi., 1999, P INT COMPUTER MUSIC, P367
   Johnson N.L., 1995, CONTINUOUS UNIVARIAT, V2
   Keogh E.J., 2001, P 2001 SIAM INT C DA, P1, DOI [DOI 10.1137/1.9781611972719.1, 10.1137/1.9781611972719.1]
   Kim SB, 2006, IEEE T KNOWL DATA EN, V18, P1457, DOI 10.1109/TKDE.2006.180
   KIM SJ, 1992, ANN STAT, V20, P1534, DOI 10.1214/aos/1176348783
   Kolesnik P., 2004, THESIS MCGILL U MONT
   Lee E., 2006, P INT C NEW INT MUS, P260
   Lichtenauer JF, 2008, IEEE T PATTERN ANAL, V30, P2040, DOI 10.1109/TPAMI.2008.123
   Luck G, 2006, MUSIC PERCEPT, V24, P189, DOI 10.1525/mp.2006.24.2.189
   Maes PJ, 2013, INT J HUM-COMPUT INT, V29, P471, DOI 10.1080/10447318.2012.720197
   Mandanici M., 2012, P 9 SOUND MUS COMP C, P271
   Min BW, 1997, IEEE SYS MAN CYBERN, P4232, DOI 10.1109/ICSMC.1997.637364
   Muller M., 2007, Information retrieval for music and motion, P69, DOI [10.1007/978-3-540-74048-3_4, DOI 10.1007/978-3-540-74048-3_4]
   Nakra T. M., 2009, P 2009 C NEW INT MUS, P250
   Raileanu LE, 2004, ANN MATH ARTIF INTEL, V41, P77, DOI 10.1023/B:AMAI.0000018580.96245.c6
   Rajko S., 2007, CVPR, P1
   Ramachandran K. M., 2020, Mathematical statistics with applications in R
   Rudolf Max., 1980, GRAMMAR CONDUCTING, V2nd
   Sarasua A., 2014, NIME, P195
   SAVITZKY A, 1964, ANAL CHEM, V36, P1627, DOI 10.1021/ac60214a047
   Schramm R., 2007, P COMP GRAPH INT PET, P71
   Sigal L, 2010, LECT NOTES COMPUT SC, V6313, P243
   Steinherz T, 2009, IEEE T PATTERN ANAL, V31, P193, DOI 10.1109/TPAMI.2008.68
   Sutherland JW, 1992, T N AM MANUF RES I S, V20, P347
   Toh LW., 2013, 2013 IEEE International Conference on Multimedia and Expo (ICME), P1, DOI DOI 10.1109/ICME.2013.6607481
   Uchida S, 2012, INT C PATT RECOG, P2294
   Wang JS, 2012, IEEE T IND ELECTRON, V59, P2998, DOI 10.1109/TIE.2011.2167895
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Webb A. R., 2002, STAT PATTERN RECOGNI
   Wöllner C, 2012, J EXP PSYCHOL HUMAN, V38, P1390, DOI 10.1037/a0028130
   Zhang Y, 2008, P AMER CONTR CONF, P2864, DOI 10.1109/ACC.2008.4586928
   Zhou F, 2012, PROC CVPR IEEE, P1282, DOI 10.1109/CVPR.2012.6247812
NR 49
TC 16
Z9 18
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2015
VL 17
IS 2
BP 243
EP 255
DI 10.1109/TMM.2014.2377553
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AZ4RN
UT WOS:000348210500008
DA 2024-07-18
ER

PT J
AU Hu, ZZ
   Liu, S
   Jiang, JG
   Hong, RC
   Wang, M
   Yan, SC
AF Hu, Zhenzhen
   Liu, Si
   Jiang, Jianguo
   Hong, Richang
   Wang, Meng
   Yan, Shuicheng
TI PicWords: Render a Picture by Packing Keywords
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Calligram; keywords; non-photorealistic rendering; picture; PicWords
AB In this paper, we propose a novel text-art system: input a source picture and some keywords introducing the information about the picture, and the output is the so-called PicWords in the form of the source picture composed of the introduction keywords. Different from traditional text-graphics which are created by highly skilled artists and involve a huge amount of tedious manual work, PicWords is an automatic non-photorealistic rendering (NPR) packing system. Given a source picture, we first generate its silhouette, which is a binary image containing a Yang part and a Yin part. Yang part is for keywords placing while the Yin part can be ignored. Next, the Yang part is further over-segmented into small patches, each of which serves as a container for one keyword. To make sure that more important keywords are put into more salient and larger image patches, we rank both the patches and keywords and construct a correspondence between the patch list and keyword list. Then, mean value coordinates method is used for the keyword-patch warping. Finally, certain post-processing techniques are adopted to improve the aesthetics of PicWords. Extensive experimental results well demonstrate the effectiveness of the proposed PicWords system.
C1 [Hu, Zhenzhen; Jiang, Jianguo; Hong, Richang; Wang, Meng] Hefei Univ Technol, Hefei 230009, Peoples R China.
   [Liu, Si; Yan, Shuicheng] Natl Univ Singapore, Singapore 117548, Singapore.
C3 Hefei University of Technology; National University of Singapore
RP Liu, S (corresponding author), Natl Univ Singapore, Singapore 117548, Singapore.
EM dcslius@nus.edu.sg
RI Yan, Shuicheng/HCI-1431-2022; Wang, Meng/ITR-8699-2023
FU State Key Development Program of Basic Research of China [2013CB336500];
   NSFC [61272393, 61322201]; Program for New Century Excellent Talents in
   University [NCET-12-0836]; Open Project Program of the National
   Laboratory of Pattern Recognition (NLPR); Singapore National Research
   Foundation under its International Research Centre @Singapore Funding
   Initiative
FX This work was supported in part by State Key Development Program of
   Basic Research of China 2013CB336500; in part by the NSFC under Grant
   nos. 61272393, 61322201, the Program for New Century Excellent Talents
   in University under grant NCET-12-0836, and the Open Project Program of
   the National Laboratory of Pattern Recognition (NLPR); and in part by
   the Singapore National Research Foundation under its International
   Research Centre @Singapore Funding Initiative and administered by the
   IDM Programme Office. The work was performed when Z. Hu was visiting
   National University of Singapore. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Weisi Lin.
CR Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Carroll R, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778864
   Cheng WH, 2007, IEEE T CIRC SYST VID, V17, P43, DOI 10.1109/TCSVT.2006.885717
   Christoudias CM, 2002, INT C PATT RECOG, P150, DOI 10.1109/ICPR.2002.1047421
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   GOMES J, 1999, WARPING MORPHING GR, V1
   Hausner A, 2001, COMP GRAPH, P573, DOI 10.1145/383259.383327
   Hormann K, 2006, ACM T GRAPHIC, V25, P1424, DOI 10.1145/1183287.1183295
   Kim J, 2002, ACM T GRAPHIC, V21, P657
   KURIBAYASHI M, 2011, FORUM INF TECHNOL, V4, P517
   Kyprianidis JE, 2013, IEEE T VIS COMPUT GR, V19, P866, DOI 10.1109/TVCG.2012.160
   MAHARIK R, 2011, ACM TRANS GRAPH
   Nacenta M, 2012, PROCEEDINGS OF THE INTERNATIONAL WORKING CONFERENCE ON ADVANCED VISUAL INTERFACES, P407, DOI 10.1145/2254556.2254636
   Orchard J., 2008, P 6 INT S NONPH AN R, P79, DOI DOI 10.1145/1377980.1377997
   REN Z, 2011, PROC 2011 IEEE INT C, P303
   STROTHOTTE T, 2002, NON PHOTOREALIST CO
   XU J, 2007, P GRAPH INT, P43
   Xu XM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778789
   YEH JY, 2011, PROC 19 ACM INT CONF, P1565
NR 19
TC 7
Z9 7
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2014
VL 16
IS 4
BP 1156
EP 1164
DI 10.1109/TMM.2014.2305635
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AJ8LN
UT WOS:000337955800021
DA 2024-07-18
ER

PT J
AU Wang, MH
   Ngan, KN
   Xu, L
AF Wang, Miaohui
   Ngan, King Ngi
   Xu, Long
TI Efficient H.264/AVC Video Coding with Adaptive Transforms
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video coding; transform coding; 2D separable Karhunen-Loeve transform;
   H.264/AVC
ID SINGULAR-VALUE DECOMPOSITION; DIRECTIONAL TRANSFORM; INTRA-PREDICTION
AB Transform has been widely used to remove spatial redundancy of prediction residuals in the modern video coding standards. However, since the residual blocks exhibit diverse characteristics in a video sequence, conventional transform methods with fixed transform kernels may result in low efficiency. To tackle this problem, we propose a novel content adaptive transform framework for the H.264/AVC-based video coding. The proposed method utilizes pixel rearrangement to dynamically adjust the transform kernels to adapt to the video content. In addition, unlike the traditional adaptive transforms, the proposed method obtains the transform kernels from the reconstructed block, and hence it consumes only one logic indicator for each transform unit. Moreover, a spiral-scanning method is developed to reorder the transform coefficients for better entropy coding. Experimental results on the Key Technical Area (KTA) platform show that the proposed method can achieve an average bitrate reduction of about 7.95% and 7.0% under all-intra and low-delay configurations, respectively.
C1 [Wang, Miaohui; Ngan, King Ngi; Xu, Long] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
RP Wang, MH (corresponding author), Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
EM mhwang@ee.cuhk.edu.hk; knngan@ee.cuhk.edu.hk; xulong@ee.cuhk.edu.hk
RI Ngan, N/E-8240-2014; Xu, Long/AAH-9908-2019
OI Ngan, N/0000-0003-1946-3235; Xu, Long/0000-0002-9286-2876
FU Research Grants Council of the Hong Kong SAR, China [CUHK 416010]
FX This work was supported in part by a grant from the Research Grants
   Council of the Hong Kong SAR, China (Project CUHK 416010). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Ebroul Izquierdo.
CR Alshina Elena, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3689, DOI 10.1109/ICIP.2011.6116520
   [Anonymous], 2001, VCEGM33
   Biswas M, 2010, IEEE IMAGE PROC, P165, DOI 10.1109/ICIP.2010.5652136
   Chang CL, 2010, IEEE T IMAGE PROCESS, V19, P1740, DOI 10.1109/TIP.2010.2044964
   Cohen RA, 2010, IEEE IMAGE PROC, P185, DOI 10.1109/ICIP.2010.5651058
   Dapena A, 2002, IEEE T CIRC SYST VID, V12, P114, DOI 10.1109/76.988658
   Ding C, 2005, SIAM PROC S, P32
   Dong J, 2012, IEEE T CIRC SYST VID, V22, P619, DOI 10.1109/TCSVT.2011.2171212
   GOLUB GH, 1970, NUMER MATH, V14, P403, DOI 10.1007/BF02163027
   Gu ZY, 2012, IEEE T IMAGE PROCESS, V21, P674, DOI 10.1109/TIP.2011.2166969
   Han JN, 2012, IEEE T IMAGE PROCESS, V21, P1874, DOI 10.1109/TIP.2011.2169976
   JAIN AK, 1979, IEEE T PATTERN ANAL, V1, P356, DOI 10.1109/TPAMI.1979.4766944
   Kim C, 2007, IEEE T CIRC SYST VID, V17, P441, DOI 10.1109/TCSVT.2006.888829
   Lim SC, 2008, CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOL 1, PROCEEDINGS, P389, DOI 10.1109/CISP.2008.226
   Malvar HS, 2003, IEEE T CIRC SYST VID, V13, P598, DOI 10.1109/TCSVT.2003.814964
   Miaohui Wang, 2013, 2013 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP), P547, DOI 10.1109/ChinaSIP.2013.6625400
   Piao Y, 2010, IEEE T CIRC SYST VID, V20, P1915, DOI 10.1109/TCSVT.2010.2090423
   Rao K. R., 2000, The transform and data compression handbook
   Saxena A, 2011, IEEE IMAGE PROC, P1685, DOI 10.1109/ICIP.2011.6115780
   Sezer OG, 2011, IEEE DATA COMPR CONF, P63, DOI 10.1109/DCC.2011.14
   Smith T., 1932, Transactions of the Optical Society, V33, P73
   Suhring K., 2009, KTA 2 6R1 REFERENCE
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tsai AC, 2008, IEEE T CIRC SYST VID, V18, P694, DOI 10.1109/TCSVT.2008.919113
   Wang MH, 2013, PICT COD SYMP, P13, DOI 10.1109/PCS.2013.6737671
   Wang MH, 2009, IEEE SIGNAL PROC LET, V16, P679, DOI 10.1109/LSP.2009.2022147
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Ye Y, 2008, IEEE IMAGE PROC, P2116, DOI 10.1109/ICIP.2008.4712205
   Yeo CH, 2012, IEEE T CIRC SYST VID, V22, P545, DOI 10.1109/TCSVT.2011.2168291
   Yu L, 2009, SIGNAL PROCESS-IMAGE, V24, P247, DOI 10.1016/j.image.2009.02.003
   Zeng B, 2008, IEEE T CIRC SYST VID, V18, P305, DOI 10.1109/TCSVT.2008.918455
   Zhao X, 2012, IEEE T CIRC SYST VID, V22, P138, DOI 10.1109/TCSVT.2011.2158363
NR 32
TC 17
Z9 17
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2014
VL 16
IS 4
BP 933
EP 946
DI 10.1109/TMM.2014.2305579
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AJ8LN
UT WOS:000337955800004
DA 2024-07-18
ER

PT J
AU Chen, F
   De Vleeschouwer, C
   Cavallaro, A
AF Chen, Fan
   De Vleeschouwer, Christophe
   Cavallaro, Andrea
TI Resource Allocation for Personalized Video Summarization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive fast-forwarding; personalized video summarization; resource
   allocation
ID FAST-FORWARD; ABSTRACTION; EXTRACTION; ATTENTION; LIMITS
AB We propose a hybrid personalized summarization framework that combines adaptive fast-forwarding and content truncation to generate comfortable and compact video summaries. We formulate video summarization as a discrete optimization problem, where the optimal summary is determined by adopting Lagrangian relaxation and convex-hull approximation to solve a resource allocation problem. To trade-off playback speed and perceptual comfort we consider information associated to the still content of the scene, which is essential to evaluate the relevance of a video, and information associated to the scene activity, which is more relevant for visual comfort. We perform clip-level fast-forwarding by selecting the playback speeds from discrete options, which naturally include content truncation as special case with infinite playback speed. We demonstrate the proposed summarization framework in two use cases, namely summarization of broadcasted soccer videos and surveillance videos. Objective and subjective experiments are performed to demonstrate the relevance and efficiency of the proposed method.
C1 [Chen, Fan] Japan Adv Inst Sci & Technol, Sch Informat Sci, Nomi 9231211, Japan.
   [De Vleeschouwer, Christophe] Catholic Univ Louvain, ICTEAM, B-1348 Louvain, Belgium.
   [Cavallaro, Andrea] Queen Mary Univ London, Ctr Intelligent Sensing, London E1 4NS, England.
C3 Japan Advanced Institute of Science & Technology (JAIST); Universite
   Catholique Louvain; University of London; Queen Mary University London
RP Chen, F (corresponding author), Japan Adv Inst Sci & Technol, Sch Informat Sci, Nomi 9231211, Japan.
EM chen-fan@jaist.ac.jp; christophe.devleeschouwer@uclouvain.be;
   andrea.cavallaro@eecs.qmul.ac.uk
FU Belgian NSF; Walloon Region project SPORTIC; UK Engineering and Physical
   Sciences Research Council (EPSRC) [EP/K007491/1];  [23700110];
   Grants-in-Aid for Scientific Research [23700110] Funding Source: KAKEN;
   EPSRC [EP/K007491/1] Funding Source: UKRI
FX This work was supported in part by the Japan Grant-in-Aid for Young
   Scientists (B)(No. 23700110), by the Belgian NSF, by the Walloon Region
   project SPORTIC, and by the UK Engineering and Physical Sciences
   Research Council (EPSRC), under grant EP/K007491/1. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Vasileios Mezaris.
CR Albanese M, 2006, INFORM SYST, V31, P679, DOI 10.1016/j.is.2005.12.003
   [Anonymous], USABILITY ENG
   [Anonymous], ACOUST SPEECH SIG PR
   [Anonymous], TRACKING RESULTS SUR
   [Anonymous], SUPPLEMENTAL MAT
   [Anonymous], MULTIMEDIA EXPO ICME
   [Anonymous], 2009, 2009 IEEE INT S BROA
   [Anonymous], 2007, P 15 ACM INT C MULTI
   [Anonymous], 2011, CHEM BIOLOGICAL FLOC
   [Anonymous], JAIST MULT SURV VID
   [Anonymous], P MMSP 05
   [Anonymous], TV SPORTS PRODUCTION
   [Anonymous], BEHAV COMPUTER ASSIS
   [Anonymous], P ICASSP 13
   [Anonymous], P ICASSP 03
   Babaguchi N, 2004, IEEE T MULTIMEDIA, V6, P575, DOI [10.1109/TMM.2004.830811, 10.1109/tmm.2004.830811]
   Baysal Sermetcan, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1727, DOI 10.1109/ICPR.2010.427
   Cai R, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P37
   Chen BW, 2009, IEEE T MULTIMEDIA, V11, P295, DOI 10.1109/TMM.2008.2009703
   Chen F, 2012, IEEE IMAGE PROC, P1337, DOI 10.1109/ICIP.2012.6467115
   Chen F, 2011, IEEE T CIRC SYST VID, V21, P193, DOI 10.1109/TCSVT.2011.2106271
   Chen F, 2010, IEEE INT CON MULTI, P837, DOI 10.1109/ICME.2010.5582561
   Cheng KY, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P789
   Cooper M., 2005, 2005 IEEE International Conference on Multimedia and Expo
   de Silva G. C., 2005, 13th Annual ACM International Conference on Multimedia, P820, DOI 10.1145/1101149.1101329
   Delannay D., 2009, PROC ICDSC 09, P1
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   Eung Kwan Kang, 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P260, DOI 10.1109/ICIP.1999.817113
   EVERETT H, 1963, OPER RES, V11, P399, DOI 10.1287/opre.11.3.399
   Fan JP, 2004, IEEE T MULTIMEDIA, V6, P70, DOI 10.1109/TMM.2003.819583
   Feng SK, 2012, PROC CVPR IEEE, P2082, DOI 10.1109/CVPR.2012.6247913
   Ferman AM, 2003, IEEE T MULTIMEDIA, V5, P244, DOI 10.1109/TMM.2003.811617
   Fernandez Ivan Alen, 2010, Proceedings of the Second International Conference on Advances in Multimedia (MMEDIA 2010), P155, DOI 10.1109/MMEDIA.2010.28
   Helbing D, 2001, ENVIRON PLANN B, V28, P361, DOI 10.1068/b2697
   Höferlin B, 2011, MULTIMED TOOLS APPL, V55, P127, DOI 10.1007/s11042-010-0606-z
   Höferlin M, 2012, IEEE T VIS COMPUT GR, V18, P2095, DOI 10.1109/TVCG.2012.222
   Holcombe AO, 2009, TRENDS COGN SCI, V13, P216, DOI 10.1016/j.tics.2009.02.005
   Jinjun Wang, 2005, 13th Annual ACM International Conference on Multimedia, P735, DOI 10.1145/1101149.1101309
   Lai JL, 2012, J VIS COMMUN IMAGE R, V23, P114, DOI 10.1016/j.jvcir.2011.08.005
   Li Z, 2005, IEEE T CIRC SYST VID, V15, P1245, DOI 10.1109/TCSVT.2005.854230
   Li Z, 2009, IEEE T IMAGE PROCESS, V18, P2572, DOI 10.1109/TIP.2009.2026677
   Ma YF, 2002, IEEE IMAGE PROC, P129
   Money AG, 2008, LECT NOTES COMPUT SC, V4868, P194, DOI 10.1007/978-3-540-85099-1_17
   Ortega A, 1996, DCC '96 - DATA COMPRESSION CONFERENCE, PROCEEDINGS, P349, DOI 10.1109/DCC.1996.488340
   PALMER J, 1990, J EXP PSYCHOL HUMAN, V16, P332, DOI 10.1037/0096-1523.16.2.332
   Peker KA, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P414, DOI 10.1109/ICIP.2001.958139
   Petrovic N, 2005, MULTIMED TOOLS APPL, V26, P327, DOI 10.1007/s11042-005-0895-9
   Pritch Y, 2008, IEEE T PATTERN ANAL, V30, P1971, DOI 10.1109/TPAMI.2008.29
   SHOHAM Y, 1988, IEEE T ACOUST SPEECH, V36, P1445, DOI 10.1109/29.90373
   Sivic J, 2006, INT J COMPUT VISION, V67, P189, DOI 10.1007/s11263-005-4264-y
   Smith MA, 1998, 1998 IEEE INTERNATIONAL WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO DATABASE, PROCEEDINGS, P61, DOI 10.1109/CAIVD.1998.646034
   Snoek CGM, 2007, IEEE T MULTIMEDIA, V9, P280, DOI 10.1109/TMM.2006.886275
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Tseng BL, 2003, P SOC PHOTO-OPT INS, V5242, P14, DOI 10.1117/12.512987
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Zhao S., 2013, INT C MULT MOD MMM 2, P7732, DOI DOI 10.1007/978-3-642-35725-1_34
   Zhuang YT, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P866, DOI 10.1109/ICIP.1998.723655
NR 57
TC 36
Z9 36
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 455
EP 469
DI 10.1109/TMM.2013.2291967
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lie, WN
   Lee, CM
   Yeh, CH
   Gao, ZW
AF Lie, Wen-Nung
   Lee, Chang-Ming
   Yeh, Chung-Hua
   Gao, Zhi-Wei
TI Motion Vector Recovery for Video Error Concealment by Using Iterative
   Dynamic-Programming Optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Boundary matching algorithm; dynamic programming; error concealment
ID ALGORITHM; FIELD
AB This paper proposes an error concealment technique for video transmission, focusing on motion vector (MV) recovery for both inter-and intra-coded frames, to improve video quality at decoder when video bit stream data incur transmission errors. The proposed algorithm considers slice (i.e., a row of macroblocks (MBs)) errors and uses DP (Dynamic Programming) optimization technique to estimate the lost MVs in a global manner, differing from the traditional Boundary Matching Algorithm (BMA) and others that recover MVs independently for individual MBs in an erroneous slice. We also propose an iterative DP process based on 8 x 8 pixels blocks to resolve finer motions (for 8 x 8, 8 x 16, and 16 x 8 pixels blocks) that will aid in the enhancement of reconstruction quality. Experiment results show that our algorithm outperforms the well-known BMA by up to 7.28 dB and the DMVE and another prior work by Qian et al. by up to 1.0 dB at a packet loss rate of 15%. Subjective evaluation shows that our algorithm is especially promising in preserving line/curve features and motion details.
C1 [Lie, Wen-Nung; Yeh, Chung-Hua] Natl Chung Cheng Univ, Dept Elect Engn, Chiayi, Taiwan.
   [Lie, Wen-Nung] Natl Chung Cheng Univ, Adv Inst Mfg High Tech Innovat AIM HI, Chiayi, Taiwan.
   [Lee, Chang-Ming] Natl Chung Cheng Univ, Dept Commun Engn, Chiayi, Taiwan.
   [Gao, Zhi-Wei] TECO Grp Res Inst, Taipei 115, Taiwan.
C3 National Chung Cheng University; National Chung Cheng University;
   National Chung Cheng University
RP Lie, WN (corresponding author), Natl Chung Cheng Univ, Dept Elect Engn, Chiayi, Taiwan.
EM ieewnl@ccu.edu.tw
RI Lie, Wen-Nung/AFP-1266-2022; Lee, Chang-Ming/AAR-5684-2021
OI Lee, Chang-Ming/0000-0003-3775-5469
CR Ali Abidah, 2010, Proceedings of the 2010 IEEE Student Conference on Research and Development (SCOReD 2010). Engineering: Innovation & Beyond, P421, DOI 10.1109/SCORED.2010.5704046
   [Anonymous], IEEE T CIRCUITS SYST
   Chen Y, 2008, IEEE T MULTIMEDIA, V10, P2, DOI 10.1109/TMM.2007.911223
   Feng J, 1996, 1996 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS - CONVERGING TECHNOLOGIES FOR TOMORROW'S APPLICATIONS, VOLS. 1-3, P1406, DOI 10.1109/ICC.1996.533640
   Haskell P., 1992, ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech and Signal Processing (Cat. No.92CH3103-9), P545, DOI 10.1109/ICASSP.1992.226155
   Kim ET, 1999, SIGNAL PROCESS, V73, P291, DOI 10.1016/S0165-1684(98)00242-4
   KWOK W, 1993, IEEE T CONSUM ELECTR, V39, P455, DOI 10.1109/30.234620
   Lam W. M., 1993, ICASSP-93. 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing (Cat. No.92CH3252-4), P417, DOI 10.1109/ICASSP.1993.319836
   Lie W.-N., 2010, P IEEE INT C IM PROC
   Lie W.-N., 2005, P IEEE INT S CIRC SY
   Lie WN, 2007, J VIS COMMUN IMAGE R, V18, P310, DOI 10.1016/j.jvcir.2007.04.006
   Lie WN, 2006, IEEE T CIRC SYST VID, V16, P982, DOI 10.1109/TCSVT.2006.879119
   Liu L., 2008, P IEEE INT C MULT EX
   Ma MY, 2010, IEEE T CIRC SYST VID, V20, P382, DOI 10.1109/TCSVT.2009.2035839
   Persson D, 2008, IEEE T IMAGE PROCESS, V17, P145, DOI 10.1109/TIP.2007.914151
   Qian XM, 2012, SIGNAL IMAGE VIDEO P, V6, P9, DOI 10.1007/s11760-010-0166-8
   Qian XM, 2009, IEEE T MULTIMEDIA, V11, P683, DOI 10.1109/TMM.2009.2017609
   STOCKHAMMER T, 2002, P 2002 INT PACK VID
   Su YB, 2005, IEEE T CIRC SYST VID, V15, P232, DOI 10.1109/TCSVT.2004.841656
   Su YP, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P703, DOI 10.1109/ICME.2004.1394289
   Tsai TH, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 5, PROCEEDINGS, P433
   Tsekeridou S, 1999, INT CONF ACOUST SPEE, P3397, DOI 10.1109/ICASSP.1999.757571
   Wu GL, 2010, IEEE T CIRC SYST VID, V20, P1409, DOI 10.1109/TCSVT.2010.2077471
   Wu TH, 2008, IEEE INT SYMP CIRC S, P3466, DOI 10.1109/ISCAS.2008.4542205
   Xueming Qian, 2007, Signal, Image and Video Processing, V1, P179, DOI 10.1007/s11760-007-0004-9
   Zhang J, 2000, IEEE T CIRC SYST VID, V10, P659, DOI 10.1109/76.845011
   Zhang J, 1997, TENCON IEEE REGION, P777, DOI 10.1109/TENCON.1997.648539
   Zhang YB, 2012, IEEE T CIRC SYST VID, V22, P12, DOI 10.1109/TCSVT.2011.2130450
   Zheng JH, 2005, IEEE T MULTIMEDIA, V7, P507, DOI 10.1109/TMM.2005.843343
   Zheng JH, 2003, IEEE T BROADCAST, V49, P383, DOI 10.1109/TBC.2003.819050
   Zhou J, 2011, IEEE T BROADCAST, V57, P75, DOI 10.1109/TBC.2010.2086771
NR 31
TC 30
Z9 34
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 216
EP 227
DI 10.1109/TMM.2013.2281587
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100018
DA 2024-07-18
ER

PT J
AU Azgin, A
   AlRegib, G
   Altunbasak, Y
AF Azgin, Aytac
   AlRegib, Ghassan
   Altunbasak, Yucel
TI Cooperative Delivery Techniques to Support Video-on-Demand Service in
   IPTV Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content delivery networks; cooperative networking; IPTV; peer-to-peer
   streaming; video-on-demand
ID SYSTEM; CHALLENGES; DYNAMICS; DESIGN
AB In this paper, we study the use of peer-assisted server-based cooperative transmission strategies in IPTV networks for the delivery of on-demand services to end users. The proposed techniques aim to support the resource efficient delivery of on-demand content to end users in a timely manner. Within the proposed framework, a cooperative transmission strategy suggests that users who have access to the requested content cooperatively transmit to the targeted set of users. In doing so, we can minimize the servicing requirements at the server side and improve the scalability performance in the network. We conducted extensive simulations and showed that significant performance improvements can be achieved with the proposed delivery techniques to enable efficient access to an ever growing on-demand content. We also showed the robustness of the proposed techniques in regard to variations observed in network state.
C1 [Azgin, Aytac; AlRegib, Ghassan] Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
   [Altunbasak, Yucel] Sci & Technol Res Council Turkey, Ankara, Turkey.
C3 University System of Georgia; Georgia Institute of Technology; Turkiye
   Bilimsel ve Teknolojik Arastirma Kurumu (TUBITAK)
RP Azgin, A (corresponding author), Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
EM aytaca@gatech.edu; alregib@gatech.edu; yucel.altunbasak@tubitak.gov.tr
OI AlRegib, Ghassan/0000-0001-6818-8001
CR Acharya S., 2000, P ACM NOSSDAV
   Aggarwal CC, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P253, DOI 10.1109/MMCS.1996.534983
   Almeroth KC, 1996, IEEE J SEL AREA COMM, V14, P1110, DOI 10.1109/49.508282
   [Anonymous], THESIS MIT CAMBRIDGE
   [Anonymous], 1984, ACM Transaction on Computer Systems
   Azgin A., 2013, GTMSL1305
   Bermudez I, 2011, IEEE J SEL AREA COMM, V29, P1863, DOI 10.1109/JSAC.2011.111010
   CAI Y, 1999, P SPIE ACM C MULT CO, P204
   Choi J., 2012, IEEE COMMUN SURVEYS, V14
   Costa C., 2004, P 13 INT C WORLD WID, P534
   da Fonseca NLS, 2002, IEEE T MULTIMEDIA, V4, P114, DOI 10.1109/6046.985559
   Diminico P., 2011, COMM SYST NETW COMSN, P1
   Eager D, 2001, IEEE T KNOWL DATA EN, V13, P742, DOI 10.1109/69.956098
   GAO L, 1998, P ACM NOSSDAV
   Gill P, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P15
   Gopalakrishnan V, 2009, IEEE INFOCOM SER, P91, DOI 10.1109/INFCOM.2009.5061910
   Guo Y., 2003, Proceedings of the 12th International Conference on World Wide Web, P301, DOI DOI 10.1145/775152.775195
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   Ho KM, 2009, IEEE T CIRC SYST VID, V19, P361, DOI 10.1109/TCSVT.2009.2013506
   Hongliang Yu, 2006, Operating Systems Review, V40, P333, DOI 10.1145/1218063.1217968
   Hu AL, 2001, IEEE INFOCOM SER, P508, DOI 10.1109/INFCOM.2001.916754
   Hua K., 1997, PROC SIGCOMM, P89
   Hua K. A., 1998, Proceedings ACM Multimedia 98, P191, DOI 10.1145/290747.290771
   Hua KA, 2004, P IEEE, V92, P1439, DOI 10.1109/JPROC.2004.832954
   Huang C, 2007, ACM SIGCOMM COMP COM, V37, P133, DOI 10.1145/1282427.1282396
   Huang Y, 2008, ACM SIGCOMM COMP COM, V38, P375, DOI 10.1145/1402946.1403001
   Juhn LS, 1998, IEEE T BROADCAST, V44, P100, DOI 10.1109/11.713059
   Kerpez K, 2006, IEEE COMMUN MAG, V44, P166, DOI 10.1109/MCOM.2006.1705994
   Kim HJ, 1998, IEEE T CONSUM ELECTR, V44, P969, DOI 10.1109/30.713221
   Liao X., 2006, P IEEE INFOCOM, P1
   Lixin Gao, 2001, IEEE Transactions on Multimedia, V3, P405, DOI 10.1109/6046.966112
   Padmanabhan V.N., 2002, P ACM NOSSDAV
   Paknikar S., 2000, Proceedings ACM Multimedia 2000, P13, DOI 10.1145/354384.354397
   Pâris JF, 2005, SIXTH MEXICAN INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE, PROCEEDINGS, P240
   Qiu TQ, 2009, PERF E R SI, V37, P275
   Sendonaris A, 2003, IEEE T COMMUN, V51, P1927, DOI 10.1109/TCOMM.2003.818096
   Suh K, 2007, IEEE J SEL AREA COMM, V25, P1706, DOI 10.1109/JSAC.2007.071209
   Vilas M, 2005, EUROMICRO-SEAA 2005: 31st EUROMICRO Conference on Software Engineering and Advanced Applications, Proceedings, P330
   VISWANATHAN S, 1995, P SOC PHOTO-OPT INS, V2417, P66, DOI 10.1117/12.206080
   Xiao Y, 2007, IEEE COMMUN MAG, V45, P126, DOI 10.1109/MCOM.2007.4378332
   Yiu WPK, 2007, IEEE MULTIMEDIA, V14, P50, DOI 10.1109/MMUL.2007.30
   Yue Lu, 2009, International Journal of Internet Protocol Technology, V4, P11, DOI 10.1504/IJIPT.2009.024166
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
NR 43
TC 3
Z9 3
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 2149
EP 2161
DI 10.1109/TMM.2013.2280439
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900034
DA 2024-07-18
ER

PT J
AU Tang, XO
   Luo, W
   Wang, XG
AF Tang, Xiaoou
   Luo, Wei
   Wang, Xiaogang
TI Content-Based Photo Quality Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clarity contrast; composition geometry; content-based; dark channel; hue
   composition; photo quality assessment; scene composition.
AB Automatically assessing photo quality from the perspective of visual aesthetics is of great interest in high-level vision research and has drawn much attention in recent years. In this paper, we propose content-based photo quality assessment using both regional and global features. Under this framework, subject areas, which draw the most attentions of human eyes, are first extracted. Then regional features extracted from both subject areas and background regions are combined with global features to assess photo quality. Since professional photographers adopt different photographic techniques and have different aesthetic criteria in mind when taking different types of photos ( e. g., landscape versus portrait), we propose to segment subject areas and extract visual features in different ways according to the variety of photo content. We divide the photos into seven categories based on their visual content and develop a set of new subject area extraction methods and new visual features specially designed for different categories. The effectiveness of this framework is supported by extensive experimental comparisons of existing photo quality assessment approaches as well as our new features on different categories of photos. In addition, we propose an approach of online training an adaptive classifier to combine the proposed features according to the visual content of a test photo without knowing its category. Another contribution of this work is to construct a large and diversified benchmark dataset for the research of photo quality assessment. It includes 17,673 photos with manually labeled ground truth. This new benchmark dataset can be down loaded at http://mmlab.ie.cuhk.edu.hk/CUHKPQ/Dataset.htm.
C1 [Tang, Xiaoou; Luo, Wei; Wang, Xiaogang] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Hong Kong, Peoples R China.
   [Tang, Xiaoou] Chinese Acad Sci, Key Lab Comp Vis & Pattern Recognit, Shenzhen Inst Adv Technol, Beijing 100864, Peoples R China.
C3 Chinese University of Hong Kong; Chinese Academy of Sciences; Shenzhen
   Institute of Advanced Technology, CAS
RP Tang, XO (corresponding author), Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Hong Kong, Peoples R China.
EM xtang@ie.cuhk.edu.hk; awesomekeane@gmail.com; xgwang@ee.cuhk.edu.hk
RI wang, Xiaogang/J-5003-2017
FU Hong Kong SAR through GRF projects [CUHK416510, CUHK417110, CUHK417011];
   National Natural Science Foundation of China [61005057]; Guangdong
   Province through Introduced Innovative R&D Team of Guangdong Province
   [201001D0104648280]
FX This work was supported in part by Hong Kong SAR through GRF projects
   CUHK416510, CUHK417110, CUHK417011, National Natural Science Foundation
   of China (61005057), and by Guangdong Province through Introduced
   Innovative R&D Team of Guangdong Province 201001D0104648280.
CR [Anonymous], COMPLETE GUIDE LIGHT
   [Anonymous], P INT C MULT MOD
   [Anonymous], P INT C COMP VIS
   [Anonymous], P EUR C COMP VIS
   [Anonymous], 1995, IEEE INT WORKSH AUT
   Bhattacharya S., 2010, P ACM MULT
   Carucci J., 1995, Capturing the night with your camera: How to take great photographs after dark
   Cohen-Or D., 2006, P ACM SIGGRAPH
   Cui J., 2008, P ACM MULT
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Daly Scott, 1993, P179
   Damera-Venkata N, 2000, IEEE T IMAGE PROCESS, V9, P636, DOI 10.1109/83.841940
   Datta R., 2006, P EUR C COMP VIS
   Datta R., 2008, P IEEE INT C IM PROC
   Dhar S., 2011, P IEEE INT C COMP VI
   DijK A. M., 1995, P ADV IM VID COMM ST
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gao XP, 2007, COLOR RES APPL, V32, P223, DOI 10.1002/col.20321
   Goodale Mark, 2007, PHOTOGRAPHERS EYE CO, P1
   Grey C., 2004, MASTER LIGHTING GUID
   He K, 2009, P IEEE INT C COMP VI
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hoiem D, 2007, INT J COMPUT VISION, V75, P151, DOI 10.1007/s11263-006-0031-y
   Isola P., 2011, P IEEE INT C COMP VI
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jin X., 2010, P EUR C COMP VIS
   Ke Y., 2006, P IEEE INT C COMP VI
   L Lo, 2012, P INF SEC INT CONTR
   Levin A., 2007, P ADV NEUR INF PROC
   Leyvand T., 2008, P ACM SIGGRAPH
   Li X., 2002, P IEEE INT C IM PROC
   London B., 1998, SHORT COURSE PHOTOGR
   Luo W., 2011, P INT C COMP VIS
   Luo Y, 2008, P EUR C COMP VIS
   Machajdik J., 2010, P ACM MULT
   Manav B, 2007, COLOR RES APPL, V32, P144, DOI 10.1002/col.20294
   Mante H., 1972, COLOR DESIGN PHOTOGR
   Muja M., 2009, P VISAPP INT C COMP
   Nishiyama M., 2009, P ACM MULT
   Nishiyama M., 2011, P IEEE INT C COMP VI
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P1918, DOI 10.1109/TIP.2005.854492
   Sun X., 2009, P ACM MULT
   Tang XO, 2012, IEEE T PATTERN ANAL, V34, P1342, DOI 10.1109/TPAMI.2011.242
   Tokumaru M., 2002, P IEEE INT C FUZZ SY
   Tong H., 2004, P PAC RIM C MULT
   Torralba A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P273
   Wang X., 2011, P INT C COMP VIS
   Wang Y, 2012, IEEE T CIRC SYST VID, V22, P989, DOI 10.1109/TCSVT.2012.2186745
   White L., 1995, INFRARED PHOTOGRAPHY
   Williams C.K.I., PASCAL VISUAL OBJECT
   Wong L. K., 2009, P IEEE INT C IM PROC
   Xiao R., 2007, P INT C COMP VIS
   Yeh M., 2012, P IEEE INT C IM PROC
NR 53
TC 177
Z9 192
U1 2
U2 48
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 1930
EP 1943
DI 10.1109/TMM.2013.2269899
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ren, SL
   van der Schaar, M
AF Ren, Shaolei
   van der Schaar, Mihaela
TI Efficient Resource Provisioning and Rate Selection for Stream Mining in
   a Community Cloud
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Energy efficiency; mobile cloud; real-time stream mining; resource
   management; stochastic control
ID RETRIEVAL
AB Real-time stream mining such as surveillance and personal health monitoring, which involves sophisticated mathematical operations, is computation-intensive and prohibitive for mobile devices due to the hardware/computation constraints. To satisfy the growing demand for stream mining in mobile networks, we propose to employ a cloud-based stream mining system in which the mobile devices send via wireless links unclassified media streams to the cloud for classification. We aim at minimizing the classification-energy cost, defined as an affine combination of classification cost and energy consumption at the cloud, subject to an average stream mining delay constraint (which is important in real-time applications). To address the challenge of time-varying wireless channel conditions without a priori information about the channel statistics, we develop an online algorithm in which the cloud operator can dynamically adjust its resource provisioning on the fly and the mobile devices can adapt their transmission rates to the instantaneous channel conditions. It is proved that, at the expense of increasing the average stream mining delay, the online algorithm achieves a classification-energy cost that can be pushed arbitrarily close to the minimum cost achieved by the optimal offline algorithm. Extensive simulations are conducted to validate the analysis.
C1 [Ren, Shaolei] Florida Int Univ, Sch Comp & Informat Sci, Miami, FL 33199 USA.
   [van der Schaar, Mihaela] Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
C3 State University System of Florida; Florida International University;
   University of California System; University of California Los Angeles
RP Ren, SL (corresponding author), Florida Int Univ, Sch Comp & Informat Sci, Miami, FL 33199 USA.
EM sren@cs.fiu.edu; mihaela@ee.ucla.edu
RI Ren, Shaolei/K-6526-2013
OI Ren, Shaolei/0000-0001-9003-4324; van der schaar,
   Mihaela/0000-0003-3933-6049
FU National Science Foundation [1016081]; Direct For Computer & Info Scie &
   Enginr; Division Of Computer and Network Systems [1016081] Funding
   Source: National Science Foundation
FX This work was supported in part by National Science Foundation under
   Grant No. 1016081. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Xiaoqing Zhu.
CR [Anonymous], P IEEE INT C COMP CO
   [Anonymous], 2009, DEP ELECT ENG COMPUT
   [Anonymous], P ACM SIGC
   [Anonymous], P ACM SIGM
   [Anonymous], 1974, MICROWAVE MOBILE COM
   Balasubramanian N., 2009, P ACM USENIX INT MEA
   Buchbinder N., 2011, IFIP NETW
   Chen JJ, 2008, IEEE T MULTIMEDIA, V10, P209, DOI 10.1109/TMM.2007.911821
   Ducasse R, 2010, IEEE J-STSP, V4, P620, DOI 10.1109/JSTSP.2009.2039180
   Fan JP, 2008, IEEE T MULTIMEDIA, V10, P167, DOI 10.1109/TMM.2007.911775
   Foo B., 2008, IS T SPIE MULTIMEDIA
   Fung CHF, 2007, IEEE T COMMUN, V55, P188, DOI 10.1109/TCOMM.2006.885095
   Girod B, 2011, IEEE MULTIMEDIA, V18, P86, DOI 10.1109/MMUL.2011.48
   Goldsmith AJ, 1998, IEEE T COMMUN, V46, P595, DOI 10.1109/26.668727
   Guenter B., 2011, P IEEE INFOCOM
   Jagannatha S., 2006, P IEEE GLOBECOM
   Kumar K, 2010, COMPUTER, V43, P51, DOI 10.1109/MC.2010.98
   Lin M., 2011, P IEEE INFOCOM
   Lorch J. R., 2001, P ACM SIGM
   Neely M. J., 2010, STOCHASTIC NETWORK O
   Satyanarayanan M, 2009, IEEE PERVAS COMPUT, V8, P14, DOI 10.1109/MPRV.2009.82
   Shen JL, 2006, IEEE T MULTIMEDIA, V8, P1179, DOI 10.1109/TMM.2006.884618
   Shyu ML, 2008, IEEE T MULTIMEDIA, V10, P252, DOI 10.1109/TMM.2007.911830
   Singh V. K., 2009, P ACM SIGMM WORKSH S
   Zhang XW, 2011, MOBILE NETW APPL, V16, P270, DOI 10.1007/s11036-011-0305-7
   Zhu XQ, 2004, ARTIF INTELL REV, V22, P177, DOI 10.1007/s10462-004-0751-8
NR 26
TC 27
Z9 30
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2013
VL 15
IS 4
BP 723
EP 734
DI 10.1109/TMM.2013.2240673
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148FK
UT WOS:000319228500002
DA 2024-07-18
ER

PT J
AU Rudinac, S
   Hanjalic, A
   Larson, M
AF Rudinac, Stevan
   Hanjalic, Alan
   Larson, Martha
TI Generating Visual Summaries of Geographic Areas Using
   Community-Contributed Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Automatic evaluation of visual summaries; graph-based models; image set
   diversity; image set representativeness; multimodal fusion; social
   media; visual summarization of geographic areas
ID DIVERSITY
AB In this paper, we present a novel approach for automatic visual summarization of a geographic area that exploits user-contributed images and related explicit and implicit metadata collected from popular content-sharing websites. By means of this approach, we search for a limited number of representative but diverse images to represent the area within a certain radius around a specific location. Our approach is based on the random walk with restarts over a graph that models relations between images, visual features extracted from them, associated text, as well as the information on the uploader and commentators. In addition to introducing a novel edge weighting mechanism, we propose in this paper a simple but effective scheme for selecting the most representative and diverse set of images based on the information derived from the graph. We also present a novel evaluation protocol, which does not require input of human annotators, but only exploits the geographical coordinates accompanying the images in order to reflect conditions on image sets that must necessarily be fulfilled in order for users to find them representative and diverse. Experiments performed on a collection of Flickr images, captured around 207 locations in Paris, demonstrate the effectiveness of our approach.
C1 [Rudinac, Stevan; Hanjalic, Alan; Larson, Martha] Delft Univ Technol, Multimedia Informat Retrieval Lab, Delft, Netherlands.
C3 Delft University of Technology
RP Rudinac, S (corresponding author), Delft Univ Technol, Multimedia Informat Retrieval Lab, Delft, Netherlands.
EM s.rudinac@tudelft.nl; a.hanjalic@tudelft.nl; m.a.larson@tudelft.nl
RI Larson, Martha/E-9983-2014
OI Hanjalic, Alan/0000-0002-5771-2549
FU European Commission [216444]
FX This work was supported by the European Commission's 7th Framework
   Programme (FP7) under Grant 216444 (NoE Peta-Media). This paper resulted
   from an expansion and revision of the short paper, entitled "Finding
   Representative and Diverse Community-contributed Images to Create Visual
   Summaries of Geographic Areas", by Stevan Rudinac, Alan Hanjalic and
   Martha Larson, published at the ACM Multimedia 2011 (MM' 11) conference.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Samson Cheung.
CR [Anonymous], THESIS TU DELFT DELF
   [Anonymous], 2011, P 3 ACM SIGMM INT WO
   Cao LL, 2010, INT CONF ACOUST SPEE, P2274, DOI 10.1109/ICASSP.2010.5495905
   Carbonell J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P335, DOI 10.1145/290941.291025
   Clements M, 2010, ACM T INFORM SYST, V28, DOI 10.1145/1852102.1852107
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Hao Qiang., 2009, Proceedings of the ACM International Conference on Multimedia, P801
   Jana U., 2006, 8th ACM International Workshop on Multimedia Information Retrieval MIR'06, P117
   Kennedy L.S., 2008, P ACM INT C WORLD WI, P297
   Kennedy L.S., 2006, Proc. ACM Multimedia Information Retrieval, P249, DOI DOI 10.1145/1178677.1178712
   Konstas I, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P195, DOI 10.1145/1571941.1571977
   Over P., 2010, P TRECVID
   Page L., 1999, PAGERANK CITATION RA, DOI DOI 10.1109/IISWC.2012.6402911
   Pan J-Y, 2004, P 10 ACM SIGKDD INT, P653, DOI DOI 10.1145/1014052.1014135
   Pang YW, 2011, COMPUT VIS IMAGE UND, V115, P352, DOI 10.1016/j.cviu.2010.10.010
   Paramita ML, 2010, LECT NOTES COMPUT SC, V6242, P45, DOI 10.1007/978-3-642-15751-6_6
   Park YJ, 2008, RECSYS'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON RECOMMENDER SYSTEMS, P11
   Popescu A., 2009, ACM International Conference on Information and Knowledge Management, P1713, DOI DOI 10.1145/1645953.1646211
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   Sanderson M, 2009, LECT NOTES COMPUT SC, V5478, P562, DOI 10.1007/978-3-642-00958-7_51
   Song K., 2006, ACM Multimedia, P707
   Strehl A, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P93, DOI 10.1162/153244303321897735
   Tang JY, 2010, LECT NOTES COMPUT SC, V5993, P179, DOI 10.1007/978-3-642-12275-0_18
   Van Leuken Reinier H, 2009, P 18 INT C WORLD WID, P341, DOI 10.1145/1526709.1526756
   Wang C., 2010, P 18 ACM INT C MULT, P391, DOI [10.1145/ 1873951.1874005, DOI 10.1145/1873951.1874005]
   Yen L, 2009, DATA KNOWL ENG, V68, P338, DOI 10.1016/j.datak.2008.10.006
NR 26
TC 30
Z9 30
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2013
VL 15
IS 4
BP 921
EP 932
DI 10.1109/TMM.2013.2237896
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148FK
UT WOS:000319228500019
DA 2024-07-18
ER

PT J
AU Lee, CH
   Hsu, SB
   Shih, JL
   Chou, CH
AF Lee, Chang-Hsing
   Hsu, Sheng-Bin
   Shih, Jau-Ling
   Chou, Chih-Hsun
TI Continuous Birdsong Recognition Using Gaussian Mixture Modeling of Image
   Shape Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Angular radial transform (ART); birdsong recognition; Gaussian mixture
   models (GMM)
ID SOUNDS; CLASSIFICATION
AB Traditional birdsong recognition approaches used acoustic features based on the acoustic model of speech production or the perceptual model of the human auditory system to identify the associated bird species. In this paper, a new feature descriptor that uses image shape features is proposed to identify bird species based on the recognition of fixed-duration birdsong segments where their corresponding spectrograms are viewed as gray-level images. The MPEG-7 angular radial transform (ART) descriptor, which can compactly and efficiently describe the gray-level variations within an image region in both angular and radial directions, will be employed to extract the shape features from the spectrogram image. To effectively capture both frequency and temporal variations within a birdsong segment using ART, a sector expansion algorithm is proposed to transform its spectrogram image into a corresponding sector image such that the frequency and temporal axes of the spectrogram image will align with the radial and angular directions of the ART basis functions, respectively. For the classification of 28 bird species using Gaussian mixture models (GMM), the best classification accuracy is 86.30% and 94.62% for 3-second and 5-second birdsong segments using the proposed ART descriptor, which is better than traditional descriptors such as LPCC, MFCC, and TDMFCC.
C1 [Lee, Chang-Hsing; Hsu, Sheng-Bin; Shih, Jau-Ling; Chou, Chih-Hsun] Chung Hua Univ, Dept Comp Sci & Informat Engn, Hsinchu 300, Taiwan.
C3 Chung Hua University
RP Lee, CH (corresponding author), Chung Hua Univ, Dept Comp Sci & Informat Engn, Hsinchu 300, Taiwan.
EM chlee@chu.edu.tw; m09602002@chu.edu.tw; sjl@chu.edu.tw; chc@chu.edu.tw
OI Lee, Chang-Hsing/0000-0002-5761-421X
FU National Science Council of R.O.C. [NSC-100-2221-E-216-035]
FX Manuscript received November 22, 2011; revised April 25, 2012; accepted
   July 03, 2012. Date of publication November 27, 2012; date of current
   version January 15, 2013. This work was supported in part by the
   National Science Council of R.O.C. under contract
   NSC-100-2221-E-216-035. The associate editor coordinating the review of
   this manuscript and approving it for publication was Xian-Sheng Hua.
CR Anderson SE, 1996, J ACOUST SOC AM, V100, P1209, DOI 10.1121/1.415968
   [Anonymous], 2002, Introduction to MPEG- 7: Multimedia content description interface
   [Anonymous], P 13 EUR SIGN PROC C
   Aucouturier JJ, 2003, J NEW MUSIC RES, V32, P83, DOI 10.1076/jnmr.32.1.83.16801
   Bardeli R, 2009, IEEE T MULTIMEDIA, V11, P68, DOI 10.1109/TMM.2008.2008920
   Briggs F, 2009, IEEE DATA MINING, P51, DOI 10.1109/ICDM.2009.65
   Chen ZX, 2006, J ACOUST SOC AM, V120, P2974, DOI 10.1121/1.2345831
   Chou C. H., 2007, P 2 INT C INN COMP I
   Duda R., 1973, Pattern Classification and Scene Analysis
   Fagerlund A., EURASIP J ADV SIGNAL, V2007
   Härmä A, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P545
   Juang CF, 2007, NEUROCOMPUTING, V71, P121, DOI 10.1016/j.neucom.2007.08.011
   Kingsbury BED, 1998, SPEECH COMMUN, V25, P117, DOI 10.1016/S0167-6393(98)00032-6
   Kinnunen T., 2006, P IEEE INT C AC SPEE, V1, P14
   Kogan JA, 1998, J ACOUST SOC AM, V103, P2185, DOI 10.1121/1.421364
   Lakshminarayanan B, 2009, EIGHTH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS, PROCEEDINGS, P53, DOI 10.1109/ICMLA.2009.79
   Lee C., 2006, Journal of Information Technology and Applications, V1, P17
   Lee CH, 2008, IEEE T AUDIO SPEECH, V16, P1541, DOI 10.1109/TASL.2008.2005345
   Lee CH, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P204, DOI 10.1109/INOW.2007.4302953
   Lee CH, 2006, IMECS 2006: INTERNATIONAL MULTICONFERENCE OF ENGINEERS AND COMPUTER SCIENTISTS, P331
   Lee CH, 2009, IEEE T MULTIMEDIA, V11, P670, DOI 10.1109/TMM.2009.2017635
   McIlraith AL, 1997, IEEE T SIGNAL PROCES, V45, P2740, DOI 10.1109/78.650100
   McIlraith AL, 1995, IEEE WESCANEX '95 - COMMUNICATIONS, POWER, AND COMPUTING, CONFERENCE PROCEEDINGS, VOLS 1 AND 2, P409, DOI 10.1109/WESCAN.1995.494065
   McIlraith AL, 1997, 1997 CANADIAN CONFERENCE ON ELECTRICAL AND COMPUTER ENGINEERING, CONFERENCE PROCEEDINGS, VOLS I AND II, P63, DOI 10.1109/CCECE.1997.614790
   McIlraith AL, 1997, 1997 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-4, P100, DOI 10.1109/ICNN.1997.611645
   Mörchen F, 2006, IEEE T AUDIO SPEECH, V14, P81, DOI 10.1109/TSA.2005.860352
   PARKER TA, 1991, AUK, V108, P443
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   REYNOLDS DA, 1995, IEEE T SPEECH AUDI P, V3, P72, DOI 10.1109/89.365379
   Selin A., EURASIP J ADV SIGNAL, V2007
   Singh NC, 2003, J ACOUST SOC AM, V114, P3394, DOI 10.1121/1.1624067
   Somervuo P, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P825
   Somervuo P, 2006, IEEE T AUDIO SPEECH, V14, P2252, DOI 10.1109/TASL.2006.872624
   Sukittanon S, 2004, IEEE T SIGNAL PROCES, V52, P3023, DOI 10.1109/TSP.2004.833861
   Tyagi V., 2003, P WORKSH AUT SPEECH
   Vallejo EE, 2007, LECT NOTES ARTIF INT, V4828, P212
   Wang H., 2007, 37 ANN M SOC NEUR
   Yushan National Park, 1996, CD SOUND MOUNT
   Yushan National Park, 1995, CD SOUND MOUNT
NR 39
TC 44
Z9 55
U1 2
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2013
VL 15
IS 2
BP 454
EP 464
DI 10.1109/TMM.2012.2229969
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 075HN
UT WOS:000313875500019
DA 2024-07-18
ER

PT J
AU Yuan, H
   Liu, J
   Sun, JD
   Liu, HC
   Li, YJ
AF Yuan, Hui
   Liu, Ju
   Sun, Jiande
   Liu, Hechao
   Li, Yujun
TI Affine Model Based Motion Compensation Prediction for Zoom
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE H.264/AVC; motion compensation; video coding; zoom motion model
AB Zoom motion is classified into two categories, i.e., global zoom motion and local zoom motion. A simple affine motion model with four parameters is utilized to describe zoom motion efficiently based on the analyses of camera imaging principles. Based on the motion model, a basic candidate motion vector (BCMV) of a block could be derived when model parameters are confirmed. Then a set of candidate motion vectors (CMVs) could be obtained by modifying the BCMV. Thereafter, template matching is used to choose the optimal CMV (OCMV). Finally, the block is coded with the optimal CMV as an independent mode, and a rate distortion (RD) criterion is used to determine whether to use the mode or not. Experimental results demonstrate that by implementing the proposed method into Key Technology Area test platform version 2.6r1 (KTA2.6r1), a maximum -21.99% and average -8.72% bit rate savings can be achieved for videos involving zoom motion, while maintaining the same quality (evaluated by PSNR) of reconstructed videos when IPPPP coding structure is used. When Hierarchical B coding structure is employed, the maximum and average bit rate savings are -10.48% and -5.575% when the qualities of reconstructed videos remain unchanged. Besides, for videos involving camera rotation, translation, etc., an average -2.04% bit rate savings could also be achieved; while for videos containing common motions, an average -1.19% bit rate saving could be achieved at the same quality of reconstructed videos.
C1 [Yuan, Hui; Liu, Ju; Sun, Jiande] Shandong Univ, Sch Informat Sci & Engn, Jinan 250100, Peoples R China.
   [Liu, Ju; Li, Yujun] Digital Multimedia Technol Co LTD, Hisense State Key Lab, Qingdao 266061, Peoples R China.
   [Liu, Ju] Southeast Univ, Natl Mobile Commun Res Lab, Nanjing 210096, Jiangsu, Peoples R China.
   [Liu, Hechao] Xidian Univ, State Key Lab ISN, Xian 710071, Peoples R China.
C3 Shandong University; Hisense; Southeast University - China; Xidian
   University
RP Yuan, H (corresponding author), Shandong Univ, Sch Informat Sci & Engn, Jinan 250100, Peoples R China.
EM yuanhui0325@gmail.com
RI wang, yingying/GRS-3058-2022; Sun, Jiande/B-4681-2018; Yuan,
   Hui/HDO-3699-2022; Yuan, Hui/B-6738-2013
OI Yuan, Hui/0000-0001-5212-3393; Yuan, Hui/0000-0001-5212-3393
FU National Basic Research Program of China (973Program) [2009CB320905,
   2010CB735906]; China Postdoctoral Science Foundation [2011M501131,
   2011M501092]; Special Fund for Postdoctoral Innovative Projects of
   Shandong Province [201103003]; Independent Innovation Foundation of
   Shandong University [2011GN061, 2011GN062, 2010JC007]; National Natural
   Science Foundation of China [60872024, 61001180]; Cultivation Fund of
   the Key Scientific and Technical Innovation Project [708059]; National
   Mobile Communications Research Laboratory, Southeast University
   [2010D10]; National Digital Multimedia Key Laboratory [2011-1-1557]
FX This work was supported in part by the National Basic Research Program
   of China (973Program.2009CB320905, 2010CB735906); China Postdoctoral
   Science Foundation funded project (2011M501131 and 2011M501092); Special
   Fund for Postdoctoral Innovative Projects of Shandong Province
   (201103003); Independent Innovation Foundation of Shandong University
   (2011GN061, 2011GN062, and 2010JC007); the National Natural Science
   Foundation of China (60872024, 61001180); the Cultivation Fund of the
   Key Scientific and Technical Innovation Project (708059); the open
   research fund of the National Mobile Communications Research Laboratory,
   Southeast University (2010D10); and the open research fund of National
   Digital Multimedia Key Laboratory (2011-1-1557). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Charles D. (Chuck) Creusere.
CR [Anonymous], 2010, P 39 M ITU T VID COD
   [Anonymous], 2008, ISOIECJTC1SC29WG11N9
   Bjontegaard G., 2008, P 35 M ITU T VID COD
   Intel Co. Ltd., 2008, P ITU T VID COD EXP
   Kordasiewicz RC, 2007, IEEE T CIRC SYST VID, V17, P1388, DOI 10.1109/TCSVT.2007.903777
   Krutz A., 2010, P 2 M JOINT COLL TEA
   Li ZB, 2010, SIGNAL PROCESS-IMAGE, V25, P610, DOI 10.1016/j.image.2010.05.007
   Muhit AA, 2010, IEEE T CIRC SYST VID, V20, P661, DOI 10.1109/TCSVT.2010.2045804
   Park S., 2010, P 2 M JOINT COLL TEA
   Po LM, 2010, IEEE T CIRC SYST VID, V20, P1625, DOI 10.1109/TCSVT.2010.2087474
   Sprljan N., 2010, P 2 M JOINT COLL TEA
   Sugimoto K, 2004, IEEE IMAGE PROC, P465
   Telecommunication Standardization Sector of ITU, 2011, TEL STAND SECT ITU H
   Vautard M. A. A., 2011, IEEE T CIRCUITS SYST, V21, P946
   Video Quality Experts Group (VQEG), SVT HIGH DEF MULT TE
   Wedi T, 2003, IEEE T CIRC SYST VID, V13, P577, DOI 10.1109/TCSVT.2003.815171
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P688, DOI 10.1109/TCSVT.2003.815168
   Yuan H, 2010, IEEE SIGNAL PROC LET, V17, P787, DOI 10.1109/LSP.2010.2055051
NR 18
TC 14
Z9 19
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
BP 1370
EP 1375
DI 10.1109/TMM.2012.2190393
PN 2
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QM
UT WOS:000306599400021
DA 2024-07-18
ER

PT J
AU Suau, X
   Ruiz-Hidalgo, J
   Casas, JR
AF Suau, Xavier
   Ruiz-Hidalgo, Javier
   Casas, Josep R.
TI Real-Time Head and Hand Tracking Based on 2.5D Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Multimedia and Expo (ICME)
CY JUL 11-15, 2011
CL Univ Ramon Llull, La Salle, Barcelona, SPAIN
SP IEEE, IEEE Signal Proc Soc, IEEE Circuits & Syst Soc, IEEE Comp Soc, IEEE Commun Soc
HO Univ Ramon Llull, La Salle
DE Gesture recognition; hand tracking; head tracking; interactivity; range
   camera
ID MOTION; SHAPE; POSE
AB A novel real-time algorithm for head and hand tracking is proposed in this paper. This approach is based on data from a range camera, which is exploited to resolve ambiguities and overlaps. The position of the head is estimated with a depth-based template matching, its robustness being reinforced with an adaptive search zone. Hands are detected in a bounding box attached to the head estimate, so that the user may move freely in the scene. A simple method to decide whether the hands are open or closed is also included in the proposal. Experimental results show high robustness against partial occlusions and fast movements. Accurate hand trajectories may be extracted from the estimated hand positions, and may be used for interactive applications as well as for gesture classification purposes.
C1 [Suau, Xavier; Ruiz-Hidalgo, Javier; Casas, Josep R.] Univ Politecn Cataluna, ES-08034 Barcelona, Spain.
C3 Universitat Politecnica de Catalunya
RP Suau, X (corresponding author), Univ Politecn Cataluna, ES-08034 Barcelona, Spain.
EM xavier.suau@upc.edu; josep.ramon.casas@upc.edu; j.ruiz@upc.edu
RI Castellvi, Josep/C-5092-2014; Ruiz-Hidalgo, Javier/F-8137-2013; Casas,
   Josep R./A-2851-2010
OI Castellvi, Josep/0000-0001-5745-9996; Ruiz-Hidalgo,
   Javier/0000-0001-6774-685X; Casas, Josep R./0000-0003-4639-6904
CR Alcoverro M., 2010, LECT NOTES COMPUTER, V6169
   [Anonymous], 2011, SR4000 MESA IM
   [Anonymous], 2006 IEEE INT C VIDE, DOI DOI 10.1109/AVSS.2006.92
   Bohme M., 2009, Proceedings of the IEEE International Symposium on Signals, Circuits Systems, P1, DOI DOI 10.1109/ISSCS.2009.5206211
   Brubaker MA, 2010, INT J COMPUT VISION, V87, P140, DOI 10.1007/s11263-009-0274-5
   Corazza S, 2010, INT J COMPUT VISION, V87, P156, DOI 10.1007/s11263-009-0284-3
   Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745
   Gall J, 2009, PROC CVPR IEEE, P1746, DOI 10.1109/CVPRW.2009.5206755
   Ganapathi V, 2010, PROC CVPR IEEE, P755, DOI 10.1109/CVPR.2010.5540141
   Grest D, 2007, LECT NOTES COMPUT SC, V4522, P719
   Guan P, 2009, IEEE I CONF COMP VIS, P1381, DOI 10.1109/iccv.2009.5459300
   Haker Martin, 2007, P ISSCS JUL, P3
   Hasler N, 2010, PROC CVPR IEEE, P1823, DOI 10.1109/CVPR.2010.5539853
   Knoop S, 2006, IEEE INT CONF ROBOT, P1686
   Kolb A, 2010, COMPUT GRAPH FORUM, V29, P141, DOI 10.1111/j.1467-8659.2009.01583.x
   Kuo AD, 2002, J BIOMECH ENG-T ASME, V124, P113, DOI 10.1115/1.1427703
   Lehment NH, 2010, IEEE IMAGE PROC, P2465, DOI 10.1109/ICIP.2010.5651643
   Malassiotis S, 2005, PATTERN RECOGN, V38, P1153, DOI 10.1016/j.patcog.2004.11.020
   Maneewongvatana S., 1999, CTR GEOM COMP 4 ANN, P8
   Neumann Joachim, 2007, PERS UBIQUIT COMPUT, V13, P15
   Nichau M, 2010, COMPUT VIS IMAGE UND, V114, P1346, DOI 10.1016/j.cviu.2009.11.003
   Plagemann C, 2010, IEEE INT CONF ROBOT, P3108, DOI 10.1109/ROBOT.2010.5509559
   Pons-Moll G., 2010, ELEMENTS, P2
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Suau X., 2011, PROC IEEE INT C MULT, P1
   Sundaresan A, 2009, IEEE T IMAGE PROCESS, V18, P2114, DOI 10.1109/TIP.2009.2022290
   Yan JY, 2008, IEEE T PATTERN ANAL, V30, P865, DOI 10.1109/TPAMI.2007.70739
   Youding Zhu, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563163
NR 28
TC 38
Z9 44
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 575
EP 585
DI 10.1109/TMM.2012.2189853
PN 1
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 943ZG
UT WOS:000304166300009
DA 2024-07-18
ER

PT J
AU Wang, TH
   Collomosse, J
AF Wang, Tinghuai
   Collomosse, John
TI Probabilistic Motion Diffusion of Labeling Priors for Coherent Video
   Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computer vision; image segmentation; image sequences
ID TEXTURE
AB We present a robust algorithm for temporally coherent video segmentation. Our approach is driven by multi-label graph cut applied to successive frames, fusing information from the current frame with an appearance model and labeling priors propagated forwarded from past frames. We propagate using a novel motion diffusion model, producing a per-pixel motion distribution that mitigates against cumulative estimation errors inherent in systems adopting "hard" decisions on pixel motion at each frame. Further, we encourage spatial coherence by imposing label consistency constraints within image regions (super-pixels) obtained via a bank of unsupervised frame segmentations, such as mean-shift. We demonstrate quantitative improvements in accuracy over state-of-the-art methods on a variety of sequences exhibiting clutter and agile motion, adopting the Berkeley methodology for our comparative evaluation.
C1 [Wang, Tinghuai; Collomosse, John] Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
C3 University of Surrey
RP Wang, TH (corresponding author), Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
EM Tinghuai.Wang@surrey.ac.uk; J.Collomosse@surrey.ac.uk
FU Hewlett-Packard Labs IRP
FX Manuscript received May 22, 2011; revised September 14, 2011; accepted
   November 08, 2011. Date of publication November 22, 2011; date of
   current version March 21, 2012. This work was supported under the
   Hewlett-Packard Labs IRP Programme. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Alan Hanjalic.
CR Ahmed MA, 2009, IEEE IMAGE PROC, P2757, DOI 10.1109/ICIP.2009.5414134
   Alahari K., 2008, IEEE C COMP VIS PATT
   [Anonymous], 2001, Interactive graph cuts for optimal boundary & region segmentation of objects in nd images, DOI DOI 10.1109/ICCV.2001.937505
   [Anonymous], P INT C IM PROC ICIP
   [Anonymous], 2010, P NPAR
   [Anonymous], 2001, PROC 18 INT C MACH L
   Arbelaez P., 2008, Proc. IEEE Conf. Computer Vision and Pattern Recognition CVPR 2008, P1
   Bai X, 2010, LECT NOTES COMPUT SC, V6315, P617
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5
   Brendel W., 2009, P CVPR
   Collomosse J., 2009, P ICCV
   Collomosse JP, 2005, IEEE T VIS COMPUT GR, V11, P540, DOI 10.1109/TVCG.2005.85
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Dementhon D., 2002, J IMAGE VIS COMPUT
   Goldman DB, 2006, ACM T GRAPHIC, V25, P862, DOI 10.1145/1141911.1141967
   Greenspan H, 2002, LECT NOTES COMPUT SC, V2353, P461
   Grundmann M., 2010, P CVPR
   He XM, 2006, LECT NOTES COMPUT SC, V3951, P338
   Hoiem D, 2005, IEEE I CONF COMP VIS, P654
   Khan S., 2001, P COMP VIS PATT REC
   Kim TH, 2010, PROC CVPR IEEE, P3201, DOI 10.1109/CVPR.2010.5540078
   Kohli P., 2009, INT J COMPUT VIS IJC
   Lee YJ, 2012, IEEE T PATTERN ANAL, V34, P346, DOI 10.1109/TPAMI.2011.122
   Leung T, 2001, INT J COMPUT VISION, V43, P29, DOI 10.1023/A:1011126920638
   Liu C, 2008, LECT NOTES COMPUT SC, V5304, P28, DOI 10.1007/978-3-540-88690-7_3
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Moscheni F, 1998, IEEE T PATTERN ANAL, V20, P897, DOI 10.1109/34.713358
   Paris S, 2008, LECT NOTES COMPUT SC, V5303, P460, DOI 10.1007/978-3-540-88688-4_34
   Patras I., 2001, IEEE T PATTERN ANAL, V32, P1553
   Price B.L., 2009, Proc. of ICCV
   Rabinovich Andrew., 2006, CVPR, V1, P1130
   Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10
   Ristivojevic M, 2006, IEEE T IMAGE PROCESS, V15, P364, DOI 10.1109/TIP.2005.860616
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Russell B.C., 2006, Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on, V2, P1605, DOI DOI 10.1109/CVPR.2006.326
   Shi JB, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P1154, DOI 10.1109/ICCV.1998.710861
   Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1
   Varma M, 2005, INT J COMPUT VISION, V62, P61, DOI 10.1007/s11263-005-4635-4
   Wang J, 2004, LECT NOTES COMPUT SC, V3022, P238
   Wang T., 2010, P IEEE INT C IM PROC
   Winn J, 2005, IEEE I CONF COMP VIS, P1800
NR 43
TC 28
Z9 32
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2012
VL 14
IS 2
BP 389
EP 400
DI 10.1109/TMM.2011.2177078
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OZ
UT WOS:000302702500013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Alay, O
   Liu, P
   Wang, Y
   Erkip, E
   Panwar, SS
AF Alay, Oezgue
   Liu, Pei
   Wang, Yao
   Erkip, Elza
   Panwar, Shivendra S.
TI Cooperative Layered Video Multicast Using Randomized Distributed Space
   Time Codes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Layered video coding; randomized distributed space time coding; user
   cooperation; video multicast; wireless networks
ID BLOCK-CODES; WIRELESS; DIVERSITY; NETWORKS; FEC
AB With the increased popularity of mobile multimedia services, efficient and robust video multicast strategies are of critical importance. Cooperative communications has been shown to improve the robustness and the data rates for point-to-point transmission. In this paper, a two-hop cooperative transmission scheme for multicast in infrastructure-based networks is used, where multiple relays forward the data simultaneously using randomized distributed space time codes (RDSTC). This randomized cooperative transmission is further integrated with layered video coding and packet level forward error correction (FEC) to enable efficient and robust video multicast. Three different schemes are proposed to find the system operating parameters based on the availability of the channel information at the source station: RDSTC with full channel information, RDSTC with limited channel information, and RDSTC with node count. The performance of these three schemes are compared with rate adaptive direct transmission and conventional multicast that does not use rate adaptation. The results show that while rate-adaptive direct transmission provides better video quality than conventional multicast, all three proposed randomized cooperative schemes outperform both strategies significantly as long as the network has enough nodes. Furthermore, the performance gap between RDSTC with full channel information and RDSTC with limited channel information or node count is relatively small, indicating the robustness of the proposed cooperative multicast system using RDSTC.
C1 [Alay, Oezgue; Liu, Pei; Wang, Yao; Erkip, Elza; Panwar, Shivendra S.] NYU, Polytech Inst, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
C3 New York University; New York University Tandon School of Engineering
RP Alay, O (corresponding author), NYU, Polytech Inst, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
EM ozgu@vision.poly.edu; pliu@poly.edu; yao@poly.edu; elza@poly.edu;
   panwar@catt.poly.edu
RI Erkip, Elza/A-2992-2012; Panwar, Shivendra/K-6473-2019; Panwar,
   Shivendra S/A-6884-2016; Liu, Pei/C-8824-2012
OI Panwar, Shivendra S/0000-0002-9822-6838; Wang, Yao/0000-0003-3199-3802;
   Erkip, Elza/0000-0001-8718-8648
FU National Science Foundation (NSF) [0430885, 0905446]; New York State
   Center for Advanced Technology in Telecommunications (CATT); Wireless
   Internet Center for Advanced Technology (WICAT); NSF Industry/University
   Cooperative Research Center at Polytechnic Institute of NYU; Direct For
   Computer & Info Scie & Enginr; Division of Computing and Communication
   Foundations [0430885] Funding Source: National Science Foundation;
   Directorate For Engineering; Div Of Industrial Innovation & Partnersh
   [0933985] Funding Source: National Science Foundation; Division Of
   Computer and Network Systems; Direct For Computer & Info Scie & Enginr
   [0905446] Funding Source: National Science Foundation
FX Manuscript received April 28, 2010; revised September 07, 2010 and
   December 21, 2010; accepted May 10, 2011. Date of publication May 27,
   2011; date of current version September 16, 2011. This work was
   supported in part by National Science Foundation (NSF) under the awards
   0430885 and 0905446, and the New York State Center for Advanced
   Technology in Telecommunications (CATT). The work is also supported by
   Wireless Internet Center for Advanced Technology (WICAT), an NSF
   Industry/University Cooperative Research Center at Polytechnic Institute
   of NYU. This paper was presented in part at INFOCOM-MOVID 2009 and
   ICASSP 2010. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Zhihai (Henry) He.
CR Alamouti SM, 1998, IEEE J SEL AREA COMM, V16, P1451, DOI 10.1109/49.730453
   ALAY O, IEEE T CIRC IN PRESS
   ALAY O, 2010, P IEEE PIMRC IST TUR
   ALAY O, 2010, P IEEE ICASSP DALL T
   Alay O., 2009, P IEEE INF MOVID WOR
   Alay Ö, 2010, MOBILE NETW APPL, V15, P425, DOI 10.1007/s11036-009-0202-5
   [Anonymous], 2016, IEEE Standard 802.11-2020
   Bajic IV, 2007, IEEE T BROADCAST, V53, P276, DOI 10.1109/TBC.2006.889211
   Chan SHG, 2006, IEEE T MULTIMEDIA, V8, P370, DOI 10.1109/TMM.2005.864340
   Chou CT, 2006, IEEE J SEL AREA COMM, V24, P2081, DOI 10.1109/JSAC.2006.881621
   Cordeiro CD, 2003, IEEE NETWORK, V17, P52, DOI 10.1109/MNET.2003.1174178
   Gündüz D, 2007, IEEE T INFORM THEORY, V53, P3454, DOI 10.1109/TIT.2007.904963
   Jafarkhani H., 2005, Space-Time Coding, Theory and Practice
   *JOINT VID TEAM, 2007, JVTX203
   Kim T, 2005, IEEE T MULTIMEDIA, V7, P1123, DOI 10.1109/TMM.2005.858376
   Laneman JN, 2004, IEEE T INFORM THEORY, V50, P3062, DOI 10.1109/TIT.2004.838089
   LEE S, 2000, P IEEE INFOCOM
   Lee TWA, 2002, IEEE T CIRC SYST VID, V12, P1059, DOI 10.1109/TCSVT.2002.806816
   LIU P, 2009, P IEEE GLOBECOM DEC
   Liu P., 2008, P IEEE GLOBECOM NOV
   Lu KJ, 2005, IEEE T INFORM THEORY, V51, P4340, DOI 10.1109/TIT.2005.858943
   Majumdar A, 2002, IEEE T CIRC SYST VID, V12, P524, DOI 10.1109/TCSVT.2002.800315
   MAO S, 2004, P IEEE BROADNETS
   Proakis JG., 2001, Digital Communications
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Sendonaris A, 2003, IEEE T COMMUN, V51, P1939, DOI 10.1109/TCOMM.2003.819238
   Sendonaris A, 2003, IEEE T COMMUN, V51, P1927, DOI 10.1109/TCOMM.2003.818096
   Sharp M, 2009, IEEE T COMMUN, V57, P64, DOI 10.1109/TCOMM.2009.0901.070003
   Shutoy HY, 2007, IEEE J-STSP, V1, P295, DOI 10.1109/JSTSP.2007.901516
   Sirkeci-Mergen B, 2007, IEEE T SIGNAL PROCES, V55, P5003, DOI 10.1109/TSP.2007.896061
   Tarokh V, 1999, IEEE T INFORM THEORY, V45, P1456, DOI 10.1109/18.771146
   VERDE F, 2008, P IEEE SPAWC JUL
   Vereecken W, 2009, ANALOG CIRC SIG PROC, P1
   Wei W, 2007, IEEE T CIRC SYST VID, V17, P2, DOI 10.1109/TCSVT.2006.885719
   Yousefi'zadeh H, 2006, IEEE T MULTIMEDIA, V8, P1219, DOI 10.1109/TMM.2006.884612
   ZHU X, 2008, P VCIP
NR 37
TC 26
Z9 31
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 1127
EP 1140
DI 10.1109/TMM.2011.2158088
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300024
DA 2024-07-18
ER

PT J
AU Chen, OTC
   Hsia, ML
   Chen, CC
AF Chen, Oscal T. -C.
   Hsia, Meng-Lin
   Chen, Chih-Chang
TI Low-Complexity Inverse Transforms of Video Codecs in an Embedded
   Programmable Platform
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computational complexity; end-of-block point; H.264/AVC; inverse
   transform; MPEG-2; video coding
ID ALGORITHM; DECODER; DCT
AB This work presents an efficient low-complexity method to compute a 2-D inverse transform flexibly adapted to its end-of-block (EOB) point and corner coefficients. First, the EOB point is obtained from a bitstream or derived from the other parameters. Second, the values of bottom-left and/or top-right corner coefficients before an EOB point are verified as zero or not. Third, the operational mode, based on the EOB point and corner coefficient value(s), determines the reduced dimensional sizes of 1-D inverse transforms in the row and column. Additionally, the computational order of row-after-column or column-after-row is decided to minimize computational complexity. Finally, a 2-D inverse transform at the determined operational mode is computed using simplified 1-D inverse transforms in a row-by-row and column-by-column manner. Particularly, how to implement cycle-efficient structures of inverse transforms in an embedded programmable platform is investigated. Simulation results demonstrate that the proposed method has less computational complexity than conventional methods when executing 2-D inverse transforms at MPEG-2 and H. 264/AVC video streams. Notably, the proposed method can reduce computational time required by conventional methods by 14.6%-92.9% at a fairly increased code size. Therefore, the proposed method is very suitable for various applications demanding low-complexity computations of inverse transforms.
C1 [Chen, Oscal T. -C.; Hsia, Meng-Lin; Chen, Chih-Chang] Natl Chung Cheng Univ, Dept Elect Engn, Signal & Media Lab, Chiayi 621, Taiwan.
C3 National Chung Cheng University
RP Chen, OTC (corresponding author), Natl Chung Cheng Univ, Dept Elect Engn, Signal & Media Lab, Chiayi 621, Taiwan.
EM oscal@ee.ccu.edu.tw; mlhsia@samlab.ee.ccu.edu.tw;
   ccchen@samlab.ee.ccu.edu.tw
FU National Science Council, Taiwan [NSC98-2622-E-194-004-A2, NSC
   98-2221-E-194-038]
FX Manuscript received October 31, 2010; revised March 21, 2011; accepted
   May 02, 2011. Date of publication May 23, 2011; date of current version
   September 16, 2011. This work was supported in part by the National
   Science Council, Taiwan, under contract numbers NSC98-2622-E-194-004-A2
   and NSC 98-2221-E-194-038. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Ming-Ting
   Sun.
CR [Anonymous], P INT C AC SPEECH SI
   Britanak V., 2007, Discrete Cosine and Sine Transforms: General Properties, Fast Algorithms and Integer Approximations
   Budagavi M, 2000, IEEE SIGNAL PROC MAG, V17, P36, DOI 10.1109/79.814645
   Chao YC, 2009, IEEE T CIRC SYST VID, V19, P53, DOI 10.1109/TCSVT.2008.2009251
   Chen YK, 2006, J VIS COMMUN IMAGE R, V17, P509, DOI 10.1016/j.jvcir.2005.05.004
   Choi K, 2010, IEEE T CONSUM ELECTR, V56, P1822, DOI 10.1109/TCE.2010.5606332
   Fan CP, 2006, IEICE T INF SYST, VE89D, P3006, DOI 10.1093/ietisy/e89-d.12.3006
   GORDON S, 2004, JVTK028 ISOIEC ITUT
   Hsia ML, 2010, IEEE INT CON MULTI, P826, DOI 10.1109/ICME.2010.5583070
   HSIA ML, 2009, P IASTED INT C SIGN, P236
   Ji XY, 2009, IEEE T CIRC SYST VID, V19, P1755, DOI 10.1109/TCSVT.2009.2026839
   Kim SD, 2005, IEEE INT SYMP CIRC S, P3323, DOI 10.1109/ISCAS.2005.1465339
   Lin HC, 2009, IEEE INT CON MULTI, P1114, DOI 10.1109/ICME.2009.5202694
   Malvar HS, 2003, IEEE T CIRC SYST VID, V13, P598, DOI 10.1109/TCSVT.2003.814964
   MURATA E, 1998, P IEEE INT C AC SPEE, V5, P3105
   Pao IM, 1999, IEEE T CIRC SYST VID, V9, P608, DOI 10.1109/76.767126
   Plonka G, 2005, LINEAR ALGEBRA APPL, V394, P309, DOI 10.1016/j.laa.2004.07.015
   *TEX INSTR, 2001, TMS320C54X DSP REF S, V1
   WIEGAND T, 2003, H264ISOIEC1449610AVC
   WINGER L, 2008, Patent No. 7366236
   Zhang DM, 2007, J VIS COMMUN IMAGE R, V18, P59, DOI 10.1016/j.jvcir.2006.10.005
NR 21
TC 3
Z9 5
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 905
EP 921
DI 10.1109/TMM.2011.2155640
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300007
DA 2024-07-18
ER

PT J
AU Li, SN
   Zhang, F
   Ma, L
   Ngan, KN
AF Li, Songnan
   Zhang, Fan
   Ma, Lin
   Ngan, King Ngi
TI Image Quality Assessment by Separately Evaluating Detail Losses and
   Additive Impairments
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contrast masking; contrast sensitivity function; decoupling algorithm;
   human visual system
ID COSINE TRANSFORM; VISIBILITY; METRICS
AB In the research field of image processing, mean squared error (MSE) and peak signal-to-noise ratio (PSNR) are extensively adopted as the objective visual quality metrics, mainly because of their simplicity for calculation and optimization. However, it has been well recognized that these pixel-based difference measures correlate poorly with the human perception. Inspired by existing works [1]-[3], in this paper we propose a novel algorithm which separately evaluates detail losses and additive impairments for image quality assessment. The detail loss refers to the loss of useful visual information which affects the content visibility, and the additive impairment represents the redundant visual information whose appearance in the test image will distract viewer's attention from the useful contents causing unpleasant viewing experience. To separate detail losses and additive impairments, a wavelet-domain decoupling algorithm is developed which can be used for a host of distortion types. Two HVS characteristics, i.e., the contrast sensitivity function and the contrast masking effect, are taken into account to approximate the HVS sensitivities. We propose two simple quality measures to correlate detail losses and additive impairments with visual quality, respectively. Based on the findings in [3] that observers judge low-quality images in terms of the ability to interpret the content, the outputs of the two quality measures are adaptively combined to yield the overall quality index. By conducting experiments based on five subjectively-rated image databases, we demonstrate that the proposed metric has a better or similar performance in matching subjective ratings when compared with the state-of-the-art image quality metrics.
C1 [Li, Songnan; Zhang, Fan; Ma, Lin; Ngan, King Ngi] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
RP Li, SN (corresponding author), Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
EM snli@ee.cuhk.edu.hk; fzhang@ee.cuhk.edu.hk; lma@ee.cuhk.edu.hk;
   knngan@ee.cuhk.edu.hk
RI Ngan, N/E-8240-2014
OI Ngan, N/0000-0003-1946-3235
CR [Anonymous], A57 DATABASE
   [Anonymous], 2005, SUBJECTIVE QUALITY A
   [Anonymous], 2003, Final report from the video quality experts group on the validation of objective models of video quality assessment
   [Anonymous], 2020, INT TELECOMMUNICATIO
   [Anonymous], MICT image quality evaluation database
   [Anonymous], The genetic algorithm toolbox for MATLAB
   Bradley AP, 1999, IEEE T IMAGE PROCESS, V8, P717, DOI 10.1109/83.760338
   Chandler D. M., 2010, CSIQ DATABASE
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Cheng H, 2005, PROC SPIE, V5666, P160, DOI 10.1117/12.587947
   DALY S, 1992, P SOC PHOTO-OPT INS, V1666, P2, DOI 10.1117/12.135952
   Damera-Venkata N, 2000, IEEE T IMAGE PROCESS, V9, P636, DOI 10.1109/83.841940
   Gaubatz M., MeTriX MuX visual quality assessment package
   *ITU R, 1994, BT8141 ITUR
   ITU-R, 1998, BT7104 ITUR
   Lai YK, 2000, J VIS COMMUN IMAGE R, V11, P17, DOI 10.1006/jvci.1999.0433
   Laparra V, 2010, J OPT SOC AM A, V27, P852, DOI 10.1364/JOSAA.27.000852
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lee S, 2002, IEEE T MULTIMEDIA, V4, P129, DOI 10.1109/6046.985561
   Li SN, 2010, IEEE INT SYMP CIRC S, P3373, DOI 10.1109/ISCAS.2010.5537870
   Lubin J., 1993, DIGITAL IMAGES HUMAN
   Marziliano P, 2004, SIGNAL PROCESS-IMAGE, V19, P163, DOI 10.1016/j.image.2003.08.003
   Masry M, 2006, IEEE T CIRC SYST VID, V16, P260, DOI 10.1109/TCSVT.2005.861946
   MITSA KVT, 1993, P ICASSP, P301
   Narwaria M, 2010, IEEE T NEURAL NETWOR, V21, P515, DOI 10.1109/TNN.2010.2040192
   NGAN KN, 1989, IEEE T ACOUST SPEECH, V37, P1743, DOI 10.1109/29.46556
   NILL NB, 1985, IEEE T COMMUN, V33, P551, DOI 10.1109/TCOM.1985.1096337
   Ong EP, 2003, SEVENTH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, VOL 1, PROCEEDINGS, P469, DOI 10.1109/ISSPA.2003.1224741
   PELI E, 1990, J OPT SOC AM A, V7, P2032, DOI 10.1364/JOSAA.7.002032
   Ponomarenko N., 2008, Tampere image database
   PONOMARENKO N., 2009, 4 INT WORKSH VID PRO, P6
   Ramasubramanian M, 1999, COMP GRAPH, P73, DOI 10.1145/311535.311543
   ROUSE DM, 2009, HUMAN VIS ELECT IMAG, V7240
   Seshadrinathan K, 2008, IEEE IMAGE PROC, P1200, DOI 10.1109/ICIP.2008.4711976
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P1918, DOI 10.1109/TIP.2005.854492
   Sheikh HR, 2005, LIVE IMAGE QUALITY A, DOI DOI 10.1109/CVPR.2015.7298594
   TEO PC, 1994, P SOC PHOTO-OPT INS, V2179, P127, DOI 10.1117/12.172664
   Wang Y., 2002, VIDEO PROCESSING COM
   Wang Z, 2004, ADV NEUR IN, V16, P1435
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Watson A.B., 1993, SOC INFORM DISPLAY D, Vxxiv, P946
   Watson AB, 1997, IEEE T IMAGE PROCESS, V6, P1164, DOI 10.1109/83.605413
   Winkler S, 1999, PROC SPIE, V3644, P175, DOI 10.1117/12.348438
   Wu HR, 1997, IEEE SIGNAL PROC LET, V4, P317, DOI 10.1109/97.641398
   Yang CL, 2008, IEEE IMAGE PROC, P377, DOI 10.1109/ICIP.2008.4711770
   Zhai GT, 2005, IEEE WRK SIG PRO SYS, P331
   Zhang F, 2010, PROC SPIE, V7744, DOI 10.1117/12.863083
   ZHANG M, 2008, P ICIP, P381
NR 52
TC 172
Z9 196
U1 1
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 935
EP 949
DI 10.1109/TMM.2011.2152382
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300009
DA 2024-07-18
ER

PT J
AU Xia, PY
   Chan, SHG
   Jin, X
AF Xia, Pengye
   Chan, S. -H. Gary
   Jin, Xing
TI Optimal Bandwidth Assignment for Multiple-Description-Coded Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multiple-description-coded video; optimal description bandwidth
   assignment; simulated annealing; streaming
AB In video streaming over multicast network, user bandwidth requirement is often heterogeneous possibly with orders of magnitude difference (say, from hundreds of kb/s for mobile devices to tens of Mb/s for high-definition TV). Multiple description coding (MDC) can be used to address this bandwidth heterogeneity issue. In MDC, the video source is encoded into multiple independent descriptions. A receiver, depending on its available bandwidth, joins different descriptions to meet their bandwidth requirements.
   An important but challenging problem for MDC video multicast is how to assign bandwidth to each description in order to maximize overall user satisfaction. In this paper, we investigate this issue by formulating it as an optimization problem, with the objective to maximize user bandwidth experience by taking into account the encoding inefficiency due to MDC.
   We prove that the optimization problem is NP-hard. However, if the description number is larger than or equal to a certain threshold (e. g., if the minimum and maximum bandwidth requirements are 100 kb/s and 10 Mb/s, respectively, such threshold is seven descriptions), there is an exact and simple solution to achieve maximum user satisfaction, i.e., meeting all the bandwidth requirements. For the case when the description number is smaller, we present an efficient heuristic called simulated annealing for MDC bandwidth assignment (SAMBA) to assign bandwidth to each description given the distribution of user bandwidth requirement.
   We evaluate our algorithm using simulations. SAMBA achieves virtually the same optimal performance based on exhaustive search. By comparing with other assignment algorithms, SAMBA significantly improves user satisfaction. We also show that, if the coding efficiency decreases with the number of descriptions, there is an optimal description number to achieve maximal user satisfaction.
C1 [Jin, Xing] Facebook Inc, Data Infrastruct Team, Palo Alto, CA 94304 USA.
   [Xia, Pengye; Chan, S. -H. Gary] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Kowloon, Hong Kong, Peoples R China.
C3 Facebook Inc; Hong Kong University of Science & Technology
EM xiapengye@cse.ust.hk; gchan@cse.ust.hk; xjin@fb.com
OI Chan, Gary Shueng Han/0000-0003-4207-764X
FU Research Grant Council of the Hong Kong Special Administrative Region,
   China [611209]; HKUST [PCF.005.09/10, PCFX05-15C00610/11ON]
FX This work was supported in part by the General Research Fund from the
   Research Grant Council of the Hong Kong Special Administrative Region,
   China (611209) and Proof-of-Concept Fund at the HKUST (PCF.005.09/10 and
   PCFX05-15C00610/11ON). The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Jiangchuan
   (JC) Liu.
CR Abanoz TB, 2009, IEEE IMAGE PROC, P3741, DOI 10.1109/ICIP.2009.5414496
   Akyol E, 2007, IEEE J-STSP, V1, P231, DOI 10.1109/JSTSP.2007.901527
   [Anonymous], P 6 IEEE CONS COMM N
   BADARNEH O, 2009, P CAN C EL COMP ENG, P604
   Castro M., 2003, Operating Systems Review, V37, P298, DOI 10.1145/1165389.945474
   CHEN YF, 2007, P ACM NOSSDAV JUN
   Crave O, 2010, IEEE T CIRC SYST VID, V20, P769, DOI 10.1109/TCSVT.2010.2045805
   Firooz MH, 2007, ICT-MICC: 2007 IEEE INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS AND MALAYSIA INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1 AND 2, PROCEEDINGS, P242, DOI 10.1109/ICTMICC.2007.4448642
   GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P74, DOI 10.1109/79.952806
   KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671
   Lee YC, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-5, P35
   Li B, 2003, IEEE NETWORK, V17, P24
   Li B, 2002, 5TH INTERNATIONAL SYMPOSIUM ON WIRELESS PERSONAL MULTIMEDIA COMMUNICATIONS, VOLS 1-3, PROCEEDINGS, P802, DOI 10.1109/WPMC.2002.1088287
   LIU J, 2002, P IEEE INFOCOM 02, V3, P1520
   Lu MT, 2007, IEEE T MULTIMEDIA, V9, P1568, DOI 10.1109/TMM.2007.907456
   Ma M, 2008, IEEE T MULTIMEDIA, V10, P1638, DOI 10.1109/TMM.2008.2007282
   Milani S, 2010, IEEE SIGNAL PROC LET, V17, P51, DOI 10.1109/LSP.2010.2051619
   Padmanabhan V.N., 2002, P 12 INT WORKSHOP NE, P177
   Tillo T, 2007, IEEE T IMAGE PROCESS, V16, P1269, DOI 10.1109/TIP.2007.891799
   Venkataramani R, 2003, IEEE T INFORM THEORY, V49, P2106, DOI 10.1109/TIT.2003.815767
   Wien M, 2007, IEEE T CIRC SYST VID, V17, P1194, DOI 10.1109/TCSVT.2007.905530
   WITSENHAUSEN HS, 1981, AT&T TECH J, V60, P2281, DOI 10.1002/j.1538-7305.1981.tb00226.x
   XY Y, 2009, P 5 INT C IM GRAPH I, P819
   YUN YT, 2007, IEEE COMMUN MAG, V45, P100
   ZHOU Y, 2005, P IEEE GLOB TEL C GL, V4
NR 26
TC 12
Z9 12
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2011
VL 13
IS 2
BP 366
EP 375
DI 10.1109/TMM.2010.2098021
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 738SG
UT WOS:000288661800017
DA 2024-07-18
ER

PT J
AU Tsai, TH
   Fang, CL
AF Tsai, Tsung-Han
   Fang, Chih-Lun
TI Text-Video Completion Using Structure Repair and Texture Propagation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Spatio-temporal consistency; structure-texture completion; text removal;
   video completion
ID ADAPTIVE SPARSE RECONSTRUCTIONS; IMAGE; REMOVAL; CAMERA; MOTION
AB Today, more superimposed text is embedded within videos. Usually some text is unnecessary. Thus, one requires an approach to remove the text and complete the video. However, few conventional approaches complete the video well due to the large-sized text, structure regions, and various types of videos. In response, this study designed a text-video completion algorithm that poses text-video completion as structure repair and texture propagation. To repair the structure regions, the structure interpolation uses the new model's rotated block matching to estimate the initial location of completed regions and later refine the coordinates of completed regions. The information in the neighboring frames then fills the structure regions. To complete the structure regions without tedious manual interaction, the structure extension utilizes the spline curve estimation. Afterwards, derivative propagation realizes the texture region completion. The experiment results are based on several real TV programs, where all of the text regions were completed with spatio-temporal consistency. Additionally, comparisons present that the performance of the proposed algorithm is superior to those of conventional approaches. Its advantages include the reduction of design complexity by only integrating the structure information in multi-frame and the demonstration of structure consistency for realistic videos.
C1 [Tsai, Tsung-Han; Fang, Chih-Lun] Natl Cent Univ, Dept Elect Engn, Tao Yuan 320, Taiwan.
C3 National Central University
RP Tsai, TH (corresponding author), Natl Cent Univ, Dept Elect Engn, Tao Yuan 320, Taiwan.
EM han@ee.ncu.edu.tw; allen@dsp.ee.ncu.edu.tw
FU CIC; National Science Council, Taiwan, R.O.C. [NSC99-2220-E-008-003]
FX Manuscript received March 01, 2010; revised October 29, 2010; accepted
   October 31, 2010. Date of publication November 11, 2010; date of current
   version January 19, 2011. This work was supported by the CIC and the
   National Science Council, Taiwan, R.O.C., under Grant
   NSC99-2220-E-008-003. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Nadia
   Magnenat-Thalmann.
CR [Anonymous], P IEEE COMP SOC C CO
   Ballester C, 2001, IEEE T IMAGE PROCESS, V10, P1200, DOI 10.1109/83.935036
   Bertalmío M, 2001, PROC CVPR IEEE, P355
   Bertalmio M, 2003, IEEE T IMAGE PROCESS, V12, P882, DOI 10.1109/TIP.2003.815261
   BERTALMIO M, 2000, P ACM C COMP GRAPH S
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen Y, 2008, IEEE T MULTIMEDIA, V10, P2, DOI 10.1109/TMM.2007.911223
   Chen YL, 2005, IEICE T FUND ELECTR, VE88A, P2826, DOI 10.1093/ietfec/e88-a.10.2826
   CHUN BT, 1999, P C IEEE SYST MAN CY
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   FANG CL, 2008, P IEEE INT C MULT EX
   Guleryuz OG, 2006, IEEE T IMAGE PROCESS, V15, P539, DOI 10.1109/TIP.2005.863057
   Guleryuz OG, 2006, IEEE T IMAGE PROCESS, V15, P555, DOI 10.1109/TIP.2005.863055
   Jia JY, 2006, IEEE T PATTERN ANAL, V28, P832, DOI 10.1109/TPAMI.2006.108
   KIM JB, 2003, P TENCON C
   KOMODAKIS N, 2006, P CVPR, P442
   Lee CW, 2003, PATTERN RECOGN LETT, V24, P2607, DOI 10.1016/S0167-8655(03)00105-3
   Oliveira M., 2001, P INT C VIS IM IM PR, P261
   Patwardhan KA, 2003, IEEE IMAGE PROC, P857
   Patwardhan KA, 2007, IEEE T IMAGE PROCESS, V16, P545, DOI 10.1109/TIP.2006.888343
   Shen YP, 2006, INT C PATT RECOG, P63
   Shih TK, 2005, THIRD INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY AND APPLICATIONS, VOL 1, PROCEEDINGS, P15
   Sun J, 2005, ACM T GRAPHIC, V24, P861, DOI 10.1145/1073204.1073274
   TSAI TH, 2006, P INT MULT ENG COMP
   Tsai TH, 2009, PATTERN RECOGN, V42, P1496, DOI 10.1016/j.patcog.2008.10.009
   Venkatesh MV, 2009, PATTERN RECOGN LETT, V30, P168, DOI 10.1016/j.patrec.2008.03.011
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Yamauchi H, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P120, DOI 10.1109/CGI.2003.1214456
NR 28
TC 6
Z9 6
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2011
VL 13
IS 1
BP 29
EP 39
DI 10.1109/TMM.2010.2091497
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 708TJ
UT WOS:000286386900004
DA 2024-07-18
ER

PT J
AU Mu, YD
   Zhou, BF
   Yan, SC
AF Mu, Yadong
   Zhou, Bingfeng
   Yan, Shuicheng
TI Information-Theoretic Analysis of Input Strokes in Visual Object Cutout
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image analysis; information entropy; object segmentation
ID IMAGE
AB Semantic object cutout serves as a basic unit in various image editing systems. In a typical scenario, users are required to provide several strokes which indicate part of the pixels as image background or objects. However, most existing approaches are passive in the sense of accepting input strokes without checking the consistence with user's intention. Here we argue that an active strategy may potentially reduce the interaction burden. Before any real calculation for segmentation, the program can roughly estimate the uncertainty for each image element and actively provide useful suggestions to users. Such a pre-processing is particularly useful for beginners unaware of feeding the underlying cutout algorithms with optimal strokes.
   We develop such an active object cutout algorithm, named ActiveCut, which makes it possible to automatically detect ambiguity given current user-supplied strokes, and synthesize "suggestive strokes" as feedbacks. Generally, suggestive strokes come from the ambiguous image parts and have the maximal potentials to reduce label uncertainty. Users can continuously refine their inputs following these suggestive strokes. In this way, the number of user-program interaction iterations can thus be greatly reduced. Specifically, the uncertainty is modeled by mutual information between user strokes and unlabeled image regions.
   To ensure that ActiveCut works at a user-interactive rate, we adopt superpixel lattice based image representation, whose computation depends on scene complexity rather than original image resolution. Moreover, it retains the 2-D-lattice topology and is thus more suitable for parallel computing. While for the most time-consuming calculation of probabilistic entropy, variational approximation is utilized for acceleration. Finally, based on submodular function theory, we provide a theoretic analysis for the performance lower bound of the proposed greedy algorithm. Various user studies are conducted on the MSRC image dataset to validate the effectiveness of our proposed algorithm.
C1 [Mu, Yadong; Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore.
   [Zhou, Bingfeng] Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.
C3 National University of Singapore; Peking University
RP Mu, YD (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore.
EM muyadong@gmail.com; cczbf@pku.edu.cn; eleyans@nus.edu.sg
RI Yan, Shuicheng/HCI-1431-2022
FU NSF from China [60973054]; NUS at Singapore [R-263-000-528-646]
FX Manuscript received August 31, 2009; revised April 01, 2010; accepted
   May 20, 2010. Date of publication August 09, 2010; date of current
   version November 17, 2010. This work was supported in part by NSF from
   China under Grant number 60973054 and in part by NUS Cross-faculty Grant
   of R-263-000-528-646 at Singapore. Part of the work in this paper was
   done when Y. Mu was a Ph.D. student at Peking University. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Nicu Sebe.
CR [Anonymous], 2008, P CVPR
   [Anonymous], 1995, Markov random field modeling in computer vision
   BAGON S, 2008, P ECCV
   BESAG J, 1974, J ROY STAT SOC B MET, V36, P192
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   CHEN S, 2007, P CVPR
   CUI J, 2008, P CVPR
   Duchenne O., 2008, P CVPR
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Khuller S, 1999, INFORM PROCESS LETT, V70, P39, DOI 10.1016/S0020-0190(99)00031-9
   KOHLI P, 2006, P ECCV
   KOLMOGOROV V, 2005, P CVPR
   Krause A., 2007, AAAI, V7, P1650
   Lempitsky V, 2009, IEEE I CONF COMP VIS, P277, DOI 10.1109/ICCV.2009.5459262
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Li Y, 2005, ACM T GRAPHIC, V24, P595, DOI 10.1145/1073204.1073234
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   MEYER F, 1994, SIGNAL PROCESS, V38, P113, DOI 10.1016/0165-1684(94)90060-4
   Minka T., 2001, P UAI
   MU Y, 2007, P ACCV
   REN X, 2003, P ICCV
   Ren X., 2005, P ICCV
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Rother Carsten., 2006, P CVPR
   SHARMA V, 2007, P CVPR
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   SUN J, 2007, P CVPR
   Sun J, 2006, ACM T GRAPHIC, V25, P772, DOI 10.1145/1141911.1141954
   Tian Y., 2007, P CVPR
   TOSHEV A, 2007, P CVPR
   Veksler O., 2008, P ECCV
   Vese LA, 2002, INT J COMPUT VISION, V50, P271, DOI 10.1023/A:1020874308076
   WANG F, 2007, P ICMLA
   Wang J, 2005, ACM T GRAPHIC, V24, P585, DOI 10.1145/1073204.1073233
   WANG J, 2006, P COMP GRAPH INT
   Wang JD, 2009, IEEE T PATTERN ANAL, V31, P1600, DOI 10.1109/TPAMI.2008.216
   Wang J, 2007, FOUND TRENDS COMPUT, V3, P97, DOI 10.1561/0600000019
   Xiaojin Z, 2002, CMUCALD02107
NR 40
TC 1
Z9 3
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2010
VL 12
IS 8
BP 843
EP 852
DI 10.1109/TMM.2010.2064759
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 681ZW
UT WOS:000284365100006
DA 2024-07-18
ER

PT J
AU Yuan, JS
   Luo, JB
   Wu, Y
AF Yuan, Junsong
   Luo, Jiebo
   Wu, Ying
TI Mining Compositional Features From GPS and Visual Cues for Event
   Recognition in Photo Collections
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compositional feature mining; event recognition; geo-tagged images;
   image data mining
ID CONTEXT
AB As digital cameras with Global Positioning System (GPS) capability become available and people geotag their photos using other means, it is of great interest to annotate semantic events (e. g., hiking, skiing, party) characterized by a collection of geotagged photos with timestamps and GPS information at the capture. We address this emerging event classification problem by mining informative features derived from image contents and spatio-temporal traces of GPS coordinates that characterize the underlying movement patterns of various event types, both based on the entire collection as opposed to individual photos. Considering that events are better described by the co-occurrence of objects and scenes, we bundle primitive features such as color and texture histograms or GPS features to form the discriminative compositional feature. A data mining method is proposed to efficiently discover discriminative compositional features of small classification errors. A theoretical analysis is also presented to guide the selection of the data mining parameters. Upon compositional feature mining, we perform the multiclass AdaBoost to further integrate the mined compositional features. Finally, the GPS and visual modalities are united through a confidence-based fusion. Based on a dataset of more than 3000 geotagged images, experimental results have shown the synergy of all of the components in our proposed approach to event classification.
C1 [Yuan, Junsong] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Luo, Jiebo] Kodak Res Labs, Rochester, NY 14650 USA.
   [Wu, Ying] Northwestern Univ, Dept Elect Engn & Comp Sci, Evanston, IL 60208 USA.
C3 Nanyang Technological University; Eastman Kodak; Northwestern University
RP Yuan, JS (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM jsyuan@ntu.edu.sg; jiebo.luo@kodak.com; yingwu@ece.northwestern.edu
RI wu, yiping/JEF-4104-2023; Yuan, Junsong/A-5171-2011; Luo,
   Jiebo/AAI-7549-2020; Yuan, Junsong/R-4352-2019; Wu, Ying/B-7283-2009
OI Yuan, Junsong/0000-0002-7901-8793; Luo, Jiebo/0000-0002-4516-9729;
   Koochak, Atousa/0000-0001-6547-2728
FU Nanyang Assistant Professorship [M58040015]; National Science Foundation
   [IIS-0347877, IIS-0916607]
FX Manuscript received August 06, 2009; revised March 25, 2010; accepted
   April 08, 2010. Date of publication June 01, 2010; date of current
   version October 15, 2010. This work was supported in part by the Nanyang
   Assistant Professorship (SUG M58040015) to Junsong Yuan and in part by
   National Science Foundation Grants IIS-0347877 and IIS-0916607. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Andrea Cavallaro.
CR Amores J, 2007, IEEE T PATTERN ANAL, V29, P1818, DOI 10.1109/TPAMI.2007.1098
   [Anonymous], 2006, PROC IEEE C COMPUTER
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2007, P IEEE INT C COMP VI
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P ACM MULT
   Cao LL, 2009, IEEE T MULTIMEDIA, V11, P208, DOI 10.1109/TMM.2008.2009693
   CHANG SF, 2007, P ACM INT WORKSH INF
   CHENG H, 2008, P INT C DAT ENG
   CHENG H, 2007, P INT C DAT ENG
   DONG G, 1999, P SIGKDD
   EBADOLLAHI S, 2006, P IEEE C MULT EXP
   FAN W, 2008, P ACM SIGKDD
   Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223
   Grahne G, 2005, IEEE T KNOWL DATA EN, V17, P1347, DOI 10.1109/TKDE.2005.166
   Han JW, 2007, DATA MIN KNOWL DISC, V15, P55, DOI 10.1007/s10618-006-0059-1
   Kennedy L., 2007, P ACM MULT
   Li J., 2007, P ACM SIGKDD
   LI W, 2001, P INT C DAT MIN
   Lim JH, 2003, IEEE MULTIMEDIA, V10, P28, DOI 10.1109/MMUL.2003.1237548
   LIU B, 1998, P SIGKDD
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo J., 2008, ACM International Conference on Multimedia, P1071, DOI DOI 10.1145/1459359.1459574MULTIMEDIA-MM'PLACE
   Luo JB, 2006, IEEE SIGNAL PROC MAG, V23, P101
   Luo JB, 2005, IEEE T PATTERN ANAL, V27, P715, DOI 10.1109/TPAMI.2005.96
   Mitchell T. M., 1997, MACHINE LEARNING
   MOXLEY E, 2008, P IEEE C MULT EXP
   Peelen M. V., 2009, NATURE
   REYZIN L, 2006, P INT C MACH LEARN
   SIVIC J, 2004, P IEEE C COMP VIS PA
   Torralba A, 2007, IEEE T PATTERN ANAL, V29, P854, DOI 10.1109/TPAMI.2007.1055
   Xu D, 2008, IEEE T PATTERN ANAL, V30, P1985, DOI 10.1109/TPAMI.2008.129
   Yan R.J., 2007, P ACM SIGKDD
   Yang Jun, 2007, P ACM MULT
   Yin Xiaoxin, 2003, P SIAM INT C DAT MIN
   YUAN J, 2008, P ACM INT C MULT INF
   Yuan JS, 2008, PROC CVPR IEEE, P47
   Yuan J, 2007, PROC CVPR IEEE, P1930, DOI 10.1109/CVPR.2007.383222
   Yuan JS, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P864, DOI 10.1145/1281192.1281284
   Zhu Ji., 2005, MULTICLASS ADABOOST
NR 40
TC 21
Z9 22
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2010
VL 12
IS 7
BP 705
EP 716
DI 10.1109/TMM.2010.2051868
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 670SV
UT WOS:000283448500008
DA 2024-07-18
ER

PT J
AU Irie, G
   Satou, T
   Kojima, A
   Yamasaki, T
   Aizawa, K
AF Irie, Go
   Satou, Takashi
   Kojima, Akira
   Yamasaki, Toshihiko
   Aizawa, Kiyoharu
TI Affective Audio-Visual Words and Latent Topic Driving Model for
   Realizing Movie Affective Scene Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Affective audio-visual word; affective scene classification; latent
   topic driving model; Plutchik's basic emotions
ID PLEASURE-AROUSAL-DOMINANCE; EMOTIONS; VALENCE
AB This paper presents a novel method for movie affective scene classification that outputs the emotion (in the form of labels) that the scene is likely to arouse in viewers. Since the affective preferences of users play an important role in movie selection, affective scene classification has the potential to develop more attractive user-centric movie search and browsing applications. Two main issues in designing movie affective scene classification are considered. One is "how to extract features that are strongly related to the viewer's emotions", and the other is "how to map the extracted features to the emotion categories". For the former, we propose a method to extract emotion-category-specific audio-visual features named affective audio-visual words (AAVWs). For the latter issue, we propose a classification model named latent topic driving model (LTDM). Assuming that viewers' emotions are dynamically changed by the movie scene sequences, LTDM models emotions as Markovian dynamic systems driven by the sequential stimuli of the movie content. Experiments on 206 movie scenes extracted from 24 movie titles and the corresponding labels of eight emotion categories given by 16 subjects show that our method outperforms conventional approaches in terms of the subject agreement rate.
C1 [Irie, Go; Satou, Takashi; Kojima, Akira] NTT Corp, NTT Cyber Solut Labs, Kanagawa 2390847, Japan.
   [Yamasaki, Toshihiko] Univ Tokyo, Dept Informat & Commun Engn, Grad Sch Informat Sci & Technol, Tokyo 1138656, Japan.
   [Aizawa, Kiyoharu] Univ Tokyo, Dept Informat & Commun Engn, Interfac Initiat Informat Studies, Tokyo 1138656, Japan.
C3 Nippon Telegraph & Telephone Corporation; University of Tokyo;
   University of Tokyo
RP Irie, G (corresponding author), NTT Corp, NTT Cyber Solut Labs, Kanagawa 2390847, Japan.
EM irie.go@lab.ntt.co.jp; satou.takashi@lab.ntt.co.jp;
   kojima.akira@lab.ntt.co.jp; yamasaki@hal.t.u-tokyo.ac.jp;
   aizawa@hal.t.u-tokyo.ac.jp
CR Aizawa A, 2003, INFORM PROCESS MANAG, V39, P45, DOI 10.1016/S0306-4573(02)00021-3
   [Anonymous], 2009, P 17 ACM INT C MULT
   Arifin S., 2007, Proceedings of the 15th International Conference on Multimedia, P68
   Arifin S, 2008, IEEE T MULTIMEDIA, V10, P1325, DOI 10.1109/TMM.2008.2004911
   Barrett LF, 1998, COGNITION EMOTION, V12, P579, DOI 10.1080/026999398379574
   Blei DM, 2004, ADV NEUR IN, V16, P17
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Coan J. A., 2007, HDB EMOTION ELICITAT
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   diaeresis>tze Hinrich Schu<spacing, 2008, INTRO INFORM RETRIEV, V39
   Ekman P., 1971, Nebraska symposium on motivation, V19, P207
   Fine S, 1998, MACH LEARN, V32, P41, DOI 10.1023/A:1007469218079
   Fletcher PC, 2001, NAT NEUROSCI, V4, P1043, DOI 10.1038/nn733
   GILDEA DANIEL., 1999, Proceedings of EUROSPEECH 1999, Budapest: Technical University of Budapest, P2167
   GROSS JJ, 1995, COGNITION EMOTION, V9, P87, DOI 10.1080/02699939508408966
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649
   Hofmann T, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P682
   Irie G, 2009, IEEE INT CON MULTI, P522, DOI 10.1109/ICME.2009.5202548
   Kang H.-B., 2003, Proceedings of the 11th ACM International Conference on Multimedia, P259
   Kneser R., 1997, Proceedings of EUROSPEECH, P1971
   LANG A, 1995, J BROADCAST ELECTRON, V39, P313, DOI 10.1080/08838159509364309
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Lu S., 2003, P 9 INT C DISTRIBUTE, P456
   McCallum A., 2000, P 17 INT C MACH LEAR, V17, P591
   Mehrabian A, 1996, CURR PSYCHOL, V14, P261, DOI 10.1007/BF02686918
   Nepal S., 2001, ACM Multimedia, P261
   Oliver N, 2004, COMPUT VIS IMAGE UND, V96, P163, DOI 10.1016/j.cviu.2004.02.004
   Plutchik R, 2001, AM SCI, V89, P344, DOI 10.1511/2001.4.344
   Rui Y., 2000, Proceedings ACM Multimedia 2000, P105, DOI 10.1145/354384.354443
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Schultz W, 2000, ANNU REV NEUROSCI, V23, P473, DOI 10.1146/annurev.neuro.23.1.473
   Souvannavong F., 2004, MIR'04: Proceedings of the 6th ACM SIGMM International Workshop on Multimedia Information Retrieval, P243
   Sun K, 2007, LECT NOTES COMPUT SC, V4738, P594
   Talkin D., 1995, Speech coding and synthesis, V495, P518
   Tonomura Y., 1994, IEEE Multimedia, V1, P34, DOI 10.1109/MMUL.1994.318984
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Tsoumakas G., 2007, INT J DATA WAREHOUS, V3, P1, DOI DOI 10.4018/JDWM.2007070101
   Xiong ZY, 2003, IEEE IMAGE PROC, P5
   Xu C., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P361, DOI 10.1145/1076034.1076097
   Xu M, 2005, 2005 IEEE International Conference on Multimedia and Expo (ICME), Vols 1 and 2, P622
   Xu M., 2008, Proceeding of the 16th ACM International Conference on Multimedia, P677, DOI DOI 10.1145/1459359.1459457
   Zhao Z, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1613, DOI 10.1109/ICME.2006.262855
NR 43
TC 53
Z9 56
U1 1
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2010
VL 12
IS 6
BP 523
EP 535
DI 10.1109/TMM.2010.2051871
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 668TB
UT WOS:000283291900006
DA 2024-07-18
ER

PT J
AU Bobarshad, H
   van der Schaar, M
   Shikh-Bahaei, MR
AF Bobarshad, Hossein
   van der Schaar, Mihaela
   Shikh-Bahaei, Mohammad R.
TI A Low-Complexity Analytical Modeling for Cross-Layer Adaptive Error
   Protection in Video Over WLAN
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-layer; IEEE-WLAN; queuing system; retransmission; retry-limit
ID MULTIMEDIA TRANSMISSION; WIRELESS; NETWORKS; DIFFERENTIATION
AB We find a low-complicity and accurate model to solve the problem of optimizing MAC-layer transmission of real-time video over wireless local area networks (WLANs) using cross-layer techniques. The objective in this problem is to obtain the optimal MAC retry limit in order to minimize the total packet loss rate. First, the accuracy of Fluid and M/M/1/K analytical models is examined. Then we derive a closed-form expression for service time in WLAN MAC transmission, and will use this in mathematical formulation of our optimization problem based on M/G/1 model. Subsequently we introduce an approximate and simple formula for MAC-layer service time, which leads to the M/M/1 model. Compared with M/G/1, we particularly show that our M/M/1-based model provides a low-complexity and yet quite accurate means for analyzing MAC transmission process in WLAN. Using our M/M/1 model-based analysis, we derive closed-form formulas for the packet overflow drop rate and optimum retry-limit. These closed-form expressions can be effectively invoked for analyzing adaptive retry-limit algorithms. Simulation results (network simulator-2) will verify the accuracy of our analytical models.
C1 [Bobarshad, Hossein; Shikh-Bahaei, Mohammad R.] Kings Coll London, Ctr Telecommun Res, London WC2R 2LS, England.
   [van der Schaar, Mihaela] Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
C3 University of London; King's College London; University of California
   System; University of California Los Angeles
RP Bobarshad, H (corresponding author), Kings Coll London, Ctr Telecommun Res, London WC2R 2LS, England.
EM hossein.bobarshad@kcl.ac.uk; mihaela@ee.ucla.edu; m.sbahaei@kcl.ac.uk
OI Shikh-Bahaei, Mohammad/0000-0001-7450-7574
CR [Anonymous], EvalVid A Video Quality Evaluation Tool-set
   [Anonymous], 1999, 80211 IEEE WG 11
   [Anonymous], 2009, 2009 IEEE WIRELESS C
   Bianchi G, 2000, IEEE J SEL AREA COMM, V18, P535, DOI 10.1109/49.840210
   Bobarshad H., 2010, THESIS KINGS COLL LO
   Chatzimisios P, 2003, GLOB TELECOMM CONF, P950
   Galluccio L, 2005, IEEE T WIREL COMMUN, V4, P2777, DOI 10.1109/TWC.2005.858028
   Gross D., 1998, FUNDAMENTALS QUEUEIN
   Han B., 2007, Testbeds and Research Infrastructure for the Development of Networks and 126 Communities, P1
   Kaur HT, 2003, PROCEEDINGS OF THE 11TH IEEE/ACM INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS AND SIMULATION OF COMPUTER TELECOMMUNICATIONS SYSTEMS, P79
   KHALILI R, 2004, P IEEE INT S MOD AN
   Kleinrock L., 1975, Queueing Systems-Volume 1: Theory, V1
   KOUVATSOS DD, 1989, PERFORM EVALUATION, V10, P169, DOI 10.1016/0166-5316(89)90009-6
   Kuo WK, 2008, IET COMMUN, V2, P92, DOI 10.1049/iet-com:20060575
   Lai YC, 2007, IET COMMUN, V1, P880, DOI 10.1049/iet-com:20060245
   Li Q, 2004, IEEE T MULTIMEDIA, V6, P278, DOI 10.1109/TMM.2003.822792
   Milani S, 2009, IEEE T MULTIMEDIA, V11, P810, DOI 10.1109/TMM.2009.2021791
   N SS, 2007, IEEE T VEH TECHNOL, V56, P2346, DOI 10.1109/TVT.2007.897646
   NAGARAJAN R, 1991, IEEE J SEL AREA COMM, V9, P368, DOI 10.1109/49.76635
   Oh BJ, 2009, IEEE T MULTIMEDIA, V11, P1052, DOI 10.1109/TMM.2009.2026083
   Pathak Bhumin, 2010, International Journal of Computer and Network Security, V2, P70
   *ROHD SCHW, 80211 PACK ERR RAT T
   van der Schaar M, 2006, IEEE T MOBILE COMPUT, V5, P755, DOI 10.1109/TMC.2006.81
   van der Schaar M, 2005, IEEE WIREL COMMUN, V12, P50, DOI 10.1109/MWC.2005.1497858
   van der Schaar M, 2007, IEEE T MULTIMEDIA, V9, P185, DOI 10.1109/TMM.2006.886384
   van Dijk TA, 2006, DISCOURSE STUD, V8, P5, DOI 10.1177/1461445606059544
   Zhao L, 2008, IET COMMUN, V2, P329, DOI 10.1049/iet-com:20070048
   EVALUATE MPEG VIDEO
   WORK PREPARE MULTIME
NR 29
TC 17
Z9 17
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2010
VL 12
IS 5
BP 427
EP 438
DI 10.1109/TMM.2010.2050734
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 656DN
UT WOS:000282306500007
DA 2024-07-18
ER

PT J
AU Cossalter, M
   Valenzise, G
   Tagliasacchi, M
   Tubaro, S
AF Cossalter, Michele
   Valenzise, Giuseppe
   Tagliasacchi, Marco
   Tubaro, Stefano
TI Joint Compressive Video Coding and Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compressive sensing; video coding and analysis
AB Traditionally, video acquisition, coding and analysis have been designed and optimized as independent tasks. This has a negative impact in terms of consumed resources, as most of the raw information captured by conventional acquisition devices is discarded in the coding phase, while the analysis step only requires a few descriptors of salient video characteristics. Recent compressive sensing literature has partially broken this paradigm by proposing to integrate sensing and coding in a unified architecture composed by a light encoder and a more complex decoder, which exploits sparsity of the underlying signal for efficient recovery. However, a clear understanding of how to embed video analysis in this scheme is still missing. In this paper, we propose a joint compressive video coding and analysis scheme and, as a specific application example, we consider the problem of object tracking in video sequences. We show that, weaving together compressive sensing and the information computed by the analysis module, the bit-rate required to perform reconstruction and tracking of the foreground objects can be considerably reduced, with respect to a conventional disjoint approach that postpones the analysis after the video signal is recovered in the pixel domain. These findings suggest that considerable gains in performance can be potentially obtained in video analysis applications, provided that a joint analysis-aware design of acquisition, coding and signal recovery is carried out.
C1 [Cossalter, Michele; Valenzise, Giuseppe; Tagliasacchi, Marco; Tubaro, Stefano] Politecn Milan, Dipartimento Elettron & Informat, I-20133 Milan, Italy.
C3 Polytechnic University of Milan
RP Cossalter, M (corresponding author), Politecn Milan, Dipartimento Elettron & Informat, I-20133 Milan, Italy.
EM cossalter@elet.polimi.it; valen-zise@elet.polimi.it;
   marco.tagliasacchi@polimi.it; tubaro@elet.polimi.it
CR [Anonymous], P PICT COD S PCS BEI
   [Anonymous], P BMVA S 3D VID AN D
   Baraniuk R, 2008, CONSTR APPROX, V28, P253, DOI 10.1007/s00365-007-9003-x
   Baraniuk RG, 2007, IEEE SIGNAL PROC MAG, V24, P6, DOI 10.1109/MSP.2007.909718
   BONARINI A, 2005, P 2 AIXIA WORKSH AMB
   Boyd S., 2004, CONVEX OPTIMIZATION
   Candes E. J., 2006, P INT C MATH MADR SP
   Candès EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124
   Candès EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x
   Cevher V., 2008, P EUR C COMP VIS MAR
   COSSALTER M, 2009, P INT C AUT VID SIGN
   Davenport M., 2007, Proceedings of SPIE Symposium on Electronic Imaging: Computational Imaging, P6498
   DAVENPORT M, 2006, TREE0610 RIC U EL CO
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   DUARTE MF, P IEEE INT C AC SPEE, P5137
   FERGUS R, 2006, MIT COMPUTER SCI ART, V58, P1
   Fowler JE, 2009, IEEE T IMAGE PROCESS, V18, P2230, DOI 10.1109/TIP.2009.2025089
   Gersho A., 2003, Vector Quantization and Signal Compression
   HAUPT J, 2007, P IEEE INT C AC SPEE, V3
   JACQUES L, 2008, CMOS COMPRESSED IMAG
   MARCIA RF, P IEEE INT C AC SPEE, P833
   Marpe D, 2003, IEEE T CIRC SYST VID, V13, P620, DOI 10.1109/TCSVT.2003.815173
   Papadimitriou C. H., 1998, Proceedings of the Seventeenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems. PODS 1998, P159, DOI 10.1145/275487.275505
   Park J. Y., 2009, P PICT COD S CHIC IL
   Piccardi M, 2004, IEEE SYS MAN CYBERN, P3099, DOI 10.1109/ICSMC.2004.1400815
   Ristic B., 2003, Beyond the Kalman Filter: Particle Filters for Tracking Applications
   Rivenson Y, 2009, IEEE SIGNAL PROC LET, V16, P449, DOI 10.1109/LSP.2009.2017817
   Rowe D, 2005, LECT NOTES COMPUT SC, V3617, P1158, DOI 10.1007/11553595_142
   STANKOVIC V, 2008, P EUS 2008 16 EUR SI
   Tagliasacchi M, 2009, IEEE T IMAGE PROCESS, V18, P2491, DOI 10.1109/TIP.2009.2028251
   van den Berg E., 2007, Tech. Rep. TR-2007-19
   Wakin MB, 2006, IEEE IMAGE PROC, P1273, DOI 10.1109/ICIP.2006.312577
   Woods R. E., 2007, DIGITAL IMAGE PROCES, V3
NR 33
TC 30
Z9 45
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2010
VL 12
IS 3
BP 168
EP 183
DI 10.1109/TMM.2010.2041105
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 570IN
UT WOS:000275666900002
DA 2024-07-18
ER

PT J
AU Moxley, E
   Mei, T
   Manjunath, BS
AF Moxley, Emily
   Mei, Tao
   Manjunath, B. S.
TI Video Annotation Through Search and Graph Reinforcement Mining
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data mining; graph theory; video annotation; video content analysis
AB Unlimited vocabulary annotation of multimedia documents remains elusive despite progress solving the problem in the case of a small, fixed lexicon. Taking advantage of the repetitive nature of modern information and online media databases with independent annotation instances, we present an approach to automatically annotate multimedia documents that uses mining techniques to discover new annotations from similar documents and to filter existing incorrect annotations. The annotation set is not limited to words that have training data or for which models have been created. It is limited only by the words in the collective annotation vocabulary of all the database documents. A graph reinforcement method driven by a particular modality (e.g., visual) is used to determine the contribution of a similar document to the annotation target. The graph supplies possible annotations of a different modality (e.g., text) that can be mined for annotations of the target. Experiments are performed using videos crawled from YouTube. A customized precision-recall metric shows that the annotations obtained using the proposed method are superior to those originally existing for the document. These extended, filtered tags are also superior to a state-of-the-art semi-supervised technique for graph reinforcement learning on the initial user-supplied annotations.
C1 [Moxley, Emily; Manjunath, B. S.] Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.
C3 University of California System; University of California Santa Barbara
RP Moxley, E (corresponding author), Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.
EM emoxley@ece.ucsb.edu; tmei@microsoft.com; manj@ece.ucsb.edu
RI Mei, Tao/GQZ-0596-2022; Manjunath, B S/AAM-8190-2020
OI Mei, Tao/0000-0002-5990-7307; Manjunath, B S/0000-0003-2804-3611
CR [Anonymous], 2005, STRUCTURE COLLABORAT
   [Anonymous], P CVPR
   [Anonymous], 2004, ADV NEURAL INFORM PR
   [Anonymous], 2005, SEMISUPERVISED LEARN
   [Anonymous], Google image labeler
   Baeza-Yates Ricardo, 1999, MODERN INFORM RETRIE, V463
   Cha M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P1
   Chapelle O., 2006, SEMISUPERVISED LEARN
   Cho JH, 2007, IEEE INTERNET COMPUT, V11, P13, DOI 10.1109/MIC.2007.130
   Furnas G.W., 2006, CHI 06 EXTENDED ABST, P36, DOI DOI 10.1145/1125451.1125462
   Geisler G, 2007, ACM-IEEE J CONF DIG, P480, DOI 10.1145/1255175.1255279
   Jing Y., 2008, P INT WORLD WID WEB
   Kennedy Lyndon., 2007, P 15 INT C MULTIMEDI, P631
   LAVRENKO V, 2004, P ICASSP
   Leon-Garcia A., 1993, PROBABILITY RANDOM P
   Marlow C., 2006, P 17 C HYP HYPERTEXT, P31, DOI [https://doi.org/10.1145/1149941.1149949, DOI 10.1145/1149941.1149949]
   Mei T., 2007, P TRECVID 2007 WORKS
   Moxley E., 2008, P ACM MULT INF RETR
   MOXLEY E, 2008, P ICME JUN
   Natsev A.P., 2004, PROC 10 ACM SIGKDD I, P641
   QI GJ, 2007, P CVPR
   RUI X, 2007, P ACM MULT
   Shirky C., ONTOLOGY IS OVERRATE
   SONG Y, 2006, P ICASSP
   TONG H, 2005, P ACM MULT
   *TRECVID, TRECVID RETR EV
   Tseng VS, 2008, IEEE T MULTIMEDIA, V10, P260, DOI 10.1109/TMM.2007.911832
   VELIVELLI A, 2006, P CVPR WORKSH
   VILLACORTA A, 2007, P ACM SIGGRAPH 2007, P254
   Wang M., 2007, ACM Multi- media, P862
   Wu X., 2007, ACM MULTIMEDIA 07, P168
   Xing E.P., 2005, P C UNCERTAINTY ARTI, P633
   YAN R, 2007, P SIGKDD 2007
   Zhu X., 2005, Time-sensitive Dirichlet process mixture models
NR 34
TC 36
Z9 37
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2010
VL 12
IS 3
BP 184
EP 193
DI 10.1109/TMM.2010.2041101
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 570IN
UT WOS:000275666900003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ciullo, D
   Garcia, MA
   Horvath, A
   Leonardi, E
   Mellia, M
   Rossi, D
   Miklos, T
   Veglia, P
AF Ciullo, Delia
   Garcia, Maria Antonieta
   Horvath, Akos
   Leonardi, Emilio
   Mellia, Marco
   Rossi, Dario
   Miklos Telek
   Veglia, Paolo
TI Network Awareness of P2P Live Streaming Applications: A Measurement
   Study
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Locality awareness; multimedia streaming; neighbor selection; overlay;
   peer-to-peer networks
AB Early P2P-TV systems have already attracted millions of users, and many new commercial solutions are entering this market. Little information is however available about how these systems work, due to their closed and proprietary design. In this paper, we present large scale experiments to compare three of the most successful P2P-TV systems, namely PPLive, SopCast and TVAnts.
   Our goal is to assess what level of "network awareness" has been embedded in the applications. We first define a general framework to quantify which network layer parameters leverage application choices, i.e., what parameters mainly drive the peer selection and data exchange. We then apply the methodology to a large dataset, collected during a number of experiments where we deployed about 40 peers in several European countries.
   From analysis of the dataset, we observe that TVAnts and PPLive exhibit a mild preference to exchange data among peers in the same autonomous system the peer belongs to, while this clustering effect is less intense in SopCast. However, no preference versus country, subnet or hop count is shown. Therefore, we believe that next-generation P2P live streaming applications definitively need to improve the level of network-awareness, so to better localize the traffic in the network and thus increase their network-friendliness as well.
C1 [Ciullo, Delia; Garcia, Maria Antonieta; Leonardi, Emilio; Mellia, Marco] Politecn Torino, I-10129 Turin, Italy.
   [Horvath, Akos; Miklos Telek] Budapest Univ Technol & Econ, H-1521 Budapest, Hungary.
   [Rossi, Dario; Veglia, Paolo] Ecole Natl Super Telecommun Bretagne, F-75014 Paris, France.
C3 Polytechnic University of Turin; Budapest University of Technology &
   Economics; IMT - Institut Mines-Telecom; IMT Atlantique
RP Ciullo, D (corresponding author), Politecn Torino, I-10129 Turin, Italy.
EM ciullo@tlc.polito.it; antonieta@tlc.polito.it; horvakos@hit.bme.hu;
   leonardi@tlc.polito.it; mellia@tlc.polito.it; dario.rossi@enst.fr;
   telek@hit.bme.hu; paolo.veglia@enst.fr
RI Rossi, Dario/HSG-3271-2023; Mellia, Marco/G-7997-2018; Leonardi,
   Emilio/A-1700-2011
OI Rossi, Dario/0000-0003-3936-8876; Horvath, Akos/0000-0003-3545-3694;
   Mellia, Marco/0000-0003-1859-6693; Leonardi, Emilio/0000-0002-3070-4274
FU European Commission
FX This work was supported by the European Commission under the FP7 STREP
   Project "Network-Aware P2P-TV Application over Wise Networks"
   (NAPA-WINE). The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Jiangchuan (JC)
   Liu.
CR AGARWAL S, 2008, P IEEE IWQOS 08 ENSC
   ALESSANDRIA E, 2009, P IEEE INF 09 RIO DE
   Ali S., 2006, P WORKSH REC ADV PEE
   BANERJEE S, 2002, P ACM SIGCOMM PITTSB
   CASTRO M, 2003, P ACM SOSP 03 LAK BO
   Hei XJ, 2007, IEEE J SEL AREA COMM, V25, P1640, DOI 10.1109/JSAC.2007.071204
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   HUANG G, 2007, P SIGCOMM 2007 PEER
   *IETF WORK GROUP, APPL LAYER TRAFF OPT
   LI B, 2008, P IEEE INFOCOM 08 PH
   MELLIA M, 2009, P IEEE IPDPS 09 HOTP
   PADMANABHAN VN, 2002, P ACM NOSSDAV 02 MIA
   SILVERSTON T, 2007, P ACM NOSSDAV 07 URB
   VU L, 2007, P 16 INT S HIGH PERF
   WANG F, 2008, P IEEE INF 08 PHOEN
   WU C, 2008, P IEEE INFOCOM 08 PH
   XIE H, 2008, P ACM SIGCOMM SEATTL
   YANGHUA C, 2001, P ACM SIGCOMM SAN DI
   ZHANG X, 2005, P IEEE INFOCOM 2005
NR 19
TC 47
Z9 57
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2010
VL 12
IS 1
BP 54
EP 63
DI 10.1109/TMM.2009.2036231
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 533SJ
UT WOS:000272844800005
OA Green Published
DA 2024-07-18
ER

PT J
AU Akhaee, MA
   Sahraeian, SME
   Sankur, B
   Marvasti, F
AF Akhaee, Mohammad Ali
   Sahraeian, S. Mohammad Ebrahim
   Sankur, Bulent
   Marvasti, Farokh
TI Robust Scaling-Based Image Watermarking Using Maximum-Likelihood Decoder
   With Optimum Strength Factor
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human visual system (HVS); maximum-likelihood decoder; scaling-based
   image watermarking; wavelet transform
ID INFORMATION; PERFORMANCE; VISIBILITY; SPECTRUM
AB In this paper, a new scaling-based image-adaptive watermarking system has been presented, which exploits human visual model for adapting the watermark data to local properties of the host image. Its improved robustness is due to embedding in the low-frequency wavelet coefficients and optimal control of its strength factor from HVS point of view. Maximum-likelihood (ML) decoder is used aided by the channel side information. The performance of the proposed scheme is analytically calculated and verified by simulation. Experimental results confirm the imperceptibility of the proposed method and its higher robustness against attacks compared to alternative watermarking methods in the literature.
C1 [Akhaee, Mohammad Ali; Marvasti, Farokh] Sharif Univ Technol, Dept Elect Engn, ACRI, Tehran 1458889694, Iran.
   [Sankur, Bulent] Bogazici Univ, Dept Elect & Elect Engn, TR-34342 Istanbul, Turkey.
C3 Sharif University of Technology; Bogazici University
RP Akhaee, MA (corresponding author), Sharif Univ Technol, Dept Elect Engn, ACRI, Tehran 1458889694, Iran.
EM akhaee@ee.sharif.edu; msahraeian@tamu.edu; bulent.sankur@boun.edu.tr;
   marvasti@sharif.edu
RI Sankur, Bulent/N-4663-2017
CR [Anonymous], 2000, Digital Watermarking
   Barni M, 2003, IEEE T SIGNAL PROCES, V51, P1118, DOI 10.1109/TSP.2003.809371
   Barni M, 2001, IEEE T IMAGE PROCESS, V10, P755, DOI 10.1109/83.918568
   Bi N, 2007, IEEE T IMAGE PROCESS, V16, P1956, DOI 10.1109/TIP.2007.901206
   Chakravarti Laha., 1967, HDB METHODS APPL STA, VI, P392
   Cheng Q, 2003, IEEE T SIGNAL PROCES, V51, P906, DOI 10.1109/TSP.2003.809374
   Cheng Q, 2001, IEEE T MULTIMEDIA, V3, P273, DOI 10.1109/6046.944472
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Cox IJ, 1999, P IEEE, V87, P1127, DOI 10.1109/5.771068
   DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425
   GEMBICKI FW, 1975, IEEE T AUTOMAT CONTR, V20, P769, DOI 10.1109/TAC.1975.1101105
   Hernández JR, 2000, IEEE T IMAGE PROCESS, V9, P55, DOI 10.1109/83.817598
   Huang NE, 1998, P ROY SOC A-MATH PHY, V454, P903, DOI 10.1098/rspa.1998.0193
   Kutter M., 1998, Proceedings of SPIE, V3628, P423
   Langelaar GC, 2000, IEEE SIGNAL PROC MAG, V17, P20, DOI 10.1109/79.879337
   Lu C.-S., 2004, Multimedia Security: Steganography and Digital Watermarking Techniques for Protection of Intellectual Property: Steganography and Digital Watermarking Techniques for Protection of Intellectual Property
   Maor A, 2005, IEEE T INFORM THEORY, V51, P3166, DOI 10.1109/TIT.2005.853315
   Mihçak MK, 1999, IEEE SIGNAL PROC LET, V6, P300, DOI 10.1109/97.803428
   Moulin P, 2003, IEEE T INFORM THEORY, V49, P563, DOI 10.1109/TIT.2002.808134
   Ng TM, 2005, 2005 39TH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS AND COMPUTERS, VOLS 1 AND 2, P1680
   NG TM, 2004, P SPIE C SEC STEG WA, V5306, P854
   Ng TM, 2005, IEEE SIGNAL PROCESS, V12, P345
   ORuanaidh JJK, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL I, P536, DOI 10.1109/ICIP.1997.647968
   Pérez-González F, 2001, SIGNAL PROCESS, V81, P1215, DOI 10.1016/S0165-1684(01)00040-8
   Petitcolas FAP, 2000, IEEE SIGNAL PROC MAG, V17, P58, DOI 10.1109/79.879339
   Ramkumar M, 2001, IEEE T IMAGE PROCESS, V10, P1252, DOI 10.1109/83.935040
   Seitz J., 2005, DIGITAL WATERMARKING
   Solachidis V, 2004, EURASIP J APPL SIG P, V2004, P2522, DOI 10.1155/S1110865704408014
   Steinberg Y, 2001, IEEE T INFORM THEORY, V47, P1410, DOI 10.1109/18.923724
   Tirkel AZ, 1998, SIGNAL PROCESS, V66, P373, DOI 10.1016/S0165-1684(98)00016-4
   Tsai MJ, 2007, INT CONF ACOUST SPEE, P173
   Wang JW, 2008, SIGNAL PROCESS, V88, P117, DOI 10.1016/j.sigpro.2007.07.012
   Wang YW, 2002, IEEE T IMAGE PROCESS, V11, P77, DOI 10.1109/83.982816
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Watson AB, 1997, IEEE T IMAGE PROCESS, V6, P1164, DOI 10.1109/83.605413
NR 36
TC 66
Z9 66
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2009
VL 11
IS 5
BP 822
EP 833
DI 10.1109/TMM.2009.2012922
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 474KN
UT WOS:000268282300003
DA 2024-07-18
ER

PT J
AU Wu, F
   Liu, YA
   Zhuang, YT
AF Wu, Fei
   Liu, Yanan
   Zhuang, Yueting
TI Tensor-Based Transductive Learning for Multimodality Video Semantic
   Concept Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contextual temporal associated cooccurrence (CTAC); dimensionality
   reduction; higher-order SVD (HOSVD); multimodality video semantic
   concept detection; TensorShot; transductive support tensor machines
   (TSTM)
AB Interaction and integration of multimodality media types such as visual, audio, and textual data in video are the essence of video semantic analysis. Contextual information propagation is useful for both intra-and inter-shot correlations. However, the traditional concatenated vector representation of videos weakens the power of the propagation and compensation among the multiple modalities. In this paper, we introduce a higher-order tensor framework for video analysis. We represent image frame, audio, and text in video shots as data points by the 3rd-order tensor. Then we propose a novel dimension reduction algorithm which explicitly considers the manifold structure of the tensor space from contextual temporal associated cooccurring multimodal media data. Our algorithm inherently preserves the intrinsic structure of the submanifold where tensorshots are sampled and is also able to map out-of-sample data points directly. We propose a new transductive support tensor machines algorithm to train effective classifier using large amount of unlabeled data together with the labeled data. Experiment results on TREVID 2005 data set show that our method improves the performance of video semantic concept detection.
C1 [Wu, Fei; Liu, Yanan; Zhuang, Yueting] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
C3 Zhejiang University
RP Wu, F (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Yuquan Campus, Hangzhou 310027, Peoples R China.
EM wufei@zju.edu.cn; liu.yanan@yahoo.com; yzhuang@zju.edu.cn
FU National Natural Science Foundation of China [60533090, 60525108];
   National High Technology Research and Development Program of China
   [2006AA010107]; Program for Changjiang Scholars and Innovative Research
   Team in University [IRT0652]; NSFC [60603096]
FX This work was supported in part by the National Natural Science
   Foundation of China (60533090 and 60525108), in part by the National
   High Technology Research and Development Program of China
   (2006AA010107), in part by the Program for Changjiang Scholars and
   Innovative Research Team in University (IRT0652, PCSIRT), and in part by
   the NSFC (60603096). The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Jiebo Luo.
CR [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], 1997, REGIONAL C SERIES MA
   [Anonymous], IEEE T PATTERN ANAL
   BADER BW, 2006, SAND020067592
   Belkin M, 2002, ADV NEUR IN, V14, P585
   DAI G, P 21 NAT C ART INT A
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696
   De Lathauwer L, 2007, SIGNAL PROCESS, V87, P322, DOI 10.1016/j.sigpro.2005.12.015
   He X., 2005, LAPLACIAN SCORE FEAT
   Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200
   LATHAUWER LD, 1997, THESIS K U LEUVEN LE
   LIU N, P 5 IEEE INT C DAT M
   Liu Y, 2008, PACIIA: 2008 PACIFIC-ASIA WORKSHOP ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION, VOLS 1-3, PROCEEDINGS, P91, DOI 10.1145/1459359.1459372
   LIU YN, 2006, P 13 INT MULT MOD C
   Qi G.J., 2007, P 15 ACM INT C MULTI, P17, DOI DOI 10.1145/1291233.1291245
   Tao DC, 2005, FIFTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P450
   Wang M., 2007, ACM Multi- media, P862
   Yan R, 2005, PROC CVPR IEEE, P657
   Yang J., 2006, PROC ACM INT WORKSHO, P33
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   YU HC, P 18 INT C PATT REC
   Zhang H., 2007, Mobile HCI'07, ACM Press, P273, DOI DOI 10.1145/1291233.1291290
   Zhu X., 2007, P INT C MACH LEARN
   TREVID
   CHINA US MILLION BOO
NR 25
TC 27
Z9 35
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2009
VL 11
IS 5
BP 868
EP 878
DI 10.1109/TMM.2009.2021724
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 474KN
UT WOS:000268282300007
DA 2024-07-18
ER

PT J
AU Paul, M
   Frater, MR
   Arnold, JF
AF Paul, Manoranjan
   Frater, Michael R.
   Arnold, John F.
TI An Efficient Mode Selection Prior to the Actual Encoding for H.264/AVC
   Encoder
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE H.264 mode selection; motion estimation; teleconferencing; video coding
ID ALGORITHM; DECISION
AB Many video compression algorithms require decisions to be made to select between different coding modes. In the case of H.264, this includes decisions about whether or not motion compensation is used, and the block size to be used for motion compensation. It has been proposed that constrained optimization techniques, such as the method of Lagrange multipliers, can be used to trade off between the quality of the compressed video and the bit rate generated. In this paper, we show that in many cases of practical interest, very similar results can be achieved with much simpler optimizations. Mode selection by simply minimizing the distortion with motion vectors and header information produces very similar performance to the full constrained optimization, while it reduces the mode selection and over all encoding time by 31% and 12%, respectively. The proposed approach can be applied together with fast motion search algorithms and the mode filtering algorithms for further speed up.
C1 [Paul, Manoranjan] Monash Univ, Gippsland Sch Informat Technol, Churchill, Vic 3842, Australia.
   [Frater, Michael R.; Arnold, John F.] Univ New S Wales, Australian Def Force Acad, Univ Coll, Sch Informat Technol & Elect Engn, Canberra, ACT 2600, Australia.
C3 Federation University Australia; Monash University; University of New
   South Wales Sydney; Australian Defense Force Academy
RP Paul, M (corresponding author), Monash Univ, Gippsland Sch Informat Technol, Churchill, Vic 3842, Australia.
EM Manoranjan.Paul@infotech.monash.edu.au; m.frater@adfa.edu.au;
   j.arnold@adfa.edu.au
RI Paul, Manoranjan/AAD-4047-2021
OI Paul, Manoranjan/0000-0001-6870-5056
CR Ahmad A, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P173
   [Anonymous], JVTJ033
   [Anonymous], 1999, IMAGE VIDEO COMPRESS
   BJONTEGAARD G, 2001, ITUTQ6SG16 VCEG
   FAN CP, IEEE T CI 2 IN PRESS
   *ITU, ITURBT50010
   Jing X, 2004, ELECTRON LETT, V40, P1050, DOI 10.1049/el:20045243
   Kim D, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1709, DOI 10.1109/ICME.2006.262879
   Kim H, 2004, IEEE IMAGE PROC, P765
   Kim YH, 2004, ELECTRON LETT, V40, P1172, DOI 10.1049/el:20046155
   Kondo S., 2007, Systems and Computers in Japan, V38, P12, DOI 10.1002/scj.20714
   Malvar HS, 2003, IEEE T CIRC SYST VID, V13, P598, DOI 10.1109/TCSVT.2003.814964
   Ostermann J., 2004, IEEE Circuits and Systems Magazine, V4, P7, DOI 10.1109/MCAS.2004.1286980
   Richardson E.G., 2003, H.264 and MPEG-4 Video Compression: Video Coding for Next-Generation Multimedia
   Shang JS, 2000, COMPUT SCI ENG, V2, P10, DOI 10.1109/5992.814651
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Wang HL, 2007, IEEE T MULTIMEDIA, V9, P882, DOI 10.1109/TMM.2007.893345
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P688, DOI 10.1109/TCSVT.2003.815168
   WIEGAND T, 2001, P IEEE INT C IM PROC, V1, P542
   Yang LB, 2005, IEEE T CIRC SYST VID, V15, P784, DOI 10.1109/TCSVT.2005.848306
NR 20
TC 31
Z9 33
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2009
VL 11
IS 4
BP 581
EP 588
DI 10.1109/TMM.2009.2017610
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 449LA
UT WOS:000266330900001
OA Green Published
DA 2024-07-18
ER

PT J
AU Zhu, XQ
   Agrawal, P
   Singh, JP
   Alpcan, T
   Girod, B
AF Zhu, Xiaoqing
   Agrawal, Piyush
   Singh, Jatinder Pal
   Alpcan, Tansu
   Girod, Bernd
TI Distributed Rate Allocation Policies for Multihomed Video Streaming Over
   Heterogeneous Access Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distributed rate allocation; heterogeneous access networks; multihomed
   video streaming
ID CONGESTION CONTROL; TRANSMISSION
AB We consider the problem of rate allocation among multiple simultaneous video streams sharing multiple heterogeneous access networks. We develop and evaluate an analytical framework for optimal rate allocation based on observed available bit rate (ABR) and round-trip time (RTT) over each access network and video distortion-rate (DR) characteristics. The rate allocation is formulated as a convex optimization problem that minimizes the total expected distortion of all video streams. We present a distributed approximation of its solution and compare its performance against H-infinity-optimal control and two heuristic schemes based on TCP-style additive-increase-multiplicative-decrease (AIMD) principles. The various rate allocation schemes are evaluated in simulations of multiple high-definition (HD) video streams sharing multiple access networks. Our results demonstrate that, in comparison with heuristic AIMD-based schemes, both media-aware allocation and H-infinity-optimal control benefit from proactive congestion avoidance and reduce the average packet loss rate from 45% to below 2%. Improvement in average received video quality ranges between 1.5 to 10.7 dB in PSNR for various background traffic loads and video playout deadlines. Media-aware allocation further exploits its knowledge of the video DR characteristics to achieve a more balanced video quality among all streams.
C1 [Zhu, Xiaoqing; Agrawal, Piyush; Girod, Bernd] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
   [Singh, Jatinder Pal] Deutsch Telekom Inc, R&D Lab, Altos, CA 94022 USA.
   [Alpcan, Tansu] Deutsch Telekom Labs, D-10587 Berlin, Germany.
C3 Stanford University; Deutsche Telekom AG; Deutsche Telekom AG
RP Zhu, XQ (corresponding author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
EM zhuxq@stanford.edu; piyushag@stanford.edu; jatinder.singh@telekom.com;
   tansu.alpcan@telekom.de; bgirod@stanford.edu
RI Alpcan, Tansu/D-7089-2014
OI Alpcan, Tansu/0000-0002-7434-3239
CR Allman M., 1999, IETF RFC 2581
   Alpcan T, 2005, IEEE ACM T NETWORK, V13, P1261, DOI 10.1109/TNET.2005.860099
   Alpcan T, 2003, 42ND IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-6, PROCEEDINGS, P1092
   ALPCAN T, IEEE T MOBI IN PRESS
   [Anonymous], P 5 INT S MOD OPT MO
   [Anonymous], 1998, J. Oper. Res. Soc.
   [Anonymous], 80221 IEEE
   [Anonymous], 1995, H>Optimal Control and Related Minimax Design Problems
   [Anonymous], P ACM MULT
   [Anonymous], NS 2
   [Anonymous], 1988, ACM SIGCOMM COMPUTER
   [Anonymous], 2003, 3448 RFC
   [Anonymous], P 25 IEEE INT C COMP
   Boyd S., 2004, CONVEX OPTIMIZATION
   Cuevas A, 2006, IEEE COMMUN MAG, V44, P75, DOI 10.1109/MCOM.2006.1678113
   Floyd S, 1999, IEEE ACM T NETWORK, V7, P458, DOI 10.1109/90.793002
   *ITU T, 2003, H264ISOIEC1449610AVC
   JURCA D, 2006, P 15 INT PACK VID WO, V7, P713
   Navratil J., 2003, P PASS ACT MEAS PAM
   Shakkottai S, 2007, IEEE J SEL AREA COMM, V25, P1207, DOI 10.1109/JSAC.2007.070814
   Singh JP, 2007, I S WORLD WIREL MOBI, P66
   SINGH JP, 2007, P WORKSH MIDDL NEXT
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Szwabe A., 2006, Journal of Zhejiang University (Science), V7, P63, DOI 10.1631/jzus.2006.AS0063
   Thompson G.P. R., 2006, Proc. Seventh Weather Research and Forecasting Model Workshop, P1
   Vidales P, 2005, IEEE J SEL AREA COMM, V23, P2288, DOI 10.1109/JSAC.2005.857198
   Wang ZX, 2004, PROCEEDINGS OF THE 2004 CHINA-JAPAN JOINT MEETING ON MICROWAVES, P82, DOI 10.1145/1005847.1005866
   Yaïche H, 2000, IEEE ACM T NETWORK, V8, P667, DOI 10.1109/90.879352
   ZHU X, 2006, J ZHEJIANG UNIV-SC A, V7, P727
   Zhu XQ, 2005, SIGNAL PROCESS-IMAGE, V20, P773, DOI 10.1016/j.image.2005.05.005
   X 264
NR 31
TC 64
Z9 70
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2009
VL 11
IS 4
BP 752
EP 764
DI 10.1109/TMM.2009.2017641
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 449LA
UT WOS:000266330900016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Banerjee, S
AF Banerjee, Serene
TI Low-Power Content-Based Video Acquisition for Super-Resolution
   Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive sampling; content-guided acquisition; video processing and
   enhancement
ID RECONSTRUCTION
AB Motivated by the optimization of power and improvement of video resolution, this paper proposes a content-based adaptive sampling system for video acquisition. Blind sampling suffers from the lack of resolution and blurring. However, use of a priori knowledge can provide intelligent sampling function that will reduce the blur artifacts. This paper proposes an information theoretic criteria-based sampling function. Higher sampling is proposed at high motion and edge regions while lower sampling at the low-frequency regions. This helps in providing better resolution with lower power consumption. Previous researches focus on enhancing the coding performance after the video acquisition stage. The proposed adaptive sampling scheme naturally performs super resolution without requiring extensive postprocessing. The proposed scheme has been tested on ten exemplary video sequences. Quality of the proposed adaptive sampling method is over 10-16 dB better than the coarsely sampled video. The power savings is approximate to 30-40% compared to acquiring the full resolution video.
C1 Hewlett Packard Res Labs, Bangalore, Karnataka, India.
C3 Hewlett-Packard
RP Banerjee, S (corresponding author), Hewlett Packard Res Labs, Bangalore, Karnataka, India.
EM serene_bnj@yahoo.com
CR Altunbasak Y, 2002, IEEE T CIRC SYST VID, V12, P217, DOI 10.1109/76.999200
   Baccichet P., 2006, P PICT COD S APR
   Banerjee S, 2007, IEEE T IMAGE PROCESS, V16, P1807, DOI 10.1109/TIP.2007.898992
   Barjatya A., 2004, Block matching algorithms for motion estimation
   Battiato S, 2003, PROC SPIE, V5017, P323, DOI 10.1117/12.476749
   CHENG H, 2006, Patent No. 20060077255
   CURZAN JP, 2002, P SPIE C AER INFR TE
   De Boor C, 1978, A Pratical Guide to Splines, V27
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   HUANG Q, 1998, P IEEE INT C IM PROC, V2, P808
   JENKINS B, 2000, Patent No. 6028608
   Kim H, 2005, IEEE T CIRC SYST VID, V15, P823, DOI 10.1109/TCSVT.2005.848354
   LECONTE J, 2006, ATMEL J, P37
   LUKIN A, 2006, P INT C COMP GRAPH A, P239
   Patti AJ, 2001, IEEE T IMAGE PROCESS, V10, P179, DOI 10.1109/83.892456
   RUI Y, 2004, UNIFIED FRAMEWORK VI
   Sayood K, 2017, Introduction to data compression
   TECHNOLOGY CONTENT B
NR 18
TC 7
Z9 7
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2009
VL 11
IS 3
BP 455
EP 464
DI 10.1109/TMM.2009.2012925
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 425JH
UT WOS:000264632300011
DA 2024-07-18
ER

PT J
AU Mastronarde, NH
   van der Schaar, M
AF Mastronarde, Nicholas H.
   van der Schaar, Mihaela
TI Automated Bidding for Media Services at the Edge of a Content Delivery
   Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content delivery networks; multiagent learning; multimedia service
   middleware; resource negotiation; session negotiation
AB We investigate the problem of providing media services to multiple autonomous wireless users at the edge of a content delivery network (CDN) in a setting where wireless resources are priced based on real-time market demands. Our focus is on the multimedia service resource negotiation process, which is performed prior to the actual media transmission. We adopt the progressive second price (PSP) auction mechanism, which is used to determine the network resource allocation to the users and a corresponding tax for the consumed resources. Our interest in this negotiation mechanism lies in understanding a single user's (or agent's) ability to learn to improve its bids over time in order to increase its own utility in the face of time-varying resource valuations and contention for resources with other users. We pay particular attention to the implementation complexity and the information requirements of the agent's deployed learning rule, and we quantify the impact of these factors on the rule's ultimate performance (i.e., the cumulative utility achieved over time) and efficiency (i.e., the utility gained per unit of computation). These factors are especially important in the mobile video streaming context, where limited resources must be efficiently utilized, and where communication and computation overheads can significantly impact the quality of service experienced by the user.
C1 [Mastronarde, Nicholas H.; van der Schaar, Mihaela] Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
C3 University of California System; University of California Los Angeles
RP Mastronarde, NH (corresponding author), Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
EM nhmastro@ee.ucla.edu; Mihaela@ee.ucla.edu
RI Mastronarde, Nicholas/W-5332-2019; Mastronarde, Nicholas/IZD-7746-2023
OI Mastronarde, Nicholas/0000-0002-8474-7237; Mastronarde,
   Nicholas/0000-0002-8474-7237
FU NSF [CCF-0541453, CNS-0509522]
FX Manuscript received November 06, 2007: revised October 25, 2008. First
   published March 10, 2009: current version published March 18, 2009. This
   work was supported in part by NSF CAREER Award CCF-0541453 and in part
   by NSF CNS-0509522. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Hayder Radha.
CR Chou P.A., 2007, Multimedia over IP and Wireless Networks: Compression, Networking, and Systems
   Fudenberg D., 1991, GAME THEORY
   Hart S, 2005, ECONOMETRICA, V73, P1401, DOI 10.1111/j.1468-0262.2005.00625.x
   Jackson M.O., 2003, ENCY LIFE SUPPORT SY
   Lazar A., 1999, TELECOMMUN SYST
   MAILLE P, 2004, P IEEE INFOCOM   MAR
   NADIMINTI R, 1996, DECIS SUPPORT SYST, P241
   VETRO A, IEEE T MULT IN PRESS
   WEE S, 2007, MULTIMEDIA IP WIRELE
   YOUNG HP, 1998, STRATEGIC LEARNING I
NR 10
TC 7
Z9 7
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2009
VL 11
IS 3
BP 543
EP 555
DI 10.1109/TMM.2009.2012926
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 425JH
UT WOS:000264632300019
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Mylonas, P
   Spyrou, E
   Avrithis, Y
   Kollias, S
AF Mylonas, Phivos
   Spyrou, Evaggelos
   Avrithis, Yannis
   Kollias, Stefanos
TI Using Visual Context and Region Semantics for High-Level Concept
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Concept detection; contextualization; region thesaurus; region types;
   visual context
ID IMAGE; SEGMENTATION; ONTOLOGY
AB In this paper we investigate detection of high-level concepts in multimedia content through an integrated approach of visual thesaurus analysis and visual context. In the former, detection is based on model vectors that represent image composition in terms of region types, obtained through clustering over a large data set. The latter deals with two aspects, namely high-level concepts and region types of the thesaurus, employing a model of a priori specified semantic relations among concepts and automatically extracted topological relations among region types; thus it combines both conceptual and topological context. A set of algorithms is presented, which modify either the confidence values of detected concepts, or the model vectors based on which detection is performed. Visual context exploitation is evaluated on TRECVID and Corel data sets and compared to a number of related visual thesaurus approaches.
C1 [Mylonas, Phivos; Spyrou, Evaggelos; Avrithis, Yannis; Kollias, Stefanos] Natl Tech Univ Athens, Image Video & Multimedia Lab, Athens 15780, Greece.
C3 National Technical University of Athens
RP Mylonas, P (corresponding author), Natl Tech Univ Athens, Image Video & Multimedia Lab, Zographou Campus,PC, Athens 15780, Greece.
EM fmylonas@image.ntua.gr; espyrou@image.ntua.gr; iavr@image.ntua.gr;
   stefanos@image.ntua.gr
RI Mylonas, Phivos/AAF-2497-2019; Kollias, Stefanos/ACY-7285-2022
OI Mylonas, Phivos/0000-0002-6916-3129; Kollias,
   Stefanos/0000-0003-2899-0598
CR [Anonymous], 2007, ACM MULTIMEDIA, DOI DOI 10.1145/1291233.1291379
   [Anonymous], 2006, P ECCV
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], 2021 C N AM CHAPTER
   [Anonymous], CONTEXTUAL MODELS OB
   [Anonymous], 2002, CP3451 JEITA
   [Anonymous], P INT WORKSH VER LOW
   [Anonymous], 1980, PHILOS GRAMMAR
   [Anonymous], P 5 INT INT C MOD US
   Athanasiadis T, 2007, IEEE T CIRC SYST VID, V17, P298, DOI 10.1109/TCSVT.2007.890636
   Avrithis YS, 1999, COMPUT VIS IMAGE UND, V75, P3, DOI 10.1006/cviu.1999.0761
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   BENITEZ AB, 2003, P IEEE INT C IM PROC
   BIEDERMAN I, 1982, COGNITIVE PSYCHOL, V14, P143, DOI 10.1016/0010-0285(82)90007-X
   BOUJEMAA N, 2004, IS T SPIE C STOR RET
   Boutell M, 2005, MULTIMEDIA SYST, V11, P82, DOI 10.1007/s00530-005-0202-7
   Boutell M, 2004, PROC CVPR IEEE, P623
   BOUTELL M, 2004, P IEEE WORKSH STAT R
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Carbonetto P, 2004, LECT NOTES COMPUT SC, V3021, P350
   Chang SF, 2001, IEEE T CIRC SYST VID, V11, P688, DOI 10.1109/76.927421
   Dance C., 2004, P ECCV INT WORKSH ST
   Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97
   EDMONDS B, 1999, P 2 INT INT C MOD US, V1688, P119
   Fan JP, 2008, IEEE T IMAGE PROCESS, V17, P407, DOI 10.1109/TIP.2008.916999
   Fan JP, 2008, IEEE T MULTIMEDIA, V10, P167, DOI 10.1109/TMM.2007.911775
   Fan Jianping., 2004, ACM International Conference on Multimedia, P540, DOI [DOI 10.1145/1027527, DOI 10.1145/1027527.1027660]
   GHOSHAL A, 2003, P SIGIR 2003, P544
   Gokalp D., 2007, P IEEE C COMP VIS PA
   HOIEM D, 2005, P ICCI
   Hudelot C, 2008, FUZZY SET SYST, V159, P1929, DOI 10.1016/j.fss.2008.02.011
   *ISO IEC, 2001, JTC1SC29WG11 ISOIEC
   Jeon J., 2003, P 26 ANN INT ACM SIG
   JIANG YG, 2006, AS PAC WORKSH VIS IN
   Jin R., 2004, Proceedings of the 12th annual ACM international conference on Multimedia, P892, DOI DOI 10.1145/1027527.1027732
   JITEN J, 2006, SPIE, V6073, P211
   KIM S, 2006, P 18 INT C PATT REC
   Klir G., 1995, Fuzzy sets and fuzzy logic, V4
   Kumar S., 2003, P ICCV
   Kumar S., 2005, P ICCV
   LAZEBNIK S, 2005, P INT C COMP VIS ICC
   Li J, 2000, IEEE T SIGNAL PROCES, V48, P517, DOI 10.1109/78.823977
   LIPSON P, 1997, P IEEE C COMP VIS PA
   LUO J, 2001, P IEEE INT C IM PROC
   LUO J, 2002, P IEEE INT C MULT EX
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   Mathes A., 2004, Computer Mediated Communication
   MCCARTHY J, 1993, P 13 INT JOINT C ART, P81
   MIYAJIMA K, 1994, P 3 IEEE C FUZZ SYST
   Miyamoto S., 1990, FUZZY SETS INFORM RE
   MOLDOVAN D, 2005, P 19 INT JOINT C ART
   Murphy P, 2003, ADV NEUR INFORM PROC, V16
   MYLONAS P, 2007, P 1 WORKSH MULT ANN
   MYLONAS P, 2006, P 7 INT WORKSH IM AN
   MYLONAS P, 2007, P 2 INT WORKSH SEM M
   OLIVA A, 2005, PROGR BRAIN RES VIS, P23
   OPELT A, 2006, P CVPR 2006 WASH DC
   PALETTA L, 2000, P 15 INT C PATT REC, V1, P1695
   PANTOFARU C, 2007, P BRIT MACH VIS C SE
   SAUX B, 2004, P INT C COMP VIS GRA
   Singhal A, 2003, PROC CVPR IEEE, P235
   SMEATON AF, 2006, P 8 ACM INT WORKSH M, P26
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Snoek CGM, 2007, IEEE T MULTIMEDIA, V9, P280, DOI 10.1109/TMM.2006.886275
   SNOEK CGM, 2006, LEARNED LEXICON DRIV
   Souvannavong F, 2005, IEE P-VIS IMAGE SIGN, V152, P859, DOI 10.1049/ip-vis:20045184
   SOUVANNAVONG F, 2005, CBMI 2005 4 INT WORK
   SPYROU E, 2007, P 4 IFIP C ART INT A
   SPYROU E, 2008, P 9 INT WORKSH IM AN
   SPYROU E, 2007, P 5 TRECVID WORKSH G
   SPYROU E, 2007, P 2 INT C SEM DIG ME
   SPYROU E, 2005, P INT C ART NEUR NET
   Staab S., 2004, INT HDB INFORM SYSTE
   Torralba A, 2003, INT J COMPUT VISION, V53, P169, DOI 10.1023/A:1023052124951
   Torralba Antonio, 2005, P586, DOI 10.1016/B978-012375731-9/50100-2
   TSECHPENAKIS G, 2002, P EUR S INT TECHN HY
   Tu ZW, 2005, INT J COMPUT VISION, V63, P113, DOI 10.1007/s11263-005-6642-x
   VAILAYA A, 2000, P SPIE JAN, V3972
   VOISINE N, 2005, P 6 INT WORKSH IM AN
   Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P947, DOI 10.1109/34.955109
   WANG L, 2007, P 8 AS C COMP VIS AC
   WU Y, 2004, P IEEE INT C MULT EX
   Yang C., 2005, P ACM INT C MULTIMED, P435
   YANG L, 2007, P 2007 COMP VIS PATT
   RDF REIFICATION
   MARVEL IBM MULTIMEDI
   [No title captured]
NR 88
TC 23
Z9 27
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2009
VL 11
IS 2
SI SI
BP 229
EP 243
DI 10.1109/TMM.2008.2009681
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EG
UT WOS:000262714800005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chun, YD
   Kim, NC
   Jang, IH
AF Chun, Young Deok
   Kim, Nam Chul
   Jang, Ick Hoon
TI Content-Based Image Retrieval Using Multiresolution Color and Texture
   Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content-based image retrieval; multiresolution representation; color and
   texture features
ID SEARCH
AB In this paper, we propose a content-based image retrieval method based on an efficient combination of multiresolution color and texture features. As its color features, color autocorrelograms of the hue and saturation component images in HSV color space are used. As its texture features, BDIP and BVLC moments of the value component image are adopted. The color and texture features are extracted in multiresolution wavelet domain and combined. The dimension of the combined feature vector is determined at a point where the retrieval accuracy becomes saturated. Experimental results show that the proposed method yields higher retrieval accuracy than some conventional methods even though its feature vector dimension is not higher than those of the latter for six test DBs. Especially, it demonstrates more excellent retrieval accuracy for queries and target images of various resolutions. In addition, the proposed method almost always shows performance gain in precision versus recall and in ANMRR over the other methods.
C1 [Chun, Young Deok] Samsung Elect Co Ltd, GPGI, SW Lab, Div Mobile Commun, Gumi 730350, South Korea.
   [Kim, Nam Chul] Kyungpook Natl Univ, Dept Elect Engn, Lab Visual Commun, Taegu 702701, South Korea.
   [Jang, Ick Hoon] Kyungwoon Univ, Dept Elect Engn, Gumi 730850, South Korea.
C3 Samsung; Kyungpook National University; Kyungwoon University
RP Chun, YD (corresponding author), Samsung Elect Co Ltd, GPGI, SW Lab, Div Mobile Commun, Gumi 730350, South Korea.
EM yd.chun@samsung.com; nckim@ee.knu.ac.kr; ihjang@ikw.ac.kr
CR Androutsos D, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 2, P770, DOI 10.1109/ICIP.1998.723652
   Ankerst M, 1998, IEEE T KNOWL DATA EN, V10, P996, DOI 10.1109/69.738362
   [Anonymous], ISO WG11 MPEG M GEN
   [Anonymous], 159383FDIS ISOIEC
   Brunelli R, 2000, IEEE T MULTIMEDIA, V2, P164, DOI 10.1109/6046.865481
   Chang SF, 1998, IEEE T CIRC SYST VID, V8, P602, DOI 10.1109/76.718507
   Chang T, 1993, IEEE T IMAGE PROCESS, V2, P429, DOI 10.1109/83.242353
   Chun Y., 2005, THESIS KYUNGPOOK NAT
   Chun YD, 2003, IEEE T CIRC SYST VID, V13, P951, DOI 10.1109/TCSVT.2003.816507
   CHUN YD, 2005, P SPIE STORAGE RETRI, P195
   Feng D., 2003, FUNDAMENTALS CONTENT
   Gersho A., 2003, Vector Quantization and Signal Compression
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Huang J, 1997, PROC CVPR IEEE, P762, DOI 10.1109/CVPR.1997.609412
   *ISO IEC, 1999, ISOIECJTC1SC29WG11MP
   Liapis S, 2004, IEEE T MULTIMEDIA, V6, P676, DOI 10.1109/TMM.2004.834858
   Liu S, 2001, IEEE T IMAGE PROCESS, V10, P874, DOI 10.1109/83.923284
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   Ojala T., 2001, Proceedings of 12th Scandinavian Conference on Image Analysis, P621
   PEARSON DE, 1985, P IEEE, V73, P795, DOI 10.1109/PROC.1985.13202
   Permuter H, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P569
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   RYOO YJ, 1988, ELECTRON LETT, V24, P461, DOI 10.1049/el:19880312
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   SMITH JR, 1994, IEEE IMAGE PROC, P407, DOI 10.1109/ICIP.1994.413817
   Song BC, 2001, IEEE T CIRC SYST VID, V11, P673, DOI 10.1109/76.920197
   STOLLNITZ E.J., 1996, WAVELETS COMPUTER GR
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Vadivel A, 2004, PROCEEDINGS OF INTERNATIONAL CONFERENCE ON INTELLIGENT SENSING AND INFORMATION PROCESSING, P127
NR 30
TC 144
Z9 156
U1 2
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 1073
EP 1084
DI 10.1109/TMM.2008.2001357
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600011
DA 2024-07-18
ER

PT J
AU Czarlinska, A
   Kundur, D
AF Czarlinska, Alexandra
   Kundur, Deepa
TI Reliable event-detection in wireless visual sensor networks through
   scalar collaboration and game-theoretic consideration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE actuation; event-detection; game theory; scalar-sensors; sensor network
   security; wireless visual sensor networks (WVSNs)
AB In this work we consider an event-driven wireless visual sensor network (WVSN) comprised of untethered camera nodes and scalar sensors deployed in a hostile environment. In the event-driven paradigm, each camera node transmits a surveillance frame to the cluster-head only if an event of interest was captured in the frame, for energy and bandwidth conservation. We thus examine a simple image processing algorithm at the camera nodes based on difference frames and the chi-squared detector. We show that the test statistic of the chi-squared detector is equivalent to that of a robust (non-parametric) detector and that this simple algorithm performs well on indoor surveillance sequences and some, but not all, outdoor sequences. In outdoor sequences containing significant changes in background and lighting, this simple detector may produce a high probability of error and benefits from the inclusion of scalar sensor decisions. The scalar sensor decisions are, however, prone to attack and may exhibit errors that are arbitrarily frequent, pervasive throughout the network and difficult to predict. To achieve attack prediction and mitigation given an attacker whose actions are not known a priori, we employ game-theoretic analysis. We show that the scalar sensor error can be controlled through cluster-head checking and appropriate selection of cluster size n. Given this attack mitigation, we employ real-life sequences to determine the total probability of error when individual and combined decisions are utilized and we discuss the ensuing ramifications and performance issues.
C1 [Czarlinska, Alexandra; Kundur, Deepa] Texas A&M Univ, Elect & Comp Dept, College Stn, TX 77843 USA.
C3 Texas A&M University System; Texas A&M University College Station
RP Czarlinska, A (corresponding author), Texas A&M Univ, Elect & Comp Dept, College Stn, TX 77843 USA.
EM czlinska@ece.tamu.edu; deepa@ece.tamu.edu
CR AACH T, 1995, SIGNAL PROCESS-IMAGE, V7, P147, DOI 10.1016/0923-5965(95)00003-F
   AACH T, 1993, SIGNAL PROCESS, V31, P165, DOI 10.1016/0165-1684(93)90063-G
   Akyildiz I., 2004, AD HOC NETW, V2, P3351
   Akyildiz IF, 2007, COMPUT NETW, V51, P921, DOI 10.1016/j.comnet.2006.10.002
   Basharat A, 2005, Third IEEE International Conference on Pervasive Computing and Communications, Workshops, P385, DOI 10.1109/PERCOMW.2005.6
   Chow KY, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/95076
   CHOW KY, 2006, P IEEE INT S WIR PER
   Czarlinska A., 2006, PROC IEEE WORKSHOP D, P3
   CZARLINSKA A, 2007, ACM MULT SEC WORKSH
   CZARLINSKA A, 2007, P IEEE GLOB NOV 26 3
   Ford RM, 1997, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS '97, PROCEEDINGS, P610, DOI 10.1109/MMCS.1997.609780
   Fu FW, 2007, IEEE T MULTIMEDIA, V9, P851, DOI 10.1109/TMM.2007.895676
   He ZH, 2006, IEEE T CIRC SYST VID, V16, P590, DOI 10.1109/TCSVT.2006.873154
   HONGLIANG L, 2004, MOBILE COMPUT COMMUN, V6, P624
   Joyce RA, 2006, IEEE T MULTIMEDIA, V8, P130, DOI 10.1109/TMM.2005.861285
   KUNDUR D, 2007, P IEEE SPEC IN PRESS
   Lelescu D, 2003, IEEE T MULTIMEDIA, V5, P106, DOI 10.1109/TMM.2003.808819
   Liu K, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P873
   Ma H, 2005, 2005 INTERNATIONAL CONFERENCE ON WIRELESS NETWORKS, COMMUNICATIONS AND MOBILE COMPUTING, VOLS 1 AND 2, P987
   Maniezzo D, 2002, 2002 4TH INTERNATIONAL WORKSHOP ON MOBILE AND WIRELESS COMMUNICATION NETWORK, P373, DOI 10.1109/MWCN.2002.1045791
   Mao YN, 2006, IEEE T IMAGE PROCESS, V15, P2061, DOI 10.1109/TIP.2006.873426
   MARDIA KV, 1989, BIOMETRIKA, V76, P271
   Margi C.B., 2006, P 2 INT C TESTB RES
   Osborne Martin J, 1994, COURSE GAME THEORY
   Ott R.L., 2015, INTRO STAT METHODS D
   Radke RJ, 2005, IEEE T IMAGE PROCESS, V14, P294, DOI 10.1109/TIP.2004.838698
   Rodriguez V, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P813
   Rosin PL, 2003, PATTERN RECOGN LETT, V24, P2345, DOI 10.1016/S0167-8655(03)00060-6
   Rosin PL, 2002, COMPUT VIS IMAGE UND, V86, P79, DOI 10.1006/cviu.2002.0960
   Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677
   van Trees H. L, 2001, Detection, Estimation, and Modulation Theory: Part III. Radar-sonar Signal Processing and Gaussian Signal in Noise
   VEERARAGHAVAN K, 2005, P IEEE INT C EL TECH, P22
NR 32
TC 18
Z9 19
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2008
VL 10
IS 5
BP 675
EP 690
DI 10.1109/TMM.2008.922775
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 334LR
UT WOS:000258223800002
DA 2024-07-18
ER

PT J
AU Nguyen, T
   Kolazhi, K
   Kamath, R
   Cheung, SCS
   Tran, DA
AF Nguyen, Thinh
   Kolazhi, Krishnan
   Kamath, Rohit
   Cheung, Sen-ching S.
   Tran, Duc A.
TI Efficient multimedia distribution in source constraint networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE multimedia screening; P2P networks
ID MULTICAST
AB In recent years, the number of Peer-to-Peer (P2P) applications has increased significantly. One important problem in many P2P applications is how to efficiently disseminate data from a single source to multiple receivers on the Internet. A successful model used for analyzing this problem is a graph consisting of nodes and edges, with a capacity assigned to each edge. In some situations however, it is inconvenient to use this model. To that end, we propose to study the problem of efficient data dissemination in a source constraint network. A source constraint network is modeled as a graph in which, the capacity is associated with a node, rather than an edge. The contributions of this paper include (a) a quantitative data dissemination in any source constraint network, (b) a set of topologies suitable for data dissemination in P2P networks, and (c) an architecture and implementation of a P2P system based on the proposed optimal topologies. We will present the experimental results of our P2P system deployed on PlanetLab nodes demonstrating that our approach achieves near optimal throughput while providing scalability, low delay and bandwidth fairness among peers.
C1 [Nguyen, Thinh; Kolazhi, Krishnan; Kamath, Rohit] Oregon State Univ, Sch Elect Engn & Comp Sci, Corvallis, OR 97331 USA.
   [Cheung, Sen-ching S.] Univ Kentucky, Lexington, KY 40506 USA.
   [Tran, Duc A.] Univ Massachusetts, Boston, MA 02125 USA.
C3 Oregon State University; University of Kentucky; University of
   Massachusetts System; University of Massachusetts Boston
RP Nguyen, T (corresponding author), Oregon State Univ, Sch Elect Engn & Comp Sci, Corvallis, OR 97331 USA.
EM thinhq@eecs.orst.edu; prashkrish200@gmail.com; rohitvkamath@gmail.com;
   cheung@engr.uky.edu; duc@cs.umb.edu
OI Tran, Duc/0000-0001-8129-0940
FU NSF [0524831, CNS-0615055]; Division Of Computer and Network Systems;
   Direct For Computer & Info Scie & Enginr [0753066] Funding Source:
   National Science Foundation; Div Of Information & Intelligent Systems;
   Direct For Computer & Info Scie & Enginr [0524831] Funding Source:
   National Science Foundation
FX This work was supported under by the NSF under Grant Cyber Trust 0524831
   and Grant CNS-0615055. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Hayder Radha.
CR Banerjee S., 2003, P IEEE INFOCOM
   BYERS J, 2004, IEEE ACM T NETW OCT, V12
   Castro M., 2003, SOSP
   Chou PA, 2001, IEEE T MULTIMEDIA, V3, P108, DOI 10.1109/6046.909598
   Deering S, 1996, IEEE ACM T NETWORK, V4, P153, DOI 10.1109/90.490743
   EDMONDS J, 1972, J ACM, V19, P248, DOI 10.1145/321694.321699
   HE Z, 2003, P IEEE INFOCOM APR
   KOSTIC D, 2003, P SOSP OCT
   LEVIN D, 2006, P INT WORKSH PEER PE
   LI PAC, 2005, P ACM SIGC AS WORKSH
   MINSKY Y, 2001, P IEEE INT S INF THE
   Nguyen T, 2004, IEEE T MULTIMEDIA, V6, P315, DOI 10.1109/TMM.2003.822790
   NGUYEN T, 2006, P IEEE INT C MULT EX
   NGUYEN T, 2005, SOURCE CONSTRAINT NE
   NGUYEN T, 2005, P IEEE INT C MULT AC
   PADMANABHAN V, 2002, P ACM NOSSDAV MIAM F
   Rowstron A, 2003, IFIPACM INT C DISTRI
   ROWSTRON A, 2001, P NGC NOV
   SAVROU A, 2002, P IEEE ICNP NOV
   Sherwood R., 2004, P IEEE INFOCOM
   TAN W, 1999, P 6 INT C IM PROC OC, V1, P401
   Yang X., 2004, P IEEE INFOCOM
   INTERNET TOPOLOGY GE
   SWARMCAST
NR 24
TC 14
Z9 14
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 523
EP 537
DI 10.1109/TMM.2008.917351
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100020
DA 2024-07-18
ER

PT J
AU Xu, L
   Doermann, D
   Li, HP
AF Xu Liu
   Doermann, David
   Li, Huiping
TI VCode - Pervasive data transfer using video barcode
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE error correction coding; image processing; multimedia communication
ID RECOGNITION; INVARIANTS; VIEWPOINT
AB In this paper, we describe a novel data transfer scheme that uses the camera in a smart phone as an alternative data channel. The data is encoded as a sequence of 2-D barcode images, displayed on a flat panel display, acquired by the camera, and decoded in real time by the software embedded in device. The decoded data is written to a file. Compared with existing data channels, such as CDMA/GPRS, cables, Bluetooth, and Infrared, our method relies on visual communication and does not require special hardware or data plans. Users only need to point the camera at a monitor displaying the VCode to download. Technical challenges to overcome include correction of perspective distortion, compensation for contrast variation, and efficient implementation of small footprint software into a mobile device. We address these challenges and present our solution in detail. We have implemented a prototype which allows users to download various types of files successfully, including pictures, ring tones and Java games onto camera phones running Symbian and Windows Mobile platforms. We discuss the limitations of our solution and outline future work to overcome these limitations.
C1 [Xu Liu; Doermann, David; Li, Huiping] Appl Media Anal Inc, College Pk, MD 20742 USA.
   [Xu Liu; Doermann, David; Li, Huiping] Univ Maryland, Language & Media Proc Lab, Inst Adv Comp Studies, College Pk, MD 20742 USA.
C3 University System of Maryland; University of Maryland College Park
RP Xu, L (corresponding author), Univ Maryland, Language & Media Proc Lab, Inst Adv Comp Studies, College Pk, MD 20742 USA.
EM liuxu@umiacs.umd.edu; doermann@umud.edu; huiping@mo-bileama.com
RI Li, Huiping/H-2798-2012
OI Li, Huiping/0000-0002-8598-1869
CR ANDO S, 2001, P IEEE INT VEH EL C, P49
   Bae KS, 2005, P INT COMP SOFTW APP, P539
   Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210
   Chen XL, 2002, SIXTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P32, DOI 10.1109/ACV.2002.1182151
   Criminisi A, 1999, IMAGE VISION COMPUT, V17, P625, DOI 10.1016/S0262-8856(98)00183-8
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242
   HARALICK RM, PATTERN RECOGNIT, V22, P225
   HEE H, 2002, P 6 INT C SIGN PROC, V2, P1791
   IJIRI Y, 2006, P 7 INT C MOB DAT MA, P49
   Koga M, 2005, PROC INT CONF DOC, P635, DOI 10.1109/ICDAR.2005.65
   Kwok SKW, 1996, ELECTRON LETT, V32, P1775, DOI 10.1049/el:19961217
   LIU X, 2005, P 4 INT C MOB UB MUL, P103
   Mindru F, 2004, COMPUT VIS IMAGE UND, V94, P3, DOI 10.1016/j.cviu.2003.10.011
   Ohbuchi E, 2004, 2004 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P260, DOI 10.1109/CW.2004.23
   OTERO A, 1999, P 1999 INT C INF INT, P313
   Ottaviani E, 1999, IEE CONF PUBL, P652, DOI 10.1049/cp:19990404
   Richardson TJ, 2001, IEEE T INFORM THEORY, V47, P638, DOI 10.1109/18.910579
   STELZREID CC, 1999, NASAS DEEP SPACE TEL
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Vucetic B., 2000, TURBO CODES PRINCIPL, DOI 10.1007/978-1-4615-4469-2
   WEISS I, 1993, INT J COMPUT VISION, V10, P207, DOI 10.1007/BF01539536
   Wicker S.B., 1999, Reed-Solomon codes and their applications
   2002, P 4 IEEE INT C MULT, P217
NR 23
TC 18
Z9 24
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 361
EP 371
DI 10.1109/TMM.2008.917353
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100006
DA 2024-07-18
ER

PT J
AU Wang, Y
   Kim, JG
   Chang, SF
   Kim, HM
AF Wang, Yong
   Kim, Jae-Gon
   Chang, Shih-Fu
   Kim, Hyung-Myung
TI Utility-based video adaptation for universal multimedia access (UMA) and
   content-based utility function prediction for real-time video
   transcoding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content-based prediction; universal media access; utility function;
   video adaptation
AB Many techniques exist for adapting videos to satisfy heterogeneous resource conditions or user preferences, whereas selection of the best adaptation operation among various choices usually is either ad hoc or inefficient. To provide a systematic solution, we present a conceptual framework based on utility function (UF), which models video entity, adaptation, resource, utility, and the relations among them. In order to support real-time video adaptation, we present a content-based statistical paradigm to facilitate the prediction of UF for real-time transcoding of live videos. Instead of modelling the UF through analytical models, as in the conventional rate-distortion framework, the proposed approach formulates the prediction as a classification and regression problem. Each video clip is classified into one of distinctive categories and then local regression is used to accurately predict the utility value. Our extensive experiment results based on MPEG-4 transcoding demonstrate that the proposed method achieves very promising performance-up to 89% accuracy in choosing the optimal transcoding operation (in both,spatial and temporal dimensions) with the highest quality over a diverse range of target bit rates.
C1 Columbia Univ, Dept Elect Engn, New York, NY 10027 USA.
   Elect & Telecommun Res Inst, Dept Broadcasting Media Technol, Taejon 305350, South Korea.
   Korea Adv Inst Sci & Technol, Dept Elect Engn & Comp Sci, Taejon 305701, South Korea.
C3 Columbia University; Electronics & Telecommunications Research Institute
   - Korea (ETRI); Korea Advanced Institute of Science & Technology (KAIST)
RP Wang, Y (corresponding author), Columbia Univ, Dept Elect Engn, New York, NY 10027 USA.
EM wangyong@gmail.com; jgkim@etri.re.kr; sfchang@ee.columbia.edu;
   hmkim@csplab.kaist.ac.kr
RI Kim, Hyung-Myung/C-1935-2011
CR [Anonymous], 1998, STAT LEARNING THEORY
   Berger Toby, 1971, RATE DISTORTION THEO
   Bjork N., 2000, Proceedings of the 2000 ACM Workshops on Multimedia, P75
   BOCHECK P, 1999, P IEEE PACKET VIDEO
   CHANG SF, 2002, P INT WORKSH DIG COM
   Chen PS, 2004, IEEE T CIRC SYST VID, V14, P1183, DOI 10.1109/TCSVT.2004.833165
   ELEFTHERIADIS A, 1995, THESIS COLUMBIA U NE
   Hang HM, 1997, IEEE T CIRC SYST VID, V7, P287
   KIM JG, 2003, P INT C MULT EXP ICM
   Mohan R, 1999, IEEE T MULTIMEDIA, V1, P104, DOI 10.1109/6046.748175
   Mukherjee D, 2005, IEEE T MULTIMEDIA, V7, P454, DOI 10.1109/TMM.2005.846798
   Platt JC, 2000, ADV NEUR IN, V12, P547
   Reed EC, 2002, IEEE T IMAGE PROCESS, V11, P873, DOI 10.1109/TIP.2002.801122
   Shanableh T, 2000, IEEE T MULTIMEDIA, V2, P101, DOI 10.1109/6046.845014
   Vetro A, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P534, DOI 10.1109/ICIP.2001.958169
   WANG Y, 2004, P SPIE VID COMM IM P
   WANG Y, 2003, MPEG 4 REAL TIME FD
   WANG Y, 2003, P INT C IM PROC ICIP, V1, P189
   Werner O, 1999, IEEE T IMAGE PROCESS, V8, P179, DOI 10.1109/83.743853
   Yin P, 2000, IEEE IMAGE PROC, P972, DOI 10.1109/ICIP.2000.901123
   Zhang B., 2000, HPL2000137
   ZHONG D, 2001, THESIS COLUMBIA U NE
NR 22
TC 29
Z9 42
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2007
VL 9
IS 2
BP 213
EP 220
DI 10.1109/TMM.2006.886253
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 135CO
UT WOS:000244131900001
DA 2024-07-18
ER

PT J
AU Zhu, BS
   Wu, JK
   Kankanhalli, MS
AF Zhu, Baoshi
   Wu, Jiankang
   Kankanhalli, Mohan S.
TI Render sequence encoding for document protection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE authentication; Exact Travel of Salesman Problem (XTSP); render sequence
   encoding; tamper detection
ID COPYRIGHT PROTECTION; IDENTIFICATION; MARKING
AB We present in this paper a novel electronic document watermarking method, render sequence encoding (RSE), and then further develop a RSE authentication method for electronic documents. RSE watermarks an electronic document by modulating the display sequences of words or characters. It features large information-carrying capacity and robustness over document format transcoding. The RSE authentication method is based on the NP-complete Exact Traveling Salesman Problem, which provides a rigorous foundation for security. The RSE authentication method is secure in the sense it is extremely difficult to forge the authentication process. RSE authentication process is also easy to operate, especially in comparison to digital signatures which requires Public Key Infrastructure for its operation.
C1 Trustcopy Pte Ltd, Singapore 088934, Singapore.
   Inst Infocomm Res, Singapore, Singapore.
   Natl Univ Singapore, Sch Comp, Dept Comp Sci, Singapore 117543, Singapore.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); National University of Singapore
RP Zhu, BS (corresponding author), Trustcopy Pte Ltd, Singapore Technol Bldg, Singapore 088934, Singapore.
EM baoshi.zhu@gmail.com; jiankangwu@gmail.com; mohan@comp.nus.edu.sg
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015
CR [Anonymous], 2016, HDB APPL CRYPTOGRAPH
   BRASSIL J, 1996, P 1 ACM INT C DIG LI, P177
   BRASSIL J, 1995, P C INF SCI SYST CIS
   BRASSIL JT, 1995, IEEE J SEL AREA COMM, V13, P1495, DOI 10.1109/49.464718
   Brassil JT, 1999, P IEEE, V87, P1181, DOI 10.1109/5.771071
   BRASSIL JT, 1994, P 12 ICPR, V2, P315
   Brualdi R.A., 1999, Introductory Combinatorics, V3rd
   COX IJ, 2001, P IEEE INT WORKSH MU
   GARDNER M, 1974, SCI AM           NOV, P122
   HALL M, 1963, P S PURE MATH, V6, P203
   Johnson S.M., 1963, Math. Comput., V17, P282, DOI 10.2307/2003846
   Low SH, 1998, IEEE J SEL AREA COMM, V16, P561, DOI 10.1109/49.668978
   Low SH, 1998, IEEE T COMMUN, V46, P372, DOI 10.1109/26.662643
   LOW SH, 1995, IEEE INFOCOM SER, P853, DOI 10.1109/INFCOM.1995.515956
   LUCKS S, 1995, LNCS
   LUCKS S, 1994, FAST SOFTWARE ENCRYP, P298
   Maxemchuk NF, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL III, P13, DOI 10.1109/ICIP.1997.631958
   Memon N, 1999, PROC SPIE, V3528, P412, DOI 10.1117/12.337430
   PATARIN J, 1994, LNCS, V773, P391
   POINTCHEVAL D, 1995, LNCS, V921, P319, DOI DOI 10.1007/3-540-49264-X
   SHAMIR A, 1990, LECT NOTES COMPUT SC, V435, P606
   STERN J, 1995, LECT NOTES COMPUTER, V839, P164
   Stern J., 1993, Lecture Notes in Computer Science, Advances in Cryptology-CRYPTO', V773, P13, DOI DOI 10.1007/3-540-48329-22
   TROTTER HF, 1962, COMMUN ACM, V5, P434, DOI 10.1145/368637.368660
   Zheng Y., 1992, Advances in Cryptology, P83
   1999, ADOBE SYSTEM INC, P176
NR 26
TC 6
Z9 7
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2007
VL 9
IS 1
BP 16
EP 24
DI 10.1109/TMM.2006.886334
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 119VE
UT WOS:000243041500003
DA 2024-07-18
ER

PT J
AU Chen, ZZ
   Han, JW
   Ngan, KN
AF Chen, Zhenzhong
   Han, Junwei
   Ngan, King Ngi
TI Dynamic bit allocation for multiple video object coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE bit allocation; MPEG-4; multiple video object; rate control;
   rate-distortion
ID MULTIGRID MOTION ESTIMATION; QUALITY ASSESSMENT; VISUAL-ATTENTION;
   MPEG-4 VIDEO; SEGMENTATION; COMPRESSION; MODEL; FOVEATION; VISION
AB In MPEG-4, a visual scene may be treated as a composition of video objects and coded at object level. Such a flexible video coding framework makes it possible to code different video objects with different priority according to human perceptual characteristics. In this paper, we introduce a novel dynamic bit allocation framework to improve the subjective quality in such an object-based video coding system. We incorporate the rate distortion models with the dynamic priorities of the video objects and jointly encode video objects to minimize the weighted distortion within the bit budget constraint. We guarantee the human-interested video objects a better reconstructed quality by using the weighted bit allocation strategy in favour of the video objects with higher priority. To obtain the priority automatically, we apply a visual attention model. Comparing with traditional bit allocation algorithms, the objective quality of the object with higher priority is significantly improved under this framework. These results demonstrate the usefulness of this dynamic bit allocation framework.
C1 Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
RP Chen, ZZ (corresponding author), Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong, Peoples R China.
EM zchen@ee.cuhk.edu.hk; jwhan@ee.cuhk.edu.hk; knngan@ee.cuhk.edu.hk
RI Chen, Zhenzhong/B-3110-2011; Chen, Zhenzhong/J-8017-2012; 陈,
   震中/C-6857-2014; Chen, Zhenzhong/C-2529-2015; Ngan, N/E-8240-2014
OI Ngan, N/0000-0003-1946-3235
CR [Anonymous], 1449621999 ISOIEC
   [Anonymous], IMAGE COMMUN
   Chen Z, 2004, IEE P-VIS IMAGE SIGN, V151, P250, DOI 10.1049/ip-vis:20040517
   CHEN Z, 2004, IEEE INT C IM PROC S
   Chen ZZ, 2005, IEEE T CIRC SYST VID, V15, P1170, DOI 10.1109/TCSVT.2005.852621
   Chen ZZ, 2004, IEEE T CIRC SYST VID, V14, P869, DOI 10.1109/TCSVT.2004.828331
   Chiang TH, 1997, IEEE T CIRC SYST VID, V7, P246, DOI 10.1109/76.554439
   Chien SY, 2003, IEEE T CIRC SYST VID, V13, P453, DOI 10.1109/TCSVT.2003.811605
   EVERETT H, 1963, OPER RES, V11, P399, DOI 10.1287/opre.11.3.399
   HAN J, 2004, INT S INT SIGN PROC
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P840, DOI 10.1109/TCSVT.2002.804883
   *ISO IEC, 2001, JTC1SC29WG11
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   KUNT M, 1985, P IEEE, V73, P549, DOI 10.1109/PROC.1985.13184
   Lee C, 2006, OPT ENG, V45, DOI 10.1117/1.2160515
   Lee HJ, 2000, IEEE T CIRC SYST VID, V10, P878, DOI 10.1109/76.867926
   Lee JW, 2003, IEEE T CIRC SYST VID, V13, P488, DOI 10.1109/TCSVT.2003.813421
   Lee S, 2001, IEEE T IMAGE PROCESS, V10, P977, DOI 10.1109/83.931092
   Malo J, 2000, ELECTRON LETT, V36, P807, DOI 10.1049/el:20000645
   Malo J, 2001, IEEE T IMAGE PROCESS, V10, P1411, DOI 10.1109/83.951528
   Meier T, 1999, IEEE T CIRC SYST VID, V9, P1190, DOI 10.1109/76.809155
   ORTEGA A, 1994, IEEE T IMAGE PROCESS, V3, P26, DOI 10.1109/83.265978
   Ronda JI, 1999, IEEE T CIRC SYST VID, V9, P1243, DOI 10.1109/76.809159
   *SARN CORP, JNDMETRIX TECHN
   Schuster G., 1997, RATE DISTORTION BASE
   SHOHAM Y, 1988, IEEE T ACOUST SPEECH, V36, P1445, DOI 10.1109/29.90373
   Silsbee PL, 1993, IEEE T CIRC SYST VID, V3, P291, DOI 10.1109/76.257218
   Vetro A, 1999, IEEE T CIRC SYST VID, V9, P186, DOI 10.1109/76.744285
   VETRO A, 2001, P IEEE INT C IM PROC, P383
   Wang HH, 2005, IEEE T CIRC SYST VID, V15, P1113, DOI 10.1109/TCSVT.2005.852629
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2003, IEEE T IMAGE PROCESS, V12, P243, DOI 10.1109/TIP.2003.809015
   Watson AB, 2001, J ELECTRON IMAGING, V10, P20, DOI 10.1117/1.1329896
   WATSON AB, 2002, P IEEE INT C IM PROC
   Winkler S, 1999, SIGNAL PROCESS, V78, P231, DOI 10.1016/S0165-1684(99)00062-6
   Yang XG, 1999, IEEE T IMAGE PROCESS, V8, P332, DOI 10.1109/83.748889
   Zhang XM, 1998, SIGNAL PROCESS, V70, P201, DOI 10.1016/S0165-1684(98)00125-X
   [No title captured]
NR 39
TC 32
Z9 34
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2006
VL 8
IS 6
BP 1117
EP 1124
DI 10.1109/TMM.2006.884633
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109MG
UT WOS:000242311700002
DA 2024-07-18
ER

PT J
AU Yu, XG
   Leong, HW
   Xu, CS
   Tian, Q
AF Yu, Xinguo
   Leong, Hon Wai
   Xu, Changsheng
   Tian, Qi
TI Trajectory-based ball detection and tracking in broadcast soccer video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE algorithms; anti-model; ball detection and tracking; sports video;
   trajectory-based
ID HOUGH TRANSFORM; MOTION; SEGMENTATION; ALGORITHM
AB This paper presents a novel trajectory-based detection and tracking algorithm for locating the ball in broadcast soccer video (BSV). The problem of ball detection and tracking in BSV is well known to be very challenging because of the wide variation in the appearance of the ball over frames. Direct detection algorithms do not work well because the image of the ball may be distorted due to the high speed of the ball, occlusion, or merging with other objects in the frame. To overcome these challenges, we propose a two-phase trajectory-based algorithm in which we first generate a set of ball-candidates for each frame, and then use them to compute the set of ball trajectories. Informally, the two key ideas behind our strategy are 1) while it is very challenging to achieve high accuracy in locating the precise location of the ball, it is relatively easy to achieve very high accuracy in locating the ball among a set of ball-like candidates and 2) it is much better to study the trajectory information of the ball since the ball is the "most active" object in the BSV. Once the ball trajectories are computed, the ball locations can be reliably recovered from them. One important advantage of our algorithm is that it is able to reliably detect partially occluded or merged balls in the sequence. Two videos from the 2002 FIFA World Cup were used to evaluate our algorithm. It achieves a high accuracy of about 81% for ball location.
C1 Inst Infocomm Res Singapore, Singapore 117543, Singapore.
   Natl Univ Singapore, Dept Comp Sci, Singapore 117548, Singapore.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); National University of Singapore
RP Yu, XG (corresponding author), Inst Infocomm Res Singapore, Singapore 117543, Singapore.
EM xinguo@i2r.a-star.edu.sg; leonghw@comp.nus.edu.sg;
   xucs@i2r.a-star.edu.sg; tian@i2r.a-star.edu.sg
RI de Barros, Ricardo Machado Leite/AAF-4555-2020; chen, yue/JXW-9556-2024;
   Leong, Wai/G-6642-2013; xu, cj/HJZ-3488-2023
OI de Barros, Ricardo Machado Leite/0000-0002-9554-1381; leong, wai
   yie/0000-0002-5389-1121
CR [Anonymous], P 12 ANN ACM INT C M
   [Anonymous], 1995, P IEEE INT C MULT CO
   Assfalg E, 2003, COMPUT VIS IMAGE UND, V92, P285, DOI 10.1016/j.cviu.2003.06.004
   BOUTHEMY P, 1993, INT J COMPUT VISION, V10, P157, DOI 10.1007/BF01420735
   Boyd JE, 2003, MACH VISION APPL, V13, P344, DOI 10.1007/s00138-002-0100-3
   Brown R., 2012, Introduction to Random Signals and Applied Kalman Filtering With MATLAB Exercises, V4th
   Chang SF, 2002, IEEE MULTIMEDIA, V9, P6, DOI 10.1109/93.998041
   COX IJ, 1993, INT J COMPUT VISION, V10, P53, DOI 10.1007/BF01440847
   D'Orazio T, 2004, PATTERN RECOGN, V37, P393, DOI 10.1016/S0031-3203(03)00228-0
   D'Orazio T, 2002, INT C PATT RECOG, P210, DOI 10.1109/ICPR.2002.1044654
   ILLINGWORTH J, 1988, COMPUT VISION GRAPH, V44, P87, DOI 10.1016/S0734-189X(88)80033-1
   KARMANN KP, 1990, SIGNAL PROCESSING V : THEORIES AND APPLICATIONS, VOLS 1-3, P951
   Keren D, 2001, IEEE T PATTERN ANAL, V23, P747, DOI 10.1109/34.935848
   KIM T, 1998, P ICCV
   KOLLER D, 1993, INT J COMPUT VISION, V10, P257, DOI 10.1007/BF01539538
   Kuhne G., 2001, MULTIMEDIA 01, P41
   LOWE DG, 1992, INT J COMPUT VISION, V8, P113, DOI 10.1007/BF00127170
   Ohno Y, 2000, INT C PATT RECOG, P145, DOI 10.1109/ICPR.2000.905293
   OHNO Y, 1999, P INT C MULT FUS INT
   Rasmussen C, 1998, PROC CVPR IEEE, P16, DOI 10.1109/CVPR.1998.698582
   Rasmussen C, 2001, IEEE T PATTERN ANAL, V23, P560, DOI 10.1109/34.927458
   Seo Y., 1997, International Conference on Image Analysis and Processing, P196, DOI DOI 10.1007/3-540-63508-4
   SMITH P, 1975, IEEE T AUTOMAT CONTR, VAC20, P101, DOI 10.1109/TAC.1975.1100851
   Tovinkere Vasanth., 2001, Proceedings of the 2001 IEEE International Conference on Multimedia and Expo (ICME'Ol), P1040
   XU L, 1993, CVGIP-IMAG UNDERSTAN, V57, P131, DOI 10.1006/ciun.1993.1009
   Yamamoto A, 2003, CURR METH INORG CHEM, V3, P1
   YU X, 2003, P PCM 2003, P929
   YU X, 2004, P ICIP, P1049
   Yu X., 2003, PROC 11 ACM INT C MU, P11
   YU X, 2004, THESIS NAT U SINGAPO
   Yu XG, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P273
   Yu XG, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P265
   YUEN HK, 1989, IMAGE VISION COMPUT, V7, P31, DOI 10.1016/0262-8856(89)90017-6
   ZHANG ZY, 1992, INT J COMPUT VISION, V7, P211, DOI 10.1007/BF00126394
   Zhao T, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P710, DOI 10.1109/ICCV.2001.937593
NR 35
TC 63
Z9 68
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2006
VL 8
IS 6
BP 1164
EP 1178
DI 10.1109/TMM.2006.884621
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109MG
UT WOS:000242311700007
DA 2024-07-18
ER

PT J
AU Dominguez, SM
   Keaton, T
   Sayed, AH
AF Dominguez, Sylvia M.
   Keaton, Trish
   Sayed, Ali H.
TI A robust finger tracking method for multimodal wearable computer
   interfacing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE finger tracking; genetic algorithm; human-machine interface; Kalman
   filter; robust filtering; state-space model; wearable computing
AB Mobile wearable computers are intended to provide users with real-time access to information in a natural and unobtrusive manner. Computing and sensing in these devices must be reliable, easy to interact with, transparent, and configured to support different needs and complexities. This paper presents a visionbased robust finger tracking algorithm combined with audio-based control commands that is integrated into a multimodal unobtrusive user interface, wherein the interface may be used to segment out objects of interest in the environment by encircling them with the user's pointing fingertip. In order to quickly extract the objects encircled by the user from a complex scene, this unobtrusive interface uses a single head-mounted camera to capture color images, which are then processed using algorithms to perform: color segmentation, fingertip shape analysis, perturbation model learning, and robust fingertip tracking. This interface is designed to be robust to changes in the environment and user's movements by incorporating a state-space estimation with uncertain models algorithm, which attempts to control the influence of uncertain environment conditions on the system's fingertip tracking performance by adapting the tracking model to compensate for the uncertainties inherent in the data collected with a wearable computer.
C1 Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
   HRL Labs LLC, Informat Sci Lab, Malibu, CA 90265 USA.
C3 University of California System; University of California Los Angeles;
   HRL Laboratories
RP Dominguez, SM (corresponding author), Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
EM sylvia@summavision.com; trish.keaton@gmail.com; sayed@ee.ucla.edu
RI Sayed, Ali/D-6251-2012
OI Sayed, Ali/0000-0002-5125-5519
CR [Anonymous], 1995, P INT WORKSH AUT FAC
   [Anonymous], 1999, P ISWC SAN FRANC US
   BROWN T, 2000, P AUSTR US INT C AUI, V1, P11
   Comaniciu D, 1997, PROC CVPR IEEE, P750, DOI 10.1109/CVPR.1997.609410
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   DEVERDIERE VC, 1998, P EUR C COMP VIS FRI
   Dominguez SM, 2001, CONF REC ASILOMAR C, P342, DOI 10.1109/ACSSC.2001.986948
   DOMINGUEZ SM, 2001, P PERC US INT ORL FL
   Duda R. O., 2000, PATTERN CLASSIFICATI
   Ghahramani Z., 1996, Tech. Rep.
   Imagawa K, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P462, DOI 10.1109/AFGR.1998.670991
   Jennings C., 1999, Proceedings International Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems. In Conjunction with ICCV'99 (Cat. No.PR00378), P152, DOI 10.1109/RATFG.1999.799238
   Kailath T, 2000, PR H INF SY, pXIX
   Keaton T., 2002, Proceedings of the Sixth Digital Image Computing Techniques and Applications. Dicta 2002, P92
   Keaton T, 2002, SIXTH INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P75, DOI 10.1109/ISWC.2002.1167221
   Keaton T, 1999, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES (CBAIVL'99) - PROCEEDINGS, P69, DOI 10.1109/IVL.1999.781126
   Keaton T, 2005, PERS UBIQUIT COMPUT, V9, P343, DOI 10.1007/s00779-004-0316-5
   MEHRA RK, 1970, IEEE T AUTOMAT CONTR, VAC15, P175, DOI 10.1109/TAC.1970.1099422
   Sayed AH, 2001, IEEE T AUTOMAT CONTR, V46, P998, DOI 10.1109/9.935054
   Sayed AH., 2003, FUNDAMENTALS ADAPTIV
   Schneiderman H, 1998, PROC CVPR IEEE, P45, DOI 10.1109/CVPR.1998.698586
   Tomasi C, 1991, DETECTION TRACKING P
   van der Heijden F., 1994, IMAGE BASED MEASUREM
   Wu A., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P536, DOI 10.1109/AFGR.2000.840686
   ZHU X, 2000, P 4 INT C AUT FAC GE, P446
NR 25
TC 25
Z9 27
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2006
VL 8
IS 5
BP 956
EP 972
DI 10.1109/TMM.2006.879872
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 091TY
UT WOS:000241052400008
DA 2024-07-18
ER

PT J
AU Erkücük, S
   Krishnan, SS
   Zeytinoglu, M
AF Erkucuk, Serhat
   Krishnan, Sridhar (Sri)
   Zeytinoglu, Mehmet
TI A robust audio watermark representation based on linear chirps
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio watermarking; Hough-Radon transform; linear chirps; time-frequency
   distribution
AB In this paper, we introduce a novel watermark representation for audio watermarking, where we embed linear chirps as watermark signals. Different chirp rates, i.e., slopes on the time-frequency (TF) plane, represent watermark messages such that each slope corresponds to a unique message. These watermark signals, i.e., linear chirps, are embedded and extracted using an existing watermarking algorithm. The extracted chirps are then postprocessed at the receiver using a line detection algorithm based on the Hough-Radon transform (HRT). The HRT is an optimal line-detection algorithm, which detects directional components that satisfy a parametric constraint equation in the image of a TF plane, even at discontinuities corresponding to bit errors. Simulation results show that HRT correctly detects the embedded watermark message after common signal processing operations for bit error rates up to 20%. The new watermark representation and the postprocessing stage based on HRT significantly improve the performance of the watermark detection process and can be combined with existing watermark embedding/extraction algorithms for increased robustness.
C1 Simon Fraser Univ, Sch Engn Sci, Burnaby, BC V5A 1S6, Canada.
   Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
C3 Simon Fraser University; Toronto Metropolitan University
RP Erkücük, S (corresponding author), Simon Fraser Univ, Sch Engn Sci, Burnaby, BC V5A 1S6, Canada.
EM serkucuk@sfu.ca; krishnan@ee.ryerson.ca; mzeytin@ee.ryerson.ca
RI Krishnan, Sridhar/AAA-2542-2019; Erkucuk, Serhat/AAH-2378-2019
OI Erkucuk, Serhat/0000-0001-7041-8013; Krishnan,
   Sridhar/0000-0002-4659-564X
CR [Anonymous], 2000, Digital Watermarking
   Arnold M, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1013, DOI 10.1109/ICME.2000.871531
   Bassia P, 2001, IEEE T MULTIMEDIA, V3, P232, DOI 10.1109/6046.923822
   Bender W., 1996, IBM SYST J, V35, DOI DOI 10.1147/SJ.353.0313
   COHEN L, 1989, P IEEE, V77, P941, DOI 10.1109/5.30749
   Cox I. J., 2002, Digital Watermarking
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   CVEJIC N, 2003, P IEEE INT C MULT EX, V1, P217
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242
   ERKUCUK S, 2003, P IEEE INT C MULT EX, V2, P513
   Gang L, 2001, INT CONF ACOUST SPEE, P1365, DOI 10.1109/ICASSP.2001.941182
   Gordy JD, 2000, PROCEEDINGS OF THE 43RD IEEE MIDWEST SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOLS I-III, P456, DOI 10.1109/MWSCAS.2000.951682
   Herman G. T, 1980, IMAGE RECONSTRUCTION
   Huang JW, 2002, 2002 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL III, PROCEEDINGS, P627
   Kirovski D, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P219, DOI 10.1109/MMSP.2001.962737
   KRISHNAN S, 1999, THESIS U CALGARY CAL
   Lee SK, 2000, IEEE T CONSUM ELECTR, V46, P744, DOI 10.1109/30.883441
   LIE WN, 2001, P IEEE INT S CIRC SY, V2, P45
   Rangayyan RM, 2001, PATTERN RECOGN, V34, P1147, DOI 10.1016/S0031-3203(00)00073-X
   Seok JW, 2001, ELECTRON LETT, V37, P60, DOI 10.1049/el:20010029
   Su JK, 2002, IEEE T MULTIMEDIA, V4, P551, DOI 10.1109/TMM.2002.806535
   Swanson MD, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P19, DOI 10.1109/MMCS.1999.779114
   Wei L, 2003, DES AUT CON, P554
NR 23
TC 22
Z9 23
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2006
VL 8
IS 5
BP 925
EP 936
DI 10.1109/TMM.2006.879879
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 091TY
UT WOS:000241052400005
DA 2024-07-18
ER

PT J
AU Lazic, N
   Aarabi, P
AF Lazic, Nevena
   Aarabi, Parham
TI Communication over an acoustic channel using data hiding techniques
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE acoustic communication; data hiding
ID ROBUST AUDIO WATERMARKING
AB This work proposes an audio data hiding system that hides information into signals not known beforehand. The system can hide data into live music, or ambient sounds in general, and can be used to communicate information acoustically from one device to another. An important benefit of such a system is backward-compatibility, as the transmitter is a speaker, and the receiver is a microphone, both of which are already present in numerous devices and environments. The highest data rate achieved was 213 bits/s, while keeping the error rate under 10%. The developed technique was applied in a simple navigation system, where acoustic data embedded into background music indicates the location of the receiver.
C1 Univ Toronto, Dept Elect & Comp Engn, Toronto, ON M6P 2P4, Canada.
C3 University of Toronto
RP Lazic, N (corresponding author), Univ Toronto, Dept Elect & Comp Engn, Toronto, ON M6P 2P4, Canada.
EM lazic@ecf.utoronto.ca
CR Bassia P, 2001, IEEE T MULTIMEDIA, V3, P232, DOI 10.1109/6046.923822
   Bender W, 1996, IBM SYST J, V35, P313, DOI 10.1147/sj.353.0313
   Chen B, 2001, IEEE T INFORM THEORY, V47, P1423, DOI 10.1109/18.923725
   CHEN B, 1999, P 1999 IEEE INT C CO, V2, P823
   CHOU J, 2003, P IEEE INT C AC SPEE, V2, P337
   CHOU J, 2001, P INT C INF TECHN CO
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Dong XX, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P377
   Ikeda M, 1999, INT CONF ACOUST SPEE, P2315, DOI 10.1109/ICASSP.1999.758401
   ISO/IEC, 1993, 111723 ISOIEC
   JAYANT N, 1993, P IEEE, V81, P1385, DOI 10.1109/5.241504
   Kim HJ, 2003, IEEE T CIRC SYST VID, V13, P885, DOI 10.1109/TCSVT.2003.815950
   Kirovski D, 2003, IEEE T SIGNAL PROCES, V51, P1020, DOI 10.1109/TSP.2003.809384
   Kuo S, 2002, INT CONF ACOUST SPEE, P1753
   Lathi BP, 1998, Modern digital and analog communication systems, Vthird
   Lemma AN, 2003, IEEE T SIGNAL PROCES, V51, P1088, DOI 10.1109/TSP.2003.809372
   LIE WN, 2001, 2001 IEEE INT S CIRC, V2, P45
   Liu Z, 2003, IEEE T CIRC SYST VID, V13, P801, DOI 10.1109/TCSVT.2003.815960
   Marvel LM, 1999, IEEE T IMAGE PROCESS, V8, P1075, DOI 10.1109/83.777088
   Oh HO, 2001, INT CONF ACOUST SPEE, P1341, DOI 10.1109/ICASSP.2001.941176
   Painter T, 2000, P IEEE, V88, P451, DOI 10.1109/5.842996
   Papadopoulos HC, 1998, IEEE T COMMUN, V46, P1233, DOI 10.1109/26.718565
   PRADHAN SS, 1999, 33 AS C SIGN SYST CO, V2, P1503
   Scharf B., 1970, Foundations of modern auditory theory
   SCHROEDER MR, 1979, J ACOUST SOC AM, V66, P1647, DOI 10.1121/1.383662
   Swanson MD, 1998, SIGNAL PROCESS, V66, P337, DOI 10.1016/S0165-1684(98)00014-0
   Tachibana R, 2004, EURASIP J APPL SIG P, V2004, P1955, DOI 10.1155/S1110865704403138
   TERHARDT E, 1979, HEARING RES, V1, P155, DOI 10.1016/0378-5955(79)90025-X
   Zwicker E., 2013, Psychoacoustics: Facts and Models
NR 29
TC 24
Z9 27
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2006
VL 8
IS 5
BP 918
EP 924
DI 10.1109/TMM.2006.879880
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 091TY
UT WOS:000241052400004
DA 2024-07-18
ER

PT J
AU Bao, P
   Gourlay, D
AF Bao, P
   Gourlay, D
TI A framework for remote rendering of 3-D scenes on limited mobile devices
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE image based rendering; mobile low-end devices; residual images; 3-D
   warping
AB This correspondence describes a networked three-dimensional (3-D) rendering framework that allows low-powered mobile devices to simulate the interactive rendering of remote dense 3-D environment. The framework is based on 3-D image warping, especially suited to low-resolution screens and involves the transmission of sparse residual images. The use of an oversized reference image, superview, to cope with the visibility gap errors and the acceleration techniques aimed at improving the frame-rates are explored. It is shown that the described framework offers a potential alternative for interactive rendering of remote 3-D environment on low-powered clients where the triangle mesh display is impractical.
C1 Univ S Florida, Dept Informat Technol, Tampa, FL 33260 USA.
   Chongqing Univ, Sch Comp, Chongqing 630044, Peoples R China.
   Chinese Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China.
C3 State University System of Florida; University of South Florida;
   Chongqing University; Chinese University of Hong Kong
RP Univ S Florida, Dept Informat Technol, Tampa, FL 33260 USA.
EM pbao@lakeland.usf.edu; douglas@ie.cuhk.edu.hk
CR [Anonymous], P IEEE SPIE MULT COM
   [Anonymous], THESIS U N CAROLINA
   Attene M, 2003, ACM T GRAPHIC, V22, P982, DOI 10.1145/944020.944022
   Bao P, 2004, IEEE T MULTIMEDIA, V6, P786, DOI 10.1109/TMM.2004.837248
   Bao P, 2003, IEEE T CONSUM ELECTR, V49, P177, DOI 10.1109/TCE.2003.1205473
   Cohen-Or D, 1999, COMP GRAPH, P261, DOI 10.1145/311535.311564
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   HOSSEINI M, 2002, P 7 INT C 3D WEB TEC, P19
   Koller D, 2004, ACM T GRAPHIC, V23, P695, DOI 10.1145/1015706.1015782
   Mann Y, 1997, COMPUT GRAPH FORUM, V16, pC201, DOI 10.1111/1467-8659.00157
   MARK W, P 1997 S INT 3D GRAP, P7
   Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882
   Taubin G, 1998, ACM T GRAPHIC, V17, P84, DOI 10.1145/274363.274365
   TAUBIN G, 1998, SNHC VERIFICATION MO
   Wolberg G., 1990, Digital image warping
NR 15
TC 13
Z9 14
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2006
VL 8
IS 2
BP 382
EP 389
DI 10.1109/TMM.2005.864337
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 028GY
UT WOS:000236476300017
DA 2024-07-18
ER

PT J
AU Chen, CY
   Wang, JC
   Wang, JF
AF Chen, CY
   Wang, JC
   Wang, JF
TI Efficient news video querying and browsing based on distributed news
   video servers
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE distributed server; maximum difference measure; news video;
   querying/browsing; semantic analysis
AB This study presents an efficient news video querying and browsing system based on distributed news video servers. The proposed architecture includes distributed news video preprocessing (NVP) server and visualized querying/browsing (VQB) server. The distributed NVP server receives the news video story from news video web (NVW) server and generates the story abstract, i.e., key frames and key sentences. These story abstract are then combined with the news script of a news story, then sent to the VQB server for news video querying and browsing. The VQB server performs semantic clustering using news script to categorize all the news video stories, it also provides a visualized interface displaying the story abstract so that users can fast grasp the main idea of a news story. The superiority of the proposed system has been demonstrated "sing news video obtained from NVW servers of EraNews, ETtoday and Formosa TV stations in Taiwan.
C1 Natl Cheng Kung Univ, Dept Elect Engn, Multimedia & Commun IC Design Lab, Tainan 701, Taiwan.
C3 National Cheng Kung University
RP Natl Cheng Kung Univ, Dept Elect Engn, Multimedia & Commun IC Design Lab, 1 Univ Rd, Tainan 701, Taiwan.
EM chency@icwang.ee.ncku.edu.tw
CR [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   BARNETT SA, 1996, IEEE J SEL AREAS COM, V14
   Cheng HN, 2003, COMPUT-AIDED CHEM EN, V15, P172
   Foltz PW, 1996, BEHAV RES METH INSTR, V28, P197, DOI 10.3758/BF03204765
   Gao XB, 2002, IEEE T CIRC SYST VID, V12, P765, DOI 10.1109/TCSVT.2002.800510
   Gunsel B, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P128, DOI 10.1109/ICIP.1998.727150
   Hanjalic A, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P148, DOI 10.1109/ICIP.1998.727156
   HEESCH D, 2003, P TREC VID RETR EV T
   HEESCH D, 2004, P TREC VID RETR EV T
   KONDO K, 1995, NLC9562 IEICE
   Landauer TK, 1998, DISCOURSE PROCESS, V25, P259, DOI 10.1080/01638539809545028
   Liu TM, 2003, IEEE T CIRC SYST VID, V13, P1006, DOI 10.1109/TCSVT.2003.816521
   OTSUJI K, 1991, P SPIE VIS COMM IM P, P980
   PETER MW, 1999, P INT JOINT C ART IN, P932
   REN F, 2001, P IEEE INT C SYST MA, P1705
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   SERPANOS N, 2000, IEEE T CIRCUITS SYST, V10, P1483
   SNOEK CGM, 2004, P TREC VID RETR EV T
   Sun XD, 1998, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS, P190, DOI 10.1109/MMCS.1998.693638
   Tanaka K, 1997, ICICS - PROCEEDINGS OF 1997 INTERNATIONAL CONFERENCE ON INFORMATION, COMMUNICATIONS AND SIGNAL PROCESSING, VOLS 1-3, P578, DOI 10.1109/ICICS.1997.647165
   Tonomura Y., 1994, IEEE Multimedia, V1, P34, DOI 10.1109/MMUL.1994.318984
   Wang XF, 2002, 2002 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-4, PROCEEDINGS, P515, DOI 10.1109/ICMLC.2002.1176809
   Wolf W, 1996, INT CONF ACOUST SPEE, P1228, DOI 10.1109/ICASSP.1996.543588
   Yeung MM, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pA338
   Zhuang YT, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P866, DOI 10.1109/ICIP.1998.723655
NR 25
TC 3
Z9 4
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2006
VL 8
IS 2
BP 257
EP 269
DI 10.1109/TMM.2005.864272
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 028GY
UT WOS:000236476300007
DA 2024-07-18
ER

PT J
AU Leung, KC
   Li, VOK
AF Leung, KC
   Li, VOK
TI A paracasting model for concurrent access to replicated Internet content
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content distribution; high speed networks; Internet; paracasting;
   performance modeling; server replication
AB In this paper, we develop a model to study how to effectively download a document from a set of replicated servers. We propose a generalized application-layer anycasting protocol, known as paracasting, to advocate concurrent access of a subset of replicated servers to cooperatively satisfy a client's request. Each participating server satisfies the request in part by transmitting a subset of the requested file to the client. The client can recover the complete file when different parts of the file sent from the participating servers are received. This model allows us to estimate the average time to download a file from the set of homogeneous replicated servers, and the request blocking probability when each server can accept and serve a finite number of concurrent requests. Our results show that the file download time drops when a request is served concurrently by a larger number of homogeneous replicated servers, although the performance improvement quickly saturates when the number of servers increases. If the total number of requests that a server can handle simultaneously is finite, the request blocking probability increases with the number of replicated servers used to serve a request concurrently. Therefore, paracasting is effective when a small number of servers, say, up to four, are used to serve a request concurrently.
C1 Univ Hong Kong, Dept Elect & Elect Engn, Hong Kong, Hong Kong, Peoples R China.
C3 University of Hong Kong
RP Univ Hong Kong, Dept Elect & Elect Engn, Hong Kong, Hong Kong, Peoples R China.
EM kcleung@ieee.org; vli@eee.hku.hk
RI Li, Victor On Kwok/C-1858-2009; Leung, Ka Cheong/C-1857-2009
CR [Anonymous], 1992, NUMERICAL RECIPES C
   CHIOU SN, 1987, P IEEE ICC 87 SEATTL, V2, P968
   COHEN JW, 1979, ACTA INFORM, V12, P245, DOI 10.1007/BF00264581
   Fielding R., 1999, Tech. Rep
   Fredj SB, 2001, ACM SIGCOMM COMP COM, V31, P111, DOI 10.1145/964723.383068
   Gkantsidis C, 2003, WIAPP 2003: THIRD IEEE WORKSHOP ON INTERNET APPLICATIONS, PROCEEDINGS, P79, DOI 10.1109/WIAPP.2003.1210291
   Ka-Cheong Leung, 1999, 1999 IEEE International Conference on Communications (Cat. No. 99CH36311), P1239, DOI 10.1109/ICC.1999.765516
   Kleinrock L., 1975, Queueing Systems-Volume 1: Theory, V1
   Koo SGM, 2003, NINTH IEEE WORKSHOP ON FUTURE TRENDS OF DISTRIBUTED COMPUTING SYSTEMS, PROCEEDINGS, P128, DOI 10.1109/FTDCS.2003.1204324
   Leung KC, 2003, CCW 2003: IEEE 18TH ANNUAL WORKSHOP ON COMPUTER COMMUNICATIONS, PROCEEDINGS, P105, DOI 10.1109/CCW.2003.1240797
   LEUNG KC, 2000, P ICNP 2000 OS JAP 1, P305
   MAXEMCHUK NF, 1993, COMPUT NETWORKS ISDN, V25, P645, DOI 10.1016/0169-7552(93)90059-D
   Ng TSE, 2003, IEEE INFOCOM SER, P2199
   Obraczka K, 1998, IEEE J SEL AREA COMM, V16, P369, DOI 10.1109/49.669045
   Ranjan S, 2004, IEEE INFOCOM SER, P816
   Rodriguez P, 2002, IEEE ACM T NETWORK, V10, P455, DOI 10.1109/TNET.2002.801413
   SAYAL M, 1998, ACM SIGMETRICS PERFO, V26, P44
   Zegura EW, 2000, IEEE ACM T NETWORK, V8, P455, DOI 10.1109/90.865074
NR 18
TC 3
Z9 3
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2006
VL 8
IS 1
BP 90
EP 100
DI 10.1109/TMM.2005.861288
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 005UR
UT WOS:000234850000009
OA Green Published
DA 2024-07-18
ER

PT J
AU Cheung, G
   Tan, WT
   Yoshimura, T
AF Cheung, G
   Tan, WT
   Yoshimura, T
TI Real-time video transport optimization using streaming agent over 3G
   wireless networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE computer networks; multimedia communication; multimedia systems
AB Feedback adaptation has been the basis for many media streaming schemes, whereby the media being sent is adapted in real time according to feedback information about the observed network state and application state. Central to the success of such adaptive schemes, the feedback must: 1) arrive in a timely manner and 2) carry enough information to effect useful adaptation. In this paper, we examine the use of feedback adaptation for media streaming in 3G wireless networks, where the media servers are located in wired networks while the clients are wireless. We argue that end-to-end feedback adaptation using only information provided by 3G standards is neither timely nor contain enough information for media adaptation at the server. We first show how the introduction of a streaming agent (SA) at the junction of the wired and wireless network can be used to provide useful information in a timely manner for media adaptation. We then show how optimization algorithms can be designed to take advantage of SA feedbacks to improve performance. The improvement of SA feedbacks in peak signal-to-noise ratio is significant over nonagent-based systems.
C1 Hewlett Packard Labs, Suginami Ku, Tokyo 1680072, Japan.
   Hewlett Packard Labs, Palo Alto, CA 94306 USA.
   NTT DoCoMo Multimedia Labs, Yokosuka, Kanagawa, Japan.
C3 Hewlett-Packard; Hewlett-Packard; NTT Docomo
RP Hewlett Packard Labs, Suginami Ku, Tokyo 1680072, Japan.
EM gene-cs.cheung@hp.com
RI Cheung, Gene/AAB-9284-2020
OI Cheung, Gene/0000-0002-5571-4137
CR *3GPP TS, 2001, 26233 3GPP TS
   Balakrishnan H, 1997, IEEE ACM T NETWORK, V5, P756, DOI 10.1109/90.650137
   BLOM R, 2001, SECURE REAL TIME TRA
   Chakareski J, 2002, INT CONF ACOUST SPEE, P2513
   Chakareski J, 2002, IEEE DATA COMPR CONF, P53, DOI 10.1109/DCC.2002.999943
   CHAKARESKI J, 2002, P AS C SIGN SYST COM
   CHAKARESKI J, 2002, IEEE WORKSH MULT SIG
   Chande V, 2000, IEEE J SEL AREA COMM, V18, P850, DOI 10.1109/49.848239
   Floyd S., 2000, P ACM SIGCOMM STOCKH
   Günter M, 2002, IEEE NETWORK, V16, P22, DOI 10.1109/MNET.2002.1002996
   Holma H., 2001, WCDMA UMTS RADIO ACC, V1st
   LEE A, 2001, P GLOBECOM SAN ANT T
   MATSUOKA H, 2002, P AS INT MOB COMP C
   MIAO Z, 2000, P AS C SIGN SYST COM
   PODOLSKY M, 1998, UCBCSD981024
   SCHULZRINE H, 1996, 1889 IETF RFC
   SHOHAM Y, 1988, IEEE T ACOUST SPEECH, V36, P1445, DOI 10.1109/29.90373
   YOSHIMURA T, 2002, P IEEE INT C COMM NE
   ZHANG Q, 2001, P IEEE INT C IM PROC
NR 19
TC 14
Z9 15
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2005
VL 7
IS 4
BP 777
EP 785
DI 10.1109/TMM.2005.850974
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 946XQ
UT WOS:000230607000017
DA 2024-07-18
ER

PT J
AU Bescós, J
   Cisneros, G
   Martínez, JM
   Menéndez, JM
   Cabrera, J
AF Bescós, J
   Cisneros, G
   Martínez, JM
   Menéndez, JM
   Cabrera, J
TI A unified model for techniques on video-shot transition detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE cut detection; detection model; gradual transition detection; shot
   transition detection; video analysis
ID SCENE CHANGE DETECTION; BOUNDARY DETECTION
AB A first step required to allow video indexing and retrieval of visual data is to perform a temporal segmentation, that is, to find the location of camera-shot transitions, which can be either abrupt (i.e., cuts) or gradual (e.g., fades, dissolves, wipes). After a critical review of most approaches seeking to solve this problem, we propose a unified detection model (both for abrupt and all types of gradual transitions), as well as an implementation whose results improve upon those of all the inspected reports. The innovation of the approach presented here is centered on mapping the space of inter-frame distances onto a new space of decision better suited to achieving a sequence-independent thresholding. This mapping aims to consider frame ordering information within the thresholding process; it is based on the parametric modeling of the patterns that transitions generate on the distances' output. As opposed to most reviewed works, our results are detailed over a large and representative sample of more than 1500 cuts and 250 gradual transitions, which make up a significant part (200 min) of the MPEG-7 testing material; this ensures a high degree of confidence in the validity of our approach.
C1 Univ Autonoma Madrid, Image Proc Grp, E-28049 Madrid, Spain.
   Univ Politecn Madrid, Image Proc Grp, E-28040 Madrid, Spain.
C3 Autonomous University of Madrid; Universidad Politecnica de Madrid
RP Univ Autonoma Madrid, Image Proc Grp, E-28049 Madrid, Spain.
EM J.Bescos@uam.es; gcisneros@ssr.upm.es; JoseM.Martinez@uam.es;
   jmmenendez@ssr.upm.es; Julian.Cabrera@gti.ssr.upm.es
RI QUESADA, JULIAN CABRERA/Y-7544-2019; Menendez, Jose-Manuel/L-1159-2014;
   Menéndez, José Manuel/AAS-8430-2020; Martinez, Jose/A-1185-2008; Bescos,
   Jesus/C-4327-2014
OI QUESADA, JULIAN CABRERA/0000-0002-7154-2451; Menendez,
   Jose-Manuel/0000-0003-0584-2250; Menéndez, José
   Manuel/0000-0003-0584-2250; Martinez, Jose/0000-0002-2236-1769; Bescos,
   Jesus/0000-0001-6238-6859
CR Akutsu A., 1994, Proceedings ACM Multimedia '94, P349, DOI 10.1145/192593.192697
   [Anonymous], 1995, P ACM MULT, DOI DOI 10.1145/217279.215266
   [Anonymous], 1991, P VDB
   [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   ARDIZZONE E, 1996, P ICIP 96, V3, P831
   Arman F., 1994, MULTIMEDIA SYST, V1, P211, DOI DOI 10.1007/BF01268945
   BESCOS J, 2000, P IEEE SW S IM AN IN, P53
   BESCOS J, 2002, P ICIP 2002 ROCH NY
   BESCOS J, 2000, P ICIP 2000 VANC BC, P10
   Boreczky JS, 1996, P SOC PHOTO-OPT INS, V2670, P170, DOI 10.1117/12.234794
   Bouthemy P, 1999, IEEE T CIRC SYST VID, V9, P1030, DOI 10.1109/76.795057
   DAILIANAS A, 1995, P SPIE C INT ISS LAR
   Ford RM, 2000, MULTIMEDIA SYST, V8, P37, DOI 10.1007/s005300050003
   Hampapur A., 1995, Multimedia Tools and Applications, V1, P9, DOI 10.1007/BF01261224
   Hanjalic A, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P710, DOI 10.1109/MMCS.1999.778571
   KASTURI R, 1991, COMPUTER VISION PRIN, P469
   Kim H, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P827, DOI 10.1109/MMCS.1999.779308
   Koprinska I, 2001, SIGNAL PROCESS-IMAGE, V16, P477, DOI 10.1016/S0923-5965(00)00011-4
   Little T. D. C., 1993, Proceedings ACM Multimedia 93, P427, DOI 10.1145/166266.168450
   Martínez JM, 2002, IEEE MULTIMEDIA, V9, P78, DOI 10.1109/93.998074
   *MPEG, 1998, MPEG ROM M DEC
   *MPEG, 1998, MPEG ATL CIT M OCT
   Naphade MR, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P884, DOI 10.1109/ICIP.1998.723662
   Ngo CW, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P750, DOI 10.1109/MMCS.1999.779293
   Otsuji K., 1993, Proceedings ACM Multimedia 93, P251, DOI 10.1145/166266.166295
   SHAHRARAY B, 1995, P SOC PHOTO-OPT INS, V2419, P2, DOI 10.1117/12.206348
   SONG SM, 1998, P SOC PHOTO-OPT INS, V3312, P404
   Ueda H., 1991, P SIGCHI C HUMAN FAC, P343
   WACTLAR HD, 1997, SMPTE J, P524
   WEI J, 1998, P SOC PHOTO-OPT INS, V3312, P188
   Xiong W, 1998, COMPUT VIS IMAGE UND, V71, P166, DOI 10.1006/cviu.1998.0711
   YEO BL, 1995, P INT C MULT COMP SY, P81
   Yu H, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL II, P498, DOI 10.1109/ICIP.1997.638817
   Yu H.H., 1998, PROC C STORAGE RETRI, V3312, P176
   YUSOFF Y, 1999, P IEEE MULT SYST FLO, V1, P700
   ZHANG HJ, 1995, P SOC PHOTO-OPT INS, V2417, P389, DOI 10.1117/12.206066
NR 36
TC 53
Z9 64
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2005
VL 7
IS 2
BP 293
EP 307
DI 10.1109/TMM.2004.840598
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 909OO
UT WOS:000227869400011
DA 2024-07-18
ER

PT J
AU Su, K
   Kundur, D
   Hatzinakos, D
AF Su, K
   Kundur, D
   Hatzinakos, D
TI Spatially localized image-dependent watermarking for statistical
   invisibility and collusion resistance
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE image feature extraction; linear collusion; robust digital video
   watermarking; statistical invisibility
AB In this paper, we develop a novel video watermarking framework based on the collusion-resistant design rules formulated in a companion paper. We propose to employ a spatially-localized image dependent approach to create a watermark whose pairwise frame correlations approximate those of the host video. To characterize the spread of its spatially-localized energy distribution, the notion of a watermark footprint is introduced. Then we explain how a particular type of image dependent footprint structure, comprised of subframes centered around a set of visually significant anchor points, can lead to two advantageous results: pairwise watermark frame correlations that more closely match those of the host video for statistical invisibility, and the ability to apply image watermarks directly to a frame sequence without sacrificing collusion-resistance. In the ensuing overview of the proposed video watermark, two new ideas are put forward: synchronizing the sub-frame locations using visual content rather than structural markers and exploiting the inherent spatial diversity of the subframe-based watermark to improve detector performance. Simulation results are presented to show that the proposed scheme provides improved resistance to linear frame collusion, while still being embedded and extracted using relatively low complexity frame-based algorithms.
C1 Univ Cambridge, Commun Engn Lab, Cambridge, England.
   Univ Toronto, Dept Elect & Comp Engn, Toronto, ON, Canada.
C3 University of Cambridge; University of Toronto
RP Univ Cambridge, Commun Engn Lab, Cambridge, England.
EM deepa@ee.tanm.edu
CR BLACK J, 1997, 31 AS C SIGN SYST CO, V1, P315
   BRISBANE G, 1999, LECT NOTES COMPUTER, V1729, P425
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Darmstaedter V, 1998, LECT NOTES COMPUT SC, V1425, P190
   DEGNILLAUME F, 2000, P SOC PHOTO-OPT INS, V3971, P346
   Fridrich J, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P536, DOI 10.1109/MMCS.1999.778542
   FRIDRICH J, 2000, P SPIE SEC WAT MUL 3, V4314, P286
   Hartung F, 1998, SIGNAL PROCESS, V66, P283, DOI 10.1016/S0165-1684(98)00011-5
   Hartung F, 1998, COMPUT GRAPH, V22, P425, DOI 10.1016/S0097-8493(98)00032-6
   Kalker T, 1999, PROC SPIE, V3657, P103, DOI 10.1117/12.344661
   Kundur D, 2001, IEEE T SIGNAL PROCES, V49, P2383, DOI 10.1109/78.950793
   KUNDUR D, 1999, P IEEE INT C IM PROC, V4, P240
   LANE T, JPEG IMAGE COMPRES 1
   Manjunath BS, 1996, PATTERN RECOGN, V29, P627, DOI 10.1016/0031-3203(95)00115-8
   Mobasseri BG, 1999, P SOC PHOTO-OPT INS, V3657, P96, DOI 10.1117/12.344660
   PEI SC, 1995, IMAGE VISION COMPUT, V13, P711, DOI 10.1016/0262-8856(95)98753-G
   Pereira S, 2000, IEEE T IMAGE PROCESS, V9, P1123, DOI 10.1109/83.846253
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Petitcolas FAP, 1998, LECT NOTES COMPUT SC, V1525, P218
   Qiao LT, 1998, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS, P276, DOI 10.1109/MMCS.1998.693656
   SU JK, 2000, P EUR SIGN PROC C
   Su K, 2005, IEEE T MULTIMEDIA, V7, P52, DOI 10.1109/TMM.2004.840614
   SU K, 2001, THESIS U TORONTO TOR
   SU PC, 1999, P IS T PROC IM QUAL
   Swanson MD, 1998, IEEE J SEL AREA COMM, V16, P540, DOI 10.1109/49.668976
   Termont P, 2000, IEEE IMAGE PROC, P407
   TorkamaniAzar F, 1996, IEEE T IMAGE PROCESS, V5, P1573, DOI 10.1109/83.541427
   Trucco E., 1998, Introductory techniques for 3-D computer vision, V201
   Vasconcelos N, 2000, IEEE T IMAGE PROCESS, V9, P3, DOI 10.1109/83.817595
   Voloshynovskiy S, 2000, LECT NOTES COMPUT SC, V1768, P211
   Voloshynovskiy S, 2001, SIGNAL PROCESS, V81, P1177, DOI 10.1016/S0165-1684(01)00039-1
   Zhu WW, 1999, IEEE T CIRC SYST VID, V9, P545, DOI 10.1109/76.767121
NR 32
TC 17
Z9 28
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2005
VL 7
IS 1
BP 52
EP 66
DI 10.1109/TMM.2004.840614
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 893CQ
UT WOS:000226697300006
DA 2024-07-18
ER

PT J
AU Wang, PC
   Chan, CT
   Hu, SC
   Lee, CL
   Tseng, WC
AF Wang, PC
   Chan, CT
   Hu, SC
   Lee, CL
   Tseng, WC
TI High-speed packet classification for differentiated services in
   next-generation networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE high-speed network; internet; IP address lookup; packet classification
AB In next-generation networks, packet classification is important in fulfilling the requirements of multimedia services, including VoIP and VoD. Using pre-defined filters, the incoming packets can be categorized that determines to which forwarding class a packet belongs. Packet classification is essentially a problem of multidimensional range matching. The tuple space search is a well-known solution based on multiple hash accesses for various filter length combinations. The tuple-based algorithm, a rectangle search, is highly scalable with respect to the number of filters; however, it suffers from the memory-explosion problem. Besides, the lookup performance of the rectangle search is not sufficiently fast to accomplish high-speed packet classification. This work proposes an improved scheme to reduce the required storage and realize OC-192 wire-speed forwarding. The scheme consists of two parts. The "Tuple Reduction Algorithm" drastically reduces the number of tuples by duplicating filters. Dynamic programming is used to optimize the tuple reduction and two heuristic approaches are introduced to simplify the optimization process. Furthermore, the "Look-ahead Caching" scheme is presented to improve the lookup performance. The basic idea is to prevent unnecessary tuple probing by filtering out the "un-matched" situation of the incoming packet. The experimental results show that combining the tuple reduction algorithm with look-ahead caching increases the lookup speed by a factor of six while requiring only around one third of the storage. Additionally, an extension of multiple fields to more general filters is addressed.
C1 Chunghwa Telecom Co Ltd, Telecommun Labs, Taipei, Taiwan.
   Ming Hsin Univ Sci & Technol, Dept Informat Management, Hsinchu, Taiwan.
C3 Chunghwa Telecom
RP Chunghwa Telecom Co Ltd, Telecommun Labs, Taipei, Taiwan.
EM abu@cht.com.tw; ctchan@cht.com.tw; schu@mis.must.edu.tw;
   chlilee@cht.com.tw; wctseng@cht.com.tw
RI Wang, Pi-Chung/AAQ-3600-2020
OI Wang, Pi-Chung/0000-0002-4220-2853
CR BUDDHIKOT M, 2000, P IFIP 6 INT WORKSH
   DEGERMARK M, 1997, P ACM SIGCOMM, P3
   Feldman A., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P1193, DOI 10.1109/INFCOM.2000.832493
   Gupta P, 2001, IEEE NETWORK, V15, P2
   Gupta P, 1999, COMP COMM R, V29, P147, DOI 10.1145/316194.316217
   Gupta P., 1999, HOT INTERCONNECTS 7, V40
   Gupta P., 1999, P IEEE INFOCOM MAR, P1240
   Huang NF, 1999, IEEE J SEL AREA COMM, V17, P1093
   LAKSHMAN TV, 1998, P ACM SIGCOMM, P203
   *NAT LAB APPL NETW, NLANR PROJ
   Nilsson S, 1999, IEEE J SEL AREA COMM, V17, P1083, DOI 10.1109/49.772439
   Srinivasan V, 1999, ACM T COMPUT SYST, V17, P1, DOI 10.1145/296502.296503
   Srinivasan V, 1999, COMP COMM R, V29, P135, DOI 10.1145/316194.316216
   Srinivasan V., 1998, P IEEEACM SIGCOMM, P191
   Waldvogel Marcel., 1997, Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication, SIGCOMM '97, P25
   Woo T. Y. C., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P1213, DOI 10.1109/INFCOM.2000.832499
   Yu DX, 1999, GLOBECOM'99: SEAMLESS INTERCONNECTION FOR UNIVERSAL SERVICES, VOL 1-5, P1556, DOI 10.1109/GLOCOM.1999.830041
NR 17
TC 23
Z9 24
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2004
VL 6
IS 6
BP 925
EP 935
DI 10.1109/TMM.2004.837263
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 872QU
UT WOS:000225224200014
DA 2024-07-18
ER

PT J
AU Tavanapong, W
   Zhou, JY
AF Tavanapong, W
   Zhou, JY
TI Shot clustering techniques for story browsing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content-based indexing and retrieval; feature extraction; scene
   segmentation; video browsing
ID VIDEO-RETRIEVAL; REPRESENTATION
AB Automatic video segmentation is the first and necessary step for organizing a long video file into several smaller units. The smallest basic unit is a shot. Relevant shots are typically grouped into a high-level unit called a scene. Each scene is part of a story. Browsing these scenes unfolds the entire story of a film, enabling users to locate their desired video segments quickly and efficiently.
   Existing scene definitions are rather broad, making it difficult to compare the performance of existing techniques and to develop a better one. This paper introduces a stricter scene definition for narrative films and presents ShotWeave, a novel technique for clustering relevant shots into a scene using the stricter definition. The crux of ShotWeave is its feature extraction and comparison. Visual features are extracted from selected regions of representative frames of shots. These regions capture essential information needed to maintain viewers' thought in the presence of shot breaks. The new feature comparison is developed based on common continuity-editing techniques used in film making. Experiments were performed on full-length films with a wide range of camera motions and a complex composition of shots. The experimental results show that ShotWeave outperforms two recent techniques utilizing global visual features in terms of segmentation accuracy and time.
C1 Iowa State Univ, Dept Comp Sci, Ames, IA 50011 USA.
C3 Iowa State University
RP Iowa State Univ, Dept Comp Sci, Ames, IA 50011 USA.
EM tavanapo@cs.iastate.edu
CR Adams B, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P283, DOI 10.1109/ICIP.2000.899358
   AIGRAIN P, 1994, COMPUT GRAPH, V18, P93, DOI 10.1016/0097-8493(94)90120-1
   BIMBO AD, 1999, CONTENT BASED VIDEO
   Bordwell D., 1997, FILM ART INTRO
   Corridoni JM, 1998, PATTERN RECOGN, V31, P2027, DOI 10.1016/S0031-3203(98)00061-2
   DAWOOD A, 1999, P IEE INT C IM PROC, V1, P285
   Ferman AM, 1997, P SOC PHOTO-OPT INS, V3229, P23, DOI 10.1117/12.290352
   Gamaz N, 1998, 1998 IEEE SOUTHWEST SYMPOSIUM ON IMAGE ANALYSIS AND INTERPRETATION, P12, DOI 10.1109/IAI.1998.666852
   Girgensohn A, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P756, DOI 10.1109/MMCS.1999.779294
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P580, DOI 10.1109/76.767124
   Lin T, 2000, INT C PATT RECOG, P39, DOI 10.1109/ICPR.2000.902860
   Nang J, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P23, DOI 10.1145/319463.320047
   OH J, 2000, ACM SIGMOD MANAGEMEN, P415
   Rui Y, 1999, MULTIMEDIA SYST, V7, P359, DOI 10.1007/s005300050138
   SHIN T, 1998, P IEEE INT S CIRC SY, V4, P253
   Sundaram H., 2000, Proceedings ACM Multimedia 2000, P95, DOI 10.1145/354384.354440
   Sundaram H, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1145, DOI 10.1109/ICME.2000.871563
   Veneau E, 2000, INT C PATT RECOG, P254, DOI 10.1109/ICPR.2000.902907
   Wolf W, 1996, INT CONF ACOUST SPEE, P1228, DOI 10.1109/ICASSP.1996.543588
   Xiong W, 1997, MACH VISION APPL, V10, P51, DOI 10.1007/s001380050059
   Yeo BL, 1995, IEEE T CIRC SYST VID, V5, P533, DOI 10.1109/76.475896
   Yeung MM, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pA338
   Zhang HJ, 1997, PATTERN RECOGN, V30, P643, DOI 10.1016/S0031-3203(96)00109-4
   ZHANG HJ, 1993, ACM MULTIMEDIA SYSTE, V1, P10
   Zhuang YT, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P866, DOI 10.1109/ICIP.1998.723655
NR 25
TC 62
Z9 66
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2004
VL 6
IS 4
BP 517
EP 527
DI 10.1109/tmm.2004.830810
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 838RC
UT WOS:000222729800001
DA 2024-07-18
ER

PT J
AU Li, J
   Sun, HH
AF Li, J
   Sun, HH
TI On interactive browsing of large images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE cache; interactive browsing; JPEG 2000; media access; Virtual media
   (Vmedia); streaming
ID COMPRESSION; JPEG-2000
AB A new effective mechanism is proposed for the browsing of large compressed images over the Internet. The image is compressed with the JPEG 2000 into one single bitstream and put on the server. During the browsing process, the user specifies a region of interest (ROI) with certain spatial and resolution constraint. The browser only downloads the portion of the compressed bitstream that covers the current ROI, and the download is performed in a progressive fashion so that a coarse view of the ROI can be rendered very quickly and then gradually refined as more and more bitstream arrives. In the case of the switch of ROI, e.g., zooming in/out or panning around, the browser uses existing compressed bitstream in cache to quickly render a coarse view of the new ROI, and in the same time, request a new set of compressed bitstream corresponding to the updated view. The system greatly improves the experience of browsing large images over the slow networks.
C1 Microsoft Res, Commun Collaborat & Signal Proc Grp, Redmond, WA 98052 USA.
   Microsoft Res Asia, Beijing, Peoples R China.
C3 Microsoft; Microsoft; Microsoft Research Asia
RP Li, J (corresponding author), Microsoft Res, Commun Collaborat & Signal Proc Grp, Redmond, WA 98052 USA.
CR Jao C S, 1999, IEEE Trans Inf Technol Biomed, V3, P70, DOI 10.1109/4233.748977
   *JPEG, 1999, JPEG 2000 VER MOD 5
   MARCELLIN MW, JPEG 2000 IMAGE COMP
   Pennebaker W. B., 1993, JPEG: Still image data compression standard
   Skodras A, 2001, IEEE SIGNAL PROC MAG, V18, P36, DOI 10.1109/79.952804
   Taubman D, 2000, IEEE T IMAGE PROCESS, V9, P1158, DOI 10.1109/83.847830
   Usevitch BE, 2001, IEEE SIGNAL PROC MAG, V18, P22, DOI 10.1109/79.952803
   *W3C MIT, 1999, HTTP11 W3CMIT
   ZHANG C, 2001, P SPIE VIS COMM IM P, V4310
NR 9
TC 17
Z9 20
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2003
VL 5
IS 4
BP 581
EP 590
DI 10.1109/TTM.2003.813284
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 742VA
UT WOS:000186537700008
DA 2024-07-18
ER

PT J
AU Cheung, G
   McCanne, S
AF Cheung, G
   McCanne, S
TI A framework for computation-memory algorithmic optimization for signal
   processing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE computation theory; memory management; packet switching; signal
   processing; vector quantization
ID CODE GENERATION
AB The heterogeneity of today's computing environment means computation-intensive signal processing algorithms, must be optimized for performance in a machine dependent fashion. In this paper, we present a dynamic memory model and associated optimization framework that finds a machine-dependent, near-optimal implementation of an algorithm by exploiting the computation-memory tradeoff. By. optimal, we mean an implementation that has the fastest running time given the specification of the machine memory hierarchy. We discuss two instantiations of the framework: fast IP address lookup, and fast nonuniform scalar quantizer and unstructured vector quantizer encoding. Experiments show that both instantiations outperform techniques that ignore this computation-memory tradeoff.
C1 HP Labs Japan, Tokyo 1680072, Japan.
   Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
C3 University of California System; University of California Berkeley
RP Cheung, G (corresponding author), HP Labs Japan, Tokyo 1680072, Japan.
RI Cheung, Gene/AAB-9284-2020
OI Cheung, Gene/0000-0002-5571-4137
CR Bhattacharyya SS, 2000, IEEE T CIRCUITS-II, V47, P849, DOI 10.1109/82.868454
   CHEUNG G, 1999, INFOCOM 99       MAR
   CHEUNG G, 2000, UCBCSD991085
   CHEUNG G, 1999, ICNP 99          NOV
   Degermark Mikael., 1997, SMALL FORWARDING TAB, P3
   ENGLER DR, 1996, PLDI 96
   ENGLER DR, 1994, ASPLOS 94
   GEBOTYS C, 1996, P CUST INT CIRC C 19
   Gebotys CH, 1999, IEEE T COMPUT AID D, V18, P726, DOI 10.1109/43.766724
   Gersho A., 2003, Vector Quantization and Signal Compression
   Goyal VK, 1997, INT CONF ACOUST SPEE, P2729, DOI 10.1109/ICASSP.1997.595353
   GUAN L, 1992, PATTERN RECOGN LETT, V13, P693, DOI 10.1016/0167-8655(92)90098-K
   HUANG M, 2001, ISLPED 2001
   LENGWEHASATIT K, 1997, ICASSP 97
   NILSSON S, 1998, INT C BROADBAND  APR
   Patterson D.A., 1997, COMPUTER ORG DESIGN, VSecond
   SHOHAM Y, 1988, IEEE T ACOUST SPEECH, V36, P1445, DOI 10.1109/29.90373
   SKLOWER K, TREE BASED ROUTING T
   SRINIVASAN V, ACM SIGMETRICS 98
   WALDVOGEL M, 1997, SIGCOMM 97
NR 20
TC 0
Z9 0
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2003
VL 5
IS 2
BP 174
EP 185
DI 10.1109/TMM.2003.811625
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 695HB
UT WOS:000183824100003
DA 2024-07-18
ER

PT J
AU Cai, YQ
   Ma, ZW
   Lu, CH
   Wang, CB
   He, GQ
AF Cai, Yiqing
   Ma, Zhenwei
   Lu, Changhong
   Wang, Changbo
   He, Gaoqi
TI Global Representation Guided Adaptive Fusion Network for Stable Video
   Crowd Counting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive fusion; crowd counting; global temporal representation;
   spatio-temporal consistency; video understanding
ID RECOGNITION; PEOPLE
AB Modern crowd counting methods in natural scenes, even when video datasets are available, are mostly based on images. Because of background interference or occlusion in the scene, these methods can easily lead to mutations and instability in density prediction. There has been minimal research on how to exploit the inherent consistency among adjacent frames to achieve high estimation accuracy of video sequences. In this study, we explore the long-term global temporal consistency in the video sequence and propose a novel Global Representation Guided Adaptive Fusion Network (GRGAF) for video crowd counting. The primary aim is to establish a long-term temporal representation among consecutive frames to guide the density estimation of local frames, which can alleviate the prediction instability caused by background noise and occlusions in crowd scenes. Moreover, in order to further enforce the temporal consistency, we apply the generative adversarial learning scheme and design a global-local joint loss, which can make the estimated density maps more temporally coherent. Extensive experiments on four challenging video-based crowd counting datasets (FDST, DroneCrowd, MALL and UCSD) demonstrate that our method makes effective use of spatio-temporal information of video and outperforms the other state-of-the-art approach.
C1 [Cai, Yiqing; Wang, Changbo] East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.
   [Ma, Zhenwei] East China Univ Sci & Technol, Sch Informat Sci & Engn, Shanghai 200237, Peoples R China.
   [Lu, Changhong] East China Normal Univ, Sch Math Sci, Shanghai 200062, Peoples R China.
   [He, Gaoqi] East China Normal Univ, Sch Comp Sci & Technol, Shanghai Key Lab Mental Hlth & Psychol Crisis Inte, Shanghai 200062, Peoples R China.
C3 East China Normal University; East China University of Science &
   Technology; East China Normal University; East China Normal University
RP He, GQ (corresponding author), East China Normal Univ, Sch Comp Sci & Technol, Shanghai Key Lab Mental Hlth & Psychol Crisis Inte, Shanghai 200062, Peoples R China.
EM 51194501002@stu.ecnu.edu.cn; 960901625@qq.com; chlu@math.ecnu.edu.cn;
   cbwang@cs.ecnu.edu.cn; gqhe@cs.ecnu.edu.cn
OI Cai, Yiqing/0000-0001-7315-0024; He, Gaoqi/0000-0001-8365-0970; Wang,
   Changbo/0000-0001-8940-6418
FU Natural Science Foundation of Shanghai [19ZR1415800]; National Natural
   Science Foundation of China [62002121, 62072183]; Science and Technology
   Commission of Shanghai Municipality [19JC1420100]; Research Project of
   Shanghai Science and Technology Commission [20DZ2260300]; Science
   Popularization Foundation of Shanghai [19DZ2301100]
FX This work was supported in part by the Natural Science Foundation of
   Shanghai under Grant 19ZR1415800, in part by the National Natural
   Science Foundation of China under Grant 62002121 and 62072183, in part
   by the Science and Technology Commission of Shanghai Municipality under
   Grant 19JC1420100, in part by The Research Project of Shanghai Science
   and Technology Commission under Grant 20DZ2260300, and in part by the
   Science Popularization Foundation of Shanghai under Grant 19DZ2301100.
CR An S, 2007, PROC CVPR IEEE, P1033
   Boominathan L, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P640, DOI 10.1145/2964284.2967300
   Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45
   Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569
   Chan AB, 2009, IEEE I CONF COMP VIS, P545, DOI 10.1109/ICCV.2009.5459191
   Chen K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.21
   Chen K, 2013, PROC CVPR IEEE, P2467, DOI 10.1109/CVPR.2013.319
   Cheng ZQ, 2019, IEEE I CONF COMP VIS, P6151, DOI 10.1109/ICCV.2019.00625
   Fang YY, 2020, NEUROCOMPUTING, V392, P98
   Fang YY, 2019, IEEE INT CON MULTI, P814, DOI 10.1109/ICME.2019.00145
   He GQ, 2019, IEEE INT CON MULTI, P1120, DOI 10.1109/ICME.2019.00196
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang SY, 2018, IEEE T IMAGE PROCESS, V27, P1049, DOI 10.1109/TIP.2017.2740160
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Ihaddadene N, 2008, INT C PATT RECOG, P217
   Jaderberg M, 2015, ADV NEUR IN, V28
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai WC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P202, DOI 10.1145/3394171.3413602
   Lempitsky V., 2010, P ADV NEUR INF PROC, V23, P1, DOI DOI 10.5555/2997189.2997337
   Li M, 2008, INT C PATT RECOG, P1998
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Li YK, 2018, IEEE T MULTIMEDIA, V20, P3289, DOI 10.1109/TMM.2018.2834873
   Liu LB, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P849
   Liu N, 2019, PROC CVPR IEEE, P3220, DOI 10.1109/CVPR.2019.00334
   Liu WZ, 2019, IEEE INT C INT ROBOT, P244, DOI [10.1109/iros40897.2019.8967852, 10.1109/IROS40897.2019.8967852]
   Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524
   Liu Xinyan, 2021, P IEEECVF INT C COMP, P3215
   Ma YJ, 2022, IEEE T MULTIMEDIA, V24, P261, DOI 10.1109/TMM.2021.3050059
   Ma ZH, 2021, AAAI CONF ARTIF INTE, V35, P2319
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Miao YQ, 2019, PATTERN RECOGN LETT, V125, P113, DOI 10.1016/j.patrec.2019.04.012
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Oñoro-Rubio D, 2016, LECT NOTES COMPUT SC, V9911, P615, DOI 10.1007/978-3-319-46478-7_38
   Sam DB, 2017, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2017.429
   Saxena S, 2008, LECT NOTES COMPUT SC, V5259, P970, DOI 10.1007/978-3-540-88458-3_88
   Shen Z, 2018, PROC CVPR IEEE, P5245, DOI 10.1109/CVPR.2018.00550
   Shi XJ, 2015, ADV NEUR IN, V28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sindagi VA, 2019, IEEE I CONF COMP VIS, P1221, DOI 10.1109/ICCV.2019.00131
   Song QY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3345, DOI 10.1109/ICCV48922.2021.00335
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan J, 2021, PROC CVPR IEEE, P1974, DOI 10.1109/CVPR46437.2021.00201
   Wang B., 2020, Advances in Neural Information Processing Systems, V33, P1595, DOI DOI 10.48550/ARXIV.2009.13077
   Wang MJ, 2023, IEEE T MULTIMEDIA, V25, P2074, DOI 10.1109/TMM.2022.3142398
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Weizhe Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P723, DOI 10.1007/978-3-030-58555-6_43
   Wen LY, 2021, PROC CVPR IEEE, P7808, DOI 10.1109/CVPR46437.2021.00772
   Wu XJ, 2020, NEUROCOMPUTING, V403, P13, DOI 10.1016/j.neucom.2020.04.071
   Xiong F, 2017, IEEE I CONF COMP VIS, P5161, DOI 10.1109/ICCV.2017.551
   Xu ML, 2018, IEEE T CIRC SYST VID, V28, P2814, DOI 10.1109/TCSVT.2017.2731866
   Xu Y., 2021, P IEEE CVF INT C COM, P15570
   Zeng LK, 2017, IEEE IMAGE PROC, P465, DOI 10.1109/ICIP.2017.8296324
   Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zhang ZX, 2015, NEUROCOMPUTING, V166, P151, DOI 10.1016/j.neucom.2015.03.083
   Zou Z., 2019, BMVC, P250
   Zou ZK, 2018, IEEE ACCESS, V6, P60745, DOI 10.1109/ACCESS.2018.2875495
NR 59
TC 1
Z9 1
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5222
EP 5233
DI 10.1109/TMM.2022.3189246
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300044
DA 2024-07-18
ER

PT J
AU Chang, ZY
   Chan, SHG
AF Chang, Zhangyu
   Chan, S. -H. Gary
TI Bi-Criteria Approximation for a Multi-Origin Multi-Channel Auto-Scaling
   Live Streaming Cloud
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Auto-scaling; bi-criteria approximation; live streaming cloud; multiple
   origins; multiple channels; overlay optimization
ID SERVICE; ALLOCATION
AB Live video traffic has been widely observed to vary significantly within short timescale. In order to manage such traffic dynamic of overlay live streaming, the Content Provider (CP) may deploy a set of geo-dispersed auto-scaling servers where the pay-as-you-go deployment cost is charged by the amount of resources used due to server uploading and data transmission between servers. To support geo-distributed user demands, we study a novel multi-origin multi-channel auto-scaling live streaming cloud that pushes each channel stream in the core network overlay as a tree covering the end servers who have local demand for the channel. The Origin-to-End (O2E) delay from an origin to an end server is due to the Server-to-Server (S2S) delays of the overlay links along the path. By optimizing the overlay of the core network, we seek to minimize the deployment cost and O2E delays of the channels (i.e., a bi-criteria problem), which can be equivalently phrased as minimizing the deployment cost while meeting certain given maximum O2E delay constraints. We formulate a realistic problem capturing the major cost and delay components, and show its NP-hardness. We propose Cost-optimized Multi-Origin Multi-Channel Overlay Streaming (COCOS), a novel, efficient and near-optimal bi-criteria approximation algorithm with proven approximation ratio. Trace-driven extensive experimental results based on real-world live streaming service data validate that COCOS outperforms other state-of-the-art schemes by a wide margin (cutting the cost in general by more than 50%).
C1 [Chang, Zhangyu; Chan, S. -H. Gary] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Kowloon, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Chang, ZY (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Kowloon, Hong Kong, Peoples R China.
EM zchang@cse.ust.hk; gchan@cse.ust.hk
OI Chang, Zhangyu/0000-0002-1069-4048; Chan, Gary Shueng
   Han/0000-0003-4207-764X
FU Hong Kong General Research Fund [16200120]
FX This work was supported in part by Hong Kong General Research Fund under
   Grant 16200120. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Dr. Ali C. Begen.
   (Corresponding author: Zhangyu Chang.)
CR [Anonymous], 2014, P 16 INT TEL NETW ST
   [Anonymous], 2012, PROC 22 INT WORKSHOP
   [Anonymous], 2014, WORKSHOP DESIGN QUAL
   Azarpira H, 2012, 2012 SIXTH INTERNATIONAL SYMPOSIUM ON TELECOMMUNICATIONS (IST), P644, DOI 10.1109/ISTEL.2012.6483067
   Barnett T., 2019, Cisco visual networking index (VNI) global and americas/EMEAR mobile data traffic forecast, 2017-2022
   Bentaleb A, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3371040
   Budhkar S, 2020, PEER PEER NETW APPL, V13, P190, DOI 10.1007/s12083-019-00755-x
   Chang ZY, 2021, IEEE T MULTIMEDIA, V23, P3714, DOI 10.1109/TMM.2020.3031068
   Dai J, 2015, IEEE ICC, P6959, DOI 10.1109/ICC.2015.7249435
   Ding C., 2012, PROC IEEE 20 INT WOR, P1
   Fernando T, 2013, INT CONF ADV ICT, P160, DOI 10.1109/ICTer.2013.6761172
   Gabow HN, 1998, MATH PROGRAM, V82, P83, DOI 10.1007/BF01585866
   Haouari F., 2019, IEEE ICC, P1, DOI DOI 10.1109/icc.2019.8761591
   Hu C, 2012, PEER PEER NETW APPL, V5, P312, DOI 10.1007/s12083-012-0140-z
   Hu SH, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3328997
   Irondi I, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3269494
   Jannapureddy R, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9071417
   Jin X, 2010, HANDBOOK OF PEER-TO-PEER NETWORKING, P117, DOI 10.1007/978-0-387-09751-0_5
   Jin X, 2009, IEEE T MULTIMEDIA, V11, P1024, DOI 10.1109/TMM.2009.2021804
   Kraft D, 1988, Technical report, Technical Report DFVLR-FB 88-28
   Kucharzak M., 2013, P 15 INT C TRANSP OP, P1
   Liao X., 2006, P IEEE INFOCOM, P1
   Liao XF, 2007, IEEE T PARALL DISTR, V18, P1663, DOI 10.1109/TPDS.2007.70708
   Liu J, 2013, INT SYMP ASYNCHRON C, P1, DOI 10.1109/ASYNC.2013.29
   Lombardi F, 2019, FUTURE GENER COMP SY, V98, P342, DOI 10.1016/j.future.2019.03.003
   Lu ZH, 2011, INT C PAR DISTRIB SY, P581, DOI 10.1109/ICPADS.2011.113
   Magharei N, 2014, IEEE ACM T NETWORK, V22, P244, DOI 10.1109/TNET.2013.2257840
   Magharei N, 2009, IEEE ACM T NETWORK, V17, P1052, DOI 10.1109/TNET.2008.2007434
   Marathe MV, 1998, J ALGORITHM, V28, P142, DOI 10.1006/jagm.1998.0930
   Naor JS, 1997, ANN IEEE SYMP FOUND, P536, DOI 10.1109/SFCS.1997.646142
   Niño-Mora J, 2019, COMPUT OPER RES, V103, P221, DOI 10.1016/j.cor.2018.11.012
   Ren DN, 2009, IEEE T MULTIMEDIA, V11, P1446, DOI 10.1109/TMM.2009.2032677
   Rui-Xiao Zhang, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P3678, DOI 10.1145/3394171.3413918
   Singh R, 2021, PROCEEDINGS OF THE 18TH USENIX SYMPOSIUM ON NETWORKED SYSTEM DESIGN AND IMPLEMENTATION, P201
   Tan B, 2013, IEEE ACM T NETWORK, V21, P566, DOI 10.1109/TNET.2012.2208199
   Tan XR, 2005, 2005 SYSTEMS COMMUNICATIONS, PROCEEDINGS, P141, DOI 10.1109/ICW.2005.33
   Valliyammai C, 2019, ADV INTELL SYST, V755, P309, DOI 10.1007/978-981-13-1951-8_28
   Veloso E, 2006, IEEE ACM T NETWORK, V14, P133, DOI 10.1109/TNET.2005.863709
   Wittig A., 2018, Amazon Web Services in Action
   Wu C, 2008, IEEE INFOCOM SER, P2029
   Wu D, 2009, IEEE INFOCOM SER, P2726, DOI 10.1109/INFCOM.2009.5062220
   Yarnagula HK, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3311749
   Yun S, 2015, PEER PEER NETW APPL, V8, P631, DOI 10.1007/s12083-014-0278-y
   Zhang C., 2015, ACM Workshop on Network and Operating Systems Support for Digital Audio and Video, P55, DOI DOI 10.1145/2736084.2736091
   Zhang F, 2019, FUTURE GENER COMP SY, V98, P672, DOI 10.1016/j.future.2018.09.009
   Zhang RX, 2019, PROCEEDINGS OF THE 29TH ACM WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO (NOSSDAV'19), P55, DOI 10.1145/3304112.3325607
   Zhang RX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P420, DOI 10.1145/3343031.3351013
   Zhao H, 2019, MULTIMED TOOLS APPL, V78, P21827, DOI 10.1007/s11042-019-7457-z
   Zhao YH, 2014, IEEE INFOCOM SER, P298, DOI 10.1109/INFOCOM.2014.6847951
   Zhenyun Zhuang, 2011, 2011 IEEE 9th International Symposium on Parallel and Distributed Processing with Applications (ISPA), P183, DOI 10.1109/ISPA.2011.44
   Zhou F, 2013, INT CONF NETW SER, P161, DOI 10.1109/CNSM.2013.6727829
NR 51
TC 0
Z9 0
U1 2
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2839
EP 2850
DI 10.1109/TMM.2022.3152093
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600030
DA 2024-07-18
ER

PT J
AU Chen, T
   Yao, YZ
   Zhang, L
   Wang, Q
   Xie, GS
   Shen, FM
AF Chen, Tao
   Yao, Yazhou
   Zhang, Lei
   Wang, Qiong
   Xie, Guo-Sen
   Shen, Fumin
TI Saliency Guided Inter- and Intra-Class Relation Constraints for Weakly
   Supervised Semantic Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantic segmentation; weak supervision; saliency guidance; relation
   constraint
AB Weakly supervised semantic segmentation with only image-level labels aims to reduce annotation costs for the segmentation task. Existing approaches generally leverage class activation maps (CAMs) to locate the object regions for pseudo label generation. However, CAMs can only discover the most discriminative parts of objects, thus leading to inferior pixel-level pseudo labels. To address this issue, we propose a saliency guided Inter- and Intra-Class Relation Constrained ((ICRC)-C-2) framework to assist the expansion of the activated object regions in CAMs. Specifically, we propose a saliency guided class-agnostic distance module to pull the intra-category features closer by aligning features to their class prototypes. Further, we propose a class-specific distance module to push the inter-class features apart and encourage the object region to have a higher activation than the background. Besides strengthening the capability of the classification network to activate more integral object regions in CAMs, we also introduce an object guided label refinement module to take a full use of both the segmentation prediction and the initial labels for obtaining superior pseudo-labels. Extensive experiments on PASCAL VOC 2012 and COCO datasets demonstrate well the effectiveness of I-2 CRC over other state-of-the-art counterparts.
C1 [Chen, Tao; Yao, Yazhou; Wang, Qiong; Xie, Guo-Sen] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Zhang, Lei] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.
   [Shen, Fumin] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
C3 Nanjing University of Science & Technology; Northwestern Polytechnical
   University; University of Electronic Science & Technology of China
RP Wang, Q; Xie, GS (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
EM taochen@njust.edu.cn; yazhou.yao@njust.edu.cn; nwpuzhanglei@nwpu.edu.cn;
   wangq@njust.edu.cn; gsxiehm@gmail.com; fumin.shen@gmail.com
RI Shen, Fumin/R-2121-2016; Chen, Tao/ABB-5983-2022
OI Wang, Qiong/0000-0003-4193-0960; Chen, Tao/0000-0001-8239-1698; xie,
   guosen/0000-0002-5487-9845; Yao, Yazhou/0000-0002-0337-9410
FU National Natural Science Foundation of China [62102182, 61976116,
   61905114, 61702163]; Natural Science Foundation of Jiangsu Province
   [BK20210327]; Fundamental Research Funds for the Central Universities
   [30920021135]; National Key R&D Program of China [2021YFF0602101]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 62102182, 61976116, 61905114, and 61702163, in part
   by the Natural Science Foundation of Jiangsu Province under Grant
   BK20210327, in part by the Fundamental Research Funds for the Central
   Universities under Grant 30920021135), and in part by the National Key
   R&D Program of China under Grant 2021YFF0602101.
CR Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231
   Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bearman Amy, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9911, P549, DOI 10.1007/978-3-319-46478-7_34
   Bin Jin, 2017, PROC CVPR IEEE, P1705, DOI 10.1109/CVPR.2017.185
   Chaudhry A., 2017, PROC BRIT MACH VIS C
   Chen L., 2020, P 16 EUR C COMP VIS, P347
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen T, 2022, IEEE T MULTIMEDIA, V24, P1042, DOI 10.1109/TMM.2021.3106095
   Chen T, 2022, IEEE T MULTIMEDIA, V24, P968, DOI 10.1109/TMM.2021.3061816
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan JS, 2020, PROC CVPR IEEE, P4282, DOI 10.1109/CVPR42600.2020.00434
   Fan Junsong, 2020, EUR C COMP VIS
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Guolei Sun, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P347, DOI 10.1007/978-3-030-58536-5_21
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong S, 2017, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2017.239
   Hou Q., 2018, ADV NEURAL INF PROCE, P549
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Huang ZL, 2018, PROC CVPR IEEE, P7014, DOI 10.1109/CVPR.2018.00733
   Jiang PT, 2019, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2019.00216
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Khoreva A, 2017, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2017.181
   Kim D, 2017, IEEE I CONF COMP VIS, P3554, DOI 10.1109/ICCV.2017.382
   Kolesnikov A, 2016, LECT NOTES COMPUT SC, V9908, P695, DOI 10.1007/978-3-319-46493-0_42
   Lee J, 2019, PROC CVPR IEEE, P5262, DOI 10.1109/CVPR.2019.00541
   Li KP, 2018, PROC CVPR IEEE, P9215, DOI 10.1109/CVPR.2018.00960
   Li XY, 2021, AAAI CONF ARTIF INTE, V35, P1984
   Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu CX, 2019, PROC CVPR IEEE, P82, DOI 10.1109/CVPR.2019.00017
   Liu Y, 2022, IEEE T PATTERN ANAL, V44, P1415, DOI 10.1109/TPAMI.2020.3023152
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ronneberger O, 2015, PROC INT C MED IMAGE, P1, DOI [10.1007/978-3-319-24574-4_28, DOI 10.1007/978-3-319-24574-4_28]
   Saleh F, 2016, LECT NOTES COMPUT SC, V9912, P413, DOI 10.1007/978-3-319-46484-8_25
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shimoda W, 2019, IEEE I CONF COMP VIS, P5207, DOI 10.1109/ICCV.2019.00531
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song CF, 2019, PROC CVPR IEEE, P3131, DOI 10.1109/CVPR.2019.00325
   Sun K., 2021, P IEEE CVF INT C COM, P7283
   Tianyi Zhang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P663, DOI 10.1007/978-3-030-58542-6_40
   Vernaza P, 2017, PROC CVPR IEEE, P2953, DOI 10.1109/CVPR.2017.315
   Wang QR, 2019, IEEE T MULTIMEDIA, V21, P1839, DOI 10.1109/TMM.2018.2890360
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang X, 2020, INT J COMPUT VISION, V128, P1736, DOI 10.1007/s11263-020-01293-3
   Wang X, 2018, PROC CVPR IEEE, P1354, DOI 10.1109/CVPR.2018.00147
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687
   Wei YC, 2018, PROC CVPR IEEE, P7268, DOI 10.1109/CVPR.2018.00759
   Wei YC, 2017, IEEE T PATTERN ANAL, V39, P2314, DOI 10.1109/TPAMI.2016.2636150
   Xie GS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7273, DOI 10.1109/ICCV48922.2021.00720
   Xie GS, 2021, PROC CVPR IEEE, P5471, DOI 10.1109/CVPR46437.2021.00543
   Yao YZ, 2021, PROC CVPR IEEE, P2623, DOI 10.1109/CVPR46437.2021.00265
   Yu-Ting Chang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8988, DOI 10.1109/CVPR42600.2020.00901
   Yude Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12272, DOI 10.1109/CVPR42600.2020.01229
   Zeng Y, 2019, IEEE I CONF COMP VIS, P7222, DOI 10.1109/ICCV.2019.00732
   ZHANG D, 2020, ADV NEURAL INFORM PR, V33, P655, DOI DOI 10.5555/3495724.3495780
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang XL, 2018, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2018.00144
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 67
TC 12
Z9 12
U1 4
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1727
EP 1737
DI 10.1109/TMM.2022.3157481
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, TI
   Liu, YC
   Su, HT
   Chang, YC
   Lin, YH
   Yeh, JF
   Chen, WC
   Hsu, WH
AF Chen, Tung-, I
   Liu, Yueh-Cheng
   Su, Hung-Ting
   Chang, Yu-Cheng
   Lin, Yu-Hsiang
   Yeh, Jia-Fong
   Chen, Wen-Chin
   Hsu, Winston H.
TI Dual-Awareness Attention for Few-Shot Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Object detection; Detectors; Correlation; Task
   analysis; Power capacitors; Adaptation models; Deep learning; object
   detection; visual attention; few-shot object detection
ID NETWORKS
AB While recent progress has significantly boosted few-shot classification (FSC) performance, few-shot object detection (FSOD) remains challenging for modern learning systems. Existing FSOD systems follow FSC approaches, ignoring critical issues such as spatial variability and uncertain representations, and consequently result in low performance. Observing this, we propose a novel Dual-Awareness Attention (DAnA) mechanism that enables networks to adaptively interpret the given support images. DAnA transforms support images into query-position-aware (QPA) features, guiding detection networks precisely by assigning customized support information to each local region of the query. In addition, the proposed DAnA component is flexible and adaptable to multiple existing object detection frameworks. By adopting DAnA, conventional object detection networks, Faster R-CNN and RetinaNet, which are not designed explicitly for few-shot learning, reach state-of-the-art performance in FSOD tasks. In comparison with previous methods, our model significantly increases the performance by 47% (+6.9 AP), showing remarkable ability under various evaluation settings.
C1 [Chen, Tung-, I; Liu, Yueh-Cheng; Su, Hung-Ting; Chang, Yu-Cheng; Lin, Yu-Hsiang; Yeh, Jia-Fong; Chen, Wen-Chin; Hsu, Winston H.] Natl Taiwan Univ, Taipei 106, Taiwan.
   [Hsu, Winston H.] Mobile Drive Technol, Taipei 236, Taiwan.
C3 National Taiwan University
RP Hsu, WH (corresponding author), Natl Taiwan Univ, Taipei 106, Taiwan.
EM r08922a09@ntu.edu.tw; yliu0610@gmail.com; htsu@cmlab.csie.ntu.edu.tw;
   vic85821@gmail.com; wl01154336@gmail.com; jiafongyeh@ieee.org;
   wcchen@csie.ntu.edu.tw; whsu@ntu.edu.tw
RI Su, Hung-Ting/HNC-5008-2023
OI HSU, WINSTON/0000-0002-3330-0638; Yeh, Jia-Fong/0000-0002-7512-9920
FU Ministry of Science and Technology, Taiwan [MOST 110-2634-F-002-026]
FX This work was supported in part by the Ministry of Science and
   Technology, Taiwan under Grant MOST 110-2634-F-002-026.
CR [Anonymous], 2013, ADV NEURAL INFORM PR
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chen H, 2018, Arxiv, DOI arXiv:1803.01529
   Dollár P, 2008, LECT NOTES COMPUT SC, V5303, P211, DOI 10.1007/978-3-540-88688-4_16
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Fan Q, 2021, PROC CVPR IEEE, P12283, DOI 10.1109/CVPR46437.2021.01211
   Fan Q, 2020, PROC CVPR IEEE, P4012, DOI 10.1109/CVPR42600.2020.00407
   Fan Z., 2020, P IEEE CVF C COMP VI, P9172, DOI [10.1109/cvpr42600.2020.00919, DOI 10.1109/CVPR42600.2020.00919]
   Finn C, 2017, PR MACH LEARN RES, V70
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gehring J, 2017, PR MACH LEARN RES, V70
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Han JW, 2018, IEEE T CIRC SYST VID, V28, P2473, DOI 10.1109/TCSVT.2017.2706264
   Hu H, 2019, IEEE I CONF COMP VIS, P3463, DOI 10.1109/ICCV.2019.00356
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jiaxi Wu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P456, DOI 10.1007/978-3-030-58517-4_27
   Jinlu Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P741, DOI 10.1007/978-3-030-58452-8_43
   Kang BY, 2019, IEEE I CONF COMP VIS, P8419, DOI 10.1109/ICCV.2019.00851
   Karlinsky L, 2019, PROC CVPR IEEE, P5192, DOI 10.1109/CVPR.2019.00534
   Koch G., 2015, ICML DEEP LEARNING W, V2
   Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1
   Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Li ZG, 2017, Arxiv, DOI arXiv:1707.09835
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu WD, 2020, PROC CVPR IEEE, P4164, DOI 10.1109/CVPR42600.2020.00422
   Luong MT, 2015, Arxiv, DOI arXiv:1508.04025
   Minghao Yin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P191, DOI 10.1007/978-3-030-58555-6_12
   Nichol A, 2018, Arxiv, DOI [arXiv:1803.02999, DOI 10.48550/ARXIV.1803.02999]
   Perez-Rua Juan-Manuel, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13843, DOI 10.1109/CVPR42600.2020.01386
   Qi G.-J., 2007, PROC IEEE C COMPUT V, P1
   Ravi S, 2016, OPTIMIZATION MODEL F
   Ren S., 2015, ADV NEURAL INFORM PR, V28, P91
   Snell J, 2017, ADV NEUR IN, V30
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang X., 2020, ICML, p2020b
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Yan XP, 2019, IEEE I CONF COMP VIS, P9576, DOI 10.1109/ICCV.2019.00967
   Yang Xiao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P192, DOI 10.1007/978-3-030-58520-4_12
   Yonglong Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P266, DOI 10.1007/978-3-030-58568-6_16
   Zhang DW, 2015, IEEE I CONF COMP VIS, P594, DOI 10.1109/ICCV.2015.75
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhou XY, 2019, Arxiv, DOI [arXiv:1904.07850, 10.48550/arXiv.1904.07850]
   Zhu XZ, 2019, IEEE I CONF COMP VIS, P6687, DOI 10.1109/ICCV.2019.00679
NR 55
TC 29
Z9 32
U1 30
U2 73
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 291
EP 301
DI 10.1109/TMM.2021.3125195
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 8C6OL
UT WOS:000917725300004
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Dang, YJ
   Huang, C
   Chen, P
   Liang, RH
   Yang, X
   Cheng, KT
AF Dang, Yuanjie
   Huang, Chong
   Chen, Peng
   Liang, Ronghua
   Yang, Xin
   Cheng, Kwang-Ting
TI Path-Analysis-Based Reinforcement Learning Algorithm for Imitation
   Filming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cinematography system; imitation filming; learning from demonstrations;
   unmanned aerial vehicles
ID ACTION RECOGNITION
AB Imitation filming has been applied to autonomous filming by mimicking human operators. To imitate the operation of cameramen when filming multiple human actions, existing methods plan the camera motion through time series prediction or train multiple models to handle a particular style in a specific situation. As a result, these methods require various settings to adapt to different scenarios. In this work, we overcome such limitations and propose an end-to-end imitation learning framework for drone cinematography systems. The framework consists of two main components: (1) an efficient motion feature extraction module for generating a compact motion feature space, (2) a path-analysis-based reinforcement learning (PABRL) algorithm for imitating multiple filming styles from demonstrations and incorporating aesthetical features for improved perspective shots. Our PABRL method is based on the actor-critic network, which regards multiple human motion variables, camera translations, and image composition as inputs and then outputs an aesthetical filming strategy related to the subject motion. In addition, we propose an attention mechanism and a long-short-term rewarding function to enhance the motion feature space and the integrity of the generated trajectory, respectively. Extensive experimental results in simulated and real outdoor environments demonstrate that compared with state-of-the-art methods, our method can achieve 69.8% higher performance in terms of trajectory planning accuracy while successfully incorporating aesthetical features into the captured videos.
C1 [Dang, Yuanjie; Chen, Peng; Liang, Ronghua] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310023, Peoples R China.
   [Huang, Chong] Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA.
   [Yang, Xin] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
   [Cheng, Kwang-Ting] Hong Kong Univ Sci & Technol, Sch Engn, Hong Kong, Peoples R China.
C3 Zhejiang University of Technology; University of California System;
   University of California Santa Barbara; Huazhong University of Science &
   Technology; Hong Kong University of Science & Technology
RP Chen, P (corresponding author), Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310023, Peoples R China.
EM dangyj@zjut.edu.cn; chonghuang@umail.ucsb.edu; chenpeng@zjut.edu.cn;
   rhliang@zjut.edu.cn; xinyang2014@hust.edu.cn; timcheng@ust.hk
OI Dang, Yuanjie/0000-0002-8302-1338; Chen, Peng/0000-0001-6122-0574;
   Cheng, Kwang-Ting Tim/0000-0002-3885-4912
FU Natural Science Foundation of China [U1909203, 62036009]; Zhejiang
   Provincial Natural Science Foundation of China [LQ22F020007]; Zhejiang
   Postdoctoral Science Foundation [ZY21191190003]; Ten Thousand Talent
   Program of Zhejiang Province
FX This work was supported in part by the Natural Science Foundation of
   China under Grants U1909203 and 62036009, in part by the Zhejiang
   Provincial Natural Science Foundation of China under Grant LQ22F020007,
   in part by the Zhejiang Postdoctoral Science Foundation under Grant
   ZY21191190003, and in part by the Ten Thousand Talent Program of
   Zhejiang Province.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], 2002, CARN MELL MOC DAT
   [Anonymous], 2018, DJI QUICKSH
   [Anonymous], 2015, DJI ACT TRACK
   Assa J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409068
   Bonatti R, 2020, SPR PROC ADV ROBOT, V11, P119, DOI 10.1007/978-3-030-33950-0_11
   Bonatti R, 2019, IEEE INT C INT ROBOT, P229, DOI [10.1109/IROS40897.2019.8968163, 10.1109/iros40897.2019.8968163]
   Bonatti R, 2020, J FIELD ROBOT, V37, P606, DOI 10.1002/rob.21931
   Bowen C. J., 2013, Grammar of the Shot
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen JH, 2016, PROC CVPR IEEE, P4688, DOI 10.1109/CVPR.2016.507
   Chen JH, 2015, IEEE WINT CONF APPL, P215, DOI 10.1109/WACV.2015.36
   Galvane Q., 2016, Proceedings of the eurographics workshop on intelligent cinematography and editing, P23, DOI [DOI 10.2312/WICED.20161097, 10.2312/wiced.20161097doi.org/10.2312/wiced.20161097, DOI 10.2312/WICED.20161097DOI.ORG/10.2312/WICED.20161097]
   Galvane Q, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3181975
   Gschwindt M, 2019, IEEE INT C INT ROBOT, P1107, DOI [10.1109/IROS40897.2019.8967592, 10.1109/iros40897.2019.8967592]
   Huang C, 2019, PROC CVPR IEEE, P4239, DOI 10.1109/CVPR.2019.00437
   Huang C, 2019, IEEE INT CONF ROBOT, P1871, DOI [10.1109/icra.2019.8793915, 10.1109/ICRA.2019.8793915]
   Huang C, 2018, IEEE INT C INT ROBOT, P4692, DOI 10.1109/IROS.2018.8594333
   Huang C, 2018, IEEE INT CONF ROBOT, P7039, DOI 10.1109/ICRA.2018.8460703
   Hwangbo J, 2019, SCI ROBOT, V4, DOI 10.1126/scirobotics.aau5872
   Jaderberg M, 2019, SCIENCE, V364, P859, DOI 10.1126/science.aau6249
   Jeong SW, 2017, IEEE T MULTIMEDIA, V19, P2692, DOI 10.1109/TMM.2017.2710802
   Kang H, 2018, IEEE ROBOT AUTOM LET, V3, P3717, DOI 10.1109/LRA.2018.2856271
   Kingma D. P., 2014, arXiv
   Kiran BR, 2022, IEEE T INTELL TRANSP, V23, P4909, DOI 10.1109/TITS.2021.3054625
   Konda VR, 2000, ADV NEUR IN, V12, P1008
   Kwon JY, 2008, VISUAL COMPUT, V24, P475, DOI 10.1007/s00371-008-0228-x
   Lan K, 2016, J ARTIF INTELL SOFT, V6, P255, DOI 10.1515/jaiscr-2016-0019
   Lan ZQ, 2017, ROBOTICS: SCIENCE AND SYSTEMS XIII
   Li-Wei He, 1996, Computer Graphics Proceedings. SIGGRAPH '96, P217
   Merel J, 2017, Arxiv, DOI arXiv:1707.02201
   Montes-Romero A, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10041494
   Nägeli T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073712
   Pfeiffer M, 2018, IEEE ROBOT AUTOM LET, V3, DOI 10.1109/LRA.2018.2869644
   Seyler SL, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004568
   Silver D, 2018, SCIENCE, V362, P1140, DOI 10.1126/science.aar6404
   Smith C., 2016, The Photographer's Guide to Drones, V1st, P98
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang JY, 2021, IEEE T MULTIMEDIA, V23, P3227, DOI 10.1109/TMM.2020.3021984
   Xin Wang, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P6622, DOI 10.1109/CVPR.2019.00679
   Yeh HH, 2013, IEEE T MULTIMEDIA, V15, P1944, DOI 10.1109/TMM.2013.2280250
   Zhu W, 2020, IEEE T NEUR NET LEAR, V31, P4487, DOI 10.1109/TNNLS.2019.2955699
   Zhu WT, 2016, AAAI CONF ARTIF INTE, P3697
NR 44
TC 1
Z9 1
U1 1
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2812
EP 2824
DI 10.1109/TMM.2022.3151463
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600028
DA 2024-07-18
ER

PT J
AU Du, CH
   Yu, F
   Jiang, MH
   Hua, AL
   Wei, X
   Peng, T
   Hu, XR
AF Du, Chenghu
   Yu, Feng
   Jiang, Minghua
   Hua, Ailing
   Wei, Xiong
   Peng, Tao
   Hu, Xinrong
TI VTON-SCFA: A Virtual Try-On Network Based on the Semantic Constraints
   and Flow Alignment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clothing; Semantics; Neck; Image reconstruction; Generative adversarial
   networks; Three-dimensional displays; Strain; Virtual try-on; human
   parsing; semantic constraint; flow aligning
AB An image-based virtual try-on system transfers an in-shop garment to the corresponding garment region of a reference person, which has huge application potential and commercial value in online clothing shopping. Existing methods have difficulty preserving garment texture and body details because of rough garment alignment and imperfect detail-retention strategies. To address this problem, we propose a virtual try-on network based on semantic constraints and flow alignment. The key idea of the framework is as follows: 1) a global-local semantic predictor (GLSP) is proposed to generate a reasonable target semantic map, which clearly guides the correct alignment of the in-shop garment with the body and the generation of try-on result; and 2) a novel appearance flow-based garment alignment network (AFGAN) is proposed to align the in-shop garment with the body, which is important to preserve maximum garment detail and ensure natural and realistic warping; and 3) we propose a synthesis strategy to integrate the aligned garment and the human body to preserve maximum body detail for generating a realistic result and preventing cross-occlusion and pixel confusion between different body parts. Experiments on the existing benchmark dataset demonstrate that the proposed method achieves the best performance on qualitative and quantitative experiments among the state-of-the-art virtual try-on techniques.
C1 [Du, Chenghu; Yu, Feng; Jiang, Minghua; Hua, Ailing; Wei, Xiong; Peng, Tao; Hu, Xinrong] Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan 430200, Hubei, Peoples R China.
   [Du, Chenghu; Yu, Feng; Jiang, Minghua; Hua, Ailing; Wei, Xiong; Peng, Tao; Hu, Xinrong] Engn Res Ctr Hubei Prov Clothing Informat, Wuhan 430200, Hubei, Peoples R China.
C3 Wuhan Textile University
RP Yu, F (corresponding author), Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan 430200, Hubei, Peoples R China.
EM duceh_lzy@163.com; yufeng@wtu.edu.cn; minghuajiang@wtu.edu.cn;
   hal_wtu@163.com; wx_wh@wtu.edu.cn; pt@wtu.edu.cn; hxr@wtu.edu.cn
RI Yu, Feng/JTD-1798-2023
OI Yu, Feng/0000-0001-8252-5131; Du, Chenghu/0000-0001-7275-5064
FU Young Talents Programme of the Scientific Research Program of the Hubei
   Education Department [Q20201709]; research on the Key Technology of
   Flexible Intelligent Manufacturing of Clothing Based on Digital Twin of
   Hubei Key Research and Development [2021BAA042]; Open Topic of
   Engineering Research Center of Hubei Province for Clothing Information
   [900204]
FX This work was supported in part by the Young Talents Programme of the
   Scientific Research Program of the Hubei Education Department under
   Project Q20201709, in part by research on the Key Technology of Flexible
   Intelligent Manufacturing of Clothing Based on Digital Twin of Hubei Key
   Research and Development Program under Project 2021BAA042, and in part
   the Open Topic of Engineering Research Center of Hubei Province for
   Clothing Information under Project 900204. The associate editor
   coordinat-ing the review of this manuscript and approving it for
   publication was Dr. Zhi Wang.
CR [Anonymous], 1977, Construction theory of functions of several variables, DOI [DOI 10.1007/BFB0086566, 10.1007/BFb0086566]
   Ayush K, 2019, IEEE INT CONF COMP V, P3193, DOI 10.1109/ICCVW.2019.00397
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Cui YR, 2018, COMPUT GRAPH FORUM, V37, P109, DOI 10.1111/cgf.13552
   Ge CJ, 2021, PROC CVPR IEEE, P16923, DOI 10.1109/CVPR46437.2021.01665
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gundogdu E, 2019, IEEE I CONF COMP VIS, P8738, DOI 10.1109/ICCV.2019.00883
   Hahn F, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601160
   Han XT, 2019, IEEE I CONF COMP VIS, P10470, DOI 10.1109/ICCV.2019.01057
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Han Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7847, DOI 10.1109/CVPR42600.2020.00787
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heming Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P512, DOI 10.1007/978-3-030-58452-8_30
   Hensel M, 2017, ADV NEUR IN, V30
   Hidayati SC, 2021, IEEE T MULTIMEDIA, V23, P365, DOI 10.1109/TMM.2020.2980195
   Honda S., 2019, EUROGRAPHICS, P9
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jandial Surgan, 2020, 2020 IEEE Winter Conference on Applications of Computer Vision (WACV). Proceedings, P2171, DOI 10.1109/WACV45572.2020.9093458
   Jetchev N, 2017, IEEE INT CONF COMP V, P2287, DOI 10.1109/ICCVW.2017.269
   Jo Y, 2019, IEEE I CONF COMP VIS, P1745, DOI 10.1109/ICCV.2019.00183
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kim BK, 2020, IEEE T MULTIMEDIA, V22, P298, DOI 10.1109/TMM.2019.2929000
   Lee HJ, 2019, IEEE INT CONF COMP V, P3129, DOI 10.1109/ICCVW.2019.00381
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma QL, 2020, PROC CVPR IEEE, P6468, DOI 10.1109/CVPR42600.2020.00650
   Minar M. R., 2020, CVPR WORKSH
   Mir A, 2020, PROC CVPR IEEE, P7021, DOI 10.1109/CVPR42600.2020.00705
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Oktay O., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.03999
   Osman Ahmed A. A., 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P598, DOI 10.1007/978-3-030-58539-6_36
   Patel Chaitanya, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7363, DOI 10.1109/CVPR42600.2020.00739
   Pumarola A, 2019, IEEE I CONF COMP VIS, P2242, DOI [10.1109/ICCV.2019.00233, 10.1109/ICCV.2019.2019.00233]
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Salimans T, 2016, ADV NEUR IN, V29
   Sharma M, 2017, PATTERN RECOGN LETT, V94, P172, DOI 10.1016/j.patrec.2017.03.023
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang W, 2019, IEEE I CONF COMP VIS, P2142, DOI 10.1109/ICCV.2019.00223
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yu RY, 2019, IEEE I CONF COMP VIS, P10510, DOI 10.1109/ICCV.2019.01061
   Yuan ML, 2013, IEEE T MULTIMEDIA, V15, P1958, DOI 10.1109/TMM.2013.2280560
   Zhan HJ, 2021, IEEE T MULTIMEDIA, V23, P133, DOI 10.1109/TMM.2020.2978669
   Zhang F, 2020, PROC CVPR IEEE, P7091, DOI 10.1109/CVPR42600.2020.00712
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XS, 2017, IEEE T MULTIMEDIA, V19, P2533, DOI 10.1109/TMM.2017.2696825
   Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18
NR 53
TC 9
Z9 10
U1 7
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 777
EP 791
DI 10.1109/TMM.2022.3152367
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900008
DA 2024-07-18
ER

PT J
AU Duan, HY
   Shen, W
   Min, XK
   Tian, Y
   Jung, JH
   Yang, XK
   Zhai, GT
AF Duan, Huiyu
   Shen, Wei
   Min, Xiongkuo
   Tian, Yuan
   Jung, Jae-Hyun
   Yang, Xiaokang
   Zhai, Guangtao
TI Develop Then Rival: A Human Vision-Inspired Framework for Superimposed
   Image Decomposition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Develop then rival; illumination correction; rain removal; reflection
   removal; shadow removal; superimposed image decomposition
ID BLIND SEPARATION; RESTORATION; ENHANCEMENT; REMOVAL; NETWORK
AB A single superimposed image containing two image views causes visual confusion for both human vision and computer vision. Human vision needs a "develop-then-rival" process to decompose the superimposed image into two individual images, which effectively suppresses visual confusion. However, separating individual image views from a single superimposed image has been an important but challenging task in computer vision area for a long time. In this paper, we propose a human vision-inspired framework for single superimposed image decomposition. We first propose a network to simulate the development stage, which tries to understand and distinguish the semantic information of the two layers of a single superimposed image. To further simulate the rivalry activation/suppression process in human brains, we carefully design a rivalry stage, which incorporates the original mixed input (superimposed image), the activated visual information (outputs of the development stage) together, and then rivals to get images without ambiguity. Experimental results show that our novel framework effectively separates the superimposed images and significantly improves the performance with better output quality compared with state-of-the-art methods. The proposed method also achieves state-of-the-art results on related applications including single image reflection removal, single image rain removal, single image shadow removal, and illumination correction, etc., which validates the generalization of the framework.
C1 [Duan, Huiyu; Shen, Wei; Min, Xiongkuo; Tian, Yuan; Yang, Xiaokang; Zhai, Guangtao] Shanghai Jiao Tong Univ, AI Inst, MoE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China.
   [Jung, Jae-Hyun] Harvard Med Sch, Schepens Eye Res Inst, Boston, MA 02114 USA.
C3 Shanghai Jiao Tong University; Harvard University; Schepens Eye Research
   Institute; Harvard Medical School
RP Shen, W; Zhai, GT (corresponding author), Shanghai Jiao Tong Univ, AI Inst, MoE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China.
EM huiyuduan@sjtu.edu.cn; shenwei1231@gmail.com; minxiongkuo@sjtu.edu.cn;
   ee_tianyuan@sjtu.edu.cn; jaehyun_jung@meei.harvard.edu;
   xkyang@sjtu.edu.cn; zhaiguangtao@sjtu.edu.cn
RI Tian, Yuan/JCF-1666-2023; Duan, Huiyu/N-7039-2018; Zhai,
   Guangtao/X-5949-2019; Min, Xiongkuo/A-7097-2019
OI Tian, Yuan/0000-0001-6073-8582; Duan, Huiyu/0000-0003-1755-0431; Zhai,
   Guangtao/0000-0001-8165-9322; Jung, Jae-Hyun/0000-0002-5709-6045; Min,
   Xiongkuo/0000-0001-5693-0416; Duan, Huiyu/0000-0002-6519-4067
FU National Natural Science Foundation of China [61831015, 61901260,
   62176159]; National Key R&D Program of China [2021YFE0206700]; Natural
   Science Foundation of Shanghai [21ZR1432200]; Shanghai Municipal Science
   and Technology Major Project [2021SHZDZX0102]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61831015, 61901260, and 62176159, in
   part by the National Key R&D Program of China under Grant
   2021YFE0206700, in part by the Natural Science Foundation of Shanghai
   under Grant 21ZR1432200, and in part by the Shanghai Municipal Science
   and Technology Major Project under Grant 2021SHZDZX0102.
CR Abdullah-Al-Wadud M, 2007, IEEE T CONSUM ELECTR, V53, P593, DOI 10.1109/TCE.2007.381734
   Apfelbaum H, 2015, TRANSL VIS SCI TECHN, V4, DOI 10.1167/tvst.4.6.8
   Arvanitopoulos N, 2017, PROC CVPR IEEE, P1752, DOI 10.1109/CVPR.2017.190
   Blake R, 2002, NAT REV NEUROSCI, V3, P13, DOI 10.1038/nrn701
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Deng ZJ, 2019, IEEE I CONF COMP VIS, P2453, DOI 10.1109/ICCV.2019.00254
   Ding B, 2019, IEEE I CONF COMP VIS, P10212, DOI 10.1109/ICCV.2019.01031
   Fan QN, 2017, IEEE I CONF COMP VIS, P3258, DOI 10.1109/ICCV.2017.351
   Feng X, 2021, IEEE T IMAGE PROCESS, V30, P4867, DOI 10.1109/TIP.2021.3076589
   Finlayson GD, 2009, INT J COMPUT VISION, V85, P35, DOI 10.1007/s11263-009-0243-z
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Gai K, 2012, IEEE T PATTERN ANAL, V34, P19, DOI 10.1109/TPAMI.2011.87
   Gai K, 2009, PROC CVPR IEEE, P1881, DOI 10.1109/CVPRW.2009.5206825
   Gandelsman Y, 2019, PROC CVPR IEEE, P11018, DOI 10.1109/CVPR.2019.01128
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Gong H., 2014, P BRIT MACH VIS C BM, P1
   Guo RQ, 2013, IEEE T PATTERN ANAL, V35, P2956, DOI 10.1109/TPAMI.2012.214
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu XW, 2019, PROC CVPR IEEE, P8014, DOI 10.1109/CVPR.2019.00821
   Hu XW, 2020, IEEE T PATTERN ANAL, V42, P2795, DOI 10.1109/TPAMI.2019.2919616
   Huang X, 2011, IEEE I CONF COMP VIS, P898, DOI 10.1109/ICCV.2011.6126331
   Hyvärinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Khosla A., 2011, P CVPR WORKSH FIN GR
   Kingma D. P., 2014, arXiv
   Kui Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8343, DOI 10.1109/CVPR42600.2020.00837
   Levin A, 2007, IEEE T PATTERN ANAL, V29, P1647, DOI 10.1109/TPAMI.2007.1106
   Li C, 2020, PROC CVPR IEEE, P3562, DOI 10.1109/CVPR42600.2020.00362
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li Y., 2020, arXiv
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Li Y, 2014, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2014.346
   Lin X, 2021, IEEE T MULTIMEDIA, V23, P664, DOI 10.1109/TMM.2020.2987703
   Nilsback M.-E., 2006, IEEE C COMP VIS PATT, V2, P1447, DOI [DOI 10.1109/CVPR.2006.42, 10.1109/CVPR.2006.42]
   O'Shea RP, 2009, VISION RES, V49, P671, DOI 10.1016/j.visres.2009.01.020
   Pan ZQ, 2022, IEEE T IMAGE PROCESS, V31, P1613, DOI 10.1109/TIP.2022.3144892
   Peli E, 2017, OPTOMETRY VISION SCI, V94, P817, DOI 10.1097/OPX.0000000000001102
   Qian R, 2018, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2018.00263
   Qu LQ, 2017, PROC CVPR IEEE, P2308, DOI 10.1109/CVPR.2017.248
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shih YC, 2015, PROC CVPR IEEE, P3193, DOI 10.1109/CVPR.2015.7298939
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wan RJ, 2017, IEEE I CONF COMP VIS, P3942, DOI 10.1109/ICCV.2017.423
   Wang JF, 2018, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2018.00192
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang YL, 2021, IEEE T MULTIMEDIA, V23, P2481, DOI 10.1109/TMM.2020.3013383
   Wang YL, 2017, IEEE T IMAGE PROCESS, V26, P3936, DOI 10.1109/TIP.2017.2708502
   Wei C, 2018, Arxiv, DOI arXiv:1808.04560
   Wei KX, 2019, PROC CVPR IEEE, P8170, DOI 10.1109/CVPR.2019.00837
   Wei W, 2019, PROC CVPR IEEE, P3872, DOI 10.1109/CVPR.2019.00400
   Wen Q, 2019, PROC CVPR IEEE, P3766, DOI 10.1109/CVPR.2019.00389
   Wolfe B, 2017, APPL ERGON, V65, P316, DOI 10.1016/j.apergo.2017.07.009
   Wu CL, 2012, J SCI COMPUT, V50, P145, DOI 10.1007/s10915-011-9477-3
   Wu CL, 2011, INVERSE PROBL IMAG, V5, P237, DOI 10.3934/ipi.2011.5.237
   Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1
   Yang J, 2018, LECT NOTES COMPUT SC, V11207, P675, DOI 10.1007/978-3-030-01219-9_40
   Yang QX, 2012, IEEE T IMAGE PROCESS, V21, P4361, DOI 10.1109/TIP.2012.2208976
   Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P3461, DOI 10.1109/TIP.2021.3062184
   Yang WH, 2020, PROC CVPR IEEE, P3060, DOI 10.1109/CVPR42600.2020.00313
   Yang WH, 2021, IEEE T PATTERN ANAL, V43, P4059, DOI 10.1109/TPAMI.2020.2995190
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yasarla R, 2019, PROC CVPR IEEE, P8397, DOI 10.1109/CVPR.2019.00860
   Yu FS, 2016, Arxiv, DOI arXiv:1506.03365
   Zhang C., 2008, P IEEE C COMP VIS PA, P1
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhang HD, 2020, IEEE T MULTIMEDIA, V22, P2012, DOI 10.1109/TMM.2019.2951461
   Zhang X, 2018, PROC CVPR IEEE, P4786, DOI 10.1109/CVPR.2018.00503
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zou Z., 2020, P IEEE CVF C COMP VI, P12806
NR 80
TC 7
Z9 7
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4267
EP 4281
DI 10.1109/TMM.2022.3172882
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200014
DA 2024-07-18
ER

PT J
AU Guan, TX
   Li, CF
   Gu, K
   Liu, HT
   Zheng, YH
   Wu, XJ
AF Guan, Tuxin
   Li, Chaofeng
   Gu, Ke
   Liu, Hantao
   Zheng, Yuhui
   Wu, Xiao-jun
TI Visibility and Distortion Measurement for No-Reference Dehazed Image
   Quality Assessment via Complex Contourlet Transform
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Distortion; Distortion measurement; Image quality;
   Brightness; Image color analysis; Task analysis; Complex contourlet
   transform; dehazed image quality assessment; distortion-aware features;
   support vector regression; visibility-aware features
ID SCALE
AB Recently, most dehazed image quality assessment (DQA) methods have focused on estimating remaining haze and omitting distortion impact from the side effect of dehazing algorithms, which leads to their limited performance. Addressing this problem, we propose a method for learning both visibility and distortion-aware features no-reference (NR) dehazed image quality assessment (VDA-DQA). Visibility-aware features are exploited to characterize clarity optimization after dehazing, including the brightness-, contrast-, and sharpness-aware features extracted by the complex contourlet transform (CCT). Then, distortion-aware features are employed to measure the distortion artifacts of images, including the normalized histogram of the local binary pattern (LBP) from the reconstructed dehazed image and the statistics of the CCT subbands corresponding to the chroma and saturation map. Finally, all the above features are mapped into quality scores by support vector regression (SVR). Extensive experimental results on six public DQA datasets verify the superiority of the proposed VDA-DQA method in terms of consistency with subjective visual perception and outperform state-of-the-art methods.
C1 [Guan, Tuxin; Li, Chaofeng] Shanghai Maritime Univ, Inst Logist Sci & Engn, Shanghai 201306, Peoples R China.
   [Gu, Ke] Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
   [Liu, Hantao] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF243AA, Wales.
   [Zheng, Yuhui] NanjingUnivers Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Peoples R China.
   [Zheng, Yuhui] Minist Educ, Engn Res Ctr Digital Forens, Nanjing 210044, Peoples R China.
   [Wu, Xiao-jun] Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Jiangsu, Peoples R China.
C3 Shanghai Maritime University; Beijing University of Technology; Cardiff
   University; Jiangnan University
RP Li, CF (corresponding author), Shanghai Maritime Univ, Inst Logist Sci & Engn, Shanghai 201306, Peoples R China.; Zheng, YH (corresponding author), NanjingUnivers Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Peoples R China.
EM txguan6896@163.com; wxlichaofeng@126.com; guke.doctor@gmail.com;
   liuh35@cardiff.ac.uk; zhengyh@vip.126.com; wu_xiaojun@jiangnan.edu.cn
OI Wu, Xiao-Jun/0000-0002-0310-5778; Li, Chaofeng/0000-0002-3236-3143;
   Guan, Tuxin/0000-0003-1714-7204
FU National Natural Science Foundation of China [62176150, U20B2065];
   Shanghai Maritime University Graduate Top-notch Innovative Talent
   Cultivation Project [2021YBR020]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62176150 and U20B2065, and in part by
   Shanghai Maritime University Graduate Top-notch Innovative Talent
   Cultivation Project under Grant 2021YBR020.
CR Ancuti C, 2016, IEEE IMAGE PROC, P2226, DOI 10.1109/ICIP.2016.7532754
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen DP, 2005, PROC WRLD ACAD SCI E, V7, P342
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   da Cunha AL, 2006, IEEE T IMAGE PROCESS, V15, P3089, DOI 10.1109/TIP.2006.877507
   Fang S, 2011, CHIN CONT DECIS CONF, P610, DOI 10.1109/CCDC.2011.5968254
   Gu K, 2021, IEEE T NEUR NET LEAR, V32, P4278, DOI 10.1109/TNNLS.2021.3105394
   Gu K, 2021, IEEE T IND INFORM, V17, P2261, DOI 10.1109/TII.2020.2991208
   Gu K, 2020, IEEE T MULTIMEDIA, V22, P311, DOI 10.1109/TMM.2019.2929009
   Gu K, 2018, IEEE T NEUR NET LEAR, V29, P1301, DOI 10.1109/TNNLS.2017.2649101
   Gu K, 2017, IEEE T CYBERNETICS, V47, P4559, DOI 10.1109/TCYB.2016.2575544
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Gupta P, 2018, SIGNAL PROCESS-IMAGE, V66, P87, DOI 10.1016/j.image.2018.05.009
   Hassen R, 2013, IEEE T IMAGE PROCESS, V22, P2798, DOI 10.1109/TIP.2013.2251643
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Jiang YT, 2017, IEEE T IMAGE PROCESS, V26, P3397, DOI 10.1109/TIP.2017.2700720
   Khosravi MH, 2020, IEEE T CIRC SYST VID, V30, P48, DOI 10.1109/TCSVT.2018.2890457
   Li C, 2011, ELECTRON LETT, V47, P962, DOI 10.1049/el.2011.0921
   Li CF, 2021, SIGNAL PROCESS-IMAGE, V91, DOI 10.1016/j.image.2020.116064
   Li CF, 2021, IET IMAGE PROCESS, V15, P443, DOI 10.1049/ipr2.12034
   Li Q, 2009, IEEE J-STSP, V3, P202, DOI 10.1109/JSTSP.2009.2014497
   Li QH, 2016, IEEE SIGNAL PROC LET, V23, P541, DOI 10.1109/LSP.2016.2537321
   Liu LX, 2020, SIGNAL PROCESS-IMAGE, V87, DOI 10.1016/j.image.2020.115915
   Liu LX, 2019, IEEE T MULTIMEDIA, V21, P2305, DOI 10.1109/TMM.2019.2900941
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Liu W, 2021, IEEE T IMAGE PROCESS, V30, P176, DOI 10.1109/TIP.2020.3033402
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503
   Ma KD, 2015, IEEE IMAGE PROC, P3600, DOI 10.1109/ICIP.2015.7351475
   Mahajan P, 2021, IEEE T IND INFORM, V17, P8046, DOI 10.1109/TII.2021.3065439
   Meng GD, 2013, IEEE INT C SOL DIEL, P662, DOI 10.1109/ICSD.2013.6619828
   Min XK, 2019, IEEE T INTELL TRANSP, V20, P2879, DOI 10.1109/TITS.2018.2868771
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Min XK, 2019, IEEE T MULTIMEDIA, V21, P2319, DOI 10.1109/TMM.2019.2902097
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Olshausen BA, 2005, NEURAL COMPUT, V17, P1665, DOI 10.1162/0899766054026639
   Pan XX, 2016, IEEE GEOSCI REMOTE S, V13, P1855, DOI 10.1109/LGRS.2016.2614890
   Sandic-Stankovic DD, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2919416
   Selesnick IW, 2005, IEEE SIGNAL PROC MAG, V22, P123, DOI 10.1109/MSP.2005.1550194
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   SHEN W, 2017, J NETW INTELL, V2, P139
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Vu PV, 2012, IEEE SIGNAL PROC LET, V19, P423, DOI 10.1109/LSP.2012.2199980
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   WU Q, 2016, P 15 INT C OPT COMM, P1
   Wu QB, 2020, IEEE T CIRC SYST VID, V30, P3883, DOI 10.1109/TCSVT.2020.2972566
   Yue GH, 2019, IEEE T IND ELECTRON, V66, P3784, DOI 10.1109/TIE.2018.2851984
   Zhang JH, 2022, IEEE T INTELL TRANSP, V23, P3087, DOI 10.1109/TITS.2020.3030673
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhao SY, 2020, IEEE T IMAGE PROCESS, V29, P6947, DOI 10.1109/TIP.2020.2995264
   Zhu H., 2020, P IEEE CVF C COMP VI, P14143
NR 54
TC 13
Z9 13
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3934
EP 3949
DI 10.1109/TMM.2022.3168438
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500030
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Han, G
   Su, JP
   Liu, YM
   Zhao, YQ
   Kwong, S
AF Han, Guang
   Su, Jinpeng
   Liu, Yaoming
   Zhao, Yuqiu
   Kwong, Sam
TI Multi-Stage Visual Tracking With Siamese Anchor-Free Proposal Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target tracking; Feature extraction; Object tracking; Task analysis;
   Interference; Deep learning; Training; Anchor-free; feature
   purification; object tracking; siamese network
ID OBJECT TRACKING
AB The austere challenge of visual object tracking is to find the target to be tracked in various noise interference and obtain its accurate bounding box coordinates. Recently, the object tracking technology based on the Siamese network has made great breakthroughs, and more and more Siamese network trackers have been proposed with superior performance. They still have some shortcomings. To this end, a new Multi-Stage visual tracking algorithm with Siamese Anchor-Free Proposal Network (MS-SiamAFPN) is proposed in this paper. The algorithm is a three-stage Siamese network tracker composed of Feature Extraction and Fusion (FEF) sub-network, Classification and Regression (CR) sub-network, Validation and Regression (VR) sub-network in series. Firstly, the Anchor-Free Proposal Network (AFPN) module is designed in the CR stage, which can make full use of positive and negative samples for training while reducing neural network parameters. Secondly, aim to achieve better robustness and recognizability in the VR stage, on the one hand, a novel Feature Purification (FP) module is designed, which can automatically select the important channels, and extract the features of irregular regions on the input fusion features, so as to strengthen the representation ability of image features. On the other hand, the target recognition and position regression are regarded as different processing tasks, and the recognition score and position fine-tuning of candidate targets are obtained by newly designing the Dual-Branch Network (DBN) structure, thereby avoiding feature ambiguity. Due to the synergy of the above these innovations, MS-SiamAFPN has obtained a large performance improvement, and achieved SOTA performance in multiple public dataset benchmarks.
C1 [Han, Guang; Su, Jinpeng; Liu, Yaoming] Nanjing Univ Posts & Telecommun, Engn Res Ctr Wideband Wireless Commun Technol, Minist Educ, Nanjing 210003, Peoples R China.
   [Zhao, Yuqiu] Nanjing Univ Sci & Technol, Sch Design Art & Media, Nanjing 210094, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Nanjing University of
   Science & Technology; City University of Hong Kong
RP Han, G (corresponding author), Nanjing Univ Posts & Telecommun, Engn Res Ctr Wideband Wireless Commun Technol, Minist Educ, Nanjing 210003, Peoples R China.
EM hanguang8848@163.com; 15005187922@163.com; 814893648@qq.com;
   18362903309@163.com; cssamk@cityu.edu.hk
RI Kwong, Sam/C-9319-2012
OI Kwong, Sam/0000-0001-7484-7261
FU Natural Science Foundation of China NSFC [61871445, 61302156]; Key R & D
   Foundation Project of Jiangsu Province [BE2016001-4]
FX This work was supported in part by the Natural Science Foundation of
   China NSFC under Grants 61871445 and 61302156 and in part by the Key R &
   D Foundation Project of Jiangsu Province under Grant BE2016001-4. The
   Associate Editor coordinating the review of this manuscript and
   approving it for publication was Dr. Palaiahnakote Shivakumara.
CR Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Bottou Leon, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P421, DOI 10.1007/978-3-642-35289-8_25
   Chen K, 2019, IEEE T MULTIMEDIA, V21, P86, DOI 10.1109/TMM.2018.2846405
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   Han G, 2019, IEEE ACCESS, V7, P123934, DOI 10.1109/ACCESS.2019.2937998
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508
   Hu HW, 2019, IEEE T MULTIMEDIA, V21, P510, DOI 10.1109/TMM.2018.2859831
   Hu K, 2018, NEUROCOMPUTING, V309, P179, DOI 10.1016/j.neucom.2018.05.011
   Huang LH, 2019, Arxiv, DOI arXiv:1810.11981
   Kristan M, 2015, LECT NOTES COMPUT SC, V8926, P191, DOI 10.1007/978-3-319-16181-5_14
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li J, 2019, IEEE T MULTIMEDIA, V21, P2531, DOI 10.1109/TMM.2019.2908350
   Li PX, 2019, IEEE I CONF COMP VIS, P6161, DOI 10.1109/ICCV.2019.00626
   Li YM, 2021, IEEE T MULTIMEDIA, V23, P810, DOI 10.1109/TMM.2020.2990064
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu YL, 2017, PROC CVPR IEEE, P3454, DOI 10.1109/CVPR.2017.368
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Müller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Niculescu-Mizil Alexandru, 2005, P 21 C UNC ART INT, P413, DOI DOI 10.5555/3020336.3020388
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279
   Tian Z, 2022, IEEE T PATTERN ANAL, V44, P1922, DOI 10.1109/TPAMI.2020.3032166
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661
   Wang GT, 2019, PROC CVPR IEEE, P3638, DOI 10.1109/CVPR.2019.00376
   Wang MM, 2017, PROC CVPR IEEE, P4800, DOI 10.1109/CVPR.2017.510
   Wang N, 2018, PROC CVPR IEEE, P4844, DOI 10.1109/CVPR.2018.00509
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wang QR, 2019, IEEE T MULTIMEDIA, V21, P930, DOI 10.1109/TMM.2018.2869277
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Yunhua Zhang, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11213), P355, DOI 10.1007/978-3-030-01240-3_22
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang LC, 2019, IEEE I CONF COMP VIS, P4009, DOI 10.1109/ICCV.2019.00411
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhang ZZ, 2017, PROC CVPR IEEE, P3549, DOI 10.1109/CVPR.2017.378
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
   Zhou X., 2019, arXiv
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
   Zhu Z, 2018, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2018.00064
   Zhu Z, 2017, IEEE INT CONF COMP V, P1973, DOI 10.1109/ICCVW.2017.231
NR 58
TC 7
Z9 7
U1 5
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 430
EP 442
DI 10.1109/TMM.2021.3127357
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800008
DA 2024-07-18
ER

PT J
AU Huang, YJ
   Jing, ME
   Zhou, JJ
   Liu, YH
   Fan, YB
AF Huang, Yujie
   Jing, Minge
   Zhou, Jinjia
   Liu, Yuhao
   Fan, Yibo
TI LCCStyle: Arbitrary Style Transfer With Low Computational Complexity
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Shape; Computational complexity; Training; Deep learning; Fans;
   Computational efficiency; Task analysis; Style transfer; meta-learning;
   deep learning
AB Surprising performance has been achieved in style transfer since deep learning was introduced to it. However, the existing state-of-the-art (SOTA) algorithms either suffer from quality issues or high computational complexity. The quality issues include shape retention and the adequacy of style migration, and the computational complexity is reflected in the network complexity and additional updates when the style changes. To deal with the above problems, we propose a novel low computational complexity arbitrary style transfer algorithm (LCCStyle) that mainly consists of a transformation feature module (TFM) and learning transformation module (LTM). The TFM is responsible for transforming the content feature map into the stylized feature map without impact on the integrity of content information, which contributes to good shape retention and full style migration. In addition, to avoid additional updates when the style changes, we propose a new training mechanism for arbitrary style transfer to directly generate the parameters of the TFM by a hyper-network. However, the widely used hyper-networks are composed of fully connected layers, which cause a large number of parameters. Hence, we designed a hyper-network (LTM) consisting of one-dimensional convolution to adapt to the characteristics of the Gram matrix of the style feature map, contributing to a small model size and having no impact on quality. Quantitative comparison and user study show that LCCStyle achieves high performance both on the adequacy of style migration and shape retention. Furthermore, compared with the SOTAs, the size of the proposed model is reduced by a large margin of nearly 51.4%$\sim$99.6%. When the input is 512x512 pixels, the processing speeds in the cases of unchanged style and constantly changing style are increased by at least 135% and 227%, respectively. On an Nvidia TITAN RTX GPU, LCCStyle reaches 60fps for 720p video and takes only 1 s to process 8 K images. https://github.com/HuangYujie94/LCCStyle.
C1 [Huang, Yujie; Jing, Minge; Fan, Yibo] Fudan Univ, Coll Microelect, State Key Lab ASIC & Syst, Shanghai 200000, Peoples R China.
   [Zhou, Jinjia] Hosei Univ, Koganei, Tokyo 1848584, Japan.
   [Liu, Yuhao] Dalian Univ Technol, Sch Comp Sci & Technol, Dalian 116024, Peoples R China.
   [Fan, Yibo] ZTE Corp, State Key Lab Mobile Network & Mobile Multimedia T, Shenzhen 518057, Peoples R China.
C3 Fudan University; Hosei University; Dalian University of Technology; ZTE
RP Fan, YB (corresponding author), Fudan Univ, Coll Microelect, State Key Lab ASIC & Syst, Shanghai 200000, Peoples R China.; Fan, YB (corresponding author), ZTE Corp, State Key Lab Mobile Network & Mobile Multimedia T, Shenzhen 518057, Peoples R China.
EM 19112020091@fudan.edu.cn; mejing@fudan.edu.cn;
   jinjia.zhou.35@hosei.ac.jp; yuhaoLiu7456@gmail.com; fanyibo@fudan.edu.cn
RI fan, Yibo/HHM-9469-2022
OI Liu, Yuhao/0000-0003-0550-4788; Huang, Yujie/0000-0001-7934-7872
FU National Natural Science Foundation of China [62031009, 61525401];
   Shanghai Science and Technology Committee (STCSM) [19511104300]; Alibaba
   Innovative Research (AIR) Program; Innovation Program of Shanghai
   Municipal Education Commission; Fudan University-CIOMP Joint Fund
   [FC2019-001]; Fudan-ZTE Joint Lab; Pioneering Project of Academy for
   Engineering and Technology, Fudan University [gyy2021-001]; National Key
   Research and Development Program of China [2019YFB2204403]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62031009, in part by the National
   Natural Science Foundation of China under Grant 61525401, in part by
   Shanghai Science and Technology Committee (STCSM) under Grant
   19511104300, in part by Alibaba Innovative Research (AIR) Program, in
   part by the Innovation Program of Shanghai Municipal Education
   Commission, in part by Fudan University-CIOMP Joint Fund (FC2019-001),
   in part by Fudan-ZTE Joint Lab, in part by the Pioneering Project of
   Academy for Engineering and Technology, Fudan University (gyy2021-001),
   and in part by the National Key Research and Development Program of
   China under Grant 2019YFB2204403. The Associate Editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Alexandros (Alexis) Michael Tourapis.
CR Andrychowicz M, 2016, ADV NEUR IN, V29
   Chen XY, 2019, IEEE T IMAGE PROCESS, V28, P546, DOI 10.1109/TIP.2018.2869695
   Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461
   De Brabandere B, 2016, ADV NEUR IN, V29
   Fan QN, 2021, IEEE T PATTERN ANAL, V43, P33, DOI 10.1109/TPAMI.2019.2925793
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gu SY, 2018, PROC CVPR IEEE, P8222, DOI 10.1109/CVPR.2018.00858
   Ha David, 2016, arXiv
   Hu R, 2018, PROC CVPR IEEE, P4233, DOI 10.1109/CVPR.2018.00445
   Hu XC, 2019, PROC CVPR IEEE, P1575, DOI 10.1109/CVPR.2019.00167
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jing YC, 2019, Arxiv, DOI arXiv:1911.06953
   Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim BK, 2020, IEEE T MULTIMEDIA, V22, P298, DOI 10.1109/TMM.2019.2929000
   Kingma D. P., 2015, INT C LEARNING REPRE
   Kotovenko D, 2019, IEEE I CONF COMP VIS, P4421, DOI 10.1109/ICCV.2019.00452
   Kotovenko D, 2019, PROC CVPR IEEE, P10024, DOI 10.1109/CVPR.2019.01027
   Li YJ, 2017, ADV NEUR IN, V30
   Liu C, 2020, PROC CVPR IEEE, P6886, DOI 10.1109/CVPR42600.2020.00692
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P1724, DOI 10.1109/TMM.2017.2780761
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740
   Munkhdalai T, 2017, PR MACH LEARN RES, V70
   Nicol K, 2016, Painter by Numbers Wikiart
   Noh H, 2016, PROC CVPR IEEE, P30, DOI 10.1109/CVPR.2016.11
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Ravi S., 2016, INT C LEARNING REPRE
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen FL, 2018, PROC CVPR IEEE, P8061, DOI 10.1109/CVPR.2018.00841
   Sheng L, 2018, PROC CVPR IEEE, P8242, DOI 10.1109/CVPR.2018.00860
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song ZC, 2017, IEEE T MULTIMEDIA, V19, P702, DOI 10.1109/TMM.2016.2631123
   Svoboda Jan, 2020, CVPR
   Ulyanov D, 2016, PR MACH LEARN RES, V48
   Virtusio JJ, 2021, IEEE T MULTIMEDIA, V23, P2273, DOI 10.1109/TMM.2020.3009484
   Wang H, 2020, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR42600.2020.00193
   Wang YX, 2016, LECT NOTES COMPUT SC, V9910, P616, DOI 10.1007/978-3-319-46466-4_37
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang T., 2018, Advances in Neural Information Processing Systems
   Yao Y, 2019, PROC CVPR IEEE, P1467, DOI 10.1109/CVPR.2019.00156
   Zhang C, 2019, AAAI CONF ARTIF INTE, P1254
   Zhang W, 2013, IEEE T MULTIMEDIA, V15, P1594, DOI 10.1109/TMM.2013.2265675
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zhou YF, 2019, IEEE T MULTIMEDIA, V21, P3136, DOI 10.1109/TMM.2019.2920613
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 47
TC 3
Z9 3
U1 2
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 501
EP 514
DI 10.1109/TMM.2021.3128058
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800013
DA 2024-07-18
ER

PT J
AU Li, Q
   Chen, Y
   Zhang, AY
   Jiang, Y
   Zou, LH
   Xu, ZM
   Muntean, GM
AF Li, Qing
   Chen, Ying
   Zhang, Aoyang
   Jiang, Yong
   Zou, Longhao
   Xu, Zhimin
   Muntean, Gabriel-Miro
TI A Super-Resolution Flexible Video Coding Solution for Improving Live
   Streaming Quality
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Streaming media; Bandwidth; Uplink; Bit rate; Video coding; Servers;
   Video recording; Live streaming; super-resolution; video coding; video
   delivery
ID INTERPOLATION; DELIVERY
AB In the context of the latest growing popularity of live video streaming, ensuring high video quality has become one of the most significant challenges faced by all live streaming platforms. Insufficient uplink bandwidth is an important factor that influences these live video transmissions, affecting their bitrate and latency and consequently the associated video streaming quality. This paper proposes a novel flexible super-resolution-based video coding and uploading framework (FlexSRVC) that improves the quality of live video streaming in limited uplink network bandwidth conditions. FlexSRVC includes a flexible video coding scheme, which compresses high-resolution key and non-key video frames to a lower bitrate in order to reduce the upload delay. A new flexible bitrate adaptation algorithm is also proposed to select dynamically the number of frames to be compressed and the compression ratio by jointly considering uplink network conditions and available cloud computing resources. Trace-driven emulations demonstrate that FlexSRVC provides the same quality while reducing up to 25% of the required bandwidth compared to the original encoding method (H.264). FlexSRVC improves users' QoE by at least 50% compared to a super resolution-based method which employs reconstruction of all video frames in uplink bandwidth constrained conditions.
C1 [Li, Qing; Jiang, Yong] Peng Cheng Lab PCL, Dept Math & Theories, Shenzhen 518066, Peoples R China.
   [Chen, Ying] Tsinghua Univ, Tsinghua Berkeley Shenzhen Inst, Shenzhen 100190, Peoples R China.
   [Zhang, Aoyang; Xu, Zhimin] Beijing Bytedance Technol Co Ltd, Multimedia Lab, Beijing 100098, Peoples R China.
   [Zou, Longhao] Peng Cheng Lab PCL, Dept Broadband Commun, Shenzhen 518066, Peoples R China.
   [Zou, Longhao] Southern Univ Sci & Technol, Shenzhen 518055, Peoples R China.
   [Jiang, Yong] Tsinghua Shenzhen Int Grad Sch, Div Informat Sci & Technol, Shenzhen 518071, Peoples R China.
   [Muntean, Gabriel-Miro] Dublin City Univ, Performance Engn Lab, Dublin, Ireland.
C3 Tsinghua Shenzhen International Graduate School; Tsinghua University;
   Southern University of Science & Technology; Tsinghua Shenzhen
   International Graduate School; Dublin City University
RP Zou, LH (corresponding author), Peng Cheng Lab PCL, Dept Broadband Commun, Shenzhen 518066, Peoples R China.
EM andyliqing@gmail.com; chen-yin18@mails.tsinghua.edu.cn;
   zhangaoyang@bytedance.com; jiangy@sz.tsinghua.edu.cn;
   zoulonghao@gmail.com; zhiminxu@pku.edu.cn; gabriel.muntean@dcu.ie
RI Zou, Longhao/C-6869-2019; Li, Qing/U-8995-2018
OI Muntean, Gabriel-Miro/0000-0002-9332-4770; Li, Qing/0000-0002-6071-473X
FU Key-Area Research and Development Program of Guangdong Province
   [2020B0101130006]; National Natural Science Foundation of China
   [61972189, 62201244]; Shenzhen Key Lab of Software Defined Networking
   [ZDSYS20140509172959989]; Major Key Project of PCL [PCL2021A03-1];
   Science Foundation Ireland Research Centres Programme [12/RC/2289_P2]
FX This work was supported in part by the Key-Area Research and Development
   Program of Guangdong Province underGrant 2020B0101130006, in part by the
   National Natural Science Foundation of China under Grants 61972189 and
   62201244, in part by the Shenzhen Key Lab of Software Defined Networking
   under Grant ZDSYS20140509172959989, and in part by the Major Key Project
   of PCL under Grant PCL2021A03-1. The work of Gabriel-Miro Muntean was
   supported by Science Foundation Ireland Research Centres Programme under
   Grant 12/RC/2289_P2 (Insight). The Associate Editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Chonggang Wang.
CR Adams J, 2007, IEEE ICC, P4548, DOI 10.1109/ICC.2007.751
   Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Amazon Web Services, 2022, Video latency in live streaming
   [Anonymous], 2020, Cisco annual Internet report (2018-2023) white paper
   Apple, 2022, HLS authoring specification for apple devices
   Bakar G, 2019, IEEE T MULTIMEDIA, V21, P429, DOI 10.1109/TMM.2018.2856629
   Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Carlucci G, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P133, DOI 10.1145/2910017.2910605
   Django, 2022, About us
   Dogga P., 2019, P USENIX WORKSH HOT
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   DUCHON CE, 1979, J APPL METEOROL, V18, P1016, DOI 10.1175/1520-0450(1979)018<1016:LFIOAT>2.0.CO;2
   FFmpeg, 2022, H.264 Video Encoding Guide
   Geng YF, 2015, 2015 VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Glaister J., 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P105, DOI 10.1109/ISM.2011.25
   HOU HS, 1978, IEEE T ACOUST SPEECH, V26, P508
   Huang TC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P429, DOI 10.1145/3343031.3351014
   Hubert B., 2022, Linux advanced routing & traffic control howto
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim J, 2020, SIGCOMM '20: PROCEEDINGS OF THE 2020 ANNUAL CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION ON THE APPLICATIONS, TECHNOLOGIES, ARCHITECTURES, AND PROTOCOLS FOR COMPUTER COMMUNICATION, P107, DOI 10.1145/3387514.3405856
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kurdoglu E, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P122, DOI 10.1145/2910017.2910608
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li Y, 2018, IEEE T CIRC SYST VID, V28, P2316, DOI 10.1109/TCSVT.2017.2727682
   Li ZJ, 2016, NANOMATERIALS-BASEL, V6, DOI 10.3390/nano6080152
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lin HW, 2019, IEEE T MULTIMEDIA, V21, P3010, DOI 10.1109/TMM.2019.2919433
   Liu K., 2018, P IEEE VIS COMM IM P, P1
   Muntean GM, 2007, IEEE T BROADCAST, V53, P92, DOI 10.1109/TBC.2006.886451
   Muntean GM, 2007, IEEE T BROADCAST, V53, P362, DOI 10.1109/TBC.2007.891710
   Muntean GM, 2006, IEEE T BROADCAST, V52, P230, DOI 10.1109/TBC.2006.875612
   Nginx, 2022, About us
   Raca D, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P460, DOI 10.1145/3204949.3208123
   Shen MM, 2011, IEEE T CIRC SYST VID, V21, P755, DOI 10.1109/TCSVT.2011.2130390
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   uwsgi- docs. readthedocs.io, 2022, uWsgi documents
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wang ZY, 2020, IEEE INFOCOM SER, P1093, DOI [10.1109/INFOCOM41043.2020.9155257, 10.1109/infocom41043.2020.9155257]
   Williams S, 2009, COMMUN ACM, V52, P65, DOI 10.1145/1498765.1498785
   Xu ZM, 2019, IEEE T CIRC SYST VID, V29, P1781, DOI 10.1109/TCSVT.2018.2849015
   Yaqoob A, 2019, INT WIREL COMMUN, P643, DOI 10.1109/IWCMC.2019.8766648
   Yeo H, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P645
   Yeo Hyunho, 2020, P 26 ANN INT C MOB C, DOI [10.1145/3372224.3419185, DOI 10.1145/3372224.3419185]
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zhang RX, 2019, PROCEEDINGS OF THE 29TH ACM WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO (NOSSDAV'19), P55, DOI 10.1145/3304112.3325607
   Zhang YJ, 2020, IEEE INFOCOM SER, P1957, DOI [10.1109/infocom41043.2020.9155384, 10.1109/INFOCOM41043.2020.9155384]
   Zou LH, 2019, IEEE ACCESS, V7, P89172, DOI 10.1109/ACCESS.2019.2926207
NR 50
TC 3
Z9 3
U1 3
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6341
EP 6355
DI 10.1109/TMM.2022.3207580
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500049
DA 2024-07-18
ER

PT J
AU Liang, XP
   Tang, ZJ
   Wu, JL
   Li, ZX
   Zhang, XP
AF Liang, Xiaoping
   Tang, Zhenjun
   Wu, Jingli
   Li, Zhixin
   Zhang, Xinpeng
TI Robust Image Hashing With Isomap and Saliency Map for Copy Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Robustness; Transforms; Classification algorithms;
   Discrete cosine transforms; Resists; Image color analysis; Copy
   detection; dimension reduction; image hashing; Isomap; saliency map
ID VISUAL-ATTENTION; RING PARTITION; MODEL; LOCALIZATION; TRANSFORM;
   RESISTANT; PATTERN; SIFT
AB Compression technology for representing image is on demand for efficiently processing images in the Big Data era. Image hashing is an effective compression technology for computing a short representation based on visual content of input image. Currently, most reported image hashing algorithms have weakness in making a desirable classification between discrimination and robustness and thus can not reach good performance in copy detection. To address these issues, this paper proposes a new robust image hashing with Isometric Mapping (Isomap) and saliency map for copy detection. A key contribution is hash generation with saliency map determined by the Frequency Tuned (FT) method, which can guarantee robustness of the proposed image hashing. Another contribution is the use of Isomap in deriving hash from the FT-based saliency map. Since Isomap can discover the internal geometry features of image, the use of Isomap can learn discriminative image features and thus discrimination of the proposed image hashing is ensured. Experiments on open image databases are carried out. Comparison results illustrate that the proposed image hashing is better than some state-of-the-art algorithms in the performances of classification and copy detection.
C1 [Liang, Xiaoping; Tang, Zhenjun; Wu, Jingli; Li, Zhixin] Guangxi Normal Univ, Guangxi Key Lab Multisource Informat Min & Secur, Guilin 541004, Peoples R China.
   [Zhang, Xinpeng] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
C3 Guangxi Normal University; Fudan University
RP Tang, ZJ (corresponding author), Guangxi Normal Univ, Guangxi Key Lab Multisource Informat Min & Secur, Guilin 541004, Peoples R China.
EM 511863138@qq.com; tangzj230@163.com; wjlhappy@gxnu.edu.cn;
   lizx@gxnu.edu.cn; zhangxinpeng@fudan.edu.cn
RI LI, LIXIN/KFS-0074-2024
OI Li, Zhixin/0000-0002-5313-6134; Tang, Zhenjun/0000-0003-3664-1363
FU National Natural Science Foundation of China [61962008, 61762015,
   62162006, 61966004, U1936214]; Guangxi "Bagui Scholar" Team for
   Innovation and Research; Guangxi Talent Highland Project of Big Data
   Intelligence and Application; Guangxi Collaborative Innovation Center of
   Multi-source Information
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61962008, 61762015, 62162006, 61966004,
   and U1936214, in part by the Guangxi "Bagui Scholar" Team for Innovation
   and Research, in part by the Guangxi Talent Highland Project of Big Data
   Intelligence and Application, and in part by the Guangxi Collaborative
   Innovation Center of Multi-source Information. The Associate Editor
   coordinating the re-view of this manuscript and approving it for
   publication was Dr. Michele Merler.
CR Abdullahi SM, 2020, IEEE T INF FOREN SEC, V15, P2587, DOI 10.1109/TIFS.2020.2971142
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Davarzani R, 2016, MULTIMED TOOLS APPL, V75, P4639, DOI 10.1007/s11042-015-2496-6
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Hosny KM, 2018, CIRC SYST SIGNAL PR, V37, P5441, DOI 10.1007/s00034-018-0822-8
   Hou XD, 2007, PROC CVPR IEEE, P2280
   Hu YP, 2021, IEEE T IMAGE PROCESS, V30, P4667, DOI 10.1109/TIP.2021.3073867
   Huang X, 2016, IEEE TRUST BIG, P14, DOI [10.1109/TrustCom.2016.39, 10.1109/TrustCom.2016.0040]
   Huang ZQ, 2021, IEEE T CIRC SYST VID, V31, P2808, DOI 10.1109/TCSVT.2020.3027001
   Huang ZQ, 2021, IEEE T MULTIMEDIA, V23, P1516, DOI 10.1109/TMM.2020.2999188
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jin S, 2021, IEEE T IMAGE PROCESS, V30, P6130, DOI 10.1109/TIP.2021.3091895
   Kang C, 2019, IEEE T MULTIMEDIA, V21, P1563, DOI 10.1109/TMM.2018.2883868
   Kang LW, 2009, IEEE IMAGE PROC, P1285, DOI 10.1109/ICIP.2009.5413606
   Ke Y, 2004, PROC CVPR IEEE, P506
   Kim C, 2003, SIGNAL PROCESS-IMAGE, V18, P169, DOI 10.1016/S0923-5965(02)00130-3
   Lefebvre F., 2002, P EUR SIGN PROC C, P299
   Lei YQ, 2011, SIGNAL PROCESS-IMAGE, V26, P280, DOI 10.1016/j.image.2011.04.007
   Li B, 2013, IEEE T INSTRUM MEAS, V62, P304, DOI 10.1109/TIM.2012.2216476
   Li YN, 2018, IEEE SIGNAL PROC LET, V25, P140, DOI 10.1109/LSP.2017.2777881
   Li YN, 2012, IEEE T IMAGE PROCESS, V21, P1963, DOI 10.1109/TIP.2011.2171698
   Liang XP, 2021, MULTIMEDIA SYST, V27, P389, DOI 10.1007/s00530-020-00696-z
   Ling HF, 2013, SIGNAL PROCESS, V93, P2328, DOI 10.1016/j.sigpro.2012.08.011
   Liu SG, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3355394
   Lu CS, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P731, DOI 10.1109/ICME.2004.1394296
   Lv XD, 2012, IEEE T INF FOREN SEC, V7, P1081, DOI 10.1109/TIFS.2012.2190594
   Ou Y, 2009, 2009 INTERNATIONAL SYMPOSIUM ON INTELLIGENT SIGNAL PROCESSING AND COMMUNICATION SYSTEMS (ISPACS 2009), P595, DOI 10.1109/ISPACS.2009.5383770
   Ouyang JL, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2978572
   Paul Madhumita, 2019, 2019 International Conference on Automation, Computational and Technology Management (ICACTM). Proceedings, P464, DOI 10.1109/ICACTM.2019.8776713
   Pun CM, 2018, MULTIMED TOOLS APPL, V77, P11609, DOI 10.1007/s11042-017-4809-4
   Qin C, 2021, IEEE T CIRC SYST VID, V31, P4523, DOI 10.1109/TCSVT.2020.3047142
   Qin C, 2019, IEEE ACCESS, V7, P45460, DOI 10.1109/ACCESS.2019.2908029
   Qin C, 2018, INFORM SCIENCES, V423, P284, DOI 10.1016/j.ins.2017.09.060
   Qin C, 2018, SIGNAL PROCESS, V142, P194, DOI 10.1016/j.sigpro.2017.07.019
   r0k.us, Kodak Lossless True Color Image Suite
   robots, The PASCAL Visual Object Classes Challenge 2012 (VOC2012)
   Shen Q, 2020, SIGNAL PROCESS, V166, DOI 10.1016/j.sigpro.2019.107244
   Singh S. P., IEEE T DEPEND SECURE
   Swaminathan A, 2006, IEEE T INF FOREN SEC, V1, P215, DOI 10.1109/TIFS.2006.873601
   Tang ZJ, 2021, COMPUT J, V64, P1656, DOI 10.1093/comjnl/bxz127
   Tang ZJ, 2019, MATH BIOSCI ENG, V16, P6103, DOI 10.3934/mbe.2019305
   Tang ZJ, 2019, IEEE T KNOWL DATA EN, V31, P549, DOI 10.1109/TKDE.2018.2837745
   Tang ZJ, 2018, COMPUT J, V61, P1695, DOI 10.1093/comjnl/bxy047
   Tang ZJ, 2018, NEUROCOMPUTING, V308, P147, DOI 10.1016/j.neucom.2018.04.057
   Tang ZJ, 2017, SIGNAL PROCESS, V137, P240, DOI 10.1016/j.sigpro.2017.02.008
   Tang ZJ, 2016, COMPUT SECUR, V62, P133, DOI 10.1016/j.cose.2016.07.006
   Tang ZJ, 2016, IEEE T INF FOREN SEC, V11, P200, DOI 10.1109/TIFS.2015.2485163
   Tang ZJ, 2014, OPTIK, V125, P5102, DOI 10.1016/j.ijleo.2014.05.015
   Tang ZJ, 2014, IET IMAGE PROCESS, V8, P142, DOI 10.1049/iet-ipr.2013.0332
   Tang ZJ, 2014, IEEE T KNOWL DATA EN, V26, P711, DOI 10.1109/TKDE.2013.45
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Verma R, 2007, IEEE T MED IMAGING, V26, P772, DOI 10.1109/TMI.2006.891484
   Vikram TN, 2012, PATTERN RECOGN, V45, P3114, DOI 10.1016/j.patcog.2012.02.009
   Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P947, DOI 10.1109/34.955109
   Wang P., 2018, PROC IEEE INT C DIGI, P1
   Wang XF, 2015, IEEE T INF FOREN SEC, V10, P1336, DOI 10.1109/TIFS.2015.2407698
   Wu D, 2009, SIGNAL PROCESS, V89, P2415, DOI 10.1016/j.sigpro.2009.05.016
   Yang B, 2016, PATTERN RECOGN, V55, P215, DOI 10.1016/j.patcog.2016.02.001
   Yao JL, 2015, IEEE SIGNAL PROC LET, V22, P1404, DOI 10.1109/LSP.2014.2377795
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhao Y, 2020, IEEE ACCESS, V8, P26041, DOI 10.1109/ACCESS.2020.2970757
   Zhao Y, 2013, IEEE T INF FOREN SEC, V8, P55, DOI 10.1109/TIFS.2012.2223680
   Zhou ZL, 2017, IEEE T INF FOREN SEC, V12, P48, DOI 10.1109/TIFS.2016.2601065
   Zhu L, 2020, IEEE T IMAGE PROCESS, V29, P4643, DOI 10.1109/TIP.2020.2974065
NR 65
TC 14
Z9 14
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1085
EP 1097
DI 10.1109/TMM.2021.3139217
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100005
DA 2024-07-18
ER

PT J
AU Liu, H
   Ma, YN
   Hu, QY
   Guo, YL
AF Liu, Hao
   Ma, Yanni
   Hu, Qingyong
   Guo, Yulan
TI CenterTube: Tracking Multiple 3D Objects With 4D Tubelets in Dynamic
   Point Clouds
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D multi-object tracking; point cloud sequence; 4D tubelet
AB 3D Multi-Object Tracking (MOT) in dynamic point cloud sequences is a fundamental research problem for several downstream tasks such as motion planning and action recognition. Existing methods usually rely on the traditional tracking-by-detection (TBD) paradigm, which performs the tracking based on the results achieved by dedicated detectors. However, this two-stage framework usually cannot sufficiently exploit spatial-temporal information and end-to-end optimization, leading to sub-optimal tracking performance, especially when the object is partially or completely occluded. In this article, we propose a joint detection and tracking framework named CenterTube for dynamic point cloud sequences. The key to our approach is to formulate the problem of multiple object trajectory predictions as 4D tubelet detections. In particular, the proposed CenterTube is composed of three head branches, including a center branch, a regression branch, and a movement branch for the estimation of object center, object size, instance movement, and frame interval, respectively. Additionally, a Tube BEV-IoU (TB-IoU) is also presented to link the generated clip-level tubelets and form the final tracks. Extensive experiments conducted on the KITTI-MOT and nuScenes datasets demonstrate that our model achieves competitive performances even if no ready-made detection results is adopted.
C1 [Liu, Hao; Ma, Yanni; Hu, Qingyong; Guo, Yulan] Sun Yat Sen Univ, Sch Elect & Commun Engn, Shenzhen Campus, Shenzhen 518107, Peoples R China.
   [Hu, Qingyong] Univ Oxford, Dept Comp Sci, Oxford, England.
   [Hu, Qingyong] Natl Univ Def Technol, Coll Elect Sci & Technol, Changsha, Peoples R China.
C3 Sun Yat Sen University; University of Oxford; National University of
   Defense Technology - China
RP Guo, YL (corresponding author), Sun Yat Sen Univ, Sch Elect & Commun Engn, Shenzhen Campus, Shenzhen 518107, Peoples R China.
EM liuh327@mail2.sysu.edu.cn; mayn3@mail2.sysu.edu.cn;
   qingyong.hu@cs.ox.ac.uk; guoyulan@sysu.edu.cn
OI Hu, Qingyong/0000-0003-0337-9207; Liu, Hao/0000-0001-5955-1577
FU National Key Research and Development Program of China
FX No Statement Available
CR Aygün M, 2021, PROC CVPR IEEE, P5523, DOI 10.1109/CVPR46437.2021.00548
   Bai HX, 2021, PROC CVPR IEEE, P6715, DOI 10.1109/CVPR46437.2021.00665
   Baser E, 2019, IEEE INT VEH SYM, P1426, DOI [10.1109/ivs.2019.8813779, 10.1109/IVS.2019.8813779]
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164
   Chenhang He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11870, DOI 10.1109/CVPR42600.2020.01189
   Chiu HK, 2021, IEEE INT CONF ROBOT, P14227, DOI 10.1109/ICRA48506.2021.9561754
   Dong XP, 2021, IEEE T PATTERN ANAL, V43, P1515, DOI 10.1109/TPAMI.2019.2956703
   Dong XP, 2019, IEEE T IMAGE PROCESS, V28, P3516, DOI 10.1109/TIP.2019.2898567
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Feichtenhofer C, 2017, IEEE I CONF COMP VIS, P3057, DOI 10.1109/ICCV.2017.330
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Hou R, 2017, IEEE I CONF COMP VIS, P5823, DOI 10.1109/ICCV.2017.620
   Hu HN, 2019, IEEE I CONF COMP VIS, P5389, DOI 10.1109/ICCV.2019.00549
   Kalogeiton V, 2017, IEEE I CONF COMP VIS, P4415, DOI 10.1109/ICCV.2017.472
   Kang K, 2018, IEEE T CIRC SYST VID, V28, P2896, DOI 10.1109/TCSVT.2017.2736553
   Kang K, 2016, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2016.95
   Kim A, 2022, LECT NOTES COMPUT SC, V13682, P41, DOI 10.1007/978-3-031-20047-2_3
   Kim A, 2021, IEEE INT CONF ROBOT, P11315, DOI 10.1109/ICRA48506.2021.9562072
   Kingma D. P., 2014, arXiv
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Liang M, 2019, PROC CVPR IEEE, P7337, DOI 10.1109/CVPR.2019.00752
   Liang ZD, 2021, PROC CVPR IEEE, P7136, DOI 10.1109/CVPR46437.2021.00706
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu CY, 2020, IEEE T IMAGE PROCESS, V29, P277, DOI 10.1109/TIP.2019.2928123
   Liu H, 2021, IEEE T MULTIMEDIA, V23, P2045, DOI 10.1109/TMM.2020.3007331
   Luo CX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10468, DOI 10.1109/ICCV48922.2021.01032
   Meinhardt T, 2022, PROC CVPR IEEE, P8834, DOI 10.1109/CVPR52688.2022.00864
   Meng QH, 2022, IEEE T PATTERN ANAL, V44, P4454, DOI 10.1109/TPAMI.2021.3063611
   Osep A., 2017, P IEEE INT C ROB AUT, P1988
   Pang B, 2020, PROC CVPR IEEE, P6307, DOI 10.1109/CVPR42600.2020.00634
   Pang JM, 2021, PROC CVPR IEEE, P164, DOI 10.1109/CVPR46437.2021.00023
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Sharma S, 2018, IEEE INT CONF ROBOT, P3508, DOI 10.1109/ICRA.2018.8461018
   Shen JB, 2022, IEEE T PATTERN ANAL, V44, P8896, DOI 10.1109/TPAMI.2021.3127492
   Shen JB, 2020, IEEE T CYBERNETICS, V50, P3068, DOI 10.1109/TCYB.2019.2936503
   Shen JB, 2019, IEEE T CYBERNETICS, V49, P1990, DOI 10.1109/TCYB.2018.2803217
   Shen JB, 2018, IEEE T INTELL TRANSP, V19, P162, DOI 10.1109/TITS.2017.2750082
   Shenoi A, 2020, IEEE INT C INT ROBOT, P10335, DOI 10.1109/IROS45743.2020.9341635
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Shi WJ, 2020, PROC CVPR IEEE, P1708, DOI 10.1109/CVPR42600.2020.00178
   Shuai B, 2021, PROC CVPR IEEE, P12367, DOI 10.1109/CVPR46437.2021.01219
   Simon M, 2019, IEEE COMPUT SOC CONF, P1190, DOI 10.1109/CVPRW.2019.00158
   Sun Jiaming, 2021, 2021 IEEE/CVF International Conference on Computer Vision (ICCV), P3265, DOI 10.1109/ICCV48922.2021.00317
   Sun PZ, 2021, Arxiv, DOI arXiv:2012.15460
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang SK, 2021, IEEE ROBOT AUTOM LET, V6, P3397, DOI 10.1109/LRA.2021.3062016
   Wang SK, 2020, IEEE ROBOT AUTOM LET, V5, P3206, DOI 10.1109/LRA.2020.2974392
   Weber M., 2021, arXiv
   Weng X., 2020, P EUR C COMP VIS WOR
   Weng XS, 2020, PROC CVPR IEEE, P6498, DOI 10.1109/CVPR42600.2020.00653
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Wu H, 2022, IEEE T INTELL TRANSP, V23, P5668, DOI 10.1109/TITS.2021.3055616
   Wu Hai, 2021, P INT JOINT C ART IN, P1165, DOI [10.24963/ijcai.2021/161, DOI 10.24963/IJCAI.2021/161]
   Wu JL, 2021, PROC CVPR IEEE, P12347, DOI 10.1109/CVPR46437.2021.01217
   Xingyi Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P474, DOI 10.1007/978-3-030-58548-8_28
   Yan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11710, DOI 10.1109/CVPR42600.2020.01173
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yin JB, 2023, IEEE T PATTERN ANAL, V45, P9822, DOI 10.1109/TPAMI.2021.3125981
   Yin JB, 2020, PROC CVPR IEEE, P6767, DOI 10.1109/CVPR42600.2020.00680
   Yin TW, 2021, PROC CVPR IEEE, P11779, DOI 10.1109/CVPR46437.2021.01161
   Zaech JN, 2022, IEEE ROBOT AUTOM LET, V7, P5103, DOI 10.1109/LRA.2022.3145952
   Zeng FG, 2022, LECT NOTES COMPUT SC, V13687, P659, DOI 10.1007/978-3-031-19812-0_38
   Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105
   Zhang WW, 2019, IEEE I CONF COMP VIS, P2365, DOI 10.1109/ICCV.2019.00245
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
   Zhang YP, 2021, IEEE T MULTIMEDIA, V23, P4232, DOI 10.1109/TMM.2020.3038310
   Zheng W, 2021, PROC CVPR IEEE, P14489, DOI 10.1109/CVPR46437.2021.01426
   Zhou XY, 2019, Arxiv, DOI [arXiv:1904.07850, 10.48550/arXiv.1904.07850]
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
   Zhu BJ, 2019, Arxiv, DOI arXiv:1908.09492
   Zhu G., 2016, P AS C COMP VIS, P288
   Zhu HQ, 2023, IEEE T MULTIMEDIA, V25, P5291, DOI 10.1109/TMM.2022.3189778
NR 75
TC 3
Z9 3
U1 11
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8793
EP 8804
DI 10.1109/TMM.2023.3241548
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000046
DA 2024-07-18
ER

PT J
AU Liu, H
   Ma, M
   Gao, ZX
   Deng, ZY
   Li, FJ
   Li, ZD
AF Liu, Hao
   Ma, Mei
   Gao, Zixian
   Deng, Zongyong
   Li, Fengjun
   Li, Zhendong
TI Siamese Graph Learning for Semi-Supervised Age Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Aging; Training; Estimation; Faces; Annotations; Semisupervised
   learning; Feature extraction; Contrastive learning; facial age
   estimation; graph learning; semi-supervised learning
AB In this paper, we propose a Siamese graph learning (SGL) approach to alleviate aging dataset bias. While numerous semi-supervised algorithms have been successfully applied to classification tasks, most of them assume that both the labeled and unlabeled samples are drawn from identical distributions. However, this assumption may not hold due to the heterogeneity of face aging data, which gives rise to a bias and unpromising prediction. Motivated by this, our SGL learns to align the sparse distribution with the dense one for dataset debias with preserving the real aging smoothness. To achieve this, we adopt a mixup strategy to plausibly generate hallucinatory samples, which leverages amounts of unlabeled data to enhance the diversity of unbalanced classes. Moreover, we develop a graph contrastive regularization to suppress the noise introduced by auxiliary unlabeled samples. Extensive experimental results show compelling performance by only utilizing the limited scalability of training annotations.
C1 [Liu, Hao; Ma, Mei; Li, Zhendong] Ningxia Univ, Sch Informat Engn, Yinchuan 750021, Peoples R China.
   [Gao, Zixian; Deng, Zongyong] Ningxia Univ, Yinchuan 750021, Peoples R China.
   [Li, Fengjun] Ningxia Univ, Sch Math & Stat, Yinchuan 750021, Peoples R China.
   [Deng, Zongyong] Sichuan Univ, Chengdu 610065, Peoples R China.
C3 Ningxia University; Ningxia University; Ningxia University; Sichuan
   University
RP Li, ZD (corresponding author), Ningxia Univ, Sch Informat Engn, Yinchuan 750021, Peoples R China.
EM liuhao@nxu.edu.cn; mamei@stu.nxu.edu.cn; zixian.gao_nxu@outlook.com;
   dengzongyong@stu.scu.edu.cn; fjli@nxu.edu.cn;
   lizhendong13@mails.ucas.ac.cn
OI Mei, Ma/0009-0000-2800-2663
FU National Natural Science Foundation of China
FX No Statement Available
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Angulu R, 2018, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-018-0278-6
   Bao ZH, 2023, IEEE T INF FOREN SEC, V18, P221, DOI 10.1109/TIFS.2022.3218431
   Berthelot D, 2019, ADV NEUR IN, V32
   Calder J, 2020, PR MACH LEARN RES, V119
   Chang KY, 2011, PROC CVPR IEEE, P585, DOI 10.1109/CVPR.2011.5995437
   Chen J., 2020, P 37 INT C MACHINE L, P1704
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cowie R, 2001, IEEE SIGNAL PROC MAG, V18, P32, DOI 10.1109/79.911197
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng Z., 2020, P IEEE INT C MULT EX, P1
   Deng ZY, 2021, PROC CVPR IEEE, P10498, DOI 10.1109/CVPR46437.2021.01036
   El Dib MY, 2010, IEEE IMAGE PROC, P1589, DOI 10.1109/ICIP.2010.5651440
   Escalera S, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P243, DOI 10.1109/ICCVW.2015.40
   Flament F, 2021, SKIN RES TECHNOL, V27, P1081, DOI 10.1111/srt.13061
   Fu Y, 2008, IEEE T MULTIMEDIA, V10, P578, DOI 10.1109/TMM.2008.921847
   Gao BB, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P712
   Gao BB, 2017, IEEE T IMAGE PROCESS, V26, P2825, DOI 10.1109/TIP.2017.2689998
   Ge SM, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3536426
   Ge SM, 2019, IEEE T IMAGE PROCESS, V28, P2051, DOI 10.1109/TIP.2018.2883743
   Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733
   Graves A., 2017, INT C MACHINE LEARNI, P1311
   Guarnera L, 2020, IEEE COMPUT SOC CONF, P2841, DOI 10.1109/CVPRW50498.2020.00341
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou P, 2017, AAAI CONF ARTIF INTE, P2015
   Hu ZJ, 2021, PROC CVPR IEEE, P15094, DOI 10.1109/CVPR46437.2021.01485
   Kimet J., 2020, P 34 INT C NEUR INF
   Kingma D. P., 2015, P INT C LEARN REPR, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Lai YX, 2021, IEEE T KNOWL DATA EN, V33, P1877, DOI 10.1109/TKDE.2019.2952596
   Laine Samuli, 2017, 5 INT C LEARNING REP, DOI DOI 10.48550/ARXIV.1610.02242
   Li C.-L., 2020, Advances in neural information processing systems, V33, P596
   Li S., 2021, IEEE Trans. Multimedia, V25, P1045
   Li SK, 2022, PROC CVPR IEEE, P316, DOI 10.1109/CVPR52688.2022.00041
   Li WH, 2021, PROC CVPR IEEE, P13891, DOI 10.1109/CVPR46437.2021.01368
   Li WH, 2019, PROC CVPR IEEE, P1145, DOI 10.1109/CVPR.2019.00124
   LIN JH, 1991, IEEE T INFORM THEORY, V37, P145, DOI 10.1109/18.61115
   Liu H, 2017, IEEE INT CONF AUTOMA, P157, DOI 10.1109/FG.2017.28
   Liu N, 2020, IEEE ACCESS, V8, P92441, DOI 10.1109/ACCESS.2020.2994322
   Liu X, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P258, DOI 10.1109/ICCVW.2015.42
   Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264
   Lokhande Vishnu Suresh, 2020, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, V2020, P11432, DOI [10.1109/cvpr42600.2020.01145, 10.1109/CVPR42600.2020.01145]
   Loshchilov I., 2017, P INT C LEARN REPR
   Mérillou S, 2008, COMPUT GRAPH-UK, V32, P159, DOI 10.1016/j.cag.2008.01.003
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Niu ZX, 2016, PROC CVPR IEEE, P4920, DOI 10.1109/CVPR.2016.532
   Pan HY, 2018, PROC CVPR IEEE, P5285, DOI 10.1109/CVPR.2018.00554
   Panis G, 2016, IET BIOMETRICS, V5, P37, DOI 10.1049/iet-bmt.2014.0053
   Qizhe Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10684, DOI 10.1109/CVPR42600.2020.01070
   Radosavovic I, 2018, PROC CVPR IEEE, P4119, DOI 10.1109/CVPR.2018.00433
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Rothe R, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P252, DOI 10.1109/ICCVW.2015.41
   Shen W, 2018, PROC CVPR IEEE, P2304, DOI 10.1109/CVPR.2018.00245
   Shu XB, 2018, IEEE T PATTERN ANAL, V40, P905, DOI 10.1109/TPAMI.2017.2705122
   Tan ZC, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3548
   Tan ZC, 2018, IEEE T PATTERN ANAL, V40, P2610, DOI 10.1109/TPAMI.2017.2779808
   Tarvainen A., 2017, P INT C NEUR INF PRO
   Verma V, 2019, PR MACH LEARN RES, V97
   Wang Q, 2019, IEEE I CONF COMP VIS, P1466, DOI 10.1109/ICCV.2019.00155
   Xie JC, 2020, IEEE T INF FOREN SEC, V15, P2361, DOI 10.1109/TIFS.2020.2965298
   Xin Wen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P379, DOI 10.1007/978-3-030-58592-1_23
   Yang H., 2015, P BRIT MACH VIS C
   Yang Shuo, 2022, INT C MACHINE LEARNI, P25302
   Yi D, 2015, LECT NOTES COMPUT SC, V9005, P144, DOI 10.1007/978-3-319-16811-1_10
   Zhang C, 2019, PROC CVPR IEEE, P12579, DOI 10.1109/CVPR.2019.01287
   Zhang K, 2020, IEEE T CIRC SYST VID, V30, P3140, DOI 10.1109/TCSVT.2019.2936410
   Zhang X., 2020, INT C MACHINE LEARNI, P11288
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhu CC, 2020, AAAI CONF ARTIF INTE, V34, P13090
NR 69
TC 1
Z9 1
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9586
EP 9596
DI 10.1109/TMM.2023.3256065
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200033
DA 2024-07-18
ER

PT J
AU Park, S
   Han, DK
   Elhilali, M
AF Park, Sangwook
   Han, David K.
   Elhilali, Mounya
TI Cross-Referencing Self-Training Network for Sound Event Detection in
   Audio Mixtures
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Pseudo label; sound event detection; semi-supervised learning;
   self-training
AB Sound event detection is an important facet of audio tagging that aims to identify sounds of interest and define both the sound category and time boundaries for each sound event in a continuous recording. With advances in deep neural networks, there has been tremendous improvement in the performance of sound event detection systems, although at the expense of costly data collection and labeling efforts. In fact, current state-of-the-art methods employ supervised training methods that leverage large amounts of data samples and corresponding labels in order to facilitate identification of sound category and time stamps of events. As an alternative, the current study proposes a semi-supervised method for generating pseudo-labels from unsupervised data using a student-teacher scheme that balances self-training and cross-training. Additionally, this paper explores post-processing which extracts sound intervals from network prediction, for further improvement in sound event detection performance. The proposed approach is evaluated on sound event detection task for the DCASE2020 challenge. The results of these methods on both "validation" and "public evaluation" sets of DESED database show significant improvement compared to the state-of-the art systems in semi-supervised learning.
C1 [Park, Sangwook] Gangneung Wonju Natl Univ, Dept Elect Engn, Kangnung 25457, South Korea.
   [Han, David K.] Drexel Univ, Dept Elect & Comp Engn, Philadelphia, PA 19104 USA.
   [Elhilali, Mounya] Johns Hopkins Univ, Dept Elect & Comp Engn, Baltimore, MD 21218 USA.
   [Elhilali, Mounya] Johns Hopkins Univ, Dept Psychol & Brain Sci, Baltimore, MD 21218 USA.
C3 Kangnung Wonju National University; Drexel University; Johns Hopkins
   University; Johns Hopkins University
RP Elhilali, M (corresponding author), Johns Hopkins Univ, Dept Elect & Comp Engn, Baltimore, MD 21218 USA.; Elhilali, M (corresponding author), Johns Hopkins Univ, Dept Psychol & Brain Sci, Baltimore, MD 21218 USA.
EM spark190@jhu.edu; dkh42@drexel.edu; mounya@jhu.edu
RI Elhilali, Mounya/A-3396-2010
OI Elhilali, Mounya/0000-0003-2597-738X
FU NIH [U01AG058532]; ONR [N00014-17-1-2736, N00014-19-1-2689]; NSF
   [1734744]
FX This work was supported in part by NIH under Grant U01AG058532, in part
   by ONR under Grants N00014-17-1-2736 and N00014-19-1-2689, and in part
   by NSF under Grant 1734744.
CR Ahmad K, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3306240
   Bengio Y., 2006, P SEM LEARN, P29
   Broadwater JB, 2010, IEEE T SIGNAL PROCES, V58, P490, DOI 10.1109/TSP.2009.2031285
   Çakir E, 2017, IEEE-ACM T AUDIO SPE, V25, P1291, DOI 10.1109/TASLP.2017.2690575
   Chapelle O., 2005, Proceedings of the tenth international workshop on artificial intelligence and statistics, P57
   Dara R, 2002, IEEE IJCNN, P2237, DOI 10.1109/IJCNN.2002.1007489
   Dekkers G., 2017, Detection and Classification of Acoustic Scenes and Events, P1
   Derrick B, 2017, QUANT METH PSYCHOL, V13, P120, DOI 10.20982/tqmp.13.2.p120
   Dópido I, 2013, IEEE T GEOSCI REMOTE, V51, P4032, DOI 10.1109/TGRS.2012.2228275
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Font F., 2013, P 2013 ACM MULTIMEDI, P411, DOI 10.1145/2502081.2502245
   Gençay R, 2001, STUD NONLINEAR DYN E, V5, P213, DOI 10.1162/10811820160080103
   Grandvalet Y., 2004, P ADV NEUR INF PROC, P1
   Gulati A, 2020, INTERSPEECH, P5036, DOI 10.21437/Interspeech.2020-3015
   Huang Y., 2020, P DCASE, P61
   Johnson JM, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0192-5
   Kang S, 2017, J KOREAN STAT SOC, V46, P487, DOI 10.1016/j.jkss.2017.02.003
   Kim NK, 2021, IEEE ACCESS, V9, P7564, DOI 10.1109/ACCESS.2020.3048675
   Koh CY, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P376, DOI 10.1109/ICASSP39728.2021.9414350
   Kong QQ, 2020, IEEE-ACM T AUDIO SPE, V28, P2450, DOI 10.1109/TASLP.2020.3014737
   Kothinti S, 2019, INT CONF ACOUST SPEE, P36, DOI 10.1109/ICASSP.2019.8682772
   Lagarias JC, 1998, SIAM J OPTIMIZ, V9, P112, DOI 10.1137/S1052623496303470
   Lavner Y., 2017, P IEEE INT C SCI EL, P1
   Lee D.-H., 2013, WORKSHOP CHALLENGES, V3, P896
   Lee H, 2016, IEEE IMAGE PROC, P3713, DOI 10.1109/ICIP.2016.7533053
   Lin LW, 2020, INT CONF ACOUST SPEE, P626, DOI [10.1109/ICASSP40776.2020.9053584, 10.1109/icassp40776.2020.9053584]
   Liu Y., 2020, P IEEECVF C COMPUTER, P4
   Mesaros A, 2016, APPL SCI-BASEL, V6, DOI 10.3390/app6060162
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Miyazaki K., 2020, P DET CLASS AC SCEN, P2
   Mun S, 2017, INT CONF ACOUST SPEE, P796, DOI 10.1109/ICASSP.2017.7952265
   Oliver A, 2018, ADV NEUR IN, V31
   Park S, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P341, DOI 10.1109/ICASSP39728.2021.9414450
   Park S, 2017, INT CONF ACOUST SPEE, P761, DOI 10.1109/ICASSP.2017.7952258
   Pouyanfar S, 2018, IEEE 1ST CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2018), P112, DOI 10.1109/MIPR.2018.00027
   Rosenberg C, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P29
   Sajjadi M, 2016, ADV NEUR IN, V29
   Samuli L., 2017, ICLR, P1
   Serizel, 2020, P DET CLASS AC SCEN, P200
   Shi BW, 2019, INT CONF ACOUST SPEE, P750, DOI 10.1109/ICASSP.2019.8683710
   Sindhwani V., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P477, DOI 10.1145/1148170.1148253
   Snyder D, 2015, Arxiv, DOI arXiv:1510.08484
   Tarvainen A, 2017, ADV NEUR IN, V30
   Turpault N., 2019, WORKSH DET CLASS AC, DOI DOI 10.33682/006B-JX26
   van Engelen JE, 2020, MACH LEARN, V109, P373, DOI 10.1007/s10994-019-05855-6
   Verma V, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3635
   Vesperini F, 2019, IEEE J-STSP, V13, P310, DOI 10.1109/JSTSP.2019.2902305
   Wei C., 2021, INT C LEARN REPR
   Yang MS, 2012, PATTERN RECOGN, V45, P3950, DOI 10.1016/j.patcog.2012.04.031
   Yao T., 2020, P DET CLASS AC SCEN, P205
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
NR 51
TC 4
Z9 4
U1 11
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4573
EP 4585
DI 10.1109/TMM.2022.3178591
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200036
PM 37928617
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Pei, JL
   Cheng, TY
   Tang, H
   Chen, CB
AF Pei, Jialun
   Cheng, Tianyang
   Tang, He
   Chen, Chuanbo
TI Transformer-Based Efficient Salient Instance Segmentation Networks With
   Orientative Query
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Task analysis; Object detection; Image segmentation;
   Decoding; Visualization; Training; Salient instance segmentation; deep
   learning; vision transformer; attention model
ID OBJECT DETECTION
AB Salient instance segmentation (SIS) can be considered as the next generation task for the saliency detection community. Most of the existing state-of-the-art methods used for this novel challenging task are built on the mainstream Mask R-CNN architecture. However, this mechanism relies heavily on hand-designed anchors and NMS post-processing. In this paper, we provide a one stage SIS framework with transformers, termed Orientative Query Transformer (OQTR). To leverage the long-range dependencies of transformers, a cross fusion module is designed to efficiently fuse the global features in the encoder and salient query features for salient mask prediction. Furthermore, derived from the center prior in traditional saliency models, we propose an orientative query that is considered as the initial salient object query to accelerate convergence. In addition, to mitigate the issue of the lack of a large-scale dataset with salient instance labels, we collect a new SIS dataset (SIS10 K) containing over 10 K images elaborately annotated with both object- and instance-level labels to promote the community. Without any post-processing, our end-to-end OQTR framework significantly surpasses the top-1 RDPNet by an average of 13.1% AP scores across all three challenging datasets, demonstrating the strong performance of the proposed OQTR. The code and the dataset proposed in this work are available at: https://github.com/ssecv/OQTR.
C1 [Pei, Jialun] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
   [Cheng, Tianyang; Tang, He; Chen, Chuanbo] Huazhong Univ Sci & Technol, Sch Software Engn, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology
RP Tang, H (corresponding author), Huazhong Univ Sci & Technol, Sch Software Engn, Wuhan 430074, Peoples R China.
EM peijl@hust.edu.cn; patrickcty@hust.edu.cn; hetang@hust.edu.cn;
   chuanboc@163.com
RI Pei, jialun/HGA-9920-2022
OI Pei, jialun/0000-0002-2630-2838; Tang, He/0000-0002-8454-1407
FU National Natural Science Foundation of China [61902139]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61902139.
CR Al-Rfou R, 2019, AAAI CONF ARTIF INTE, P3159
   Arbeláez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Dai ZG, 2021, PROC CVPR IEEE, P1601, DOI 10.1109/CVPR46437.2021.00165
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Duan LJ, 2011, PROC CVPR IEEE, P473, DOI 10.1109/CVPR.2011.5995676
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P4339, DOI 10.1109/TPAMI.2021.3060412
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fan DP, 2018, LECT NOTES COMPUT SC, V11219, P196, DOI 10.1007/978-3-030-01267-0_12
   Fan RC, 2019, PROC CVPR IEEE, P6096, DOI 10.1109/CVPR.2019.00626
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Han K, 2022, Arxiv, DOI arXiv:2012.12556
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong S, 2015, PR MACH LEARN RES, V37, P597
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Huang NAC, 2022, IEEE T MULTIMEDIA, V24, P1651, DOI 10.1109/TMM.2021.3069297
   Jun Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13022, DOI 10.1109/CVPR42600.2020.01304
   Lazebnik S, 2005, IEEE T PATTERN ANAL, V27, P1265, DOI 10.1109/TPAMI.2005.151
   Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Li YX, 2020, IEEE T MULTIMEDIA, V22, P1153, DOI 10.1109/TMM.2019.2940851
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu RJ, 2021, IEEE WINT CONF APPL, P3693, DOI 10.1109/WACV48630.2021.00374
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Loshchilov I, 2019, Arxiv, DOI arXiv:1711.05101
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Pei JL, 2020, NEUROCOMPUTING, V402, P423, DOI 10.1016/j.neucom.2020.04.022
   Schlemper J, 2019, MED IMAGE ANAL, V53, P197, DOI 10.1016/j.media.2019.01.012
   Tay Y., 2021, PMLR, P10183
   Tu ZZ, 2020, IEEE T MULTIMEDIA, V22, P160, DOI 10.1109/TMM.2019.2924578
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang XG, 2021, PROC CVPR IEEE, P10220, DOI 10.1109/CVPR46437.2021.01009
   Wu YH, 2021, IEEE T IMAGE PROCESS, V30, P3897, DOI 10.1109/TIP.2021.3065822
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1109/CSTIC.2018.8369274, 10.1007/s11263-019-01198-w]
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xinlong Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P649, DOI 10.1007/978-3-030-58523-5_38
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yang SC, 2017, IEEE T IND ELECTRON, V64, P1092, DOI 10.1109/TIE.2016.2612175
   Yanhong Zeng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P528, DOI 10.1007/978-3-030-58517-4_31
   Yuqing Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9310, DOI 10.1109/CVPR42600.2020.00933
   Zeng Y, 2019, IEEE I CONF COMP VIS, P7222, DOI 10.1109/ICCV.2019.00732
   Zhang JM, 2016, PROC CVPR IEEE, P5733, DOI 10.1109/CVPR.2016.618
   Zhang JM, 2013, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2013.26
   Zhang J, 2022, IEEE T PATTERN ANAL, V44, P5761, DOI 10.1109/TPAMI.2021.3073564
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang YF, 2022, IEEE T MULTIMEDIA, V24, P755, DOI 10.1109/TMM.2021.3058788
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhu XZ, 2021, Arxiv, DOI arXiv:2010.04159
NR 66
TC 11
Z9 11
U1 2
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1964
EP 1978
DI 10.1109/TMM.2022.3141891
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100030
DA 2024-07-18
ER

PT J
AU Roodaki, H
   Bojnordi, MN
AF Roodaki, Hoda
   Bojnordi, Mahdi Nazm
TI Compressed Geometric Arrays for Point Cloud Processing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point cloud; data representation; memory bandwidth
AB The ever-increasing demand for immersive applications has made point cloud an important data type for 3D processing. Tree-based data structures are commonly used for representing point clouds where memory pointers are used to realize the connection among points. The significant cost of data storage and irregular access patterns for processing points make such data structures largely inefficient. In this paper, we examine a point cloud representation using compressed geometric arrays (CGA) that reduces the size of point cloud and limits the amount of memory indirection. Our experimental results on a set of critical point cloud operations indicate 998x speed-up, 410x better bandwidth utilization, and 58% storage reduction for CGA over the state-of-the-art point cloud library (PCL).
C1 [Roodaki, Hoda] KN Toosi Univ Technol, Dept Comp Engn, Tehran 163151355, Iran.
   [Roodaki, Hoda] Inst Res Fundamental Sci IPM, Sch Comp Sci, Tehran, Iran.
   [Bojnordi, Mahdi Nazm] Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.
C3 K. N. Toosi University of Technology; Utah System of Higher Education;
   University of Utah
RP Roodaki, H (corresponding author), KN Toosi Univ Technol, Dept Comp Engn, Tehran 163151355, Iran.
EM hroodaki@kntu.ac.ir; bojnordi@cs.utah.edu
CR Baciu S, 2020, INT C INTELL COMP CO, P293, DOI [10.1109/ICCP51029.2020.9266131, 10.1109/ICCP48234.2019.8959666]
   Chen JX, 2019, INT CONF SOFTW ENG, P681, DOI [10.1109/ICSESS47205.2019.9040768, 10.1109/icsess47205.2019.9040768]
   Chen SH, 2021, IEEE SIGNAL PROC MAG, V38, P68, DOI 10.1109/MSP.2020.2984780
   dEon E., 2018, ISO/IEC JTC1/SC29 WG11 (MPEG) input document m42914
   Du J, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21051625
   GARGANTINI I, 1982, COMMUN ACM, V25, P905, DOI 10.1145/358728.358741
   Graziosi D, 2020, APSIPA TRANS SIGNAL, V9, DOI 10.1017/ATSIP.2020.12
   Guo JY, 2022, IEEE T CIRC SYST VID, V32, P3659, DOI 10.1109/TCSVT.2021.3105820
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Jang ES, 2019, IEEE SIGNAL PROC MAG, V36, P118, DOI 10.1109/MSP.2019.2900721
   Krivokuca Maja, 2018, ISO/ IEC JTC1/ SC29 WG11 (MPEG) input document m42914
   Lersch JR, 2004, P SOC PHOTO-OPT INS, V5412, P345, DOI 10.1117/12.542701
   Lyu Y., 2020, P IEEE CVF C COMP VI, P12252, DOI DOI 10.1109/CVPR42600.2020.01227
   Matiukas V, 2011, ELEKTRON ELEKTROTECH, P73
   Mekuria R, 2017, IEEE T CIRC SYST VID, V27, P828, DOI 10.1109/TCSVT.2016.2543039
   Miknis M, 2015, INT CONF SYST SIGNAL, P153, DOI 10.1109/IWSSIP.2015.7314200
   Miltiadou M, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13040559
   Niessner M., 2013, ACM T GRAPHIC, V32, DOI DOI 10.1145/2508363.2508374
   opencellid, US
   Pinkham R, 2020, INT S HIGH PERF COMP, P180, DOI 10.1109/HPCA47549.2020.00024
   Reinders James, 2007, Intel threading building blocks-outfitting C++ for multi-core processor parallelism
   Roodaki H, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P1925, DOI 10.1109/ICASSP39728.2021.9413902
   Roynard X, 2018, INT J ROBOT RES, V37, P545, DOI 10.1177/0278364918767506
   Samet H., 1989, DESIGN ANAL SPATIAL
   software.intel, Intel vtune profiler
   Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187
   Truong Q.H., 2013, Knowledge-based 3D point clouds processing
   Vallet B, 2015, COMPUT GRAPH-UK, V49, P126, DOI 10.1016/j.cag.2015.03.004
   Wang J., 2021, PROC DATA COMPRESSIO, P1
   Wang JQ, 2021, IEEE T CIRC SYST VID, V31, P4909, DOI 10.1109/TCSVT.2021.3051377
   Xu YQ, 2021, IEEE T CIRC SYST VID, V31, P1968, DOI 10.1109/TCSVT.2020.3015901
   Zhang X, 2020, IEEE DATA COMPR CONF, P73, DOI 10.1109/DCC47342.2020.00015
NR 32
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8204
EP 8211
DI 10.1109/TMM.2022.3233256
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000061
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Shao, SW
   Li, R
   Pei, ZC
   Liu, Z
   Chen, WH
   Zhu, WT
   Wu, XM
   Zhang, BC
AF Shao, Shuwei
   Li, Ran
   Pei, Zhongcai
   Liu, Zhong
   Chen, Weihai
   Zhu, Wentao
   Wu, Xingming
   Zhang, Baochang
TI Towards Comprehensive Monocular Depth Estimation: Multiple Heads are
   Better Than One
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Monocular depth estimation; ensemble learning; deep learning;
   transformer; convolutional neural network
ID NEURAL-NETWORK
AB Depth estimation attracts widespread attention in the computer vision community. However, it is still quite difficult to recover an accurate depth map using only one RGB image. We observe a phenomenon that existing methods tend to fail in different cases, caused by differences in network architecture, loss function and so on. In this work, we investigate into the phenomenon and propose to integrate the strengths of multiple weak depth predictor to build a comprehensive and accurate depth predictor, which is critical for many real-world applications, e.g., 3D reconstruction. Specifically, we construct multiple base (weak) depth predictors by utilizing different Transformer-based and convolutional neural network (CNN)-based architectures. Transformer establishes long-range correlation while CNN preserves local information ignored by Transformer due to the spatial inductive bias. Therefore, the coupling of Transformer and CNN contributes to the generation of complementary depth estimates, which are essential to achieve a comprehensive depth predictor. Then, we design mixers to learn from multiple weak predictions and adaptively fuse them into a strong depth estimate. The resultant model, which we refer to as Transformer-assisted depth ensembles (TEDepth). On the standard NYU-Depth-v2 and KITTI datasets, we thoroughly explore how the neural ensembles affect the depth estimation and demonstrate that our TEDepth achieves better results than previous state-of-the-art approaches. To validate the generalizability across cameras, we directly apply the models trained on NYU-Depth-v2 to the SUN RGB-D dataset without any fine-tuning, and the superior results emphasize its strong generalizability.
C1 [Shao, Shuwei; Pei, Zhongcai; Chen, Weihai] Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China.
   [Shao, Shuwei; Pei, Zhongcai; Chen, Weihai] Beihang Univ, Hangzhou Innovat Inst, Hangzhou 100191, Peoples R China.
   [Li, Ran; Liu, Zhong; Wu, Xingming; Zhang, Baochang] Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China.
   [Li, Ran; Liu, Zhong; Wu, Xingming; Zhang, Baochang] Beihang Univ, Hangzhou Innovat Inst, Hangzhou, Peoples R China.
   [Zhu, Wentao] Amazon, Seattle, WA 98109 USA.
C3 Beihang University; Beihang University; Beihang University; Beihang
   University; Amazon.com
RP Chen, WH (corresponding author), Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China.; Liu, Z; Zhang, BC (corresponding author), Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China.
EM swshao@buaa.edu.cn; rnlee1998@buaa.edu.cn; peizc@buaa.edu.cn;
   liuzhong@buaa.edu.cn; whchen@buaa.edu.cn; wentaoz1@uci.edu;
   wxmbuaa@163.com; bczhang@buaa.edu.cn
RI Shen, Yan/KEJ-4617-2024; Liu, Yining/KHC-6217-2024; Qi,
   Ling/KHE-3068-2024; zhou, chen/KBC-4023-2024; Li, Ran/GVT-7550-2022
OI Liu, Yining/0000-0002-2218-2349; Li, Ran/0000-0002-0487-1570; Chen,
   Weihai/0000-0001-7912-4505; Shao, Shuwei/0000-0001-8057-1599
FU Macao Science and Technology Development Fund [0022/2019/AKP]; Key
   Research and Development Program of Zhejiang Province [2020C01109];
   Beijing Natural Science Foundation [L223024]; National Natural Science
   Foundation of China [62076016, 61620106012, 61573048]
FX This work was supported in part by the Macao Science and Technology
   Development Fund under Grant 0022/2019/AKP, in part by the Key Research
   and Development Program of Zhejiang Province under Grant 2020C01109, in
   part by Beijing Natural Science Foundation under Grant L223024, and in
   part by the National Natural Science Foundation of China under Grants
   62076016, 61620106012, and 61573048.
CR Bauer E, 1999, MACH LEARN, V36, P105, DOI 10.1023/A:1007515423169
   Cao YZH, 2018, IEEE T CIRC SYST VID, V28, P3174, DOI 10.1109/TCSVT.2017.2740321
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen XJ, 2021, IEEE T NEUR NET LEAR, V32, P5034, DOI 10.1109/TNNLS.2020.3026669
   Clevert DA, 2016, Arxiv, DOI arXiv:1511.07289
   Dong XB, 2020, FRONT COMPUT SCI-CHI, V14, P241, DOI 10.1007/s11704-019-8208-z
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Eigen D, 2014, ADV NEUR IN, V27
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Gao W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2866, DOI 10.1109/ICCV48922.2021.00288
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Hazirbas C, 2017, LECT NOTES COMPUT SC, V10111, P213, DOI 10.1007/978-3-319-54181-5_14
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G., 2017, ARXIV170400109
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Islam M, 2003, IEEE T NEURAL NETWOR, V14, P820, DOI 10.1109/TNN.2003.813832
   Jiang Yifan, 2021, ARXIV210207074
   Jinqing Zhang, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12538), P149, DOI 10.1007/978-3-030-66823-5_9
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Lakshminarayanan B, 2017, ADV NEUR IN, V30
   Lam Huynh, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P581, DOI 10.1007/978-3-030-58574-7_35
   Lee Jin Han, 2019, arXiv
   Lee W., 2011, ICAT '11, P126
   Li J, 2017, IEEE I CONF COMP VIS, P3392, DOI 10.1109/ICCV.2017.365
   Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191
   Ling CW, 2022, IEEE T MULTIMEDIA, V24, P2938, DOI 10.1109/TMM.2021.3091308
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long XX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12829, DOI 10.1109/ICCV48922.2021.01261
   Loshchilov I., 2018, INT C LEARN REPR ICL, P1
   Paszke A., 2017, ADV NEURAL INF PROCE, V9, P1, DOI DOI 10.1017/CB09781107707221.009
   Patil V, 2022, PROC CVPR IEEE, P1600, DOI 10.1109/CVPR52688.2022.00166
   Peng ZL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P357, DOI 10.1109/ICCV48922.2021.00042
   Qi XJ, 2018, PROC CVPR IEEE, P283, DOI 10.1109/CVPR.2018.00037
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saxena A., 2005, ADV NEURAL INFORM PR, V18, P1
   Shao SW, 2022, MED IMAGE ANAL, V77, DOI 10.1016/j.media.2021.102338
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Solovyev R, 2021, IMAGE VISION COMPUT, V107, DOI 10.1016/j.imavis.2021.104117
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Song WF, 2020, IEEE T MULTIMEDIA, V22, P1220, DOI 10.1109/TMM.2019.2941776
   Vaswani A, 2017, ADV NEUR IN, V30
   Wenzel F., 2020, ADV NEURAL INFORM PR, P6514
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang GL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16249, DOI 10.1109/ICCV48922.2021.01596
   Yang H, 2021, IEEE T ROBOT, V37, P314, DOI 10.1109/TRO.2020.3033695
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P2701, DOI 10.1109/TMM.2019.2912121
   Yang YQ, 2021, PATTERN RECOGN, V109, DOI 10.1016/j.patcog.2020.107582
   Yin W, 2019, IEEE I CONF COMP VIS, P5683, DOI 10.1109/ICCV.2019.00578
   Yuan L, 2023, IEEE T PATTERN ANAL, V45, P6575, DOI 10.1109/TPAMI.2022.3206108
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou ZH, 2002, ARTIF INTELL, V137, P239, DOI 10.1016/S0004-3702(02)00190-X
NR 54
TC 3
Z9 3
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7660
EP 7671
DI 10.1109/TMM.2022.3224810
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Shu, XC
   Zhang, XD
   Gao, QX
   Yang, M
   Wang, R
   Gao, XB
AF Shu, Xiaochuang
   Zhang, Xiangdong
   Gao, Quanxue
   Yang, Ming
   Wang, Rong
   Gao, Xinbo
TI Self-Weighted Anchor Graph Learning for Multi-View Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Anchor graph learning; connectivity constraint; multi-view clustering
ID LOW-RANK
AB Graph-based multi-view clustering method has attracted considerable attention in multi-media data analyse community due to its good clustering performance and efficiency in characterizing the relationship between data. But the existing graph-based clustering methods still have many shortcomings. Firstly, they have high computational complexity due to the eigenvalue decomposition. Secondly, the complementary information and spatial structure embedded in different views can affect the clustering performance. However, some existing graph-based clustering methods do not consider these two points. In this article, we use the anchor graphs of different views as input, which effectively reduces the computational complexity. And then we explicitly consider the complementary information and spatial structure between anchor graphs of different views by minimizing the tensor Schatten p-norm, aiming to achieve a better tensor with low-rank approximation. Finally, we learn the view-consensus anchor graph with connectivity constraints, which can directly indicate clusters by self-weighted strategy. An efficient alternating algorithm is then derived to optimize the proposed multi-view special clustering model. Furthermore, the constructed sequence was proved to converge to the stationary KKT point. Experiments show that our proposed method not only reduces the time cost, but also outperforms the most advanced methods.
C1 [Shu, Xiaochuang; Zhang, Xiangdong; Gao, Quanxue] Xidian Univ, State Key Lab Integrated Serv Networks, Xidian 710071, Peoples R China.
   [Yang, Ming] Westfield State Univ, Dept Math & Comp & Informat Sci, Westfield, MA 01086 USA.
   [Wang, Rong] Northwestern Polytech Univ, Sch Artificial Intelligence Opt & Elect, Xian 710072, Peoples R China.
   [Gao, Xinbo] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
   [Gao, Xinbo] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
C3 Xidian University; Massachusetts System of Public Higher Education;
   Westfield State University; Northwestern Polytechnical University;
   Chongqing University of Posts & Telecommunications; Xidian University
RP Gao, QX (corresponding author), Xidian Univ, State Key Lab Integrated Serv Networks, Xidian 710071, Peoples R China.
EM 1760136011@qq.com; 578653865@qq.com; qxgao@xidian.edu.cn;
   myang@westfield.ma.edu; wangrong07@tsinghua.org.cn; xd_gxb_pr@163.com
RI Liu, XiaoYan/GQA-7216-2022; Yang, Ming/JCE-4730-2023; Wang,
   Rongming/AAB-1228-2019
OI Wang, Rongming/0000-0002-5445-541X; Yang, Ming/0000-0003-1810-1566;
   Wang, Rong/0000-0001-9240-6726
FU National Natural Science Foundation of China [62176203]; National
   Laboratory of Pattern Recognition (NLPR) through the Open Project
   Program [202200035]; Natural Science Basic Re-search Plan in Shaanxi
   Province [2020JZ-19]; Special Project on Technological Innovation and
   Application Development [cstc2020jscx-dxwtB0032]; Chongqing Excellent
   Scientist Project [cstc2021ycjh-bgzxm0339]; Fundamental Research Funds
   for the Central Universities; Innovation Fund of Xidian University
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62176203, in part by the National
   Laboratory of Pattern Recognition (NLPR) through the Open Project
   Program under Grant 202200035, in part by the Natural Science Basic
   Re-search Plan in Shaanxi Province under Grant 2020JZ-19, in part by
   Special Project on Technological Innovation and Application Development
   under Grant cstc2020jscx-dxwtB0032, in part by Chongqing Excellent
   Scientist Project under Grant cstc2021ycjh-bgzxm0339, in part by the
   Fundamental Research Funds for the Central Universities, and in part by
   the Innovation Fund of Xidian University.
CR [Anonymous], 2005, Comput. Vis. Image Understanding
   APTE C, 1994, ACM T INFORM SYST, V12, P233, DOI 10.1145/183422.183423
   Bickel S, 2004, FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P19, DOI 10.1109/ICDM.2004.10095
   Cai D, 2015, IEEE T CYBERNETICS, V45, P1669, DOI 10.1109/TCYB.2014.2358564
   Chen YY, 2021, IEEE T IMAGE PROCESS, V30, P4022, DOI 10.1109/TIP.2021.3068646
   Chen YY, 2020, IEEE T MULTIMEDIA, V22, P1985, DOI 10.1109/TMM.2019.2952984
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Cruickshank IJ, 2020, APPL NETW SCI, V5, DOI 10.1007/s41109-020-00317-8
   Ding C, 2010, IEEE T PATTERN ANAL, V32, P45, DOI 10.1109/TPAMI.2008.277
   Dua D., 2017, UCI MACHINE LEARNING
   FAN K, 1949, P NATL ACAD SCI USA, V35, P652, DOI 10.1073/pnas.35.11.652
   Gao QX, 2020, AAAI CONF ARTIF INTE, V34, P3930
   Gao QX, 2021, IEEE T PATTERN ANAL, V43, P2133, DOI 10.1109/TPAMI.2020.3017672
   Gao QX, 2020, NEURAL NETWORKS, V126, P335, DOI 10.1016/j.neunet.2020.03.020
   Hu ZX, 2020, INFORM FUSION, V55, P251, DOI 10.1016/j.inffus.2019.09.005
   Huang AP, 2021, IEEE T IMAGE PROCESS, V30, P6772, DOI 10.1109/TIP.2021.3096086
   Huang ZY, 2021, IEEE T IMAGE PROCESS, V30, P5352, DOI 10.1109/TIP.2021.3083072
   Ji X, 2020, IEEE ACCESS, V8, P175150, DOI 10.1109/ACCESS.2020.3025881
   Kang Z, 2020, AAAI CONF ARTIF INTE, V34, P4412
   Kilmer ME, 2021, P NATL ACAD SCI USA, V118, DOI 10.1073/pnas.2015851118
   Kilmer ME, 2011, LINEAR ALGEBRA APPL, V435, P641, DOI 10.1016/j.laa.2010.09.020
   Kumar A., 2011, P 28 INT C MACHINE L, P393
   Kumar P., 2011, Adv. Neural Inf. Process. Syst., P1413, DOI DOI 10.5555/2986459.2986617
   Lewis A, 2005, SET-VALUED ANAL, V13, P213, DOI 10.1007/s11228-004-7197-7
   Li Deng, 2012, IEEE Signal Processing Magazine, V29, P141, DOI [10.1109/MSP.2012.2211477, DOI 10.1109/MSP.2012.2211477]
   Li XL, 2022, IEEE T PATTERN ANAL, V44, P330, DOI 10.1109/TPAMI.2020.3011148
   Li YQ, 2015, AAAI CONF ARTIF INTE, P2750
   Liu, 2010, P 27 INT C MACH LEAR, P679, DOI DOI 10.1007/s11263-007-0090-8
   Lu X, 2020, IEEE T MULTIMEDIA, V22, P2048, DOI 10.1109/TMM.2019.2947358
   Luo SR, 2018, AAAI CONF ARTIF INTE, P3730
   Mucha PJ, 2010, SCIENCE, V328, P876, DOI 10.1126/science.1184819
   Nie F., 2016, IJCAI, P1881
   Nie FP, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2564
   Nie FP, 2018, IEEE T IMAGE PROCESS, V27, P1501, DOI 10.1109/TIP.2017.2754939
   Nie FP, 2016, AAAI CONF ARTIF INTE, P1969
   Nie FP, 2012, IEEE DATA MINING, P566, DOI 10.1109/ICDM.2012.160
   Pai S, 2019, MOL SYST BIOL, V15, DOI 10.15252/msb.20188497
   Qiao LS, 2018, NEUROCOMPUTING, V312, P336, DOI 10.1016/j.neucom.2018.05.084
   Semerci O, 2014, IEEE T IMAGE PROCESS, V23, P1678, DOI 10.1109/TIP.2014.2305840
   Strehl A, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P93, DOI 10.1162/153244303321897735
   Tan JP, 2021, IEEE T MULTIMEDIA, V23, P2943, DOI 10.1109/TMM.2020.3019683
   Trosten DJ, 2021, PROC CVPR IEEE, P1255, DOI 10.1109/CVPR46437.2021.00131
   Wen J, 2021, IEEE T MULTIMEDIA, V23, P2493, DOI 10.1109/TMM.2020.3013408
   Winn J, 2005, IEEE I CONF COMP VIS, P756
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2149
   Xia W, 2021, IEEE T MULTIMEDIA, V24, P3182, DOI 10.1109/TMM.2021.3094296
   Xia W, 2022, IEEE T CYBERNETICS, V52, P8962, DOI 10.1109/TCYB.2021.3052352
   Xie DY, 2021, NEURAL NETWORKS, V133, P57, DOI 10.1016/j.neunet.2020.10.010
   Xie Y, 2016, IEEE T IMAGE PROCESS, V25, P4842, DOI 10.1109/TIP.2016.2599290
   Xu HL, 2020, NEURAL NETWORKS, V132, P245, DOI 10.1016/j.neunet.2020.08.019
   Yang M, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108311
   Zha ZY, 2020, IEEE T IMAGE PROCESS, V29, P5094, DOI 10.1109/TIP.2020.2972109
   Zhan K, 2018, IEEE T CYBERNETICS, V48, P2887, DOI 10.1109/TCYB.2017.2751646
   Zhang CQ, 2020, IEEE T PATTERN ANAL, V42, P86, DOI 10.1109/TPAMI.2018.2877660
   Zhao HD, 2017, AAAI CONF ARTIF INTE, P2921
   Zhao YJ, 2022, NEUROCOMPUTING, V468, P257, DOI 10.1016/j.neucom.2021.09.052
   Zong LL, 2017, NEURAL NETWORKS, V88, P74, DOI 10.1016/j.neunet.2017.02.003
NR 57
TC 10
Z9 10
U1 2
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5485
EP 5499
DI 10.1109/TMM.2022.3193855
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300061
DA 2024-07-18
ER

PT J
AU Tan, WT
   Zhu, L
   Li, JJ
   Zhang, Z
   Zhang, HX
AF Tan, Wentao
   Zhu, Lei
   Li, Jingjing
   Zhang, Zheng
   Zhang, Huaxiang
TI Partial Multi-Modal Hashing via Neighbor-Aware Completion Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Codes; Semantics; Training; Data models; Task analysis; Kernel; Deep
   learning; Partial multi-modal hashing; neighbor-aware completion;
   self-attention; joint optimization
AB Multi-modal hashing technology can support large-scale multimedia retrieval well, because of its fast query speed and low storage consumption. Although many multi-modal hashing methods have been developed in the literature, they still suffer from three important problems: 1) Most multi-modal hashing methods assume that multi-modal data are complete. This ideal assumption limits their application to practical retrieval scenarios, where the modality-missing is common. 2) Existing partial multi-modal hashing methods directly model incomplete multi-modal data for hash learning, which may result in partial multi-modal semantics in the learned hash codes. 3) Most of the methods are based on the shallow learning framework, which inevitably suffers from limited representation capability. To solve the above problems, we propose a flexible deep partial multi-modal hash learning framework, named Neighbor-aware Completion Hashing (NCH). Our framework jointly performs the cross-modal completion learning for incomplete multi-modal data and the multi-modal hash learning. It can not only support model training with incomplete multi-modal data but also handle incomplete multi-modal queries. Besides, we design a neighbor-aware completion learning module to capture neighbor semantics and generate distribution-consistent completed features. Finally, we conduct extensive experiments to evaluate our method on both fully-paired and partial multi-modal retrieval scenarios. The experimental results verify the superiority of our proposed method over the state-of-the-art baselines.
C1 [Tan, Wentao; Zhu, Lei; Zhang, Huaxiang] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Peoples R China.
   [Li, Jingjing] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Zhang, Zheng] Harbin Inst Technol, Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.
C3 Shandong Normal University; University of Electronic Science &
   Technology of China; Harbin Institute of Technology
RP Zhu, L (corresponding author), Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Peoples R China.
EM tan.wt.lucky@gmail.com; leizhu0608@gmail.com; lijin117@yeah.net;
   darrenzz219@gmail.com; huaxzhang@163.com
RI Zhang, Zheng/M-6325-2014; Zhang, Zhang/JAX-2097-2023; Zhu,
   Lei/AAC-6810-2019; Li, Jingjing/T-6522-2019
OI Zhang, Zheng/0000-0003-1470-6998; Zhu, Lei/0000-0002-2993-7142; zhang,
   hua xiang/0000-0001-6259-7533
FU National Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], 2010, PROC ACM SIGMM INT C
   Bazaraa M.S., 2013, NONLINEAR PROGRAMMIN
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Cui H, 2020, IEEE T IMAGE PROCESS, V29, P1271, DOI 10.1109/TIP.2019.2940693
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P29
   Guo J, 2020, IEEE T IMAGE PROCESS, V29, P1344, DOI 10.1109/TIP.2019.2941858
   Jing MM, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3283, DOI 10.1145/3394171.3413676
   Kingma D. P., 2014, arXiv
   Kipf TN, 2017, INT C LEARN REPR
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Li WH, 2023, IEEE T MULTIMEDIA, V25, P543, DOI 10.1109/TMM.2021.3128744
   Liu AA, 2022, IEEE T CIRC SYST VID, V32, P8809, DOI 10.1109/TCSVT.2022.3191761
   Liu AA, 2022, IEEE T CIRC SYST VID, V32, P3685, DOI 10.1109/TCSVT.2021.3107035
   Liu H, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1589, DOI 10.1145/3240508.3240684
   Liu L, 2015, IEEE T IMAGE PROCESS, V24, P956, DOI 10.1109/TIP.2015.2390975
   Liu LY, 2020, NEURAL PROCESS LETT, V52, P1765, DOI 10.1007/s11063-020-10221-y
   Liu X., 2012, PROC ACM MULTIMEDIA, P881
   Lu X, 2020, IEEE T MULTIMEDIA, V22, P2048, DOI 10.1109/TMM.2019.2947358
   Lu X, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1414, DOI 10.1145/3474085.3475598
   Lu X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1129, DOI 10.1145/3343031.3350999
   Lu X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P715, DOI 10.1145/3331184.3331217
   Radford A, 2021, PR MACH LEARN RES, V139
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Shen FM, 2018, IEEE T PATTERN ANAL, V40, P3034, DOI 10.1109/TPAMI.2018.2789887
   Shen XB, 2018, ACM T INTEL SYST TEC, V9, DOI 10.1145/3178119
   Shen XB, 2017, IEEE T CYBERNETICS, V47, P4275, DOI 10.1109/TCYB.2016.2606441
   Shen XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P831, DOI 10.1145/2733373.2806342
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song JK, 2018, AAAI CONF ARTIF INTE, P394
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Vaswani A, 2017, ADV NEUR IN, V30
   Velickovic Petar, 2018, INT C LEARN REPR
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang QF, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3904
   Wright SJ, 2015, MATH PROGRAM, V151, P3, DOI 10.1007/s10107-015-0892-3
   Yan CG, 2021, IEEE T PATTERN ANAL, V43, P1445, DOI 10.1109/TPAMI.2020.2975798
   Yang R, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P180, DOI 10.1145/3078971.3078981
   Zeng Z., 2022, arXiv
   Zeng ZX, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1125, DOI 10.1145/3404835.3462867
   Zheng CQ, 2021, IEEE T MULTIMEDIA, V23, P4079, DOI 10.1109/TMM.2020.3037456
   Zheng CQ, 2020, IEEE T KNOWL DATA EN, V32, P2171, DOI 10.1109/TKDE.2019.2913388
   Zhou X, 2020, IEEE T CYBERNETICS, V50, P1460, DOI 10.1109/TCYB.2018.2883970
   Zhu L, 2020, ACM T INTEL SYST TEC, V11, DOI 10.1145/3365841
   Zhu L, 2020, IEEE T IMAGE PROCESS, V29, P4643, DOI 10.1109/TIP.2020.2974065
   Zhu L, 2018, IEEE T NEUR NET LEAR, V29, P5264, DOI 10.1109/TNNLS.2018.2797248
NR 45
TC 3
Z9 4
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8499
EP 8510
DI 10.1109/TMM.2023.3238308
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000024
DA 2024-07-18
ER

PT J
AU Wang, GZ
   Guo, YY
   Xu, ZW
   Wong, YK
   Kankanhalli, MS
AF Wang, Guangzhi
   Guo, Yangyang
   Xu, Ziwei
   Wong, Yongkang
   Kankanhalli, Mohan S.
TI Semantic-Aware Triplet Loss for Image Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep metric learning; image classification; semantics; triplet loss
AB Successful image classification requires a discriminative representation learning model for images. To approach this idea, deep metric learning (DML), serving as building a basic feature space with a pre-defined metric, has demonstrated compelling performance over the years. DML is often implemented with a carefully crafted loss function, such as the representative triplet loss, which encourages a positive sample to be by a fixed margin closer to the anchor than the negative. Despite its efficacy, the negative samples are treated uniformly, rendering the feature space less informative since different negative samples can be largely different from the anchor. In this work, we, for the first time, propose to exploit the semantic information inherent in discrete class labels as an aid for the triplet loss. Specifically, we build a bi-level negative sampling strategy, i.e., strong negative and weak negative sampling, with the guidance of an external knowledge source, from which rich class semantics can be extracted. With several fine-grained and complementary triplet losses based on this strategy, our method is enhanced with semantic awareness for image classification. In addition, to coordinate with the complicated training dynamics, we devise an ad-hoc Semantic Relation Weighting module, which consistently inspects model states and dynamically adjusts the importance of each triplet loss. It is worth noting that our method is plug-and-play, and we thus test its validity over various backbones and knowledge sources. Both qualitative and quantitative experimental results on benchmark datasets demonstrate the effectiveness of employing semantics for image classification.
C1 [Wang, Guangzhi] Natl Univ Singapore, Inst Data Sci, Grad Sch Integrat Sci & Engn, Singapore 119077, Singapore.
   [Guo, Yangyang; Xu, Ziwei; Wong, Yongkang; Kankanhalli, Mohan S.] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
C3 National University of Singapore; National University of Singapore
RP Guo, YY (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
EM guangzhi.wang@u.nus.edu; guoyang.eric@gmail.com;
   ziwei-xu@comp.nus.edu.sg; yongkang.wong@nus.edu.sg;
   mohan@comp.nus.edu.sg
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015; Guo,
   Yangyang/0000-0001-8691-5372; Wong, Yongkang/0000-0002-1239-4428
FU National Research Foundation, Singapore
FX This work was supported by the National Research Foundation, Singapore,
   under its Strategic Capability Research Centres Funding Initiative.
CR Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   DeVries T, 2017, Arxiv, DOI [arXiv:1708.04552, DOI 10.48550/ARXIV.1708.04552]
   Fan HH, 2022, IEEE T CIRC SYST VID, V32, P275, DOI 10.1109/TCSVT.2021.3058688
   Fellbaum C, 1998, LANG SPEECH & COMMUN, P1
   Finn C, 2017, PR MACH LEARN RES, V70
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Fu ZY, 2015, PROC CVPR IEEE, P2635, DOI 10.1109/CVPR.2015.7298879
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   Ge WF, 2018, LECT NOTES COMPUT SC, V11210, P272, DOI 10.1007/978-3-030-01231-1_17
   Guo JC, 2021, IEEE T MULTIMEDIA, V23, P524, DOI 10.1109/TMM.2020.2984091
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hospedales T, 2022, IEEE T PATTERN ANAL, V44, P5149, DOI 10.1109/TPAMI.2021.3079209
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jin Y, 2022, IEEE T MULTIMEDIA, V24, P1896, DOI 10.1109/TMM.2021.3073624
   Kingma D. P., 2014, arXiv
   Koch G., 2015, ICML DEEP LEARNING W, V2
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li ZG, 2017, Arxiv, DOI arXiv:1707.09835
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212
   Liu WY, 2016, PR MACH LEARN RES, V48
   Liu Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4235, DOI 10.1145/3394171.3413600
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Min SB, 2021, IEEE T MULTIMEDIA, V23, P3919, DOI 10.1109/TMM.2020.3033124
   Mishra N., 2018, INT C LEARN REPR
   Movshovitz-Attias Y, 2017, IEEE I CONF COMP VIS, P360, DOI 10.1109/ICCV.2017.47
   Neil Rabinowitz C., 2020, MLMA WORKSHOP
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Qiao SY, 2018, PROC CVPR IEEE, P7229, DOI 10.1109/CVPR.2018.00755
   Ravi S., 2016, INT C LEARNING REPRE
   Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Semedo D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1152, DOI 10.1145/3394171.3413540
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   Vasudeva B., 2021, P IEEE INT C COMP VI, P10634
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang S., 2021, P IEEECVF INT C COMP, P13475
   Wang X, 2020, PROC CVPR IEEE, P6387, DOI 10.1109/CVPR42600.2020.00642
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309
   Xu ZW, 2021, IEEE T MULTIMEDIA, V24, P3652, DOI 10.1109/TMM.2021.3104411
   You RC, 2020, AAAI CONF ARTIF INTE, V34, P12709
   Yu BS, 2019, IEEE I CONF COMP VIS, P6499, DOI 10.1109/ICCV.2019.00659
   Zagoruyko S., 2016, ARXIV160507146, DOI DOI 10.5244/C.30.87
   Zhang Y, 2015, ARXIV PREPRINT ARXIV
   Zhang YY, 2019, AAAI CONF ARTIF INTE, P9243
   Zhao YR, 2018, LECT NOTES COMPUT SC, V11213, P508, DOI 10.1007/978-3-030-01240-3_31
   Zheng WZ, 2019, PROC CVPR IEEE, P72, DOI 10.1109/CVPR.2019.00016
   Zhong XB, 2021, INT J COMPUT VISION, V129, P1910, DOI 10.1007/s11263-021-01458-8
NR 60
TC 0
Z9 0
U1 3
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4563
EP 4572
DI 10.1109/TMM.2022.3177929
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200035
DA 2024-07-18
ER

PT J
AU Wang, PF
   Ding, CX
   Tan, WT
   Gong, MM
   Jia, K
   Tao, DC
AF Wang, Pengfei
   Ding, Changxing
   Tan, Wentao
   Gong, Mingming
   Jia, Kui
   Tao, Dacheng
TI Uncertainty-Aware Clustering for Unsupervised Domain Adaptive Object
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Domain adaptation; object re-identification; unsupervised learning
ID NETWORK
AB Unsupervised Domain Adaptive (UDA) object re-identification (Re-ID) aims at adapting a model trained on a labeled source domain to an unlabeled target domain. State-of-the-art object Re-ID approaches adopt clustering algorithms to generate pseudo-labels for the unlabeled target domain. However, the inevitable label noise caused by the clustering procedure significantly degrades the discriminative power of Re-ID model. To address this problem, we propose an uncertainty-aware clustering framework (UCF) for UDA tasks. First, a novel hierarchical clustering scheme is proposed to promote clustering quality. Second, an uncertainty-aware collaborative instance selection method is introduced to select images with reliable labels for model training. Combining both techniques effectively reduces the impact of noisy labels. In addition, we introduce a strong baseline that features a compact contrastive loss. Our UCF method consistently achieves state-of-the-art performance in multiple UDA tasks for object Re-ID, and significantly reduces the performance gap between unsupervised and supervised Re-ID. In particular, the performance of our unsupervised UCF method in the MSMT17$\to$Market1501 task is better than that of the fully supervised setting on Market1501. The code of UCF is available at https://github.com/Wang-pengfei/UCF.
C1 [Wang, Pengfei; Ding, Changxing; Tan, Wentao; Jia, Kui] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510000, Peoples R China.
   [Ding, Changxing] Pazhou Lab, Guangzhou 510330, Peoples R China.
   [Gong, Mingming] Univ Melbourne, Sch Math & Stat, Melbourne, Vic 3010, Australia.
   [Tao, Dacheng] JD Explore Acad JD com, Beijing 100023, Peoples R China.
C3 South China University of Technology; Pazhou Lab; University of
   Melbourne
RP Ding, CX (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510000, Peoples R China.; Ding, CX (corresponding author), Pazhou Lab, Guangzhou 510330, Peoples R China.
EM eepengfei.wang@mail.scut.edu.cn; chxding@scut.edu.cn;
   eewentaotan@mail.scut.edu.cn; mingming.gong@unimelb.edu.au;
   kuijia@scut.edu.cn; taodacheng@jd.com
RI Tao, Dacheng/A-5449-2012
OI Tao, Dacheng/0000-0001-7225-5449; Wang, Pengfei/0000-0002-3675-9508
FU National Natural Science Foundation of China [62076101, 61702193];
   Program for Guangdong Introducing Innovative and Entrepreneurial Teams
   [2017ZT07X183]; CCF-Baidu Open Fund
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62076101 and 61702193, in part by the
   Program for Guangdong Introducing Innovative and Entrepreneurial Teams
   under Grant 2017ZT07X183, and in part by CCF-Baidu Open Fund. The
   Associate Editor coordinating the review of this manuscript and
   approving it for publication was Prof. Ferdous Sohel.& nbsp;
CR Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen YB, 2019, IEEE I CONF COMP VIS, P232, DOI 10.1109/ICCV.2019.00032
   Chuanchen Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P224, DOI 10.1007/978-3-030-58555-6_14
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Ding CX, 2022, IEEE T PATTERN ANAL, V44, P1474, DOI 10.1109/TPAMI.2020.3024900
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   McDaid AF, 2013, Arxiv, DOI [arXiv:1110.2515, DOI 10.48550/ARXIV.1110.2515]
   Fang Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P526, DOI 10.1007/978-3-030-58621-8_31
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ge Y., 2020, arXiv
   Ge Y., 2020, P NIPS, V33, P11309
   Ge Yixiao, 2020, ARXIV200101526
   Gong X, 2022, IEEE T MULTIMEDIA, V24, P217, DOI 10.1109/TMM.2021.3050082
   Grill Jean-Bastien, 2020, ADV NEURAL INFORM PR
   Guan DY, 2022, IEEE T MULTIMEDIA, V24, P2502, DOI 10.1109/TMM.2021.3082687
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jian Liang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P123, DOI 10.1007/978-3-030-58621-8_8
   Jiang B, 2021, IEEE T MULTIMEDIA, V24, P3218, DOI 10.1109/TMM.2021.3095789
   Jianing Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P483, DOI 10.1007/978-3-030-58586-0_29
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liu DM, 2019, AM STAT, V73, P70, DOI 10.1080/00031305.2018.1459315
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Liu XC, 2016, LECT NOTES COMPUT SC, V9906, P869, DOI 10.1007/978-3-319-46475-6_53
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Lyu Y, 2020, Arxiv, DOI arXiv:1905.10045
   Malach E, 2017, ADV NEUR IN, V30
   Mekhazni Djebril, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P159, DOI 10.1007/978-3-030-58583-9_10
   Naphade M, 2020, IEEE COMPUT SOC CONF, P2665, DOI 10.1109/CVPRW50498.2020.00321
   Paszke A, 2019, ADV NEUR IN, V32
   Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Song LC, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2019.107173
   Song XL, 2021, IEEE T MULTIMEDIA, V24, P3229, DOI 10.1109/TMM.2021.3096014
   Sun XX, 2019, PROC CVPR IEEE, P608, DOI 10.1109/CVPR.2019.00070
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tang Z, 2019, IEEE I CONF COMP VIS, P211, DOI 10.1109/ICCV.2019.00030
   Tarvainen A, 2017, ADV NEUR IN, V30
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wan CQ, 2020, IEEE T MULTIMEDIA, V22, P1605, DOI 10.1109/TMM.2019.2946486
   Wang D., 2020, P IEEE CVF C COMP VI, P10981
   Wang K, 2021, IEEE T IMAGE PROCESS, V30, P3405, DOI 10.1109/TIP.2021.3060909
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu AC, 2019, IEEE I CONF COMP VIS, P6921, DOI 10.1109/ICCV.2019.00702
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xiang SC, 2021, Arxiv, DOI arXiv:2104.02265
   Xie Y., 2021, BMVC
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Yan C, 2022, IEEE T MULTIMEDIA, V24, P1665, DOI 10.1109/TMM.2021.3069562
   Yang FX, 2020, AAAI CONF ARTIF INTE, V34, P12597
   Yang QZ, 2019, PROC CVPR IEEE, P3628, DOI 10.1109/CVPR.2019.00375
   Yang Zou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P87, DOI 10.1007/978-3-030-58536-5_6
   Yao Y, 2020, Arxiv, DOI arXiv:1912.08855
   Yixiao Ge, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P369, DOI 10.1007/978-3-030-58548-8_22
   Yu HX, 2019, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR.2019.00225
   Yu Xingrui, 2019, PROC INT C MACH LEAR, P7164
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zhai Y, 2020, COMPUTER VISION ECCV, P594, DOI DOI 10.1007/978-3-030-58571-6_35
   Zhang P, 2021, IEEE T MULTIMEDIA, V23, P3562, DOI 10.1109/TMM.2020.3028461
   Zhang XY, 2019, IEEE I CONF COMP VIS, P8221, DOI 10.1109/ICCV.2019.00831
   Zhang ZZ, 2019, PROC CVPR IEEE, P667, DOI 10.1109/CVPR.2019.00076
   Zhao CR, 2020, IEEE T MULTIMEDIA, V22, P3180, DOI 10.1109/TMM.2020.2972125
   Zheng K., 2021, P IEEE CVF C COMP VI, P5310
   Zheng KC, 2021, AAAI CONF ARTIF INTE, V35, P3538
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
NR 74
TC 11
Z9 11
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2624
EP 2635
DI 10.1109/TMM.2022.3149629
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, R
   Liu, J
   Ke, QH
   Peng, D
   Lei, YJ
AF Wang, Rui
   Liu, Jun
   Ke, Qiuhong
   Peng, Duo
   Lei, Yinjie
TI Dear-Net: Learning Diversities for Skeleton-Based Early Action
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Reliability; Predictive models; Early action recognition; diversity
   learning; action diversity learning strategy; skeleton data;
   weakly-supervised learning
AB Early actionrecognition, i.e., recognizing an action before it is fully performed, is a challenging and important task. Existing works mainly focus on deterministic early action recognition outputting only a single class, and ignore the uncertainty and diversity that essentially exist in this task. Intuitively, when only the early portion of the action is observed, there could be multiple possibilities of the full action, as diversified actions can share almost identical early segments in many scenarios. Thus taking uncertainties and diversities into account, and outputting multiple plausible predictions, instead of a single one, can be important for the sake of authenticity and requirement of many practical applications. To this end, we propose a novel Diversified Early Action Recognition Network (Dear-Net) that is capable of outputting multiple reasonable action classes for each partial sequence by utilizing mode conversion. Specifically, we introduce an effective action diversity learning strategy to drive our network towards predicting diverse and reasonable results, in which each learnable action class is matched with the most suitable mode. Meanwhile, the collapsed modes which fail to receive any action class, are also considered in this strategy in order to ensure diversity. Moreover, we design a sequence decoder within our network to capture latent global information for better early action recognition. It provides a feasible scheme for weakly-supervised setting in which the Dear-Net leverages unlabelled data to improve performance. Experimental results on three challenging datasets clearly show the effectiveness of our approach.
C1 [Wang, Rui; Peng, Duo; Lei, Yinjie] Sichuan Univ, Coll Elect & Informat Engn, Chengdu 610065, Peoples R China.
   [Liu, Jun] Singapore Univ Technol & Design, Informat Syst Technol & Design Pillar, Singapore 487372, Singapore.
   [Ke, Qiuhong] Univ Melbourne, Sch Comp & Informat Syst, Melbourne, Vic 3010, Australia.
C3 Sichuan University; Singapore University of Technology & Design;
   University of Melbourne
RP Lei, YJ (corresponding author), Sichuan Univ, Coll Elect & Informat Engn, Chengdu 610065, Peoples R China.
EM wang_rui@stu.scu.edu.cn; jun_liu@sutd.edu.sg; qiuhong.ke@unimelb.edu.au;
   duo_peng@stu.scu.edu.cn; yinjie@scu.edu.cn
RI wang, rui/JVO-3788-2024; Peng, Duo/AAX-9689-2021
OI wang, rui/0000-0002-0499-9451; Duo, Peng/0000-0003-3281-0772; Liu,
   Jun/0000-0002-4365-4165
CR Aliakbarian MS, 2017, IEEE I CONF COMP VIS, P280, DOI 10.1109/ICCV.2017.39
   [Anonymous], 2018, PROC ASIAN C COMPUT
   Baldi Pierre, 2013, Advances in Neural Information Processing Systems, P2814
   Batra D, 2012, LECT NOTES COMPUT SC, V7576, P1, DOI 10.1007/978-3-642-33715-4_1
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Bouthillier X, 2016, Arxiv, DOI [arXiv:1506.08700, 10.48550/arXiv.1506.08700]
   Caetano C, 2019, 2019 16TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), DOI 10.1109/avss.2019.8909840
   Caetano C, 2019, SIBGRAPI, P16, DOI 10.1109/SIBGRAPI.2019.00011
   Chen C, 2015, IEEE IMAGE PROC, P168, DOI 10.1109/ICIP.2015.7350781
   Chen L, 2021, IEEE T CIRC SYST VID, V31, P231, DOI 10.1109/TCSVT.2020.2975065
   Dey D, 2015, IEEE I CONF COMP VIS, P2947, DOI 10.1109/ICCV.2015.337
   Domigos P., 1997, Proceedings of the Third International Conference on Knowledge Discovery and Data Mining, P155
   Du Y, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P579, DOI 10.1109/ACPR.2015.7486569
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Firman M, 2018, PROC CVPR IEEE, P5598, DOI 10.1109/CVPR.2018.00587
   Gammulle H, 2019, IEEE I CONF COMP VIS, P5561, DOI 10.1109/ICCV.2019.00566
   Guzman-Rivera A., 2012, P ADV NEUR INF PROC, P1799
   Guzman-Rivera A, 2014, JMLR WORKSH CONF PRO, V33, P284
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu JF, 2019, IEEE T PATTERN ANAL, V41, P2568, DOI 10.1109/TPAMI.2018.2863279
   Hu JF, 2016, LECT NOTES COMPUT SC, V9905, P280, DOI 10.1007/978-3-319-46448-0_17
   Jain A, 2016, IEEE INT CONF ROBOT, P3118, DOI 10.1109/ICRA.2016.7487478
   Ke QH, 2020, IEEE T IMAGE PROCESS, V29, P959, DOI 10.1109/TIP.2019.2937757
   Ke Q, 2018, IEEE T MULTIMEDIA, V20, P1712, DOI 10.1109/TMM.2017.2778559
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kirillov A, 2015, IEEE I CONF COMP VIS, P1814, DOI 10.1109/ICCV.2015.211
   Kong Y., 2017, ARXIV, V31
   Kong Y, 2018, AAAI CONF ARTIF INTE, P7000
   Kong Y, 2020, IEEE T PATTERN ANAL, V42, P539, DOI 10.1109/TPAMI.2018.2882805
   Kong Y, 2017, PROC CVPR IEEE, P3662, DOI 10.1109/CVPR.2017.390
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lan T, 2014, LECT NOTES COMPUT SC, V8691, P689, DOI 10.1007/978-3-319-10578-9_45
   Lee S, 2016, ADV NEUR IN, V29
   Lee SF, 2015, Arxiv, DOI [arXiv:1511.06314, DOI 10.48550/ARXIV.1511.06314]
   Li B, 2017, IEEE INT C COMPUT, P187, DOI 10.1109/CSE-EUC.2017.217
   Li C., 2017, 2017 IEEE INT C MULT, P585, DOI DOI 10.1109/ICMEW.2017.8026287
   Li C, 2018, Arxiv, DOI arXiv:1804.06055
   Li C, 2017, IEEE INT CONF MULTI
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P1453, DOI 10.1109/TPAMI.2019.2898954
   Liu J, 2018, PROC CVPR IEEE, P8349, DOI 10.1109/CVPR.2018.00871
   Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Pang GL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P897
   Perez M, 2022, IEEE T MULTIMEDIA, V24, P366, DOI 10.1109/TMM.2021.3050642
   Rahmani H, 2018, IEEE T PATTERN ANAL, V40, P667, DOI 10.1109/TPAMI.2017.2691768
   Ryoo MS, 2011, IEEE I CONF COMP VIS, P1036, DOI 10.1109/ICCV.2011.6126349
   SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1023/A:1022648800760
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tang YS, 2018, PROC CVPR IEEE, P5323, DOI 10.1109/CVPR.2018.00558
   Tian K, 2019, PROC CVPR IEEE, P6342, DOI 10.1109/CVPR.2019.00651
   Tianjiao Li, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P420, DOI 10.1007/978-3-030-58621-8_25
   Wang HS, 2017, PROC CVPR IEEE, P3633, DOI 10.1109/CVPR.2017.387
   Wang XH, 2019, PROC CVPR IEEE, P3551, DOI 10.1109/CVPR.2019.00367
   Weng JW, 2020, IEEE T CIRC SYST VID, V30, P4626, DOI 10.1109/TCSVT.2020.2976789
   Xu WR, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P611, DOI 10.1145/3343031.3351073
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI [10.1109/ICCV.2017.233, 10.1109/ICCV.2017.231]
   Zhu WH, 2016, PROC INT CONF ANTI, P1, DOI 10.1109/ICASID.2016.7873885
NR 66
TC 1
Z9 1
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1175
EP 1189
DI 10.1109/TMM.2021.3139768
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100012
DA 2024-07-18
ER

PT J
AU Xue, Y
   Cao, Y
   Feng, XB
   Xie, ML
   Li, K
   Zhang, XJ
   Qian, XM
AF Xue, Yao
   Cao, Yu
   Feng, Xubin
   Xie, Meilin
   Li, Ke
   Zhang, Xingjun
   Qian, Xueming
TI Towards Handling Sudden Changes in Feature Maps During Depth Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Estimation; Decoding; Training; Image edge detection; Laser radar; Task
   analysis; Semantics; Depth estimation; depth representation; multiscale
   feature
ID NETWORK
AB Depth estimation aims to predict depth map from RGB images without high cost equipments. Deep learning based depth estimation methods have shown their effectiveness. However in existing methods, depth information is represented by a per-pixel depth map. Such depth map representation is fragile facing different kinds of depth changes. This paper proposes a Compressive Sensing based Depth Representation (CSDR) scheme, which formulates the problem of depth estimation in pixel space into the task of fixed-length vector regression in representation space. In this way, deep model training errors will not directly interfere depth estimation, and distortions in estimated depth maps can be restrained in the greatest extent. In addition, we improve depth estimation from two other aspects: model structure and loss function. To capture the features in different scales, we propose a Multiscale Encoder & Multiscale Decoder (MEMD) structure as the vector regression model. To further deal with depth change, we also modify the loss function, where the curvature difference between ground truth and estimation is directly incorporated. With the support of CSDR, MEMD and the curvature loss, the proposed approach achieves superior performance on a challenging depth estimation dataset: NYU-Depth-v2. A range of experiments support our claim that regression in CSDR space performs better than traditionally direct depth map estimation in pixel space.
C1 [Xue, Yao; Li, Ke] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Xian 710049, Peoples R China.
   [Cao, Yu] Chinese Acad Sci, Xian Inst Opt & Precis Mech, Xian 710119, Peoples R China.
   [Cao, Yu] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
   [Cao, Yu] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Cao, Yu] Chinese Acad Sci, Key Lab Space Precis Measurement Technol, Xian 710119, Peoples R China.
   [Feng, Xubin; Xie, Meilin] Chinese Acad Sci, Xian Inst Opt & Precis Mech, Space Precis Measurement Lab, Xian 710119, Peoples R China.
   [Zhang, Xingjun] Xi An Jiao Tong Univ, Sch Comp Sci & Technol, Xian 710049, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Key Lab Intelligent Networks & Network Secur, Minist Educ, Xian 710049, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, SMILES LAB, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University; Chinese Academy of Sciences; Xi'an Institute
   of Optics & Precision Mechanics, CAS; Xi'an Jiaotong University; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Chinese Academy of Sciences; Chinese Academy of Sciences; Xi'an
   Institute of Optics & Precision Mechanics, CAS; Xi'an Jiaotong
   University; Xi'an Jiaotong University; Xi'an Jiaotong University
RP Xie, ML (corresponding author), Chinese Acad Sci, Xian Inst Opt & Precis Mech, Space Precis Measurement Lab, Xian 710119, Peoples R China.; Qian, XM (corresponding author), Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Key Lab Intelligent Networks & Network Secur, Minist Educ, Xian 710049, Peoples R China.; Qian, XM (corresponding author), Xi An Jiao Tong Univ, SMILES LAB, Xian 710049, Peoples R China.
EM xueyao@xjtu.edu.cn; caoyu@opt.ac.cn; fengxubin@opt.ac.cn;
   xiemeilin@opt.ac.cn; likedongxi@163.com; xjzhang@mail.xjtu.edu.cn;
   qianxm@mail.xjtu.edu.cn
RI Cao, Yu/KHW-9931-2024
OI Cao, Yu/0000-0002-0250-7353; Feng, Xubin/0000-0003-4348-7632
FU NSFC [62103317, 61732008]; China Postdoctoral Science Foundation
   [2021M702600]; Natural Science Foundation of Shaanxi Province
   [2021JQ-058]; Beilin District Science and Technology Program [GX2130];
   Science and Technology Program of Xi'an, China [21RGZN0017]
FX This work is supported in part by the NSFC under Grants 62103317, and
   61732008, in part by the China Postdoctoral Science Foundation under
   Grant 2021M702600, in part by the Natural Science Foundation of Shaanxi
   Province under Grant 2021JQ-058, in part by the Beilin District Science
   and Technology Program GX2130, and in part by the Science and Technology
   Program of Xi'an, China under Grant 21RGZN0017.
CR AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784
   [Anonymous], 2012, LECT NOTES COMPUT SC, DOI [10.1007/978-3-642-33715-4_54, DOI 10.1007/978-3-642-33715-4_54]
   Cai TT, 2011, IEEE T INFORM THEORY, V57, P4680, DOI 10.1109/TIT.2011.2146090
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P4296, DOI 10.1109/TIP.2020.2968250
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Eigen D., 2014, NIPS, DOI DOI 10.5555/2969033.2969091
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hsu Daniel J, 2009, Proc. of Neural Information Processing Systems, P772
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu JJ, 2019, IEEE WINT CONF APPL, P1043, DOI 10.1109/WACV.2019.00116
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Huang JG, 2000, PROC CVPR IEEE, P324, DOI 10.1109/CVPR.2000.855836
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Lee JH, 2018, PROC CVPR IEEE, P330, DOI 10.1109/CVPR.2018.00042
   Lee W., 2011, ICAT '11, P126
   Li B, 2015, PROC CVPR IEEE, P1119, DOI 10.1109/CVPR.2015.7298715
   Li K, 2021, IEEE T MULTIMEDIA, V23, P257, DOI 10.1109/TMM.2020.2981237
   Li Y, 2021, IEEE T IMAGE PROCESS, V30, P2288, DOI 10.1109/TIP.2021.3051761
   Miangoleh S. Mahdi H., 2021, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P9680, DOI 10.1109/CVPR46437.2021.00956
   Pan Y, 2017, IEEE T MULTIMEDIA, V19, P685, DOI 10.1109/TMM.2016.2646179
   Park SJ, 2017, IEEE I CONF COMP VIS, P4990, DOI 10.1109/ICCV.2017.533
   Qi XJ, 2018, PROC CVPR IEEE, P283, DOI 10.1109/CVPR.2018.00037
   Ranftl R, 2022, IEEE T PATTERN ANAL, V44, P1623, DOI 10.1109/TPAMI.2020.3019967
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132
   Song WF, 2020, IEEE T MULTIMEDIA, V22, P1220, DOI 10.1109/TMM.2019.2941776
   Song XB, 2021, IEEE T IMAGE PROCESS, V30, P4691, DOI 10.1109/TIP.2021.3074306
   Tian LC, 2018, IEEE T MULTIMEDIA, V20, P2249, DOI 10.1109/TMM.2018.2803526
   Tomioka R, 2011, J MACH LEARN RES, V12, P1537
   Whelan T, 2015, INT J ROBOT RES, V34, P598, DOI 10.1177/0278364914551008
   Xu D, 2018, PROC CVPR IEEE, P3917, DOI 10.1109/CVPR.2018.00412
   Xu D, 2017, PROC CVPR IEEE, P161, DOI 10.1109/CVPR.2017.25
   Zhang YY, 2020, IEEE T IMAGE PROCESS, V29, P7019, DOI 10.1109/TIP.2020.2997247
   Zuo YF, 2021, IEEE T MULTIMEDIA, V23, P772, DOI 10.1109/TMM.2020.2987706
NR 36
TC 1
Z9 1
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4002
EP 4012
DI 10.1109/TMM.2022.3171400
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500034
DA 2024-07-18
ER

PT J
AU Yang, WM
   Wang, XK
   Tian, BW
   Xu, W
   Cheng, WQ
AF Yang, Weiming
   Wang, Xianke
   Tian, Bowen
   Xu, Wei
   Cheng, Wenqing
TI A Multi-Stage Automatic Evaluation System for Sight-Singing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Music; Spectrogram; Feature extraction; Hidden Markov models; Detectors;
   Convolutional neural networks; Deep learning; Automatic sight-singing
   system; note alignment; sight-singing transcription; systematic
   evaluation measure
ID TRANSCRIPTION; QUERY
AB Sight-singing exercises are a fundamental part of music education. In this paper, we present an objective and complete automatic evaluation system for sight-singing, which has two critical stages: note transcription and note alignment. In the first stage, we use an onset detector based on the convolutional recurrent neural network (CRNN) for note segmentation and the pitch extractor described in (Kim et al. 2018) for note labeling. In the second stage, an alignment algorithm based on relative pitch modeling is proposed. Due to the lack of datasets for sight-singing note alignment and the overall system evaluation, we construct the sight-singing vocal dataset (SSVD). Each module of the system and the entire system are tested on this dataset. The onset detector achieves an F-measure of 90.61%, and the stages of note transcription and note alignment achieve an F-measure of 88.42% and 94.79%, respectively. In addition, we propose an objective criterion for the sight-singing evaluation system. Based on this criterion, our automatic sight-singing system achieves an F-measure of 77.95% on the SSVD dataset.
C1 [Yang, Weiming; Wang, Xianke; Tian, Bowen; Xu, Wei; Cheng, Wenqing] Huazhong Univ Sci & Technol, Hubei Key Lab Smart Internet Technol, Wuhan 430074, Peoples R China.
   [Yang, Weiming; Wang, Xianke; Tian, Bowen; Xu, Wei; Cheng, Wenqing] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology
RP Xu, W (corresponding author), Huazhong Univ Sci & Technol, Hubei Key Lab Smart Internet Technol, Wuhan 430074, Peoples R China.
EM yyweiming@hust.edu.cn; m202072113@hust.edu.cn; m202072111@hust.edu.cn;
   xuwei@hust.edu.cn; chengwq@mail.hust.edu.cn
RI yan, su/KHT-1728-2024; Wang, Siying/KHX-1894-2024; Tian,
   Bowen/HOF-5636-2023
OI Xu, Wei/0000-0003-4705-7189; Wang, Xianke/0000-0001-7037-9800
FU National Natural Science Foundation of China [61877060]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61877060.
CR [Anonymous], 2014, Improved Musical Onset Detection with Convolutional Neural Networks, DOI DOI 10.1109/ICASSP.2014.6854953
   Arzt A, 2012, EUR SIGNAL PR CONF, P2689
   Arzt Andreas., 2018, Proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR), P592
   Babacan O, 2013, INT CONF ACOUST SPEE, P7815, DOI 10.1109/ICASSP.2013.6639185
   Bock Sebastian, 2012, P 13 INT SOC MUS INF, P49
   Boersma P., 1993, P I PHONETIC SCI, P97, DOI DOI 10.1371/JOURNAL.PONE.0069107
   Bogdanov D, 2011, IEEE T MULTIMEDIA, V13, P687, DOI 10.1109/TMM.2011.2125784
   Chun-Ta Chen, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1365, DOI 10.1109/ICASSP.2014.6853820
   Dannenberg R, 1984, P 1984 INT COMP MUS, P193
   de Cheveigné A, 2002, J ACOUST SOC AM, V111, P1917, DOI 10.1121/1.1458024
   Dixon S., 2005, Proceedings of the International Conference on Music Information Retrieval, P492
   DUBNOWSKI JJ, 1976, IEEE T ACOUST SPEECH, V24, P2, DOI 10.1109/TASSP.1976.1162765
   Ellis DPW, 2007, INT CONF ACOUST SPEE, P1429
   Ewert S, 2009, INT CONF ACOUST SPEE, P1869, DOI 10.1109/ICASSP.2009.4959972
   Ghias A., 1995, PROC ACM MULTIMEDIA, P231
   Gingras B, 2011, J NEW MUSIC RES, V40, P43, DOI 10.1080/09298215.2010.545422
   Goto M., 2002, P 3 INT C MUS INF RE, V2, P287
   Grachten M., 2013, P INT SOC MUSIC INFO, P607
   Gupta C, 2020, IEEE-ACM T AUDIO SPE, V28, P13, DOI 10.1109/TASLP.2019.2947737
   Gupta C, 2018, ASIAPAC SIGN INFO PR, P990, DOI 10.23919/APSIPA.2018.8659545
   Gupta C, 2017, ASIAPAC SIGN INFO PR, P577, DOI 10.1109/APSIPA.2017.8282110
   Heo H., 2013, 2013 IEEE International Conference of IEEE Region 10, P1
   HUANG J, 2020, P 21ST INT SOC MUSIC, P936
   Jang Jyh-ShingRoger., 2001, MULTIMEDIA 01, P401
   Joder C, 2010, INT CONF ACOUST SPEE, P409, DOI 10.1109/ICASSP.2010.5495784
   Jouvet D, 2017, EUR SIGNAL PR CONF, P1614, DOI 10.23919/EUSIPCO.2017.8081482
   Kim JW, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P161, DOI 10.1109/ICASSP.2018.8461329
   Kingma D. P., 2014, arXiv
   Kroher N, 2018, IEEE T MULTIMEDIA, V20, P1291, DOI 10.1109/TMM.2017.2771450
   Kroher N, 2016, IEEE-ACM T AUDIO SPE, V24, P901, DOI 10.1109/TASLP.2016.2531284
   Liu NH, 2014, IEEE T MULTIMEDIA, V16, P1407, DOI 10.1109/TMM.2014.2311326
   LU L, 2001, P ICME, P22
   Mauch M, 2014, IEEE INT C ACOUSTICS, DOI DOI 10.1109/ICASSP.2014.6853678
   Mauch Matthias, 2015, P INT C TECHN MUS NO, P23
   McFee B., 2015, P 14 PYTH SCI C, P18, DOI [DOI 10.25080/MAJORA-7B98E3ED-003, 10.25080/Majora-7b98e3ed-003]
   MCNAB RJ, 1999, P AUSTRALAS COMPUT S, P301
   Molina E, 2015, IEEE-ACM T AUDIO SPE, V23, P252, DOI 10.1109/TASLP.2014.2331102
   Molina E, 2013, INT CONF ACOUST SPEE, P744, DOI 10.1109/ICASSP.2013.6637747
   Molina Emilio, 2014, P 15 INT SOC MUS INF, P567
   Nakamura E., 2017, P INT SOC MUSIC INFO, P347
   Nakano T, 2006, INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5, P1706
   NEEDLEMAN SB, 1970, J MOL BIOL, V48, P443, DOI 10.1016/0022-2836(70)90057-4
   Nishikimi R, 2019, INT CONF ACOUST SPEE, P161, DOI [10.1109/icassp.2019.8683024, 10.1109/ICASSP.2019.8683024]
   OSCAR M, 2009, P 35TH INT C AUDIO G, P1
   Payne D., 2005, American Music Teacher, V54, P26
   Raffel C., 2014, P ISMIR, P367
   Schramm R., 2015, ISMIR, P183
   Silva DF, 2019, IEEE T MULTIMEDIA, V21, P29, DOI 10.1109/TMM.2018.2849563
   STRMBERGSSON S, 2016, P C INT SPEECH COMMU, P525
   Sungkyun Chang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P629, DOI 10.1109/ICASSP.2014.6853672
   Talkin D., 1995, Speech coding and synthesis, V495, P518
   Tsai WH, 2015, J INF SCI ENG, V31, P821
   Tsai WH, 2012, IEEE T AUDIO SPEECH, V20, P1233, DOI 10.1109/TASL.2011.2174224
   VONDEMKNESEBECK A, 2010, P 13TH INT C DIGIT A, P525
   Wang JY, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P276, DOI 10.1109/ICASSP39728.2021.9414601
   Yang L, 2017, PROCEEDINGS OF THE 23RD ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING (MOBICOM '17), P301, DOI 10.1145/3117811.3117835
   Yu HM, 2008, IEEE T MULTIMEDIA, V10, P1626, DOI 10.1109/TMM.2008.2007345
   Zhang N, 2019, INT CONF ACOUST SPEE, P466, DOI 10.1109/ICASSP.2019.8682665
NR 58
TC 2
Z9 2
U1 2
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3881
EP 3893
DI 10.1109/TMM.2022.3168132
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500026
DA 2024-07-18
ER

PT J
AU Yang, Y
   Yu, J
   Zhang, J
   Han, WD
   Jiang, HL
   Huang, QM
AF Yang, Yan
   Yu, Jun
   Zhang, Jian
   Han, Weidong
   Jiang, Hanliang
   Huang, Qingming
TI Joint Embedding of Deep Visual and Semantic Features for Medical Image
   Report Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Medical image report generation; visual-semantic joint embedding;
   encoder-decoder framework; deep neural network
AB Medical image report generation (MeIRG) aims at generating associated diagnosis descriptions with natural language sentences from medical images, which is essential in the computer-aided diagnosis system. Nevertheless, this task remains challenging in that medical images and linguistic expressions should be understood jointly which however show great discrepancies in the modality. To fill this visual-to-semantic gap, we propose a novel framework that follows the encoder-decoder pipeline. Our framework is characterized by encoding both deep visual and semantic embeddings through a triple-branch network (TriNet) during the encoding phase. The visual attention branch captures attended visual embeddings from medical images with the soft-attention mechanism. The medical report (MeRP) embedding branch predicts semantic report embeddings. The embedding branch of medical subject headings (MeSH) obtains semantic embeddings of related medical tags as complementary information. Then, outputs of these branches are fused and fed into a decoder for the report generation. Experimental results on two benchmark datasets have demonstrated the excellent performance of our method. Related codes are available at https://github.com/yangyan22/Medical-Report-Generation-TriNet.
C1 [Yang, Yan; Yu, Jun] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Key Lab Complex Syst Modeling & Simulat, Hangzhou 310018, Peoples R China.
   [Zhang, Jian] Zhejiang Int Studies Univ, Hangzhou 310018, Peoples R China.
   [Han, Weidong] Zhejiang Univ, Sir Run Run Shaw Hosp, Coll Med, Dept Med Oncol, Hangzhou 310016, Peoples R China.
   [Jiang, Hanliang] Zhejiang Univ, Sir Run Run Shaw Hosp, Reg Med Ctr, Natl Inst Resp Dis,Coll Med, Hangzhou 310016, Peoples R China.
   [Huang, Qingming] Univ Chinese Acad Sci, Sch Comp & Control Engn, Beijing 101408, Peoples R China.
C3 Hangzhou Dianzi University; Zhejiang International Studies University;
   Zhejiang University; Zhejiang University; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS
RP Yu, J (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Key Lab Complex Syst Modeling & Simulat, Hangzhou 310018, Peoples R China.; Han, WD (corresponding author), Zhejiang Univ, Sir Run Run Shaw Hosp, Coll Med, Dept Med Oncol, Hangzhou 310016, Peoples R China.
EM yangyanyy@hdu.edu.cn; yujun@hdu.edu.cn; jeyzhang@outlook.com;
   hanwd@zju.edu.cn; aock@zju.edu.cn; qmhuang@ucas.ac.cn
OI Yan, Yang/0000-0001-5598-1692; Zhang, Jian/0000-0001-6478-9192; Han,
   Weidong/0000-0001-7227-3671
FU National Natural Science Foundation for Distinguished Young Scholars of
   China [62125201, 62176230]
FX Manuscript received 15 January 2021; revised 16 July 2021; accepted 19
   October 2021. Date of publication 8 November 2021; date of current
   version 13 January 2023. This work was supported in part by the National
   Natural Science Foundation for Distinguished Young Scholars of China
   under Grant 62125201 and in part by the National Natural Science
   Foundation of China under Grant 62176230. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Jianguo Zhang. (Corresponding authors: Jun Yu;
   Weidong Han.)
CR Choi Y, 2011, P 15 C COMPUTATIONAL, P220
   Dai B, 2017, IEEE I CONF COMP VIS, P2989, DOI 10.1109/ICCV.2017.323
   Demner-Fushman D, 2016, J AM MED INFORM ASSN, V23, P304, DOI 10.1093/jamia/ocv080
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Gao LL, 2020, IEEE T PATTERN ANAL, V42, P1112, DOI 10.1109/TPAMI.2019.2894139
   Guo LT, 2020, IEEE T MULTIMEDIA, V22, P2149, DOI 10.1109/TMM.2019.2951226
   Harzig P., 2019, PROC BRIT MACH VIS C, V144, P1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Hossain MZ, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3295748
   Irvin J, 2019, AAAI CONF ARTIF INTE, P590
   Jing BY, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6570
   Jing BY, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2577
   Johnson AEW, 2019, SCI DATA, V6, DOI 10.1038/s41597-019-0322-0
   Kingma D. P., 2015, INT C LEARNING REPRE
   Kisilev P, 2015, IBM J RES DEV, V59, DOI 10.1147/JRD.2015.2393193
   Krause J, 2017, PROC CVPR IEEE, P3337, DOI 10.1109/CVPR.2017.356
   Li CY, 2019, AAAI CONF ARTIF INTE, P6666
   Li CY, 2018, 32 C NEURAL INFORM P
   Li XR, 2019, IEEE T MULTIMEDIA, V21, P2347, DOI 10.1109/TMM.2019.2896494
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Ordonez V., 2011, ADV NEURAL INFORM PR, P1143
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Peng Yifan, 2018, AMIA Jt Summits Transl Sci Proc, V2017, P188
   Ren Z, 2017, PROC CVPR IEEE, P1151, DOI 10.1109/CVPR.2017.128
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   Shetty R, 2017, IEEE I CONF COMP VIS, P4155, DOI 10.1109/ICCV.2017.445
   Shin HC, 2016, J MACH LEARN RES, V17
   Shin HC, 2016, PROC CVPR IEEE, P2497, DOI 10.1109/CVPR.2016.274
   Shizhe Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9959, DOI 10.1109/CVPR42600.2020.00998
   Tan JH, 2019, IEEE T MULTIMEDIA, V21, P2686, DOI 10.1109/TMM.2019.2904878
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang XS, 2018, PROC CVPR IEEE, P9049, DOI 10.1109/CVPR.2018.00943
   Xie XC, 2019, LECT NOTES COMPUT SC, V11448, P448, DOI 10.1007/978-3-030-18590-9_64
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu N, 2020, IEEE T MULTIMEDIA, V22, P1372, DOI 10.1109/TMM.2019.2941820
   Xue Y, 2018, LECT NOTES COMPUT SC, V11070, P457, DOI 10.1007/978-3-030-00928-1_52
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yu NG, 2019, IEEE T IMAGE PROCESS, V28, P2743, DOI 10.1109/TIP.2018.2889922
   Yuan JB, 2019, LECT NOTES COMPUT SC, V11769, P721, DOI 10.1007/978-3-030-32226-7_80
   Zhang YX, 2020, AAAI CONF ARTIF INTE, V34, P12910
NR 44
TC 7
Z9 7
U1 10
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 167
EP 178
DI 10.1109/TMM.2021.3122542
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9E8KM
UT WOS:000937028400012
DA 2024-07-18
ER

PT J
AU Zhang, L
   Du, YJ
   Shen, JY
   Zhen, XT
AF Zhang, Lei
   Du, Yingjun
   Shen, Jiayi
   Zhen, Xiantong
TI Learning to Learn With Variational Inference for Cross-Domain Image
   Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross domain learning; image classification; meta variational inference
AB Learning models that can generalize to previously unseen domains to which we have no access is a fundamental yet challenging problem in machine learning. In this paper, we propose meta variational inference (MetaVI), a variational Bayesian framework of meta-learning for cross domain image classification. Within the meta learning setting, MetaVI is derived to learn a probabilistic latent variable model by maximizing a meta evidence lower bound (Meta ELBO) for knowledge transfer across domains. To enhance the discriminative ability of the model, we further introduce a Wasserstein distance based constraint to the variational objective, leading to the Wasserstein MetaVI, which largely improves classification performance. By casting into a probabilistic inference problem, MetaVI offers the first, principled variational meta-learning framework for cross domain learning. In addition, we collect a new visual recognition dataset to contribute a more challenging benchmark for cross domain learning, which will be released to the public. Extensive experimental evaluation and ablation studies on four benchmarks show that our Wasserstein MetaVI achieves new state-of-the-art performance and surpasses previous methods, demonstrating its great effectiveness.
C1 [Zhang, Lei; Zhen, Xiantong] Guangdong Univ Petrochem Technol, Coll Comp Sci, Maoming 525011, Guangdong, Peoples R China.
   [Zhen, Xiantong] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.
   [Du, Yingjun; Shen, Jiayi] Univ Amsterdam, Informat Inst, NL-1012 WX Amsterdam, Netherlands.
C3 Guangdong University of Petrochemical Technology; University of
   Amsterdam
RP Zhen, XT (corresponding author), Guangdong Univ Petrochem Technol, Coll Comp Sci, Maoming 525011, Guangdong, Peoples R China.
EM zhanglei@gdupt.edu.cn; y.du@uva.nl; j.shen@uva.nl; zhenxt@gmail.com
OI Zhang, Lei/0000-0002-8494-0504; Du, Yingjun/0000-0001-7537-6457
FU Natural Science Foundation of China [62176068, 61976060]; Project of
   Educational Commission of Guangdong province of China [2018KCXTD019];
   Natural Science Foundation of Guangdong Province of China
   [2021A1515011846]; Special Fund for Science and Technology of Guangdong
   Province [2020S00055]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 62176068 and 61976060, in part by the Project of
   Educational Commission of Guangdong province of China under Grant
   2018KCXTD019, in part by the Natural Science Foundation of Guangdong
   Province of China under Grant 2021A1515011846, and in part by the
   Special Fund for Science and Technology of Guangdong Province under
   Grant 2020S00055. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Prof. Engin
   Erzin.(Corresponding author:Xiantong Zhen.)
CR Alemi A. A., 2018, P MACHINE LEARNING R, P159
   Andrychowicz M, 2016, ADV NEUR IN, V29
   Arjovsky M, 2020, Arxiv, DOI arXiv:1907.02893
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Balaji Y, 2018, ADV NEUR IN, V31
   Bengio Samy, 1992, P C OPT ART BIOL NEU, P281
   Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773
   Bousmalis K, 2016, ADV NEUR IN, V29
   Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233
   Chattopadhyay Prithvijit, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P301, DOI 10.1007/978-3-030-58545-7_18
   Choi MJ, 2010, PROC CVPR IEEE, P129, DOI 10.1109/CVPR.2010.5540221
   D'Innocente Antonio, 2019, Pattern Recognition. 40th German Conference, GCPR 2018. Proceedings: Lecture Notes in Computer Science (LNCS 11269), P187, DOI 10.1007/978-3-030-12939-2_14
   Dou Q., 2019, PROC ADV INT C NEURA
   Du Y., 2020, P INT C LEARN REPR
   Du YJ, 2024, IEEE T PATTERN ANAL, V46, P1464, DOI 10.1109/TPAMI.2022.3154930
   Edwards H., 2016, PROC INT C LEARN REP
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fengchun Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12553, DOI 10.1109/CVPR42600.2020.01257
   Finn C, 2018, ADV NEUR IN, V31
   Finn C, 2017, PR MACH LEARN RES, V70
   Fu H., 2020, P IEEE CVF C COMP VI, P14570
   Ganin Y, 2016, J MACH LEARN RES, V17
   Garnelo M, 2018, ICML WORKSHOP THEORE
   Garnelo M, 2018, PR MACH LEARN RES, V80
   Ghifary M, 2015, IEEE I CONF COMP VIS, P2551, DOI 10.1109/ICCV.2015.293
   Gordon J., 2019, PROC INT C LEARN REP
   Griffin G., 2007, CALTECH 256 OBJECT C
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Higgins I., 2017, P INT C LEARNING REP
   Kim H., 2019, PROC INT C LEARN REP
   Kim T., 2018, P ADV INT C NEUR INF
   Kingma D. P., 2014, arXiv
   Kingma DP, 2013, ARXIV
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li D, 2018, AAAI CONF ARTIF INTE, P3490
   Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591
   Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566
   Li YJ, 2019, PR MACH LEARN RES, V97
   Lin YX, 2020, IEEE MULTIMEDIA, V27, P44, DOI 10.1109/MMUL.2020.3008529
   Lu WT, 2013, IEEE T MULTIMEDIA, V15, P1920, DOI 10.1109/TMM.2013.2280895
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Malinin A, 2018, ADV NEUR IN, V31
   Matsuura T, 2020, AAAI CONF ARTIF INTE, V34, P11749
   Muandet Krikamol, 2013, INT C MACH LEARN, P10
   Nichol A, 2018, Arxiv, DOI [arXiv:1803.02999, DOI 10.48550/ARXIV.1803.02999]
   Qiao SY, 2018, PROC CVPR IEEE, P7229, DOI 10.1109/CVPR.2018.00755
   Ravi S, 2016, PROC INT C LEARN REP
   Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Rusu Andrei A., 2019, PROC INT C LEARN REP
   Schmidhuber J., 1987, Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta- ...  hook ...
   Shankar S., 2018, INT C LEARN REPRESEN
   Shu R, 2018, ADV NEUR IN, V31
   Shu Y, 2021, PROC CVPR IEEE, P9619, DOI 10.1109/CVPR46437.2021.00950
   Sohn K, 2015, ADV NEUR IN, V28
   Song G, 2019, IEEE T MULTIMEDIA, V21, P1261, DOI 10.1109/TMM.2018.2877122
   Titsias M., 2010, P 13 INT C ART INT S, P844
   Tolstikhin I., 2017, PROC INT C LEARN REP
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yingjun Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P200, DOI 10.1007/978-3-030-58607-2_12
   Zhang J, 2008, MACH LEARN, V73, P221, DOI 10.1007/s10994-008-5050-1
   Zhen X., 2020, Advances in Neural Information Processing Systems, V33, P9122
   Zhen X., 2020, ICML, P11409
NR 66
TC 2
Z9 2
U1 5
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3319
EP 3328
DI 10.1109/TMM.2022.3158072
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200028
DA 2024-07-18
ER

PT J
AU Zhang, Y
   Zhang, X
   Shi, CH
   Wu, X
   Li, XJ
   Peng, J
   Cao, KL
   Lv, JC
   Zhou, JL
AF Zhang, Yang
   Zhang, Xian
   Shi, Canghong
   Wu, Xi
   Li, Xiaojie
   Peng, Jing
   Cao, Kunlin
   Lv, Jiancheng
   Zhou, Jiliu
TI Pluralistic Face Inpainting With Transformation of Attribute Information
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attribute transformation; adversarial generative network; pluralistic
   face inpainting
ID IMAGE; COMPLETION
AB Most face-inpainting methods perform well in face repair. However, these methods can only complete a single face image per input. Although existing various image-inpainting methods can achieve pluralistic image inpainting, they typically produce faces with distorted structures or the same texture. To resolve these shortcomings and achieve high-quality diverse face inpainting, we propose PFTANet, a two-stage pluralistic face-inpainting network that transforms attribute information. In the first stage, the face-parsing network is fine-tuned to obtain semantic facial region information. In the second stage, a generator consisting of SNBlock, CF_ShiftBlocks, and CF_MergeBlock, which ensures that high-quality pluralistic face results are generated, is used. Specifically, CF_ShiftBlocks completes pluralistic face generation by transforming the attribute information from the conditional face extracted by the attribute extractor and ensuring the consistency of the attribute information between the conditional and generated faces. CF_MergeBlock ensures structural consistency between the masked and background regions of the generated face using facial region semantic information. A multi-patch discriminator is used to enhance facial detail generation. Experimental results for the CelebA and CelebA-HQ datasets indicated that PFTANet achieved pluralistic and visually realistic face inpainting.
C1 [Zhang, Yang; Zhang, Xian; Wu, Xi; Li, Xiaojie; Peng, Jing; Zhou, Jiliu] Chengdu Univ Informat Technol, Coll Comp Sci, Chengdu 610225, Peoples R China.
   [Shi, Canghong] Xihua Univ, Chengdu 610039, Peoples R China.
   [Cao, Kunlin] CuraCloud Corp, Seattle, WA 98125 USA.
   [Lv, Jiancheng] Sichuan Univ, Coll Comp Sci, Chengdu 610065, Peoples R China.
C3 Chengdu University of Information Technology; Xihua University; Sichuan
   University
RP Li, XJ; Peng, J (corresponding author), Chengdu Univ Informat Technol, Coll Comp Sci, Chengdu 610225, Peoples R China.
EM yang_zhangath@163.com; zhangxian318@163.com; canghongshi@163.com;
   wuxi@cuit.edu.cn; lixj@cuit.edu.cn; pengj@cuit.edu.cn;
   cao@curacloudcorp.com; lvjiancheng@scu.edu.cn; zhoujiliu@cuit.edu.cn
FU National Natural Science Foundation of China [42130608]; Sichuan Science
   and Technology Program [23NSFSC2224, 2021YFQ0053, 2022YFG0152,
   23ZHSF0169, 2020JDTD0020, 2022YFG0026, 2021YFG0018, 22ZDYF3709]
FX This work was supported in part by the National Natural Science
   Foundation of China, under Grant 42130608, and in part by the Sichuan
   Science and Technology Program, under Grants 23NSFSC2224, 2021YFQ0053,
   2022YFG0152, 23ZHSF0169, 2020JDTD0020, 2022YFG0026, 2021YFG0018, and
   22ZDYF3709.
CR Ballester C, 2001, IEEE T IMAGE PROCESS, V10, P1200, DOI 10.1109/83.935036
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Barrett WA, 2002, ACM T GRAPHIC, V21, P777, DOI 10.1145/566570.566651
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Chen RW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2003, DOI 10.1145/3394171.3413630
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Darabi S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185578
   Demir U, 2018, Arxiv, DOI arXiv:1803.07422
   Hays J, 2008, COMMUN ACM, V51, P87, DOI 10.1145/1400181.1400202
   Hongyu Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P725, DOI 10.1007/978-3-030-58536-5_43
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jeon YH, 2017, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2017.200
   Jingyuan Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7757, DOI 10.1109/CVPR42600.2020.00778
   Jolicoeur-Martineau A, 2018, Arxiv, DOI arXiv:1807.00734
   Karras T, 2018, Arxiv, DOI [arXiv:1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Lee D, 2019, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2019.00259
   Li HL, 2022, IEEE T CIRC SYST VID, V32, P4271, DOI 10.1109/TCSVT.2021.3130196
   Li WB, 2022, PROC CVPR IEEE, P10748, DOI 10.1109/CVPR52688.2022.01049
   Li YJ, 2017, PROC CVPR IEEE, P5892, DOI 10.1109/CVPR.2017.624
   Liao L, 2021, PROC CVPR IEEE, P6535, DOI 10.1109/CVPR46437.2021.00647
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu HY, 2021, PROC CVPR IEEE, P9367, DOI 10.1109/CVPR46437.2021.00925
   Liu HY, 2019, IEEE I CONF COMP VIS, P4169, DOI 10.1109/ICCV.2019.00427
   Miyato T, 2018, Arxiv, DOI arXiv:1802.05957
   Nazeri K., 2019, arXiv
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Quan WZ, 2022, IEEE T IMAGE PROCESS, V31, P2405, DOI 10.1109/TIP.2022.3152624
   Shen Yujun, 2020, P IEEE CVF C COMP VI, P9243, DOI DOI 10.1109/CVPR42600.2020.00926
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Sun HY, 2023, IEEE T MULTIMEDIA, V25, P4240, DOI 10.1109/TMM.2022.3174413
   Sun YJ, 2018, IEEE T IMAGE PROCESS, V27, P4160, DOI 10.1109/TIP.2018.2834737
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4672, DOI 10.1109/ICCV48922.2021.00465
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Xinhua Liu, 2021, 2021 IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC), P2581, DOI 10.1109/IAEAC50856.2021.9390983
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Yu T, 2020, AAAI CONF ARTIF INTE, V34, P12733
   Yu YC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P69, DOI 10.1145/3474085.3475436
   Zeng YH, 2019, PROC CVPR IEEE, P1486, DOI 10.1109/CVPR.2019.00158
   Zhang N, 2023, IEEE T MULTIMEDIA, V25, P3217, DOI 10.1109/TMM.2022.3157036
   Zhang RS, 2020, COMPUT GRAPH FORUM, V39, P471, DOI 10.1111/cgf.14160
   Zhang X, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108415
   Zhang X, 2021, APPL SOFT COMPUT, V111, DOI 10.1016/j.asoc.2021.107626
   Zhao L, 2020, PROC CVPR IEEE, P5740, DOI 10.1109/CVPR42600.2020.00578
   Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153
   Zhou T., 2020, IEEECVF C COMPUTER V, P7680
   Zhu JY, 2017, ADV NEUR IN, V30
   zllrunning, 2019, face-parsing
NR 54
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7967
EP 7979
DI 10.1109/TMM.2022.3229968
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400027
DA 2024-07-18
ER

PT J
AU Zhao, TS
   Huang, YH
   Feng, WZ
   Xu, YW
   Kwong, S
AF Zhao, Tiesong
   Huang, Yuhang
   Feng, Weize
   Xu, Yiwen
   Kwong, Sam
TI Efficient VVC Intra Prediction Based on Deep Feature Fusion and
   Probability Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Encoding; Feature extraction; Computational modeling; Complexity theory;
   Video coding; Kernel; Convolutional neural networks; Intra coding;
   rate-distortion (RD); versatile video coding (VVC); video coding
ID CU SIZE DECISION; DEPTH DECISION; PARTITION DECISION; MODE; ALGORITHM;
   STRATEGY
AB The ever-growing multimedia traffic has underscored the importance of effective multimedia codecs. Among them, the up-to-date lossy video coding standard, Versatile Video Coding (VVC), has been attracting attentions of video coding community. However, the gain of VVC is achieved at the cost of significant encoding complexity, which brings the need to realize fast encoder with comparable Rate Distortion (RD) performance. In this paper, we propose to optimize the VVC complexity at intra-frame prediction, with a two-stage framework of deep feature fusion and probability estimation. At the first stage, we employ the deep convolutional network to extract the spatial-temporal neighboring coding features. Then we fuse all reference features obtained by different convolutional kernels to determine an optimal intra coding depth. At the second stage, we employ a probability-based model and the spatial-temporal coherence to select the candidate partition modes within the optimal coding depth. Finally, these selected depths and partitions are executed whilst unnecessary computations are excluded. Experimental results on standard database demonstrate the superiority of proposed method, especially for High Definition (HD) and Ultra-HD (UHD) video sequences.
C1 [Zhao, Tiesong; Huang, Yuhang; Feng, Weize; Xu, Yiwen] Fuzhou Univ, Coll Phys & Informat Engn, Fujian Key Lab Intelligent Proc & Wireless Transm, Fuzhou 350108, Peoples R China.
   [Zhao, Tiesong] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Fuzhou University; Peng Cheng Laboratory; City University of Hong Kong
RP Xu, YW (corresponding author), Fuzhou Univ, Coll Phys & Informat Engn, Fujian Key Lab Intelligent Proc & Wireless Transm, Fuzhou 350108, Peoples R China.
EM t.zhao@fzu.edu.cn; n191127013@fzu.edu.cn; 201127019@fzu.edu.cn;
   xu_yiwen@fzu.edu.cn; cssamk@cityu.edu.hk
RI Kwong, Sam/C-9319-2012
OI Kwong, Sam/0000-0001-7484-7261
FU National Science Foundation of China [62171134]; Natural Science
   Foundation of Fujian Province, China [2022J02015]
FX This work was supported in part by the National Science Foundation of
   China under Grant 62171134 and in part by the Natural Science Foundation
   of Fujian Province, China under Grant 2022J02015. The Associate Editor
   coordinating the reviewof this manuscript and approving it for
   publication was Prof. James She.
CR Amestoy T, 2020, IEEE T IMAGE PROCESS, V29, P1313, DOI 10.1109/TIP.2019.2938670
   [Anonymous], 1999, Tech. Rep. ITU-T P.910
   Bae JH, 2019, J SIGNAL PROCESS SYS, V91, P863, DOI 10.1007/s11265-018-1399-y
   Bjontegaard G, 2001, VCEGM33
   Boyce J., 2018, P JVET J1010 SAN DIE
   Bross B, 2021, IEEE T CIRC SYST VID, V31, P3736, DOI 10.1109/TCSVT.2021.3101953
   Cebrián-Márquez G, 2019, IEEE T CIRC SYST VID, V29, P1448, DOI 10.1109/TCSVT.2018.2839026
   Chen K., 2018, P INT C SOL STAT INT, P1
   cisco.com, 2022, Cisco visual networking index: Forecast and methodology, 2017-2022
   Cui J, 2020, IEEE DATA COMPR CONF, P103, DOI 10.1109/DCC47342.2020.00018
   Dong XC, 2022, IEEE T MULTIMEDIA, V24, P400, DOI 10.1109/TMM.2021.3052348
   Duan K, 2019, IEEE DATA COMPR CONF, P570, DOI 10.1109/DCC.2019.00082
   Duanmu F, 2019, IEEE T CIRC SYST VID, V29, P3068, DOI 10.1109/TCSVT.2018.2874475
   Fan YB, 2020, IEEE ACCESS, V8, P107900, DOI 10.1109/ACCESS.2020.3000565
   Feng ZQ, 2018, IEEE ACCESS, V6, P45262, DOI 10.1109/ACCESS.2018.2864881
   Fernández DG, 2018, MULTIMED TOOLS APPL, V77, P5907, DOI 10.1007/s11042-017-4503-6
   Fu T, 2019, IEEE INT CON MULTI, P55, DOI 10.1109/ICME.2019.00018
   Galpin F, 2019, IEEE DATA COMPR CONF, P162, DOI 10.1109/DCC.2019.00024
   Gao H, 2020, IEEE DATA COMPR CONF, P93, DOI 10.1109/DCC47342.2020.00017
   Goswami K, 2018, IEEE T IND ELECTRON, V65, P8861, DOI 10.1109/TIE.2018.2815941
   Grellert M, 2019, IEEE T CIRC SYST VID, V29, P1741, DOI 10.1109/TCSVT.2018.2849941
   Gu JW, 2018, IEEE SIGNAL PROC LET, V25, P159, DOI 10.1109/LSP.2017.2766766
   Jamali M, 2019, IEEE T BROADCAST, V65, P109, DOI 10.1109/TBC.2018.2847464
   Jia LH, 2019, IEEE T MULTIMEDIA, V21, P835, DOI 10.1109/TMM.2018.2866762
   Jin ZP, 2018, IEEE ACCESS, V6, P54660, DOI 10.1109/ACCESS.2018.2872492
   Kim K, 2019, IEEE T CIRC SYST VID, V29, P1462, DOI 10.1109/TCSVT.2018.2839113
   Kuanar S, 2018, IEEE INT CONF MULTI
   Kuang W, 2020, IEEE T IMAGE PROCESS, V29, P170, DOI 10.1109/TIP.2019.2924810
   Lee JK, 2020, IEEE ACCESS, V8, P95906, DOI 10.1109/ACCESS.2020.2993566
   Lei M, 2019, IEEE IMAGE PROC, P4120, DOI [10.1109/ICIP.2019.8803421, 10.1109/icip.2019.8803421]
   Li TY, 2021, IEEE T IMAGE PROCESS, V30, P5377, DOI 10.1109/TIP.2021.3083447
   Li TY, 2017, IEEE INT CON MULTI, P1255, DOI 10.1109/ICME.2017.8019316
   Li Y, 2021, IEEE T BROADCAST, V67, P710, DOI 10.1109/TBC.2021.3073556
   Liu XG, 2019, IEEE T CIRC SYST VID, V29, P144, DOI 10.1109/TCSVT.2017.2777903
   Lu G, 2019, PROC CVPR IEEE, P10998, DOI 10.1109/CVPR.2019.01126
   Mercat A, 2019, J SIGNAL PROCESS SYS, V91, P1021, DOI 10.1007/s11265-018-1426-z
   Moorthy AK, 2012, IEEE J-STSP, V6, P652, DOI 10.1109/JSTSP.2012.2212417
   Pan ZQ, 2021, IEEE SIGNAL PROC LET, V28, P1260, DOI 10.1109/LSP.2021.3086692
   Park SH, 2021, IEEE T MULTIMEDIA, V23, P4388, DOI 10.1109/TMM.2020.3042062
   Park SH, 2019, IEEE ACCESS, V7, P172597, DOI 10.1109/ACCESS.2019.2956196
   Ryu S, 2018, IEEE T IMAGE PROCESS, V27, P5525, DOI 10.1109/TIP.2018.2857404
   Saldanha M, 2022, IEEE T CIRC SYST VID, V32, P3947, DOI 10.1109/TCSVT.2021.3108671
   Shen LQ, 2019, IEEE T MULTIMEDIA, V21, P2714, DOI 10.1109/TMM.2019.2909859
   Shen LQ, 2013, IEEE T CONSUM ELECTR, V59, P207, DOI 10.1109/TCE.2013.6490261
   Sun CT, 2018, IEEE IMAGE PROC, P1787, DOI 10.1109/ICIP.2018.8451178
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang N, 2019, 2019 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS (APCCAS 2019), P361, DOI [10.1109/apccas47518.2019.8953076, 10.1109/APCCAS47518.2019.8953076]
   Tech G, 2021, IEEE DATA COMPR CONF, P3, DOI 10.1109/DCC50243.2021.00008
   Tsang SH, 2019, IEEE T MULTIMEDIA, V21, P2433, DOI 10.1109/TMM.2019.2907472
   Tsang SH, 2019, IEEE T MULTIMEDIA, V21, P269, DOI 10.1109/TMM.2018.2856078
   vcgit.hhi.fraunhofer.de, 2021, VTM-12.0
   Wang DY, 2020, IEEE T MULTIMEDIA, V22, P833, DOI 10.1109/TMM.2019.2937240
   Wang Z, 2018, IEEE IMAGE PROC, P2550, DOI 10.1109/ICIP.2018.8451258
   Wu GQ, 2021, IEEE INT SYMP CIRC S
   Wu SL, 2022, IEEE T CIRC SYST VID, V32, P5638, DOI 10.1109/TCSVT.2022.3146061
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P2141, DOI 10.1109/TMM.2014.2356795
   Xu M, 2018, IEEE T IMAGE PROCESS, V27, P5044, DOI 10.1109/TIP.2018.2847035
   Yang H, 2020, IEEE T CIRC SYST VID, V30, P1668, DOI 10.1109/TCSVT.2019.2904198
   Yeh CH, 2018, IEEE ACCESS, V6, P50087, DOI 10.1109/ACCESS.2018.2867342
   Zhang HB, 2018, IEEE T CIRC SYST VID, V28, P513, DOI 10.1109/TCSVT.2016.2612693
   Zhang J, 2018, IEEE T MULTIMEDIA, V20, P29, DOI 10.1109/TMM.2017.2723238
   Zhang QW, 2020, IEEE ACCESS, V8, P117539, DOI 10.1109/ACCESS.2020.3004580
   Zhao TS, 2010, IEEE IMAGE PROC, P3389, DOI 10.1109/ICIP.2010.5652737
NR 63
TC 9
Z9 9
U1 5
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6411
EP 6421
DI 10.1109/TMM.2022.3208516
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500054
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhou, WH
   Zhang, H
   Yan, ZM
   Wang, WS
   Lin, LL
AF Zhou, Wenhui
   Zhang, Hua
   Yan, Zhengmao
   Wang, Weisheng
   Lin, Lili
TI DecoupledPoseNet: Cascade Decoupled Pose Learning for Unsupervised
   Camera Ego-Motion Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image sequences; Camera ego-motion; decoupling structure; pose
   estimation; rigid-aware; unsupervised learning
ID VISUAL ODOMETRY; ACCURATE; STEREO
AB Although many impressive works on learning-based camera ego-motion estimation methods have been proposed recently, most of them promote the accuracy of camera pose estimation by various sequential learning with loop closure optimization, while neglecting the improvement of PoseNet itself. In this paper, we focus on the coupling of rotation and translation in ego-motion estimation, and design a cascade decoupling structure to separately learn the rotation and translation of camera relative motion between adjacent frames. Meanwhile, a rigid-aware unsupervised learning framework with iterative pose refinement scheme is proposed for camera ego-motion estimation. It can disambiguate rigid motion and deformations in dynamic scenarios by jointly learning of optical flow, stereo disparity and camera pose. Validated with evaluation experiments on the public available datasets, our method is superior to the state-of-the-art unsupervised methods, and can achieve comparable results with the supervised ones.
C1 [Zhou, Wenhui; Zhang, Hua; Yan, Zhengmao; Wang, Weisheng] Hangzhou Dianzi Univ, Coll Comp Sci & Technol, Hangzhou 310018, Peoples R China.
   [Lin, Lili] Zhejiang Gongshang Univ, Sch Informat & Elect Engn, Hangzhou, Peoples R China.
C3 Hangzhou Dianzi University; Zhejiang Gongshang University
RP Lin, LL (corresponding author), Zhejiang Gongshang Univ, Sch Informat & Elect Engn, Hangzhou, Peoples R China.
EM zhouwenhui@hdu.edu.cn; zhangh@hdu.edu.cn; 201050024@hdu.edu.cn;
   weishengwang@hdu.edu.cn; lili_lin@zjgsu.edu.cn
RI Zhou, Wenhui/AAL-8470-2021
OI Zhou, Wenhui/0000-0003-2216-8878
FU National Key Ramp;D Program of China
FX No Statement Available
CR Alvarez L, 2007, INT J COMPUT VISION, V75, P371, DOI 10.1007/s11263-007-0041-4
   Bazin JC, 2010, COMPUT VIS IMAGE UND, V114, P254, DOI 10.1016/j.cviu.2009.04.006
   Bian JW, 2019, ADV NEUR IN, V32
   Blanco-Claraco JL, 2014, INT J ROBOT RES, V33, P207, DOI 10.1177/0278364913507326
   Brahmbhatt S, 2018, PROC CVPR IEEE, P2616, DOI 10.1109/CVPR.2018.00277
   Carlone L, 2015, IEEE INT CONF ROBOT, P4597, DOI 10.1109/ICRA.2015.7139836
   Chang Shu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P572, DOI 10.1007/978-3-030-58529-7_34
   Clark R, 2017, PROC CVPR IEEE, P2652, DOI 10.1109/CVPR.2017.284
   Davison AJ, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1403
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Eigen D, 2014, ADV NEUR IN, V27
   Feng T, 2019, IEEE ROBOT AUTOM LET, V4, P4431, DOI 10.1109/LRA.2019.2925555
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Geiger A, 2011, IEEE INT VEH SYM, P963, DOI 10.1109/IVS.2011.5940405
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Gong XX, 2021, IEEE T MULTIMEDIA, V23, P2820, DOI 10.1109/TMM.2020.3017886
   Guan BL, 2018, IEEE INT CONF ROBOT, P2320
   Jiao JC, 2019, IEEE ACCESS, V7, P94118, DOI 10.1109/ACCESS.2019.2926350
   Jiao Y, 2021, PROC CVPR IEEE, P5534, DOI 10.1109/CVPR46437.2021.00549
   Kaess M, 2009, IEEE INT CONF ROBOT, P973
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Khan NH, 2017, MULTIMED TOOLS APPL, V76, P16581, DOI 10.1007/s11042-016-3939-4
   Kim P., 2017, P BRIT MACH VIS C, P1
   Kim P, 2018, IEEE INT CONF ROBOT, P7247
   Kingma D. P, 2015, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1412.6980
   Lai HY, 2019, PROC CVPR IEEE, P1890, DOI 10.1109/CVPR.2019.00199
   Li RH, 2018, IEEE INT CONF ROBOT, P7286, DOI 10.1109/ICRA.2018.8461251
   Li RH, 2018, IEEE T AUTOM SCI ENG, V15, P651, DOI 10.1109/TASE.2017.2664920
   Li SK, 2020, PROC CVPR IEEE, P6338, DOI 10.1109/CVPR42600.2020.00637
   Li Y, 2019, IEEE INT CONF ROBOT, P5439, DOI [10.1109/ICRA.2019.8793706, 10.1109/icra.2019.8793706]
   Ling CW, 2022, IEEE T MULTIMEDIA, V24, P2938, DOI 10.1109/TMM.2021.3091308
   Liu L, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P876
   Luo CX, 2020, IEEE T PATTERN ANAL, V42, P2624, DOI 10.1109/TPAMI.2019.2930258
   Mahjourian R, 2018, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2018.00594
   Martins R, 2017, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2017.8206466
   Min ZX, 2020, PROC CVPR IEEE, P4897, DOI 10.1109/CVPR42600.2020.00495
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   PRAZDNY K, 1980, BIOL CYBERN, V36, P87, DOI 10.1007/BF00361077
   Ranjan A, 2019, PROC CVPR IEEE, P12232, DOI 10.1109/CVPR.2019.01252
   Raudies F, 2012, COMPUT VIS IMAGE UND, V116, P606, DOI 10.1016/j.cviu.2011.04.004
   Scaramuzza D, 2011, IEEE ROBOT AUTOM MAG, V18, P80, DOI 10.1109/MRA.2011.943233
   Shen TW, 2019, IEEE INT CONF ROBOT, P6359, DOI [10.1109/icra.2019.8793479, 10.1109/ICRA.2019.8793479]
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   UMEYAMA S, 1991, IEEE T PATTERN ANAL, V13, P376, DOI 10.1109/34.88573
   Wagstaff B, 2020, IEEE INT CONF ROBOT, P2331, DOI [10.1109/icra40945.2020.9197562, 10.1109/ICRA40945.2020.9197562]
   Wang DM, 2022, IEEE T MULTIMEDIA, V24, P4394, DOI 10.1109/TMM.2021.3117092
   Wang R, 2019, PROC CVPR IEEE, P5647, DOI 10.1109/CVPR.2019.00570
   Wang S., 2017, ICRA, DOI [10.1109/icra.2017.7989236, DOI 10.1109/ICRA.2017.7989236]
   Wang S, 2018, INT J ROBOT RES, V37, P513, DOI 10.1177/0278364917734298
   Wang Y, 2019, PROC CVPR IEEE, P8063, DOI 10.1109/CVPR.2019.00826
   Wang Y, 2018, PROC CVPR IEEE, P4884, DOI 10.1109/CVPR.2018.00513
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xue F, 2019, IEEE I CONF COMP VIS, P2841, DOI 10.1109/ICCV.2019.00293
   Xue F, 2019, PROC CVPR IEEE, P8567, DOI 10.1109/CVPR.2019.00877
   Xue F, 2019, LECT NOTES COMPUT SC, V11366, P293, DOI 10.1007/978-3-030-20876-9_19
   Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212
   Zhan HY, 2020, IEEE INT CONF ROBOT, P4203, DOI [10.1109/icra40945.2020.9197374, 10.1109/ICRA40945.2020.9197374]
   Zhan HY, 2018, PROC CVPR IEEE, P340, DOI 10.1109/CVPR.2018.00043
   Zhang H, 2022, IEEE T MULTIMEDIA, V24, P3144, DOI 10.1109/TMM.2021.3093771
   Zhao Wang, 2020, P IEEE CVF C COMP VI
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
   Zhu A.Z., 2018, arXiv
NR 64
TC 3
Z9 3
U1 1
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1636
EP 1648
DI 10.1109/TMM.2022.3144958
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD7R0
UT WOS:001116593700002
DA 2024-07-18
ER

PT J
AU Carballeira, P
   Carmona, C
   Díaz, C
   Berjón, D
   Corregidor, D
   Cabrera, J
   Morán, F
   Doblado, C
   Arnaldo, S
   Martín, MD
   García, N
AF Carballeira, Pablo
   Carmona, Carlos
   Diaz, Cesar
   Berjon, Daniel
   Corregidor, Daniel
   Cabrera, Julian
   Moran, Francisco
   Doblado, Carmen
   Arnaldo, Sergio
   Martin, Maria del Mar
   Garcia, Narciso
TI FVV Live: A Real-Time Free-Viewpoint Video System With Consumer
   Electronics Hardware
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cameras; Streaming media; Real-time systems; Rendering (computer
   graphics); Bit rate; Hardware; Servers; Visual communications;
   free-viewpoint video; consumer electronics; multiview video; depth
   coding; depth image-based rendering; subjective assessment
ID MULTIVIEW; IMAGES; DIBR
AB FVV Live is a novel end-to-end free-viewpoint video system, designed for real-time operation, using consumer-grade cameras and hardware, which enables low deployment costs and easy installation for immersive event-broadcasting or videoconferencing. All the blocks of the system have been designed to maximize perceptual video quality, overcoming the limitations imposed by hardware and network, which impact directly the accuracy of depth data and thus the quality of virtual view synthesis. Therefore, it does not sacrifice perceptual video quality with respect to high-end counterparts. The results presented in this paper correspond to an implementation with nine stereo-based depth cameras. However, the design of the acquisition block of FVV Live allows scalability for an arbitrary number of cameras. In addition, FVV Live presents low motion-to-photon and end-to-end delays, which enables a responsive free-viewpoint navigation and bilateral immersive communications. Moreover, the visual quality of FVV Live has been assessed through subjective assessment with satisfactory results, and additional comparative tests show that it is preferred over state-of-the-art DIBR alternatives.
C1 [Carballeira, Pablo] Univ Politecn Madrid, Grp Tratamiento Imagenes, Madrid, Spain.
   [Carballeira, Pablo] Univ Autonoma Madrid, Escuela Politecn Super, Video Proc & Understanding Lab, Madrid 28049, Spain.
   [Carmona, Carlos; Diaz, Cesar; Berjon, Daniel; Corregidor, Daniel; Cabrera, Julian; Moran, Francisco; Doblado, Carmen; Arnaldo, Sergio; Martin, Maria del Mar; Garcia, Narciso] Univ Politecn Madrid, Grp Tratamiento Imagenes, Informat Proc & Telecommun Ctr, Madrid 28040, Spain.
   [Carmona, Carlos; Diaz, Cesar; Berjon, Daniel; Corregidor, Daniel; Cabrera, Julian; Moran, Francisco; Doblado, Carmen; Arnaldo, Sergio; Martin, Maria del Mar; Garcia, Narciso] Univ Politecn Madrid, ETSI Telecomunicac, Madrid 28040, Spain.
C3 Universidad Politecnica de Madrid; Autonomous University of Madrid;
   Universidad Politecnica de Madrid; Centro de I+D+I en Procesado de la
   Informacion Telecomunicaciones (IPT); Universidad Politecnica de Madrid
RP Carballeira, P (corresponding author), Univ Politecn Madrid, Grp Tratamiento Imagenes, Madrid, Spain.; Carballeira, P (corresponding author), Univ Autonoma Madrid, Escuela Politecn Super, Video Proc & Understanding Lab, Madrid 28049, Spain.
EM pablo.carballeira@uam.es; ccv@gti.ssr.upm.es; cdm@gti.ssr.upm.es;
   dbd@gti.ssr.upm.es; dcl@gti.ssr.upm.es; julian.cabrera@gti.ssr.upm.es;
   fmb@gti.ssr.upm.es; cdo@gti.ssr.upm.es; sad@gti.ssr.upm.es;
   mmp@gti.ssr.upm.es; narciso@gti.ssr.upm.es
RI Díaz, César/Y-9989-2019; Berjón, Daniel/Y-4800-2019; Carballeira,
   Pablo/I-5983-2019; Morán, Francisco/F-2552-2016; QUESADA, JULIAN
   CABRERA/Y-7544-2019; García, Narciso/E-8603-2011
OI Díaz, César/0000-0003-2030-9390; Berjón, Daniel/0000-0003-0584-7166;
   Carballeira, Pablo/0000-0002-7199-698X; Morán,
   Francisco/0000-0003-3837-692X; QUESADA, JULIAN
   CABRERA/0000-0002-7154-2451; García, Narciso/0000-0002-0397-894X
FU Ministerio de Ciencia, Innovacion y Universidades (AEI/FEDER) of the
   Spanish Government [TEC2016-75981]; Huawei Technologies Company, Ltd.;
   European Union [H2020-957102]
FX This work was supported by the Ministerio de Ciencia, Innovacion y
   Universidades (AEI/FEDER) of the Spanish Government under Project
   TEC2016-75981 (IVME), by Huawei Technologies Company, Ltd., and by the
   European Union under Project H2020-957102 (5G-RECORDS). The associate
   editor coordinating the reviewof this manuscript and approving it for
   publication was Dr. Sanjeev Mehrotra.
CR 4dreplay, 4D REPLAY
   Alcantarilla PF, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.13
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   [Anonymous], 2019, BT50014 ITUR
   [Anonymous], 2012, 3DTV C TRUE VISION C
   [Anonymous], 2016, ISOIEC JTC 1SC 29WG
   Berger K., 2011, VISION MODELING VISU, P317
   Berger M, 2017, COMPUT GRAPH FORUM, V36, P301, DOI 10.1111/cgf.12802
   Berjn D, 2020, PROC IEEE INT C MULT, P12
   Berjn D., 2016, PROC 6 INT C IMAGE P, P16
   Berjon D., 2011, PROC IEEEINT C MULTI, P1
   Camplani M, 2013, IEEE T CYBERNETICS, V43, P1560, DOI 10.1109/TCYB.2013.2271112
   Carballeira P, 2016, SIGNAL PROCESS-IMAGE, V41, P128, DOI 10.1016/j.image.2015.12.007
   Carmona C, 2018, DOCUMENT VQEG IMG 20
   Carranza J, 2003, ACM T GRAPHIC, V22, P569, DOI 10.1145/882262.882309
   Chan D., 2008, WORKSHOP MULTICAMERA
   Clarke L., 1994, PROGRAMMING ENV MASS, P213
   Cohen Michael F., 1993, Radiosity and realistic image synthesis
   Collet A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766945
   Cubelos J, 2020, IEEE T MULTIMEDIA, V22, P69, DOI 10.1109/TMM.2019.2924575
   Dziembowski A, 2016, PICT COD SYMP, DOI 10.1109/PCS.2016.7906380
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Furukawa Y., 2015, MULTIVIEW STEREO TUT, V9
   Gallego G, 2013, IEEE T SIGNAL PROCES, V61, P4387, DOI 10.1109/TSP.2013.2269047
   Hall D. S, 2006, U.S. Patent US, Patent No. [7969558B2, 7969558]
   Hinds AT, 2017, IEEE INT CON MULTI, P1171, DOI 10.1109/ICME.2017.8019543
   Vu HH, 2012, IEEE T PATTERN ANAL, V34, P889, DOI 10.1109/TPAMI.2011.172
   Hornung A, 2009, COMPUT GRAPH FORUM, V28, P2090, DOI 10.1111/j.1467-8659.2009.01416.x
   IEEE, 2008, IEEE Std 1588-2019, P1
   intel, INTEL TRUE VIEW
   Intel, Intel RealSense Depth Camera D435
   ITU-T, 2016, Recommendation P.915, P913
   ITU-T, 2008, ITU T P910 SUBJECTIV
   ITU-T rec, 2003, ITU T REC H264 ISOIE
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Karlsson G, 1996, IEEE COMMUN MAG, V34, P118, DOI 10.1109/35.533930
   Khoshelham K, 2011, INT ARCH PHOTOGRAMM, V38-5, P133
   Liu YB, 2010, IEEE T VIS COMPUT GR, V16, P407, DOI 10.1109/TVCG.2009.88
   Luo GB, 2017, IEEE T CIRC SYST VID, V27, P2118, DOI 10.1109/TCSVT.2016.2583978
   Maimone A, 2012, IEEE VIRTUAL REALITY CONFERENCE 2012 PROCEEDINGS, P51, DOI 10.1109/VR.2012.6180879
   Mark W. R., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P7, DOI 10.1145/253284.253292
   Mekuria R, 2017, IEEE T CIRC SYST VID, V27, P828, DOI 10.1109/TCSVT.2016.2543039
   Mustafa A, 2016, PROC CVPR IEEE, P4660, DOI 10.1109/CVPR.2016.504
   Nonaka K, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING (IEEE VCIP)
   Ortiz L. E., 2018, ELCVIA Electron. Lett. Comput. Vis. Image Anal., V17, P1
   Orts-Escolano S, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P741, DOI 10.1145/2984511.2984517
   Pagés R, 2018, J VIS COMMUN IMAGE R, V53, P192, DOI 10.1016/j.jvcir.2018.03.012
   Pece Fabrizio, 2011, EGVE EUROVR, P59
   Qingxiong Yang, 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P69, DOI 10.1109/MMSP.2010.5661996
   Sabirin H, 2018, IEEE MULTIMEDIA, V25, P61, DOI 10.1109/MMUL.2018.112142739
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Schierl T, 2011, P IEEE, V99, P671, DOI 10.1109/JPROC.2010.2091370
   Seitz S.M., 2006, IEEE COMP SOC C COMP, P519
   Senoh T., 2019, MPEG CONTRIB M50095
   Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882
   Smolic A, 2011, PATTERN RECOGN, V44, P1958, DOI 10.1016/j.patcog.2010.09.005
   Stankiewicz O., 2018, Academic Press Library in Signal Processing, V6, P3
   Stankiewicz O, 2018, IEEE T MULTIMEDIA, V20, P2182, DOI 10.1109/TMM.2018.2790162
   stereolabs, ZED STEREO CAMERA
   Suenaga R, 2015, PROC SPIE, V9393, DOI 10.1117/12.2077524
   Tech G, 2016, IEEE T CIRC SYST VID, V26, P35, DOI 10.1109/TCSVT.2015.2477935
   Tian SS, 2021, NEUROCOMPUTING, V423, P158, DOI 10.1016/j.neucom.2020.09.062
   Wang Y.-K., 2011, RFC 6184
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Wu GC, 2017, IEEE J-STSP, V11, P926, DOI 10.1109/JSTSP.2017.2747126
   Yang L, 2015, IEEE SENS J, V15, P4275, DOI 10.1109/JSEN.2015.2416651
   Zabatani A, 2020, IEEE T PATTERN ANAL, V42, P2333, DOI 10.1109/TPAMI.2019.2915841
   Zhu C, 2016, IEEE T BROADCAST, V62, P82, DOI 10.1109/TBC.2015.2475697
   Zou XJ, 2012, MACH VISION APPL, V23, P43, DOI 10.1007/s00138-010-0291-y
NR 69
TC 15
Z9 15
U1 2
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2378
EP 2391
DI 10.1109/TMM.2021.3079711
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Janani, T
   Brindha, M
AF Janani, T.
   Brindha, M.
TI SEcure Similar Image Matching (SESIM): An Improved Privacy Preserving
   Image Retrieval Protocol over Encrypted Cloud Database
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retrieval; Cloud computing; Feature extraction; Protocols;
   Privacy; Image matching; Encryption; Image matching; feature vectors;
   encryption; similarity matching and cloud
ID SEARCH; EFFICIENT; SCHEME
AB The emergence of cloud computing provides new dimension for the user to perform computations and store huge amount of data say images, video, audio etc,. However, the benefits of outsourcing the tasks bring privacy issues for the data that are outsourced. Consequently, to ensure privacy, multimedia data is encrypted and offloaded to the cloud database. Though images are encrypted, during retrieval, cloud server performs similarity computation on plaintext features. Fully homomorphic shows great results in computation over encrypted data yet due to its computation burden it is not applicable for practical usage. Thus, to guarantee the secrecy of outsourced image features, the proposed paper introduced an efficient SEcure Similar Image Matching (SESIM) protocol under encrypted domain. The computation overhead of the proposed SESIM protocol is compared with the existing secure distance metrics. The experiments and performance analysis show the effectiveness and security of the proposed scheme under encrypted cloud database.
C1 [Janani, T.; Brindha, M.] Natl Inst Technol, Dept Comp Sci & Engn, Tiruchirappalli 620015, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Tiruchirappalli
RP Brindha, M (corresponding author), Natl Inst Technol, Dept Comp Sci & Engn, Tiruchirappalli 620015, India.
EM saijanani.308@gmail.com; brindham@nitt.edu
CR Abduljabbar Z. A., 2016, APPL SCI, V6, P1
   Abdulsada A. I., 2016, J KUFA MATH COMPUT, V3, P25
   Ahmed A, 2020, IEEE ACCESS, V8, P194541, DOI 10.1109/ACCESS.2020.3033504
   Androutsos D., 1998, P IEEE 9 EUR SIGN PR, P1
   Chang YC, 2005, LECT NOTES COMPUT SC, V3531, P442
   Chen L, 2021, GEO-SPAT INF SCI, V24, P58, DOI 10.1080/10095020.2020.1843376
   Cheng E, 2009, IEEE T IMAGE PROCESS, V18, P1350, DOI 10.1109/TIP.2009.2017128
   Dong XJ, 2022, IEEE T SERV COMPUT, V15, P1678, DOI 10.1109/TSC.2020.3008957
   Gao ZN, 2016, IEEE T MULTIMEDIA, V18, P1661, DOI 10.1109/TMM.2016.2568748
   Huang X, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12193158
   Huang ZB, 2019, IEEE ACCESS, V7, P174541, DOI 10.1109/ACCESS.2019.2957497
   Iida K, 2020, IEEE ACCESS, V8, P200038, DOI 10.1109/ACCESS.2020.3035563
   Janani T, 2021, IRBM, V42, P83, DOI 10.1016/j.irbm.2020.02.005
   Jiang LZ, 2020, IEEE T DEPEND SECURE, V17, P179, DOI 10.1109/TDSC.2017.2751476
   Jiang YG, 2013, IEEE T MULTIMEDIA, V15, P442, DOI 10.1109/TMM.2012.2231061
   Li BT, 2003, MULTIMEDIA SYST, V8, P512, DOI 10.1007/s00530-002-0069-9
   Li JS, 2020, IEEE ACCESS, V8, P114940, DOI 10.1109/ACCESS.2020.3003928
   Li XL, 2021, Arxiv, DOI arXiv:2102.03517
   Li YY, 2022, IEEE T CLOUD COMPUT, V10, P1142, DOI 10.1109/TCC.2020.2989923
   Lindell Y, 2005, Encyclopedia of Data Warehousing and Mining, P1005
   Ling HF, 2020, MULTIMED TOOLS APPL, V79, P5595, DOI 10.1007/s11042-019-08422-2
   Lu W, 2010, PROC SPIE, V7541, DOI 10.1117/12.838745
   Neumann D., 2006, ACM Trans. Appl. Percept, V3, P31, DOI DOI 10.1145/1119766.1119769
   Pang L, 2016, AAAI CONF ARTIF INTE, P2793
   Qin JH, 2019, IEEE ACCESS, V7, P24626, DOI 10.1109/ACCESS.2019.2894673
   Rahim N, 2018, COMPUT COMMUN, V127, P75, DOI 10.1016/j.comcom.2018.06.001
   Samanthula BK, 2015, IEEE T KNOWL DATA EN, V27, P1261, DOI 10.1109/TKDE.2014.2364027
   Sayyad Suhel, 2020, Proceedings of Second International Conference on Inventive Research in Computing Applications (ICIRCA 2020), P139, DOI 10.1109/ICIRCA48905.2020.9183133
   Shen JL, 2021, INFORM SCIENCES, V569, P469, DOI 10.1016/j.ins.2020.11.026
   Shen M, 2019, IEEE NETWORK, V33, P27, DOI 10.1109/MNET.001.1800503
   Song DXD, 2000, P IEEE S SECUR PRIV, P44, DOI 10.1109/SECPRI.2000.848445
   Sun XQ, 2020, IEEE T EMERG TOP COM, V8, P352, DOI 10.1109/TETC.2018.2794611
   Tong Q., 2021, IEEE T SERVICES COMP
   Wang H, 2020, IEEE ACCESS, V8, P61138, DOI 10.1109/ACCESS.2020.2983194
   Wang YH, 2021, J VIS COMMUN IMAGE R, V74, DOI 10.1016/j.jvcir.2020.102998
   Wang Y, 2019, SOFT COMPUT, V23, P2101, DOI 10.1007/s00500-017-2927-6
   Xia ZH, 2021, IEEE T NETW SCI ENG, V8, P318, DOI 10.1109/TNSE.2020.3038218
   Xia ZH, 2022, IEEE T SERV COMPUT, V15, P202, DOI 10.1109/TSC.2019.2927215
   Xia ZH, 2017, INFORM SCIENCES, V387, P195, DOI 10.1016/j.ins.2016.12.030
   Xia ZH, 2016, IEEE T INF FOREN SEC, V11, P2594, DOI 10.1109/TIFS.2016.2590944
   Xia ZH, 2018, IEEE T CLOUD COMPUT, V6, P276, DOI 10.1109/TCC.2015.2491933
   Xu YY, 2019, IEEE ACCESS, V7, P160082, DOI 10.1109/ACCESS.2019.2951175
   Zhang Y, 2014, INT CONF DIGIT SIG, P269, DOI 10.1109/ICDSP.2014.6900669
   Zhang ZZ, 2019, IEEE T MULTIMEDIA, V21, P2878, DOI 10.1109/TMM.2019.2915036
   Zhang ZY, 2020, IEEE ACCESS, V8, P66828, DOI 10.1109/ACCESS.2020.2984916
   Zou Q, 2017, SOFT COMPUT, V21, P2959, DOI 10.1007/s00500-016-2153-7
NR 46
TC 9
Z9 9
U1 5
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3794
EP 3806
DI 10.1109/TMM.2021.3107681
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400010
DA 2024-07-18
ER

PT J
AU Liu, JX
   Zhao, YH
   Chen, SH
   Zhang, Y
AF Liu, Jinxiang
   Zhao, Yangheng
   Chen, Siheng
   Zhang, Ya
TI A 3D Mesh-Based Lifting-and-Projection Network for Human Pose Transfer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Task analysis; Visualization; Solid
   modeling; Shape; Image reconstruction; Biological system modeling; Human
   pose transfer; lifting-and-projection; graph convolutional networks; 3D
   mesh recovery
AB Human pose transfer has typically been modeled as a 2D image-to-image translation problem. This formulation ignores the human body shape prior in 3D space and inevitably causes implausible artifacts, especially when facing occlusion. To address this issue, we propose a lifting-and-projection framework to perform pose transfer in the 3D mesh space. The core of our framework is a foreground generation module, that consists of two novel networks: a lifting-and-projection network (LPNet) and an appearance detail compensating network (ADCNet). To leverage the human body shape prior, LPNet exploits the topological information of the body mesh to learn an expressive visual representation for the target person in the 3D mesh space. To preserve texture details, ADCNet is further introduced to enhance the feature produced by LPNet with the source foreground image. Such design of the foreground generation module enables the model to better handle difficult cases such as those with occlusions. Experiments on the iPER and Fashion datasets empirically demonstrate that the proposed lifting-and-projection framework is effective and outperforms the existing image-to-image-based and mesh-based methods on human pose transfer task in both self-transfer and cross-transfer settings.
C1 [Liu, Jinxiang; Zhao, Yangheng; Chen, Siheng; Zhang, Ya] Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Zhang, Y (corresponding author), Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
EM jinxliu@sjtu.edu.cn; zhaoyangheng-sjtu@sjtu.edu.cn; sihengc@sjtu.edu.cn;
   ya_zhang@sjtu.edu.cn
RI Zhang, Ya/Y-8255-2019
OI Zhang, Ya/0000-0002-5390-9053; Liu, Jinxiang/0000-0003-2583-8881
FU National Key R&D Program of China [2019YFB1804304]; 111 plan
   [BP0719010]; STCSM [18DZ2270700]; State Key Laboratory of UHD Video and
   Audio Production and Presentation
FX This work was supported in part by the National Key R&D Program of China
   Under Grant 2019YFB1804304, in part by 111 plan Under Grant BP0719010,
   in part by STCSM Under Grant 18DZ2270700, and in part by the State Key
   Laboratory of UHD Video and Audio Production and Presentation.
CR AlBahar B, 2019, IEEE I CONF COMP VIS, P9015, DOI 10.1109/ICCV.2019.00911
   Balakrishnan G, 2018, PROC CVPR IEEE, P8340, DOI 10.1109/CVPR.2018.00870
   Barratt S, 2018, Arxiv, DOI [arXiv:1801.01973, DOI 10.48550/ARXIV.1801.01973]
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Dong H., 2018, NeurIPS, P474
   Dong HY, 2019, IEEE I CONF COMP VIS, P1161, DOI 10.1109/ICCV.2019.00125
   Ge LH, 2019, PROC CVPR IEEE, P10825, DOI 10.1109/CVPR.2019.01109
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Grigorev A, 2019, PROC CVPR IEEE, P12127, DOI 10.1109/CVPR.2019.01241
   Guan SY, 2019, AAAI CONF ARTIF INTE, P8352
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Hao Tang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P717, DOI 10.1007/978-3-030-58595-2_43
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang WT, 2020, PROC CVPR IEEE, P5193, DOI 10.1109/CVPR42600.2020.00524
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411
   Kim Y, 2020, PROC CVPR IEEE, P3479, DOI 10.1109/CVPR42600.2020.00354
   Kipf Thomas N., 2017, 5 INT C LEARN REPRES
   Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463
   Kowalski Marek, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P299, DOI 10.1007/978-3-030-58621-8_18
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kulon Dominik, 2019, P BRIT MACH VIS C BM
   Li K, 2020, IEEE T IMAGE PROCESS, V29, P9584, DOI 10.1109/TIP.2020.3029455
   Li YN, 2019, PROC CVPR IEEE, P3688, DOI 10.1109/CVPR.2019.00381
   Litany O, 2018, PROC CVPR IEEE, P1886, DOI 10.1109/CVPR.2018.00202
   Liu S, 2015, IEEE T MULTIMEDIA, V17, P1347, DOI 10.1109/TMM.2015.2443559
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Liu W, 2019, IEEE I CONF COMP VIS, P5903, DOI 10.1109/ICCV.2019.00600
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Long J., 2015, P IEEE C COMP VIS PA, P3431
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Loper MM, 2014, LECT NOTES COMPUT SC, V8695, P154, DOI 10.1007/978-3-319-10584-0_11
   Ma LQ, 2017, ADV NEUR IN, V30
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Neverova N, 2018, LECT NOTES COMPUT SC, V11207, P128, DOI 10.1007/978-3-030-01219-9_8
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Ranjan A, 2018, LECT NOTES COMPUT SC, V11207, P725, DOI 10.1007/978-3-030-01219-9_43
   Ren YR, 2020, IEEE T IMAGE PROCESS, V29, P8622, DOI 10.1109/TIP.2020.3018224
   Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song SJ, 2019, PROC CVPR IEEE, P2352, DOI 10.1109/CVPR.2019.00246
   Sun WC, 2019, PROCEEDINGS OF THE 2019 5TH INTERNATIONAL CONFERENCE ON COMPUTER AND TECHNOLOGY APPLICATIONS (ICCTA 2019), P117, DOI 10.1145/3323933.3324091
   Sun YT, 2021, Arxiv, DOI arXiv:2003.13510
   Tang H., 2020, PROC BRIT MACH VIS C
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xia FT, 2017, PROC CVPR IEEE, P6080, DOI 10.1109/CVPR.2017.644
   Xie QK, 2021, IEEE T MULTIMEDIA, V23, P597, DOI 10.1109/TMM.2020.2985525
   Yang CH, 2018, PROCEEDINGS OF 2018 3RD INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATION SYSTEMS (ICCCS), P201, DOI 10.1109/CCOMS.2018.8463302
   Yang L, 2020, P IEEE INT C MULT EX, P1, DOI DOI 10.1142/S021800142059003X
   Yang LB, 2021, IEEE T IMAGE PROCESS, V30, P2422, DOI 10.1109/TIP.2021.3052364
   Zablotskaia P., 2019, 30 BRIT MACHINE VISI, P51
   Zanfir M, 2018, PROC CVPR IEEE, P5391, DOI 10.1109/CVPR.2018.00565
   Zeng ZL, 2020, IEEE T MULTIMEDIA, V22, P3064, DOI 10.1109/TMM.2020.2969782
   Zhang P, 2020, PROC CVPR IEEE, P5142, DOI 10.1109/CVPR42600.2020.00519
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
   Zhu Z, 2019, PROC CVPR IEEE, P2342, DOI 10.1109/CVPR.2019.00245
NR 64
TC 3
Z9 3
U1 2
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4314
EP 4327
DI 10.1109/TMM.2021.3115628
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 5C3SZ
UT WOS:000864185400004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, YH
   Zhou, WG
   Xi, M
   Shen, SJ
   Li, HQ
AF Liu, Yiheng
   Zhou, Wengang
   Xi, Mao
   Shen, Sanjing
   Li, Houqiang
TI Multi-Modal Context Propagation for Person Re-Identification With
   Wireless Positioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Wireless communication; Visualization; Wireless sensor networks;
   Trajectory; Video sequences; Wireless fidelity; Cameras; Person
   re-identification; multi-modal recognition; wireless positioning;
   unsupervised cross-domain learning
ID NETWORK; ATTENTION
AB Existing person re-identification methods mainly rely on the visual appearance captured by cameras for identity matching. However, dueto the sensitivity of visual data to occlusion, blur, clothing change, etc., existing methods struggle to distinguish pedestrians in challenging scenarios. Inspired by the fact that most pedestrians carry around smart wireless devices, e.g. mobile phones that can be sensed by WiFi or cellular networks as wireless positioning signals, we propose to exploit the free yet informative wireless signals to assist person re-identification. It is well recognized that wireless signals are robust to visual noises mentioned above, which perform as a good complement to the visual data. To make full use of these multi-modal clues for person re-identification, we propose a multi-modal context propagation framework MCPF that contains a recurrent context propagation module RCPM and an unsupervised multi-modal cross-domain method UMM-ReID. RCPM enables context information to be continuously propagated and fused between visual data and wireless data. UMM-ReID utilizes wireless signals to constrain the estimation of pseudo labels. We contribute a new wireless positioning person re-identification dataset WP-ReID to evaluate our approach. Extensive experiments demonstrate the effectiveness of the proposed method. Benefiting from the collaboration of RCPM and UMM-ReID, the proposed framework MCPF achieves a significant performance improvement over existing methods.
C1 [Liu, Yiheng; Zhou, Wengang; Xi, Mao; Shen, Sanjing; Li, Houqiang] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhou, WG; Li, HQ (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
EM lyh156@mail.ustc.edu.cn; zhwg@ustc.edu.cn; ximao@mail.ustc.edu.cn;
   ssjxjx@mail.ustc.edu.cn; lihq@ustc.edu.cn
RI Li, Houqiang Li/B-6259-2013
FU National Natural Science Foundation of China [61822208, 61836011,
   62021001]; Youth Innovation Promotion Association CAS [2018497]; GPU
   cluster built by MCC Laboratory of Information Science and Technology
   Institution, USTC;  [WK3490000005]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61822208, 61836011, and 62021001, in
   part by the Youth Innovation Promotion Association CAS under Grant
   2018497, in part by the Central Universities under Grant WK3490000005,
   and in part by the GPU cluster built by MCC Laboratory of Information
   Science and Technology Institution, USTC.
CR Alahi A, 2015, IEEE I CONF COMP VIS, P3289, DOI 10.1109/ICCV.2015.376
   [Anonymous], 2014, ARXIV14124729
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Chung D, 2017, IEEE I CONF COMP VIS, P1992, DOI 10.1109/ICCV.2017.218
   Del Peral-Rosado J. A, 2016, P 8 ESA WORKSH SAT N, P1
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ge Yixiao, 2020, ARXIV200101526
   Goswami Abhishek., 2011, Em: CoNEXT, P1, DOI DOI 10.1145/2079296.2079299
   Gu XQ, 2019, IEEE I CONF COMP VIS, P9646, DOI 10.1109/ICCV.2019.00974
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans Alexander, 2017, ARXIV170307737
   Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954
   Huang Y., 2019, P IEEE INT JOINT C N, P1
   Huang Y, 2020, IEEE T CIRC SYST VID, V30, P3459, DOI 10.1109/TCSVT.2019.2948093
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Korany B, 2019, MOBICOM'19: PROCEEDINGS OF THE 25TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, DOI 10.1145/3300061.3345437
   Kotaru M, 2015, SIGCOMM'15: PROCEEDINGS OF THE 2015 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P269, DOI 10.1145/2785956.2787487
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li JN, 2019, AAAI CONF ARTIF INTE, P8618
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li XZ, 2020, AAAI CONF ARTIF INTE, V34, P11434
   Liu Jiawei, 2016, P 24 ACM INT C MULT, P192
   Liu Y., 2020, PROC ACM INT C MULTI, P1111
   Liu YH, 2019, AAAI CONF ARTIF INTE, P8786
   Liu Y, 2017, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2017.499
   Lu CX, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P1175, DOI 10.1145/3308558.3313398
   McLaughlin N, 2016, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2016.148
   Papaioannou S, 2015, SENSYS'15: PROCEEDINGS OF THE 13TH ACM CONFERENCE ON EMBEDDED NETWORKED SENSOR SYSTEMS, P239, DOI 10.1145/2809695.2809712
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Ro Y, 2019, AAAI CONF ARTIF INTE, P8859
   Song GL, 2018, AAAI CONF ARTIF INTE, P7347
   Sun YF, 2019, PROC CVPR IEEE, P393, DOI 10.1109/CVPR.2019.00048
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang DD, 2019, INT CONF COMPUT NETW, P185, DOI [10.1109/iccnc.2019.8685597, 10.1109/ICCNC.2019.8685597]
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wigren T., 2012, P IEEE VEH TECHN C, P1
   Wu Y, 2018, PROC CVPR IEEE, P5177, DOI 10.1109/CVPR.2018.00543
   Xie QK, 2021, IEEE T MULTIMEDIA, V23, P597, DOI 10.1109/TMM.2020.2985525
   Yang JW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1074, DOI 10.1145/3240508.3240645
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zhang ZZ, 2019, PROC CVPR IEEE, P667, DOI 10.1109/CVPR.2019.00076
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
NR 52
TC 6
Z9 6
U1 1
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3060
EP 3073
DI 10.1109/TMM.2021.3092579
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000030
DA 2024-07-18
ER

PT J
AU Yan, C
   Pang, GS
   Bai, X
   Liu, CH
   Ning, X
   Gu, L
   Zhou, J
AF Yan, Cheng
   Pang, Guansong
   Bai, Xiao
   Liu, Changhong
   Ning, Xin
   Gu, Lin
   Zhou, Jun
TI Beyond Triplet Loss: Person Re-Identification With Fine-Grained
   Difference-Aware Pairwise Loss
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Benchmark testing; Training; Feature extraction; Task analysis; Gallium
   nitride; Semantics; Pose estimation; Fine-grained difference; pairwise
   loss; person re-identification; representation learning; triplet loss
AB Person Re-IDentification (ReID) aims at re-identifying persons from different viewpoints across multiple cameras. Capturing the fine-grained appearance differences is often the key to accurate person ReID, because many identities can be differentiated only when looking into these fine-grained differences. However, most state-of-the-art person ReID approaches, typically driven by a triplet loss, fail to effectively learn the fine-grained features as they are focused more on differentiating large appearance differences. To address this issue, we introduce a novel pairwise loss function that enables ReID models to learn the fine-grained features by adaptively enforcing an exponential penalization on the images of small differences and a bounded penalization on the images of large differences. The proposed loss is generic and can be used as a plugin to replace the triplet loss to significantly enhance different types of state-of-the-art approaches. Experimental results on four benchmark datasets show that the proposed loss substantially outperforms a number of popular loss functions by large margins; and it also enables significantly improved data efficiency.
C1 [Yan, Cheng; Bai, Xiao] Beihang Univ, Sch Comp Sci & Engn, Jiangxi Res Inst, State Key Lab Software Dev Environm, Beijing 100191, Peoples R China.
   [Pang, Guansong] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia.
   [Liu, Changhong] Jiangxi Normal Univ, Sch Comp & Informat Engn, Nanchang 5793, Jiangxi, Peoples R China.
   [Ning, Xin] Chinese Acad Sci, Inst Semicond, Beijing, Peoples R China.
   [Gu, Lin] Univ Tokyo, Natl Inst Informat, Bunkyo Ku, Tokyo 1138654, Japan.
   [Zhou, Jun] Griffith Univ, Sch Informat & Commun Technol, Nathan, Qld 4111, Australia.
C3 Beihang University; University of Adelaide; Jiangxi Normal University;
   Chinese Academy of Sciences; Institute of Semiconductors, CAS; Research
   Organization of Information & Systems (ROIS); National Institute of
   Informatics (NII) - Japan; University of Tokyo; Griffith University
RP Yan, C (corresponding author), Beihang Univ, Sch Comp Sci & Engn, Jiangxi Res Inst, State Key Lab Software Dev Environm, Beijing 100191, Peoples R China.
EM beihangyc@buaa.edu.cn; pangguansong@gmail.com; baixiao@buaa.edu.cn;
   liuch@jxnu.edu.cn; ninxin@semi.ac.cn; lin.gu@riken.jp;
   jun.zhou@griffith.edu.au
RI Zhou, Jun/W-2233-2019; Pang, Guansong/ABD-7725-2021; Yan,
   cheng/C-5162-2011; Ning, Xin/M-9479-2018
OI Zhou, Jun/0000-0001-5822-8233; Ning, Xin/0000-0001-7897-1673; Pang,
   Guansong/0000-0002-9877-2716; Gu, Lin/0000-0002-7419-6240
FU National Natural Science Foundation of China [62067004]; Beijing Natural
   Science Foundation [4202039]; Jiangxi Research Institute of Beihang
   University
FX The work of Cheng Yan, Xiao Bai was supported in part by the National
   Natural Science Foundation of China under Project 61772057, in part by
   Beijing Natural Science Foundation under Project 4202039, and in part by
   the Jiangxi Research Institute of Beihang University. Changhong Liu was
   supported by the National Natural Science Foundation of China under
   Project 62067004. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. D. Zeng.
CR [Anonymous], 2020, PROC CVPR IEEE, DOI DOI 10.1109/CVPR42600.2020.00643
   Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225
   Chen D, 2018, LECT NOTES COMPUT SC, V11211, P764, DOI 10.1007/978-3-030-01234-2_45
   Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Cover Thomas M, 1999, Elements of information theory
   Dai ZZ, 2019, IEEE I CONF COMP VIS, P3690, DOI 10.1109/ICCV.2019.00379
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   He B, 2019, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2019.00412
   Hermans Alexander, 2017, ARXIV170307737
   Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954
   Huang HJ, 2018, PROC CVPR IEEE, P5098, DOI 10.1109/CVPR.2018.00535
   Huang Y, 2019, IEEE I CONF COMP VIS, P9526, DOI 10.1109/ICCV.2019.00962
   Huang Y, 2019, IEEE T IMAGE PROCESS, V28, P1391, DOI 10.1109/TIP.2018.2874715
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Kanaci A., 2018, P GERM C PATT REC GC, P377, DOI 10.48550/arXiv.1809.09409
   Khorramshahi P, 2019, IEEE I CONF COMP VIS, P6131, DOI 10.1109/ICCV.2019.00623
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liao SC, 2015, IEEE I CONF COMP VIS, P3685, DOI 10.1109/ICCV.2015.420
   Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Liu XM, 2019, IEEE J BIOMED HEALTH, V23, P1404, DOI [10.1109/JBHI.2018.2856276, 10.1145/3297156.3297158]
   Liu XC, 2018, IEEE T MULTIMEDIA, V20, P645, DOI 10.1109/TMM.2017.2751966
   Liu XC, 2016, LECT NOTES COMPUT SC, V9906, P869, DOI 10.1007/978-3-319-46475-6_53
   Lou YH, 2019, PROC CVPR IEEE, P3230, DOI 10.1109/CVPR.2019.00335
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2905, DOI 10.1109/TMM.2020.2965491
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Paszke A, 2019, ADV NEUR IN, V32
   Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40
   Ristani E, 2018, PROC CVPR IEEE, P6036, DOI 10.1109/CVPR.2018.00632
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shen YT, 2018, PROC CVPR IEEE, P2265, DOI 10.1109/CVPR.2018.00241
   Shen YT, 2017, IEEE I CONF COMP VIS, P1918, DOI 10.1109/ICCV.2017.210
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Su C, 2016, LECT NOTES COMPUT SC, V9906, P475, DOI 10.1007/978-3-319-46475-6_30
   Sun YF, 2019, PROC CVPR IEEE, P393, DOI 10.1109/CVPR.2019.00048
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang J, 2017, IEEE T CIRC SYST VID, V27, P513, DOI 10.1109/TCSVT.2016.2586851
   Wang Z, 2019, IEEE T MULTIMEDIA, V21, P2376, DOI 10.1109/TMM.2019.2898753
   Wang ZD, 2017, IEEE I CONF COMP VIS, P379, DOI 10.1109/ICCV.2017.49
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Xie QK, 2021, IEEE T MULTIMEDIA, V23, P597, DOI 10.1109/TMM.2020.2985525
   Yao HT, 2019, IEEE T IMAGE PROCESS, V28, P2860, DOI 10.1109/TIP.2019.2891888
   Zhu YX, 2020, NEUROCOMPUTING, V386, P97, DOI 10.1016/j.neucom.2019.12.100
   Zeng ZL, 2020, IEEE T MULTIMEDIA, V22, P3064, DOI 10.1109/TMM.2020.2969782
   Zhai Y, 2019, IEEE COMPUT SOC CONF, P1526, DOI 10.1109/CVPRW.2019.00194
   Zhang G., 2018, ACML, P208
   Zhang Xiangyu, 2017, CoRRabs/1711.08184
   Zhang ZZ, 2019, PROC CVPR IEEE, P667, DOI 10.1109/CVPR.2019.00076
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
   Zhou Y, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00679
NR 62
TC 121
Z9 124
U1 15
U2 64
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1665
EP 1677
DI 10.1109/TMM.2021.3069562
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200032
OA Green Accepted, Green Submitted, Green Published
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Zhang, H
   Wang, XW
   Yin, XC
   Du, MX
   Liu, CJ
   Chen, QJ
AF Zhang, Hui
   Wang, Xiangwei
   Yin, Xiaochuan
   Du, Mingxiao
   Liu, Chengju
   Chen, Qijun
TI Geometry-Constrained Scale Estimation for Monocular Visual Odometry
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Roads; Cameras; Estimation; Visual odometry; Robots; Visualization;
   Measurement; Monocular visual odometry; scale recovery;
   geometry-constrained
ID DEPTH PREDICTION; SLAM; LOCALIZATION; RETRIEVAL; REGION; SFM
AB We propose a robust geometry-constrained scale estimation approach for monocular visual odometry, which takes the camera height as an absolute reference. Visual odometry is an essential module for robot self-localization and autonomous navigation in unexplored environments. Scale recovery is an indispensable requirement for monocular visual odometry, since it compensates for the metric information lost by a single camera and helps to reduce the scale drift. When the camera height is considered the absolute reference, the precision of scale recovery depends on the accuracy of the road point selection and road geometric model calculation. However, most of the previous approaches solve these two problems sequentially, and their road point selection is based on the color model of the road or prior-knowledge-based fixed region. In this paper, we propose combining and iteratively solving these two problems. We adopt the geometric model, instead of the color model, of the road to select the road points. Furthermore, the selected road feature points are used to estimate the road model, which limits the road point selection. In detail, we segment our feature points with Delaunay triangulation and select road points based on the depth consistency and road model consistency. The experiments on the KITTI dataset show that our method achieves the best performance among state-of-the-art monocular visual odometry methods.
C1 [Zhang, Hui; Wang, Xiangwei; Yin, Xiaochuan; Du, Mingxiao; Liu, Chengju; Chen, Qijun] Tongji Univ, Sch Elect & Informat Engn, Shanghai 201804, Peoples R China.
   [Liu, Chengju] Tongji Artificial Intelligence Suzhou Res Inst, Suzhou 215100, Jiangsu, Peoples R China.
C3 Tongji University
RP Chen, QJ (corresponding author), Tongji Univ, Sch Elect & Informat Engn, Shanghai 201804, Peoples R China.
EM 1510456@tongji.edu.cn; 1410450@tongji.edu.cn; 052705@tongji.edu.cn;
   1410449@tongji.edu.cn; liuchengju@tongji.edu.cn; qjchen@tongji.edu.cn
RI guo, ppdop/KAL-9865-2024
FU National Natural Science Foundations of China [61733013, 62073245,
   U1713211]; Jiangsu Key Research, and Development Project [BE2020101]
FX This work was supported in part by the National Natural Science
   Foundations of China under Grants 61733013, 62073245, and U1713211 and
   in part by Jiangsu Key Research, and Development Project under Grant
   BE2020101.
CR Chadha A, 2017, IEEE T MULTIMEDIA, V19, P1596, DOI 10.1109/TMM.2017.2673415
   Chen L, 2012, IEEE T MULTIMEDIA, V14, P1057, DOI 10.1109/TMM.2012.2187435
   Chen OTC, 2007, IEEE T MULTIMEDIA, V9, P1333, DOI 10.1109/TMM.2007.906572
   Choi S., 1997, J COMPUT VIS, V24, P271, DOI [10.1023/A:1007927408552, DOI 10.1023/A:1007927408552]
   CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142
   Costante G, 2016, IEEE ROBOT AUTOM LET, V1, P18, DOI 10.1109/LRA.2015.2505717
   Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577
   Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54
   Fanani N, 2017, IEEE INT VEH SYM, P1714, DOI 10.1109/IVS.2017.7995955
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Forster C, 2014, IEEE INT CONF ROBOT, P15, DOI 10.1109/ICRA.2014.6906584
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Geiger A, 2011, LECT NOTES COMPUT SC, V6492, P25, DOI 10.1007/978-3-642-19315-6_3
   Geiger A, 2011, IEEE INT VEH SYM, P963, DOI 10.1109/IVS.2011.5940405
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Handa A, 2014, IEEE INT CONF ROBOT, P1524, DOI 10.1109/ICRA.2014.6907054
   Hoiem D, 2007, INT J COMPUT VISION, V75, P151, DOI 10.1007/s11263-006-0031-y
   Kitt B., 2011, PROC EUR C MOBILE RO
   Klein George, 2007, P1
   Lee R, 2015, IEEE INT CONF ROBOT, P5232, DOI 10.1109/ICRA.2015.7139928
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Lin J, 2017, IEEE T MULTIMEDIA, V19, P1968, DOI 10.1109/TMM.2017.2713410
   Liu H., 2012, Proceedings_of_the_20th_ACM_ International_Conference_on_Multimedia, MM'12, P9
   Luo HC, 2019, IEEE T MULTIMEDIA, V21, P470, DOI 10.1109/TMM.2018.2859034
   Luong QT, 1996, INT J COMPUT VISION, V17, P43, DOI 10.1007/BF00127818
   Mirabdollah MH, 2015, LECT NOTES COMPUT SC, V9358, P297, DOI 10.1007/978-3-319-24947-6_24
   Mouragnon E., 2006, 2006 IEEE COMP SOC C, V1, P363, DOI DOI 10.1109/CVPR.2006.236
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513
   Nistér D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17
   Nister David, 2006, CVPR
   Pereira FI, 2018, IEEE T INTELL TRANSP, V19, P3584, DOI 10.1109/TITS.2018.2853579
   Piao JC, 2019, IEEE T MULTIMEDIA, V21, P2827, DOI 10.1109/TMM.2019.2913324
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34
   Salarian M, 2018, IEEE T MULTIMEDIA, V20, P3298, DOI 10.1109/TMM.2018.2839893
   Scaramuzza D, 2011, IEEE ROBOT AUTOM MAG, V18, P80, DOI 10.1109/MRA.2011.943233
   Shewchuk J.R., 1996, WORKSHOP APPL COMPUT, V1148, P203
   Song SY, 2016, IEEE T PATTERN ANAL, V38, P730, DOI 10.1109/TPAMI.2015.2469274
   Song SY, 2014, PROC CVPR IEEE, P1566, DOI 10.1109/CVPR.2014.203
   Song SY, 2013, IEEE INT CONF ROBOT, P4698, DOI 10.1109/ICRA.2013.6631246
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Tateno K, 2017, PROC CVPR IEEE, P6565, DOI 10.1109/CVPR.2017.695
   Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298
   Wang XW, 2018, IEEE INT CONF ROBOT, P988
   Xue F, 2020, IEEE INT C INT ROBOT, P2330, DOI 10.1109/IROS45743.2020.9340802
   Yang N, 2018, LECT NOTES COMPUT SC, V11212, P835, DOI 10.1007/978-3-030-01237-3_50
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P2701, DOI 10.1109/TMM.2019.2912121
   Yin XC, 2017, IEEE I CONF COMP VIS, P5871, DOI 10.1109/ICCV.2017.625
   Zhan HY, 2020, IEEE INT CONF ROBOT, P4203, DOI [10.1109/icra40945.2020.9197374, 10.1109/ICRA40945.2020.9197374]
   Zhou DF, 2016, IEEE INT VEH SYM, P490, DOI 10.1109/IVS.2016.7535431
NR 52
TC 2
Z9 2
U1 4
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3144
EP 3156
DI 10.1109/TMM.2021.3093771
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000036
DA 2024-07-18
ER

PT J
AU Zheng, ZQ
   Yu, ZB
   Zheng, HY
   Yang, Y
   Shen, HT
AF Zheng, Ziqiang
   Yu, Zhibin
   Zheng, Haiyong
   Yang, Yang
   Shen, Heng Tao
TI One-Shot Image-to-Image Translation via Part-Global Learning With a
   Multi-Adversarial Framework
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Semantics; Gallium nitride; Task analysis; Image synthesis;
   Image reconstruction; Visualization; One-shot; generative adversarial
   networks; image-to-image translation; unpaired cross-domain translation
AB It is well known that humans can learn and recognize objects effectively from several limited image samples. However, learning from just a few images is still a tremendous challenge for existing main-stream deep neural networks. Inspired by analogical reasoning in the human mind, a feasible strategy is to "translate" the abundant images of a rich source domain to enrich the relevant yet different target domain with insufficient image data. To achieve this goal, we propose a novel, effective multi-adversarial framework (MA) based on part-global learning, which accomplishes the one-shot cross-domain image-to-image translation. In specific, we first devise a part-global adversarial training scheme to provide an efficient way for feature extraction and prevent discriminators from being overfitted. Then, a multi-adversarial mechanism is employed to enhance the image-to-image translation ability to unearth the high-level semantic representation. Moreover, a balanced adversarial loss function is presented, which aims to balance the training data and stabilize the training process. Extensive experiments demonstrate that the proposed approach can obtain impressive results on various datasets between two extremely imbalanced image domains and outperform state-of-the-art methods on one-shot image-to-image translation. Our code will be released with this paper at https://github.com/zhengziqiang/OST.
C1 [Zheng, Ziqiang; Yang, Yang; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Sichuan, Peoples R China.
   [Yu, Zhibin; Zheng, Haiyong] Ocean Univ China, Sch Elect Informat Engn, Qingdao 266100, Shandong, Peoples R China.
C3 University of Electronic Science & Technology of China; Ocean University
   of China
RP Yang, Y (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Sichuan, Peoples R China.
EM zhengziqiang1@gmail.com; yuzhibin@ouc.edu.cn; zhenghaiyong@ouc.edu.cn;
   dlyyang@gmail.com; shenhengtao@hotmail.com
RI Lang, Ming/HIK-0758-2022; Yu, Zhibin/Z-1138-2019; yang,
   yang/HGT-7999-2022; Shen, Heng Tao/ABD-5331-2021; yang,
   yang/GVT-5210-2022; Zheng, Haiyong/I-7771-2014
OI Yu, Zhibin/0000-0003-4372-1767; Zheng, Haiyong/0000-0002-8027-0734
FU National Natural Science Foundation of China [U20B2063]; Sichuan Science
   and Technology Program, China [2020YFS0057]; National Key Research and
   Development Program of China [2018AAA0102200]; Fundamental Research
   Funds for the Central Universities [ZYGX2019Z015]; Dongguan Songshan
   Lake Introduction Program of Leading Innovative and Entrepreneurial
   Talents
FX This work was supported in part by National Natural Science Foundation
   of China under Grant U20B2063, in part by the Sichuan Science and
   Technology Program, China, under Grant 2020YFS0057, in part by the
   National Key Research and Development Program of China under Grant
   2018AAA0102200, in part by the Fundamental Research Funds for the
   Central Universities under Project ZYGX2019Z015, and in part by Dongguan
   Songshan Lake Introduction Program of Leading Innovative and
   Entrepreneurial Talents. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Jianguo
   Zhang.
CR [Anonymous], 2000, PROC VIS
   [Anonymous], 2018, SEMANTIC FEATURE AUG
   [Anonymous], 2017, ICLR
   [Anonymous], 2018, ARXIV180900946
   [Anonymous], 2018, ARXIV181103850
   Benaim S, 2018, P ADV NEUR INF PROC, P2108
   Borji A, 2019, COMPUT VIS IMAGE UND, V179, P41, DOI 10.1016/j.cviu.2018.10.009
   Cai Q, 2018, PROC CVPR IEEE, P4080, DOI 10.1109/CVPR.2018.00429
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Duan Y, 2017, ADV NEUR IN, V30
   Gatys L., 2016, Journal of Vision, V16, P326, DOI DOI 10.1167/16.12.326
   Gokaslan A, 2018, LECT NOTES COMPUT SC, V11216, P662, DOI 10.1007/978-3-030-01258-8_40
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Hosseini-Asl E, 2018, INTERSPEECH, P3758, DOI 10.21437/Interspeech.2018-1535
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Inoue N, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1110, DOI 10.1145/3240508.3240592
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Ji X, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1654, DOI 10.1145/3123266.3123429
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Kang LW, 2011, IEEE T MULTIMEDIA, V13, P1019, DOI 10.1109/TMM.2011.2159197
   Kautz Jan, 2018, LECT NOTES COMPUT SC, DOI [DOI 10.1007/978-3-030-01219-9_11, 10.1007/978-3-030-01219-9_11]
   Kim TH, 2017, IEEE I CONF COMP VIS, P4058, DOI 10.1109/ICCV.2017.435
   King DB, 2015, ACS SYM SER, V1214, P1
   Kingma D. P., 2013, ARXIV13126114
   Koch G., 2015, ICML DEEP LEARNING W, V2
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Laffont PY, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601101
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li FF, 2002, P NATL ACAD SCI USA, V99, P9596, DOI 10.1073/pnas.092277599
   Liao S, 2015, IEEE T MULTIMEDIA, V17, P1058, DOI 10.1109/TMM.2015.2436057
   Liu M.-Y., 2017, NIPS
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Long Y, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P636, DOI 10.1145/3123266.3123323
   Lucic M., 2018, ADV NEUR IN, P700
   Luo YD, 2018, PATTERN RECOGN, V75, P128, DOI 10.1016/j.patcog.2017.02.034
   Miller EG, 2000, PROC CVPR IEEE, P464, DOI 10.1109/CVPR.2000.855856
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Mishra Ashutosh, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P35, DOI 10.1007/978-3-319-46604-0_3
   Peng YX, 2020, IEEE T MULTIMEDIA, V22, P2061, DOI 10.1109/TMM.2019.2951462
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Potapov Alexey, 2018, Artificial General Intelligence. 11th International Conference, AGI 2018. Proceedings: Lecture Notes in Artificial Intelligence (LNAI 10999), P196, DOI 10.1007/978-3-319-97676-1_19
   Qiu S, 2020, IEEE T MULTIMEDIA, V22, P1333, DOI 10.1109/TMM.2019.2942480
   Shaham TR, 2019, IEEE I CONF COMP VIS, P4569, DOI 10.1109/ICCV.2019.00467
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen HT, 2021, IEEE T KNOWL DATA EN, V33, P3351, DOI [10.1109/TKDE.2020.2970050, 10.1109/TNNLS.2020.2995708]
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang W, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P25, DOI 10.1145/2733373.2806232
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   Wang YB, 2020, IEEE T MULTIMEDIA, V22, P1458, DOI 10.1109/TMM.2019.2947197
   Wang Z, 2019, PATTERN RECOGN LETT, V122, P60, DOI 10.1016/j.patrec.2019.02.007
   Wu P., 2013, P 21 ACM INT C MULT, P153, DOI DOI 10.1145/2502081.2502112
   Xu X, 2020, IEEE T CYBERNETICS, V50, P2400, DOI 10.1109/TCYB.2019.2928180
   Yang Y, 2015, IEEE T CYBERNETICS, V45, P1069, DOI 10.1109/TCYB.2014.2344015
   Yao YZ, 2019, IEEE T MULTIMEDIA, V21, P184, DOI 10.1109/TMM.2018.2847248
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang W, 2011, PROC CVPR IEEE, P513, DOI 10.1109/CVPR.2011.5995324
   Zheng Z., 2019, ARXIV190110895
   Zhou WG, 2014, IEEE T MULTIMEDIA, V16, P601, DOI 10.1109/TMM.2014.2301979
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Ziqiang Zheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P155, DOI 10.1007/978-3-030-58580-8_10
NR 71
TC 7
Z9 7
U1 3
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 480
EP 491
DI 10.1109/TMM.2021.3053775
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300037
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhou, SJ
   Wang, LZ
   Zhang, SH
   Wang, Z
   Zhu, WW
AF Zhou, Shiji
   Wang, Lianzhe
   Zhang, Shanghang
   Wang, Zhi
   Zhu, Wenwu
TI Active Gradual Domain Adaptation: Dataset and Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptation models; Uncertainty; Data models; Diversity reception; Deep
   learning; Performance evaluation; Internet; Active domain adaptation;
   gradual domain adaptation; gradual domain drift; web noise data
AB Adapting deep neural networks to the changing environments is critical in practical utility, especially for online web applications, where the data distribution changes gradually due to the evolving environments. For instance, the web photos of cellphones change gradually over years due to appearance changes. This paper deals with such a problem via active gradual domain adaptation, where the learner continually and actively selects the most informative labels from the target to enhance labeling efficiency and utilizes both labeled and unlabeled samples to improve the model adaptation under gradual domain drift. We propose the active gradual self-training (AGST) algorithm with novel designs of active pseudolabeling and gradual semi-supervised domain adaptation. Specifically, AGST pseudolabels the samples with high confidence, and selects the most informative labels from the unconfident samples based on both uncertainty and diversity, and then gradually self-trains itself by confident pseudolabels and queried labels. To study the gradual domain shift problem in the web data and verify the proposed algorithm, we create a new dataset -- Evolving-Image-Search (EVIS), collected from the web search engine and covers a 12-years range. Since the appearance of the products evolves over these years, such dataset naturally contains gradual domain drift. We extensively evaluate AGST on the synthetic dataset, real-world dataset, and EVIS dataset. AGST achieves up to 62% accuracy improvement (absolute value) against unsupervised gradual self-training with only 5% additional labels, and 19% accuracy improvement against directly applying CLUE, demonstrating the effectiveness of the designs of active pseudolabel and gradual semi-supervised domain adaptation.
C1 [Zhou, Shiji; Wang, Lianzhe] Tsinghua Univ, Tsinghua Berkeley Shenzhen Inst, Shenzhen 100084, Peoples R China.
   [Zhang, Shanghang] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Wang, Zhi] Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
   [Zhu, Wenwu] Tsinghua Univ, Comp Sci Dept, Beijing 100084, Peoples R China.
C3 Tsinghua University; Tsinghua Shenzhen International Graduate School;
   University of California System; University of California Berkeley;
   Tsinghua University; Tsinghua Shenzhen International Graduate School;
   Tsinghua University
RP Zhang, SH (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.; Wang, Z (corresponding author), Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.; Zhu, WW (corresponding author), Tsinghua Univ, Comp Sci Dept, Beijing 100084, Peoples R China.
EM zsj17@mails.tsinghua.edu.cn; wanglz20@mails.tsinghua.edu.cn;
   shz@eecs.berkeley.edu; wangzhi@sz.tsinghua.edu.cn; wwzhu@tsinghua.edu.cn
RI Zhang, Lisa/AAW-9795-2021
OI Wang, Zhi/0000-0002-5462-6178
FU National Key Research and Development Program of China [2020AAA0106300];
   National Natural Science Foundation of China [62050110, 61872215];
   Shenzhen Science and Technology Innovation Program
   [RCYX20200714114523079]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020AAA0106300, in part by the
   National Natural Science Foundation of China under Grants 62050110 and
   61872215, and in part by the Shenzhen Science and Technology Innovation
   Program of under Grant RCYX20200714114523079. The guest editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Shin'ichi Satoh.
CR [Anonymous], 2015, Proceedings of the IEEE International Conference on Computer Vision Workshops (ICCV-W)
   Ash Jordan T, 2019, INT C LEARN REPR
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bitarafan A, 2016, IEEE T KNOWL DATA EN, V28, P2128, DOI 10.1109/TKDE.2016.2551241
   Bo Fu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P567, DOI 10.1007/978-3-030-58555-6_34
   Bobu A., 2018, INT C LEARN REPR WOR, P1
   COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1023/A:1022673506211
   Csurka G, 2017, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-3-319-58347-1
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Huang J, 2017, PROC CVPR IEEE, P3296, DOI 10.1109/CVPR.2017.351
   Kumar A, 2020, PR MACH LEARN RES, V119
   Meagher M, 2007, IEEE INT CONF INF VI, P601
   Ortiz-Jimenez G., 2019, ARXIV190911448
   Prabhu V, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8485, DOI 10.1109/ICCV48922.2021.00839
   Rai P., 2010, Proceedings of the NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing, P27
   Saito K, 2019, IEEE I CONF COMP VIS, P8049, DOI 10.1109/ICCV.2019.00814
   Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4
   Shu R., 2018, P 6 INT C LEARN REPR
   Su JC, 2020, IEEE WINT CONF APPL, P728, DOI [10.1109/wacv45572.2020.9093390, 10.1109/WACV45572.2020.9093390]
   Sugiyama M, 2007, J MACH LEARN RES, V8, P985
   Sun Y., 2019, ARXIV
   Tzeng E., 2014, ARXIV14123474
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang Hao, 2020, ICML, P9898
   Wang M, 2018, NEUROCOMPUTING, V312, P135, DOI 10.1016/j.neucom.2018.05.083
   Wulfmeier M, 2018, IEEE INT CONF ROBOT, P4489
   Xu JL, 2019, IEEE ACCESS, V7, P156694, DOI 10.1109/ACCESS.2019.2949697
   Ye J., 2013, INT C MACH LEARN, P253
   Zhang YF, 2020, IEEE T IMAGE PROCESS, V29, P7834, DOI 10.1109/TIP.2020.3006377
   Zhao H, 2019, PR MACH LEARN RES, V97
   Zhao SC, 2022, IEEE T NEUR NET LEAR, V33, P473, DOI 10.1109/TNNLS.2020.3028503
   Zhdanov Fedor, 2019, ARXIV190105954
NR 36
TC 9
Z9 9
U1 2
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1210
EP 1220
DI 10.1109/TMM.2022.3142524
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ZN1SD
UT WOS:000764821800017
DA 2024-07-18
ER

PT J
AU Chai, XY
   Chen, J
   Liang, C
   Xu, DS
   Lin, CW
AF Chai, Xiaoyu
   Chen, Jun
   Liang, Chao
   Xu, Dongshu
   Lin, Chia-Wen
TI Expression-Aware Face Reconstruction via a Dual-Stream Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Faces; Geometry; Face recognition; Three-dimensional displays; Image
   reconstruction; Semantics; Solid modeling; Attribute spatial map;
   dual-stream network; expression-aware; face reconstruction; facial
   texture synthesis
ID FACIAL EXPRESSION
AB Recently, 3D face reconstruction from a single image has achieved promising progress by adopting the 3D Morphable Model (3DMM). However, face images taken in-the-wild usually involve expressions with a large range of variety. This poses difficulty to use 3DMM to represent such various facial expressions owing to the limited expressive ability of its linear model, thereby resulting in distortion and ambiguity in local facial regions. To tackle this problem, we present a novel dual-stream network composed of a geometry stream and a texture stream to deal with expression variations. Specifically, in the geometry stream, we propose novel Attribute Spatial Maps (ASMs) to decompose a face into the identity and expression attributes and then separately record the essential spatial information of the two facial attributes in the 2D image space. This avoids the interaction between the two attributes, thus preserving the identity information and further improving the ability of coping with expression variations. In the texture stream, we propose to generate facial appearance with realistic texture and canonical layout by our Semantic Region Stylization Mechanism (SRSM), that transfers the style from an input face to a 3DMM albedo map in a region-adaptive manner. Moreover, we also propose a Shared Semantic Region Prediction Module (SSRPM) to explore the common correspondence of semantic regions between the above two face texture representations. Both quantitative and qualitative evaluations on public datasets demonstrate the effectiveness of our approach in face reconstruction under expression variations.
C1 [Chai, Xiaoyu; Chen, Jun; Liang, Chao; Xu, Dongshu] Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 30013, Taiwan.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Inst Commun Engn, Hsinchu 30013, Taiwan.
C3 Wuhan University; National Tsing Hua University; National Tsing Hua
   University
RP Chen, J (corresponding author), Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Sch Comp Sci, Wuhan 430072, Peoples R China.
EM stevenchai@whu.edu.cn; chenj.whu@gmail.com; cliang@whu.edu.cn;
   xudongshu@whu.edu.cn; cwlin@ee.nthu.edu.tw
RI Chen, Jun/AAD-8167-2022; Lin, Chia-Wen/M-4571-2013
OI Lin, Chia-Wen/0000-0002-9097-2318; Chai, Xiaoyu/0000-0003-2278-1955
FU National Nature Science Foundation of China [62071338, U1611461,
   U1736206, U1903214, 61876135, 61872362, 61671336, 61801335, 61862015];
   Hubei Province Technological Innovation Major Project [2017AAA123,
   2018AAA062, 2018CFA024, 2019CFB472]; Ministry of Science and Technology,
   Taiwan [MOST 109-2634-F-007-013]; Nature Science Foundation of Hubei
   Province [2018CFA024, 2019CFB472]
FX This work was supported in part by National Nature Science Foundation of
   China 62071338, U1611461, U1736206, U1903214, 61876135, 61872362,
   61671336, 61801335, and 61862015, in part by Hubei ProvinceTechnological
   InnovationMajor Project under Grants 2017AAA123, 2018AAA062, 2018CFA024,
   and 2019CFB472, in part by Nature Science Foundation of Hubei Province
   2018CFA024, 2019CFB472, and in part by the Ministry of Science and
   Technology, Taiwan, under Grant MOST 109-2634-F-007-013. The guest
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Jian Zhang.
CR Bagdanov Andrew D, 2011, P JOINT ACM WORKSH H
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Chang FJ, 2018, IEEE INT CONF AUTOMA, P122, DOI 10.1109/FG.2018.00027
   Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097
   Chen TC, 2017, AGEING SOC, V37, P1798, DOI 10.1017/S0144686X16000623
   Chu B, 2014, PROC CVPR IEEE, P1907, DOI 10.1109/CVPR.2014.245
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng JK, 2018, IEEE INT CONF AUTOMA, P399, DOI 10.1109/FG.2018.00064
   Dou P., 2017, PROC CVPR IEEE, P5908, DOI DOI 10.1109/CVPR.2017.164
   Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33
   Ferrari C, 2017, IEEE T MULTIMEDIA, V19, P2666, DOI 10.1109/TMM.2017.2707341
   Fisher K, 2016, NEUROPSYCHOLOGIA, V80, P115, DOI 10.1016/j.neuropsychologia.2015.11.011
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gecer B, 2019, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2019.00125
   Geng ZL, 2019, PROC CVPR IEEE, P9813, DOI 10.1109/CVPR.2019.01005
   Genova K, 2018, PROC CVPR IEEE, P8377, DOI 10.1109/CVPR.2018.00874
   Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352
   Han XJ, 2020, IEEE T MULTIMEDIA, V22, P1619, DOI 10.1109/TMM.2019.2945197
   Han Y, 2017, IEEE T MULTIMEDIA, V19, P80, DOI 10.1109/TMM.2016.2608000
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hu HM, 2020, IEEE T MULTIMEDIA, V22, P1485, DOI 10.1109/TMM.2019.2944260
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Huber P, 2015, IEEE IMAGE PROC, P1195, DOI 10.1109/ICIP.2015.7350989
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jackson AS, 2017, IEEE I CONF COMP VIS, P1031, DOI 10.1109/ICCV.2017.117
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Jourabloo A, 2016, PROC CVPR IEEE, P4188, DOI 10.1109/CVPR.2016.454
   King DB, 2015, ACS SYM SER, V1214, P1
   Li HB, 2017, IEEE T MULTIMEDIA, V19, P2816, DOI 10.1109/TMM.2017.2713408
   Li S, 2019, INT J COMPUT VISION, V127, P884, DOI 10.1007/s11263-018-1131-1
   Liang C., 2020, PROC IEEECONF MULTIM, P1
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lou JW, 2020, IEEE T MULTIMEDIA, V22, P730, DOI 10.1109/TMM.2019.2933338
   Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767
   Mechrez R, 2018, LECT NOTES COMPUT SC, V11218, P800, DOI 10.1007/978-3-030-01264-9_47
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Nirkin Y, 2018, IEEE INT CONF AUTOMA, P98, DOI 10.1109/FG.2018.00024
   Pan Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P5142, DOI 10.1109/CVPR42600.2020.00519
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Ruan WJ, 2020, NEUROCOMPUTING, V384, P200, DOI 10.1016/j.neucom.2019.11.102
   Ruan WJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P284, DOI 10.1145/3343031.3350984
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Shao H.-C, 2020, P AS C COMP VIS DEC, P309
   Shen FL, 2018, PROC CVPR IEEE, P8061, DOI 10.1109/CVPR.2018.00841
   Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241
   Shu ZX, 2017, PROC CVPR IEEE, P5444, DOI 10.1109/CVPR.2017.578
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tewari A, 2018, PROC CVPR IEEE, P2549, DOI 10.1109/CVPR.2018.00270
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Trân AT, 2017, PROC CVPR IEEE, P1493, DOI 10.1109/CVPR.2017.163
   Tran L, 2021, IEEE T PATTERN ANAL, V43, P157, DOI 10.1109/TPAMI.2019.2927975
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   Wang Y, 2020, PROC CVPR IEEE, P5093, DOI 10.1109/CVPR42600.2020.00514
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Yamaguchi S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201364
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Zhou YX, 2019, PROC CVPR IEEE, P1097, DOI 10.1109/CVPR.2019.00119
   Zhu XY, 2019, IEEE T PATTERN ANAL, V41, P78, DOI 10.1109/TPAMI.2017.2778152
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
NR 65
TC 6
Z9 6
U1 1
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2998
EP 3012
DI 10.1109/TMM.2021.3068567
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000004
DA 2024-07-18
ER

PT J
AU Chen, ZM
   Cui, Q
   Wei, XS
   Jin, X
   Guo, YW
AF Chen, Zhao-Min
   Cui, Quan
   Wei, Xiu-Shen
   Jin, Xin
   Guo, Yanwen
TI Disentangling, Embedding and Ranking Label Cues for Multi-Label Image
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlation; Image recognition; Streaming media; Recurrent neural
   networks; Task analysis; Computational modeling; Measurement;
   Multi-label image recognition; deep learning; label correlation; CNNs;
   disentangling; embedding; ranking
ID VIEW
AB Multi-label image recognition is a fundamental but challenging computer vision and multimedia task. Great progress has been achieved by exploiting label correlations among these multiple labels associated with a single image, which is the most crucial issue for multi-label image recognition. In this paper, to explicitly model label correlations, we propose a unified deep learning framework to Disentangle, Embed and Rank (DER) the corresponding label cues. Specifically, we first obtain class-aware disentangled maps (CADMs) by reforming deep activations in accordance with the class-specific recognition weights. Then, after transforming CADMs into the corresponding label vectors, we propose an embedding operation from a metric learning perspective to pull the relevant label vectors together and push irrelevant label vectors away. Furthermore, a ranking operation is employed, which aims to accurately and robustly measure the similarity/dissimilarity of these label vectors. Our model can be trained in an end-to-end manner with only image-level supervision, during which the proposed embedding and ranking operations can contribute to the CADMs learning through back-propagation. In addition, the obtained CADMs are aggregated and further used as an essential feature stream for the final multi-label classification. We conduct extensive experiments on three commonly used multi-label benchmark datasets. Quantitative results show that our model can significantly and consistently outperform previous competitive methods. Moreover, qualitative analysis of our DER proposal also reveals the effectiveness of our proposed model.
C1 [Chen, Zhao-Min; Guo, Yanwen] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing, Peoples R China.
   [Cui, Quan] Waseda Univ, Grad Sch Informat Prod & Syst, Tokyo 1698050, Japan.
   [Wei, Xiu-Shen] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Key Lab Intelligent Percept & Syst High Dimens In, PCA Lab,Minist Educ, Nanjing 210094, Peoples R China.
   [Wei, Xiu-Shen] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Socia, Nanjing 210094, Peoples R China.
   [Jin, Xin] Megvii Technol, Megvii Res Nanjing, Nanjing 210000, Peoples R China.
   [Guo, Yanwen] Nanjing Lanzhong Intelligent Technol Co Ltd, Nanjing 210018, Peoples R China.
C3 Nanjing University; Waseda University; Nanjing University of Science &
   Technology; Nanjing University of Science & Technology
RP Guo, YW (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing, Peoples R China.; Wei, XS (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Key Lab Intelligent Percept & Syst High Dimens In, PCA Lab,Minist Educ, Nanjing 210094, Peoples R China.; Wei, XS (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Socia, Nanjing 210094, Peoples R China.; Guo, YW (corresponding author), Nanjing Lanzhong Intelligent Technol Co Ltd, Nanjing 210018, Peoples R China.
EM chenzhaomin123@gmail.com; cui-quan@toki.waseda.jp; weixs.gm@gmail.com;
   jinxin@megvii.com; ywguo@nju.edu.cn
RI jin, xin/GQZ-5811-2022
OI Wei, Xiu-Shen/0000-0002-8200-1845
FU Fundamental Research Funds for the Central Universities [020914380080];
   National Natural Science Foundation of China [61772257, 61672279];
   National Key R&D Program of China [2017YFA0700800]; "111" Program
   [B13022]
FX This work was supported in part by the Fundamental Research Funds for
   the Central Universities 020914380080, in part by the National Natural
   Science Foundation of China under Grants 61772257 and 61672279, in part
   by the National Key R&D Program of China under Grant 2017YFA0700800 and
   in part by "111" Program B13022.
CR [Anonymous], 2014, INT C MACH LEARN
   [Anonymous], 2017, CVPR
   [Anonymous], 2016, NIPS 16 P 30 INT C N, DOI DOI 10.5555/3157096.3157304
   [Anonymous], 2006, PROC IEEE COMPUT SOC
   [Anonymous], 2014, ICLR
   [Anonymous], 2018, ECCV
   Bai Y, 2018, IEEE T MULTIMEDIA, V20, P2385, DOI 10.1109/TMM.2018.2796240
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Cao B, 2018, AAAI CONF ARTIF INTE, P6682
   Chen SF, 2018, AAAI CONF ARTIF INTE, P6714
   Chen SZ, 2016, IEEE T IMAGE PROCESS, V25, P2353, DOI 10.1109/TIP.2016.2545929
   Chen TS, 2019, IEEE I CONF COMP VIS, P522, DOI 10.1109/ICCV.2019.00061
   Chen TS, 2018, AAAI CONF ARTIF INTE, P6730
   Chen WH, 2017, AAAI CONF ARTIF INTE, P3988
   Chen YT, 2018, AAAI CONF ARTIF INTE, P2852
   Chen Z.-M., 2019, PROC CVPR IEEE, P5177, DOI DOI 10.1109/CVPR.2019.00532
   Chen ZM, 2019, IEEE INT CON MULTI, P622, DOI 10.1109/ICME.2019.00113
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Dong L, 2016, IEEE T MULTIMEDIA, V18, P714, DOI 10.1109/TMM.2016.2530399
   Dong Z, 2017, AAAI CONF ARTIF INTE, P4009
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Ge WF, 2018, PROC CVPR IEEE, P1277, DOI 10.1109/CVPR.2018.00139
   González-Díaz I, 2017, IEEE T MULTIMEDIA, V19, P544, DOI 10.1109/TMM.2016.2616298
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He K, 2018, PROC CVPR IEEE, P4023, DOI 10.1109/CVPR.2018.00423
   Hu HX, 2016, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2016.323
   Ivasic-Kos M, 2016, PATTERN RECOGN, V52, P287, DOI 10.1016/j.patcog.2015.10.017
   Jiao JN, 2018, AAAI CONF ARTIF INTE, P6967
   Lee CW, 2018, PROC CVPR IEEE, P1576, DOI 10.1109/CVPR.2018.00170
   Leibe B., 2017, ARXIV170307737CS
   Li Q, 2016, PROC CVPR IEEE, P2977, DOI 10.1109/CVPR.2016.325
   Li YN, 2016, LECT NOTES COMPUT SC, V9910, P684, DOI 10.1007/978-3-319-46466-4_41
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu C, 2018, AAAI CONF ARTIF INTE, P3635
   Liu LQ, 2017, IEEE T PATTERN ANAL, V39, P2305, DOI 10.1109/TPAMI.2016.2637921
   Liu WW, 2017, J MACH LEARN RES, V18
   Liu WW, 2019, IEEE T PATTERN ANAL, V41, P408, DOI 10.1109/TPAMI.2018.2794976
   Liu WW, 2017, J MACH LEARN RES, V18
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Liu Z, 2018, IEEE T MULTIMEDIA, V20, P1321, DOI 10.1109/TMM.2017.2767781
   Opitz M, 2020, IEEE T PATTERN ANAL, V42, P276, DOI 10.1109/TPAMI.2018.2848925
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shao J, 2016, PROC CVPR IEEE, P5620, DOI 10.1109/CVPR.2016.606
   Shi WW, 2018, IEEE T NEUR NET LEAR, V29, P2896, DOI 10.1109/TNNLS.2017.2705222
   Simonyan K., 2014, CORR
   Song LY, 2018, IEEE T IMAGE PROCESS, V27, P6025, DOI 10.1109/TIP.2018.2864920
   Wang HB, 2016, IEEE T MULTIMEDIA, V18, P1579, DOI 10.1109/TMM.2016.2569412
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang XS, 2019, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR.2019.00535
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wang ZX, 2017, IEEE I CONF COMP VIS, P464, DOI 10.1109/ICCV.2017.58
   Wei X-S, 2019, ARXIV190107249
   Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu XZ, 2017, PR MACH LEARN RES, V70
   Yang H, 2016, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2016.37
   Zhang JJ, 2018, IEEE T MULTIMEDIA, V20, P2801, DOI 10.1109/TMM.2018.2812605
   Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu Y, 2018, IEEE T KNOWL DATA EN, V30, P1081, DOI 10.1109/TKDE.2017.2785795
NR 65
TC 15
Z9 15
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1827
EP 1840
DI 10.1109/TMM.2020.3003779
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100002
DA 2024-07-18
ER

PT J
AU Li, MH
   Peng, LB
   Wu, TF
   Peng, ZM
AF Li, Meihui
   Peng, Lingbing
   Wu, Tianfu
   Peng, Zhenming
TI A Bottom-Up and Top-Down Integration Framework for Online Object
   Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target tracking; Correlation; Encoding; Object tracking; Benchmark
   testing; Visualization; Online object tracking; bottom-up and top-down;
   graph regularized sparse coding; alternating direction method of
   multipliers
ID VISUAL TRACKING; MODEL; MAP
AB Robust online object tracking entails integrating short-term memory based trackers and long-term memory based trackers in an elegant framework to handle structural and appearance variations of unknown objects in an online manner. The integration and synergy between short-term and long-term memory based trackers have yet studied well in the literature, especially in pre-training free settings. To address this issue, this paper presents a bottom-up and top-down integration framework. The bottom-up component realizes a data-driven approach for particle generation. It exploits a short-term memory based tracker to generate bounding box proposals in a new frame. In the top-down component, this paper presents a graph regularized sparse coding scheme as the long-term memory based tracker. The over-complete bases for sparse coding are composed of part-based representations learned from earlier tracking results and new observations to form a space with rich temporal context information. A particle graph is computed whose nodes are the bottom-up discriminative particles and edges are formed on-the-fly in terms of appearance and spatial-temporal similarities between particles. The particle graph induces a regularization term in optimizing the sparse coding coefficients for bottom-up particles. In experiments, the proposed method is tested on the widely used OTB-100 benchmark and the VOT2016 benchmark with better performance obtained than baselines including deep learning based trackers. In addition, the outputs from the top-down sparse coding are potentially useful for downstream tasks such as action recognition, multiple-object tracking, and object re-identification.
C1 [Li, Meihui] NC State Univ, Raleigh, NC 27695 USA.
   [Li, Meihui; Peng, Lingbing; Peng, Zhenming] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
   [Li, Meihui; Peng, Zhenming] Univ Elect Sci & Technol China, Lab Imaging Detect & Intelligent Percept, Chengdu 610054, Peoples R China.
   [Wu, Tianfu] North Caroline State Univ, Dept Elect & Comp Engn, Raleigh, NC 27606 USA.
   [Wu, Tianfu] North Caroline State Univ, Visual Narrat Initiat, Raleigh, NC 27606 USA.
C3 North Carolina State University; University of Electronic Science &
   Technology of China; University of Electronic Science & Technology of
   China; North Carolina State University; North Carolina State University
RP Peng, ZM (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.; Wu, TF (corresponding author), North Caroline State Univ, Dept Elect & Comp Engn, Raleigh, NC 27606 USA.; Wu, TF (corresponding author), North Caroline State Univ, Visual Narrat Initiat, Raleigh, NC 27606 USA.
EM aimee0208@outlook.com; lbpeng163@163.com; tianfu_wu@ncsu.edu;
   zmpeng@uestc.edu.cn
RI cheng, cheng/JBR-8359-2023; zhang, luyu/JJC-4227-2023; LI,
   Xiang-Yang/JZE-0275-2024
OI Wu, Tianfu/0000-0001-8911-5506; Peng, Zhenming/0000-0002-4148-3331
FU National Natural Science Foundation of China [61775030, 61571096];
   Sichuan Science and Technology Program [2019YJ0167]; Open Research Fund
   of Key Laboratory of Optical Engineering, Chinese Academy of Sciences
   [2017LBC003]; China Scholarship Council; ARO [W911NF1810295]; NSF
   [IIS-1909644]; U.S. Department of Defense (DOD) [W911NF1810295] Funding
   Source: U.S. Department of Defense (DOD)
FX The work of M. Li and Z. Peng was supported in part by the National
   Natural Science Foundation of China under Grants 61775030 and 61571096,
   in part by Sichuan Science and Technology Program 2019YJ0167, and in
   part by the Open Research Fund of Key Laboratory of Optical Engineering,
   Chinese Academy of Sciences underGrant 2017LBC003. The work of M. Li was
   also supported by the financial support from China Scholarship Council.
   The work of T. Wu was supported in part by ARO under Grant W911NF1810295
   and in part by NSF IIS-1909644.
CR Afonso MV, 2014, IEEE T MULTIMEDIA, V16, P1, DOI 10.1109/TMM.2013.2281023
   Agrawal P, 2014, LECT NOTES COMPUT SC, V8695, P329, DOI 10.1007/978-3-319-10584-0_22
   [Anonymous], 2017, CVPR
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2016, CVPR
   [Anonymous], 2014, LECT NOTES COMPUT SC, DOI DOI 10.1007/978-3-319-10599-4_13
   [Anonymous], 2017, CVPR
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bibi A, 2016, LECT NOTES COMPUT SC, V9910, P419, DOI 10.1007/978-3-319-46466-4_25
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Cehovin L, 2016, IEEE T IMAGE PROCESS, V25, P1261, DOI 10.1109/TIP.2016.2520370
   Choi J, 2017, PROC CVPR IEEE, P4828, DOI 10.1109/CVPR.2017.513
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan Martin, 2014, P BRIT MACH VIS C 20
   Du DW, 2017, IEEE T CYBERNETICS, V47, P4182, DOI 10.1109/TCYB.2016.2626275
   Felsberg M, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P121, DOI 10.1109/ICCVW.2013.22
   Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478
   Ghadimi E, 2015, IEEE T AUTOMAT CONTR, V60, P644, DOI 10.1109/TAC.2014.2354892
   Girdhar R, 2018, PROC CVPR IEEE, P350, DOI 10.1109/CVPR.2018.00044
   Grabner C, 2006, INT POWER ELECT ELEC, P1
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hong S, 2015, PR MACH LEARN RES, V37, P597
   Hong ZB, 2013, IEEE I CONF COMP VIS, P649, DOI 10.1109/ICCV.2013.86
   Jae-Yeong Lee, 2011, 2011 IEEE International Conference on Robotics and Biomimetics (ROBIO), P2860, DOI 10.1109/ROBIO.2011.6181739
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kristan M., 2014, P 12 AS C COMP VIS S, P391
   Kristan M, 2015, P IEEE INT C COMPUTE, P1
   Kristan M, 2015, LECT NOTES COMPUT SC, V8926, P191, DOI 10.1007/978-3-319-16181-5_14
   Kurzhals K, 2017, IEEE T VIS COMPUT GR, V23, P301, DOI 10.1109/TVCG.2016.2598695
   Li MH, 2019, NEUROCOMPUTING, V323, P319, DOI 10.1016/j.neucom.2018.10.007
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liu BY, 2011, PROC CVPR IEEE, P1313, DOI 10.1109/CVPR.2011.5995730
   Liu S, 2016, PROC CVPR IEEE, P4312, DOI 10.1109/CVPR.2016.467
   Lukezic A, 2018, IEEE T CYBERNETICS, V48, P1849, DOI 10.1109/TCYB.2017.2716101
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Mei X, 2011, IEEE T PATTERN ANAL, V33, P2259, DOI 10.1109/TPAMI.2011.66
   Milan A, 2017, AAAI CONF ARTIF INTE, P4225
   Montero AS, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P587, DOI 10.1109/ICCVW.2015.80
   Mueller M, 2017, PROC CVPR IEEE, P1387, DOI 10.1109/CVPR.2017.152
   Peng ZM, 2007, OPT ENG, V46, DOI 10.1117/1.2748398
   Phan R, 2014, IEEE T MULTIMEDIA, V16, P122, DOI 10.1109/TMM.2013.2283451
   Raaijmakers J. G., 2002, STEVENS HDB EXP PSYC, V2, DOI [10.1002/0471214426.pas0202, DOI 10.1002/0471214426.PAS0202]
   Roffo G., 2016, BRIT MACH VIS C, DOI [10.5244/C.30.120, DOI 10.5244/C.30.120]
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Schwarz M, 2015, IEEE INT CONF ROBOT, P1329, DOI 10.1109/ICRA.2015.7139363
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Somandepalli K, 2018, IEEE T MULTIMEDIA, V20, P539, DOI 10.1109/TMM.2017.2745712
   Song Y., 2017, IEEE INT C COMP VIS, P2555
   Tang M, 2015, IEEE I CONF COMP VIS, P3038, DOI 10.1109/ICCV.2015.348
   Valmadre J., 2017, P IEEE C COMP VIS PA, P2805
   Viola P, 2005, INT J COMPUT VISION, V63, P153, DOI 10.1007/s11263-005-6644-8
   Vojir T, 2014, PATTERN RECOGN LETT, V49, P250, DOI 10.1016/j.patrec.2014.03.025
   Wang D, 2013, IEEE T IMAGE PROCESS, V22, P314, DOI 10.1109/TIP.2012.2202677
   Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357
   Wang N, 2019, IEEE T CIRC SYST VID, V29, P730, DOI 10.1109/TCSVT.2018.2816570
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu H, 2010, IEEE T PATTERN ANAL, V32, P1443, DOI 10.1109/TPAMI.2009.135
   Yang X, 2013, INT SYMP COMP INTELL, P447, DOI 10.1109/CINTI.2013.6705238
   Zhang KH, 2016, IEEE T IMAGE PROCESS, V25, P1779, DOI 10.1109/TIP.2016.2531283
   Zhang KH, 2014, LECT NOTES COMPUT SC, V8693, P127, DOI 10.1007/978-3-319-10602-1_9
   Zhang Q, 2018, IEEE ACCESS, V6, P33076, DOI 10.1109/ACCESS.2018.2845390
   Zhang TZ, 2012, LECT NOTES COMPUT SC, V7577, P470, DOI 10.1007/978-3-642-33783-3_34
   Zhang TZ, 2013, INT J COMPUT VISION, V101, P367, DOI 10.1007/s11263-012-0582-z
   Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908
   Zhou Y, 2017, IEEE T MULTIMEDIA, V19, P1798, DOI 10.1109/TMM.2017.2689918
   Zhuang BH, 2014, IEEE T IMAGE PROCESS, V23, P1872, DOI 10.1109/TIP.2014.2308414
NR 77
TC 8
Z9 8
U1 1
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 105
EP 119
DI 10.1109/TMM.2020.2978623
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600009
OA hybrid
DA 2024-07-18
ER

PT J
AU Li, YX
   Wang, WC
   Liu, ML
   Jiang, ZJ
   He, QH
AF Li, Yanxiong
   Wang, Wucheng
   Liu, Mingle
   Jiang, Zhongjie
   He, Qianhua
TI Speaker Clustering by Co-Optimizing Deep Representation Learning and
   Cluster Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Estimation; Feature extraction; Clustering methods; Clustering
   algorithms; Decoding; Neural networks; Speaker recognition; Speaker
   clustering; deep representation; deep convolutional autoencoder network;
   audio document analysis
ID MEAN-SHIFT; DIARIZATION; RECOGNITION; TRACKING; DISTANCE; SPACE; AUDIO
AB Speaker clustering is a task to merge speech segments uttered by the same speaker into a single cluster, which is an effective tool for alleviating the management of massive amount of audio documents. In this paper, we present a work for co-optimizing the two main steps of speaker clustering, namely, feature learning and cluster estimation. In our method, the deep representation feature is learned by a deep convolutional autoencoder network (DCAN), while the cluster estimation is realized by a softmax layer that is combined with the DCAN. We devise an integrated loss function to simultaneously minimize the reconstruction loss (for deep representation learning) and the clustering loss (for cluster estimation). Many state-of-the-art audio features and clustering methods are evaluated on experimental datasets selected from two publicly available speech corpora (the AISHELL-2 and the VoxCeleb1). The results show that the proposed method exceeds other speaker clustering methods in regard to the normalized mutual information (NMI) and the clustering accuracy (CA). Additionally, the proposed deep representation feature outperforms other features that were widely used in previous works, in terms of both NMI and CA.
C1 [Li, Yanxiong; Wang, Wucheng; Liu, Mingle; Jiang, Zhongjie; He, Qianhua] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510640, Peoples R China.
C3 South China University of Technology
RP Li, YX (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510640, Peoples R China.
EM eeyxli@scut.edu.cn; 347710323@qq.com; 1324903969@qq.com;
   870782975@qq.com; eeqhhe@scut.edu.cn
RI Jiang, Zhongjie/I-7347-2012
FU National Natural Science Foundation of China [61771200]
FX Manuscript received December 17, 2019; revised May 8, 2020 and July 2,
   2020; accepted September 1, 2020. Date of publication September 21,
   2020; date of current version September 24, 2021. Thisworkwas supported
   by theNational Natural Science Foundation of China under Grant 61771200.
   (Corresponding author: Yanxiong Li.)
CR AMARI S, 1993, NEUROCOMPUTING, V5, P185, DOI 10.1016/0925-2312(93)90006-O
   Miro XA, 2012, IEEE T AUDIO SPEECH, V20, P356, DOI 10.1109/TASL.2011.2125954
   [Anonymous], 2017, P IEEE MLSP 2017
   [Anonymous], 2014, ISCA ODYSSEY
   [Anonymous], 1998, P DARPA BROADC NEWS
   Bozonnet S, 2010, INT CONF ACOUST SPEE, P4958, DOI 10.1109/ICASSP.2010.5495088
   Burileanu D, 2000, LECT NOTES ARTIF INT, V1902, P177
   Chao YH, 2009, COMPUT SPEECH LANG, V23, P376, DOI 10.1016/j.csl.2009.01.002
   Chen WY, 2011, IEEE T PATTERN ANAL, V33, P568, DOI 10.1109/TPAMI.2010.88
   Cumani S, 2019, PATTERN RECOGN, V95, P235, DOI 10.1016/j.patcog.2019.06.018
   Cumani S, 2018, IEEE-ACM T AUDIO SPE, V26, P736, DOI 10.1109/TASLP.2018.2791806
   Das S, 2008, IEEE T SYST MAN CY A, V38, P218, DOI 10.1109/TSMCA.2007.909595
   Dimitriadis D, 2017, INTERSPEECH, P2739, DOI 10.21437/Interspeech.2017-166
   Du J., 2018, Agricultural transition in China: Domestic and international perspectives on technology and institutional change, P1
   Fiscus JG, 2008, LECT NOTES COMPUT SC, V4625, P373
   Font R, 2019, INT CONF ACOUST SPEE, P6016, DOI 10.1109/ICASSP.2019.8683525
   FUJINAMI Y, 2019, P 5 INT S FUT ACT SA, P1
   Garcia-Romero D, 2017, INT CONF ACOUST SPEE, P4930, DOI 10.1109/ICASSP.2017.7953094
   Iso K, 2010, INT CONF ACOUST SPEE, P4986, DOI 10.1109/ICASSP.2010.5495078
   Kenny P, 2005, IEEE T SPEECH AUDI P, V13, P345, DOI 10.1109/TSA.2004.840940
   Kilic V, 2016, IEEE T MULTIMEDIA, V18, P2417, DOI 10.1109/TMM.2016.2599150
   Kinnunen T, 2007, LECT NOTES COMPUT SC, V4642, P58
   Kuhn R, 2000, IEEE T SPEECH AUDI P, V8, P695, DOI 10.1109/89.876308
   LEE HS, 2017, P ICASSP, P5375
   Lee KA, 2013, INTERSPEECH, P3618
   Li YX, 2009, SIGNAL PROCESS, V89, P1625, DOI 10.1016/j.sigpro.2009.03.001
   Li YX, 2020, IEEE T MULTIMEDIA, V22, P1385, DOI 10.1109/TMM.2019.2947199
   Li YX, 2018, IEEE ACCESS, V6, P58043, DOI 10.1109/ACCESS.2018.2872931
   Li YX, 2018, IEEE T INF FOREN SEC, V13, P965, DOI 10.1109/TIFS.2017.2774505
   Li YX, 2017, COMPUT SPEECH LANG, V42, P81, DOI 10.1016/j.csl.2016.09.002
   Li YX, 2014, IET SIGNAL PROCESS, V8, P844, DOI 10.1049/iet-spr.2013.0340
   Lin QJ, 2019, INTERSPEECH, P366, DOI 10.21437/Interspeech.2019-1388
   Liu JH, 2015, 2015 1ST IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM), P124, DOI 10.1109/BigMM.2015.46
   Liu QJ, 2018, IEEE T MULTIMEDIA, V20, P1767, DOI 10.1109/TMM.2017.2777671
   Meignier S, 2006, COMPUT SPEECH LANG, V20, P303, DOI 10.1016/j.csl.2005.08.002
   Minotto VP, 2015, IEEE T MULTIMEDIA, V17, P1694, DOI 10.1109/TMM.2015.2463722
   Nagrani A, 2020, COMPUT SPEECH LANG, V60, DOI 10.1016/j.csl.2019.101027
   Qian XY, 2019, IEEE T MULTIMEDIA, V21, P2576, DOI 10.1109/TMM.2019.2902489
   RAHMAN MH, 2019, P ICASSP, P5811
   Rouvier M., 2012, ODYSSEY, P146
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sell G, 2018, INTERSPEECH, P2808, DOI 10.21437/Interspeech.2018-1893
   Senoussaoui M, 2014, IEEE-ACM T AUDIO SPE, V22, P217, DOI 10.1109/TASLP.2013.2285474
   Settle S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4819, DOI 10.1109/ICASSP.2018.8461893
   Shum SH, 2013, IEEE T AUDIO SPEECH, V21, P2015, DOI 10.1109/TASL.2013.2264673
   Snyder D, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5329
   Sun HW, 2010, INT CONF ACOUST SPEE, P4982, DOI 10.1109/ICASSP.2010.5495077
   Tang H, 2012, IEEE T PATTERN ANAL, V34, P959, DOI 10.1109/TPAMI.2011.174
   Todisco M., 2016, P SPEAK LANG REC WOR, V25, P249, DOI DOI 10.21437/ODYSSEY.2016-41
   Tsai WH, 2007, IEEE T AUDIO SPEECH, V15, P1461, DOI 10.1109/TASL.2007.894525
   Tsai WH, 2009, COMPUT SPEECH LANG, V23, P165, DOI 10.1016/j.csl.2008.05.001
   Tsai WH, 2005, INT CONF ACOUST SPEE, P725
   Vallet F, 2013, IEEE T MULTIMEDIA, V15, P509, DOI 10.1109/TMM.2012.2233724
   Vijayasenan D, 2009, IEEE T AUDIO SPEECH, V17, P1382, DOI 10.1109/TASL.2009.2015698
   Wang Q, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5239, DOI 10.1109/ICASSP.2018.8462628
   Wooters C, 2008, LECT NOTES COMPUT SC, V4625, P509
   Yilmaz E, 2019, INTERSPEECH, P411, DOI 10.21437/Interspeech.2019-1399
   Yu CZ, 2017, IEEE-ACM T AUDIO SPE, V25, P2188, DOI 10.1109/TASLP.2017.2747097
   Yu D, 2015, SIGNALS COMMUN TECHN, P1, DOI 10.1007/978-1-4471-5779-3
   Zhang AN, 2019, INT CONF ACOUST SPEE, P6301, DOI 10.1109/ICASSP.2019.8683892
NR 60
TC 8
Z9 8
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3377
EP 3387
DI 10.1109/TMM.2020.3024667
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000033
DA 2024-07-18
ER

PT J
AU Li, YM
   Fu, CH
   Huang, ZY
   Zhang, YQ
   Pan, J
AF Li, Yiming
   Fu, Changhong
   Huang, Ziyuan
   Zhang, Yinqiang
   Pan, Jia
TI Intermittent Contextual Learning for Keyfilter-Aware UAV Object Tracking
   Using Deep Convolutional Feature
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Unmanned aerial vehicle; keyfilter-aware visual tracking; intermittent
   contextual learning; deep feature
AB Visual tracking, one of the most favorable multimedia applications, has been widely used in unmanned aerial vehicle (UAV) for civil infrastructure monitoring, aerial cinematography, autonomous navigation, etc. Most existing trackers utilize deep convolutional feature to enhance tracking robustness in scenarios of various appearance variation. However, they commonly neglect speed which is crucial for UAV with restricted calculation resources. In this work, a novel correlation filter-based keyfilter-aware tracker with a new intermittent context learning strategy is proposed to efficiently and effectively alleviate the problems of background clutter, deficient description, occlusion, illumination change, etc. Specifically, context information is utilized to empower the filter higher discriminating ability through response repression of the omnidirectional context patches. Furthermore, keyfilter is produced from the periodically selected keyframe. The latest produced keyfilter is used to restrain the current filter's corrupted changes. Most importantly, context learning of correlation filter is implemented intermittently to fully increase the tracking efficiency. This intermittent learning strategy can ensure every filter maintain context awareness owing to the restriction of keyfilter, periodically enhancing the context awareness. Substantial experiments on three challenging UAV benchmarks totally with 213 image sequences have shown that our tracker surpasses the state-of-the-art results, and exhibits a remarkable generality in short-term and long-term UAV tracking tasks as well as a variety of challenging attributes.
C1 [Li, Yiming; Fu, Changhong] Tongji Univ, Sch Mech Engn, Shanghai 200092, Peoples R China.
   [Huang, Ziyuan] Natl Univ Singapore, Adv Robot Ctr, Singapore 119077, Singapore.
   [Zhang, Yinqiang] Tech Univ Munich, Dept Mech Engn, D-80333 Munich, Germany.
   [Pan, Jia] Univ Hong Kong, Dept Comp Sci, Hong Kong 999077, Peoples R China.
C3 Tongji University; National University of Singapore; Technical
   University of Munich; University of Hong Kong
RP Fu, CH (corresponding author), Tongji Univ, Sch Mech Engn, Shanghai 200092, Peoples R China.
EM 1551896@tongji.edu.cn; changhongfu@tongji.edu.cn;
   ziyuan.huang@u.nus.edu; yinqiang.zhang@tum.de; jpan@cs.hku.hk
RI Fu, Changhong/AAN-2859-2020; li, yiming/JTS-9207-2023
OI Fu, Changhong/0000-0002-9897-6022; Huang, Ziyuan/0000-0002-4544-0427;
   Zhang, Yinqiang/0000-0002-5423-1774; Li, Yiming/0000-0002-0157-6218
FU National Natural Science Foundation of China [61806148]; Fundamental
   Research Funds for the Central Universities [22120180009]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61806148 and in part by the Fundamental
   Research Funds for the Central Universities under Grant 22120180009.
   This article was presented in part at the IEEE International Conference
   on Robotics and Automation, 2020.
CR Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bonatti, 2019, P IEEE RSJ INT C INT, P1
   Boyd S., 2011, FOUND TRENDS MACH LE, V3, P1, DOI DOI 10.1561/2200000016
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen K, 2019, IEEE T MULTIMEDIA, V21, P86, DOI 10.1109/TMM.2018.2846405
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2016, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2016.159
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Fu C., 2019, P IEEE RSJ INT C INT, P1
   Fu CH, 2014, IEEE INT CONF ROBOT, P5441, DOI 10.1109/ICRA.2014.6907659
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Guo Q., 2017, IEEE INTCONF COMPUT
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henriques J, 2012, LNCS, P702, DOI DOI 10.1007/978-3-642-33765-9_50
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hong ZB, 2015, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2015.7298675
   Hsiao Ming, 2017, 2017 IEEE INT C ROBO, P5110
   Huang ZY, 2019, IEEE I CONF COMP VIS, P2891, DOI 10.1109/ICCV.2019.00298
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li F, 2017, IEEE INT CONF COMP V, P2001, DOI 10.1109/ICCVW.2017.234
   Li S, 2017, P AAAI C ARTIFICIAL, P1, DOI [10.1609/aaai.v31i1.11205, DOI 10.1609/AAAI.V31I1.11205]
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Li YM, 2020, IEEE INT CONF ROBOT, P193, DOI 10.1109/icra40945.2020.9196943
   Lukezic A, 2018, INT J COMPUT VISION, V126, P671, DOI 10.1007/s11263-017-1061-3
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Mueller M, 2017, PROC CVPR IEEE, P1387, DOI 10.1109/CVPR.2017.152
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Vedaldi A., 2015, Proceedings of the 23rd ACM international conference on Multimedia, P689, DOI DOI 10.1145/2733373.2807412
   Wang C, 2018, PROCEEDINGS OF THE 12TH ACM/IEEE INTERNATIONAL SYMPOSIUM ON EMPIRICAL SOFTWARE ENGINEERING AND MEASUREMENT (ESEM 2018), DOI 10.1145/3239235.3268916
   Wang D., 2019, CVPR
   Wang N, 2019, PROC CVPR IEEE, P1308, DOI 10.1109/CVPR.2019.00140
   Wang N, 2018, PROC CVPR IEEE, P4844, DOI 10.1109/CVPR.2018.00509
   Wang QR, 2019, IEEE T MULTIMEDIA, V21, P930, DOI 10.1109/TMM.2018.2869277
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Yang M, 2009, IEEE T PATTERN ANAL, V31, P1195, DOI 10.1109/TPAMI.2008.146
   Yao R, 2017, IEEE T MULTIMEDIA, V19, P772, DOI 10.1109/TMM.2016.2631727
   Yuan C, 2015, INT CONF UNMAN AIRCR, P639, DOI 10.1109/ICUAS.2015.7152345
   Yun S, 2017, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2017.148
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang KH, 2014, LECT NOTES COMPUT SC, V8693, P127, DOI 10.1007/978-3-319-10602-1_9
   Zhang TZ, 2017, PROC CVPR IEEE, P4819, DOI [10.1109/CVPR.2017.512, 10.1109/ICCV.2017.469]
NR 57
TC 31
Z9 34
U1 4
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 810
EP 822
DI 10.1109/TMM.2020.2990064
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QA6NL
UT WOS:000613560200020
DA 2024-07-18
ER

PT J
AU Liao, JW
   Qi, C
   Cao, JZ
AF Liao, Jiawen
   Qi, Chun
   Cao, Jianzhong
TI Temporal Constraint Background-Aware Correlation Filter With Saliency
   Map
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target tracking; Correlation; Robustness; Adaptation models; Strain;
   Computational modeling; Deformable models; Correlation filter; drift;
   saliency; Sherman-Morrison; temporal constraint
ID TRACKING
AB Correlation filter (CF) based trackers have recently drawn great attention in visual tracking community due to their impressive performance, and computational efficiency on benchmark datasets. However, the performance of most existing trackers using correlation filter is hampered by two aspects: i) Included background information in the selected rectangular target patch is considered as part of the target, and they are treated as important as the real target in training new filter model, it causes the filter easily drift when target shape changes dramatically. ii) Existing filters use a moving average operation with an empirical weight to update the filter model in each frame, such per frame adaptation constantly introduces new information of the target patch, but never consider the consistence of the historical information, and the newly obtained one, further increases the risk of drifting. This paper presents a new framework including saliency map, and a novel CF regression model. We reformulate the original optimization problem, and provide a closed form solution for multidimensional features which is solved efficiently using alternating direction method of multipliers (ADMM), and accelerated using Sherman-Morrison lemma, our algorithm as a new framework can be easily integrated into CF base trackers to boost their tracking performance. We perform comprehensive experiments on five benchmarks: OTB-2015, VOT2016, VOT2018, UAV123, and TempleColor-128. Results show that the proposed method performs favorably against lots of state-of-the-art methods with a speed close to real-time. Our method with deep features performs much better on all 5 datasets.
C1 [Liao, Jiawen; Cao, Jianzhong] Xian Inst Opt & Precis Mech, Xian 710119, Peoples R China.
   [Qi, Chun] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
C3 Chinese Academy of Sciences; Xi'an Institute of Optics & Precision
   Mechanics, CAS; Xi'an Jiaotong University
RP Liao, JW (corresponding author), Xian Inst Opt & Precis Mech, Xian 710119, Peoples R China.
EM liaojiawen@126.com; qichun@mail.xjtu.edu.cn; cjz@opt.ac.cn
OI jiawen, liao/0000-0003-1956-4431
FU National Natural Science Foundation of China [61572395, 61675161,
   51905529]
FX Manuscript received August 12, 2019; revised May 20, 2020; accepted
   September 3, 2020. Date of publication September 14, 2020; date of
   current version September 24, 2021. This work was supported by the
   National Natural Science Foundation ofChina underGrants 61572395,
   61675161, and 51905529. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Elisa Ricci.
   (Corresponding author: Jiawen Liao.)
CR Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bhat G, 2018, LECT NOTES COMPUT SC, V11206, P493, DOI 10.1007/978-3-030-01216-8_30
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Boyd S., 2011, FOUND TRENDS MACH LE, V3, P1, DOI DOI 10.1561/2200000016
   Choi J, 2018, PROC CVPR IEEE, P479, DOI 10.1109/CVPR.2018.00057
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Danelljan Martin, 2014, P BRIT MACH VIS C 20
   Fan H, 2019, IEEE T IMAGE PROCESS, V28, P4130, DOI 10.1109/TIP.2019.2904789
   Feng W, 2019, IEEE T IMAGE PROCESS, V28, P3232, DOI 10.1109/TIP.2019.2895411
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hong S, 2015, PR MACH LEARN RES, V37, P597
   Huang ZY, 2019, IEEE I CONF COMP VIS, P2891, DOI 10.1109/ICCV.2019.00298
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuai YL, 2018, J VIS COMMUN IMAGE R, V51, P104, DOI 10.1016/j.jvcir.2018.01.008
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li X, 2019, PROC CVPR IEEE, P1369, DOI 10.1109/CVPR.2019.00146
   Liang NX, 2018, IEEE T MULTIMEDIA, V20, P2289, DOI 10.1109/TMM.2018.2803518
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu C, 2018, IEEE T MULTIMEDIA, V20, P889, DOI 10.1109/TMM.2017.2760633
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Matej K., 2017, P IEEE INT C COMP VI, P1949
   Mueller M, 2017, PROC CVPR IEEE, P1387, DOI 10.1109/CVPR.2017.152
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Park E, 2018, LECT NOTES COMPUT SC, V11207, P587, DOI 10.1007/978-3-030-01219-9_35
   Possegger H, 2015, PROC CVPR IEEE, P2113, DOI 10.1109/CVPR.2015.7298823
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Real E, 2017, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR.2017.789
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   SHERMAN J, 1950, ANN MATH STAT, V21, P124, DOI 10.1214/aoms/1177729893
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Tang M, 2018, PROC CVPR IEEE, P4874, DOI 10.1109/CVPR.2018.00512
   Touil DE, 2019, SIGNAL IMAGE VIDEO P, V13, P359, DOI 10.1007/s11760-018-1364-z
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang N, 2019, PROC CVPR IEEE, P1308, DOI 10.1109/CVPR.2019.00140
   Wang N, 2019, IEEE T CIRC SYST VID, V29, P730, DOI 10.1109/TCSVT.2018.2816570
   Wang N, 2018, PROC CVPR IEEE, P4844, DOI 10.1109/CVPR.2018.00509
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Zabih R., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P151, DOI 10.1007/BFb0028345
   Zhang K., 2013, FAST TRACKING SPATIO
   Zhang LC, 2019, IEEE I CONF COMP VIS, P4009, DOI 10.1109/ICCV.2019.00411
   Zhang TZ, 2017, PROC CVPR IEEE, P4819, DOI [10.1109/CVPR.2017.512, 10.1109/ICCV.2017.469]
   Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7
NR 57
TC 4
Z9 4
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3346
EP 3361
DI 10.1109/TMM.2020.3023794
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000031
DA 2024-07-18
ER

PT J
AU Liu, JQ
   Zhang, WZ
   Huang, SQ
   Du, HP
   Zheng, QH
AF Liu, Junquan
   Zhang, Weizhan
   Huang, Shouqin
   Du, Haipeng
   Zheng, Qinghua
TI QoE-driven HAS Live Video Channel Placement in the Media Cloud
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE QoE-driven; HTTP adaptive streaming; live video streaming; HAS channel
   placement; media cloud
ID MIGRATION; QUALITY
AB HTTP adaptive streaming (HAS) technology has been increasingly employed by video service providers (VSPs) due to its prominent benefits such as reducing interruptions of video playback and achieving higher bandwidth utilization and outstanding quality of experience (QoE). And many VSPs have deployed HAS applications in the media cloud to provide large-scale video streaming services. At present, research into the media cloud typically focuses on the management and optimization of cloud resources, such as the placement and migration of virtual machines in media cloud data centers. However, considering the HAS live video streaming service, existing related works have not adequately discussed the specific impact of the consumption of computing and bandwidth resources of media cloud servers on the user experience (QoE), particularly under the resource constraints in the media cloud. In this paper, we first investigate and formulate the computing and bandwidth resource consumption characteristics of HAS live video streaming with different frame rates and resolutions, and we further establish a resources-aware QoE model to quantify the user experience of live video channels (i.e., programs). Then, based on the model, we p rest nt a QoE-driven HAS live video channel placement approach (including a placement algorithm HCP and a rescheduling algorithm HCR) to optimize the channel allocation in media cloud servers, aiming to maximize the average user QoE. We abstract the maximization problem into an MMKP problem, and employ a heuristic solution to address this problem. The experimental results demonstrate the effectiveness of our proposed approach compared with benchmark solutions.
C1 [Liu, Junquan; Zhang, Weizhan; Huang, Shouqin; Du, Haipeng; Zheng, Qinghua] Xi An Jiao Tong Univ, MOEKLINNS Lab, Dept Comp Sci & Technol, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University
RP Zhang, WZ (corresponding author), Xi An Jiao Tong Univ, MOEKLINNS Lab, Dept Comp Sci & Technol, Xian 710049, Peoples R China.
EM liujunquan1994@stu.xjtu.edu.cn; zhangwzh@xjtu.edu.cn;
   hsq0323@stu.xjtu.edu.cn; duhaipeng@xjtu.edu.cn; qhzheng@mail.xjtu.edu.cn
RI yang, zhou/KBB-6972-2024
OI Du, Haipeng/0000-0002-1120-7096
FU "The Fundamental Theory and Applications of Big Data with Knowledge
   Engineering" under the National Key Research and Development Program of
   China [2016YFB1000903]; National Science Foundation of China [61772414,
   61721002, 61532015, 61532004, 61702400]; MOE Innovation Research Team
   [IRT17R86]; Project of the China Knowledge Centre for Engineering
   Science and Technology; consulting research project of Chinese academy
   of engineering
FX This work was supported in part by "The Fundamental Theory and
   Applications of Big Data with Knowledge Engineering" under the National
   Key Research and Development Program of China under Grant
   2016YFB1000903, in part by the National Science Foundation of China
   under Grants 61772414, 61721002, 61532015, 61532004, and 61702400, in
   part by the MOE Innovation Research Team under Grant IRT17R86, in part
   by the Project of the China Knowledge Centre for Engineering Science and
   Technology, and in part by the consulting research project of Chinese
   academy of engineering "The Online and Offline Mixed Educational Service
   System for `The Belt and Road' Training in MOOC China". The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Mea Wang.
CR Alasaad A, 2015, IEEE T PARALL DISTR, V26, P1021, DOI 10.1109/TPDS.2014.2316827
   Alsarhan A, 2018, IEEE T PARALL DISTR, V29, P31, DOI 10.1109/TPDS.2017.2748578
   [Anonymous], 2019, FREESTYLE SKIING
   [Anonymous], 2018, BIG BUCK BUNNY
   [Anonymous], 2015, PER TITL ENC OPT
   [Anonymous], 2015, P 6 ACM MULTIMEDIA S, DOI DOI 10.1145/2713168.2713195
   Aparicio-Pardo Ramon., 2015, Proceedings of the 6th ACM Multimedia Systems Conference, MMSys'15, P49
   Balachandran A, 2013, ACM SIGCOMM COMP COM, V43, P339, DOI 10.1145/2534169.2486025
   Belmudez B., 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P464, DOI 10.1109/ISM.2011.82
   Beloglazov A, 2012, FUTURE GENER COMP SY, V28, P755, DOI 10.1016/j.future.2011.04.017
   Bentaleb A, 2018, IEEE T BROADCAST, V64, P575, DOI 10.1109/TBC.2018.2816789
   Bentaleb A, 2017, IEEE T MULTIMEDIA, V19, P2136, DOI 10.1109/TMM.2017.2733344
   Bentaleb A, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1296, DOI 10.1145/2964284.2964332
   Calheiros RN, 2011, SOFTWARE PRACT EXPER, V41, P23, DOI 10.1002/spe.995
   Cofano G, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P24, DOI 10.1145/2910017.2910597
   Cofano G, 2017, ACM T MULTIMEDIA COM, V13, P1
   Du J, 2016, IEEE T MULTIMEDIA, V18, P820, DOI 10.1109/TMM.2016.2537781
   Gao GY, 2015, IEEE T MULTIMEDIA, V17, P1286, DOI 10.1109/TMM.2015.2438713
   García-Pineda M, 2017, 2017 FOURTH INTERNATIONAL CONFERENCE ON SOFTWARE DEFINED SYSTEMS (SDS), P100, DOI 10.1109/SDS.2017.7939148
   Hao Jin, 2012, GLOBECOM 2012 - 2012 IEEE Global Communications Conference, P2505, DOI 10.1109/GLOCOM.2012.6503493
   He J, 2014, IEEE T CIRC SYST VID, V24, P669, DOI 10.1109/TCSVT.2013.2283430
   Nguyen H, 2018, PROCEEDINGS OF THE 2018 CONFERENCE ON RESEARCH IN ADAPTIVE AND CONVERGENT SYSTEMS (RACS 2018), P100, DOI 10.1145/3264746.3264794
   Huang TY, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P187, DOI 10.1145/2619239.2626296
   Huysegems R, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P541, DOI 10.1145/2733373.2806264
   Jin YC, 2016, IEEE T MULTIMEDIA, V18, P807, DOI 10.1109/TMM.2016.2537199
   Jin YC, 2015, IEEE T CIRC SYST VID, V25, P1914, DOI 10.1109/TCSVT.2015.2402892
   Laboratory for Image& Video Engineering(LIVE), 2018, IMAGE VIDEO QUALITY
   Li CL, 2018, IEEE T CIRC SYST VID, V28, P1648, DOI 10.1109/TCSVT.2017.2681024
   Li CL, 2018, IEEE T MULTIMEDIA, V20, P965, DOI 10.1109/TMM.2017.2757761
   Lin YH, 2017, IEEE T PARALL DISTR, V28, P431, DOI 10.1109/TPDS.2016.2563428
   Liu JY, 2018, IEEE T NETW SERV MAN, V15, P503, DOI 10.1109/TNSM.2017.2740432
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Mu M, 2016, IEEE J SEL AREA COMM, V34, P2168, DOI 10.1109/JSAC.2016.2577318
   Ou YF, 2011, IEEE T CIRC SYST VID, V21, P286, DOI 10.1109/TCSVT.2010.2087833
   Ou YF, 2008, IEEE IMAGE PROC, P689, DOI 10.1109/ICIP.2008.4711848
   Oyman O, 2012, IEEE COMMUN MAG, V50, P20, DOI 10.1109/MCOM.2012.6178830
   Peng MG, 2016, IEEE T MULTIMEDIA, V18, P879, DOI 10.1109/TMM.2016.2535722
   Petrangeli S, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2818361
   Politis I., 2017, IEEE INT C COMMUNICA, DOI [10.1109/ICC.2017.7996601, DOI 10.1109/ICC.2017.7996601]
   Seufert, 2018, P IEEE IFIP NETW OP, P1
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Su Z, 2016, IEEE T MULTIMEDIA, V18, P1650, DOI 10.1109/TMM.2016.2566584
   Sun Y, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P272, DOI 10.1145/2934872.2934898
   Video Quality Experts Group (VQEG), 2018, VQEG SUBJ RAT DAT
   Wagenaar A, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P221, DOI 10.1145/3083187.3083227
   Wei L, 2017, IEEE T CIRC SYST VID, V27, P49, DOI 10.1109/TCSVT.2016.2589621
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Yan CG, 2019, IEEE T MULTIMEDIA, V21, P2675, DOI 10.1109/TMM.2019.2903448
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yan CG, 2018, IEEE T MULTIMEDIA, V20, P3389, DOI 10.1109/TMM.2018.2838320
   Yan ZS, 2017, IEEE T CIRC SYST VID, V27, P209, DOI 10.1109/TCSVT.2016.2539827
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zhang WZ, 2017, IEEE T PARALL DISTR, V28, P2808, DOI 10.1109/TPDS.2017.2697381
   Zhang YX, 2019, IEEE INFOCOM SER, P1252, DOI [10.1109/INFOCOM.2019.8737361, 10.1109/infocom.2019.8737361]
   Zhao H, 2018, IEEE T PARALL DISTR, V29, P1385, DOI 10.1109/TPDS.2018.2794369
   Zheng YH, 2017, IEEE T CIRC SYST VID, V27, P1777, DOI 10.1109/TCSVT.2016.2556584
   Zhou L, 2017, IEEE T CIRC SYST VID, V27, P84, DOI 10.1109/TCSVT.2016.2539698
   Zhou L, 2016, IEEE T MULTIMEDIA, V18, P905, DOI 10.1109/TMM.2016.2537782
   Zhu WW, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940269
   Zink M, 2009, COMPUT NETW, V53, P501, DOI 10.1016/j.comnet.2008.09.022
NR 61
TC 4
Z9 6
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1530
EP 1541
DI 10.1109/TMM.2020.2999176
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300006
DA 2024-07-18
ER

PT J
AU Lu, P
   Zhang, H
   Peng, XJ
   Jin, XF
AF Lu, Peng
   Zhang, Hao
   Peng, Xujun
   Jin, Xiaofu
TI Learning the Relation Between Interested Objects and Aesthetic Region
   for Image Cropping
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Task analysis; Object detection; Image color
   analysis; Visualization; Probabilistic logic; Training; Deep learning;
   aesthetics; convolutional network; image composition
ID VISUAL-ATTENTION; PHOTO; MODEL
AB As one of the fundamental techniques for image editing, image cropping discards irrelevant contents and remains the pleasing portions of the image to enhance the overall composition and achieve better visual/aesthetic perception. In this paper, we primarily focus on improving the efficiency of automatic image cropping, and on further exploring its potential in public datasets with high accuracy. From this perspective, we propose a deep learning based framework to learn the objects composition from photos with high aesthetic qualities, where an interested object region is detected through a convolutional neural network (CNN) based on the saliency map. The features of the detected interested objects are then fed into a regression network to obtain the final cropping result. Unlike the conventional methods that multiple candidates are proposed and evaluated iteratively, only a single interested object region is produced in our model, which is mapped to the final output directly. Thus, low computational resources are required for the proposed approach. The experimental results on the public datasets show that as a weakly supervised method, the proposed network outperforms the other weakly supervised methods on FLMS and FCD datasets and achieves comparable results to the existing methods on CUHK dataset. Furthermore, the proposed method is more efficient than these methods, where the processing speed is as fast as 20 ms per image.
C1 [Lu, Peng; Zhang, Hao; Jin, Xiaofu] Beijing Univ Posts & Telecommun, Sch Comp Sci, Beijing 100876, Peoples R China.
   [Peng, Xujun] Univ Southern Calif, Informat Sci Inst, Los Angeles, CA 90007 USA.
C3 Beijing University of Posts & Telecommunications; University of Southern
   California
RP Lu, P (corresponding author), Beijing Univ Posts & Telecommun, Sch Comp Sci, Beijing 100876, Peoples R China.
EM lupeng@bupt.edu.cn; lichds@bupt.edu.cn; xpeng@isi.edu;
   suffvier@bupt.edu.cn
RI peng, xujun/H-7659-2012
OI lu, peng/0000-0002-0162-6449
CR Bradski GR, 1998, FOURTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV'98, PROCEEDINGS, P214, DOI 10.1109/ACV.1998.732882
   Chen JS, 2016, PROC CVPR IEEE, P507, DOI 10.1109/CVPR.2016.61
   Chen YL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P37, DOI 10.1145/3123266.3123274
   Chen YL, 2017, IEEE WINT CONF APPL, P226, DOI 10.1109/WACV.2017.32
   Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Doulamis N, 1998, IEEE T CIRC SYST VID, V8, P928, DOI 10.1109/76.736718
   Fang C, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1105, DOI 10.1145/2647868.2654979
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Grill T., 1983, PHOTOGRAPHIC COMPOSI
   Guo GJ, 2018, IEEE T MULTIMEDIA, V20, P2073, DOI 10.1109/TMM.2018.2794262
   Guo YM, 2016, NEUROCOMPUTING, V187, P27, DOI 10.1016/j.neucom.2015.09.116
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38
   Islam MB, 2017, MULTIMED TOOLS APPL, V76, P9517, DOI 10.1007/s11042-016-3561-5
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2000, VISION RES, V40, P1489, DOI 10.1016/S0042-6989(99)00163-7
   Itti L, 1999, P SOC PHOTO-OPT INS, V3644, P473, DOI 10.1117/12.348467
   Ji YZ, 2018, NEUROCOMPUTING, V322, P130, DOI 10.1016/j.neucom.2018.09.061
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kao YY, 2017, INT CONF ACOUST SPEE, P1982, DOI 10.1109/ICASSP.2017.7952503
   Kao YY, 2017, IEEE T IMAGE PROCESS, V26, P1482, DOI 10.1109/TIP.2017.2651399
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Li DB, 2018, PROC CVPR IEEE, P8193, DOI 10.1109/CVPR.2018.00855
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Zhenguang, 2018, IEEE Trans Image Process, DOI 10.1109/TIP.2018.2828326
   Lu P, 2019, SIGNAL PROCESS-IMAGE, V77, P1, DOI 10.1016/j.image.2019.05.010
   Lu P, 2016, SIGNAL PROCESS, V120, P731, DOI 10.1016/j.sigpro.2014.12.008
   Ma S, 2017, PROC CVPR IEEE, P722, DOI 10.1109/CVPR.2017.84
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Moon P, 1944, J OPT SOC AM, V34, P46, DOI 10.1364/JOSA.34.000046
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Nishiyama M., 2009, P 17 ACM INT C MULTI, P669
   Park J, 2012, IEEE IMAGE PROC, P2741, DOI 10.1109/ICIP.2012.6467466
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Samii A, 2015, COMPUT GRAPH FORUM, V34, P141, DOI 10.1111/cgf.12465
   Suh Bongwon, 2003, P ACM S US INT SOFTW, P95, DOI DOI 10.1145/964696.964707
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   Vig E, 2014, PROC CVPR IEEE, P2798, DOI 10.1109/CVPR.2014.358
   Voulodimos A, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/7068349
   Wang P, 2015, IEEE WINT CONF APPL, P448, DOI 10.1109/WACV.2015.66
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang WG, 2017, IEEE I CONF COMP VIS, P2205, DOI 10.1109/ICCV.2017.240
   Wang X, 2016, IEEE IMAGE PROC, P1042
   Wei ZJ, 2018, PROC CVPR IEEE, P5437, DOI 10.1109/CVPR.2018.00570
   Yan JZ, 2013, PROC CVPR IEEE, P971, DOI 10.1109/CVPR.2013.130
   Zhang LM, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2303650
   Zhang LM, 2014, IEEE T IMAGE PROCESS, V23, P2235, DOI 10.1109/TIP.2014.2311658
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P94, DOI 10.1109/TMM.2013.2286817
   Zhang LM, 2013, IEEE T IMAGE PROCESS, V22, P802, DOI 10.1109/TIP.2012.2223226
   Zhou ZH, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P301, DOI 10.1145/2733373.2806248
NR 60
TC 8
Z9 9
U1 1
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3618
EP 3630
DI 10.1109/TMM.2020.3029882
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100015
DA 2024-07-18
ER

PT J
AU Pezzulli, S
   Martini, MG
   Barman, N
AF Pezzulli, Sergio
   Martini, Maria G.
   Barman, Nabajeet
TI Estimation of Quality Scores From Subjective Tests-Beyond Subjects' MOS
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quality of experience; Sociology; Statistics; Videos; Video sequences;
   Reliability; Estimation; Quality of experience; quality estimation;
   subjective experiments; MOS; maximum likelihood
ID FRACTIONAL POLYNOMIALS
AB Subjective tests for the assessment of the quality of experience (QoE) are typically run with a pool of subjects providing their opinion scores using a 5-point scale. The subjects' mean opinion score (MOS) is generally assumed as the best estimation of the average score in the target population. Indeed, for a large enough sample, we may assume that the mean of the variations across the subjects approaches zero, but this is not the case for the limited number of subjects typically considered in subjective tests. In this paper, we propose an approach based on generalized linear models (GLMs) for estimation of the population average QoE. The motivating dataset is composed of the individual scores assigned by 25 subjects to a set of gaming videos evaluated under different resolutions and compression ratios. The approach recognizes the multinomial nature of the data and allows for correlation between scores of the same subject. The resulting estimated average QoE is shown to follow more credible patterns than the MOS, particularly for higher bitrates, for which the model estimates present more coherent behavior. Similar convincing results are found on a second dataset, showing the validity of the approach.
C1 [Pezzulli, Sergio] Univ Roma La Sapienza, I-00185 Rome, Italy.
   [Martini, Maria G.; Barman, Nabajeet] Kingston Univ London, Kingston Upon Thames KT1 2EE, Surrey, England.
C3 Sapienza University Rome; Kingston University
RP Martini, MG (corresponding author), Kingston Univ London, Kingston Upon Thames KT1 2EE, Surrey, England.
EM sergio.pezzulli@uniroma1.it; m.martini@kingston.ac.uk;
   n.barman@kingston.ac.uk
RI Barman, Nabajeet/T-5060-2019; Martini, Maria/AAC-8754-2022
OI Barman, Nabajeet/0000-0003-2587-7370; Martini, Maria
   G/0000-0002-8710-7550
FU European Union [643072]; EPSRC [EP/P022715/1]; EPSRC [EP/P022723/1,
   EP/P022715/1] Funding Source: UKRI
FX This work was supported in part by the European Union's Horizon 2020
   research and innovation program, under Grant Agreement 643072 (QoE-Net)
   and in part by EPSRC under Grant EP/P022715/1 (IoSiRe).
CR Agresti A., 2018, INTRO CATEGORICAL DA, V3rd
   Akaike H., 1973, INT S INF THEOR, P267, DOI [10.1007/978-1-4612-1694-0, 10.1007/978-1-4612-0919-5_38]
   [Anonymous], 2008, SUBJ VID QUAL ASS ME SUBJ VID QUAL ASS ME
   [Anonymous], 2012, METH SUBJ ASS QUAL T METH SUBJ ASS QUAL T
   Barman N., 2018, 2018 16 ANN WORKSH N P 16 ANN WORKSH NETW, P1
   Barman N, 2020, INT J NETW MANAG, V30, DOI 10.1002/nem.2054
   Bosse S., 2019, QUAL USER EXPERIENCE, V4, P1
   Brunnstrom K., 2018, J ELECT IMAG, V27, P1
   Chang HS, 2018, IEEE T MULTIMEDIA, V20, P3337, DOI 10.1109/TMM.2018.2831639
   Cui J, 2009, AM J EPIDEMIOL, V169, P113, DOI 10.1093/aje/kwn292
   Heagerty PJ, 1996, J AM STAT ASSOC, V91, P1024, DOI 10.2307/2291722
   Hossfeld T, 2011, INT WORK QUAL MULTIM, P131, DOI 10.1109/QoMEX.2011.6065690
   Jamieson Susan, 2004, Med Educ, V38, P1217, DOI 10.1111/j.1365-2929.2004.02012.x
   Janowski L, 2015, IEEE T MULTIMEDIA, V17, P2210, DOI 10.1109/TMM.2015.2484963
   Kendall M. G., 1987, Kendall's Advanced Theory of Statistics
   Li J., 2018, P 10 INT C QUAL MULT P 10 INT C QUAL MULT, P1
   Li Z, 2017, IEEE DATA COMPR CONF, P52, DOI 10.1109/DCC.2017.26
   LIANG KY, 1986, BIOMETRIKA, V73, P13, DOI 10.1093/biomet/73.1.13
   Likert R., 1932, Archives of Pyschology, V22, P5
   MANDEL J, 1991, CHEMOMETR INTELL LAB, V11, P109, DOI 10.1016/0169-7439(91)80058-X
   McCullagh P., 2019, Generalized linear models
   Narwaria M, 2018, IEEE T MULTIMEDIA, V20, P2063, DOI 10.1109/TMM.2018.2794266
   Norman G, 2010, ADV HEALTH SCI EDUC, V15, P625, DOI 10.1007/s10459-010-9222-y
   Ostaszewska A., 2010, WEARABLE AUTONOMOUS, P315
   Pan W, 2001, BIOMETRICS, V57, P120, DOI 10.1111/j.0006-341X.2001.00120.x
   ROYSTON P, 1994, J ROY STAT SOC C, V43, P429, DOI 10.2307/2986270
   SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136
   Seufert M., 2019, P 11 INT C QUAL MULT P 11 INT C QUAL MULT, P1
   Stokes ME., 2012, CATEGORICAL DATA ANA, V3rd
   VANDIJK AM, 1995, P SOC PHOTO-OPT INS, V2451, P90, DOI 10.1117/12.201231
NR 30
TC 10
Z9 10
U1 1
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2505
EP 2519
DI 10.1109/TMM.2020.3013349
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800027
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Rossetto, L
   Gasser, R
   Lokoc, J
   Bailer, W
   Schoeffmann, K
   Muenzer, B
   Soucek, T
   Nguyen, PA
   Bolettieri, P
   Leibetseder, A
   Vrochidis, S
AF Rossetto, Luca
   Gasser, Ralph
   Lokoc, Jakub
   Bailer, Werner
   Schoeffmann, Klaus
   Muenzer, Bernd
   Soucek, Tomas
   Nguyen, Phuong Anh
   Bolettieri, Paolo
   Leibetseder, Andreas
   Vrochidis, Stefanos
TI Interactive Video Retrieval in the Age of Deep Learning - Detailed
   Evaluation of VBS 2019
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Visualization; Browsers; Annotations; Deep learning;
   Semantics; Tools; Interactive video retrieval; video browsing; video
   content analysis; content-based retrieval; evaluations
ID IMAGE
AB Despite the fact that automatic content analysis has made remarkable progress over the last decade - mainly due to significant advances in machine learning - interactive video retrieval is still a very challenging problem, with an increasing relevance in practical applications. The Video Browser Showdown (VBS) is an annual evaluation competition that pushes the limits of interactive video retrieval with state-of-the-art tools, tasks, data, and evaluation metrics. In this paper, we analyse the results and outcome of the 8th iteration of the VBS in detail. We first give an overview of the novel and considerably larger V3C1 dataset and the tasks that were performed during VBS 2019. We then go on to describe the search systems of the six international teams in terms of features and performance. And finally, we perform an in-depth analysis of the per-team success ratio and relate this to the search strategies that were applied, the most popular features, and problems that were experienced. A large part of this analysis was conducted based on logs that were collected during the competition itself. This analysis gives further insights into the typical search behavior and differences between expert and novice users. Our evaluation shows that textual search and content browsing are the most important aspects in terms of logged user interactions. Furthermore, we observe a trend towards deep learning based features, especially in the form of labels generated by artificial neural networks. But nevertheless, for some tasks, very specific content-based search features are still being used. We expect these findings to contribute to future improvements of interactive video search systems.
C1 [Rossetto, Luca] Univ Zurich, CH-8006 Zurich, Switzerland.
   [Gasser, Ralph] Univ Basel, CH-4001 Basel, Switzerland.
   [Lokoc, Jakub; Soucek, Tomas] Charles Univ Prague, Prague 11000, Czech Republic.
   [Bailer, Werner] JOANNEUM RES, DIGITAL, A-8010 Graz, Austria.
   [Schoeffmann, Klaus; Muenzer, Bernd; Leibetseder, Andreas] Alpen Adria Univ Klagenfurt, A-9020 Klagenfurt, Austria.
   [Nguyen, Phuong Anh] City Univ Hong Kong, Hong Kong 999077, Peoples R China.
   [Bolettieri, Paolo] CNR, I-56124 Pisa, Italy.
   [Vrochidis, Stefanos] Ctr Res & Technol Hellas, Informat Technol Inst, Thessaloniki 57001, Greece.
C3 University of Zurich; University of Basel; Charles University Prague;
   University of Klagenfurt; City University of Hong Kong; Consiglio
   Nazionale delle Ricerche (CNR); Centre for Research & Technology Hellas
RP Rossetto, L (corresponding author), Univ Zurich, CH-8006 Zurich, Switzerland.
EM rossetto@ifi.uzh.ch; ralph.gasser@unibas.ch; Lokoc@ksi.ms.mff.cuni.cz;
   werner.bailer@joanneum.at; ks@itec.aau.at; bernd@itec.aau.at;
   tomas.soucek1@gmail.com; panguyen2-c@my.cityu.edu.hk;
   paolo.bolettieri@isti.cnr.it; aleibets@itec.aau.at; stefanos@iti.gr
RI Nguyen, Phuong Anh/AAQ-4427-2021; Leibetseder, Andreas/AAI-2725-2020;
   Lokoč, Jakub/P-1216-2017; Rossetto, Luca/AAI-8684-2020
OI Nguyen, Phuong Anh/0000-0003-1289-3785; Leibetseder,
   Andreas/0000-0002-9535-966X; Rossetto, Luca/0000-0002-5389-9465; Gasser,
   Ralph Marc Philipp/0000-0002-3016-1396; Vrochidis,
   Stefanos/0000-0002-2505-9178; Bailer, Werner/0000-0003-2442-4900
FU Czech Science Foundation (GACR) [19-22071Y]; European Unions Horizon
   2020 research and innovation programme: V4Design [779962]; MARCONI
   [761802]; H2020 - Industrial Leadership [779962] Funding Source: H2020 -
   Industrial Leadership
FX This work was supported in part by Czech Science Foundation (GACR)
   Project 19-22071Y, in part by projects that have received funding from
   the European Unions Horizon 2020 research and innovation programme:
   V4Design under Grant 779962, and in part by MARCONI under Grant 761802.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. X. Cao.
CR Albitar S., 2012, INT C WEB INF SYST E, V7651, P326, DOI 10.1007/978-3-642-35063-4
   Amato G., 2017, P 15 INT WORKSH CONT, V26, P1
   Amato G, 2019, LECT NOTES COMPUT SC, V11296, P591, DOI 10.1007/978-3-030-05716-9_51
   Amato G, 2016, LECT NOTES COMPUT SC, V9939, P93, DOI 10.1007/978-3-319-46759-7_7
   Andreadis S, 2019, LECT NOTES COMPUT SC, V11296, P602, DOI 10.1007/978-3-030-05716-9_53
   [Anonymous], 2018, COMPUTER VISION PATT
   [Anonymous], 2012, P 8 INT C LANG RES E
   [Anonymous], 2017, ARXIV170701340
   [Anonymous], 2016, LECT NOTES COMPUT SC, DOI DOI 10.1007/978-3-319-43946-4_14
   [Anonymous], ICMR19 P 2019 ACM
   [Anonymous], 2009, PROC 1STWORKSHOPWEB
   [Anonymous], 2019, WORLD WIDE WEB, DOI DOI 10.1007/s11280-018-0541-x
   [Anonymous], ICMR19 P 2019 ACM
   Awad G., 2018, P TRECVID
   Berns F, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P334, DOI 10.1145/3323873.3325051
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen Liang-Chieh, 2014, Comput Sci, DOI DOI 10.48550/ARXIV.1412.7062
   Chen YD, 2019, IEEE T MULTIMEDIA, V21, P1934, DOI 10.1109/TMM.2018.2890361
   Chien SY, 2004, IEEE T MULTIMEDIA, V6, P732, DOI 10.1109/TMM.2004.834868
   Cobârzan C, 2017, MULTIMED TOOLS APPL, V76, P5539, DOI 10.1007/s11042-016-3661-2
   Devlin J., 2018, BERT PRE TRAINING DE
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Dong Z, 2016, AAAI CONF ARTIF INTE, P3471
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Dang-Nguyen DT, 2018, LSC '18: PROCEEDINGS OF THE 2018 ACM WORKSHOP ON THE LIFELOG SEARCH CHALLENGE, P1, DOI 10.1145/3210539.3210540
   Francis D, 2019, LECT NOTES COMPUT SC, V11296, P278, DOI 10.1007/978-3-030-05716-9_23
   Furuta R, 2019, MULTIMED TOOLS APPL, V78, P18713, DOI 10.1007/s11042-018-7148-1
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Gasser R, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P391, DOI 10.1145/3323873.3326921
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XT, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1740, DOI 10.1145/3343031.3350974
   Hu WM, 2011, IEEE T SYST MAN CY C, V41, P797, DOI 10.1109/TSMCC.2011.2109710
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kay W., 2017, CORR ABS170506950
   Ke X, 2019, IEEE T MULTIMEDIA, V21, P2093, DOI 10.1109/TMM.2019.2895511
   Kuznetsova A., 2018, The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale
   Leibetseder A, 2018, LECT NOTES COMPUT SC, V10705, P425, DOI 10.1007/978-3-319-73600-6_45
   Li XR, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1786, DOI 10.1145/3343031.3350906
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lokoc J, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P2, DOI 10.1145/3323873.3326588
   Lokoc J, 2018, LSC '18: PROCEEDINGS OF THE 2018 ACM WORKSHOP ON THE LIFELOG SEARCH CHALLENGE, P15, DOI 10.1145/3210539.3210543
   Lokoc J, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P177, DOI 10.1145/3323873.3325034
   Lokoc J, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3295663
   Lokoc J, 2018, IEEE T MULTIMEDIA, V20, P3361, DOI 10.1109/TMM.2018.2830110
   Lou YH, 2017, IEEE DATA COMPR CONF, P420, DOI 10.1109/DCC.2017.31
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Lu JS, 2019, ADV NEUR IN, V32
   Lu YJ, 2017, LECT NOTES COMPUT SC, V10133, P463, DOI 10.1007/978-3-319-51814-5_42
   Markatopoulou F, 2019, IEEE T CIRC SYST VID, V29, P1631, DOI 10.1109/TCSVT.2018.2848458
   Markatopoulou F, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P412, DOI 10.1145/3078971.3079041
   Mettes P, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P175, DOI 10.1145/2911996.2912036
   Nguyen P. A., 2019, P INT C MULT MOD, P609
   Nguyen P. A., 2019, P INT C MULT MOD, P609
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Nguyen PA, 2018, LECT NOTES COMPUT SC, V10705, P407, DOI 10.1007/978-3-319-73600-6_42
   Primus MJ, 2018, LECT NOTES COMPUT SC, V10705, P438, DOI 10.1007/978-3-319-73600-6_47
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Rossetto Luca, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015. Proceedings: LNCS 8936, P255, DOI 10.1007/978-3-319-14442-9_24
   Rossetto L., 2019, ARXIV190912526 CORR
   Rossetto L., 2019, ARXIV190210647
   Rossetto L, 2018, ACM SIGMULTIMEDIA RE, V9
   Rossetto L., 2018, SIG MULTIMEDIA RECOR, V9, DOI 10.1145/3178422.3178430
   Rossetto L, 2019, LECT NOTES COMPUT SC, V11296, P616, DOI 10.1007/978-3-030-05716-9_55
   Rossetto L, 2017, LECT NOTES COMPUT SC, V10133, P469, DOI 10.1007/978-3-319-51814-5_43
   Rossetto L, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1183, DOI 10.1145/2964284.2973797
   Rossetto L, 2014, 2014 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P18, DOI 10.1109/ISM.2014.38
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schoeffmann K., 2010, SPIE Reviews, V1, P018004, DOI DOI 10.1117/6.0000005
   Schoeffmann K, 2019, LECT NOTES COMPUT SC, V11296, P585, DOI 10.1007/978-3-030-05716-9_50
   Schoeffmann K, 2014, IEEE T MULTIMEDIA, V16, P1942, DOI 10.1109/TMM.2014.2333666
   Seddati O, 2017, MULTIMED TOOLS APPL, V76, P22333, DOI 10.1007/s11042-017-4799-2
   Soucek Toma<prime>.s, 2019, ARXIV190603363
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tolias G., 2015, ARXIV151105879
   Tolias G., 2016, P INT C LEARN REPR S, P1
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   Wang W, 2016, VLDB J, V25, P79, DOI 10.1007/s00778-015-0391-4
   Wu GS, 2019, IEEE T IMAGE PROCESS, V28, P1993, DOI 10.1109/TIP.2018.2882155
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Ye GG, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P471, DOI 10.1145/2733373.2806221
   Zhang W., 2014, P NIST TRECVID WORKS
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 87
TC 31
Z9 31
U1 1
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 243
EP 256
DI 10.1109/TMM.2020.2980944
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600019
DA 2024-07-18
ER

PT J
AU Tian, CW
   Xu, Y
   Zuo, WM
   Zhang, B
   Fei, LK
   Lin, CW
AF Tian, Chunwei
   Xu, Yong
   Zuo, Wangmeng
   Zhang, Bob
   Fei, Lunke
   Lin, Chia-Wen
TI Coarse-to-Fine CNN for Image Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Training; Image reconstruction; Fuses;
   Visualization; Residual neural networks; Cascaded structure;
   convolutional neural network; feature fusion; feature refinement; Image
   super-resolution
ID NETWORK
AB Deep convolutional neural networks (CNNs) have been popularly adopted in image super-resolution (SR). However, deep CNNs for SR often suffer from the instability of training, resulting in poor image SR performance. Gathering complementary contextual information can effectively overcome the problem. Along this line, we propose a coarse-to-fine SR CNN (CFSRCNN) to recover a high-resolution (HR) image from its low-resolution version. The proposed CFSRCNN consists of a stack of feature extraction blocks (FEBs), an enhancement block (EB), a construction block (CB) and, a feature refinement block (FRB) to learn a robust SR model. Specifically, the stack of FEBs learns the long- and short-path features, and then fuses the learned features by expending the effect of the shallower layers to the deeper layers to improve the representing power of learned features. A compression unit is then used in each FEB to distill important information of features so as to reduce the number of parameters. Subsequently, the EB utilizes residual learning to integrate the extracted features to prevent from losing edge information due to repeated distillation operations. After that, the CB applies the global and local LR features to obtain coarse features, followed by the FRB to refine the features to reconstruct a high-resolution image. Extensive experiments demonstrate the high efficiency and good performance of our CFSRCNN model on benchmark datasets compared with state-of-the-art SR models. The code of CFSRCNN is accessible on https://github.com/hellloxiaotian/CFSRCNN.
C1 [Tian, Chunwei; Xu, Yong] Harbin Inst Technol, Biocomp Res Ctr, Shenzhen, Peoples R China.
   [Tian, Chunwei; Xu, Yong] Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.
   [Xu, Yong; Zuo, Wangmeng] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Zuo, Wangmeng] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Zhang, Bob] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
   [Fei, Lunke] Guangdong Univ Technol, Sch Comp Sci & Technol, Guangzhou 510006, Peoples R China.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 300, Taiwan.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Inst Commun Engn, Hsinchu 300, Taiwan.
C3 Harbin Institute of Technology; Peng Cheng Laboratory; Harbin Institute
   of Technology; University of Macau; Guangdong University of Technology;
   National Tsing Hua University; National Tsing Hua University
RP Xu, Y (corresponding author), Harbin Inst Technol, Biocomp Res Ctr, Shenzhen, Peoples R China.; Xu, Y (corresponding author), Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.
EM chunweitian@163.com; yongxu@ymail.com; wmzuo@hit.edu.cn;
   bobzhang@umac.mo; flksxm@126.com; cwlin@ee.nthu.edu.tw
RI Zuo, Wangmeng/B-3701-2008; Lin, Chia-Wen/ABH-6075-2020; Zhang,
   Bob/ABD-5926-2021; Zhang, Bob/HIR-3656-2022; Lin, Chia-Wen/M-4571-2013
OI Zhang, Bob/0000-0003-2497-9519; Zhang, Bob/0000-0001-6512-0474; Zuo,
   Wangmeng/0000-0002-3330-783X; Lin, Chia-Wen/0000-0002-9097-2318; Fei,
   Lunke/0000-0001-6072-7875
FU National Nature Science Foundation of China [61876051]; Shenzhen Key
   Laboratory of Visual Object Detection and Recognition
   [ZDSYS20190902093015527]
FX This work was supported in part by the National Nature Science
   Foundation of China Gant 61876051 and in part by the Shenzhen Key
   Laboratory of Visual Object Detection and Recognition under Grant
   ZDSYS20190902093015527. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr Lamberto Ballan.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Ahn N, 2018, P IEEE C COMPUTER VI, P791
   Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chang CY, 2019, INT CONF ACOUST SPEE, P1742, DOI [10.1109/ICASSP.2019.8683719, 10.1109/icassp.2019.8683719]
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   Cheng MC, 2018, INT C DIGITAL HOME, P24, DOI 10.1109/ICDH.2018.00012
   Cui Z, 2014, LECT NOTES COMPUT SC, V8693, P49, DOI 10.1007/978-3-319-10602-1_4
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   DOUILLARD C, 1995, EUR T TELECOMMUN, V6, P507, DOI 10.1002/ett.4460060506
   Du B, 2019, IEEE T GEOSCI REMOTE, V57, P9976, DOI 10.1109/TGRS.2019.2930682
   Duan CQ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4033
   Fan YC, 2017, IEEE COMPUT SOC CONF, P1157, DOI 10.1109/CVPRW.2017.154
   Guo CL, 2019, IEEE T IMAGE PROCESS, V28, P2545, DOI 10.1109/TIP.2018.2887029
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Hu Y, 2018, BLOCKCHAIN BASED SMA
   Hu YT, 2020, IEEE T CIRC SYST VID, V30, P3911, DOI 10.1109/TCSVT.2019.2915238
   Hu YT, 2016, IEEE T IMAGE PROCESS, V25, P4091, DOI 10.1109/TIP.2016.2580942
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Huang YW, 2017, PROC CVPR IEEE, P5787, DOI 10.1109/CVPR.2017.613
   Hui Z, 2018, INT C PATT RECOG, P2670, DOI 10.1109/ICPR.2018.8545648
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Lee J.-S., 2018, ARXIV181112546
   Li F, 2019, NEUROCOMPUTING, V358, P285, DOI 10.1016/j.neucom.2019.05.042
   Li S, 2019, PROC CVPR IEEE, P10514, DOI 10.1109/CVPR.2019.01077
   Li Siyuan, 2018, ARXIV180402688
   Li Y., 2018, EUR C COMP VIS WORKS
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Liang X, 2021, IEEE T SYST MAN CY-S, V51, P1534, DOI 10.1109/TSMC.2019.2898684
   Liu PJ, 2018, IEEE COMPUT SOC CONF, P886, DOI 10.1109/CVPRW.2018.00121
   Liu Q, 2020, IEEE T MULTIMEDIA, V22, P666, DOI 10.1109/TMM.2019.2932615
   Lu XQ, 2014, IEEE T CYBERNETICS, V44, P366, DOI 10.1109/TCYB.2013.2256347
   Luo XL, 2019, PATTERN RECOGN, V93, P283, DOI 10.1016/j.patcog.2019.04.027
   Mao XJ, 2016, ADV NEUR IN, V29
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Ni M, 2017, IEEE ACCESS, V5, P26666, DOI 10.1109/ACCESS.2017.2773141
   Pan JS, 2019, IEEE T PATTERN ANAL, V41, P1412, DOI 10.1109/TPAMI.2018.2832125
   Polatkan G, 2015, IEEE T PATTERN ANAL, V37, P346, DOI 10.1109/TPAMI.2014.2321404
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Ren HY, 2017, IEEE COMPUT SOC CONF, P1050, DOI 10.1109/CVPRW.2017.142
   Schulter S, 2015, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2015.7299003
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shi Y, 2017, IEEE T MULTIMEDIA, V19, P2804, DOI 10.1109/TMM.2017.2711263
   Singh P, 2019, PROC CVPR IEEE, P4830, DOI 10.1109/CVPR.2019.00497
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tian C., 2019, ARXIV191213171
   Tian CW, 2020, NEURAL NETWORKS, V124, P117, DOI 10.1016/j.neunet.2019.12.024
   Tian CW, 2020, NEURAL NETWORKS, V121, P461, DOI 10.1016/j.neunet.2019.08.022
   Timofte R, 2016, PROC CVPR IEEE, P1865, DOI 10.1109/CVPR.2016.206
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Wang C., 2019, CoRR
   Wang YF, 2019, IEEE ACCESS, V7, P31959, DOI 10.1109/ACCESS.2019.2903582
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50
   Xu JL, 2018, CAMB CHINA LIBR, P1
   Xu J, 2017, IEEE I CONF COMP VIS, P1105, DOI 10.1109/ICCV.2017.125
   Xu J, 2015, IEEE I CONF COMP VIS, P244, DOI 10.1109/ICCV.2015.36
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang WM, 2019, IEEE SIGNAL PROC LET, V26, P538, DOI 10.1109/LSP.2018.2890770
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
   Zhang L, 2019, IEEE ACCESS, V7, P109729, DOI 10.1109/ACCESS.2018.2865613
   Zhang SQ, 2018, IEEE T MULTIMEDIA, V20, P1576, DOI 10.1109/TMM.2017.2766843
   Zhang W, 2018, IEEE CONF COMPUT
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhao LJ, 2019, PATTERN RECOGN, V88, P356, DOI 10.1016/j.patcog.2018.11.028
   Zhong ZS, 2018, ADV NEUR IN, V31
   Zuo WM, 2011, IEEE T IMAGE PROCESS, V20, P2748, DOI 10.1109/TIP.2011.2131665
NR 75
TC 126
Z9 134
U1 10
U2 99
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1489
EP 1502
DI 10.1109/TMM.2020.2999182
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300003
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Wang, W
   Gao, JY
   Yang, XS
   Xu, CS
AF Wang, Wei
   Gao, Junyu
   Yang, Xiaoshan
   Xu, Changsheng
TI Learning Coarse-to-Fine Graph Neural Networks for Video-Text Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Encoding; Task analysis; Semantics; Data models;
   Cognition; Focusing; Video-text retrieval; graph neural network;
   coarse-to-fine strategy
ID RECOGNITION; FEATURES; IMAGE
AB We address the problem of video-text retrieval that searches videos via natural language description or vice versa. Most state-of-the-art methods only consider cross-modal learning for two or three data points in isolation, ignoring to get benefit from the structural information of other data points from a global view. In this paper, we propose to exploit the comprehensive relationships among cross-modal samples via Graph Neural Networks (GNN). To improve the discriminative ability for accurately finding the positive sample, a Coarse-to-Fine GNN is constructed, which can progressively optimize the retrieval results via multi-step reasoning. Specifically, we first adopt heuristic edge features to represent relationships. Then we design a scoring module in each layer to rank the edges connected to the query node and drop the edges with lower scores. Finally, to alleviate the class imbalance issue, we propose a random-drop focal loss to optimize the whole framework. Extensive experimental results show that our method consistently outperforms the state-of-the-arts on four benchmarks.
C1 [Wang, Wei; Gao, Junyu; Yang, Xiaoshan; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Wang, Wei; Gao, Junyu; Yang, Xiaoshan; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Wang, Wei; Gao, Junyu; Yang, Xiaoshan; Xu, Changsheng] PengCheng Lab, Shenzhen, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Xu, CS (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.; Xu, CS (corresponding author), PengCheng Lab, Shenzhen, Peoples R China.
EM wangwei2018@ia.ac.cn; gaojunyu2015@ia.ac.cn;
   xiaoshan.yang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn
RI Gao, Junyu/HDO-5516-2022; xu, cj/HJZ-3488-2023; Xu, Chang/GQP-7280-2022
OI xu, chang sheng/0000-0001-8343-9665
FU National Key Research and Development Program of China [2018AAA0102200];
   National Natural Science Foundation of China [61720106006, 61721004,
   61832002, 61702511, 61751211, 61532009, U1836220, U1705262, 61872424,
   61936005]; Key Research Program of Frontier Sciences of CAS
   [QYZDJSSWJSC039]; Research Program of National Laboratory of Pattern
   Recognition [Z-2018007]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018AAA0102200, in part by the
   National Natural Science Foundation of China under Grants 61720106006,
   61721004, 61832002, 61702511, 61751211, 61532009, U1836220, U1705262,
   61872424, and 61936005, in part by theKey Research Program of Frontier
   Sciences of CAS under Grant QYZDJSSWJSC039, and in part by the Research
   Program of National Laboratory of Pattern Recognition under Grant
   Z-2018007. The associate editor coordinating the reviewof this
   manuscript and approving it for publication was Prof. Andrew D.
   Bagdanov.
CR [Anonymous], 2011, P 19 ACM INT C MULT
   [Anonymous], 2007, CIVR '07
   [Anonymous], 2016, 4 INT C LEARN REPR I
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Chang C, 2019, PROC CVPR IEEE, P9415, DOI 10.1109/CVPR.2019.00965
   Chen D., 2011, ANN M ASS COMPUTATIO, P190
   Dong JF, 2019, PROC CVPR IEEE, P9338, DOI 10.1109/CVPR.2019.00957
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Gao J., 2020, IEEE T PATTERN ANAL
   Gao JY, 2020, IEEE T MULTIMEDIA, V22, P3088, DOI 10.1109/TMM.2020.2969787
   Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478
   Gao JY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P690, DOI 10.1145/3240508.3240566
   Gao JY, 2019, AAAI CONF ARTIF INTE, P8303
   Gong LY, 2019, PROC CVPR IEEE, P9203, DOI 10.1109/CVPR.2019.00943
   Hao YB, 2017, IEEE T MULTIMEDIA, V19, P1, DOI 10.1109/TMM.2016.2610324
   Jiang YG, 2016, IEEE T MULTIMEDIA, V18, P2161, DOI 10.1109/TMM.2016.2614233
   Jing W., WORLD WIDE WEB, V22, P771
   Kim J, 2019, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2019.00010
   Kingma D. P., 2014, arXiv
   Kipf T., 2018, PR MACH LEARN RES
   Kipf TN, 2017, INT C LEARN REPR
   Li XR, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1786, DOI 10.1145/3343031.3350906
   Li YC, 2016, PROC CVPR IEEE, P4641, DOI 10.1109/CVPR.2016.502
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Y., 2019, P BRIT MACH VIS C, P279
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu X, 2020, IEEE T MULTIMEDIA, V22, P2048, DOI 10.1109/TMM.2019.2947358
   Ma XH, 2020, IEEE T MULTIMEDIA, V22, P3101, DOI 10.1109/TMM.2020.2969792
   Magliani F, 2019, LECT NOTES COMPUT SC, V11752, P537, DOI 10.1007/978-3-030-30645-8_49
   Mithun NC, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/3206025.3206064
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Prates M, 2019, AAAI CONF ARTIF INTE, P4731
   Satorras V.G., 2018, ICLR
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Schlichtkrull M, 2018, LECT NOTES COMPUT SC, V10843, P593, DOI 10.1007/978-3-319-93417-4_38
   Shizhe Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10635, DOI 10.1109/CVPR42600.2020.01065
   Socher R, 2010, PROC CVPR IEEE, P966, DOI 10.1109/CVPR.2010.5540112
   Sohn K, 2016, ADV NEUR IN, V29
   Song J., 2011, P ACM INT C MULT, P423
   Song Y, 2019, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2019.00208
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang XS, 2019, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR.2019.00535
   Wu X., 2007, P ACM MULT, P218
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu R, 2015, AAAI CONF ARTIF INTE, P2346
   Xu RQ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P982
   Yang X., 2016, ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), V12, P1, DOI DOI 10.1145/2818709
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P346, DOI 10.1109/TMM.2015.2393635
   Yu J, 2018, LECT NOTES COMPUT SC, V11164, P223, DOI 10.1007/978-3-030-00776-8_21
   Yu Y, 2018, LECT NOTES COMPUT SC, V11211, P487, DOI 10.1007/978-3-030-01234-2_29
   Yu Y, 2017, PROC CVPR IEEE, P3261, DOI 10.1109/CVPR.2017.347
   Zhang J, 2018, AAAI CONF ARTIF INTE, P539
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
NR 57
TC 23
Z9 25
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2386
EP 2397
DI 10.1109/TMM.2020.3011288
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800018
DA 2024-07-18
ER

PT J
AU Xie, N
   Zhang, QQ
   Chen, YC
   Hu, J
   Luo, G
   Chen, CS
AF Xie, Ning
   Zhang, Qiqi
   Chen, Yicong
   Hu, Ji
   Luo, Gang
   Chen, Changsheng
TI Low-Cost Anti-Copying 2D Barcode by Exploiting Channel Noise
   Characteristics
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Two dimensional displays; Integrated circuits; Authentication; Printing;
   Law; Two-dimensional barcodes; anti-copying; illegal channel;
   theoretical modeling
ID QR CODE; PARAMETER; ROBUST
AB In this paper, to overcome the drawbacks of prior approaches for defending against Illegally-Copying (IC) attacks, such as low generality, high cost, and high overhead, we propose a Low-Cost Anti-Copying (LCAC) 2D barcode by exploiting the difference between the noise characteristics of legal and illegal channels. An embedding strategy is proposed, and for a variant of this strategy, we also perform a corresponding analysis. To accurately evaluate the performance of our approach, a theoretical model of the noise in an illegal channel is established by using a generalized Gaussian distribution. By comparing the experimental results based on various printers, scanners, and mobile phones, it can be found that the sample histogram and fitting curve of the theoretical model match well, so it can be concluded that the theoretical model works well. To evaluate the security of the proposed LCAC code, in addition to the Direct-Copying (DC) attack, an improved version, called the Synthesized-Copying (SC) attack, is also considered in this paper. Based on the theoretical model, we build a prediction function to optimize the parameters of our approach. Parameters optimization seeks a tradeoff between the production cost and the cost of IC attacks. The experimental results show that the proposed LCAC code with two printers and two scanners can detect DC attacks effectively and resist SC attacks up to the access of 14 legal copies.
C1 [Xie, Ning; Zhang, Qiqi; Chen, Yicong; Hu, Ji; Luo, Gang; Chen, Changsheng] Shenzhen Univ, Coll Elect & Informat Engn, Shenzhen Key Lab Media Secur, Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
C3 Shenzhen University
RP Chen, CS (corresponding author), Shenzhen Univ, Coll Elect & Informat Engn, Shenzhen Key Lab Media Secur, Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
EM ningxie@szu.edu.cn; 2654892524@qq.com; 394484153@qq.com;
   190696074@qq.com; 821402413@qq.com; cschen@szu.edu.cn
RI c, yc/HTP-0901-2023; Xie, Ning/U-7934-2019
OI Xie, Ning/0000-0002-6443-8111
FU Natural Science Foundations of China [61972262, 62072313]; Natural
   Science Foundation of Guangdong, China [2016A030313046,
   2020A1515010563]; Fundamental Research Programs of Shenzhen City
   [JCYJ20180305124648757]; Shenzhen RD Program [GJHZ20180928155814437];
   China Scholarship Council [201908440031]
FX Manuscript received February 11, 2020; revised August 15, 2020
   andOctober 7, 2020; accepted October 11, 2020. Date of publication
   October 15, 2020; date of current version October 19, 2021. This work
   supported in part by the Natural Science Foundations of China under
   Grants 61972262 and 62072313, in part by the Natural Science Foundation
   of Guangdong, China under Grants 2016A030313046, 2020A1515010563, in
   part by the Fundamental Research Programs of Shenzhen City under Grant
   JCYJ20180305124648757, in part by Shenzhen R&D Program under Grant
   GJHZ20180928155814437, and in part by China Scholarship Council under
   Grant 201908440031. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. P. K. Atrey.
   (Corresponding author: Changsheng Chen.)
CR [Anonymous], 2017, 340622017 GBT
   Chen CS, 2020, IEEE T INF FOREN SEC, V15, P1056, DOI 10.1109/TIFS.2019.2934861
   Chen CS, 2018, IEEE T CIRC SYST VID, V28, P3300, DOI 10.1109/TCSVT.2017.2741472
   Chen CS, 2016, IEEE T IMAGE PROCESS, V25, P3444, DOI 10.1109/TIP.2016.2573592
   Fridrich J., 2000, Proceedings International Conference on Information Technology: Coding and Computing (Cat. No.PR00540), P178, DOI 10.1109/ITCC.2000.844203
   Ho ATP, 2014, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2014-9
   Jaynes E.T., 2004, Math. Intell., V57, P76, DOI DOI 10.1063/1.1825273
   Krombholz K, 2014, QR CODE SECURITY SUR
   Lewis M. E, 2009, CHINAS COSMOPOLITAN, V3
   Lin SS, 2015, IEEE T MULTIMEDIA, V17, P1515, DOI 10.1109/TMM.2015.2437711
   Lin YH, 2013, IEEE T MULTIMEDIA, V15, P2198, DOI 10.1109/TMM.2013.2271745
   Maehara T, 2014, JOINT INT CONF SOFT, P149, DOI 10.1109/SCIS-ISIS.2014.7044673
   Malarchelvi P. D. Sheba Kezia, 2013, International Journal of Network Security, V15, P365
   Marguerettaz X., 2014, US Patent, Patent No. [8,740,088, 8740088]
   Motahari A, 2015, IEEE T MULTIMEDIA, V17, P118, DOI 10.1109/TMM.2014.2366601
   Nadarajah S, 2005, J APPL STAT, V32, P685, DOI 10.1080/02664760500079464
   Niu YJ, 2011, COMMUN NONLINEAR SCI, V16, P1986, DOI 10.1016/j.cnsns.2010.08.015
   Rungraungsilp S., 2012, P INT C COMP COMM TE, P144
   Sakib MN, 2013, IEEE PHOTONIC TECH L, V25, P2274, DOI 10.1109/LPT.2013.2286105
   Scholes D. S., 2015, U.S. Patent, Patent No. [8 950 684, 8950684]
   SHARIFI K, 1995, IEEE T CIRC SYST VID, V5, P52, DOI 10.1109/76.350779
   Soury H, 2015, 2015 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P1017, DOI 10.1109/GlobalSIP.2015.7418351
   Swaminathan A, 2006, IEEE T INF FOREN SEC, V1, P215, DOI 10.1109/TIFS.2006.873601
   Tkachenko I, 2018, IEEE INT WORKS INFOR
   Tkachenko I, 2016, SIGNAL PROCESS-IMAGE, V41, P46, DOI 10.1016/j.image.2015.11.007
   Tkachenko I, 2016, IEEE T INF FOREN SEC, V11, P571, DOI 10.1109/TIFS.2015.2506546
   Vidas T, 2013, LECT NOTES COMPUT SC, V7862, P52, DOI 10.1007/978-3-642-41320-9_4
   Villán R, 2006, IEEE T INF FOREN SEC, V1, P405, DOI 10.1109/TIFS.2006.885022
   Voloshynovskiy S, 2016, INT CONF ACOUST SPEE, P2029, DOI 10.1109/ICASSP.2016.7472033
   Wang TY, 2006, INT CONF SIGN PROCES, P101
   Wang XY, 2010, COMMUN NONLINEAR SCI, V15, P4052, DOI 10.1016/j.cnsns.2010.02.014
   Wave D., 2015, INT ORG STANDARDIZAT, V18004
   Wong CW, 2017, IEEE T INF FOREN SEC, V12, P1885, DOI 10.1109/TIFS.2017.2694404
   Wong CY, 2017, IEEE T BIO-MED ENG, V64, P629, DOI 10.1109/TBME.2016.2571060
   Xie RS, 2015, NEUROCOMPUTING, V167, P625, DOI 10.1016/j.neucom.2015.04.026
   Xu L, 2008, IEEE T MULTIMEDIA, V10, P361, DOI 10.1109/TMM.2008.917353
   Yonghui Zhao, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2725, DOI 10.1109/ICIP.2011.6116232
   You ML, 2016, NANOSCALE, V8, P10096, DOI 10.1039/c6nr01353h
   Zhang L, 2019, IEEE T IMAGE PROCESS, V28, P464, DOI 10.1109/TIP.2018.2868383
   Zhu JF, 2019, IEEE ACCESS, V7, P181184, DOI 10.1109/ACCESS.2019.2938270
   Zhu X., 2016, SECURE EFFICIENT MOB
NR 41
TC 6
Z9 7
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3752
EP 3767
DI 10.1109/TMM.2020.3031083
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100025
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zheng, XT
   Qi, L
   Ren, YT
   Lu, XQ
AF Zheng, Xiangtao
   Qi, Lei
   Ren, Yutao
   Lu, Xiaoqiang
TI Fine-Grained Visual Categorization by Localizing Object Parts With
   Single Image
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Detectors; Training; Image representation;
   Visualization; Semantics; Birds; Fine-grained visual categorization;
   Part localization; Part relationship; Spectral clustering; Dropout
   learning
ID RECOGNITION; CLASSIFICATION; NETWORKS; DROPOUT
AB Fine-grained visual categorization (FGVC) refers to assigning fine-grained labels to images which belong to the same base category. Due to the high inter-class similarity, it is challenging to distinguish fine-grained images under different subcategories. Recently, researchers have proposed to firstly localize key object parts within images and then find discriminative clues on object parts. To localize object parts, existing methods train detectors for different kinds of object parts. However, due to the fact that the same kind of object part in different images often changes intensely in appearance, the existing methods face two shortages: 1) Training part detector for object parts with diverse appearance is laborious; 2) Discriminative parts with unusual appearance may be neglected by the trained part detectors. To localize the key object parts efficiently and accurately, a novel FGVC method is proposed in the paper. The main novelty is that the proposed method localizes the key object parts within each image only depending on a single image and hence avoid the influence of diversity between parts in different images. The proposed FGVC method consists of two key steps. Firstly, the proposed method localizes the key parts in each image independently. To this end, potential object parts in each image are identified and then these potential parts are merged to generate the final representative object parts. Secondly, two kinds of features are extracted for simultaneously describing the discriminative clues within each part and the relationship between object parts. In addition, a part based dropout learning technique is adopted to boost the classification performance further in the paper. The proposed method is evaluated in comparison experiments and the experiment results show that the proposed method can achieve comparable or better performance than state-of-the-art methods.
C1 [Zheng, Xiangtao; Qi, Lei; Lu, Xiaoqiang] Chinese Acad Sci, Key Lab Spectral Imaging Technol CAS, Xian Inst Opt & Precis Mech, Xian 710119, Peoples R China.
   [Ren, Yutao] Wuhan Univ Technol, Wuhan 430070, Peoples R China.
C3 Chinese Academy of Sciences; Xi'an Institute of Optics & Precision
   Mechanics, CAS; Wuhan University of Technology
RP Lu, XQ (corresponding author), Chinese Acad Sci, Key Lab Spectral Imaging Technol CAS, Xian Inst Opt & Precis Mech, Xian 710119, Peoples R China.
EM xiangtaoz@gmail.com; 553054612@qq.com; taoyao0204@163.com;
   luxq666666@gmail.com
OI Lu, Xiaoqiang/0000-0002-7037-5188; Zheng, Xiangtao/0000-0002-8398-6324;
   Ren, Yutao/0000-0002-0840-3918
FU National Natural Science Found for Distinguished Young Scholars
   [61925112]; National Key R&D Program of China [2017YFB0502900]; National
   Natural Science Foundation of China [61806193, 61702498, 61772510];
   Young Top-Notch Talent Program of Chinese Academy of Sciences
   [QYZDB-SSW-JSC015]; CAS "Light of West China" Program [XAB2017B26,
   XAB2017B15]; Key Research Program of Frontier Sciences, Chinese Academy
   of Sciences [QYZDY-SSW-JSC044]
FX This work was supported in part by the National Natural Science Found
   for Distinguished Young Scholars under Grant 61925112, in part by the
   National Key R&D Program of China under Grant 2017YFB0502900, in part by
   the National Natural Science Foundation of China under Grants 61806193,
   61702498, and 61772510, in part by the Young TopNotch Talent Program of
   Chinese Academy of Sciences under Grant QYZDB-SSW-JSC015, in part by the
   CAS "Light of West China" Program under Grants XAB2017B26 and
   XAB2017B15, and in part by the Key Research Program of Frontier
   Sciences, Chinese Academy of Sciences under Grant QYZDY-SSW-JSC044.
CR Achille A, 2018, IEEE T PATTERN ANAL, V40, P2897, DOI 10.1109/TPAMI.2017.2784440
   Akata Z, 2014, IEEE T PATTERN ANAL, V36, P507, DOI 10.1109/TPAMI.2013.146
   Angelova A, 2013, PROC CVPR IEEE, P811, DOI 10.1109/CVPR.2013.110
   Baldi P, 2014, ARTIF INTELL, V210, P78, DOI 10.1016/j.artint.2014.02.004
   Barré P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005
   Berg T, 2014, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2014.259
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Du Y, 2017, IEEE ENER CONV, P2921, DOI 10.1109/ECCE.2017.8096539
   Dubey A, 2018, LECT NOTES COMPUT SC, V11216, P71, DOI 10.1007/978-3-030-01258-8_5
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Gavves E, 2013, IEEE I CONF COMP VIS, P1713, DOI 10.1109/ICCV.2013.215
   Guo P, 2019, IEEE WINT CONF APPL, P1876, DOI 10.1109/WACV.2019.00204
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Huang C, 2017, IEEE T MULTIMEDIA, V19, P673, DOI 10.1109/TMM.2016.2631122
   Huang C, 2016, IEEE T MULTIMEDIA, V18, P2372, DOI 10.1109/TMM.2016.2602060
   Jaderberg Max, 2015, ADV NEURAL INFORM PR, P8
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Khosla A., 2011, P IEEE C COMP VIS PA, V2
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Li ZC, 2017, IEEE INT CONF COMP V, P1199, DOI 10.1109/ICCVW.2017.145
   Liang XD, 2016, IEEE T MULTIMEDIA, V18, P1175, DOI 10.1109/TMM.2016.2542983
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Lu XQ, 2020, IEEE T GEOSCI REMOTE, V58, P1985, DOI 10.1109/TGRS.2019.2951636
   Lu XQ, 2018, IEEE T IMAGE PROCESS, V27, P106, DOI 10.1109/TIP.2017.2755766
   Luo FL, 2019, IEEE T CYBERNETICS, V49, P2406, DOI 10.1109/TCYB.2018.2810806
   Luo W, 2019, IEEE I CONF COMP VIS, P8241, DOI 10.1109/ICCV.2019.00833
   Penatti OAB, 2014, PATTERN RECOGN, V47, P705, DOI 10.1016/j.patcog.2013.08.012
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Qiao Y, 2018, INT POW ELEC APPLICA, P420
   Spampinato C, 2016, MULTIMED TOOLS APPL, V75, P1701, DOI 10.1007/s11042-015-2601-x
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   Wang DQ, 2015, IEEE I CONF COMP VIS, P2399, DOI 10.1109/ICCV.2015.276
   Wang YM, 2018, PROC CVPR IEEE, P4148, DOI 10.1109/CVPR.2018.00436
   Wang YM, 2016, PROC CVPR IEEE, P1163, DOI 10.1109/CVPR.2016.131
   Wei Di, 2013, 2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P8, DOI 10.1109/CVPRW.2013.6
   Wei XS, 2017, IEEE T IMAGE PROCESS, V26, P2868, DOI 10.1109/TIP.2017.2688133
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Xie LX, 2015, IEEE T MULTIMEDIA, V17, P636, DOI 10.1109/TMM.2015.2408566
   Yuan Y, 2016, IMAGE VISION COMPUT, V55, P77, DOI 10.1016/j.imavis.2016.04.001
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang LB, 2019, IEEE I CONF COMP VIS, P8330, DOI 10.1109/ICCV.2019.00842
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang XY, 2016, IEEE T PATTERN ANAL, V38, P1943, DOI 10.1109/TPAMI.2015.2502579
   Zhang XP, 2017, IEEE T MULTIMEDIA, V19, P2736, DOI 10.1109/TMM.2017.2710803
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, P1713, DOI 10.1109/TIP.2016.2531289
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zheng XT, 2020, IEEE T IMAGE PROCESS, V29, P4747, DOI 10.1109/TIP.2020.2972104
   Zheng XT, 2019, IEEE T GEOSCI REMOTE, V57, P4799, DOI 10.1109/TGRS.2019.2893115
   Zhou B., 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.319
   Zhu L, 2015, IEEE T MULTIMEDIA, V17, P981, DOI 10.1109/TMM.2015.2431496
NR 56
TC 15
Z9 15
U1 2
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1187
EP 1199
DI 10.1109/TMM.2020.2993960
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RU3SE
UT WOS:000645068200003
DA 2024-07-18
ER

PT J
AU Wen, GH
   Chang, TY
   Li, HH
   Jiang, LJ
AF Wen, Guihua
   Chang, Tianyuan
   Li, Huihui
   Jiang, Lijun
TI Dynamic Objectives Learning for Facial Expression Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face recognition; Covariance matrices; Feature extraction; Residual
   neural networks; Convolution; Symmetric matrices; Facial expression
   recognition; loss function; dynamic objectives learning; deep neural
   network; prior knowledge
ID MARGIN SOFTMAX; FACE; REPRESENTATION; FEATURES; PATTERN; ROBUST
AB Facial expression recognition has been widely used to solve the problems such as lie detection and human-machine interaction. However, due to the difficulties to control the application environments, current methods have the lower recognition accuracy in practice. This paper proposes a new method for facial expression recognition by considering several aspects. First, human beings are easy to recognize some expressions, while difficult to recognize others. Inspired by this intuition, a new loss function is proposed to enlarge the distances between samples from easily confused categories. Second, human learning is divided into many stages, and the learning objective of each stage is different. Thus, dynamic objectives learning is proposed, where each objective at different stage is defined by the corresponding loss function. In order to better realize the above ideas, a new deep neural network for facial expression recognition is proposed, which integrates the covariance pooling layer and residual network units into the deep convolution neural network so as to better perform dynamic objectives learning. The experimental results on the standard databases verify the effectiveness and the superior performance of our methods.
C1 [Wen, Guihua; Chang, Tianyuan] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Li, Huihui] Guangdong Polytech Normal Univ, Sch Comp Sci, Guangzhou 510665, Guangdong, Peoples R China.
   [Jiang, Lijun] South China Univ Technol, Sch Mat Sci & Engn, Guangzhou 510006, Peoples R China.
C3 South China University of Technology; Guangdong Polytechnic Normal
   University; South China University of Technology
RP Li, HH (corresponding author), Guangdong Polytech Normal Univ, Sch Comp Sci, Guangzhou 510665, Guangdong, Peoples R China.
EM crghwen@scut.edu.cn; tianyuan_chang@163.com; 29777562@qq.com;
   imljiang@scut.edu.cn
OI LI, HUIHUI/0000-0003-0463-8178
FU China National Science Foundation [61273363]; Science and Technology
   Planning Project of Guangdong Province [2015A020217002]; Guangzhou
   Science and Technology Planning Project [201604020179, 201803010088]
FX This work was supported in part by the China National Science Foundation
   under Grant 61273363, in part by the Science and Technology Planning
   Project of Guangdong Province under Grant 2015A020217002, and in part by
   the Guangzhou Science and Technology Planning Project under Grants
   201604020179 and 201803010088. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Jingdong Wang.
CR Acharya Dinesh, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Proceedings, P480, DOI 10.1109/CVPRW.2018.00077
   Acharya D, 2018, IEEE COMPUT SOC CONF, P480, DOI 10.1109/CVPRW.2018.00077
   Agrawal A, 2020, VISUAL COMPUT, V36, P405, DOI 10.1007/s00371-019-01630-9
   [Anonymous], 2013, ARXIV13060239
   Ben Amor B, 2014, IEEE T CYBERNETICS, V44, P2443, DOI 10.1109/TCYB.2014.2308091
   Berretti S, 2011, VISUAL COMPUT, V27, P1021, DOI 10.1007/s00371-011-0611-x
   Chang TY, 2019, APPL INTELL, V49, P4319, DOI 10.1007/s10489-019-01491-8
   Chen H, 2018, 2018 13TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA), P407, DOI 10.1109/WCICA.2018.8630711
   Chen JY, 2018, MULTIMED TOOLS APPL, V77, P29871, DOI 10.1007/s11042-018-5909-5
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Connie Tee, 2017, Multi-disciplinary Trends in Artificial Intelligence. 11th International Workshop, MIWAI 2017. Proceedings: LNAI 10607, P139, DOI 10.1007/978-3-319-69456-6_12
   Dahmane Mohamed, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P884, DOI 10.1109/FG.2011.5771368
   Dapogny A, 2019, IEEE T AFFECT COMPUT, V10, P167, DOI 10.1109/TAFFC.2017.2708106
   Deng J, 2019, IEEE ACCESS, V7, P9848, DOI 10.1109/ACCESS.2019.2891668
   Dhall A, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P423, DOI 10.1145/2818346.2829994
   Ding H, 2017, IEEE INT CONF AUTOMA, P118, DOI 10.1109/FG.2017.23
   Sang DV, 2017, INT CONF KNOWL SYS, P130, DOI 10.1109/KSE.2017.8119447
   Fan YR, 2018, LECT NOTES COMPUT SC, V11139, P84, DOI 10.1007/978-3-030-01418-6_9
   Ghimire D, 2017, MULTIMED TOOLS APPL, V76, P7803, DOI 10.1007/s11042-016-3418-y
   Goodfellow IJ, 2015, NEURAL NETWORKS, V64, P59, DOI 10.1016/j.neunet.2014.09.005
   Gu WF, 2012, PATTERN RECOGN, V45, P80, DOI 10.1016/j.patcog.2011.05.006
   Gui LK, 2017, IEEE INT CONF AUTOMA, P505, DOI 10.1109/FG.2017.68
   Guo Y., 2016, P IEEE INT C MULT EX, P226
   Gupta O, 2019, IEEE T AFFECT COMPUT, V10, P290, DOI 10.1109/TAFFC.2017.2713355
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ng HW, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P443, DOI 10.1145/2818346.2830593
   Jaffar MA, 2017, INT J ADV COMPUT SC, V8, P449
   Khan S, 2020, IEEE T AFFECT COMPUT, V11, P348, DOI 10.1109/TAFFC.2017.2780838
   Kim BK, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P427, DOI 10.1145/2818346.2830590
   Kim DH, 2019, IEEE T AFFECT COMPUT, V10, P223, DOI 10.1109/TAFFC.2017.2695999
   Li DY, 2019, KNOWL INF SYST, V59, P219, DOI 10.1007/s10115-018-1176-z
   Li DY, 2018, MULTIMED TOOLS APPL, V77, P15251, DOI 10.1007/s11042-017-5105-z
   Li HH, 2019, APPL INTELL, V49, P2956, DOI 10.1007/s10489-019-01427-2
   Li J., 2015, PROC IEEE INT C IMAG, P1
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Li WJ, 2018, IEEE INT CONF AUTOMA, P24, DOI 10.1109/FG.2018.00014
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P2439, DOI 10.1109/TIP.2018.2886767
   Lin F, 2018, IEEE IMAGE PROC, P1957, DOI 10.1109/ICIP.2018.8451039
   Liu K, 2016, 2016 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P163, DOI 10.1109/CW.2016.34
   Liu MY, 2015, NEUROCOMPUTING, V159, P126, DOI 10.1016/j.neucom.2015.02.011
   Liu WY, 2016, PR MACH LEARN RES, V48
   Liu Y, 2017, CHIN CONT DECIS CONF, P713, DOI 10.1109/CCDC.2017.7978485
   Liu YY, 2018, IEEE INT CONF AUTOMA, P458, DOI 10.1109/FG.2018.00074
   Lopes AT, 2017, PATTERN RECOGN, V61, P610, DOI 10.1016/j.patcog.2016.07.026
   Mehrabian A., 2008, COMMUN THEORY, V51, P193
   Mollahosseini A, 2016, IEEE WINT CONF APPL
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Pan HY, 2018, PROC CVPR IEEE, P5285, DOI 10.1109/CVPR.2018.00554
   Rivera AR, 2013, IEEE T IMAGE PROCESS, V22, P1740, DOI 10.1109/TIP.2012.2235848
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Savran A, 2015, IEEE T CYBERNETICS, V45, P1927, DOI 10.1109/TCYB.2014.2362101
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shi W., 2016, PROC INT JOINT C ART, P2004
   Shi WW, 2019, IEEE T NEUR NET LEAR, V30, P683, DOI 10.1109/TNNLS.2018.2852721
   Shi WW, 2018, IEEE T NEUR NET LEAR, V29, P2872, DOI 10.1109/TNNLS.2017.2705682
   Shi WW, 2018, IEEE T NEUR NET LEAR, V29, P2896, DOI 10.1109/TNNLS.2017.2705222
   Simonyan K., 2014, 14091556 ARXIV
   Sun YX, 2017, NEUROCOMPUTING, V230, P397, DOI 10.1016/j.neucom.2016.12.043
   Tang Y., 2015, ARXIV13060239
   Tong Y, 2007, IEEE T PATTERN ANAL, V29, P1683, DOI 10.1109/TPAMI.2007.1094
   Vielzeuf V, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P589, DOI 10.1145/3242969.3264980
   Wan WT, 2018, PROC CVPR IEEE, P9117, DOI 10.1109/CVPR.2018.00950
   Wang F, 2018, IEEE SIGNAL PROC LET, V25, P926, DOI 10.1109/LSP.2018.2822810
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang J, 2017, IEEE INFOCOM SER
   Wang Z, 2016, NEUROCOMPUTING, V174, P756, DOI 10.1016/j.neucom.2015.09.083
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Xu M, 2015, 2015 11TH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION (ICNC), P702, DOI 10.1109/ICNC.2015.7378076
   Yao YQ, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3131345
   Yu Kaicheng, 2017, 2 ORDER CONVOLUTIONA
   Yu ZD, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P435
   Zagoruyko S., 2016, P BRIT MACH VIS C, P513
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhang WC, 2005, IEEE I CONF COMP VIS, P786
   Zhang Zhang X. X., 2017, IEEE I CONF COMP VIS, P5409, DOI DOI 10.1109/ICCV.2017.578
   Zhen QK, 2016, IEEE T MULTIMEDIA, V18, P1438, DOI 10.1109/TMM.2016.2557063
NR 77
TC 25
Z9 25
U1 2
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2020
VL 22
IS 11
BP 2914
EP 2925
DI 10.1109/TMM.2020.2966858
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OJ8YR
UT WOS:000584239900012
DA 2024-07-18
ER

PT J
AU Chen, PH
   Gan, C
   Shen, GY
   Huang, WB
   Zeng, RH
   Tan, MK
AF Chen, Peihao
   Gan, Chuang
   Shen, Guangyao
   Huang, Wenbing
   Zeng, Runhao
   Tan, Mingkui
TI Relation Attention for Temporal Action Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Temporal action localization; relation attention
ID ACTION RECOGNITION; OBJECT; VIDEO
AB Temporal action localization aims to accurately localize and recognize all possible action instances from an untrimmed video automatically. Most existing methods perform this task by first generating a set of proposals and then recognizing each independently. However, due to the complex structures and large content variations in action instances, recognizing them individually can be difficult. Fortunately, some proposals often share information regarding one specific action. Such information, which is ignored in existing methods, can be used to boost recognition performance. In this paper, we propose a novel mechanism, called relation attention, to exploit informative relations among proposals based on their appearance or optical flowfeatures. Specifically, we propose a relation attention module to enhance representation power by capturing useful information from other proposals. This module does not change the dimensions of the original input and output and does not rely on any specific proposal generation methods or feature extraction backbone networks. Experimental results show that the proposed relation attention mechanism improves performance significantly on both Thumos14 and ActivityNet1.3 datasets compared to existing architectures. For example, relying on Structured Segment Networks (SSN), the proposed relation attention module helps to increase the mAP from 41.4 to 43.7 on the Thumos14 dataset and outperforms the state-of-the-art results.
C1 [Chen, Peihao; Zeng, Runhao; Tan, Mingkui] South China Univ Technol, Sch Software Engn, Guangzhou 510630, Peoples R China.
   [Chen, Peihao] Peng Cheng Lab, Shenzhen 518000, Peoples R China.
   [Gan, Chuang] MIT, IBM Watson AI Lab, Cambridge, MA 02142 USA.
   [Shen, Guangyao; Huang, Wenbing] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
C3 South China University of Technology; Peng Cheng Laboratory;
   Massachusetts Institute of Technology (MIT); Tsinghua University
RP Tan, MK (corresponding author), South China Univ Technol, Sch Software Engn, Guangzhou 510630, Peoples R China.
EM phchencs@gmail.com; ganchuang1990@gmail.com; thusgy2012@gmail.com;
   hwenbing@126.com; runhaozeng.cs@gmail.com; mingkuitan@scut.edu.cn
RI Huang, Wenbing/AHB-1846-2022; Huang, Wenbing/AAI-7943-2021
OI Huang, Wenbing/0000-0002-2566-4159; Huang, Wenbing/0000-0002-2566-4159;
   Gan, Chuang/0000-0003-4031-5886; Shen, Guangyao/0000-0001-7530-332X;
   Chen, Peihao/0000-0002-6847-1621; Zeng, Runhao/0000-0001-8694-4245
FU Guangdong Provincial Scientific and Technological Funds
   [2018B010107001]; key project of NSFC [61836003]; National Natural
   Science Foundation of China (NSFC) [61602185]; Program for Guangdong
   Introducing Innovative and Entrepreneurial Teams [2017ZT07X183]; Tencent
   AI Lab Rhino-Bird Focused Research Program [JR201902]; Fundamental
   Research Funds for the Central Universities [D2191240]; Open Project of
   State Key Laboratory of Subtropical Building Science for International
   Cooperation Research [2019ZA01]
FX This work was partially supported by Guangdong Provincial Scientific and
   Technological Funds under Grant 2018B010107001, key project of NSFC (No.
   61836003), National Natural Science Foundation of China (NSFC) 61602185,
   Program for Guangdong Introducing Innovative and Entrepreneurial Teams
   2017ZT07X183, Tencent AI Lab Rhino-Bird Focused Research Program (No.
   JR201902), Fundamental Research Funds for the Central Universities
   D2191240, Open Project of State Key Laboratory of Subtropical Building
   Science for International Cooperation Research (No. 2019ZA01).
CR Boutell MR, 2007, IEEE T MULTIMEDIA, V9, P136, DOI 10.1109/TMM.2006.886372
   Buch S., 2017, BMVC
   Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Cao JZ, 2018, PR MACH LEARN RES, V80
   Cao Jiezhang, 2019, Advances in Neural Information Processing Systems, P1774
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Chen BH, 2014, IEEE T MULTIMEDIA, V16, P837, DOI 10.1109/TMM.2014.2298377
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Chen XL, 2017, IEEE I CONF COMP VIS, P4106, DOI 10.1109/ICCV.2017.440
   Choi MJ, 2012, IEEE T PATTERN ANAL, V34, P240, DOI 10.1109/TPAMI.2011.119
   Cristani M, 2007, IEEE T MULTIMEDIA, V9, P257, DOI 10.1109/TMM.2006.886263
   Dai XY, 2017, IEEE I CONF COMP VIS, P5727, DOI 10.1109/ICCV.2017.610
   Deng CR, 2018, PROC CVPR IEEE, P7746, DOI 10.1109/CVPR.2018.00808
   Divvala SK, 2009, PROC CVPR IEEE, P1271, DOI 10.1109/CVPRW.2009.5206532
   Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gan C, 2019, IEEE I CONF COMP VIS, P7052, DOI 10.1109/ICCV.2019.00715
   Gao J., 2017, P BRIT MACH VIS C
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352
   Guo Y, 2018, AAAI CONF ARTIF INTE, P3134
   Hariharakrishnan K, 2005, IEEE T MULTIMEDIA, V7, P853, DOI 10.1109/TMM.2005.854437
   Hou R., 2017, P BRIT MACH VIS C
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang D., 2020, P ASS ADV ART INT
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jiang R, 2012, KALLIKREIN-RELATED PEPTIDASES, VOL 2: NOVEL CANCER-RELATED BIOMARKERS, P45
   Jiang Y., 2014, ECCV WORKSH
   Lea C, 2017, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2017.113
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Ren S., 2017, IEEETransactionsonPatternAnalysisandMachineIntelligence, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sabirin H, 2012, IEEE T MULTIMEDIA, V14, P657, DOI 10.1109/TMM.2012.2187777
   Shen GY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2505, DOI 10.1145/3343031.3350981
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Stewart R, 2016, PROC CVPR IEEE, P2325, DOI 10.1109/CVPR.2016.255
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CB, 2019, P INT COMP SOFTW APP, P730, DOI 10.1109/COMPSAC.2019.00109
   Wang S, 2014, IEEE T MULTIMEDIA, V16, P289, DOI 10.1109/TMM.2013.2293060
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wu CY, 2019, PROC CVPR IEEE, P284, DOI 10.1109/CVPR.2019.00037
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Xu MZ, 2019, IEEE I CONF COMP VIS, P5531, DOI 10.1109/ICCV.2019.00563
   Yuan J, 2016, PROC CVPR IEEE, P3093, DOI 10.1109/CVPR.2016.337
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Yuan ZH, 2017, PROC CVPR IEEE, P3215, DOI 10.1109/CVPR.2017.342
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zeng RH, 2019, IEEE T IMAGE PROCESS, V28, P5797, DOI 10.1109/TIP.2019.2922108
   Zhang W, 2010, IEEE T MULTIMEDIA, V12, P300, DOI 10.1109/TMM.2010.2047607
   Zhang Y., 2019, P MED IM MEETS NEURI
   Zhang YB, 2019, I IEEE EMBS C NEUR E, P360, DOI [10.1109/NER.2019.8717177, 10.1109/ner.2019.8717177]
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhuang ZW, 2018, ADV NEUR IN, V31
NR 65
TC 47
Z9 51
U1 4
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2020
VL 22
IS 10
BP 2723
EP 2733
DI 10.1109/TMM.2019.2959977
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NT0FL
UT WOS:000572628000016
DA 2024-07-18
ER

PT J
AU Soltanian, M
   Amini, S
   Ghaemmaghami, S
AF Soltanian, Mohammad
   Amini, Sajjad
   Ghaemmaghami, Shahrokh
TI Spatio-Temporal VLAD Encoding of Visual Events Using Temporal Ordering
   of the Mid-Level Deep Semantics
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Encoding; Visualization; Semantics; Task analysis; Convex functions;
   Principal component analysis; Training; Convolutional neural network;
   Columbia consumer video (CCV); unstructured social activity attribute
   (USAA); FCVID; YouTube-8M; kinetics vector of locally aggregated
   descriptors; alternating direction method of multipliers; projected
   gradient descent; support vector machine
ID ACTION RECOGNITION; LATE FUSION; FEATURES; VECTOR; IMAGES; VIDEOS
AB Classification of video events based on frame-level descriptors is a common approach to video recognition. In the meanwhile, proper encoding of the frame-level descriptors is vital to the whole event classification procedure. While there are some pretty efficient video descriptor encoding methods, temporal ordering of the descriptors is often ignored in these encoding algorithms. In this paper, we show that by taking into account the temporal inter-frame dependencies and tracking the chronological order of video sub-events, accuracy of event recognition is further improved. First, the frame-level descriptors are extracted using convolutional neural networks (CNNs) pre-trained on ImageNet, which are fine-tuned on a portion of training video frames. Then, a spatio-temporal encoding is applied to the derived descriptors. The proposed spatio-temporal encoding, as the main contribution of this work, is inspired from the well-known vector of locally aggregated descriptors (VLAD) encoding in spatial domain and from total variation de-noising (TVD) in temporal domain. The proposed unified spatio-temporal encoding is then shown to be in the form of a convex optimization problem which is solved efficiently with alternating direction method of multipliers (ADMM) algorithm. The experimental results show superiority of the proposed encoding method in terms of recognition accuracy over both frame-level video encoding approaches and spatio-temporal video representations. As compared to the state-of-the-art approaches, our encoding method improves the mean average precision (mAP) over both Columbia consumer video (CCV), unstructured social activity attribute (USAA), YouTube-8M, and Kinetics datasets and is very competitive on FCVID dataset.
C1 [Soltanian, Mohammad; Amini, Sajjad; Ghaemmaghami, Shahrokh] Sharif Univ Technol, Elect Engn Dept, Tehran 22222, Iran.
   [Soltanian, Mohammad; Amini, Sajjad; Ghaemmaghami, Shahrokh] Sharif Univ Technol, Elect Res Inst, Tehran 22222, Iran.
C3 Sharif University of Technology; Sharif University of Technology
RP Ghaemmaghami, S (corresponding author), Sharif Univ Technol, Elect Engn Dept, Tehran 22222, Iran.; Ghaemmaghami, S (corresponding author), Sharif Univ Technol, Elect Res Inst, Tehran 22222, Iran.
EM soltanian_m@ee.sharif.edu; sa1368_elec@yahoo.com; ghaemmag@sharif.edu
RI Amini, Sajjad/AAX-4276-2020
OI Amini, Sajjad/0000-0002-0322-9324
CR Abbas A., 2015, 2015 IEEE 42nd Photovoltaic Specialist Conference (PVSC). Proceedings, P1, DOI 10.1109/PVSC.2015.7356441
   Abu-El-Haija Sami, 2016, arXiv
   Aly R., 2013, NIST TRECVID VID RET
   Amato G., 2013, P 10 INT WORKSH LARG, P44
   [Anonymous], 2017, ARXIV170704555
   [Anonymous], 2015, MATLAB VIDEOUTILS
   [Anonymous], 2013, P 21 ACM INT C MULT, DOI 10.1145/2502081.2502171
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Arandjelovic R, 2013, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR.2013.207
   Ballan L, 2011, MULTIMED TOOLS APPL, V51, P279, DOI 10.1007/s11042-010-0643-7
   Boureau Y. L., 2010, P 27 INT C MACH LEAR, P111
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Cai JJ, 2016, IEEE IMAGE PROC, P4155, DOI 10.1109/ICIP.2016.7533142
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen D, 2013, SIGNAL PROCESS, V93, P2316, DOI 10.1016/j.sigpro.2012.06.005
   Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   de Campos T, 2012, COMPUT VIS IMAGE UND, V116, P68, DOI 10.1016/j.cviu.2011.07.011
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Douze M, 2010, LECT NOTES COMPUT SC, V6311, P522, DOI 10.1007/978-3-642-15549-9_38
   Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114
   Duta IC, 2017, LECT NOTES COMPUT SC, V10132, P365, DOI 10.1007/978-3-319-51811-4_30
   Eggert C, 2014, IEEE IMAGE PROC, P3018, DOI 10.1109/ICIP.2014.7025610
   Fu YW, 2012, LECT NOTES COMPUT SC, V7575, P530, DOI 10.1007/978-3-642-33765-9_38
   Fu YW, 2014, IEEE T PATTERN ANAL, V36, P303, DOI 10.1109/TPAMI.2013.128
   Gangopadhyay A., 2015, P CORR
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Girdhar R., 2017, PROC CVPR IEEE, P971, DOI DOI 10.1109/CVPR.2017.337
   GOLDSTEIN AA, 1964, B AM MATH SOC, V70, P709, DOI 10.1090/S0002-9904-1964-11178-2
   Hamner B., 2019, MACHINE LEARNING EVA
   Han YH, 2014, IEEE T MULTIMEDIA, V16, P1665, DOI 10.1109/TMM.2014.2321530
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Imura J., 2011, P 19 ACM INT C MULT, P1085
   Inoue N, 2012, IEEE T MULTIMEDIA, V14, P1196, DOI 10.1109/TMM.2012.2191395
   Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jégou H, 2012, LECT NOTES COMPUT SC, V7573, P774, DOI 10.1007/978-3-642-33709-3_55
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jhuo IH, 2014, MACH VISION APPL, V25, P33, DOI 10.1007/s00138-013-0567-0
   Jiang Y. -G., 2011, P 1 ACM INT C MULT R, P29
   Jiang Y.-G., 2012, P 20 ACM INT C MULT, P1347
   Jiang YG, 2018, IEEE T PATTERN ANAL, V40, P352, DOI 10.1109/TPAMI.2017.2670560
   Jiang YG, 2015, IEEE T MULTIMEDIA, V17, P1174, DOI 10.1109/TMM.2015.2436813
   Kang SH, 2018, LECT NOTES COMPUT SC, V11218, P402, DOI 10.1007/978-3-030-01264-9_24
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kay W., 2017, ARXIV170506950
   Klaser A., 2008, P BMVC, P275, DOI DOI 10.5244/C.22.99
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai KT, 2014, PROC CVPR IEEE, P2251, DOI 10.1109/CVPR.2014.288
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li WH, 2018, IEEE ACCESS, V6, P44211, DOI 10.1109/ACCESS.2018.2863943
   Li XR, 2012, IEEE T MULTIMEDIA, V14, P1091, DOI 10.1109/TMM.2012.2191943
   Li YS, 2017, IEEE ACCESS, V5, P10323, DOI 10.1109/ACCESS.2017.2712789
   Lin RC, 2019, LECT NOTES COMPUT SC, V11132, P206, DOI 10.1007/978-3-030-11018-5_19
   Liu D, 2013, PROC CVPR IEEE, P803, DOI 10.1109/CVPR.2013.109
   Liu J., 2016, ARXIV161102644, P1, DOI DOI 10.5244/C.30.73
   Liu KH, 2008, IEEE T MULTIMEDIA, V10, P240, DOI 10.1109/TMM.2007.911826
   Liu LQ, 2011, IEEE I CONF COMP VIS, P2486, DOI 10.1109/ICCV.2011.6126534
   Long X, 2018, PROC CVPR IEEE, P7834, DOI 10.1109/CVPR.2018.00817
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo MN, 2018, IEEE T CYBERNETICS, V48, P648, DOI 10.1109/TCYB.2017.2647904
   Ma AJ, 2014, INT J COMPUT VISION, V109, P233, DOI 10.1007/s11263-014-0723-7
   Ma Z, 2013, IEEE T MULTIMEDIA, V15, P1628, DOI 10.1109/TMM.2013.2264928
   Markatopoulou F., 2013, P TRECVID WORKSH, P12
   Mazloom M, 2014, IEEE T MULTIMEDIA, V16, P2214, DOI 10.1109/TMM.2014.2359771
   Miech A., 2017, ARXIV PREPRINT ARXIV
   Moldovan AC, 2019, IEEE INT ULTRA SYM, P2056, DOI [10.1109/ultsym.2019.8925549, 10.1109/ULTSYM.2019.8925549]
   Murray N, 2014, PROC CVPR IEEE, P2473, DOI 10.1109/CVPR.2014.317
   Negrel R, 2012, IEEE IMAGE PROC, P2425, DOI 10.1109/ICIP.2012.6467387
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Oneata D, 2013, IEEE I CONF COMP VIS, P1817, DOI 10.1109/ICCV.2013.228
   Ostyakov P., 2018, P EUR C COMP VIS, P10
   Pei WJ, 2017, PROC CVPR IEEE, P820, DOI 10.1109/CVPR.2017.94
   Peng X., 2015, P THUMOS CHALL WORKS, P4321
   Peng XJ, 2014, LECT NOTES COMPUT SC, V8691, P660, DOI 10.1007/978-3-319-10578-9_43
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Perronnin F, 2007, PROC CVPR IEEE, P2272
   Picard D, 2011, IEEE IMAGE PROC, P669, DOI 10.1109/ICIP.2011.6116641
   Ramanathan V, 2016, PROC CVPR IEEE, P3043, DOI 10.1109/CVPR.2016.332
   Reddy M., 2013, Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG), Fourth National Conference on, P1
   Reddy M. K., 2014, POW EN SYST C SUST E, P1, DOI DOI 10.1109/PESTSE.2014.6805265
   Revaud J, 2013, PROC CVPR IEEE, P2459, DOI 10.1109/CVPR.2013.318
   Scovanner P., 2007, P 15 ACM INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Sheng BY, 2018, IEEE T CIRC SYST VID, V28, P1788, DOI 10.1109/TCSVT.2016.2637379
   Sheng BY, 2015, CAN CON EL COMP EN, P620, DOI 10.1109/CCECE.2015.7129346
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Shyu ML, 2008, IEEE T MULTIMEDIA, V10, P252, DOI 10.1109/TMM.2007.911830
   Simonyan K., 2015, P INT C LEARN REPR, P76
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Skalic M., 2018, P 2 WORKSH YOUTUBE 8, P297
   Soltanian M, 2019, IEEE T MULTIMEDIA, V21, P157, DOI 10.1109/TMM.2018.2844101
   Spyromitros-Xioufis E, 2014, IEEE T MULTIMEDIA, V16, P1713, DOI 10.1109/TMM.2014.2329648
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Suha Kwak, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3345, DOI 10.1109/CVPR.2011.5995435
   Sun C, 2013, IEEE WORK APP COMP, P15, DOI 10.1109/WACV.2013.6474994
   Sun JJ, 2016, AER ADV ENG RES, V68, P10
   Sylvester J., 1884, CR HEBD ACAD SCI, V99, P67
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tang Y., 2013, DEEP LEARNING USING
   Tolias G, 2013, IEEE I CONF COMP VIS, P1401, DOI 10.1109/ICCV.2013.177
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Tseng VS, 2008, IEEE T MULTIMEDIA, V10, P260, DOI 10.1109/TMM.2007.911832
   Tu ZG, 2019, IEEE T IMAGE PROCESS, V28, P2799, DOI 10.1109/TIP.2018.2890749
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   Vedaldi A., 2010, Proceedings of the international conference on Multimedia - MM'10, P1469
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang H.D., 2017, ARXIV170605150
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang L, 2017, SIGNAL PROCESS, V140, P45, DOI 10.1016/j.sigpro.2017.05.005
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang XZ, 2014, INT SYM COMPUT INTEL, P185, DOI 10.1109/ISCID.2014.238
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wang Z, 2016, INT CONF ACOUST SPEE, P1258, DOI 10.1109/ICASSP.2016.7471878
   Wu ZX, 2016, PROC CVPR IEEE, P3112, DOI 10.1109/CVPR.2016.339
   Wu ZX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P167, DOI 10.1145/2647868.2654931
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Xu ZW, 2013, IEEE I CONF COMP VIS, P3440, DOI 10.1109/ICCV.2013.427
   Yang K, 2013, INT CONF MEAS, P9, DOI 10.1109/ICMTMA.2013.13
   Ye GN, 2012, PROC CVPR IEEE, P3021, DOI 10.1109/CVPR.2012.6248032
   Zha Shengxin., 2015, BMVC
   Zhang JX, 2019, IEEE ACCESS, V7, P15222, DOI 10.1109/ACCESS.2019.2895472
   Zhang K, 2016, LECT NOTES COMPUT SC, V9911, P766, DOI 10.1007/978-3-319-46478-7_47
   Zhang XS, 2016, IEEE T IMAGE PROCESS, V25, P1033, DOI 10.1109/TIP.2015.2511585
   Zhang XS, 2015, IEEE T MULTIMEDIA, V17, P1562, DOI 10.1109/TMM.2015.2449660
   Zhao H, 2013, 2013 INTERNATIONAL CONFERENCE ON CYBER-ENABLED DISTRIBUTED COMPUTING AND KNOWLEDGE DISCOVERY (CYBERC), P109, DOI 10.1109/CyberC.2013.26
   Zhao RW, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231739
   Zhao X, 2012, IEEE IMAGE PROC, P3121, DOI 10.1109/ICIP.2012.6467561
   Zhao ZC, 2016, NEUROCOMPUTING, V208, P378, DOI 10.1016/j.neucom.2016.06.002
   Zheng LG, 2012, IEEE T INF FOREN SEC, V7, P1578, DOI 10.1109/TIFS.2012.2206386
   Zhu WJ, 2016, PROC CVPR IEEE, P1991, DOI 10.1109/CVPR.2016.219
   Zhu Yizhe., 2017, CoRR
NR 137
TC 7
Z9 7
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2020
VL 22
IS 7
BP 1769
EP 1784
DI 10.1109/TMM.2019.2959426
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MG4GJ
UT WOS:000545990500010
DA 2024-07-18
ER

PT J
AU Du, YT
   Wang, X
   Cui, YB
   Wang, H
   Su, C
AF Du, Youtian
   Wang, Xue
   Cui, Yunbo
   Wang, Hang
   Su, Chang
TI Kernel-Based Mixture Mapping for Image and Text Association
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Correlation; Probabilistic logic; Visualization; Data models;
   Analytical models; Optimization; Image-text association; semantic
   correlation; probabilistic mixture model; kernel-based mapping
ID MODEL; PROPAGATION; ANNOTATION; RETRIEVAL
AB Modeling the relationship between multimodal media, including images, videos, and text, can reduce the gap between the modalities and promote cross-media retrieval, image annotation, etc. In this paper, we propose a new approach called kernel-based mixture mapping (KMM) to model the semantic correlations between web images and text. With this approach, we first construct latent high-dimensional feature spaces based on kernel theory to address the nonlinearity of both the data distributions in the input spaces and the cross-model correlation. Second, we present a probabilistic neighborhood model to describe the spatial locality of semantics by assuming that proximate examples in feature spaces generally have the same semantics and a conditional model to describe cross-modal conditional dependency. Finally, we build a probabilistic mixture model to jointly model the spatial locality of semantics and the conditional dependency between different modalities. By combining nonlinear transformation and probabilistic models, KMM can address the nonlinearity of cross-modal correlation, the complexity of semantic distributions at the global scale, and the continuity of semantic distributions at the local scale. We present a hybrid optimization algorithm to find the solution of KMM based on expectation-maximization and subgradient ascent; this algorithm avoids estimating the parameters of KMM in high-dimensional feature space and is proved to converge to an (local) optimal solution. We demonstrate the performance of KMM using four public datasets. The experimental results show that our approach outperforms the compared methods when modeling the relationships between images and text.
C1 [Du, Youtian; Wang, Xue; Cui, Yunbo; Wang, Hang] Xi An Jiao Tong Univ, Key Lab Intelligent Networks & Network Secur, Minist Educ, Xian 710049, Shaanxi, Peoples R China.
   [Su, Chang] Cornell Univ, Weill Cornell Med, Dept Healthcare Policy & Res, New York, NY 10065 USA.
C3 Xi'an Jiaotong University; Cornell University; Weill Cornell Medicine
RP Du, YT (corresponding author), Xi An Jiao Tong Univ, Key Lab Intelligent Networks & Network Secur, Minist Educ, Xian 710049, Shaanxi, Peoples R China.
EM duyt@mail.xjtu.edu.cn; nimowangxue1989@stu.xjtu.edu.cn;
   qq790093890@stu.xjtu.edu.cn; wanghang1128@stu.xjtu.edu.cn;
   chs4001@med.cornell.edu
RI Wang, Hang/HHR-9240-2022
OI Wang, Xue/0000-0002-5719-7794; Du, Youtian/0000-0002-1714-3433
FU National Natural Science Foundation [61375040, 61772415, 61572397];
   Natural Science Basic Research Plan in Shaanxi Province [2019JM424,
   20180509]; Fundamental Research Funds for the Central Universities
   [xjj2017063]
FX This work was supported in part by the National Natural Science
   Foundation (61375040, 61772415, 61572397), in part by Natural Science
   Basic Research Plan in Shaanxi Province (2019JM424), in part by Research
   Fund from Bytedance Company (20180509), and in part by Fundamental
   Research Funds for the Central Universities (xjj2017063), of China. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Wolfgang Hurst.
CR [Anonymous], 2016, P INT C LEARN REPR
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2009, P ACM INT C IM VID R
   Bilmes JA., 1998, INTCOMPUT SCIINST, V4, P126
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142
   Deleforge A, 2015, STAT COMPUT, V25, P893, DOI 10.1007/s11222-014-9461-5
   Du YT, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1259, DOI 10.1145/2733373.2806331
   Eisenschtat Aviv, 2017, P IEEE C COMP VIS PA, P4601
   Engilberge M, 2018, PROC CVPR IEEE, P3984, DOI 10.1109/CVPR.2018.00419
   Feng YS, 2013, IEEE T PATTERN ANAL, V35, P797, DOI 10.1109/TPAMI.2012.118
   Grangier D, 2008, IEEE T PATTERN ANAL, V30, P1371, DOI 10.1109/TPAMI.2007.70791
   Guo HF, 2018, INT J PATTERN RECOGN, V32, DOI 10.1142/S021800141859005X
   Hannah LA, 2011, J MACH LEARN RES, V12, P1923
   He Kaiming, 2016, P INT C COMP VIS PAT, DOI 10.1109/CVPR.2016.90
   He R, 2015, IEEE T IMAGE PROCESS, V24, P5543, DOI 10.1109/TIP.2015.2466106
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Hong RC, 2017, IEEE T IMAGE PROCESS, V26, P4128, DOI 10.1109/TIP.2017.2710635
   Hua Y, 2016, IEEE T MULTIMEDIA, V18, P1201, DOI 10.1109/TMM.2016.2535864
   Jeon J., 2003, P 26 ANN INT ACM SIG
   Jia YQ, 2011, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2011.6126524
   Jiang T, 2009, IEEE T KNOWL DATA EN, V21, P161, DOI 10.1109/TKDE.2008.150
   Jiao Xue, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015. Proceedings: LNCS 8936, P427, DOI 10.1007/978-3-319-14442-9_48
   Karpathy A, 2017, IEEE T PATTERN ANAL, V39, P664, DOI 10.1109/TPAMI.2016.2598339
   Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073
   Lazaridis M, 2013, SIGNAL PROCESS-IMAGE, V28, P351, DOI 10.1016/j.image.2012.04.001
   Li CW, 2012, 2011 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND NETWORK TECHNOLOGY (ICCSNT), VOLS 1-4, P2083
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin WX, 2012, LECT NOTES COMPUT SC, V7131, P740
   Liu D, 2011, IEEE T MULTIMEDIA, V13, P702, DOI 10.1109/TMM.2011.2134078
   Liu Hong, 2016, IJCAI, P1767, DOI DOI 10.1109/TIP.2016.2564638
   Liu Y, 2017, IEEE I CONF COMP VIS, P4127, DOI 10.1109/ICCV.2017.442
   Moghaddam B, 1997, IEEE T PATTERN ANAL, V19, P696, DOI 10.1109/34.598227
   Monay F, 2007, IEEE T PATTERN ANAL, V29, P1802, DOI 10.1109/TPAMI.2007.1097
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Pham AT, 2015, PR MACH LEARN RES, V37, P2427
   Le Q, 2014, PR MACH LEARN RES, V32, P1188
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Tang JH, 2013, IEEE T KNOWL DATA EN, V25, P1510, DOI 10.1109/TKDE.2012.87
   Wand M. P., 1994, KERNEL SMOOTHING, DOI DOI 10.1201/B14876
   Wang B, 2018, AAAI CONF ARTIF INTE, P7380
   Wang JD, 2003, LECT NOTES ARTIF INT, V2842, P159
   Wang LW, 2019, IEEE T PATTERN ANAL, V41, P394, DOI 10.1109/TPAMI.2018.2797921
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang M, 2012, ACM COMPUT SURV, V44, DOI 10.1145/2333112.2333120
   Wu L, 2013, IEEE T PATTERN ANAL, V35, P716, DOI 10.1109/TPAMI.2012.124
   Wu Y, 2000, PROC CVPR IEEE, P222, DOI 10.1109/CVPR.2000.855823
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   You QZ, 2018, PROC CVPR IEEE, P5735, DOI 10.1109/CVPR.2018.00601
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Zhai XH, 2012, LECT NOTES COMPUT SC, V7131, P312
   Zhang H., 2007, P 15 INT C MULTIMEDI, P273
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, P2212, DOI 10.1109/TIP.2015.2419074
   Zhang L, 2017, IEEE T MULTIMEDIA, V19, P1220, DOI 10.1109/TMM.2016.2646219
   Zhang RF, 2005, IEEE I CONF COMP VIS, P846
   Zhang T, 2016, PROC CVPR IEEE, P2036, DOI 10.1109/CVPR.2016.224
   Zhuang YT, 2008, IEEE T MULTIMEDIA, V10, P221, DOI 10.1109/TMM.2007.911822
NR 58
TC 4
Z9 4
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2020
VL 22
IS 2
BP 365
EP 379
DI 10.1109/TMM.2019.2930336
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KH5FZ
UT WOS:000510676300007
DA 2024-07-18
ER

PT J
AU Cubelos, J
   Carballeira, P
   Gutiérrez, J
   García, N
AF Cubelos, Javier
   Carballeira, Pablo
   Gutierrez, Jesus
   Garcia, Narciso
TI QoE Analysis of Dense Multiview Video With Head-Mounted Devices
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multiview video; lightfield; head-mounted device; quality of experience;
   subjective evaluation; smoothness; motion parallax
ID QUALITY
AB This paper presents a system and methodology for the analysis of quality of experience factors for dense multiview (MV) video using a head-mounted device (HMD). An MV-HMD player has been designed and implemented to immerse the users in a virtual environment, where they are placed in front of a virtual lightfield display that shows a different viewpoint depending on the position of their head. This paper describes a methodology for the analysis of the subjective perception of the transition among views (motion parallax), which is specific to the visualization of MV content. While previous works simulated the user movement by predefined view paths or used complex devices to track them, this system allows the observer to move freely, varying the perspective of the scene while easily tracking the observer's position. This paper is, up to our knowledge, the first providing a complete framework for the assessment of this subjective factor using an HMD. The subjective results obtained using this framework are used to 1) assess the influence of the user movement, display settings, and content characteristics in the perception of smoothness in the view transition, and 2) analyze the performance and limitations of a prediction model for subjective smoothness scores.
C1 [Cubelos, Javier; Garcia, Narciso] Univ Politecn Madrid, Grp Tratamiento Imagenes, Informat Proc & Telecommun Ctr, E-28040 Madrid, Spain.
   [Cubelos, Javier; Garcia, Narciso] Univ Politecn Madrid, Escuela Tecn Super Ingenieros Telecomunicac, E-28040 Madrid, Spain.
   [Carballeira, Pablo] Univ Politecn Madrid, Grp Tratamiento Imagenes, E-28040 Madrid, Spain.
   [Carballeira, Pablo] Univ Autonoma Madrid, Video Proc & Understanding Lab, Escuela Politecn Super, E-28049 Madrid, Spain.
   [Gutierrez, Jesus] Univ Nantes, Lab Sci Numer Nantes, Equipe Image Percept & Interact, F-44300 Nantes, France.
C3 Universidad Politecnica de Madrid; Centro de I+D+I en Procesado de la
   Informacion Telecomunicaciones (IPT); Universidad Politecnica de Madrid;
   Universidad Politecnica de Madrid; Autonomous University of Madrid;
   Nantes Universite
RP Carballeira, P (corresponding author), Univ Politecn Madrid, Grp Tratamiento Imagenes, E-28040 Madrid, Spain.; Carballeira, P (corresponding author), Univ Autonoma Madrid, Video Proc & Understanding Lab, Escuela Politecn Super, E-28049 Madrid, Spain.
EM jco@gti.ssr.upm.es; pablo.carballeira@uam.es;
   jesus.gutierrez@univ-nantes.fr; narciso@gti.ssr.upm.es
RI Carballeira, Pablo/I-5983-2019; García, Narciso/E-8603-2011; Gutiérrez,
   Jesús/AAB-8858-2021
OI Carballeira, Pablo/0000-0002-7199-698X; García,
   Narciso/0000-0002-0397-894X; Gutiérrez, Jesús/0000-0001-7878-4712;
   Cubelos Ordas, Javier/0000-0001-7447-7993
FU Ministerio de Ciencia, Innovacion y Universidades (AEI/FEDER) of the
   Spanish Government [TEC2016-75981]; Spanish Administration Agency CDTI
   [IDI-20180015]; EU [PCOFUND-GA-2013-609102]
FX This work was supported in part by the Ministerio de Ciencia, Innovacion
   y Universidades (AEI/FEDER) of the Spanish Government under Project
   TEC2016-75981 (IVME) and in part by the Spanish Administration Agency
   CDTI under Project IDI-20180015 (VINEDO). The work of J. Gutierrez was
   supported by the People Programme (Marie Curie Actions) of the EU's
   FP7/2007-2013 (REA Grant Agreement PCOFUND-GA-2013-609102), through the
   PRESTIGE Programme (by Campus France). The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Chuan Wu.
CR Adhikarla VK, 2017, PROC CVPR IEEE, P3720, DOI 10.1109/CVPR.2017.396
   [Anonymous], 2016, INT C 3D IMAGING IC3
   [Anonymous], 2012, SUBJ METH ASS STER 3
   [Anonymous], 2012, METH SUBJ ASS QUAL T
   Apostolopoulos JG, 2012, P IEEE, V100, P974, DOI 10.1109/JPROC.2011.2182069
   Carballeira P., 2015, 7 INT WORKSH QUAL MU, P1
   Carballeira P, 2017, IEEE J-STSP, V11, P113, DOI 10.1109/JSTSP.2016.2617302
   Chatfield M, 2009, STATA J, V9, P299, DOI 10.1177/1536867X0900900208
   Darukumalli S., 2016, P INT C 3D IM IC3D L, P1
   Dricot A, 2015, SIGNAL PROCESS-IMAGE, V39, P369, DOI 10.1016/j.image.2015.04.012
   Ernst MD, 2004, STAT SCI, V19, P676, DOI 10.1214/088342304000000396
   Gutierrez Jesus, 2015, Journal of Display Technology, V11, P967, DOI 10.1109/JDT.2015.2448758
   Hopf K, 2006, PROC SPIE, V6392, DOI 10.1117/12.685881
   ISO/IEC JTC1/SC29/WG11, 2018, 123 MPEG M LJUBLJ SI
   ISO/IEC JTC1/SC29/WG11, 2015, 112 MPEG M WARS PL J
   ISO/IEC JTC1/SC29WG11, 2019, 125 MPEG M MARR MA J
   ITU-T, 2016, METH SUBJ ASS VID QU
   ITU-T, 2016, SUBJ ASS METH 3D VID
   Janowski L, 2015, IEEE T MULTIMEDIA, V17, P2210, DOI 10.1109/TMM.2015.2484963
   Kim T, 2014, IEEE T MULTIMEDIA, V16, P387, DOI 10.1109/TMM.2013.2292592
   Kovacs P. T., 2015, 112 MPEG M WARS POL
   Kurutepe E, 2007, IEEE T CIRC SYST VID, V17, P1558, DOI 10.1109/TCSVT.2007.903664
   Lebreton P, 2014, T-LAB SER TELECOMMUN, P299, DOI 10.1007/978-3-319-02681-7_20
   Park MC, 2015, J DISP TECHNOL, V11, P877, DOI 10.1109/JDT.2015.2389212
   Reichelt S, 2010, PROC SPIE, V7690, DOI 10.1117/12.850094
   Ribeiro FML, 2018, IEEE T MULTIMEDIA, V20, P1, DOI 10.1109/TMM.2017.2714425
   Runde D, 2000, IEEE T CIRC SYST VID, V10, P376, DOI 10.1109/76.836282
   Senoh T., 2016, 116 MPEG M CHENGD CH
   Serrano A, 2019, IEEE T VIS COMPUT GR, V25, P1817, DOI 10.1109/TVCG.2019.2898757
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Son JY, 2017, P IEEE, V105, P789, DOI 10.1109/JPROC.2017.2666538
   Speranza F, 2005, P SOC PHOTO-OPT INS, V5664, P72, DOI 10.1117/12.587170
   Takaki Y., 2014, ITE Transactions on Media Technology and Applications, V2, P8
   Tamboli RR, 2016, SIGNAL PROCESS-IMAGE, V47, P42, DOI 10.1016/j.image.2016.05.010
   Tanimoto M., 2008, 84 MPEG M ARCH FRANC
   Thatte J., 2018, ELECT IMAG, V2018, P1
   Upenik E, 2018, EUR SIGNAL PR CONF, P246, DOI 10.23919/EUSIPCO.2018.8553424
   Wu GC, 2017, IEEE J-STSP, V11, P926, DOI 10.1109/JSTSP.2017.2747126
NR 38
TC 8
Z9 8
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2020
VL 22
IS 1
BP 69
EP 81
DI 10.1109/TMM.2019.2924575
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KB6AZ
UT WOS:000506577000008
DA 2024-07-18
ER

PT J
AU Hao, YB
   Ngo, CW
   Huet, B
AF Hao, Yanbin
   Ngo, Chong-Wah
   Huet, Benoit
TI Neighbourhood Structure Preserving Cross-Modal Embedding for Video
   Hyperlinking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Visualization; Joining processes; Gallium nitride;
   Benchmark testing; Feature extraction; Neural networks; Video
   hyperlinking; cross-modal translation; structure-preserving learning
ID RETRIEVAL; LOCALIZATION; TEXT
AB Video hyperlinking is a task aiming to enhance the accessibility of large archives, by establishing links between fragments of videos. The links model the aboutness between fragments for efficient traversal of video content. This paper addresses the problem of link construction from the perspective of cross-modal embedding. To this end, a generalized multi-modal auto-encoder is proposed.& x00A0;The encoder learns two embeddings from visual and speech modalities, respectively, whereas each of the embeddings performs self-modal and cross-modal translation of modalities. Furthermore, to preserve the neighbourhood structure of fragments, which is important for video hyperlinking, the auto-encoder is devised to model data distribution of fragments in a dataset. Experiments are conducted on Blip10000 dataset using the anchor fragments provided by TRECVid Video Hyperlinking (LNK) task over the years of 2016 and 2017. This paper shares the empirical insights on a number of issues in cross-modal learning, including the preservation of neighbourhood structure in embedding, model fine-tuning and issue of missing modality, for video hyperlinking.
C1 [Hao, Yanbin; Ngo, Chong-Wah] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Huet, Benoit] EURECOM, Data Sci Dept, F-06904 Sophia Antipolis, France.
C3 City University of Hong Kong; IMT - Institut Mines-Telecom; EURECOM
RP Hao, YB (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
EM haoyanbin@hotmail.com; cwngo@cs.cityu.edu.hk; benoit.huet@eurecom.fr
RI Hao, Yanbin/AAC-8050-2019
OI Hao, Yanbin/0000-0002-0695-1566; Huet, Benoit/0000-0002-0608-6939
FU Research Grants Council (RGC) of the Hong Kong Special Administrative
   Region, China [CityU 11250716]; PROCORE-France/Hong Kong Joint Research
   Scheme - RGC of Hong Kong; Consulate General of France in Hong Kong
   [F-CityU104/17]
FX This work was supported in part by a grant from the Research Grants
   Council (RGC) of the Hong Kong Special Administrative Region, China
   (CityU 11250716) and in part by a grant from PROCORE-France/Hong Kong
   Joint Research Scheme sponsored by the RGC of Hong Kong and the
   Consulate General of France in Hong Kong (F-CityU104/17). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. MengWang.
CR [Anonymous], 2014, CEUR WORKSHOP P
   [Anonymous], P MEDIAEVAL WORKSH
   [Anonymous], 2013, INT C MACHINE LEARNI
   [Anonymous], 2014, P TREC VID RETR EV
   [Anonymous], 2011, P ICML
   [Anonymous], ARXIV151106238
   [Anonymous], 2015, P 3 EDITION WORKSHOP
   [Anonymous], P TRECVIDEO RETR EV
   [Anonymous], P MEDIAEVAL WORKSH
   [Anonymous], 2013, MMSYS
   [Anonymous], P TREC VID RETR EV
   [Anonymous], P TREC VID RETR EV
   [Anonymous], P TRECVID 21 INT WOR
   [Anonymous], 2013, WWW
   [Anonymous], 2016, P TRECVID 2016 WORKS
   [Anonymous], 2015, SLAM
   [Anonymous], P TREC 20 INT WORKH
   Cheng Z, 2017, INT SYM COMPUT INTEL, P284, DOI 10.1109/ISCID.2017.95
   Chou CL, 2015, IEEE T MULTIMEDIA, V17, P382, DOI 10.1109/TMM.2015.2391674
   Demirtas M., 2017, P 25 SIGN PROC COMM, P1, DOI [DOI 10.1109/SIU.2017.7960421, 10.1109/SIU.2017.7960421]
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Hao YB, 2020, IEEE T KNOWL DATA EN, V32, P1909, DOI 10.1109/TKDE.2019.2913379
   Hao YB, 2017, IEEE T IMAGE PROCESS, V26, P5531, DOI 10.1109/TIP.2017.2737329
   Hao YB, 2017, IEEE T MULTIMEDIA, V19, P1, DOI 10.1109/TMM.2016.2610324
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Henning C, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P14, DOI 10.1145/3078971.3078991
   Hong RC, 2017, IEEE T IMAGE PROCESS, V26, P4128, DOI 10.1109/TIP.2017.2710635
   Hong RC, 2016, IEEE T IMAGE PROCESS, V25, P5814, DOI 10.1109/TIP.2016.2614132
   Kojima A, 2002, INT J COMPUT VISION, V50, P171, DOI 10.1023/A:1020346032608
   Lamel L, 2008, LECT NOTES ARTIF INT, V5221, P4
   Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750
   Li ZC, 2015, IEEE T MULTIMEDIA, V17, P1989, DOI 10.1109/TMM.2015.2477035
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1234, DOI 10.1109/TMM.2016.2646180
   Luo CZ, 2016, IEEE T MULTIMEDIA, V18, P40, DOI 10.1109/TMM.2015.2495248
   Mettes P, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P175, DOI 10.1145/2911996.2912036
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Mu TT, 2018, IEEE T PATTERN ANAL, V40, P1323, DOI 10.1109/TPAMI.2017.2715806
   Ordelman RJF, 2015, WWW'15 COMPANION: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P727, DOI 10.1145/2740908.2742915
   Paccanaro A, 2002, ADV NEUR IN, V14, P857
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   SALTON G, 1991, SCIENCE, V253, P974, DOI 10.1126/science.253.5023.974
   Salvador A, 2017, PROC CVPR IEEE, P3068, DOI 10.1109/CVPR.2017.327
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Tang JH, 2015, IEEE T IMAGE PROCESS, V24, P2827, DOI 10.1109/TIP.2015.2421443
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Venna J, 2010, J MACH LEARN RES, V11, P451
   Vukotic V., 2016, P 2016 ACM WORKSH VI, P37
   Vukotic V, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P421, DOI 10.1145/3078971.3079038
   Vukotic V, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P343, DOI 10.1145/2911996.2912064
   Wang HL, 2017, IEEE T MULTIMEDIA, V19, P908, DOI 10.1109/TMM.2016.2645398
   Wang M, 2012, IEEE T IMAGE PROCESS, V21, P4649, DOI 10.1109/TIP.2012.2207397
   Wang M, 2012, IEEE T MULTIMEDIA, V14, P975, DOI 10.1109/TMM.2012.2185041
   Wang M, 2012, IEEE T MULTIMEDIA, V14, P858, DOI 10.1109/TMM.2012.2187181
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Yang SJ, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P413, DOI 10.1145/2911996.2912029
   Yang XS, 2018, IEEE T MULTIMEDIA, V20, P2360, DOI 10.1109/TMM.2018.2807588
   Zhang L, 2018, IEEE T MULTIMEDIA, V20, P128, DOI 10.1109/TMM.2017.2723841
   Zhou BL, 2014, ADV NEUR IN, V27
NR 59
TC 12
Z9 13
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2020
VL 22
IS 1
BP 188
EP 200
DI 10.1109/TMM.2019.2923121
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KB6AZ
UT WOS:000506577000017
OA Green Published
DA 2024-07-18
ER

PT J
AU Li, LX
   Wen, GQ
   Wang, ZM
   Yang, YX
AF Li, Lixiang
   Wen, Guoqian
   Wang, Zeming
   Yang, Yixian
TI Efficient and Secure Image Communication System Based on Compressed
   Sensing for IoT Monitoring Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Internet of Things (IoT); image transmission; compressed sensing;
   chaotic systems
ID WIRELESS SENSORS; DECOMPOSITION; ENCRYPTION; SCHEME
AB The Internet of Things (IoT) has attracted extensive attention in the information field. Its rapid development has promoted several monitoring application domains. However, the resource constraint of sensor nodes and the security of data transmission have emerged as significant issues. In this paper, an image communication system for IoT monitoring applications is exploited to solve the above-mentioned problems simultaneously. The proposed system can satisfy the requirements of sensor nodes for low computational complexity, low-energy consumption, and low storage overhead. We also present a new compressed sensing (CS) model, as well as the corresponding parallel reconstruction algorithm, which help to reduce the image encryption/decryption time. Based on chaotic systems, we integrate the quantization and diffusion operations into the system to further enhance the transmission security. The simulations are executed to demonstrate the feasibility and the effectiveness of the proposed method. Compared with the traditional CS, our numerical results indicate that the proposed model reduces 413 ms computation time and 3.13 x 10(6) elements stored for large-scale images. Besides, we verify the flexibility and the diversity of choosing two submatrices for different-sized images. Experimental results also show the proposed system performs well in terms of security performance. Particularly the key space reaches 2(253).
C1 [Li, Lixiang; Wen, Guoqian; Wang, Zeming; Yang, Yixian] Beijing Univ Posts & Telecommun, Informat Secur Ctr, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.
   [Li, Lixiang; Wen, Guoqian; Wang, Zeming; Yang, Yixian] Beijing Univ Posts & Telecommun, Natl Engn Lab Disaster Backup & Recovery, Beijing 100876, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Beijing University of
   Posts & Telecommunications
RP Li, LX (corresponding author), Beijing Univ Posts & Telecommun, Informat Secur Ctr, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.
EM lixiang@bupt.edu.cn; wgqian0728@163.com; wzm970527@gmail.com;
   yxyang@bupt.edu.cn
RI Li, lixiang/G-6222-2011
OI Lixiang, Li/0000-0001-9949-8731
FU National Key R&D Program of China [2016YFB0800602]; National Natural
   Science Foundation of China [61771071, 61573067]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2016YFB0800602 and in part by the National Natural Science
   Foundation of China under Grant 61771071 and Grant 61573067. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was. Dr. Sanjeev Mehrotra.
CR Abolghasemi V, 2010, EUR SIGNAL PR CONF, P427
   Al-Fuqaha A, 2015, IEEE COMMUN SURV TUT, V17, P2347, DOI 10.1109/COMST.2015.2444095
   Almowuena S, 2016, IEEE T MULTIMEDIA, V18, P102, DOI 10.1109/TMM.2015.2502067
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candès EJ, 2008, CR MATH, V346, P589, DOI 10.1016/j.crma.2008.03.014
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Cen N, 2017, IEEE T MULTIMEDIA, V19, P1117, DOI 10.1109/TMM.2017.2653770
   Challa S, 2017, IEEE ACCESS, V5, P3028, DOI 10.1109/ACCESS.2017.2676119
   Chen SSB, 2001, SIAM REV, V43, P129, DOI [10.1137/S003614450037906X, 10.1137/S1064827596304010]
   Chiou Paul T., 2017, 2017 9th Computer Science and Electronic Engineering (CEEC). Proceedings, P65, DOI 10.1109/CEEC.2017.8101601
   Deepu CJ, 2017, IEEE T BIOMED CIRC S, V11, P245, DOI 10.1109/TBCAS.2016.2591923
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100
   Elad M, 2007, IEEE T SIGNAL PROCES, V55, P5695, DOI 10.1109/TSP.2007.900760
   Fang XJ, 2017, 2017 4TH INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND CONTROL ENGINEERING (ICISCE), P639, DOI 10.1109/ICISCE.2017.139
   Frunzete M, 2011, SPA 2011: SIGNAL PROCESSING ALGORITHMS, ARCHITECTURES, ARRANGEMENTS, AND APPLICATIONS CONFERENCE PROCEEDINGS, P11
   Gope P, 2016, IEEE SENS J, V16, P1368, DOI 10.1109/JSEN.2015.2502401
   He DL, 2015, IEEE T MULTIMEDIA, V17, P1658, DOI 10.1109/TMM.2015.2451956
   Huang J, 2017, IEEE WIREL COMMUN, V24, P67, DOI 10.1109/MWC.2017.1600193WC
   Karakus C, 2013, IEEE SENS J, V13, P1999, DOI 10.1109/JSEN.2013.2244036
   Kodali Ravi Kishore, 2013, 2013 International Conference on Advanced Electronic Systems (ICAES), P292, DOI 10.1109/ICAES.2013.6659411
   Louw J, 2016, IEEE INTL CONF IND I, P1166, DOI 10.1109/INDIN.2016.7819342
   Luo C, 2009, FIFTEENTH ACM INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING (MOBICOM 2009), P145
   Mohimani H, 2009, IEEE T SIGNAL PROCES, V57, P289, DOI 10.1109/TSP.2008.2007606
   Mois G, 2017, IEEE T INSTRUM MEAS, V66, P2056, DOI 10.1109/TIM.2017.2677619
   Muhammad K, 2018, IEEE T IND INFORM, V14, P3679, DOI 10.1109/TII.2018.2791944
   Orsdemir A, 2008, IEEE MILIT COMMUN C, P1040
   Pareschi F, 2017, IEEE T BIOMED CIRC S, V11, P1278, DOI 10.1109/TBCAS.2017.2740059
   Peng HP, 2017, IEEE T BIOMED CIRC S, V11, P558, DOI 10.1109/TBCAS.2017.2665659
   Rachlin Y, 2008, ANN ALLERTON CONF, P813, DOI 10.1109/ALLERTON.2008.4797641
   Rasheed M.B., 2013, 26th IEEE Canadian Conference on Electrical and Computer Engineering (CCECE), Regina, 5-8 May 2013, P1
   Rauhut H, 2008, IEEE T INFORM THEORY, V54, P2210, DOI 10.1109/TIT.2008.920190
   Song TY, 2017, IEEE INTERNET THINGS, V4, P1844, DOI 10.1109/JIOT.2017.2707489
   Song XD, 2017, IEEE T MULTIMEDIA, V19, P1351, DOI 10.1109/TMM.2017.2654123
   Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Wang AS, 2016, IEEE T IND INFORM, V12, P15, DOI 10.1109/TII.2015.2482946
   Wang XY, 2018, IEEE ACCESS, V6, P23733, DOI 10.1109/ACCESS.2018.2805847
   Wang YH, 2017, IEEE T BIOMED CIRC S, V11, P255, DOI 10.1109/TBCAS.2016.2597310
   Wu JY, 2018, IEEE T MULTIMEDIA, V20, P457, DOI 10.1109/TMM.2017.2741425
   Xie D, 2016, DIGIT SIGNAL PROCESS, V58, P85, DOI 10.1016/j.dsp.2016.07.003
   Yang HL, 2017, CHINA AND THE WEST: MUSIC, REPRESENTATION, AND RECEPTION, P1
   Yu L, 2010, IEEE SIGNAL PROC LET, V17, P731, DOI 10.1109/LSP.2010.2052243
   Yuan Xin, 2017, ARXIV170601000
   Yuan XL, 2016, IEEE T MULTIMEDIA, V18, P2002, DOI 10.1109/TMM.2016.2602758
   Zhou YC, 2014, SIGNAL PROCESS, V97, P172, DOI 10.1016/j.sigpro.2013.10.034
NR 46
TC 43
Z9 44
U1 2
U2 39
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2020
VL 22
IS 1
BP 82
EP 95
DI 10.1109/TMM.2019.2923111
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KB6AZ
UT WOS:000506577000009
DA 2024-07-18
ER

PT J
AU Li, MP
   Zhou, ZM
   Liu, XG
AF Li, Miaopeng
   Zhou, Zimeng
   Liu, Xinguo
TI Multi-Person Pose Estimation Using Bounding Box Constraint and LSTM
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human pose estimation; convolutional neural network; bottom-up; bounding
   box constraint; long short-term memory
ID REPRESENTATION
AB This paper presents a new method for single-image pose estimation of multiple people combining the traditional bottom-up and the top-down methods. Specifically, we extract features from the input image by a residual network and use a multistage CNN to learn both the confidence maps of joints and the connection relationships, between joints. During testing, we perform the network feedforwarding in a bottom-up manner, and then use the predicted confidence maps, the connection relationships, and the corresponding bounding boxes to parse the poses of all people in a top-down manner. In contrast to the previous top-down methods, our method is robust to bounding box shift and tightness, works well for largely overlapped people, and achieves faster running speed. In contrast to the bottom-up method, our method avoids mistake propagation across different people, and addresses disconnected joints effectively. To estimate human pose from videos, we impose a weight-sharing scheme to the multi-stage CNN, and rewrite it as a recurrent neural network. Thus, we can reuse the prediction results from the previous frames so as to reduce the total stage number, yielding significantly faster speed in invoking the network on videos. And we adopt LSTM units between frames to capture the temporal correlation among video frames. We found that LSTM handles input-quality degradation in videos well and successfully stabilizes the sequential outputs.
C1 [Li, Miaopeng; Zhou, Zimeng; Liu, Xinguo] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Liu, XG (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
EM li_miaopeng@zju.edu.cn; zmzhou@zju.edu.cn; xgliu@cad.zju.edu.cn
OI Liu, Xinguo/0000-0003-4650-1970; Li, Miaopeng/0000-0002-1063-3931
FU NSFC [61872317]; FaceUnity Technology
FX This work was supported in part by NSFC (61872317) and in part by
   FaceUnity Technology.
CR Andriluka M, 2009, PROC CVPR IEEE, P1014, DOI 10.1109/CVPRW.2009.5206754
   [Anonymous], 2014, P IEEE INT C COMP VI
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bradley D.M., 2010, Learning in modular systems
   Buitelaar P, 2018, IEEE T MULTIMEDIA, V20, P2454, DOI 10.1109/TMM.2018.2798287
   Bulat A, 2016, LECT NOTES COMPUT SC, V9911, P717, DOI 10.1007/978-3-319-46478-7_44
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen X., 2014, Advances in neural information processing systems, P1736, DOI DOI 10.1109/CVPR.2018.00742
   Einfalt M, 2018, IEEE WINT CONF APPL, P446, DOI 10.1109/WACV.2018.00055
   Fan ZX, 2019, IEEE T MULTIMEDIA, V21, P363, DOI 10.1109/TMM.2018.2859620
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   FISCHLER MA, 1973, IEEE T COMPUT, VC 22, P67, DOI 10.1109/T-C.1973.223602
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Iqbal U, 2016, LECT NOTES COMPUT SC, V9914, P627, DOI 10.1007/978-3-319-48881-3_44
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kingma D. P., 2014, arXiv
   Li MP, 2018, INT C PATT RECOG, P115, DOI 10.1109/ICPR.2018.8546194
   Li ZY, 2018, COMPUT VIS IMAGE UND, V166, P41, DOI 10.1016/j.cviu.2017.10.011
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu HL, 2016, IEEE T MULTIMEDIA, V18, P1233, DOI 10.1109/TMM.2016.2556859
   Luo Y, 2018, PROC CVPR IEEE, P5207, DOI 10.1109/CVPR.2018.00546
   Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539
   Marcos-Ramiro A, 2015, IEEE T MULTIMEDIA, V17, P1721, DOI 10.1109/TMM.2015.2464152
   Newell A, 2017, ADV NEUR IN, V30
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Ning GH, 2018, IEEE T MULTIMEDIA, V20, P1246, DOI 10.1109/TMM.2017.2762010
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Pfister T, 2015, IEEE I CONF COMP VIS, P1913, DOI 10.1109/ICCV.2015.222
   Pinheiro PO, 2014, PR MACH LEARN RES, V32
   Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi XS, 2015, 2015 IEEE ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IAEAC), P802, DOI 10.1109/IAEAC.2015.7428667
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Torres C, 2018, IEEE T MULTIMEDIA, V20, P3057, DOI 10.1109/TMM.2018.2829162
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Wu J., 2017, ARXIV171106475
   Zhang WY, 2013, IEEE I CONF COMP VIS, P2248, DOI 10.1109/ICCV.2013.280
NR 43
TC 48
Z9 53
U1 1
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2019
VL 21
IS 10
BP 2653
EP 2663
DI 10.1109/TMM.2019.2903455
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JD1IM
UT WOS:000489728400018
DA 2024-07-18
ER

PT J
AU Chen, SZ
   Jin, Q
   Chen, J
   Hauptmann, AG
AF Chen, Shizhe
   Jin, Qin
   Chen, Jia
   Hauptmann, Alexander G.
TI Generating Video Descriptions With Latent Topic Guidance
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video Captioning; Latent Topic; Multimodal Ensemble
AB Automatic video description generation (a.k.a video captioning) is one of the ultimate goals for video understanding. Despite the wide range of applications such as video indexing and retrieval etc., the video captioning task remains quite challenging due to the complexity and diversity of video content. First, open-domain videos cover a broad range of topics, which results in highly variable vocabularies and expression styles to describe the video contents. Second, videos naturally contain multiple modalities including image, motion, and acoustic media. The information provided by different modalities differs in different conditions. In this paper, we propose a novel topic-guided video captioning model to address the above-mentioned challenges in video captioning. Our model consists of two joint tasks, namely, latent topic generation and topic-guided caption generation. The topic generation task aims to automatically predict the latent topic of the video. Since there is no groundtruth topic information, we mine multimodal topics in an unsupervised fashion based on video contents and annotated captions, and then distill the topic distribution to a topic prediction model. In the topic-guided generation task, we employ the topic guidance for two purposes. The first is to narrow down the language complexity across topics, where we propose the topic-aware decoder to leverage the latent topics to induce topic-related language models. The decoder is also generic and can be integrated with a temporal attention mechanism. The second is to dynamically attend to important modalities by topics, where we propose a flexible topic-guided multimodal ensemble framework and use the topic gating network to determine the attention weights. The two tasks are correlated with each other, and they collaborate to generate more detailed and accurate video captions. Our extensive experiments on two public benchmark datasets MSR-VTT and Youtube2Text demonstrate the effectiveness of the proposed topic-guided video captioning system, which achieves state-of-the-art performance on both datasets.
C1 [Chen, Shizhe; Jin, Qin] Renmin Univ China, Sch Informat, Beijing 100872, Peoples R China.
   [Chen, Jia] Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA.
   [Hauptmann, Alexander G.] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
C3 Renmin University of China; Carnegie Mellon University; Carnegie Mellon
   University
RP Jin, Q (corresponding author), Renmin Univ China, Sch Informat, Beijing 100872, Peoples R China.
EM cszhe1@ruc.edu.cn; qjin@ruc.edu.cn; jiac@cs.cmu.edu; alex@cs.cmu.edu
OI Chen, Shizhe/0000-0002-7313-9703; Jin, Qin/0000-0001-6486-6020
FU National Natural Science Foundation of China [61772535]; National Key
   Research and Development Plan [2016YFB1001202]; Research Foundation of
   Beijing Municipal Science & Technology Commission [Z181100008918002]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61772535, in part by the National Key
   Research and Development Plan under Grant 2016YFB1001202, and in part by
   the Research Foundation of Beijing Municipal Science & Technology
   Commission under Grant Z181100008918002. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Engin Erzin.
CR Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2010, P MACH LEARN RES
   [Anonymous], ACM MULTIMEDIA VIDEO
   [Anonymous], 2016, PROC 24 ACM INT C MU, DOI [DOI 10.1145/2964284.2984065, 10.1145/2964284.2984065]
   [Anonymous], 2015, STAT
   [Anonymous], P ASS COMP LING WORK
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], P 14 INT C ART INT S
   [Anonymous], 2017, P IEEE C COMP VIS PA
   Arora S., 2016, SIMPLE TOUGH TO BEAT
   Ba LJ, 2014, ADV NEUR IN, V27
   Bahdanau Dzmitry, 2015, P 3 INT C LEARN REPR
   Baraldi L, 2017, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2017.339
   Chen SZ, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P5, DOI 10.1145/3078971.3079000
   Chen SZ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1838, DOI 10.1145/3123266.3123420
   Cheng ZY, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1069, DOI 10.1145/2911451.2914765
   DAVIS SB, 1980, IEEE T ACOUST SPEECH, V28, P357, DOI 10.1109/TASSP.1980.1163420
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Gan Z, 2017, PROC CVPR IEEE, P1141, DOI 10.1109/CVPR.2017.127
   Garmash Ekaterina, 2016, P COLING 2016 26 INT, P1409
   Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hori C, 2017, IEEE I CONF COMP VIS, P4203, DOI 10.1109/ICCV.2017.450
   Jin Q, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P239, DOI 10.1145/2911996.2912043
   Kingma D. P., 2015, P INT C LEARN REPR, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Liu AA, 2017, COMPUT VIS IMAGE UND, V163, P113, DOI 10.1016/j.cviu.2017.04.013
   Memisevic R., 2007, IEEE Conference on Computer Vision and Pattern Recognition, P1
   Pan PB, 2016, PROC CVPR IEEE, P1029, DOI 10.1109/CVPR.2016.117
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Pancoast Stephanie, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1370, DOI 10.1109/ICASSP.2014.6853821
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Shen ZQ, 2017, PROC CVPR IEEE, P5159, DOI 10.1109/CVPR.2017.548
   Shetty R., 2016, P 24 ACM INT C MULTI, P1073
   Song JK, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2737
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S., 2014, COMPUT SCI
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang JB, 2018, PROC CVPR IEEE, P7512, DOI 10.1109/CVPR.2018.00784
   Wang X, 2018, PROC CVPR IEEE, P4213, DOI 10.1109/CVPR.2018.00443
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu X, 2015, IEEE ICC, P2048, DOI 10.1109/ICC.2015.7248627
   Yang Z, 2016, INT CONF ACOUST SPEE, P3236, DOI 10.1109/ICASSP.2016.7472275
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 53
TC 30
Z9 32
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2019
VL 21
IS 9
BP 2407
EP 2418
DI 10.1109/TMM.2019.2896515
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IT6VX
UT WOS:000483015200020
DA 2024-07-18
ER

PT J
AU Fu, ZY
   Angelini, F
   Chambers, J
   Naqvi, SM
AF Fu, Zeyu
   Angelini, Federico
   Chambers, Jonathon
   Naqvi, Syed Mohsen
TI Multi-Level Cooperative Fusion of GM-PHD Filters for Online Multiple
   Human Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multiple human tracking; GM-PHD filter; data fusion
ID MULTITARGET TRACKING; TARGET
AB In this paper, we propose a multi-level cooperative fusion approach to address the online multiple human tracking problem in a Gaussian mixture probability hypothesis density (GM-PHD) filter framework. The proposed fusion approach consists essentially of three steps. First, we integrate two human detectors with different characteristics (full-body and body-parts), and investigate their complementary benefits for tracking multiple targets. For each detector domain, we then propose a novel discriminative correlation matching model, and fuse it with spatio-temporal information to address ambiguous identity association in the CNI-PHD filter. Finally, we develop a robust fusion center with virtual and real zones to make a global decision based on preliminary candidate targets generated by each detector. This center also mitigates the sensitivity of missed detections in the generalized covariance intersection fusion process, thereby improving the fusion performance and tracking consistency. Experiments on the MOTChallenge Benchmark demonstrate that the proposed method achieves improved performance over other state-of-the-art RFS-based tracking methods.
C1 [Fu, Zeyu; Angelini, Federico; Naqvi, Syed Mohsen] Newcastle Univ, Sch Engn, Intelligent Sensing & Commun Res Grp, Newcastle Upon Tyne NE1 7RU, Tyne & Wear, England.
   [Chambers, Jonathon] Univ Leicester, Dept Engn, Leicester LE1 7RH, Leics, England.
C3 Newcastle University - UK; University of Leicester
RP Fu, ZY; Naqvi, SM (corresponding author), Newcastle Univ, Sch Engn, Intelligent Sensing & Commun Res Grp, Newcastle Upon Tyne NE1 7RU, Tyne & Wear, England.
EM z.fu2@ncl.ac.uk; F.Angelini2@newcastle.ac.uk;
   Jonathon.Chambers@newcastle.ac.uk; Mohsen.Naqvi@newcastle.ac.uk
RI FU, ZEYU/AAP-7322-2020
OI FU, ZEYU/0000-0002-2076-7597; Naqvi, Syed Mohsen/0000-0002-1547-0908
CR [Anonymous], 2016 3 INT C MECH
   [Anonymous], 2017, 2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)
   [Anonymous], 2016, ARXIV PREPRINT ARXIV
   [Anonymous], ARXIV151205990
   [Anonymous], 2018, ECCV
   [Anonymous], IEEE T CYBERN
   [Anonymous], IEEE T MULTIMEDIA
   [Anonymous], 2018, CVPR WORKSHOPS
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2010, SENSOR SIGNAL PROCES, DOI DOI 10.1049/IC.2010.0233
   [Anonymous], 2016, J. Adv. Inf. Fusion
   [Anonymous], IEEE T CIRCUITS SYST
   Bae SH, 2018, IEEE T PATTERN ANAL, V40, P595, DOI 10.1109/TPAMI.2017.2691769
   Baisa NL, 2019, J VIS COMMUN IMAGE R, V59, P257, DOI 10.1016/j.jvcir.2019.01.026
   Battistelli G, 2013, IEEE J-STSP, V7, P508, DOI 10.1109/JSTSP.2013.2250911
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Biresaw TA, 2015, IEEE T CIRC SYST VID, V25, P776, DOI 10.1109/TCSVT.2014.2360027
   Chen JH, 2017, IEEE COMPUT SOC CONF, P2143, DOI 10.1109/CVPRW.2017.266
   Chen L, 2017, IEEE IMAGE PROC, P645, DOI 10.1109/ICIP.2017.8296360
   Cheng L, 2018, PROCEEDINGS OF 2018 6TH INTERNATIONAL CONFERENCE ON BIOINFORMATICS AND COMPUTATIONAL BIOLOGY (ICBCB 2018), P1, DOI 10.1145/3194480.3194482
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Eiselein V, 2012, 2012 IEEE NINTH INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL-BASED SURVEILLANCE (AVSS), P325, DOI 10.1109/AVSS.2012.59
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Feng PM, 2017, IEEE T MULTIMEDIA, V19, P725, DOI 10.1109/TMM.2016.2638206
   Fu ZY, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4299, DOI 10.1109/ICASSP.2018.8461946
   Fu ZY, 2018, 2018 21ST INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P1976, DOI 10.23919/ICIF.2018.8455432
   Fu ZY, 2018, IEEE ACCESS, V6, P14764, DOI 10.1109/ACCESS.2018.2816805
   Gravina R, 2017, INFORM FUSION, V35, P68, DOI 10.1016/j.inffus.2016.09.005
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Khalid O, 2017, IEEE T CIRC SYST VID, V27, P1527, DOI 10.1109/TCSVT.2016.2542699
   Kim C, 2015, IEEE I CONF COMP VIS, P4696, DOI 10.1109/ICCV.2015.533
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Lan L, 2018, IEEE T IMAGE PROCESS, V27, P4585, DOI 10.1109/TIP.2018.2843129
   Li XL, 2017, IEEE RAD CONF, P17, DOI 10.1109/RADAR.2017.7944163
   Liu QJ, 2018, IEEE T MULTIMEDIA, V20, P1767, DOI 10.1109/TMM.2017.2777671
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Mahler R, 2000, P SOC PHOTO-OPT INS, V4052, P128, DOI 10.1117/12.395064
   Mahler RPS, 2003, IEEE T AERO ELEC SYS, V39, P1152, DOI 10.1109/TAES.2003.1261119
   Panta K, 2009, IEEE T AERO ELEC SYS, V45, P1003, DOI 10.1109/TAES.2009.5259179
   Park SH, 2016, IEEE IMAGE PROC, P3484, DOI 10.1109/ICIP.2016.7533007
   Ristic B, 2012, IEEE T AERO ELEC SYS, V48, P1656, DOI 10.1109/TAES.2012.6178085
   Sadeghian A, 2017, IEEE I CONF COMP VIS, P300, DOI 10.1109/ICCV.2017.41
   Sanchez-Matilla R, 2016, LECT NOTES COMPUT SC, V9914, P84, DOI 10.1007/978-3-319-48881-3_7
   Son Jeany, 2017, P IEEE C COMP VIS PA, P5620
   Song Y., 2016, 2016 IEEE INT C VEHI, P1, DOI DOI 10.1109/ICVES.2016.7548171
   Tang SY, 2016, LECT NOTES COMPUT SC, V9914, P100, DOI 10.1007/978-3-319-48881-3_8
   Üney M, 2013, IEEE J-STSP, V7, P521, DOI 10.1109/JSTSP.2013.2257162
   Ur-Rehman A, 2016, IEEE T SIGNAL PROCES, V64, P1320, DOI 10.1109/TSP.2015.2504340
   Vasic M, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4172, DOI 10.1109/IROS.2016.7759614
   Vasic M, 2015, IEEE INT C INTELL TR, P491, DOI 10.1109/ITSC.2015.87
   Vo BN, 2006, IEEE T SIGNAL PROCES, V54, P4091, DOI 10.1109/TSP.2006.881190
   Vo BN, 2005, IEEE T AERO ELEC SYS, V41, P1224, DOI 10.1109/TAES.2005.1561884
   Wang BL, 2015, 2015 18TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P253
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wojke N, 2018, IEEE WINT CONF APPL, P748, DOI 10.1109/WACV.2018.00087
   Yang YB, 2017, 2017 14TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV 2017), P209, DOI 10.1109/CRV.2017.18
   Zhou XL, 2015, SENSORS-BASEL, V15, P30240, DOI 10.3390/s151229794
NR 59
TC 60
Z9 66
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2019
VL 21
IS 9
BP 2277
EP 2291
DI 10.1109/TMM.2019.2902480
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IT6VX
UT WOS:000483015200010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tian, SS
   Zhang, L
   Morin, L
   Déforges, O
AF Tian, Shishun
   Zhang, Lu
   Morin, Luce
   Deforges, Olivier
TI A Benchmark of DIBR Synthesized View Quality Assessment Metrics on a New
   Database for Immersive Media Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Depth-image-based-rendering (DIBR); FVV; view synthesis; QoE; quality
   assessment
ID IMAGE; COMPRESSION
AB Depth-image-based rendering (DIBR) is a fundamental technology in several 3-D-related applications, such as free viewpoint video, virtual reality, and augmented reality. However, new challenges have also been brought in assessing the quality of DIBR-synthesized views since this process induces some new types of distortions, which are inherently different from the distortion caused by video coding. In this paper, we present a new DIBR-synthesized image database with the associated subjective scores. We also test the performances of the state-of-the-art objective quality metrics on this database. This paper focuses on the distortions only induced by different DIBR synthesis methods. Seven state-of-the-art DIBR algorithms, including interview synthesis and single-view-based synthesis methods, are considered in this database. The quality of synthesized views was assessed subjectively by 41 observers and objectively using 14 state-of-the-art objective metrics. Subjective test results show that the interview synthesis methods, having more input information, significantly outperform the single-view-based ones. Correlation results between the tested objective metrics and the subjective scores on this database reveal that further studies are still needed for a better objective quality metric dedicated to the DIBR-synthesized views.
C1 [Tian, Shishun; Zhang, Lu; Morin, Luce; Deforges, Olivier] Univ Rennes, Natl Inst Appl Sci INSA Rennes, F-35042 Rennes, France.
   [Tian, Shishun; Zhang, Lu; Morin, Luce; Deforges, Olivier] CNRS, UMR 6164, Inst Elect & Telecommun Rennes, F-35708 Rennes, France.
C3 Universite de Rennes; Universite de Rennes; Centre National de la
   Recherche Scientifique (CNRS); CNRS - Institute for Engineering &
   Systems Sciences (INSIS)
RP Tian, SS (corresponding author), Univ Rennes, Natl Inst Appl Sci INSA Rennes, F-35042 Rennes, France.; Tian, SS (corresponding author), CNRS, UMR 6164, Inst Elect & Telecommun Rennes, F-35708 Rennes, France.
EM shishun.tian@insa-rennes.fr; lu.ge@insa-rennes.fr;
   luce.morin@insa-rennes.fr; olivier.deforges@insa-rennes.fr
RI Tian, Shishun/T-6019-2019; zhang, lu/AAP-5753-2020
OI Tian, Shishun/0000-0002-7616-8382; zhang, lu/0000-0002-8859-5453;
   DEFORGES, olivier/0000-0003-0750-0959
FU China Scholarship Council
FX This work was supported by China Scholarship Council. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Pablo Cesar.
CR Ahn I, 2013, IEEE T BROADCAST, V59, P614, DOI 10.1109/TBC.2013.2281658
   [Anonymous], 3DTV C TRUE VIS TRAN
   [Anonymous], 2001, P IEEE COMP SOC C CO
   [Anonymous], 2007, METH SUBJ ASS VID QU
   [Anonymous], 2013, 3D TV SYSTEM DEPTH I, DOI DOI 10.1007/978-1-4419-9964-1_15
   [Anonymous], 2017, IND DEMONSTRATION
   [Anonymous], 2016, FUTURE VIDEO ENABLIN
   Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355
   Benoit A, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/659024
   Blin J. L., 2006, P INT WORKSH VID PRO, P1
   Bosc E, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P249, DOI 10.1109/PCS.2012.6213339
   Bosc E, 2011, IEEE J-STSP, V5, P1332, DOI 10.1109/JSTSP.2011.2166245
   Cheung CH, 2018, IEEE T MULTIMEDIA, V20, P1376, DOI 10.1109/TMM.2017.2772442
   Conze  P.-H., 2012, P IS T SPIE EL IM
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Egiazarian K., 2006, proceedings of the second international workshop on video processing and quality metrics, P4
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Feng X., 2006, P SOC PHOTO-OPT INS, V6076, P74
   Gu K, 2018, IEEE T IMAGE PROCESS, V27, P394, DOI 10.1109/TIP.2017.2733164
   IVC-IRCCyN Lab, IRCCYN IVC DIBR IM D
   Jantet V, 2011, IEEE IMAGE PROC, P125, DOI 10.1109/ICIP.2011.6115662
   Jiang F, 2017, IEEE INT CON MULTI, P67, DOI 10.1109/ICME.2017.8019522
   Jung YJ, 2016, IEEE T CIRC SYST VID, V26, P1201, DOI 10.1109/TCSVT.2015.2430632
   Le Meur O., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3401, DOI 10.1109/ICIP.2011.6116441
   Li LD, 2018, IEEE T MULTIMEDIA, V20, P914, DOI 10.1109/TMM.2017.2760062
   Li S, 2018, IEEE T MULTIMEDIA, V20, P1948, DOI 10.1109/TMM.2018.2791810
   Lie WN, 2018, IEEE T MULTIMEDIA, V20, P1075, DOI 10.1109/TMM.2017.2763319
   Liu XK, 2015, IEEE T IMAGE PROCESS, V24, P4847, DOI 10.1109/TIP.2015.2469140
   Luo GB, 2016, PROC CVPR IEEE, P1781, DOI 10.1109/CVPR.2016.197
   Mao Y, 2016, IEEE T MULTIMEDIA, V18, P1453, DOI 10.1109/TMM.2016.2573142
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Mori Y, 2009, SIGNAL PROCESS-IMAGE, V24, P65, DOI 10.1016/j.image.2008.10.013
   Mueller TJ, 2009, INT J MICRO AIR VEH, V1, P1, DOI 10.1260/1756-8293.1.1.1
   Ndjiki-Nya P, 2010, IEEE INT CON MULTI, P424, DOI 10.1109/ICME.2010.5583559
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   Ponomarenko N., 2007, P 3 INT WORKSH VID P
   Ren DN, 2015, IEEE T MULTIMEDIA, V17, P307, DOI 10.1109/TMM.2015.2389714
   Rheingold H., 1991, VIRTUAL REALITY EXPL
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sabater N, 2017, IEEE COMPUT SOC CONF, P1743, DOI 10.1109/CVPRW.2017.221
   Sandic-Stankovic D., 2015, P IEEE 7 INT WORKSH, P1
   Sandic-Stankovic D, 2016, J ELECTR ENG-SLOVAK, V67, P3, DOI 10.1515/jee-2016-0001
   Sandie-Stankovie D., 2017, EURASIP J IMAGE VIDE, V1, P1
   Schmeing M, 2015, IEEE T MULTIMEDIA, V17, P2160, DOI 10.1109/TMM.2015.2476372
   Solh M, 2012, IEEE J-STSP, V6, P495, DOI 10.1109/JSTSP.2012.2204723
   Song R, 2015, J INF SCI ENG, V31, P1593
   Sullivan GJ, 2013, IEEE J-STSP, V7, P1001, DOI 10.1109/JSTSP.2013.2283657
   Tanimoto M., 2008, JTC1SC29WG11 ISOIEC
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Telea A., 2004, Journal of Graphics Tools, V9, P23, DOI 10.1080/10867651.2004.10487596
   Tian S., 2018, ELECT IMAGING
   Tian SS, 2018, IEEE T IMAGE PROCESS, V27, P1652, DOI 10.1109/TIP.2017.2781420
   TIAN SS, 2017, P IEEE INT C AC SPEE, P1248
   Video Quality Experts Group, 2008, FIN REP VID QUAL EXP
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wang Z, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P981, DOI 10.1109/ICIP.2000.899622
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Yoon SS, 2014, IEEE IMAGE PROC, P2883, DOI 10.1109/ICIP.2014.7025583
   Yuen M, 1998, SIGNAL PROCESS, V70, P247, DOI 10.1016/S0165-1684(98)00128-5
   Zhou ZQ, 2018, IEEE T MULTIMEDIA, V20, P1392, DOI 10.1109/TMM.2017.2772438
   Zhu C, 2016, IEEE T BROADCAST, V62, P82, DOI 10.1109/TBC.2015.2475697
   2007, P IEEE INT C IM PROC, P201
   2010, P IEEE INT C IM PROC, P1809
NR 66
TC 43
Z9 44
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2019
VL 21
IS 5
BP 1235
EP 1247
DI 10.1109/TMM.2018.2875307
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HV8HY
UT WOS:000466223600013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Agudo, A
   Moreno-Noguer, F
AF Agudo, Antonio
   Moreno-Noguer, Francesc
TI Shape Basis Interpretation for Monocular Deformable 3-D Reconstruction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deformable shape analysis; dynamic modeling; structure from motion;
   low-rank representation; optimization
ID STRUCTURE-FROM-MOTION; 3D RECONSTRUCTION; NONRIGID MOTION; DEFORMATIONS;
   RECOVERY; MODELS
AB In this paper, we propose a novel interpretable shape model to encode object nonrigidity. We first use the initial frames of a monocular video to recover a rest shape, used later to compute a dissimilarity measure based on a distance matrix measurement. Spectral analysis is then applied to this matrix to obtain a reduced shape basis, that in contrast to existing approaches, can be physically interpreted. In turn, these precomputed shape bases are used to linearly span the deformation of a wide variety of objects. We introduce the low-rank basis into a sequential approach to recover both camera motion and nonrigid shape from the monocular video, by simply optimizing the weights of the linear combination using bundle adjustment. Since the number of parameters to optimize per frame is relatively small, specially when physical priors are considered, our approach is fast and can potentially run in real time. Validation is done in a wide variety of real-world objects, undergoing both inextensible and extensible deformations. Our approach achieves remarkable robustness to artifacts such as noisy and missing measurements and shows an improved performance to competing methods.
C1 [Agudo, Antonio; Moreno-Noguer, Francesc] UPC, CSIC, Inst Robot & Informat Ind, Barcelona 08028, Spain.
C3 Consejo Superior de Investigaciones Cientificas (CSIC); CSIC - Institut
   de Robotica i Informatica Industrial (IRII); Universitat Politecnica de
   Catalunya
RP Agudo, A (corresponding author), UPC, CSIC, Inst Robot & Informat Ind, Barcelona 08028, Spain.
EM aagudo@iri.upc.edu; fmoreno@iri.upc.edu
RI Agudo, Antonio/J-1805-2016; Agudo, Antonio/C-5147-2017
OI Agudo, Antonio/0000-0001-9900-5677; Agudo, Antonio/0000-0001-6845-4998
FU Google Faculty Research Award; MINECO project HuMoUR [TIN2017-90086-R];
   MINECO project Maria de Maeztu Seal of Excellence [MDM-2016-0656]
FX This work was supported in part by a Google Faculty Research Award, in
   part by the MINECO projects HuMoUR TIN2017-90086-R and Maria de Maeztu
   Seal of Excellence MDM-2016-0656. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Xilin Chen. (Corresponding author: Antonio Agudo.)
CR Abdi H., 2005, Proceedings of the IEEE Computer Society: International Conference on Computer Vision and Pattern Recognition, San Diego, CA, P42, DOI DOI 10.1109/CVPR.2005.445
   Agudo A., 2014, P BMVC
   Agudo A., 2016, P IEEE WINT C APPL C, P1
   Agudo A, 2017, INT J COMPUT VISION, V122, P371, DOI 10.1007/s11263-016-0972-8
   Agudo A, 2017, J MATH IMAGING VIS, V57, P75, DOI 10.1007/s10851-016-0668-2
   Agudo A, 2016, COMPUT VIS IMAGE UND, V153, P37, DOI 10.1016/j.cviu.2016.05.004
   Agudo A, 2015, IEEE I CONF COMP VIS, P756, DOI 10.1109/ICCV.2015.93
   Agudo A, 2014, PROC CVPR IEEE, P1558, DOI 10.1109/CVPR.2014.202
   Akhter I, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2159516.2159523
   Akhter I, 2011, IEEE T PATTERN ANAL, V33, P1442, DOI 10.1109/TPAMI.2010.201
   [Anonymous], 1982, FINITE ELEMENT PROCE
   [Anonymous], 2008, P IEEE C COMP VIS PA
   Barbic J, 2005, ACM T GRAPHIC, V24, P982, DOI 10.1145/1073204.1073300
   Barth AdamT., 2008, BodyNets'08: Proceedings of the ICST 3rd international conference on Body area networks, P1
   Bartoli A, 2012, PROC CVPR IEEE, P2026, DOI 10.1109/CVPR.2012.6247906
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941
   Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1
   Bronstein AM, 2003, LECT NOTES COMPUT SC, V2688, P62
   Brunet F, 2014, COMPUT VIS IMAGE UND, V125, P138, DOI 10.1016/j.cviu.2014.04.003
   Chhatkuli A., 2014, P BRIT MACH VIS C
   Chhatkuli A, 2014, PROC CVPR IEEE, P708, DOI 10.1109/CVPR.2014.96
   Crane K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516977
   Dai YC, 2012, PROC CVPR IEEE, P2018, DOI 10.1109/CVPR.2012.6247905
   Del Bue A., 2006, PROC IEEE C COMPUTER, V1, P1191
   Del Bue A, 2011, IEEE I CONF COMP VIS, P675, DOI 10.1109/ICCV.2011.6126303
   Elad A, 2003, IEEE T PATTERN ANAL, V25, P1285, DOI 10.1109/TPAMI.2003.1233902
   Garg R, 2013, PROC CVPR IEEE, P1272, DOI 10.1109/CVPR.2013.168
   Garg R, 2013, INT J COMPUT VISION, V104, P286, DOI 10.1007/s11263-012-0607-7
   Garrido P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508380
   Golub G.H., 1989, MATRIX COMPUTATIONS
   Gotardo P. F. U., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3065, DOI 10.1109/CVPR.2011.5995560
   Gotardo PFU, 2011, IEEE I CONF COMP VIS, P802, DOI 10.1109/ICCV.2011.6126319
   Jain V, 2007, COMPUT AIDED DESIGN, V39, P398, DOI 10.1016/j.cad.2007.02.009
   Jolliffe I. T., 2002, PRINCIPAL COMPONENT
   Jongwoo Lim, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3489, DOI 10.1109/CVPR.2011.5995511
   Khan I, 2014, IEEE T MULTIMEDIA, V16, P1350, DOI 10.1109/TMM.2014.2308415
   Klein George, 2007, P1
   Lee M, 2014, PROC CVPR IEEE, P1550, DOI 10.1109/CVPR.2014.201
   Lee M, 2013, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR.2013.169
   Marques M., 2008, P IEEE WORKSH MOT VI, P1
   Moreno-Noguer F, 2011, PROC CVPR IEEE, P1289, DOI 10.1109/CVPR.2011.5995532
   Mouragnon E, 2009, IMAGE VISION COMPUT, V27, P1178, DOI 10.1016/j.imavis.2008.11.006
   Müller M, 2005, ACM T GRAPHIC, V24, P471, DOI 10.1145/1073204.1073216
   Östlund J, 2012, LECT NOTES COMPUT SC, V7574, P412, DOI 10.1007/978-3-642-33712-3_30
   Paladini M, 2010, LECT NOTES COMPUT SC, V6312, P15, DOI 10.1007/978-3-642-15552-9_2
   Paladini M, 2009, PROC CVPR IEEE, P2890
   Park HS, 2010, LECT NOTES COMPUT SC, V6313, P158
   PENTLAND A, 1991, IEEE T PATTERN ANAL, V13, P730, DOI 10.1109/34.85661
   Perriollat M, 2011, INT J COMPUT VISION, V95, P124, DOI 10.1007/s11263-010-0352-8
   Pinkall U., 1993, Exp. Math., V2, P15, DOI 10.1080/10586458.1993.10504266
   Salzmann M., 2010, Synthesis Lectures on Computer Vision, V2, P1
   Salzmann M, 2009, PROC CVPR IEEE, P1054, DOI 10.1109/CVPRW.2009.5206759
   SCLAROFF S, 1995, IEEE T PATTERN ANAL, V17, P545, DOI 10.1109/34.387502
   Smeets D, 2012, PATTERN RECOGN, V45, P2817, DOI 10.1016/j.patcog.2012.01.020
   Tao LL, 2013, COMPUT VIS IMAGE UND, V117, P1287, DOI 10.1016/j.cviu.2013.03.005
   Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752
   Valmadre J, 2012, PROC CVPR IEEE, P1394, DOI 10.1109/CVPR.2012.6247826
   Vicente S, 2012, LECT NOTES COMPUT SC, V7574, P426, DOI 10.1007/978-3-642-33712-3_31
   Wendel A, 2012, PROC CVPR IEEE, P1450, DOI 10.1109/CVPR.2012.6247833
   Xiao J, 2004, PROC CVPR IEEE, P535
   Zhou Z, 2015, IEEE T MULTIMEDIA, V17, P171, DOI 10.1109/TMM.2014.2384396
   Zhu JK, 2006, LECT NOTES COMPUT SC, V3951, P186
   Zhu YY, 2014, PROC CVPR IEEE, P1542, DOI 10.1109/CVPR.2014.200
NR 64
TC 3
Z9 3
U1 1
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2019
VL 21
IS 4
BP 821
EP 834
DI 10.1109/TMM.2018.2870081
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HQ4XD
UT WOS:000462413700002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yang, P
   Zhang, N
   Zhang, S
   Yu, L
   Zhang, JS
   Shen, XM
AF Yang, Peng
   Zhang, Ning
   Zhang, Shan
   Yu, Li
   Zhang, Junshan
   Shen, Xuemin (Sherman)
TI Content Popularity Prediction Towards Location-Aware Mobile Edge Caching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mobile edge computing; dynamic content caching; popularity prediction;
   location awareness
ID SMALL-CELL; WIRELESS; INFORMATION; NETWORKS; DELIVERY; DYNAMICS; CLOUD
AB Mobile edge caching aims to enable content delivery within the radio access network, which effectively alleviates the backhaul burden and reduces response time. To fully exploit edge storage resources, the most popular contents should be identified and cached. Observing that user demands on certain contents vary greatly at different locations, this paper devises location-customized caching schemes to maximize the total content hit rate. Specifically, a linear model is used to estimate the future content hit rate. For the case with zero-mean noise, a ridge regression-based online algorithm with positive perturbation is proposed. Regret analysis indicates that the hit rate achieved by the proposed algorithm asymptotically approaches that of the optimal caching strategy in the long run. When the noise structure is unknown, an H-infinity filter-based online algorithm is devised by taking a prescribed threshold as input, which guarantees prediction accuracy even under the worst-case noise process. Both online algorithms require no training phases and, hence, are robust to the time-varying user demands. The estimation errors of both algorithms are numerically analyzed. Moreover, extensive experiments using real-world datasets are conducted to validate the applicability of the proposed algorithms. It is demonstrated that those algorithms can be applied to scenarios with different noise features, and are able to make adaptive caching decisions, achieving a content hit rate that is comparable to that via the hindsight optimal strategy.
C1 [Yang, Peng; Yu, Li] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Hubei, Peoples R China.
   [Zhang, Ning] Texas A&M Univ Corpus Christi, Dept Comp Sci, Corpus Christi, TX 78412 USA.
   [Zhang, Shan] Beihang Univ, Beijing Key Lab Comp Networks, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
   [Zhang, Junshan] Arizona State Univ, Sch Elect Comp & Energy Engn, Tempe, AZ 85287 USA.
   [Shen, Xuemin (Sherman)] Univ Waterloo, Dept Elect & Comp Engn, Waterloo, ON N2L 3G1, Canada.
C3 Huazhong University of Science & Technology; Texas A&M University
   System; Texas A&M University Corpus Christi; Beihang University; Arizona
   State University; Arizona State University-Tempe; University of Waterloo
RP Yu, L (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Hubei, Peoples R China.
EM yangpeng@hust.edu.cn; ning.zhang@tamucc.edu; zhangshan18@buaa.edu.cn;
   hustlyu@hust.edu.cn; junshan.zhang@asu.edu; sshen@uwaterloo.ca
RI Zhang, Ning/T-5396-2018; Shen, Xuemin/AAH-2564-2020
OI Zhang, Ning/0000-0002-8781-4925; Shen, Xuemin/0000-0002-4140-287X
FU National Natural Science Foundation of China [61871437, 61801011];
   Natural Sciences and Engineering Research Council of Canada; China
   Scholarship Council
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61871437 and 61801011 and in part by
   the Natural Sciences and Engineering Research Council of Canada. The
   work of P. Yang was supported by the China Scholarship Council. This
   paper was presented in part at the IEEE Global Communications
   Conference, Singapore, Dec. 4-8, 2017. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Xiaoqing Zhu. (Corresponding author: Li Yu.)
CR Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312
   Akhshabi Saamer, 2013, Proceeding of the 23rd ACM Workshop on Network and Operating Systems Support for Digital Audio and Video-NOSSDAV'13
   [Anonymous], IMC07 P 2007 ACM
   [Anonymous], 2015, P 16 ACM INT S MOB A
   [Anonymous], 2016, 002 ETSI GS MEC
   [Anonymous], P 26 INT WORKSH NETW
   Bharath BN, 2016, IEEE T COMMUN, V64, P1674, DOI 10.1109/TCOMM.2016.2536728
   Blasco P, 2014, IEEE ICC, P1897, DOI 10.1109/ICC.2014.6883600
   Cai J, 2004, IEEE T WIREL COMMUN, V3, P2060, DOI 10.1109/TWC.2004.837295
   Chu Wei, 2011, P 14 INT C ARTIFICIA, P208
   Dhar S, 2011, COMMUN ACM, V54, P121, DOI 10.1145/1941487.1941515
   Feng YY, 2015, FOUND TRENDS SIGNAL, V9, P1, DOI 10.1561/2000000072
   Friedman J., 2008, ELEMENTS STAT LEARNI, VSecond
   Gregori M, 2016, IEEE J SEL AREA COMM, V34, P1222, DOI 10.1109/JSAC.2016.2545413
   Hu W, 2016, J COMPUT SCI TECH-CH, V31, P1072, DOI 10.1007/s11390-016-1683-x
   Ipsen ICF, 2009, SIAM J MATRIX ANAL A, V31, P40, DOI 10.1137/070682745
   Ji MY, 2016, IEEE J SEL AREA COMM, V34, P176, DOI 10.1109/JSAC.2015.2452672
   Li SH, 2016, IEEE T MULTIMEDIA, V18, P2503, DOI 10.1109/TMM.2016.2596042
   Liang B, 2017, KEY TECHNOLOGIES FOR 5G WIRELESS SYSTEMS, P76
   Liu A, 2015, IEEE T SIGNAL PROCES, V63, P57, DOI 10.1109/TSP.2014.2367473
   Lu Z, 2018, IEEE T MULTIMEDIA, V20, P1848, DOI 10.1109/TMM.2017.2772802
   Luan TH, 2010, IEEE T MULTIMEDIA, V12, P64, DOI 10.1109/TMM.2009.2036294
   Ma G, 2017, IEEE INT CON MULTI, P7, DOI 10.1109/ICME.2017.8019404
   Ma G, 2017, IEEE J SEL AREA COMM, V35, P1076, DOI 10.1109/JSAC.2017.2680958
   Müller S, 2017, IEEE T WIREL COMMUN, V16, P1024, DOI 10.1109/TWC.2016.2636139
   Roy SD, 2013, IEEE T MULTIMEDIA, V15, P1255, DOI 10.1109/TMM.2013.2265079
   Shanmugam K, 2013, IEEE T INFORM THEORY, V59, P8402, DOI 10.1109/TIT.2013.2281606
   Simon D., 2006, OPTIMAL STATE ESTIMA, DOI [10.1002/0470045345, DOI 10.1002/0470045345.CH11]
   Tandon R, 2016, IEEE COMMUN MAG, V54, P44, DOI 10.1109/MCOM.2016.7537176
   Tao MX, 2016, IEEE T WIREL COMMUN, V15, P6118, DOI 10.1109/TWC.2016.2578922
   Tong L., 2016, P 35 ANN IEEE INT C, P1
   Wang XF, 2014, IEEE COMMUN MAG, V52, P131, DOI 10.1109/MCOM.2014.6736753
   Yang P., 2017, 2017 Asia Communications and Photonics Conference (ACP), P1
   Yang P, 2017, IEEE NETWORK, V31, P14, DOI 10.1109/MNET.2017.1600078
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zhang S, 2017, IEEE ICC
   Zhang S, 2017, IEEE T VEH TECHNOL, V66, P11264, DOI 10.1109/TVT.2017.2724547
   Zhou YP, 2015, IEEE T MULTIMEDIA, V17, P1273, DOI 10.1109/TMM.2015.2447277
   Zhuang WH, 1999, IEEE T VEH TECHNOL, V48, P126, DOI 10.1109/25.740072
NR 39
TC 137
Z9 142
U1 0
U2 48
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2019
VL 21
IS 4
BP 915
EP 929
DI 10.1109/TMM.2018.2870521
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HQ4XD
UT WOS:000462413700009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Rana, A
   Valenzise, G
   Dufaux, F
AF Rana, Aakanksha
   Valenzise, Giuseppe
   Dufaux, Frederic
TI Learning-Based Tone Mapping Operator for Efficient Image Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE High dynamic range; tone mapping operator; image matching; stochastic
   gradient descent; machine learning
ID PERFORMANCE; RETRIEVAL; FEATURES
AB In this paper, we propose a new framework to optimally tone map the high dynamic range (HDR) content for image matching under drastic illumination variations. Since tone mapping operators (TMO) have traditionally been used for displaying HDR scenes, their design is suboptimal when used for computer vision tasks, such as image matching. We address this suboptimality by proposing a two-step framework, consisting of: first, a luminance-invariant guidance model based on a support vector regressor (SVR) to optimally adapt the tone mapping function for image matching; and second, an energy maximization model to generate appropriate training samples for learning the SVR. At each step, we collectively address both stages of keypoint detection and descriptor extraction in the feature matching framework. By locally altering the intrinsic characteristics of the tone mapping function, the learned guidance model facilitates the extraction of local invariant features in the presence of illumination variations. We demonstrate that the proposed TMO significantly outperforms perceptually driven state-of-the-art TMOs on a dataset of HDR scenes characterized by challenging lighting variations, such as day/night transitions.
C1 [Rana, Aakanksha] Univ Paris Saclay, Lab Traitement & Commun Informat, Telecom ParisTech, F-75013 Paris, France.
   [Valenzise, Giuseppe; Dufaux, Frederic] Univ Paris Sud, L2S, UMR 8506, CNRS,Cent Supelec, F-91400 Paris, France.
C3 Universite Paris Cite; Universite Paris Saclay; IMT - Institut
   Mines-Telecom; Institut Polytechnique de Paris; Telecom Paris; Centre
   National de la Recherche Scientifique (CNRS); Universite Paris Cite;
   Universite Paris Saclay
RP Rana, A (corresponding author), Univ Paris Saclay, Lab Traitement & Commun Informat, Telecom ParisTech, F-75013 Paris, France.
EM aakanksha.rana@telecom-paristech.fr;
   giuseppe.valenzise@l2s.centralesupelec.fr;
   frederic.dufaux@l2s.centralesupelec.fr
RI Dufaux, Frederic/HJJ-1496-2023; Rana, Aakanksha/AAT-4734-2021
OI Dufaux, Frederic/0000-0001-6388-4112; 
FU BPIFrance; Region Ile-de-France, in the framework of the FUI 18 Plein
   Phare Project
FX This work was supported in part by the BPIFrance and in part by the
   Region Ile-de-France, in the framework of the FUI 18 Plein Phare
   Project.
CR Agrafiotis Panagiotis, 2015, 10th International Conference on Computer Vision Theory and Applications (VISAPP 2015). Proceedings, P623
   Alahi A, 2012, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2012.6247715
   Altintakan UL, 2015, IEEE T MULTIMEDIA, V17, P323, DOI 10.1109/TMM.2014.2388312
   [Anonymous], 2014, AS C COMP VIS ACCV
   [Anonymous], 2008, An Open and Portable Library of Computer Vision Algorithms
   [Anonymous], 2002, Introduction to MPEG-7: Multimedia Content Description Interface
   Banterle F, 2011, ADVANCED HIGH DYNAMIC RANGE IMAGING: THEORY AND PRACTICE, P1
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bober M, 2001, IEEE T CIRC SYST VID, V11, P716, DOI 10.1109/76.927426
   Boschetti A, 2011, EUR SIGNAL PR CONF, P274
   Bottou L., 2012, Neural networks: Tricks of the trade, P421, DOI DOI 10.1007/978-3-642-35289-8_25
   Cadík M, 2008, COMPUT GRAPH-UK, V32, P330, DOI 10.1016/j.cag.2008.04.003
   Chalmers A, 2017, SIGNAL PROCESS-IMAGE, V54, P49, DOI 10.1016/j.image.2017.02.003
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chapelle O, 2010, INFORM RETRIEVAL, V13, P216, DOI 10.1007/s10791-009-9110-3
   Chermak L., 2012, P 11 IEEE INT C CYB, P64
   CHIU K, 1993, GRAPH INTER, P245
   Debevec Paul E, 2008, ACM SIGGRAPH 2008 CL, P1, DOI DOI 10.1145/1401132.1401174
   Dong JM, 2015, PROC CVPR IEEE, P5097, DOI 10.1109/CVPR.2015.7299145
   Dong YY, 2016, IEEE T MULTIMEDIA, V18, P549, DOI 10.1109/TMM.2016.2522639
   Drago F, 2003, COMPUT GRAPH FORUM, V22, P419, DOI 10.1111/1467-8659.00689
   Dufaux F., 2016, High Dynamic Range Video: From Acquisition, to Display and Applications
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Fattal R, 2002, ACM T GRAPHIC, V21, P249
   Förstner W, 2009, IEEE I CONF COMP VIS, P2256, DOI 10.1109/ICCV.2009.5459458
   Hadizadeh H, 2018, IEEE T MULTIMEDIA, V20, P392, DOI 10.1109/TMM.2017.2740023
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   Hayat T, 2016, 2016 INTERNATIONAL CONFERENCE ON COMPUTING, ELECTRONIC AND ELECTRICAL ENGINEERING (ICE CUBE), P1, DOI 10.1109/ICECUBE.2016.7495203
   Hong RC, 2016, IEEE T IMAGE PROCESS, V25, P1124, DOI 10.1109/TIP.2016.2514499
   Hunter J., 2002, P ARCH MUS INF
   Jinno T., 2013, P 21 EUR SIGN PROC C, P1
   Kontogianni G, 2015, INT ARCH PHOTOGRAMM, V40-5, P325, DOI 10.5194/isprsarchives-XL-5-W4-325-2015
   Korshunov P., 2015, Proceedings of the Fourth International Workshop on Crowdsourcing for Multimedia, P39
   Ledda P, 2005, ACM T GRAPHIC, V24, P640, DOI 10.1145/1073204.1073242
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Lin YT, 2017, IEEE T MULTIMEDIA, V19, P196, DOI 10.1109/TMM.2016.2605499
   Liu Z, 2015, IEEE T MULTIMEDIA, V17, P538, DOI 10.1109/TMM.2015.2399851
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mai ZC, 2013, IEEE T MULTIMEDIA, V15, P1503, DOI 10.1109/TMM.2013.2266633
   Makantasis K, 2016, MULTIMED TOOLS APPL, V75, P3593, DOI 10.1007/s11042-014-2191-z
   Mantiuk R., 2006, ACM Transactions on Applied Perception, V3, P286, DOI DOI 10.1145/1166087.1166095
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Ntregka Georgopoulos., 2013, ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, VII-5/W1, P209, DOI DOI 10.5194/ISPRSANNALS-II-5-W1-209-2013
   Pribyl B., 2012, SPRING C COMP GRAPH, P156
   Pribyl B, 2016, J VIS COMMUN IMAGE R, V38, P141, DOI 10.1016/j.jvcir.2016.02.007
   Rana A, 2017, IEEE IMAGE PROC, P2374, DOI 10.1109/ICIP.2017.8296707
   Rana A, 2017, IEEE INT CON MULTI, P337, DOI 10.1109/ICME.2017.8019394
   Rana A, 2015, IEEE INT SYM MULTIM, P289, DOI 10.1109/ISM.2015.58
   Reinhard E., 2002, Journal of Graphics Tools, V7, P45, DOI 10.1080/10867651.2002.10487554
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Reinhard E., 2005, The Morgan Kaufmann Series in Computer Graphics
   Rerabek M., 2014, P SPIE, V9217
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Schmid C, 2000, INT J COMPUT VISION, V37, P151, DOI 10.1023/A:1008199403446
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Suma R, 2016, VIRTUAL ARCHAEOL REV, V7, P54, DOI 10.4995/var.2016.6319
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Tuytelaars T., 2008, LOCAL INVARIANT FEAT
   Verdie Y, 2015, PROC CVPR IEEE, P5279, DOI 10.1109/CVPR.2015.7299165
   Wang TH, 2015, IEEE T MULTIMEDIA, V17, P470, DOI 10.1109/TMM.2015.2403612
   Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28
   Zagoruyko S., 2015, PROC CVPR IEEE, P4353, DOI DOI 10.1109/CVPR.2015.7299064
   Zhou H, 2016, LECT NOTES COMPUT SC, V9915, P724, DOI 10.1007/978-3-319-49409-8_60
NR 67
TC 29
Z9 29
U1 2
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2019
VL 21
IS 1
BP 256
EP 268
DI 10.1109/TMM.2018.2839885
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HF5DV
UT WOS:000454253700022
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, JY
   Yang, S
   Fang, YM
   Guo, ZM
AF Liu, Jiaying
   Yang, Shuai
   Fang, Yuming
   Guo, Zongming
TI Structure-Guided Image Inpainting Using Homography Transformation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image inpainting; image self-similarity; homography transformation;
   linear structure; image completion
ID COMPLETION
AB In this paper, we present a novel structure-guided framework for exemplar-based image inpainting to maintain the neighborhood consistence and structure coherence of an inpainted region. The proposed method consists of a data term for pixel validity and boundary continuity, a smoothness term to depict the compatibility of neighboring pixels for contextual continuity, and a coherence term to investigate image inherent regularities to ensure image self-similarity. To better reconstruct image structures, the method utilizes image regularity statistics to extract dominant linear structures of the target image. Guided by these structures, homography transformations are estimated and combined to globally repair the missing region using the Markov random field model. To reduce computational complexity, a hierarchical process is implemented to utilize the regularity effectively. The experimental results demonstrate that our method yields better results for various real-world scenes than existing state-of-the-art image inpainting techniques.
C1 [Liu, Jiaying; Yang, Shuai; Fang, Yuming; Guo, Zongming] Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.
C3 Peking University
RP Liu, JY (corresponding author), Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.
EM liujiaying@pku.edu.cn; williamyang@plat.edu.cn; FA0001NG@e.ntu.edu.sg;
   guozongming@pku.edu.cn
RI Liu, JY/GYJ-0138-2022
OI Liu, Jiaying/0000-0002-0468-9576
FU National Natural Science Foundation of China [61772043]; CCF-Tencent
   Open Research Fund
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61772043 and in part by the CCF-Tencent
   Open Research Fund.
CR [Anonymous], MSRTR200404
   [Anonymous], ACM T GRAPH
   [Anonymous], 2001, COMP SCI W
   Barnes C., 2010, LECT NOTES COMPUT SC, V6313, P29
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Bugeau A, 2010, IEEE T IMAGE PROCESS, V19, P2634, DOI 10.1109/TIP.2010.2049240
   Chan TF, 2001, J VIS COMMUN IMAGE R, V12, P436, DOI 10.1006/jvci.2001.0487
   Chan TF, 2002, SIAM J APPL MATH, V62, P1019, DOI 10.1137/S0036139900368844
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Fang CW, 2009, IEEE T IMAGE PROCESS, V18, P2769, DOI 10.1109/TIP.2009.2027635
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Guillemot C, 2014, IEEE SIGNAL PROC MAG, V31, P127, DOI 10.1109/MSP.2013.2273004
   He KM, 2014, IEEE T PATTERN ANAL, V36, P2423, DOI 10.1109/TPAMI.2014.2330611
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508373
   Huang J., 2013, PROC INT C LOCALIZAT, P1
   Huang JB, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601205
   Komodakis N, 2007, IEEE T IMAGE PROCESS, V16, P2649, DOI 10.1109/TIP.2007.906269
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Le Meur O., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3401, DOI 10.1109/ICIP.2011.6116441
   Le Meur O, 2013, IEEE T IMAGE PROCESS, V22, P3779, DOI 10.1109/TIP.2013.2261308
   Li ZD, 2015, IEEE T IMAGE PROCESS, V24, P1138, DOI 10.1109/TIP.2014.2383322
   Liu YQ, 2013, IEEE T IMAGE PROCESS, V22, P1699, DOI 10.1109/TIP.2012.2218828
   Martinez- Noriega R., 2012, PROC MACH LEARN SIGN, P1
   Mosleh A, 2012, IEEE T MULTIMEDIA, V14, P1591, DOI 10.1109/TMM.2012.2198802
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   Ruzic T, 2015, IEEE T IMAGE PROCESS, V24, P444, DOI 10.1109/TIP.2014.2372479
   Simakov D, 2008, PROC CVPR IEEE, P3887
   Sun J, 2005, ACM T GRAPHIC, V24, P861, DOI 10.1145/1073204.1073274
   Tang NC, 2011, IEEE T MULTIMEDIA, V13, P602, DOI 10.1109/TMM.2011.2112642
   Tsai A, 2001, IEEE T IMAGE PROCESS, V10, P1169, DOI 10.1109/83.935033
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Wong A, 2008, IEEE IMAGE PROC, P2600, DOI 10.1109/ICIP.2008.4712326
   Xu ZB, 2010, IEEE T IMAGE PROCESS, V19, P1153, DOI 10.1109/TIP.2010.2042098
NR 36
TC 38
Z9 43
U1 2
U2 42
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2018
VL 20
IS 12
BP 3252
EP 3265
DI 10.1109/TMM.2018.2831636
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HA4EY
UT WOS:000450212600006
DA 2024-07-18
ER

PT J
AU Huang, MX
   Li, JJ
   Ngai, G
   Leong, HV
   Hua, KA
AF Huang, Michael Xuelin
   Li, Jiajia
   Ngai, Grace
   Leong, Hong Va
   Hua, Kien A.
TI Fast-PADMA: Rapidly Adapting Facial Affect Mode From Similar Individuals
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Affective computing; facial affect; rapid modeling; user-adaptive model
ID EXPRESSION; CLASSIFICATION
AB A user-specific model generally performs better in facial affect recognition. Existing solutions, however, have usability issues since the annotation can be long and tedious for the end users (e.g., consumers). We address this critical issue by presenting a more user-friendly user-adaptive model to make the personalized approach more practical. This paper proposes a novel user-adaptive model, which we have called fast-Personal Affect Detection with Minimal Annotation (Fast-PADMA). Fast-PADMA integrates data from multiple source subjects with a small amount of data from the target subject. Collecting this target subject data is feasible since fast-PADMA requires only one self-reported affect annotation per facial video segment. To alleviate overfitting in this context of limited individual training data, we propose an efficient bootstrapping technique, which strengthens the contribution of multiple similar source subjects. Specifically, we employ an ensemble classifier to construct pretrained weak generic classifiers from data of multiple source subjects, which is weighted according to the available data from the target user. The result is a model that does not require expensive computation, such as distribution dissimilarity calculation or model retraining. We evaluate our method with in-depth experimental evaluations on five publicly available facial datasets, with results that compare favorably with the state-of-the-art performance on classifying pain, arousal, and valence. Our findings show that fast-PADMA is effective at rapidly constructing a user-adaptive model that outperforms both its generic and user-specific counterparts. This efficient technique has the potential to significantly improve user-adaptive facial affect recognition for personal use and, therefore, enable comprehensive affect-aware applications.
C1 [Huang, Michael Xuelin] Max Planck Inst Informat, D-66123 Saarbrucken, Germany.
   [Li, Jiajia; Ngai, Grace; Leong, Hong Va] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Hong Kong, Peoples R China.
   [Hua, Kien A.] Univ Cent Florida, Sch Elect Engn & Comp Sci, Orlando, FL 32816 USA.
C3 Max Planck Society; Hong Kong Polytechnic University; State University
   System of Florida; University of Central Florida
RP Li, JJ (corresponding author), Hong Kong Polytech Univ, Dept Comp, Hong Kong, Hong Kong, Peoples R China.
EM mhuang@mpi-inf.mpg.de; lijiajia.simg@gmail.com;
   csgngai@comp.polyu.edu.hk; cshleong@comp.polyu.edu.hk;
   kienhua@cs.ucf.edu
RI ; Ngai, Grace/A-1846-2014
OI LEONG, Hong-va/0000-0001-7682-9032; Ngai, Grace/0000-0002-2027-168X
FU Hong Kong Research Grant Council; Hong Kong Polytechnic University
   [PolyU 5222/13E]
FX The work was conducted while the authors were with The Hong Kong
   Polytechnic University. This work was supported in part by the Hong Kong
   Research Grant Council and in part by The Hong Kong Polytechnic
   University under Grant PolyU 5222/13E. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Chengcui Zhang.
CR Al-Stouhi S, 2011, LECT NOTES ARTIF INT, V6911, P60, DOI 10.1007/978-3-642-23780-5_14
   Almaev T., 2016, P IEEE INT C COMP VI, P3774
   ALTMAN NS, 1992, AM STAT, V46, P175, DOI 10.2307/2685209
   [Anonymous], 2012, ACM Trans. Knowl. Discov. Data, DOI DOI 10.1145/2382577.2382582
   [Anonymous], BMVC 2014
   [Anonymous], ARXIV170105360
   [Anonymous], 2010, Proceedings of 2010 IEEE Conference on Computer Vision and Pattern Recognition, DOI DOI 10.1109/CVPR.2010.5539857
   Ashraf AB, 2009, IMAGE VISION COMPUT, V27, P1788, DOI 10.1016/j.imavis.2009.05.007
   Bernhardt D, 2007, LECT NOTES COMPUT SC, V4738, P59
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Chen JX, 2013, PATTERN RECOGN LETT, V34, P1964, DOI 10.1016/j.patrec.2013.02.002
   Chen YX, 2006, IEEE T PATTERN ANAL, V28, P1931, DOI 10.1109/TPAMI.2006.248
   Cheplygina V, 2016, IEEE T NEUR NET LEAR, V27, P1379, DOI 10.1109/TNNLS.2015.2424254
   Chu Wen-Sheng, 2013, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, V2013, P3515
   Dai Wenyuan, 2007, P 24 INT C MACHINE L, P193
   Dhall A, 2012, IEEE MULTIMEDIA, V19, P34, DOI 10.1109/MMUL.2012.26
   Duan LX, 2012, IEEE T NEUR NET LEAR, V23, P504, DOI 10.1109/TNNLS.2011.2178556
   Eleftheriadis S., 2016, P IEEE C CVPR WORKSH, P18
   Fontaine JRJ, 2007, PSYCHOL SCI, V18, P1050, DOI 10.1111/j.1467-9280.2007.02024.x
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Fu ZY, 2011, IEEE T PATTERN ANAL, V33, P958, DOI 10.1109/TPAMI.2010.155
   Gheisari M, 2015, NEUROCOMPUTING, V165, P300, DOI 10.1016/j.neucom.2015.03.020
   Huang MX, 2016, IEEE T AFFECT COMPUT, V7, P360, DOI 10.1109/TAFFC.2015.2495222
   Huang XH, 2016, COMPUT VIS IMAGE UND, V147, P114, DOI 10.1016/j.cviu.2015.09.015
   Keerthi SS, 2001, NEURAL COMPUT, V13, P637, DOI 10.1162/089976601300014493
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Lucey P, 2012, IMAGE VISION COMPUT, V30, P197, DOI 10.1016/j.imavis.2011.12.003
   Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Romera-Paredes B., 2013, 2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, P1
   Sangineto E, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P357, DOI 10.1145/2647868.2654916
   Sariyanidi E, 2015, IEEE T PATTERN ANAL, V37, P1113, DOI 10.1109/TPAMI.2014.2366127
   Sikka K, 2016, PROC CVPR IEEE, P5580, DOI 10.1109/CVPR.2016.602
   Sikka K, 2014, IMAGE VISION COMPUT, V32, P659, DOI 10.1016/j.imavis.2014.02.008
   Soleymani M, 2012, IEEE T AFFECT COMPUT, V3, P42, DOI 10.1109/T-AFFC.2011.25
   Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463
   Valstar MF, 2012, IEEE T SYST MAN CY B, V42, P966, DOI 10.1109/TSMCB.2012.2200675
   Viola P., 2006, P ADV NEUR INF PROC
   Werner P, 2017, IEEE T AFFECT COMPUT, V8, P286, DOI 10.1109/TAFFC.2016.2537327
   Wu CC, 2015, IEEE GLOB COMM CONF, DOI [10.1109/FG.2015.7163116, 10.1109/GLOCOM.2015.7417352]
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Xiao YS, 2014, IEEE T CYBERNETICS, V44, P500, DOI 10.1109/TCYB.2013.2257749
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yan K., IEEE T CYBERN, P1
   Zen G, 2016, IEEE T MULTIMEDIA, V18, P775, DOI 10.1109/TMM.2016.2523421
NR 46
TC 2
Z9 2
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1901
EP 1915
DI 10.1109/TMM.2017.2775206
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100024
DA 2024-07-18
ER

PT J
AU Ling, ZG
   Gong, JW
   Fan, GL
   Lu, X
AF Ling, Zhigang
   Gong, Jianwei
   Fan, Guoliang
   Lu, Xiao
TI Optimal Transmission Estimation via Fog Density Perception for Efficient
   Single Image Defogging
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Single image defogging; fog-relevant features; fog density evaluator;
   optimal transmission model
ID QUALITY ASSESSMENT; VISION; HAZE; STATISTICS; VISIBILITY; FRAMEWORK;
   WEATHER; SPACE; MODEL
AB Single image defogging algorithms based on prior assumptions or constraints have captured much attention because of their simplicity and practicality. However, they still have some challenges to deal with foggy images captured under weather conditions where these assumptions or constraints may not be effective or efficient enough. In this paper, we aim to develop a novel image defogging algorithm by directly predicting the fog density of recovered images rather than adopting prior assumptions or constraints. In order to achieve this goal, two specific steps are introduced. First, we adopt three fog-relevant statistical features derived from foggy images, and further develop a simple fog density evaluator (SFDE) by creating a linear combination of these fog-relevant features. This proposed evaluator can efficiently perceive the fog density of a single image without reference to a corresponding fog-free image and has a low computational load compared with an existing method. Second, a physics-based mathematical relationship between the transmission and the fog density score of the recovered image is developed via SFDE, thus image defogging can be posed as a minimization problem on the fog density score of the recovered image. As a result, two optimal transmission models, called an optimal transmission model via SFDE (OTSFDE) and a simpler optimal transmission models via SFDE (SOTSFDE), are present to determine the key transmission map for efficient fog removal. Compared to OTSFDE, SOTSFDE has low computational complexity with slight performance degradation. Experimental results demonstrate that the proposed algorithms can effectively remove fog and are not confined by any assumptions or constraints, both quantitatively and qualitatively, compared with some existing algorithms.
C1 [Ling, Zhigang; Gong, Jianwei] Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Hunan, Peoples R China.
   [Ling, Zhigang; Gong, Jianwei] Natl Engn Lab Robot Visual Percept & Control Tech, Changsha 410082, Hunan, Peoples R China.
   [Fan, Guoliang] Xian Univ Technol, Sch Automat & Informat Engn, Xian 710048, Shaanxi, Peoples R China.
   [Fan, Guoliang] Oklahoma State Univ, Sch Elect & Comp Engn, Stillwater, OK 74078 USA.
   [Lu, Xiao] Hunan Normal Univ, Coll Engn & Design, Changsha 410081, Hunan, Peoples R China.
C3 Hunan University; Xi'an University of Technology; Oklahoma State
   University System; Oklahoma State University - Stillwater; Hunan Normal
   University
RP Ling, ZG (corresponding author), Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Hunan, Peoples R China.
EM zgling_hunan@126.com; 307273128@qq.com; guoliang.fan@okstate.edu;
   xlu_hnu@163.com
RI Gong, Jianwei/I-1710-2013; lu, xiao/AAS-2542-2020; Fan,
   Guoliang/G-2893-2011
OI Lu, Xiao/0000-0003-0880-0160; Fan, Guoliang/0000-0002-8584-9040
FU National Natural Science Foundation of China [61471166, 61473227,
   61671204, 61703155]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61471166, Grant 61473227, Grant 61671204, and Grant
   61703155.
CR Ancuti CO, 2013, IEEE T IMAGE PROCESS, V22, P3271, DOI 10.1109/TIP.2013.2262284
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Grabner M, 2011, OPT EXPRESS, V19, P3379, DOI 10.1364/OE.19.003379
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Hasler D, 2003, P SOC PHOTO-OPT INS, V5007, P87, DOI 10.1117/12.477378
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   Hautiere N., 2007, 2007 IEEE C COMP VIS, P1
   Hautière N, 2010, IEEE T INTELL TRANSP, V11, P474, DOI 10.1109/TITS.2010.2046165
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Kopf J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409069
   Koschmieder H., 1925, Beitr. Phys. freien Atm, V12, P33
   Lai YH, 2015, IEEE T CIRC SYST VID, V25, P1, DOI 10.1109/TCSVT.2014.2329381
   Ling ZG, 2017, NEUROCOMPUTING, V224, P82, DOI 10.1016/j.neucom.2016.10.050
   Ling ZG, 2016, IEEE IMAGE PROC, P2296, DOI 10.1109/ICIP.2016.7532768
   Liu M, 2016, NEUROCOMPUTING, V208, P309, DOI 10.1016/j.neucom.2015.12.124
   McKnight D., 2014, P EUR DYN POS C OCT, P1
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Oakley JP, 2007, IEEE T IMAGE PROCESS, V16, P511, DOI 10.1109/TIP.2006.887736
   Pearson K., 1895, P R SOC LOND, V58, P240, DOI 10.1098/rspl.1895.0041
   Reshef DN, 2011, SCIENCE, V334, P1518, DOI 10.1126/science.1205438
   Ruderman DL, 1998, J OPT SOC AM A, V15, P2036, DOI 10.1364/JOSAA.15.002036
   RUDERMAN DL, 1994, NETWORK-COMP NEURAL, V5, P517, DOI 10.1088/0954-898X/5/4/006
   Schechner YY, 2007, IEEE T PATTERN ANAL, V29, P1655, DOI 10.1109/TPAMI.2007.1141
   Schechner YY, 2003, APPL OPTICS, V42, P511, DOI 10.1364/AO.42.000511
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tang KT, 2014, PROC CVPR IEEE, P2995, DOI 10.1109/CVPR.2014.383
   Tarel JP, 2012, IEEE INTEL TRANSP SY, V4, P6, DOI 10.1109/MITS.2012.2189969
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Tarel JP, 2010, IEEE INT VEH SYM, P478, DOI 10.1109/IVS.2010.5548128
   Tripathi AK, 2012, IET IMAGE PROCESS, V6, P966, DOI 10.1049/iet-ipr.2011.0472
   Wang WC, 2017, IEEE T MULTIMEDIA, V19, P1142, DOI 10.1109/TMM.2017.2652069
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu HT, 2014, IEEE T MULTIMEDIA, V16, P68, DOI 10.1109/TMM.2013.2283453
   Xu Z., 2009, 2009 INT C COMP INT, P1
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Yoon I, 2012, IEEE T CONSUM ELECTR, V58, P111, DOI 10.1109/TCE.2012.6170062
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhou JJ, 2013, 2013 2ND INTERNATIONAL SYMPOSIUM ON INSTRUMENTATION AND MEASUREMENT, SENSOR NETWORK AND AUTOMATION (IMSNA), P243, DOI 10.1109/IMSNA.2013.6743260
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 47
TC 22
Z9 25
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1699
EP 1711
DI 10.1109/TMM.2017.2778565
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100009
DA 2024-07-18
ER

PT J
AU Wu, S
   Ji, QJ
   Wang, SF
   Wong, HS
   Yu, ZW
   Xu, Y
AF Wu, Si
   Ji, Qiujia
   Wang, Shufeng
   Wong, Hau-San
   Yu, Zhiwen
   Xu, Yong
TI Semi-Supervised Image Classification With Self-Paced Cross-Task Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image classification; semi-supervised learning; cross-task network;
   self-paced paradigm
AB In a semi-supervised setting, direct training of a deep discriminative model on partially labeled images often suffers from overfitting and poor performance, because only a small number of labeled images are available, and errors in label propagation are, in many cases, inevitable. In this paper, we introduce an auxiliary clustering task to explore the structure of the image data, and judiciously weigh unlabeled data to alleviate the influence of ambiguous data on model training. For this purpose, we propose a cross-task network composed of two streams to jointly learn two tasks: classification and clustering. Based on the model predictions, a large number of pairwise constraints can be generated from unlabeled images, and are fed to the clustering stream. Since pairwise constraints encode weak supervision information, the clustering is tolerant of errors in labeling. Unlabeled images are weighted according to the distances to the clusters discovered, and a better discriminative model is trained on the classification stream associated with a weighted softmax loss. Furthermore, a self-paced learning paradigm is adopted to gradually train our deep model from easy examples to difficult ones. Experimental results on widely used image classification datasets confirm the effectiveness and superiority of the proposed approach.
C1 [Wu, Si; Ji, Qiujia; Wang, Shufeng; Yu, Zhiwen; Xu, Yong] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Guangdong, Peoples R China.
   [Wong, Hau-San] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
C3 South China University of Technology; City University of Hong Kong
RP Wu, S (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Guangdong, Peoples R China.
EM cswusi@scut.edu.cn; ji.qiujia@mail.scut.edu.cn;
   w.shufeng@mail.scut.edu.cn; cshswong@city.edu.hk; zhwyu@scut.edu.cn;
   yxu@scut.edu.cn
OI Yu, Zhiwen/0000-0002-0935-5890; WONG, Hau-San/0000-0002-1530-7529
FU National Natural Science Foundation of China [61502173, 61572199,
   61722205]; Natural Science Foundation of Guangdong Province
   [2016A030310422]; Research Grants Council of the Hong Kong Special
   Administration Region [CityU 11300715]; City University of Hong Kong
   [7004674]
FX This work was supported in part by the National Natural Science
   Foundation of China (Project 61502173, 61572199, 61722205), in part by
   the Natural Science Foundation of Guangdong Province (Project
   2016A030310422), in part by the Research Grants Council of the Hong Kong
   Special Administration Region (Project CityU 11300715), and in part by
   City University of Hong Kong (Project 7004674). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Guillaume Gravid.
CR Andriluka M., 2008, PROC IEEE CONCOMMA V, P1
   [Anonymous], P IEEE WINT C APPL C
   [Anonymous], 2016, ARXIV
   [Anonymous], P INT C LEARN REPR
   [Anonymous], 2015, ARXIV PREPRINT ARXIV
   [Anonymous], PROC CVPR IEEE
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2009, Technical report
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2001, Proceedings of the 18th International Conference on Machine Learning, ICML '01
   [Anonymous], P INT C LEARN REPR
   [Anonymous], 2011, P NEURIPS WORKSH GRA
   [Anonymous], 2004, P 2004WORKSHOP STAT
   [Anonymous], 2015, LASAGNE 1 RELEASE
   [Anonymous], P INT C MACH LEARN
   [Anonymous], 2017, P 2017 IEEE C PATT R
   [Anonymous], IEEE T CIRC IN PRESS
   [Anonymous], 2016, P INT C MACH LEARN
   [Anonymous], 2016, Residual Networks Behave Like Ensembles of Relatively Shallow Networks
   [Anonymous], P INT C MACH LEARN
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Bengio Y., 2009, P 26 ANN INT C MACH, V60, P6, DOI [DOI 10.1145/1553374.1553380, 10.1145/1553374.1553380]
   Bennett KP, 1999, ADV NEUR IN, V11, P368
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Cheng Yanhua, 2016, Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, P3345
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J., IMAGENET LARGE SCALE
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Fergus Rob, 2009, Advances in Neural Information Processing Systems, P522
   Gong C, 2016, IEEE T IMAGE PROCESS, V25, P3249, DOI 10.1109/TIP.2016.2563981
   He D, 2016, ADV NEUR IN, V29
   Hoi S.C., 2008, CVPR, P1, DOI DOI 10.1109/CVPR.2008.4587351
   Jian M, 2016, IEEE T MULTIMEDIA, V18, P458, DOI 10.1109/TMM.2016.2515367
   Jiang L, 2014, ADV NEUR IN, V27
   Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kemp C., 2003, P 16 INT C NEUR INF, P257
   Kingma D. P., 2014, Advances in neural information processing systems, P3581
   Krizhevsky A., 2014, P ADV NEUR INF PROC, P1097
   Kubota Ando Rie, 2007, P 24 INT C MACHINE L, P25
   Lafferty J. D., 2003, P INT C MACH LEARN, P912, DOI DOI 10.5555/3041838.3041953
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Li L, 2012, IEEE T MULTIMEDIA, V14, P1401, DOI 10.1109/TMM.2012.2194993
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Liang JQ, 2017, IEEE T MULTIMEDIA, V19, P1077, DOI 10.1109/TMM.2016.2644862
   Liu CL, 2016, IEEE T CYBERNETICS, V46, P462, DOI 10.1109/TCYB.2015.2403573
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maaloe L., 2016, PMLR
   Meng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3401, DOI 10.1109/CVPR.2011.5995698
   Misra I, 2015, PROC CVPR IEEE, P3593, DOI 10.1109/CVPR.2015.7298982
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Nie FP, 2010, NEURAL COMPUT APPL, V19, P549, DOI 10.1007/s00521-009-0305-8
   Nigam K, 2000, MACH LEARN, V39, P103, DOI 10.1023/A:1007692713085
   Pang JB, 2011, IEEE T IMAGE PROCESS, V20, P1388, DOI 10.1109/TIP.2010.2103951
   Pawan Kumar M., 2010, NIPS
   Pitelis Nikolaos, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P565, DOI 10.1007/978-3-662-44851-9_36
   Ranzato M., 2008, P 25 INT C MACH LEAR, P792, DOI DOI 10.1145/1390156.1390256
   Rasmus Antti, 2015, PROC 28 INT C NEURAL, P3546
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rosenberg C, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P29
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Springenberg JT, 2016, ICLR ICLR
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang XG, 2014, IEEE T PATTERN ANAL, V36, P361, DOI 10.1109/TPAMI.2013.124
   Wang Y., 2009, ADV NEURAL INFORM PR, P2008, DOI DOI 10.1097/EDE.0B013E318231D67A
   Weston J., 2008, P 25 INT C MACH LEAR, P1168
   Yan Y, 2016, IEEE T MULTIMEDIA, V18, P2494, DOI 10.1109/TMM.2016.2602938
   Zeng XY, 2014, LECT NOTES COMPUT SC, V8691, P472, DOI 10.1007/978-3-319-10578-9_31
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhou ZH, 2010, KNOWL INF SYST, V24, P415, DOI 10.1007/s10115-009-0209-z
NR 71
TC 30
Z9 29
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2018
VL 20
IS 4
BP 851
EP 865
DI 10.1109/TMM.2017.2758522
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FZ5HM
UT WOS:000427623000007
DA 2024-07-18
ER

PT J
AU Zhang, CJ
   Cheng, J
   Tian, Q
AF Zhang, Chunjie
   Cheng, Jian
   Tian, Qi
TI Multiview Label Sharing for Visual Representations and Classifications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-view learning; linear transformation; shared space; image
   representation; visual classification
ID IMAGE CLASSIFICATION; LOW-RANK
AB Different views represent different aspects of images. It is more effective to combine them for visual classifications. This paper proposes a novel multiview label sharing method to combine the discriminative power of different views for classifications. Especially, we linearly transfer different views into a shared space for representations. The inter-view similarities are kept in the shared space for each view. We also ensure the intra-view similarities of the same class between different views are preserved in the shared space. We jointly learn the classifiers and transformation matrices by minimizing the summed classification loss along with the inter-view and intra-view similarity constraints. In this paper, the inter-view constraints refer to the similarities between images of the corresponding view, whereas the intra-view constraints refer to the similarities between different views of images with the same semantics. Experimental results and analysis on several public datasets show the effectiveness of the proposed multiview label sharing method for visual classifications.
C1 [Zhang, Chunjie] Chinese Acad Sci, Inst Automat, Res Ctr Brain Inspired Intelligence, Beijing 100190, Peoples R China.
   [Zhang, Chunjie; Cheng, Jian] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Cheng, Jian] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
   [Cheng, Jian] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing 100190, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Texas System; University of Texas at
   San Antonio (UTSA)
RP Zhang, CJ (corresponding author), Chinese Acad Sci, Inst Automat, Res Ctr Brain Inspired Intelligence, Beijing 100190, Peoples R China.
EM chunjie.zhang@ia.ac.cn; jcheng@nlpr.ia.ac.cn; qitian@cs.utsa.edu
RI zhang, chunjie/Z-3035-2019; , chengjian/KGL-5551-2024
OI zhang, chunjie/0000-0002-1161-8995; , chengjian/0000-0003-1289-2758
FU National Natural Science Foundation of China [61303154, 61332016];
   Scientific Research Key Program of Beijing Municipal Commission of
   Education [KZ201610005012]; ARO Grant [W911NF-15-1-0290]; NEC
   Laboratories of America; Blippar; National Science Foundation of China
   [61429201]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61303154 and 61332016, and in part by
   the Scientific Research Key Program of Beijing Municipal Commission of
   Education under Grant KZ201610005012. The work of Q. Tian was supported
   in part by ARO Grant W911NF-15-1-0290, in part by the Faculty Research
   Gift Awards by NEC Laboratories of America and Blippar, and in part by
   the National Science Foundation of China under Grant 61429201. The
   associate editor coordinating the review or this manuscript and
   approving it for publication was Prof. Abdulmotaleb El Saddik.
CR Angelova A, 2013, PROC CVPR IEEE, P811, DOI 10.1109/CVPR.2013.110
   [Anonymous], 2010, Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE, DOI [DOI 10.1109/CVPR.2010.5540018, 10.1109/CVPR.2010.5540018]
   [Anonymous], 2014, 2 INT C LEARN REPR I
   [Anonymous], 2008, P 8 IEEE INT C AUT F
   [Anonymous], 2010, P NIPS
   [Anonymous], 2014, INT C INT C MACHINE
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.37
   [Anonymous], P UNC ART INT
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bo LF, 2013, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2013.91
   Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598
   Chai YN, 2012, LECT NOTES COMPUT SC, V7572, P794, DOI 10.1007/978-3-642-33718-5_57
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chou CL, 2017, IEEE T MULTIMEDIA, V19, P354, DOI 10.1109/TMM.2016.2614426
   Dai Wenyuan., 2009, ANN C NEURAL INFORM, P353
   Ding ZM, 2015, IEEE T IMAGE PROCESS, V24, P4322, DOI 10.1109/TIP.2015.2462023
   Dong J, 2013, PROC CVPR IEEE, P827, DOI 10.1109/CVPR.2013.112
   Gao SH, 2013, IEEE T PATTERN ANAL, V35, P92, DOI 10.1109/TPAMI.2012.63
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Griffin G., 2007, CALTECH 256 OBJECT C
   Han J, 2015, IEEE T IMAGE PROCESS, V24, P5177, DOI 10.1109/TIP.2015.2447735
   Harzallah H, 2009, IEEE I CONF COMP VIS, P237, DOI 10.1109/ICCV.2009.5459257
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang C, 2017, IEEE T MULTIMEDIA, V19, P673, DOI 10.1109/TMM.2016.2631122
   Kan MN, 2012, LECT NOTES COMPUT SC, V7572, P808, DOI 10.1007/978-3-642-33718-5_58
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Landauer TK, 1998, DISCOURSE PROCESS, V25, P259, DOI 10.1080/01638539809545028
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li S, 2016, IEEE T NEUR NET LEAR, V27, P2160, DOI 10.1109/TNNLS.2015.2464090
   Li WX, 2017, IEEE T MULTIMEDIA, V19, P367, DOI 10.1109/TMM.2016.2616279
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1234, DOI 10.1109/TMM.2016.2646180
   Liu QS, 2017, IEEE T IMAGE PROCESS, V26, P452, DOI 10.1109/TIP.2016.2621671
   Ming Shao, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163103
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Perronnin F, 2015, PROC CVPR IEEE, P3743, DOI 10.1109/CVPR.2015.7298998
   Puthenputhussery A, 2017, IEEE T MULTIMEDIA, V19, P1757, DOI 10.1109/TMM.2017.2685179
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Shao M, 2017, IEEE T NEUR NET LEAR, V28, P451, DOI 10.1109/TNNLS.2016.2517014
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sohn K, 2011, IEEE I CONF COMP VIS, P2643, DOI 10.1109/ICCV.2011.6126554
   Wang C, 2016, MULTIMED TOOLS APPL, V75, P9255, DOI 10.1007/s11042-016-3380-8
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang KY, 2016, IEEE T PATTERN ANAL, V38, P2010, DOI 10.1109/TPAMI.2015.2505311
   Wang SP, 2017, IEEE T MULTIMEDIA, V19, P1454, DOI 10.1109/TMM.2017.2663324
   Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929
   Wu J, 2017, IEEE T MULTIMEDIA, V19, P1156, DOI 10.1109/TMM.2017.2652065
   Xie NH, 2010, PROC CVPR IEEE, P2313, DOI 10.1109/CVPR.2010.5539917
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Yuan XT, 2010, PROC CVPR IEEE, P3493, DOI 10.1109/CVPR.2010.5539967
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang CJ, 2010, IEEE T CIRCUITS-I, V57, P2729, DOI 10.1109/TCSI.2010.2046974
   Zhang CJ, 2018, IEEE T NEUR NET LEAR, V29, P4479, DOI 10.1109/TNNLS.2017.2748952
   Zhang CJ, 2018, IEEE T CIRC SYST VID, V28, P1719, DOI 10.1109/TCSVT.2017.2694060
   Zhang CJ, 2018, IEEE T CYBERNETICS, V48, P2012, DOI 10.1109/TCYB.2017.2726079
   Zhang CJ, 2017, IEEE T CIRC SYST VID, V27, P1691, DOI 10.1109/TCSVT.2016.2527380
   Zhang CJ, 2017, IEEE T NEUR NET LEAR, V28, P1550, DOI 10.1109/TNNLS.2016.2545112
   Zhang CJ, 2014, NEUROCOMPUTING, V142, P248, DOI 10.1016/j.neucom.2014.03.059
   Zhang CJ, 2014, COMPUT VIS IMAGE UND, V123, P14, DOI 10.1016/j.cviu.2014.02.013
   Zhang CJ, 2013, PATTERN RECOGN LETT, V34, P1046, DOI 10.1016/j.patrec.2013.02.013
   Zhang CJ, 2011, PROC CVPR IEEE, P1673, DOI 10.1109/CVPR.2011.5995484
   Zhang TZ, 2013, IEEE I CONF COMP VIS, P281, DOI 10.1109/ICCV.2013.42
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
NR 65
TC 27
Z9 28
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2018
VL 20
IS 4
BP 903
EP 913
DI 10.1109/TMM.2017.2759500
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FZ5HM
UT WOS:000427623000011
DA 2024-07-18
ER

PT J
AU Cui, P
   Liu, SW
   Zhu, WW
AF Cui, Peng
   Liu, Shaowei
   Zhu, Wenwu
TI General Knowledge Embedded Image Representation Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image representation learning; knowledge base; multirelational graph
   embedding
AB Image representation learning is a fundamental problem in understanding semantics of images. However, traditional classification-based representation learning methods face the noisy and incomplete problem of the supervisory labels. In this paper, we propose a general knowledge base embedded image representation learning approach, which uses general knowledge graph, which is a multitype relational knowledge graph consisting of human commonsense beyond image space, as external semantic resource to capture the relations of concepts in image representation learning. A relational regularized regression CNN (R-3 CNN) model is designed to jointly optimize the image representation learning problem and knowledge graph embedding problem. In this manner, the learnt representation can capture not only labeled tags but also related concepts of images, which involves more precise and complete semantics. Comprehensive experiments are conducted to investigate the effectiveness and transferability of our approach in tag prediction task, zero-shot tag inference task, and content-based image retrieval task. The experimental results demonstrate that the proposed approach performs significantly better than the existing representation learning methods. Finally, observation of the learnt relations show that our approach can somehow refine the knowledge base to describe images and label the images with structured tags.
C1 [Cui, Peng; Liu, Shaowei; Zhu, Wenwu] Tsinghua Univ, Dept Comp Sci, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Cui, P (corresponding author), Tsinghua Univ, Dept Comp Sci, Beijing 100084, Peoples R China.
EM cuip@tsinghua.edu.cn; liushaowei@mails.tsinghua.edu.cn;
   wwzhu@tsinghua.edu.cn
FU research fund of Tsinghua-Tencent Joint Laboratory for Internet
   Innovation Technology; Young Elite Scientist Sponsorship Program by CAST
FX The authors would like to thank the research fund of Tsinghua-Tencent
   Joint Laboratory for Internet Innovation Technology and the Young Elite
   Scientist Sponsorship Program by CAST.
CR Abdulnabi AH, 2015, IEEE T MULTIMEDIA, V17, P1949, DOI 10.1109/TMM.2015.2477680
   Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111
   [Anonymous], 2015, Deep Residual Learning for Image Recognition
   [Anonymous], 2013, P 26 INT C NEUR INF, DOI DOI 10.5555/2999792.2999923
   [Anonymous], 2013, P 21 ACM INT C MULTI
   [Anonymous], 2013, People's Web Meets NLP
   [Anonymous], CORR
   Arandjelovic R, 2013, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR.2013.207
   Auer S., 2007, Dbpedia: A nucleus for a web of open data
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Gong Yunchao., 2013, Deep convolutional ranking for multilabel image annotation
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P309, DOI 10.1109/ICCV.2009.5459266
   Im DH, 2015, MULTIMED TOOLS APPL, V74, P2273, DOI 10.1007/s11042-014-1855-z
   Izadinia H, 2015, MMCOMMONS'15: PROCEEDINGS OF THE 2015 WORKSHOP ON COMMUNITY-ORGANIZED MULTIMODAL MINING: OPPORTUNITIES FOR NOVEL SOLUTIONS, P13, DOI 10.1145/2814815.2814821
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2
   Li XR, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P879, DOI 10.1145/2766462.2767773
   Li XR, 2009, IEEE T MULTIMEDIA, V11, P1310, DOI 10.1109/TMM.2009.2030598
   Liu SW, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P109, DOI 10.1145/2733373.2806247
   Lu WT, 2013, IEEE T MULTIMEDIA, V15, P1920, DOI 10.1109/TMM.2013.2280895
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Simonyan K., 2014, 14091556 ARXIV
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang DX, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2291
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Yang XS, 2016, IEEE T MULTIMEDIA, V18, P1832, DOI 10.1109/TMM.2016.2582379
   Zhang H., 2017, CORR
   Zhang HW, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1079, DOI 10.1145/2733373.2806286
   Zhang XS, 2015, IEEE T MULTIMEDIA, V17, P1562, DOI 10.1109/TMM.2015.2449660
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
NR 32
TC 48
Z9 52
U1 2
U2 40
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2018
VL 20
IS 1
BP 198
EP 207
DI 10.1109/TMM.2017.2724843
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FQ3WS
UT WOS:000418289700016
DA 2024-07-18
ER

PT J
AU Ioannakis, G
   Koutsoudis, A
   Pratikakis, I
   Chamzas, C
AF Ioannakis, George
   Koutsoudis, Anestis
   Pratikakis, Ioannis
   Chamzas, Christodoulos
TI RETRIEVAL-An Online Performance Evaluation Tool for Information
   Retrieval Methods
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Evaluation; information; metrics; performance; retrieval; Web
ID RELEVANCE
AB Performance evaluation is one of the main research topics in information retrieval. Evaluation metrics are used to quantify various performance aspects of a retrieval method. These metrics assist in identifying the optimum method for a specific retrieval challenge but also to allow its parameters fine-tuning in order to achieve a robust operation for a given set of requirements specification. In this work, we present RETRIEVAL, a Web-based integrated information retrieval performance evaluation platform. It offers a number of metrics that are popular within the scientific community, so as to compose an efficient framework for implementing performance evaluation. We discuss the functionality of RETRIEVAL by citing important aspects such as the data input approaches, the user-level performance metrics parameterization, the evaluation scenarios, the interactive plots, and the performance reports repository that offers both archiving and download functionalities.
C1 [Ioannakis, George; Pratikakis, Ioannis; Chamzas, Christodoulos] Democritus Univ Thrace, Sch Engn, Dept Elect & Comp Engn, GR-67100 Xanthi, Greece.
   [Koutsoudis, Anestis] Athena Res & Innovat Ctr Informat Commun & Knowled, Multimedia Res Grp, Xanthi 67100, Greece.
C3 Democritus University of Thrace
RP Koutsoudis, A (corresponding author), Athena Res & Innovat Ctr Informat Commun & Knowled, Multimedia Res Grp, Xanthi 67100, Greece.
EM gioannak@ee.duth.gr; akontsou@ceti.gr; ipratika@ee.duth.gr;
   chamzas@ee.duth.gr
RI Chamzas, Christodoulos/AAU-4428-2020; PRATIKAKIS, IOANNIS/AAD-3387-2019
OI PRATIKAKIS, IOANNIS/0000-0002-4124-3688; Koutsoudis,
   Anestis/0000-0001-9862-7335; Ioannakis, George/0000-0001-6230-7030
CR Agosti M., 2012, DIRECTIONS DESIGN SP, P88
   Angelini M., 2012, P 4 INFORM INTERACTI, P194
   Angelini M, 2014, J VISUAL LANG COMPUT, V25, P394, DOI 10.1016/j.jvlc.2013.12.003
   [Anonymous], 2015, PLOS ONE, DOI DOI 10.1371/JOURNAL.PONE.0118432
   [Anonymous], 2006, SHAPE RETRIEVAL CONT
   [Anonymous], 2015, CORR
   Armstrong TG, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P834
   Benavent X, 2013, IEEE T MULTIMEDIA, V15, P2009, DOI 10.1109/TMM.2013.2267726
   Chatzichristofis SA, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P1289
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chen J, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618492
   Daras P, 2012, IEEE T MULTIMEDIA, V14, P374, DOI 10.1109/TMM.2011.2176111
   Davis Jesse, 2006, P 23 INT C MACH LEAR, P233, DOI [DOI 10.1145/1143844.1143874, 10.1145/1143844.1143874]
   Ferro Nicola, 2015, Advances in Information Retrieval. 37th European Conference on IR Research (ECIR 2015). Proceedings: LNCS 9022, P768, DOI 10.1007/978-3-319-16354-3_83
   Ferro N., 2014, ACM SIGIR Forum, V48, P31, DOI [10.1145/2701583.2701587, DOI 10.1145/2701583.2701587]
   Ferro N., 2014, MATTERS MATLAB TOOLK
   Gao Y, 2011, IEEE T MULTIMEDIA, V13, P1007, DOI 10.1109/TMM.2011.2160619
   Gao Yue, 2015, P EUR WORKSH 3D OBJ, P129
   Gentleman R., 2015, R  FREE SOFTWARE ENV
   Gvert N., 2003, REPREC
   Ishioka T, 2003, IEEE/WIC INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE, PROCEEDINGS, P425
   Jones K.Sparck., 1997, READINGS INFORM RETR
   Kekäläinen J, 2005, INFORM PROCESS MANAG, V41, P1019, DOI [10.1016/j.ipm.2005.01.004, 10.1016/j.ipm.2004.01.004]
   Kelly Diane, 2009, Foundations and Trends in Information Retrieval, V3, P1, DOI 10.1561/1500000012
   Koutsoudis A., 2015, 3DOR, P95
   Koutsoudis A, 2013, 3D RES, V4, DOI 10.1007/3DRes.04(2013)3
   Li T, 2006, IEEE T MULTIMEDIA, V8, P564, DOI 10.1109/TMM.2006.870730
   Manning Christopher D., 2008, INTRO INFORM RETRIEV
   MediaEval, 2011, MEDIAEVAL MED BENCHM
   Moffat A, 2009, ACM T INFORM SYST, V27, DOI 10.1145/1416950.1416952
   NIST, 2002, TRECTRACKS
   NIST, 2008, TREC
   NTCIR, 1999, NTCIR NII TESTB COMM
   Petsa G., 2016, THESIS
   Pickup D., 2014, PROC 7 EUROGRAPHICS, P101
   Sakai T., 2004, P 4 NTCIR WORKSH TOK
   Saracevic T., 1995, SIGIR Forum, P138
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   SHREC, 2016, TRACK PART SHAP QUER
   SHRECa, 2015, NONR 3D SHAP RETR SH
   SHRECb, 2009, SHAP RETR CONT PART
   SHRECc, 2009, SHAP RETR CONT NEW G
   SHRECd, 2016, LARG SCAL 3D SHAR RE
   Silvello G., 2016, INT J DIGIT LIB, V18, P1
   Sing T, 2005, BIOINFORMATICS, V21, P3940, DOI 10.1093/bioinformatics/bti623
   Sormunen E., 2003, INFORM RES, V9, P2
   Stavropoulos G, 2010, IEEE T MULTIMEDIA, V12, P692, DOI 10.1109/TMM.2010.2053023
   Tao DC, 2006, IEEE T MULTIMEDIA, V8, P716, DOI 10.1109/TMM.2005.861375
   THALES, 2013, 3D OBJ RETR
   TRECa, 2012, CONT SUGG TREC
   TRECb, 2013, TEMP SUMM TRACK TREC
   TRECc, 2013, WEB TRACK
   Urbano J., 2013, NFIRE FRAMEWORK INFO
   Vedaldi A., 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Voorhees EM, 2002, LECT NOTES COMPUT SC, V2406, P355
   Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P947, DOI 10.1109/34.955109
   Zhang C, 2002, IEEE T MULTIMEDIA, V4, P260, DOI 10.1109/TMM.2002.1017738
NR 57
TC 10
Z9 10
U1 3
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2018
VL 20
IS 1
BP 119
EP 127
DI 10.1109/TMM.2017.2716193
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FQ3WS
UT WOS:000418289700010
DA 2024-07-18
ER

PT J
AU Hajiesmaili, MH
   Mak, LT
   Wang, Z
   Wu, C
   Chen, MH
   Khonsari, A
AF Hajiesmaili, Mohammad H.
   Mak, Lok To
   Wang, Zhi
   Wu, Chuan
   Chen, Minghua
   Khonsari, Ahmad
TI Cost-Effective Low-Delay Design for Multiparty Cloud Video Conferencing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cloud computing; network combinatorial optimization; parallel algorithm;
   video conferencing
ID ALLOCATION; TRANSMISSION
AB Multiparty cloud video conferencing architecture has been recently advocated to exploit rich computing and bandwidth resources in the cloud to effectively improve video conferencing performance. As a typical design in this architecture, multiple agents, i.e., virtual machines, are deployed in different cloud sites, and users are assigned to the agents. Then, the users communicate through the agents, and the agents might transcode the recorded videos given the heterogeneities among devices in terms of hardware specification and connectivity. In this architecture, two critical and nontrivial challenges are: 1) assigning users to agents to reduce the operational cost and the user-to-user conferencing delay and 2) identifying best agents to perform transcoding tasks, taking into account the heterogeneous bandwidth and processing availabilities. To address these challenges, we cast a joint problem of user-to-agent assignment and transcoding-agent selection. The ultimate objective is to simultaneously minimize the cost of the service provider and the conferencing delay. The problem is combinatorial in nature, which belongs to the NP-hard node assignment problems. We leverage the Markov approximation framework and devise an adaptive parallel algorithm that finds a close-to-optimal solution to our problem with a bounded performance guarantee. To evaluate the performance of our solution, we implement a prototype video conferencing system and carry out trace-driven experiments. In a set of largescale experiments using PlanetLab traces, our solution decreases the operational cost by 77% and simultaneously yields lower conferencing delay compared with an existing alternative.
C1 [Hajiesmaili, Mohammad H.; Mak, Lok To; Chen, Minghua] Chinese Univ Hong Kong, Dept Informat Engn, Shatin, Hong Kong, Peoples R China.
   [Hajiesmaili, Mohammad H.] Johns Hopkins Univ, Whiting Sch Engn, Baltimore, MD 21218 USA.
   [Wang, Zhi] Tsinghua Univ, Grad Sch Shenzhen, Shenzhen 518055, Peoples R China.
   [Wu, Chuan] Univ Hong Kong, Dept Comp Sci, Pokfulam, Hong Kong, Peoples R China.
   [Khonsari, Ahmad] Univ Tehran, Sch Elect & Comp Engn, Coll Engn, Tehran 1417466191, Iran.
   [Khonsari, Ahmad] Inst Res Fundamental Sci, Sch Comp Sci, Tehran 193955746, Iran.
C3 Chinese University of Hong Kong; Johns Hopkins University; Tsinghua
   Shenzhen International Graduate School; Tsinghua University; University
   of Hong Kong; University of Tehran
RP Khonsari, A (corresponding author), Univ Tehran, Sch Elect & Comp Engn, Coll Engn, Tehran 1417466191, Iran.
EM hajiesmaili@jhu.edu; mlt014@ie.cuhk.edu.hk; wangzhi@sz.tsinghua.edu.cn;
   cwu@cs.hku.edu.hk; minghua@ie.cuhk.edu.hk; ak@ipm.ir
RI Khonsari, Ahmad/J-1279-2016; Wu, Chuan/E-9919-2010; Chen,
   Minghua/A-7476-2012
OI Khonsari, Ahmad/0000-0002-8669-4001; Wu, Chuan/0000-0002-3144-4398;
   Chen, Minghua/0000-0003-4763-0037
FU National Basic Research Program of China [2013CB336700]; University
   Grants Committee of the Hong Kong Special Administrative Region, China
   under Area of Excellence Grant Project [AoE/E-02/08]; University Grants
   Committee of the Hong Kong Special Administrative Region, China under
   Collaborative Research Fund [C7036-15G]; National Natural Science
   Foundation of China [61402247]; Hong Kong RGC Grant [718513, 17204715,
   17225516]
FX This work was supported in part by National Basic Research Program of
   China under Project 2013CB336700 and in part by the University Grants
   Committee of the Hong Kong Special Administrative Region, China, under
   Area of Excellence Grant Project AoE/E-02/08 and Collaborative Research
   Fund C7036-15G, in part by the National Natural Science Foundation of
   China under Grant 61402247, and in part by Hong Kong RGC under Grant
   718513, Grant 17204715, and Grant 17225516. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Honggang Wang. The paper was presented in part at
   the IEEE 36th International Conference on Distributed Computing Systems,
   Ohara Fujimino, Japan, June 2016 [1]. (Corresponding author: Ahmad
   Khonsari.)
CR Alinia Bahram, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P226, DOI 10.1109/INFOCOM.2015.7218386
   Andersen D. G., 2002, THEORETICAL APPROACH
   [Anonymous], 2003, ONE WAY TRANSMISSION
   [Anonymous], 2017, Amazon elastic compute cloud
   [Anonymous], 2014, CISC VIS NETW IND GL
   [Anonymous], 2013, White Paper
   Bailis P., 2014, P VLDB, P181
   Bianchini M., 2005, ACM Transactions on Internet Technology, V5, P92, DOI 10.1145/1052934.1052938
   Bobarshad H, 2012, IEEE T MULTIMEDIA, V14, P401, DOI 10.1109/TMM.2011.2173477
   Boyd S., 2004, CONVEX OPTIMIZATION
   Bremaud P., 1999, TEXT APPL M, V31, DOI 10.1007/978-1-4757-3124-8
   Chen MH, 2013, IEEE T INFORM THEORY, V59, P6301, DOI 10.1109/TIT.2013.2268923
   Chen X., 2011, Proc. 19th ACM International Conference on Multimedia (MM '11), P493
   Cheng X, 2011, ACM SIGCOMM COMP COM, V41, P39, DOI 10.1145/1971162.1971168
   Chuah SP, 2015, IEEE T MULTIMEDIA, V17, P687, DOI 10.1109/TMM.2015.2413354
   Chun B, 2003, ACM SIGCOMM COMP COM, V33, P3, DOI 10.1145/956993.956995
   Doulamis ND, 2014, IEEE T COMPUT, V63, P461, DOI 10.1109/TC.2012.222
   Fischer A, 2013, IEEE COMMUN SURV TUT, V15, P1888, DOI 10.1109/SURV.2013.013013.00155
   Gao GY, 2015, IEEE T MULTIMEDIA, V17, P1286, DOI 10.1109/TMM.2015.2438713
   Hajiesmaili MH, 2015, INT CON DISTR COMP S, P103, DOI 10.1109/ICDCS.2015.19
   Hajiesmaili MH, 2012, J NETW COMPUT APPL, V35, P2016, DOI 10.1016/j.jnca.2012.07.024
   Hu YC, 2016, IEEE T MULTIMEDIA, V18, P840, DOI 10.1109/TMM.2016.2538721
   Jin YC, 2016, IEEE T MULTIMEDIA, V18, P807, DOI 10.1109/TMM.2016.2537199
   Jin YC, 2014, IEEE T MULTIMEDIA, V16, P1739, DOI 10.1109/TMM.2014.2329370
   Khalek AA, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2332304
   Kurdoglu E, 2016, IEEE T MULTIMEDIA, V18, P90, DOI 10.1109/TMM.2015.2496872
   Liao J, 2013, IEEE T MULTIMEDIA, V15, P670, DOI 10.1109/TMM.2012.2235416
   Liu Y, 2012, IEEE INFOCOM SER, P1332, DOI 10.1109/INFCOM.2012.6195496
   Maguluri ST, 2012, IEEE INFOCOM SER, P702, DOI 10.1109/INFCOM.2012.6195815
   Rodriguez Esteban, 2013, 2013 IEEE International Conference on Communications (ICC), P2262, DOI 10.1109/ICC.2013.6654865
   Si XB, 2012, IEEE INT WORKS INFOR, P1, DOI 10.1109/WIFS.2012.6412616
   Talebi MS, 2010, COMPUT COMMUN, V33, P1543, DOI 10.1016/j.comcom.2010.03.031
   Van Laarhoven P., 1987, SIMULATED ANNEALING
   Wang SX, 2012, IEEE ICC, P2081, DOI 10.1109/ICC.2012.6364497
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Wu Y., 2013, P ACM SPEC INT GROUP, P33
   Wu Z, 2013, ACM SIGCOMM COMP COM, V43, P13, DOI 10.1145/2479957.2479960
   Xi C, 2012, IEEE IJCNN, P1
   Xu Y., 2012, P 2012 INTERNET MEAS, P371, DOI DOI 10.1145/2398776.2398816
   Yu ML, 2008, ACM SIGCOMM COMP COM, V38, P19, DOI 10.1145/1355734.1355737
   Zhang SQ, 2014, IEEE ACM T NETWORK, V22, P717, DOI 10.1109/TNET.2013.2270915
   Zhang Shuopeng., 2014, Proceedings_of_Network_and_Operating_System Support_on_Digital_Audio_and_Video_Workshop, P43
NR 42
TC 12
Z9 13
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2017
VL 19
IS 12
BP 2760
EP 2774
DI 10.1109/TMM.2017.2710799
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FN8HG
UT WOS:000416263200010
DA 2024-07-18
ER

PT J
AU Wu, D
   Zhou, L
   Cai, YM
AF Wu, Dan
   Zhou, Liang
   Cai, Yueming
TI Social-Aware Rate Based Content Sharing Mode Selection for D2D Content
   Sharing Scenarios
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Device-to-device (D2D) content sharing; matroid; sharing mode selection;
   social-aware rate; submodular maximization
ID TO-DEVICE COMMUNICATION; WIRELESS; INSIGHTS; 5G
AB Device-to-device (D2D) content sharing has become a promising solution to support the growing popularity of multimedia contents for local services. Considering the randomness of content location, the limited storage and transmission capability of devices, and the coexistence of altruistic and selfish user behaviors, how to optimally match the demanders to the providers of contents and how to stimulate an efficient cooperation are of importance for achieving the full benefits of D2D content sharing. Especially when the base-station-to-device (B2D), D2D, and novel multi-D2D sharing modes coexist, the issue of content sharing mode selection plays the predominant role in such matching. In this paper, we introduce a notion of social-aware rate, which combines the social selfishness from the social knowledge with the link rate to ensure the physical link quality and the effective cooperation together. Then, the social-aware rate-based content sharing mode selection problem is modeled as a maximum weighted mixed matching problem, which can be computationally reduced to a submodular welfare problem subject to a matroid constraint. Subsequently, we develop a best-effort distributed algorithm framework, which displays alternatives of various computation complexities and approximation ratios to satisfy the diverse practical needs.
C1 [Wu, Dan; Cai, Yueming] PLA Univ Sci & Technol, Coll Commun Engn, Nanjing 100091, Jiangsu, Peoples R China.
   [Zhou, Liang] Nanjing Univ Posts & Telecommun, Nanjing 210003, Jiangsu, Peoples R China.
C3 Army Engineering University of PLA; Nanjing University of Posts &
   Telecommunications
RP Wu, D (corresponding author), PLA Univ Sci & Technol, Coll Commun Engn, Nanjing 100091, Jiangsu, Peoples R China.
EM wujing1958725@126.com; liang.zhou@njupt.edu.cn; caiym@vip.sina.com
FU National Natural Science Foundation of China [61671474, 61571240,
   61371122]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61671474, Grant 61571240, and Grant 61371122. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Yonggang Wen. (Corresponding
   author: Dan Wu.)
CR [Anonymous], AD HOC SENSOR WIRELE
   [Anonymous], IEEE T SERV CO UNPUB
   [Anonymous], ENGLEWOOD CLIFFS
   [Anonymous], 2007, WEB SERVICES DESCRIP
   [Anonymous], 2011, 4G LTE LTE ADV FORMO
   [Anonymous], 2010, P IEEE INFOCOM APR
   Bai B, 2016, IEEE WIREL COMMUN, V23, P74, DOI 10.1109/MWC.2016.7553029
   Bastug E, 2014, IEEE COMMUN MAG, V52, P82, DOI 10.1109/MCOM.2014.6871674
   Cai YM, 2015, IEEE ICC, P2931, DOI 10.1109/ICC.2015.7248772
   Calinescu G, 2011, SIAM J COMPUT, V40, P1740, DOI 10.1137/080733991
   Chen SZ, 2014, IEEE COMMUN MAG, V52, P36, DOI 10.1109/MCOM.2014.6815891
   Chen X, 2015, IEEE ACM T NETWORK, V23, P1471, DOI 10.1109/TNET.2014.2329956
   Dobzinski S, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1064, DOI 10.1145/1109557.1109675
   Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059
   Feng W, 2017, IEEE J SEL AREA COMM, V35, P1459, DOI 10.1109/JSAC.2017.2698898
   Feng W, 2013, IEEE J SEL AREA COMM, V31, P2067, DOI 10.1109/JSAC.2013.131009
   Fodor G, 2012, IEEE COMMUN MAG, V50, P170, DOI 10.1109/MCOM.2012.6163598
   Gao GY, 2015, IEEE T MULTIMEDIA, V17, P1286, DOI 10.1109/TMM.2015.2438713
   Jiang JJ, 2016, IEEE J SEL AREA COMM, V34, P82, DOI 10.1109/JSAC.2015.2452493
   Jin YC, 2016, IEEE T MULTIMEDIA, V18, P807, DOI 10.1109/TMM.2016.2537199
   Li Y, 2014, IEEE COMMUN MAG, V52, P150, DOI 10.1109/MCOM.2014.6829957
   Maddah-Ali MA, 2014, IEEE T INFORM THEORY, V60, P2856, DOI 10.1109/TIT.2014.2306938
   Militano L, 2014, IEEE INT CONF COMM, P296, DOI 10.1109/ICCW.2014.6881212
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Reche Alberto, 2015, Ad-hoc Networks and Wireless. ADHOC-NOW 2014 International Workshops ETSD, MARSS, MWaoN, SecAN, SSPA, and WiSARN. Revised Selected Papers: LNCS 8629, P139, DOI 10.1007/978-3-662-46338-3_12
   Shanmugam K, 2013, IEEE T INFORM THEORY, V59, P8402, DOI 10.1109/TIT.2013.2281606
   Wan PJ, 2013, IEEE INFOCOM SER, P2121
   Wang L, 2016, IEEE J SEL AREA COMM, V34, P2650, DOI 10.1109/JSAC.2016.2605239
   Wang RY, 2016, IEEE WIREL COMMUN, V23, P28, DOI 10.1109/MWC.2016.7553023
   Wu D, 2015, IEEE T WIREL COMMUN, V14, P5417, DOI 10.1109/TWC.2015.2438292
   Wu DP, 2017, IEEE T MULTIMEDIA, V19, P1908, DOI 10.1109/TMM.2017.2692648
   Zhang HM, 2015, IEEE COMMUN MAG, V53, P67, DOI 10.1109/MCOM.2015.7355587
   Zhang YR, 2015, IEEE J SEL AREA COMM, V33, P2144, DOI 10.1109/JSAC.2015.2435356
   Zhang YR, 2015, IEEE T WIREL COMMUN, V14, P177, DOI 10.1109/TWC.2014.2334661
NR 34
TC 89
Z9 93
U1 0
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2017
VL 19
IS 11
BP 2571
EP 2582
DI 10.1109/TMM.2017.2700621
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FJ9CT
UT WOS:000413068200017
DA 2024-07-18
ER

PT J
AU Samain, J
   Carofiglio, G
   Muscariello, L
   Papalini, M
   Sardara, M
   Tortelli, M
   Rossi, D
AF Samain, Jacques
   Carofiglio, Giovanna
   Muscariello, Luca
   Papalini, Michele
   Sardara, Mauro
   Tortelli, Michele
   Rossi, Dario
TI Dynamic Adaptive Video Streaming: Towards a Systematic Comparison of ICN
   and TCP/IP
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive video streaming; information centric networking; testbed-based
   comparison
AB Streaming of video content over the Internet is experiencing an unprecedented growth. While video permeates every application, it also puts tremendous pressure in the network-to support users having heterogeneous accesses and expecting a high quality of experience, in a furthermore cost-effective manner. In this context, future internet paradigms, such as information centric networking (ICN), are particularly well suited to not only enhance video delivery at the client (as in the dynamic adaptive streaming over HTTP (DASH) approach), but to also naturally and seamlessly extend video support deeper in the network functions. In this paper, we contrast ICN and transmission control protocol/internet protocol (TCP/IP) with an experimental approach, where we employ several state-of-the-art DASH controllers (PANDA, AdapTech, and BOLA) on an ICN versus TCP/IP network stack. Our campaign, based on tools that we developed and made available as open-source software, includes multiple clients (homogeneous vesrus heterogeneous mixture and synchronous vesrus asynchronous arrivals), videos (up to 4k resolution), channels (e.g., DASH profiles, emulated WiFi and LTE, and real 3G/4G traces), and levels of integration with an ICN network (i.e., vanilla named data networking (NDN), wireless loss detection and recovery at the access point, and load balancing). Our results clearly illustrate, as well as quantitatively assess, the benefits of ICN-based streaming, warning about potential pitfalls that are however easy to avoid.
C1 [Samain, Jacques; Tortelli, Michele; Rossi, Dario] Telecom ParisTech, F-75634 Paris, France.
   [Samain, Jacques; Carofiglio, Giovanna; Muscariello, Luca; Papalini, Michele; Sardara, Mauro] Cisco Syst, F-92700 Issy Les Molineaux, France.
C3 IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; Telecom
   Paris
RP Tortelli, M (corresponding author), Telecom ParisTech, F-75634 Paris, France.
EM jsamain@cisco.com; gcarofig@cisco.com; lumuscar@cisco.com;
   micpapal@cisco.com; msardara@cisco.com;
   michele.tortelli@telecom-paristech.fr; dario.rossi@telecom-paristech.fr
RI Muscariello, Luca/X-8613-2019; Rossi, Dario/HSG-3271-2023
OI Rossi, Dario/0000-0003-3936-8876
FU NewNet@Paris, Cisco's Chair "NETWORKS FOR THE FUTURE" at Telecom
   ParisTech
FX This work was supported by NewNet@Paris, Cisco's Chair "NETWORKS FOR THE
   FUTURE" at Telecom ParisTech (http://newnet.telecom-paristech.fr). The
   guest editor coordinating the review of this manuscript and approving it
   for publication was Dr. Mahbub Hassan. (Corresponding author: Michele
   Tortelli.)
CR Akhshabi S, 2012, SIGNAL PROCESS-IMAGE, V27, P271, DOI 10.1016/j.image.2011.10.003
   Akhshabi Saamer, 2013, Proceeding of the 23rd ACM Workshop on Network and Operating Systems Support for Digital Audio and Video-NOSSDAV'13
   Alberti C, 2013, INT WORK QUAL MULTIM, P58, DOI 10.1109/QoMEX.2013.6603211
   [Anonymous], ADAPTIVE VI IN PRESS
   [Anonymous], 2012, P 2 EDITION ICN WORK, DOI DOI 10.1145/2342488.2342497
   [Anonymous], P ACM 7 INT C MULT S
   [Anonymous], TECH REP
   [Anonymous], 2013, P IEEE 20 INT PACK V
   [Anonymous], 2016, CISCO VISUAL NETWORK
   [Anonymous], ACM T MULTIM COMPUT
   [Anonymous], COMPUT NETW
   [Anonymous], 2016, IEEE INFOCOM 2016-The 35th Annual IEEE International Conference on Computer Communications
   [Anonymous], UND INF CENTR NETW M
   [Anonymous], 2015, PACKAGE PRACTICES CR
   [Anonymous], 2013, 2013 OCEANS SAN DIEG
   [Anonymous], P ACM VIDEONEXT WORK
   Carofiglio G, 2016, PROCEEDINGS OF THE 2016 3RD ACM CONFERENCE ON INFORMATION-CENTRIC NETWORKING (ACM-ICN '16), P50, DOI 10.1145/2984356.2984361
   Carofiglio G, 2016, COMPUT NETW, V110, P133, DOI 10.1016/j.comnet.2016.09.019
   Carofiglio G, 2013, I C NETWORK PROTOCOL
   Chen JC, 2016, PROCEEDINGS OF THE 2016 3RD ACM CONFERENCE ON INFORMATION-CENTRIC NETWORKING (ACM-ICN '16), P11, DOI 10.1145/29843562984370
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Egilmez HE, 2013, IEEE T MULTIMEDIA, V15, P710, DOI 10.1109/TMM.2012.2232645
   El Essaili A., 2013, 2013 IEEE International Conference on Communications (ICC), P2480, DOI 10.1109/ICC.2013.6654905
   Georgopoulos P., 2013, Proceedings of the 2013 ACM SIGCOMM Workshop on Future Human-centric Multimedia Networking, FhMN '13, (New York, NY, USA), P15, DOI DOI 10.1145/2491172.2491181
   Goldoni Emanuele, 2009, 2009 Fourth International Conference on Internet Monitoring and Protection (ICIMP 2009), P130, DOI 10.1109/ICIMP.2009.28
   Hossfeld Tobias, 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P494, DOI 10.1109/ISM.2011.87
   Houzé P, 2016, IEEE ICC, DOI 10.1109/ICC.2016.7511550
   Huang T.Y., 2012, P 2012 ACM C INT MEA, P225, DOI 10.1145/2398776.2398800
   Huang TY, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P187, DOI 10.1145/2619239.2626296
   James C, 2016, I S MOD ANAL SIM COM, P331, DOI 10.1109/MASCOTS.2016.75
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Kapoor R, 2004, ACM SIGCOMM COMP COM, V34, P67, DOI 10.1145/1030194.1015476
   Karagkioules Theodoros, 2017, P 27 WORKSH NETW OP, P1, DOI [10.1145/3083165.3083170, DOI 10.1145/3083165.3083170]
   Lederer S., 2012, P 3 MULT SYST C, P89
   Lederer S, 2014, IEEE NETWORK, V28, P91, DOI 10.1109/MNET.2014.6963810
   Lederer S, 2013, IEEE INT CONF COMM, P677, DOI 10.1109/ICCW.2013.6649319
   Li WJ, 2015, IEEE ICC, P5747, DOI 10.1109/ICC.2015.7249238
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Liu YN, 2013, IEEE ICC, P3629, DOI 10.1109/ICC.2013.6655116
   Miller K., 2012, 2012 Proceedings of the 19th International Packet Video Workshop (PV 2012), P173, DOI 10.1109/PV.2012.6229732
   Miller K, 2015, IEEE T MULTIMEDIA, V17, P1309, DOI 10.1109/TMM.2015.2441002
   Nam H., 2016, IEEE INFOCOM 2016-The 35th Annual IEEE International Conference on Computer Communications, P1
   Navratil J., 2003, P 4 PASSIVE ACTIVE M
   Papageorge P, 2009, ACM SIGCOMM COMP COM, V39, P279, DOI 10.1145/1594977.1592601
   Petrowski S, 2016, POLAR BIOL, V39, P2141, DOI 10.1007/s00300-015-1654-7
   Plass M.F., 2009, P 5 INT C EMERGING N, P1, DOI [10.1145/1658939.1658941, DOI 10.1145/1658939.1658941]
   Puri A, 2004, SIGNAL PROCESS-IMAGE, V19, P793, DOI 10.1016/j.image.2004.06.003
   Rainer B, 2016, IEEE J SEL AREA COMM, V34, P2130, DOI 10.1109/JSAC.2016.2577365
   Ribeiro V.J., 2003, PASSIVE ACTIVE MEASU
   Rossini G., 2014, P 1 INT C INFORM CEN, P127
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Sieber C, 2013, 2013 IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT (IM 2013), P1318
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Taleb T, 2011, IEEE T BROADCAST, V57, P662, DOI 10.1109/TBC.2011.2159529
   Thang TC, 2014, IEEE J SEL AREA COMM, V32, P693, DOI 10.1109/JSAC.2014.140403
   Thang TC, 2012, IEEE T CONSUM ELECTR, V58, P78, DOI 10.1109/TCE.2012.6170058
   van der Hooft J, 2016, IEEE COMMUN LETT, V20, P2177, DOI 10.1109/LCOMM.2016.2601087
   Wang C, 2016, PR INT CONGR SOUND V
   Xylomenos G, 2014, IEEE COMMUN SURV TUT, V16, P1024, DOI 10.1109/SURV.2013.070813.00063
   Yin XQ, 2015, SIGCOMM'15: PROCEEDINGS OF THE 2015 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P325, DOI 10.1145/2785956.2787486
   Zhang LX, 2014, ACM SIGCOMM COMP COM, V44, P66, DOI 10.1145/2656877.2656887
NR 62
TC 36
Z9 39
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2017
VL 19
IS 10
BP 2166
EP 2181
DI 10.1109/TMM.2017.2733340
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5YG
UT WOS:000411247600004
DA 2024-07-18
ER

PT J
AU Yao, SS
   Niu, BN
   Liu, JQ
AF Yao, Shanshan
   Niu, Baoning
   Liu, Jianquan
TI Audio Identification by Sampling Sub-fingerprints and Counting Matches
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio fingerprint; audio retrieval; dynamic threshold; inverted index;
   multi-granularity filter
ID COMPUTER VISION
AB It is challenging to retrieve audio clips from large audio datasets not only due to the high dimensionality of audio but also due to the large number of audios. Fingerprinting methods primarily focus on the use of semantic-level techniques to speed up retrieval and neglect low-level support. This paper shows that the performance of audio retrieval can be exploited by properly organizing andmanipulating audio fingerprint data. The proposed sampling and counting method markedly improves the retrieval speed while maintaining a high recall rate and high precision for short audio clips. The proposed inverted index structure for fingerprints quickly shrinks the scope of the candidate set while requiring considerably less memory. The experiments show that the proposed method is faster and yields more consistent performance in terms of recall rate and precision than do the state-of-the-art methods. The proposed method is scalable for big audio data and is robust to background noise, resampling, MP3 conversion, white noise addition, bandpass filtering, chorus, echo, flanger, gsm compression, and tremolo. The average precision and recall rates are 99.995% and 99.78%, respectively, for dataset 1 and average 98.97% and 95.15%, respectively, for dataset 2. Specifically, the precision and recall rates for query audio clips of 3 s in length in the targeted dataset 1 are 99.94% and 99.64%, respectively.
C1 [Yao, Shanshan; Niu, Baoning] Taiyuan Univ Technol, Sch Comp Sci & Technol, Taiyuan 030024, Shanxi, Peoples R China.
   [Liu, Jianquan] NEC Corp Ltd, Syst Platform Res Labs, Tokyo 1088001, Japan.
   [Liu, Jianquan] Hosei Univ, Grad Sch Sci & Engn, Tokyo 1028160, Japan.
C3 Taiyuan University of Technology; NEC Corporation; Hosei University
RP Niu, BN (corresponding author), Taiyuan Univ Technol, Sch Comp Sci & Technol, Taiyuan 030024, Shanxi, Peoples R China.; Liu, JQ (corresponding author), NEC Corp Ltd, Syst Platform Res Labs, Tokyo 1088001, Japan.
EM yaoshanshan0037@link.tyut.edu.cn; niubaoning@tyut.edu.cn;
   j-liu@ct.jp.nec.com
RI LIU, JIANQUAN/B-1296-2011; Yao, Shanshan/JJC-4929-2023; Niu,
   Baoning/ACC-8776-2022
OI Niu, Baoning/0000-0002-7924-3384
FU National Key Technology Support Program of China [2015BAH37F01];
   National Natural Science Foundation of China [61572345]; International
   Cooperation Project of the Major R&D Program of Shanxi [201603D421015]
FX This work was supported in part by the National Key Technology Support
   Program of China under Grant 2015BAH37F01, in part by the National
   Natural Science Foundation of China under Grant 61572345, and in part by
   the International Cooperation Project of the Major R&D Program of Shanxi
   under Grant 201603D421015. The guest editor coordinating the review of
   this manuscript and approving it for publication was Prof. Nicu Sebe.
   (Corresponding authors: Baoning Niu; Jianquan Liu.)
CR Anguera X., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P455, DOI 10.1109/ICME.2012.137
   [Anonymous], 2003, ISMIR
   Baluja S, 2007, INT CONF ACOUST SPEE, P213
   Cha GH, 2012, INT CONF MULTIMED, P7, DOI 10.1109/ICMCS.2012.6320259
   Chandrasekhar Vijay., 2011, ISMIR, V20, P801
   Chen M., 2013, P ICAIEES, P219
   Cohen E., 1999, P 16 INT DAT DAT ENG, P489
   Haitsma J, 2003, INT CONF ACOUST SPEE, P728
   Haitsma J., 2002, P ISMIR 2002 3 INT C, P107
   Ke Y, 2005, PROC CVPR IEEE, P597
   Liu C.C., 2011, Proc. ACM International Conference on Advances in Mobile Computing and Mul- timedia (MoMM'11), P190
   MYERS CS, 1981, AT&T TECH J, V60, P1389, DOI 10.1002/j.1538-7305.1981.tb00272.x
   Seo JS, 2006, IEEE SIGNAL PROC LET, V13, P209, DOI 10.1109/LSP.2005.863678
   SEO JS, 2002, P 1 WORKSH MOD BAS P, P45
   Shibuya T, 2013, P ICME, P1
   Six J., 2014, P INT SOC MUS INF RE, P259
   Sonnleitner R, 2016, IEEE-ACM T AUDIO SPE, V24, P409, DOI 10.1109/TASLP.2015.2509248
   Yao SS, 2015, IEEE T MULTIMEDIA, V17, P1450, DOI 10.1109/TMM.2015.2460121
   [张雪源 Zhang Xueyuan], 2012, [电子与信息学报, Journal of Electronics & Information Technology], V34, P2561
NR 19
TC 7
Z9 8
U1 1
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2017
VL 19
IS 9
BP 1984
EP 1995
DI 10.1109/TMM.2017.2723846
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5XC
UT WOS:000411244200003
DA 2024-07-18
ER

PT J
AU Wu, DP
   Yan, JJ
   Wang, HG
   Wu, DL
   Wang, RY
AF Wu, Dapeng
   Yan, Junjie
   Wang, Honggang
   Wu, Dalei
   Wang, Ruyan
TI Social Attribute Aware Incentive Mechanism for Device-to-Device Video
   Distribution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Device-to-device (D2D) communication; incentive mechanism; multicast;
   social attributes; video distribution
ID CELLULAR NETWORKS; COMMUNICATION; EFFICIENCY; ALGORITHM; SELECTION
AB To offload and alleviate the heavy base station (BS) traffic load caused by the rapidly growing video services, device-to-device (D2D) communication, as one of the most indispensable technologies of the future cellular networks, can be potentially exploited by mobile users to distribute videos for a BS. In this paper, an effective pricing-based multicast video distribution system and a grid-based clustering method are proposed to support the distribution. Moreover, with the consideration of users' mobility and social characteristics, we classify them into multicast and core types by studying the user stay probability and familiarity. In particular, core users can cooperate with the BS to distribute videos to the multicast users through intracluster D2D multicast. However, core users cannot selflessly help the BS to distribute videos; instead, they will evaluate their personal benefits before distributing the videos to the multicast users. Further, a Stackelberg game-based pricing mechanism is proposed to inspire the core users to distribute videos. Simulation results demonstrate that the proposed mechanism can not only effectively alleviate the BS traffic load, but also significantly improve the effectiveness and reliability of video transmission.
C1 [Wu, Dapeng; Yan, Junjie; Wang, Ruyan] Chongqing Univ Posts & Telecommun, Chongqing 400065, Peoples R China.
   [Wu, Dapeng; Wang, Honggang] Univ Massachusetts, Dartmouth, MA 02747 USA.
   [Wu, Dalei] Univ Tennessee, Chattanooga, TN 37403 USA.
C3 Chongqing University of Posts & Telecommunications; University of
   Massachusetts System; University Massachusetts Dartmouth; University of
   Tennessee System; University of Tennessee at Chattanooga
RP Wang, HG (corresponding author), Univ Massachusetts, Dartmouth, MA 02747 USA.
EM wudapengphd@gmail.com; cqupt2013yjj@sina.com; hwang1@umassd.edu;
   dalei-wu@utc.edu; wan-gry@cqupt.edu.cn
RI Wu, Dapeng/IWE-0674-2023; Wu, Dalei/A-6884-2012; Yan, Jun/IXD-7801-2023;
   Wang, Honggang/D-6079-2013
OI Wu, Dapeng/0000-0003-2105-9418; , Dalei/0000-0002-9362-3906; Wang,
   Honggang/0000-0001-9475-2630
FU Natural Science Foundation of China [61371097, 61271261]; Youth Talents
   Training Project of Chongqing Science & Technology Commission
   [CSTC2014KJRC-QNRC40001]; Program for Innovation Team Building at
   Institutions of Higher Education in Chongqing [CXTDX201601020]; National
   Science Foundation [1429120, 1451629]; Direct For Computer & Info Scie &
   Enginr; Division Of Computer and Network Systems [1429120] Funding
   Source: National Science Foundation; Direct For Computer & Info Scie &
   Enginr; Division Of Computer and Network Systems [1451629] Funding
   Source: National Science Foundation
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61371097 and Grant 61271261, in part by the Youth
   Talents Training Project of Chongqing Science & Technology Commission
   (CSTC2014KJRC-QNRC40001), in part by the Program for Innovation Team
   Building at Institutions of Higher Education in Chongqing under Grant
   CXTDX201601020, and in part by the National Science Foundation (1429120,
   1451629). The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Liang Zhou.
   (Corresponding author: Honggang Wang.)
CR Andreev S, 2014, IEEE COMMUN MAG, V52, P20, DOI 10.1109/MCOM.2014.6807943
   [Anonymous], 2005, THESIS INRIA
   [Anonymous], IMC07 P 2007 ACM
   [Anonymous], IEEE T CIRC IN PRESS
   Antonopoulos A, 2014, IEEE COMMUN MAG, V52, P125, DOI 10.1109/MCOM.2014.6829954
   Argyriou A, 2015, IEEE T MULTIMEDIA, V17, P736, DOI 10.1109/TMM.2015.2408254
   Boyd S., 2004, CONVEX OPTIMIZATION
   Cao Y, 2016, IEEE T MOBILE COMPUT, V15, P1528, DOI 10.1109/TMC.2015.2461214
   Cao Y, 2015, IEEE WIREL COMMUN, V22, P124, DOI 10.1109/MWC.2015.7143335
   Chaintreau A, 2007, IEEE T MOBILE COMPUT, V6, P606, DOI 10.1109/TMC.2007.1060
   Cheng X, 2012, IEEE T MULTIMEDIA, V14, P1558, DOI 10.1109/TMM.2012.2217735
   Golrezaei N, 2014, IEEE T WIREL COMMUN, V13, P3665, DOI 10.1109/TWC.2014.2316817
   Han B, 2010, PROCEEDINGS OF THE 5TH ACM WORKSHOP ON CHALLENGED NETWORKS (CHANTS '10), P31, DOI 10.1145/1859934.1859943
   Holden J., 2016, ELEVATE FAST FORWARD
   Huang SL, 2016, IEEE T MULTIMEDIA, V18, P752, DOI 10.1109/TMM.2016.2530411
   Jian Li, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P2119, DOI 10.1109/INFOCOM.2015.7218597
   Jiasi Chen, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P1266, DOI 10.1109/INFOCOM.2015.7218502
   Karagiannis T, 2010, IEEE T MOBILE COMPUT, V9, P1377, DOI 10.1109/TMC.2010.99
   Kurdoglu E, 2016, IEEE T MULTIMEDIA, V18, P90, DOI 10.1109/TMM.2015.2496872
   Lee JYB, 2005, IEEE T MULTIMEDIA, V7, P366, DOI 10.1109/TMM.2005.843356
   Li Y, 2014, IEEE T MOBILE COMPUT, V13, P1579, DOI 10.1109/TMC.2013.61
   Li Y, 2015, IEEE T WIREL COMMUN, V14, P4093, DOI 10.1109/TWC.2015.2416715
   LIGNOLA MB, 1995, J OPTIMIZ THEORY APP, V84, P145, DOI 10.1007/BF02191740
   Liu Q, 2016, IEEE T VEH TECHNOL, V65, P7064, DOI 10.1109/TVT.2015.2492240
   Luo CQ, 2014, IEEE T PARALL DISTR, V25, P3211, DOI 10.1109/TPDS.2013.2297922
   Mallozzi L., 1995, Optimization, V32, P269, DOI 10.1080/02331939508844050
   MALLOZZI L, 1993, J OPTIMIZ THEORY APP, V78, P303, DOI 10.1007/BF00939672
   Militano L, 2014, IEEE INT CONF COMM, P296, DOI 10.1109/ICCW.2014.6881212
   Luong NC, 2016, IEEE COMMUN SURV TUT, V18, P2546, DOI 10.1109/COMST.2016.2582841
   Niu GL, 2014, IEEE T MULTIMEDIA, V16, P2025, DOI 10.1109/TMM.2014.2340133
   Pan ZQ, 2016, J VIS COMMUN IMAGE R, V40, P516, DOI 10.1016/j.jvcir.2016.07.018
   Pan ZQ, 2016, IEEE T BROADCAST, V62, P675, DOI 10.1109/TBC.2016.2580920
   Pan ZQ, 2015, IEEE T BROADCAST, V61, P166, DOI 10.1109/TBC.2015.2419824
   Peng MG, 2015, IEEE NETWORK, V29, P6, DOI 10.1109/MNET.2015.7064897
   Peng MG, 2014, IEEE WIREL COMMUN, V21, P126, DOI 10.1109/MWC.2014.7000980
   Qin M, 2010, IEEE T MULTIMEDIA, V12, P317, DOI 10.1109/TMM.2010.2046275
   Scott James., 2009, CRAWDAD trace cambridge/haggle/imote/infocom2006 (v. 2009-05-29)
   Thilakarathna K, 2014, IEEE T MOBILE COMPUT, V13, P2058, DOI 10.1109/TMC.2013.89
   Van der Auwera G, 2008, IEEE COMMUN MAG, V46, P164, DOI 10.1109/MCOM.2008.4689260
   Wang TY, 2015, IEEE T WIREL COMMUN, V14, P7004, DOI 10.1109/TWC.2015.2463281
   Wang WQ, 2012, INT CONF SIGN PROCES, P2007, DOI 10.1109/ICoSP.2012.6491974
   Wang XF, 2014, IEEE WIREL COMMUN, V21, P28, DOI 10.1109/MWC.2014.6845046
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Wu DP, 2016, IEEE T VEH TECHNOL, V65, P7634, DOI 10.1109/TVT.2015.2493516
   Wu DP, 2015, IEEE COMMUN MAG, V53, P92, DOI 10.1109/MCOM.2015.7180514
   Yang CG, 2016, IEEE WIREL COMMUN, V23, P64, DOI 10.1109/MWC.2016.7422407
   Yoo JW, 2011, IEEE T MOBILE COMPUT, V10, P491, DOI 10.1109/TMC.2010.161
   Yu GD, 2016, IET COMMUN, V10, P479, DOI 10.1049/iet-com.2015.0560
   Zhang ZF, 2014, INT J COMMUN SYST, V27, P534, DOI 10.1002/dac.2765
   Zhou B, 2013, IEEE T VEH TECHNOL, V62, P2315, DOI 10.1109/TVT.2012.2237557
   Zhou L, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2886776
   Zhuo XJ, 2014, IEEE T MOBILE COMPUT, V13, P541, DOI 10.1109/TMC.2013.15
NR 52
TC 147
Z9 148
U1 7
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2017
VL 19
IS 8
BP 1908
EP 1920
DI 10.1109/TMM.2017.2692648
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5VV
UT WOS:000411240400018
OA Bronze
DA 2024-07-18
ER

PT J
AU Yao, YZ
   Zhang, J
   Shen, FM
   Hua, XS
   Xu, JS
   Tang, ZM
AF Yao, Yazhou
   Zhang, Jian
   Shen, Fumin
   Hua, Xiansheng
   Xu, Jingsong
   Tang, Zhenmin
TI Exploiting Web Images for Dataset Construction: A Domain Robust Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Domain robust; image dataset construction; multi-instance learning
   (MIL); multiple query expansions
AB Labeled image datasets have played a critical role in high-level image understanding. However, the process of manual labeling is both time-consuming and labor intensive. To reduce the cost of manual labeling, there has been increased research interest in automatically constructing image datasets by exploiting web images. Datasets constructed by existing methods tend to have a weak domain adaptation ability, which is known as the "dataset bias problem." To address this issue, we present a novel image dataset construction framework that can be generalized well to unseen target domains. Specifically, the given queries are first expanded by searching the Google Books Ngrams Corpus to obtain a rich semantic description, from which the visually nonsalient and less relevant expansions are filtered out. By treating each selected expansion as a "bag" and the retrieved images as "instances," image selection can be formulated as a multi-instance learning problem with constrained positive bags. We propose to solve the employed problems by the cutting-plane and concave-convex procedure algorithm. By using this approach, images from different distributions can be kept while noisy images are filtered out. To verify the effectiveness of our proposed approach, we build an image dataset with 20 categories. Extensive experiments on image classification, cross-dataset generalization, diversity comparison, and object detection demonstrate the domain robustness of our dataset.
C1 [Yao, Yazhou; Zhang, Jian; Xu, Jingsong] Univ Technol Sydney, Global Big Data Technol Ctr, Sydney, NSW 2007, Australia.
   [Shen, Fumin] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610051, Sichuan, Peoples R China.
   [Hua, Xiansheng] Alibaba Grp, Hangzhou 311121, Zhejiang, Peoples R China.
   [Tang, Zhenmin] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China.
C3 University of Technology Sydney; University of Electronic Science &
   Technology of China; Alibaba Group; Nanjing University of Science &
   Technology
RP Shen, FM (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610051, Sichuan, Peoples R China.
EM yazhou.yao@outlook.com; Jian.Zhang@uts.edu.au; fumin.shen@gmail.com;
   xiansheng.hxs@alibaba-inc.co; jingsong.xu@uts.edu.au;
   Tzm.cs@njust.edu.cn
RI Tang, Zhenmin/AAY-6058-2020
OI Tang, Zhenmin/0000-0001-6708-2205; Yao, Yazhou/0000-0002-0337-9410; Xu,
   Jingsong/0000-0002-9102-3616; Zhang, Jian/0000-0002-7240-3541
FU National Natural Science Foundation of China [61473154]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61473154. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Shu-Ching
   Chen. (Corresponding author: Fumin Shen.)
CR Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   [Anonymous], 2013, People's Web Meets NLP
   Bach F., 2004, P ACM INT C MACH LEA, P220
   Boyd S.P., 2004, Convex optimization, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441]
   Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P394, DOI 10.1109/TPAMI.2007.61
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Cilibrasi RL, 2007, IEEE T KNOWL DATA EN, V19, P370, DOI 10.1109/TKDE.2007.48
   Coates A., 2011, P 14 INT C ART INT S, P215
   Collins B, 2008, LECT NOTES COMPUT SC, V5302, P86, DOI 10.1007/978-3-540-88682-2_8
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Divvala SK, 2014, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2014.412
   Duan LX, 2011, IEEE T IMAGE PROCESS, V20, P3280, DOI 10.1109/TIP.2011.2159227
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Ewerth R, 2012, IEEE T MULTIMEDIA, V14, P1008, DOI 10.1109/TMM.2012.2186956
   Griffin G., 2007, CALTECH 256 OBJECT C
   Hua XS, 2015, AAAI CONF ARTIF INTE, P137
   KELLEY JE, 1960, J SOC IND APPL MATH, V8, P703, DOI 10.1137/0108053
   Kim SJ, 2008, SIAM J OPTIMIZ, V19, P1344, DOI 10.1137/060677586
   KIWIEL KC, 1990, MATH PROGRAM, V46, P105, DOI 10.1007/BF01585731
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Li LJ, 2010, INT J COMPUT VISION, V88, P147, DOI 10.1007/s11263-009-0265-6
   Li Y., 2009, P INT C ARTIFICIAL I, P344
   Li ZC, 2015, IEEE T MULTIMEDIA, V17, P1989, DOI 10.1109/TMM.2015.2477035
   Lin Y., 2012, P ACL 2012 SYSTEM DE, P169, DOI [DOI 10.1007/978-1-4614-2311-9_8, 10.2307/j.ctv18pgr3b.13]
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Niu L, 2015, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2015.7298894
   Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185
   Prest A, 2012, PROC CVPR IEEE, P3282, DOI 10.1109/CVPR.2012.6248065
   Rakotomamonjy A, 2008, J MACH LEARN RES, V9, P2491
   Schroff F, 2011, IEEE T PATTERN ANAL, V33, P754, DOI 10.1109/TPAMI.2010.133
   Shen FM, 2016, IEEE T IMAGE PROCESS, V25, P5610, DOI 10.1109/TIP.2016.2612883
   Shen FM, 2015, IEEE T IMAGE PROCESS, V24, P1839, DOI 10.1109/TIP.2015.2405340
   Siddiquie B, 2010, PROC CVPR IEEE, P2979, DOI 10.1109/CVPR.2010.5540044
   Siva P, 2011, IEEE I CONF COMP VIS, P343, DOI 10.1109/ICCV.2011.6126261
   Tang YX, 2017, IEEE T MULTIMEDIA, V19, P393, DOI 10.1109/TMM.2016.2614862
   TORRALBA A, 2011, PROC CVPR IEEE, P1521, DOI DOI 10.1109/CVPR.2011.5995347
   Vijayanarasimhan S., 2008, Proc. CVPR, P1, DOI DOI 10.1109/CVPR.2008.4587632
   Vijayanarasimhan S, 2014, INT J COMPUT VISION, V108, P97, DOI 10.1007/s11263-014-0721-9
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Yao YZ, 2017, NEUROCOMPUTING, V236, P23, DOI 10.1016/j.neucom.2016.07.066
   Yao YY, 2016, MOB INF SYST, V2016, P1, DOI 10.1155/2016/8907267
   Yao YX, 2016, ADV SOC SCI EDUC HUM, V64, P212
   Yuille AL, 2003, NEURAL COMPUT, V15, P915, DOI 10.1162/08997660360581958
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P94, DOI 10.1109/TMM.2013.2286817
NR 45
TC 62
Z9 63
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2017
VL 19
IS 8
BP 1771
EP 1784
DI 10.1109/TMM.2017.2684626
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5VV
UT WOS:000411240400007
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Huang, SR
   Zhang, J
   Schonfeld, D
   Wang, L
   Hua, XS
AF Huang, Shangrong
   Zhang, Jian
   Schonfeld, Dan
   Wang, Lei
   Hua, Xian-Sheng
TI Two-Stage Friend Recommendation Based on Network Alignment and Series
   Expansion of Probabilistic Topic Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Friend recommendation; series expansion; topic model
AB Precise friend recommendation is an important problem in social media. Although most social websites provide some kinds of auto friend searching functions, their accuracies are not satisfactory. In this paper, we propose a more precise auto friend recommendation method with two stages. In the first stage, by utilizing the information of the relationship between texts and users, as well as the friendship information between users, we align different social networks and choose some "possible friends." In the second stage, with the relationship between image features and users, we build a topic model to further refine the recommendation results. Because some traditional methods, such as variational inference and Gibbs sampling, have their limitations in dealing with our problem, we develop a novel method to find out the solution of the topic model based on series expansion. We conduct experiments on the Flickr dataset to show that the proposed algorithm recommends friends more precisely and faster than traditional methods.
C1 [Huang, Shangrong; Zhang, Jian] Univ Technol Sydney, Sch Comp & Commun, Ultimo, NSW 2007, Australia.
   [Schonfeld, Dan] Univ Illinois, Multimedia Commun Lab, Chicago, IL 60607 USA.
   [Wang, Lei] Univ Wollongong, Sch Comp & Informat Technol, Wollongong, NSW 2522, Australia.
   [Hua, Xian-Sheng] Alibaba Grp, Hangzhou 311121, Zhejiang, Peoples R China.
C3 University of Technology Sydney; University of Illinois System;
   University of Illinois Chicago; University of Illinois Chicago Hospital;
   University of Wollongong; Alibaba Group
RP Huang, SR (corresponding author), Univ Technol Sydney, Sch Comp & Commun, Ultimo, NSW 2007, Australia.
EM shangrong.huang@student.uts.edu.au; Jian.Zhang@uts.edu.au; dans@uic.edu;
   leiw@uow.edu.au; xianshenghua@gmail.com
RI Wang, Lei/AAL-9684-2020; Wang, Lei/D-9079-2013
OI Zhang, Jian/0000-0002-7240-3541; Wang, Lei/0000-0002-0961-0441
CR Ahlfors L. A., 1979, Complex Analysis, V3rd
   Ahmed S.S., 2011, Microwave Symposium Digest (MTT), 2011 IEEE MTT-S International, P1
   [Anonymous], UKRANINIAN MATH J
   [Anonymous], 2012, WHY IS IT HARD MAKE
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], P 4 INT WORKSH MOD S
   [Anonymous], THESIS
   [Anonymous], 2010, J. Math. Res
   [Anonymous], TECH REP
   [Anonymous], SOURCEBOOK MATHEMATI
   [Anonymous], 2012, P 20 ACM INT C MULTI
   [Anonymous], 2014, ACMMM
   Balser W., 1994, From Divergent Power Series to Analytic Functions: Theory and Application of Multisummable Power Series
   Bateman H., 1954, Practical Gear Design, V1
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   CARTER BD, 1977, SIAM J APPL MATH, V33, P542, DOI 10.1137/0133036
   GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596
   Griffiths T. L., 2002, P 24 ANN C COGN SCI, P381
   Guo GB, 2015, AAAI CONF ARTIF INTE, P123
   Huang SR, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P315, DOI 10.1145/2671188.2749325
   Huang SR, 2016, IEEE T MULTIMEDIA, V18, P287, DOI 10.1109/TMM.2015.2510333
   Jiang Meng., 2012, PROC 21 ACM INT C IN, P1422
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li L.-H., 2009, Proceedings of the 3rd International Conference on Ubiquitous Information Management and Communication', ICUIMC'09, ACM, P88
   Liu D., 2012, ACM MM'12', P659
   Lu DY, 2017, IEEE T MULTIMEDIA, V19, P1299, DOI 10.1109/TMM.2016.2646181
   Ma Hao, 2008, P CIKM08 C INFORM KN, P931
   Mallik RK, 2011, IEEE T COMMUN, V59, P3353, DOI 10.1109/TCOMM.2011.101011.110046
   Michiel Hazewinkel:., 2001, Encyclopedia of Mathematics
   Min WQ, 2015, IEEE T MULTIMEDIA, V17, P1787, DOI 10.1109/TMM.2015.2463226
   SPRINGER MD, 1970, SIAM J APPL MATH, V18, P721, DOI 10.1137/0118065
   Springer MD., 1979, The algebra of random variables
   Wang ZB, 2015, IEEE T MOBILE COMPUT, V14, P538, DOI 10.1109/TMC.2014.2322373
   Welling M., 2008, Proceedings of the 24th Annual Conference on Uncertainty in Artificial Intelligence (UAI-08), P587
   Zeng W., 2013, Proceedings of the 28th Annual ACM Symposium on Applied Computing', SAC'13, ACM, P237
   Zhao HY, 2013, PROCEEDINGS OF 2013 IEEE INTERNATIONAL CONFERENCE ON GREY SYSTEMS AND INTELLIGENT SERVICES (GSIS), P200, DOI 10.1109/GSIS.2013.6714773
   Zhao T., 2014, P 23 ACM INT C C INF, P261, DOI DOI 10.1145/2661829.2661998
   Zhao X., 2015, Proceedings of the 2015 International Conference on The Theory of Information Retrieval', IC-TIR'15, ACM, P71
   Zheng N, 2015, IEEE T SYST MAN CY-S, V45, P1245, DOI 10.1109/TSMC.2015.2391262
NR 39
TC 26
Z9 28
U1 1
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2017
VL 19
IS 6
BP 1314
EP 1326
DI 10.1109/TMM.2017.2652074
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5YS
UT WOS:000404059400016
OA Green Published
DA 2024-07-18
ER

PT J
AU Ma, XQ
   Zhang, C
   Liu, JC
   Shea, R
   Fu, D
AF Ma, Xiaoqiang
   Zhang, Cong
   Liu, Jiangchuan
   Shea, Ryan
   Fu, Di
TI Live Broadcast With Community Interactions: Bottlenecks and
   Optimizations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Community interactions; live broadcast
ID DELIVERY
AB Recent years have witnessed the rapid growth of new live broadcast services, represented by Twitch.tv and YouTube live events, where videos are crowdsourced from amateur users (e.g., game players), rather than from commercial and professional TV broadcaster or content providers. The viewers also actively contribute to the content through embedded open-chat channels. Such community interactions among viewers, or even between broadcasters and viewers, make content generation highly diversified and engaging, particularly for the young generation. In this context, cross-viewer synchronization is highly desirable; otherwise the viewers with shorter broadcast latency may act as spoilers, significantly affecting the user experience of other viewers. In this paper, we show that the end-to-end delay has a dramatically amplified impact on the broadcast latency for individual viewers. We suggest smart rate adaptation to achieve cross-viewer synchronization, and develop distributed algorithms based on dual decomposition. We further extend our solution to the cloud environment, and present the concept of ShadowCast, which moves broadcasters to the cloud to provide high-quality streams beyond broadcasters' network bandwidth constraint. Its practicability and effectiveness is demonstrated by our implementation and test bed experiments.
C1 [Ma, Xiaoqiang] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
   [Zhang, Cong; Liu, Jiangchuan; Shea, Ryan; Fu, Di] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
C3 Huazhong University of Science & Technology; Simon Fraser University
RP Liu, JC (corresponding author), Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
EM maxiaoqiang@hust.edu.cn; congz@sfu.ca; jcliu@sfu.ca; rws1@sfu.ca;
   dif@sfu.ca
FU Fundamental Research Funds for the Central Universities [2016YXMS293];
   Canada NSERC Discovery Grant; Canada NSERC Strategic Project Grant
FX This work was supported in part by the Fundamental Research Funds for
   the Central Universities under Grant 2016YXMS293, in part by a Canada
   NSERC Discovery Grant, and in part by a Canada NSERC Strategic Project
   Grant. The associate editor coordinating the review of this manuscript
   and approving it for publication was Prof. Honggang Wang. (Corresponding
   author: Jiangchuan Liu.)
CR [Anonymous], 1996, APPROXIMATION ALGORI
   [Anonymous], 2011, Proc. second annu. acm conf. multimed. syst.-mmsys'11, DOI DOI 10.1145/1943552.1943574
   Balachandran A, 2013, ACM SIGCOMM COMP COM, V43, P339, DOI 10.1145/2534169.2486025
   Bertsekas D. P., 2003, CONVEX ANAL OPTIMIZA
   Boyd S., 2004, CONVEX OPTIMIZATION
   Carbone M, 2010, ACM SIGCOMM COMP COM, V40, P13, DOI 10.1145/1764873.1764876
   Chen MH, 2008, PERF E R SI, V36, P169, DOI 10.1145/1384529.1375477
   Claypool Mark, 2010, Proceedings of the 1st Annual ACM SIGMM Conference on Multimedia Systems, P215, DOI [10.1145/1730836.1730863, DOI 10.1145/1730836.1730863]
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Gao GY, 2015, IEEE T MULTIMEDIA, V17, P1286, DOI 10.1109/TMM.2015.2438713
   Grant M, 2014, CVX MATLAB SOFTWARE
   Grant MC, 2008, LECT NOTES CONTR INF, V371, P95, DOI 10.1007/978-1-84800-155-8_7
   Hamilton WA, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P1315, DOI 10.1145/2556288.2557048
   Hong Xu, 2012, 2012 IEEE 5th International Conference on Cloud Computing (CLOUD), P9, DOI 10.1109/CLOUD.2012.16
   Huang JW, 2008, IEEE T CIRC SYST VID, V18, P582, DOI 10.1109/TCSVT.2008.919109
   Huang ZX, 2011, IEEE INFOCOM SER, P201, DOI 10.1109/INFCOM.2011.5935009
   Kelly F, 1997, EUR T TELECOMMUN, V8, P33, DOI 10.1002/ett.4460080106
   Li Z., 2012, Proc. of ACM Workshop on Network and Operating System Support for Digital Audio and Video (NOSSDAV), P33, DOI DOI 10.1145/2229087.2229097
   Liang C, 2011, IEEE ACM T NETWORK, V19, P1704, DOI 10.1109/TNET.2011.2141680
   Palomar DP, 2007, IEEE T AUTOMAT CONTR, V52, P2254, DOI 10.1109/TAC.2007.910665
   Palomar DP, 2006, IEEE J SEL AREA COMM, V24, P1439, DOI 10.1109/JSAC.2006.879350
   Sanchez Y., 2011, ACM MMSys'11, P257
   Shea R., 2015, P 6 ACM MULT SYST C, P97
   Shea R, 2013, IEEE NETWORK, V27, P16, DOI 10.1109/MNET.2013.6574660
   Wang Z, 2014, IEEE INFOCOM SER, P91, DOI 10.1109/INFOCOM.2014.6847928
   Wang Z, 2015, IEEE T MULTIMEDIA, V17, P867, DOI 10.1109/TMM.2015.2425216
   Zhang C., 2015, ACM Workshop on Network and Operating Systems Support for Digital Audio and Video, P55, DOI DOI 10.1145/2736084.2736091
   Zhang G, 2015, IEEE T MULTIMEDIA, V17, P229, DOI 10.1109/TMM.2014.2383617
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
NR 29
TC 9
Z9 9
U1 4
U2 74
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2017
VL 19
IS 6
BP 1184
EP 1194
DI 10.1109/TMM.2016.2646182
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5YS
UT WOS:000404059400006
DA 2024-07-18
ER

PT J
AU Min, WQ
   Jiang, SQ
   Sang, JT
   Wang, HY
   Liu, XD
   Herranz, L
AF Min, Weiqing
   Jiang, Shuqiang
   Sang, Jitao
   Wang, Huayang
   Liu, Xinda
   Herranz, Luis
TI Being a Supercook: Joint Food Attributes and Multimodal Content Modeling
   for Recipe Retrieval and Exploration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cuisine classification; recipe image retrieval; ingredient inference;
   multitask deep belief network
AB This paper considers the problem of recipe-oriented image-ingredient correlation learning with multi-attributes for recipe retrieval and exploration. Existing methods mainly focus on food visual information for recognition while we model visual information, textual content (e.g., ingredients), and attributes (e.g., cuisine and course) together to solve extended recipe-oriented problems, such as multimodal cuisine classification and attributeenhanced food image retrieval. As a solution, we propose a multimodal multitask deep belief network (M3TDBN) to learn joint image-ingredient representation regularized by different attributes. By grouping ingredients into visible ingredients (which are visible in the food image, e.g., "chicken" and "mushroom") and nonvisible ingredients (e. g., "salt" and "oil"), M3TDBN is capable of learning both midlevel visual representation between images and visible ingredients and nonvisual representation. Furthermore, in order to utilize different attributes to improve the intermodality correlation, M3TDBN incorporates multitask learning to make different attributes collaborate each other. Based on the proposed M3TDBN, we exploit the derived deep features and the discovered correlations for three extended novel applications: 1) multimodal cuisine classification; 2) attribute-augmented cross-modal recipe image retrieval; and 3) ingredient and attribute inference fromfood images. The proposed approach is evaluated on the constructed Yummly dataset and the evaluation results have validated the effectiveness of the proposed approach.
C1 [Min, Weiqing; Jiang, Shuqiang; Wang, Huayang; Herranz, Luis] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Sang, Jitao] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Liu, Xinda] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; Institute of Automation, CAS; Beihang
   University
RP Min, WQ (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM weiqing.min@vipl.ict.ac.cn; shuqiang.jiang@vipl.ict.ac.cn;
   jtsang@nlpr.ia.ac.cn; huayang.wang@vipl.ict.ac.cn; xinda.azz@gmail.com;
   luis.herranz@vipl.ict.ac.cn
RI lin, yuan/JXL-9592-2024; Liu, Xinda/X-1337-2019; Herranz,
   Luis/B-4573-2016
OI Liu, Xinda/0000-0002-1981-7243; Herranz, Luis/0000-0002-7022-3395
FU National Natural Science Foundation of China [61322212, 61532018,
   61550110505, 61602437, 61373122]; National High Technology Research and
   Development 863 Program of China [2014AA015202]; Beijing Municipal
   Commission of Science and Technology [D161100001816001]; Lenovo
   Outstanding Young Scientists Program; National Program for Special
   Support of Eminent Professionals; China Post-doctoral Science Foundation
   [2016M590135]; National Program for Support of Top-Notch Young
   Professionals
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61322212, Grant 61532018, Grant
   61550110505, Grant 61602437, and Grant 61373122, in part by the National
   High Technology Research and Development 863 Program of China under
   Grant 2014AA015202, in part by the Beijing Municipal Commission of
   Science and Technology under Grant D161100001816001, in part by the
   Lenovo Outstanding Young Scientists Program, in part by National Program
   for Special Support of Eminent Professionals and National Program for
   Support of Top-Notch Young Professionals, and in part by China
   Post-doctoral Science Foundation under Grant 2016M590135. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Benoit Huet.
CR Ahn YY, 2011, SCI REP-UK, V1, DOI 10.1038/srep00196
   Aizawa K, 2015, IEEE MULTIMEDIA, V22, P4, DOI 10.1109/MMUL.2015.39
   Aizawa K, 2013, IEEE T MULTIMEDIA, V15, P2176, DOI 10.1109/TMM.2013.2271474
   Amano S, 2015, 2015 1ST IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM), P48, DOI 10.1109/BigMM.2015.54
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], 2012, P INT C NEUR INF PRO
   [Anonymous], 2012, P ICML REPR LEARN WO
   [Anonymous], 2010, PROC ACM SIGMM INT C
   [Anonymous], 2008, P 17 INT C WORLD WID, DOI DOI 10.1145/1367497.1367629
   [Anonymous], 2011, P ICML
   [Anonymous], 2011, P 5 ACM C REC SYST, DOI 10.1145/2043932.2043979
   [Anonymous], 2012, Proceedings of the 2nd ACM international workshop on Interactive multimedia on mobile and portable devices
   [Anonymous], ACM INT C MULT
   Beijbom O, 2015, IEEE WINT CONF APPL, P844, DOI 10.1109/WACV.2015.117
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bettadapura V, 2015, IEEE WINT CONF APPL, P580, DOI 10.1109/WACV.2015.83
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Chia-Jen Lin, 2014, Advances in Knowledge Discovery and Data Mining. 18th Pacific-Asia Conference, PAKDD 2014. Proceedings: LNCS 8444, P560, DOI 10.1007/978-3-319-06605-9_46
   Christodoulidis S, 2015, LECT NOTES COMPUT SC, V9281, P458, DOI 10.1007/978-3-319-23222-5_56
   Dehais J, 2015, LECT NOTES COMPUT SC, V9281, P433, DOI 10.1007/978-3-319-23222-5_53
   Farinella GM, 2014, IEEE IMAGE PROC, P5212, DOI 10.1109/ICIP.2014.7026055
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Fischer A, 2014, PATTERN RECOGN, V47, P25, DOI 10.1016/j.patcog.2013.05.025
   Hessel Jack., 2015, CoRR
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 2012, Neural networks: tricks of the trade, P599
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2009, INT C NEURAL INF PRO, P1607
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang Y, 2015, IEEE T MULTIMEDIA, V17, P1923, DOI 10.1109/TMM.2015.2476658
   Jia YQ, 2011, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2011.6126524
   Kawano Y, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P761, DOI 10.1145/2647868.2654869
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuo FF., 2012, Proceedings of the ACM multimedia 2012 workshop on multimedia for cooking and eating activities, P1
   Liu Xiaodong., 2015, Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, P912
   Myers A, 2015, IEEE I CONF COMP VIS, P1233, DOI 10.1109/ICCV.2015.146
   Oliveira L, 2014, PATTERN RECOGN, V47, P1941, DOI 10.1016/j.patcog.2013.12.006
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Rozin P, 1999, APPETITE, V33, P163, DOI 10.1006/appe.1999.0244
   Srivastava N, 2014, J MACH LEARN RES, V15, P2949
   Su H, 2014, PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP'14 ADJUNCT), P565, DOI 10.1145/2638728.2641335
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Teng CY, 2012, PROCEEDINGS OF THE 3RD ANNUAL ACM WEB SCIENCE CONFERENCE, 2012, P298
   Wang X, 2015, PROD LOGIST, P1, DOI 10.1007/978-3-658-06869-1
   Xu RH, 2015, IEEE T MULTIMEDIA, V17, P1187, DOI 10.1109/TMM.2015.2438717
   Yang Longqi., 2015, Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, P183, DOI [DOI 10.1145/2806416, 10.1145/2806416]
   Yang S, 2010, PROC CVPR IEEE, P2249, DOI 10.1109/CVPR.2010.5539907
   Yongqi Liu, 2014, Artificial Neural Networks and Machine Learning - ICANN 2014. 24th International Conference on Artificial Neural Networks. Proceedings: LNCS 8681, P773, DOI 10.1007/978-3-319-11179-7_97
   Yuan ZQ, 2014, IEEE T MULTIMEDIA, V16, P1624, DOI 10.1109/TMM.2014.2322338
NR 51
TC 72
Z9 73
U1 1
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2017
VL 19
IS 5
BP 1100
EP 1113
DI 10.1109/TMM.2016.2639382
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5XS
UT WOS:000404056000017
DA 2024-07-18
ER

PT J
AU Li, SH
   Xu, J
   van der Schaar, M
   Li, WP
AF Li, Suoheng
   Xu, Jie
   van der Schaar, Mihaela
   Li, Weiping
TI Trend-Aware Video Caching Through Online Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cache replacement; content caching; online learning
ID POPULARITY; PERFORMANCE; PREDICTION; BANDITS
AB This paper presents Trend-Caching, a novel cache replacement method that optimizes cache performance according to the trends of video content. Trend-Caching explicitly learns the popularity trend of video content and uses it to determine which video it should store and which it should evict from the cache. Popularity is learned in an online fashion and requires no training phase, hence it is more responsive to continuously changing trends of videos. We prove that the learning regret of Trend-Caching (i.e., the gap between the hit rate achieved by Trend-Caching and that by the optimal caching policy with hindsight) is sublinear in the number of video requests, thereby guaranteeing both fast convergence and asymptotically optimal cache hit rate. We further validate the effectiveness of Trend-Caching by applying it to a movie.douban.com dataset that contains over 38 million requests. Our results show significant cache hit rate lift compared to existing algorithms, and the improvements can exceed 40% when the cache capacity is limited. Furthermore, Trend-Caching has low complexity.
C1 [Li, Suoheng; Li, Weiping] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.
   [Li, Suoheng; van der Schaar, Mihaela] Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90002 USA.
   [Xu, Jie] Univ Miami, Dept Elect & Comp Engn, Coral Gables, FL 33146 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; University of California System; University of California
   Los Angeles; University of Miami
RP Li, SH (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.; Li, SH (corresponding author), Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90002 USA.
EM lzzitc@mail.ustc.edu.cn; jiexu@miami.edu; mihaela@ee.ucla.edu;
   wpli@ustc.edu.cn
FU Intel Collaborative Research Institute on Mobile Networking and
   Computing; National Science Foundation [NSF CCF 1524417]
FX The work of W. Li and S. Li was supported in part by the Intel
   Collaborative Research Institute on Mobile Networking and Computing. The
   work of J. Xu and M. van der Schaar was supported in part by the
   National Science Foundation under Grant NSF CCF 1524417. The guest
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Qian Zhang.
CR Ahlgren B, 2012, IEEE COMMUN MAG, V50, P26, DOI 10.1109/MCOM.2012.6231276
   Andreev K., 2003, SPAA, P149
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], 2013, CORR
   [Anonymous], P WORKSH US PROF DAT
   BELADY LA, 1966, IBM SYST J, V5, P78, DOI 10.1147/sj.52.0078
   Blasco P, 2014, IEEE ICC, P1897, DOI 10.1109/ICC.2014.6883600
   Borst S, 2010, IEEE INFOCOM SER, DOI 10.1109/infcom.2010.5461964
   Dai J, 2012, IEEE INFOCOM SER, P2444, DOI 10.1109/INFCOM.2012.6195634
   Dilley J, 2002, IEEE INTERNET COMPUT, V6, P50, DOI 10.1109/MIC.2002.1036038
   Dilley J, 1999, IEEE INTERNET COMPUT, V3, P44, DOI 10.1109/4236.806998
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Famaey J, 2013, J NETW COMPUT APPL, V36, P219, DOI 10.1016/j.jnca.2012.08.014
   Gürsun G, 2011, IEEE INFOCOM SER, P16, DOI 10.1109/INFCOM.2011.5934965
   Jacobson V, 2012, COMMUN ACM, V55, P117, DOI 10.1145/2063176.2063204
   Jaleel A, 2010, CONF PROC INT SYMP C, P60, DOI 10.1145/1815961.1815971
   Kleinberg R, 2008, ACM S THEORY COMPUT, P681
   Li HT, 2015, 2015 IEEE 8TH INTERNATIONAL CONFERENCE ON CLOUD COMPUTING, P877, DOI 10.1109/CLOUD.2015.120
   Li H, 2013, TRANSGENIC RES, V22, P169, DOI 10.1007/s11248-012-9623-1
   Li SQ, 2016, AER ADV ENG RES, V90, P1
   Manning Christopher D., 2008, INTRO INFORM RETRIEV
   Niu D, 2011, IEEE INFOCOM SER, P421, DOI 10.1109/INFCOM.2011.5935196
   Nygren E., 2010, SIGOPS OPER SYST REV, V44, P2, DOI [10.1145/1842733.1842736, DOI 10.1145/1842733.1842736]
   Ren SL, 2012, IEEE T MULTIMEDIA, V14, P1566, DOI 10.1109/TMM.2012.2217120
   Rossi D, 2011, CACHING PERFORMANCE
   Roy SD, 2013, IEEE T MULTIMEDIA, V15, P1255, DOI 10.1109/TMM.2013.2265079
   Scellato S., 2011, Proceedings of the 20th International Conference on World Wide Web, P457
   Shafiq M. Zubair, 2014, ACM SIGMETRICS Performance Evaluation Review, V42, P567, DOI 10.1145/2591971.2592021
   Sitaraman RK, 2014, WILEY SER PARA DIST, P307
   Slivkins A, 2014, J MACH LEARN RES, V15, P2533
   Szabo G, 2010, COMMUN ACM, V53, P80, DOI 10.1145/1787234.1787254
   Tekin C, 2015, IEEE T MULTIMEDIA, V17, P549, DOI 10.1109/TMM.2015.2403234
   Traverso S, 2013, ACM SIGCOMM COMP COM, V43, P6
   Wang Z, 2012, IEEE INFOCOM SER, P2901, DOI 10.1109/INFCOM.2012.6195726
   Wu Y, 2015, IEEE ACM T NETWORK, V23, P689, DOI 10.1109/TNET.2014.2308254
   Xu J, 2015, IEEE J-STSP, V9, P330, DOI 10.1109/JSTSP.2014.2370942
   Zhou YP, 2015, IEEE T MULTIMEDIA, V17, P1273, DOI 10.1109/TMM.2015.2447277
NR 37
TC 76
Z9 78
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2016
VL 18
IS 12
BP 2503
EP 2516
DI 10.1109/TMM.2016.2596042
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ED5VK
UT WOS:000388920200016
OA hybrid
DA 2024-07-18
ER

PT J
AU Kang, JW
   Ryu, SK
   Kim, NY
   Kang, MJ
AF Kang, Je-Won
   Ryu, Soo-Kyung
   Kim, Na-Young
   Kang, Min-Joo
TI Efficient Residual DPCM Using an <i>l</i><sub>1</sub> Robust Linear
   Prediction in Screen Content Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE l(1) optimization; lossy and lossless coding; machine learning;
   parallelism; residual differential pulse code modulation (RDPCM); screen
   content coding (SCC)
ID LOSSLESS; EXTENSIONS; ALGORITHM; COMPRESSION; SHRINKAGE
AB In this paper, a residual differential pulse code modulation (RDPCM) coding technique using a weighted linear combination of neighboring residual samples is proposed to provide coding efficiency in the screen content video coding. The RDPCM performs the sample-based prediction of residue to reduce spatial redundancies. The proposed method uses the l(1) optimization in the weight derivation by considering the statistical characteristics of graphical components in videos in an intracoding. Specifically we use the least absolute shrinkage and selection operator to derive the weights because the solution is accurate in high variance residue. Furthermore, we enhance parallelism in a line processing by restricting the support to the row-wise prediction to above samples or the column-wise prediction to the left samples. The proposed method uses an explicit RDPCM scheme, so a coding mode determined by rate-distortion optimization is transmitted to a decoder. For coding the overhead, we develop a context design in CABAC based on correlation between an intraprediction direction and an RDPCM prediction mode. It is demonstrated with the experimental results that the proposed method provides a significant coding gain over the state-of-the-art reference codec for screen content video coding.
C1 [Kang, Je-Won; Ryu, Soo-Kyung; Kim, Na-Young; Kang, Min-Joo] Ewha Womans Univ, Dept Elect Engn, Seoul 120750, South Korea.
C3 Ewha Womans University
RP Kang, JW (corresponding author), Ewha Womans Univ, Dept Elect Engn, Seoul 120750, South Korea.
EM sagittak@gmail.com; rsk19912@gmail.com; 12skdud21@naver.com;
   joo2950@hanmail.net
RI kim, nayoung/IWV-4038-2023; Kang, Jewon/AAU-9722-2020
FU Basic Science Research Program through the National Research Foundation
   of Korea (NRF) - Ministry of Education [NRF-2014R1A1A2056587]
FX This work was supported by the Basic Science Research Program through
   the National Research Foundation of Korea (NRF) funded by the Ministry
   of Education (NRF-2014R1A1A2056587). The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Yonghong Tian. (Corresponding author: Je-Won Kang.)
CR Amon P., 2013, JVTO013 ISOIEC JTC1S
   [Anonymous], 2014, JVTO013 ISOIEC JTC1S
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Boyce JM, 2016, IEEE T CIRC SYST VID, V26, P20, DOI 10.1109/TCSVT.2015.2461951
   Chen H., 2013, JVTO013 ISOIEC JTC1S
   Cheung A., 2011, JVTO013 ISOIEC JTC1S
   Donoho DL, 2008, IEEE T INFORM THEORY, V54, P4789, DOI 10.1109/TIT.2008.929958
   Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067
   Figueiredo MAT, 2007, IEEE J-STSP, V1, P586, DOI 10.1109/JSTSP.2007.910281
   Flynn D, 2016, IEEE T CIRC SYST VID, V26, P4, DOI 10.1109/TCSVT.2015.2478707
   Fuldseth A., 2011, JCTVCE408
   Gao P, 2015, IEEE T MULTIMEDIA, V17, P1153, DOI 10.1109/TMM.2015.2438711
   Garcia DC, 2010, IEEE SIGNAL PROC LET, V17, P831, DOI 10.1109/LSP.2010.2060192
   Golub G.H., 1989, MATRIX COMPUTATIONS
   Han KH, 2009, IEICE T FUND ELECTR, VE92A, P1386, DOI 10.1587/transfun.E92.A.1386
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P970, DOI 10.1109/TCSVT.2002.805511
   Henry F., 2011, JVTO013 ISOIEC JTC1S
   Hong SW, 2013, SIGNAL PROCESS-IMAGE, V28, P1335, DOI 10.1016/j.image.2013.09.004
   Joshi R., 2013, JCTVCN0052 ISOIECJTC
   Joshi R., 2015, JCTVCT1005 ISOIECJTC
   Joshi R., 2014, JCTVCS1014 ISOIECJTC
   Kang JH, 2014, PHYS REV X, V4, DOI 10.1103/PhysRevX.4.031005
   Kim K, 2014, 10TH INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY AND INTERNET-BASED SYSTEMS SITIS 2014, P652, DOI 10.1109/SITIS.2014.87
   Kwon D., 2013, JVTO013 ISOIEC JTC1S
   Lee S., 2013, JVTO013 ISOIEC JTC1S
   Lee YL, 2006, IEEE T IMAGE PROCESS, V15, P2610, DOI 10.1109/TIP.2006.877396
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P813, DOI 10.1109/83.923277
   Liu L., 2007, PICT COD S LISB PORT
   Motta G, 2000, P IEEE, V88, P1790, DOI 10.1109/5.892714
   Müller K, 2013, IEEE T IMAGE PROCESS, V22, P3366, DOI 10.1109/TIP.2013.2264820
   Myers R. H., 1990, CLASSICAL MODERN REG
   Naccari M., 2013, JCTVCM0442 ISOIECJTC
   Naccari M, 2013, PICT COD SYMP, P361, DOI 10.1109/PCS.2013.6737758
   Pang C., 2013, JCTVCN0256 ISOIECJTC
   Peng X., 2013, JCTVCN0288 ISOIECJTC
   Puet W., 2013, JVTO013 ISOIEC JTC1S
   Sole J., 2011, JVTO013 ISOIEC JTC1S
   Sole J., 2013, JCTVCM0315 ISOIECJTC
   Sullivan G., 2012, CIRCUITS SYSTEMS VID, V12, P1649, DOI DOI 10.1109/TCSVT.2012.2221191
   Sullivan GJ, 2013, IEEE J-STSP, V7, P1001, DOI 10.1109/JSTSP.2013.2283657
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Weinberger MJ, 2000, IEEE T IMAGE PROCESS, V9, P1309, DOI 10.1109/83.855427
   Wu XL, 1997, IEEE T COMMUN, V45, P437, DOI 10.1109/26.585919
   Xiulian Peng, 2010, 2010 IEEE International Symposium on Circuits and Systems. ISCAS 2010, P4221, DOI 10.1109/ISCAS.2010.5537573
   Xu JZ, 2016, IEEE T CIRC SYST VID, V26, P50, DOI 10.1109/TCSVT.2015.2478706
   Yu H, 2014, JCTVCS1015
   Zhang L., 2014, JVTO013 ISOIEC JTC1S
   Zhou M., 2011, JVTO013 ISOIEC JTC1S
   Zhu WJ, 2015, IEEE T MULTIMEDIA, V17, P935, DOI 10.1109/TMM.2015.2428171
   Zhu WJ, 2014, IEEE T MULTIMEDIA, V16, P1316, DOI 10.1109/TMM.2014.2315782
NR 50
TC 8
Z9 10
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2016
VL 18
IS 10
BP 2054
EP 2065
DI 10.1109/TMM.2016.2595259
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DX8NC
UT WOS:000384644800012
DA 2024-07-18
ER

PT J
AU Su, Z
   Xu, QC
   Fei, MR
   Dong, MX
AF Su, Zhou
   Xu, Qichao
   Fei, Minrui
   Dong, Mianxiong
TI Game Theoretic Resource Allocation in Media Cloud With Mobile Social
   Users
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Media cloud; mobile social networks (MSNs); resource allocation;
   Stackelberg game
ID MULTIMEDIA CLOUD; NETWORKS; MANAGEMENT; DESIGN
AB Due to the rapid increases in both the population of mobile social users and the demand for quality of experience (QoE), providing mobile social users with satisfied multimedia services has become an important issue. Media cloud has been shown to be an efficient solution to resolve the above issue, by allowing mobile social users to connect to it through a group of distributed brokers. However, as the resource in media cloud is limited, how to allocate resource among media cloud, brokers, and mobile social users becomes a new challenge. Therefore, in this paper, we propose a game theoretic resource allocation scheme for media cloud to allocate resource to mobile social users though brokers. First, a framework of resource allocation among media cloud, brokers, and mobile social users is presented. Media cloud can dynamically determine the price of the resource and allocate its resource to brokers. A mobile social user can select his broker to connect to the media cloud by adjusting the strategy to achieve the maximum revenue, based on the social features in the community. Next, we formulate the interactions among media cloud, brokers, and mobile social users by a four-stage Stackelberg game. In addition, through the backward induction method, we propose an iterative algorithm to implement the proposed scheme and obtain the Stackelberg equilibrium. Finally, simulation results show that each player in the game can obtain the optimal strategy where the Stackelberg equilibrium exists stably.
C1 [Su, Zhou; Xu, Qichao; Fei, Minrui] Shanghai Univ, Sch Mechatron Engn & Automat, Shanghai 200072, Peoples R China.
   [Dong, Mianxiong] Muroran Inst Technol, Dept Informat & Elect Engn, Muroran, Hokkaido 0508585, Japan.
C3 Shanghai University; Muroran Institute of Technology
RP Su, Z; Fei, MR (corresponding author), Shanghai Univ, Sch Mechatron Engn & Automat, Shanghai 200072, Peoples R China.
RI Dong, Mianxiong/O-7489-2019
OI Dong, Mianxiong/0000-0002-2788-3451
FU National Natural Science Foundation of China [61571286, 61533010]; Key
   Project of Science and Technology Commission of Shanghai Municipality
   [12JC1404201, 14JC1402200]; Shanghai Key Laboratory of Power Station
   Automation Technology; JSPS [16K00117]; JSPS A3 Foresight Program;
   Grants-in-Aid for Scientific Research [16K00117] Funding Source: KAKEN
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61571286 and Grant 61533010, in part by
   the Key Project of Science and Technology Commission of Shanghai
   Municipality under Grant 12JC1404201 and Grant 14JC1402200, in part by
   Shanghai Key Laboratory of Power Station Automation Technology, in part
   by the JSPS KAKENHI Grant 16K00117, and in part by the JSPS A3 Foresight
   Program. The guest editor coordinating the review of this manuscript and
   approving it for publication was Prof. Honggang Wang. (Corresponding
   authors: Zhou Su and Minrui Fei.)
CR Aggarwal V, 2013, IEEE T MULTIMEDIA, V15, P789, DOI 10.1109/TMM.2013.2240287
   Alasaad A, 2015, IEEE T PARALL DISTR, V26, P1021, DOI 10.1109/TPDS.2014.2316827
   [Anonymous], CISC VIS NETW IND GL
   [Anonymous], 2004, ELEMENTS APPL BIFURC
   [Anonymous], 2006, INT J THERMO PHYS
   Bohai Hong, 2013, 2013 IEEE 4th International Conference on Software Engineering and Service Science (ICSESS), P841, DOI 10.1109/ICSESS.2013.6615436
   Chard Kyle, 2010, 2010 IEEE 3rd International Conference on Cloud Computing (CLOUD 2010), P99, DOI 10.1109/CLOUD.2010.28
   Kiani Saad Liaquat, 2010, Proceedings of the 2010 IEEE 10th International Conference on Computer and Information Technology (CIT 2010), P2964, DOI 10.1109/CIT.2010.495
   Kim SH, 2012, IEEE T CONSUM ELECTR, V58, P691, DOI 10.1109/TCE.2012.6227478
   Li Pei, 2014, ScientificWorldJournal, V2014, P914907, DOI 10.1155/2014/914907
   Li Z, 2014, IEEE INFOCOM SER, P1941, DOI 10.1109/INFOCOM.2014.6848134
   Lu P, 2015, IEEE T MULTIMEDIA, V17, P1297, DOI 10.1109/TMM.2015.2441004
   Lu ZQ, 2014, IEEE INFOCOM SER, P1932, DOI 10.1109/INFOCOM.2014.6848133
   Magedanz T, 2014, 2014 IEEE FIFTH INTERNATIONAL CONFERENCE ON COMMUNICATIONS AND ELECTRONICS (ICCE), P168, DOI 10.1109/CCE.2014.6916698
   Nan XM, 2012, IEEE INT SYMP CIRC S, P1111
   Niyato D, 2012, IEEE WCNC, P3128, DOI 10.1109/WCNC.2012.6214343
   Niyato D, 2009, IEEE T MOBILE COMPUT, V8, P1009, DOI 10.1109/TMC.2008.157
   Qiu XJ, 2014, IEEE INT CONF CLOUD, P296, DOI 10.1109/CLOUD.2014.48
   Ren J, 2015, IEEE COMMUN MAG, V53, P98, DOI 10.1109/MCOM.2015.7060488
   Ren SL, 2013, IEEE T MULTIMEDIA, V15, P723, DOI 10.1109/TMM.2013.2240673
   Sardis F, 2013, IEEE T MULTIMEDIA, V15, P769, DOI 10.1109/TMM.2013.2240286
   Stanojev I, 2013, IEEE ICC, P5495, DOI 10.1109/ICC.2013.6655465
   Su Z, 2015, SPRINGER THESES-RECO, P1, DOI 10.1007/978-3-662-46651-3
   Su Z, 2016, IEEE WIREL COMMUN, V23, P90, DOI 10.1109/MWC.2016.7553031
   Su Z, 2016, IEEE NETWORK, V30, P52, DOI 10.1109/MNET.2016.7389831
   Su Z, 2015, IEEE NETWORK, V29, P62, DOI 10.1109/MNET.2015.7166192
   Su Z, 2015, IEEE COMMUN MAG, V53, P66, DOI 10.1109/MCOM.2015.7120047
   Wang XF, 2013, IEEE T MULTIMEDIA, V15, P811, DOI 10.1109/TMM.2013.2239630
   Wang YS, 2013, IEEE T WIREL COMMUN, V12, P6043, DOI 10.1109/TWC.2013.102313.121508
   Wu J, 2012, IEEE INFOCOM SER, P1368, DOI 10.1109/INFCOM.2012.6195500
   Wu Y, 2015, IEEE ACM T NETWORK, V23, P689, DOI 10.1109/TNET.2014.2308254
   Xu QC, 2015, IEEE T EMERG TOP COM, V3, P399, DOI 10.1109/TETC.2015.2414792
   Xu QC, 2016, IEEE T VEH TECHNOL, V65, P6692, DOI 10.1109/TVT.2015.2472289
   Yao-Jen Chang, 2007, 2007 International Conference on Convergence Information Technology - ICCIT '07, P151, DOI 10.1109/ICCIT.2007.132
   Yin WY, 2014, IEEE T MULTIMEDIA, V16, P184, DOI 10.1109/TMM.2013.2283468
   Yin ZY, 2015, IEEE T WIREL COMMUN, V14, P4020, DOI 10.1109/TWC.2015.2416177
   Zhan SC, 2014, IEEE INT SYMP DYNAM, P439, DOI 10.1109/DySPAN.2014.6817827
   Zhang WW, 2013, IEEE T MULTIMEDIA, V15, P1431, DOI 10.1109/TMM.2013.2247583
   Zheng K, 2016, IEEE NETWORK, V30, P44, DOI 10.1109/MNET.2016.7389830
NR 39
TC 75
Z9 79
U1 0
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2016
VL 18
IS 8
BP 1650
EP 1660
DI 10.1109/TMM.2016.2566584
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR5YJ
UT WOS:000379978000017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, J
   Wang, JP
   Lu, KJ
   Qian, Y
   Gu, NJ
AF Wang, Jin
   Wang, Jianping
   Lu, Kejie
   Qian, Yi
   Gu, Naijie
TI On the Optimal Linear Network Coding Design for Information
   Theoretically Secure Unicast Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Confidentiality; linear network coding (LNC); passive attack; security;
   unicast streaming
ID MULTIMEDIA; CODES; RESILIENT; FRAMEWORK; SCHEME
AB The continuous growth of media-rich content calls for more efficient and secure methods for content delivery. In this paper, we will address the optimal linear network coding (LNC) design for secure unicast streaming against passive attacks, under the requirement of information theoretical security. The objectives include 1) satisfying the information theoretical security requirement, 2) maximizing the transmission rate of a unicast stream, 3) minimizing the number of additional random symbols, and 4) minimizing the total bandwidth cost of content delivery. To fulfill the first three objectives, we formulate an information theoretically secure unicast streaming (ITSUS) problem, and then solve it by transforming it to a maximum network flow problem with node-capacity constraints. Based on the solution of the ITSUS problem, we develop an efficient algorithm that can find the optimal transmission topology with minimum bandwidth cost in a polynomial amount of time. With the optimal transmission topology, we investigate the design of both deterministic LNC and random LNC. For the deterministic LNC design, we not only prove that it achieves the four objectives but also analyze the size of required finite field. Moreover, for the random LNC design, we analyze the probability that a random LNC scheme satisfies the information theoretical security requirement. Finally, extensive simulation experiments have been conducted, and the results demonstrate the effectiveness of the proposed algorithms.
C1 [Wang, Jin] Soochow Univ, Dept Comp Sci & Technol, Suzhou 215006, Peoples R China.
   [Wang, Jianping] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   [Lu, Kejie] Shanghai Univ Elect Power, Coll Comp Sci & Technol, Shanghai 200090, Peoples R China.
   [Lu, Kejie] Univ Puerto Rico, Dept Elect & Comp Engn, Mayaguez, PR 00680 USA.
   [Qian, Yi] Univ Nebraska, Dept Elect & Comp Engn, Lincoln, NE 68588 USA.
   [Gu, Naijie] Univ Sci & Technol China, Dept Comp Sci, Hefei 230026, Peoples R China.
C3 Soochow University - China; City University of Hong Kong; Shanghai
   University of Electric Power; University of Puerto Rico; University of
   Puerto Rico Mayaguez; University of Nebraska System; University of
   Nebraska Lincoln; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Wang, J (corresponding author), Soochow Univ, Dept Comp Sci & Technol, Suzhou 215006, Peoples R China.; Wang, JP (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.; Lu, KJ (corresponding author), Shanghai Univ Elect Power, Coll Comp Sci & Technol, Shanghai 200090, Peoples R China.; Qian, Y (corresponding author), Univ Nebraska, Dept Elect & Comp Engn, Lincoln, NE 68588 USA.; Gu, NJ (corresponding author), Univ Sci & Technol China, Dept Comp Sci, Hefei 230026, Peoples R China.
EM wjin1985@suda.edu.cn; jianwang@cityu.edu.hk; kejie.lu@upr.edu;
   yqian2@unl.edu; gunj@ustc.edu.cn
RI qian, yi/HZH-4175-2023; Qian, Yi/KEI-0952-2024
OI WANG, Jianping/0000-0002-9318-1482
FU National Natural Science Foundation of China [61202378, 61272462,
   61572310]; Shanghai Oriental Scholar Program; General Research Fund from
   Hong Kong Research Grant Council [122913]; China Postdoctoral Science
   Foundation [2013M531402, 2014T70544]; Application Foundation; Research
   of Suzhou [SYG201401]; Collaborative Innovation Center of Novel Software
   Technology and Industrialization
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61202378, Grant 61272462, and Grant
   61572310, in part by the Shanghai Oriental Scholar Program, in part by
   the General Research Fund from Hong Kong Research Grant Council under
   Grant 122913, in part by the China Postdoctoral Science Foundation under
   Grant 2013M531402 and Grant 2014T70544, in part by the Application
   Foundation. Research of Suzhou under Grant SYG201401, and in part by the
   Collaborative Innovation Center of Novel Software Technology and
   Industrialization. This paper was presented in part at the IEEE
   International Conference on Computer Communications, Shanghai, China,
   April 2011. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Enrico Magli.
CR Ahuja R. K., 1993, Network flows: theory, algorithms, and applications
   [Anonymous], 2005, P 1 WORKSH NETW COD
   Bourtsoulatze E, 2014, IEEE T MULTIMEDIA, V16, P1752, DOI 10.1109/TMM.2014.2328320
   Cai N, 2002, PROCEEDINGS OF 2002 IEEE INFORMATION THEORY WORKSHOP, P119, DOI 10.1109/ITW.2002.1115432
   CAI N, 2002, IEEE INT S INF THEOR
   Cisco Systems Inc, 2014, CISC VIS NETW IND FO
   FELDMAN J, 2004, 42 ANN ALL C COMM CO
   Fiandrotti A, 2014, IEEE T MULTIMEDIA, V16, P521, DOI 10.1109/TMM.2013.2285518
   Li SYR, 2003, IEEE T INFORM THEORY, V49, P371, DOI 10.1109/TIT.2002.807285
   Líma L, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY PROCEEDINGS, VOLS 1-7, P546, DOI 10.1109/ISIT.2007.4557282
   Lima L, 2010, IEEE J SEL AREA COMM, V28, P377, DOI 10.1109/JSAC.2010.100409
   Liu SC, 2014, C LOCAL COMPUT NETW, P474, DOI 10.1109/LCN.2014.6925820
   Lu H., 2011, P 7 INT C PRED MOD S, P1
   Magli E, 2013, IEEE T MULTIMEDIA, V15, P1195, DOI 10.1109/TMM.2013.2241415
   Sergeev I, 2013, 2013 IEEE MILITARY COMMUNICATIONS CONFERENCE (MILCOM 2013), P458, DOI 10.1109/MILCOM.2013.85
   Sheikh AM, 2014, IEEE T MULTIMEDIA, V16, P2294, DOI 10.1109/TMM.2014.2357716
   Silva D, 2011, IEEE T INFORM THEORY, V57, P1124, DOI 10.1109/TIT.2010.2090212
   Vukobratovic D, 2014, IEEE T MULTIMEDIA, V16, P277, DOI 10.1109/TMM.2013.2282129
   Wang J., 2010, P INF COMM INFOCOM, P2240
   Wang J, 2013, IEEE T PARALL DISTR, V24, P2025, DOI 10.1109/TPDS.2012.265
   Wang J, 2011, IEEE INFOCOM SER, P757, DOI 10.1109/INFCOM.2011.5935296
   Wang J, 2014, COMP BIOCHEM PHYS D, V11, P1, DOI 10.1016/j.cbd.2014.05.001
   Wang P, 2013, IEEE T MULTIMEDIA, V15, P684, DOI 10.1109/TMM.2012.2236304
   WAXMAN BM, 1988, IEEE J SEL AREA COMM, V6, P1617, DOI 10.1109/49.12889
   Wu C, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1386109.1386112
   Xiangmao Chang, 2010, Proceedings of the 2010 IEEE 30th International Conference on Distributed Computing Systems. ICDCS 2010, P526, DOI 10.1109/ICDCS.2010.27
   Yao HY, 2014, IEEE ACM T NETWORK, V22, P1978, DOI 10.1109/TNET.2013.2294254
   Yu QY, 2013, IEEE J SEL AREA COMM, V31, P142, DOI 10.1109/JSAC.2013.SUP.0513013
   Zhang P, 2014, IEEE T PARALL DISTR, V25, P2211, DOI 10.1109/TPDS.2013.161
NR 29
TC 11
Z9 12
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2016
VL 18
IS 6
BP 1149
EP 1162
DI 10.1109/TMM.2016.2545403
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DM1LP
UT WOS:000376107100017
DA 2024-07-18
ER

PT J
AU Liu, YN
   Niu, D
   Li, BC
AF Liu, Yinan
   Niu, Di
   Li, Baochun
TI Delay-Optimized Video Traffic Routing in Software-Defined
   Interdatacenter Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Inter-datacenter networks; packet delays; routing algorithm;
   software-defined networking; video traffic
AB Many video streaming applications operate their geo-distributed services in the cloud, taking advantage of superior connectivities between datacenters to push content closer to users or to relay live video traffic between end users at a higher throughput. In the meantime, inter-datacenter networks also carry high volumes of other types of traffic, including service replication and data backups, e.g., for storage and email services. It is an important research topic to optimally engineer and schedule inter-datacenter traffic, taking into account the stringent latency requirements of video flows when transmitted along inter-datacenter links shared with other types of traffic. Since inter-datacenter networks are usually overprovisioned, unlike prior work that mainly aims to maximize link utilization, we propose a delay-optimized traffic routing scheme to explicitly differentiate path selection for different sessions according to their delay sensitivities, leading to a software-defined inter-datacenter networking overlay implemented at the application layer. We show that our solution can yield sparse path selection by only solving linear programs, and thus, in contrast to prior traffic engineering solutions, does not lead to overly fine-grained traffic splitting, further reducing packet resequencing overhead and the number of forwarding rules to be installed in each forwarding unit. Real-world experiments based on a deployment on six globally distributed Amazon EC2 datacenters have shown that our system can effectively prioritize and improve the delay performance of inter-datacenter video flows at a low cost.
C1 [Liu, Yinan; Li, Baochun] Univ Toronto, Dept Elect & Comp Engn, Toronto, ON M5S 3G4, Canada.
   [Niu, Di] Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB T6G 2V4, Canada.
C3 University of Toronto; University of Alberta
RP Liu, YN; Li, BC (corresponding author), Univ Toronto, Dept Elect & Comp Engn, Toronto, ON M5S 3G4, Canada.; Niu, D (corresponding author), Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB T6G 2V4, Canada.
EM yinan@ece.toronto.edu; dniu@ualberta.ca; bli@ece.toronto.edu
RI baochun, Li/AAD-3188-2022
FU NSERC Discovery Research Program; SAVI NSERC Strategic Networks Grant at
   the University of Toronto
FX This work was supported by the NSERC Discovery Research Program and the
   SAVI NSERC Strategic Networks Grant at the University of Toronto. The
   guest editor coordinating the review of this manuscript and approving it
   for publication was Prof. Dapeng Oliver Wu.
CR Adhikari V.K., 2010, Proc. 10th Internet Measurement Conference IMC, P431
   Al-Fares M., 2010, USENIX S NETW SYST D
   [Anonymous], 1998, 2386 RFC
   [Anonymous], 2002, MATRIX RANK MINIMIZA
   [Anonymous], 1999, RFC 2702
   [Anonymous], 2475 RFC
   Benson T., 2011, ACM INT C EM NETW EX
   Benson T., 2010, P INT NETW MAN C RES
   Braden R., 1994, INTEGRATED SERVICES
   Callon R., 2001, 3031 RFC
   Chen M., 2008, ACM SIGMETRICS ANN M
   Chen X., 2011, 19 ACM INT C MULT SC
   Chen Y., 2011, P IEEE INFOCOM, P620
   Chowdhury M, 2011, ACM SIGCOMM COMP COM, V41, P98, DOI 10.1145/2043164.2018448
   Cockcroft A., 2011, Netflix in the Cloud
   Curtis AR, 2011, ACM SIGCOMM COMP COM, V41, P254, DOI 10.1145/2043164.2018466
   Danna E, 2012, IEEE INFOCOM SER, P846, DOI 10.1109/INFCOM.2012.6195833
   Elwalid A, 2001, IEEE INFOCOM SER, P1300, DOI 10.1109/INFCOM.2001.916625
   Fazel M, 2003, P AMER CONTR CONF, P2156, DOI 10.1109/acc.2003.1243393
   Feng Y., 2012, Proc. 20th ACM international conference on Multimedia, P259
   Hong CY, 2013, ACM SIGCOMM COMP COM, V43, P15, DOI 10.1145/2534169.2486012
   Jain S, 2013, ACM SIGCOMM COMP COM, V43, P3, DOI 10.1145/2534169.2486019
   Jiang J., 2012, IEEE INFOCOM ORL FL
   Krishnan R, 2009, IMC'09: PROCEEDINGS OF THE 2009 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P190
   Li J., 2004, MSRTR200498
   McKeown N., 2012, OPENFLOW SPECIFICATI
   Meyer M., 2010, 5712 RFC
   Pathak Abhinav., 2011, P 2011 ACM SIGCOMM C, P463
   Ponec M, 2009, IEEE INT CON MULTI, P1406, DOI 10.1109/ICME.2009.5202767
   Reitblatt M, 2012, ACM SIGCOMM COMP COM, V42, P323, DOI 10.1145/2377677.2377748
   Si XB, 2012, IEEE INT WORKS INFOR, P1, DOI 10.1109/WIFS.2012.6412616
   Wu Y, 2012, IEEE INFOCOM SER, P684, DOI 10.1109/INFCOM.2012.6195813
   Zhang L., 1993, IEEE Network, V7, P8, DOI 10.1109/65.238150
NR 33
TC 41
Z9 43
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2016
VL 18
IS 5
SI SI
BP 865
EP 878
DI 10.1109/TMM.2016.2538718
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DK5YD
UT WOS:000374996200007
DA 2024-07-18
ER

PT J
AU Liu, LQ
   Xiong, C
   Zhang, HW
   Niu, ZH
   Wang, M
   Yan, SC
AF Liu, Luoqi
   Xiong, Chao
   Zhang, Hanwang
   Niu, Zhiheng
   Wang, Meng
   Yan, Shuicheng
TI Deep Aging Face Verification With Large Gaps
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-age; deep learning; face verification
ID RECOGNITION; DIMENSIONALITY
AB Along with the long-time evolution of popular social networks, e.g. Facebook, social media analysis research inevitably arrived at the era of considering face/user recognition with large age gaps. However, related research with adequate subjects and large age gaps is surprisingly rare. In this work, we first collect a so-called cross-age face (CAFE) dataset, ranging from child, to young, to adult, to old groups. Then, we propose a novel framework, called deep aging face verification (DAFV), for this challenging task. DAFV includes two modules: aging pattern synthesis and aging face verification. The aging pattern synthesis module synthesizes the faces of all age groups for the input face of an arbitrary age, and the core structure is a deep aging-aware denoising auto-encoder (a(2)-DAE) with multiple outputs. The aging face verification module then takes the synthesized aging patterns of a face pair as the input, and each pair of synthesized images of the same age group is fed into a parallel CNN; finally, all parallel CNN outputs are fused to provide similar/dissimilar prediction. For DAFV, the training of the aging face verification module easily suffers from the overfitting results from the aging pattern synthesis module, and we propose to use the cross-validation strategy to produce error-aware outputs for the synthesis module. Extensive experiments on the CAFE dataset well demonstrate the superiority of the proposed DAFV framework over other solutions for aging face verification.
C1 [Liu, Luoqi; Niu, Zhiheng; Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117583, Singapore.
   [Zhang, Hanwang] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
   [Xiong, Chao] Univ London Imperial Coll Sci Technol & Med, Dept Elect & Elect Engn, London W3 0DF, England.
   [Wang, Meng] Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Peoples R China.
C3 National University of Singapore; National University of Singapore;
   Imperial College London; Hefei University of Technology
RP Liu, LQ (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117583, Singapore.
RI Wang, Meng/ITR-8699-2023; Yan, Shuicheng/HCI-1431-2022
OI Zhang, Hanwang/0000-0001-7374-8739
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Albert A.M., 2004, The aging adult skull and face
   [Anonymous], 2013, PROC 30 INT C MACH L
   [Anonymous], 2013, CORR
   [Anonymous], P PYTH SCI COMP C
   [Anonymous], 2012, IMPROVING NEURAL NET
   [Anonymous], ENCY STAT
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Chen BC, 2015, IEEE T MULTIMEDIA, V17, P804, DOI 10.1109/TMM.2015.2420374
   Chen BC, 2013, IEEE T MULTIMEDIA, V15, P1163, DOI 10.1109/TMM.2013.2242460
   Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389
   Choi JY, 2011, IEEE T MULTIMEDIA, V13, P14, DOI 10.1109/TMM.2010.2087320
   De Berg M., 2008, Computational Geometry: Algorithms and Applications, V17
   Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733
   GOSHTASBY A, 1986, PATTERN RECOGN, V19, P459, DOI 10.1016/0031-3203(86)90044-0
   Guo D, 2009, PROC CVPR IEEE, P73, DOI 10.1109/CVPRW.2009.5206833
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hsieh CK, 2009, IEEE T MULTIMEDIA, V11, P600, DOI 10.1109/TMM.2009.2017606
   Huang G.B., 2008, PROC WORKSHOP FACES
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li HX, 2013, PROC CVPR IEEE, P3499, DOI 10.1109/CVPR.2013.449
   Li P, 2012, IEEE T PATTERN ANAL, V34, P144, DOI 10.1109/TPAMI.2011.104
   Li ZF, 2011, IEEE T INF FOREN SEC, V6, P1028, DOI 10.1109/TIFS.2011.2156787
   Luo P, 2012, PROC CVPR IEEE, P2480, DOI 10.1109/CVPR.2012.6247963
   Park U, 2010, IEEE T PATTERN ANAL, V32, P947, DOI 10.1109/TPAMI.2010.14
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Sun Y, 2014, ADV NEUR IN, V27
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Suo JL, 2012, IEEE T PATTERN ANAL, V34, P2083, DOI 10.1109/TPAMI.2012.22
   Suo JL, 2010, IEEE T PATTERN ANAL, V32, P385, DOI 10.1109/TPAMI.2009.39
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196
   Wang HT, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P819
   Wu T, 2012, LECT NOTES COMPUT SC, V7577, P58, DOI 10.1007/978-3-642-33783-3_5
   Zhu ZeyuanAllen., 2014, CoRR
NR 38
TC 45
Z9 45
U1 0
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2016
VL 18
IS 1
BP 64
EP 75
DI 10.1109/TMM.2015.2500730
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CZ5JW
UT WOS:000367139700007
DA 2024-07-18
ER

PT J
AU Chen, L
   Shen, JB
   Wang, WG
   Ni, BB
AF Chen, Lin
   Shen, Jianbing
   Wang, Wenguan
   Ni, Bingbing
TI Video Object Segmentation Via Dense Trajectories
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dense trajectories; energy optimization; global motion information;
   point trajectory clustering; video object segmentation
ID EXTRACTION
AB In this paper, we propose a novel approach to segment moving object in video by utilizing improved point trajectories. First, point trajectories are densely sampled from video and tracked through optical flow, which provides information of long-term temporal interactions among objects in the video sequence. Second, a novel affinity measurement method considering both global and local information of point trajectories is proposed to cluster trajectories into groups. Finally, we propose a new graph-based segmentation method which adopts both local and global motion information encoded by the tracked dense point trajectories. The proposed approach achieves good performance on trajectory clustering, and it also obtains accurate video object segmentation results on both the Moseg dataset and our new dataset containing more challenging videos.
C1 [Chen, Lin; Shen, Jianbing; Wang, Wenguan] Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
   [Ni, Bingbing] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
C3 Beijing Institute of Technology; Shanghai Jiao Tong University
RP Chen, L (corresponding author), Beijing Inst Technol, Comp Sci, Beijing 100081, Peoples R China.
EM shenjianbing@bit.edu.cn; nibingbing1983@gmail.com
RI Wang, Wenguan/AAA-5782-2022; Shen, Jianbing/U-8796-2019
OI Wang, Wenguan/0000-0002-0802-9567; Shen, Jianbing/0000-0002-4109-8353
FU National Basic Research Program of China (973 Program) [2013CB328805];
   National Natural Science Foundation of China [61272359]; Fok Ying-Tong
   Education Foundation for Young Teachers; Specialized Fund for Joint
   Building Program of Beijing Municipal Education Commission
FX This work was supported in part by the National Basic Research Program
   of China (973 Program) under Grant 2013CB328805, by the National Natural
   Science Foundation of China under Grant 61272359, by the Fok Ying-Tong
   Education Foundation for Young Teachers, and by the Specialized Fund for
   Joint Building Program of Beijing Municipal Education Commission. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Vasileios Mezaris.
CR Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Brostow G.J., 2006, CVPR, P594, DOI DOI 10.1109/CVPR.2006.320
   Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143
   Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21
   Brutzer S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1937, DOI 10.1109/CVPR.2011.5995508
   Carreira J, 2010, PROC CVPR IEEE, P3241, DOI 10.1109/CVPR.2010.5540063
   Chen X., 2011, P 25 AAAI C ART INT, P313
   Chockalingam P, 2009, IEEE I CONF COMP VIS, P1530, DOI 10.1109/ICCV.2009.5459276
   Endres I, 2010, LECT NOTES COMPUT SC, V6315, P575, DOI 10.1007/978-3-642-15555-0_42
   Fragkiadaki K, 2012, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2012.6247883
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Jiang HQ, 2015, IEEE T MULTIMEDIA, V17, P3, DOI 10.1109/TMM.2014.2368273
   Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200
   Lee YJ, 2011, IEEE I CONF COMP VIS, P1995, DOI 10.1109/ICCV.2011.6126471
   Lezama J., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3369, DOI 10.1109/CVPR.2011.6044588
   Ma TY, 2012, PROC CVPR IEEE, P670, DOI 10.1109/CVPR.2012.6247735
   Ochs P, 2014, IEEE T PATTERN ANAL, V36, P1187, DOI 10.1109/TPAMI.2013.242
   Ochs P, 2011, IEEE I CONF COMP VIS, P1583, DOI 10.1109/ICCV.2011.6126418
   Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223
   Prest A, 2012, PROC CVPR IEEE, P3282, DOI 10.1109/CVPR.2012.6248065
   Price BL, 2009, IEEE I CONF COMP VIS, P779, DOI 10.1109/ICCV.2009.5459293
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Tsai D, 2012, INT J COMPUT VISION, V100, P190, DOI 10.1007/s11263-011-0512-5
   Wang W, 2015, PROC CVPR IEEE, P3395, DOI 10.1109/CVPR.2015.7298816
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Wang Wenguan, 2015, IEEE Trans Image Process, V24, P3137, DOI 10.1109/TIP.2015.2438550
   Wertheimer M., 1938, SOURCE BOOK GESTALT, DOI [10.1037/11496-0053, DOI 10.1037/11496-0053, DOI 10.1037/11496-005]
   Yuen J, 2009, IEEE I CONF COMP VIS, P1451, DOI 10.1109/ICCV.2009.5459289
   Zhang D, 2013, PROC CVPR IEEE, P628, DOI 10.1109/CVPR.2013.87
NR 31
TC 51
Z9 52
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2015
VL 17
IS 12
BP 2225
EP 2234
DI 10.1109/TMM.2015.2481711
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CW9JR
UT WOS:000365315500010
DA 2024-07-18
ER

PT J
AU Ji, W
   Frossard, P
   Chen, BW
   Chen, YQ
AF Ji, Wen
   Frossard, Pascal
   Chen, Bo-Wei
   Chen, Yiqiang
TI Profit Optimization for Wireless Video Broadcasting Systems Based on
   Polymatroidal Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Heterogeneous; optimization; polymatroid; pricing; video broadcasting
ID RESOURCE-ALLOCATION; ADAPTATION; NETWORKS
AB This study addresses the problem of profit maximization between wireless service providers (WSPs) and content providers (CPs) in wireless broadcasting systems, while simultaneously providing high quality of experience for end-users (EUs). We first study the profit model in wireless broadcasting networks with a particular attention to the heterogeneous requirements of EUs, e.g., different display sizes and variable channel conditions. Then, we propose a profit formulation that describes the requirements of wireless service providers and content providers, as well as the satisfaction of EUs that essentially depends on video quality and service charges. We propose a new polymatroidal theoretic framework for maximizing the resulting three-side achievable profit through proper bandwidth allocation. Our framework exploits two particular structures, namely the underlying polymatroidal structure of the profit region and the contra-polymatroidal structure of the rate region. We then propose a profit maximization solution by finding a rate allocation vector on the sum-rate facet that satisfies the maximal achievable profit among the WSP, CPs, and EUs. Experiments on different broadcasting scenarios demonstrate the effectiveness of the proposed method. The WSP is capable of generating more revenues by applying the proposed approach to their marketing strategies while satisfying the demands from CPs and EUs.
C1 [Ji, Wen; Chen, Yiqiang] Chinese Acad Sci, ICT, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100190, Peoples R China.
   [Frossard, Pascal] Ecole Polytech Fed Lausanne, Signal Proc Lab, CH-1015 Lausanne, Switzerland.
   [Chen, Bo-Wei] Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA.
C3 Chinese Academy of Sciences; Swiss Federal Institutes of Technology
   Domain; Ecole Polytechnique Federale de Lausanne; Princeton University
RP Ji, W (corresponding author), Chinese Acad Sci, ICT, Beijing 100190, Peoples R China.
EM jiwen@ict.ac.cn; pascal.frossard@epfl.ch; dennisbwc@gmail.com;
   yqchen@ict.ac.cn
RI Frossard, Pascal/AAF-2268-2019; Chen, Bowei/AAB-7002-2021
OI Chen, Bowei/0000-0002-4045-3253
FU National Natural Science Foundation of China [61572466]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61572466. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Tommaso
   Melodia.
CR [Anonymous], 2012, MOV US BAS PRIC CONN
   [Anonymous], GLOB MOB DAT TRAFF F
   [Anonymous], 1970, COMBINATORIAL STRUCT
   Gizelis CA, 2011, IEEE COMMUN SURV TUT, V13, P126, DOI 10.1109/SURV.2011.060710.00028
   Hande P., 2010, P IEEE INFOCOM, P938
   Hande P, 2009, IEEE INFOCOM SER, P990, DOI 10.1109/INFCOM.2009.5062010
   Hua S, 2011, IEEE T MULTIMEDIA, V13, P402, DOI 10.1109/TMM.2010.2103929
   Huang JW, 2008, IEEE T CIRC SYST VID, V18, P582, DOI 10.1109/TCSVT.2008.919109
   Ji W, 2015, IEEE T MOBILE COMPUT, V14, P1659, DOI 10.1109/TMC.2014.2362919
   Ji W, 2012, IEEE T MULTIMEDIA, V14, P443, DOI 10.1109/TMM.2011.2177645
   Jia JC, 2009, MOBIHOC'09 PROCEEDINGS OF THE TENTH ACM INTERNATIONAL SYMPOSIUM ON MOBILE AD HOC NETWORKING AND COMPUTING, P3
   Krause A, 2008, J MACH LEARN RES, V9, P2761
   Lee JW, 2005, IEEE ACM T NETWORK, V13, P827, DOI 10.1109/TNET.2005.852876
   Li SYR, 2012, PROCEEDINGS OF THE 3RD IEEE INTERNATIONAL CONFERENCE ON NETWORK INFRASTRUCTURE AND DIGITAL CONTENT (IEEE IC-NIDC 2012), P1, DOI 10.1109/ICNIDC.2012.6418845
   Li SQ, 2014, IEEE ACM T NETWORK, V22, P703, DOI 10.1109/TNET.2013.2258173
   Liu JC, 2005, IEEE T CIRC SYST VID, V15, P402, DOI 10.1109/TCSVT.2004.825532
   Liu PJ, 2004, IEEE T WIREL COMMUN, V3, P533, DOI 10.1109/TWC.2004.825363
   Maddah-Ali MA, 2009, IEEE T INFORM THEORY, V55, P2128, DOI 10.1109/TIT.2009.2016058
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Ou YF, 2014, IEEE T IMAGE PROCESS, V23, P2473, DOI 10.1109/TIP.2014.2303636
   Pandey V, 2007, IEEE COMMUN SURV TUT, V9, P88, DOI 10.1109/COMST.2007.382409
   Parack S., 2012, 2012 IEEE International Conference on Technology Enhanced Education (Ictee 2012), P1
   Parakh S., 2012, P INT C SIGN PROC CO, P1
   Schierl T, 2007, IEEE T CIRC SYST VID, V17, P1204, DOI 10.1109/TCSVT.2007.905528
   Sen S, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543582
   Sen S, 2012, IEEE COMMUN MAG, V50, P91, DOI 10.1109/MCOM.2012.6353688
   Shokrollahi A, 2006, IEEE T INFORM THEORY, V52, P2551, DOI 10.1109/TIT.2006.874390
   Singhal C, 2014, IEEE T MOBILE COMPUT, V13, P1522, DOI 10.1109/TMC.2013.138
   Talebi M. S., 2011, CORR
   Tse DNC, 1998, IEEE T INFORM THEORY, V44, P2796, DOI 10.1109/18.737513
   Tsiropoulou EE, 2012, IEEE T PARALL DISTR, V23, P61, DOI 10.1109/TPDS.2011.98
   Wang WH, 2006, IEEE ACM T NETWORK, V14, P1282, DOI 10.1109/TNET.2006.886318
   Wang YZ, 2009, BIOFABRICATION, V1, DOI 10.1088/1758-5082/1/1/015001
   Yang L, 2011, IEEE INFOCOM SER, P2228, DOI 10.1109/INFCOM.2011.5935038
   Zhou L, 2013, IEEE T WIREL COMMUN, V12, P3733, DOI 10.1109/TWC.2013.051413.120597
NR 35
TC 21
Z9 24
U1 0
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2015
VL 17
IS 12
BP 2310
EP 2327
DI 10.1109/TMM.2015.2479860
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CW9JR
UT WOS:000365315500017
DA 2024-07-18
ER

PT J
AU Minotto, VP
   Jung, CR
   Lee, B
AF Minotto, Vicente Peruffo
   Jung, Claudio Rosito
   Lee, Bowon
TI Multimodal Multi-Channel On-Line Speaker Diarization Using Sensor Fusion
   Through SVM
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Beamforming; multimodal fusion; on-line speaker diarization; sound
   source localization; speaker labeling; SRP-PHAT; voice activity
   detection
ID VOICE ACTIVITY DETECTION; RECOGNITION; INFORMATION; LOCALIZATION;
   MEETINGS
AB Speaker diarization (SD) is the process of assigning speech segments of an audio stream to its corresponding speakers, thus comprising the problem of voice activity detection (VAD), speaker labeling/identification, and often sound source localization (SSL). Most research activities in the past aimed towards applications as broadcast news, meetings, conversational telephony, and automatic multimodal data annotation, where SD may be performed off-line. However, a recent research focus is human-computer interaction (HCI) systems where SD must be performed on-line, and in real-time, as in modern gaming devices and interaction with large displays. Often, such applications further suffer from noise, reverberations, and overlapping speech, making them increasingly challenging. In such situations, multimodal/multisensory approaches can provide more accurate results than unimodal ones, given a data stream may compensate for occasional instabilities of other modalities. Accordingly, this paper presents an on-line multimodal SD algorithm designed to work in a realistic environment with multiple, overlapping speakers. Our work employs a microphone array, a color camera, and a depth sensor as input streams, from which speech-related features are extracted to be later merged through a support vector machine approach consisting of VAD and SSL modules. Speaker identification is incorporated through a hybrid technique of face positioning history and face recognition. Our final SD approach experimentally achieves an average diarization error rate of 11.48% in scenarios with up to three simultaneous speakers, and is able to run 3.2 x real-time.
C1 [Minotto, Vicente Peruffo; Jung, Claudio Rosito; Lee, Bowon] Univ Fed Rio Grande do Sul, Inst Informat, BR-91501970 Porto Alegre, RS, Brazil.
   [Lee, Bowon] Inha Univ, Dept Elect Engn, Inchon 402751, South Korea.
C3 Universidade Federal do Rio Grande do Sul; Inha University
RP Minotto, VP (corresponding author), Univ Fed Rio Grande do Sul, Inst Informat, BR-91501970 Porto Alegre, RS, Brazil.
EM vpminotto@inf.ufrgs.br; crjung@inf.ufrgs.br; bowon.lee@inha.ac.kr
RI Lee, Bowon/GMX-1775-2022
FU CNPq; National Research Foundation of Korea (NRF) - Ministry of
   Education [2010-0020163]; Inha University [INHA-51376]
FX The work of C. Jung was supported in part by the CNPq. The work of B.
   Lee was supported by the Basic Science Research Program, National
   Research Foundation of Korea (NRF), funded by the Ministry of Education
   under Grant 2010-0020163, and by Inha University under Research Grant
   INHA-51376. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Gokhan Tur.
CR Ahonen T, 2004, LECT NOTES COMPUT SC, V3021, P469
   Miro XA, 2012, IEEE T AUDIO SPEECH, V20, P356, DOI 10.1109/TASL.2011.2125954
   Anguera X, 2005, LECT NOTES COMPUT SC, V3869, P402
   Anguera X, 2011, INT CONF ACOUST SPEE, P4428
   [Anonymous], P 15 EUR SIGN PROC C
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], 2006, P IEEE INT C AC SPEE
   [Anonymous], LITHISYR2326 LINKP U
   [Anonymous], P INTERSPEECH
   [Anonymous], 2008, SIGN PROC C 2008 16
   [Anonymous], P 2 INT C COMP ENG T
   [Anonymous], P 21 IR C EL ENG
   [Anonymous], 2005, PROC MACH LEARN MACH
   [Anonymous], 2013, INTERSPEECH
   [Anonymous], 1978, DIGITAL PROCESSING S
   [Anonymous], P 16 INT C DIG SIGN
   [Anonymous], P INTERSPEECH
   [Anonymous], P IEEE WORKSH APPL S
   [Anonymous], 2011, 2011 8 INT C INF COM, DOI DOI 10.1109/ICICS.2011.6174265
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   [Anonymous], 2012, P C INT SPEECH COMM
   Asoh H., 2004, Seventh International Conference on Information Fusion, P805
   Barras C, 2006, IEEE T AUDIO SPEECH, V14, P1505, DOI 10.1109/TASL.2006.878261
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Brandstein M, 2001, DIGITAL SIGNAL PROC, P133
   Brutti A, 2008, INT CONF ACOUST SPEE, P4349, DOI 10.1109/ICASSP.2008.4518618
   Burger S., 2002, Proc. INTERSPEECH, P1
   Butko T, 2008, INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2008, VOLS 1-5, P123
   Carletta J, 2005, LECT NOTES COMPUT SC, V3869, P28
   Do HA, 2008, INT CONF ACOUST SPEE, P301
   Do H, 2010, INT CONF ACOUST SPEE, P125, DOI 10.1109/ICASSP.2010.5496133
   Fan RE, 2005, J MACH LEARN RES, V6, P1889
   Firoozabadi AD, 2012, 2012 SIXTH INTERNATIONAL SYMPOSIUM ON TELECOMMUNICATIONS (IST), P907, DOI 10.1109/ISTEL.2012.6483115
   Friedland G, 2009, INT CONF ACOUST SPEE, P4069, DOI 10.1109/ICASSP.2009.4960522
   Garofolo John., 2004, P LREC, P1411
   Herbig T, 2012, COMPUT SPEECH LANG, V26, P210, DOI 10.1016/j.csl.2011.11.002
   Huang Y, 2007, 2007 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING, VOLS 1 AND 2, P693, DOI 10.1109/ASRU.2007.4430196
   Hung Hayley., 2008, P OF THE 10 INT C ON, P233, DOI DOI 10.1145/1452392.1452441
   Jaimes A, 2007, COMPUT VIS IMAGE UND, V108, P116, DOI 10.1016/j.cviu.2006.10.019
   Janin A, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P364
   Jhanwar N, 2004, EURASIP J APPL SIG P, V2004, P2640, DOI 10.1155/S1110865704408026
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Kwon S, 2005, IEEE T SPEECH AUDI P, V13, P1004, DOI 10.1109/TSA.2005.851981
   Kwon S, 2003, ASRU'03: 2003 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING ASRU '03, P423, DOI 10.1109/ASRU.2003.1318478
   Li M, 2013, COMPUT SPEECH LANG, V27, P151, DOI 10.1016/j.csl.2012.01.008
   Markov K, 2007, 2007 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING, VOLS 1 AND 2, P699, DOI 10.1109/ASRU.2007.4430197
   Markov K, 2008, INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2008, VOLS 1-5, P363
   Minotto Vicente P., 2014, IEEE Transactions on Multimedia, V16, P1032, DOI 10.1109/TMM.2014.2305632
   Minotto VP, 2013, IEEE J-STSP, V7, P147, DOI 10.1109/JSTSP.2012.2237379
   Minotto VP, 2013, INT J HIGH PERFORM C, V27, P291, DOI 10.1177/1094342012452166
   Moattar MH, 2012, SPEECH COMMUN, V54, P1065, DOI 10.1016/j.specom.2012.05.002
   Noulas A, 2012, IEEE T PATTERN ANAL, V34, P79, DOI 10.1109/TPAMI.2011.47
   Noulas AK, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P350
   Quinlan J. R., 1993, PROGRAMS MACHINE LEA
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   Schmalenstroeer J, 2010, IEEE J-STSP, V4, P845, DOI 10.1109/JSTSP.2010.2050519
   Scott D, 2009, 2009 11TH IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM 2009), P80, DOI 10.1109/ISM.2009.41
   Shotton J, 2013, Commun. ACM, V56, P119, DOI [DOI 10.1145/2398356.2398381, 10.1145/2398356.2398381]
   Sohn J, 1999, IEEE SIGNAL PROC LET, V6, P1, DOI 10.1109/97.736233
   Taghizadeh M. J., 2011, 2011 Joint Workshop on Hands-Free Speech Communication and Microphone Arrays (HSCMA 2011), P92, DOI 10.1109/HSCMA.2011.5942417
   Tanyer SG, 2000, IEEE T SPEECH AUDI P, V8, P478, DOI 10.1109/89.848229
   Tiawongsombat P, 2012, PATTERN RECOGN, V45, P783, DOI 10.1016/j.patcog.2011.07.011
   Turk M. A., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P586, DOI 10.1109/CVPR.1991.139758
   Vallet F, 2013, IEEE T MULTIMEDIA, V15, P509, DOI 10.1109/TMM.2012.2233724
   Vijayasenan D, 2009, IEEE T AUDIO SPEECH, V17, P1382, DOI 10.1109/TASL.2009.2015698
   Weiping Cai, 2010, Proceedings of the 2010 International Conference on Electrical and Control Engineering (ICECE 2010), P1246, DOI 10.1109/iCECE.2010.310
   Wooters C, 2008, LECT NOTES COMPUT SC, V4625, P509
   Yücesoy E, 2013, SIG PROCESS COMMUN
   Zelenák M, 2012, IEEE T AUDIO SPEECH, V20, P436, DOI 10.1109/TASL.2011.2160167
   Zhang C, 2007, INT CONF ACOUST SPEE, P125
NR 70
TC 31
Z9 33
U1 1
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2015
VL 17
IS 10
BP 1694
EP 1705
DI 10.1109/TMM.2015.2463722
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CR9OD
UT WOS:000361685400002
DA 2024-07-18
ER

PT J
AU Xu, RH
   Herranz, L
   Jiang, SQ
   Wang, S
   Song, XH
   Jain, R
AF Xu, Ruihan
   Herranz, Luis
   Jiang, Shuqiang
   Wang, Shuang
   Song, Xinhang
   Jain, Ramesh
TI Geolocalized Modeling for Dish Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Food recognition; geolocation; image tagging; mobile applications
AB Food-related photos have become increasingly popular, due to social networks, food recommendations, and dietary assessment systems. Reliable annotation is essential in those systems, but unconstrained automatic food recognition is still not accurate enough. Most works focus on exploiting only the visual content while ignoring the context. To address this limitation, in this paper we explore leveraging geolocation and external information about restaurants to simplify the classification problem. We propose a framework incorporating discriminative classification in geolocalized settings and introduce the concept of geolocalized models, which, in our scenario, are trained locally at each restaurant location. In particular, we propose two strategies to implement this framework: geolocalized voting and combinations of bundled classifiers. Both models show promising performance, and the later is particularly efficient and scalable. We collected a restaurant-oriented food dataset with food images, dish tags, and restaurant-level information, such as the menu and geolocation. Experiments on this dataset show that exploiting geolocation improves around 30% the recognition performance, and geolocalized models contribute with an additional 3-8% absolute gain, while they can be trained up to five times faster.
C1 [Xu, Ruihan; Herranz, Luis; Jiang, Shuqiang; Wang, Shuang; Song, Xinhang] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Jain, Ramesh] Univ Calif Irvine, Irvine, CA 92697 USA.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   University of California System; University of California Irvine
RP Jiang, SQ (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM ruihan.xu@vipl.ict.ac.cn; luis.herranz@vipl.ict.ac.cn;
   shuqiang.jiang@vipl.ict.ac.cn; shuang.wang@vipl.ict.ac.cn;
   xinhang.song@vipl.ict.ac.cn; jain@ics.uci.edu
RI xu, ruihan/JSK-6518-2023; Herranz, Luis/B-4573-2016
OI Herranz, Luis/0000-0002-7022-3395
FU National Basic Research Program of China (973 Program) [2012CB316400];
   National Natural Science Foundation of China [61322212, 61450110446];
   National Hi-Tech Development Program (863 Program) of China
   [2014AA015202]; CAS President's International Fellowship Initiative
   [2011Y1GB05]; Lenovo Outstanding Young Scientists Program
FX This work was supported in part by the National Basic Research Program
   of China (973 Program) under Grant 2012CB316400, in part by the National
   Natural Science Foundation of China under Grant 61322212 and Grant
   61450110446, in part by the National Hi-Tech Development Program (863
   Program) of China under Grant 2014AA015202, in part by the CAS
   President's International Fellowship Initiative under 2011Y1GB05, and by
   the Lenovo Outstanding Young Scientists Program. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Ebroul Izquierdo.
CR [Anonymous], 2014, P 2014 ACM INT JOINT
   [Anonymous], NEURAL INFORM PROCES
   [Anonymous], 2014, P 31 INT C INT C MAC
   Chen DM, 2011, PROC CVPR IEEE, P737, DOI 10.1109/CVPR.2011.5995610
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Fanyu Kong, 2011, 2011 8th International Conference on Body Sensor Networks (BSN), P127, DOI 10.1109/BSN.2011.19
   Girod B, 2011, IEEE MULTIMEDIA, V18, P86, DOI 10.1109/MMUL.2011.48
   Hoashi H., 2010, Proceedings 2010 IEEE International Symposium on Multimedia (ISM 2010), P296, DOI 10.1109/ISM.2010.51
   Holmes NP, 2005, CURR BIOL, V15, pR762, DOI 10.1016/j.cub.2005.08.058
   Hsu CW, 2002, IEEE T NEURAL NETWOR, V13, P415, DOI 10.1109/72.991427
   Ji RR, 2015, ACM T INTEL SYST TEC, V6, DOI 10.1145/2597181
   Joutou T, 2009, IEEE IMAGE PROC, P285, DOI 10.1109/ICIP.2009.5413400
   Kawano Yoshiyuki, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8326, P369, DOI 10.1007/978-3-319-04117-9_38
   Kawano Y, 2015, LECT NOTES COMPUT SC, V8927, P3, DOI 10.1007/978-3-319-16199-0_1
   Kawano Y, 2013, IEEE COMPUT SOC CONF, P1, DOI 10.1109/CVPRW.2013.5
   Li J, 2013, IEEE T MULTIMEDIA, V15, P2058, DOI 10.1109/TMM.2013.2280127
   Li Z, 2012, IEEE SIGNAL PROC LET, V19, P459, DOI 10.1109/LSP.2012.2203120
   Luo JB, 2011, MULTIMED TOOLS APPL, V51, P187, DOI 10.1007/s11042-010-0623-y
   Maruyama Yuto, 2010, 2010 Proceedings of 16th International Conference on Virtual Systems and Multimedia (VSMM 2010), P75, DOI 10.1109/VSMM.2010.5665964
   Matsuda Y., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P25, DOI 10.1109/ICME.2012.157
   Matsuda Y, 2012, INT C PATT RECOG, P2017
   Nister David, 2006, CVPR
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Platt JC, 2000, ADV NEUR IN, P61
   Qian XM, 2014, IEEE T KNOWL DATA EN, V26, P1763, DOI 10.1109/TKDE.2013.168
   Qian XM, 2013, NEUROCOMPUTING, V111, P144, DOI 10.1016/j.neucom.2012.12.021
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Yaegashi Keita, 2010, Computer Vision - ACCV 2010. 10th Asian Conference on Computer Vision. Revised Selected Papers, P360, DOI 10.1007/978-3-642-19309-5_28
   Yaegashi K, 2009, LECT NOTES COMPUT SC, V5414, P361, DOI 10.1007/978-3-540-92957-4_32
   Yang S, 2010, PROC CVPR IEEE, P2249, DOI 10.1109/CVPR.2010.5539907
   Yap KH, 2010, IEEE INTELL SYST, V25, P48, DOI 10.1109/MIS.2010.12
   Zhang M. M., 2011, 190 CSE U CAL
   Zheng YT, 2009, PROC CVPR IEEE, P1085, DOI 10.1109/CVPRW.2009.5206749
   Zhimin Zong, 2010, Proceedings 2010 IEEE International Symposium on Multimedia (ISM 2010), P204, DOI 10.1109/ISM.2010.37
NR 35
TC 58
Z9 65
U1 1
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1187
EP 1199
DI 10.1109/TMM.2015.2438717
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000006
DA 2024-07-18
ER

PT J
AU Zhu, WJ
   Ding, WP
   Xu, JZ
   Shi, YH
   Yin, BC
AF Zhu, Weijia
   Ding, Wenpeng
   Xu, Jizheng
   Shi, Yunhui
   Yin, Baocai
TI Hash-Based Block Matching for Screen Content Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hash-based block matching; high efficiency video coding (HEVC)
   framework; large motions in screen videos; long-distance repeated
   patterns; screen content coding
AB By considering the increasing importance of screen contents, the high efficiency video coding (HEVC) standard includes screen content coding as one of its requirements. In this paper, we demonstrate that enabling frame level block searching in HEVC can significantly improve coding efficiency on screen contents. We propose a hash-based block matching scheme for the intra block copy mode and the motion estimation process, which enables frame level block searching in HEVC without changing the HEVC syntaxes. In the proposed scheme, the blocks sharing the same hash values with the current block are selected as prediction candidates. Then the hash-based block selection is employed to select the best candidates. To achieve the best coding efficiency, the rate distortion optimization is further employed to improve the proposed scheme by balancing the coding cost of motion vectors and prediction difference. Compared with HEVC, the proposed scheme achieves 21% and 37% bitrate saving with all intra and low delay configurations with encoding time reduction. Up to 59% bitrate saving can be achieved on sequences with large motions.
C1 [Zhu, Weijia; Xu, Jizheng] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Ding, Wenpeng; Shi, Yunhui; Yin, Baocai] Beijing Univ Technol, Coll Metropolitan Transportat, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
C3 Microsoft Research Asia; Microsoft; Beijing University of Technology
RP Ding, WP (corresponding author), Beijing Univ Technol, Coll Metropolitan Transportat, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
EM sparkjj1985@gmail.com; wpding@bjut.edu.cn; jzxu@microsoft.com;
   syhzm@bjut.edu.cn; ybc@bjut.edu.cn
RI Zhu, Weijia/A-1598-2019; Xu, Jizheng/JDD-5152-2023; Zhu,
   Weijia/C-3087-2017
OI Zhu, Weijia/0000-0001-9202-1260
FU NSFC [61390510, 61370118, 61170103, 61472018]; PHR (IHLB)
FX This work was supported by the NSFC under Grant 61390510, Grant
   61370118, Grant 61170103, and Grant 61472018, and by the PHR (IHLB). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Jing-Ming Guo. (Corresponding
   author: Wenpeng Ding.)
CR [Anonymous], 2013, COMM COND SCREEN CON
   [Anonymous], 2013, COMM TEST COND SOFTW
   [Anonymous], 2011, JCTVCE145 ITUTISOIEC
   [Anonymous], 2012, INT RANSF SKIPP
   [Anonymous], 2014, AD MOT VECT RES SCRE
   [Anonymous], 2013, AHG8 VID COD US INTR
   [Anonymous], 2012, INTR TRANSF SKIPP
   [Anonymous], 2010, MVC SOFTW REF MAN JM
   [Anonymous], 2013, JCTVC M0323
   [Anonymous], 2001, CALC AV PSNR DIFF RD
   [Anonymous], 2015, JCTVC T0064
   DING W, 2007, P IEEE INT C IM PROC, V2, P337
   Houghton A., 1996, The Engineer's Error Coding Handbook, P12, DOI [10.1007/978-1-4613-0447-0_3, DOI 10.1007/978-1-4613-0447-0_3]
   Kuo CM, 2009, IEEE T CIRC SYST VID, V19, P893, DOI 10.1109/TCSVT.2009.2017420
   Lin T, 2013, IEEE T CIRC SYST VID, V23, P173, DOI 10.1109/TCSVT.2012.2223871
   Monga V, 2006, IEEE T IMAGE PROCESS, V15, P3452, DOI 10.1109/TIP.2006.881948
   Pan ZT, 2013, IEEE T CIRC SYST VID, V23, P949, DOI 10.1109/TCSVT.2013.2243056
   Purnachand N., 2012, 2012 IEEE Second International Conference on Consumer Electronics - Berlin (ICCE-Berlin), P34, DOI 10.1109/ICCE-Berlin.2012.6336494
   Purnachand N., 2012, 2012 International Conference on Systems, Signals and Image Processing (IWSSIP), P388
   Sullivan G., 2012, M 9 M JOINT COLL TEA
   Sullivan G., 2013, M 14 M JOINT COLL TE
   Sullivan G., 2012, M 10 M JOINT COLL TE
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Zhu WJ, 2014, IEEE T MULTIMEDIA, V16, P1316, DOI 10.1109/TMM.2014.2315782
   Zhu WJ, 2014, IEEE DATA COMPR CONF, P43, DOI 10.1109/DCC.2014.11
   Zhu WJ, 2013, PICT COD SYMP, P373, DOI 10.1109/PCS.2013.6737761
NR 27
TC 44
Z9 53
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2015
VL 17
IS 7
BP 935
EP 944
DI 10.1109/TMM.2015.2428171
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CK8XC
UT WOS:000356522300002
DA 2024-07-18
ER

PT J
AU Jiang, SH
   Qian, XM
   Shen, JL
   Fu, Y
   Mei, T
AF Jiang, Shuhui
   Qian, Xueming
   Shen, Jialie
   Fu, Yun
   Mei, Tao
TI Author Topic Model-Based Collaborative Filtering for Personalized POI
   Recommendations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data mining; recommendation system; text mining; travel recommendation
ID USER INTEREST; PHOTOS
AB From social media has emerged continuous needs for automatic travel recommendations. Collaborative filtering (CF) is the most well-known approach. However, existing approaches generally suffer from various weaknesses. For example, sparsity can significantly degrade the performance of traditional CF. If a user only visits very few locations, accurate similar user identification becomes very challenging due to lack of sufficient information for effective inference. Moreover, existing recommendation approaches often ignore rich user information like textual descriptions of photos which can reflect users' travel preferences. The topic model (TM) method is an effective way to solve the "sparsity problem," but is still far from satisfactory. In this paper, an author topic model-based collaborative filtering (ATCF) method is proposed to facilitate comprehensive points of interest (POIs) recommendations for social users. In our approach, user preference topics, such as cultural, cityscape, or landmark, are extracted from the geo-tag constrained textual description of photos via the author topic model instead of only from the geo-tags (GPS locations). Advantages and superior performance of our approach are demonstrated by extensive experiments on a large collection of data.
C1 [Jiang, Shuhui; Qian, Xueming] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
   [Shen, Jialie] Singapore Management Univ, Sch Informat Syst, Singapore 188065, Singapore.
   [Fu, Yun] Northeastern Univ, Coll Engn, Dept Elect & Comp Engn, Boston, MA 02115 USA.
   [Fu, Yun] Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA.
   [Mei, Tao] Microsoft Res Asia, Beijing 100190, Peoples R China.
C3 Xi'an Jiaotong University; Singapore Management University; Northeastern
   University; Northeastern University; Microsoft; Microsoft Research Asia
RP Qian, XM (corresponding author), Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
EM shjiang@coe.neu.edu; qianxm@mail.xjtu.edu.cn; jlshen@smu.edu.sg;
   yunfu@ece.neu.edu; tmei@microsoft.com
RI SHEN, Jialie/E-8573-2012; Jiang, Shuhui/W-6907-2019; Shen,
   Jialie/AAX-6851-2020; Mei, Tao/GQZ-0596-2022
OI Mei, Tao/0000-0002-5990-7307
FU Program 973 [2012CB31640]; NSFC [60903121, 61173109, 61332018];
   Microsoft Research Asia; Ministry of Education Academic Research Fund
   Tier 2 of Singapore [MOE2013-T2-2-156]
FX This work was supported by Program 973 under Grant 2012CB31640, by the
   NSFC under Grant 60903121, Grant 61173109, and Grant 61332018, by
   Microsoft Research Asia, and by the Ministry of Education Academic
   Research Fund Tier 2 of Singapore under Grant MOE2013-T2-2-156. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Vasileios Mezaris. (Corresponding
   author: Xueming Qian.)
CR [Anonymous], 2014, P 8 INT S MED INF CO
   Bao J., 2012, P 20 INT C ADV GEOGR, P199, DOI [DOI 10.1145/2424321.2424348, 10.1145/2424321.2424348]
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Chen TY, 2013, IEEE INT CONF COMMUN, P11, DOI 10.1109/ICCChinaW.2013.6670558
   Cheng A.-J., 2011, P 19 ACM INT C MULTI, P83
   Clements M, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P851
   Feng H, 2014, NEUROCOMPUTING, V129, P409, DOI 10.1016/j.neucom.2013.09.018
   Gao H., 2015, P 29 INT C AAAI AUST, V28, P1
   Gao Y., 2010, Proceedings of Descriptional Complexity of Formal Systems 12th Workshop (DCFS 2010), P123
   Huang HS, 2014, INT J DATA MIN MODEL, V6, P333, DOI 10.1504/IJDMMM.2014.066762
   Kori H., 2006, Advances in Multimedia Modeling. 13th International Multimedia Modeling Conference, MMM 2007. Proceedings (Lecture Notes in Computer Science Vol.4351), P690
   Krestel R., 2009, Proceedings of the 3rd ACM Conference on Recommender Systems, P61, DOI [DOI 10.1145/1639714.1639726, 10.1145/1639714.1639726]
   Kurashima T., 2010, CIKM, P579, DOI DOI 10.1145/1871437.1871513
   Kurashima T, 2006, LECT NOTES COMPUT SC, V4080, P213
   Li J, 2013, IEEE T MULTIMEDIA, V15, P2058, DOI 10.1109/TMM.2013.2280127
   Liu Q, 2012, IEEE T SYST MAN CY B, V42, P218, DOI 10.1109/TSMCB.2011.2163711
   Liu XZ, 2014, ADV MATER INTERFACES, V1, DOI 10.1002/admi.201300053
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Luo W, 2012, IEEE GEOSCI REMOTE S, V9, P634, DOI 10.1109/LGRS.2011.2177064
   Pennacchiotti Marco, 2011, P 20 INT C COMPANION, P101
   Qian XM, 2014, IEEE T CYBERNETICS, V44, P2493, DOI 10.1109/TCYB.2014.2309593
   Qian XM, 2014, IEEE T KNOWL DATA EN, V26, P1763, DOI 10.1109/TKDE.2013.168
   Qian XM, 2013, NEUROCOMPUTING, V111, P144, DOI 10.1016/j.neucom.2012.12.021
   Ramage D., 2010, P INT AAAI C WEB SOC, V4, P130
   Rosen-Zvi M, 2010, ACM T INFORM SYST, V28, DOI 10.1145/1658377.1658381
   Rosen-Zvi Michal., 2004, UAI
   Sang Jitao., 2012, SIGSPATIALGIS, P402
   Sarwar B., 2001, P 10 INT C WORLD WID, P285, DOI 10.1145/371920.372071
   Shi Y, 2011, Proceedings of the 5th AAAI Conference on Weblogs and Social Media, V5, P622
   Shuhui Jiang, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015. Proceedings: LNCS 8936, P392, DOI 10.1007/978-3-319-14442-9_45
   Steyvers M., 2004, P 2004 ACM SIGKDD IN, DOI [DOI 10.1145/1014052.1014087, 10.1145/1014052, DOI 10.1145/1014052]
   Wei L., 2012, P 18 ACM SIGKDD INT, P195, DOI [10.1145/2339530.2339562, DOI 10.1145/2339530.2339562]
   WILBUR WJ, 1992, J INF SCI, V18, P45, DOI 10.1177/016555159201800106
   Xing Wei, 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P178
   Xue Y., 2013, PROC IEEE INT TEST C, P1
   Yin HG, 2012, IEEE INT CONF MULTI, P540, DOI 10.1109/ICMEW.2012.100
   Yuan Q., 2014, P 23 ACM INT C C INF, P659, DOI DOI 10.1145/2661829.2661983
   Zhang C., 2015, KNOWL INF SYST, P1
   Zheng VW, 2010, P 19 INT C WORLD WID, P1029
   Zheng Y., 2009, WWW, P791, DOI [10.1145/1526709.1526816, DOI 10.1145/1526709.1526816]
NR 40
TC 169
Z9 195
U1 1
U2 104
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2015
VL 17
IS 6
BP 907
EP 918
DI 10.1109/TMM.2015.2417506
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CI1TM
UT WOS:000354527500012
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Liu, W
   Mei, T
   Zhang, YD
AF Liu, Wu
   Mei, Tao
   Zhang, Yongdong
TI Instant Mobile Video Search With Layered Audio-Video Indexing and
   Progressive Transmission
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio-video signature; layered audio-video indexing; mobile video
   search; progressive transmission
ID VISUAL-SEARCH
AB The proliferation of mobile devices is producing a new wave of applications that enable users to sense their surroundings with smart phones. People are preferring mobile devices to search and browse video content on the move. In this paper, we have developed an innovative mobile video search system through which users can discover videos by simply pointing their phones at a screen to capture a very few seconds of what they are watching. Different than most existing mobile video search applications, the proposed system is aiming at instant and progressive video search by leveraging the light-weight computing capacity of mobile devices. In particular, the system is able to index large-scale video data using a new layered audio-video indexing approach in the cloud, as well as generate lightweight joint audio-video signatures with progressive transmission and perform progressive search on mobile devices. Furthermore, we showcase that the system can be applied to two novel applications-video entity search and video clip localization. The evaluations on the real-world mobile video query dataset show that our system significantly improves user's search experience due to search accuracy, low retrieval latency, and very short recording duration.
C1 [Liu, Wu; Zhang, Yongdong] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Liu, Wu] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Mei, Tao] Microsoft Res, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Microsoft
RP Liu, W (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM liuwu@live.cn; tmei@mi-crosoft.com; zhyd@ict.ac.cn
RI Liu, Wu/AAG-3615-2019; LIU, Wu-Ming/A-8383-2015; Mei, Tao/GQZ-0596-2022
OI Liu, Wu/0000-0003-1633-7575; Mei, Tao/0000-0002-5990-7307
FU National High Technology Research and Development Program of China
   [2014AA015202]; National Key Technology Research and Development Program
   of China [2012BAH39B02]
FX This work was supported in part by the National High Technology Research
   and Development Program of China under Grant 2014AA015202 and the
   National Key Technology Research and Development Program of China under
   Grant 2012BAH39B02. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Shin'ichi Satoh.
CR Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   [Anonymous], 2008, Proceedings of the 21st International Conference on Neural Information Processing Systems
   [Anonymous], 2012, P 20 ACM INT C MULT, DOI DOI 10.1145/2393347.2393427
   [Anonymous], 2012, P ACM MULT
   [Anonymous], 2012, P 20 ACM INT C MULT
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Chandrasekhar V., 2010, ACM multimedia workshop on Mobile cloud media computing, P41
   Chandrasekhar V, 2012, INT J COMPUT VISION, V96, P384, DOI 10.1007/s11263-011-0453-z
   Chen D, 2010, IEEE IMAGE PROC, P1017, DOI 10.1109/ICIP.2010.5650653
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Girod B, 2011, IEEE SIGNAL PROC MAG, V28, P61, DOI 10.1109/MSP.2011.940881
   He JF, 2012, PROC CVPR IEEE, P3005, DOI 10.1109/CVPR.2012.6248030
   Ji R., 2011, Proceedings of the 19th ACM International Conference on Multimedia, MM'11, P573
   Li HQ, 2013, IEEE T MULTIMEDIA, V15, P594, DOI 10.1109/TMM.2012.2234730
   Liu H., 2012, Proceedings_of_the_20th_ACM_ International_Conference_on_Multimedia, MM'12, P9
   Liu JJ, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2501654.2501658
   Liu Wu, 2013, P ACM INT C MULT, P887
   Liu Y, 2011, INT J COMPUT MATH, V88, P3803, DOI 10.1080/00207160.2011.583351
   Lv Q., 2007, P 33 INT C VER LARG, P950
   Mei T, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2536798
   Miluzzo E, 2008, SENSYS'08: PROCEEDINGS OF THE 6TH ACM CONFERENCE ON EMBEDDED NETWORKED SENSOR SYSTEMS, P337
   Muja M., 2012, 2012 Canadian Conference on Computer and Robot Vision, P404, DOI 10.1109/CRV.2012.60
   Norouzi M.E., 2011, ICML
   Norouzi M, 2012, PROC CVPR IEEE, P3108, DOI 10.1109/CVPR.2012.6248043
   Pang L., 2012, P ACM MULT, P1461
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Sang JT, 2013, IEEE T MULTIMEDIA, V15, P1665, DOI 10.1109/TMM.2013.2268052
   Shen HT, 2009, IEEE T KNOWL DATA EN, V21, P321, DOI 10.1109/TKDE.2008.168
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399, DOI 10.1145/1101149.1101236
   Su Y., 2013, ACM Multimedia Conference, MM'13, Barcelona, Spain, October 21-25, 2013, P73
   Wan J, 2013, IEEE IMAGE PROC, P2670, DOI 10.1109/ICIP.2013.6738550
   Wang A., 2003, P 4 INT SOC MUSIC IN, P7, DOI DOI 10.1109/IITAW.2009.110
   Wu GL, 2013, IEEE MULTIMEDIA, V20, P47, DOI 10.1109/MMUL.2013.13
   Wu Y., 2012, P ACM MULT, P989
   Xie HT, 2011, IEEE T MULTIMEDIA, V13, P1319, DOI 10.1109/TMM.2011.2167224
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P3025, DOI 10.1109/TIP.2014.2326010
   Zhang YD, 2014, IEEE T MULTIMEDIA, V16, P1127, DOI 10.1109/TMM.2014.2306392
NR 38
TC 37
Z9 74
U1 0
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2014
VL 16
IS 8
BP 2242
EP 2255
DI 10.1109/TMM.2014.2359332
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AU4HL
UT WOS:000345571300014
DA 2024-07-18
ER

PT J
AU Shepstone, SE
   Tan, ZH
   Jensen, SH
AF Shepstone, Sven Ewan
   Tan, Zheng-Hua
   Jensen, Soren Holdt
TI Using Audio-Derived Affective Offset to Enhance TV Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Affective offset; circumplex model of affect; critique-based
   recommenders; emotions; EPG; moods
ID CONTENT REPRESENTATION; CIRCUMPLEX MODEL; EMOTION; RECOGNITION
AB This paper introduces the concept of affective offset, which is the difference between a user's perceived affective state and the affective annotation of the content they wish to see. We show how this affective offset can be used within a framework for providing recommendations for TV programs. First a user's mood profile is determined using 12-class audio-based emotion classifications. An initial TV content item is then displayed to the user based on the extracted mood profile. The user has the option to either accept the recommendation, or to critique the item once or several times, by navigating the emotion space to request an alternative match. The final match is then compared to the initial match, in terms of the difference in the items' affective parameterization. This offset is then utilized in future recommendation sessions. The system was evaluated by eliciting three different moods in 22 separate users and examining the influence of applying affective offset to the users' sessions. Results show that, in the case when affective offset was applied, better user satisfaction was achieved: the average ratings went from 7.80 up to 8.65, with an average decrease in the number of critiquing cycles which went from 29.53 down to 14.39.
C1 [Shepstone, Sven Ewan] Bang & Olufsen AS, DK-7600 Struer, Denmark.
   [Tan, Zheng-Hua; Jensen, Soren Holdt] Aalborg Univ, Dept Elect Syst, DK-9220 Aalborg, Denmark.
C3 Aalborg University
RP Shepstone, SE (corresponding author), Bang & Olufsen AS, DK-7600 Struer, Denmark.
EM ssh@bang-olufsen.dk; zt@es.aau.dk; shj@es.aau.dk
RI Jensen, Søren H/A-3139-2011; Tan, Zheng-Hua/B-6889-2015
OI Tan, Zheng-Hua/0000-0001-6856-8928
FU Bang and Olufsen A/S; Danish Ministry of Science, Innovation, and Higher
   Education [12-122802]
FX This work was supported by Bang and Olufsen A/S and by Denmark and the
   Danish Ministry of Science, Innovation, and Higher Education under Grant
   12-122802. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Ali C. Begen.
CR Amolochitis E, 2013, INFORM PROCESS MANAG, V49, P1326, DOI 10.1016/j.ipm.2013.07.002
   [Anonymous], 2013, P INT
   [Anonymous], 1999, A4 U FLOR CTR RES PS
   [Anonymous], P INTERSPEECH
   [Anonymous], 2004, PROC LREC
   Bänziger T, 2012, EMOTION, V12, P1161, DOI 10.1037/a0025827
   Barragáns-Martínez AB, 2010, INFORM SCIENCES, V180, P4290, DOI 10.1016/j.ins.2010.07.024
   Bjelica M, 2011, IEEE T CONSUM ELECTR, V57, P658, DOI 10.1109/TCE.2011.5955205
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   Burkhardt F, 2005, INTERSPEECH, V5, P1517, DOI DOI 10.21437/INTERSPEECH.2005-446
   Chen L, 2012, USER MODEL USER-ADAP, V22, P125, DOI [10.1007/s11257-011-9115-7, 10.1007/s11257-011-9108-6]
   Dehak N, 2011, IEEE T AUDIO SPEECH, V19, P788, DOI 10.1109/TASL.2010.2064307
   Dulewicz Victor., 1994, NATURE EMOTION, P56
   Felfernig A, 2011, RECOMMENDER SYSTEMS HANDBOOK, P187, DOI 10.1007/978-0-387-85820-3_6
   Gosztolya G., 2013, P INT 2013
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   Irtel H., 2008, PXLAB SELF ASS MAN S
   Larcher A., 2013, P INT 2013, P1
   Li M, 2013, COMPUT SPEECH LANG, V27, P151, DOI 10.1016/j.csl.2012.01.008
   Milton A., 2013, COMPUT SPEECH LANG
   Peter C, 2006, INTERACT COMPUT, V18, P139, DOI 10.1016/j.intcom.2005.10.006
   Remington NA, 2000, J PERS SOC PSYCHOL, V79, P286, DOI 10.1037//0022-3514.79.2.286
   Russell JA, 1999, J PERS SOC PSYCHOL, V76, P805, DOI 10.1037/0022-3514.76.5.805
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Schuller B, 2013, COMPUT SPEECH LANG, V27, P1, DOI 10.1016/j.csl.2012.06.002
   Sun K, 2009, IEEE INT CON MULTI, P566, DOI 10.1109/ICME.2009.5202559
   Tkalcic M., 2011, PROC RECSYS 2011 WOR, P9
   Vig Jesse, 2011, Pro- ceedings of the 16th international conference on Intelligent user interfaces, P93
   Winoto P, 2010, EXPERT SYST APPL, V37, P6086, DOI 10.1016/j.eswa.2010.02.117
   Xia R., 2012, P INTERSPEECH, P2230
NR 30
TC 19
Z9 23
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 1999
EP 2010
DI 10.1109/TMM.2014.2337845
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300017
DA 2024-07-18
ER

PT J
AU Yang, M
   Groves, T
   Zheng, NN
   Cosman, P
AF Yang, Meng
   Groves, Theodore
   Zheng, Nanning
   Cosman, Pamela
TI Iterative Pricing-Based Rate Allocation for Video Streams With
   Fluctuating Bandwidth Availability
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cognitive radios; H.264/AVC; pareto optimality; rate allocation; video
   compression
ID COGNITIVE RADIO NETWORKS; RESOURCE-ALLOCATION; BIT ALLOCATION;
   TRANSMISSION; EQUILIBRIUM; MECHANISM; GAME
AB We consider rate allocation for video users in the case where the available bandwidth fluctuates. Simply minimizing the objective distortion or optimizing the stability of video qualities does not optimize subjective quality. We formulate a utility-based solution, considering that a user's preference of video quality often varies over a range with upper and lower thresholds of quality. Our iterative pricing-based resource allocation procedure reallocates the bandwidth not only between different users within a time slot but also between different time slots, such that no user suffers quality degradation on average by participating in the multiplexing process. Experimental results show that, compared with equal resource allocation and existing rate allocation solutions, the subjective result becomes increasingly better with the increase of bandwidth fluctuation rate or bandwidth fluctuation range. Moreover, as the number of users increases, the results improve.
C1 [Yang, Meng; Zheng, Nanning] Xi An Jiao Tong Univ, Dept Automat, Xian 710049, Peoples R China.
   [Groves, Theodore] Univ Calif San Diego, Dept Econ, La Jolla, CA 92093 USA.
   [Cosman, Pamela] Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
C3 Xi'an Jiaotong University; University of California System; University
   of California San Diego; University of California System; University of
   California San Diego
RP Zheng, NN (corresponding author), Xi An Jiao Tong Univ, Dept Automat, Xian 710049, Peoples R China.
EM myang59@gmail.com; tgroves@ucsd.edu; nnzheng@mail.xjtu.edu.cn;
   pcosman@ece.ucsd.edu
OI Cosman, Pamela/0000-0002-4012-0176
FU NSF [CCF-1160832]; Program 973 [2012CB316400]; NSFC [61231018]; Direct
   For Computer & Info Scie & Enginr; Division of Computing and
   Communication Foundations [1160832] Funding Source: National Science
   Foundation
FX This research was supported in part by the NSF under Grant CCF-1160832,
   in part by Program 973 No. 2012CB316400, and in part by the NSFC Key
   Project No. 61231018. The associate editor co-ordinating the review of
   this manuscript and approving it for publication was Prof. Klara
   Nahrstedt. (Corresponding author: Nanning Zheng.)
CR Balakrishnan M, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL I, P377, DOI 10.1109/ICIP.1997.647785
   Chen ZZ, 2006, SIGNAL PROCESS-IMAGE, V21, P273, DOI 10.1016/j.image.2005.11.001
   Fattahi AR, 2007, IEEE J SEL AREA COMM, V25, P601, DOI 10.1109/JSAC.2007.070410
   Fu FW, 2007, IEEE J-STSP, V1, P264, DOI 10.1109/JSTSP.2007.901519
   Malinvaud E., 1967, ACTIVITY ANAL THEORY, P170
   Martello Silvano, 1990, Knapsack Problems: Algorithms and Computer Implementations
   Mas-Colell A., 1995, Microeconomics Theory
   Mohsenian-Rad AH, 2009, IEEE T WIREL COMMUN, V8, P4110, DOI 10.1109/TWC.2009.080568
   Niyato D, 2008, IEEE T WIREL COMMUN, V7, P4273, DOI 10.1109/T-WC.2008.070546
   Niyato D, 2008, IEEE J SEL AREA COMM, V26, P192, DOI 10.1109/JSAC.2008.080117
   Niyato D, 2008, IEEE T WIREL COMMUN, V7, P5150, DOI 10.1109/T-WC.2008.070609
   Park H, 2007, IEEE T SIGNAL PROCES, V55, P3496, DOI 10.1109/TSP.2007.893755
   Rhee W, 2000, 2000 IEEE 51ST VEHICULAR TECHNOLOGY CONFERENCE, PROCEEDINGS, VOLS 1-3, P1085, DOI 10.1109/VETECS.2000.851292
   Ronda JI, 1999, IEEE T CIRC SYST VID, V9, P1243, DOI 10.1109/76.809159
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Su GM, 2005, IEEE T CIRC SYST VID, V15, P1124, DOI 10.1109/TCSVT.2005.852626
   Tagliasacchi M, 2008, IEEE T IMAGE PROCESS, V17, P1129, DOI 10.1109/TIP.2008.924278
   Tang L, 2011, IEEE T WIREL COMMUN, V10, P1063, DOI 10.1109/TWC.2011.020111.101870
   Tiwari M, 2011, IEEE T IMAGE PROCESS, V20, P3219, DOI 10.1109/TIP.2011.2146262
   Tiwari M, 2010, IEEE T IMAGE PROCESS, V19, P1009, DOI 10.1109/TIP.2009.2038777
   Vetro A, 1999, IEEE T CIRC SYST VID, V9, P186, DOI 10.1109/76.744285
   Wang LM, 1996, IEEE T CONSUM ELECTR, V42, P300, DOI 10.1109/30.536124
   Wang LM, 1999, IEEE T CIRC SYST VID, V9, P949, DOI 10.1109/76.785733
   Wang Y, 2010, IEEE T CIRC SYST VID, V20, P829, DOI 10.1109/TCSVT.2010.2045919
   Zhu XQ, 2005, IEEE IMAGE PROC, P1513
NR 25
TC 7
Z9 7
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 1849
EP 1862
DI 10.1109/TMM.2014.2343943
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300005
OA Green Published
DA 2024-07-18
ER

PT J
AU Xu, QQ
   Xiong, JC
   Huang, QM
   Yao, Y
AF Xu, Qianqian
   Xiong, Jiechao
   Huang, Qingming
   Yao, Yuan
TI Online HodgeRank on Random Graphs for Crowdsourceable QoE Evaluation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Crowdsourcing; Hodge theory; online algorithms; paired comparison;
   persistent homology; quality of experience; random graphs; Robbins-Monro
   procedure; stochastic approximation
ID MODEL
AB HodgeRank on random graphs is proposed recently as an effective framework for multimedia quality assessment problem based on paired comparison methods. With a random design on graphs, it is particularly suitable for large scale crowd-sourcing experiments on the Internet. However, there still lacks a systematic study about online schemes to deal with the rising streaming and massive data in crowdsourceable scenarios. To fill in this gap, we propose in this paper an online ranking/rating scheme based on stochastic approximation of HodgeRank on random graphs for Quality of Experience (QoE) evaluation, where assessors and rating pairs enter the system in a sequential or streaming way. The scheme is shown in both theory and experiments to be efficient in obtaining global ranking by exhibiting the same asymptotic performance as batch HodgeRank under a general edge-independent sampling process. Moreover, the proposed framework enables us to monitor topological changement and triangular inconsistency in real time. Among a wide spectrum of choices, two particular types of random graphs are studied in detail, i.e., Erdos-Renyi random graph and preferential attachment random graph. The former is the simplest I.I.D. (independent and identically distributed) sampling and the latter may achieve more efficient performance in ranking the top-items due to its Rich-get-Richer property. We demonstrate the effectiveness of the proposed framework on LIVE and IVC databases.
C1 [Xu, Qianqian] Peking Univ, BICMR, Beijing 100871, Peoples R China.
   [Xu, Qianqian] Univ Chinese Acad Sci, Beijing 100871, Peoples R China.
   [Xiong, Jiechao; Yao, Yuan] Peking Univ, Sch Math Sci, BICMR LMAM LMEQF LMP, Beijing 100871, Peoples R China.
   [Huang, Qingming] Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
   [Huang, Qingming] Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
C3 Peking University; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; Peking University; Chinese Academy of
   Sciences; University of Chinese Academy of Sciences, CAS; Chinese
   Academy of Sciences; Institute of Computing Technology, CAS
RP Xu, QQ (corresponding author), Peking Univ, BICMR, Beijing 100871, Peoples R China.
EM qqxu@jdl.ac.cn; xiongjiechao@pku.edu.cn; qmhuang@jdl.ac.cn;
   yuany@math.pku.edu.cn
FU National Basic Research Program of China (973 Program) [2012CB825501,
   2012CB316400]; National Natural Science Foundation of China [61025011,
   61071157, 61332016, 61370004]
FX This work was supported in part by National Basic Research Program of
   China (973 Program): 2012CB825501 and 2012CB316400, and in part by
   National Natural Science Foundation of China: 61025011, 61071157,
   61332016, and 61370004. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Sheng-Wei
   (Kuan-Ta) Chen.
CR Ailon N, 2012, J MACH LEARN RES, V13, P137
   [Anonymous], 1993, TEXTS APPL MATH
   [Anonymous], 2005, SUBJECTIVE QUALITY A
   [Anonymous], 2003, Oxford Stud. in Probab.
   [Anonymous], 2008, LIVE IMAGE VIDEO QUA
   [Anonymous], 1985, Minimization methods for non-differentiable functions
   Barabási AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509
   Bollobas B, 2001, Cambridge Studies in Advanced Mathematics, V2nd, DOI DOI 10.1017/CBO9780511814068
   Candogan O, 2011, MATH OPER RES, V36, P474, DOI 10.1287/moor.1110.0500
   Carlsson G, 2009, B AM MATH SOC, V46, P255, DOI 10.1090/S0273-0979-09-01249-X
   Chen KT, 2009, MATH COMPUT SCI ENG, P491, DOI 10.1145/1631272.1631339
   Chung FR, 2006, CBMS Regional Conference Series in Mathematics, DOI 10.1090/cbms/107
   Cucker F, 2002, B AM MATH SOC, V39, P1
   David H., 1988, METHOD PAIRED COMP, V41
   Edelsbrunner H, 2002, DISCRETE COMPUT GEOM, V28, P511, DOI 10.1007/s00454-002-2885-2
   Edelsbrunner H., 2010, American Mathematical Soc., DOI DOI 10.1090/MBK/069
   Eichhorn A, 2010, NOSSDAV 2010: PROCEEDINGS OF THE 20TH INTERNATIONAL WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO, P63
   Elo A. E., 1978, The Rating of Chessplayers, Past and Present
   Erdos P., 1959, Publicationes Mathematicae Debrecen, V6, P18
   Glickman M.E., 1993, Paired Comparision Models with Time-Varying Parameters, DOI [10.1080/02664760120059219, DOI 10.1080/02664760120059219]
   Glickman ME, 1999, J ROY STAT SOC C-APP, V48, P377, DOI 10.1111/1467-9876.00159
   Herbrich Ralf, 2006, Proceedings of the 19th international conference on neural information processing systems, P569
   Hossfeld Tobias, 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P494, DOI 10.1109/ISM.2011.87
   ITU-R, 1996, METH SUBJ DET TRANSM
   Jamieson K., 2011, P ANN C NEUR INF PRO
   Jiang XY, 2011, MATH PROGRAM, V127, P203, DOI 10.1007/s10107-010-0419-x
   Jiang Y.-G, 2013, P AAAI C ART INT
   Kalman R E., 1960, J BASIC ENG, V82, P35, DOI DOI 10.1115/1.3662552
   Kalman R.E., 1961, Trans. ASME, V83, P95, DOI [10.1115/1.3658902, DOI 10.1115/1.3658902]
   Kendall M. G., 1948, Rank correlation methods.
   Kim M, 2010, LECT NOTES COMPUT SC, V6516, P62, DOI 10.1007/978-3-642-18009-5_7
   Kittur A, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P453
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Ma WY, 2011, PROC CVPR IEEE, P153, DOI 10.1109/CVPR.2011.5995422
   Osting B., 2013, AIMS J INVERSE PROBL
   Osting Braxton., 2013, INT C MACHINE LEARNI, P489
   Rakhlin A., 2012, LECT NOTES U PENNSYL
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Schatz R., 2012, SPRINGERS COMPUTER C
   Sexton H., 2009, JPLEX JAVA SOFTWARE
   Smale S, 2006, FOUND COMPUT MATH, V6, P145, DOI 10.1007/s10208-004-0160-z
   Vapnik V., 1998, STAT LEARNING THEORY, V3
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Widrow B, 1960, TECH REP
   Wu CC, 2013, IEEE T MULTIMEDIA, V15, P1121, DOI 10.1109/TMM.2013.2241043
   Xu Q., 2013, P ACM MULT IN PRESS
   Xu Q., 2012, P 20 ACM INT C MULT, P359, DOI DOI 10.1145/2393347.2393400
   Xu Q, 2012, IEEE T MULTIMEDIA, V14, P844, DOI 10.1109/TMM.2012.2190924
   Xu Qianqian., 2011, ACM INT C MULTIMEDIA, P393
   Yao YA, 2010, IEEE T INFORM THEORY, V56, P6470, DOI 10.1109/TIT.2010.2079010
   Yuan J, 2009, J MATH IMAGING VIS, V33, P169, DOI 10.1007/s10851-008-0122-1
   Zomorodian A, 2005, DISCRETE COMPUT GEOM, V33, P249, DOI 10.1007/s00454-004-1146-y
NR 53
TC 9
Z9 10
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 373
EP 386
DI 10.1109/TMM.2013.2292568
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800008
DA 2024-07-18
ER

PT J
AU Liu, S
   Feng, JS
   Domokos, C
   Xu, H
   Huang, JS
   Hu, ZZ
   Yan, SC
AF Liu, Si
   Feng, Jiashi
   Domokos, Csaba
   Xu, Hui
   Huang, Junshi
   Hu, Zhenzhen
   Yan, Shuicheng
TI Fashion Parsing With Weak Color-Category Labels
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fashion parsing; Markov random fields; weakly-supervised learning
AB In this paper we address the problem of automatically parsing the fashion images with weak supervision from the user-generated color-category tags such as "red jeans" and "white T-shirt". This problem is very challenging due to the large diversity of fashion items and the absence of pixel-level tags, which make the traditional fully supervised algorithms inapplicable. To solve the problem, we propose to combine the human pose estimation module, the MRF-based color and category inference module and the (super) pixel-level category classifier learning module to generate multiple well-performing category classifiers, which can be directly applied to parse the fashion items in the images. Besides, all the training images are parsed with color-category labels and the human poses of the images are estimated during the model learning phase in this work. We also construct a new fashion image dataset called Colorful-Fashion, in which all 2,682 images are labeled with pixel-level color-category labels. Extensive experiments on this dataset clearly show the effectiveness of the proposed method for the weakly supervised fashion parsing task.
C1 [Liu, Si; Feng, Jiashi; Domokos, Csaba; Huang, Junshi; Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117548, Singapore.
   [Xu, Hui] Chinese Acad Sci, Chongqing Inst Green & Intelligent Technol, Chongqing, Peoples R China.
   [Hu, Zhenzhen] Hefei Univ Technol, Hefei, Peoples R China.
C3 National University of Singapore; Chinese Academy of Sciences; Chongqing
   Institute of Green & Intelligent Technology, CAS; Hefei University of
   Technology
RP Liu, S (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117548, Singapore.
EM dcslius@nus.edu.sg; jiashi@nus.edu.sg; eledc@nus.edu.sg;
   xuhui@cigit.ac.cn; junshi.huang@nus.edu.sg; huzhen.ice@gmail.com;
   eleyans@nus.edu.sg
RI Yan, Shuicheng/HCI-1431-2022; Feng, Jiashi/AGX-6209-2022
FU Singapore National Research Foundation under its International Research
   Centre@Singapore Funding Initiative
FX This work was supported by the Singapore National Research Foundation
   under its International Research Centre@Singapore Funding Initiative and
   administered by the IDM Programme Office. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Sheng-Wei (Kuan-Ta) Chen.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], P ICCV
   Berg T., P ECCV
   Bossard L., P ACCV
   Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231
   Carsten R., P TOG
   Chen H., P ECCV
   Chen H., P CVPR
   Dalal N., P CVPR
   Daniel K., P CVPR
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Gabriel J. B., P ECCV
   Han Y., 2012, P CVPR
   Hasan B., 2010, P BMVC
   Hoiem D, 2011, INT J COMPUT VISION, V91, P328, DOI 10.1007/s11263-010-0400-4
   Ian E., P ECCV
   Joost V. D. W., P CVPR
   Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0
   Liu C., P CVPR
   Liu S., P ACM MM
   Liu S., P CVPR
   Liu X., P MM
   Liu X., P CVPR
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ming Y., P ICIP
   Park D., P ICCV
   Sathiya S. K., 2008, P KDD
   Shotton J., 2006, P ECCV
   Vezhnevets A., P CVPR
   Vezhnevets A., P ICCV
   Wang G., P CVPR
   Yamaguchi K., P CVPR
   Yang Y., P CVPR
NR 34
TC 129
Z9 141
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 253
EP 265
DI 10.1109/TMM.2013.2285526
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100021
DA 2024-07-18
ER

PT J
AU Liu, Y
   Duan, JZ
   Tang, Q
   Zhang, YD
AF Liu, Yu
   Duan, Jizhong
   Tang, Qiang
   Zhang, Yongdong
TI A Simple and Efficient Re-Scrambling Scheme for DTV Programs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE DTV; re-scrambling; interleaving multiplex
ID CONDITIONAL ACCESS
AB In order to guarantee pay-TV services, data of digital television (DTV) programs are scrambled by conditional access systems (CAS). In practical applications, some DTV transmitting nodes need descramble the scrambled DTV programs for editing purposes. After editing, how to re-scramble these edited programs is a challenging task since building CAS at transmitting nodes is expensive and insecure. In this paper, we proposed a novel scheme to solve this problem. Together with the recommended scheme, we also proposed techniques regarding video key data selection and extraction, synchronization of scrambled and descrambled transport stream (TS) packets, and scrambled/unscrambled interleaving multiplexer. The proposed scheme requires much lower complexity than existing methods while maintaining enough security for practical applications. Neither professional CAS equipment nor real-time common key (CK) transmitting is required by our re-scrambling scheme. Various experimental results demonstrated that the proposed re-scrambling scheme achieved superior performance in practical DTV systems, and it also obtained good compatibility with different CAS algorithms.
C1 [Liu, Yu; Duan, Jizhong] Tianjin Univ, Sch Elect & Informat, Tianjin 300072, Peoples R China.
   [Tang, Qiang] Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
   [Zhang, Yongdong] Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
C3 Tianjin University; University of British Columbia; Chinese Academy of
   Sciences; Institute of Computing Technology, CAS
RP Liu, Y (corresponding author), Tianjin Univ, Sch Elect & Informat, Tianjin 300072, Peoples R China.
EM yul@princeton.edu; duanjz@tju.edu.cn; qiangt@ece.ubc.ca; zhyd@ict.ac.cn
RI Yu, Liu/AAC-8646-2019
FU National Natural Science Foundation of China [61373102, 61272323,
   61379084, 61102101]; National Key Technology Research and Development
   Program of China [2012BAH06B01]; Chinese Scholarship Council (CSC)
FX This work was supported in part by the National Natural Science
   Foundation of Chinaunder Grant 61373102, Grant 61272323, Grant 61379084,
   and Grant 61102101, and the National Key Technology Research and
   Development Program of China (2012BAH06B01). The work of Y. Liu was
   supported by the Chinese Scholarship Council (CSC). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Pal Halvorsen.
CR [Anonymous], 1996, ETSI 289 DIGITAL VID
   [Anonymous], 1996, EN 50221 COMMON INTE
   [Anonymous], 1998, TECHNICAL SPECIFICAT
   [Anonymous], 1999, 103197 ETSI TS
   [Anonymous], 1996, 289 ETSI ETR
   Cutts DJ, 1997, ELECTRON COMMUN ENG, V9, P21, DOI 10.1049/ecej:19970104
   Huang YL, 2004, IEEE T MULTIMEDIA, V6, P760, DOI 10.1109/TMM.2004.834861
   Jianbo L., 2007, P INT C COMP INT SEC, P608
   Joon-Young J., P 8 INT C ADV COMM T, V3, p[4, 1916]
   Kamperman F, 2001, IEEE T CONSUM ELECTR, V47, P47, DOI 10.1109/30.920419
   Kim WH, 1997, IEEE T CONSUM ELECTR, V43, P980, DOI 10.1109/30.628778
   Kuei-Yi C., 2011, P 13 AS PAC NETW OP, P1
   Mengyao Z., P INT C COMP INT SEC, V2, P1532
   Tianpu J., 2004, P 9 INT C COMM SYST, P326
   Weinmann K. W. R.-P., 2004, P C COMM MULT SEC
NR 15
TC 4
Z9 4
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 137
EP 146
DI 10.1109/TMM.2013.2283848
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100012
DA 2024-07-18
ER

PT J
AU Zhu, C
   Jia, HZ
   Zhang, SH
   Huang, XF
   Xie, XD
   Gao, W
AF Zhu, Chuang
   Jia, Huizhu
   Zhang, Shanghang
   Huang, Xiaofeng
   Xie, Xiaodong
   Gao, Wen
TI On a Highly Efficient RDO-Based Mode Decision Pipeline Design for AVS
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE AVS; mode decision; pipeline; RDO; throughput.
ID VIDEO CODING STANDARD; H.264/AVC ENCODER; ALGORITHM; ARCHITECTURE;
   CODERS
AB Rate distortion optimization (RDO) is the best known mode decision method, while the high implementation complexity limits its applications and almost no real-time hardware encoder is truly full-featured RDO based. In this paper, first, a full-featured RDO-based mode decision (MD) algorithm is presented, which makes more modes enter RDO process. Second, the throughput of RDO-based MD pipeline is thoroughly analyzed and modeled. Third, a highly efficient adaptive block-level pipelining architecture of RDO-based MD for AVS video encoder is proposed which can achieve the highest throughput to alleviate the RDO burden. Our design is described in high-level Verilog/VHDL hardware description language and implemented under SMIC 0.18-mu m CMOS technology with 232 K logic gates and 85 Kb SRAMs. The implementation results validate our architectural design and the proposed architecture can support real time processing of 1080P@30 fps. The coding efficiency of our adopted method far outperforms (0.57 dB PSNR gain in average) the traditional low-complexity MD (LCMD) methods and the throughput of our designed pipeline is increased by 11.3%, 19% and 17% for I, P and B frames, respectively, compared with the existed RDO-based architecture.
C1 [Zhu, Chuang; Jia, Huizhu; Zhang, Shanghang; Huang, Xiaofeng; Xie, Xiaodong; Gao, Wen] Peking Univ, Inst Digital Media, Beijing 100871, Peoples R China.
C3 Peking University
RP Zhu, C (corresponding author), Peking Univ, Inst Digital Media, Beijing 100871, Peoples R China.
EM hzjia@pku.edu.cn
RI Zhang, Lisa/AAW-9795-2021; Zhu, Chuang/ABB-3367-2021
OI Zhu, Chuang/0000-0001-5155-7069
FU Chinese National Natural Science Foundation [61035001, 61176139];
   Development Program of China (863 Program) [2012AA011703]
FX This work was supported in part by grants from the Chinese National
   Natural Science Foundation under contract No. 61035001 and No. 61176139,
   and Development Program of China (863 Program) under contract No.
   2012AA011703.
CR [Anonymous], 2001, P 13 VCEG M33 M AUST
   [Anonymous], 144962 ISOIEC
   [Anonymous], 2003, JVTG050DOC
   [Anonymous], 2002, VIDEO CODEC DESIGN D, P1
   *AVS, 2003, AVSN1063
   Babionitakis K, 2008, J REAL-TIME IMAGE PR, V3, P43, DOI 10.1007/s11554-007-0054-9
   CHEN TC, 2006, P ACM AS S PAC DES A
   Chen TC, 2006, IEEE T CIRC SYST VID, V16, P673, DOI 10.1109/TCSVT.2006.873163
   Eom M., 2007, P IEEE INT C IM PROC
   Gao W., 2004, NATL ASS BROADCASTER
   Gao W, 2010, STUD COMPUT INTELL, V280, P125
   Hashimoto R., P PICT COD S THPM4 9
   Kim T. S., 2011, P 54 IEEE INT MIDW S
   Ku CW, 2006, IEEE T CIRC SYST VID, V16, P917, DOI 10.1109/TCSVT.2006.879992
   Liu ZY, 2009, IEEE J SOLID-ST CIRC, V44, P594, DOI 10.1109/JSSC.2008.2010797
   Pan F, 2005, IEEE T CIRC SYST VID, V15, P813, DOI 10.1109/TCSVT.2005.848356
   Sarwer MG, 2007, IEEE T CIRC SYST VID, V17, P1402, DOI 10.1109/TCSVT.2007.903787
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Tseng CH, 2006, IEEE T CIRC SYST VID, V16, P1027, DOI 10.1109/TCSVT.2006.878146
   Tu YK, 2006, IEEE T CIRC SYST VID, V16, P600, DOI 10.1109/TCSVT.2006.873160
   Wang JF, 2006, IEEE INT SYMP CIRC S, P3498
   Wang JC, 2007, IEEE T CIRC SYST VID, V17, P1414, DOI 10.1109/TCSVT.2007.903786
   Wang X., 2010, LECT NOTES COMPUTER, P62
   Wei KJ, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P373, DOI 10.1109/PCS.2012.6213368
   Wei Z., 2007, P 32 INT C AC SPEECH
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu D, 2005, IEEE T CIRC SYST VID, V15, P953, DOI 10.1109/TCSVT.2005.848304
   Yang W., 2010, 2010 INT C CONS EL I
   Yin H., 2009, P 2 INT C IM SIGN PR
   Zhao X, 2010, IEEE T CIRC SYST VID, V20, P647, DOI 10.1109/TCSVT.2010.2045803
   Zhu C., 2011, P ICME JUL
NR 31
TC 3
Z9 3
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 1815
EP 1829
DI 10.1109/TMM.2013.2280446
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900008
DA 2024-07-18
ER

PT J
AU Chen, DY
   Luo, YS
AF Chen, Duan-Yu
   Luo, Yi-Shiou
TI Preserving Motion-Tolerant Contextual Visual Saliency for Video Resizing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video signal processing; image motion analysis; image quality
ID ATTENTION; MODEL
AB State of the art video resizing methods usually produce perceivable visual discontinuities, especially in videos containing significant motion. To resolve the problem, contextual information about the focus of interest in consecutive video frames should be considered in order to preserve the visual continuity. In this paper, to detect the focus of interest with motion-tolerance, we propose a novel approach for modelling visual dynamics based on spatiotemporal slices (STS), which provide rich visual patterns along a large temporal scale. First, patch-based visual patterns are computed to generate a codebook of the automatically specified spatiotemporal extent determined by the contextual information in the STS. The codebook is then used to compute its associated response in each video frame, and eventually an importance map covering the focus of interest in a video clip can be obtained. To preserve the visual continuity of the content, particularly an important area, a multi-cue approach is used to guide a mesh-based non-homogeneous warping operation constrained by the trajectories in the STS. For the performance evaluation, we present a novel measure that utilizes patch-based Kullback-Leibler divergence (KL-divergence) to gauge the deformation of the focus of interest under the proposed video resizing approach. Experimental results show that the STS-based approach can generate retargeted videos effectively, while maintaining their isotropic manipulation and the continuous dynamics of visual perception.
C1 [Chen, Duan-Yu; Luo, Yi-Shiou] Yuan Ze Univ, Dept Elect Engn, Tao Yuan, Taiwan.
C3 Yuan Ze University
RP Chen, DY (corresponding author), Yuan Ze Univ, Dept Elect Engn, Tao Yuan, Taiwan.
EM dychen@saturn.yzu.edu.tw; s984629@mail.yzu.edu.tw
CR [Anonymous], PG 08
   [Anonymous], MUM 05
   [Anonymous], ACM TRANS GRAPH
   [Anonymous], ICCV 07
   [Anonymous], 2006, P 14 ACM INT C MULT
   [Anonymous], MULTIMEDIA TOOLS APP
   Chen HT, 2010, IEEE IMAGE PROC, P1117, DOI 10.1109/ICIP.2010.5650014
   Chen LQ, 2003, MULTIMEDIA SYST, V9, P353, DOI 10.1007/s00530-003-0105-4
   Deselaers T., 2008, PROC IEEE COMPUTER S, P1
   Fan Xin., 2003, P ACM MULTIMEDIA, P247
   Gal R., 2006, P 17 EUROGRAPHICS C, P297, DOI DOI 10.2312/EGWR/EGSR06/297-303
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Krähenbühl P, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1616452.1618472, 10.1145/1618452.1618472]
   Liu Hao., 2003, P ACM INT C MULTIMED, P148
   Ngo CW, 2003, IEEE T IMAGE PROCESS, V12, P341, DOI 10.1109/TIP.2003.809020
   Ngo CW, 2002, IEEE T MULTIMEDIA, V4, P446, DOI 10.1109/TMM.2002.802022
   Ngo CW, 2001, IEEE T CIRC SYST VID, V11, P941, DOI 10.1109/76.937435
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Santella A., 2006, Conference on Human Factors in Computing Systems. CHI2006, P771
   Suh Bongwon, 2003, P ACM S US INT SOFTW, P95, DOI DOI 10.1145/964696.964707
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang YS, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618473
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
NR 23
TC 11
Z9 14
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2013
VL 15
IS 7
BP 1616
EP 1627
DI 10.1109/TMM.2013.2267725
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 236PQ
UT WOS:000325811800013
DA 2024-07-18
ER

PT J
AU Nathwani, K
   Pandit, P
   Hegde, RM
AF Nathwani, Karan
   Pandit, Pranav
   Hegde, Rajesh M.
TI Group Delay Based Methods for Speaker Segregation and its Application in
   Multimedia Information Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Group delay cross correlation function; iterative graph cut method;
   multimedia information retrieval; multi-speaker speech recognition;
   speaker separation
ID FORMANT EXTRACTION; SEPARATION; SPEECH; SIGNAL; PITCH; MODULATION;
   FREQUENCY; TRACKING; MASKING; AUDIO
AB A novel method of single channel speaker segregation using the group delay cross correlation function is proposed in this paper. The group delay function, which is the negative derivative of the phase spectrum, yields robust spectral estimates. Hence the group delay spectral estimates are first computed over frequency sub-bands after passing the speech signal through a bank of filters. The filter bank spacing is based on a multi-pitch algorithm that computes the pitch estimates of the competing speakers. An affinity matrix is then computed from the group delay spectral estimates of each frequency sub-band. This affinity matrix represents the correlations of the different sub-bands in the mixed broadband speech signal. The grouping of correlated harmonics present in the mixed speech signal is then carried out by using a new iterative graph cut method. The signals are reconstructed from the respective harmonic groups which represent individual speakers in the mixed speech signal. Spectrographic masks are then applied on the reconstructed signals to refine their perceptual quality. The quality of separated speech is evaluated using several objective and subjective criteria. Experiments on multi-speaker automatic speech recognition are conducted using mixed speech data from the GRID corpus. A cell phone based multimedia information retrieval system (MIRS) for multi-source meeting environments are also developed.
C1 [Nathwani, Karan; Pandit, Pranav; Hegde, Rajesh M.] Indian Inst Technol, Dept Elect Engn, Kanpur 208016, Uttar Pradesh, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Kanpur
RP Nathwani, K (corresponding author), Indian Inst Technol, Dept Elect Engn, Kanpur 208016, Uttar Pradesh, India.
EM nathwani@iitk.ac.in; pranavpandit89@gmail.com; rhegde@iitk.ac.in
FU BSNL-IIT Kanpur Telecom Center of Excellence; DeitY Government of India
FX Manuscript received May 11, 2012; revised July 17, 2012 and September
   13, 2012; accepted November 11, 2012. Date of publication February 25,
   2013; date of current version September 13, 2013. This work was
   supported in part by the BSNL-IIT Kanpur Telecom Center of Excellence
   and in part by the DeitY Government of India. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Ebroul Izquierdo.
CR Abe T., 1996, Proceedings of ICSLP 1996, P1292
   [Anonymous], 1999, Auditory Scene Analysis: The Perceptual Organization of Sound, DOI DOI 10.7551/MITPRESS/1486.001.0001
   [Anonymous], 2006, Computational auditory scene analysis: Principles, algorithms, and applications
   [Anonymous], 1978, DIGITAL PROCESSING S
   Bach F.R., 2004, P ADV NEUR INF PROC
   Bach FR, 2005, INT CONF ACOUST SPEE, P489
   BACON SP, 1989, J ACOUST SOC AM, V85, P2575, DOI 10.1121/1.397751
   Barker J, 2006, INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5, P85
   BOASHASH B, 1992, P IEEE, V80, P520, DOI 10.1109/5.135376
   Campbell JP, 1997, P IEEE, V85, P1437, DOI 10.1109/5.628714
   Cao YC, 1997, IEEE T SPEECH AUDI P, V5, P209, DOI 10.1109/89.568728
   Charpentier F. J., 1986, ICASSP 86 Proceedings. IEEE-IECEJ-ASJ International Conference on Acoustics, Speech and Signal Processing (Cat. No.86CH2243-4), P113
   Cooke M, 2006, J ACOUST SOC AM, V120, P2421, DOI 10.1121/1.2229005
   Damaschke J., 2002, P 3 K SCHWERP DTSCH, P50
   Dau T, 1997, J ACOUST SOC AM, V102, P2892, DOI 10.1121/1.420344
   ELLIS DPW, 1994, P 12 INT C PATT REC
   Emiya V, 2011, IEEE T AUDIO SPEECH, V19, P2046, DOI 10.1109/TASL.2011.2109381
   Goetze S, 2010, INT CONF ACOUST SPEE, P2450, DOI 10.1109/ICASSP.2010.5496301
   Gu L., 2010, THESIS CARNEGIE MELL
   Hansen M., 1996, P WORKS QUAL ASS SPE, P7
   Hegde R., 2007, EURASIP J AUDIO SPEE, V2007, P5
   Hegde R., 2009, P NCC 2009 IIT GUW I
   HERMES DJ, 1988, J ACOUST SOC AM, V83, P257, DOI 10.1121/1.396427
   Hu GN, 2004, IEEE T NEURAL NETWOR, V15, P1135, DOI 10.1109/TNN.2004.832812
   Huber R, 2006, IEEE T AUDIO SPEECH, V14, P1902, DOI 10.1109/TASL.2006.883259
   Lu GJ, 2001, MULTIMED TOOLS APPL, V15, P269, DOI 10.1023/A:1012491016871
   Ma J., 2010, SPEECH COMMUN
   MELLINGER D, 1991, THESIS STANFORD U ST
   Murthy HA, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P68
   MURTHY HA, 1991, SPEECH COMMUN, V10, P209, DOI 10.1016/0167-6393(91)90011-H
   Oppenheim A.V., DISCRETE TIME SIGNAL, V2
   Parsons T. W., 1999, IEEE T NEURAL NETWOR, V10, P684
   Raj B, 2005, IEEE WORK APPL SIG, P17, DOI 10.1109/ASPAA.2005.1540157
   Raj B., 2006, P IEEE INT C AC SPEE, V5, pV
   Reddy AM, 2007, IEEE T AUDIO SPEECH, V15, P1766, DOI 10.1109/TASL.2007.901310
   Schimmel S.M., 2005, Proceedings of ICASSP 2005, P1221
   Schmidt M. N., 2006, P INT C SPOK LANG PR, P1
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Sun X., 2002, P IEEE INT C AC SPEE, V1
   Titze I., 1994, Principles of Voice Production
   Viswanathan M., 2000, International Journal on Document Analysis and Recognition, V2, P147
   Wang DL, 2005, SPEECH SEPARATION BY HUMANS AND MACHINES, P181, DOI 10.1007/0-387-22794-6_12
   Wang DLL, 1999, IEEE T NEURAL NETWOR, V10, P684, DOI 10.1109/72.761727
   Weinstein E, 1993, IEEE T SPEECH AUDI P, V1, P405, DOI 10.1109/89.242486
   Weintraub M., 1986, ICASSP 86 Proceedings. IEEE-IECEJ-ASJ International Conference on Acoustics, Speech and Signal Processing (Cat. No.86CH2243-4), P81
   Weintraub M., 1985, THESIS STANFORD U ST
   Weiss Rabbi Luzer, 2011, COMMUNICATION
   Wu MY, 2003, IEEE T SPEECH AUDI P, V11, P229, DOI 10.1109/TSA.2003.811539
   YEGNANARAYANA B, 1984, IEEE T ACOUST SPEECH, V32, P610, DOI 10.1109/TASSP.1984.1164365
   YEGNANARAYANA B, 1992, IEEE T SIGNAL PROCES, V40, P2281, DOI 10.1109/78.157227
   YEGNANARAYANA B, 1978, J ACOUST SOC AM, V63, P1638, DOI 10.1121/1.381864
NR 51
TC 12
Z9 12
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2013
VL 15
IS 6
BP 1326
EP 1339
DI 10.1109/TMM.2013.2247391
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222XH
UT WOS:000324765400010
DA 2024-07-18
ER

PT J
AU Servia-Rodríguez, S
   Fernández-Vilas, A
   Díaz-Redondo, RP
   Pazos-Arias, JJ
AF Servia-Rodriguez, Sandra
   Fernandez-Vilas, Ana
   Diaz-Redondo, Rebeca P.
   Pazos-Arias, Jose J.
TI Inferring Contexts From Facebook Interactions: A Social Publicity
   Scenario
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Social web; data mining; NLP; social publicity; social contexts
ID RECOMMENDER SYSTEMS
AB The great acceptation of the Social Web has converted social networks, blogs and wikis in almost perfect advertising mediums. However, many of the current social publicity strategies do not exploit all the potential of these mediums, since they obviate users' online life: the social contexts in which they are involved. Our proposal to reverse this situation is a model to infer users' social contexts by the application of several Natural Language Processing (NLP) and data mining techniques over users' interaction data on Facebook. We take advantage of both Facebook and Groupon APIs to provide a deployment scenario in which knowing users' social life allows ads to target the most potential customers, which is beneficial for both companies and possible customers.
C1 [Servia-Rodriguez, Sandra; Fernandez-Vilas, Ana; Diaz-Redondo, Rebeca P.; Pazos-Arias, Jose J.] Univ Vigo, Dept Telemat Engn, SSI Grp, Vigo 36301, Spain.
C3 Universidade de Vigo; atlanTTic
RP Servia-Rodríguez, S (corresponding author), Univ Vigo, Dept Telemat Engn, SSI Grp, Vigo 36301, Spain.
EM sandra@det.uvigo.es; avilas@det.uvigo.es; rebeca@det.uvigo.es;
   jose@det.uvigo.es
RI Vilas, Ana Fernández/L-2055-2014; Arias, José/ITR-8005-2023; José, Pazos
   Arias/F-6788-2016; Díaz Redondo, Rebeca P./L-3108-2014
OI Vilas, Ana Fernández/0000-0003-1047-2143; José, Pazos
   Arias/0000-0002-0424-5481; Díaz Redondo, Rebeca P./0000-0002-2367-2219
FU Spanish Ministry of Economy and Competitiveness under the National
   Science Program [TIN2010-20797]; Galician Regional Government under the
   Program for Consolidation and Structuring of Competitive Research Units;
   EU ERDF; University of Vigo [CN 2011/023, CN 2012/260]
FX Manuscript received September 06, 2012; revised February 08, 2013 and
   April 16, 2013; accepted April 25, 2013. Date of publication May 30,
   2013; date of current version September 13, 2013. This work was
   supported in part by the Spanish Ministry of Economy and Competitiveness
   under the National Science Program (TIN2010-20797); the Galician
   Regional Government under the Program for Consolidation and Structuring
   of Competitive Research Units (partially funded by EU ERDF): competitive
   reference group CN 2011/023; and the AtlantTIC Research Center (CN
   2012/260) of the University of Vigo. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Jiebo Luo.
CR Abel Fabian., 2011, User Modeling and User-Adapted Interaction (UMUAI), Special Issue on Personalization in Social Web Systems, P1
   Adomavicius G, 2005, IEEE T KNOWL DATA EN, V17, P734, DOI 10.1109/TKDE.2005.99
   [Anonymous], 2004, P 4 INT C LANG RES E
   Beer D, 2007, SOCIOL RES ONLINE, V12
   Birdsall W.F., 2007, Webology, V4, P5
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Burke R, 2002, USER MODEL USER-ADAP, V12, P331, DOI 10.1023/A:1021240730564
   Cilibrasi RL, 2007, IEEE T KNOWL DATA EN, V19, P370, DOI 10.1109/TKDE.2007.48
   Duan J., 2012, ADV KNOWL DISCOV DAT, DOI [10.1007/978-3-642-30220-6_13: 145-156, DOI 10.1007/978-3-642-30220-6_13]
   Fernandez-Vilas A., 2012, P INT C CONS EL ICCE
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Gjoka M, 2011, IEEE J SEL AREA COMM, V29, P1872, DOI 10.1109/JSAC.2011.111011
   Golder S., 2005, CS0508082 ARXIV
   Jin X., 2011, P 17 ACM SIGKDD INT, V11, P753
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Palsetia D., 2012, P 6 INT WORKSH SOC N
   PEDERSEN T, 2004, HLT NAACL 2004, P38
   Servia-Rodriguez S., 2012, P 2 INT C CLOUD COMP
   Servia-Rodriguez S., 2012, P 3 INT S AMB INT MA
   Shapira B., 2012, USER MODEL USER-ADAP, P1
   Xie J., 2011, OVERLAPPING IN PRESS
   Zhao X., 2012, NEUROCOMPUTING
NR 22
TC 8
Z9 8
U1 0
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2013
VL 15
IS 6
BP 1296
EP 1303
DI 10.1109/TMM.2013.2265168
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222XH
UT WOS:000324765400007
DA 2024-07-18
ER

PT J
AU Cheng, X
   Liu, JC
   Dale, C
AF Cheng, Xu
   Liu, Jiangchuan
   Dale, Cameron
TI Understanding the Characteristics of Internet Short Video Sharing: A
   YouTube-Based Measurement Study
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; peer-to-peer; social network; YouTube
ID PEER-TO-PEER
AB Established in 2005, YouTube has become the most successful Internet website providing a new generation of short video sharing service. Today, YouTube alone consumes as much bandwidth as did the entire Internet in year 2000 [1]. Understanding the features of YouTube and similar video sharing sites is thus crucial to their sustainable development and to network traffic engineering. In this paper, using traces crawled in a 1.5-year span (from February 2007 to September 2008), we present an in-depth and systematic measurement study on the characteristics of YouTube videos. We find that YouTube videos have noticeably different statistics compared to traditional streaming videos, ranging from length, access pattern, to their active life span. The series of datasets also allow us to identify the growth trend of this fast evolving Internet site, which has seldom been explored before. We also look closely at the social networking aspect of YouTube, as this is a key driving force toward its success. In particular, we find that the links to related videos generated by uploaders' choices form a small-world network. This suggests that the videos have strong correlations with each other, and creates opportunities for developing novel caching and peer-to-peer distribution schemes to efficiently deliver videos to end users.
C1 [Cheng, Xu; Liu, Jiangchuan; Dale, Cameron] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
C3 Simon Fraser University
RP Cheng, X (corresponding author), Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
EM xuc@cs.sfu.ca; jcliu@cs.sfu.ca; camerond@cs.sfu.ca
FU Canada NSERC Discovery Grant; NSERC Strategic Project Grant
FX This work was supported in part by a Canada NSERC Discovery Grant and an
   NSERC Strategic Project Grant. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof. Pal
   Halvorsen.
CR Acharya S, 2000, P SOC PHOTO-OPT INS, V3969, P130
   Albert R, 1999, NATURE, V401, P130, DOI 10.1038/43601
   Almeida J.M., 2001, Proceedings of the 11th international workshop on Network and operating systems support for digital audio and video, P21, DOI 10.1145/378344.378348
   [Anonymous], 2009, YOUTUBES CHAD HURLEY
   Breslau L, 1999, IEEE INFOCOM SER, P126, DOI 10.1109/INFCOM.1999.749260
   Cha M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P1
   Cheng X, 2008, INT WORKSH QUAL SERV, P249
   Cheng X, 2009, IEEE INFOCOM SER, P1152, DOI 10.1109/INFCOM.2009.5062028
   Gill P, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P15
   Halvey M.J., 2007, WWW 07, P1273, DOI DOI 10.1145/1242572.1242804
   Hossfeld T, 2008, 2008 SECOND INTERNATIONAL CONFERENCE ON COMMUNICATIONS AND ELECTRONICS, P155
   Huang C, 2007, ACM SIGCOMM COMP COM, V37, P133, DOI 10.1145/1282427.1282396
   Liu JC, 2004, IEEE COMMUN MAG, V42, P88, DOI 10.1109/MCOM.2004.1321397
   Liu JC, 2008, P IEEE, V96, P11, DOI 10.1109/JPROC.2007.909921
   MILGRAM S, 1967, PSYCHOL TODAY, V1, P61
   Mislove A, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P29
   Ravasz E, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.026112
   Saxena Mohit., 2008, NOSSDAV '08: Proceedings of the 18th International Workshop on Network and Operating Systems Support for Digital Audio and Video, P39
   Sen S, 1999, IEEE INFOCOM SER, P1310, DOI 10.1109/INFCOM.1999.752149
   Tang W., 2003, HPL200323
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
   Zipf GK., 1935, PSYCHO BIOL LANGUAGE
NR 23
TC 153
Z9 166
U1 27
U2 217
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2013
VL 15
IS 5
BP 1184
EP 1194
DI 10.1109/TMM.2013.2265531
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222WT
UT WOS:000324763600020
DA 2024-07-18
ER

PT J
AU Dähne, S
   Biessmann, F
   Meinecke, FC
   Mehnert, J
   Fazli, S
   Müller, KR
AF Daehne, Sven
   Biessmann, Felix
   Meinecke, Frank C.
   Mehnert, Jan
   Fazli, Siamac
   Mueller, Klaus-Robert
TI Integration of Multivariate Data Streams With Bandpower Signals
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE EEG; EEG-NIRS; multimodal; neuroimaging; NIRS
ID HEMODYNAMIC-RESPONSE; COMPONENT ANALYSIS; EEG-FMRI; BRAIN; FUSION;
   MODEL; LOCALIZATION; ACTIVATION; SYSTEMS; CORTEX
AB The urge to further our understanding of multimodal neural data has recently become an important topic due to the ever increasing availability of simultaneously recorded data from different neural imaging modalities. In case where EEG is one of the modalities, it is of interest to relate a nonlinear function of the raw EEG time-domain signal, say, EEG band power, to another modality such as the hemodynamic response, as measured with NIRS or fMRI. In this work we tackle exactly this problem defining a novel algorithm that we denote multimodal source power correlation analysis (mSPoC). The validity and high performance of the mSPoC framework is demonstrated for simulated and real-world multimodal data.
C1 [Daehne, Sven; Biessmann, Felix; Meinecke, Frank C.; Mehnert, Jan; Fazli, Siamac; Mueller, Klaus-Robert] Berlin Inst Technol, Dept Machine Learning, Berlin, Germany.
   [Daehne, Sven] Bernstein Ctr Computat Neurosci, Berlin, Germany.
   [Biessmann, Felix] MPI Tubingen, Dept Physiol Cognit Proc, Tubingen, Germany.
   [Mehnert, Jan] Charite, Berlin NeuroImaging Ctr, D-13353 Berlin, Germany.
   [Mehnert, Jan] Max Planck Inst Human Cognit & Brain Sci, Leipzig, Germany.
   [Mehnert, Jan; Fazli, Siamac; Mueller, Klaus-Robert] Korea Univ, Dept Brain & Cognit Engn, Seoul, South Korea.
C3 Technical University of Berlin; Free University of Berlin; Humboldt
   University of Berlin; Charite Universitatsmedizin Berlin; Max Planck
   Society; Korea University
RP Dähne, S (corresponding author), Berlin Inst Technol, Dept Machine Learning, Berlin, Germany.
RI Mehnert, Jan/AAX-8791-2020; Mueller, Klaus-Robert/C-3196-2013; Fazli,
   Siamac/GYQ-9627-2022
OI Mehnert, Jan/0000-0001-7088-740X; Mueller,
   Klaus-Robert/0000-0002-3861-7685; Fazli, Siamac/0000-0003-3397-0647
FU Bernstein Focus Neurotechnology [BMBF-Fkz 01GQ0850]; BMBF project ALICE,
   Automonous Learning in Complex Environments [01IB10003B]; BMBF project
   'Adaptive BCI' [FKZ 01GQ1115]; European ICT Programme Project
   [FP7-224631, GRK 1589/1]; World Class University Program through the
   National Research Foundation of Korea; Ministry of Education, Science,
   and Technology [R31-10008]
FX This work was supported in part by the Bernstein Focus Neurotechnology
   (BMBF-Fkz 01GQ0850), the BMBF project ALICE, Automonous Learning in
   Complex Environments (01IB10003B), the BMBF project 'Adaptive BCI' (FKZ
   01GQ1115), the European ICT Programme Project FP7-224631, GRK 1589/1,
   and by the World Class University Program through the National Research
   Foundation of Korea funded by the Ministry of Education, Science, and
   Technology, under Grant R31-10008. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Tulay
   Adali.
CR Akaho S., 1999, P INT JOINT C NEUR N
   [Anonymous], 2005, Electric Fields of the Brain: The Neurophysics of Eeg
   Attwell D, 2001, J CEREBR BLOOD F MET, V21, P1133, DOI 10.1097/00004647-200110000-00001
   Baillet S, 2001, IEEE SIGNAL PROC MAG, V18, P14, DOI 10.1109/79.962275
   BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129
   Berger H, 1929, ARCH PSYCHIAT NERVEN, V87, P527, DOI 10.1007/BF01797193
   Berwick J, 2008, J NEUROPHYSIOL, V99, P787, DOI 10.1152/jn.00658.2007
   Biessmann F, 2012, NEUROIMAGE, V61, P1031, DOI 10.1016/j.neuroimage.2012.04.015
   Biessmann Felix, 2011, IEEE Rev Biomed Eng, V4, P26, DOI 10.1109/RBME.2011.2170675
   Biessmann F, 2010, MACH LEARN, V79, P5, DOI 10.1007/s10994-009-5153-3
   Blankertz B, 2011, NEUROIMAGE, V56, P814, DOI 10.1016/j.neuroimage.2010.06.048
   Bojak I, 2011, PHILOS T R SOC A, V369, P3785, DOI 10.1098/rsta.2011.0080
   Boynton GM, 1996, J NEUROSCI, V16, P4207, DOI 10.1523/jneurosci.16-13-04207.1996
   Calhoun VD, 2009, NEUROIMAGE, V45, pS163, DOI 10.1016/j.neuroimage.2008.10.057
   CARDOSO JF, 2001, P ICA 2001 WORKSH SA
   Cichocki A., 2009, NONNEGATIVE MATRIX T, DOI 10.1002/9780470747278
   Correa NM, 2010, NEUROIMAGE, V50, P1438, DOI 10.1016/j.neuroimage.2010.01.062
   Dale AM, 2000, NEURON, V26, P55, DOI 10.1016/S0896-6273(00)81138-1
   DALE AM, 1993, J COGNITIVE NEUROSCI, V5, P162, DOI 10.1162/jocn.1993.5.2.162
   Daunizeau J, 2007, NEUROIMAGE, V36, P69, DOI 10.1016/j.neuroimage.2007.01.044
   DELPY DT, 1988, PHYS MED BIOL, V33, P1433, DOI 10.1088/0031-9155/33/12/008
   Eichele T., 2010, EEG FMRI PHYSL TECHN
   Fazli S, 2012, NEUROIMAGE, V59, P519, DOI 10.1016/j.neuroimage.2011.07.084
   Friston KJ, 2009, SCIENCE, V326, P399, DOI 10.1126/science.1174521
   Habermehl C, 2012, NEUROIMAGE, V59, P3201, DOI 10.1016/j.neuroimage.2011.11.062
   Hamel E, 2006, J APPL PHYSIOL, V100, P1059, DOI 10.1152/japplphysiol.00954.2005
   Handwerker DA, 2004, NEUROIMAGE, V21, P1639, DOI 10.1016/j.neuroimage.2003.11.029
   Harshman R., 2006, P 34 ANN M STAT SOC
   Haufe S, 2011, NEUROIMAGE, V54, P851, DOI 10.1016/j.neuroimage.2010.09.003
   Herrmann MJ, 2008, HUM BRAIN MAPP, V29, P28, DOI 10.1002/hbm.20368
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hyvärinen A, 2001, INDEPENDENT COMPONENT ANALYSIS: PRINCIPLES AND PRACTICE, P71
   JOBSIS FF, 1977, SCIENCE, V198, P1264, DOI 10.1126/science.929199
   Koch Stefan P, 2010, Front Neuroenergetics, V2, P12, DOI 10.3389/fnene.2010.00012
   Koch SP, 2009, J NEUROSCI, V29, P13962, DOI 10.1523/JNEUROSCI.1402-09.2009
   Kocsis L, 2006, PHYS MED BIOL, V51, pN91, DOI 10.1088/0031-9155/51/5/N02
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Lee H, 2007, INT J NEURAL SYST, V17, P305, DOI 10.1142/S0129065707001159
   Lemm S, 2011, NEUROIMAGE, V56, P387, DOI 10.1016/j.neuroimage.2010.11.004
   Liu AK, 1998, P NATL ACAD SCI USA, V95, P8945, DOI 10.1073/pnas.95.15.8945
   Liu ZM, 2008, NEUROIMAGE, V39, P1198, DOI 10.1016/j.neuroimage.2007.10.003
   Logothetis NK, 2001, NATURE, V412, P150, DOI 10.1038/35084005
   Martínez-Montes E, 2004, NEUROIMAGE, V22, P1023, DOI 10.1016/j.neuroimage.2004.03.038
   Moosmann M, 2003, NEUROIMAGE, V20, P145, DOI 10.1016/S1053-8119(03)00344-6
   Moosmann M, 2008, INT J PSYCHOPHYSIOL, V67, P212, DOI 10.1016/j.ijpsycho.2007.05.016
   Morup M, 2008, NEUROIMAGE, V42, P1439, DOI 10.1016/j.neuroimage.2008.05.062
   Murayama Y, 2010, MAGN RESON IMAGING, V28, P1095, DOI 10.1016/j.mri.2009.12.016
   OGAWA S, 1990, MAGN RESON MED, V14, P68, DOI 10.1002/mrm.1910140108
   Ostwald D, 2010, NEUROIMAGE, V49, P498, DOI 10.1016/j.neuroimage.2009.07.038
   Parra LC, 2005, NEUROIMAGE, V28, P326, DOI 10.1016/j.neuroimage.2005.05.032
   Pascual-Marqui RD, 2002, METHOD FIND EXP CLIN, V24, P5
   Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720
   Riera J, 2005, PHILOS T ROY SOC B, V360, P1025, DOI 10.1098/rstb.2005.1646
   Ritter P, 2009, HUM BRAIN MAPP, V30, P1168, DOI 10.1002/hbm.20585
   Rosa MJ, 2010, J INTEGR NEUROSCI, V9, P453, DOI 10.1142/S0219635210002512
   Salek-Haddadi A, 2003, BRAIN RES REV, V43, P110, DOI 10.1016/S0165-0173(03)00193-0
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Sun L., 2009, P INT JOINT C ART IN
   Takeuchi M, 2009, BRAIN TOPOGR, V22, P197, DOI 10.1007/s10548-009-0109-2
   Tsai PS, 2009, J NEUROSCI, V29, P14553, DOI 10.1523/JNEUROSCI.3287-09.2009
   Valdes-Sosa PA, 2009, HUM BRAIN MAPP, V30, P2701, DOI 10.1002/hbm.20704
   Yacoub E, 2006, J CEREBR BLOOD F MET, V26, P634, DOI 10.1038/sj.jcbfm.9600239
NR 62
TC 25
Z9 31
U1 0
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2013
VL 15
IS 5
BP 1001
EP 1013
DI 10.1109/TMM.2013.2250267
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222WT
UT WOS:000324763600004
DA 2024-07-18
ER

PT J
AU Su, Z
   Luo, XN
   Deng, ZJ
   Liang, Y
   Ji, Z
AF Su, Zhuo
   Luo, Xiaonan
   Deng, Zhengjie
   Liang, Yun
   Ji, Zhen
TI Edge-Preserving Texture Suppression Filter Based on Joint Filtering
   Schemes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Degenerative scheme; edge preserving; image smoothing; oscillation;
   texture suppression
ID AWARE IMAGE; ENHANCEMENT; DETAIL; DECOMPOSITION; PHOTOGRAPHY; FLASH
AB Obtaining a texture-smoothing and edge-preserving filtered output is significant to image decomposition. Although the edge and the texture have salient difference in human vision, automatically distinguishing them is a difficult task, for they have similar intensity difference or gradient response. The state-of-the-art edge-preserving smoothing (EPS) based decomposition approaches are hard to obtain a satisfactory result. We propose a novel edge-preserving texture suppression filter, exploiting the joint bilateral filter as a bridge to achieve the purpose of both properties of texture-smoothing and edge-preserving. We develop the iterative asymmetric sampling and the local linear model to produce the degenerative image to suppress the texture, and apply the edge correction operator to achieve edge-preserving. An efficient accelerating implementation is introduced to improve the performance of filtering response. The experiments demonstrate that our filter produces satisfactory outputs with both properties of texture-smoothing and edge-preserving, while compared with the results of other popular EPS approaches in signal, visual and time analysis. Finally, we extend our filter to a variety of image processing applications.
C1 [Su, Zhuo; Luo, Xiaonan] Sun Yat Sen Univ, Sch Informat Sci & Technol, State Prov Joint Lab Digital Home Interact Applic, Natl Engn Res Ctr Digital Life, Guangzhou 510275, Guangdong, Peoples R China.
   [Deng, Zhengjie] Hainan Normal Univ, Sch Informat Sci & Technol, Haikou, Peoples R China.
   [Liang, Yun] South China Agr Univ, Sch Informat, Guangzhou, Guangdong, Peoples R China.
   [Ji, Zhen] Shenzhen Univ, Dept Comp Sci, Coll Comp Sci & Software Engn, Shenzhen, Peoples R China.
C3 Sun Yat Sen University; Hainan Normal University; South China
   Agricultural University; Shenzhen University
RP Su, Z (corresponding author), Sun Yat Sen Univ, Sch Informat Sci & Technol, State Prov Joint Lab Digital Home Interact Applic, Natl Engn Res Ctr Digital Life, Guangzhou 510275, Guangdong, Peoples R China.
EM suzhuoi@gmail.com; lnslxn@mail.sysu.edu.cn; hsdengzj@163.com;
   sdliangyun@163.com; jizhen@szu.edu.cn
RI Su, Zhuo/AAO-4506-2020; Deng, Zhengjie/JCO-9393-2023
OI Su, Zhuo/0000-0002-6090-0110; Deng, Zhengjie/0000-0002-1936-0714
FU NSFC-Guangdong Joint Fund [U0935004, U1135003]; National Key Basic
   Research and Development Program of China 973 [2013CB329505]; National
   Key Technology RD Program [2011BAH27B01]; National Science Fund of China
   [61232011, 61262050, 61202293]; Ministry of Education
FX This work was supported by the NSFC-Guangdong Joint Fund (U0935004,
   U1135003), the National Key Basic Research and Development Program of
   China 973 (2013CB329505), the National Key Technology R&D Program
   (2011BAH27B01), the National Science Fund of China (61232011, 61262050,
   61202293), and the Scholarship Award for Excellent Doctoral Student
   granted by the Ministry of Education 2012. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Eckehard G. Steinbach.
CR Adams A, 2010, COMPUT GRAPH FORUM, V29, P753, DOI 10.1111/j.1467-8659.2009.01645.x
   Adams A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531327
   [Anonymous], P SIGGRAPH AS
   [Anonymous], ACM T GRAPH
   [Anonymous], 2005, P IEEE INT C MULT EX
   Aujol JF, 2006, INT J COMPUT VISION, V67, P111, DOI 10.1007/s11263-006-4331-z
   Baek J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866191
   Bhat P, 2008, LECT NOTES COMPUT SC, V5303, P114, DOI 10.1007/978-3-540-88688-4_9
   Bhat P, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731048
   Chao SM, 2010, IEEE IMAGE PROC, P4145, DOI 10.1109/ICIP.2010.5653571
   Chao SM, 2010, PATTERN RECOGN LETT, V31, P2012, DOI 10.1016/j.patrec.2010.06.004
   Choudhury P., 2003, Eurographics Symposium on Rendering. 14th Eurographics Workshop on Rendering, P186
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778
   Farbman Z, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866171
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fattal R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531328
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239502, 10.1145/1276377.1276441]
   Felsberg M, 2006, IEEE T PATTERN ANAL, V28, P209, DOI 10.1109/TPAMI.2006.29
   Gastal ESL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185529
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   Gonzalez R C, 2008, DIGITAL IMAGE PROCES
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Kass M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778837
   LEV A, 1977, IEEE T SYST MAN CYB, V7, P435, DOI 10.1109/TSMC.1977.4309740
   Paris S, 2006, LECT NOTES COMPUT SC, V3954, P568
   Paris S, 2008, FOUND TRENDS COMPUT, V4, P1, DOI 10.1561/0600000020
   Paris S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964963
   Paris S, 2009, INT J COMPUT VISION, V81, P24, DOI 10.1007/s11263-007-0110-8
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Porikli F., 2008, PROC IEEE C COMPUT V, P1
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Subr K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618493
   Takeda H, 2007, IEEE T IMAGE PROCESS, V16, P349, DOI 10.1109/TIP.2006.888330
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   van de Weijer J, 2001, PROC CVPR IEEE, P428
   Wang YL, 2008, SIAM J IMAGING SCI, V1, P248, DOI 10.1137/080724265
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weickert J., 1998, ANISOTROPIC DIFFUSIO, V1
   Weiss B, 2006, ACM T GRAPHIC, V25, P519, DOI 10.1145/1141911.1141918
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Yang QX, 2012, LECT NOTES COMPUT SC, V7572, P399, DOI 10.1007/978-3-642-33718-5_29
   Yang QX, 2010, PROC CVPR IEEE, P1775, DOI 10.1109/CVPR.2010.5539847
   Yang QX, 2009, PROC CVPR IEEE, P557, DOI 10.1109/CVPRW.2009.5206542
   Yoshizawa S, 2010, COMPUT GRAPH FORUM, V29, P60, DOI 10.1111/j.1467-8659.2009.01544.x
NR 46
TC 50
Z9 56
U1 2
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2013
VL 15
IS 3
BP 535
EP 548
DI 10.1109/TMM.2012.2237025
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109ZM
UT WOS:000316410900006
DA 2024-07-18
ER

PT J
AU Wang, Z
   Sun, LF
   Zhu, WW
   Yang, SQ
   Li, HZ
   Wu, DP
AF Wang, Zhi
   Sun, Lifeng
   Zhu, Wenwu
   Yang, Shiqiang
   Li, Hongzhi
   Wu, Dapeng
TI Joint Social and Content Recommendation for User-Generated Videos in
   Online Social Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video recommendation; online social network; social propagation
ID MODEL
AB Online social network is emerging as a promising alternative for users to directly access video contents. By allowing users to import videos and re-share them through the social connections, a large number of videos are available to users in the online social network. The rapid growth of the user-generated videos provides enormous potential for users to find the ones that interest them; while the convergence of online social network service and online video sharing service makes it possible to perform recommendation using social factors and content factors jointly. In this paper, we design a joint social-content recommendation framework to suggest users which videos to import or re-share in the online social network. In this framework, we first propose a user-content matrix update approach which updates and fills in cold user-video entries to provide the foundations for the recommendation. Then, based on the updated user-content matrix, we construct a joint social-content space to measure the relevance between users and videos, which can provide a high accuracy for video importing and re-sharing recommendation. We conduct experiments using real traces from Tencent Weibo and Youku to verify our algorithm and evaluate its performance. The results demonstrate the effectiveness of our approach and show that our approach can substantially improve the recommendation accuracy.
C1 [Wang, Zhi; Sun, Lifeng; Zhu, Wenwu; Yang, Shiqiang] Tsinghua Univ, Dept Comp Sci & Technol, Beijing Key Lab Networked Multimedia, Beijing 100084, Peoples R China.
   [Li, Hongzhi] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.
   [Wu, Dapeng] Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA.
C3 Tsinghua University; Columbia University; State University System of
   Florida; University of Florida
RP Wang, Z (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing Key Lab Networked Multimedia, Beijing 100084, Peoples R China.
EM wangzhi04@mails.tsinghua.edu.cn; sunlf@tsinghua.edu.cn;
   wwzhu@tsinghua.edu.cn; yangshq@tsinghua.edu.cn;
   hongzhili@cs.columbia.edu; wu@ece.ufl.edu
RI yang, shiqiang/AAH-5484-2019; Wang, Zhi/H-2753-2013
OI Wang, Zhi/0000-0001-8893-5852; Wu, Dapeng/0000-0003-1755-0183
FU National Basic Research Program of China (973) [2011CB302206]; National
   Natural Science Foundation of China [60933013, 61272231]; National
   Significant Science and Technology Projects of China
   [2012ZX01039001-003]; Tsinghua National Laboratory for Information
   Science and Technology [916-042003091]; Tsinghua-Tencent Joint
   Laboratory for Internet Innovation Technology; Joint Research Fund for
   Overseas Chinese Young Scholars [61228101]
FX This work was supported in part by the National Basic Research Program
   of China (973) under Grant No. 2011CB302206, the National Natural
   Science Foundation of China under Grant No. 60933013 and 61272231, the
   National Significant Science and Technology Projects of China under
   Grant No. 2012ZX01039001-003, the Tsinghua National Laboratory for
   Information Science and Technology under Grant No. 916-042003091, the
   research fund of Tsinghua-Tencent Joint Laboratory for Internet
   Innovation Technology, and the Joint Research Fund for Overseas Chinese
   Young Scholars under Grant No. 61228101. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Marie-Jose Montpetit.
CR [Anonymous], 2010, P ACM IMC
   [Anonymous], 2003, P 9 ACM SIGKDD INT C
   Balachander Krishnamurthy M. A., 2008, P ACM WOSN
   Baluja S., 2008, P ACM WWW
   Basilico J., 2004, P ACM ICML
   Basu C., 1998, P NAT C ART INT
   Benevenuto F., 2009, P ACM IMC
   COPPENS T, 2004, P EUROITV
   Davidson J., 2010, P ACM RECSYS
   Debnath S., 2008, P ACM WWW
   Dodds PS, 2005, J THEOR BIOL, V232, P587, DOI 10.1016/j.jtbi.2004.09.006
   Domingos P., 2001, P ACM SIGKDD
   HARTLINE J., 2008, P ACM WWW
   Herlocker J. L., 2000, CSCW 2000. ACM 2000 Conference on Computer Supported Cooperative Work, P241, DOI 10.1145/358916.358995
   Herlocker JL, 2004, ACM T INFORM SYST, V22, P5, DOI 10.1145/963770.963772
   Isaacman S., 2011, P ACM RECSYS
   Johnson C.R., 1990, Matrix Theory and Applications, V40, P171
   Kwak H., 2010, P ACM WWW
   Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344
   MELVILLE P., 2002, P NAT C ART INT
   MISLOVE A, 2007, P ACM IMC
   Oehlberg L, 2006, P EUROITV
   Pazzani M. J., 2007, The Adaptive Web. Methods and Strategies of Web Personalization, P325
   Pazzani MJ, 1999, ARTIF INTELL REV, V13, P393, DOI 10.1023/A:1006544522159
   Sarwar B., 2001, PROC ACM INT WORLD W, P285
   Schafer J. B., 2007, The Adaptive Web. Methods and Strategies of Web Personalization, P291
   Schatz R., 2007, P 29 IEEE INT C INF
   Walter FE, 2008, AUTON AGENT MULTI-AG, V16, P57, DOI 10.1007/s10458-007-9021-x
   Wang X., 2012, P IEEE ICME
   Wang Z., 2011, P ACM MULT
   Wasko MM, 2005, MIS QUART, V29, P35, DOI 10.2307/25148667
   Xu R, 2005, IEEE T NEURAL NETWOR, V16, P645, DOI 10.1109/TNN.2005.845141
   [No title captured]
NR 33
TC 78
Z9 95
U1 1
U2 101
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2013
VL 15
IS 3
BP 698
EP 709
DI 10.1109/TMM.2012.2237022
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109ZM
UT WOS:000316410900019
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Izadinia, H
   Saleemi, I
   Shah, M
AF Izadinia, Hamid
   Saleemi, Imran
   Shah, Mubarak
TI Multimodal Analysis for Identification and Segmentation of
   Moving-Sounding Objects
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio-visual analysis; audio-visual synchronization; canonical
   correlation analysis; video segmentation
ID CANONICAL CORRELATION-ANALYSIS; FUSION; LOCALIZATION; TRACKING
AB In this paper, we propose a novel method that exploits correlation between audio-visual dynamics of a video to segment and localize objects that are the dominant source of audio. Our approach consists of a two-step spatiotemporal segmentation mechanism that relies on velocity and acceleration of moving objects as visual features. Each frame of the video is segmented into regions based on motion and appearance cues using the QuickShift algorithm, which are then clustered over time using K-means, so as to obtain a spatiotemporal video segmentation. The video is represented by motion features computed over individual segments. The Mel-Frequency Cepstral Coefficients (MFCC) of the audio signal, and their first order derivatives are exploited to represent audio. The proposed framework assumes there is a non-trivial correlation between these audio features and the velocity and acceleration of the moving and sounding objects. The canonical correlation analysis (CCA) is utilized to identify the moving objects which are most correlated to the audio signal. In addition to moving-sounding object identification, the same framework is also exploited to solve the problem of audio-video synchronization, and is used to aid interactive segmentation. We evaluate the performance of our proposed method on challenging videos. Our experiments demonstrate significant increase in performance over the state-of-the-art both qualitatively and quantitatively, and validate the feasibility and superiority of our approach.
C1 [Izadinia, Hamid; Saleemi, Imran; Shah, Mubarak] Univ Cent Florida, Dept Elect Engn & Comp Sci, Comp Vis Lab, Orlando, FL 32816 USA.
C3 State University System of Florida; University of Central Florida
RP Izadinia, H (corresponding author), Univ Cent Florida, Dept Elect Engn & Comp Sci, Comp Vis Lab, Orlando, FL 32816 USA.
EM izadinia@eecs.ucf.edu; imran@eecs.ucf.edu; shah@eecs.ucf.edu
RI Sahoo, Sarat Kumar/J-8765-2014
OI Sahoo, Sarat Kumar/0000-0001-5734-6844; Shah,
   Mubarak/0000-0001-6172-5572
FU Intelligence Advanced Research Projects Activity (IARPA) via Department
   of Interior National Business Center [D11PC20071]
FX Manuscript received February 08, 2012; revised June 13, 2012; accepted
   June 20, 2012. Date of publication November 20, 2012; date of current
   version January 15, 2013. This work was supported by the Intelligence
   Advanced Research Projects Activity (IARPA) via Department of Interior
   National Business Center contract number D11PC20071. The U.S. Government
   is authorized to reproduce and distribute reprints for Governmental
   purposes notwithstanding any copyright annotation thereon. The views and
   conclusions contained herein are those of the authors and should not be
   interpreted as necessarily representing the official policies or
   endorsements, either expressed or implied, of IARPA, DoI/NBC, or the
   U.S. Government. The associate editor coordinating the review of this
   manuscript and approving it for publication was Chong-Wah Ngo.
CR [Anonymous], 2008, P ECCV
   [Anonymous], P ACM MULT
   Barzelay Z, 2010, IEEE T MULTIMEDIA, V12, P108, DOI 10.1109/TMM.2009.2037387
   Beal MJ, 2003, IEEE T PATTERN ANAL, V25, P828, DOI 10.1109/TPAMI.2003.1206512
   Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143
   Chen SSB, 2001, SIAM REV, V43, P129, DOI [10.1137/S003614450037906X, 10.1137/S1064827596304010]
   Cutler R., 2000, P ICME
   Darrell T., 2000, P ICMI
   Fisher J., 2000, P NIPS
   Fisher JW, 2004, IEEE T MULTIMEDIA, V6, P406, DOI 10.1109/TMM.2004.827503
   Friedland G., 2009, P ACM MULT
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hong R., 2010, P ACM MULT
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   KIDRON E, 2005, P CVPR
   Kidron E, 2007, IEEE T SIGNAL PROCES, V55, P1390, DOI 10.1109/TSP.2006.888095
   Kim TK, 2007, IEEE T PATTERN ANAL, V29, P1005, DOI 10.1109/TPAMI.2007.1037
   Kim TK, 2009, IEEE T PATTERN ANAL, V31, P1415, DOI 10.1109/TPAMI.2008.167
   Koprinska I, 2001, SIGNAL PROCESS-IMAGE, V16, P477, DOI 10.1016/S0923-5965(00)00011-4
   Li D., 2003, P ACM MULT
   Liu Y., 2009, P ACM MULT
   Loy CC, 2009, P CVPR
   Monaci G., 2006, P CVPR WORKSH
   Murphy D., 2003, P GEST WORKSH
   Nakadai K., 2002, P ICRA
   NOCK HJ, 2002, P ACM MULT
   ODonovan A., 2007, P CVPR
   Pérez P, 2004, P IEEE, V92, P495, DOI 10.1109/JPROC.2003.823147
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   Sargin ME, 2007, IEEE T MULTIMEDIA, V9, P1396, DOI 10.1109/TMM.2007.906583
   SLANEY M, 2001, P NIPS
   Vajaria H, 2008, IEEE T CIRC SYST VID, V18, P1608, DOI 10.1109/TCSVT.2008.2005602
NR 32
TC 53
Z9 64
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2013
VL 15
IS 2
BP 378
EP 390
DI 10.1109/TMM.2012.2228476
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 075HN
UT WOS:000313875500013
DA 2024-07-18
ER

PT J
AU Shen, LQ
   Liu, Z
   Zhang, XP
   Zhao, WQ
   Zhang, ZY
AF Shen, Liquan
   Liu, Zhi
   Zhang, Xinpeng
   Zhao, Wenqiang
   Zhang, Zhaoyang
TI An Effective CU Size Decision Method for HEVC Encoders
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE CU size decision; HEVC; motion estimation
AB The emerging high efficiency video coding standard (HEVC) adopts the quadtree-structured coding unit (CU). Each CU allows recursive splitting into four equal sub-CUs. At each depth level (CU size), the test model of HEVC (HM) performs motion estimation (ME) with different sizes including 2N x 2N, 2N x N, N x 2N, and N x N. ME process in HM is performed using all the possible depth levels and prediction modes to find the one with the least rate distortion (RD) cost using Lagrange multiplier. This achieves the highest coding efficiency but requires a very high computational complexity. In this paper, we propose a fast CU size decision algorithm for HM. Since the optimal depth level is highly content-dependent, it is not efficient to use all levels. We can determine CU depth range (including the minimum depth level and the maximum depth level) and skip some specific depth levels rarely used in the previous frame and neighboring CUs. Besides, the proposed algorithm also introduces early termination methods based on motion homogeneity checking, RD cost checking and SKIP mode checking to skip ME on unnecessary CU sizes. Experimental results demonstrate that the proposed algorithm can significantly reduce computational complexity while maintaining almost the same RD performance as the original HEVC encoder.
C1 [Shen, Liquan; Liu, Zhi; Zhao, Wenqiang; Zhang, Zhaoyang] Shanghai Univ, Minist Educ, Key Lab Adv Display & Syst Applicat, Shanghai 200072, Peoples R China.
   [Zhang, Xinpeng] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200072, Peoples R China.
C3 Shanghai University; Shanghai University
RP Shen, LQ (corresponding author), Shanghai Univ, Minist Educ, Key Lab Adv Display & Syst Applicat, Shanghai 200072, Peoples R China.
EM jsslq@163.com; zhiliu_sjtu@shu.edu.cn; xzhang@shu.edu.cn;
   zhaowen-qiangfly@163.com; zhyzhang@shu.edu.cn
RI Shen, Liquan/D-4832-2012; LIU, Zhi/D-4518-2012; Zhang,
   Zhaoyang/AFQ-9161-2022
OI LIU, Zhi/0000-0002-8428-1131; 
FU Shanghai Municipal Education Commission [11QA1402400, 13ZZ069]; National
   Natural Science Foundation of China [60832003, 60902085, 61171084]
FX Manuscript received November 07, 2011; revised February 23, 2012 and May
   05, 2012; accepted July 23, 2012. Date of publication November 30, 2012;
   date of current version January 15, 2013. This work is sponsored by
   Shanghai Rising-Star Program (11QA1402400) and Innovation Program of
   Shanghai Municipal Education Commission (13ZZ069), and is supported by
   the National Natural Science Foundation of China under grant No.
   60832003, 60902085, and 61171084. The associate editor coordinating the
   review of this manuscript and approving it for publication was Charles
   D. (Chuck) Creusere.
CR [Anonymous], 2011, 5 M GEN SWITZ APR, P16
   [Anonymous], 2010, 91 MPEG M KYOT JAP A
   [Anonymous], 2011, 6 M TOR IT JUL, P14
   [Anonymous], 2011, 4 M DAEG KOR JAN, P20
   [Anonymous], 2010, M M JOINT COLL TEAM
   Bjontegaard G., 2001, ITU T SG16 Q 6 DOCUM
   Chao YC, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P333, DOI 10.1109/ICME.2008.4607439
   Chiang CK, 2011, IEEE T CIRC SYST VID, V21, P1304, DOI 10.1109/TCSVT.2011.2147250
   Han WJ, 2010, IEEE T CIRC SYST VID, V20, P1709, DOI 10.1109/TCSVT.2010.2092612
   Jie Leng, 2011, 2011 International Conference on Multimedia and Signal Processing (CMSP), P56, DOI 10.1109/CMSP.2011.167
   Kannangara CS, 2009, IEEE T MULTIMEDIA, V11, P433, DOI 10.1109/TMM.2009.2012937
   Lee B, 2011, IEEE SIGNAL PROC LET, V18, P571, DOI 10.1109/LSP.2011.2163935
   Ostermann J., 2004, IEEE Circuits and Systems Magazine, V4, P7, DOI 10.1109/MCAS.2004.1286980
   Paul M, 2011, IEEE T IMAGE PROCESS, V20, P461, DOI 10.1109/TIP.2010.2063436
   Ri SH, 2009, IEEE T CIRC SYST VID, V19, P302, DOI 10.1109/TCSVT.2008.2009257
   Shen LQ, 2008, IEEE T MULTIMEDIA, V10, P1208, DOI 10.1109/TMM.2008.2001358
   Shen LQ, 2010, IEEE SIGNAL PROC LET, V17, P887, DOI 10.1109/LSP.2010.2066966
   Sullivan J., 2010, P SOC PHOTO-OPT INS, V7798, P7798
   Wang HL, 2007, IEEE T MULTIMEDIA, V9, P882, DOI 10.1109/TMM.2007.893345
NR 19
TC 345
Z9 386
U1 0
U2 112
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2013
VL 15
IS 2
BP 465
EP 470
DI 10.1109/TMM.2012.2231060
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 075HN
UT WOS:000313875500020
DA 2024-07-18
ER

PT J
AU Lai, KF
   Wang, D
AF Lai, Kunfeng
   Wang, Dan
TI Understanding the External Links of Video Sharing Sites: Measurement and
   Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video sharing; YouTube; Youku; External links
AB Recently, many video sharing sites provide external links so that their video or audio contents can be embedded into external web sites. For example, users can copy the embedded URLs of the videos of YouTube and post the URL links on their own blogs. Clearly, the purpose of such function is to increase the distribution of the videos and the associated advertisement. Does this function fulfill its purpose and what is the quantification? In this paper, we provide a comprehensive measurement study and analysis on these external links to answer these two questions. With the traces collected from two major video sharing sites, YouTube and Youku of China, we show that the external links have various impacts on the popularity of the video sharing sites. More specifically, for videos that have been uploaded for eight months in Youku, around 15% of views can come from external links. Some contents are densely linked. For example, comedy videos can attract more than 800 external links on average. We also study the relationship between the external links and the internal links. We show that there are correlations; for example, if a video is popular itself, it is likely to have a large number of external links. Another observation we find is that the external links usually have a higher impact on Youku than that of YouTube. We conjecture that it is more likely that the external links have higher impact for a regional site than a worldwide site.
C1 [Lai, Kunfeng; Wang, Dan] Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China.
C3 Hong Kong Polytechnic University
RP Lai, KF (corresponding author), Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China.
EM cskflai@comp.polyu.edu.hk; csdwang@comp.polyu.edu.hk
RI Wang, Daiwei/ABR-1241-2022
OI Wang, Daiwei/0000-0003-2811-1071; WANG, Dan/0000-0002-0921-2726
FU  [PolyU/APJ19];  [A-PK95];  [A-PL23];  [A-PL86]
FX The work of D. Wang was supported by Hong Kong PolyU/APJ19, A-PK95,
   A-PL23, and A-PL86. A preliminary version of this paper appeared in
   Proc. ACM NOSSDAV'10, Amsterdam, the Netherlands, June 2010. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Jiangchuan Liu.
CR Adhikari V., 2012, P IEEE INF 12 ORL FL
   Borghol Y, 2011, PERFORM EVALUATION, V68, P1037, DOI 10.1016/j.peva.2011.07.008
   Brodersen A., 2011, P ACM WWW 11 LYON FR
   Cha M., 2007, P ACM IMC 07 SAN DIE
   Chen W., 2009, P ACM WWW 09 MADR SP
   Cheng X., 2008, P IEEE IWQOS 08 ENSC
   Cheng X., 2009, P IEEE INF 09 RIO JA
   Clauset A, 2009, SIAM REV, V51, P661, DOI 10.1137/070710111
   Ding Y., 2011, P ACM IMC 11 BERL GE
   Figueiredo F., 2011, P ACM WSDM 11 HONG K
   Finamore A., 2011, P ACM IMC 11 BERL GE
   Fuxman A., 2008, P ACM WWW 08 BEIJ CH
   Gill P., 2007, P ACM IMC 07 SAN DIE, P15
   Gill P., 2008, P SPIE MULT COMP NET
   Jiang J., 2010, P ACM IMC 10 MELB AU
   Lai K., 2010, P ACM NOSSDAV 10 AMS
   Lai K., 2009, P IEEE GLOB 09 HON H
   Leskovec J., 2010, P ACM WWW 10 RAL NC
   Leskovec J., 2008, P ACM WWW 08 BEIJ CH
   Mislove A., 2007, P ACM IMC 07 SAN DIE
   Mitra S, 2011, ACM T WEB, V5, DOI 10.1145/1961659.1961662
   Ribeiro B., 2010, P ACM IMC 10 MELB AU
   Scellato S., 2011, P ACM WWW 11 HYD IND
   Szabo G, 2010, COMMUN ACM, V53, P80, DOI 10.1145/1787234.1787254
   Tang J., 2009, P ACM SIGCOMM WOSN 0
   Torkjazi M., 2009, P ACM SIGCOMM WOSN 0
   Valafar M., 2009, P ACM SIGCOMM WOSN 0
   Viswanath B., 2009, P ACM SIGCOMM WOSN 0
   Xie L., 2011, P ACM MM 11 SCOTTSD
   Zhang J., 2011, P ACM SBNMA 11 SCOTT
   Zhou J., 2011, P ACM IMC 11 BERL GE
   Zhou R., 2010, P ACM IMC 10 MELB AU
NR 32
TC 3
Z9 5
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2013
VL 15
IS 1
BP 224
EP 235
DI 10.1109/TMM.2012.2225030
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 058PS
UT WOS:000312646600018
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Fan, JP
   He, XF
   Zhou, N
   Peng, JY
   Jain, R
AF Fan, Jianping
   He, Xiaofei
   Zhou, Ning
   Peng, Jinye
   Jain, Ramesh
TI Quantitative Characterization of Semantic Gaps for Learning Complexity
   Estimation and Inference Model Selection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Concept classifier training; inference model selection; inner-concept
   visual homogeneity score; inter-concept discrimination complexity score;
   learning complexity estimation; quantitative characterization of
   semantic gaps
ID CONCEPT ONTOLOGY; RETRIEVAL
AB In this paper, a novel data-driven algorithm is developed for achieving quantitative characterization of the semantic gaps directly in the visual feature space, where the visual feature space is the common space for concept classifier training and automatic concept detection. By supporting quantitative characterization of the semantic gaps, more effective inference models can automatically be selected for concept classifier training by: 1) identifying the image concepts with small semantic gaps (i.e., the isolated image concepts with high inner-concept visual consistency) and training their one-against-all SVM concept classifiers independently; 2) determining the image concepts with large semantic gaps (i.e., the visually-related image concepts with low inner-concept visual consistency) and training their inter-related SVM concept classifiers jointly; and 3) using more image instances to achieve more reliable training of the concept classifiers for the image concepts with large semantic gaps. Our experimental results on NUS-WIDE [18] and ImageNet [11] image sets have obtained very promising results.
C1 [Fan, Jianping; Peng, Jinye] Northwest Univ, Sch Informat Sci & Technol, Xian 710069, Peoples R China.
   [Fan, Jianping; Zhou, Ning] Univ N Carolina, Dept Comp Sci, Charlotte, NC 28223 USA.
   [He, Xiaofei] Zhejiang Univ, Key Lab CAD & CG, Hangzhou, Zhejiang, Peoples R China.
   [Jain, Ramesh] Univ Calif Irvine, Sch Informat & Comp Sci, Irvine, CA 92697 USA.
C3 Northwest University Xi'an; University of North Carolina; University of
   North Carolina Charlotte; Zhejiang University; University of California
   System; University of California Irvine
RP Fan, JP (corresponding author), Northwest Univ, Sch Informat Sci & Technol, Xian 710069, Peoples R China.
EM jfan@uncc.edu; xiaofeihe@cad.zju.edu.cn; nzhou@uncc.edu; pjy@nwu.edu.cn;
   jain@ics.uci.edu
RI Peng, Jin/HZH-6965-2023
FU National Natural Science Foundation of China (NSFC) [61125203,
   61075014]; Doctoral Program of Higher Education of China
   [20096102110025, 20116102110027]; Program for New Century Excellent
   Talents in University [NCET-10-0071]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC Grants 61125203 and 61075014), Doctoral
   Program of Higher Education of China (Grant No. 20096102110025,
   20116102110027), and Program for New Century Excellent Talents in
   University under NCET-10-0071.
CR Ahuja N., 2007, P ICCV
   [Anonymous], 2009, P IEEE CVPR
   [Anonymous], P ACM CIVR
   Bart E., 2008, P IEEE CVPR, P1
   Blaschko MB, 2008, LECT NOTES COMPUT SC, V5302, P2, DOI 10.1007/978-3-540-88682-2_2
   Deselaers T., 2011, P IEEE CVPR
   Dorai C, 2003, IEEE MULTIMEDIA, V10, P15, DOI 10.1109/MMUL.2003.1195157
   Enser P., 2003, P ACM CIVR, P163
   Evgeniou T, 2005, J MACH LEARN RES, V6, P615
   Fan J., 2010, P IEEE CVPR
   Fan JP, 2008, IEEE T IMAGE PROCESS, V17, P407, DOI 10.1109/TIP.2008.916999
   Fan JP, 2008, IEEE T MULTIMEDIA, V10, P167, DOI 10.1109/TMM.2007.911775
   Fellbaum C., 1998, WORDNET ELECT LEXICA, DOI DOI 10.7551/MITPRESS/7287.001.0001
   Griffin G., 2009, P IEEE CVPR
   Hare J., 2006, P 3 EUR SEM WEB C
   Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150
   Hauptmann Alexander., 2007, CIVR 07, P627
   Li J, 2010, IEEE INT C SOL DIEL
   Lu Y., 2009, P IEEE CVPR
   Lu YJ, 2010, IEEE T MULTIMEDIA, V12, P288, DOI 10.1109/TMM.2010.2046292
   Ma H, 2010, IEEE T MULTIMEDIA, V12, P462, DOI 10.1109/TMM.2010.2051360
   Marszalek M., 2007, P IEEE CVPR
   Marszalek M., 2008, P ECCV HER CRET GREE
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Natsev A.P., 2004, PROC 10 ACM SIGKDD I, P641
   Rasiwasia N, 2007, IEEE T MULTIMEDIA, V9, P923, DOI 10.1109/TMM.2007.900138
   Santini S, 2001, IEEE T KNOWL DATA EN, V13, P337, DOI 10.1109/69.929893
   Schreiber AT, 2001, IEEE INTELL SYST APP, V16, P66, DOI 10.1109/5254.940028
   Sivic J., 2008, P IEEE CVPR
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Snoek CGM, 2006, IEEE T PATTERN ANAL, V28, P1678, DOI 10.1109/TPAMI.2006.212
   Torralba A, 2007, IEEE T PATTERN ANAL, V29, P854, DOI 10.1109/TPAMI.2007.1055
   Wang C., 2008, ACM SIGIR, P355, DOI DOI 10.1145/1390334.1390396
   Wu L., 2008, P ACM MULT
   Zhao R, 2002, IEEE T MULTIMEDIA, V4, P189, DOI 10.1109/TMM.2002.1017733
NR 35
TC 26
Z9 26
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2012
VL 14
IS 5
BP 1414
EP 1428
DI 10.1109/TMM.2012.2197604
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 008XT
UT WOS:000308990600004
DA 2024-07-18
ER

PT J
AU Li, XR
   Snoek, CGM
   Worring, M
   Smeulders, AWM
AF Li, Xirong
   Snoek, Cees G. M.
   Worring, Marcel
   Smeulders, Arnold W. M.
TI Harvesting Social Images for Bi-Concept Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bi-concept; semantic index; visual search
ID ANNOTATION; DETECTORS; RETRIEVAL; SCENE; GAP
AB Searching for the co-occurrence of two visual concepts in unlabeled images is an important step towards answering complex user queries. Traditional visual search methods use combinations of the confidence scores of individual concept detectors to tackle such queries. In this paper we introduce the notion of bi-concepts, a new concept-based retrieval method that is directly learned from social-tagged images. As the number of potential bi-concepts is gigantic, manually collecting training examples is infeasible. Instead, we propose a multimedia framework to collect de-noised positive as well as informative negative training examples from the social web, to learn bi-concept detectors from these examples, and to apply them in a search engine for retrieving bi-concepts in unlabeled images. We study the behavior of our bi-concept search engine using 1.2 M social-tagged images as a data source. Our experiments indicate that harvesting examples for bi-concepts differs from traditional single-concept methods, yet the examples can be collected with high accuracy using a multi-modal approach. We find that directly learning bi-concepts is better than oracle linear fusion of single-concept detectors, with a relative improvement of 100%. This study reveals the potential of learning high-order semantics from social images, for free, suggesting promising new lines of research.
C1 [Li, Xirong] Renmin Univ China, MOE Key Lab Data Engn & Knowledge Engn, Sch Informat, Beijing, Peoples R China.
   [Snoek, Cees G. M.; Worring, Marcel; Smeulders, Arnold W. M.] Univ Amsterdam, Intelligent Syst Lab Amsterdam, NL-1098 XH Amsterdam, Netherlands.
C3 Renmin University of China; University of Amsterdam
RP Li, XR (corresponding author), Renmin Univ China, MOE Key Lab Data Engn & Knowledge Engn, Sch Informat, Beijing, Peoples R China.
EM xirong.li@gmail.com
RI Worring, Marcel/JRW-7059-2023; Li, Xirong/AAD-3347-2019
OI Li, Xirong/0000-0002-0220-8310; Worring, Marcel/0000-0003-4097-4136;
   Snoek, Cees/0000-0001-9092-1556
FU Dutch national program COMMIT; STW SEARCHER project
FX This work was supported in part by the Dutch national program COMMIT and
   in part by the STW SEARCHER project. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Samson Cheung.
CR Allan M., 2009, P BMVC
   Aly R., 2008, P CIVR
   [Anonymous], P CIVR
   [Anonymous], P ICCV
   [Anonymous], 2011, P CVPR
   [Anonymous], P ACM MULT
   [Anonymous], P CVPR
   [Anonymous], P TRECVID WORKSH
   Aslam Javed A., 2001, P SIGIR
   Cilibrasi RL, 2007, IEEE T KNOWL DATA EN, V19, P370, DOI 10.1109/TKDE.2007.48
   Datta R, 2007, IEEE MULTIMEDIA, V14, P24, DOI 10.1109/MMUL.2007.67
   Douze M., 2009, P CIVR
   Fellbaum C., 1998, WORDNET ELECT LEXICA, DOI DOI 10.7551/MITPRESS/7287.001.0001
   Guillaumin M., 2010, P CVPR
   Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150
   Heikkilä M, 2009, PATTERN RECOGN, V42, P425, DOI 10.1016/j.patcog.2008.08.014
   HUANG J, 1997, P CVPR
   Huiskes M.J., 2010, P ACM MIR
   Japkowicz N., 2002, Intelligent Data Analysis, V6, P429
   Kennedy L., 2006, P MIR
   Li J, 2008, IEEE T PATTERN ANAL, V30, P985, DOI 10.1109/TPAMI.2007.70847
   Li LJ, 2010, INT J COMPUT VISION, V88, P147, DOI 10.1007/s11263-009-0265-6
   Li X., 2007, P ACM CIVR
   Li X., 2010, P CIVR
   Li X., 2011, P ICMR
   Lin HT, 2007, MACH LEARN, V68, P267, DOI 10.1007/s10994-007-5018-6
   Liu D, 2010, P ACM MULT
   Liu YM, 2011, IEEE T PATTERN ANAL, V33, P1022, DOI 10.1109/TPAMI.2010.142
   Mihalcea Rada, 2006, P AAAI
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Natsev A., 2007, P ACM MULT
   NATSEV AP, 2005, P ACM MULT
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Sawant N, 2011, MULTIMED TOOLS APPL, V51, P213, DOI 10.1007/s11042-010-0650-8
   Schroff F, 2011, IEEE T PATTERN ANAL, V33, P754, DOI 10.1109/TPAMI.2010.133
   Shen Y., 2010, P ACM MULT
   SMEATON AF, 2006, P ACM MIR
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Snoek CGM, 2007, IEEE T MULTIMEDIA, V9, P975, DOI 10.1109/TMM.2007.900156
   Snoek CGM, 2010, COMPUTER, V43, P76, DOI 10.1109/MC.2010.183
   Sonnenburg S, 2010, J MACH LEARN RES, V11, P1799
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Ulges A, 2010, COMPUT VIS IMAGE UND, V114, P429, DOI 10.1016/j.cviu.2009.08.002
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   Vapnik VN, 2000, NATURE STAT LEARNING, DOI DOI 10.1007/978-1-4757-3264-1
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Wei XY, 2011, IEEE T CIRC SYST VID, V21, P62, DOI 10.1109/TCSVT.2011.2105597
   YAN R, 2003, P ACM MULT
   Yanai K., 2005, P ACM MIR
   Yang KY, 2011, IEEE T MULTIMEDIA, V13, P662, DOI 10.1109/TMM.2011.2147777
   Yu H., 2002, P ICIP
   Yuan J, 2011, IEEE T MULTIMEDIA, V13, P1343, DOI 10.1109/TMM.2011.2168813
   ZHANG L, 2006, P ACM MULT
   Zhu G., 2010, P ACM MULT
   Zhu S., 2010, P CIVR
NR 56
TC 21
Z9 24
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
SI SI
BP 1091
EP 1104
DI 10.1109/TMM.2012.2191943
PN 1
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QL
UT WOS:000306599300014
DA 2024-07-18
ER

PT J
AU Yan, B
   Zhou, J
AF Yan, Bo
   Zhou, Jie
TI Efficient Frame Concealment for Depth Image-Based 3-D Video Transmission
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE H.264/AVC; video error concealment; whole-frame losses; 3-D video
   transmission
ID ERROR CONCEALMENT; ALGORITHM
AB In depth image-based 3-D video transmission, the compressed video stream is very likely to be corrupted by channel errors. Due to the high compression ratio of H. 264/AVC, it is often common that an entire coded picture is packetized into one packet. Thus the loss of a packet may result in the loss of the whole video frame. Currently, most of the frame concealment methods are mainly for 2-D video transmission. In this paper, we have proposed an efficient frame concealment algorithm for depth image-based 3-D video transmission, which is able to provide accurate estimation for the motion vectors of the lost frame with the help of the depth information. Simulation results show that it is highly effective and significantly outperforms other existing frame recovery methods by up to 2.91 dB.
C1 [Yan, Bo; Zhou, Jie] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
C3 Fudan University
RP Yan, B (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
EM byan@fudan.edu.cn
RI Yan, Bo/AFQ-7025-2022
OI Yan, Bo/0000-0002-7775-1270
FU NSFC [61073067]
FX This work was supported by NSFC under Grant No.: 61073067. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Charles D. (Chuck) Creusere.
CR Belfiore S, 2005, IEEE T MULTIMEDIA, V7, P316, DOI 10.1109/TMM.2005.843347
   Belfiore S, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P649
   Chen Y., 2004, P IEEE PICT COD S PC
   Chung TY, 2010, IEEE IMAGE PROC, P441, DOI 10.1109/ICIP.2010.5654236
   Fehn C, 2003, CONF REC ASILOMAR C, P1529
   Gharavi H, 2008, INT CONF ACOUST SPEE, P1153, DOI 10.1109/ICASSP.2008.4517819
   Huang CM, 2004, IEEE IMAGE PROC, P537
   Lee YC, 2002, IEEE T IMAGE PROCESS, V11, P1314, DOI 10.1109/TIR2002.804275
   Liu YQ, 2010, IEEE T CIRC SYST VID, V20, P600, DOI 10.1109/TCSVT.2009.2035838
   Nasiopoulos P, 2005, IEEE INT SYMP CIRC S, P320, DOI 10.1109/ISCAS.2005.1464589
   Peng Q, 2002, 2002 INTERNATIONAL CONFERENCE ON COMMUNICATIONS, CIRCUITS AND SYSTEMS AND WEST SINO EXPOSITION PROCEEDINGS, VOLS 1-4, P10, DOI 10.1109/ICCCAS.2002.1180560
   Redert A, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P313
   WU Z, 2006, P IEEE ICASSP 06 MAY
   Yan B, 2007, IEEE T CONSUM ELECTR, V53, P1546, DOI 10.1109/TCE.2007.4429250
   Yan B, 2010, IEEE T IMAGE PROCESS, V19, P98, DOI 10.1109/TIP.2009.2032311
NR 15
TC 31
Z9 34
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 936
EP 941
DI 10.1109/TMM.2012.2184743
PN 2
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700023
DA 2024-07-18
ER

PT J
AU Liu, S
   Yan, SC
   Zhang, TZ
   Xu, CS
   Liu, J
   Lu, HQ
AF Liu, Si
   Yan, Shuicheng
   Zhang, Tianzhu
   Xu, Changsheng
   Liu, Jing
   Lu, Hanqing
TI Weakly Supervised Graph Propagation Towards Collective Image Parsing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Concept map-based image retrieval; convex concave programming (CCCP);
   image annotation; nonnegative multiplicative updating; weakly supervised
   image parsing
ID SEGMENTATION
AB In this work, we propose a weakly supervised graph propagation method to automatically assign the annotated labels at image level to those contextually derived semantic regions. The graph is constructed with the over-segmented patches of the image collection as nodes. Image-level labels are imposed on the graph as weak supervision information over subgraphs, each of which corresponds to all patches of one image, and the contextual information across different images at patch level are then mined to assist the process of label propagation from images to their descendent regions. The ultimate optimization problem is efficiently solved by Convex Concave Programming (CCCP). Extensive experiments on four benchmark datasets clearly demonstrate the effectiveness of our proposed method for the task of collective image parsing. Two extensions including image annotation and concept map based image retrieval demonstrate the proposed image parsing algorithm can effectively aid other vision tasks.
C1 [Liu, Si; Xu, Changsheng; Liu, Jing; Lu, Hanqing] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Liu, Si; Yan, Shuicheng; Zhang, Tianzhu; Xu, Changsheng; Liu, Jing; Lu, Hanqing] China Singapore Inst Digital Media, Singapore 119613, Singapore.
   [Zhang, Tianzhu] Adv Digital Sci Ctr, Singapore 138632, Singapore.
   [Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; National
   University of Singapore
RP Liu, S (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM sliu@nlpr.ia.ac.cn; eleyans@nus.edu.sg; tzzhang@nlpr.ia.ac.cn;
   csxu@nlpr.ia.ac.cn; liu-jingmgm@gmail.com; luhq@nlpr.ia.ac.cn
RI Li, Chun/KBC-9591-2024; Yan, Shuicheng/HCI-1431-2022; xu,
   cj/HJZ-3488-2023; Zhang, Tianzhu/AGY-9389-2022
OI Zhang, Tianzhu/0000-0003-0764-6106
FU NExT Research Cente; MDA, Singapore [WBS:R-252-300-001-490]; 973 Program
   [2010CB327905, 2012CB316304]; National Natural Science Foundation of
   China [60903146, 60833006, 90920303]
FX Manuscript received December 19, 2010; revised July 16, 2011; accepted
   October 19, 2011. Date of publication November 04, 2011; date of current
   version March 21, 2012. This work was supported in part by the "NExT
   Research Center" funded by MDA, Singapore, under Grant
   WBS:R-252-300-001-490, 973 Program under Project 2010CB327905 and
   Project 2012CB316304, and the National Natural Science Foundation of
   China under Grant 60903146, Grant 60833006, and Grant 90920303. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Ketan Mayer-Patel.
CR [Anonymous], 2007, ACM MULTIMEDIA, DOI DOI 10.1145/1291233.1291379
   [Anonymous], 2006, P 23 INT C MACH LEAR, DOI [10.1145/1143844.1143933, DOI 10.1145/1143844.1143933]
   [Anonymous], 2007, 2007 IEEE 11 INT C C
   [Anonymous], 2009, PROC 17 ACM INT C MU
   Borenstein E, 2004, LECT NOTES COMPUT SC, V3023, P315
   Boyd S.P., 2004, Convex optimization, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441]
   Chen G., 2008, P SIAM INT C DAT MIN, P410, DOI DOI 10.1137/1.9781611972788.37
   Chen Y, 2008, P IEEE C COMP VIS PA, V1, P1
   Cheng B, 2010, IEEE T IMAGE PROCESS, V19, P858, DOI 10.1109/TIP.2009.2038764
   Cheng H, 2009, IEEE I CONF COMP VIS, P317, DOI 10.1109/ICCV.2009.5459267
   Chu W.-S., 2010, Proc. of the Asian Conference on Computer Vision (ACCV), P355, DOI DOI 10.1007/978-3-642-19315-6_28
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Galleguillos C, 2008, LECT NOTES COMPUT SC, V5302, P193, DOI 10.1007/978-3-540-88682-2_16
   Gould S, 2008, INT J COMPUT VISION, V80, P300, DOI 10.1007/s11263-008-0140-x
   Gould S, 2009, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2009.5459211
   He J., 2004, P 12 ANN ACM INT C M, P9, DOI DOI 10.1145/1027527.1027531
   Hochbaum DS, 2009, IEEE I CONF COMP VIS, P269, DOI 10.1109/ICCV.2009.5459261
   Joulin Armand, 2010, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2010.5539868
   Leibe Bastian, 2004, WORKSH STAT LEARN CO
   Li LJ, 2009, PROC CVPR IEEE, P2036, DOI 10.1109/CVPRW.2009.5206718
   Liu C., 2009, P IEEE C COMP VIS PA, P115
   Liu D., 2010, Proceedings of ACM International Conference on Multimedea, P25
   Liu D., 2009, P INT C WORLD WID WE, P747
   Liu XB, 2009, IEEE DATA MINING, P307, DOI 10.1109/ICDM.2009.18
   Mukherjee L., 2011, P IEEE C COMP VIS PA, P2028
   Mukherjee L, 2009, PROC CVPR IEEE, P2028, DOI 10.1109/CVPRW.2009.5206652
   Pan Jia-Yu., 2004, CVPRW 04 P 2004 C CO, V9, P146
   Russell B., 2009, Advances in Neural Information Processing Systems, P1580
   Shotton J., 2009, INT J COMPUT VIS, V81
   Tu ZW, 2005, INT J COMPUT VISION, V63, P113, DOI 10.1007/s11263-005-6642-x
   Williams C.K.I., PASCAL VISUAL OBJECT
   Winn J, 2005, IEEE I CONF COMP VIS, P1800
   Xu H., 2010, P SIGIR
   Yan S., 2009, P SIAM INT C DAT MIN, P858
   Yuan J., 2008, P 2008 INT WORKSH EA, P1, DOI [10.1109/EORSA.2008.4620341, DOI 10.1109/EORSA.2008.4620341]
   Yuille AL, 2003, NEURAL COMPUT, V15, P915, DOI 10.1162/08997660360581958
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhu Guangyu., 2010, Proceedings of the International Conference on Multimedia, MM '10, P461
NR 39
TC 45
Z9 46
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2012
VL 14
IS 2
BP 361
EP 373
DI 10.1109/TMM.2011.2174780
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OZ
UT WOS:000302702500011
DA 2024-07-18
ER

PT J
AU Chen, ZS
   Jang, JSR
   Lee, CH
AF Chen, Zhi-Sheng
   Jang, Jyh-Shing Roger
   Lee, Chin-Hui
TI A Kernel Framework for Content-Based Artist Recommendation System in
   Music
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Collaborative filtering; content-based music recommendation; kernel
   function; maximum a posterior adaptation; ordinal regression; universal
   background model
ID RETRIEVAL
AB This paper proposes a content-based artist recommendation framework which learns relationships between users' preference and music contents through ordinal regression. In particular, an artist is characterized by the parameters of its corresponding acoustical model which is adapted from a universal background model. These artist-specific acoustic features together with their preference rankings are then used as input vectors for the proposed order preserving projection (OPP) algorithm which tries to find a suitable subspace such that the desired ranking order of the data after projection can be kept as much as possible. The proposed linear OPP can be kernelized to learn the nonlinear relationship between music contents and users' artist rank orders. Under the proposed framework of kernelized OPP (KOPP), we can derive the nonlinear relationship and, more importantly, efficiently fuse acoustic and symbolic features obtained from the artist recommended meta-data. Experimental results demonstrate that OPP attains comparable results with those obtained with a conventional ordinal regression method, Prank. Moreover, by exploring the nonlinear relationship among training examples and combining acoustic and symbolic features, KOPP outperforms previous approaches to artist recommendation.
C1 [Chen, Zhi-Sheng; Jang, Jyh-Shing Roger] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
   [Lee, Chin-Hui] Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
C3 National Tsing Hua University; University System of Georgia; Georgia
   Institute of Technology
RP Chen, ZS (corresponding author), Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
EM bobon@mirlab.org; jang@cs.nthu.edu.tw; chl@ece.gatech.edu
OI JANG, JYH-SHING/0000-0002-7319-9095
FU National Science Council, Taiwan, R.O.C. [NSC98-2917-I-007-112]
FX Manuscript received April 07, 2011; revised July 03, 2011; accepted
   August 11, 2011. Date of publication August 30, 2011; date of current
   version November 18, 2011. This work was performed during the first
   author's visiting stay at Georgia Tech which was supported in part by
   the National Science Council, Taiwan, R.O.C. under Grant
   NSC98-2917-I-007-112. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Eckehard
   Steinbach.
CR Ali Kamal., 2004, ACM SIGKDD INT C KNO, P394, DOI DOI 10.1145/1014052.1014097
   [Anonymous], P ISMIR C
   [Anonymous], NAT ASS RECORD MERCH
   [Anonymous], P 18 NAT C ART INT E
   [Anonymous], P INT C AC SPEECH SI
   [Anonymous], P 32 ACM SIGIR
   [Anonymous], P ISMIR PHIL PA SEP
   [Anonymous], 2008, HDB DATA VISUALIZATI
   [Anonymous], P ISMIR C
   [Anonymous], P NEUR INF PROC SYST
   [Anonymous], 2003, Journal of machine learning research
   [Anonymous], 1999, P SIGIR WORKSH REC S
   [Anonymous], P INT C AC SPEECH SI
   [Anonymous], 2007, Austrain Computer Society
   [Anonymous], P SIGIR 07
   [Anonymous], P ISMIR C
   [Anonymous], YAH MUS US RAT MUS A
   [Anonymous], P INT C AC SPEECH SI
   [Anonymous], P 18 ACM C INF KNOWL
   [Anonymous], P NEUR INF PROC SYST
   [Anonymous], MILLION SONG DATASET
   [Anonymous], THESIS POMPEU FABRA
   [Anonymous], P ICSLP
   [Anonymous], P KDD CUP WORKSH
   [Anonymous], 2006, P ACM C KNOWLEDGE DI
   [Anonymous], ADV ARTIF INTELL
   [Anonymous], 2006, 1 IEEE RAS EMBS INT, DOI [DOI 10.1109/BIOROB.2006.1639102, 10.1109/BIOR013.2006.1639102]
   Aucouturier Jean-Julien., 2002, Proceedings of the 3rd International Conference on Music Information Retrieval, ISMIR, P157
   Aucouturier JJ, 2005, IEEE T MULTIMEDIA, V7, P1028, DOI 10.1109/TMM.2005.858380
   Basu C, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P714
   Berenzweig A, 2004, COMPUT MUSIC J, V28, P63, DOI 10.1162/014892604323112257
   Bishop C, 2007, RECOGNITION PATTERN
   Burges Chris, 2005, P 22 INT C MACH LEAR, P89, DOI DOI 10.1145/1102351.1102363
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Gauvain JL, 1994, IEEE T SPEECH AUDI P, V2, P291, DOI 10.1109/89.279278
   Golub G.H., 1989, MATRIX COMPUTATIONS
   Herbrich R, 2000, ADV NEUR IN, P115
   Herlocker JL, 2004, ACM T INFORM SYST, V22, P5, DOI 10.1145/963770.963772
   Juang BH, 1997, IEEE T SPEECH AUDI P, V5, P257, DOI 10.1109/89.568732
   Koren Y, 2010, ACM T KNOWL DISCOV D, V4, DOI 10.1145/1644873.1644874
   Koren Y, 2011, RECOMMENDER SYSTEMS HANDBOOK, P145, DOI 10.1007/978-0-387-85820-3_5
   Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27
   Lee C.H., 1996, AUTOMATIC SPEECH SPE
   Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344
   Logan Beth, 2004, ISMIR, P425
   Mooney R.J., 2000, Proceedings of the fifth ACM conference on Digital libraries', DL'00, P195
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Resnick P., 1994, P ACM C COMP SUPP CO, P175
   Schein A. I., 2002, Proceedings of SIGIR 2002. Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P253, DOI 10.1145/564376.564421
   Shawe-Taylor J., 2004, KERNEL METHODS PATTE
   Su XY, 2006, PROC INT C TOOLS ART, P497
   Turnbull D, 2008, IEEE T AUDIO SPEECH, V16, P467, DOI 10.1109/TASL.2007.913750
   Vasconcelos N, 2001, PROC CVPR IEEE, P3
   WILCOXON F, 1945, BIOMETRICS BULL, V1, P80, DOI 10.1093/jee/39.2.269
   You CH, 2009, IEEE SIGNAL PROC LET, V16, P49, DOI 10.1109/LSP.2008.2006711
NR 55
TC 23
Z9 26
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2011
VL 13
IS 6
BP 1371
EP 1380
DI 10.1109/TMM.2011.2166380
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 852GD
UT WOS:000297343400016
DA 2024-07-18
ER

PT J
AU Ma, Z
   Hu, H
   Wang, Y
AF Ma, Zhan
   Hu, Hao
   Wang, Yao
TI On Complexity Modeling of H.264/AVC Video Decoding and Its Application
   for Energy Efficient Decoding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Complexity modeling and prediction; dynamic voltage and frequency
   scaling (DVFS); H.264/AVC video decoding
ID POWER
AB This paper proposes a new complexity model for H.264/AVC video decoding. The model is derived by decomposing the entire decoder into several decoding modules (DM), and identifying the fundamental operation unit (termed complexity unit or CU) in each DM. The complexity of each DM is modeled by the product of the average complexity of one CU and the number of CUs required. The model is shown to be highly accurate for software video decoding both on Intel Pentium mobile 1.6-GHz and ARM Cortex A8 600-MHz processors, over a variety of video contents at different spatial and temporal resolutions and bit rates. We further show how to use this model to predict the required clock frequency and hence perform dynamic voltage and frequency scaling (DVFS) for energy efficient video decoding. We evaluate achievable power savings on both the Intel and ARM platforms, by using analytical power models for these two platforms as well as real experiments with the ARM-based TI OMAP35x EVM board. Our study shows that for the Intel platform where the dynamic power dominates, a power saving factor of 3.7 is possible. For the ARM processor where the static leakage power is not negligible, a saving factor of 2.22 is still achievable.
C1 [Ma, Zhan; Hu, Hao; Wang, Yao] Polytech Inst New York Univ, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
   [Ma, Zhan] Samsung Telecommun Amer, Dallas Technol Lab, Richardson, TX 75082 USA.
C3 New York University; New York University Tandon School of Engineering;
   Samsung
RP Ma, Z (corresponding author), Polytech Inst New York Univ, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
EM zhan.ma@ieee.org; hhu01@students.poly.edu; yao@poly.edu
RI Ma, Zhan/HKW-2859-2023
OI Ma, Zhan/0000-0003-3686-4057; Wang, Yao/0000-0003-3199-3802
CR Akyol E, 2008, IEEE T CIRC SYST VID, V18, P1300, DOI 10.1109/TCSVT.2008.928886
   [Anonymous], CPUFREQ GOVERNOR
   [Anonymous], TI OMAP35X EVM
   [Anonymous], ARM CORT A8 PROC
   [Anonymous], INT PENT MOB PROC
   [Anonymous], 2007, JSVM SOFTW
   Burd TD, 2000, IEEE J SOLID-ST CIRC, V35, P1571, DOI 10.1109/4.881202
   He ZH, 2005, IEEE T CIRC SYST VID, V15, P645, DOI 10.1109/TCSVT.2005.846433
   Horowitz M, 2003, IEEE T CIRC SYST VID, V13, P704, DOI 10.1109/TCSVT.2003.814967
   Hu H., 2009, COMPLEXITY PROFILER
   Hu Y., 2007, P IEEE ISCAS
   Jejurikar R., 2004, P 41 ANN C DES AUT
   Kontorinis N, 2009, IEEE T CIRC SYST VID, V19, P1000, DOI 10.1109/TCSVT.2009.2020256
   Lee S.-W., 2007, P MMSP
   Lee S.-W., 2007, P SPIE
   Lee S.-W., 2008, P IEEE ISCAS SEATTL
   Lee S.-W., 2007, P IEEE ICIP
   List P, 2003, IEEE T CIRC SYST VID, V13, P614, DOI 10.1109/TCSVT.2003.815175
   Ma Z., 2008, P ICIP
   Rabaey JanM., 1996, DIGITAL INTEGRATED C
   SCHWARZ H, 2005, HIERARCHICAL B PICTU
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   van der Schaar M, 2005, IEEE T MULTIMEDIA, V7, P471, DOI 10.1109/TMM.2005.846790
   Wiegand T., 2007, TEXT ISO IEC 14496 1
   Zhou M., 2005, HDB IMAGE VIDEO PROC, V2nd
NR 25
TC 41
Z9 43
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2011
VL 13
IS 6
BP 1240
EP 1255
DI 10.1109/TMM.2011.2165056
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 852GD
UT WOS:000297343400006
DA 2024-07-18
ER

PT J
AU Yuan, J
   Zha, ZJ
   Zheng, YT
   Wang, M
   Zhou, XD
   Chua, TS
AF Yuan, Jin
   Zha, Zheng-Jun
   Zheng, Yan-Tao
   Wang, Meng
   Zhou, Xiangdong
   Chua, Tat-Seng
TI Utilizing Related Samples to Enhance Interactive Concept-Based Video
   Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Complex query; concept-based video search; interactive search; related
   sample
ID IMAGE RETRIEVAL; DETECTORS; ONTOLOGY
AB One of the main challenges in interactive concept-based video search is the problem of insufficient relevant samples, especially for queries with complex semantics. In this paper, "related samples" are exploited to enhance interactive video search. The related samples refer to those video segments that are relevant to part of the query rather than the entire query. Compared to the relevant samples which may be rare, the related samples are usually plentiful and easy to find in search results. Generally, the related samples are visually similar and temporally neighboring to the relevant samples. Based on these two characters, we develop a visual ranking model that simultaneously exploits the relevant, related, and irrelevant samples, as well as a temporal ranking model to leverage the temporal relationship between related and relevant samples. An adaptive fusion method is then proposed to optimally explore these two ranking models to generate search results. We conduct extensive experiments on two real-world video datasets: TRECVID 2008 and YouTube datasets. As the experimental results show, our approach achieves at least 96% and 167% performance improvements against the state-of-the-art approaches on the TRECVID 2008 and YouTube datasets, respectively.
C1 [Yuan, Jin; Zha, Zheng-Jun; Wang, Meng; Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
   [Zheng, Yan-Tao] Inst Infocomm Res I2R, Singapore, Singapore.
   [Zhou, Xiangdong] Fudan Univ China, Dept Comp, Shanghai, Peoples R China.
   [Zhou, Xiangdong] Fudan Univ China, Sch Comp Sci, Shanghai, Peoples R China.
C3 National University of Singapore; Agency for Science Technology &
   Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R); Fudan
   University; Fudan University
RP Yuan, J (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
EM zhazj@comp.nus.edu.sg
RI Zha, Zheng-Jun/AAF-8667-2020; Zha, Zheng-Jun/AAE-8408-2020; Wang,
   Meng/ITR-8699-2023
OI Zha, Zheng-Jun/0000-0003-2510-8993; 
FU Singapore National Research Foundation AMP;AMP; Interactive Digital
   Media R&D Program Office (MDA) [R-252-300-001-490]; NSFC [61073002]
FX Manuscript received February 22, 2011; revised July 11, 2011; accepted
   August 25, 2011. Date of publication September 19, 2011; date of current
   version November 18, 2011. This work was supported in part by the
   Singapore National Research Foundation & Interactive Digital Media R&D
   Program Office (MDA under research grant R-252-300-001-490) and in part
   by NSFC Grant 61073002. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Jia Li.
CR Aly R., 2007, P SIGIR WORKSH MULT
   [Anonymous], PROGR BRAIN RES
   [Anonymous], 2009, NUSWIDE: A real-world web image database from National University of Singapore, DOI DOI 10.1145/1646396.1646452
   [Anonymous], 2005, SEMISUPERVISED LEARN
   [Anonymous], P TRECVID WORKSH
   Bach F. R., 2004, P IEEE INT C MACH LE
   Cilibrasi RL, 2007, IEEE T KNOWL DATA EN, V19, P370, DOI 10.1109/TKDE.2007.48
   Dalal N., 2005, P ACM INT C COMP VIS
   Gargi U, 2000, IEEE T CIRC SYST VID, V10, P1, DOI 10.1109/76.825852
   Hauptmann A. G., 2006, P ACM INT C MULT
   Hu X., 2011, P ACM INT C INF KNOW
   Jiang Y.-G., VIREO 374 LSCOM SEMA
   Jiang Y.-G., 2009, P ACM INT C MULT
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235
   Koskela M., 2009, P 2 C IM AN
   Mei T., 2008, P TRECVID WORKSH
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Neo S.-Y., 2006, P ACM INT C IM VID R
   Pickens J., 2008, TRECVID WORKING NOTE
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Snoek C., 2008, TRECVID WORKING NOTE
   Snoek C., 2009, FDN TRENDS INF RETRI
   Snoek CGM, 2007, IEEE T MULTIMEDIA, V9, P975, DOI 10.1109/TMM.2007.900156
   SNOEK CGM, 2006, P ACM INT C MULT
   TANG S, 2008, P TRECVID WORKSH
   Toharia P., 2009, P ACM INT C COMP AN
   Wang D., 2007, P ACM INT C MULT
   Wang Z., 2008, P ACM INT C IM VID R
   Wei XY, 2008, IEEE T MULTIMEDIA, V10, P1085, DOI 10.1109/TMM.2008.2001382
   Wu L., 2008, P ACM INT C MULT
   YANAGAWA A, 2007, 22220068 ADVENT
   Yuan J., 2010, P ACM INT C IM VID R
   Yuan J., 2011, P ACM INT C MULT
   Zha Z. J., 2008, P ACM INT C COMP VIS
   Zha Z. J., 2009, P ACM INT C MULT
   Zha ZJ, 2012, IEEE T MULTIMEDIA, V14, P17, DOI 10.1109/TMM.2011.2174782
   Zha ZJ, 2009, J VIS COMMUN IMAGE R, V20, P97, DOI 10.1016/j.jvcir.2008.11.009
   Zheng Y.-T., 2009, P ACM INT C IM VID R
NR 39
TC 8
Z9 12
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2011
VL 13
IS 6
BP 1343
EP 1355
DI 10.1109/TMM.2011.2168813
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 852GD
UT WOS:000297343400014
DA 2024-07-18
ER

PT J
AU Tournoux, PU
   Lochin, E
   Lacan, J
   Bouabdallah, A
   Roca, V
AF Tournoux, Pierre Ugo
   Lochin, Emmanuel
   Lacan, Jerome
   Bouabdallah, Amine
   Roca, Vincent
TI On-the-Fly Erasure Coding for Real-Time Video Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Delay recovery; erasure code; reliability; video-conferencing
ID CORRECTION CODES; MATRICES
AB This paper introduces a robust point-to-point transmission scheme: Tetrys, that relies on a novel on-the-fly erasure coding concept which reduces the delay for recovering lost data at the receiver side. In current erasure coding schemes, the packets that are not rebuilt at the receiver side are either lost or delayed by at least one RTT before transmission to the application. The present contribution aims at demonstrating that Tetrys coding scheme can fill the gap between real-time applications requirements and full reliability. Indeed, we show that in several cases, Tetrys can recover lost packets below one RTT over lossy and best-effort networks. We also show that Tetrys allows to enable full reliability without delay compromise and as a result: significantly improves the performance of time constrained applications. For instance, our evaluations present that video-conferencing applications obtain a PSNR gain up to 7 dB compared to classic block-based erasure codes.
C1 [Tournoux, Pierre Ugo; Lochin, Emmanuel; Bouabdallah, Amine] CNRS, LAAS, F-31077 Toulouse, France.
   [Tournoux, Pierre Ugo; Lochin, Emmanuel; Lacan, Jerome; Bouabdallah, Amine] Univ Toulouse, UPS, INSA, INP,ISAE,LAAS, F-31077 Toulouse, France.
   [Roca, Vincent] INRIA, Planete Res Team, French Res Inst, Grenoble, France.
C3 Centre National de la Recherche Scientifique (CNRS); Universite Federale
   Toulouse Midi-Pyrenees (ComUE); Universite de Toulouse; Institut
   National des Sciences Appliquees de Toulouse; Universite Toulouse III -
   Paul Sabatier; Centre National de la Recherche Scientifique (CNRS); CNRS
   - Institute of Physics (INP); Institut Superieur de l'Aeronautique et de
   l'Espace (ISAE-SUPAERO); Institut National Polytechnique de Toulouse;
   Inria
RP Tournoux, PU (corresponding author), CNRS, LAAS, F-31077 Toulouse, France.
RI Izquierdo-Roca, victor/P-7364-2018
OI Lochin, Emmanuel/0000-0002-6305-2188
FU French ANR [2006 TCOM 019, ANR-09-VERS-019-02]
FX This work was supported by the French ANR grants 2006 TCOM 019
   (CAPRI-FEC project) and ANR-09-VERS-019-02 (ARSSO project). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Beatrice Pesquet-Popescu.
CR AGHAREBPARAST F, 2003, P IEEE GLOB TEL C DE, V7
   Ahmad S, 2010, IEEE T CIRC SYST VID, V20, P275, DOI 10.1109/TCSVT.2009.2031545
   Albanese A, 1996, IEEE T INFORM THEORY, V42, P1737, DOI 10.1109/18.556670
   Aldous D., Reversible markov chains and random walks on graphs
   [Anonymous], 2009, 5510 RFC
   [Anonymous], 2009, R: A Language and Environment for Statistical Computing
   BLOEMER J, 1995, TR95048 INT COMP SCI
   BOGINO M, 2007, P IEEE INT S CIRC SY
   BOUABDALLAH A, 2006, J ZHEJIANG UNIV-SC A, V7, P27
   CARTA DG, 1990, COMMUN ACM, V33, P87, DOI 10.1145/76372.76379
   Cox D. R., 1965, THEORY STOCHASTIC PR
   GANGULY B, 2007, P IEEE MIL COMM C MI, P1
   Hutchinson R, 2008, LINEAR ALGEBRA APPL, V428, P2585, DOI 10.1016/j.laa.2008.02.011
   Kahn J, 2001, COMB PROBAB COMPUT, V10, P137, DOI 10.1017/S096354830100462X
   Klaue J, 2003, LECT NOTES COMPUT SC, V2794, P255, DOI 10.1007/978-3-540-45232-4_16
   Korhonen J, 2009, SIGNAL PROCESS-IMAGE, V24, P229, DOI 10.1016/j.image.2008.12.005
   LACAN J, 2008, P INT WORKSH SAT SPA
   Landström S, 2007, ACM SIGCOMM COMP COM, V37, P5, DOI 10.1145/1273445.1273447
   Luby M, 2007, IEEE T BROADCAST, V53, P235, DOI 10.1109/TBC.2007.891703
   Martinian E, 2004, IEEE T INFORM THEORY, V50, P2494, DOI 10.1109/TIT.2004.834844
   MATHIS M, 1996, IETF
   MATSUZONO K, 2010, P 35 IEEE C LOC COMP
   Nguyen D, 2009, IEEE T VEH TECHNOL, V58, P914, DOI 10.1109/TVT.2008.927729
   Rizzo L., 1997, Computer Communication Review, V27, P24, DOI 10.1145/263876.263881
   ROCA V, 2008, 5170 RFC
   Sahai A, 2008, IEEE T INFORM THEORY, V54, P1860, DOI 10.1109/TIT.2008.920339
   Shokrollahi A, 2006, IEEE T INFORM THEORY, V52, P2551, DOI 10.1109/TIT.2006.874390
   Soro Alexandre., 2010, Consumer Communications and Networking Conference (CCNC), 2010 7th IEEE, P1
   Sundararajan JK, 2008, IEEE INT SYMP INFO, P1651, DOI 10.1109/ISIT.2008.4595268
   Tobagi FA, 1996, IEEE J SEL AREA COMM, V14, P1436, DOI 10.1109/49.536490
   TOURNOUX PU, 2009, P ACM MULT 2009 SYST
   WATSON M, 2010, FORW ERR CO IN PRESS
   Wenger S, 2003, IEEE T CIRC SYST VID, V13, P645, DOI 10.1109/TCSVT.2003.814966
   H 264 AVC JM REFEREN
NR 34
TC 35
Z9 36
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2011
VL 13
IS 4
BP 797
EP 812
DI 10.1109/TMM.2011.2126564
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 805EX
UT WOS:000293710300018
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tawari, A
   Trivedi, MM
AF Tawari, Ashish
   Trivedi, Mohan Manubhai
TI Speech Emotion Analysis: Exploring the Role of Context
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Affect analysis; affective computing; context analysis; emotional
   speech; emotion intelligence; emotion recognition; vocal expression
ID ANNOTATION
AB Automated analysis of human affective behavior has attracted increasing attention in recent years. With the research shift toward spontaneous behavior, many challenges have come to surface ranging from database collection strategies to the use of new feature sets (e. g., lexical cues apart from prosodic features). Use of contextual information, however, is rarely addressed in the field of affect expression recognition, yet it is evident that affect recognition by human is largely influenced by the context information. Our contribution in this paper is threefold. First, we introduce a novel set of features based on cepstrum analysis of pitch and intensity contours. We evaluate the usefulness of these features on two different databases: Berlin Database of emotional speech (EMO-DB) and locally collected audiovisual database in car settings (CVRRCar-AVDB). The overall recognition accuracy achieved for seven emotions in the EMO-DB database is over 84% and over 87% for three emotion classes in CVRRCar-AVDB. This is based on tenfold stratified cross validation. Second, we introduce the collection of a new audiovisual database in an automobile setting (CVRRCar-AVDB). In this current study, we only use the audio channel of the database. Third, we systematically analyze the effects of different contexts on two different databases. We present context analysis of subject and text based on speaker/text-dependent/-independent analysis on EMO-DB. Furthermore, we perform context analysis based on gender information on EMO-DB and CVRRCar-AVDB. The results based on these analyses are promising.
C1 [Tawari, Ashish; Trivedi, Mohan Manubhai] Univ Calif San Diego, Comp Vis & Robot Res Lab, La Jolla, CA 92093 USA.
C3 University of California System; University of California San Diego
RP Tawari, A (corresponding author), Univ Calif San Diego, Comp Vis & Robot Res Lab, La Jolla, CA 92093 USA.
EM atawari@ucsd.edu; mtrivedi@ucsd.edu
FU UC Discovery Project
FX Manuscript received December 15, 2009; revised April 01, 2010; accepted
   June 02, 2010. Date of current version September 15, 2010. This work was
   supported in part by a grant from the UC Discovery Project. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Hamid K. Aghajan.
CR [Anonymous], 2004, P 42 ANN M ASS COMPU, DOI DOI 10.3115/1218955.1219000
   [Anonymous], 2008, P 2008 IEEE INT C SE
   [Anonymous], 2002, PROC INT C SPOKEN LA
   AUSTERMANN A, 2005, P IEEE RSJ INT C INT, P1138
   Banse R, 1996, J PERS SOC PSYCHOL, V70, P614, DOI 10.1037/0022-3514.70.3.614
   Batliner A, 2003, SPEECH COMMUN, V40, P117, DOI 10.1016/S0167-6393(02)00079-1
   BOZKURT E, 2008, P 3DTV C TRUE VIS CA, P273
   Burkhardt F, 2005, INTERSPEECH, V5, P1517, DOI DOI 10.21437/INTERSPEECH.2005-446
   Callejas Z, 2008, SPEECH COMMUN, V50, P416, DOI 10.1016/j.specom.2008.01.001
   COHN JF, 1987, DEV PSYCHOL, V23, P68, DOI 10.1037/0012-1649.23.1.68
   Devillers L, 2005, NEURAL NETWORKS, V18, P407, DOI 10.1016/j.neunet.2005.03.007
   Devillers L., 2006, P INT C SPOK LANG PR, P801
   Duric Z, 2002, P IEEE, V90, P1272, DOI 10.1109/JPROC.2002.801449
   EKMAN P, 2005, WHAT FACE REVEALS, P429
   Forbes-Riley K, 2004, HLT-NAACL 2004: HUMAN LANGUAGE TECHNOLOGY CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE MAIN CONFERENCE, P201
   Grimm M, 2007, LECT NOTES COMPUT SC, V4738, P126
   Kapoor A, 2007, INT J HUM-COMPUT ST, V65, P724, DOI 10.1016/j.ijhcs.2007.02.003
   Kwon O, 2004, SECOND IEEE WORKSHOP ON SOFTWARE TECHNOLOGIES FOR FUTURE EMBEDDED AND UBIQUITOUS SYSTEMS, PROCEEDINGS, P172
   Lisetti C.L., 2002, Proceedings of the 10th International Conference on Multimedia (MULTIMEDIA), P161
   Maat L., 2006, PROC AGM INT C MULTI, P171, DOI DOI 10.1145/1180995.1181032
   Pantic M., 2007, FACE RECOGNITION, P377
   PAUL B, 1993, ACCURATE SHORT TERM
   Roisman GI, 2004, DEV PSYCHOL, V40, P776, DOI 10.1037/0012-1649.40.5.776
   SCHERER T, 1998, P INT WORKSH SPEECH
   SHIVAPPA S, P IEEE IN PRESS
   Shivappa ST, 2009, INT CONF ACOUST SPEE, P3557, DOI 10.1109/ICASSP.2009.4960394
   Steeneken HJM, 1999, INT CONF ACOUST SPEE, P2079, DOI 10.1109/ICASSP.1999.758342
   TAWARI A, 2010, P 20 ICPR
   Tawari A, 2010, IEEE INT VEH SYM, P174, DOI 10.1109/IVS.2010.5547956
   Trivedi MM, 2005, IEEE T SYST MAN CY A, V35, P145, DOI 10.1109/TSMCA.2004.838480
   TRIVEDI MM, 2007, IEEE COMPUTER, V40
   Trivedi MM, 2007, IEEE T INTELL TRANSP, V8, P108, DOI 10.1109/TITS.2006.889442
   Vogt T., 2006, LREC, P1123
   Vogt T., 2009, INTERSPEECH, P328
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   ZHANG T, 2004, P ICSLP
NR 36
TC 68
Z9 76
U1 1
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2010
VL 12
IS 6
BP 502
EP 509
DI 10.1109/TMM.2010.2058095
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 668TB
UT WOS:000283291900004
DA 2024-07-18
ER

PT J
AU Damnjanovic, I
   Barry, D
   Dorran, D
   Reiss, JD
AF Damnjanovic, Ivan
   Barry, Dan
   Dorran, David
   Reiss, Joshua D.
TI A Real-Time Framework for Video Time and Pitch Scale Modification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive video refresh rate; audio/visual synchronization; time-scale
   modification
ID PHASE VOCODER; PLAYOUT
AB A framework is presented which addresses the issues related to the real-time implementation of synchronized video and audio time-scale and pitch-scale modification algorithms. It allows for seamless real-time transition between continually varying, independent time-scale and pitch-scale parameters arising as a result of manual or automatic intervention. We illuminate the problems which arise in a real-time context as well as provide novel solutions to prevent artifacts, minimize latency, and improve synchronization. The time and pitch scaling approach is based on a modified phase vocoder with optional phase locking and an integrated transient detector which enables high-quality transient preservation in real-time. A novel method for audio/visual synchronization was implemented in order to ensure no perceptible latency between audio and video while real-time time scaling and pitch shifting is applied. Evaluation results are reported which demonstrate both high audio quality and minimal synchronization error.
C1 [Damnjanovic, Ivan; Reiss, Joshua D.] Univ London, London E1 4NS, England.
   [Barry, Dan; Dorran, David] Dublin Inst Technol, Audio Res Grp, Dublin 8, Ireland.
C3 University of London; Technological University Dublin
RP Damnjanovic, I (corresponding author), Univ London, London E1 4NS, England.
EM ivan.damnjanovic@elec.qmul.ac.uk; dan.barry@dit.ie; david.dorran@dit.ie;
   josh.reiss@elec.qmul.ac.uk
OI Dorran, David/0000-0001-5469-0658; Barry, Dan/0000-0002-8548-1549
FU European Community [IST-033902]
FX This work was supported in part by the European Community under the
   Information Society Technologies (IST) programme of the 6th FP for
   RTD-project EASAIER contract IST-033902. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Shrikanth Narayanan.
CR Barry D., 2005, IEE Irish Signals and Systems Conference 2005, P13, DOI 10.1049/cp:20050280
   Bonada Jordi., 2000, Proceedings of International Computer Music Conference, P396
   DOLSON M, 1986, COMPUT MUSIC J, V10, P14, DOI 10.2307/3680093
   DUFFY C, 2004, INT MUS MULT APPL BA
   DUXBURY C, 2002, 112 AES CONV MUN GER, P1
   FLANAGAN JL, 1965, J ACOUST SOC AM, V38, P939, DOI 10.1121/1.1939800
   Harrigan K., 2000, Journal of Research on Computing in Education, V33, P77
   *INT TEL UN, 1993, 11A47E INT TEL UN
   ITU-R Recommendation, 1998, BT13591 ITUR
   Kalman M, 2004, IEEE T CIRC SYST VID, V14, P841, DOI 10.1109/TCSVT.2004.828335
   LABARBERA P, 1979, J MARKETING, V43, P30, DOI 10.2307/1250755
   LANDONE C, 2007, 8 INT C MUS INF RETR
   Laroche J, 1999, IEEE T SPEECH AUDI P, V7, P323, DOI 10.1109/89.759041
   LAROCHE J, 1993, IEEE WORKSH APPS SIG, P131
   LI FC, 2000, ACM CHI HAG NETH
   Liang YJ, 2001, INT CONF ACOUST SPEE, P1445, DOI 10.1109/ICASSP.2001.941202
   OLSON JS, 1985, P ANN CONV ASS ED CO
   PORTNOFF MR, 1976, IEEE T ACOUST SPEECH, V24, P243, DOI 10.1109/TASSP.1976.1162810
   Yuang MC, 1998, MULTIMED TOOLS APPL, V6, P47, DOI 10.1023/A:1009638628952
NR 19
TC 1
Z9 1
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2010
VL 12
IS 4
BP 247
EP 256
DI 10.1109/TMM.2010.2046296
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 596ER
UT WOS:000277668100002
OA Green Published
DA 2024-07-18
ER

PT J
AU Luo, HY
   Argyriou, A
   Wu, DL
   Ci, S
AF Luo, Haiyan
   Argyriou, Antonios
   Wu, Dalei
   Ci, Song
TI Joint Source Coding and Network-Supported Distributed Error Control for
   Video Streaming in Wireless Multihop Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Automatic repeat request (ARQ); joint source and channel coding (JSCC);
   joint source and distributed error control (JSDEC); proxy;
   rate-distortion (R-D) model; real-time encoding; video streaming
ID REAL-TIME; PRONE NETWORKS; SCALABLE VIDEO; MODE SELECTION; BIT
   ALLOCATION; TCP
AB Real-time video communication over wireless multihop networks has gained significant interest in the last few years. In this paper, we focus our attentions on the problem of source coding and link adaptation for packetized video streaming in wireless multihop networks when network nodes are media-aware. We consider a system where source coding is employed at the video encoder by selecting the encoding mode of each individual macro-block, while error control is exercised through application-layer retransmissions at each media-aware network node. For this system model, the contribution of each communication link on the end-to-end video distortion is considered separately in order to achieve globally optimal source coding and ARQ error control. To reach the globally optimal solution, we formulate the problem of joint source and distributed error control (JSDEC) and devise a low-complexity solution algorithm based on dynamic programming. Extensive experiments have been carried out on the basis of H.264/AVC codec to demonstrate the effectiveness of the proposed algorithm over the existing joint source and channel coding (JSCC) algorithm in terms of PSNR perceived at the decoder under time-varying multihop wireless links.
C1 [Luo, Haiyan; Wu, Dalei; Ci, Song] Univ Nebraska, Dept Comp & Elect Engn, Omaha, NE 68182 USA.
   [Argyriou, Antonios] Philips Res, NL-5656 AE Eindhoven, Netherlands.
C3 University of Nebraska System; Philips; Philips Research
RP Luo, HY (corresponding author), Univ Nebraska, Dept Comp & Elect Engn, Omaha, NE 68182 USA.
EM haiyan.luo@huskers.unl.edu; anargyr@ieee.org; dwu@huskers.unl.edu;
   sci@engr.unl.edu
RI Ci, Song/R-8324-2019; Argyriou, Antonios/J-5170-2012; Argyriou,
   Antonios/AAF-9586-2021; Wu, Dalei/A-6884-2012
OI Argyriou, Antonios/0000-0002-2510-3124; Argyriou,
   Antonios/0000-0002-2510-3124; , Dalei/0000-0002-9362-3906
FU NSF [CCF-0830493, CNS-0707944]; Layman Foundation
FX Manuscript received September 12, 2008; revised May 05, 2009. First
   published August 18, 2009; current version published October 16, 2009.
   This work was supported in part by the NSF under Grants CCF-0830493 and
   CNS-0707944 and in part by the Layman Foundation. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. S.-H. Gary Chan.
CR Akan OB, 2004, IEEE ACM T NETWORK, V12, P634, DOI 10.1109/TNET.2004.833155
   Andreopoulos Y, 2006, IEEE J SEL AREA COMM, V24, P2104, DOI 10.1109/JSAC.2006.881614
   [Anonymous], MSRTR200135
   [Anonymous], JVT REFERENCE SOFTWA
   [Anonymous], 2003, 3448 RFC
   ARGYRIOU A, 2007, P PACK VID WORKSH LA
   Argyriou A, 2007, SIGNAL PROCESS-IMAGE, V22, P374, DOI 10.1016/j.image.2007.01.001
   Begen AC, 2007, IEEE T MULTIMEDIA, V9, P332, DOI 10.1109/TMM.2006.886282
   Bertseka D., 1987, Data Networks
   Chakareski J, 2006, IEEE ACM T NETWORK, V14, P1302, DOI 10.1109/TNET.2006.886299
   CHEN M, 2004, P IEEE INT C IM PROC
   Cheung G, 2000, IEEE T IMAGE PROCESS, V9, P340, DOI 10.1109/83.826773
   Côté G, 2000, IEEE J SEL AREA COMM, V18, P952, DOI 10.1109/49.848249
   Gao LX, 2003, IEEE ACM T NETWORK, V11, P884, DOI 10.1109/TNET.2003.820423
   HARTANTO F, 1999, P IEEE 10 WORKSH LOC
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P511, DOI 10.1109/TCSVT.2002.800313
   *ITU T, 2003, H264ISOIEC1449610AVC
   Kompella S, 2007, IEEE J SEL AREA COMM, V25, P831, DOI 10.1109/JSAC.2007.070518
   KONDI L, 2001, P ICASSP
   MUKHERJEE A, 1992, MSCIS9283 U PENNS
   NGUYEN D, 2007, P PACK VID 2007 LAUS
   Schuster G., 1997, RATE DISTORTION BASE
   SEFEROGLU H, 2005, P ICIP
   SHAN Y, 2004, P INT C IM PROC ICIP
   THOMOS N, 2008, P IEEE INT IN PRESS
   van der Schaar M., 2007, MULTIMEDIA IP WIRELE
   Wang Y., 2002, VIDEO PROCESSING COM
   WESTERLUND M, 2007, RTP TOPOLOGIES
   Wu DP, 2000, IEEE T CIRC SYST VID, V10, P923, DOI 10.1109/76.867930
   Yan JY, 2006, IEEE T MULTIMEDIA, V8, P196, DOI 10.1109/TMM.2005.864265
   Zhai F, 2006, IEEE T IMAGE PROCESS, V15, P40, DOI 10.1109/TIP.2005.860353
   ZHAI F, 2004, THESIS NW U EVANSTON
   Zhang Q, 2002, IEEE T CIRC SYST VID, V12, P398, DOI 10.1109/TCSVT.2002.800322
   Zhang R, 2000, IEEE J SEL AREA COMM, V18, P966, DOI 10.1109/49.848250
   Zhang Y, 2007, IEEE T MULTIMEDIA, V9, P445, DOI 10.1109/TMM.2006.887989
NR 35
TC 6
Z9 13
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2009
VL 11
IS 7
BP 1362
EP 1372
DI 10.1109/TMM.2009.2030639
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 506GB
UT WOS:000270761300014
DA 2024-07-18
ER

PT J
AU Mower, E
   Mataric, MJ
   Narayanan, S
AF Mower, Emily
   Mataric, Maja J.
   Narayanan, Shrikanth (Shri)
TI Human Perception of Audio-Visual Synthetic Character Emotion Expression
   in the Presence of Ambiguous and Conflicting Information
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Expressive animation; multimodality; synthesis
ID RECOGNITION; INTEGRATION; EAR
AB Computer simulated avatars and humanoid robots have an increasingly prominent place in today's world. Acceptance of these synthetic characters depends on their ability to properly and recognizably convey basic emotion states to a user population. This study presents an analysis of the interaction between emotional audio (human voice) and video (simple animation) cues. The emotional relevance of the channels is analyzed with respect to their effect on human perception and through the study of the extracted audio-visual features that contribute most prominently to human perception. As a result of the unequal level of expressivity across the two channels, the audio was shown to bias the perception of the evaluators. However, even in the presence of a strong audio bias, the video data were shown to affect human perception. The feature sets extracted from emotionally matched audio-visual displays contained both audio and video features while feature sets resulting from emotionally mismatched audio-visual displays contained only audio information. This result indicates that observers integrate natural audio cues and synthetic video cues only when the information expressed is in congruence. It is therefore important to properly design the presentation of audio-visual cues as incorrect design may cause observers to ignore the information conveyed in one of the channels.
C1 [Mower, Emily; Narayanan, Shrikanth (Shri)] Univ So Calif, Dept Elect Engn, Los Angeles, CA 90089 USA.
   [Mataric, Maja J.; Narayanan, Shrikanth (Shri)] Univ So Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
C3 University of Southern California; University of Southern California
RP Mower, E (corresponding author), Univ So Calif, Dept Elect Engn, Univ Pk, Los Angeles, CA 90089 USA.
EM mower@usc.edu; mataric@usc.edu; shri@sipi.usc.edu
RI Narayanan, Shrikanth S/D-5676-2012
OI Provost, Emily/0000-0003-1870-6063
FU National Science Foundation; U. S. Army; Herbert Kunzel Engineering
   Fellowship; Div Of Information & Intelligent Systems; Direct For
   Computer & Info Scie & Enginr [0803565] Funding Source: National Science
   Foundation
FX This work was supported in part by the National Science Foundation, in
   part by the U. S. Army, and in part by the Herbert Kunzel Engineering
   Fellowship. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Zhengyou Zhang.
CR [Anonymous], P INT C SPOK LANG PR
   [Anonymous], 1998, STAT LEARNING THEORY
   [Anonymous], P INT C SPEECH PROS
   [Anonymous], 1999, P ICONIP ANZIIS ANNE
   Bartlett MS, 2005, PROC CVPR IEEE, P568
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   Breazeal C, 2003, INT J HUM-COMPUT ST, V59, P119, DOI 10.1016/S1071-5819(03)00018-1
   Bulut M., 2004, P 6 INT C MULT INT, P205
   Bulut M, 2008, INT CONF ACOUST SPEE, P4629, DOI 10.1109/ICASSP.2008.4518688
   BUSSO C, 2007, P IEEE INT WORKSH MU
   BUSSO C, J LANG RESO IN PRESS
   Cowie R, 2001, IEEE SIGNAL PROC MAG, V18, P32, DOI 10.1109/79.911197
   De Gelder B, 2003, TRENDS COGN SCI, V7, P460, DOI 10.1016/j.tics.2003.08.014
   de Gelder B, 2000, COGNITION EMOTION, V14, P289, DOI 10.1080/026999300378824
   de Gelder B, 1999, NEUROSCI LETT, V260, P133, DOI 10.1016/S0304-3940(98)00963-X
   EKMAN P, 2002, FASC EBOOK
   Ekman P, 1978, FACIAL ACTION CODING
   Gosselin F, 2001, VISION RES, V41, P2261, DOI 10.1016/S0042-6989(01)00097-9
   Gratch J., 2004, Cogn. Syst. Res, V5, P269, DOI [10.1016/j.cogsys.2004.02.002, DOI 10.1016/J.COGSYS.2004.02.002]
   GRIMM M, 2005, P C TEL MULT COMM KA
   Grimm M, 2007, SPEECH COMMUN, V49, P787, DOI 10.1016/j.specom.2007.01.010
   Hanjalic A, 2006, IEEE SIGNAL PROC MAG, V23, P90, DOI 10.1109/MSP.2006.1621452
   Hietanen J, 2004, EUR J COGN PSYCHOL, V16, P769, DOI 10.1080/09541440340000330
   Lee CM, 2005, IEEE T SPEECH AUDI P, V13, P293, DOI 10.1109/TSA.2004.838534
   Lin YL, 2005, PROCEEDINGS OF 2005 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-9, P4898
   Massaro DW, 2000, COGNITION EMOTION, V14, P313, DOI 10.1080/026999300378833
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Meeren HKM, 2005, P NATL ACAD SCI USA, V102, P16518, DOI 10.1073/pnas.0507650102
   Mitchell T. M., 1997, MACHINE LEARNING
   Mobbs D, 2006, SOC COGN AFFECT NEUR, V1, P95, DOI 10.1093/scan/nsl014
   MOWER E, 2008, P INT S MULT ISM BER
   Mower E, 2008, INT CONF ACOUST SPEE, P2201, DOI 10.1109/ICASSP.2008.4518081
   Mower E, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P961, DOI 10.1109/ICME.2008.4607596
   NICOLAO M, 2006, P INT C SPOK LANG PR, P1794
   *OFF MR BILL, DREAMS PROD
   Oudeyer P., 2003, INT J HUM-COMPUT ST, V59, P157, DOI DOI 10.1016/S1071-581(02)00141-6
   Picard R. W., 1997, AFFECTIVE COMPUTING
   Rani P, 2006, PATTERN ANAL APPL, V9, P58, DOI 10.1007/s10044-006-0025-y
   Sutton S., 1998, P INT C SPOK LANG PR, P3221
   Swartout W, 2006, AI MAG, V27, P96
   Vogt T, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P474, DOI 10.1109/ICME.2005.1521463
NR 41
TC 41
Z9 43
U1 1
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2009
VL 11
IS 5
BP 843
EP 855
DI 10.1109/TMM.2009.2021722
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 474KN
UT WOS:000268282300005
DA 2024-07-18
ER

PT J
AU Shih, HC
   Hwang, JN
   Huang, CL
AF Shih, Huang-Chia
   Hwang, Jenq-Neng
   Huang, Chung-Lin
TI Content-Based Attention Ranking Using Visual and Contextual Attention
   Model for Baseball Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention modeling; contextual analysis; information retrieval;
   interactive systems; sports videos
ID RELEVANCE FEEDBACK; EXTRACTION; FRAMEWORK
AB The attention analysis of multimedia data is challenging since different models have to be constructed according to different attention characteristics. This paper analyzes how people are excited about the watched video content and proposes a content-driven attention ranking strategy which enables client users to iteratively browse the video according to their preference. The proposed attention rank (AR) algorithm, which is extended from the Google PageRank algorithm that sorts the websites; based on the importance, can effectively measure the user interest (W) level for each video frame. The degree of attention is derived by integrating the object-based visual attention model (VAM) with the contextual attention model (CAM), which not only can more reliably take advantage of the human perceptual characteristics, but also can effectively identify which video content may attract users' attention. The information of users' feedback is utilized in re-ranking procedure to further improve the retrieving accuracy. The proposed algorithm is specifically evaluated on broadcasted baseball videos.
C1 [Shih, Huang-Chia; Huang, Chung-Lin] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu, Taiwan.
   [Huang, Chung-Lin] Fo Guang Univ, Dept Informat, Ilan, Taiwan.
   [Hwang, Jenq-Neng] Univ Washington, Dept Elect Engn, Informat Proc Lab, Seattle, WA 98195 USA.
C3 National Tsing Hua University; Fo Guang University; University of
   Washington; University of Washington Seattle
RP Shih, HC (corresponding author), Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu, Taiwan.
EM hc.shih@ieee.org; hwang@u.washington.edu; huang.chunglin@gmail.com
RI Shih, Huang-Chia/AAH-4966-2021
CR [Anonymous], 1998, Computer Networks and ISDN Systems, DOI [DOI 10.1016/S0169-7552(98)00110-X, 10.1016/S0169-7552(98)00110-X]
   [Anonymous], 19990120 STANF U STA
   [Anonymous], 1991, Grammar of the film language
   Babaguchi N, 2004, IEEE T MULTIMEDIA, V6, P575, DOI [10.1109/TMM.2004.830811, 10.1109/tmm.2004.830811]
   Beaver Frank E., 1994, Dictionary of Film Terms
   Bicego M, 2006, COMPUT VIS IMAGE UND, V102, P22, DOI 10.1016/j.cviu.2005.09.001
   Chang SF, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P313, DOI 10.1145/266180.266382
   Chen HC, 2007, IET ELECTR POWER APP, V1, P43, DOI 10.1049/iet-epa:20060140
   Choi KH, 2005, IEEE T MULTIMEDIA, V7, P628, DOI 10.1109/TMM.2005.850964
   Cornsweet T.N., 1970, Visual Perception
   DAVENPORT G, 2002, IEEE COMPUT GRAPH, V11, P121
   Doulamis AD, 2004, IEEE T CIRC SYST VID, V14, P757, DOI 10.1109/TCSVT.2004.828348
   Doulamis N. D., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P116, DOI 10.1109/ICIP.1999.822866
   HENZINGER MR, 2001, IEEE INTERNET COMPUT, V1, P45
   Huang CL, 2006, IEEE T MULTIMEDIA, V8, P749, DOI 10.1109/TMM.2006.876289
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   ITTI L, 1999, P SPIE HUMAN VISION, V4
   Kim C, 2002, IEEE T CIRC SYST VID, V12, P122, DOI 10.1109/76.988659
   Kim C, 2001, J VLSI SIG PROC SYST, V29, P7, DOI 10.1023/A:1011115312953
   KIM C, 1999, P IEEE ICIP 99 KOB J
   Kim MK, 1997, IEEE T CONSUM ELECTR, V43, P1010, DOI 10.1109/30.642366
   Li BX, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P169
   LIE WN, 2005, P IEEE ICME 05 JUL 6
   Liu KH, 2008, IEEE T MULTIMEDIA, V10, P240, DOI 10.1109/TMM.2007.911826
   Lyu MR, 2005, IEEE T CIRC SYST VID, V15, P243, DOI 10.1109/TCSVT.2004.841653
   Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410
   Naphade MR, 2002, IEEE T CIRC SYST VID, V12, P40
   Naphade MR, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P475, DOI 10.1109/ICME.2000.869642
   ROSENFELD A, 1976, IEEE T SYST MAN CYB, V6, P420, DOI 10.1109/TSMC.1976.4309519
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Ruthven I, 2003, KNOWL ENG REV, V18, P95, DOI 10.1017/S0269888903000638
   Shih HC, 2005, IEEE T BROADCAST, V51, P449, DOI 10.1109/TBC.2005.854169
   SHIH HC, 2007, P IEEE MMSP07 CHAN C
   Shih HC, 2008, IEEE T BROADCAST, V54, P333, DOI 10.1109/TBC.2008.2001143
   SONG SH, 2002, P IEEE ICPR 02 AUG 1
   TSOTSOS JK, 1995, ARTIF INTELL, V78, P507, DOI 10.1016/0004-3702(95)00025-9
   Walther D, 2006, NEURAL NETWORKS, V19, P1395, DOI 10.1016/j.neunet.2006.10.001
   [张爱敏 ZHANG Aimin], 2006, [贵金属, Precious Metals], V27, P33
   ZHANG D, 2002, P IEEE ICIP 02 SEP
   Zhang RF, 2006, IMAGE VISION COMPUT, V24, P211, DOI 10.1016/j.imavis.2005.11.004
   ZHANG Z, 1999, P IEEE ICTAI 99, P121
   ZHONG D, 1997, P IEEE ICIP 97 SANT
NR 42
TC 10
Z9 12
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2009
VL 11
IS 2
SI SI
BP 244
EP 255
DI 10.1109/TMM.2008.2009682
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EG
UT WOS:000262714800006
DA 2024-07-18
ER

PT J
AU Li, F
   Dai, QG
   Xu, WL
   Er, GH
AF Li, Fei
   Dai, Qionghai
   Xu, Wenli
   Er, Guihua
TI Multilabel Neighborhood Propagation for Region-Based Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Label propagation; manifold ranking; region-based image retrieval;
   relevance feedback; semi-supervised learning
ID RELEVANCE FEEDBACK; FEATURE-SELECTION
AB Content-based image retrieval (CBIR) has been an active research topic in the last decade. As one of the promising approaches, graph-based semi-supervised learning has attracted many researchers. However, while the related work mainly focused on global visual features, little attention has been paid to region-based image retrieval (RBIR). In this paper, a framework based on multilabel neighborhood propagation is proposed for RBIR, which can be characterized by three key properties: 1) For graph construction, in order to determine the edge weights robustly and automatically, mixture distribution is introduced into the earth mover's distance (EMD) and a linear programming framework is involved. 2) Multiple low-level labels for each image can be obtained based on a generative model, and the correlations among different labels are explored when the labels are propagated simultaneously on the weighted graph. 3) By introducing multilayer semantic representation (MSR) and support vector machine (SVM) into the long-term learning, more exact weighted graph for label propagation and more meaningful high-level labels to describe the images can be calculated. Experimental results, including comparisons with the state-of-the-art retrieval systems, demonstrate the effectiveness of our proposal.
C1 [Li, Fei; Dai, Qionghai; Xu, Wenli; Er, Guihua] Tsinghua Univ, Tsinghua Natl Lab Informat Sci, Dept Automat, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Li, F (corresponding author), Tsinghua Univ, Tsinghua Natl Lab Informat Sci, Dept Automat, Beijing 100084, Peoples R China.
EM f-104@mails.tsinghua.edu.cn; qhdai@mail.tsinghua.edu.cn;
   xuwl@mail.tsinghua.edu.cn; ergh@mail.tsinghua.edu.cn
RI Dai, Qionghai/ABD-5298-2021
OI Dai, Qionghai/0000-0001-7043-3061
FU National Natural Science Foundation of China [60772048, 60525111,
   60721003]
FX Manuscript received November 18, 2007: revised May 20, 2008. Current
   version published December 10, 2008. This work was supported by the
   National Natural Science Foundation of China under Grants 60772048,
   60525111 and 60721003. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Alan Hanjalic.
CR [Anonymous], 2004, IEEE COMP SOC C COMP
   [Anonymous], 2007, P 15 ACM INT C MULT, DOI DOI 10.1145/1291233.1291430
   [Anonymous], P ACM INT C MULT
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], Learning from labeled and unlabeled data with label propagation
   Carson C, 2002, IEEE T PATTERN ANAL, V24, P1026, DOI 10.1109/TPAMI.2002.1023800
   Chen YX, 2005, IEEE T IMAGE PROCESS, V14, P1187, DOI 10.1109/TIP.2005.849770
   Chen YX, 2002, IEEE T PATTERN ANAL, V24, P1252, DOI 10.1109/TPAMI.2002.1033216
   Datta AK, 2008, OPT LASER TECHNOL, V40, P1, DOI 10.1016/j.optlastec.2007.04.006
   He J., 2004, P 12 ANN ACM INT C M, P9, DOI DOI 10.1145/1027527.1027531
   He JR, 2006, IEEE T IMAGE PROCESS, V15, P3170, DOI 10.1109/TIP.2006.877491
   Jain AK, 1998, PATTERN RECOGN, V31, P1369, DOI 10.1016/S0031-3203(97)00131-3
   Jiang W, 2006, IEEE T IMAGE PROCESS, V15, P702, DOI 10.1109/TIP.2005.863105
   Jiang W, 2005, PROC CVPR IEEE, P244
   Jiang W, 2005, PATTERN RECOGN, V38, P2007, DOI 10.1016/j.patcog.2005.03.007
   Jing F, 2004, IEEE T IMAGE PROCESS, V13, P699, DOI 10.1109/TIP.2004.826125
   Jing F, 2004, IEEE T CIRC SYST VID, V14, P672, DOI 10.1109/TCSVT.2004.826775
   Kang F., 2006, CVPR, V2, P1719
   Law MHC, 2004, IEEE T PATTERN ANAL, V26, P1154, DOI 10.1109/TPAMI.2004.71
   Liu Y, 2007, PATTERN RECOGN, V40, P262, DOI 10.1016/j.patcog.2006.04.045
   Lu K, 2006, PATTERN RECOGN, V39, P717, DOI 10.1016/j.patcog.2005.11.009
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Santini S, 1999, IEEE T PATTERN ANAL, V21, P871, DOI 10.1109/34.790428
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Szummer M., 2002, ADV NEURAL INFORM PR
   TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999
   Tang J., 2007, P 15 ACM INT C MULT, P297
   Tao DC, 2006, IEEE T PATTERN ANAL, V28, P1088, DOI 10.1109/TPAMI.2006.134
   Tong H, 2005, PROC CVPR IEEE, P230
   TONG H, 2005, P ACM MULT 2005, P862, DOI DOI 10.1145/1101149.1101337
   Tong S, 2001, P 9 ACM INT C MULT, P107, DOI DOI 10.1145/500141.500159
   Vapnik V., 1999, NATURE STAT LEARNING
   WANG F, 2006, P INT C MACH LEARN
   Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P947, DOI 10.1109/34.955109
   Zhang RF, 2007, IEEE T IMAGE PROCESS, V16, P562, DOI 10.1109/TIP.2006.888350
   Zhou Dengyong, 2003, ADV NEURAL INFORM PR
   Zhou XS, 2003, MULTIMEDIA SYST, V8, P536, DOI 10.1007/s00530-002-0070-3
   Zhou ZH, 2006, ACM T INFORM SYST, V24, P219, DOI 10.1145/1148020.1148023
   Zhu X.J, 2003, P 20 INT C MACH LEAR
NR 42
TC 24
Z9 26
U1 1
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2008
VL 10
IS 8
BP 1592
EP 1604
DI 10.1109/TMM.2008.2004914
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 390KS
UT WOS:000262163600014
DA 2024-07-18
ER

PT J
AU Zhai, GT
   Cai, JF
   Lin, WS
   Yang, XK
   Zhang, WJ
   Etoh, M
AF Zhai, Guangtao
   Cai, Jianfei
   Lin, Weisi
   Yang, Xiaokang
   Zhang, Wenjun
   Etoh, Minoru
TI Cross-Dimensional Perceptual Quality Assessment for Low Bit-Rate Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE International Symposium on Circuits and Systems
CY MAY 27-30, 2007
CL New Orleans, LA
SP IEEE
DE Perceptual visual quality; subjective view test; video adaptation; video
   quality assessment
ID IMPACT
AB Most studies in the literature for video quality assessment have been focused on the evaluation of quantized video sequences at fixed and high spatial and temporal resolutions. Only limited work has been reported for assessing video quality under different spatial and temporal resolutions. In this paper, we consider a wider scope of video quality assessment in the sense of considering multiple dimensions. In particular, we address the problem of evaluating perceptual visual quality of low bit-rate videos under different settings and requirements. Extensive subjective view tests for assessing the perceptual quality of low bit-rate videos have been conducted, which cover 150 test scenarios and include five distinctive dimensions: encoder type, video content, bit rate, frame size, and frame rate. Based on the obtained subjective testing results, we perform thorough statistical analysis to study the influence of different dimensions on the perceptual quality and some interesting observations are pointed out. We believe such a study brings new knowledge into the topic of cross-dimensional video quality assessment and it has immediate applications in perceptual video adaptation for scalable video over mobile networks.
C1 [Zhai, Guangtao; Cai, Jianfei; Lin, Weisi] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Zhai, Guangtao; Yang, Xiaokang; Zhang, Wenjun] Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai 200240, Peoples R China.
   [Etoh, Minoru] Nippon Telegraph & Tel Corp, DoCoMo Res Labs, Yokosuka, Kanagawa, Japan.
C3 Nanyang Technological University; Shanghai Jiao Tong University; Nippon
   Telegraph & Telephone Corporation
RP Zhai, GT (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Informat Proc, Shanghai 200240, Peoples R China.
EM zhaiguangtao@sjtu.edu.cn; asjfcai@ntu.edu.sg; wslin@ntu.edu.sg;
   xkyang@sjtu.edu.cn; zhangwenjun@sjtu.edu.cn; etoh@ieee.org
RI Lin, Weisi/A-3696-2011; Zhang, Wenjun/GNH-2095-2022; Lin,
   Weisi/A-8011-2012; Yang, Xiaokang/C-6137-2009; Zhai,
   Guangtao/X-5949-2019; Liu, Anmin/A-4730-2012; Cai, Jianfei/A-3691-2011
OI Lin, Weisi/0000-0001-9866-1947; Zhang, Wenjun/0000-0002-5282-3725; Yang,
   Xiaokang/0000-0003-4029-3322; Zhai, Guangtao/0000-0001-8165-9322; Cai,
   Jianfei/0000-0002-9444-3763
CR ALEKSANDR S, 2003, P SOC PHOTO-OPT INS, V5294, P82
   [Anonymous], 2000, FIN REP VID QUAL EXP
   [Anonymous], 2002, 50011 ITUR BT
   Cranley N, 2005, MULTIMEDIA SYST, V10, P392, DOI 10.1007/s00530-005-0168-5
   Ghinea G, 2005, IEEE T MULTIMEDIA, V7, P786, DOI 10.1109/TMM.2005.850960
   Hogg R. V., 1987, ENG STAT
   *ITU T, 2005, H264 ITUT
   Lu ZK, 2005, PROC SPIE, V5666, P554, DOI 10.1117/12.596845
   Martens JB, 2002, P IEEE, V90, P133, DOI 10.1109/5.982411
   Pastrana-Vidal RR, 2004, P SOC PHOTO-OPT INS, V5292, P182, DOI 10.1117/12.525746
   Rajendran RK, 2002, 2002 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL I, PROCEEDINGS, P445
   SANKAR PV, 1988, IEEE T PATTERN ANAL, V10, P271, DOI 10.1109/34.3889
   Snedecor GW, 1983, Statistical Methods, V6th
   Vatakis A, 2006, NEUROSCI LETT, V405, P132, DOI 10.1016/j.neulet.2006.06.041
   Video Quality Experts Group (VQEG), 2003, VQEG FIN REP FR TV P
   Wang Y., 2004, P 2004 IEEE ITN C MU, V3, P1119
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yuen M, 2006, SIGN PROC COMMUN SER, V28, P87
   Yuen M, 1998, SIGNAL PROCESS, V70, P247, DOI 10.1016/S0165-1684(98)00128-5
NR 19
TC 83
Z9 89
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2008
VL 10
IS 7
BP 1316
EP 1324
DI 10.1109/TMM.2008.2004910
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 378HB
UT WOS:000261310700009
DA 2024-07-18
ER

PT J
AU Jiang, H
   Fels, S
   Little, JJ
AF Jiang, Hao
   Fels, Sidney
   Little, James J.
TI Optimizing Multiple Object Tracking and Best View Video Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic programming; linear programming; multiple object tracking; video
   synthesis
ID MULTITARGET TRACKING; ALGORITHM
AB We study schemes to tackle problems of optimizing multiple object tracking and best-view video synthesis. A novel linear relaxation method is proposed for the class of multiple object tracking problems where the inter-object interaction metric is convex and the intra-object term quantifying object state continuity may use any metric. This scheme models object tracking as multi-path searching. It explicitly models track interaction, such as object spatial layout consistency or mutual occlusion, and optimizes multiple object tracks simultaneously. The proposed scheme does not rely on track initialization and complex heuristics. It has much less average complexity than previous efficient exhaustive search methods such as extended dynamic programming and can find the global optimum with high probability. Given the tracking data from our method, optimizing best-view video synthesis using multiple-view videos is further studied, which is formulated as a recursive decision problem and optimized by a dynamic programming approach. The proposed object tracking and best-view synthesis methods have found successful applications in My View-a system to enhance media content presentation of multiple-view video.
C1 [Jiang, Hao] Boston Coll, Dept Comp Sci, Chestnut Hill, MA 02467 USA.
   [Fels, Sidney] Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
C3 Boston College; University of British Columbia
RP Jiang, H (corresponding author), Boston Coll, Dept Comp Sci, Chestnut Hill, MA 02467 USA.
EM hjiang@cs.bc.edu; ssfels@ece.ubc.ca; little@cs.ubc.ca
OI Fels, Sidney/0000-0001-9279-9021
CR [Anonymous], EUR C COMP VIS
   [Anonymous], IEEE C COMP VIS PATT
   Bar-Shalom Y., 1988, TRACKING DATA ASS
   Chvatal V., 1983, Linear programming
   Cox IJ, 1996, IEEE T PATTERN ANAL, V18, P138, DOI 10.1109/34.481539
   Jiang H., 2007, IEEE C COMP VIS PATT
   KITAGAWA G, 1987, J AM STAT ASSOC, V82, P1032, DOI 10.2307/2289375
   Mahler RPS, 2003, IEEE T AERO ELEC SYS, V39, P1152, DOI 10.1109/TAES.2003.1261119
   MOREFIELD CL, 1977, IEEE T AUTOMAT CONTR, V22, P302, DOI 10.1109/TAC.1977.1101500
   Moreira P. M., 2006, International Journal of Simulation Modelling, V5, P167, DOI 10.2507/IJSIMM05(4)4.081
   NG KC, 2002, IMAGE VIS COMPUT JUN
   Nillius P., 2006, IEEE C COMP VIS PATT
   NUMMIARO K, 2003, LECT NOTES COMPUTER, V2781
   Okuma Kenji, 2004, EUR C COMP VIS
   Poore A. B., 1994, Computational Optimization and Applications, V3, P27, DOI 10.1007/BF01299390
   REID DB, 1979, IEEE T AUTOMAT CONTR, V24, P843, DOI 10.1109/TAC.1979.1102177
   SALARI V, 1990, IEEE T PATTERN ANAL, V12, P87, DOI 10.1109/34.41387
   SETHI IK, 1987, IEEE T PATTERN ANAL, V9, P56, DOI 10.1109/TPAMI.1987.4767872
   Storms PPA, 2003, COMPUT OPER RES, V30, P1067, DOI 10.1016/S0305-0548(02)00057-6
   Sudderth E., 2004, NEURAL INFORM PROCES
   VAZQUEZ PP, 2003, LNCS, V2669
   WOLF JK, 1989, IEEE T AERO ELEC SYS, V25, P287, DOI 10.1109/7.18692
NR 22
TC 25
Z9 29
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 997
EP 1012
DI 10.1109/TMM.2008.2001379
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600005
DA 2024-07-18
ER

PT J
AU Hsiao, JL
   Hung, HP
   Chen, MS
AF Hsiao, Jung-Lee
   Hung, Hao-Ping
   Chen, Ming-Syan
TI Versatile transcoding proxy for Internet content adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content adaptation; transcoding proxy
AB Recent technology advances in multimedia communication have ushered in a new era of personal communication. Users can ubiquitously access the Internet via various mobile devices. For the mobile devices featured with lower bandwidth network connectivity, transcoding can be used to reduce the object size by lowering the quality of a multimedia object. In view of the monolithic transcoders which only provide transcoding services and have limited performances due to the unknown data types and protocols in the prior research, we propose the architecture of versatile transcoding proxy (VTP). Based on the concept of the agent system, the VTP architecture can accept and execute the transcoding preference script provided by the client or the server to transform the corresponding data or protocol according to the user's specification. In order to enhance the effectiveness of the VTP architecture, we adopt the concept of dynamic cache categories and propose the scheme Maximum Profit Replacement with Dynamic Cache Categories (DCC-MPR). Based on the weighted transcoding graph, scheme DCC-MPR performs cache replacement according to the content in the caching candidate set, which is generated by the concept of dynamic programming, The experimental results show that the proposed architecture VTP and the corresponding scheme DCC-MPR have better performances in many aspects compared to the conventional transcoding proxy systems.
C1 [Hsiao, Jung-Lee] Natl Taiwan Univ, Dept Elect Engn, Taipei 10617, Taiwan.
   [Hung, Hao-Ping; Chen, Ming-Syan] Natl Taiwan Univ, Grad Inst Commun Engn, Taipei 106, Taiwan.
C3 National Taiwan University; National Taiwan University
RP Hsiao, JL (corresponding author), MediaTek Inc, Opt Storage Business Unit, Syst Dev Div 1, Hsinchu 300, Taiwan.
EM JL_Hsiao@mtk.com.tw; hphung@arbor.ee.ntu.edu.tw; mschen@cc.ee.ntu.edu.tw
OI Chen, Ming-Syan/0000-0002-0711-8197
FU National Science Council of Taiwan [NSC93-2752-E-002-006-PAE]
FX This work was supported in part by the National Science Council of
   Taiwan, R.O.C., under Contracts NSC93-2752-E-002-006-PAE.
CR [Anonymous], RESOURCE DESCRIPTION
   BARRETT R, 1999, IBM SYST J, V38
   BHARADVAJ H, 1998, P 17 IEEE S REL DIST
   BUTLER MH, 2002, HPL200235
   CARDELLINI V, 2000, ACM CIKM
   CHANDRA S, 2001, MULTIMEDIA COMPUTING
   CHANDRA S, 2000, IEEE INFOCOM
   Chang CY, 2003, IEEE T PARALL DISTR, V14, P611, DOI 10.1109/TPDS.2003.1206507
   Chang CY, 2004, IEEE T MULTIMEDIA, V6, P936, DOI 10.1109/TMM.2004.834857
   Chen MS, 2003, IEEE T KNOWL DATA EN, V15, P161, DOI 10.1109/TKDE.2003.1161588
   Cormen, INTRO ALGORITHMS
   *EEM, EEMA262
   ELSON J, IETF INTERNET DRAFTS
   FREED N, 2046 IETF RFC
   Han R, 1998, IEEE PERS COMMUN, V5, P8, DOI 10.1109/98.736473
   *IBM, WEBSPH TRANSC PUBL
   KNUTSSON B, 2002, P 7 INT WORKSH WEB C
   MAGLIO PP, 2000, COMMUN ACM       AUG
   MAHESHWARI A, 2002, RES ISSUES DATA EN E
   Mohan R, 1999, IEEE T MULTIMEDIA, V1, P104, DOI 10.1109/6046.748175
   MYERS J, 1939 IETF RFC
   PHAN T, 2002, IEEE ICDCS
   Sniedovich M., 1992, DYNAMIC PROGRAMMING
   *W3C, COMP CAP PREF PROF C
NR 24
TC 16
Z9 24
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2008
VL 10
IS 4
BP 646
EP 658
DI 10.1109/TMM.2008.921852
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EK
UT WOS:000258767200010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Song, YQ
   Zhang, CS
AF Song, Yangqiu
   Zhang, Changshui
TI Content-based information fusion for semi-supervised music genre
   classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE information fusion; music genre classification; semi-supervised learning
AB In this paper, we propose an information fusion framework for the semi-supervised distance-based music genre classification problem. We make use of the regularized least-square framework as the basic classifier, which only involves the similarity scores among different music tracks. We present a similarity score that multiplies different scores based on different distance measures. Particularly the distance measures are not restricted to the Euclidean distance. By adding a weight to each single distance based score, we propose an expectation-maximization (EM) algorithm to adaptively learn the fusion scores. Experiments on real music data set show that our approach can give promising results.
C1 [Song, Yangqiu; Zhang, Changshui] Tsinghua Natl Lab Informat Sci & Technol, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Song, YQ (corresponding author), Tsinghua Natl Lab Informat Sci & Technol, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.
EM songyq99@mails.tsinghua.edu.cn; zcs@mail.tsinghua.edu.cn
RI Zhang, Chang/HTO-2939-2023; zhang, chao/IXD-9965-2023
OI Song, Yangqiu/0000-0002-7818-6090
CR [Anonymous], 2004, ISMIR
   AUCOUTURIER JJ, 2004, J NEGATIVE RESUSLTS, V1
   AUCOUTURIER JJ, 2002, ISMIR, P13
   Belkin M, 2005, LECT NOTES COMPUT SC, V3559, P486, DOI 10.1007/11503415_33
   Belkin M., 2006, J MACHINE LEARNING R, V1, P1
   CHUNG F. R. K., 1997, CBMS REGIONAL C SER, V92
   *ISMIR, 2004, ISMIR AUD DESC CONT
   KAPOOR A, 2006, P ADV NEURAL INF PRO, V18, P627
   Kim HC, 2006, IEEE T PATTERN ANAL, V28, P1948, DOI 10.1109/TPAMI.2006.238
   LI T, 2003, P 26 ANN INT ACM SIG, P282
   Li T, 2006, IEEE T MULTIMEDIA, V8, P564, DOI 10.1109/TMM.2006.870730
   LIDY T, 2005, P 6 INT C MUS INF RE, P34
   Logan B., 2001, ICME, P22
   MOERCHEN F, 2006, ACM SIGKDD, P882
   Pampalk E., 2003, P 4 INT C MUS INF RE, P201
   Pampalk E., 2002, Proceedings of the tenth ACM international conference on Multimedia, P570, DOI DOI 10.1145/641007.641121
   Rifkin R., 2003, Computer and Systems Sciences, V190, P131
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Scaringella N, 2006, IEEE SIGNAL PROC MAG, V23, P133, DOI 10.1109/MSP.2006.1598089
   Scholkopf B., 2001, P 14 ANN C COMP LEAR, P416
   Shen JL, 2006, IEEE T MULTIMEDIA, V8, P1179, DOI 10.1109/TMM.2006.884618
   Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   Ying S, 2007, PROCEEDINGS OF THE 26TH CHINESE CONTROL CONFERENCE, VOL 3, P729
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhu X., 2003, P 20 INT C MACH LEAR, V3, P58, DOI DOI 10.1109/18.850663
   ZHU X, 1530 U WISC MAD
NR 27
TC 26
Z9 28
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2008
VL 10
IS 1
BP 145
EP 152
DI 10.1109/TMM.2007.911305
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 245QZ
UT WOS:000251952200014
DA 2024-07-18
ER

PT J
AU Song, ML
   Dong, Z
   Theobalt, C
   Wang, HQ
   Liu, ZC
   Seidel, HP
AF Song, Mingli
   Dong, Zhao
   Theobalt, Christian
   Wang, Huiqiong
   Liu, Zicheng
   Seidel, Hans-Peter
TI A generic framework for efficient 2-d and 3-d facial expression analogy
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE expression analogy; facial animation; facial image; synthesis
ID FACES
AB Facial expression analogy provides computer animation professionals with a tool to map expressions of an arbitrary source face onto an arbitrary target face. In the recent past, several algorithms have been presented in the literature that aim at putting the expression analogy paradigm into practice. Some of these methods exclusively handle expression mapping between 3-D face models, while others enable the transfer of expressions between images of faces only. None of them, however, represents a more general framework that can be applied to either of these two face representations. In this paper, we describe a novel generic method for analogy-based facial animation that employs the same efficient framework to transfer facial expressions between arbitrary 3-D face models, as well as between images of performer's faces. We propose a novel geometry encoding for triangle meshes, vertex-tent-coordinates, that enables us to formulate expression transfer in the 2-D and the 3-D case as a solution to a simple system of linear equations. Our experiments show that our method outperforms many previous analogy-based animation approaches in terms of achieved animation quality, computation time and generality.
C1 Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Peoples R China.
   Max Planck Inst Informat, Comp Grap Grp, D-66123 Saarbrucken, Germany.
   Zhejiang Univ, Dept Informat Sci & Elect Engn, Hangzhou 310027, Peoples R China.
   Microsoft Res, Redmond, WA 98052 USA.
C3 Zhejiang University; Max Planck Society; Zhejiang University; Microsoft
RP Song, ML (corresponding author), Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Peoples R China.
EM brooksong@ieec.org; dong@mpi-sb.mpg.de; theobalt@mpi-sb.mpg.de;
   huiqiongw@gmail.com; zliu@microsoft.com; hpseidel@mpi-sb.mpg.de
RI Wang, Hui-Qiong/H-4690-2011; dong, zhao/JHS-9392-2023
OI Theobalt, Christian/0000-0001-6104-6625
CR BEIER T, 1992, COMP GRAPH, V26, P35, DOI 10.1145/142920.134003
   Blanz V, 2004, COMPUT GRAPH FORUM, V23, P669, DOI 10.1111/j.1467-8659.2004.00799.x
   Botsch M., 2006, P VISION MODELING VI, P357
   Brennan S. E., 1982, Caricature generator
   CHALLIS JH, 1995, J BIOMECH, V28, P733, DOI 10.1016/0021-9290(94)00116-L
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cosatto E, 2000, IEEE T MULTIMEDIA, V2, P152, DOI 10.1109/6046.865480
   Ekman P, 1978, FACIAL ACTION CODING
   Gao W, 2003, IEEE T CIRC SYST VID, V13, P1119, DOI 10.1109/TCSVT.2003.817629
   Gautschi W., 1997, Numerical Analysis
   Gokturk SB, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P287, DOI 10.1109/AFGR.2002.1004168
   GU X, 2002, SIGGRAPH 02, P355, DOI DOI 10.1145/566570.566589
   Kahler K., 2001, PROC GRAPHICS INTERF, P37
   Lee Y.C., 1995, SIGGRAPH Proceedings, P55
   LITWINOWICZ P, 1994, SIGGRAPH 94, P409, DOI DOI 10.1145/192161.192270
   Liu ZC, 2001, COMP GRAPH, P271
   Noh JY, 2001, COMP GRAPH, P277, DOI 10.1145/383259.383290
   Ostermann J, 1998, COMP ANIM CONF PROC, P49, DOI 10.1109/CA.1998.681907
   Park B, 2005, COMPUT ANIMAT VIRT W, V16, P291, DOI 10.1002/cav.81
   PARKE F, 1998, COMPUTER FACIAL ANIM
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   PIGHIN F, 1998, SIGGRAPH P, P75
   Pyun H., 2003, SIGGRAPH/EUROGRAPHICS Symposium on Computer Animation, P167
   Sheffer A, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P68
   Song ML, 2006, IEEE IMAGE PROC, P2101, DOI 10.1109/ICIP.2006.312822
   Sorkine O., 2004, P 2004 EUR ACM SIGGR, P179
   Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736
   Wang Y, 2004, COMPUT GRAPH FORUM, V23, P677, DOI 10.1111/j.1467-8659.2004.00800.x
   Waters K., 1987, ACM SIGGRAPH Comput. Graph., V21, P17
   Williams L., 1990, Computer Graphics, V24, P235, DOI 10.1145/97880.97906
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
   Zayer R, 2005, COMPUT GRAPH FORUM, V24, P601, DOI 10.1111/j.1467-8659.2005.00885.x
   Zhang L, 2004, ACM T GRAPHIC, V23, P548, DOI 10.1145/1015706.1015759
   ZHANG Q, 2003, P ACM SIGGRAPH EUR S, P177
NR 34
TC 32
Z9 34
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2007
VL 9
IS 7
BP 1384
EP 1395
DI 10.1109/TMM.2007.906591
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 224LB
UT WOS:000250447400005
DA 2024-07-18
ER

PT J
AU Shiang, HP
   van der Schaar, M
AF Shiang, Hsien-Po
   van der Schaar, Mihaela
TI Informationally decentralized video streaming over multihop wireless
   networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE decentralized information feedback; dynamic routing; multihop wireless
   networks; video streaming
ID TAIL PROBABILITIES
AB Various packet scheduling, dynamic routing, error-protection, and channel adaptation strategies have been proposed at different layers of the protocol stack to address multi-user video streaming over multihop wireless networks. However, these cross-layer transmission strategies can be efficiently optimized only if they use accurate information about the network conditions and hence, are able to timely adapt to network changes. Due to the informationally decentralized nature of the multihop wireless network, performing centralized optimization for delay-sensitive video streaming application based on global information about the network status is not practical. Distributed solutions that adapt the transmission strategies based on timely information feedback need to be considered. To acquire this information feedback for cross-layer adaptation, we deploy an overlay infrastructure, which is able to relay the necessary information about the network status and incurred delays across different network "horizons" (i.e., across a different number of hops in a predetermined period of time). In this paper, we propose a distributed streaming approach that is optimized based on the local information feedback acquired from the various network horizons. We investigate the distributed cross-layer adaptation at each wireless node by considering the advantages resulting from an accurate and frequent network information feedback from larger horizons as well as the drawbacks resulting from an increased transmission overhead. Based on the information feedback, we can estimate the risk that packets from different priority classes will not arrive at their destination before their decoding deadline expires. Subsequently, the various transmission strategies such as packet scheduling, retransmission limit and dynamic routing policies are adapted to jointly consider the estimated risk as well as the impact in terms of distortion of the different priority classes. Our results show that the proposed dynamic routing policy based on timely information feedback outperforms existing state-of-the-art on-demand routing solutions by more than 2 dB in terms of the received video quality.
C1 Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
C3 University of California System; University of California Los Angeles
RP Shiang, HP (corresponding author), Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
EM hpshiang@ee.ucla.edu; mihaela@ee.ucla.edu
CR ABATE J, 1995, OPER RES, V43, P885, DOI 10.1287/opre.43.5.885
   Andreopoulos Y, 2004, SIGNAL PROCESS-IMAGE, V19, P653, DOI 10.1016/j.image.2004.05.007
   ANDREOPOULOS Y, IN PRESS IEEE J SELE
   AWERBUCH B, 1994, P 26 ACM S THEOR COM
   Bertsekas D. P., 1992, Data Networks, V2nd
   BUTALA A, IN PRESS EURASIP J A
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   EVANS JR, 1993, OPTIMIZATION AGORITH
   Faccin SM, 2006, IEEE WIREL COMMUN, V13, P10, DOI 10.1109/MWC.2006.1632476
   *IEEE, 2003, 80211ED50 IEEE
   JANNOTTI J, 2002, P IEEE C OP ARCH NET
   Jiang YM, 2001, IEEE COMMUN LETT, V5, P175, DOI 10.1109/4234.917105
   Johnson D.B., 1996, MOBILE COMPUTING, P153181
   Jurca D., 2007, IEEE T MULTIMEDIA, V9
   Kleinrock L., 1975, Queuing Systems, VI
   KONHEIM AG, 1980, IEEE T COMMUN, V28, P1004, DOI 10.1109/TCOM.1980.1094766
   Krishnamachari B, 2002, 22ND INTERNATIONAL CONFERENCE ON DISTRIBUTED COMPUTING SYSTEMS WORKSHOP, PROCEEDINGS, P575, DOI 10.1109/ICDCSW.2002.1030829
   KRISHNASWAMY D, INTEL TECHNOL J
   krishnaswamy D., 2002, PROC 3G WIRELESS C, P165
   Marina MK, 2001, NETWORK PROTOCOLS, P14, DOI 10.1109/ICNP.2001.992756
   Ortega A, 1998, IEEE SIGNAL PROC MAG, V15, P23, DOI 10.1109/79.733495
   Perkins CE, 1999, WMCSA '99, SECOND IEEE WORKSHOP ON MOBILE COMPUTING SYSTEMS AND APPLICATIONS, PROCEEDINGS, P90, DOI 10.1109/MCSA.1999.749281
   Qiao D., 2002, IEEE T MOBILE COMPUT, V1, P278, DOI DOI 10.1109/TMC.2002.1175541
   Setton E, 2005, IEEE WIREL COMMUN, V12, P59, DOI 10.1109/MWC.2005.1497859
   SHIANG HP, 2006, P IEEE C INT INF HID, P255
   Toumpis S, 2003, IEEE T WIREL COMMUN, V2, P736, DOI 10.1109/TWC.2003.814342
   van der Schaar M, 2006, IEEE T MOBILE COMPUT, V5, P755, DOI 10.1109/TMC.2006.81
   van der Schaar M, 2007, IEEE T MULTIMEDIA, V9, P185, DOI 10.1109/TMM.2006.886384
   Waldvogel M, 2003, ACM SIGCOMM COMP COM, V33, P101, DOI 10.1145/774763.774779
   WEI W, 2002, P INT C BROADB NETW, P496
   Wu Y, 2005, IEEE J SEL AREA COMM, V23, P136, DOI 10.1109/JSAC.2004.837362
NR 31
TC 22
Z9 23
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2007
VL 9
IS 6
BP 1299
EP 1313
DI 10.1109/TMM.2007.902845
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 215XJ
UT WOS:000249842000018
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, SQ
   Shen, B
   Wee, SS
   Zhang, XD
AF Chen, Songqing
   Shen, Bo
   Wee, Susie
   Zhang, Xiaodong
TI SProxy: A caching infrastructure to support Internet streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content distribution; streaming; proxy caching; segmentation
ID PROXY; VIDEO
AB Many algorithmic efforts have been made to address technical issues in designing a streaming media caching proxy. Typical of those are segment-based caching approaches that efficiently cache large media objects in segments which reduces the startup latency while ensuring continuous streaming. However, few systems have been practically implemented and deployed. The implementation and deployment efforts are hindered by several factors: 1) streaming of media content in complicated data formats is difficult; 2) typical streaming protocols such as RTP often run on UDP; in practice, UDP traffic is likely to be blocked by firewalls at the client side due to security considerations; and 3) coordination between caching discrete object segments and streaming continuous media data is challenging. To address these problems, we have designed and implemented a segment-based streaming media proxy, called SProxy. This proxy system has the following merits. First, SProxy leverages existing Internet infrastructure to address the flash crowd. The content server is now free of the streaming duty while hosting streaming content through a regular Web server. Thus, UDP based streaming traffic from SProxy suffers less dropping and no blocking. Second, SProxy streams and caches media objects in small segments determined by the object popularity, causing very low startup latency, and significantly reducing network traffic. Finally, prefetching techniques are used to pro-actively preload uncached segments that are likely to be used soon, thus providing continuous streaming. SProxy has been extensively tested and we show that it provides high quality streaming delivery in both local area networks and wide area networks (e.g., between Japan and the U.S.).
C1 George Mason Univ, Dept Comp Sci, Fairfax, VA 22030 USA.
   Hewlett Packard Lab, Mobile & Media Syst Lab, Palo Alto, CA 94304 USA.
   Ohio State Univ, Dept Comp Sci Engn, Columbus, OH 43210 USA.
C3 George Mason University; Hewlett-Packard; University System of Ohio;
   Ohio State University
RP Chen, SQ (corresponding author), George Mason Univ, Dept Comp Sci, Fairfax, VA 22030 USA.
EM sqchen@cs.gmu.edu; bo.shen@hp.com; susie.wee@hp.com;
   zhang@cse.ohio-state.edu
CR ACHARYA S, 2000, P ACM NOSSDAV CHAP H
   BOMMAIAH E, 2000, P IEEE RTAS WASH DC
   Chae YS, 2002, IEEE J SEL AREA COMM, V20, P1328, DOI 10.1109/JSAC.2002.802062
   CHEN S, 2004, P IEEE INFOCOM HONG
   CHEN SQ, 2003, P 8 INT WORKSH WEB C
   CHERKASOVA L, 2002, P ACM NOSSDAV MIAM F
   CUETOS PD, 2001, P PACK VID WORKSH AP
   GIBSON GA, 1996, ACM WORKSH STRAT DIR
   Gruber S, 2000, COMPUT NETW, V33, P657, DOI 10.1016/S1389-1286(00)00058-X
   GUMMADI K, 2003, P 19 ACM S OP SYST P
   GUO L, 2005, P 14 INT WORLD WID C
   HUANG L, 2002, PROXY BASED TCP FRIE
   *INKTOMI, 1999, STREAM MED CACH WHIT
   KANGASHARJU J, 2001, P IEEE INFOCOM ANCH
   KIM T, 2001, P ACM NOSSDAV 2001 P
   KOENEN R, 2001, OVERVIEW MPEG4 VERSI
   MA W, 2000, P IEEE ICME, V2, P991
   Miao ZR, 2002, IEEE J SEL AREA COMM, V20, P1315, DOI 10.1109/JSAC.2002.802061
   PADMANABHAN VN, 2003, P IEEE INT C NETW PR
   REJAIE R, 1999, P ACM SIGCOMM CAMBR
   REJAIE R, 2000, P IEEE INFOCOM TEL A
   REJAIE R, 2001, P ACM NOSSDAV PORT J
   REJAIE R, 1999, P INT WEB CACB WORKS
   ROY S, 2003, P IEEE INT C MULT EX
   SAROIU S, 2002, P 5 S OPER SYST DES
   SCHOJER P, 2003, P WWW BUD HUNG MAY
   SCHULZRINNE H, 1996, 1889 RFC
   Schulzrinne H., 1998, RFC2326
   SEN S, 1999, P IEEE INFOCOM NEW Y
   WANG B, 2002, P IEEE INFOCOM NEW Y
   WOLMAN A, 1999, P 2 USENIX S INT TEC
   WU K, 2001, P WWW HONG KONG CHIN
   Zhang ZL, 2000, IEEE ACM T NETWORK, V8, P429, DOI 10.1109/90.865072
   [No title captured]
NR 34
TC 13
Z9 19
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2007
VL 9
IS 5
BP 1062
EP 1072
DI 10.1109/TMM.2007.898943
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 193ZX
UT WOS:000248314800015
OA Green Published
DA 2024-07-18
ER

PT J
AU Assfalg, J
   Bertini, M
   Del Bimbo, A
   Pala, P
AF Assfalg, Juergen
   Bertini, Marco
   Del Bimbo, Alberto
   Pala, Pietro
TI Content-based retrieval of 3-D objects using Spin Image Signatures
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D content-based retrieval; 3-D shape description; spin images
ID RECOGNITION
AB Retrieval by content of 3-D models is becoming more and more important due to the advancements in 3-D hardware and software technologies for acquisition, authoring and display of 3-D objects, their ever-increasing availability at affordable costs, and the establishment of open standards for 3-D data interchange. In this paper, we present a new method, referred to as Spin Image Signatures, that develops on the original spin images approach, with adaptations to support effective retrieval by content. According to the method proposed, a set of spin images is derived for each model, to obtain a view-independent description of its 3-D shape and a signature is evaluated for each spin image in the set. Clustering is hence performed on the set of Spin Image Signatures to obtain a compact representation. Experimental results are presented, showing the effectiveness of the Spin Image Signatures method for retrieval, also in comparison with other methods, and its sensitivity to model deformations.
C1 Univ Florence, Dipartimento Sistemi & Informat, I-50139 Florence, Italy.
C3 University of Florence
RP Assfalg, J (corresponding author), Univ Florence, Dipartimento Sistemi & Informat, I-50139 Florence, Italy.
EM assfalg@dsi.unifi.it; bertini@dsi.unifi.it; delbimbo@dsi.unifi.it;
   pala@dsi.unifi.it
RI Bertini, Marco/X-1325-2019
OI Bertini, Marco/0000-0002-1364-218X; PALA, PIETRO/0000-0001-5670-3774;
   DEL BIMBO, ALBERTO/0000-0002-1052-8322
CR [Anonymous], 1999, Visual Information Retrieval
   [Anonymous], ED PSYCHOL MEASUREME
   [Anonymous], Pattern Classification
   ASSFALG J, 2003, P INT C MULT EXP ICM
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Berchtold S., 1997, PROC SIGMOD 97, P564
   Bezdek J., 1999, FUZZY MODELS ALGORIT
   CHEN DY, 2003, EUR COMP GRAPH FOR E, V22
   Elad M., 2001, PROC EG MULTIMEDIA, P97, DOI DOI 10.2312/EGMM/EGMM01/107-118
   ELVINS TT, 1995, P VRML 95 U ANN C VI
   FROME A, 2004, P EUR C COMP VIS ECC
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   GOSHTASBY A, 1985, IEEE T PATTERN ANAL, V7, P738, DOI 10.1109/TPAMI.1985.4767734
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kim DJ, 2001, IEICE T INF SYST, VE84D, P281
   KOLONIAS I, 2001, P INT WORKSH CONT BA, V1
   Kriegel H.-P., 1998, GeoInformatica, V2, P113, DOI 10.1023/A:1009760031965
   Lamdan Y., 1988, PROC 2 INT C COMPUTE, P238
   Mahmoudi S, 2002, INT C PATT RECOG, P457, DOI 10.1109/ICPR.2002.1048337
   Mokhtarian F, 2001, IMAGE VISION COMPUT, V19, P271, DOI 10.1016/S0262-8856(00)00076-7
   Ohbuchi R., 2003, P 5 ACM SIGMM INT WO, P39, DOI DOI 10.1145/973264.973272
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   OSADA R, 2001, P SHAP MOD INT GEN I, V1
   Paquet E, 1999, IMAGE VISION COMPUT, V17, P157, DOI 10.1016/S0262-8856(98)00119-X
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Tangelder JWH, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P145, DOI 10.1109/SMI.2004.1314502
   VANDEBORRE JP, 2002, P 1 INT S 3D DAT PRO
   Wu KN, 1997, IEEE T PATTERN ANAL, V19, P1223, DOI 10.1109/34.632982
   ZHANG C, 2001, P ICIP 2001 THESS GR
   [No title captured]
NR 30
TC 62
Z9 72
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2007
VL 9
IS 3
BP 589
EP 599
DI 10.1109/TMM.2006.886271
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148PW
UT WOS:000245088100013
DA 2024-07-18
ER

PT J
AU Pohle, T
   Knees, P
   Schedl, M
   Pampalk, E
   Widmer, G
AF Pohle, Tim
   Knees, Peter
   Schedl, Markus
   Pampalk, Elias
   Widmer, Gerhard
TI "Reinventing the wheel": A novel approach to music player interfaces
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE feature extraction; music; music playlist generation; portable media
   players; user interfaces
ID RETRIEVAL
AB We present a novel interface to (portable) music players that benefits from intelligently structured collections of audio files. For structuring, we calculate similarities between every pair of songs and model a travelling salesman problem (TSP) that is solved to obtain a playlist (i.e., the track ordering during playback) where the average distance between consecutive pieces of music is minimal according to the similarity measure. The similarities are determined using both audio signal analysis of the music tracks and web-based artist profile comparison. Indeed, we will show how to enhance the quality of the well-established methods based on audio signal processing with features derived from web pages of music artists. Using a TSP allows for creating circular playlists that can be easily browsed with a wheel as input device. We investigate the usefulness of four different TSP algorithms for this purpose. For evaluating the quality of the generated playlists, we apply a number of quality measures to two real-world music collections. It turns out that the proposed combination of audio and text-based similarity yields better results than the initial approach based on audio data only. We implemented an audio player as Java applet to demonstrate the benefits of our approach. Furthermore, we present the results of a small user study conducted to evaluate the quality of the generated playlists.
C1 Johannes Kepler Univ, Dept Computat Percept, A-2020 Linz, Austria.
   Natl Inst Adv Ind Sci & Technol, Tsukuba, Ibaraki, Japan.
C3 Johannes Kepler University Linz; National Institute of Advanced
   Industrial Science & Technology (AIST)
RP Pohle, T (corresponding author), Johannes Kepler Univ, Dept Computat Percept, A-2020 Linz, Austria.
EM tim.pohle@jku.at; peter.knees@jku.at; markus.schedl@jku.at;
   elias.pampalk@gmail.com; gerhard.widmer@jku.at
RI Widmer, Gerhard/B-8218-2017
OI Widmer, Gerhard/0000-0003-3531-1282; Knees, Peter/0000-0003-3906-1292
CR Alghoniemy M., 2001, P IEEE INT C MULT EX
   [Anonymous], 2002, P IEEE INT C MULT EX
   [Anonymous], 1997, Local Search in Combinatorial Optimization
   Aucouturier Jean-Julien., 2004, J NEGATIVE RESULTS S, V1
   Aucouturier Jean-Julien., 2002, Proceedings of the 3rd International Conference on Music Information Retrieval, ISMIR, P157
   Aucouturier JJ, 2005, IEEE T MULTIMEDIA, V7, P1028, DOI 10.1109/TMM.2005.858380
   Bishop C. M., 1995, NEURAL NETWORKS PATT
   Goto Masataka, 2005, P 6 INT C MUS INF RE
   Helsgaun Keld, 1998, DATALOGISKE SKRIFTER, V81
   KOHONEN T, 1982, BIOL CYBERN, V43, P59, DOI 10.1007/BF00337288
   LOGAN B, 2002, P INT C MUS INF RETR
   Logan B., 2001, P IEEE INT C MULT EX, P745
   Logan B., 2000, ISMIR, V270, P11
   Pampalk E., 2002, Proceedings of the tenth ACM international conference on Multimedia, P570, DOI DOI 10.1145/641007.641121
   PAMPALK E, 2002, P INT C ART NEUR NET, P871
   Pohle T., 2005, THESIS TU KAISERSLAU
   Pohle T., 2005, P DAFX, P220
   SAHNI S, 1976, J ACM, V23, P555, DOI 10.1145/321958.321975
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   Schedl M, 2005, ELEKTROTECH INFORMAT, V122, P232, DOI 10.1007/BF03054434
   Schedl M., 2006, P 7 INT C MUS INF RE
   Skiena S.S., 1997, ALGORITHM DESIGN MAN
   TZANETAKIS G, 2003, P 6 INT C DIG AUD EF
NR 23
TC 18
Z9 20
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2007
VL 9
IS 3
BP 567
EP 575
DI 10.1109/TMM.2006.887991
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148PW
UT WOS:000245088100011
DA 2024-07-18
ER

PT J
AU Xie, B
   Zeng, WJ
AF Xie, Bo
   Zeng, Wenjun
TI Fast bitstream switching algorithms for real-time adaptive video
   multicasting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptive video multicasting; bitstream switching; drift minimization;
   real-time video
AB Bitstream switching among multiple bitstreams encoded at different bit rates is an effective way to address the bandwidth variation issue in transmitting multimedia over the Internet or wireless networks. This paper proposes two new fast real-time bitstream switching algorithms that aim to minimize the drifting error, while avoiding the problems of long delay, high complexity, and bit-rate overhead for storage and transmission that often occur in prior solutions. The basic idea is to choose a switching point in a neighborhood with the highest encoding quality, within a switching window determined by the switching delay constraint. We show that they can significantly outperform a simple switching algorithm, and achieve performance that is closer to an off-line mean-square-error-optimized bitstream switching solution, when compared to our previous work based on the similarity of the reference frames. The proposed schemes are especially useful in the scenario of real-time multicasting over dynamic heterogeneous networks, where multiple bitstreams with different bit rates are generated on the fly and dynamic bitstrearn switching is required for individual clients.
C1 PacketVideo Corp, San Diego, CA 92121 USA.
   Univ Missouri, Dept Comp Sci, Columbia, MO 65211 USA.
C3 University of Missouri System; University of Missouri Columbia
RP Xie, B (corresponding author), PacketVideo Corp, San Diego, CA 92121 USA.
EM xie@packetvideo.com; zengw@missouri.edu
CR CHOU YK, 2002, P IEEE PAC RIM C MUL
   FAERBER N, 1997, P IEEE INT C IM PROC
   FLOYD S, 2000, P ACM SIGCOMM SEP
   *ISO IEC, 1998, 144961 ISOIEC
   Karczewicz M, 2003, IEEE T CIRC SYST VID, V13, P637, DOI 10.1109/TCSVT.2003.814969
   KARCZEWICZ M, 2001, ITU T VID COD EXP GR
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   Sun XY, 2004, IEEE T MULTIMEDIA, V6, P291, DOI 10.1109/TMM.2003.822818
   Sun XY, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P541, DOI 10.1109/ICME.2002.1035838
   *TSG SA, 2001, 3GPPTS26234V151 TSGS
   Wu F, 2001, IEEE T CIRC SYST VID, V11, P332, DOI 10.1109/76.911159
   XIE B, 2003, P IEEE INT C MULT EX
   XIE B, 2002, P IEEE INT C IM PROC
   Xie B., 2004, P IEEE INT C MULT EX
   XIE B, 2005, IEEE INT C MULT EXP
   2003, ER 3GPP TSGSA4 26 M
NR 16
TC 4
Z9 5
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2007
VL 9
IS 1
BP 169
EP 175
DI 10.1109/TMM.2006.886381
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 119VE
UT WOS:000243041500016
DA 2024-07-18
ER

PT J
AU Heo, SP
   Suzuki, M
   Ito, A
   Makino, S
AF Heo, SP
   Suzuki, M
   Ito, A
   Makino, S
TI An effective music information retrieval method using three-dimensional
   continuous DP
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE continuous DP; dynamic melody representation; humming; multiple pitch
   candidates; music information retrieval
AB This paper describes a music information retrieval system that uses humming as the key for retrieval. Humming is an easy way for a user to input a melody. However, there are several problems with humming that degrade the retrieval of information. One problem is the human factor. Sometimes, people do not sing accurately, especially if they are inexperienced or unaccompanied. Another problem arises from signal processing. Therefore, a music information retrieval method should be sufficiently robust to surmount various humming errors and signal processing problems. A retrieval system has to extract the pitch from the user's humming. However, pitch extraction is not perfect. It often captures half or double pitches, which are harmonic frequencies of the true pitch, even if the extraction algorithms take the continuity or the pitch into account. Considering these problems, we propose a system that takes multiple pitch candidates into account. In addition to the frequencies of the pitch candidates, the confidence measures obtained from their powers are taken into consideration as well. We also propose the use of an algorithm with three dimensions that is an extension of the conventional Dynamic Programming (DP) algorithm, so that multiple pitch candidates can be treated. Moreover, in the proposed algorithm, DP paths are changed dynamically to take deltaPitches and IOIratins (inter-onset-interval) of input and reference notes into account in order to treat notes being split or unified. We carried out an evaluation experiment to compare the proposed system with a conventional system [6]. When using three-pitch candidates with conference measure and 101 features, the top-ten retrieval accuracy was 94.1%. Thus, the proposed method gave a better retrieval performance than the conventional system.
C1 Korea Telecom, Serv Dev Lab, Seoul 137792, South Korea.
   Tohoku Univ, Grad Sch Engn, Sendai, Miyagi 9808579, Japan.
C3 Tohoku University
RP Heo, SP (corresponding author), Korea Telecom, Serv Dev Lab, Seoul 137792, South Korea.
RI Ito, Akinori/AAC-3116-2019
OI Ito, Akinori/0000-0002-8835-7877
CR [Anonymous], 2001, P 9 ACM INT C MULTIM
   Boersma P., PRAAT
   Coden Anni., 2002, INFORM RETRIEVAL TEC
   de Cheveigné A, 2002, J ACOUST SOC AM, V111, P1917, DOI 10.1121/1.1458024
   Feng D., 2003, Multimedia Information Retrieval and Management: Technological Fundamentals and Applications
   Foote J.T., 1997, P AAAI 1997 SPRING S
   GHIAS A, 1995, P ACM MULTIMEDIA
   HASHIGUCHI H, 2001, SCI 2001, V7, P280
   HEO SP, 2003, INT OWRKSH AMR2003, P189
   HEO SP, 2003, P ISMIR 2003
   KAGEYAMA T, 1993, P INT COMP MUS C
   KLAPURI A, 1999, IEEE WORKSH APPL SIG, P115
   Kosugi N., 2000, ACM Multimedia 2000, P333
   MCNAB RJ, 1996, P 19 AUSTR COMP SCI
   MCNAB RJ, 1996, P ACM DIGITAL LIB
   Nishimura T., 2001, P ISMIR 2001, P211
   PAULUS J, 2002, P ISMIR 2002
   Pauws S., 2002, P ISMIR 2002
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   Shirokaze T., 1990, Transactions of the Institute of Electronics, Information and Communication Engineers A, VJ73A, P1537
   SONODA T, 1998, P ICMC 98, P343
   *STANF U, THEMEFINDER
   *U BONN, MIDILIB
   *U KARLSR, TUNESERVER
   WEI C, 2002, P INT S MUS INF RETR
NR 25
TC 7
Z9 7
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2006
VL 8
IS 3
BP 633
EP 639
DI 10.1109/TMM.2006.870717
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046OX
UT WOS:000237822000021
DA 2024-07-18
ER

PT J
AU Lin, CH
   Chen, ALP
AF Lin, CH
   Chen, ALP
TI Indexing and matching multiple-attribute strings for efficient
   multimedia query processing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE index structure; multimedia data; q-attribute string matching and query
   processing
AB Multimedia data can be represented as a multiple-attribute string of feature values corresponding to multiple features of the data. Therefore, the retrieval problem can be transformed into the q-attribute string matching problem if q features are considered in a query. A general solution is proposed in this paper. It includes an index structure and the matching methodologies, which can be applied on different values of q. The experiment results show the efficiency of the proposed approach.
C1 Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
   Natl Chengchi Univ, Dept Comp Sci, Taipei 116, Taiwan.
C3 National Tsing Hua University; National Chengchi University
RP Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
EM edgarlin@cs.nthu.edu.tw; alpchen@cs.nccu.edu.tw
CR [Anonymous], P 16 INT C DAT ENG S
   CHEN ALP, 2000, P IEEE INT C MULT EX
   Kahveci T, 2002, 14TH INTERNATIONAL CONFERENCE ON SCIENTIFIC AND STATISTICAL DATABASE MANAGEMENT, PROCEEDINGS, P175, DOI 10.1109/SSDM.2002.1029718
   Lee W, 2000, PROC SPIE, V3972, P177
   LIN CH, 2003, MAKETR03001
   LIN CH, 2001, P IEEE INT C IM PROC
   Liu CC, 2002, IEEE T KNOWL DATA EN, V14, P106, DOI 10.1109/69.979976
   MCCREIGHT EM, 1976, J ACM, V23, P262, DOI 10.1145/321941.321946
NR 8
TC 4
Z9 4
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2006
VL 8
IS 2
BP 408
EP 411
DI 10.1109/TMM.2005.864350
PG 4
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 028GY
UT WOS:000236476300020
DA 2024-07-18
ER

PT J
AU Fang, T
   Chau, LP
AF Fang, T
   Chau, LP
TI An error-resilient GOP structure for robust video transmission
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE error-prone networks; error resilience; group-of-pictures; reverse play;
   robust video transmission; VCR
AB Recent advances in technology have resulted in a significant growth in wireless communications and widespread access to information via the Internet, which have resulted in a strong demand for reliable transmission of video data. The challenge of robust video transmission is to protect the compressed data against hostile channel conditions while bringing little impact on bandwidth efficiency. In motion-compensated video-coding schemes, such as MPEG-1 or MPEG-2, an I frame normally is followed by several P frames and possibly B frames in a group-of-pictures (GOP). In error-prone environments, error happening in the previous frames in a GOP may propagate to all the following frames until the next I frame, which is the beginning of the next GOP. In this paper, we propose a novel GOP structure for robust transmission of MPEG video bitstream. By selecting the optimal position of the I frame in a GOP, robustness can be achieved without reducing any coding efficiency. Another advantage or the proposed GOP structure is also analyzed: compared with the conventional GOP structure, it provides reverse-play operation for MPEG video streaming with much less requirement on the network bandwidth. Experimental results demonstrate both the robustness of the proposed GOP structure and the efficient reverse-play functionality it leads to.
C1 Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM tfang@pmail.ntu.edu.sg; elpchau@ntu.edu.sg
RI Chau, Lap-Pui/A-5149-2011
OI Chau, Lap-Pui/0000-0003-4932-0593
CR [Anonymous], IEEE J SEL AREAS COM
   BARBOSA LO, 2000, IEEE T BROADCAST, V46, P134
   Chen M.S., 1995, PROC 2 INT IEEE C MU, P73
   FU CH, 2003, P ICICS PCM
   Girod B, 1999, P IEEE, V87, P1707, DOI 10.1109/5.790632
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P511, DOI 10.1109/TCSVT.2002.800313
   LEGALL D, 1991, COMMUN ACM, V34, P46, DOI 10.1145/103085.103090
   Moccagatta I, 2000, IEEE J SEL AREA COMM, V18, P899, DOI 10.1109/49.848245
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Villasenor JD, 1999, P IEEE, V87, P1724, DOI 10.1109/5.790633
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   Wee SJ, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 2, P209, DOI 10.1109/ICIP.1998.723349
   WEE SJ, 1998, P SPIE C MULT SYST A, P237
   Yang XK, 2003, SIGNAL PROCESS-IMAGE, V18, P157, DOI 10.1016/S0923-5965(02)00128-5
NR 14
TC 11
Z9 13
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2005
VL 7
IS 6
BP 1131
EP 1138
DI 10.1109/TMM.2005.858399
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 986SY
UT WOS:000233471200014
DA 2024-07-18
ER

PT J
AU Zhai, F
   Luna, CE
   Eisenberg, Y
   Pappas, TN
   Berry, R
   Katsaggelos, AK
AF Zhai, F
   Luna, CE
   Eisenberg, Y
   Pappas, TN
   Berry, R
   Katsaggelos, AK
TI Joint source coding and packet classification for real-time video
   transmission over differentiated services networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE error concealment; error resilience; joint source-channel coding;
   multimedia communication; optimal resource allocation; QoS; unequal
   error protection (UEP)
ID OPTIMAL-MODE SELECTION; ALLOCATION; INTERNET
AB Differentiated Services (DiffServ) is one of the leading architectures for providing quality of service in the Internet. We propose a scheme for real-time video transmission over a DiffServ network that jointly considers video source coding, packet classification, and error concealment within a framework of cost-distortion optimization. The selections of encoding parameters and packet classification are both used to manage end-to-end delay variations and packet losses within the network. We present two dual formulations of the proposed scheme: the minimum distortion problem, in which the objective is to minimize the end-to-end distortion subject to cost and delay constraints, and the minimum cost problem, which minimizes the total cost subject to end-to-end distortion and delay constraints. A solution to these problems using Lagrangian relaxation and dynamic programming is given. Simulation results demonstrate the advantage of jointly adapting the source coding and packet classification in DiffServ networks.
C1 Texas Instruments Inc, Dallas, TX 75243 USA.
   Northwestern Univ, Dept Elect & Comp Engn, Evanston, IL 60208 USA.
   Boeing Co, Laser & Electroopt Syst, Canoga Pk, CA 91309 USA.
C3 Texas Instruments; Northwestern University; Boeing
RP Texas Instruments Inc, Dallas, TX 75243 USA.
EM fzhai@ti.com; Carlos.E.Luna@boeing.com; yeisenbe@ece.northwestern.edu;
   pappas@ece.northwestern.edu; rberry@ece.northwestern.edu;
   aggk@ece.northwestern.edu
RI Pappas, Thrasyvoulos/D-5054-2013; Berry, Randall A/B-7107-2009; Pappas,
   Thrasyvoulos N/GWV-5517-2022; Pappas, Thrasyvoulos N/B-7261-2009; Berry,
   Randall/Z-1121-2019; Katsaggelos, Aggelos K/B-7233-2009
OI Pappas, Thrasyvoulos N/0000-0002-4598-2197; Berry,
   Randall/0000-0002-1861-6722; Katsaggelos, Aggelos K/0000-0003-4554-0070
CR Bertsekas D. P., 1995, NONLINEAR PROGRAMMIN
   BLAKE S, 1998, 2475 RFC IETF
   Carpenter BE, 2002, P IEEE, V90, P1479, DOI 10.1109/JPROC.2002.802000
   CHOU PA, IN PRESS IEEE T MULT
   Côté G, 2000, IEEE J SEL AREA COMM, V18, P952, DOI 10.1109/49.848249
   Eisenberg Y, 2002, IEEE T CIRC SYST VID, V12, P411, DOI 10.1109/TCSVT.2002.800309
   FLOYD S, 2000, EQUATION BASED CONGE
   Ghosh D, 2001, IEEE T MULTIMEDIA, V3, P200, DOI 10.1109/6046.923819
   HASKELL BG, 1994, IEEE T CIRC SYST VID, V4, P417, DOI 10.1109/76.313136
   Hinds R.O., 1998, Visual Communications and Image Processing, V3309, P124
   HINDS RO, 1999, THESIS MIT CAMBRIDGE
   Hooghiemstra G., 2001, 20011020 DELFT U TEC
   Hsu CY, 1999, IEEE J SEL AREA COMM, V17, P756, DOI 10.1109/49.768193
   *ISO IEC JTC1 SC29, 1993, MPEG93457 ISOIEC JTC
   Luna CE, 2003, IEEE J SEL AREA COMM, V21, P1710, DOI 10.1109/JSAC.2003.815394
   LUNA CE, 2002, P TYRRH INT WORKSH D, P59
   NICHOLS K, 1998, 2474 RFC IETF
   PAXSON V, 1995, IEEE ACM T NETWORK, V3, P226, DOI 10.1109/90.392383
   Quaglia D, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P369
   QUAGLIA D, 2002, P IEEE INT C MULT EX, V2, P85
   RAMCHANDRAN K, 1994, IEEE T IMAGE PROCESS, V3, P533, DOI 10.1109/83.334987
   Reibman AR, 1992, IEEE T CIRC SYST VID, V2, P361, DOI 10.1109/76.168912
   Schuster G., 1997, RATE DISTORTION BASE
   Sehgal A, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P857, DOI 10.1109/ICME.2002.1035917
   Shin J, 2001, IEEE T MULTIMEDIA, V3, P219, DOI 10.1109/6046.923821
   Wang Y, 2000, IEEE SIGNAL PROC MAG, V17, P61, DOI 10.1109/79.855913
   Wu DP, 2000, P IEEE, V88, P1855, DOI 10.1109/5.899055
   Wu DP, 2000, IEEE J SEL AREA COMM, V18, P977, DOI 10.1109/49.848251
   Yajnik M, 1999, IEEE INFOCOM SER, P345, DOI 10.1109/INFCOM.1999.749301
   Zhang Q, 2001, IEEE T MULTIMEDIA, V3, P339, DOI 10.1109/6046.944477
   Zhang R, 2000, IEEE J SEL AREA COMM, V18, P966, DOI 10.1109/49.848250
NR 31
TC 16
Z9 16
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2005
VL 7
IS 4
BP 716
EP 726
DI 10.1109/TMM.2005.850989
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 946XQ
UT WOS:000230607000012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, MH
   He, Y
   Lagendijk, RL
AF Chen, MH
   He, Y
   Lagendijk, RL
TI A fragile watermark error detection scheme for wireless video
   communications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE error detection; error resilience; forced even watermark (FEW); fragile
   watermark; video coding; video communications; wireless communications
ID CONCEALMENT; RESILIENCE
AB In video communications over error-prone channels, compressed video streams are extremely sensitive to bit errors. Often random and burst bit errors impede correct decoding of parts of a received bitstream. Video decoders normally utilize error concealment techniques to repair a damaged decoded frame, but the effectiveness of these error concealment schemes relies heavily on correctly locating errors in the bitstream. In this paper, we propose a fragile watermark-based error detection and localization scheme called "force even watermarking (FEW)". A fragile watermark is forced onto quantized DCT coefficients at the encoder. If at the decoder side the watermark is no longer intact, errors exist in the bitstream associated with a particular macro-block (MB). Thanks to the watermark, bitstream errors can accurately be located at MB level, which facilitates proper error concealment. This paper describes the algorithm, model and analysis of the watermarking procedure. Our simulation results show that compared to the syntax-based error detection schemes, the proposed FEW scheme significantly improves the error detection capabilities of the video decoder, while the peak signal-to-noise ratio loss and additional computational costs due to watermark embedding and extraction are small.
C1 Tsinghua Univ, Dept Elect Engn, State Key Lab Microwave & Digital Commun, Beijing 100084, Peoples R China.
   Delft Univ Technol, Informat & Commun Theory Grp, NL-2600 AA Delft, Netherlands.
C3 Tsinghua University; Delft University of Technology
RP Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
EM minghua@eecs.berkeley.edu; hey@video.tsinghua.edu.cn;
   R.L.Lagendijk@Ewu.TUDelft.nl
RI Chen, Minghua/A-7476-2012
OI Chen, Minghua/0000-0003-4763-0037
CR [Anonymous], 138182 ISOIEC
   BARNI M, 2000, ELECT IMAGING 2000 S
   Bellifemine F., 1992, Signal Processing: Image Communication, V4, P477, DOI 10.1016/0923-5965(92)90032-B
   BIRNEY KA, 1995, IEEE T IMAGE PROCESS, V4, P186, DOI 10.1109/83.342184
   CHENG Q, 2000, P INT C MULT EXP NEW, V1, P389
   Ducla-Soares L, 1999, SIGNAL PROCESS-IMAGE, V14, P447, DOI 10.1016/S0923-5965(98)00060-5
   FRIDRICH J, 1999, P 5 INT S SIGN PROC, V1, P301
   Hartung F, 1997, INT CONF ACOUST SPEE, P2621, DOI 10.1109/ICASSP.1997.595326
   Hong MC, 1999, SIGNAL PROCESS-IMAGE, V14, P473, DOI 10.1016/S0923-5965(98)00061-7
   *ISO IEC DIS, 1991, 111722 ISOIEC DIS
   *ISO IEC JTC1 SC29, 1999, 144962 ISOIEC JTC1SC
   *ITU T STUD GROUP, 1997, TMN8 VID COD TEST MO
   *JVT ISO IEC MPEG, 2002, JOINT COMM DRAFT CD
   Kundur D, 1999, P IEEE, V87, P1167, DOI 10.1109/5.771070
   LANGELAAR GC, 1998, J VIS COMMUN IMA DEC
   Lee WS, 1997, IEEE J SEL AREA COMM, V15, P1764, DOI 10.1109/49.650049
   MULLER F, 1979, ELECTRON LETT, V15, P664
   Sklar B, 1997, IEEE COMMUN MAG, V35, P90, DOI 10.1109/35.601747
   Talluri R, 1999, SIGNAL PROCESS-IMAGE, V14, P505, DOI 10.1016/S0923-5965(98)00064-2
   WANG TS, 2000, ERROR DETECTION SCHE
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   Wolfgang RB, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P219, DOI 10.1109/ICIP.1996.560423
   ZHOU P, 2003, P IEEE INT S CIRC SY, V2, P956
NR 23
TC 41
Z9 44
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2005
VL 7
IS 2
BP 201
EP 211
DI 10.1109/TMM.2005.843367
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 909OO
UT WOS:000227869400002
OA Green Published
DA 2024-07-18
ER

PT J
AU Tsekeridou, S
   Cheikh, FA
   Gabbouj, M
   Pitas, I
AF Tsekeridou, S
   Cheikh, FA
   Gabbouj, M
   Pitas, I
TI Vector rational interpolation schemes for erroneous motion field
   estimation applied to MPEG-2 error concealment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE error concealment; motion field estimation; MPEG-2 compression; temporal
   concealment; transmission errors; vector rational interpolation
ID PACKET VIDEO; IMAGES; INFORMATION; RECOVERY
AB A study on the use of vector rational interpolation for the estimation of erroneously received motion fields of MPEG-2 predictively coded frames is undertaken in this paper, aiming further at error concealment (EC). Various rational interpolation schemes have been investigated, some of which are applied to different interpolation directions. One scheme additionally uses the boundary matching error and another one attempts to locate the direction of minimal/maximal change in the local motion field neighborhood. Another one further adopts bilinear interpolation principles, whereas a last one additionally exploits available coding mode information. The methods present temporal EC methods for predictively coded frames or frames for which motion information pre-exists in the video bitstream. Their main advantages are their capability to adapt their behavior with respect to neighboring motion information, by switching from linear to nonlinear behavior, and their real-time implementation capabilities, enabling them for real-time decoding applications. They are easily embedded in the decoder model to achieve concealment along with decoding and avoid post-processing delays. Their performance proves to be satisfactory for packet error rates up to 2% and for video sequences with different content and motion characteristics and surpass that of other state-of-the-art temporal concealment methods that also attempt to estimate unavailable motion information and perform concealment afterwards.
C1 Democritus Univ Thrace, Dept Elect & Comp Engn, GR-67100 Xanthi, Greece.
   Tampere Univ Technol, Inst Signal Proc, FIN-33101 Tampere, Finland.
   Univ Thessaloniki, Dept Informat, Thessaloniki 54124, Greece.
C3 Democritus University of Thrace; Tampere University; Aristotle
   University of Thessaloniki
RP Democritus Univ Thrace, Dept Elect & Comp Engn, GR-67100 Xanthi, Greece.
EM tsekerid@ee.duth.gr; faouzi@cs.tut.fi; Moncef.Gabbouj@tut.fi;
   pitas@zeus.csd.auth.gr
RI Tsekeridou, Sofia/AAX-3926-2021; Gabbouj, Moncef/G-4293-2014
OI Tsekeridou, Sofia/0000-0003-3830-1936; Gabbouj,
   Moncef/0000-0002-9788-2323
CR Al-Mualla M, 1999, ELECTRON LETT, V35, P215, DOI 10.1049/el:19990174
   CHEIKH FA, 1998, P SPIE EI C NONL IM, V3304
   CHEIKH FA, 1998, P 9 EUR SIGN PROC C, V2, P1033
   Khriji L, 1999, OPT ENG, V38, P893, DOI 10.1117/1.602056
   KHRIJI L, 1998, P 2 IMACS IEEE INT M
   LAM WM, 1995, IEEE T IMAGE PROCESS, V4, P533, DOI 10.1109/83.382489
   LAM WM, 1993, P ICASSP, V5, P417
   LEE XB, 1995, IEEE T IMAGE PROCESS, V4, P259, DOI 10.1109/83.366475
   Park JW, 1997, IEEE T CIRC SYST VID, V7, P845, DOI 10.1109/76.644064
   Salama P., 1998, Signal Recovery Techniques for Image and Video Compression and Transmission, P199
   Sapiro G, 1996, IEEE T IMAGE PROCESS, V5, P1582, DOI 10.1109/83.541429
   Scharcanski J, 1997, IEEE T CIRC SYST VID, V7, P397, DOI 10.1109/76.564116
   Sun HF, 1997, SIGNAL PROCESS-IMAGE, V10, P249, DOI 10.1016/S0923-5965(96)00023-9
   SUN HF, 1995, IEEE T IMAGE PROCESS, V4, P470, DOI 10.1109/83.370675
   Tsai IW, 1997, SIGNAL PROCESS-IMAGE, V9, P99, DOI 10.1016/0923-5965(95)00013-5
   Tsekeridou S, 2000, IEEE T CIRC SYST VID, V10, P646, DOI 10.1109/76.845010
   TSEKERIDOU S, 1999, P 1999 IEEE INT C AC
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   Wang Z, 1998, IEEE T IMAGE PROCESS, V7, P1056, DOI 10.1109/83.701166
   Yu GS, 1998, IEEE T CIRC SYST VID, V8, P422, DOI 10.1109/76.709409
   Zhang J, 2000, IEEE T CIRC SYST VID, V10, P659, DOI 10.1109/76.845011
NR 21
TC 8
Z9 9
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2004
VL 6
IS 6
BP 876
EP 885
DI 10.1109/TMM.2004.837266
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 872QU
UT WOS:000225224200010
DA 2024-07-18
ER

PT J
AU Guo, Y
   Gao, LX
   Towsley, D
   Sen, S
AF Guo, Y
   Gao, LX
   Towsley, D
   Sen, S
TI Smooth workload adaptive broadcast
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE feedback control; on-demand video streaming; periodic broadcast; smooth
   channel transition
AB The high-bandwidth requirements and long-lived characteristics of digital video make transmission bandwidth usage a key limiting factor in the widespread streaming of such content over the Internet. A challenging problem is to develop bandwidth-efficient techniques for delivering popular videos to a large, asynchronous client population with time-varying demand characteristics. In this paper, we propose smooth workload adaptive broadcast to address the above issues. A key component of our scheme is Flexible Periodic Broadcast (FPB). By introducing a feedback control loop into FPB, and enhancing FPB using techniques such as parsimonious transmission, smooth workload adaptive broadcast provides instantaneous or near-instantaneous playback services and can smoothly adapt to workload changes. Furthermore, FPB, as proposed in this paper, is bandwidth efficient and exhibits the periodic smooth channel transition property.
C1 MathWorks Inc, Natick, MA 01760 USA.
   Univ Massachusetts, Amherst, MA 01003 USA.
   AT&T Labs Res, Florham Pk, NJ 07932 USA.
C3 MathWorks; University of Massachusetts System; University of
   Massachusetts Amherst; AT&T
RP MathWorks Inc, Natick, MA 01760 USA.
EM yguo@mathworks.com; lgao@ecs.umass.edu; towsley@cs.umass.edu;
   sen@research.att.com
RI Yan, Miaochen/JLL-5061-2023
OI Towsley, Donald/0000-0002-7808-7375
CR Almeroth KC, 1997, IEEE INFOCOM SER, P1333, DOI 10.1109/INFCOM.1997.631166
   BARNOY A, 2002, P SPIE ACM C MULT CO
   Carter SW, 1997, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER COMMUNICATIONS AND NETWORKS, PROCEEDINGS, P200, DOI 10.1109/ICCCN.1997.623313
   Coffman EG, 2001, WEB CACHING AND CONTENT DELIVERY, P171
   Eager D, 2000, PROC SPIE, V3969, P206
   Gao L., 2001, IEEE T MULTIMEDIA, V3, P405
   GAO L, 2002, ACM MULTIMEDIA SYST, P284
   Hu AL, 2001, IEEE INFOCOM SER, P508, DOI 10.1109/INFCOM.2001.916754
   Hua K., 1997, PROC SIGCOMM, P89
   Hua K. A., 1998, Proceedings ACM Multimedia 98, P191, DOI 10.1145/290747.290771
   MAHANTI A, 2001, P ACM SIGCOMM 01, P97
   OH J, 2002, ACM SPRINGER MULTIME, V8, P258
   PARIS J, 2000, P EUR C, P107
   SEN S, 2001, P IEEE INT PERF COMP
   TSENG Y, 2000, IEEE INFOCOM 2000 MA, P727
NR 15
TC 17
Z9 18
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2004
VL 6
IS 2
BP 387
EP 395
DI 10.1109/TMM.2003.822786
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 807NO
UT WOS:000220508400017
DA 2024-07-18
ER

PT J
AU Mauve, M
   Vogel, J
   Hilt, V
   Effelsberg, W
AF Mauve, M
   Vogel, J
   Hilt, V
   Effelsberg, W
TI Local-lag and timewarp: Providing consistency for replicated continuous
   applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE consistency; local-lag; networked games; replicated continuous
   applications; timewarp
AB In this paper, we investigate how consistency can be established for replicated applications changing their state in reaction to user-initiated operations as well as the passing of time. Typical examples of these applications are networked computer games and distributed virtual environments. We give a formal definition of the terms consistency and correctness for this application class. Based on these definitions, it is shown that an important tradeoff relationship exists between the responsiveness of the application and the appearance of short-term inconsistencies. We propose to exploit the knowledge of this tradeoff by voluntarily decreasing the responsiveness of the application in order to eliminate short-term inconsistencies. This concept is called local-lag. Furthermore, a timewarp scheme is presented that complements local-lag by guaranteeing consistency and correctness for replicated continuous applications. The computational complexity of the timewarp algorithm is determined in theory and practice by examining a simple networked computer game. The timewarp scheme is then compared to the well-known dead-reckoning approach. It is shown that the choice between both schemes is application-dependent.
C1 Univ Dusseldorf, Inst Informat, D-40225 Dusseldorf, Germany.
   Univ Mannheim, Prakt Informat IV, D-68161 Mannheim, Germany.
C3 Heinrich Heine University Dusseldorf; University of Mannheim
RP Univ Dusseldorf, Inst Informat, D-40225 Dusseldorf, Germany.
EM mauve@cs.uni-duesseldorf.de; vogel@informatik.uni-mannheim.de;
   volkerh@dnrc.bell-labs.com; effelsberg@informatik.uni-mannheim.de
OI Hilt, Volker/0000-0002-1826-8297; Vogel, Juergen/0009-0006-8150-5888
CR [Anonymous], 1983, The Psychology of Human-Computer Interaction
   [Anonymous], P SIGCHI C HUM FACT
   Chengzheng Sun, 2002, ACM Transactions on Computer-Human Interaction, V9, P1, DOI 10.1145/505151.505152
   Chengzheng Sun, 1998, ACM Transactions on Computer-Human Interaction, V5, P63, DOI 10.1145/274444.274447
   DIOT C, 1999, IEEE NETWORK MAG, V13
   ELLIS CA, 1989, SIGMOD REC, V18, P399, DOI 10.1145/66926.66963
   Frecon E., 1998, Distributed Systems Engineering, V5, P91, DOI 10.1088/0967-1846/5/3/002
   FUJIMOTO RM, 1990, COMMUN ACM, V33, P30, DOI 10.1145/84537.84545
   LAMPORT L, 1978, COMMUN ACM, V21, P558, DOI 10.1145/359545.359563
   MAUVE M, 1999, P SPIE MULT COMP NET, P240
   SCHNEIDERMAN B, 1984, ACM COMPUT SURV, V16
   SINGHAL S, 1996, THESIS STANFORD U ST
   Srinivasan S, 1996, 1996 WINTER SIMULATION CONFERENCE PROCEEDINGS, P946, DOI 10.1145/256562.256867
   SUN V, 1998, P ACM 1998 C COMP SU, P59
   Vogel J, 2003, MULTIMEDIA SYST, V9, P327, DOI 10.1007/s00530-003-0103-6
NR 15
TC 118
Z9 135
U1 1
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2004
VL 6
IS 1
BP 47
EP 57
DI 10.1109/TMM.2003.819751
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765QP
UT WOS:000188295200004
DA 2024-07-18
ER

PT J
AU Yoma, NB
   Hood, J
   Busso, C
AF Yoma, NB
   Hood, J
   Busso, C
TI A real-time protocol for the Internet based on the Least Mean Square
   algorithm
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Internet; real-time applications; QoS; TCP; UDP
AB Generally, real-time applications based on User Datagram Protocol (UDP) protocol generate large volumes of data and are not sensitive to network congestion. In contrast, Transmission Control Protocol (TCP) traffic is considered "well-behaved" because it prevents the network from congestion by means of closed-loop control of packet-loss and round-trip-time. The integration of both sorts of traffic is a complex problem, and depends on solutions such as admission control that have not been deployed in the Internet yet. Moreover, the problem of quality-of-service (QoS) and resource allocation is extremely relevant from the point of view of convergence of streaming media and data transmission on the Internet. In this paper an adaptive real-time protocol based on the Least Mean Square (LMS) algorithm is proposed to estimate the application UDP bandwidth in order to reduce the quadratic error between the packet loss and a target. Moreover, the LMS algorithm is also applied to make sure that the reduction in the average bandwidth allocated to each TCP process will not be higher than a given percentage of the average bandwidth allocated before the beginning of the UDP application.
C1 Univ Chile, Dept Elect Engn, Santiago, Chile.
   Corp Nacl Cobre, Rancagua, Chile.
   Univ So Calif, Los Angeles, CA 90007 USA.
C3 Universidad de Chile; University of Southern California
RP Univ Chile, Dept Elect Engn, Santiago, Chile.
EM nbecerra@ing.uchile.cl; jhood@codelco.cl; busso@usc.edu
RI Yoma, Nestor Becerra/I-8230-2015
OI Busso, Carlos/0000-0002-4075-4072
CR [Anonymous], P ACM SIGCOMM 98
   [Anonymous], 1985, Adaptive Signal Processing
   BELLANGER MG, 1988, P IEEE INT S CIRC SY, V3, P2635
   BHATTACHARYYA S, 2000, J COMPUT COMMUN
   BYERS J, 2000, P 2 INT WORKSH NETW
   Kasera SK, 2000, IEEE NETWORK, V14, P48, DOI 10.1109/65.819171
   PADHYE J, 1999, P INT WORKSH NETW OP
   REJAIE R, 1999, P INFOCOMM 99
   Rhee I, 1999, IEEE INFOCOM SER, P1265, DOI 10.1109/INFCOM.1999.752144
   RIZZO L, 2000, ACM SIGCOMM      AUG
   Roberts JW, 2001, IEEE COMMUN MAG, V39, P94, DOI 10.1109/35.894382
   SISALEM D, 1997, DIRECT ADJUSTMENT AL
   SISALEM D, 2000, 8 INT WORKSH QUAL SE
   SISALEM D, 2000, 10 INT WORKSH NETW O
   Stallings W., 1998, High-Speed Networks: TCP/IP and ATM Design Principles
   TAN W, 1999, INT C IM PROC OCT
   Tanenbaum A. S., 1996, Computer Networks
   TURLETTI T, 1997, RR3296 INRIA
   Vicisano L, 1998, IEEE INFOCOM SER, P996, DOI 10.1109/INFCOM.1998.662909
   WANG HA, 1998, ACM SIGCOMM
   Widmer J, 2001, IEEE NETWORK, V15, P28, DOI 10.1109/65.923938
   YANO K, 2000, ACM MULT LOS ANG CA
   Zhang Q, 2001, IEEE T MULTIMEDIA, V3, P339, DOI 10.1109/6046.944477
   ZHANG Q, 2000, IEEE INT C MULT EXP
   ZHU W, 2001, P ISCAS 2001 MAY, V5, P109
NR 25
TC 5
Z9 7
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2004
VL 6
IS 1
BP 174
EP 184
DI 10.1109/TMM.2003.819582
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765QP
UT WOS:000188295200013
DA 2024-07-18
ER

PT J
AU Raghupathy, A
   Chandrachoodan, N
   Liu, KJR
AF Raghupathy, A
   Chandrachoodan, N
   Liu, KJR
TI Algorithm and VLSI architecture for high performance adaptive video
   scaling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptive scaling; oriented polynomial interpolation; video zoom; VLSI
   architecture
ID DELAY-LINE
AB We propose an efficient high-performance scaling algorithm based on the oriented polynomial image model. We develop a simple classification scheme that classifies the region around a pixel as an oriented or nonoriented block. Based on this classification, a nonlinear oriented interpolation is performed to obtain high quality video scaling. In addition, we also propose a generalization that can perform scaling for arbitrary scaling factors. Based on this algorithm, we develop an efficient architecture for image scaling. Specifically, we consider an architecture for scaling a Quarter Common Intermediate Format (QCIF) image to 4CIF format. We show the feasibility of the architecture by describing the various computation units in a hardware description language (Verilog) and synthesizing the design into a netlist of gates. The synthesis results show that an application specific integrated circuit (ASIC) design which meets the throughput requirements can be built with a reasonable silicon area.
C1 Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
   Univ Maryland, Syst Res Inst, College Pk, MD 20742 USA.
C3 University System of Maryland; University of Maryland College Park;
   University System of Maryland; University of Maryland College Park
RP Qualcomm Inc, San Diego, CA 92121 USA.
EM arunr@qualcomm.com; kjrliu@eng.umd.edu
RI Liu, K.J. Ray/C-2798-2009
OI Chandrachoodan, Nitin/0000-0002-9258-7317
CR DEJHAN K, 1989, IEEE T CONSUM ELECTR, V35, P893, DOI 10.1109/30.106914
   Hu YH, 1992, IEEE SIGNAL PROC MAG, V9, P16, DOI 10.1109/79.143467
   Jain A. K., 1989, Fundamentals of Digital Image Processing
   JENSEN K, 1990, P IEEE INT C AC SPEE, P2045
   MARTINEZ DM, 1989, P IEEE INT C AC SPEE, P1886
   MATTAUSCH HJ, 1988, IEEE J SOLID-ST CIRC, V23, P105, DOI 10.1109/4.265
   PARHI KK, 1995, J VLSI SIGNAL PROC, V9, P121, DOI 10.1007/BF02406474
   Patterson DavidA., 1996, Computer architecture: a quantitative approach, V2nd
   RAGHUPATHY A, 1998, THESIS U MARYLAND CO
   ROTHAN F, 1990, 1990 IEEE INTERNATIONAL SYMP ON CIRCUITS AND SYSTEMS, VOLS 1-4, P65, DOI 10.1109/ISCAS.1990.111914
   SALONEN J, 1994, IEEE T CONSUM ELECTR, V40, P225, DOI 10.1109/30.320799
   Volder J. E., 1959, IRE T ELECTRON COM, VEC-8, P330, DOI [10.1109/TEC.1959.5222693, DOI 10.1109/TEC.1959.5222693]
   WANG Y, 1993, IEEE T PATTERN ANAL, V15, P321, DOI 10.1109/34.206953
   Weste N.H. E., 1993, Principles of CMOS VLSI Design: A Systems Perspective, Vsecond
   1997, DIGITAL CONSUMER ELE
NR 15
TC 5
Z9 6
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2003
VL 5
IS 4
BP 489
EP 502
DI 10.1109/TMM.2003.813282
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 742VA
UT WOS:000186537700001
DA 2024-07-18
ER

PT J
AU Zeng, WJ
   Lei, SM
AF Zeng, WJ
   Lei, SM
TI Efficient frequency domain selective scrambling of digital video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE contents access control; joint encryption and compression; multimedia
   encryption; multimedia security; selective encryption; video scrambling
AB Multimedia data security is very important for multimedia commerce on the Internet such as video-on-demand and real-time video multicast. Traditional cryptographic algorithms/systems for data security are often not fast enough to process the vast amount of data generated by the multimedia applications to meet the real-time constraints. This paper presents a joint encryption and compression framework in which video data are scrambled efficiently in the frequency domain by employing selective bit scrambling, block shuffling and block rotation of the transform coefficients and motion vectors. The new approach is very simple to implement, yet provides considerable levels of security and different levels of transparency, and has very limited adverse impact on the compression efficiency and no adverse impact on error resiliency. Furthermore, it allows transcodability/scalability, and some other content processing functionalities without having to access the cryptographic key and to perform decryption and re-encryption.
C1 Amer Inc, Sharp Labs, Camas, WA 98607 USA.
RP PacketVideo Corp, San Diego, CA 92121 USA.
EM wzeng@pv.com
CR AGI I, 1996, INT SOC S NETW DISTR
   Hobbs G. L., 1998, U.S Patent, Patent No. 5815572
   *ISO IEC, 144912001AMD3 ISOIEC
   *ISO IEC, 2000, FCD1544412000 ISOIEC
   LI J, 1997, P PICT COD S BERL GE, P201
   Macq B, 1994, J INTERACTIVE MULTIM, V1, P179
   MACQ BM, 1995, P IEEE, V83, P944, DOI 10.1109/5.387094
   MAPLES T, 1995, 4 INT C COMP COMM NE
   Meyer J., 1995, SECURITY MECH MULTIM
   National Bureau of Standards, 1977, FIPS PUB, V46
   PINDER H, 1997, Patent No. 5684876
   QIAO L, 1998, INT J COMPUT GRAPH S, V22
   ROGAWAY P, 1994, LNCS, V809, P56
   TAN W, 1998, P IEEE ICIP, P458
   Tang L., 1996, P 4 ACM INT MULT C A, P219
   TAUBMAN D, 1994, IEEE T IMAGE PROCESS, V3, P572, DOI 10.1109/83.334984
   WEN J, 2002, P IEEE WORKSH MULT S, V12, P545
   Wen JT, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P435, DOI 10.1109/MMSP.2001.962772
   Wen JT, 2002, IEEE T CIRC SYST VID, V12, P545, DOI 10.1109/TCSVT.2002.800321
   Zeidler D. E., 1994, Patent, Patent No. [US5321748A, 5321748]
   ZENG W, 2002, P IEEE ICIP
   Zeng WJ, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P285, DOI 10.1145/319463.319627
NR 22
TC 207
Z9 245
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2003
VL 5
IS 1
BP 118
EP 129
DI 10.1109/TMM.2003.808817
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 675HP
UT WOS:000182688200010
DA 2024-07-18
ER

PT J
AU Chen, B
   Liu, KH
   Xu, Y
   Wu, QQ
   Yao, JF
AF Chen, Bin
   Liu, Kun-Hong
   Xu, Yong
   Wu, Qing-Qiang
   Yao, Jun-Feng
TI Block Division Convolutional Network With Implicit Deep Features
   Augmentation for Micro-Expression Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural network; data augmentation; deep features
   augmentation; micro-expression recognition
ID LOCAL BINARY PATTERNS; OPTICAL-FLOW; INFORMATION
AB Despite the development of computer vision techniques, the micro-expression (ME) recognition task still remains a great challenge because MEs have very low intensity and short duration. However, the ME recognition is of great significance since it provides important clues for real affective states detection. This paper proposes a novel Block Division Convolutional Network (BDCNN) with the implicit deep features augmentation. In detail, BDCNN learns from four optical flow features computed by the onset and apex frames of each video. It innovatively divides each image into a set of small blocks in the deep learning model, then the convolution and pooling operations are performed on these small blocks in sequence. To handle the small sample size problem in the micro-expression data, this study uses the improved implicit semantic data augmentation algorithm in the deep features space. Experiments are conducted on three publicly available databases, viz, CASME II, SMIC, and SAMM. Experimental results show that our model outperforms the state-of-the-art methods by attaining the accuracy of 84.32% and F1-score of 82.13% on the 3-class datasets, and the accuracy of 81.82% and F1-score of 75.46% on the 5-class datasets, respectively. Our source code is publicly available for non-commercial or research use at https://github.com/MLDMXM2017/BDCNN.
C1 [Chen, Bin; Liu, Kun-Hong; Wu, Qing-Qiang; Yao, Jun-Feng] Xiamen Univ, Sch Informat, Xiamen 361005, Peoples R China.
   [Xu, Yong] Fujian Univ Technol, Sch Comp Sci & Math, Fuzhou 350011, Peoples R China.
C3 Xiamen University; Fujian University of Technology
RP Liu, KH (corresponding author), Xiamen Univ, Sch Informat, Xiamen 361005, Peoples R China.
EM 24320191152511@stu.xmu.edu.cn; lkhqz@xmu.edu.cn; y.xu@fjut.edu.cn;
   wuqq@xmu.edu.cn; yao0010@xmu.edu.cn
RI Yao, Junfeng/ABE-6440-2020; Qingqiang, Wu/HDN-0973-2022; Liu,
   Kunhong/D-9400-2016
OI Liu, Kunhong/0000-0002-1222-8876; Yao, Junfeng/0000-0002-2330-7406
FU National Natural Science Foundation of China [61772023]; National Key
   R&D Program of China [2019QY1803]; Fujian Sunshine Charity Foundation
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61772023, in part by the National Key R
   & D Program of China under Grant 2019QY1803, and in part by Fujian
   Sunshine Charity Foundation.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 2009, Facial micro-expressions recognition using high speed camera and 3d-gradient descriptor, DOI DOI 10.1049/IC.2009
   [Anonymous], 1966, Methods of research in psychotherapy, DOI [DOI 10.1007/978-1-4684-6045-2_14, 10.1007/978-1-4684-6045-2_14]
   Bhushan B., 2015, Understanding Facial Expressions in Communication, P265, DOI [DOI 10.1007/978-81-322-1934-7_13, 10.1007/978-81-322-1934-7%2013, DOI 10.1007/978-81-322-1934-713]
   Chaudhry R, 2009, PROC CVPR IEEE, P1932, DOI 10.1109/CVPRW.2009.5206821
   Davison AK, 2018, J IMAGING, V4, DOI 10.3390/jimaging4100119
   Davison AK, 2018, IEEE T AFFECT COMPUT, V9, P116, DOI 10.1109/TAFFC.2016.2573832
   DeVries T, 2017, Arxiv, DOI [arXiv:1708.04552, DOI 10.48550/ARXIV.1708.04552]
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Ekman P., 2009, PHILOS DECEPTION, P118
   Gan YS, 2019, SIGNAL PROCESS-IMAGE, V74, P129, DOI 10.1016/j.image.2019.02.005
   Happy SL, 2019, IEEE T AFFECT COMPUT, V10, P394, DOI 10.1109/TAFFC.2017.2723386
   Huang XH, 2019, IEEE T AFFECT COMPUT, V10, P32, DOI 10.1109/TAFFC.2017.2713359
   Iandola F. N., 2017, PROC INT C LEARN REP
   Jia X., 2009, J POSTGRADUATE ZHONG, P22
   Khor HQ, 2018, IEEE INT CONF AUTOMA, P667, DOI 10.1109/FG.2018.00105
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li XB, 2013, IEEE INT CONF AUTOMA, DOI 10.1109/FG.2013.6553717
   Li YT, 2021, IEEE T IMAGE PROCESS, V30, P249, DOI 10.1109/TIP.2020.3035042
   Liong ST, 2020, J SIGNAL PROCESS SYS, V92, P705, DOI 10.1007/s11265-020-01523-4
   Liong ST, 2019, IEEE INT CONF AUTOMA, P658, DOI 10.1109/fg.2019.8756567
   Liong ST, 2018, SIGNAL PROCESS-IMAGE, V62, P82, DOI 10.1016/j.image.2017.11.006
   Liong ST, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P665, DOI 10.1109/ACPR.2015.7486586
   Liong ST, 2015, LECT NOTES COMPUT SC, V9009, P644, DOI 10.1007/978-3-319-16631-5_47
   Liu KH, 2021, SIGNAL PROCESS-IMAGE, V93, DOI 10.1016/j.image.2021.116153
   Liu YJ, 2021, IEEE T AFFECT COMPUT, V12, P254, DOI 10.1109/TAFFC.2018.2854166
   Liu YJ, 2016, IEEE T AFFECT COMPUT, V7, P299, DOI 10.1109/TAFFC.2015.2485205
   Peng M, 2018, IEEE INT CONF AUTOMA, P657, DOI 10.1109/FG.2018.00103
   Porter S, 2008, PSYCHOL SCI, V19, P508, DOI 10.1111/j.1467-9280.2008.02116.x
   Russell TA, 2006, BRIT J CLIN PSYCHOL, V45, P579, DOI 10.1348/014466505X90866
   Shen XB, 2012, J ZHEJIANG UNIV-SC B, V13, P221, DOI 10.1631/jzus.B1100063
   Shreve Matthew, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P51, DOI 10.1109/FG.2011.5771451
   SHREVE M., 2009, 2009 WORKSH APPL COM, P1, DOI [10.1109/WACV.2009.5403044, DOI 10.1109/WACV.2009.5403044]
   Su Y, 2022, INT J OCCUP SAF ERGO, V28, P1533, DOI 10.1080/10803548.2021.1904652
   Szegedy C., 2014, P IEEE CVF C COMP VI, DOI 10.1109/CVPR.2015.7298594
   Thuseethan S, 2022, IEEE T MULTIMEDIA, V24, P4367, DOI 10.1109/TMM.2021.3116434
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Van Quang N., 2019, IEEE INT CONF AUTOMA, DOI [10.1109/FG.2019.8756544, DOI 10.1109/fg.2019.8756544]
   Verma M, 2020, IEEE T IMAGE PROCESS, V29, P1618, DOI 10.1109/TIP.2019.2912358
   Vinciarelli A, 2009, IMAGE VISION COMPUT, V27, P1743, DOI 10.1016/j.imavis.2008.11.007
   Vrij A, 2005, CLAR SYMP, P63
   Vural E., 2008, International Conference on Automotive Technologies (ICAT), P1
   Wang CY, 2020, NEUROCOMPUTING, V410, P354, DOI 10.1016/j.neucom.2020.06.005
   Wang SJ, 2018, NEUROCOMPUTING, V312, P251, DOI 10.1016/j.neucom.2018.05.107
   Wang SJ, 2015, IEEE T IMAGE PROCESS, V24, P6034, DOI 10.1109/TIP.2015.2496314
   Wang YD, 2015, LECT NOTES COMPUT SC, V9003, P525, DOI 10.1007/978-3-319-16865-4_34
   Wang YL, 2022, IEEE T PATTERN ANAL, V44, P3733, DOI 10.1109/TPAMI.2021.3052951
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Xia ZQ, 2020, IEEE T IMAGE PROCESS, V29, P8590, DOI 10.1109/TIP.2020.3018222
   Xia ZQ, 2020, IEEE T MULTIMEDIA, V22, P626, DOI 10.1109/TMM.2019.2931351
   Xu F, 2017, IEEE T AFFECT COMPUT, V8, P254, DOI 10.1109/TAFFC.2016.2518162
   Yan WJ, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086041
   Yin X, 2019, PROC CVPR IEEE, P5697, DOI 10.1109/CVPR.2019.00585
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zong Y, 2018, IEEE T MULTIMEDIA, V20, P3160, DOI 10.1109/TMM.2018.2820321
NR 56
TC 10
Z9 10
U1 2
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1345
EP 1358
DI 10.1109/TMM.2022.3141616
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100025
DA 2024-07-18
ER

PT J
AU Chen, Y
   Zhang, L
   Shen, Y
   Zhao, BN
   Zhou, YC
AF Chen, Yang
   Zhang, Lin
   Shen, Ying
   Zhao, Brian Nlong
   Zhou, Yicong
TI Extrinsic Self-Calibration of the Surround-View System: A Weakly
   Supervised Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Extrinsic calibration; photometric loss; surround-view dataset;
   surround-view system; weakly supervised learning
ID EFFICIENT; ACCURATE; ROBUST
AB An SVS usually consists of four wide-angle fisheye cameras mounted around the vehicle to sense the surrounding environment. From the images synchronously captured by cameras, a top-down surround-view can be synthesized, on the premise that both intrinsics and extrinsics of the cameras have been calibrated. At present, the intrinsic calibration approach is relatively complete and can be pipelined, while the extrinsic calibration is still immature. To fill such a research gap, we propose a novel extrinsic self-calibration scheme which follows a weakly supervised framework, namely WESNet (Weakly-supervised Extrinsic Self-calibration Network). The training of WESNet consists of two stages. First, we utilize the corners in a few calibration site images as the weak supervision to roughly optimize the network by minimizing the geometric loss. Then, after the convergence in the first stage, we additionally introduce a self-supervised photometric loss term that can be constructed by the photometric information from natural images for further fine-tuning. Besides, to support training, we totally collected 19,078 groups of synchronously captured fisheye images under various environmental conditions. To our knowledge, thus far this is the largest surround-view dataset containing original fisheye images. By means of learning prior knowledge from the training data, WESNet takes the original fisheye images synchronously collected as the input, and directly yields extrinsics end-to-end with little labor cost. Its efficiency and efficacy have been corroborated by extensive experiments conducted on our collected dataset. To make our results reproducible, source code and the collected dataset have been released.(1)
C1 [Chen, Yang; Zhang, Lin; Shen, Ying] Tongji Univ, Sch Software Engn, Shanghai 201804, Peoples R China.
   [Zhao, Brian Nlong] Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
   [Zhou, Yicong] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
C3 Tongji University; University of Southern California; University of
   Macau
RP Zhang, L (corresponding author), Tongji Univ, Sch Software Engn, Shanghai 201804, Peoples R China.
EM 2011439@tongji.edu.cn; cslinzhang@tongji.edu.cn; yingshen@tongji.edu.cn;
   briannlongzhao@gmail.com; yicongzhou@um.edu.mo
RI Zhou, Yicong/A-8017-2009; Li, Chun/KBC-9591-2024
OI Zhou, Yicong/0000-0002-4487-6384; Shen, Ying/0000-0002-2966-7955; Zhang,
   Lin/0000-0002-4360-5523
FU National Natural Science Foundation of China [61973235, 61972285,
   61936014]; Natural Science Foundation of Shanghai [19ZR1461300];
   Shanghai Science and Technology Innovation Plan [20510760400]; Dawn
   Program of Shanghai Municipal Education Commission [21SG23]; Shanghai
   Municipal Science and Technology Major Project [2021SHZDZX0100];
   Fundamental Research Funds for the Central Universities
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61973235, 61972285, and 61936014, in
   part by the Natural Science Foundation of Shanghai under Grant
   19ZR1461300, in part by the Shanghai Science and Technology Innovation
   Planunder Grant 20510760400, in part by the Dawn Program of Shanghai
   Municipal Education Commission under Grant 21SG23, in part by the
   Shanghai Municipal Science and Technology Major Project under Grant
   2021SHZDZX0100, and inpart by the Fundamental Research Funds for the
   Central Universities.
CR Ahmed M. T., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P463, DOI 10.1109/ICCV.1999.791257
   BATTITI R, 1992, NEURAL COMPUT, V4, P141, DOI 10.1162/neco.1992.4.2.141
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Choi K, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18092956
   Du F., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P477, DOI 10.1109/CVPR.1993.341087
   Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577
   Gao Y, 2018, IEEE T INTELL TRANSP, V19, P320, DOI 10.1109/TITS.2017.2750087
   Giering M., 2015, IEEE HIGH PERF EXTR, P1
   Gong XX, 2021, IEEE T MULTIMEDIA, V23, P2820, DOI 10.1109/TMM.2020.3017886
   Gressmann M, 2011, IEEE INT C INTELL TR, P1317, DOI 10.1109/ITSC.2011.6082895
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hedi A., 2012, IFAC P, P120
   Heng L, 2014, IEEE INT CONF ROBOT, P4912, DOI 10.1109/ICRA.2014.6907579
   Heng L, 2013, IEEE INT C INT ROBOT, P1793, DOI 10.1109/IROS.2013.6696592
   Hou C., 2007, P AS C COMP VIS, P18
   Irani M., 1999, P WORKSH VIS ALG THE, P267
   Iyer G, 2018, IEEE INT C INT ROBOT, P1110, DOI 10.1109/IROS.2018.8593693
   Izquierdo E, 2003, IEEE T MULTIMEDIA, V5, P293, DOI 10.1109/TMM.2003.814910
   Kingma D. P., 2014, arXiv
   Klette R., 1998, COMPUTER VISION 3 DI, V1st
   Li LS, 2017, IEEE INT CON MULTI, P649, DOI 10.1109/ICME.2017.8019419
   Lian Duan, 2011, 2011 IEEE International Conference on Vehicular Electronics and Safety (ICVES 2011), P238, DOI 10.1109/ICVES.2011.5983821
   Lin CC, 2012, SENSORS-BASEL, V12, P4431, DOI 10.3390/s120404431
   Liu X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P383, DOI 10.1145/3343031.3350885
   Liu YC, 2008, LECT NOTES COMPUT SC, V4931, P207
   Lourakis MIA, 2010, LECT NOTES COMPUT SC, V6312, P43, DOI 10.1007/978-3-642-15552-9_4
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105
   Paszke A, 2019, ADV NEUR IN, V32
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Schneider N, 2017, IEEE INT VEH SYM, P1803, DOI 10.1109/IVS.2017.7995968
   Shao X, 2019, IEEE INT CON MULTI, P1486, DOI 10.1109/ICME.2019.00257
   Workman S, 2015, IEEE IMAGE PROC, P1369, DOI 10.1109/ICIP.2015.7351024
   Xu J, 2000, PROCEEDINGS OF THE IEEE INTELLIGENT VEHICLES SYMPOSIUM 2000, P725, DOI 10.1109/IVS.2000.898435
   Yang X, 2021, IEEE T MULTIMEDIA, V23, P4208, DOI 10.1109/TMM.2020.3038323
   Yu YK, 2009, IEEE T MULTIMEDIA, V11, P182, DOI 10.1109/TMM.2008.2008871
   Zhang BY, 2014, IEEE COMPUT SOC CONF, P676, DOI 10.1109/CVPRW.2014.103
   Zhang L, 2019, IEEE IMAGE PROC, P4150, DOI [10.1109/ICIP.2019.8803453, 10.1109/icip.2019.8803453]
   Zhang L, 2018, IEEE T IMAGE PROCESS, V27, P5350, DOI 10.1109/TIP.2018.2857407
   Zhang TJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3153, DOI 10.1145/3474085.3475461
   Zhang TJ, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102803
   Zhao K, 2014, 2014 IEEE 17TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P1490, DOI 10.1109/ITSC.2014.6957643
   Zhengyou Zhang, 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P666, DOI 10.1109/ICCV.1999.791289
   Zhu HJ, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY AND COMPUTER SCIENCE, VOL 1, PROCEEDINGS, P321, DOI 10.1109/ITCS.2009.72
NR 47
TC 2
Z9 2
U1 3
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1622
EP 1635
DI 10.1109/TMM.2022.3144889
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100003
DA 2024-07-18
ER

PT J
AU Fu, Y
   Wang, ZC
   Zhang, T
   Zhang, J
AF Fu, Ying
   Wang, Zichun
   Zhang, Tao
   Zhang, Jun
TI Low-Light Raw Video Denoising With a High-Quality Realistic Motion
   Dataset
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Raw video denoising; transformer; convolutional neural network;
   temporal-spatial self-attention
ID IMAGE; ENHANCEMENT
AB Recently, supervised deep-learning methods have shown their effectiveness on raw video denoising in low-light. However, existing training datasets have specific drawbacks, e.g., inaccurate noise modeling in synthetic datasets, simple motion created by hand or fixed motion, and limited-quality ground truth caused by the beam splitter in real captured datasets. These defects significantly decline the performance of network when tackling real low-light video sequences, where noise distribution and motion patterns are extremely complex. In this paper, we collect a raw video denoising dataset in low-light with complex motion and high-quality ground truth, overcoming the drawbacks of previous datasets. Specifically, we capture 210 paired videos, each containing short/long exposure pairs of real video frames with dynamic objects and diverse scenes displayed on a high-end monitor. Besides, since spatial self-similarity has been extensively utilized in image tasks, harnessing this property for network design is more crucial for video denoising as temporal redundancy. To effectively exploit the intrinsic temporal-spatial self-similarity of complex motion in real videos, we propose a new Transformer-based network, which can effectively combine the locality of convolution with the long-range modeling ability of 3D temporal-spatial self-attention. Extensive experiments verify the value of our dataset and the effectiveness of our method on various metrics.
C1 [Fu, Ying; Wang, Zichun; Zhang, Tao] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
   [Fu, Ying; Zhang, Jun] Yangtze Delta Reg Acad Beijing Inst Technol, Jiaxing 314000, Peoples R China.
   [Zhang, Jun] Beijing Inst Technol, MIIT Key Lab Complex Field Intelligent Explorat, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology; Beijing Institute of Technology
RP Zhang, J (corresponding author), Beijing Inst Technol, MIIT Key Lab Complex Field Intelligent Explorat, Beijing 100081, Peoples R China.
EM fuying@bit.edu.cn; wangzichun@bit.edu.cn; tzhang@bit.edu.cn;
   zhjun@bit.edu.cn
RI Wang, Zichun/AAP-3633-2020; Zhang, Tao/AAF-6116-2019
OI Wang, Zichun/0000-0002-4280-2787; tao, zhang/0000-0002-7358-0603
FU National Natural Science Foundation of China
FX No Statement Available
CR Abdelhamed A, 2018, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2018.00182
   Brooks T, 2019, PROC CVPR IEEE, P11028, DOI 10.1109/CVPR.2019.01129
   Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952
   Chen C, 2020, IEEE T PATTERN ANAL, V42, P3071, DOI 10.1109/TPAMI.2019.2921548
   Chen C, 2019, PROC CVPR IEEE, P1652, DOI 10.1109/CVPR.2019.00175
   Chen C, 2019, IEEE I CONF COMP VIS, P3184, DOI 10.1109/ICCV.2019.00328
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen HA, 2022, IEEE T MULTIMEDIA, V24, P2164, DOI 10.1109/TMM.2021.3077140
   Chen XY, 2016, PROC SPIE, V9971, DOI 10.1117/12.2239260
   Claus M, 2019, IEEE COMPUT SOC CONF, P1843, DOI 10.1109/CVPRW.2019.00235
   Dai JJ, 2013, IEEE T CIRC SYST VID, V23, P128, DOI 10.1109/TCSVT.2012.2203203
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Du Y, 2021, IEEE T MULTIMEDIA, V23, P2139, DOI 10.1109/TMM.2020.3008057
   Dussault D, 2004, P SOC PHOTO-OPT INS, V5563, P195, DOI 10.1117/12.561839
   Foi A, 2008, IEEE T IMAGE PROCESS, V17, P1737, DOI 10.1109/TIP.2008.2001399
   G. V. Research, 2016, Image sensors market analysis
   Guo LW, 2007, IEEE T CIRC SYST VID, V17, P1423, DOI 10.1109/TCSVT.2007.903797
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   Hu X., 2021, P INT JOINT C ART IN, P729
   Ji XZ, 2020, IEEE COMPUT SOC CONF, P1914, DOI 10.1109/CVPRW50498.2020.00241
   Jiang HY, 2019, IEEE I CONF COMP VIS, P7323, DOI 10.1109/ICCV.2019.00742
   Kirmani A, 2014, SCIENCE, V343, P58, DOI 10.1126/science.1246775
   Liu HF, 2018, IEEE T CIRC SYST VID, V28, P3321, DOI 10.1109/TCSVT.2017.2759187
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lv F., 2018, P BMVC, V220, P4
   Ma JY, 2022, IEEE T MULTIMEDIA, V24, P3157, DOI 10.1109/TMM.2021.3094058
   Ma RJ, 2022, IEEE T MULTIMEDIA, V24, P2366, DOI 10.1109/TMM.2021.3079697
   Maggioni M, 2021, PROC CVPR IEEE, P3465, DOI 10.1109/CVPR46437.2021.00347
   Maggioni M, 2012, IEEE T IMAGE PROCESS, V21, P3952, DOI 10.1109/TIP.2012.2199324
   Maggioni M, 2011, PROC SPIE, V7870, DOI 10.1117/12.872569
   Malladi SRSP, 2021, IEEE T MULTIMEDIA, V23, P2297, DOI 10.1109/TMM.2020.3009502
   Nagahara H, 2008, LECT NOTES COMPUT SC, V5305, P60, DOI 10.1007/978-3-540-88693-8_5
   Plötz T, 2017, PROC CVPR IEEE, P2750, DOI 10.1109/CVPR.2017.294
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Tassano M, 2020, PROC CVPR IEEE, P1351, DOI 10.1109/CVPR42600.2020.00143
   Tassano M, 2019, IEEE IMAGE PROC, P1805, DOI [10.1109/ICIP.2019.8803136, 10.1109/icip.2019.8803136]
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Varghese G, 2010, IEEE T CIRC SYST VID, V20, P1032, DOI 10.1109/TCSVT.2010.2051366
   Wadhwa N, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201329
   Wang Ruixing, 2021, P IEEECVF INT C COMP, P9700
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei KX, 2022, IEEE T PATTERN ANAL, V44, P8520, DOI 10.1109/TPAMI.2021.3103114
   Wei KX, 2020, PROC CVPR IEEE, P2755, DOI 10.1109/CVPR42600.2020.00283
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yue HJ, 2020, PROC CVPR IEEE, P2298, DOI 10.1109/CVPR42600.2020.00237
   Zeng YH, 2021, LECT NOTES COMPUT SC, V13021, P66, DOI 10.1007/978-3-030-88010-1_6
   Zhang K, 2022, IEEE T PATTERN ANAL, V44, P6360, DOI 10.1109/TPAMI.2021.3088914
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang L, 2005, IEEE T CIRC SYST VID, V15, P469, DOI 10.1109/TCSVT.2005.844456
   Zhang Yi, 2021, P IEEE CVF INT C COM, P4593
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 54
TC 3
Z9 3
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8119
EP 8131
DI 10.1109/TMM.2022.3233247
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP5R1
UT WOS:001133278300015
DA 2024-07-18
ER

PT J
AU Guo, PF
   Liu, HT
   Zeng, DL
   Xiang, T
   Li, LD
   Gu, K
AF Guo, Pengfei
   Liu, Hantao
   Zeng, Delu
   Xiang, Tao
   Li, Leida
   Gu, Ke
TI An Underwater Image Quality Assessment Metric
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Underwater; image quality; enhancement; human visual system; objective
   metric
ID VISUAL-ATTENTION; ENHANCEMENT; FRAMEWORK; VISIBILITY; VISION; LIGHT;
   COLOR
AB Various image enhancement algorithms are adopted to improve underwater images that often suffer from visual distortions. It is critical to assess the output quality of underwater images undergoing enhancement algorithms, and use the results to optimise underwater imaging systems. In our previous study, we created a benchmark for quality assessment of underwater image enhancement via subjective experiments. Building on the benchmark, this paper proposes a new objective metric that can automatically assess the output quality of image enhancement, namely UWEQM. By characterising specific underwater physics and relevant properties of the human visual system, image quality attributes are computed and combined to yield an overall metric. Experimental results show that the proposed UWEQM metric yields good performance in predicting image quality as perceived by human subjects.
C1 [Guo, Pengfei] Zhongkai Univ Agr & Engn, Sch Computat Sci, Guangzhou 510225, Peoples R China.
   [Guo, Pengfei; Zeng, Delu] South China Univ Technol, Sch Math, Guangzhou 510641, Peoples R China.
   [Liu, Hantao] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 3AA, Wales.
   [Xiang, Tao] Chongqing Univ, Coll Comp Sci, Chongqing 400044, Peoples R China.
   [Li, Leida] Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
   [Gu, Ke] Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
C3 Zhongkai University of Agriculture & Engineering; South China University
   of Technology; Cardiff University; Chongqing University; Xidian
   University; Beijing University of Technology
RP Liu, HT (corresponding author), Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 3AA, Wales.
EM guopfzhku@163.com; liuh35@cardiff.ac.uk; dltsang@xmu.edu.cn;
   txiang@cqu.edu.cn; ldli@xidian.edu.cn; guke.doctor@gmail.com
FU Guangdong basic and applied basic research foundation [2020A1515110958];
   Fundamental Research Program 401 of Guangdong, China [2020B1515310023]
FX This work was supported in part by Guangdong basic and applied basic
   research foundation under Grant 2020A1515110958 and the Fundamental
   Research Program 401 of Guangdong, China under Grant 2020B1515310023.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Joao M Ascenso.
CR Akkaynak D, 2018, PROC CVPR IEEE, P6723, DOI 10.1109/CVPR.2018.00703
   Alippi C, 2011, IEEE SENS J, V11, P45, DOI 10.1109/JSEN.2010.2051539
   Ancuti CO, 2013, IEEE T IMAGE PROCESS, V22, P3271, DOI 10.1109/TIP.2013.2262284
   Bailey GN, 2008, QUATERNARY SCI REV, V27, P2153, DOI 10.1016/j.quascirev.2008.08.012
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Carlevaris-Bianco N, 2010, OCEANS-IEEE
   Chen CLP, 2014, IEEE T GEOSCI REMOTE, V52, P574, DOI 10.1109/TGRS.2013.2242477
   Chen DQ, 2020, IEEE T IMAGE PROCESS, V29, P6496, DOI 10.1109/TIP.2020.2990342
   Chen WL, 2021, IEEE T MULTIMEDIA, V23, P1008, DOI 10.1109/TMM.2020.2991546
   Chen ZY, 2014, PROC CVPR IEEE, P3003, DOI 10.1109/CVPR.2014.384
   DUNTLEY SQ, 1963, J OPT SOC AM, V53, P214, DOI 10.1364/JOSA.53.000214
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Freitas PG, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P52, DOI 10.1145/3204949.3204960
   Gaudron JO, 2013, SENSOR ACTUAT A-PHYS, V201, P289, DOI 10.1016/j.sna.2013.07.017
   Ghani Ahmad Shahrizan Abdul, 2014, Proceedings 2014 IEEE Fourth International Conference on Consumer Electronics - Berlin (ICCEBerlin), P219, DOI 10.1109/ICCE-Berlin.2014.7034265
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Guo PF, 2022, IEEE T MULTIMEDIA, V24, P1980, DOI 10.1109/TMM.2021.3074825
   Guo PF, 2020, COMPUT ELECTRON AGR, V175, DOI 10.1016/j.compag.2020.105608
   Hadizadeh H, 2018, IEEE T MULTIMEDIA, V20, P392, DOI 10.1109/TMM.2017.2740023
   Han JF, 2015, APPL OPTICS, V54, P3294, DOI 10.1364/AO.54.003294
   Hassen R, 2013, IEEE T IMAGE PROCESS, V22, P2798, DOI 10.1109/TIP.2013.2251643
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang DM, 2018, LECT NOTES COMPUT SC, V10704, P453, DOI 10.1007/978-3-319-73603-7_37
   Huang J, 1997, PROC CVPR IEEE, P762, DOI 10.1109/CVPR.1997.609412
   Hung-Yu Yang, 2011, Proceedings of the 2011 2nd International Conference on Innovations in Bio-Inspired Computing and Applications (IBICA 2011), P17, DOI 10.1109/IBICA.2011.9
   Iqbal K., 2010, 2010 IEEE International Conference on Systems, Man and Cybernetics (SMC 2010), P1703, DOI 10.1109/ICSMC.2010.5642311
   Iqbal Kashif, 2007, IAENG International Journal of Computer Science, V34, P239
   Juric S, 2009, ELMAR PROC, P83
   Kim J, 2017, IEEE J-STSP, V11, P206, DOI 10.1109/JSTSP.2016.2639328
   Kim YT, 1997, IEEE T CONSUM ELECTR, V43, P1, DOI 10.1109/30.580378
   Krogh A., 1995, Advances in Neural Information Processing Systems 7, P231
   Leveque L, 2020, IEEE IMAGE PROC, P116, DOI 10.1109/ICIP40778.2020.9190737
   Li CY, 2016, IEEE T IMAGE PROCESS, V26, P5664, DOI 10.1109/TIP.2016.2612882
   Li CY, 2016, INT CONF ACOUST SPEE, P1731, DOI 10.1109/ICASSP.2016.7471973
   Li LD, 2017, IEEE T MULTIMEDIA, V19, P1030, DOI 10.1109/TMM.2016.2640762
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P5432, DOI 10.1109/TIP.2015.2482903
   Ling J, 2014, IEEE J OCEANIC ENG, V39, P59, DOI 10.1109/JOE.2012.2234893
   Liu Chao, 2010, 2010 2nd International Conference on Computer Engineering and Technology (ICCET), P35, DOI 10.1109/ICCET.2010.5485339
   Liu HT, 2011, IEEE T CIRC SYST VID, V21, P971, DOI 10.1109/TCSVT.2011.2133770
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Liu YT, 2019, IEEE T MULTIMEDIA, V21, P135, DOI 10.1109/TMM.2018.2849602
   Lu HM, 2016, J VIS COMMUN IMAGE R, V38, P504, DOI 10.1016/j.jvcir.2016.03.029
   LURIA SM, 1970, SCIENCE, V167, P1454, DOI 10.1126/science.167.3924.1454
   Ma KD, 2020, IEEE T PATTERN ANAL, V42, P851, DOI 10.1109/TPAMI.2018.2889948
   Miao Z., 2018, P 13 ACM INT C UND N
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2011, CONF REC ASILOMAR C, P723, DOI 10.1109/ACSSC.2011.6190099
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Ninassi A, 2007, IEEE IMAGE PROC, P733
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Panetta K, 2011, IEEE T SYST MAN CY B, V41, P460, DOI 10.1109/TSMCB.2010.2058847
   Peng YT, 2018, IEEE T IMAGE PROCESS, V27, P2856, DOI 10.1109/TIP.2018.2813092
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Schechner YY, 2005, IEEE J OCEANIC ENG, V30, P570, DOI 10.1109/JOE.2005.850871
   Schettini R, 2010, EURASIP J ADV SIG PR, DOI 10.1155/2010/746052
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Shrivastava S., 2015, P IEEE INT C COMP IN, P1
   Song W, 2018, LECT NOTES COMPUT SC, V11164, P678, DOI 10.1007/978-3-030-00776-8_62
   Stark JA, 2000, IEEE T IMAGE PROCESS, V9, P889, DOI 10.1109/83.841534
   Sun SM, 2023, IEEE T MULTIMEDIA, V25, P2912, DOI 10.1109/TMM.2022.3152942
   Wan ZL, 2020, IEEE T MULTIMEDIA, V22, P2024, DOI 10.1109/TMM.2019.2950533
   Wang Y, 2019, IEEE ACCESS, V7, P140233, DOI 10.1109/ACCESS.2019.2932130
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Yan J, 2017, J IMAGING SCI TECHN, V61, DOI 10.2352/J.ImagingSci.Technol.2017.61.5.050501
   Yan Z, 2014, IEEE GEOSCI REMOTE S, V11, P833, DOI 10.1109/LGRS.2013.2279485
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Yang M, 2014, IEEE SIGNAL PROC LET, V21, P1215, DOI 10.1109/LSP.2014.2330848
   Zhang W, 2017, IEEE T IMAGE PROCESS, V26, P2424, DOI 10.1109/TIP.2017.2681424
   Zhang W, 2016, IEEE T NEUR NET LEAR, V27, P1266, DOI 10.1109/TNNLS.2015.2461603
   Zhao MM, 2015, 2015 VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP), DOI 10.1109/VCIP.2015.7457900
   Zhao XW, 2015, OCEAN ENG, V94, P163, DOI 10.1016/j.oceaneng.2014.11.036
   Zhu R, 2018, IEEE SIGNAL PROC MAG, V35, P133, DOI 10.1109/MSP.2018.2829209
   Zuiderveld K., 1994, Graphics Gems, P474, DOI 10.1016/B978-0-12-336156-1.50061-6
NR 82
TC 3
Z9 3
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5093
EP 5106
DI 10.1109/TMM.2022.3187212
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WZ4M9
UT WOS:001258685000001
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Huang, YP
   Li, LD
   Yang, YZ
   Li, YQ
   Guo, YD
AF Huang, Yipo
   Li, Leida
   Yang, Yuzhe
   Li, Yaqian
   Guo, Yandong
TI Explainable and Generalizable Blind Image Quality Assessment via
   Semantic Attribute Reasoning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Blind image quality assessment; explainability; generalization ability;
   semantic attribute; graph convolution network
ID STATISTICS
AB Blind image quality assessment (BIQA) that can directly evaluate image quality without perfect-quality reference has been a long-standing research topic. Although the existing BIQA models have achieved very encouraging performance, the lack of explainability and generalization ability limits their real-world applications to a great extent. People usually assess image quality according to semantic attributes, e.g., brightness, color, contrast, noise and sharpness. Furthermore, judgment on image quality is also impacted by the scene presented in the image. Therefore, the inherent relationship between semantic attributes and scenes is crucial for image quality assessment, which has rarely been explored yet. With this motivation, this paper presents a Semantic Attribute Reasoning based image QUality Evaluator (SARQUE). Specifically, we propose a two-stream network to predict semantic attributes and scene categories from distorted images. To investigate the inherent relationship between the semantic attributes and scene category, a semantic reasoning module is further proposed based on the graph convolution network (GCN), producing the final quality score. Extensive experiments conducted on five in-the-wild image quality databases demonstrate the superiority of the proposed SARQUE model over the state-of-the-arts. Furthermore, the proposed model features better explainability and generalization ability due to the use of semantic attributes.
C1 [Huang, Yipo; Li, Leida] Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
   [Yang, Yuzhe; Li, Yaqian; Guo, Yandong] OPPO Res Inst, Intelligent Percept & Interact Res Dept, Shanghai 200032, Peoples R China.
C3 Xidian University
RP Li, LD (corresponding author), Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
EM huangyipo@stu.xidian.edu.cn; reader1104@hotmail.com;
   ippllewis@gmail.com; liyaqian@oppo.com; yandong.guo@live.com
OI Yang, Yuzhe/0000-0001-9098-2105
FU National Natural Science Foundation of China [62171340, 61991451,
   61771473]; OPPO Research Fund; Fundamental Research Funds for the
   Central Universities; Xidian University [YJS2202]; Key Project of
   Shaanxi Provincial Department of Education (Collaborative Innovation
   Center) [20JY024]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62171340, 61991451, and 61771473, in
   part by OPPO Research Fund, in part by the Fundamental Research Funds
   for the Central Universities, the Innovation Fund of Xidian University
   under Grant YJS2202, and in part by the Key Project of Shaanxi
   Provincial Department of Education (Collaborative Innovation Center)
   under Grant 20JY024.
CR Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Bosse S, 2016, IEEE IMAGE PROC, P3773, DOI 10.1109/ICIP.2016.7533065
   Chen ZM, 2023, IEEE T PATTERN ANAL, V45, P6969, DOI 10.1109/TPAMI.2021.3063496
   Ciancio A, 2011, IEEE T IMAGE PROCESS, V20, P64, DOI 10.1109/TIP.2010.2053549
   Dendi SVR, 2019, IEEE SIGNAL PROC LET, V26, P89, DOI 10.1109/LSP.2018.2879518
   Ding KY, 2022, IEEE T PATTERN ANAL, V44, P2567, DOI 10.1109/TPAMI.2020.3045810
   Dutta T, 2021, IEEE T MULTIMEDIA, V23, P2833, DOI 10.1109/TMM.2020.3017918
   Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Gong P, 2020, IEEE T COGN DEV SYST, V12, P73, DOI 10.1109/TCDS.2019.2902250
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Hancheng Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14131, DOI 10.1109/CVPR42600.2020.01415
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Hou WL, 2015, IEEE T NEUR NET LEAR, V26, P1275, DOI 10.1109/TNNLS.2014.2336852
   Huang YP, 2020, IEEE SIGNAL PROC LET, V27, P685, DOI 10.1109/LSP.2020.2988830
   Huang YQ, 2020, IEEE T IMAGE PROCESS, V29, P4013, DOI 10.1109/TIP.2020.2969330
   Jin Z, 2020, IEEE T MULTIMEDIA, V22, P1055, DOI 10.1109/TMM.2019.2938340
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kim J, 2019, IEEE T NEUR NET LEAR, V30, P11, DOI 10.1109/TNNLS.2018.2829819
   Kim J, 2017, IEEE SIGNAL PROC MAG, V34, P130, DOI 10.1109/MSP.2017.2736018
   Kim J, 2017, IEEE J-STSP, V11, P206, DOI 10.1109/JSTSP.2016.2639328
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Leveque L, 2020, IEEE IMAGE PROC, P116, DOI 10.1109/ICIP40778.2020.9190737
   Li DQ, 2019, IEEE T MULTIMEDIA, V21, P1221, DOI 10.1109/TMM.2018.2875354
   Li L., 2021, IEEE Trans. Circuits Syst. Video Technol., VPP, P1
   Li LD, 2017, IEEE T MULTIMEDIA, V19, P1030, DOI 10.1109/TMM.2016.2640762
   Li LD, 2016, IEEE T MULTIMEDIA, V18, P1085, DOI 10.1109/TMM.2016.2545398
   Liang KM, 2019, IEEE T PATTERN ANAL, V41, P1747, DOI 10.1109/TPAMI.2018.2836461
   Liu DY, 2020, IEEE T MULTIMEDIA, V22, P846, DOI 10.1109/TMM.2019.2934426
   Liu LX, 2016, SIGNAL PROCESS-IMAGE, V40, P1, DOI 10.1016/j.image.2015.10.005
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Liu Y, 2022, IEEE T PATTERN ANAL, V44, P3688, DOI 10.1109/TPAMI.2021.3053577
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503
   Miao YQ, 2021, IEEE T IMAGE PROCESS, V30, P7554, DOI 10.1109/TIP.2021.3106805
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Ponomarenko Nikolay, 2013, 2013 4th European Workshop on Visual Information Processing (EUVIP), P106
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Shao Zhuang, 2022, IEEE Trans Neural Netw Learn Syst, VPP, DOI 10.1109/TNNLS.2022.3152990
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Virtanen T, 2015, IEEE T IMAGE PROCESS, V24, P390, DOI 10.1109/TIP.2014.2378061
   Vu PV, 2012, IEEE SIGNAL PROC LET, V19, P423, DOI 10.1109/LSP.2012.2199980
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu JJ, 2020, IEEE T IMAGE PROCESS, V29, P7414, DOI 10.1109/TIP.2020.3002478
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2957, DOI 10.1109/TMM.2019.2914883
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2603, DOI 10.1109/TMM.2019.2904879
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Zhai GT, 2021, IEEE T MULTIMEDIA, V23, P3700, DOI 10.1109/TMM.2020.3029891
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou Y, 2019, IEEE T IMAGE PROCESS, V28, P4566, DOI 10.1109/TIP.2019.2912463
   Zhu HC, 2022, IEEE T CIRC SYST VID, V32, P1048, DOI 10.1109/TCSVT.2021.3073410
NR 62
TC 3
Z9 3
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7672
EP 7685
DI 10.1109/TMM.2022.3225728
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400005
DA 2024-07-18
ER

PT J
AU Li, M
   Fu, B
   Chen, H
   He, JJ
   Qiao, Y
AF Li, Ming
   Fu, Bin
   Chen, Han
   He, Junjun
   Qiao, Yu
TI Dual Relation Network for Scene Text Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Text recognition; Visualization; Task analysis;
   Context modeling; Data mining; Convolution; Scene text recognition;
   scene optical character recognition; deep learning
AB Local visual and long-range contextual features yield two complementary cues for human reading text in natural scene. Existing scene text recognition methods mainly extract local features at a low level and then model long-range dependencies at a high level, this sequential pipeline may be sub-optimal to construct complete and effective representation. Except for high-level features, long-range contextual relation is of importance in low-level features as well since it can help separate different characters based on the intervals between characters and thus enhance the character features. To address this issue, we develop a dual relation module to extract complementary features in a parallel manner for scene text recognition, which consists of a local visual branch and a long-range contextual branch. The local visual branch employs a topological-aware operation to model intra-character characteristic and extract discriminative features of different characters. Meanwhile, the long-range contextual branch utilizes a simple but effective strategy to incorporate inter-character relations into feature maps. Our dual relation module is a plug-and-play block which can be easily incorporated into modern deep architectures. Experimental results demonstrate that our methods achieved top performance on several standard benchmarks. Code and models will become publicly available in the future.
C1 [Li, Ming; Fu, Bin; Chen, Han; He, Junjun; Qiao, Yu] Chinese Acad Sci, Shenzhen Inst Adv Technol, Guangdong Hong Kong Macao Joint Lab Human Machine, Hong Kong 518055, Guangdong, Peoples R China.
   [Qiao, Yu] Shanghai AI Lab, Shanghai 518055, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Shanghai Artificial Intelligence Laboratory
RP Qiao, Y (corresponding author), Shanghai AI Lab, Shanghai 518055, Peoples R China.
EM ming.li3@siat.ac.cn; bin.fu@siat.ac.cn; han.chen@siat.ac.cn;
   hejunjun@sjtu.edu.cn; yu.qiao@siat.ac.cn
OI Fu, Bin/0000-0002-1526-8654; He, Junjun/0000-0002-1813-1784
FU Joint Laboratory of CAS-HK; Shenzhen Research Program
   [JSGG20191129141212311, RCJC20200714114557087]; Shanghai Committee of
   Science and Technology [21DZ1100100]
FX This work was supported in part by the Joint Laboratory of CAS-HK, in
   part by the Shenzhen Research Program under Grants JSGG20191129141212311
   and RCJC20200714114557087, and in part by the Shanghai Committee of
   Science and Technology under Grant 21DZ1100100.
CR Almazán J, 2014, IEEE T PATTERN ANAL, V36, P2552, DOI 10.1109/TPAMI.2014.2339814
   Baek J, 2019, IEEE I CONF COMP VIS, P4714, DOI 10.1109/ICCV.2019.00481
   Bai F, 2018, PROC CVPR IEEE, P1508, DOI 10.1109/CVPR.2018.00163
   Bartz C, 2018, AAAI CONF ARTIF INTE, P6674
   Beltagy I, 2020, Arxiv, DOI arXiv:2004.05150
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen XL, 2004, IEEE T IMAGE PROCESS, V13, P87, DOI 10.1109/TIP.2003.819223
   Cheng ZZ, 2018, PROC CVPR IEEE, P5571, DOI 10.1109/CVPR.2018.00584
   Cheng ZZ, 2017, IEEE I CONF COMP VIS, P5086, DOI 10.1109/ICCV.2017.543
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Zeiler MD, 2012, Arxiv, DOI [arXiv:1212.5701, DOI 10.48550/ARXIV.1212.5701]
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai PW, 2022, IEEE T MULTIMEDIA, V24, P1883, DOI 10.1109/TMM.2021.3073575
   Dai PW, 2021, IEEE T IMAGE PROCESS, V30, P1687, DOI 10.1109/TIP.2020.3045602
   Dai PW, 2020, IEEE T MULTIMEDIA, V22, P1969, DOI 10.1109/TMM.2019.2952978
   Ding HH, 2018, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2018.00254
   Fang SC, 2021, PROC CVPR IEEE, P7094, DOI 10.1109/CVPR46437.2021.00702
   Fu J., 2018, Dual Attention Network for Scene Segmentation
   Fu J, 2019, IEEE I CONF COMP VIS, P6747, DOI 10.1109/ICCV.2019.00685
   Gao YZ, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2710-7
   Gordo A, 2015, PROC CVPR IEEE, P2956, DOI 10.1109/CVPR.2015.7298914
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Hu H, 2019, IEEE I CONF COMP VIS, P3463, DOI 10.1109/ICCV.2019.00356
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Jaderberg M., 2014, P WORKSH DEEP LEARN
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z
   Karaoglu S, 2017, IEEE T IMAGE PROCESS, V26, P3965, DOI 10.1109/TIP.2017.2707805
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Kavati I., 2017, Proc. Int. J. Elect. Comput. Eng., V7, P2496
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee CY, 2016, PROC CVPR IEEE, P2231, DOI 10.1109/CVPR.2016.245
   Li H, 2019, AAAI CONF ARTIF INTE, P8610
   Liang YQ, 2020, IEEE T MULTIMEDIA, V22, P1168, DOI 10.1109/TMM.2019.2941777
   Liao MH, 2019, AAAI CONF ARTIF INTE, P8714
   Liu W, 2018, AAAI CONF ARTIF INTE, P7154
   Liu Y, 2018, LECT NOTES COMPUT SC, V11209, P449, DOI 10.1007/978-3-030-01228-1_27
   Liu ZC, 2018, AAAI CONF ARTIF INTE, P7194
   Long SB, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01369-0
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Luo CJ, 2019, PATTERN RECOGN, V90, P109, DOI 10.1016/j.patcog.2019.01.020
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Mishra A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.127
   Mishra A, 2012, PROC CVPR IEEE, P2687, DOI 10.1109/CVPR.2012.6247990
   Nguyen N, 2021, PROC CVPR IEEE, P7379, DOI 10.1109/CVPR46437.2021.00730
   Ren XH, 2017, IEEE T MULTIMEDIA, V19, P506, DOI 10.1109/TMM.2016.2625259
   Risnumawan A, 2014, EXPERT SYST APPL, V41, P8027, DOI 10.1016/j.eswa.2014.07.008
   Shi BG, 2019, IEEE T PATTERN ANAL, V41, P2035, DOI 10.1109/TPAMI.2018.2848939
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Shi BG, 2016, PROC CVPR IEEE, P4168, DOI 10.1109/CVPR.2016.452
   Simonyan K, 2015, IEEE INT C ICLR
   Tang YB, 2018, IEEE T MULTIMEDIA, V20, P2276, DOI 10.1109/TMM.2018.2802644
   Phan TQ, 2013, IEEE I CONF COMP VIS, P569, DOI 10.1109/ICCV.2013.76
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang TW, 2020, AAAI CONF ARTIF INTE, V34, P12216
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu W., 2004, Proceedings of the ACM International Conference on Multimedia, P852
   Wu XP, 2021, IEEE T MULTIMEDIA, V23, P3427, DOI 10.1109/TMM.2020.3025696
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xue ML, 2021, IEEE T MULTIMEDIA, V23, P2706, DOI 10.1109/TMM.2020.3015037
   Yan RJ, 2021, PROC CVPR IEEE, P284, DOI 10.1109/CVPR46437.2021.00035
   Yang MingKun, 2019, ICCV
   Yang X, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3280
   Yao C, 2014, IEEE T IMAGE PROCESS, V23, P4737, DOI 10.1109/TIP.2014.2353813
   Yao C, 2014, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2014.515
   Yue X., 2020, ECCV, VVolume 12364, P135, DOI [DOI 10.1007/978-3-030-58529-7_9, 10.1007/978-3-030-58529-7_9]
   Zhan FN, 2019, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2019.00216
   Zhang JM, 2021, IEEE T MULTIMEDIA, V23, P2575, DOI 10.1109/TMM.2020.3013376
   Zhang PC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2978, DOI 10.1109/ICCV48922.2021.00299
   Zhang S, 2021, IEEE T MULTIMEDIA, V23, P454, DOI 10.1109/TMM.2020.2978630
   Zhang YP, 2021, IEEE T IMAGE PROCESS, V30, P3922, DOI 10.1109/TIP.2021.3066903
   Zhao H., 2020, P IEEE CVF C COMP VI, V2020, P10076, DOI 10.1109/CVPR42600.2020.01009
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhi Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13525, DOI 10.1109/CVPR42600.2020.01354
NR 81
TC 2
Z9 2
U1 1
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4094
EP 4107
DI 10.1109/TMM.2022.3171108
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200003
DA 2024-07-18
ER

PT J
AU Li, WH
   Huang, JW
   Lyu, WJ
   Guo, BS
   Jiang, WC
   Wang, JX
AF Li, Weihe
   Huang, Jiawei
   Lyu, Wenjun
   Guo, Baoshen
   Jiang, Wanchun
   Wang, Jianxin
TI RAV: Learning-Based Adaptive Streaming to Coordinate the Audio and Video
   Bitrate Selections
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video streaming; audio chunk; video chunk; DASH
ID DASH
AB Most commercial players adopt adaptive bitrate (ABR) algorithms to dynamically decide each chunk's bitrate based on the perceived network bandwidth and buffer occupancy. However, current ABR algorithms are agnostic of audio bitrate selection since they deem it has negligible influence on video bitrate selection due to small size of audio chunks. Nevertheless, with the development of audio technologies, the bitrate of audio content increases dramatically in recent years. Thus, inappropriate audio selection can significantly affect video selection and deteriorate the viewing experience. To tackle these inefficiencies, we propose a deep Reinforcement learning-based ABR algorithm that takes Audio and Video quality into account (RAV) to circumvent a series of suboptimal performances, like low playback quality, frequent playback interruptions, poor playback smoothness, and undesirable combinations of video and audio chunks. Furthermore, RAV trains a neural network model that automatically outputs the bitrates for future audio and video chunks without relying on any presumptions about the environment, achieving good robustness to a broad spectrum of conditions. By conducting trace-driven and real-world experiments, we demonstrate that RAV significantly ameliorates the average overall viewing quality by 37.96%-118.20% over the state-of-the-art ABR algorithms. In addition, we also conduct subjective experiments by inviting 32 volunteers, and 27/32 users strongly agree that RAV provides them a better viewing experience than existing ABR solutions.
C1 [Li, Weihe] Univ Edinburgh, Sch Informat, Edinburgh, Scotland.
   [Huang, Jiawei; Jiang, Wanchun; Wang, Jianxin] Cent South Univ, Sch Comp Sci & Engn, Changsha, Peoples R China.
   [Lyu, Wenjun] Rutgers State Univ, Dept Comp Sci, New Brunswick, NJ USA.
   [Guo, Baoshen] Southeast Univ, Sch Comp Sci & Engn, Nanjing, Peoples R China.
C3 University of Edinburgh; Central South University; Rutgers University
   System; Rutgers University New Brunswick; Southeast University - China
RP Huang, JW (corresponding author), Cent South Univ, Sch Comp Sci & Engn, Changsha, Peoples R China.
EM weihe.li@ed.ac.uk; jiaweihuang@csu.edu.cn; wenjun.lyu@rutgers.edu;
   guobaoshen@seu.edu.cn; jiangwc@csu.edu.cn; jxwang@csu.edu.cn
RI ; Wang, Jianxin/V-2800-2018
OI Guo, Baoshen/0000-0002-7435-8238; Li, Weihe/0000-0002-8516-0104; Huang,
   Jiawei/0000-0002-7578-4490; Wang, Jianxin/0000-0003-1516-0480; Lyu,
   Wenjun/0000-0002-7885-3105
FU National Natural Science Foundation of China [62132022, 61872387]; Key
   Research and Development Program of Hunan [2022WK2005]; Natural Science
   Foundation of Hunan Province, China [2021JJ30867]; Project of Foreign
   Cultural and Educational Expert [G20190018003]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62132022 and 61872387, in part by the
   Key Research and Development Program of Hunan under Grant 2022WK2005, in
   part by the Natural Science Foundation of Hunan Province, China under
   Grant 2021JJ30867, and in part by the Project of Foreign Cultural and
   Educational Expert under Grant G20190018003. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Zhi Wang. (Corresponding author: Jiawei Huang.)
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   ACM The Multimedia Live Video Streaming Grand Challenge, 2018, LTE/WiFi Dataset
   Akhtar Z, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P44, DOI 10.1145/3230543.3230558
   [Anonymous], 2022, Dolby Atmos Official Site
   [Anonymous], 2021, HD_FS Dataset
   [Anonymous], 2018, Youtube-dl developers
   [Anonymous], 2001, ITU-R Rec. BS-1387-1
   [Anonymous], 2017, Dash.js
   [Anonymous], 2016, FCC Dataset
   Apple, 2017, HLS authoring specification for Apple devices
   AT&T Developer Program, 2021, Streaming separate audio and video
   FFmpeg, 2017, FFmpeg project
   Google, 2016, ExoPlayer
   Guan YS, 2020, IEEE INFOCOM SER, P1103, DOI [10.1109/INFOCOM41043.2020.9155492, 10.1109/infocom41043.2020.9155492]
   Huang T., 2022, Comyco implementation
   Huang T., 2022, Pensieve-PPO
   Huang T., 2022, Zwei implementation
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Huang TC, 2022, IEEE J SEL AREA COMM, V40, P2485, DOI 10.1109/JSAC.2022.3180804
   Huang TC, 2020, NOSSDAV '20: PROCEEDINGS OF THE 2020 WORKSHOP ON NETWORK AND OPERATING SYSTEM SUPPORT FOR DIGITAL AUDIO AND VIDEO, P7, DOI 10.1145/3386290.3396930
   Huang TC, 2020, IEEE INFOCOM SER, P1967, DOI [10.1109/infocom41043.2020.9155411, 10.1109/INFOCOM41043.2020.9155411]
   Huang TC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P429, DOI 10.1145/3343031.3351014
   Huang TC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1208, DOI 10.1145/3240508.3240545
   Jiang J., 2012, P 8 INT C EM NETW EX, P97
   Kingma D. P., 2014, arXiv
   Konda VR, 2003, SIAM J CONTROL OPTIM, V42, P1143, DOI 10.1137/S0363012901385691
   Li WH, 2023, IEEE T MULTIMEDIA, V25, P2488, DOI 10.1109/TMM.2022.3147667
   Li WH, 2022, IEEE WIREL COMMUN LE, V11, P513, DOI 10.1109/LWC.2021.3134491
   Li Z., 2021, Toward a practical perceptual video quality metric
   Lillicrap, 2015, ARXIV150902971, P1
   Lillicrap T., 2015, arXiv, DOI 10.48550/arXiv.1509.02971
   Linux, 2014, tc
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Mnih V, 2016, PR MACH LEARN RES, V48
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Qiao CY, 2021, IEEE ACM T NETWORK, V29, P289, DOI 10.1109/TNET.2020.3032416
   Qin YY, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P189, DOI 10.1145/3304109.3306231
   Qin YY, 2019, PROCEEDINGS OF THE 15TH INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT '19), P158, DOI 10.1145/3359989.3365419
   Qin YY, 2017, IEEE INFOCOM SER
   Qin YY, 2018, CONEXT'18: PROCEEDINGS OF THE 14TH INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES, P366, DOI 10.1145/3281411.3281439
   Riiser Haakon, 2013, P 4 ACM MULT SYST C, P114, DOI DOI 10.1145/2483977.2483991
   Rodrigues R, 2020, MULTIMED TOOLS APPL, V79, P24595, DOI 10.1007/s11042-020-09047-6
   Sengupta S, 2018, I C NETWORK PROTOCOL, P165, DOI 10.1109/ICNP.2018.00026
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Spiteri K, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P123, DOI 10.1145/3204949.3204953
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   Tidal, 2022, Sound quality
   Wang B, 2022, IEEE T MULTIMEDIA, V24, P323, DOI 10.1109/TMM.2021.3050086
   Wang B, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1122, DOI 10.1145/3123266.3123284
   Wang J., 2020, P IEEE GLOB COMM, P1
   Xiph.Org, 2016, Xiph.org video test media
   Xiph.Org, 2020, BBB audio
   Xiph.org, 2021, Video test media
   Xu SC, 2017, PROCEEDINGS OF THE 2017 INTERNET MEASUREMENT CONFERENCE (IMC'17), P220, DOI 10.1145/3131365.3131386
   Yan FY, 2020, PROCEEDINGS OF THE 17TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P495
   Ye DH, 2020, AAAI CONF ARTIF INTE, V34, P6672
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   YouTube, 2019, Recommended upload encoding settings
   Yuan T., 2017, TF.Learn: TensorFlow's high-level module for distributed machine learning
NR 61
TC 7
Z9 7
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5662
EP 5675
DI 10.1109/TMM.2022.3198013
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500002
DA 2024-07-18
ER

PT J
AU Li, YM
   You, JX
   Zhou, JT
   Wang, W
   Liao, X
   Li, X
AF Li, Yuanman
   You, Jiaxiang
   Zhou, Jiantao
   Wang, Wei
   Liao, Xin
   Li, Xia
TI Image Operation Chain Detection with Machine Translation Framework
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Machine translation; Feature extraction; Image forensics; Electronic
   mail; Detectors; Decoding; Correlation; Operation chain detection; image
   forensics; machine translation; Transformer
ID FORENSIC DETECTION; NETWORKS
AB The aim of operation chain detection for a given manipulated image is to reveal the operations involved and the order in which they were applied, which is significant for image processing and multimedia forensics. Currently, all existing approaches simply treat image operation chain detection as a classification problem and consider only chains of at most two operations. Considering the complex interplay between operations and the exponentially increasing solution space, detecting longer operation chains is extremely challenging. To address this issue, in this work, we devise a new methodology for image operation chain detection. Different from existing approaches based on classification modeling, we strategically conduct operation chain detection within a machine translation framework. Specifically, the chain in our work is modeled as a sentence in a target language, with each possible operation represented by a word in that language. When executing chain detection, we propose first transforming the input image into a sentence in a latent source language from the learned deep features. Then, we propose translating the latent language into the target language within a machine translation framework and finally decoding all operations, arranged in order. Besides, a chain inversion strategy and a bi-directional modeling mechanism are developed to improve the detection performance. We further design a weighted cross-entropy loss to alleviate the problems presented by imbalance among chain lengths and chain categories. Our method can detect operation chains containing up to seven operations and obtains very promising results in various scenarios for the detection of both short and long chains.
C1 [Li, Yuanman; You, Jiaxiang; Li, Xia] Shenzhen Univ, Coll Elect & Informat Engn, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
   [You, Jiaxiang] State Key Lab Internet Things Smart City, Macau 999078, Peoples R China.
   [Zhou, Jiantao] Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
   [Wang, Wei] Sun Yat Sen Univ, Sch Intelligent Syst Engn, Shenzhen 518033, Peoples R China.
   [Liao, Xin] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
C3 Shenzhen University; University of Macau; Sun Yat Sen University; Hunan
   University
RP Zhou, JT (corresponding author), Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
EM yuanmanli@szu.edu.cn; 2070436047@email.szu.edu.cn; jtzhou@umac.mo;
   wangw328@mail.sysu.edu.cn; xinliao@hnu.edu.cn; lixia@szu.edu.cn
RI ; Wang, Wei/GYU-4649-2022
OI Li, Xia/0000-0002-8043-9966; Wang, Wei/0000-0002-1717-5785
FU Natural Science Foundation of China [62001304, 61871273, 61972142,
   61971476]; Guangdong Basic and Applied Basic Research Foundation
   [2022A1515010645]; Foundation for Science and Technology Innovation of
   Shenzhen [RCBS20210609103708014]; CCF-Alibaba Innovative Research Fund
   For Young Scholars; Alibaba Group; Macau Science and Technology
   Development Fund [SKLIOTSC-2021-2023, 0072/2020/AMJ, 0022/2022/A1];
   Research Committee at University of Macau [MYRG2020-00101-FST,
   MYRG2022-00152-FST]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 62001304, 61871273, 61972142, and 61971476, in part
   by the Guangdong Basic and Applied Basic Research Foundation under Grant
   2022A1515010645, in part by the Foundation for Science and Technology
   Innovation of Shenzhen under Grant RCBS20210609103708014, in part by
   CCF-Alibaba Innovative Research Fund For Young Scholars; in part by
   Alibaba Group through Alibaba Innovative Research Program, in part by
   Macau Science and Technology Development Fund under Grants
   SKLIOTSC-2021-2023, 0072/2020/AMJ, and 0022/2022/A1, and in part by
   Research Committee at University of Macau under Grants
   MYRG2020-00101-FST and MYRG2022-00152-FST
CR Bayar B, 2018, IEEE T INF FOREN SEC, V13, P2691, DOI 10.1109/TIFS.2018.2825953
   Boroumand M, 2018, Electron. Imag., V30, DOI 10.2352/
   Cao G, 2014, IEEE T INF FOREN SEC, V9, P515, DOI 10.1109/TIFS.2014.2300937
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen BJ, 2021, IEEE T MULTIMEDIA, V23, P3506, DOI 10.1109/TMM.2020.3026868
   Chen CL, 2013, IEEE T IMAGE PROCESS, V22, P4699, DOI 10.1109/TIP.2013.2277814
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen YF, 2020, IEEE J-STSP, V14, P997, DOI 10.1109/JSTSP.2020.2998401
   Chu XY, 2016, IEEE T INF FOREN SEC, V11, P823, DOI 10.1109/TIFS.2015.2510958
   Chung JY, 2014, Arxiv, DOI arXiv:1412.3555
   Comesaña P, 2012, IEEE INT WORKS INFOR, P211, DOI 10.1109/WIFS.2012.6412651
   Dang-Nguyen D.T., 2015, P ACM MULT SYST C, P219
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fan W, 2015, IEEE INT WORKS INFOR
   Feng XY, 2012, IEEE T MULTIMEDIA, V14, P536, DOI 10.1109/TMM.2012.2191946
   Gloe T., 2010, ACM S APPL COMP, P1584, DOI DOI 10.1080/15567281.2010.531500
   Hadwiger Benjamin, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12666), P500, DOI 10.1007/978-3-030-68780-9_40
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Jiang XH, 2018, IEEE T INF FOREN SEC, V13, P170, DOI 10.1109/TIFS.2017.2745687
   Jingwen He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P53, DOI 10.1007/978-3-030-58565-5_4
   Kakar P, 2011, IEEE T MULTIMEDIA, V13, P443, DOI 10.1109/TMM.2011.2121056
   Kang XG, 2013, IEEE T INF FOREN SEC, V8, P1456, DOI 10.1109/TIFS.2013.2273394
   Li HD, 2019, IEEE I CONF COMP VIS, P8300, DOI 10.1109/ICCV.2019.00839
   Li HD, 2018, IEEE T CIRC SYST VID, V28, P31, DOI 10.1109/TCSVT.2016.2599849
   Li PZ, 2021, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR46437.2021.00560
   Li YM, 2022, IEEE T COMPUT SOC SY, V9, P1830, DOI 10.1109/TCSS.2021.3135654
   Li YM, 2022, IEEE T NEUR NET LEAR, V33, P4228, DOI 10.1109/TNNLS.2021.3056188
   Li YM, 2019, IEEE T INF FOREN SEC, V14, P1307, DOI 10.1109/TIFS.2018.2876837
   Li YM, 2017, IEEE T INF FOREN SEC, V12, P2971, DOI 10.1109/TIFS.2017.2730362
   Liao X, 2020, IEEE J-STSP, V14, P955, DOI 10.1109/JSTSP.2020.3002391
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pasquini C, 2019, IEEE T INF FOREN SEC, V14, P1928, DOI 10.1109/TIFS.2018.2889259
   Qiao T, 2019, IEEE T MULTIMEDIA, V21, P1077, DOI 10.1109/TMM.2018.2872863
   Qiu X., 2014, P 2 ACM WORKSH INF H, P165, DOI DOI 10.1145/2600918.2600941
   Stamm MC, 2013, IEEE INT WORKS INFOR, P162, DOI 10.1109/WIFS.2013.6707812
   Su LC, 2018, IEEE T MULTIMEDIA, V20, P825, DOI 10.1109/TMM.2017.2760098
   Sun X, 2017, IEEE INT CON MULTI, P301, DOI 10.1109/ICME.2017.8019361
   Touvron H., 2020, P INT C MACH LEARN, P10347
   Vaswani A, 2017, ADV NEUR IN, V30
   Wu HW, 2022, IEEE T CIRC SYST VID, V32, P1172, DOI 10.1109/TCSVT.2021.3075039
   Wu Y, 2019, PROC CVPR IEEE, P9535, DOI 10.1109/CVPR.2019.00977
   You JX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3510, DOI 10.1145/3474085.3475513
   Zhou L, 2019, T ASSOC COMPUT LING, V7, P91, DOI 10.1162/tacl_a_00256
   Zhu XZ, 2021, Arxiv, DOI arXiv:2010.04159
NR 46
TC 3
Z9 3
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6852
EP 6867
DI 10.1109/TMM.2022.3215000
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000011
DA 2024-07-18
ER

PT J
AU Li, Y
   Liu, Z
   Chang, XJ
   McAuley, J
   Yao, LA
AF Li, Yun
   Liu, Zhe
   Chang, Xiaojun
   McAuley, Julian
   Yao, Lina
TI Diversity-Boosted Generalization-Specialization Balancing for Zero-Shot
   Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Feature extraction; Semantics; Training; Schedules;
   Redundancy; Annealing; Zero-shot learning; meta-learning; dynamic
   network
ID NETWORK
AB Zero-Shot Learning (ZSL) aims to transfer classification capability from seen to unseen classes. Recent methods have proved that generalization and specialization are two essential abilities to achieve good performance in ZSL. However, focusing on only one of the abilities may result in models that are either too general with degraded classification ability or too specialized to generalize to unseen classes. In this article, we propose an end-to-end network, termed as BGSNet, which equips and balances generalization and specialization abilities at the instance and dataset level. Specifically, BGSNet consists of two branches: the Generalization Network (GNet), which applies episodic meta-learning to learn generalized knowledge, and the Balanced Specialization Network (BSNet), which adopts multiple attentive extractors to extract discriminative features and achieve instance-level balance. A novel self-adjusted diversity loss is designed to optimize BSNet with redundancy reduced and diversity boosted. We further propose a differentiable dataset-level balance and update the weights in a linear annealing schedule to simulate network pruning and thus obtain the optimal structure for BSNet with dataset-level balance achieved. Experiments on four benchmark datasets demonstrate our model's effectiveness. Sufficient component ablations prove the necessity of integrating and balancing generalization and specialization abilities.
C1 [Li, Yun; Liu, Zhe] Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW 2052, Australia.
   [Liu, Zhe] Jiangnan Univ, Jiangsu Prov Engn Lab Pat tern Recognit & Computat, Wuxi 214122, Peoples R China.
   [Chang, Xiaojun] Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
   [McAuley, Julian] Univ Calif San Diego, Sch Comp Sci & Engn, La Jolla, CA 92093 USA.
   [Yao, Lina] CSIROs, Data61, Sydney, NSW 2052, Australia.
   [Yao, Lina] Univ New South Wales, Sydney, NSW 2052, Australia.
C3 University of New South Wales Sydney; Jiangnan University; University of
   Technology Sydney; University of California System; University of
   California San Diego; Commonwealth Scientific & Industrial Research
   Organisation (CSIRO); University of New South Wales Sydney
RP Liu, Z (corresponding author), Jiangnan Univ, Jiangsu Prov Engn Lab Pat tern Recognit & Computat, Wuxi 214122, Peoples R China.
EM yun.li5@unsw.edu.au; zheliu912@gmail.com; cxj273@gmail.com;
   jmcauley@eng.ucsd.edu; lina.yao@unsw.edu.au
RI LIU, zhe/HGD-6875-2022; Chang, Xiaojun/A-2055-2015
OI Chang, Xiaojun/0000-0002-7778-8807; Liu, Zhe/0000-0003-2692-2110; Li,
   Yun/0000-0003-4442-3825
CR Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Changpinyo S, 2017, IEEE I CONF COMP VIS, P3496, DOI 10.1109/ICCV.2017.376
   Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4
   Chen L, 2018, PROC CVPR IEEE, P1043, DOI 10.1109/CVPR.2018.00115
   Chen Z, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3413, DOI 10.1145/3394171.3413813
   Chi JZ, 2020, IEEE T CIRC SYST VID, V30, P1173, DOI 10.1109/TCSVT.2019.2900171
   Chou Y.-Y., 2020, P INT C LEARN REPR
   Huynh D, 2020, PROC CVPR IEEE, P4482, DOI 10.1109/CVPR42600.2020.00454
   Demertzis K, 2020, ALGORITHMS, V13, DOI 10.3390/a13030061
   Ding ZM, 2017, PROC CVPR IEEE, P6005, DOI 10.1109/CVPR.2017.636
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Felix R, 2018, LECT NOTES COMPUT SC, V11210, P21, DOI 10.1007/978-3-030-01231-1_2
   Finn C, 2017, PR MACH LEARN RES, V70
   Gao R, 2020, IEEE T IMAGE PROCESS, V29, P3665, DOI 10.1109/TIP.2020.2964429
   Ge JN, 2021, AAAI CONF ARTIF INTE, V35, P1406
   Geng CX, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107263
   Guo JC, 2021, IEEE T MULTIMEDIA, V23, P524, DOI 10.1109/TMM.2020.2984091
   Guo YC, 2017, IEEE T IMAGE PROCESS, V26, P3277, DOI 10.1109/TIP.2017.2696747
   Guo-Sen Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P562, DOI 10.1007/978-3-030-58548-8_33
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu Y, 2022, IEEE T MULTIMEDIA, V24, P2473, DOI 10.1109/TMM.2021.3082292
   Imrattanatrai W, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P195, DOI 10.1145/3331184.3331220
   Ji Z, 2022, IEEE T CYBERNETICS, V52, P6543, DOI 10.1109/TCYB.2020.3004641
   Jiang HJ, 2019, IEEE I CONF COMP VIS, P9764, DOI 10.1109/ICCV.2019.00986
   Li J, 2019, PROC CVPR IEEE, P5458, DOI 10.1109/CVPR.2019.00561
   Li JJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1348, DOI 10.1145/3394171.3413503
   Li Y, 2023, IEEE T MULTIMEDIA, V25, P1600, DOI 10.1109/TMM.2021.3139211
   Li Y, 2022, IEEE T CIRC SYST VID, V32, P5175, DOI 10.1109/TCSVT.2022.3147902
   Liu L, 2021, POSTGRAD MED, V133, P265, DOI 10.1080/00325481.2020.1803666
   Liu L, 2020, AAAI CONF ARTIF INTE, V34, P4868
   Liu SC, 2018, ADV NEUR IN, V31
   Liu Y, 2019, IEEE I CONF COMP VIS, P6697, DOI 10.1109/ICCV.2019.00680
   Liu Z, 2021, AAAI CONF ARTIF INTE, V35, P8723
   Morgado P, 2017, PROC CVPR IEEE, P2037, DOI 10.1109/CVPR.2017.220
   Narayan Sanath, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P479, DOI 10.1007/978-3-030-58542-6_29
   Nooralahzadeh F, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P4547
   Pal A, 2019, PROC CVPR IEEE, P2184, DOI 10.1109/CVPR.2019.00229
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Serra Joan, 2018, International Conference on Machine Learning, P4548
   Shen JY, 2022, IEEE T CIRC SYST VID, V32, P634, DOI 10.1109/TCSVT.2021.3067067
   Shigeto Y, 2015, LECT NOTES ARTIF INT, V9284, P135, DOI 10.1007/978-3-319-23528-8_9
   Soh JW, 2020, PROC CVPR IEEE, P3513, DOI 10.1109/CVPR42600.2020.00357
   Sylvain T., 2019, P INT C LEARN REPR
   Verma VK, 2020, AAAI CONF ARTIF INTE, V34, P6062
   Verma VK, 2017, LECT NOTES ARTIF INT, V10535, P792, DOI 10.1007/978-3-319-71246-8_48
   Wang DQ, 2015, IEEE I CONF COMP VIS, P2399, DOI 10.1109/ICCV.2015.276
   Wang WL, 2018, AAAI CONF ARTIF INTE, P4211
   Wang X, 2019, PROC CVPR IEEE, P1831, DOI 10.1109/CVPR.2019.00193
   Welinder P., 2010, Technical Report CNS-TR-2010-001
   Xian YQ, 2019, PROC CVPR IEEE, P10267, DOI 10.1109/CVPR.2019.01052
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Xie GS, 2019, PROC CVPR IEEE, P9376, DOI 10.1109/CVPR.2019.00961
   Xu W., 2020, NEURIPS, V33, P21969
   Yan SP, 2021, PROC CVPR IEEE, P3013, DOI 10.1109/CVPR46437.2021.00303
   Yang SQ, 2021, Arxiv, DOI arXiv:2006.05938
   Ye M, 2019, PROC CVPR IEEE, P11720, DOI 10.1109/CVPR.2019.01200
   Ye YL, 2023, IEEE T MULTIMEDIA, V25, P2252, DOI 10.1109/TMM.2022.3145237
   Ye YL, 2022, IEEE T MULTIMEDIA, V24, P1325, DOI 10.1109/TMM.2021.3063616
   Ye ZH, 2022, IEEE T MULTIMEDIA, V24, P2828, DOI 10.1109/TMM.2021.3089017
   Zhang HF, 2021, NEURAL NETWORKS, V134, P11, DOI 10.1016/j.neunet.2020.11.007
   Zhang HF, 2019, IEEE T IMAGE PROCESS, V28, P506, DOI 10.1109/TIP.2018.2869696
   Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321
   Zhang XP, 2016, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR.2016.128
   Zhu PK, 2019, PROC CVPR IEEE, P2990, DOI 10.1109/CVPR.2019.00311
   Zhu Y., 2019, Adv. Neural Inf. Process. Syst, V32, P14943
NR 67
TC 1
Z9 1
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8372
EP 8382
DI 10.1109/TMM.2023.3236211
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000028
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, ZH
   Shang, YY
   Li, TM
   Chen, GL
   Wang, Y
   Hu, QH
   Zhu, PF
AF Liu, Zhihao
   Shang, Yuanyuan
   Li, Timing
   Chen, Guanlin
   Wang, Yu
   Hu, Qinghua
   Zhu, Pengfei
TI Robust Multi-Drone Multi-Target Tracking to Resolve Target Occlusion: A
   Benchmark
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target tracking; Drones; Task analysis; Cameras; Object detection;
   Object tracking; Transformers; Drone; target occlusion; multi-drone
   tracking; multi-target tracking; identity association
AB Multi-drone multi-target tracking aims at collabo- ratively detecting and tracking targets across multiple drones and associating the identities of objects from different drones, which can overcome the shortcomings of single-drone object tracking. To address the critical challenges of identity association and target occlusion in multi-drone multi-target tracking tasks, we collect an occlusion-aware multi-drone multi-target tracking dataset named MDMT. It contains 88 video sequences with 39,678 frames, including 11,454 different IDs of persons, bicycles, and cars. The MDMT dataset comprises 2,204,620 bounding boxes, of which 543,444 bounding boxes contain target occlusions. We also design a multi-device target association score (MDA) as the evaluation criteria for the ability of cross-view target association in multi-device tracking. Furthermore, we propose a Multi-matching Identity Authentication network (MIA-Net) for the multi-drone multi-target tracking task. The local-global matching algorithm in MIA-Net discovers the topological relationship of targets across drones, efficiently solves the problem of cross-drone association, and effectively complements occluded targets with the advantage of multiple drone view mapping. Extensive experiments on the MDMT dataset validate the effectiveness of our proposed MIA-Net for the task of identity association and multi-object tracking with occlusions.
C1 [Liu, Zhihao; Shang, Yuanyuan; Li, Timing; Chen, Guanlin; Wang, Yu; Hu, Qinghua; Zhu, Pengfei] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
   [Liu, Zhihao; Shang, Yuanyuan; Li, Timing; Chen, Guanlin; Wang, Yu; Hu, Qinghua; Zhu, Pengfei] Haihe Lab Informat Technol Applicat Innovat, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
C3 Tianjin University
RP Li, TM; Zhu, PF (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
EM lzhihao@tju.edu.cn; shangyuanyuan@tju.edu.cn; timingli@tju.edu.cn;
   chenguanlin@tju.edu.cn; armstrong_wangyu@tju.edu.cn;
   huqinghua@tju.edu.cn; zhupengfei@tju.edu.cn
RI Hu, Qinghua/B-8857-2008; Li, Timing/IXD-9493-2023
OI Li, Timing/0000-0003-4928-895X; Hu, Qinghua/0000-0001-7765-8095; Wang,
   Yu/0000-0002-4788-8655
FU National Natural Science Foundation of China [61876127, 61925602,
   62222608, 62106174]; China Postdoctoral Science Foundation [2021TQ0242,
   2021M690118]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61876127, 61925602 62222608, and
   62106174, and in part by the China Postdoctoral Science Foundation under
   Grants 2021TQ0242 and 2021M690118. The guest editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   cairong zhao.
CR Aharon N, 2022, Arxiv, DOI [arXiv:2206.14651, DOI 10.48550/ARXIV.2206.14651]
   Al-Shakarji NM., 2018, International Conference on Advanced Video and Signal Based Surveillance (AVSS), P1
   Bao Q, 2021, IEEE T MULTIMEDIA, V23, P161, DOI 10.1109/TMM.2020.2980194
   Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Beyer L., 2017, P IEEE COMP VIS PATT, P29
   Bredereck M, 2012, 2012 SIXTH INTERNATIONAL CONFERENCE ON DISTRIBUTED SMART CAMERAS (ICDSC)
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen GL, 2021, IEEE INT CONF COMP V, P2839, DOI 10.1109/ICCVW54120.2021.00318
   Chen WH, 2017, IEEE T CIRC SYST VID, V27, P2367, DOI 10.1109/TCSVT.2016.2589619
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
   Dendorfer P., 2020, arXiv
   Dendorfer P, 2021, INT J COMPUT VISION, V129, P845, DOI 10.1007/s11263-020-01393-0
   Dosovitskiy Alexey, 2021, 2021 INT C LEARN REP
   Du DW, 2018, LECT NOTES COMPUT SC, V11214, P375, DOI 10.1007/978-3-030-01249-6_23
   Du Y, 2023, IEEE Transactions on Multimedia
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Feng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3490, DOI 10.1109/ICCV48922.2021.00349
   Ferris J, 2009, PALG STUD THEAT PERF, P1
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gao TZ, 2022, IEEE T MULTIMEDIA, V24, P995, DOI 10.1109/TMM.2021.3062489
   Gao X, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P201, DOI 10.1145/3240508.3240548
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   He LX, 2020, Arxiv, DOI arXiv:2006.02631
   He YH, 2020, IEEE T IMAGE PROCESS, V29, P5191, DOI 10.1109/TIP.2020.2980070
   Hou XY, 2019, 2019 16TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), DOI 10.1109/avss.2019.8909903
   Hsieh MR, 2017, IEEE I CONF COMP VIS, P4165, DOI 10.1109/ICCV.2017.446
   Jiang BR, 2018, LECT NOTES COMPUT SC, V11218, P816, DOI 10.1007/978-3-030-01264-9_48
   Kuo CH, 2010, LECT NOTES COMPUT SC, V6311, P383
   Leal-Taix‚ L, 2015, Arxiv, DOI arXiv:1504.01942
   Li Liunian Harold, 2022, P IEEE CVF C COMP VI, P10965
   Li W, 2022, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), P3187
   Li X., 2020, P EUR C COMP VIS, DOI DOI 10.1007/978-3-030-58577-8_8
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Milan A, 2016, Arxiv, DOI arXiv:1603.00831
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Pang JM, 2021, PROC CVPR IEEE, P164, DOI 10.1109/CVPR46437.2021.00023
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren S., 2015, ADV NEURAL INFORM PR, V28, P91
   Ristani E, 2018, PROC CVPR IEEE, P6036, DOI 10.1109/CVPR.2018.00632
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Ruder S, 2017, Arxiv, DOI arXiv:1609.04747
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Stadler D, 2021, PROC CVPR IEEE, P10953, DOI 10.1109/CVPR46437.2021.01081
   Sun YM, 2022, IEEE T CIRC SYST VID, V32, P6700, DOI 10.1109/TCSVT.2022.3168279
   Tang Z, 2019, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2019.00900
   Vaswani A, 2017, ADV NEUR IN, V30
   Vondrick C, 2013, INT J COMPUT VISION, V101, P184, DOI 10.1007/s11263-012-0564-1
   Vu T., 2019, NEURIPS, P1430
   Wang JQ, 2019, IEEE I CONF COMP VIS, P3007, DOI 10.1109/ICCV.2019.00310
   Wen LY, 2021, PROC CVPR IEEE, P7808, DOI 10.1109/CVPR46437.2021.00772
   Wojke N, 2018, IEEE WINT CONF APPL, P748, DOI 10.1109/WACV.2018.00087
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Xingyi Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P474, DOI 10.1007/978-3-030-58548-8_28
   Xu YL, 2016, PROC CVPR IEEE, P4256, DOI 10.1109/CVPR.2016.461
   Zeng Y, 2022, 39 INT C MACHINE LEA
   Zhang YF, 2022, LECT NOTES COMPUT SC, V13682, P1, DOI 10.1007/978-3-031-20047-2_1
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhou QQ, 2019, IEEE T MULTIMEDIA, V21, P1183, DOI 10.1109/TMM.2018.2875360
   Zhu BJ, 2020, Arxiv, DOI arXiv:2007.03496
   Zhu PF, 2022, IEEE T PATTERN ANAL, V44, P7380, DOI 10.1109/TPAMI.2021.3119563
   Zhu PF, 2021, IEEE T CIRC SYST VID, V31, P4058, DOI 10.1109/TCSVT.2020.3045747
NR 68
TC 13
Z9 14
U1 32
U2 67
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1462
EP 1476
DI 10.1109/TMM.2023.3234822
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA G2GW2
UT WOS:000987415000006
DA 2024-07-18
ER

PT J
AU Ma, QT
   Wang, Y
   Zeng, TY
AF Ma, Qianting
   Wang, Yang
   Zeng, Tieyong
TI Retinex-Based Variational Framework for Low-Light Image Enhancement and
   Denoising
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Lighting; Image enhancement; Adaptation models; Noise reduction;
   Learning systems; Visualization; Task analysis; low-light; retinex;
   variational model
ID NETWORK; ALGORITHM; MODEL
AB Low-light image enhancement is an important task in the domain of computer vision. Images taken under insufficient lighting conditions manifest low visibility and unknown noises which disrupt image contents and pose considerable challenges for low-light image enhancement. Most of Retinex-based methods usually attempt to design different priors on the gradient of both illumination and reflectance. However, noises can be involved in the Retinex-based models. To address the problem, we explore the problem of low-light image restoration through joint contrast enhancement and denoising. We propose a Retinex-based variational model for low-light image enhancement that effectively generates a noise-free image, yet proves to generalize well to diverse light-conditions. First, we present a simple constraint on the fidelity term between the fractional derivative of an observed image and the fractional derivative of the recomposed one which is the product of the reflectance and illumination. This strategy aims to model spatial consistency to preserve natural variation. Second, we introduce a weighted regularization term for the reflectance that can remove noise with a adaptive texture map. We evaluate our proposed approach using three challenging datasets: NPE, LOL and GladNet. Extensive experiments demonstrate that our proposed method outperforms other competing methods in terms of visual quality and quantitative comparisons.
C1 [Ma, Qianting] Nanjing Univ Informat Sci & Technol, Sch Math & Stat, Nanjing 210044, Peoples R China.
   [Wang, Yang] Hong Kong Univ Sci & Technol, Dept Math, Kowloon, Hong Kong, Peoples R China.
   [Zeng, Tieyong] Chinese Univ Hong Kong, Dept Math, Shatin, Hong Kong, Peoples R China.
C3 Nanjing University of Information Science & Technology; Hong Kong
   University of Science & Technology; Chinese University of Hong Kong
RP Ma, QT (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Math & Stat, Nanjing 210044, Peoples R China.
EM qtma@nuist.edu.cn; yang-wang@ust.hk; zeng@math.cuhk.edu.hk
RI Zeng, Tieyong/B-7147-2009; Ma, Qianting/HKF-6159-2023
OI Ma, Qianting/0000-0001-5370-0252; ZENG, Tieyong/0000-0002-0688-202X
FU National Key R&D Program of China [2021YFE0203700]; National Natural
   Science Foundation of China [61902192,NSFC/RGC N_CUHK 415/19, ITF
   MHP/038/20, CRF 8730063, RGC14300219, 14302920, 14301121]; Chinese
   University of Hong Kong Direct Grant for Research; High-Level Innovative
   and entrepreneurial Project in Jiangsu Province, China; Jiangsu
   Personnel Office Document [[2019]20]; Startup Foundation for Introducing
   Talent of NUIST [1131111901015]
FX This work was supported in part by the National Key R & D Program of
   China under Grant 2021YFE0203700, in part by the National Natural
   Science Foundation of China under Grants 61902192,NSFC/RGC N_CUHK
   415/19, ITF MHP/038/20, CRF 8730063, and RGC14300219, 14302920, and
   14301121, in part by the Chinese University of Hong Kong Direct Grant
   for Research, the High-Level Innovative and entrepreneurial Project in
   Jiangsu Province, China, Jiangsu Personnel Office Document, No.[2019]20,
   and in part by the Startup Foundation for Introducing Talent of NUIST
   under Grant 1131111901015.
CR Chang M, 2022, IEEE T MULTIMEDIA, V24, P702, DOI 10.1109/TMM.2021.3058586
   Chien CC, 2019, PROC SPIE, V11049, DOI 10.1117/12.2521585
   Dai Q, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11040574
   Dhara SK, 2022, IEEE T CIRC SYST VID, V32, P3438, DOI 10.1109/TCSVT.2021.3113559
   Dong FF, 2019, COMPUT MATH APPL, V78, P1960, DOI 10.1016/j.camwa.2019.03.033
   Dong FF, 2018, 2018 IEEE 3RD INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING (ICSIP), P66, DOI 10.1109/SIPROCESS.2018.8600500
   Dong FF, 2016, INVERSE PROBL IMAG, V10, P27, DOI 10.3934/ipi.2016.10.27
   Dong JX, 2022, IEEE T PATTERN ANAL, V44, P8355, DOI 10.1109/TPAMI.2021.3102575
   Fan MH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2317, DOI 10.1145/3394171.3413757
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Gu K, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2439035
   Gu ZH, 2020, IEEE T IMAGE PROCESS, V29, P3239, DOI 10.1109/TIP.2019.2958144
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Jin Z, 2020, IEEE T MULTIMEDIA, V22, P1055, DOI 10.1109/TMM.2019.2938340
   Kurihara Kazuki, 2019, 2019 15th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS), P74, DOI 10.1109/SITIS.2019.00024
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lee H, 2020, IEEE SIGNAL PROC LET, V27, P251, DOI 10.1109/LSP.2020.2965824
   Li CY, 2018, PATTERN RECOGN LETT, V104, P15, DOI 10.1016/j.patrec.2018.01.010
   Li JQ, 2021, IEEE T MULTIMEDIA, V23, P3153, DOI 10.1109/TMM.2020.3021243
   Li JJ, 2021, IEEE T CIRC SYST VID, V31, P4227, DOI 10.1109/TCSVT.2021.3049940
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Liang LM, 2021, IEEE COMPUT SOC CONF, P836, DOI 10.1109/CVPRW53098.2021.00093
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Liu RS, 2019, IEEE T IMAGE PROCESS, V28, P1528, DOI 10.1109/TIP.2018.2875568
   Ma L, 2022, IEEE T NEUR NET LEAR, V33, P5666, DOI 10.1109/TNNLS.2021.3071245
   Ma QT, 2020, J COMPUT MATH, V38, P417, DOI 10.4208/jcm.1811-m2016-0763
   Ma QT, 2017, MACH VISION APPL, V28, P635, DOI 10.1007/s00138-017-0857-z
   Pan ZQ, 2022, IEEE T MULTIMEDIA, V24, P519, DOI 10.1109/TMM.2021.3054509
   Park G, 2017, INT CONF BIG DATA, P76, DOI 10.1109/BIGCOMP.2017.7881719
   Park S, 2017, IEEE T CONSUM ELECTR, V63, P178, DOI 10.1109/TCE.2017.014847
   Pu YF, 2010, IEEE T IMAGE PROCESS, V19, P491, DOI 10.1109/TIP.2009.2035980
   Ren XT, 2020, IEEE T IMAGE PROCESS, V29, P5862, DOI 10.1109/TIP.2020.2984098
   Ren XT, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351427
   Ren YR, 2019, IEEE T CIRC SYST VID, V29, P968, DOI 10.1109/TCSVT.2018.2828141
   Su HN, 2022, IEEE T MULTIMEDIA, V24, P17, DOI 10.1109/TMM.2020.3043106
   Su HA, 2017, INT CONF ACOUST SPEE, P1977, DOI 10.1109/ICASSP.2017.7952502
   Tang CY, 2019, IET IMAGE PROCESS, V13, P537, DOI 10.1049/iet-ipr.2018.5505
   Wang LW, 2020, IEEE T IMAGE PROCESS, V29, P7984, DOI 10.1109/TIP.2020.3008396
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang WJ, 2021, IEEE T IND INFORM, V17, P5208, DOI 10.1109/TII.2020.3026036
   Wang WJ, 2018, IEEE INT CONF AUTOMA, P751, DOI 10.1109/FG.2018.00118
   Wang YF, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2922106
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei C, 2018, Arxiv, DOI arXiv:1808.04560
   Wu YH, 2019, IET IMAGE PROCESS, V13, P2448, DOI 10.1049/iet-ipr.2018.6208
   Xu HT, 2014, IEEE T MULTIMEDIA, V16, P68, DOI 10.1109/TMM.2013.2283453
   Xu J, 2020, IEEE T IMAGE PROCESS, V29, P5022, DOI 10.1109/TIP.2020.2974060
   Xue ML, 2021, IEEE T MULTIMEDIA, V23, P2706, DOI 10.1109/TMM.2020.3015037
   Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P2072, DOI 10.1109/TIP.2021.3050850
   Yin JL, 2021, IEEE T MULTIMEDIA, V23, P1049, DOI 10.1109/TMM.2020.2992962
   Yu Guo, 2020, ICMLC 2020: Proceedings of the 2020 12th International Conference on Machine Learning and Computing, P406, DOI 10.1145/3383972.3384022
   Yu SY, 2019, IEEE T CIRC SYST VID, V29, P28, DOI 10.1109/TCSVT.2017.2763180
   Zhang S, 2017, NEUROCOMPUTING, V245, P1, DOI 10.1016/j.neucom.2017.03.029
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zheng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4419, DOI 10.1109/ICCV48922.2021.00440
NR 59
TC 13
Z9 16
U1 9
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5580
EP 5588
DI 10.1109/TMM.2022.3194993
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300068
DA 2024-07-18
ER

PT J
AU Thakur, S
   Jakhetiya, V
   Subudhi, BN
   Jaiswal, SP
   Li, L
   Lin, WS
AF Thakur, Sadbhawna
   Jakhetiya, Vinit
   Subudhi, Badri N.
   Jaiswal, Sunil P.
   Li, Leida
   Lin, Weisi
TI Context Region Identification Based Quality Assessment of 3D Synthesized
   Views
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Depth; energy maps; context region; foreground; disoccluded region; 3D
   synthesized views
ID IMAGES
AB Perceptual quality assessment of 3D synthesized views is an open research problem in computer vision. Researchers across the globe have developed several algorithms to identify distortions. At the same time, the existing algorithms cannot quantify the context in which these distortions affect the overall perceptual quality. According to the recently proposed 3D view synthesis algorithm, the choice of context region for the disocclusion plays a vital role in predicting the quality of 3D views. The context region taken from the background of a view produces a perceptually better quality of 3D synthesized views than when the context region is taken from the foreground. With this view, the proposed algorithm aims to identify the context region and incorporate this information for the perceptual quality assessment of 3D synthesized views. We observed that the depth energy maps of the 3D synthesized views vary significantly with the change in the context region and subsequently can identify the context region. Hence, in this work, we propose a new and efficient quality assessment algorithm based upon the variation in the depth of 3D synthesized and reference views, giving two-fold advantages: 1. It can predict the quality based on whether the context region is foreground or not. 2. It is also able to suggest the possible location of distortions. We have proposed two new algorithms for both situations when the context region is foreground or not. The overall predicted score is the direct multiplication of the quality score estimated when the context region is foreground or not. When applied to the established benchmark dataset, the proposed technique performs satisfactorily with the PLCC of 0.7707 and 0.7572 of SRCC. Also, the proposed algorithm can work as a plug-in to improve the performance of the existing algorithms.
C1 [Thakur, Sadbhawna; Jakhetiya, Vinit] Indian Inst Technol Jammu, Dept Comp Sci & Engn, Jammu 181221, India.
   [Subudhi, Badri N.] Indian Inst Technol Jammu, Dept Elect Engn, Jammu 181221, India.
   [Jaiswal, Sunil P.] K Lens, D-76131 Karlsruhe, Germany.
   [Li, Leida] Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
   [Lin, Weisi] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) Jammu; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) Jammu; Xidian University;
   Nanyang Technological University
RP Jakhetiya, V (corresponding author), Indian Inst Technol Jammu, Dept Comp Sci & Engn, Jammu 181221, India.
EM sadbhawnathakur@gmail.com; vinit.jakhetiya@iitjammu.ac.in;
   subudhi.badri@gmail.com; spjaiswal@connect.ust.hk;
   reader1104@hotmail.com; wslin@ntu.edu.sg
RI Lin, Wei/D-3353-2012; Lin, Weisi/A-3696-2011
OI Lin, Weisi/0000-0001-9866-1947; Subudhi, Badri
   Narayan/0000-0002-4378-0065
FU SRG/2020/001871
FX No Statement Available
CR AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784
   Ahn I, 2013, IEEE T BROADCAST, V59, P614, DOI 10.1109/TBC.2013.2281658
   Andrew A. M., 1992, Robotica, V10, P278
   [Anonymous], 2015, Int. J. Adv. Robotic Syst., V12, P12
   Bosc E, 2011, IEEE J-STSP, V5, P1332, DOI 10.1109/JSTSP.2011.2166245
   Brandao T, 2008, SIGNAL PROCESS, V88, P822, DOI 10.1016/j.sigpro.2007.09.017
   Cheon M, 2021, IEEE COMPUT SOC CONF, P433, DOI 10.1109/CVPRW53098.2021.00054
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   de Oliveira AQ, 2018, IEEE SIGNAL PROC LET, V25, P1705, DOI 10.1109/LSP.2018.2870342
   Fehn C, 2003, PROC 3 IASTED C VISU
   Gu K, 2018, IEEE T IMAGE PROCESS, V27, P394, DOI 10.1109/TIP.2017.2733164
   Jakhetiya V, 2021, IEEE T IND ELECTRON, V68, P423, DOI 10.1109/TIE.2020.2965469
   Jakhetiya V, 2019, IEEE T IND INFORM, V15, P4120, DOI 10.1109/TII.2018.2888861
   Jakhetiya V, 2018, NEUROCOMPUTING, V275, P366, DOI 10.1016/j.neucom.2017.08.031
   Ji C., 2021, Entropy
   Jung YJ, 2016, IEEE T CIRC SYST VID, V26, P1201, DOI 10.1109/TCSVT.2015.2430632
   Lao SS, 2022, IEEE COMPUT SOC CONF, P1139, DOI 10.1109/CVPRW56347.2022.00123
   Li L., 2019, CVPR WORKSHOPS, P17
   Li LD, 2021, IEEE T CIRC SYST VID, V31, P2509, DOI 10.1109/TCSVT.2020.3024882
   Li LD, 2021, IEEE T MULTIMEDIA, V23, P320, DOI 10.1109/TMM.2020.2980185
   Li LD, 2018, IEEE T MULTIMEDIA, V20, P914, DOI 10.1109/TMM.2017.2760062
   Li ZQ, 2018, PROC CVPR IEEE, P2041, DOI 10.1109/CVPR.2018.00218
   Ling SY, 2021, IEEE T MULTIMEDIA, V23, P4245, DOI 10.1109/TMM.2020.3038305
   Liu XK, 2015, IEEE T IMAGE PROCESS, V24, P4847, DOI 10.1109/TIP.2015.2469140
   Luo GB, 2016, PROC CVPR IEEE, P1781, DOI 10.1109/CVPR.2016.197
   Mahmoudpour S, 2020, IEEE SIGNAL PROC LET, V27, P1650, DOI 10.1109/LSP.2020.3024109
   Meng-Li Shih, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8025, DOI 10.1109/CVPR42600.2020.00805
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Saad M. A., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3093, DOI 10.1109/ICIP.2011.6116319
   Sadbhawna, 2022, IEEE T IMAGE PROCESS, V31, P2027, DOI 10.1109/TIP.2022.3147981
   Sadbhawna, 2022, IEEE T IMAGE PROCESS, V31, P1737, DOI 10.1109/TIP.2022.3145997
   Sadbhawna, 2020, IEEE INT WORKSH MULT
   Sandic-Stankovic D, 2015, INT WORK QUAL MULTIM
   Sandic-Stankovic D, 2016, J ELECTR ENG-SLOVAK, V67, P3, DOI 10.1515/jee-2016-0001
   Shao F, 2018, IEEE T MULTIMEDIA, V20, P659, DOI 10.1109/TMM.2017.2748460
   Solh M, 2012, IEEE J-STSP, V6, P495, DOI 10.1109/JSTSP.2012.2204723
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Tanimoto T., 2013, ISO/IECJTC1/SC29/WG11 MPEG 20081, Doc. M15377
   Tian S., 2018, P IEEE VIS COMM IM P, P1
   Tian SS, 2019, IEEE T MULTIMEDIA, V21, P1235, DOI 10.1109/TMM.2018.2875307
   Tian SS, 2018, IEEE T IMAGE PROCESS, V27, P1652, DOI 10.1109/TIP.2017.2781420
   Wang GC, 2022, IEEE T CIRC SYST VID, V32, P1119, DOI 10.1109/TCSVT.2021.3074181
   Wang GC, 2020, IEEE T IMAGE PROCESS, V29, P1802, DOI 10.1109/TIP.2019.2945675
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Yan JB, 2020, IEEE T IMAGE PROCESS, V29, P7443, DOI 10.1109/TIP.2020.3003218
   Yoon SS, 2014, IEEE IMAGE PROC, P2883, DOI 10.1109/ICIP.2014.7025583
   Yue GH, 2019, IEEE T IMAGE PROCESS, V28, P2075, DOI 10.1109/TIP.2018.2875913
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu C, 2016, IEEE T BROADCAST, V62, P82, DOI 10.1109/TBC.2015.2475697
   Zhu R, 2018, IEEE SIGNAL PROC MAG, V35, P133, DOI 10.1109/MSP.2018.2829209
NR 53
TC 1
Z9 1
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6183
EP 6193
DI 10.1109/TMM.2022.3206660
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP5R1
UT WOS:001133278300017
DA 2024-07-18
ER

PT J
AU Wang, L
   Zhou, M
   Niu, ZX
   Zhang, QL
   Zheng, NN
AF Wang, Le
   Zhou, Mo
   Niu, Zhenxing
   Zhang, Qilin
   Zheng, Nanning
TI Adaptive Ladder Loss for Learning Coherent Visual-Semantic Embedding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Visualization; Semantics; Training; Loss measurement; User
   experience; Extraterrestrial measurements; Coherent visual-semantic
   embedding; adaptive ladder loss; hard-contrastive sampling; coherent
   score
AB For visual-semantic embedding, the existing methods normally treat the relevance between queries and candidates in a bipolar way - relevant or irrelevant, and all "irrelevant" candidates are uniformly pushed away from the query by an equal margin in the embedding space, regardless of their various proximity to the query. This practice disregards relatively discriminative information and could lead to suboptimal ranking in the retrieval results and poorer user experience, especially in the long-tail query scenario where a matching candidate may not necessarily exist. In this paper, we introduce a continuous variable to model the relevance degree between queries and multiple candidates, and propose to learn a coherent embedding space, where candidates with higher relevance degrees are mapped closer to the query than those with lower relevance degrees. In particular, the new ladder loss is proposed by extending the triplet loss inequality to a more general inequality chain, which implements variable push-away margins according to respective relevance degrees. To adapt to the varying mini-batch statistics and improve the efficiency of the ladder loss, we also propose a Silhouette score-based method to adaptively decide the ladder level and hence the underlying inequality chain. In addition, a proper Coherent Score metric is proposed to better measure the ranking results including those "irrelevant" candidates. Extensive experiments on multiple datasets validate the efficacy of our proposed method, which achieves significant improvement over existing state-of-the-art methods.
C1 [Wang, Le; Zhou, Mo; Zheng, Nanning] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.
   [Niu, Zhenxing] Alibaba Grp, Hangzhou 311121, Zhejiang, Peoples R China.
   [Zhang, Qilin] ABB Corp Res Ctr, Raleigh, NC 27606 USA.
C3 Xi'an Jiaotong University; Alibaba Group; ABB
RP Wang, L (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.
EM lewang@mail.xjtu.edu.cn; cdluminate@gmail.com; zhenxingniu@gmail.com;
   samqzhang@gmail.com; nnzheng@mail.xjtu.edu.cn
RI Zhang, Qilin/B-5171-2018
OI Zhang, Qilin/0000-0002-7917-9749; Zhou, Mo/0000-0003-3813-4875; Wang,
   Le/0000-0001-6636-6396
FU National Key R and D Program of China [2018AAA0101400]; NSFC [62088102,
   61773312, 61976171]; Fundamental Research Funds for the Central
   Universities [XTR042021005]
FX This work was supported in part by the National Key R and D Program of
   China under Grant 2018AAA0101400,in part by NSFC under Grants 62088102,
   61773312, and 61976171, and inpart by the Fundamental Research Funds for
   the Central Universities under Grant XTR042021005. The Associate Editor
   coordinating the review ofthis manuscript and approving it for
   publication was Dr. Vasileios Mezaris
CR Andrew G., 2013, ICML, P1247
   Cer Daniel M., 2017, P 11 INT WORKSHOP SE, P1
   Chen XL, 2015, Arxiv, DOI arXiv:1504.00325
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Downey Doug, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P847, DOI 10.1145/1277741.1277939
   Faghri F., 2018, PROC BRIT MACH VIS C
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Gong YC, 2014, LECT NOTES COMPUT SC, V8692, P529, DOI 10.1007/978-3-319-10593-2_35
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong Xuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P126, DOI 10.1007/978-3-030-58568-6_8
   Huang C., 2016, ADV NEURAL INFORM PR, P1262
   Ionescu B, 2021, IEEE T MULTIMEDIA, V23, P677, DOI 10.1109/TMM.2020.2986579
   Ji X, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1654, DOI 10.1145/3123266.3123429
   Jun HJ, 2020, Arxiv, DOI arXiv:1903.10663
   Karpathy A, 2014, ADV NEUR IN, V27
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kendall MG, 1945, BIOMETRIKA, V33, P239, DOI 10.2307/2332303
   Kiros R, 2014, Arxiv, DOI arXiv:1411.2539
   Klein B, 2015, Arxiv, DOI arXiv:1411.7399
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li BH, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P9119
   Lin QB, 2021, IEEE T MULTIMEDIA, V23, P550, DOI 10.1109/TMM.2020.2984081
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu XD, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4487
   Ma XH, 2020, IEEE T MULTIMEDIA, V22, P3101, DOI 10.1109/TMM.2020.2969792
   Malinowski M, 2015, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2015.9
   Min WQ, 2020, IEEE T MULTIMEDIA, V22, P3128, DOI 10.1109/TMM.2020.2974326
   Mo Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P781, DOI 10.1007/978-3-030-58568-6_46
   Niu ZX, 2017, IEEE I CONF COMP VIS, P1899, DOI 10.1109/ICCV.2017.208
   Paszke A., 2017, NIPS W
   Peters M, 2018, STUD LATEINAMERIKA, V32, P1, DOI 10.5771/9783845286846
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Reimers N, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3982
   Roth Karsten, 2020, ICML
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Simo-Serra E, 2015, IEEE I CONF COMP VIS, P118, DOI 10.1109/ICCV.2015.22
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Socher R., 2014, Trans Assoc Comput Linguist, V2, P207, DOI [DOI 10.1162/TACLA00177, 10.1162/tacl_a_00177, DOI 10.1162/TACL_A_00177]
   Sohn K, 2016, ADV NEUR IN, V29
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Ustinova E, 2016, ADV NEUR IN, V29
   Wang Alex, 2018, ABS180407461 CORR, DOI DOI 10.18653/V1/W18-5446
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang XS, 2019, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR.2019.00535
   Wang X, 2019, PROC CVPR IEEE, P5017, DOI 10.1109/CVPR.2019.00516
   Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yuan YH, 2017, IEEE I CONF COMP VIS, P814, DOI 10.1109/ICCV.2017.94
   Zhao CR, 2020, IEEE T MULTIMEDIA, V22, P3180, DOI 10.1109/TMM.2020.2972125
   Zhou M, 2020, AAAI CONF ARTIF INTE, V34, P13050
NR 55
TC 1
Z9 1
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1133
EP 1147
DI 10.1109/TMM.2021.3139210
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100009
DA 2024-07-18
ER

PT J
AU Wang, Z
   Xu, X
   Wang, GQ
   Yang, Y
   Shen, HT
AF Wang, Zheng
   Xu, Xing
   Wang, Guoqing
   Yang, Yang
   Shen, Heng Tao
TI Quaternion Relation Embedding for Scene Graph Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Scene graph generation; interaction modeling; quaternion space; hamilton
   product; visual relation detection
AB As an important visual understanding task, scene graph generation has been drawing widespread attention and could boost a broad range of downstream vision applications. Traditional scene graph generation methods based on different context refinements are trained with probabilistic chain rule, which treats objects and relationships as independent entities. Despite their surprisingly great progress, such a plain formulation unconsciously ignores the latent geometric structure of entities and relationships. To address this issue, we move beyond the traditional real-valued representations and use Quaternion Relation Embedding (QuatRE) to generate scene graphs with more expressive hypercomplex representations. More specifically, we introduce the concept of quaternion representations, hyper-complex valued with three imaginary components for objects entities, then formulate the relation triplets with Hamilton product. Benefiting from explicitly modeling the latent inter-dependencies among all imaginary components and strong expressive capacity, our proposed QuatRE method could better capture the interactions between entities. More importantly, our novel QuatRE method can be treated as a plug-in and well generalized into other methods for performance improvement as it involves no additional layers. Finally, extensive comparisons of our proposed method against the state-of-the-art methods on two large-scale and widely-used datasets, i.e. Visual Genome and Open Images, demonstrated our superiority and generalization capability on various metrics for biased or unbiased inference.
C1 [Wang, Zheng; Xu, Xing; Wang, Guoqing; Yang, Yang; Shen, Heng Tao] Univ Elect Sci & Technol China, Ctr Future Multimedia, Chengdu 611731, Peoples R China.
   [Wang, Zheng; Xu, Xing; Wang, Guoqing; Yang, Yang; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Wang, Zheng] UESTC, Inst Elect & Informat Engn, Chengdu 523808, Guangdong, Peoples R China.
   [Shen, Heng Tao] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China; University of Electronic
   Science & Technology of China; Peng Cheng Laboratory
RP Yang, Y (corresponding author), Univ Elect Sci & Technol China, Ctr Future Multimedia, Chengdu 611731, Peoples R China.; Yang, Y (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM zh_wang@hotmail.com; xing.xu@uestc.edu.cn; gqwang0420@hotmail.com;
   dlyyang@gmail.com; shenhengtao@hotmail.com
RI Shen, Heng Tao/ABD-5331-2021
FU Sichuan Science and Technology Program
FX No Statement Available
CR Arjovsky M, 2016, PR MACH LEARN RES, V48
   Bin Y, 2019, AAAI CONF ARTIF INTE, P8110
   Bordes A., 2013, P 26 INT C NEURAL IN, P2787
   Chen TS, 2019, PROC CVPR IEEE, P6156, DOI 10.1109/CVPR.2019.00632
   Dong T, 2020, IEEE T NEUR NET LEAR, V31, P4999, DOI 10.1109/TNNLS.2019.2955165
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guan X, 2023, IEEE T NEUR NET LEAR, V34, P5112, DOI 10.1109/TNNLS.2021.3126046
   Guo ZC, 2023, IEEE T MULTIMEDIA, V25, P38, DOI 10.1109/TMM.2021.3120544
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hudson DA, 2019, PROC CVPR IEEE, P6693, DOI 10.1109/CVPR.2019.00686
   Hung ZS, 2021, IEEE T PATTERN ANAL, V43, P3820, DOI 10.1109/TPAMI.2020.2992222
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Knyazev B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15807, DOI 10.1109/ICCV48922.2021.01553
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Kuznetsova A, 2020, INT J COMPUT VISION, V128, P1956, DOI 10.1007/s11263-020-01316-z
   Li KP, 2023, IEEE T PATTERN ANAL, V45, P641, DOI 10.1109/TPAMI.2022.3148470
   Li RJ, 2021, PROC CVPR IEEE, P11104, DOI 10.1109/CVPR46437.2021.01096
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin X, 2020, PROC CVPR IEEE, P3743, DOI 10.1109/CVPR42600.2020.00380
   Liu HY, 2021, PROC CVPR IEEE, P11541, DOI 10.1109/CVPR46437.2021.01138
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Lu YC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15911, DOI 10.1109/ICCV48922.2021.01563
   Parcollet T., 2019, P INT C LEARN REPR, P1
   Pavllo D., 2018, P BRIT MACH VIS C, P1
   Pavllo D, 2020, INT J COMPUT VISION, V128, P855, DOI 10.1007/s11263-019-01245-6
   Peng L, 2022, IEEE T PATTERN ANAL, V44, P318, DOI 10.1109/TPAMI.2020.3004830
   Quan YH, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107639
   Reichert D. P., 2014, P INT C LEARN REPR, P1
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi L, 2020, INT CONF ACOUST SPEE, P4412, DOI [10.1109/ICASSP40776.2020.9053595, 10.1109/icassp40776.2020.9053595]
   Suhail M, 2021, PROC CVPR IEEE, P13931, DOI 10.1109/CVPR46437.2021.01372
   Sutskever I, 2014, ADV NEUR IN, V27
   Tang K., 2020, A scene graph generation codebase in pytorch
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   Tang KH, 2019, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2019.00678
   Tay Y, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1494
   Trabalón Carina I., 2018, Polis, V17, P163
   Trouillon T, 2016, PR MACH LEARN RES, V48
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang B, 2020, INT C LEARNING REPRE
   Wang N., 2021, PROC IEEE INT C MULT, P1
   Wang Z, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P4977, DOI 10.1145/3503161.3548237
   Wisdom S, 2016, ADV NEUR IN, V29
   Wu LX, 2021, IEEE T CIRC SYST VID, V31, P3118, DOI 10.1109/TCSVT.2020.3036860
   Wu LX, 2020, IEEE T MULTIMEDIA, V22, P808, DOI 10.1109/TMM.2019.2931815
   Xiang L., 2020, P INT C LEARN REPR
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Yan ST, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P265, DOI 10.1145/3394171.3413722
   Yang B., 2015, P 3 INT C LEARN REPR
   Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41
   Yang X, 2022, IEEE T PATTERN ANAL, V44, P2313, DOI 10.1109/TPAMI.2020.3042192
   Zareian Alireza, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P606, DOI 10.1007/978-3-030-58592-1_36
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zhang A., 2021, INT C LEARNING REPRE, P1
   Zhang HW, 2017, PROC CVPR IEEE, P3107, DOI 10.1109/CVPR.2017.331
   Zhang J, 2019, PROC CVPR IEEE, P11527, DOI 10.1109/CVPR.2019.01180
   Zhang SF, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102842
   Zhang S, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4313
NR 61
TC 6
Z9 6
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8646
EP 8656
DI 10.1109/TMM.2023.3239229
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000031
DA 2024-07-18
ER

PT J
AU Wu, KL
   Chen, J
   Yu, Y
   Ma, JY
AF Wu, Kangle
   Chen, Jun
   Yu, Yang
   Ma, Jiayi
TI ACE-MEF: Adaptive Clarity Evaluation-Guided Network With Illumination
   Correction for Multi-Exposure Image Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-exposure image fusion; unsupervised deep learning; clarity
   preservation; illumination adjustment; YCbCr
AB For a natural scene with nonuniform environment light, the captured visible images are always under- or over-exposed because of the limited dynamic range of digital imaging devices. Multi-exposure image fusion (MEF) is a mainstream and effective solution. For a local region that has friendly visual effect in one exposure setting but extremely bad-exposed in another, most existing MEF methods have the ability to transfer the scene detail information to the fused images. However, they will be affected by the over-high or -low light inevitably thus resulting in local visibility reduction. To address this issue, we propose an adaptive clarity evaluation-guided network with illumination correction for MEF in a coarse-to-fine manner, which is termed as ACE-MEF. To be specific, our ACE-MEF is mainly composed of two modules: clarity preservation network (CPN) and illumination adjustment network (IAN). Based on the adaptive clarity evaluation, CPN could be trained to coarsely preserve the environment light and texture details of the clearer regions in source images. Therefore, the need for labeled reference images that are time-consuming to obtain could be mitigated. By measuring the parameter maps of gamma function, IAN is able to refine and correct the local bad-exposed regions so that more details could be further revealed. Extensive experiments demonstrate that our method outperforms multiple state-of-the-art algorithms qualitatively and quantitatively.
C1 [Wu, Kangle; Chen, Jun] China Univ Geosci, Engn Res Ctr Intelligent Technol Geoexplorat, Sch Automat, Hubei Key Lab Adv Control & Intelligent Automation, Wuhan 430074, Peoples R China.
   [Ma, Jiayi] Wuhan Univ, Sch Elect Informat, Wuhan 430072, Peoples R China.
   [Yu, Yang] Chinese Acad Sci, Shanghai Inst Tech Phys, Key Lab Infrared Syst Detecting & Imaging Technol, Shanghai 200083, Peoples R China.
C3 China University of Geosciences; Wuhan University; Chinese Academy of
   Sciences; Shanghai Institute of Technical Physics, CAS
RP Chen, J (corresponding author), China Univ Geosci, Engn Res Ctr Intelligent Technol Geoexplorat, Sch Automat, Hubei Key Lab Adv Control & Intelligent Automation, Wuhan 430074, Peoples R China.
EM wukangle@cug.edu.cn; chenjun71983@163.com; yuyang@mail.sitp.ac.cn;
   jyma2010@gmail.com
RI yu, yang/HIZ-9682-2022; Ma, Jiayi/Y-2470-2019
OI Ma, Jiayi/0000-0003-3264-3265; Chen, Jun/0000-0001-9005-6849; Wu,
   Kangle/0000-0002-0147-756X; Wu, Kangle/0009-0009-3469-047X
FU National Natural Science Foundation of China
FX No Statement Available
CR Bulanon DM, 2009, BIOSYST ENG, V103, P12, DOI 10.1016/j.biosystemseng.2009.02.009
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Chang M, 2022, IEEE T MULTIMEDIA, V24, P702, DOI 10.1109/TMM.2021.3058586
   Chen J, 2022, IEEE T MULTIMEDIA, V24, P655, DOI 10.1109/TMM.2021.3057493
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Galdran A, 2018, SIGNAL PROCESS, V149, P135, DOI 10.1016/j.sigpro.2018.03.008
   Han D, 2022, INFORM FUSION, V79, P248, DOI 10.1016/j.inffus.2021.10.006
   Hou RC, 2020, IEEE T COMPUT IMAG, V6, P640, DOI 10.1109/TCI.2020.2965304
   Hou XL, 2021, IEEE PHOTONICS J, V13, DOI 10.1109/JPHOT.2021.3058740
   Huang F, 2018, IEEE ACCESS, V6, P42877, DOI 10.1109/ACCESS.2018.2859355
   Jiang ZQ, 2021, NEUROCOMPUTING, V454, P361, DOI 10.1016/j.neucom.2021.05.025
   Kim JH, 2021, IEEE ACCESS, V9, P164551, DOI 10.1109/ACCESS.2021.3134316
   Kinoshita Y, 2019, IEEE T IMAGE PROCESS, V28, P4101, DOI 10.1109/TIP.2019.2906501
   Kumar A, 2021, J VIS COMMUN IMAGE R, V78, DOI 10.1016/j.jvcir.2021.103122
   Li J, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3029360
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Liu SG, 2019, IEEE T CONSUM ELECTR, V65, P303, DOI 10.1109/TCE.2019.2893644
   Liu Y, 2015, J VIS COMMUN IMAGE R, V31, P208, DOI 10.1016/j.jvcir.2015.06.021
   Lu K, 2021, IEEE T MULTIMEDIA, V23, P4093, DOI 10.1109/TMM.2020.3037526
   Luo XY, 2012, IEEE T INSTRUM MEAS, V61, P1130, DOI 10.1109/TIM.2011.2174898
   Ma JY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3038013
   Ma JY, 2022, IEEE-CAA J AUTOMATIC, V9, P1200, DOI 10.1109/JAS.2022.105686
   Ma JY, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01359-2
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Ma KD, 2020, IEEE T IMAGE PROCESS, V29, P2808, DOI 10.1109/TIP.2019.2952716
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P2519, DOI 10.1109/TIP.2017.2671921
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Mertens T, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P382, DOI 10.1109/PG.2007.17
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Qu LH, 2022, AAAI CONF ARTIF INTE, P2126
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ryu J.-H., 2021, IEEE Access, V9
   Shen JB, 2014, IEEE T CYBERNETICS, V44, P1579, DOI 10.1109/TCYB.2013.2290435
   Ulucan O, 2021, SIGNAL PROCESS, V178, DOI 10.1016/j.sigpro.2020.107791
   Xiang HY, 2012, ADV MATER RES-SWITZ, V403-408, P2200, DOI 10.4028/www.scientific.net/AMR.403-408.2200
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Xu H, 2020, IEEE T IMAGE PROCESS, V29, P7203, DOI 10.1109/TIP.2020.2999855
   Zhang H, 2021, INFORM FUSION, V76, P323, DOI 10.1016/j.inffus.2021.06.008
   Zhang H, 2021, INT J COMPUT VISION, V129, P2761, DOI 10.1007/s11263-021-01501-8
   Zhang W, 2017, INFORM SCIENCES, V415, P19, DOI 10.1016/j.ins.2017.05.019
   Zhang XC, 2021, INFORM FUSION, V74, P111, DOI 10.1016/j.inffus.2021.02.005
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhu Z., 2021, IEEE Trans. Instrum. Meas., V70
NR 43
TC 3
Z9 3
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8103
EP 8118
DI 10.1109/TMM.2022.3233299
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000008
DA 2024-07-18
ER

PT J
AU Xie, JH
   Chen, XB
   Zhang, TY
   Zhang, YX
   Lu, SP
   Cesar, P
   Yang, YL
AF Xie, Jiehang
   Chen, Xuanbai
   Zhang, Tianyi
   Zhang, Yixuan
   Lu, Shao-Ping
   Cesar, Pablo
   Yang, Yulu
TI Multimodal-Based and Aesthetic-Guided Narrative Video Summarization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Narrative video summarization; multimodal information; aesthetic
   guidance
AB Narrative videos usually illustrate the main content through multiple narrative information such as audios, video frames and subtitles. Existing video summarization approaches rarely consider the multiple dimensional narrative inputs, or ignore the impact of shots artistic assembly when directly applied to narrative videos. This paper introduces a multimodal-based and aesthetic-guided narrative video summarization method. Our method leverages multimodal information including visual content, subtitles and audio information through our specified key shots selection, subtitle summarization, and highlight extraction components. Furthermore, under the guidance of cinematographic aesthetic, we design a novel shots assembly module to ensure the shot content completeness and then assemble the selected shots into a desired summary. Besides, our method also provides the flexible specification for shots selection, to achieve which it automatically selects semantically related shots according to the user-designed text. By conducting a large number of quantitative experimental evaluations and user studies, we demonstrate that our method effectively preserves important narrative information of the original video, and it is capable of rapidly producing high-quality and aesthetic-guided narrative video summaries.
C1 [Xie, Jiehang; Chen, Xuanbai; Zhang, Yixuan; Lu, Shao-Ping; Yang, Yulu] Nankai Univ, TKLNDST, CS, Nankai 300071, Peoples R China.
   [Zhang, Tianyi; Cesar, Pablo] Ctr Wiskunde & Informat, NL-098 XG Amsterdam, Netherlands.
C3 Nankai University
RP Lu, SP (corresponding author), Nankai Univ, TKLNDST, CS, Nankai 300071, Peoples R China.
EM jiehangxie@mail.nankai.edu.cn; 1711314@mail.nankai.edu.cn;
   tianyi.zhang@cwi.nl; 2011432@mail.nankai.edu.cn; slu@nankai.edu.cn;
   p.s.cesar@cwi.nl; yangyl@nankai.edu.cn
RI Zhang, Tianyi/AAG-6220-2021; Xie, Jiehang/KDM-7559-2024
OI Xie, Jiehang/0000-0001-9003-1975; Zhang, Tianyi/0000-0001-6293-881X;
   Cesar, Pablo/0000-0003-1752-6837
FU NSFC [62132012, 61972216]
FX This work was supported by NSFC under Grants 62132012 and 61972216
CR Ahmed SA, 2020, IEEE T INTELL TRANSP, V21, P3457, DOI 10.1109/TITS.2019.2929618
   Truong A, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P497, DOI 10.1145/2984511.2984569
   [Anonymous], 2018, P IEEE C COMP VIS PA
   Basavarajaiah M, 2020, ACM COMPUT SURV, V52, DOI 10.1145/3355398
   Botian Shi, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P4355, DOI 10.1145/3394171.3413498
   Cao D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P898, DOI 10.1145/3394171.3413841
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8199
   Choi JH, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1003, DOI 10.1145/2733373.2806386
   Decroos T, 2017, AAAI CONF ARTIF INTE, P1302
   diaeresis>tze Hinrich Schu<spacing, 2008, INTRO INFORM RETRIEV, V39
   Fajtl J, 2019, LECT NOTES COMPUT SC, V11367, P39, DOI 10.1007/978-3-030-21074-8_4
   Ging S., 2020, ADV NEUR IN, V33, p22 605
   Guo ZY, 2022, IEEE T MULTIMEDIA, V24, P2606, DOI 10.1109/TMM.2021.3087001
   Hu L., 2021, P IEEE INT C MULT EX, P1
   Hu TL, 2017, 2017 IEEE THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2017), P117, DOI 10.1109/BigMM.2017.19
   Huber B., 2019, P SIGCHI C HUM FACT, P1
   Jin Hanqi, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, P6244, DOI DOI 10.18653/V1/2020.ACLMAIN.556
   Li HR, 2019, IEEE T KNOWL DATA EN, V31, P996, DOI 10.1109/TKDE.2018.2848260
   Li JN, 2020, IEEE T MULTIMEDIA, V22, P554, DOI 10.1109/TMM.2019.2930041
   Li XL, 2020, IEEE T IND ELECTRON, V67, P5778, DOI 10.1109/TIE.2019.2931283
   Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750
   Li ZF, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2807705
   Lu SP, 2013, IEEE T VIS COMPUT GR, V19, P1218, DOI 10.1109/TVCG.2012.145
   Makino T, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1039
   Merler M, 2019, IEEE T MULTIMEDIA, V21, P1147, DOI 10.1109/TMM.2018.2876046
   Niu YZ, 2012, IEEE T CIRC SYST VID, V22, P1037, DOI 10.1109/TCSVT.2012.2189689
   Otani M, 2019, PROC CVPR IEEE, P7579, DOI 10.1109/CVPR.2019.00778
   Panda R, 2017, IEEE T MULTIMEDIA, V19, P2010, DOI 10.1109/TMM.2017.2708981
   Pavel A., 2014, P ACM S US INT SOFTW, P1
   Qu XY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4280, DOI 10.1145/3394171.3414053
   Saquil Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1698, DOI 10.1109/ICCV48922.2021.00174
   Stowell D, 2015, IEEE T MULTIMEDIA, V17, P1733, DOI 10.1109/TMM.2015.2428998
   Sun M, 2017, IEEE T PATTERN ANAL, V39, P2256, DOI 10.1109/TPAMI.2016.2623699
   Varini P, 2017, IEEE T MULTIMEDIA, V19, P2832, DOI 10.1109/TMM.2017.2705915
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356520
   Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062
   Xu Jiacheng, 2020, P 58 ANN M ASS COMP, P5021, DOI 10.18653/v1/2020.acl-main.451
   Xu S, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P1355
   Ying Zhang, 2014, ACM Transactions on Multimedia Computing, Communications and Applications, V11, DOI 10.1145/2659520
   Yuan L, 2020, IEEE T MULTIMEDIA, V22, P2711, DOI 10.1109/TMM.2019.2959451
   Zhang Hao, 2020, P 58 ANN M ASS COMPU, P6543
   Zhang K, 2018, LECT NOTES COMPUT SC, V11212, P391, DOI 10.1007/978-3-030-01237-3_24
   Zhao B, 2020, IEEE T NEUR NET LEAR, V31, P3989, DOI 10.1109/TNNLS.2019.2951680
   Zhao B, 2018, PROC CVPR IEEE, P7405, DOI 10.1109/CVPR.2018.00773
   Zhong M, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1049
   Zhou KY, 2018, AAAI CONF ARTIF INTE, P7582
   Zhu WC, 2021, IEEE T IMAGE PROCESS, V30, P948, DOI 10.1109/TIP.2020.3039886
NR 48
TC 3
Z9 3
U1 11
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4894
EP 4908
DI 10.1109/TMM.2022.3183394
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300021
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xing, WW
   Yao, J
   Liu, ZX
   Liu, WB
   Zhang, SL
   Wang, LQ
AF Xing, Weiwei
   Yao, Jie
   Liu, Zixia
   Liu, Weibin
   Zhang, Shunli
   Wang, Liqiang
TI Contrastive JS: A Novel Scheme for Enhancing the Accuracy and Robustness
   of Deep Models
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contrastive Learning; Deep Model Robustness; Few-Shot Object Detection;
   Image Augmentation
AB Deep learning technologies have been applied in various computer vision tasks in recent years. However, deep models suffer performance decay when some unforeseen data are contained in the testing dataset. Although data enhancement techniques can alleviate this dilemma, the diversity of real data is too tremendous to simulate. To tackle this challenge, we study a scheme for improving the robustness and efficiency of the deep network training process in visual tasks. Specifically, first, we build positive and negative sample pairs based on a class-sensitive strategy. Then, we construct a feature-consistent learning strategy based on contrastive learning to constrain the representations of interclass features while paying attention to the intraclass features. To extend the effect of the consistent strategy, we propose a novel contrastive Jensen-Shannon divergence consistency loss (JS loss) to restrict the probability distributions of different sample pairs. The proposed scheme successfully enhances the robustness and accuracy of the utilized model. We validated our approach by conducting extensive experiments in the domains of model robustness and few-shot object detection (FSOD). The results showed that the proposed method achieved remarkable gains over state-of-the-art (SOTA) methods. We obtained a 3.2% average improvement over the best-performing FSOD method.
C1 [Xing, Weiwei; Yao, Jie; Zhang, Shunli] Beijing Jiaotong Univ, Sch Software Engn, Beijing 100044, Peoples R China.
   [Yao, Jie] Beijing Informat Sci & Technol Univ, Sch Informat Management, Beijing 100192, Peoples R China.
   [Liu, Weibin] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.
   [Liu, Zixia; Wang, Liqiang] Univ Cent Florida, Dept Comp Sci, Orlando, FL 32816 USA.
C3 Beijing Jiaotong University; Beijing Information Science & Technology
   University; Beijing Jiaotong University; State University System of
   Florida; University of Central Florida
RP Yao, J (corresponding author), Beijing Jiaotong Univ, Sch Software Engn, Beijing 100044, Peoples R China.; Yao, J (corresponding author), Beijing Informat Sci & Technol Univ, Sch Informat Management, Beijing 100192, Peoples R China.
EM wwxing@bjtu.edu.cn; 17112098@bjtu.edu.cn; zixia@knights.ucf.edu;
   wbliu@bjtu.edu.cn; slzhang@bjtu.edu.cn; lwang@cs.ucf.edu
OI Liu, Weibin/0000-0001-6246-0051
FU National Natural Science Foundation of China [61876018]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61876018.
CR [Anonymous], 2014, P INT C LEARN REPR
   Azulay A, 2019, J MACH LEARN RES, V20
   Chen H, 2018, AAAI CONF ARTIF INTE, P2836
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen ZT, 2019, PROC CVPR IEEE, P8672, DOI 10.1109/CVPR.2019.00888
   Cubuk E. D., 2019, PROC IEEECVF C COMP, P1
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan Q, 2020, PROC CVPR IEEE, P4012, DOI 10.1109/CVPR42600.2020.00407
   Goldblum M., 2020, P INT C MACH LEARN I, P3607
   Goodfellow I.J., 2015, PROC 3 INT C LEARN R
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Hendrycks D., 2019, PROC INT C LEARN RE, P1
   Hendrycks D., 2020, PROC INT C LEARN RE, P1
   Hjelm R. D., 2019, PROC INT C LEARN RE, P1
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jiaxi Wu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P456, DOI 10.1007/978-3-030-58517-4_27
   Jing LL, 2021, IEEE T PATTERN ANAL, V43, P4037, DOI 10.1109/TPAMI.2020.2992393
   Kang BY, 2019, IEEE I CONF COMP VIS, P8419, DOI 10.1109/ICCV.2019.00851
   Karlinsky L, 2021, AAAI CONF ARTIF INTE, V35, P1743
   Karlinsky L, 2019, PROC CVPR IEEE, P5192, DOI 10.1109/CVPR.2019.00534
   Lakshminarayanan B, 2017, ADV NEUR IN, V30
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lipton ZC, 2018, PR MACH LEARN RES, V80
   Liu Q, 2021, IEEE T MULTIMEDIA, V23, P2114, DOI 10.1109/TMM.2020.3008028
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu X, 2023, IEEE T KNOWL DATA EN, V35, P857, DOI 10.1109/TKDE.2021.3090866
   Nguyen D, 2022, IEEE T MULTIMEDIA, V24, P1313, DOI 10.1109/TMM.2021.3063612
   Patacchiola M., 2020, Advances in Neural Information Processing Systems, P4003
   Perrett T, 2019, PROC CVPR IEEE, P7844, DOI 10.1109/CVPR.2019.00804
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Roy D, 2023, IEEE T MULTIMEDIA, V25, P2280, DOI 10.1109/TMM.2022.3145663
   Song LX, 2019, IEEE I CONF COMP VIS, P773, DOI 10.1109/ICCV.2019.00086
   Springenberg Jost Tobias, 2015, Striving for simplicity: The all convolutional net, DOI DOI 10.48550/ARXIV.1412.6806
   Sun B, 2021, PROC CVPR IEEE, P7348, DOI 10.1109/CVPR46437.2021.00727
   Termritthikun C, 2020, MULTIMED TOOLS APPL, V79, P1475, DOI 10.1007/s11042-019-08332-3
   Tramer F., 2018, PROC 6 INT C LEARN R
   Wan SH, 2020, IEEE T MULTIMEDIA, V22, P1756, DOI 10.1109/TMM.2020.2976573
   Wang X., 2020, ICML, p2020b
   Wang YX, 2019, IEEE I CONF COMP VIS, P9924, DOI 10.1109/ICCV.2019.01002
   Wang YL, 2022, IEEE T PATTERN ANAL, V44, P3733, DOI 10.1109/TPAMI.2021.3052951
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yan XP, 2019, IEEE I CONF COMP VIS, P9576, DOI 10.1109/ICCV.2019.00967
   Yang KY, 2022, PR MACH LEARN RES
   Yang Xiao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P192, DOI 10.1007/978-3-030-58520-4_12
   Yang Z, 2020, AAAI CONF ARTIF INTE, V34, P12653
   Yang ZL, 2019, ADV NEUR IN, V32
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064
   Zhang H., 2018, PROC INT C LEARN RE, P1
   Zhang Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3542820
   Zhu CC, 2021, PROC CVPR IEEE, P8778, DOI 10.1109/CVPR46437.2021.00867
NR 55
TC 1
Z9 1
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7881
EP 7893
DI 10.1109/TMM.2022.3232030
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400021
DA 2024-07-18
ER

PT J
AU Xu, CY
   Chai, ZH
   Xu, ZZ
   Li, HJ
   Zuo, QRY
   Yang, LY
   Yuan, C
AF Xu, Chengyin
   Chai, Zenghao
   Xu, Zhengzhuo
   Li, Hongjia
   Zuo, Qiruyi
   Yang, Lingyu
   Yuan, Chun
TI HHF: Hashing-Guided Hinge Function for Deep Hashing Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Codes; Quantization (signal); Image retrieval; Semantics;
   Feature extraction; Training; deep hash; metric learning; quantization
   learning
ID MINIMUM-DISTANCE BOUNDS; IMAGE RETRIEVAL; UPDATED TABLE; BINARY-CODES;
   QUANTIZATION
AB Deep hashing has shown promising performance in large-scale image retrieval. The hashing process utilizes Deep Neural Networks (DNNs) to embed images into compact continuous latent codes, then map them into binary codes by hashing function for efficient retrieval. Recent approaches perform metric loss and quantization loss to supervise the two procedures that cluster samples with the same categories and alleviate semantic information loss after binarization in the end-to-end training framework. However, we observe the incompatible conflict that the optimal cluster positions are not identical to the ideal hash positions because of the different objectives of the two loss terms, which lead to severe ambiguity and error-hashing after the binarization process. To address the problem, we borrow the Theory of Minimum-Distance Bounds for Binary Linear Codes to design the inflection point that depends on the hash bit length and category numbers and thereby propose Hashing-guided Hinge Function (HHF) to explicitly enforce the termination of metric loss to prevent the negative pairs unlimited alienated. Such modification is proven effective and essential for training, which contributes to proper intra- and inter-distances for clusters and better hash positions for accurate image retrieval simultaneously. Extensive experiments in CIFAR-10, CIFAR-100, ImageNet, and MS-COCO justify that HHF consistently outperforms existing techniques and is robust and flexible to transplant into other methods. Code is available at https://github.com/JerryXu0129/HHF.
C1 [Xu, Chengyin; Chai, Zenghao; Xu, Zhengzhuo; Li, Hongjia; Yang, Lingyu; Yuan, Chun] Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
   [Zuo, Qiruyi] Tsinghua Univ, Tsinghua Berkeley Shenzhen Inst, Shenzhen 518055, Peoples R China.
C3 Tsinghua University; Tsinghua Shenzhen International Graduate School;
   Tsinghua Shenzhen International Graduate School; Tsinghua University
RP Yuan, C (corresponding author), Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
EM xucy20@mails.tsinghua.edu.cn; zenghaochai@gmail.com;
   xzz20@mails.tsinghua.edu.cn; lhj20@mails.tsinghua.edu.cn;
   zqry20@mails.tsinghua.edu.cn; yly20@mails.tsinghua.edu.cn;
   yuanc@sz.tsinghua.edu.cn
RI WANG, SHIHAO/KHC-8263-2024
OI Xu, Zhengzhuo/0000-0003-4620-9187; Chai, Zenghao/0000-0003-3709-4947
FU SZSTC [JCYJ20190809172201639, WDZC20200820200655001]; Shenzhen Key
   Laboratory [ZDSYS20210623092001004]
FX This work was supported in part by SZSTC under Grants
   JCYJ20190809172201639 and WDZC20200820200655001, and in part by Shenzhen
   Key Laboratory under Grant ZDSYS20210623092001004.
CR [Anonymous], 2009, NIPS
   Aziere N, 2019, PROC CVPR IEEE, P7291, DOI 10.1109/CVPR.2019.00747
   Bai JL, 2019, IEEE T MULTIMEDIA, V21, P3178, DOI 10.1109/TMM.2019.2920601
   Bai JL, 2020, IEEE T MULTIMEDIA, V22, P215, DOI 10.1109/TMM.2019.2922130
   BAUMERT LD, 1973, IEEE T INFORM THEORY, V19, P134, DOI 10.1109/TIT.1973.1054939
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   BROUWER AE, 1993, IEEE T INFORM THEORY, V39, P677, DOI 10.1109/18.212302
   BROUWER AE, 1993, IEEE T INFORM THEORY, V39, P662, DOI 10.1109/18.212301
   Cao Y, 2018, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2018.00134
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Chen W, 2023, IEEE T PATTERN ANAL, V45, P7270, DOI 10.1109/TPAMI.2022.3218591
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Cui H, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1432, DOI 10.1145/3474085.3475605
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gu LC, 2021, IEEE T MULTIMEDIA, V23, P939, DOI 10.1109/TMM.2020.2991513
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   Hameed IM, 2021, COGENT ENG, V8, DOI 10.1080/23311916.2021.1927469
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HELGERT HJ, 1973, IEEE T INFORM THEORY, V19, P344, DOI 10.1109/TIT.1973.1055009
   Hoe J. T., 2021, P NIPS, P24286
   Jiang QY, 2018, IEEE T IMAGE PROCESS, V27, P5996, DOI 10.1109/TIP.2018.2864894
   Jose A, 2022, INT CONF ACOUST SPEE, P4773, DOI 10.1109/ICASSP43922.2022.9746805
   Jose A, 2022, SIGNAL PROCESS-IMAGE, V100, DOI 10.1016/j.image.2021.116529
   Kim S, 2020, PROC CVPR IEEE, P3235, DOI 10.1109/CVPR42600.2020.00330
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Li WJ, 2016, IJCAI, P1711
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu XB, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1561, DOI 10.1145/3343031.3351091
   Min WQ, 2020, IEEE T MULTIMEDIA, V22, P3128, DOI 10.1109/TMM.2020.2974326
   Movshovitz-Attias Y, 2017, IEEE I CONF COMP VIS, P360, DOI 10.1109/ICCV.2017.47
   Paszke A, 2019, ADV NEUR IN, V32
   Rawat W, 2017, NEURAL COMPUT, V29, P2352, DOI [10.1162/NECO_a_00990, 10.1162/neco_a_00990]
   Roth K, 2022, PROC CVPR IEEE, P7410, DOI 10.1109/CVPR52688.2022.00727
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shi Y, 2021, IEEE T MULTIMEDIA, V23, P3778, DOI 10.1109/TMM.2020.3031092
   Sohn K, 2016, ADV NEUR IN, V29
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vardy A, 1997, IEEE T INFORM THEORY, V43, P1757, DOI 10.1109/18.641542
   VERHOEFF T, 1987, IEEE T INFORM THEORY, V33, P665, DOI 10.1109/TIT.1987.1057356
   Nguyen VA, 2016, IEEE INT CON MULTI
   Wang JP, 2021, AAAI CONF ARTIF INTE, V35, P2755
   Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976
   Wang XF, 2017, LECT NOTES COMPUT SC, V10111, P70, DOI 10.1007/978-3-319-54181-5_5
   Wang ZJ, 2021, IEEE T MULTIMEDIA, V23, P1274, DOI 10.1109/TMM.2020.2995267
   Weng ZY, 2021, IEEE T MULTIMEDIA, V23, P1868, DOI 10.1109/TMM.2020.3004962
   WISEMAN JA, 1992, INFORM COMPUT, V98, P132, DOI 10.1016/0890-5401(92)90044-G
   Wu DY, 2018, IEEE INT CON MULTI
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Yan C, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1535, DOI 10.1145/3343031.3350927
   Yuan L, 2020, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR42600.2020.00315
   Zhang J, 2018, IEEE T MULTIMEDIA, V20, P2400, DOI 10.1109/TMM.2018.2804763
   Zhang L, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2490823
   Zhang M, 2021, INT C PATT RECOG, P10516, DOI 10.1109/ICPR48806.2021.9412086
   Zhao S, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P763, DOI 10.1145/3394171.3414033
   Zhe XF, 2020, IEEE T NEUR NET LEAR, V31, P1681, DOI 10.1109/TNNLS.2019.2921805
   Zheng CQ, 2021, IEEE T MULTIMEDIA, V23, P4079, DOI 10.1109/TMM.2020.3037456
   Zhou Q, 2018, IEEE INT CON MULTI
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
NR 66
TC 3
Z9 3
U1 2
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7428
EP 7440
DI 10.1109/TMM.2022.3222598
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000051
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xu, Z
   Wei, K
   Yang, X
   Deng, C
AF Xu, Zhe
   Wei, Kun
   Yang, Xu
   Deng, Cheng
TI Point-Supervised Video Temporal Grounding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal contrast; multi-level distribution calibration; point
   supervision; video temporal grounding
ID NETWORK
AB Given an untrimmed video and a language query, Video Temporal Grounding (VTG) aims to locate the time interval in the video semantically relevant to the query. Existing fully-supervised VTG methods require accurate annotations of temporal boundary, which is time-consuming and expensive to obtain. On the other hand, weakly-supervised VTG methods where only paired videos and queries are available during training lag far behind the fully-supervised ones. In this paper, we introduce point supervision to narrow the performance gap with affordable annotating cost and propose a novel method dubbed Point-Supervised Video Temporal Grounding (PS-VTG). Specifically, an attention-based grounding network is first employed to obtain a language activation sequence (LAS). Then pseudo segment-level label is generated based on the LAS and the given point supervision to assist the training process. In addition, multi-level distribution calibration and cross-modal contrast are framed to obtain discriminative feature representations and precisely highlight the language-relevant video segments. Experiments on three benchmarks demonstrate that our method trained with point supervision can significantly outperform weakly-supervised approaches and achieve comparable performance with fully-supervised ones.
C1 [Xu, Zhe; Wei, Kun; Yang, Xu; Deng, Cheng] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
C3 Xidian University
RP Deng, C (corresponding author), Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
EM zhexu@stu.xidian.edu.cn; weikunsk@gmail.com; xuyang.xidian@gmail.com;
   chdeng.xd@gmail.com
OI Zhe, Xu/0000-0001-6898-3443
FU National Natural Science Foundation of China [62132016, 62171343,
   62071361]; Key Research and Development Program of Shaanxi
   [2021ZDLGY01-03]; Fundamental Research Funds for the Central
   Universities [ZDRC2102]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62132016, 62171343, and 62071361, in
   part by the Key Research and Development Program of Shaanxi under Grant
   2021ZDLGY01-03, and in part by the Fundamental Research Funds for the
   Central Universities under Grant ZDRC2102.
CR Bearman A, 2016, LECT NOTES COMPUT SC, V9911, P549, DOI 10.1007/978-3-319-46478-7_34
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen L, 2020, AAAI CONF ARTIF INTE, V34, P10551
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8199
   Dong JH, 2024, IEEE T PATTERN ANAL, V46, P1664, DOI 10.1109/TPAMI.2021.3128560
   Dong JH, 2020, PROC CVPR IEEE, P4022, DOI 10.1109/CVPR42600.2020.00408
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Duan LY, 2005, IEEE T MULTIMEDIA, V7, P1066, DOI 10.1109/TMM.2005.858395
   Fan Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P420, DOI 10.1007/978-3-030-58548-8_25
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Gao MF, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P1481
   Guo DS, 2018, IEEE T MULTIMEDIA, V20, P3428, DOI 10.1109/TMM.2018.2839534
   Hahn M, 2020, Arxiv, DOI arXiv:1904.09936
   He DL, 2019, AAAI CONF ARTIF INTE, P8393
   Huang Jiabo, 2021, P IEEE CVF INT C COM, P7199
   Jonghwan Mun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10807, DOI 10.1109/CVPR42600.2020.01082
   Kingma D, 2014, ICLR P, V2014, P1
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Lee P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13628, DOI 10.1109/ICCV48922.2021.01339
   Li XY, 2021, AAAI CONF ARTIF INTE, V35, P1966
   Li Z, 2021, PROC CVPR IEEE, P8361, DOI 10.1109/CVPR46437.2021.00826
   Lin ZJ, 2020, AAAI CONF ARTIF INTE, V34, P11539
   Lu CJ, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5144
   Maninis KK, 2018, PROC CVPR IEEE, P616, DOI 10.1109/CVPR.2018.00071
   Mettes P, 2016, LECT NOTES COMPUT SC, V9909, P437, DOI 10.1007/978-3-319-46454-1_27
   Minuk Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P156, DOI 10.1007/978-3-030-58604-1_10
   Mithun NC, 2019, PROC CVPR IEEE, P11584, DOI 10.1109/CVPR.2019.01186
   Moltisanti D, 2019, PROC CVPR IEEE, P9907, DOI 10.1109/CVPR.2019.01015
   Nan GS, 2021, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR46437.2021.00279
   Paszke A, 2019, ADV NEUR IN, V32
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Rohrbach M, 2012, LECT NOTES COMPUT SC, V7572, P144, DOI 10.1007/978-3-642-33718-5_11
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Shaoxiang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P333, DOI 10.1007/978-3-030-58548-8_20
   Tan RB, 2021, IEEE WINT CONF APPL, P2082, DOI 10.1109/WACV48630.2021.00213
   Wang JZ, 2018, IEEE T MULTIMEDIA, V20, P2578, DOI 10.1109/TMM.2018.2855081
   Wang ZZ, 2022, AAAI CONF ARTIF INTE, P2613
   Wei K, 2019, IEEE I CONF COMP VIS, P3740, DOI 10.1109/ICCV.2019.00384
   Wu J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1283, DOI 10.1145/3394171.3413862
   Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062
   Yang L, 2022, IEEE T PATTERN ANAL, V44, P9814, DOI 10.1109/TPAMI.2021.3132058
   Yang WF, 2021, IEEE T IMAGE PROCESS, V30, P3252, DOI 10.1109/TIP.2021.3058614
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Zeng R., 2020, CVPR
   Zhang Hao, 2020, P 58 ANN M ASS COMPU, P6543
   Zhang SY, 2020, AAAI CONF ARTIF INTE, V34, P12870
   Zhang Z, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4098, DOI 10.1145/3394171.3413967
   Zhou Z, 2015, IEEE T MULTIMEDIA, V17, P512, DOI 10.1109/TMM.2015.2404779
NR 49
TC 4
Z9 4
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6121
EP 6131
DI 10.1109/TMM.2022.3205404
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500034
DA 2024-07-18
ER

PT J
AU Yang, L
   Song, Q
   Wang, ZH
   Liu, ZW
   Xu, SC
   Li, ZH
AF Yang, Lu
   Song, Qing
   Wang, Zhihui
   Liu, Zhiwei
   Xu, Songcen
   Li, Zhihao
TI Quality-Aware Network for Human Parsing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computer vision; image segmentation; multi-media computing
ID POSE
AB How to estimate the quality of the network output is an important issue, and currently there is no effective solution in the field of human parsing. To solve this problem, this work proposes a statistical method based on the output probability map to calculate the pixel classification quality, which is called pixel score. In addition, the Quality-Aware Module (QAM) is proposed to fuse the different quality information, the purpose of which is to estimate the quality of human parsing results. We combine QAM with a concise and effective network design to propose Quality-Aware Network (QANet) for human parsing. Benefiting from the superiority of QAM and QANet, we achieve the best performance on three multiple and one single human parsing benchmarks, including CIHP, MHP-v2, Pascal-Person-Part, ATR and LIP. Without increasing the training and inference time, QAM improves the AP$<^>\text{r}$ criterion by more than 10 points in the multiple human parsing task. QAM can be extended to other tasks with good quality estimation, e.g instance segmentation. Specifically, QAM improves Mask R-CNN by similar to% mAP on COCO and LVISv1.0 datasets. Based on the proposed QAM and QANet, our overall system wins 1st place in CVPR2021 L2ID High-resolution Human Parsing (HRHP) Challenge, and 2nd in CVPR2021 PIC Short-video Face Parsing (SFP) Challenge. Code and models are available at https://github.com/soeaver/QANet.
C1 [Yang, Lu; Song, Qing; Wang, Zhihui] Beijing Univ Posts & Telecommun, Beijing 100088, Peoples R China.
   [Liu, Zhiwei] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
   [Xu, Songcen; Li, Zhihao] Huawei Technol, Noahs Ark Lab, Shenzhen 518129, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Chinese Academy of
   Sciences; Institute of Automation, CAS; Huawei Technologies
RP Song, Q (corresponding author), Beijing Univ Posts & Telecommun, Beijing 100088, Peoples R China.
EM soeaver@bupt.edu.cn; priv@bupt.edu.cn; wangzh@bupt.edu.cn;
   zhiwei.liu@nlpr.ia.ac.cn; xusongcen@huawei.com; zhihao.li@huawei.com
RI li, zhihao/HLW-8433-2023; yang, lu/GLV-5144-2022; Liu,
   Zhiwei/AAN-8965-2021; wang, zhihui/HSF-6639-2023
OI li, zhihao/0000-0003-0892-338X; 
FU China Postdoctoral Science Foundation [2022M710467]; National Key
   Research and Development Program of China [2021YFF0500900]
FX This work was supported in part by China Postdoctoral Science Foundation
   under Grant 2022M710467, and in part by the National Key Research and
   Development Program of China under Grant 2021YFF0500900.
CR [Anonymous], 2021, 3 PERS CONT PIC WORK
   [Anonymous], 2021, LEARN LTD IMP DAT L2
   Berman M, 2018, PROC CVPR IEEE, P4413, DOI 10.1109/CVPR.2018.00464
   Cao Z., 2007, P 24 INT C MACHINE L, P129, DOI DOI 10.1145/1273496.1273513
   Chang J, 2020, PROC CVPR IEEE, P5709, DOI 10.1109/CVPR42600.2020.00575
   Chao Peng, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6181, DOI 10.1109/CVPR.2018.00647
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Cheng BW, 2022, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR52688.2022.00135
   Girshick R., 2014, P 2014 IEEE C COMPUT, P580, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Gkioxari G, 2019, IEEE I CONF COMP VIS, P9784, DOI 10.1109/ICCV.2019.00988
   Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47
   Gong K, 2019, PROC CVPR IEEE, P7442, DOI [10.1109/cvpr.2019.00763, 10.1109/CVPR.2019.00763]
   Goyal P, 2018, Arxiv, DOI arXiv:1706.02677
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Gupta A, 2019, PROC CVPR IEEE, P5351, DOI 10.1109/CVPR.2019.00550
   Hao HQ, 2022, PATTERN RECOGN, V127, DOI 10.1016/j.patcog.2022.108593
   He HY, 2020, AAAI CONF ARTIF INTE, V34, P10949
   He K., 2017, IEEE C COMP VIS PATT, P2961, DOI DOI 10.1109/ICCV.2017.322
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657
   Jin Z., 2021, ICCV, P7231
   Jin ZC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7169, DOI 10.1109/ICCV48922.2021.00710
   Kingma D. P., 2014, arXiv
   Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656
   Kirillov A, 2019, PROC CVPR IEEE, P9396, DOI 10.1109/CVPR.2019.00963
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li JS, 2018, Arxiv, DOI arXiv:1705.07206
   Li L, 2022, P IEEECVF C COMPUTER, P1246
   Li L., 2022, PROC IEEE C COMPUT V, P8719
   Li PK, 2022, IEEE T PATTERN ANAL, V44, P3260, DOI 10.1109/TPAMI.2020.3048039
   Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063
   Liang XD, 2016, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2016.347
   Liang XD, 2015, IEEE I CONF COMP VIS, P1386, DOI 10.1109/ICCV.2015.163
   Liang XD, 2015, IEEE T PATTERN ANAL, V37, P2402, DOI 10.1109/TPAMI.2015.2408360
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S, 2015, PROC CVPR IEEE, P1419, DOI 10.1109/CVPR.2015.7298748
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu XC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P338, DOI 10.1145/3343031.3350857
   Liu YA, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1670, DOI 10.1145/3394171.3413831
   Liu ZW, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1976, DOI 10.1145/3474085.3475355
   Long J., 2015, P IEEE C COMP VIS PA, P3431, DOI DOI 10.48550/ARXIV.1411.4038
   Lu Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P421, DOI 10.1007/978-3-030-58610-2_25
   Luo XH, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P654, DOI 10.1145/3240508.3240634
   Luo YW, 2018, LECT NOTES COMPUT SC, V11213, P424, DOI 10.1007/978-3-030-01240-3_26
   Miao JX, 2021, PROC CVPR IEEE, P4131, DOI 10.1109/CVPR46437.2021.00412
   Nie XC, 2018, LECT NOTES COMPUT SC, V11209, P519, DOI 10.1007/978-3-030-01228-1_31
   Pang L, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P499, DOI 10.1145/3397271.3401104
   Pei CH, 2019, RECSYS 2019: 13TH ACM CONFERENCE ON RECOMMENDER SYSTEMS, P3, DOI 10.1145/3298689.3347000
   Qiao SY, 2021, PROC CVPR IEEE, P3996, DOI 10.1109/CVPR46437.2021.00399
   Qin H., 2019, BRIT MACH VIS C
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ruan T, 2019, AAAI CONF ARTIF INTE, P4814
   Ruyi Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P205, DOI 10.1007/978-3-030-58601-0_13
   Tao Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9260, DOI 10.1109/CVPR42600.2020.00928
   Terhorst P, 2020, PROC CVPR IEEE, P5650, DOI 10.1109/CVPR42600.2020.00569
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3508, DOI 10.1109/TPAMI.2021.3055780
   Wang WG, 2019, IEEE I CONF COMP VIS, P5702, DOI 10.1109/ICCV.2019.00580
   Wang X., 2020, Advances in Neural information processing systems
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Wenguan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8926, DOI 10.1109/CVPR42600.2020.00895
   Wu Y., 2019, Detectron 2
   Xia FT, 2017, PROC CVPR IEEE, P6080, DOI 10.1109/CVPR.2017.644
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xiaomei Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P189, DOI 10.1007/978-3-030-58586-0_12
   Xiaomei Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8968, DOI 10.1109/CVPR42600.2020.00899
   Yamaguchi K, 2013, IEEE I CONF COMP VIS, P3519, DOI 10.1109/ICCV.2013.437
   Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101
   Yang LJ, 2019, IEEE I CONF COMP VIS, P5187, DOI 10.1109/ICCV.2019.00529
   Yang L, 2022, IEEE-CAA J AUTOMATIC, V9, P1111, DOI 10.1109/JAS.2022.105647
   Yang L, 2022, INT J COMPUT VISION, V130, P1837, DOI 10.1007/s11263-022-01622-8
   Yang L, 2021, IEEE T IMAGE PROCESS, V30, P39, DOI 10.1109/TIP.2020.3029901
   Yang L, 2021, MULTIMED TOOLS APPL, V80, P855, DOI 10.1007/s11042-020-09604-z
   Yang L, 2019, PROC CVPR IEEE, P364, DOI 10.1109/CVPR.2019.00045
   Yang L, 2019, IEEE T NEUR NET LEAR, V30, P1744, DOI 10.1109/TNNLS.2018.2873722
   Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P173, DOI 10.1007/978-3-030-58539-6_11
   Zeng Dan, 2021, P IEEECVF INT C COMP, P11385
   Zhang H, 2022, IEEE COMPUT SOC CONF, P2735, DOI 10.1109/CVPRW56347.2022.00309
   Zhang XM, 2023, IEEE T MULTIMEDIA, V25, P2601, DOI 10.1109/TMM.2022.3148595
   Zhang ZW, 2022, IEEE T PATTERN ANAL, V44, P8492, DOI 10.1109/TPAMI.2021.3108771
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P792, DOI 10.1145/3240508.3240509
   Zhou BL, 2018, LECT NOTES COMPUT SC, V11212, P122, DOI 10.1007/978-3-030-01237-3_8
   Zhou BL, 2019, IEEE T PATTERN ANAL, V41, P2131, DOI 10.1109/TPAMI.2018.2858759
   Zhou TF, 2021, PROC CVPR IEEE, P1622, DOI 10.1109/CVPR46437.2021.00167
   Zhu B, 2021, IEEE WINT CONF APPL, P3247, DOI 10.1109/WACV48630.2021.00329
   Ziwei Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8897, DOI 10.1109/CVPR42600.2020.00892
NR 93
TC 6
Z9 6
U1 1
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7128
EP 7138
DI 10.1109/TMM.2022.3217413
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000030
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yao, HC
   Ni, RR
   Amirpour, H
   Timmerer, C
   Zhao, Y
AF Yao, Haichao
   Ni, Rongrong
   Amirpour, Hadi
   Timmerer, Christian
   Zhao, Yao
TI Detection and Localization of Video Transcoding From AVC to HEVC Based
   on Deep Representations of Decoded Frames and PU Maps
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; dual-path network; transcoded HEVC detection and
   localization; video forensics
ID DOUBLE COMPRESSION; STEGANALYSIS; H.264
AB In general, manipulated videos will eventually undergo recompression. Video transcoding will occur when the standard of recompression is different from the prior standard. Therefore, as a special sign of recompression, video transcoding can also be considered evidence of forgery in video forensics. In this paper, we focus on the detection and localization of video transcoding from AVC to HEVC (AVC-HEVC). There are two probable cases of AVC-HEVC transcoding - whole video transcoding and partial frame transcoding. However, the existing forensic methods only consider the detection of whole video transcoding, and they do not consider partial frame transcoding localization. In view of this, we propose a framewise scheme based on a convolutional neural network. First, we analyze that the essential difference between AVC-HEVC and HEVC is reflected in the high-frequency components of decoded frames. Then, the partition and location information of prediction units (PUs) are introduced to generate frame-level PU maps to make full use of the local artifacts of PUs. Finally, taking the decoded frames and PU maps as inputs, a dual-path network including specific convolutional modules and an adaptive fusion module is proposed. Through it, the artifacts on a single frame can be better extracted, and the transcoded frames can be detected and localized. Coupled with a simple voting strategy, the results of whole transcoding detection can be easily obtained. A large number of experiments are conducted to verify the performances. The results show that the proposed scheme outperforms or rivals the state-of-the-art methods in AVC-HEVC transcoding detection and localization.
C1 [Yao, Haichao; Ni, Rongrong; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Yao, Haichao; Ni, Rongrong; Zhao, Yao] Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.
   [Amirpour, Hadi; Timmerer, Christian] AlpenAdria Univ Klagenfurt, Inst Informat Technol, Christian Doppler Lab ATHENA, A-9020 Klagenfurt, Austria.
C3 Beijing Jiaotong University
RP Ni, RR (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM 17112062@bjtu.edu.cn; rrni@bjtu.edu.cn; hadi.amirpour@aau.at;
   christian.timmerer@aau.at; yzhao@bjtu.edu.cn
OI Amirpour, Hadi/0000-0001-9853-1720; Zhao, Yao/0000-0002-8581-9554
FU National Key Ramp;D Program of China
FX No Statement Available
CR [Anonymous], HM Software
   [Anonymous], 2006, P 8 WORKSHOP MULTIME, DOI DOI 10.1145/1161366.1161375
   Bestagini P, 2012, EUR SIGNAL PR CONF, P1229
   Bian S, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11010067
   Boroumand M, 2019, IEEE T INF FOREN SEC, V14, P1181, DOI 10.1109/TIFS.2018.2871749
   Chen M, 2017, IH&MMSEC'17: PROCEEDINGS OF THE 2017 ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, P75, DOI 10.1145/3082031.3083248
   Costanzo A, 2016, EUR SIGNAL PR CONF, P2245, DOI 10.1109/EUSIPCO.2016.7760648
   Fast Forward Moving Pictures Experts Group (FFMPEG), about us
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   github, X265 Software
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He PS, 2021, IEEE T MULTIMEDIA, V23, P3179, DOI 10.1109/TMM.2020.3021234
   He PS, 2020, IEEE T CIRC SYST VID, V30, P4034, DOI 10.1109/TCSVT.2019.2951630
   He PS, 2017, NEUROCOMPUTING, V228, P84, DOI 10.1016/j.neucom.2016.09.084
   Huang ZS, 2014, 2014 IEEE CHINA SUMMIT & INTERNATIONAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (CHINASIP), P306, DOI 10.1109/ChinaSIP.2014.6889253
   Jiang XH, 2020, IEEE T INF FOREN SEC, V15, P250, DOI 10.1109/TIFS.2019.2918085
   Jiang XH, 2018, IEEE T INF FOREN SEC, V13, P170, DOI 10.1109/TIFS.2017.2745687
   Jiang XH, 2013, IEEE SIGNAL PROC LET, V20, P447, DOI 10.1109/LSP.2013.2251632
   JM Software, About us
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   Pu W, 2013, PICT COD SYMP, P121, DOI 10.1109/PCS.2013.6737698
   PyTorch, About us
   Shelke NA, 2021, MULTIMED TOOLS APPL, V80, P6247, DOI 10.1007/s11042-020-09974-4
   Singh RD, 2018, MULTIMEDIA SYST, V24, P211, DOI 10.1007/s00530-017-0538-9
   Stamm MC, 2013, IEEE ACCESS, V1, P167, DOI 10.1109/ACCESS.2013.2260814
   Su Y., 2010, P 2 INT WORKSH INT S, P1
   Tang ZJ, 2021, J VIS COMMUN IMAGE R, V79, DOI 10.1016/j.jvcir.2021.103209
   Tang ZJ, 2020, COMPUT J, V63, P1017, DOI 10.1093/comjnl/bxz060
   Vázquez-Padín D, 2012, IEEE INT WORKS INFOR, P151, DOI 10.1109/WIFS.2012.6412641
   videolan, X264 Software
   Xiph.org Video Test Media, About us
   Xu JY, 2019, IEEE T MULTIMEDIA, V21, P1633, DOI 10.1109/TMM.2018.2885921
   Xu Q, 2021, SIGNAL PROCESS-IMAGE, V92, DOI 10.1016/j.image.2020.116109
   Yao HC, 2020, MULTIMED TOOLS APPL, V79, P5789, DOI 10.1007/s11042-019-08306-5
   Ye J, 2017, IEEE T INF FOREN SEC, V12, P2545, DOI 10.1109/TIFS.2017.2710946
   Yu Li-fang, 2018, Journal of Applied Sciences - Electronics and Information Engineering, V36, P278, DOI 10.3969/j.issn.0255-8297.2018.02.007
   Yu Y, 2020, SIGNAL PROCESS, V166, DOI 10.1016/j.sigpro.2019.107269
   Yuan H, 2017, IEEE T MULTIMEDIA, V19, P1416, DOI 10.1109/TMM.2017.2669858
   Zhang ZZ, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11111343
   Zhang ZZ, 2017, IET INFORM SECUR, V11, P152, DOI 10.1049/iet-ifs.2015.0361
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 42
TC 1
Z9 1
U1 1
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5014
EP 5029
DI 10.1109/TMM.2022.3185890
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD7R0
UT WOS:001116593700003
DA 2024-07-18
ER

PT J
AU Zeng, QY
   Liu, CJ
   Liu, M
   Chen, QJ
AF Zeng, Qinyang
   Liu, Chengju
   Liu, Ming
   Chen, Qijun
TI Contrastive 3D Human Skeleton Action Representation Learning via
   CrossMoCo With Spatiotemporal Occlusion Mask Data Augmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Skeleton; Feature extraction; Spatiotemporal phenomena;
   Three-dimensional displays; Data mining; Joints; Learning systems; Cross
   contrastive learning; spatiotemporal occlusion mask; human skeleton
   action recognition
AB Self-supervised learning methods for 3D skeleton-based action recognition via contrastive learning have obtained competitive achievements compared to classical supervised methods. Current researches show that adding a Multilayer Perceptron (MLP) to the top of the base encoder can extract high-level and global positive representations. Using a negative memory bank to store negative samples dynamically can balance the ample storage and feature consistency. However, these methods need to consider that the MLP lacks accurate encoding of fine-grained local features, and a memory bank needs rich and diverse negative sample pairs to match positive representations from different encoders. This paper proposes a new method called Cross Momentum Contrast (CrossMoCo), composed of three parts: ST-GCN encoder, ST-GCN encoder with MLP encoder (ST-MLP encoder), and two independent negative memory banks. The two encoders encode the input data into two positive feature pairs. Learning the cross representations of the two positive pairs is helpful for the model to extract both the global and the local information. Two independent negative memory banks update the negative samples according to different positive representations from two encoders, diversifying the negative samples' distribution and making negative representations close to the positive features. The increasing classification difficulty will improve the model's ability of contrastive learning. In addition, the spatiotemporal occlusion mask data augmentation method is used to enhance positive samples' information diversity. This method takes the adjacent skeleton joints that can form a skeleton bone as a mask unit, which can reduce the information redundancy after data augmentation since adjacent joints may carry similar spatiotemporal information. Experiments on the PKU-MMD Part II dataset, the NTU RGB+D 60 dataset, and the NW-UCLA dataset show that the CrossMoCo framework with spatiotemporal occlusion mask data augmentation has achieved a comparable performance.
C1 [Zeng, Qinyang; Liu, Chengju; Chen, Qijun] Tongji Univ, Coll Elect & Informat Engn, Shanghai 201804, Peoples R China.
   [Liu, Chengju] Tongji Res Inst Artificial Intelligence Suzhou, Suzhou 215300, Peoples R China.
   [Liu, Ming] Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Hong Kong 999077, Peoples R China.
C3 Tongji University; Hong Kong University of Science & Technology
RP Liu, CJ (corresponding author), Tongji Univ, Coll Elect & Informat Engn, Shanghai 201804, Peoples R China.
EM qinyangz@163.com; liuchengju@tongji.edu.cn; eelium@ust.hk;
   qjchen@tongji.edu.cn
RI Liu, Ming/AAC-9891-2020
OI Liu, Ming/0000-0002-4500-238X; ZENG, Qinyang/0000-0003-3972-0454
FU National Natural Science Foundation of China [62173248, 62073245];
   Suzhou Key Industry Technological Innovation-Core Technology RD Program
   [SGC2021035]; Special funds for Jiangsu Science and Technology Plan
   [BE2022119]; Shanghai Municipal Science and Technology Major Project
   [2021SHZDZX0100]; Fundamental Research Funds for the Central
   Universities; Shanghai Science and Technology Innovation Action Plan
   [22511104900]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62173248 and 62073245, in part by the
   Suzhou Key Industry Technological Innovation-Core Technology R&D Program
   under Grant SGC2021035, in part by the Special funds for Jiangsu Science
   and Technology Plan under Grant BE2022119, in part by the Shanghai
   Municipal Science and Technology Major Project under Grant
   2021SHZDZX0100, in part by the Fundamental Research Funds for the
   Central Universities, and in part by the Shanghai Science and Technology
   Innovation Action Plan under Grant 22511104900.
CR Chen CF, 2021, PROC CVPR IEEE, P6161, DOI 10.1109/CVPR46437.2021.00610
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Gao XH, 2023, IEEE T MULTIMEDIA, V25, P405, DOI 10.1109/TMM.2021.3127040
   Guo TY, 2022, AAAI CONF ARTIF INTE, P762
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Heidari N, 2021, INT C PATT RECOG, P7907, DOI 10.1109/ICPR48806.2021.9412091
   Hussein, 2013, INT JOINT C ART INT
   Ji YL, 2020, IEEE T CIRC SYST VID, V30, P2114, DOI 10.1109/TCSVT.2019.2912988
   Kun Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9628, DOI 10.1109/CVPR42600.2020.00965
   Li LG, 2021, PROC CVPR IEEE, P4739, DOI 10.1109/CVPR46437.2021.00471
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Lin LL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2490, DOI 10.1145/3394171.3413548
   Liu JY, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3365212
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Lu YJ, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING (DSP), P422, DOI 10.1109/ICDSP.2016.7868592
   Mehr HD, 2019, 2019 7TH INTERNATIONAL ISTANBUL SMART GRIDS AND CITIES CONGRESS AND FAIR (ICSG ISTANBUL 2019), P149, DOI 10.1109/SGCF.2019.8782290
   Nalci D., 2022, P INT C HUM COMP INT, P1
   Ohn-Bar E, 2013, IEEE COMPUT SOC CONF, P465, DOI 10.1109/CVPRW.2013.76
   Pan T, 2021, PROC CVPR IEEE, P11200, DOI 10.1109/CVPR46437.2021.01105
   Rao HC, 2021, INFORM SCIENCES, V569, P90, DOI 10.1016/j.ins.2021.04.023
   Seo M, 2022, IEEE ROBOT AUTOM LET, V7, P1752, DOI 10.1109/LRA.2021.3139369
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L, 2020, IEEE T IMAGE PROCESS, V29, P9532, DOI 10.1109/TIP.2020.3028207
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Thoker FM, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1655, DOI 10.1145/3474085.3475307
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang HR, 2022, IEEE T CIRC SYST VID, V32, P3050, DOI 10.1109/TCSVT.2021.3098839
   Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339
   Wei C, 2019, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2019.00201
   Wu HB, 2022, IEEE T CIRC SYST VID, V32, P1250, DOI [10.1109/TAI.2021.3092698, 10.1109/TCSVT.2021.3077512]
   Xia L., 2012, IEEE COMP SOC C COMP, P20, DOI DOI 10.1109/CVPRW.2012.6239233
   Xin Li, 2021, 2021 International Conference on Signal Processing and Machine Learning (CONF-SPML), P239, DOI 10.1109/CONF-SPML54095.2021.00053
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang Y, 2022, IEEE T CIRC SYST VID, V32, P8623, DOI 10.1109/TCSVT.2022.3194350
   You W, 2022, IEEE ACCESS, V10, P36385, DOI 10.1109/ACCESS.2022.3165040
   Zhang PF, 2019, IEEE T PATTERN ANAL, V41, P1963, DOI 10.1109/TPAMI.2019.2896631
   Zheng NG, 2018, AAAI CONF ARTIF INTE, P2644
   Zhou JX, 2022, COMPUT VIS IMAGE UND, V222, DOI 10.1016/j.cviu.2022.103491
   Zhu KJ, 2020, IEEE T MULTIMEDIA, V22, P2977, DOI 10.1109/TMM.2019.2962304
NR 41
TC 0
Z9 0
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1564
EP 1574
DI 10.1109/TMM.2023.3253048
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA G2GW2
UT WOS:000987415000016
DA 2024-07-18
ER

PT J
AU Zhang, CJ
   Bai, HH
   Zhao, Y
AF Zhang, Chunjie
   Bai, Huihui
   Zhao, Yao
TI Fine-Grained Image Classification by Class and Image-Specific
   Decomposition With Multiple Views
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Class-specific decomposition; fine-grained image classification;
   image-specific decomposition; multiview analysis
ID CONVOLUTIONAL NEURAL-NETWORK; LOW-RANK; FEATURES
AB Fine-grained image classification attempts to accurately classify images that are similar to each other. Multiview information is often used to improve the classification accuracy. Although great progress has been made, fine-grained image classification methods still have two drawbacks. On the one hand, they often treat each image independently without considering image correlations within the same class along with the distinctive characters of each image. On the other hand, multiview correlations are often used during classifier training, leaving the correlations of different views unconsidered. To solve these two problems, in this paper, we propose a novel fine-grained image classification method by class and image-specific decomposition with multiviews (CISD-MV). For each view, we treat images of the same class jointly by decomposing the class and image-specific information. Since images of different classes are similar and correlated, we linearly model class correlations of images using decomposed low-rank parts. In addition, for each image, the representations of different views are correlated, and we use linear transformation to model view correlations. We jointly optimise for the class and image-specific components along with the class correlation and view correlation transformation matrixes. A testing image is assigned to the class that has the minimum summed reconstruction error. We conduct fine-grained image classification experiments on several public fine-grained image datasets. Experimental results and analysis show the effectiveness of the proposed method.
C1 [Zhang, Chunjie; Bai, Huihui; Zhao, Yao] Beijing Jiao Tong Univ, Inst Informat Sci, Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.
RP Zhang, CJ (corresponding author), Beijing Jiao Tong Univ, Inst Informat Sci, Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.
EM ivazhangchunjie@gmail.com; hhbai@bjtu.edu.cn; yzhao@bjtu.edu.cn
OI zhang, chunjie/0000-0002-1161-8995; Bai, Huihui/0000-0002-3879-8957;
   Zhao, Yao/0000-0002-8581-9554
FU Fundamental Research Funds for the Central Universities [2022JBMC054];
   Beijing Natural Science Foundation [JQ20022, L223022]; National Natural
   Science Foundation of China [62072026, 61972023, U1936212, 62120106009];
   Open Research Fund of the State Key Laboratory for Management and
   Control of Complex Systems [20220103]; Open Project of Anhui Provincial
   Key Laboratory of Multimodal Cognitive Computation, Anhui University
   [MMC202103]; Key Laboratory Foundation of Information Perception and
   Systems for Public Security of MIIT, Nanjing University of Science and
   Technology) [202005]; Chinese Association for Artificial Intelligence
   (CAAI)-Huawei Mind Spore Open Fund [CAAIXSJLJJ-2021-062A]
FX This work was supported in part by the Fundamental Research Funds for
   the Central Universities under Grant 2022JBMC054, in part by the Beijing
   Natural Science Foundation under Grant JQ20022, L223022, in part by the
   National Natural Science Foundation of China under Grants 62072026,
   61972023, U1936212, and 62120106009, in part by the Open Research Fund
   of the State Key Laboratory for Management and Control of Complex
   Systems under Grant 20220103, in part by the Open Project of Anhui
   Provincial Key Laboratory of Multimodal Cognitive Computation, Anhui
   University under Grant MMC202103, in part by the Key Laboratory
   Foundation of Information Perception and Systems for Public Security of
   MIIT, Nanjing University of Science and Technology) under Grant 202005,
   and in part by the Chinese Association for Artificial Intelligence
   (CAAI)-Huawei MindSpore Open Fund under Grant CAAIXSJLJJ-2021-062A. The
   Associate Editor coordinating the review of this manuscript and
   approving it for publication was Prof.Ngai-Man (Man) Cheung.
CR Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   [Anonymous], 2014, P BRIT MACH VIS C, DOI 10.5244/C.28.87
   Bai C, 2021, IEEE T MULTIMEDIA, V23, P2199, DOI 10.1109/TMM.2021.3065578
   Bai C, 2018, NEUROCOMPUTING, V303, P60, DOI 10.1016/j.neucom.2018.04.034
   Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598
   Cai Q, 2018, PROC CVPR IEEE, P4080, DOI 10.1109/CVPR.2018.00429
   Cai X, 2013, IEEE I CONF COMP VIS, P1737, DOI 10.1109/ICCV.2013.218
   Chen L, 2018, PROC CVPR IEEE, P1043, DOI 10.1109/CVPR.2018.00115
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Choi J, 2018, PROC CVPR IEEE, P3627, DOI 10.1109/CVPR.2018.00382
   Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007
   Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432
   Cui Y, 2016, PROC CVPR IEEE, P1153, DOI 10.1109/CVPR.2016.130
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Ding YF, 2021, IEEE T IMAGE PROCESS, V30, P2826, DOI 10.1109/TIP.2021.3055617
   Ding ZM, 2015, IEEE T IMAGE PROCESS, V24, P4322, DOI 10.1109/TIP.2015.2462023
   Donahue J, 2014, PR MACH LEARN RES, V32
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Ge WF, 2018, PROC CVPR IEEE, P1277, DOI 10.1109/CVPR.2018.00139
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XT, 2019, IEEE T CIRC SYST VID, V29, P1394, DOI 10.1109/TCSVT.2018.2834480
   He XT, 2017, PROC CVPR IEEE, P7332, DOI 10.1109/CVPR.2017.775
   Hui Z, 2017, IEEE T PATTERN ANAL, V39, P2060, DOI 10.1109/TPAMI.2016.2623613
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Meyer BJ, 2018, Arxiv, DOI arXiv:1705.09780
   Jaderberg Max, 2015, ADV NEURAL INFORM PR, P8
   Karpathy A, 2017, IEEE T PATTERN ANAL, V39, P664, DOI 10.1109/TPAMI.2016.2598339
   Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwitt R, 2012, LECT NOTES COMPUT SC, V7575, P359
   Lam M, 2017, PROC CVPR IEEE, P6497, DOI 10.1109/CVPR.2017.688
   Li JJ, 2017, IEEE T CYBERNETICS, V47, P3516, DOI 10.1109/TCYB.2016.2565898
   Li L.-j., 2010, NIPS
   Li S, 2016, IEEE T NEUR NET LEAR, V27, P2160, DOI 10.1109/TNNLS.2015.2464090
   Li W, 2018, IEEE T PATTERN ANAL, V40, P1114, DOI 10.1109/TPAMI.2017.2704624
   Li XX, 2021, IEEE T IMAGE PROCESS, V30, P1318, DOI 10.1109/TIP.2020.3043128
   Lin D, 2022, IEEE T NEUR NET LEAR, V33, P200, DOI 10.1109/TNNLS.2020.3027603
   Lin TY, 2018, IEEE T PATTERN ANAL, V40, P1309, DOI 10.1109/TPAMI.2017.2723400
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu X, 2017, AAAI CONF ARTIF INTE, P4190
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Moghimi M., 2016, BMVC, P1, DOI 10.5244/C.30.24
   Ng, 2007, ADV NEURAL INF PROCE, P801
   Niu L, 2018, PROC CVPR IEEE, P7171, DOI 10.1109/CVPR.2018.00749
   Peng J, 2018, IEEE T NEUR NET LEAR, V29, P657, DOI 10.1109/TNNLS.2016.2637881
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sarafianos N, 2018, LECT NOTES COMPUT SC, V11215, P708, DOI 10.1007/978-3-030-01252-6_42
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song J, 2018, LECT NOTES COMPUT SC, V11213, P474, DOI 10.1007/978-3-030-01240-3_29
   Sun M, 2018, LECT NOTES COMPUT SC, V11220, P834, DOI 10.1007/978-3-030-01270-0_49
   Szegedy C., 2015, P IEEE C COMP VIS PA, P1
   Tang JJ, 2018, IEEE T NEUR NET LEAR, V29, P3463, DOI 10.1109/TNNLS.2017.2728139
   Tao H, 2017, IEEE T IMAGE PROCESS, V26, P4283, DOI 10.1109/TIP.2017.2717191
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang Y, 2018, IEEE T NEUR NET LEAR, V29, P4833, DOI 10.1109/TNNLS.2017.2777489
   Wu J, 2018, IEEE T NEUR NET LEAR, V29, P3236, DOI 10.1109/TNNLS.2017.2703832
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Xu Z, 2018, IEEE T PATTERN ANAL, V40, P1100, DOI 10.1109/TPAMI.2016.2637331
   Yu J, 2017, IEEE T CYBERNETICS, V47, P4014, DOI 10.1109/TCYB.2016.2591583
   Yu YL, 2018, IEEE T CYBERNETICS, V48, P2908, DOI 10.1109/TCYB.2017.2751741
   Zhang CH, 2017, IEEE T IMAGE PROCESS, V26, P2604, DOI 10.1109/TIP.2017.2675205
   Zhang CJ, 2018, INFORM SCIENCES, V460, P115, DOI 10.1016/j.ins.2018.05.048
   Zhang CJ, 2019, IEEE T CYBERNETICS, V49, P3834, DOI 10.1109/TCYB.2018.2845912
   Zhang CJ, 2018, IEEE T NEUR NET LEAR, V29, P4479, DOI 10.1109/TNNLS.2017.2748952
   Zhang CJ, 2018, IEEE T MULTIMEDIA, V20, P903, DOI 10.1109/TMM.2017.2759500
   Zhang CJ, 2018, IEEE T CIRC SYST VID, V28, P428, DOI 10.1109/TCSVT.2016.2613125
   Zhang CJ, 2018, IEEE T NEUR NET LEAR, V29, P3442, DOI 10.1109/TNNLS.2017.2728060
   Zhang CJ, 2018, IEEE T CYBERNETICS, V48, P2012, DOI 10.1109/TCYB.2017.2726079
   Zhang CJ, 2018, INFORM SCIENCES, V422, P271, DOI 10.1016/j.ins.2017.09.024
   Zhang CJ, 2017, IEEE T CIRC SYST VID, V27, P1691, DOI 10.1109/TCSVT.2016.2527380
   Zhang CJ, 2017, IEEE T NEUR NET LEAR, V28, P1550, DOI 10.1109/TNNLS.2016.2545112
   Zhang CJ, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2485783
   Zhang CJ, 2014, COMPUT VIS IMAGE UND, V123, P14, DOI 10.1016/j.cviu.2014.02.013
   Zhang CJ, 2013, PATTERN RECOGN LETT, V34, P1046, DOI 10.1016/j.patrec.2013.02.013
   Zhang CJ, 2011, PROC CVPR IEEE, P1673, DOI 10.1109/CVPR.2011.5995484
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zhu FD, 2017, IEEE T IMAGE PROCESS, V26, P3542, DOI 10.1109/TIP.2017.2703099
NR 83
TC 4
Z9 4
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6756
EP 6766
DI 10.1109/TMM.2022.3214431
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000004
DA 2024-07-18
ER

PT J
AU Zhao, WD
   Wei, F
   Wang, HP
   He, Y
   Lu, HC
AF Zhao, Wenda
   Wei, Fei
   Wang, Haipeng
   He, You
   Lu, Huchuan
TI Full-Scene Defocus Blur Detection With DeFBD plus via Multi-Level
   Distillation Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Robustness; Degradation; Interference; Training;
   Testing; Spatial coherence; Full-scene defocus blur detection; local
   feature representation; global content perception; isomeric distillation
   mechanism
AB Existing defocus blur detection (DBD) methods generally perform well on a single type of unfocused blur scene (e.g., foreground focus), thereby suffering from the performance degradation for the other types of unfocused blur scenes. In this paper, we present the first exploration on full-scene DBD, and propose a separate-and-combine framework to achieve excellent performance for diverse defocus blur scenes. We firstly structure full-scene DBD dataset (named as DeFBD+) through collecting more types of unfocused blur scenes (e.g., background focus, full focus and full out of focus) with pixel-level annotations. Then, to avoid performance degradation caused by mutual interference from local feature representation and global content perception, we implement a pixel-level DBD network and an image-level DBD classification network to learn these two abilities separately. After that, we propose an isomeric distillation mechanism to combine these two abilities. Extensive experiments show that the proposed approach achieves superior performance compared with state-of-the-art methods.
C1 [Zhao, Wenda; Wei, Fei; Lu, Huchuan] Dalian Univ Technol, Key Lab Intelligent Control & Optimizat Ind Equipm, Minist Educ, Dalian 116024, Peoples R China.
   [Zhao, Wenda; Wei, Fei; Lu, Huchuan] Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Peoples R China.
   [Wang, Haipeng; He, You] Naval Aviat Univ, Res Inst Informat Fus, Yantai 264001, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology
RP Wang, HP (corresponding author), Naval Aviat Univ, Res Inst Informat Fus, Yantai 264001, Peoples R China.
EM zhaowenda@dlut.edu.cn; fwei@mail.dlut.edu.cn; whp5691@163.com;
   heyou_f@126.com; lhchuan@dlut.edu.cn
RI LU, Jia-Hong/X-1395-2019; Wang, haipeng/R-3809-2016
OI LU, Jia-Hong/0000-0002-1147-125X; 
FU National Natural Science Foundation of China
FX No Statement Available
CR Angtian Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12642, DOI 10.1109/CVPR42600.2020.01266
   Chen ZH, 2020, PROC CVPR IEEE, P5610, DOI 10.1109/CVPR42600.2020.00565
   Dong X., 2020, P IEEE C COMPUTER VI, P12895
   Golestaneh SA, 2017, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2017.71
   Guo MH, 2020, PROC CVPR IEEE, P628, DOI 10.1109/CVPR42600.2020.00071
   Gyumin Shim, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8422, DOI 10.1109/CVPR42600.2020.00845
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He T, 2019, PROC CVPR IEEE, P578, DOI 10.1109/CVPR.2019.00067
   Hendrycks D, 2019, ADV NEUR IN, V32
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Jiang ZY, 2022, IEEE T IMAGE PROCESS, V31, P3494, DOI 10.1109/TIP.2022.3171424
   Kaneko T., 2020, P IEEECVF C COMPUTER, P8404
   Karaali A, 2018, IEEE T IMAGE PROCESS, V27, P1126, DOI 10.1109/TIP.2017.2771563
   Kim B, 2018, COMPUT GRAPH FORUM, V37, P277, DOI 10.1111/cgf.13567
   Lee J, 2019, PROC CVPR IEEE, P12214, DOI 10.1109/CVPR.2019.01250
   Li QF, 2020, PROC CVPR IEEE, P7243, DOI 10.1109/CVPR42600.2020.00727
   Liang CH, 2022, IEEE T MULTIMEDIA, V24, P61, DOI 10.1109/TMM.2020.3045303
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu YF, 2019, PROC CVPR IEEE, P2599, DOI 10.1109/CVPR.2019.00271
   Ma HY, 2022, IEEE T IMAGE PROCESS, V31, P216, DOI 10.1109/TIP.2021.3127850
   Ning Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P617, DOI 10.1007/978-3-030-58607-2_36
   Pang YW, 2016, IEEE T CYBERNETICS, V46, P2220, DOI 10.1109/TCYB.2015.2472478
   Park J., 2017, PROC IEEE C COMPUT V, P1736
   Raff E, 2019, PROC CVPR IEEE, P6521, DOI 10.1109/CVPR.2019.00669
   Robinson Andreas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7404, DOI 10.1109/CVPR42600.2020.00743
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi JP, 2015, PROC CVPR IEEE, P657, DOI 10.1109/CVPR.2015.7298665
   Shi JP, 2014, PROC CVPR IEEE, P2965, DOI 10.1109/CVPR.2014.379
   Shi ZL, 2018, PROC CVPR IEEE, P5382, DOI 10.1109/CVPR.2018.00564
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su B, 2011, Proceedings of the 19th ACM International Conference on Multimedia. MM'11, DOI [DOI 10.1145/2072298.2072024, DOI 10.5555/1785794.1785825]
   Tang C, 2020, AAAI CONF ARTIF INTE, V34, P12063
   Tang C, 2021, IEEE T MULTIMEDIA, V23, P624, DOI 10.1109/TMM.2020.2985541
   Tang C, 2022, IEEE T PATTERN ANAL, V44, P955, DOI 10.1109/TPAMI.2020.3014629
   Tang C, 2019, PROC CVPR IEEE, P2695, DOI 10.1109/CVPR.2019.00281
   Tang C, 2016, IEEE SIGNAL PROC LET, V23, P1652, DOI 10.1109/LSP.2016.2611608
   Xiaodong Cun, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P747, DOI 10.1007/978-3-030-58601-0_44
   Xu GD, 2017, IEEE I CONF COMP VIS, P5381, DOI 10.1109/ICCV.2017.574
   Yi X, 2016, IEEE T IMAGE PROCESS, V25, P1626, DOI 10.1109/TIP.2016.2528042
   Yu L, 2019, PROC CVPR IEEE, P2902, DOI 10.1109/CVPR.2019.00302
   Zeng K, 2019, IEEE T IMAGE PROCESS, V28, P2107, DOI 10.1109/TIP.2018.2881830
   Zhang L, 2021, IEEE T PATTERN ANAL, V43, P982, DOI 10.1109/TPAMI.2019.2943860
   Zhang LF, 2020, PROC CVPR IEEE, P369, DOI 10.1109/CVPR42600.2020.00045
   Zhang XX, 2021, IEEE T MULTIMEDIA, V23, P3215, DOI 10.1109/TMM.2020.3021989
   Zhang ZF, 2019, PROC CVPR IEEE, P7974, DOI 10.1109/CVPR.2019.00817
   Zhao F, 2023, IEEE T MULTIMEDIA, V25, P966, DOI 10.1109/TMM.2021.3134565
   Zhao F, 2022, IEEE T CIRC SYST VID, V32, P2719, DOI 10.1109/TCSVT.2021.3095347
   Zhao WD, 2022, LECT NOTES COMPUT SC, V13690, P569, DOI 10.1007/978-3-031-20056-4_33
   Zhao WD, 2021, PROC CVPR IEEE, P6929, DOI 10.1109/CVPR46437.2021.00686
   Zhao WD, 2019, PROC CVPR IEEE, P8897, DOI 10.1109/CVPR.2019.00911
   Zhao WD, 2020, IEEE T IMAGE PROCESS, V29, P1356, DOI 10.1109/TIP.2019.2942505
   Zhao WD, 2020, IEEE T PATTERN ANAL, V42, P1884, DOI 10.1109/TPAMI.2019.2906588
   Zhao WD, 2018, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2018.00325
   Zheng S, 2016, PROC CVPR IEEE, P4480, DOI 10.1109/CVPR.2016.485
   Zhong YJ, 2020, PROC CVPR IEEE, P6826, DOI 10.1109/CVPR42600.2020.00686
NR 55
TC 3
Z9 3
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9228
EP 9240
DI 10.1109/TMM.2023.3248162
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200025
DA 2024-07-18
ER

PT J
AU Zhou, Z
   Sun, QS
   Li, HJ
   Li, CB
   Ren, ZW
AF Zhou, Ze
   Sun, Quansen
   Li, Hongjun
   Li, Chaobo
   Ren, Zhenwen
TI Regression-Selective Feature-Adaptive Tracker for Visual Object Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visual object tracking; Refined criterion; Adaptive receptive field;
   Centrality; Online update
AB As a challenging visual task, visual object tracking has recently been composed of the classification and regression subtasks. The anchor-free regression network gets rid of the dependence on the anchors, but the redundant range makes it usually regress some samples involving non-target information. Evenly dividing a target by the regular receptive field often causes ambiguous target localization. To address these issues, we propose a regression-selective feature-adaptive tracker (RSFA), where the regression-selective subnetwork can not only free the regression task from anchors, but can also select more effective regression samples using the refined criterion. The proposed feature-adaptive strategy concentrates the classification subnetwork on target feature extraction via adaptively modifying the receptive field, and the attached centrality branch offers a correction for target localization by exploiting the spatial information. Additionally, the designed online update mechanism realizes the tracker's online optimization, improving robustness against target deformation. Extensive experiments are conducted on challenging benchmarks, including GOT10 K, OTB2015, UAV123, NFS, VOT2018, VOT2019 and VOT2020-ST. Our tracker achieves satisfactory tracking results, and the evaluations of its tracking performance rank first or second in comparison with the state-of-the-art tracking algorithms.
C1 [Zhou, Ze; Sun, Quansen] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China.
   [Li, Hongjun; Li, Chaobo] Nantong Univ, Sch Informat Sci & Technol, Nantong 226019, Jiangsu, Peoples R China.
   [Ren, Zhenwen] Southwest Univ Sci & Technol, Sch Natl Def Sci & Technol, Mianyang 621010, Peoples R China.
C3 Nanjing University of Science & Technology; Nantong University;
   Southwest University of Science & Technology - China
RP Sun, QS (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China.
EM zhoumze@163.com; sunquansen@njust.edu.cn; lihongjun@ntu.edu.cn;
   1811310007@yjs.ntu.edu.cn; rzw@njust.edu.cn
OI li, hongjun/0000-0001-7500-4979; REN, ZHEN WEN/0000-0003-3791-9750;
   Zhou, Ze/0000-0001-7371-562X; Li, Chaobo/0000-0003-3772-3344
FU National Science Foundation of China [61802188, 61673220, 61976028];
   Natural Science Foundation of Jiangsu Province [BK20180458]; Nantong
   Science and Technology Program [JC2021131]
FX This article was produced by the IEEE Publication Technology Group. They
   are in Piscataway, NJ. This work was supported in part by the National
   Science Foundation of China under Grants 61802188, 61673220, and
   61976028, in part by the Natural Science Foundation of Jiangsu Province
   under Grant BK20180458, and in part by Nantong Science and Technology
   Program under Grant JC2021131.
CR Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat Goutam, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P205, DOI 10.1007/978-3-030-58592-1_13
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Bhat G, 2018, LECT NOTES COMPUT SC, V11206, P493, DOI 10.1007/978-3-030-01216-8_30
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2020, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR42600.2020.00721
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2017.128
   Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Huang B, 2020, IEEE T MULTIMEDIA, V22, P2820, DOI 10.1109/TMM.2020.2965482
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Jiang BR, 2018, LECT NOTES COMPUT SC, V11218, P816, DOI 10.1007/978-3-030-01264-9_48
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Lan XY, 2019, IEEE T IND ELECTRON, V66, P9887, DOI 10.1109/TIE.2019.2898618
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li HJ, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108173
   Li PX, 2019, IEEE I CONF COMP VIS, P6161, DOI 10.1109/ICCV.2019.00626
   Li X, 2019, PROC CVPR IEEE, P1369, DOI 10.1109/CVPR.2019.00146
   Liao JW, 2021, IEEE T MULTIMEDIA, V23, P3346, DOI 10.1109/TMM.2020.3023794
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Linyu Zheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P759, DOI 10.1007/978-3-030-58555-6_45
   Lukezic A, 2020, PROC CVPR IEEE, P7131, DOI 10.1109/CVPR42600.2020.00716
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Real E, 2017, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR.2017.789
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sheng H, 2020, IEEE T CIRC SYST VID, V30, P2971, DOI 10.1109/TCSVT.2020.2988649
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Soon FC, 2020, IEEE T VEH TECHNOL, V69, P8267, DOI 10.1109/TVT.2020.3000306
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Tian SJ, 2021, IEEE T MULTIMEDIA, V23, P120, DOI 10.1109/TMM.2020.2978636
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661
   Vu T., 2019, ADV NEUR IN, P1
   Wang N, 2019, PROC CVPR IEEE, P1308, DOI 10.1109/CVPR.2019.00140
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xu TY, 2021, INT J COMPUT VISION, V129, P1359, DOI 10.1007/s11263-021-01435-1
   Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028
   Yang PT, 2020, IEEE T VEH TECHNOL, V69, P14355, DOI 10.1109/TVT.2020.3031900
   Yu J., 2016, P 24 ACM INT C MULT, P516, DOI DOI 10.1145/2964284.2967274
   Yuan YX, 2006, J COMPUT MATH, V24, P149
   Zhang ZP, 2021, IEEE T IMAGE PROCESS, V30, P8553, DOI 10.1109/TIP.2021.3117077
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
   Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
NR 59
TC 3
Z9 3
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5444
EP 5457
DI 10.1109/TMM.2022.3192775
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300059
DA 2024-07-18
ER

PT J
AU Zhu, CC
   Li, XQ
   Li, JD
   Dai, SM
   Tong, WQ
AF Zhu, Congcong
   Li, Xiaoqiang
   Li, Jide
   Dai, Songmin
   Tong, Weiqin
TI Multi-Sourced Knowledge Integration for Robust Self-Supervised Facial
   Landmark Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Shape; Detectors; Faces; Robustness; Annotations; Semantics;
   Biostatistics; facial landmark tracking; knowledge integration;
   self-supervised learning
ID FACE ALIGNMENT; NETWORK
AB Expensive annotation costs significantly hinder the development of facial landmark tracking owing to the frame-by-frame labeling of dense landmarks. The most promising approach to address this problem is to develop a self-supervised tracker for large-scale unlabeled videos. However, existing self-supervised trackers trained using single-sourced knowledge are unstable under unconstrained environments. Herein, we propose multi-sourced knowledge integration (MSKI), a robust self-supervised tracking method. It integrates knowledge from multiple sources to provide supervisory signals, thereby improving the stability of the self-supervised tracker. Specifically, the proposed MSKI comprises two complementary modules: a temporal knowledge reasoning (TempRes) module and an interactive knowledge distillation (KnowDist) module. The TempRes module enforces the tracker to achieve cycle-consistent tracking, allowing the tracker to learn temporal correspondence based on the cycle-consistency of time. To exploit facial geometry knowledge against various occlusions, our tracker imposes a multi-level shape constraint over the structure of facial landmarks by leveraging adversarial shape learning, thereby enabling the tracking of occluded faces. Moreover, the tracker interacts with an initialization detector to further develop complementary knowledge via KnowDist. The KnowDist module distills the spatial and temporal knowledge provided by the detector and tracker to generate plausible labels automatically. Finally, these generated labels are utilized to fine-tune the detector, such that it provides high-quality initial landmarks for the cycle-consistent tracking of the tracker on unlabeled videos. The experimental results show that the proposed MSKI can stabilize the tracking trajectory and improve the robustness against various occlusions.
C1 [Zhu, Congcong] Yangzhou Univ, Coll Informat Engn, Yangzhou 225009, Jiangsu, Peoples R China.
   [Zhu, Congcong; Li, Xiaoqiang; Li, Jide; Dai, Songmin; Tong, Weiqin] Shanghai Univ, Sch Comp Engn & Sci, Shanghai 200444, Peoples R China.
C3 Yangzhou University; Shanghai University
RP Li, XQ (corresponding author), Shanghai Univ, Sch Comp Engn & Sci, Shanghai 200444, Peoples R China.
EM cczhu@yzu.edu.cn; xqli@shu.edu.cn; iavtvai@shu.edu.cn;
   laodar@shu.edu.cn; wqtong@shu.edu.cn
RI Zhu, Congcong/GVS-5977-2022
OI Zhu, Congcong/0000-0001-5146-222X; Li, Jide/0000-0002-0754-5842
FU Shanghai science and technology committee [21511100600]
FX This work was supported by Shanghai science and technology committee
   under Grant 21511100600. The Associate Editor coordinating the review of
   this manuscript and approving it for publication was Prof. Metin Sezgin.
CR [Anonymous], 2017, ICLR
   Asthana A, 2014, PROC CVPR IEEE, P1859, DOI 10.1109/CVPR.2014.240
   Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3
   Chandran P, 2020, PROC CVPR IEEE, P5860, DOI 10.1109/CVPR42600.2020.00590
   Chang CH, 2017, PROC CVPR IEEE, P3777, DOI 10.1109/CVPR.2017.402
   Dong XY, 2018, PROC CVPR IEEE, P360, DOI 10.1109/CVPR.2018.00045
   Feng ZH, 2018, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2018.00238
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Guo MH, 2018, LECT NOTES COMPUT SC, V11214, P783, DOI 10.1007/978-3-030-01249-6_47
   Hong ZB, 2013, IEEE I CONF COMP VIS, P649, DOI 10.1109/ICCV.2013.86
   Kim M, 2008, PROC CVPR IEEE, P1787
   Lin XX, 2019, IEEE T MULTIMEDIA, V21, P3053, DOI 10.1109/TMM.2019.2916455
   Liu H, 2020, IEEE T PATTERN ANAL, V42, P679, DOI 10.1109/TPAMI.2018.2885298
   Liu H, 2018, IEEE T PATTERN ANAL, V40, P2546, DOI 10.1109/TPAMI.2017.2734779
   Liu ZW, 2019, PROC CVPR IEEE, P3462, DOI 10.1109/CVPR.2019.00358
   Martins P, 2014, PROC CVPR IEEE, P1797, DOI 10.1109/CVPR.2014.232
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Mo HY, 2019, IEEE T MULTIMEDIA, V21, P943, DOI 10.1109/TMM.2018.2867262
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Peng X, 2016, LECT NOTES COMPUT SC, V9905, P38, DOI 10.1007/978-3-319-46448-0_3
   Peng X, 2015, IEEE I CONF COMP VIS, P3880, DOI 10.1109/ICCV.2015.442
   Radosavovic I, 2018, PROC CVPR IEEE, P4119, DOI 10.1109/CVPR.2018.00433
   Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218
   Romero A., 2015, ICLR, P1, DOI DOI 10.48550/ARXIV.1412.6550
   SABIDUSSI G, 1966, PSYCHOMETRIKA, V31, P581, DOI 10.1007/BF02289527
   Sagonas C, 2014, PROC CVPR IEEE, P1789, DOI 10.1109/CVPR.2014.231
   Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59
   Samuli L., 2017, P INT C LEARN REPR
   Sánchez-Lozano E, 2016, LECT NOTES COMPUT SC, V9912, P645, DOI 10.1007/978-3-319-46484-8_39
   Saragih JM, 2011, INT J COMPUT VISION, V91, P200, DOI 10.1007/s11263-010-0380-4
   Shen J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P1003, DOI 10.1109/ICCVW.2015.132
   Sun KQ, 2019, IEEE I CONF COMP VIS, P5461, DOI 10.1109/ICCV.2019.00556
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Tai Y, 2019, AAAI CONF ARTIF INTE, P8893
   Trigeorgis G, 2016, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2016.453
   Tzimiropoulos G, 2015, PROC CVPR IEEE, P3659, DOI 10.1109/CVPR.2015.7298989
   Wang XL, 2019, PROC CVPR IEEE, P2561, DOI 10.1109/CVPR.2019.00267
   Wang ZY, 2020, Arxiv, DOI arXiv:2003.09093
   Wolf L, 2011, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2011.5995566
   Wu WY, 2018, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2018.00227
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yin S, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1010
   Yin S, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2991, DOI 10.1145/3394171.3413547
   Yuan L, 2020, IEEE T MULTIMEDIA, V22, P2711, DOI 10.1109/TMM.2019.2959451
   Zhang J, 2014, LECT NOTES COMPUT SC, V8690, P1, DOI 10.1007/978-3-319-10605-2_1
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhang ZP, 2016, IEEE T PATTERN ANAL, V38, P918, DOI 10.1109/TPAMI.2015.2469286
   Zhu CC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4135, DOI 10.1145/3394171.3413993
   Zhu CC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11731, DOI 10.1109/ICCV48922.2021.01154
   Zhu CC, 2020, AAAI CONF ARTIF INTE, V34, P13090
   Zhu HY, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107354
   Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
NR 54
TC 2
Z9 2
U1 1
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6616
EP 6628
DI 10.1109/TMM.2022.3212265
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500070
DA 2024-07-18
ER

PT J
AU Zou, LM
   Li, J
   Wan, WB
   Wu, QMJ
   Sun, JD
AF Zou, Liming
   Li, Jing
   Wan, Wenbo
   Wu, Q. M. Jonathan
   Sun, Jiande
TI Robust Coverless Image Steganography Based on Neglected Coverless Image
   Dataset Construction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Coverless image dataset; coverless image steganography; efficient
   construction; high robustness
ID PIXEL-VALUE; WATERMARKING; CAPACITY; DCT
AB Most of the existing image selection-based coverless image steganography methods mainly focus on improving the capacity and robustness under the assumption that the corresponding dataset is available. But they ignore how to successfully construct the coverless image dataset, which is the foundation of such methods and has a critical impact on the capacity. In this paper, a coverless image steganography is proposed that considers how to efficiently construct the coverless image dataset. In the proposed method, the CNN-based deep hash is extracted from the image and a specific mapping rule is designed to map the high-dimensional deep hash to the low-dimensional secret message. In addition, an unsupervised clustering algorithm is adopted to construct the coverless image dataset, which makes the construction of the coverless image dataset efficient and improves the robustness of the proposed steganography method. To our best knowledge, this is the first attempt to improve the construction efficiency of the coverless image dataset in the field of coverless image steganography. Experimental results show that the construction of a large coverless image dataset is feasible and reliable, and the proposed method has better robustness and higher dataset utilization rate compared with the state-of-the-art methods.
C1 [Zou, Liming; Wan, Wenbo; Sun, Jiande] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Peoples R China.
   [Li, Jing] Shandong Normal Univ, Sch Journalism & Commun, Jinan 250358, Peoples R China.
   [Wu, Q. M. Jonathan] Univ Windsor, Dept Elect & Comp Engn, Windsor, ON N9B 3P4, Canada.
C3 Shandong Normal University; Shandong Normal University; University of
   Windsor
RP Li, J (corresponding author), Shandong Normal Univ, Sch Journalism & Commun, Jinan 250358, Peoples R China.
EM limingzou_sdnu@hotmail.com; lijingjdsun@hotmail.com;
   wanwenbo@sdnu.edu.cn; jwu@uwindsor.ca; jiandesun@hotmail.com
FU Scientific Research Leader Studio of Jinan [2021GXRC081]; Joint Project
   for Smart Computing of Shandong Natural Science Foundation
   [ZR2020LZH015]; Taishan Scholar Project of Shandong, China [ts20190924]
FX This work was supported in part by the Scientific Research Leader Studio
   of Jinan under Grant 2021GXRC081, in part by the Joint Project for Smart
   Computing of Shandong Natural Science Foundation under Grant
   ZR2020LZH015, and in part by the Taishan Scholar Project of Shandong,
   China under Grant ts20190924.
CR Barni M, 2000, IEEE T IMAGE PROCESS, V9, P1450, DOI 10.1109/83.855442
   Borges PVK, 2008, IEEE T MULTIMEDIA, V10, P1479, DOI 10.1109/TMM.2008.2007294
   Cao Y, 2020, EURASIP J IMAGE VIDE, V2020, DOI 10.1186/s13640-020-00524-4
   Chatfield K., 2014, Proc. Brit. Mach. Vis. Conf, p6:1
   Chen XY, 2022, IEEE T NETW SCI ENG, V9, P219, DOI 10.1109/TNSE.2020.3041529
   Chen XY, 2019, MATH BIOSCI ENG, V16, P4708, DOI 10.3934/mbe.2019236
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Duan X., 2020, J HENAN NORM UNIV, V48, P33, DOI DOI 10.16366/j.cnki.1000-2367.2020.01.006
   Duan XT, 2020, EURASIP J IMAGE VIDE, V2020, DOI 10.1186/s13640-020-00506-6
   Erfani Y, 2017, IEEE T INF FOREN SEC, V12, P840, DOI 10.1109/TIFS.2016.2636094
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Griffin Gregory, 2006, CALTECH TECHNICAL RE
   Guo LJ, 2014, IEEE T INF FOREN SEC, V9, P814, DOI 10.1109/TIFS.2014.2312817
   Huang FJ, 2016, IEEE T CIRC SYST VID, V26, P1610, DOI 10.1109/TCSVT.2015.2473235
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Joshi K, 2015, 2015 THIRD INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING (ICIIP), P86, DOI 10.1109/ICIIP.2015.7414745
   Kodovsky J, 2012, IEEE T INF FOREN SEC, V7, P432, DOI 10.1109/TIFS.2011.2175919
   Li Q, 2021, INFORM SCIENCES, V553, P19, DOI 10.1016/j.ins.2020.12.002
   Lin WH, 2008, IEEE T MULTIMEDIA, V10, P746, DOI 10.1109/TMM.2008.922795
   Liu Ming-ming, 2018, Journal of Applied Sciences - Electronics and Information Engineering, V36, P371, DOI 10.3969/j.issn.0255-8297.2018.02.015
   Liu Q, 2022, IEEE T CIRC SYST VID, V32, P4038, DOI 10.1109/TCSVT.2021.3108772
   Liu Q, 2020, KNOWL-BASED SYST, V192, DOI 10.1016/j.knosys.2019.105375
   Liu SC, 2012, IEEE T INF FOREN SEC, V7, P1448, DOI 10.1109/TIFS.2012.2204250
   Luo WQ, 2010, IEEE T INF FOREN SEC, V5, P201, DOI 10.1109/TIFS.2010.2041812
   Luo YJ, 2021, IEEE T CIRC SYST VID, V31, P2779, DOI 10.1109/TCSVT.2020.3033945
   Mielikainen J, 2006, IEEE SIGNAL PROC LET, V13, P285, DOI 10.1109/LSP.2006.870357
   Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964
   Peng F, 2022, IEEE T CIRC SYST VID, V32, P5817, DOI 10.1109/TCSVT.2022.3161419
   Ramkumar M, 2001, IEEE T IMAGE PROCESS, V10, P1252, DOI 10.1109/83.935040
   Sedighi V, 2016, IEEE T INF FOREN SEC, V11, P221, DOI 10.1109/TIFS.2015.2486744
   Sharma V, 2015, 2015 THIRD INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING (ICIIP), P486, DOI 10.1109/ICIIP.2015.7414821
   Stütz T, 2014, IEEE T MULTIMEDIA, V16, P1337, DOI 10.1109/TMM.2014.2310595
   Tang WX, 2019, IEEE T INF FOREN SEC, V14, P2074, DOI 10.1109/TIFS.2019.2891237
   Wu DC, 2003, PATTERN RECOGN LETT, V24, P1613, DOI 10.1016/S0167-8655(02)00402-6
   Xu GS, 2016, IEEE SIGNAL PROC LET, V23, P708, DOI 10.1109/LSP.2016.2548421
   Yang K., 2018, Proceedings of the International Workshop on Digital Watermarking, P55, DOI [10.1007/978-3-030-11389-65, DOI 10.1007/978-3-030-11389-65]
   Ye J, 2017, IEEE T INF FOREN SEC, V12, P2545, DOI 10.1109/TIFS.2017.2710946
   Yuan CS, 2017, J INTERNET TECHNOL, V18, P435, DOI 10.6138/JIT.2017.18.2.20160624c
   Zhang WM, 2016, IEEE T MULTIMEDIA, V18, P1469, DOI 10.1109/TMM.2016.2569497
   Zhang X, 2018, IEEE T MULTIMEDIA, V20, P3223, DOI 10.1109/TMM.2018.2838334
   Zhang XP, 2013, IEEE T MULTIMEDIA, V15, P316, DOI 10.1109/TMM.2012.2229262
   Zheng SL, 2017, LECT NOTES ARTIF INT, V10363, P536, DOI 10.1007/978-3-319-63315-2_47
   Zhili Zhou, 2015, Cloud Computing and Security. First International Conference, ICCCS 2015. Revised Selected Papers: LNCS 9483, P123, DOI 10.1007/978-3-319-27051-7_11
   Zhou ZL, 2022, Arxiv, DOI arXiv:2203.06598
   Zhou ZL, 2019, IEEE ACCESS, V7, P179891, DOI 10.1109/ACCESS.2019.2955990
   Zhou ZL, 2019, SOFT COMPUT, V23, P4927, DOI 10.1007/s00500-018-3151-8
   Zou LM, 2019, MULTIMED TOOLS APPL, V78, P7965, DOI 10.1007/s11042-018-6444-0
NR 50
TC 4
Z9 4
U1 5
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5552
EP 5564
DI 10.1109/TMM.2022.3194990
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300066
DA 2024-07-18
ER

PT J
AU Cao, QW
   Huang, HY
AF Cao, Qianwen
   Huang, Heyan
TI Attention Guided Relation Detection Approach for Video Visual Relation
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video visual relation detection; visual relation tagging; attention
   mechanism; neural network
ID TRACKING; RECOGNITION; NETWORK
AB Video Visual Relation Detection (VidVRD) aims at detecting the relation instances between two observed objects in the form of < subject-predicate-object >. Unlike image visual relation detection, due to the introduction of the time dimensions, the various predicates and spatial-temporal locations are both required to be tackled, making the task challenging. To balance these challenges, most existing works perform this task in two phases: first predicting relationships in segmented clips to capture the motions, and then associating them into the relation instances with proper locations in videos. These works detect different relationships by collecting the cues from multi-aspects, but treat them equally without distinction. Furthermore, due to the dynamic scenes and drifting problem in object tracking, the rigid spatial overlap used to determine the association in previous works is insufficient, which leads to missing associations. To address the problems, in this paper, we propose a novel attention guided relation detection approach for VidVRD. In order to model the distinction among different cues and strengthen the salient characteristics, we assign these cues the attention weights for relationship prediction and association decision-making. In addition, to comprehensively measure whether merging the relationships, we put forward a customized network to take both visual appearance and geometric location into account. Extensive experiment results on ImageNet-VidVRD dataset and VidOR dataset demonstrate the effectiveness of our proposed approach. And abundant ablation studies verify the component designed in the approach is essential.
C1 [Cao, Qianwen; Huang, Heyan] Beijing Inst Technol, Comp Sci, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Huang, HY (corresponding author), Beijing Inst Technol, Comp Sci, Beijing 100081, Peoples R China.
EM qwcao@bit.edu.cn; hhy63@bit.edu.cn
OI cao, qianwen/0000-0001-7491-5201
FU National Key RD Plan [2018YFB1005100]; NSFC [U19B2020, 61772076,
   61602197]; NSFB [Z181100008918002]
FX This work was supported in part by National Key R&D Plan under Grant
   2018YFB1005100, in part by NSFC under Grants U19B2020, 61772076, and
   61602197, and in part by NSFB under Grant Z181100008918002. The
   Associate Editor coordinating the review of this manuscript and
   approving it for publication was Prof. Jinhui Tang.
CR [Anonymous], 2016, SEQ NMS VIDEO OBJECT
   Cao QW, 2021, NEUROCOMPUTING, V432, P91, DOI 10.1016/j.neucom.2020.12.029
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Fan CY, 2019, PROC CVPR IEEE, P1999, DOI 10.1109/CVPR.2019.00210
   Franke U, 2005, LECT NOTES COMPUT SC, V3663, P216
   Gao JY, 2018, PROC CVPR IEEE, P6576, DOI 10.1109/CVPR.2018.00688
   Graves Alex, 2014, Generating sequences with recurrent neural networks
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hu TX, 2019, LECT NOTES COMPUT SC, V11824, P331, DOI 10.1007/978-3-030-33676-9_23
   Jiang MX, 2017, INT J COMPUT SCI ENG, V15, P330, DOI 10.1504/IJCSE.2017.10008138
   Jin L, 2019, IEEE T IMAGE PROCESS, V28, P2173, DOI 10.1109/TIP.2018.2883522
   Jing Wang, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P4337, DOI 10.1145/3394171.3413753
   Jonghwan Mun, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P6581, DOI 10.1109/CVPR.2019.00675
   Kipf TN, 2016, ARXIV
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Leal-Taixé L, 2016, IEEE COMPUT SOC CONF, P418, DOI 10.1109/CVPRW.2016.59
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li D, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P629, DOI 10.1145/3343031.3350978
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Li Z., ARXIV210409805, V2021
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu C., 2020, CVPR, P10840
   Liu S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1425, DOI 10.1145/3240508.3240667
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Luo WH, 2021, ARTIF INTELL-AMST, V293, DOI 10.1016/j.artint.2020.103448
   Plummer BA, 2017, IEEE I CONF COMP VIS, P1946, DOI 10.1109/ICCV.2017.213
   Qian XF, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P84, DOI 10.1145/3343031.3351058
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shang XD, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2652, DOI 10.1145/3343031.3356082
   Shang XD, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P279, DOI 10.1145/3323873.3325056
   Shang XD, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1300, DOI 10.1145/3123266.3123380
   SHIH FY, 1995, PATTERN RECOGN, V28, P331, DOI 10.1016/0031-3203(94)00104-T
   Son J, 2017, PROC CVPR IEEE, P3786, DOI 10.1109/CVPR.2017.403
   Su Z., 2020, IEEE T MULTIMEDIA, V23, P391
   Sun X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2657, DOI 10.1145/3343031.3356076
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Tian SJ, 2021, IEEE T MULTIMEDIA, V23, P120, DOI 10.1109/TMM.2020.2978636
   Tsai YHH, 2019, PROC CVPR IEEE, P10416, DOI 10.1109/CVPR.2019.01067
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wu XX, 2021, INT J COMPUT VISION, V129, P1484, DOI 10.1007/s11263-020-01409-9
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zhang J, 2017, PROC CVPR IEEE, P5226, DOI 10.1109/CVPR.2017.555
   Zhang Wenqiao, 2019, IEEE Transactions on Multimedia, V22, P1032
   Zheng SP, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P121, DOI 10.1145/3343031.3350962
   Zhu XZ, 2018, PROC CVPR IEEE, P7210, DOI 10.1109/CVPR.2018.00753
   Zhu XZ, 2017, IEEE I CONF COMP VIS, P408, DOI 10.1109/ICCV.2017.52
NR 51
TC 3
Z9 3
U1 5
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3896
EP 3907
DI 10.1109/TMM.2021.3109430
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 7F8JC
UT WOS:000902085900001
DA 2024-07-18
ER

PT J
AU Chang, SN
   Li, YC
   Shen, SM
   Feng, JS
   Zhou, ZY
AF Chang, Shuning
   Li, Yanchao
   Shen, Shengmei
   Feng, Jiashi
   Zhou, Zhiying
TI Contrastive Attention for Video Anomaly Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Anomaly detection; Training; Task analysis;
   Predictive models; Deep learning; Data models; Anomaly Detection;
   Contrastive Attention Module; Attention Consistency Loss
AB We consider weakly-supervised video anomaly detection in this work. This task aims to learn to localize video frames containing anomaly events with only binary video-level annotation, i.e., anomaly vs. normal. Traditional approaches usually formulate it as a multiple instance learning problem, which ignore the intrinsic data imbalance issue that positive samples are very scarce compared to negative ones. In this paper, we focus on addressing this issue to boost detection performance further. We develop a new light-weight anomaly detection model that fully utilizes enough normal videos to train a classifier with a good discriminative ability for normal videos, and we employ it to improve the selectivity for anomalous segments and filter out normal segments. Specifically, in addition to boosting anomalous prediction, a novel contrastive attention module additionally produces a converted normal feature from anomalous video to refined anomalous predictions by maximizing the classifier making a mistake. Moreover, to remove the stubborn normal segments selected by the attention module, we also design an attention consistency loss to employ the classifier with high confidence for normal features to guide the attention module. Extensive experiments on two large-scale datasets, UCF-Crime, ShanghaiTech and XD-Violence, clearly demonstrate that our model largely improves frame-level AUC over the state-of-the-art. Code is released at https://github.com/changsn/Contrastive-Attention-for-Video-Anomaly-Detection.
C1 [Chang, Shuning; Feng, Jiashi; Zhou, Zhiying] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
   [Li, Yanchao; Shen, Shengmei] Pensees, Singapore 138633, Singapore.
C3 National University of Singapore
RP Zhou, ZY (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
EM changshuning@u.nus.edu; xdln@outlook.com; shensm@gmail.com;
   elefjia@nus.edu.sg; elezzy@nus.edu.sg
RI Feng, Jiashi/AGX-6209-2022
OI , Shuning/0000-0001-5752-0128; Feng, Jiashi/0000-0001-6843-0064; Li,
   Yanchao/0000-0002-6291-0523
FU Key Laboratory Project of Publishing Science and Technology and
   Standards of the National Press and Publication Administration -Key
   Laboratory of Augmented Reality Integrated Publishing [BZ2021006]
FX Thisworkwas supported by the Key Laboratory Project of Publishing
   Science and Technology and Standards of the National Press and
   Publication Administration -Key Laboratory of Augmented Reality
   Integrated Publishing; BZ2021006. The Associate Editor coordinating the
   review of this manuscript and approving it for publicationwas Dr.
   TingYao. (Shuning Chang and Yanchao Li contributed equally to this
   work.)
CR Antic B, 2011, IEEE I CONF COMP VIS, P2415, DOI 10.1109/ICCV.2011.6126525
   Basharat A, 2008, PROC CVPR IEEE, P1301
   Bin Zhao, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3313, DOI 10.1109/CVPR.2011.5995524
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chaudhary Anshika, 2019, 2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon), P346, DOI 10.1109/COMITCon.2019.8862186
   Choe J, 2019, PROC CVPR IEEE, P2214, DOI 10.1109/CVPR.2019.00232
   Cui XY, 2011, PROC CVPR IEEE
   Datta A, 2002, INT C PATT RECOG, P433, DOI 10.1109/ICPR.2002.1044748
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fang ZW, 2021, IEEE T MULTIMEDIA, V23, P4106, DOI 10.1109/TMM.2020.3037538
   Gao Y, 2016, IMAGE VISION COMPUT, V48-49, P37, DOI 10.1016/j.imavis.2016.01.006
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86
   He CK, 2018, MULTIMED TOOLS APPL, V77, P29573, DOI 10.1007/s11042-017-5255-z
   Hospedales T, 2009, IEEE I CONF COMP VIS, P1165, DOI 10.1109/ICCV.2009.5459342
   Ilse M, 2018, PR MACH LEARN RES, V80
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kooij JFP, 2016, COMPUT VIS IMAGE UND, V144, P106, DOI 10.1016/j.cviu.2015.06.009
   Kratz L, 2009, PROC CVPR IEEE, P1446, DOI 10.1109/CVPRW.2009.5206771
   Lee S, 2020, IEEE T IMAGE PROCESS, V29, P2395, DOI 10.1109/TIP.2019.2948286
   Li WX, 2014, IEEE T PATTERN ANAL, V36, P18, DOI 10.1109/TPAMI.2013.111
   Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338
   Luo WX, 2017, IEEE I CONF COMP VIS, P341, DOI 10.1109/ICCV.2017.45
   Mengmeng Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10153, DOI 10.1109/CVPR42600.2020.01017
   Mohammadi S, 2016, LECT NOTES COMPUT SC, V9911, P3, DOI 10.1007/978-3-319-46478-7_1
   Paul S, 2018, LECT NOTES COMPUT SC, V11208, P588, DOI 10.1007/978-3-030-01225-0_35
   Peng Wu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P322, DOI 10.1007/978-3-030-58577-8_20
   Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706
   Nguyen PX, 2019, IEEE I CONF COMP VIS, P5501, DOI 10.1109/ICCV.2019.00560
   Sultani W, 2018, PROC CVPR IEEE, P6479, DOI 10.1109/CVPR.2018.00678
   Tung F, 2011, IMAGE VISION COMPUT, V29, P230, DOI 10.1016/j.imavis.2010.11.003
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan BY, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102722
   Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668
   Wu HB, 2020, IEEE T MULTIMEDIA, V22, P2293, DOI 10.1109/TMM.2019.2953814
   Wu SD, 2010, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2010.5539882
   Xu D, 2015, Arxiv, DOI [arXiv:1510.01553, DOI 10.48550/ARXIV.1510.01553]
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zaheer Muhammad Zaigham, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P358, DOI 10.1007/978-3-030-58542-6_22
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhong JX, 2019, PROC CVPR IEEE, P1237, DOI 10.1109/CVPR.2019.00133
   Zhu Yi, 2019, 30 BRIT MACH VIS C 2, P1
   Zhu YY, 2013, IEEE J-STSP, V7, P91, DOI 10.1109/JSTSP.2012.2234722
NR 44
TC 21
Z9 21
U1 2
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4067
EP 4076
DI 10.1109/TMM.2021.3112814
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400028
DA 2024-07-18
ER

PT J
AU Deng, LX
   Chen, JJ
   Ngo, CW
   Sun, QR
   Tang, S
   Zhang, YD
   Chua, TS
AF Deng, Lixi
   Chen, Jingjing
   Ngo, Chong-Wah
   Sun, Qianru
   Tang, Sheng
   Zhang, Yongdong
   Chua, Tat-Seng
TI Mixed Dish Recognition With Contextual Relation and Domain Alignment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Semantics; Feature extraction; Image recognition;
   Training; Testing; Context modeling; Mixed dish recognition; Contextual
   relation; Domain alignment
AB Mixed dish is a food category that contains different dishes mixed in one plate, and is popular in Eastern and Southeast Asia. Recognizing the individual dishes in a mixed dish image is important for health related applications, e.g. to calculate the nutrition values of the dish. However, most existing methods that focus on single dish classification are not applicable to the recognition of mixed dish images. The main challenge of mixed dish recognition comes from three aspects: a wide range of dish types, the complex dish combination with severe overlap between different dishes and the large visual variances of same dish type caused by different cooking/cutting methods applied in different canteens. In order to tackle these problems, we propose the contextual relation network that encodes the implicit and explicit contextual relations among multiple dishes from region-level features and label-level co-occurrence respectively. Besides, to address the visual variances of dish instances from different canteens, we introduce the domain adaption networks to align both local and global features, and eliminating domain gaps of dish features across different canteens. In addition, we collect a mixed dish image dataset containing 9254 mixed dish images from 6 canteens in Singapore. Extensive experiments on both our dataset and public one validate that our methods can achieve top performance for localizing and recognizing multiple dishes and solve the domain shift problem to a certain extent in mixed dish images.
C1 [Deng, Lixi; Tang, Sheng] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Deng, Lixi] JD Com, Beijing 100049, Peoples R China.
   [Deng, Lixi] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Chen, Jingjing] Fudan Univ, Shanghai Key Labortaory Intelligent Informat Proc, Sch Comp Sci, Shanghai 200433, Peoples R China.
   [Ngo, Chong-Wah; Sun, Qianru] Singapore Management Univ, Sch Comp & Informat Syst, Singapore 188065, Singapore.
   [Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230022, Peoples R China.
   [Chua, Tat-Seng] Natl Univ Singapore, Singapore 117543, Singapore.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Fudan University; Singapore Management University; Chinese Academy
   of Sciences; University of Science & Technology of China, CAS; National
   University of Singapore
RP Chen, JJ (corresponding author), Fudan Univ, Shanghai Key Labortaory Intelligent Informat Proc, Sch Comp Sci, Shanghai 200433, Peoples R China.
EM denglixi.cs@foxmail.com; chenjingjing@fudan.edu.cn; cwngo@smu.edu.sg;
   qianrusun@smu.edu.sg; ts@ict.ac.cn; zhyd73@ustc.edu.cn;
   chuats@comp.nus.edu.sg
RI chen, JJ/HGB-6029-2022; Tang, Sheng/L-5792-2013
OI Tang, Sheng/0000-0003-3573-2407
FU National Key Research and Development Program of China [2017YFB1002202];
   A*STAR under its AME YIRG [A20E6c0101]; National Natural Science
   Foundation of China [61871004, 2020A077]; Sea-NExT Joint Lab
FX This work as supported in part by the National Key Research and
   Development Program of China under Grant 2017YFB1002202, in part by the
   A*STAR under its AME YIRG under Grant A20E6c0101, in part by Sea-NExT
   Joint Lab, and in part by the National Natural Science Foundation of
   China 61871004, and 242 Project 2020A077.
CR Aguilar E, 2018, IEEE T MULTIMEDIA, V20, P3266, DOI 10.1109/TMM.2018.2831627
   [Anonymous], 2018, P INT C ART INT STEE
   [Anonymous], 2007, Ugm: A matlab toolbox for probabilistic undirected graphical models
   Ao S, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON DATA MINING WORKSHOP (ICDMW), P1196, DOI 10.1109/ICDMW.2015.203
   Beijbom O, 2015, IEEE WINT CONF APPL, P844, DOI 10.1109/WACV.2015.117
   Bettadapura V, 2015, IEEE WINT CONF APPL, P580, DOI 10.1109/WACV.2015.83
   Bochkovskiy A., 2020, PREPRINT
   Bolaños M, 2017, LECT NOTES COMPUT SC, V10590, P394, DOI 10.1007/978-3-319-70742-6_37
   Bolaños M, 2016, INT C PATT RECOG, P3140, DOI 10.1109/ICPR.2016.7900117
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Chen JJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1771, DOI 10.1145/3123266.3123428
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Chen M, 2009, IEEE IMAGE PROC, P289, DOI 10.1109/ICIP.2009.5413511
   Chen X., 2017, arXiv preprint arXiv:1705.02743
   Dehais J, 2016, MADIMA'16: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P23, DOI 10.1145/2986035.2986047
   Deng LX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P112, DOI 10.1145/3343031.3351147
   Ege T, 2018, P JOINT WORKSH MULT, P53, DOI [DOI 10.1145/3230519.3230594, 10.1145/3230519.3230594]
   Ege T, 2017, PROCEEDINGS 2017 4TH IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P646, DOI 10.1109/ACPR.2017.145
   Ege T, 2017, PROCEEDINGS OF THE FIFTEENTH IAPR INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS - MVA2017, P198, DOI 10.23919/MVA.2017.7986835
   Felzenszwalb PF, 2010, PROC CVPR IEEE, P2241, DOI 10.1109/CVPR.2010.5539906
   Fu ZH, 2017, LECT NOTES COMPUT SC, V10361, P273, DOI 10.1007/978-3-319-63309-1_25
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344
   Hassannejad H, 2016, MADIMA'16: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P41, DOI 10.1145/2986035.2986042
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Herranz L, 2017, IEEE T MULTIMEDIA, V19, P430, DOI 10.1109/TMM.2016.2614861
   Hoashi H., 2010, Proceedings 2010 IEEE International Symposium on Multimedia (ISM 2010), P296, DOI 10.1109/ISM.2010.51
   Horiguchi S, 2018, IEEE T MULTIMEDIA, V20, P2836, DOI 10.1109/TMM.2018.2814339
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Iscen Ahmet, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P286, DOI 10.1007/978-3-030-58607-2_17
   Kawano Y, 2014, PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP'14 ADJUNCT), P589, DOI 10.1145/2638728.2641339
   Kawano Y, 2015, LECT NOTES COMPUT SC, V8927, P3, DOI 10.1007/978-3-319-16199-0_1
   Kawano Y, 2013, IEEE COMPUT SOC CONF, P1, DOI 10.1109/CVPRW.2013.5
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu C, 2016, LECT NOTES COMPUT SC, V9677, P37, DOI 10.1007/978-3-319-39601-9_4
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Martinel N, 2018, IEEE WINT CONF APPL, P567, DOI 10.1109/WACV.2018.00068
   Matsuda Y., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P25, DOI 10.1109/ICME.2012.157
   Merler M, 2016, MADIMA'16: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P31, DOI 10.1145/2986035.2986036
   Ming ZY, 2018, LECT NOTES COMPUT SC, V10705, P129, DOI 10.1007/978-3-319-73600-6_12
   Myers A, 2015, IEEE I CONF COMP VIS, P1233, DOI 10.1109/ICCV.2015.146
   Pouladzadeh P, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3063592
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Robert C., 2013, MONTE CARLO STAT MET
   Saito K, 2019, PROC CVPR IEEE, P6949, DOI 10.1109/CVPR.2019.00712
   Shimoda W, 2016, MADIMA'16: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P13, DOI 10.1145/2986035.2986043
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Vaswani A, 2017, ADV NEUR IN, V30
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wang XH, 2023, IEEE T PATTERN ANAL, V45, P6605, DOI 10.1109/TPAMI.2020.3015894
   Xu RH, 2015, IEEE T MULTIMEDIA, V17, P1187, DOI 10.1109/TMM.2015.2438717
   Yang Y., COMPUTER VISION ECCV, V12372, DOI [10.1007/978- 3-030-58583- 9_21, DOI 10.1007/978-3-030-58583-9_21]
   Yosinski J, 2014, ADV NEUR IN, V27
   Yu Q, 2018, IEEE IMAGE PROC, P171, DOI 10.1109/ICIP.2018.8451422
   Zhang Weiyu, 2015, J Diabetes Sci Technol, V9, P525, DOI 10.1177/1932296815582222
   Zhu LC, 2022, IEEE T PATTERN ANAL, V44, P273, DOI 10.1109/TPAMI.2020.3007511
NR 60
TC 7
Z9 7
U1 2
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2034
EP 2045
DI 10.1109/TMM.2021.3075037
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200020
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Fang, H
   Chen, DD
   Wang, F
   Ma, ZH
   Liu, HG
   Zhou, WB
   Zhang, WM
   Yu, NH
AF Fang, Han
   Chen, Dongdong
   Wang, Feng
   Ma, Zehua
   Liu, Honggu
   Zhou, Wenbo
   Zhang, Weiming
   Yu, Nenghai
TI TERA: Screen-to-Camera Image Code With Transparency, Efficiency,
   Robustness and Adaptability
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Robustness; Cameras; Distortion; Visualization; Image coding; Decoding;
   Two dimensional displays; Adaptability; attention-guided; color
   decomposition; efficiency; robustness; screen-to-camera image code;
   transparency
AB With the rapid development of digital devices, the issue of how to transmit information among different devices with multimedia carriers has drawn much attention from the research community. This paper focuses on the important user scenario of "screen-to-camera information transmission". Along this direction, image coding-based techniques have been shown to be the most popular and effective methods in the past decades. However, after careful study, we find that none of the existing methods can satisfy the four important properties simultaneously, i.e., high transparency, high embedding efficiency, strong transmission robustness and high adaptability to device types. This is mainly because these properties are contradictory with each other. In this paper, we thus propose a screen-to-camera image code dubbed "TERA" (transparency, efficiency, robustness and adaptability), which makes it possible to circumvent the contradiction among the above four properties for the first time. Generally, TERA adopts the color decomposition principle to ensure the visual quality and the superposition-based scheme to ensure embedding efficiency. BCH-coding-based information arrangement and a powerful attention-guided information decoding network are further designed to guarantee the robustness and adaptability. Through extensive experiments, the superiority and broad applications of our method are demonstrated.
C1 [Fang, Han; Wang, Feng; Ma, Zehua; Liu, Honggu; Zhou, Wenbo; Zhang, Weiming; Yu, Nenghai] Univ Sci & Technol China, CAS Key Lab Electromagnet Space Informat, Hefei 230026, Peoples R China.
   [Chen, Dongdong] Microsoft Res, Redmond, WA 98052 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft
RP Zhang, WM; Yu, NH (corresponding author), Univ Sci & Technol China, CAS Key Lab Electromagnet Space Informat, Hefei 230026, Peoples R China.
EM fanghan@mail.ustc.edu.cn; cddlyf@gmail.com; nishi@mail.ustc.edu.cn;
   mzh045@mail.ustc.edu.cn; lhg9754@mail.ustc.edu.cn;
   welbeckz@mail.ustc.edu.cn; zhangwm@ustc.edu.cn; ynh@ustc.edu.cn
RI Liu, Honggu/HTN-2726-2023; fang, han/GSN-6404-2022; Chen,
   Dongdong/AAR-4481-2020
OI Liu, Honggu/0000-0001-9294-9624; Chen, Dongdong/0000-0002-7016-9288;
   Zhang, Weiming/0000-0001-5576-6108; Chen, Dongdong/0000-0002-4642-4373;
   Ma, Zehua/0000-0002-8153-341X
FU Natural Science Foundation of China [62072421, 62002334]; Anhui Science
   Foundation of China [2008085QF296]; Exploration Fund Project of
   University of Science and Technology of China [YD3480002001]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 62072421, and 62002334, in part by the Anhui Science
   Foundation of China under Grant 2008085QF296, and in part by the
   Exploration Fund Project of University of Science and Technology of
   China underGrant YD3480002001.
CR Bose R. C., 1960, Inf. Control, V3, P79, DOI DOI 10.1016/S0019-9958(60)90287-4
   Chen CS, 2018, IEEE T CIRC SYST VID, V28, P3300, DOI 10.1109/TCSVT.2017.2741472
   Chen CS, 2019, IEEE T IMAGE PROCESS, V28, P156, DOI 10.1109/TIP.2018.2865681
   Chen CS, 2016, IEEE T IMAGE PROCESS, V25, P3444, DOI 10.1109/TIP.2016.2573592
   Cui H, 2019, IEEE INFOCOM SER, P1315, DOI [10.1109/infocom.2019.8737627, 10.1109/INFOCOM.2019.8737627]
   Fang H, 2021, IEEE T CIRC SYST VID, V31, P1436, DOI 10.1109/TCSVT.2020.3009349
   Fang H, 2019, IEEE T INF FOREN SEC, V14, P1403, DOI 10.1109/TIFS.2018.2878541
   Garateguy GJ, 2014, IEEE T IMAGE PROCESS, V23, P2842, DOI 10.1109/TIP.2014.2321501
   Gugelmann D, 2018, INT CONF CYBER CONFL, P391, DOI 10.23919/CYCON.2018.8405027
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang Y, 2019, IEEE T MULTIMEDIA, V21, P2447, DOI 10.1109/TMM.2019.2907475
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Izz M, 2016, IEEE INFOCOM SER
   Kazemi R, 2016, IEEE T MULTIMEDIA, V18, P2345, DOI 10.1109/TMM.2016.2599149
   Li Tianxing., 2015, Proceedings of the 13th Annual International Conference on Mobile Systems, Applications, and Services, P197, DOI 10.1145/2742647.2742667
   Lin SS, 2015, IEEE T MULTIMEDIA, V17, P1515, DOI 10.1109/TMM.2015.2437711
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin YH, 2013, IEEE T MULTIMEDIA, V15, P2198, DOI 10.1109/TMM.2013.2271745
   Liu JC, 2011, OPT ENG, V50, DOI 10.1117/1.3529430
   Liu Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1509, DOI 10.1145/3343031.3351025
   Nakamura T., 2004, P 3 INT C MOB UB MUL, P101
   Nguyen V, 2016, 35 IEEE ANN INT C CO
   PETERSON WW, 1961, P IRE, V49, P228, DOI 10.1109/JRPROC.1961.287814
   Pramila A, 2017, MULTIMED TOOLS APPL, V76, P16063, DOI 10.1007/s11042-016-3895-z
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shu X, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P650, DOI 10.1145/2964284.2967302
   Tancik M., ARXIV190405343, V2019
   Nguyen T, 2015, 2015 INTERNATIONAL CONFERENCE ON INFORMATION NETWORKING (ICOIN), P369, DOI 10.1109/ICOIN.2015.7057916
   Wang Anran., 2015, ACM MOBISYS, P181, DOI [/10.1145/2742647.2742652, 10.1145/2742647.2742652]
   Woo G, 2012, INT SYM MIX AUGMENT, P59, DOI 10.1109/ISMAR.2012.6402539
   Xu ML, 2019, IEEE T MULTIMEDIA, V21, P1960, DOI 10.1109/TMM.2019.2891420
   Zhang L, 2015, MOBICOM '15: PROCEEDINGS OF THE 21ST ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P372, DOI 10.1145/2789168.2790106
   Zhong X, 2021, IEEE T MULTIMEDIA, V23, P1951, DOI 10.1109/TMM.2020.3006415
   Zhu JR, 2018, LECT NOTES COMPUT SC, V11219, P682, DOI 10.1007/978-3-030-01267-0_40
NR 34
TC 21
Z9 24
U1 0
U2 43
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 955
EP 967
DI 10.1109/TMM.2021.3061801
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100034
DA 2024-07-18
ER

PT J
AU Huang, CY
   Ng, MK
   Wu, TT
   Zeng, TY
AF Huang, Chaoyan
   Ng, Michael K.
   Wu, Tingting
   Zeng, Tieyong
TI Quaternion-Based Dictionary Learning and Saturation-Value Total
   Variation Regularization for Color Image Restoration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Dictionaries; Image color analysis; Quaternions; Imaging;
   Color; Machine learning; Dictionary learning; image restoration; pure
   quaternion; total variation
ID SPARSE REPRESENTATION; ALGORITHM; MINIMIZATION; MODEL; SVD
AB Color image restoration is a critical task in imaging sciences. Most variational methods regard the color image as a Euclidean vector or the direct combination of three monochrome images and completely ignore the inherent color structures within channels. To better describe the relationship of color channels, we represent the color image as the so-called pure quaternion matrix. Note that the celebrated dictionary learning method has attracted considerable attention for image recovery in the past decade. Following this idea, we propose a novel quaternion-based color image recovery method. This model combines the advantages of dictionary learning and the total variation method for color image restoration. The new strategy used in the proposed model manages to handle the color image restoration problem in the quaternion space. Moreover, the new proposed model can be easily solved by the classical alternating direction method of multipliers (ADMM) algorithm. Numerical results demonstrate clearly that the performance of our proposed dictionary learning method is better than some state-of-the-art color image dictionary learning and total variation methods in terms of some criteria and visual quality.
C1 [Huang, Chaoyan; Wu, Tingting] Nanjing Univ Posts & Telecommun, Sch Sci, Nanjing 210003, Peoples R China.
   [Ng, Michael K.] Univ Hong Kong, Dept Math, Hong Kong, Peoples R China.
   [Zeng, Tieyong] Chinese Univ Hong Kong, Dept Math, Shatin, Hong Kong, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; University of Hong
   Kong; Chinese University of Hong Kong
RP Wu, TT (corresponding author), Nanjing Univ Posts & Telecommun, Sch Sci, Nanjing 210003, Peoples R China.
EM 1019081805@njupt.edu.cn; mng@maths.hku.hk; wutt@njupt.edu.cn;
   zeng@math.cuhk.edu.hk
RI Zeng, Tieyong/B-7147-2009; Huang, Chaoyan/AFE-4659-2022; Ng,
   Michael/B-7189-2009
OI Huang, Chaoyan/0000-0002-9347-3151; Ng, Michael/0000-0001-6833-5227; Wu,
   Tingting/0000-0002-9880-8619; ZENG, Tieyong/0000-0002-0688-202X
FU National Key R&D Program of China [2021YFE0203700]; NSFC/RGC [N_CUHK
   415/19, RGC 14300219, 14302920, 14301121]; CUHK Direct grant for
   Research [4053405, 4053460]; Graduate Student Scientific Research
   Innovation Project of Jiangsu Province, 2020 [KYCX20_0788]; HKRGC GRF
   [12300218, 12300519, 17201020, 17300021]; Natural Science Foundation of
   China [61971234, 11501301]; 1311 Talent Plan of NUPT; QingLan Project
   for Colleges and Universities of Jiangsu Province
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2021YFE0203700, in part by NSFC/RGC N_CUHK 415/19, RGC
   14300219, 14302920, and 14301121, in part by the CUHK Direct grant for
   Research under Grants 4053405 and 4053460, in part by the Graduate
   Student Scientific Research Innovation Project of Jiangsu Province,
   2020, under Grant KYCX20_0788, in part by HKRGC GRF under Grants
   12300218, 12300519, 17201020, and 17300021, in part by the Natural
   Science Foundation of China under Grants 61971234 and 11501301, in part
   by the 1311 Talent Plan of NUPT, the QingLan Project for Colleges and
   Universities of Jiangsu Province.
CR Abdi A, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107634
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Bao BK, 2013, IEEE T IMAGE PROCESS, V22, P860, DOI 10.1109/TIP.2012.2219543
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Bredies K, 2010, SIAM J IMAGING SCI, V3, P492, DOI 10.1137/090769521
   Cai JF, 2012, J AM MATH SOC, V25, P1033, DOI 10.1090/S0894-0347-2012-00740-1
   Chen L, 2021, MATH PROGRAM, V185, P111, DOI 10.1007/s10107-019-01423-x
   Chen L, 2017, COMPUT OPTIM APPL, V66, P327, DOI 10.1007/s10589-016-9864-7
   Chen Y, 2020, IEEE ACCESS, V8, P30628, DOI 10.1109/ACCESS.2020.2973044
   Chen YY, 2020, IEEE T IMAGE PROCESS, V29, P1426, DOI 10.1109/TIP.2019.2941319
   Chen Z, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107118
   Choi JK, 2018, SIAM J IMAGING SCI, V11, P1179, DOI 10.1137/17M1131453
   Colomer A, 2015, INT CONF IMAG PROC, P429, DOI 10.1109/IPTA.2015.7367181
   Commission TI. Committee CIEC. Com- T. Committee CIEC, 1977, COLOR RES APPL, V2, P5, DOI [10.1002/j.1520-6378.1977.tb00102.x, DOI 10.1002/J.1520-6378.1977.TB00102.X]
   Danielyan A, 2012, IEEE T IMAGE PROCESS, V21, P1715, DOI 10.1109/TIP.2011.2176954
   Dong WS, 2011, IEEE T IMAGE PROCESS, V20, P1838, DOI 10.1109/TIP.2011.2108306
   Dong YQ, 2015, ADV COMPUT MATH, V41, P423, DOI 10.1007/s10444-014-9364-1
   Duan JM, 2015, J GLOBAL OPTIM, V62, P853, DOI 10.1007/s10898-015-0290-7
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Gabay D., 1976, Computers & Mathematics with Applications, V2, P17, DOI 10.1016/0898-1221(76)90003-1
   Hamilton W., 1866, ELEMENTS QUATERNIONS
   Huang Hai, 2018, International Journal of High Performance Computing and Networking, V12, P65
   Huang XS, 2012, AASRI PROC, V1, P492, DOI 10.1016/j.aasri.2012.06.077
   Ignatov A, 2019, LECT NOTES COMPUT SC, V11133, P315, DOI 10.1007/978-3-030-11021-5_20
   Jia Z, 2019, SIAM J IMAGING SCI, V12, P972, DOI 10.1137/18M1230451
   Jin ZM, 2014, SIAM J IMAGING SCI, V7, P944, DOI 10.1137/130935197
   Jin Z, 2020, IEEE T MULTIMEDIA, V22, P1055, DOI 10.1109/TMM.2019.2938340
   Li XY, 2019, IEEE SYS MAN CYBERN, P1602, DOI 10.1109/SMC.2019.8913860
   Liu J.-J., 2020, INVERSE PROBL, V36, P1
   Liu JJ, 2019, INT J MACH LEARN CYB, V10, P1051, DOI 10.1007/s13042-017-0782-5
   Liu J, 2019, J SCI COMPUT, V78, P607, DOI 10.1007/s10915-018-0785-8
   Liu J, 2013, IEEE T IMAGE PROCESS, V22, P1108, DOI 10.1109/TIP.2012.2227766
   Ma LY, 2013, SIAM J IMAGING SCI, V6, P2258, DOI 10.1137/120866452
   Ma LY, 2013, IEEE T MED IMAGING, V32, P1277, DOI 10.1109/TMI.2013.2255883
   Mairal J, 2008, IEEE T IMAGE PROCESS, V17, P53, DOI 10.1109/TIP.2007.911828
   Mou C, 2022, IEEE T MULTIMEDIA, V24, P1366, DOI 10.1109/TMM.2021.3063916
   Naderahmadian Y., 2018, 2018 IEEE STAT SIGN, P40
   Naderahmadian Y, 2016, IEEE T SIGNAL PROCES, V64, P592, DOI 10.1109/TSP.2015.2486743
   Nejati M, 2016, IEEE T IMAGE PROCESS, V25, P4900, DOI 10.1109/TIP.2016.2598483
   Newman E, 2020, SIAM J IMAGING SCI, V13, P1084, DOI 10.1137/19M1297026
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Ono S, 2014, PROC CVPR IEEE, P4090, DOI 10.1109/CVPR.2014.521
   Papyan V, 2017, J MACH LEARN RES, V18, P1
   Papyan V, 2017, IEEE T SIGNAL PROCES, V65, P5687, DOI 10.1109/TSP.2017.2733447
   Pei SC, 1997, IEEE T COMMUN, V45, P583, DOI 10.1109/26.592558
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Sánchez-Ferreira C, 2019, SIGNAL PROCESS-IMAGE, V77, P49, DOI 10.1016/j.image.2019.05.015
   Skretting K, 2010, IEEE T SIGNAL PROCES, V58, P2121, DOI 10.1109/TSP.2010.2040671
   Tosic I, 2011, IEEE SIGNAL PROC MAG, V28, P27, DOI 10.1109/MSP.2010.939537
   Wang LC, 2021, IEEE T MULTIMEDIA, V23, P2857, DOI 10.1109/TMM.2020.3017916
   Wang N., 2018, INT J ADV ROBOT SYST, V15, P1
   Wang YL, 2008, SIAM J IMAGING SCI, V1, P248, DOI 10.1137/080724265
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu WW, 2020, PHYS MED BIOL, V65, DOI 10.1088/1361-6560/aba7ce
   Xu RT, 2021, IEEE T MULTIMEDIA, V23, P1225, DOI 10.1109/TMM.2020.2994512
   Xu YY, 2016, INVERSE PROBL IMAG, V10, P563, DOI 10.3934/ipi.2016012
   Xu Y, 2015, IEEE T IMAGE PROCESS, V24, P1315, DOI 10.1109/TIP.2015.2397314
   Yan S, 2019, INVERSE PROBL IMAG, V13, P653, DOI 10.3934/ipi.2019030
   Yang CL, 2013, PATTERN RECOGN, V46, P948, DOI 10.1016/j.patcog.2012.07.011
   Yang M, 2014, INT J COMPUT VISION, V109, P209, DOI 10.1007/s11263-014-0722-8
   Yu YB, 2019, NEUROCOMPUTING, V332, P283, DOI 10.1016/j.neucom.2018.12.034
   Zach Martin, 2020, JOINT AUSTRIAN COMPU, P145
   Zamir Syed Waqas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P492, DOI 10.1007/978-3-030-58595-2_30
   Zeng XH, 2015, IEEE T IMAGE PROCESS, V24, P4556, DOI 10.1109/TIP.2015.2468172
   [詹曙 Zhan Shu], 2015, [电子学报, Acta Electronica Sinica], V43, P523
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang X., 1997, Journal of the Society for Information Display, V5, P61, DOI 10.1889/1.1985127
   Zhang Y, 2018, INVERSE PROBL, V34, DOI 10.1088/1361-6420/aabce9
   Zheng YH, 2019, IEEE T MULTIMEDIA, V21, P2292, DOI 10.1109/TMM.2019.2900166
   Zhu H, 2020, IEEE T IMAGE PROCESS, V29, P6680, DOI 10.1109/TIP.2020.2992895
NR 70
TC 15
Z9 15
U1 2
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3769
EP 3781
DI 10.1109/TMM.2021.3107162
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400008
DA 2024-07-18
ER

PT J
AU Liu, SP
   Tian, GH
   Zhang, Y
   Duan, P
AF Liu, Shaopeng
   Tian, Guohui
   Zhang, Ying
   Duan, Peng
TI Scene Recognition Mechanism for Service Robot Adapting Various Families:
   A CNN-Based Approach Using Multi-Type Cameras
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cameras; Robot vision systems; Image recognition; Robots; Service
   robots; Training; Cloud computing; Scene recognition; convolutional
   neural networks (CNNs); service robot; fish-eye camera; feature fusion
ID PLACE RECOGNITION; MOBILE ROBOTS; CLASSIFICATION; FUSION; TRACKING
AB The key challenges of scene recognition for service robots in various family environments are the view shortage of holistic scenes and poor adaptation. To address these problems, a family scene recognition mechanism for the service robot is proposed in this paper. A comprehensive application of fish-eye, pinhole, and depth cameras is provided to guarantee the sufficient view of robot. A selective CNN features fusion for the recognition of fish-eye scene images is designed to improve the training efficiency and the recognition accuracy. The mechanism is deployed in a designed hybrid cloud including public and private clouds. The proposed family scene recognition model is trained by large-scale datasets in the public cloud and runs in the private cloud. Besides, the recognition skill can be reinforced and increased by matching human guidance and CNN features to help the robot learn new scenes and improve the adaptation in different family environments. Extensive experiments are implemented to evaluate the proposed method using real scene images from six families. The experiment results show the validity and good performance of our method for the service robot scene recognition in various family environments.
C1 [Liu, Shaopeng; Tian, Guohui; Zhang, Ying] Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
   [Duan, Peng] Liaocheng Univ, Sch Comp Sci, Liaocheng 252000, Shandong, Peoples R China.
C3 Shandong University; Liaocheng University
RP Tian, GH (corresponding author), Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
EM shaopeng.liu66@mail.sdu.edu.cn; g.h.tian@sdu.edu.cn;
   zhangying0612@mail.sdu.edu.cn; duanpeng@lcu-cs.com
RI Duan, Peng/ITV-2620-2023
OI Duan, Peng/0000-0002-7396-7592; Zhang, Ying/0000-0001-8982-8223;
   Shaopeng, Liu/0000-0002-9624-846X
FU National Key R&D Program of China [2018YFB1307101]; NationalNatural
   Science Foundation of China [U1813215]
FX This work was supported in part by National Key R&D Program of China
   under Grant 2018YFB1307101 and in part by the NationalNatural Science
   Foundation of China under Grant U1813215.
CR Ammirato Phil, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P1378, DOI 10.1109/ICRA.2017.7989164
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Charoenphakdee N., 2020, ARXIV201011748
   Cheng T, 2016, AIDS BEHAV, V20, P377, DOI 10.1007/s10461-015-1101-3
   Cheng XJ, 2018, PATTERN RECOGN, V74, P474, DOI 10.1016/j.patcog.2017.09.025
   CHOW CK, 1970, IEEE T INFORM THEORY, V16, P41, DOI 10.1109/TIT.1970.1054406
   Condessa F, 2016, IEEE J-STARS, V9, P2321, DOI 10.1109/JSTARS.2015.2510032
   Cortes C., 2016, P C NEUR INF PROC SY, P1668
   Cortes C, 2016, LECT NOTES ARTIF INT, V9925, P67, DOI 10.1007/978-3-319-46379-7_5
   Cui ZP, 2019, IEEE INT CONF ROBOT, P6087, DOI 10.1109/icra.2019.8793884
   Deng LY, 2017, IEEE INT VEH SYM, P231, DOI 10.1109/IVS.2017.7995725
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Fazl-Ersi E, 2012, INT J ROBOT RES, V31, P468, DOI 10.1177/0278364911434936
   Franc V., 2019, P INT C MACH LEARN, P963
   Fu Q, 2020, IEEE ACCESS, V8, P41814, DOI 10.1109/ACCESS.2020.2976733
   Gao CX, 2016, INFORM SCIENCES, V372, P84, DOI 10.1016/j.ins.2016.08.035
   Geifman Y, 2017, ADV NEUR IN, V30
   Gu ZP, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS IEEE-ROBIO 2014, P1410, DOI 10.1109/ROBIO.2014.7090531
   Guo CA, 2017, PR MACH LEARN RES, V70
   Han SB, 2013, IEEE-ASME T MECH, V18, P1745, DOI 10.1109/TMECH.2012.2213263
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Herranz L, 2016, PROC CVPR IEEE, P571, DOI 10.1109/CVPR.2016.68
   Iandola Forrest, 2017, 2017 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS), DOI 10.1145/3125502.3125606
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kang Z., 2019, INT J CONTROL AUTOMA, V17
   Kostavelis I, 2016, ENG APPL ARTIF INTEL, V48, P173, DOI 10.1016/j.engappai.2015.11.004
   Kunze L, 2014, IEEE INT C INT ROBOT, P2910, DOI 10.1109/IROS.2014.6942963
   Li YB, 2019, PATTERN RECOGN, V90, P436, DOI 10.1016/j.patcog.2019.02.005
   Lin M., 2013, ARXIV 13124400
   Lin SS, 2021, IEEE T MULTIMEDIA, V23, P1581, DOI 10.1109/TMM.2020.3001497
   Liu PD, 2017, IEEE INT C INT ROBOT, P1746, DOI 10.1109/IROS.2017.8205988
   Liu QJ, 2018, IEEE T MULTIMEDIA, V20, P1767, DOI 10.1109/TMM.2017.2777671
   Liu SP, 2019, J ROBOT, V2019, DOI 10.1155/2019/8591035
   Liu SP, 2019, NEUROCOMPUTING, V338, P191, DOI 10.1016/j.neucom.2019.01.090
   Luckner M., 2013, P INT C KNOWL INF CR, P413
   Ni Chenri, 2019, Advances in Neural Information Processing Systems, P2582
   Ni J. H. Chenri, 2019, P C NEUR INF PROC SY, P2582
   Qian XY, 2019, IEEE T MULTIMEDIA, V21, P2576, DOI 10.1109/TMM.2019.2902489
   Qian YQ, 2020, IEEE T MULTIMEDIA, V22, P421, DOI 10.1109/TMM.2019.2929949
   Qigang Xu L. Z., 2001, COMPUT TECHNOL DEV, V11, P44
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Ramaswamy HG, 2018, ELECTRON J STAT, V12, P530, DOI 10.1214/17-EJS1388
   Razavian A. S., 2014, Workshop on IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2014), P806, DOI [10.1109/cvprw.2014.131, DOI 10.1109/CVPRW.2014.131]
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Seong H, 2020, IEEE ACCESS, V8, P82066, DOI 10.1109/ACCESS.2020.2989863
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song XH, 2017, IEEE T IMAGE PROCESS, V26, P2721, DOI 10.1109/TIP.2017.2686017
   Sünderhauf N, 2016, IEEE INT CONF ROBOT, P5729, DOI 10.1109/ICRA.2016.7487796
   Tang PJ, 2017, NEUROCOMPUTING, V225, P188, DOI 10.1016/j.neucom.2016.11.023
   Wang YH, 2021, IEEE T MULTIMEDIA, V23, P2782, DOI 10.1109/TMM.2020.3016222
   Wang Z, 2021, IEEE T MULTIMEDIA, V23, P1855, DOI 10.1109/TMM.2020.3003747
   Wang Z, 2017, IEEE T IMAGE PROCESS, V26, P2028, DOI 10.1109/TIP.2017.2666739
   Wegkamp M., 2011, P C NEUR INF PROC SY, P537
   Xie L, 2020, IEEE T MULTIMEDIA, V22, P1182, DOI 10.1109/TMM.2019.2942478
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang JF, 2015, NEUROCOMPUTING, V148, P578, DOI 10.1016/j.neucom.2014.07.005
   Yin YF, 2015, IEEE T MULTIMEDIA, V17, P1760, DOI 10.1109/TMM.2015.2458042
   Yuan M, 2010, J MACH LEARN RES, V11, P111
   Zaki HFM, 2019, AUTON ROBOT, V43, P1005, DOI 10.1007/s10514-018-9776-8
   Zeng HT, 2020, IEEE T MULTIMEDIA, V22, P1519, DOI 10.1109/TMM.2019.2944241
   Zhang XW, 2019, J INTELL ROBOT SYST, V95, P389, DOI 10.1007/s10846-018-0917-2
   Zhao WD, 2018, IEEE T MULTIMEDIA, V20, P866, DOI 10.1109/TMM.2017.2760100
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   [朱博 Zhu Bo], 2017, [自动化学报, Acta Automatica Sinica], V43, P493
   Zhuang PQ, 2021, IEEE T MULTIMEDIA, V23, P3603, DOI 10.1109/TMM.2020.3028482
   Zhuang Y, 2013, IEEE T INSTRUM MEAS, V62, P438, DOI 10.1109/TIM.2012.2216475
NR 68
TC 12
Z9 12
U1 5
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2392
EP 2406
DI 10.1109/TMM.2021.3080076
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600013
DA 2024-07-18
ER

PT J
AU Liu, Y
   Fang, FM
   Wang, TT
   Li, JC
   Sheng, Y
   Zhang, GX
AF Liu, Yang
   Fang, Faming
   Wang, Tingting
   Li, Juncheng
   Sheng, Yun
   Zhang, Guixu
TI Multi-Scale Grid Network for Image Deblurring With High-Frequency
   Guidance
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image reconstruction; Feature extraction; Image restoration; Image edge
   detection; Kernel; Image resolution; Semantics; Blind image deblurring;
   image processing; high-frequency guidance; convolutional neural
   networks; multi-scale
AB It has been demonstrated that the blurring process reduces the high-frequency information of the original sharp image, so the main challenge for image deblurring is to reconstruct high-frequency information from the blurry image. In this paper, we propose a novel image deblurring framework to focus on the reconstruction of high-frequency information, which consists of two main subnetworks: a high-frequency reconstruction subnetwork (HFRSN) and a multi-scale grid subnetwork (MSGSN). The HFRSN is built to reconstruct latent high-frequency information from multiple scale blurry images. The MSGSN performs deblurring processes with high-frequency guidance at different scales simultaneously. Besides, in order to better use high-frequency information to restore sharpening images, we designed a high-frequency information aggregation (HFAG) module and a high-frequency information attention (HFAT) module in MSGSN. The HFAG module is designed to fuse high-frequency features and image features at the feature extraction stage, and the HFAT module is built to enhance the feature reconstruction stage. Extensive experiments on different datasets show the effectiveness and efficiency of our method.
C1 [Liu, Yang; Fang, Faming; Wang, Tingting; Li, Juncheng; Zhang, Guixu] East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.
   [Fang, Faming] East China Normal Univ, Key Lab Adv Theory & Applicat Stat & Data Sci, MOE, Shanghai 200062, Peoples R China.
   [Sheng, Yun] Liverpool John Moores Univ, Liverpool L2 2QP, Merseyside, England.
C3 East China Normal University; East China Normal University; Liverpool
   John Moores University
RP Fang, FM (corresponding author), East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.; Fang, FM (corresponding author), East China Normal Univ, Key Lab Adv Theory & Applicat Stat & Data Sci, MOE, Shanghai 200062, Peoples R China.
EM andy_corleone@outlook.com; fmfang@cs.ecnu.edu.cn;
   ttwang@stu.ecnu.edu.cn; 51164500049@stu.ecnu.edu.cn; y.sheng@ljmu.ac.uk;
   gxzhang@cs.ecnu.edu.cn
RI Li, Juncheng/AHA-3971-2022
OI , Yun/0000-0002-1958-622X; Liu, Yang/0000-0002-9236-8907; Li,
   Juncheng/0000-0001-7314-6754
FU Key Project of the National Natural Science Foundation of China
   [61731009]; NSFC-RGC [61961160734]; National Natural Science Foundation
   of China [61871185]; Shanghai Rising-Star Program [21QA1402500]; Science
   Foundation of Shanghai [20ZR1416200]; Open Research Fund of
   KLATASDS-MOE,ECNU
FX This work was supported in part by the Key Project of the National
   Natural Science Foundation of China under Grant 61731009, in part by the
   NSFC-RGC under Grant 61961160734, in part by the National Natural
   Science Foundation of China under Grant 61871185, in part by the
   Shanghai Rising-Star Program underGrant 21QA1402500, in part by the
   Science Foundation of Shanghai under Grant 20ZR1416200, and in part by
   the Open Research Fund of KLATASDS-MOE,ECNU.
CR Aljadaany R, 2019, PROC CVPR IEEE, P10227, DOI 10.1109/CVPR.2019.01048
   [Anonymous], P INT C MACH LEARN L
   Boracchi G, 2012, IEEE T IMAGE PROCESS, V21, P3502, DOI 10.1109/TIP.2012.2192126
   Chan TF, 1998, IEEE T IMAGE PROCESS, V7, P370, DOI 10.1109/83.661187
   Chen L, 2019, PROC CVPR IEEE, P1742, DOI 10.1109/CVPR.2019.00184
   Cheng DC, 2017, IEEE J-STARS, V10, P5769, DOI 10.1109/JSTARS.2017.2747599
   Cheng Ma, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7766, DOI 10.1109/CVPR42600.2020.00779
   Cho SI, 2019, IEEE T MULTIMEDIA, V21, P484, DOI 10.1109/TMM.2018.2859791
   Du Y, 2021, IEEE T MULTIMEDIA, V23, P2139, DOI 10.1109/TMM.2020.3008057
   Fang FM, 2020, IEEE T IMAGE PROCESS, V29, P4656, DOI 10.1109/TIP.2020.2973769
   FARGE M, 1992, ANNU REV FLUID MECH, V24, P395, DOI 10.1146/annurev.fl.24.010192.002143
   Fourure D., 2017, ARXIV170707958, P181
   Fu Z., 2019, EDGE AWARE DEEP IMAG
   Furuta R, 2020, IEEE T MULTIMEDIA, V22, P1704, DOI 10.1109/TMM.2019.2960636
   Gao HY, 2019, PROC CVPR IEEE, P3843, DOI 10.1109/CVPR.2019.00397
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Hough P. V. C., 1962, Method and Means for Recognizing Complex Patterns
   Jiang Z, 2020, PROC CVPR IEEE, P3317, DOI 10.1109/CVPR42600.2020.00338
   Kingma D.P., 2014, ARXIV14126980
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188
   Li CY, 2020, IEEE T MULTIMEDIA, V22, P704, DOI 10.1109/TMM.2019.2933334
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu RS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1921, DOI 10.1145/3240508.3240565
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Pan JS, 2018, IEEE T PATTERN ANAL, V40, P2315, DOI 10.1109/TPAMI.2017.2753804
   Pan JS, 2014, PROC CVPR IEEE, P2901, DOI 10.1109/CVPR.2014.371
   Rao K.R, 2014, DISCRETE COSINE TRAN
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shen ZY, 2019, IEEE I CONF COMP VIS, P5571, DOI 10.1109/ICCV.2019.00567
   Suin M, 2020, PROC CVPR IEEE, P3603, DOI 10.1109/CVPR42600.2020.00366
   Sun J, 2015, PROC CVPR IEEE, P769, DOI 10.1109/CVPR.2015.7298677
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang YL, 2021, IEEE T MULTIMEDIA, V23, P2481, DOI 10.1109/TMM.2020.3013383
   Xu L, 2014, ADV NEUR IN, V27
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Yan YY, 2017, PROC CVPR IEEE, P6978, DOI 10.1109/CVPR.2017.738
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Yuan Y, 2020, PROC CVPR IEEE, P3552, DOI 10.1109/CVPR42600.2020.00361
   Zhang HG, 2019, PROC CVPR IEEE, P5971, DOI 10.1109/CVPR.2019.00613
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zheng S, 2019, IEEE SIGNAL PROC LET, V26, P1546, DOI 10.1109/LSP.2019.2939752
NR 47
TC 15
Z9 15
U1 5
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2890
EP 2901
DI 10.1109/TMM.2021.3090206
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000016
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Liu, YQ
   Wang, SQ
   Zhang, J
   Wang, SS
   Ma, SW
   Gao, W
AF Liu, Yuqing
   Wang, Shiqi
   Zhang, Jian
   Wang, Shanshe
   Ma, Siwei
   Gao, Wen
TI Iterative Network for Image Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Degradation; Superresolution; Optimization; Image restoration;
   Visualization; Convolution; Training; Single image super-resolution;
   iterative optimization; feature normalization
ID RECOVERY
AB Single image super-resolution (SISR), as a traditional ill-conditioned inverse problem, has been greatly revitalized by the recent development of convolutional neural networks (CNN). These CNN-based methods generally map a low-resolution image to its corresponding high-resolution version with sophisticated network structures and loss functions, showing impressive performances. This paper provides a new insight on conventional SISR algorithm, and proposes a substantially different approach relying on the iterative optimization. A novel iterative super-resolution network (ISRN) is proposed on top of the iterative optimization. We first analyze the observation model of image SR problem, inspiring a feasible solution by mimicking and fusing each iteration in a more general and efficient manner. Considering the drawbacks of batch normalization, we propose a feature normalization (F-Norm, FN) method to regulate the features in network. Furthermore, a novel block with FN is developed to improve the network representation, termed as FNB. Residual-in-residual structure is proposed to form a very deep network, which groups FNBs with a long skip connection for better information delivery and stabling the training phase. Extensive experimental results on testing benchmarks with bicubic (BI) degradation show our ISRN can not only recover more structural information, but also achieve competitive or better PSNR/SSIM results with much fewer parameters compared to other works. Besides BI, we simulate the real-world degradation with blur-downscale (BD) and downscale-noise (DN). ISRN and its extension ISRN+ both achieve better performance than others with BD and DN degradation models.
C1 [Liu, Yuqing] Dalian Univ Technol, Sch Software, Dalian 116620, Peoples R China.
   [Wang, Shiqi] City Univ Hong Kong, Dept Comp Sci, Kowloon Tong, Hong Kong, Peoples R China.
   [Zhang, Jian] Peking Univ, Shenzhen Grad Sch, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China.
   [Wang, Shanshe; Ma, Siwei; Gao, Wen] Peking Univ, Sch Elect Engn & Comp Sci, Inst Digital Media, Beijing 100871, Peoples R China.
C3 Dalian University of Technology; City University of Hong Kong; Peking
   University; Peking University
RP Ma, SW (corresponding author), Peking Univ, Sch Elect Engn & Comp Sci, Inst Digital Media, Beijing 100871, Peoples R China.
EM liuyuqing@mail.dlut.edu.cn; shiqwang@cityu.edu.hk;
   zhangjian.sz@pku.edu.cn; sswang@pku.edu.cn; swma@pku.edu.cn;
   wgao@pku.edu.cn
RI LIU, YU/HTR-1607-2023; Liu, LiuYuqing/GWZ-5665-2022
OI Zhang, Jian/0000-0001-5486-3125; Liu, Yuqing/0000-0001-9828-5646
FU Key-Area Research and Development Program of Guangdong Province
   [2019B010133001]; National Science Foundation of China [62025101620,
   62088102]; PKU-Baidu Fund [2019BD003]
FX This work was supported in part by the Key-Area Research and Development
   Program of Guangdong Province under Grant 2019B010133001, in part by the
   National Science Foundation of China under Grants 62025101620 and
   62088102,in part by PKU-Baidu Fund 2019BD003 and High-performance
   Computing Platform of Peking University. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Susanto Rahardja.
CR Afonso MV, 2010, IEEE T IMAGE PROCESS, V19, P2345, DOI 10.1109/TIP.2010.2047910
   Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Blau Y, 2019, LECT NOTES COMPUT SC, V11133, P334, DOI 10.1007/978-3-030-11021-5_21
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   GEMAN D, 1992, IEEE T PATTERN ANAL, V14, P367, DOI 10.1109/34.120331
   Gu JJ, 2019, PROC CVPR IEEE, P1604, DOI 10.1109/CVPR.2019.00170
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   He XY, 2019, PROC CVPR IEEE, P1732, DOI 10.1109/CVPR.2019.00183
   He ZW, 2020, IEEE T MULTIMEDIA, V22, P1042, DOI 10.1109/TMM.2019.2937688
   He ZW, 2019, IEEE T CIRC SYST VID, V29, P2310, DOI 10.1109/TCSVT.2018.2864777
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu YT, 2020, IEEE T CIRC SYST VID, V30, P3911, DOI 10.1109/TCSVT.2019.2915238
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jin Z, 2020, IEEE T MULTIMEDIA, V22, P1055, DOI 10.1109/TMM.2019.2938340
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Li ZG, 2012, IEEE T IMAGE PROCESS, V21, P4672, DOI 10.1109/TIP.2012.2207396
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Salimans T, 2016, ADV NEUR IN, V29
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang QT, 2020, IEEE T CIRC SYST VID, V30, P2418, DOI 10.1109/TCSVT.2019.2919310
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang YF, 2019, IEEE T CIRC SYST VID, V29, P1259, DOI 10.1109/TCSVT.2018.2839879
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Wu HP, 2021, IEEE T CIRC SYST VID, V31, P512, DOI 10.1109/TCSVT.2020.2988895
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2957, DOI 10.1109/TMM.2019.2914883
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Yu J., 2018, P IEEE CVF C COMP VI
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang K, 2019, PROC CVPR IEEE, P1671, DOI 10.1109/CVPR.2019.00177
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
NR 46
TC 14
Z9 14
U1 2
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2259
EP 2272
DI 10.1109/TMM.2021.3078615
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Teng, JY
   Lu, XK
   Gong, YS
   Liu, XF
   Nie, XS
   Yin, YL
AF Teng, Junya
   Lu, Xiankai
   Gong, Yongshun
   Liu, Xinfang
   Nie, Xiushan
   Yin, Yilong
TI Regularized Two Granularity Loss Function for Weakly Supervised Video
   Moment Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Iron; Barium; Ions; Integrated circuits; Legged locomotion; Erbium;
   Cameras; Weakly supervised video moment retrieval; multiple instance
   learning; temporal ensembling; segment-level loss
ID LOCALIZATION
AB Weakly supervised video moment retrieval or weakly supervised language moment retrieval aims to search the most relevant moment given a language query. In order to guide the model to capture the most matching video segments with the text description, we design a two-granularity loss function that simultaneously considers both video-level and instance-level relationships. Specifically, we first generate coarse video segments and regard each video segment as an instance. For video-level regularized multiple instance loss (MIL), we leverage the latent alignment between all intra-video segments (ie., positive bag) and text descriptions. Then, we classify these segments by regarding this procedure as a supervised learning task under noisy labels. With the instance-level regularized loss function, our model can learn to correct noisy instance-level labels so as to locate the more accurate frame boundary from all the positive instances. Comprehensive experimental results on ActivityNet and DiDeMo demonstrate that the proposed loss function sets a new state-of-the-art.
C1 [Teng, Junya; Lu, Xiankai; Gong, Yongshun; Liu, Xinfang; Yin, Yilong] Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
   [Nie, Xiushan] Shandong Jianzhu Univ, Sch Comp Sci & Technol, Jinan 250101, Peoples R China.
C3 Shandong University; Shandong Jianzhu University
RP Lu, XK; Yin, YL (corresponding author), Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
EM 201914798@mail.sdu.edu.cn; carrierlxk@gmail.com; ysgong@sdu.edu.cn;
   xinfangliu@qq.com; niexsh@hotmail.com; ylyin@sdu.edu.cn
RI Xiankai, Lu/W-5570-2019; lu, kai/KBB-4008-2024; Gong,
   Yongshun/HKW-1980-2023
OI Gong, Yongshun/0000-0003-3948-4471
FU Fundamental Research Funds of Shandong University; National Natural
   Science Foundation of China [61876098, 62106128, 62176141]; Natural
   Science Foundation of Shandong Province [ZR202102240155,
   ZR202102240751]; Taishan Scholar Project of Shandong Province
   [tsqn202103088]
FX This work was supported in part by the Fundamental Research Funds of
   Shandong University, in part by the National Natural Science Foundation
   of China under Grant 61876098, Grant 62106128, Grant 62176141, in part
   by the Natural Science Foundation of Shandong Province under Grant
   ZR202102240155 and Grant ZR202102240751, in part by the Taishan Scholar
   Project of Shandong Province under Grant tsqn202103088, and in part by
   the special funds for distinguished professors of Shandong Jianzhu
   University. The Guest Editor coordinating the review of this manuscript
   and approving it for publication was Prof. Jian Zhang.
CR Bachman P, 2014, ADV NEUR IN, V27
   Bekker AJ, 2016, INT CONF ACOUST SPEE, P2682, DOI 10.1109/ICASSP.2016.7472164
   Bruna J., 2014, P INT C LEARN REPR
   Chen JY, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P162
   Chen Zhenfang, 2020, ARXIV200109308, P8
   CHIDUME CE, 1987, P AM MATH SOC, V99, P283, DOI 10.2307/2046626
   Devlin J., 2018, BERT PRE TRAINING DE
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Duan X., 2018, NeurIPS, P3062
   Gao J., 2017, BMVC, P123
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Gao MF, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P1481
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Laine Samuli, 2017, 5 INT C LEARNING REP, DOI DOI 10.48550/ARXIV.1610.02242
   Liu M, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P843, DOI 10.1145/3240508.3240549
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Long FC, 2020, IEEE T MULTIMEDIA, V22, P1577, DOI 10.1109/TMM.2019.2943204
   Lu X., 2020, ECCV, V12348, P661, DOI DOI 10.1007/978-3-030-58580-8_39
   Lu XK, 2022, IEEE T PATTERN ANAL, V44, P2228, DOI 10.1109/TPAMI.2020.3040258
   Lu XK, 2019, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2019.00374
   Mithun NC, 2019, PROC CVPR IEEE, P11584, DOI 10.1109/CVPR.2019.01186
   Natarajan N., 2013, ADV NEURAL INFORM PR, P1196, DOI DOI 10.5555/2999611.2999745
   Ning K, 2020, IEEE T MULTIMEDIA, V22, P2434, DOI 10.1109/TMM.2019.2957854
   Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240
   Sajjadi M, 2016, ADV NEUR IN, V29
   Song H, 2019, IEEE T MULTIMEDIA, V21, P717, DOI 10.1109/TMM.2018.2866370
   Sultani W, 2018, PROC CVPR IEEE, P6479, DOI 10.1109/CVPR.2018.00678
   Sun Z., 2020, ACM MM, P92
   Tan R., 2019, P IEEE CVF WINT C AP, P2083
   Tan R., 2019, WMAN WEAKLY SUPERVIS
   Tarvainen A, 2017, ADV NEUR IN, V30
   Wan BY, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102722
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062
   Yadati K, 2018, IEEE T MULTIMEDIA, V20, P2526, DOI 10.1109/TMM.2018.2801719
   Yao Y., 2020, ACM MM, P1735
   Yao YZ, 2017, IEEE T MULTIMEDIA, V19, P1771, DOI 10.1109/TMM.2017.2684626
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Zhang D, 2019, PROC CVPR IEEE, P1247, DOI 10.1109/CVPR.2019.00134
   Zhang L, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P907, DOI 10.1145/3123266.3123317
   Zhang SW, 2020, IEEE T MULTIMEDIA, V22, P2610, DOI 10.1109/TMM.2019.2959425
   Zhang Z., 2020, P 28 ACM INT C MULT, P4098
   Zhang ZL, 2018, ADV NEUR IN, V31
   Zhang Z, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P655, DOI 10.1145/3331184.3331235
   Zhu Yi, 2019, 30 BRIT MACH VIS C 2, P1
NR 47
TC 9
Z9 9
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1141
EP 1151
DI 10.1109/TMM.2021.3120545
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ZN1SD
UT WOS:000764821800011
DA 2024-07-18
ER

PT J
AU Xu, LM
   Zeng, XH
   Li, WS
   Bai, L
AF Xu, Liming
   Zeng, Xianhua
   Li, Weisheng
   Bai, Ling
TI IDHashGAN: Deep Hashing With Generative Adversarial Nets for Incomplete
   Data Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image restoration; Kernel; Manifolds; Image reconstruction; Training;
   Gallium nitride; Deep learning; Generative adversarial nets; hash
   learning; incomplete data; supervised manifold similarity
ID IMAGE RETRIEVAL; SEARCH
AB Benefiting from low storage costs and high retrieval efficiency, hash learning has been a widely adopted technology for approximating nearest neighbor in large-scale data retrieval. Deep learning to hash greatly improves image retrieval performance by integrating feature learning and hash coding into an end-to-end framework. However, subject to application scope, most existing deep hashing methods only apply to retrieval of complete data and have undesirable results when retrieving incomplete but valuable data. In this paper we propose IDHashGAN, a novel deep hashing model with generative adversarial networks to retrieve incomplete data, in which feature restoration, feature learning and hash coding are integrated into an unified end-to-end framework. The proposed model consists of four key components: (1) reconstructive and generative loss are used to generate continuous feature of incomplete data in generative network; (2) supervised manifold similarity is proposed to improve retrieval accuracy and obtain good user acceptance; (3) adversarial and classified loss are designed to distinguish authenticity and similarity in discriminative network; and (4) encoding and quantization loss are adopted to preserve similarity and control hash quality. Extensive experiments on benchmark datasets show that IDHashGAN is competitive on complete dataset and yields substantial boosts of 70% on incomplete datasets compared to state-of-the-art hashing methods.
C1 [Xu, Liming; Zeng, Xianhua; Li, Weisheng; Bai, Ling] Chongqing Univ Posts & Telecommun, Coll Comp Sci & Technol, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
C3 Chongqing University of Posts & Telecommunications
RP Zeng, XH (corresponding author), Chongqing Univ Posts & Telecommun, Coll Comp Sci & Technol, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
EM xulimmail@gmail.com; zengxh@cqupt.edu.cn; liws@cqupt.edu.cn;
   battylingb@gmail.com
OI Xu, Liming/0000-0002-0671-8182; Zeng, Xianhua/0000-0001-5892-2372; Bai,
   Ling/0000-0002-2135-5404
FU National Natural Science Foundation of China [61672120, 62076044];
   Natural Science Foundation of Chongqing, China [cstc2019jcyj-zdxm0011];
   Doctoral Innovative Talents Project of Chongqing University of Posts and
   Telecommunications [BYJS201812]; Doctoral Research Innovation Project of
   Chongqing [CYB19173]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61672120 and 62076044, in part by the
   Natural Science Foundation of Chongqing, China under Grant
   cstc2019jcyj-zdxm0011, in part by the Doctoral Innovative Talents
   Project of Chongqing University of Posts and Telecommunications under
   Grant BYJS201812, and in part by the Doctoral Research Innovation
   Project of Chongqing under Grant CYB19173.
CR [Anonymous], 2009, NIPS
   [Anonymous], 2013, IEEE T PATTERN ANAL
   Arjovsky M., 2017, ARXIV170107875
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Cao Y, 2018, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2018.00134
   Cao Y, 2018, PROC CVPR IEEE, P1287, DOI 10.1109/CVPR.2018.00140
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Dizaji KG, 2018, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR.2018.00386
   Ercoli S, 2017, IEEE T MULTIMEDIA, V19, P2521, DOI 10.1109/TMM.2017.2697824
   Ge TZ, 2014, LECT NOTES COMPUT SC, V8695, P250, DOI 10.1007/978-3-319-10584-0_17
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Grigorova A, 2007, IEEE T MULTIMEDIA, V9, P1183, DOI 10.1109/TMM.2007.902828
   Jiang YG, 2013, IEEE T MULTIMEDIA, V15, P442, DOI 10.1109/TMM.2012.2231061
   Kang WC, 2016, AAAI CONF ARTIF INTE, P1230
   Kingma D.P., 2014, ARXIV14126980
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Li WJ, 2016, IJCAI, P1711
   Lin Kevin, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P27, DOI 10.1109/CVPRW.2015.7301269
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Lv YM, 2015, IEEE T MULTIMEDIA, V17, P1225, DOI 10.1109/TMM.2015.2437712
   Norouzi M., 2012, ADV NEURAL INFORM PR
   Qiu ZF, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P225, DOI 10.1145/3077136.3080842
   Radford A., 2015, ARXIV
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Song JK, 2018, AAAI CONF ARTIF INTE, P394
   Tiakas E, 2013, IEEE T MULTIMEDIA, V15, P1415, DOI 10.1109/TMM.2013.2247989
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang J, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P515, DOI 10.1145/3077136.3080786
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wang XF, 2017, LECT NOTES COMPUT SC, V10111, P70, DOI 10.1007/978-3-319-54181-5_5
   Wang YB, 2020, IEEE T MULTIMEDIA, V22, P1458, DOI 10.1109/TMM.2019.2947197
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Xu LM, 2020, NEUROCOMPUTING, V402, P220, DOI 10.1016/j.neucom.2020.04.011
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
NR 41
TC 5
Z9 6
U1 2
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 534
EP 545
DI 10.1109/TMM.2021.3054503
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100002
DA 2024-07-18
ER

PT J
AU Yao, H
   Zou, M
   Qin, C
   Zhang, XP
AF Yao, Heng
   Zou, Mian
   Qin, Chuan
   Zhang, Xinpeng
TI Signal-Dependent Noise Estimation for a Real-Camera Model via Weight and
   Shape Constraints
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Estimation; Cameras; Noise level; Computational modeling; Noise
   measurement; AWGN; Shape; Signal-dependent noise; noise level function;
   Confidence level; nonlinear radiometric calibration; linear transform
   domain
ID LEVEL ESTIMATION; RESPONSE FUNCTIONS; TRANSFORM-DOMAIN; SENSOR NOISE;
   IMAGE; REMOVAL; FORGERY
AB Most computer vision algorithms require parameter adjustment according to the image noise level. Conventionally, the additive white Gaussian noise (AWGN) model is widely used in most noise estimation algorithms; however, this assumption does not hold in the real world where the noise from cameras is more complex, and it is more appropriate to assume the signal-dependent noise (SDN) model. In this paper, we focus on the SDN model while considering the nonlinear radiometric calibration inside an actual camera, and propose an algorithm to efficiently estimate the noise level function (NLF), which is defined as the noise standard deviation with respect to image intensity. First, the input image is divided into overlapping patches, and noise samples are estimated in the linear transform domain. The confidence levels of the noise samples and the prior of the camera response function are then employed as constraints for the recovery of the NLF. Finally, the noise samples and constraints are represented in a convex optimization problem. The experimental results using both real and synthetic noisy images demonstrate the superiority of the proposed method. In addition, the estimated NLFs are incorporated into two well-known denoising schemes, non-local means and BM3D, and shows significant improvements in denoising SDN-polluted images.
C1 [Yao, Heng; Qin, Chuan] Univ Shanghai Sci & Technol, Sch Opt Elect & Comp Engn, Shanghai 200093, Peoples R China.
   [Zou, Mian] Univ Shanghai Sci & Technol, Sch Mech Engn, Shanghai 200093, Peoples R China.
   [Zhang, Xinpeng] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
C3 University of Shanghai for Science & Technology; University of Shanghai
   for Science & Technology; Fudan University
RP Qin, C (corresponding author), Univ Shanghai Sci & Technol, Sch Opt Elect & Comp Engn, Shanghai 200093, Peoples R China.
EM hyao@usst.edu.cn; chnzm366aq@163.com; qin@usst.edu.cn;
   zhangxinpeng@fudan.edu.cn
RI Qin, Chuan/C-1106-2017; Yao, Heng/J-9457-2019
OI Qin, Chuan/0000-0002-0370-4623; Yao, Heng/0000-0002-3784-4157; Zou,
   Mian/0000-0003-4306-8980
FU Natural Science Foundation ofChina [61702332, U20B2051, 61672354,
   U1936214, U1636206, 61525203]; STCSM Capability Construction Project for
   Shanghai Municipal Universities [20060502300]
FX This work was supported in part by the Natural Science Foundation
   ofChina under Grants 61702332, U20B2051, 61672354, U1936214, U1636206,
   and 61525203, and in part by the STCSM Capability Construction Project
   for Shanghai Municipal Universities (20060502300).
CR Amer A, 2005, IEEE T CIRC SYST VID, V15, P113, DOI 10.1109/TCSVT.2004.837017
   Bovik A, 2005, HANDBOOK OF IMAGE AND VIDEO PROCESSING, 2ND EDITION, pV, DOI 10.1016/B978-012119792-6/50062-0
   Boyd S., 2004, CONVEX OPTIMIZATION
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Chen C, 2019, IEEE WINT CONF APPL, P1961, DOI 10.1109/WACV.2019.00213
   Chen MJ, 2011, IEEE T MULTIMEDIA, V13, P1195, DOI 10.1109/TMM.2011.2166538
   Cho SI, 2018, IEEE T MULTIMEDIA, V20, P1738, DOI 10.1109/TMM.2017.2781371
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   De Stefano A, 2004, EURASIP J APPL SIG P, V2004, P2400, DOI 10.1155/S1110865704401218
   Deng CW, 2020, IEEE T CYBERNETICS, V50, P1146, DOI 10.1109/TCYB.2018.2889376
   Dong L, 2018, IEEE T IMAGE PROCESS, V27, P2715, DOI 10.1109/TIP.2018.2812083
   Dong L, 2017, IEEE T IMAGE PROCESS, V26, P1017, DOI 10.1109/TIP.2016.2639447
   Foi A, 2008, IEEE T IMAGE PROCESS, V17, P1737, DOI 10.1109/TIP.2008.2001399
   Ghazal M, 2011, IEEE T IMAGE PROCESS, V20, P1788, DOI 10.1109/TIP.2010.2097272
   Grossberg MD, 2004, IEEE T PATTERN ANAL, V26, P1272, DOI 10.1109/TPAMI.2004.88
   Kobayashi M, 2010, IEEE T INF FOREN SEC, V5, P883, DOI 10.1109/TIFS.2010.2074194
   Lagarias JC, 1998, SIAM J OPTIMIZ, V9, P112, DOI 10.1137/S1052623496303470
   Lin S, 2005, PROC CVPR IEEE, P66
   Lin S, 2004, PROC CVPR IEEE, P938
   Liu C, 2008, IEEE T PATTERN ANAL, V30, P299, DOI 10.1109/TPAMI.20071176
   Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899
   Liu XH, 2014, IEEE T IMAGE PROCESS, V23, P4361, DOI 10.1109/TIP.2014.2347204
   Liu XH, 2013, IEEE T IMAGE PROCESS, V22, P5226, DOI 10.1109/TIP.2013.2283400
   Liu XH, 2012, IEEE IMAGE PROC, P665, DOI 10.1109/ICIP.2012.6466947
   Lyu SW, 2014, INT J COMPUT VISION, V110, P202, DOI 10.1007/s11263-013-0688-y
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsushita Y., 2007, P IEEE C COMPUTER VI, P1, DOI DOI 10.1109/CVPR.2007.383213
   Petersen K. B., 2012, MATRIX COOKBOOK, DOI DOI 10.1017/CBO9780511470943.008
   Ponomarenko NN, 2007, INT GEOSCI REMOTE SE, P472, DOI 10.1109/IGARSS.2007.4422833
   Pun CM, 2016, J VIS COMMUN IMAGE R, V38, P195, DOI 10.1016/j.jvcir.2016.03.005
   Pyatykh S, 2013, IEEE T IMAGE PROCESS, V22, P687, DOI 10.1109/TIP.2012.2221728
   Shin DH, 2005, IEEE T CONSUM ELECTR, V51, P218, DOI 10.1109/TCE.2005.1405723
   Suo JL, 2016, J VIS COMMUN IMAGE R, V36, P130, DOI 10.1016/j.jvcir.2016.01.009
   Tai YW, 2013, IEEE T PATTERN ANAL, V35, P2498, DOI 10.1109/TPAMI.2013.40
   Takamatsu J, 2008, LECT NOTES COMPUT SC, V5305, P623, DOI 10.1007/978-3-540-88693-8_46
   Takamatsu Jun., 2008, 2008 IEEE C COMPUTER, P1
   Tan HL, 2019, COMPUT INTEL NEUROSC, V2019, DOI 10.1155/2019/4970508
   Tang CW, 2015, IEEE T CIRC SYST VID, V25, P1283, DOI 10.1109/TCSVT.2014.2380196
   Tsin Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P480, DOI 10.1109/ICCV.2001.937555
   Uss M, 2018, LECT NOTES COMPUT SC, V11182, P414, DOI 10.1007/978-3-030-01449-0_35
   Yang JY, 2017, IEEE IMAGE PROC, P2418, DOI 10.1109/ICIP.2017.8296716
   Yang JY, 2015, IEEE T IMAGE PROCESS, V24, P1561, DOI 10.1109/TIP.2015.2405417
   Yang JY, 2012, IEEE IMAGE PROC, P673, DOI 10.1109/ICIP.2012.6466949
   Yang SM, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3476329
   Yao H, 2017, MULTIMED TOOLS APPL, V76, P12457, DOI 10.1007/s11042-016-3660-3
   Yin JL, 2018, IEEE T MULTIMEDIA, V20, P3045, DOI 10.1109/TMM.2018.2820910
   Zoran D, 2009, IEEE I CONF COMP VIS, P2209, DOI 10.1109/ICCV.2009.5459476
NR 47
TC 5
Z9 5
U1 2
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 640
EP 654
DI 10.1109/TMM.2021.3056879
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100010
DA 2024-07-18
ER

PT J
AU Zhang, MY
   Tian, GH
   Zhang, Y
   Duan, P
AF Zhang, Mengyang
   Tian, Guohui
   Zhang, Ying
   Duan, Peng
TI Reinforcement Learning for Logic Recipe Generation: Bridging Gaps From
   Images to Plans
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Reinforcement learning; Feature extraction; Decoding; Artificial neural
   networks; Generators; Visualization; Task analysis; Question answering;
   attention mechanism; reinforcement learning; recipe generation
AB It is a challenging task to produce recipes from images, due to the difficulty in bridging the gap from intuitive, static images to sequential, dynamic recipes. In this paper, we propose a novel recipe generation system for producing effective recipes from images. As medium steps, ingredient generation is introduced to guide recipe generation in our system. With potential information in ingredient lists, ingredient selection and ingredient sequence, the system is taught to generate effective recipes. For information representation, a hierarchical attention mechanism is designed to extract effective features for ingredient production and recipe generation. In order to guarantee the comprehensiveness and logic in recipes, a specific and explicit criterion around ingredients is designed under the framework of reinforcement learning. In ingredient generation, the system is required to generate ingredients with correct sequence in cooking procedures. And in recipe generation, ingredients in recipes are required to be consistent with produced ingredients. In experiments, the proposed method is compared with state-of-the-art methods to evaluate the feasibility. The results indicate that the proposed system achieves a better performance than other methods on both aspects of producing proper ingredients and effective recipes.
C1 [Zhang, Mengyang; Tian, Guohui; Zhang, Ying] Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
   [Duan, Peng] Liaocheng Univ, Sch Comp Sci, Liaocheng 252000, Shandong, Peoples R China.
C3 Shandong University; Liaocheng University
RP Tian, GH (corresponding author), Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
EM z.m.y.007@sdu.edu.cn; g.h.tian@sdu.edu.cn;
   zhangying0612@mail.sdu.ed-u.cn; duaupeng@lcu-cs.com
RI Duan, Peng/ITV-2620-2023
OI Duan, Peng/0000-0002-7396-7592; Zhang, Ying/0000-0001-8982-8223
FU National Natural Science Foundation of China [U1813215, 61773239];
   Special fund for Taishan Scholars Program of Shandong Province; China
   Postdoctoral Science Foundation [2020M672060]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants U1813215 and 61773239, in part by the
   Special fund for Taishan Scholars Program of Shandong Province, and in
   part by the China Postdoctoral Science Foundation under Grant
   2020M672060. The associate editor coordinating the reviewof this
   manuscript and approving it for publicationwas Prof. Guo-Jun Qi.
   (Corresponding author: Guohui Tian.)
CR [Anonymous], 2016, P 54 ANN M ASS COMP
   [Anonymous], 2012, P SIGGRAPH ASIA 2012
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Branavan S.R.K., 2011, P 49 ANN M ASS COMPU, V1, P268
   Chen JJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1771, DOI 10.1145/3123266.3123428
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Gao XY, 2020, IEEE T MULTIMEDIA, V22, P1647, DOI 10.1109/TMM.2019.2945180
   Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494
   Jurcícek F, 2012, COMPUT SPEECH LANG, V26, P168, DOI 10.1016/j.csl.2011.09.004
   Ke X, 2019, IEEE T MULTIMEDIA, V21, P2093, DOI 10.1109/TMM.2019.2895511
   Kingma D. P., 2015, PROC INT C LEARN REP, P1
   Lee KH, 2018, PROC CVPR IEEE, P5447, DOI 10.1109/CVPR.2018.00571
   Li GX, 2018, IEEE T SYST MAN CY-S, V48, P982, DOI 10.1109/TSMC.2016.2627050
   Li J., 2016, EMNLP, DOI [DOI 10.18653/V1/D16-1127.URL, 10.18653/v1/D16-1127, DOI 10.18653/V1/D16-1127]
   Li Jiwei, 2016, NAACL, P110
   Li XL, 2021, IEEE T CYBERNETICS, V51, P913, DOI 10.1109/TCYB.2019.2914351
   Liu WS, 2015, PROCEEDINGS OF 2015 IEEE INTERNATIONAL CONFERENCE ON BEHAVIORAL, ECONOMIC, SOCIO-CULTURAL COMPUTING (BESC), P18, DOI 10.1109/BESC.2015.7365951
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Mao JH, 2015, IEEE I CONF COMP VIS, P2533, DOI 10.1109/ICCV.2015.291
   Mnih V., 2013, P ADV C NEUR INF PRO, P1
   Mudrakarta PK, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1896
   Myers A, 2015, IEEE I CONF COMP VIS, P1233, DOI 10.1109/ICCV.2015.146
   Narasimhan K., 2015, ARXIV150608941
   Ofli F, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P509, DOI 10.1145/3038912.3052663
   Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI [DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]
   Qi GJ, 2016, PROC CVPR IEEE, P2267, DOI 10.1109/CVPR.2016.249
   Rakthanmanon Thanawin, 2012, KDD, V2012, P262, DOI 10.1145/2339530.2339576
   Salvador A, 2019, PROC CVPR IEEE, P10445, DOI 10.1109/CVPR.2019.01070
   Salvador A, 2017, PROC CVPR IEEE, P3068, DOI 10.1109/CVPR.2017.327
   Shen ZQ, 2017, PROC CVPR IEEE, P5159, DOI 10.1109/CVPR.2017.548
   Shetty R, 2018, IEEE MULTIMEDIA, V25, P34, DOI 10.1109/MMUL.2018.112135923
   Shu XB, 2021, IEEE T PATTERN ANAL, V43, P1110, DOI 10.1109/TPAMI.2019.2942030
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Sordoni Alessandro, 2015, P 2015 C N AM CHAPT, P196, DOI [10.3115/v1/N15-1020, DOI 10.3115/V1/N15-1020]
   Tang JH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2998574
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C., 2016, P 2016 ACM MULT C, P988, DOI 10.1145/2964284.2964299
   Wang P, 2018, IEEE T PATTERN ANAL, V40, P2413, DOI 10.1109/TPAMI.2017.2754246
   Yoo YH, 2021, IEEE T CYBERNETICS, V51, P1704, DOI 10.1109/TCYB.2019.2933548
   Yu A, 2018, 2018 OPTICAL FIBER COMMUNICATIONS CONFERENCE AND EXPOSITION (OFC)
   Yuan MK, 2020, IEEE T MULTIMEDIA, V22, P1955, DOI 10.1109/TMM.2019.2951463
   Zeiler M. D., 2012, ADADELTA ANADAPTIVE
   Zhang W, 2020, IEEE T PATTERN ANAL, V42, P3088, DOI 10.1109/TPAMI.2019.2920899
NR 44
TC 3
Z9 3
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 352
EP 365
DI 10.1109/TMM.2021.3050090
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300027
DA 2024-07-18
ER

PT J
AU Zhang, ZJ
   Wu, Q
   Wang, Y
   Chen, F
AF Zhang, Zongjian
   Wu, Qiang
   Wang, Yang
   Chen, Fang
TI Exploring Pairwise Relationships Adaptively From Linguistic Context in
   Image Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Linguistics; Decoding; Modulation; Context modeling;
   Adaptation models; Semantics; Bilinear attention; bilinear
   self-attention; context-adaptive attention; dynamic linguistic context;
   image captioning; visual relationship attention
AB For image captioning, recent works start to focus on exploring visual relationships for generating high-quality interactive words (i.e. verbs and prepositions). However, many existing works only focus on semantic level by analysing the feature similarity between objects in the visual domain but ignore the linguistic context included in the caption decoder. When captioning is being carried out, the entity words can be inferred based on visual information of objects. The interactive words representing the relationships between entity words can only be inferred based on high-level language meaning generated in the process of captioning decoding. Such high-level language meaning is called linguistic context, which refers to the relational context between words or phrases in the caption sentences. The linguistic context can be used as strong guidance to explore related visual relationships between different objects effectively. To achieve this, we propose a novel context-adaptive attention module that is strongly driven by the linguistic context from the caption decoder. In this module, a novel design of visual relationship attention is proposed based on a bilinear self-attention model to explore related visual relationships and encode more discriminative features under the linguistic context. To achieve the adaptive process of attending to related visual relationships for generating interactive words or related visual objects for entity words, an attention modulator is integrated as an attention channel controller responding to the changing linguistic context of the caption decoder dynamically. Experimented on MSCOCO dataset, our model achieves promising performances compared with all counterpart models that explore visual relationships.
C1 [Zhang, Zongjian; Wu, Qiang; Wang, Yang; Chen, Fang] Univ Technol Sydney, Sch Elect & Data Engn FEIT, Sydney, NSW 2007, Australia.
C3 University of Technology Sydney
RP Zhang, ZJ (corresponding author), Univ Technol Sydney, Sch Elect & Data Engn FEIT, Sydney, NSW 2007, Australia.
EM Zongjian.Zhang@student.uts.edu.au; Qiang.Wu@uts.edu.au;
   Yang.Wang@uts.edu.au; Fang.Chen@uts.edu.au
OI Wang, Yang/0000-0002-6815-0879; Wu, Qiang/0000-0001-5641-2483; Chen,
   Fang/0000-0003-4971-8729
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Banerjee S, 2005, ACL WORKSHOP INTRINS, P65
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Cornia M, 2019, PROC CVPR IEEE, P8299, DOI 10.1109/CVPR.2019.00850
   Dauphin YN, 2017, PR MACH LEARN RES, V70
   Gu JX, 2018, AAAI CONF ARTIF INTE, P6837
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Jiang WH, 2018, LECT NOTES COMPUT SC, V11206, P510, DOI 10.1007/978-3-030-01216-8_31
   Jun SH, 2017, ICEC'17: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON ELECTRONIC COMMERCE, DOI 10.1145/3154943.3154947
   Kanai Sekitoshi., 2018, Proceedings of the 32nd International Conference on Neural Information Processing, P284
   Kingma D. P, 2015, International Conference on Learning Representations
   Kipf T. N., 2017, ARXIV
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu CX, 2017, AAAI CONF ARTIF INTE, P4176
   Longteng Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10324, DOI 10.1109/CVPR42600.2020.01034
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pedersoli M, 2017, IEEE I CONF COMP VIS, P1251, DOI 10.1109/ICCV.2017.140
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Tavakoli HR, 2017, IEEE I CONF COMP VIS, P2506, DOI 10.1109/ICCV.2017.272
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wu Q, 2018, IEEE T PATTERN ANAL, V40, P1367, DOI 10.1109/TPAMI.2017.2708709
   Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yang Y, 2018, IEEE T IMAGE PROCESS, V27, P5600, DOI 10.1109/TIP.2018.2855422
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Ye SM, 2018, IEEE T IMAGE PROCESS, V27, P5514, DOI 10.1109/TIP.2018.2855406
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhang ZJ, 2018, IEEE WINT CONF APPL, P1709, DOI 10.1109/WACV.2018.00190
   Zhao B, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2916757
   Zhou L, 2020, IEEE T IMAGE PROCESS, V29, P694, DOI 10.1109/TIP.2019.2928144
NR 42
TC 17
Z9 17
U1 2
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3101
EP 3113
DI 10.1109/TMM.2021.3093725
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000033
DA 2024-07-18
ER

PT J
AU Deng, YY
   Tang, F
   Dong, WM
   Ma, CY
   Huang, FY
   Deussen, O
   Xu, CS
AF Deng, Yingying
   Tang, Fan
   Dong, Weiming
   Ma, Chongyang
   Huang, Feiyue
   Deussen, Oliver
   Xu, Changsheng
TI Exploring the Representativity of Art Paintings
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Painting; Art; Image color analysis; Feature extraction; Task analysis;
   Engineering profession; Electronic mail; Representativity; style
   enhancement; feature representation; artwork evaluation
ID STYLE; AESTHETICS; DISCOVERY
AB Art painting evaluation is sophisticated for a novice with no or limited knowledge on art criticism, and history. In this study, we propose the concept of representativity to evaluate paintings instead of using professional concepts, such as genre, media, and style, which may be confusing to non-professionals. We define the concept of representativity to evaluate quantitatively the extent to which a painting can represent the characteristics of an artists creations. We begin by proposing a novel deep representation of art paintings, which is enhanced by style information through a weighted pooling feature fusion module. In contrast to existing feature extraction approaches, the proposed framework embeds painting styles, and authorship information, and learns specific artwork characteristics in a single framework. Subsequently, we propose a graph-based learning method for representativity learning, which considers intra-category, and extra-category information. In view of the significance of historical factors in the art domain, we introduce the creation time of a painting into the learning process. User studies demonstrate our approach helps the public effectively access the creation characteristics of artists through sorting paintings by representativity from highest to lowest.
C1 [Deng, Yingying; Tang, Fan; Dong, Weiming; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Deng, Yingying; Tang, Fan; Dong, Weiming; Xu, Changsheng] Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.
   [Ma, Chongyang] Kuaishou Technol, Beijing 100085, Peoples R China.
   [Huang, Feiyue] Tencent, YouTu Lab, Shanghai 200233, Peoples R China.
   [Deussen, Oliver] Univ Konstanz, D-78464 Constance, Germany.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Tencent;
   University of Konstanz
RP Dong, WM (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
EM dengyingying2017@ia.ac.cn; tangfan2013@ia.ac.cn; weiming.dong@ia.ac.cn;
   chongyangma@kuaishou.com; garyhuang@tencent.com;
   oliver.deussen@uni-konstanz.de; changsheng.xu@ia.ac.cn
RI DONG, Weiming/AAG-7678-2020; Deussen, Oliver/HKF-2004-2023; Xu,
   Chang/GQP-7280-2022; Tang, Fan/O-3923-2018; xu, cj/HJZ-3488-2023
OI DONG, Weiming/0000-0001-6502-145X; Tang, Fan/0000-0002-3975-2483; 
FU National Natural Science Foundation of China [61832016, 61672520,
   61702488]; CASIA-Tencent Youtu joint research project
FX Manuscript received July 18, 2019; revised June 4, 2020; accepted August
   6, 2020. Date of publication August 14, 2020; date of current version
   August 24, 2021. This work was supported in part by National Natural
   Science Foundation of China under Grants 61832016, 61672520, and
   61702488 and in part by CASIA-Tencent Youtu joint research project. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Federica Battisti. (Corresponding
   author: Weiming Dong.)
CR [Anonymous], 2016, P 24 ACM INT C MULT
   [Anonymous], 2018, ECCV 2018
   Carneiro G, 2012, LECT NOTES COMPUT SC, V7575, P143, DOI 10.1007/978-3-642-33765-9_11
   Cetinic E, 2018, EXPERT SYST APPL, V114, P107, DOI 10.1016/j.eswa.2018.07.026
   Chen LY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2459, DOI 10.1145/3343031.3350977
   Chu WT, 2018, IEEE T MULTIMEDIA, V20, P2491, DOI 10.1109/TMM.2018.2801718
   Deng YY, 2019, MULTIMED TOOLS APPL, V78, P19305, DOI 10.1007/s11042-019-7271-7
   Deng YY, 2017, SIGGRAPH ASIA 2017 POSTERS (SA'17), DOI 10.1145/3145690.3145706
   Dewhurst M., 2014, Social justice art: A framework for activist art pedagogy
   Doyle L, 2019, COMPUT VIS MEDIA, V5, P33, DOI 10.1007/s41095-019-0129-0
   Elgammal A, 2018, AAAI CONF ARTIF INTE, P2183
   Feng S.-Y., 2014, COMPUTER VISION, V8916, P124
   Floater MS, 2003, COMPUT AIDED GEOM D, V20, P19, DOI 10.1016/S0167-8396(03)00002-5
   Fraiberger SP, 2018, SCIENCE, V362, P825, DOI 10.1126/science.aau7224
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gombrich E. H., 1995, STORY ART, V12, P155
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hua KL, 2020, MULTIMED TOOLS APPL, V79, P12635, DOI 10.1007/s11042-019-08547-4
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Hulsker J, 1996, NEW COMPLETE GOGH PA
   Jangtjik KA, 2016, P 24 ACM INT C MULT, P635, DOI DOI 10.1145/2964284.2967299
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim D, 2014, SCI REP-UK, V4, DOI 10.1038/srep07370
   Kim M., 2013, P INT C HUM COMP INT, P258
   Liu GW, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2162
   Liu J, 2017, MULTIMED TOOLS APPL, V76, P1017, DOI 10.1007/s11042-015-3104-5
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P1724, DOI 10.1109/TMM.2017.2780761
   Ma DQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1174, DOI 10.1145/3123266.3123325
   Mao H, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1183, DOI 10.1145/3123266.3123405
   Ornes S, 2019, P NATL ACAD SCI USA, V116, P4760, DOI 10.1073/pnas.1900883116
   Pelowski M, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01729
   Perc M, 2020, J R SOC INTERFACE, V17, P1
   PERKINS DN, 1988, J AESTHET EDUC, V22, P111, DOI 10.2307/3332969
   Saleh B, 2016, MULTIMED TOOLS APPL, V75, P3565, DOI 10.1007/s11042-014-2193-x
   Saleh B, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON DATA MINING WORKSHOP (ICDMW), P1254, DOI 10.1109/ICDMW.2015.93
   Sargentis GF, 2020, HERITAGE-BASEL, V3, P283, DOI 10.3390/heritage3020017
   Sargentis GF, 2019, ENERGIES, V12, DOI 10.3390/en12142817
   Sartori A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P311, DOI 10.1145/2733373.2806250
   Shamir L, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1670671.1670672
   Shen IC, 2015, IEEE T MULTIMEDIA, V17, P526, DOI 10.1109/TMM.2015.2405350
   Sigaki HYD, 2018, P NATL ACAD SCI USA, V115, pE8585, DOI 10.1073/pnas.1800083115
   Tan WR, 2016, IEEE IMAGE PROC, P3703, DOI 10.1109/ICIP.2016.7533051
   Tarvainen J, 2014, IEEE T MULTIMEDIA, V16, P2085, DOI 10.1109/TMM.2014.2357688
   Taylor RP, 1999, NATURE, V399, P422, DOI 10.1038/20833
   van der Maaten L, 2014, J MACH LEARN RES, V15, P3221
   van Noord N, 2015, IEEE SIGNAL PROC MAG, V32, P46, DOI 10.1109/MSP.2015.2406955
   Wynen Daan., 2018, Advances in Neural Information Processing Systems, V31, P6584
   Yang JF, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1154, DOI 10.1145/3240508.3240593
   Zhang K, 2012, LEONARDO, V45, P243, DOI 10.1162/LEON_a_00366
   Zhou YF, 2019, IEEE T MULTIMEDIA, V21, P3136, DOI 10.1109/TMM.2019.2920613
NR 50
TC 7
Z9 7
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2794
EP 2805
DI 10.1109/TMM.2020.3016887
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600020
DA 2024-07-18
ER

PT J
AU Gu, ZX
   Niu, L
   Zhao, HH
   Zhang, LQ
AF Gu, Zhangxuan
   Niu, Li
   Zhao, Haohua
   Zhang, Liqing
TI Hard Pixel Mining for Depth Privileged Semantic Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Image segmentation; Training; Task analysis; Fuses;
   Measurement uncertainty; Testing; Semantic segmentation; hard samples
   mining; privileged information; RGBD semantic segmentation
ID DEEP; INFORMATION; NETWORKS
AB Semantic segmentation has achieved remarkable progress but remains challenging due to the complex scene, object occlusion, and so on. Some research works have attempted to use extra information such as a depth map to help RGB based semantic segmentation because the depth map could provide complementary geometric cues. However, due to the inaccessibility of depth sensors, depth information is usually unavailable for the test images. In this paper, we leverage only the depth of training images as the privileged information to mine the hard pixels in semantic segmentation, in which depth information is only available for training images but not available for test images. Specifically, we propose a novel Loss Weight Module, which outputs a loss weight map by employing two depth-related measurements of hard pixels: Depth Prediction Error and Depth-aware Segmentation Error. The loss weight map is then applied to segmentation loss, with the goal of learning a more robust model by paying more attention to the hard pixels. Besides, we also explore a curriculum learning strategy based on the loss weight map. Meanwhile, to fully mine the hard pixels on different scales, we apply our loss weight module to multi-scale side outputs. Our hard pixels mining method achieves the state-of-the-art results on three benchmark datasets, and even outperforms the methods which need depth input during testing.
C1 [Gu, Zhangxuan; Niu, Li; Zhao, Haohua; Zhang, Liqing] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, MOE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Zhang, LQ (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, MOE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China.
EM zhangxgu@126.com; ustcnewly@sjtu.edu.cn; haoh.zhao@sjtu.edu.cn;
   zhang-lq@cs.sjtu.edu.cn
FU National Key R&D Program of China [2018AAA0100704]; Science and
   Technology Commission of Shanghai, China [20511100300]; National Natural
   Science Foundation of China [61902247]; Shanghai Sailing Program
   [19YF1424400]
FX Manuscript received March 10, 2020; revised July 19, 2020 and August 31,
   2020; accepted October 11, 2020. Date of publication November 2, 2020;
   date of current version October 19, 2021. This work was supported in
   part by the National Key R&D Program of China under Grant
   2018AAA0100704, in part by the Science and Technology Commission of
   Shanghai, China under Grant 20511100300, and in part by the National
   Natural Science Foundation of China Grant 61902247 and Shanghai Sailing
   Program 19YF1424400. (Corresponding authors: Li Niu; Liqing Zhang.)
CR [Anonymous], 2017, P BRIT MACH VIS C IM
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bulò SR, 2017, PROC CVPR IEEE, P7082, DOI 10.1109/CVPR.2017.749
   Chandra S, 2016, LECT NOTES COMPUT SC, V9911, P402, DOI 10.1007/978-3-319-46478-7_25
   Chen L. Z, 2016, P INT JOINT C ART IN
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Chen YH, 2019, PROC CVPR IEEE, P1841, DOI 10.1109/CVPR.2019.00194
   Cheng YH, 2017, PROC CVPR IEEE, P1475, DOI 10.1109/CVPR.2017.161
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai JF, 2015, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2015.7299025
   Deng Liuyuan, 2019, RFBNET DEEP MULTIMOD
   Ding HH, 2018, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2018.00254
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Feyereisl J, 2012, INFORM SCIENCES, V194, P4, DOI 10.1016/j.ins.2011.04.025
   Fouad S, 2013, IEEE IJCNN
   Fouad S, 2013, IEEE T NEUR NET LEAR, V24, P1086, DOI 10.1109/TNNLS.2013.2251470
   Graves A, 2017, PR MACH LEARN RES, V70
   Gupta S, 2015, INT J COMPUT VISION, V112, P133, DOI 10.1007/s11263-014-0777-6
   Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79
   Hazirbas C, 2017, LECT NOTES COMPUT SC, V10111, P213, DOI 10.1007/978-3-319-54181-5_14
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoffman J, 2016, PROC CVPR IEEE, P826, DOI 10.1109/CVPR.2016.96
   Hu XX, 2019, IEEE IMAGE PROC, P1440, DOI [10.1109/ICIP.2019.8803025, 10.1109/icip.2019.8803025]
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Hung SW, 2019, IEEE IMAGE PROC, P2374, DOI [10.1109/icip.2019.8803360, 10.1109/ICIP.2019.8803360]
   Jampani V, 2016, PROC CVPR IEEE, P4452, DOI 10.1109/CVPR.2016.482
   Jiao JB, 2018, LECT NOTES COMPUT SC, V11219, P55, DOI 10.1007/978-3-030-01267-0_4
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Lambert J, 2018, PROC CVPR IEEE, P8886, DOI 10.1109/CVPR.2018.00926
   Lee K.-H., 2019, ICLR, P1
   Li J, 2019, IEEE T MULTIMEDIA, V21, P2531, DOI 10.1109/TMM.2019.2908350
   Li W, 2014, LECT NOTES COMPUT SC, V8693, P437, DOI 10.1007/978-3-319-10602-1_29
   Li XX, 2017, PROC CVPR IEEE, P6459, DOI 10.1109/CVPR.2017.684
   Lin D, 2017, IEEE I CONF COMP VIS, P1320, DOI 10.1109/ICCV.2017.147
   Lin D, 2020, IEEE T CYBERNETICS, V50, P1120, DOI 10.1109/TCYB.2018.2885062
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu J, 2018, IEEE T NEUR NET LEAR, V29, P5655, DOI 10.1109/TNNLS.2017.2787781
   Liu ZW, 2015, IEEE I CONF COMP VIS, P1377, DOI 10.1109/ICCV.2015.162
   Lotter W, 2017, LECT NOTES COMPUT SC, V10553, P169, DOI 10.1007/978-3-319-67558-9_20
   Mostajahi M, 2015, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2015.7298959
   Nekrasov V, 2018, P BRIT MACH VIS C BM
   Nekrasov V, 2019, IEEE INT CONF ROBOT, P7101, DOI [10.1109/icra.2019.8794220, 10.1109/ICRA.2019.8794220]
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Olszewska JI, 2015, NEUROCOMPUTING, V161, P65, DOI 10.1016/j.neucom.2014.12.089
   Park SJ, 2017, IEEE I CONF COMP VIS, P4990, DOI 10.1109/ICCV.2017.533
   Pawan Kumar M., 2010, NIPS
   Pentina A, 2015, PROC CVPR IEEE, P5492, DOI 10.1109/CVPR.2015.7299188
   Qi XJ, 2017, IEEE I CONF COMP VIS, P5209, DOI 10.1109/ICCV.2017.556
   Qiurui Wang, 2019, IEEE Transactions on Multimedia, V21, P1839, DOI 10.1109/TMM.2018.2890360
   Romera E, 2018, IEEE T INTELL TRANSP, V19, P263, DOI 10.1109/TITS.2017.2750080
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sarafianos N, 2017, IEEE INT CONF COMP V, P2637, DOI 10.1109/ICCVW.2017.313
   Schneider L, 2017, LECT NOTES COMPUT SC, V10269, P98, DOI 10.1007/978-3-319-59126-1_9
   Sharmanska V, 2013, IEEE I CONF COMP VIS, P825, DOI 10.1109/ICCV.2013.107
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Sungha Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9370, DOI 10.1109/CVPR42600.2020.00939
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Vapnik V, 2009, NEURAL NETWORKS, V22, P544, DOI 10.1016/j.neunet.2009.06.042
   Vemulapalli R, 2016, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2016.351
   Vu TH, 2019, IEEE I CONF COMP VIS, P7363, DOI 10.1109/ICCV.2019.00746
   Wang JH, 2016, LECT NOTES COMPUT SC, V9909, P664, DOI 10.1007/978-3-319-46454-1_40
   Wang P, 2015, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2015.7298897
   Wang WY, 2018, LECT NOTES COMPUT SC, V11215, P144, DOI 10.1007/978-3-030-01252-6_9
   Wang WG, 2016, IEEE T MULTIMEDIA, V18, P1011, DOI 10.1109/TMM.2016.2545409
   Xu D, 2018, PROC CVPR IEEE, P675, DOI 10.1109/CVPR.2018.00077
   Xu XX, 2015, IEEE T NEUR NET LEAR, V26, P3150, DOI 10.1109/TNNLS.2015.2405574
   Yang H, 2017, PROC CVPR IEEE, P5996, DOI 10.1109/CVPR.2017.635
   Yang HM, 2013, ADV INTEL SYS RES, V37, P1, DOI 10.1186/1687-1847-2013-148
   Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022
   Zhang Y, 2017, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2017.223
   Zhang ZY, 2019, PROC CVPR IEEE, P4101, DOI 10.1109/CVPR.2019.00423
   Zhang ZY, 2018, LECT NOTES COMPUT SC, V11214, P238, DOI 10.1007/978-3-030-01249-6_15
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zhou F, 2018, LECT NOTES COMPUT SC, V11166, P3, DOI 10.1007/978-3-030-00764-5_1
   Zhu Y, 2019, PROC CVPR IEEE, P8848, DOI 10.1109/CVPR.2019.00906
NR 84
TC 17
Z9 17
U1 2
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3738
EP 3751
DI 10.1109/TMM.2020.3035231
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100024
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hu, J
   Qian, SS
   Fang, Q
   Xu, CS
AF Hu, Jun
   Qian, Shengsheng
   Fang, Quan
   Xu, Changsheng
TI Heterogeneous Community Question Answering via Social-Aware Multi-Modal
   Co-Attention Convolutional Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Semantics; Knowledge discovery; Context modeling;
   Portable computers; Task analysis; Object detection; Question-answering;
   attention; multi-modal; social multimedia
ID RECOMMENDATION
AB Nowadays, community-based question answering (CQA) systems are popular and have accumulated a large number of questions and answers provided by users. How to accurately match relevant answers for a given question is an essential function in CQA tasks. Recent effective methods utilize word-pair interactions between questions and answers for CQA matching. However, these approaches usually encode questions and answers independently and ignore the fact that they can complement and enhance each other to provide better representations and thus more implicit interactions can be captured. In addition, the visual information, social information and the variable-length problem are usually ignored by most existing approaches. In this paper, a Social-aware Multi-modal Co-attention Convolutional Matching method (SMCACM) is proposed, which models the multi-modal content and social context of questions and answers in a unified framework for CQA matching. A novel co-attention network is proposed to extract complementary information from questions and answers to enhance each other for obtaining better representations, through which our model can capture more implicit interactions between questions and answers. In addition to textual content, our model uses object detection techniques and a meta-path based heterogeneous social representation learning approach to take advantage of the visual content and social context in CQA systems, respectively. Finally, a pooling-based convolutional matching network is designed to infer the matching score based on the complemented questions and answers, which can accept variable-length answers as inputs without padding or cutting. Experimental results on two real-world datasets demonstrate the superior performance of SMCACM compared with other state-of-the-art algorithms.
C1 [Hu, Jun; Qian, Shengsheng; Fang, Quan; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Qian, Shengsheng; Fang, Quan; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Xu, Changsheng] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Peng Cheng Laboratory
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Xu, CS (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
EM hujunxianligong@gmail.com; shengsheng.qian@nlpria.ac.cn;
   qfang@nlpr.ia.ac.cn; csxu@nlpria.ac.cn
RI xu, cj/HJZ-3488-2023; Xu, Chang/GQP-7280-2022
OI xu, chang sheng/0000-0001-8343-9665
FU National Key Research and Development Program of China [2017YFB1002804];
   National Natural Science Foundation of China [61720106006, 61572503,
   61802405, 61872424, 61702509, 61832002, 61936005, U1705262]; Key
   Research Program of Frontier Sciences, CAS [QYZDJ-SSW-JSC039]; K. C.
   Wong Education Foundation
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2017YFB1002804, in part by the
   National Natural Science Foundation of China under Grants, 61720106006,
   61572503, 61802405, 61872424, 61702509, 61832002, 61936005, and
   U1705262, in part by the Key Research Program of Frontier Sciences, CAS,
   under Grant QYZDJ-SSW-JSC039, and in part by the K. C. Wong Education
   Foundation. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. James She.
CR [Anonymous], 2014, Advances in Neural Information Processing Systems
   [Anonymous], 2014, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), DOI [DOI 10.3115/V1/D14-1181, 10.3115/v1/D14-1181]
   [Anonymous], 2016, P 25 INT JOINT C ART
   [Anonymous], 2009, ACM WORKSH LARG SCAL
   Bachrach Y, 2017, PROC INT C TOOLS ART, P425, DOI 10.1109/ICTAI.2017.00072
   Bilotti M.W., 2010, Proceedings of the 19th ACM international conference on Information and knowledge management, CIKM '10, P459
   Fang HY, 2016, AAAI CONF ARTIF INTE, P122
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Guo JF, 2016, CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P55, DOI 10.1145/2983323.2983769
   Gurevych Iryna, 2017, P IWCS 2017 12 INT C
   Hu J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P456, DOI 10.1145/3240508.3240626
   Huang PS, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P2333
   Huang XW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P447, DOI 10.1145/3240508.3240609
   Iida S., 2019, P 57 C ASS COM LING, V2
   Le Quoc V., 2014, P INT C MACH LEARN I
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Nie LQ, 2013, IEEE T MULTIMEDIA, V15, P426, DOI 10.1109/TMM.2012.2229971
   Pang L, 2016, AAAI CONF ARTIF INTE, P2793
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Qian Shengsheng, 2016, P 24 ACM INT C MULTI
   Qiu XP, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1305
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Shen YL, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P373, DOI 10.1145/2567948.2577348
   Shen YK, 2015, AAAI CONF ARTIF INTE, P275
   Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang XZ, 2018, J COMPUT SCI TECH-CH, V33, P625, DOI 10.1007/s11390-018-1845-0
   Xiong CY, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P55, DOI 10.1145/3077136.3080809
   Yang L, 2016, CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P287, DOI 10.1145/2983323.2983818
   Zhang J, 2017, IEEE T MULTIMEDIA, V19, P2439, DOI 10.1109/TMM.2017.2701641
   Zhang WQ, 2020, IEEE T MULTIMEDIA, V22, P1032, DOI 10.1109/TMM.2019.2935678
   Zhang XD, 2017, AAAI CONF ARTIF INTE, P3525
   Zhang YY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1089, DOI 10.1145/3343031.3351033
   Zhao Z, 2018, IEEE T MULTIMEDIA, V20, P430, DOI 10.1109/TMM.2017.2740022
NR 34
TC 6
Z9 6
U1 2
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2321
EP 2334
DI 10.1109/TMM.2020.3009491
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800013
DA 2024-07-18
ER

PT J
AU Ji, W
   Poor, HV
AF Ji, Wen
   Poor, H. Vincent
TI Risk Optimization for Revenue-Driven Wireless Video Broadcasting
   Systems: A Copula-Based Framework
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Wireless communication; Broadcasting; Multimedia communication; Reactive
   power; Pricing; Analytical models; Uncertainty; Wireless video
   broadcasting; risk; VaR; revenue; copula; polymatroid
ID PROFIT MAXIMIZATION; RESOURCE-ALLOCATION; DELIVERY; NETWORKS
AB The revenue of wireless service providers (WSPs) relies on their ability to efficiently satisfy the variable demands from end users (EUs). However, emerging video services create new risks owing to the diverse content requirements of heterogeneous EUs operating in uncertain markets. It is challenging as the risks created by demands and prices are highly uncertain. In this work, a risk problem of high revenues is studied in which multiple video content items with different prices are broadcast to wireless EUs. The video content prices differ in their value functions in terms of popularity, ratings, and types. The objective is to maximize the revenue of WSPs under a certain Value-at-Risk (VaR) by adjusting the content prices and allocation of bandwidth resources. Furthermore, a VaR-based optimization framework for wireless video broadcasting systems is presented. First, the content characteristics are analyzed, and a copula model is then used to build the content value structure. The copula of a multivariate distribution corresponds to the description of the price-dependent structure. Second, a risk analysis for the effects of price fluctuations on revenues caused by uncertainty is conducted. A VaR model is associated with changes in the prices and allocated rates. Copulas are used to derive a bound on the VaR for functions of dependent risks. Subsequently, detailed representations are provided to identify the distributional bounds for revenue functions of dependent risks. Lastly, a VaR-based framework that optimizes pricing and bandwidth provision is presented. For the solution, the risk regions of WSPs are modeled as polymatroidal structures to minimize the risk caused by different service demands and variable market prices. Experiments on different price markets demonstrated that the proposed method is effective, thereby verifying the feasibility of the proposed method.
C1 [Ji, Wen] Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100190, Peoples R China.
   [Ji, Wen] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Poor, H. Vincent] Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Peng Cheng Laboratory; Princeton University
RP Ji, W (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100190, Peoples R China.
EM jiwen@ict.ac.cn; poor@princeton.edu
RI Poor, H. Vincent/S-5027-2016
OI Poor, H. Vincent/0000-0002-2062-131X; Ji, Wen/0000-0001-6895-3404
FU National Key R&D Program of China [2017YFB1400100]; National Natural
   Science Foundation of China [61572466]; BeijingNatural Science
   Foundation [4202072]; U.S. National Science Foundation [CCF-0939370,
   CCF-1908308]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2017YFB1400100, in part by the National Natural Science
   Foundation of China under Grant 61572466, in part by the BeijingNatural
   Science Foundation under Grant 4202072, and in part by the U.S. National
   Science Foundation under Grants CCF-0939370 and CCF-1908308.
CR Artzner P, 1999, MATH FINANC, V9, P203, DOI 10.1111/1467-9965.00068
   Bagci KT, 2018, IEEE T MULTIMEDIA, V20, P3084, DOI 10.1109/TMM.2018.2823907
   Choudhry M, 2007, INTRO VALUE RISK
   D'Aronco S, 2017, IEEE MULTIMEDIA, V24, P20, DOI 10.1109/MMUL.2017.41
   Derrode S, 2013, COMPUT STAT DATA AN, V63, P81, DOI 10.1016/j.csda.2013.01.027
   Elgabli A, 2019, IEEE ACM T NETWORK, V27, P1138, DOI 10.1109/TNET.2019.2911523
   Embrechts P, 2003, FINANC STOCH, V7, P145, DOI 10.1007/s007800200085
   Gómez JF, 2019, IEEE T NETW SERV MAN, V16, P755, DOI 10.1109/TNSM.2019.2903985
   Fritz T, 2013, IEEE T INFORM THEORY, V59, P803, DOI 10.1109/TIT.2012.2222863
   Gao GY, 2017, IEEE T MULTIMEDIA, V19, P836, DOI 10.1109/TMM.2016.2635019
   GroupLens, 2019, MOV LAT DAT
   Hande P, 2009, IEEE INFOCOM SER, P990, DOI 10.1109/INFCOM.2009.5062010
   Hoiles W, 2021, IEEE T CLOUD COMPUT, V9, P331, DOI 10.1109/TCC.2018.2855160
   Huang XL, 2020, IEEE T MULTIMEDIA, V22, P201, DOI 10.1109/TMM.2019.2925960
   Huard D, 2006, COMPUT STAT DATA AN, V51, P809, DOI 10.1016/j.csda.2005.08.010
   Iyengar SG, 2011, IEEE T SIGNAL PROCES, V59, P2308, DOI 10.1109/TSP.2011.2105483
   Ji W, 2016, IEEE T MOBILE COMPUT, V15, P2064, DOI 10.1109/TMC.2015.2485984
   Ji W, 2015, IEEE T MULTIMEDIA, V17, P2310, DOI 10.1109/TMM.2015.2479860
   Ji W, 2015, IEEE T MOBILE COMPUT, V14, P1659, DOI 10.1109/TMC.2014.2362919
   Joe-Wong C., 2015, Proc. of IEEE INFOCOM, P1499
   Jung SY, 2005, IEEE T KNOWL DATA EN, V17, P834, DOI 10.1109/TKDE.2005.86
   Kwitt R, 2011, IEEE T IMAGE PROCESS, V20, P2063, DOI 10.1109/TIP.2011.2108663
   Li SQ, 2014, IEEE ACM T NETWORK, V22, P703, DOI 10.1109/TNET.2013.2258173
   Li SQ, 2014, IEEE T MOBILE COMPUT, V13, P526, DOI 10.1109/TMC.2013.10
   Lu Z, 2019, IEEE T MULTIMEDIA, V21, P197, DOI 10.1109/TMM.2018.2847240
   Luong NC, 2017, IEEE COMMUN SURV TUT, V19, P954, DOI 10.1109/COMST.2017.2647981
   Nadarajah S., COMPENDIUM COPULAS
   Nelsen, 2006, INTRO COPULAS
   Parack S., 2012, 2012 IEEE International Conference on Technology Enhanced Education (Ictee 2012), P1
   Parakh S., 2012, P INT C SIGN PROC CO, P1
   Pizzi S, 2016, IEEE T WIREL COMMUN, V15, P8063, DOI 10.1109/TWC.2016.2612201
   Rainer B, 2017, IEEE T MULTIMEDIA, V19, P849, DOI 10.1109/TMM.2016.2629761
   Ren SL, 2012, IEEE T MULTIMEDIA, V14, P1566, DOI 10.1109/TMM.2012.2217120
   Sen S, 2014, WILEY SER INF COMMUN, P1, DOI 10.1002/9781118899250
   SILVERMAN BW, 1982, APPL STAT-J ROY ST C, V31, P93, DOI 10.2307/2347084
   Singhal C, 2019, IEEE T MOBILE COMPUT, V18, P1703, DOI 10.1109/TMC.2018.2864567
   Sun LF, 2017, IEEE T MULTIMEDIA, V19, P609, DOI 10.1109/TMM.2016.2635589
   Trestian R, 2018, IEEE COMMUN SURV TUT, V20, P945, DOI 10.1109/COMST.2018.2789722
   Tse DNC, 1998, IEEE T INFORM THEORY, V44, P2796, DOI 10.1109/18.737513
   Wang CY, 2015, IEEE T WIREL COMMUN, V14, P2353, DOI 10.1109/TWC.2014.2385773
   Wang WH, 2006, IEEE ACM T NETWORK, V14, P1282, DOI 10.1109/TNET.2006.886318
   Zenghelis D, 2011, CISC VIS NETW IND GL
   Zhou LP, 2016, IEEE T PATTERN ANAL, V38, P2269, DOI 10.1109/TPAMI.2015.2511754
NR 43
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1757
EP 1771
DI 10.1109/TMM.2020.3002612
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300022
OA hybrid
DA 2024-07-18
ER

PT J
AU Lan, T
   Cai, ZC
AF Lan, Ting
   Cai, Zhanchuan
TI A Novel Image Representation Method Under a Non-Standard Positional
   Numeral System
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Positional numeral system; complex base; Gaussian integer; 0 similar to
   1 sequence; image representation; secure encryption
ID NUMBER-SYSTEMS; ENCRYPTION; ALGORITHM
AB Image representation is an active research area in the field of image processing. This paper proposes a novel image representation method under a non-standard positional numeral system, wherein complex numbers are used as bases in such nonstandard positional numeral system. It is different with binary code and decimal code, where the digit 2 is used as the base in the binary system, and the digit 10 is used as the base in the decimal system. In the proposed image representation method, a two-dimensional image is transformed into a one-dimensional 0 similar to 1 sequence, and its Gaussian integer is calculated based on the derived onedimensional 0 similar to 1 sequence. On the contrary, the original twodimensional image can be recovered from its Gaussian integer. When images are represented as Gaussian integers, the classical geometrical operations are introduced into image processing. Then, the relationship of different images is established by the methods of plane geometry, i.e., the addition, subtraction, multiplication, division, conjugate, and inverse operations of images are defined. The experimental results show that the selection of complex number as base in the positional numeral system is a very special coding method, a given digital image is effectively converted to a Gaussian integer by using the proposed image representation method, and the image arithmetic is also successfully achieved. In addition, three applications based on the proposed image representation method including image camouflage, image sharing, and image scrambling are selected to demonstrate that the new image representation has good and potential practical applicability in the field of secure encryption of digital images.
C1 [Lan, Ting; Cai, Zhanchuan] Macau Univ Sci & Technol, Fac Informat Technol, Macau 999078, Peoples R China.
C3 Macau University of Science & Technology
RP Cai, ZC (corresponding author), Macau Univ Sci & Technol, Fac Informat Technol, Macau 999078, Peoples R China.
EM lantingleo@gmail.com; zccai@must.edu.mo
RI Lan, Ting/AGP-1142-2022; Lan, Ting/GWV-2392-2022
OI LAN, TING/0000-0002-3553-4323
FU National Basic Research Program of China (973 Program) [2011CB302400];
   Science and Technology Development Fund of Macau [0052/2020/AFJ,
   0038/2020/A, 0012/2018/A1, 0069/2018/A2]; Major Scientific Research
   Project for Universities of Guangdong Province [2017KTSCX207]; Open Fund
   of the State Key Laboratory of Remote Sensing Science [OFSLRSS201901];
   Open Project Program of the State Key Laboratory of Virtual Reality
   Technology and Systems at Beihang University [VRLAB2019C02]; Open
   Project Program of the State Key Laboratory of CAD&CG at Zhejiang
   University [A1910]
FX This work was supported in part by the National Basic Research Program
   of China (973 Program) under Grant 2011CB302400, in part by the Science
   andTechnology Development Fund of Macau underGrants 0052/2020/AFJ,
   0038/2020/A, 0012/2018/A1, and 0069/2018/A2, in part by the Major
   Scientific Research Project forUniversities of Guangdong Province under
   Grant 2017KTSCX207, in part by the Open Fund of the State Key Laboratory
   of Remote Sensing Science under Grant OFSLRSS201901, in part by the Open
   Project Program of the State Key Laboratory of Virtual Reality
   Technology and Systems at Beihang University under Grant VRLAB2019C02,
   and in part by the Open Project Program of the State Key Laboratory of
   CAD&CG at Zhejiang University under Grant A1910.
CR Abdelhamed A, 2019, IEEE COMPUT SOC CONF, P2197, DOI 10.1109/CVPRW.2019.00273
   Bao L, 2015, INFORM SCIENCES, V324, P197, DOI 10.1016/j.ins.2015.06.049
   Bergman G., 1957, Math. Mag., V31, P98
   Christgen SL, 2019, METHOD ENZYMOL, V620, P1, DOI 10.1016/bs.mie.2019.03.004
   De Cosmis S, 2012, APPL MATH COMPUT, V218, P8029, DOI 10.1016/j.amc.2011.07.042
   Dimitrov V. S., 2003, IEEE Circuits and Systems Magazine, V3, P6, DOI 10.1109/MCAS.2003.1242832
   Elmasry A, 2012, THEOR COMPUT SYST, V50, P185, DOI 10.1007/s00224-011-9357-0
   Farwa S, 2017, 3D RES, V8, DOI 10.1007/s13319-017-0135-x
   Gao ZN, 2016, IEEE T MULTIMEDIA, V18, P1661, DOI 10.1109/TMM.2016.2568748
   Gilbert W. J., 1982, Math. Intell., V4, P78
   GILBERT WJ, 1982, CAN J MATH, V34, P1335, DOI 10.4153/CJM-1982-093-4
   Hamidi M, 2018, MULTIMED TOOLS APPL, V77, P27181, DOI 10.1007/s11042-018-5913-9
   Heuberger C, 2008, MONATSH MATH, V155, P349, DOI 10.1007/s00605-008-0008-8
   Hou DD, 2018, J VIS COMMUN IMAGE R, V53, P134, DOI 10.1016/j.jvcir.2017.11.014
   Hou DD, 2016, J VIS COMMUN IMAGE R, V40, P225, DOI 10.1016/j.jvcir.2016.06.018
   Islam MB, 2018, IEEE T MULTIMEDIA, V20, P2964, DOI 10.1109/TMM.2018.2820324
   Iudin DI, 2015, COMMUN NONLINEAR SCI, V20, P861, DOI 10.1016/j.cnsns.2014.06.031
   Kanso A, 2017, OPT LASER ENG, V90, P196, DOI 10.1016/j.optlaseng.2016.10.009
   KATAI I, 1975, ACTA SCI MATH, V37, P255
   Kim K, 2011, IEEE T MULTIMEDIA, V13, P1208, DOI 10.1109/TMM.2011.2168197
   KNUTH DE, 1960, COMMUN ACM, V3, P245, DOI 10.1145/367177.367233
   Kumar A, 2019, J INF SECUR APPL, V45, P35, DOI 10.1016/j.jisa.2019.01.004
   Lagarias JC, 1996, T AM MATH SOC, V348, P99, DOI 10.1090/S0002-9947-96-01538-3
   Lama RK, 2014, MULTIMED TOOLS APPL, V73, P873, DOI 10.1007/s11042-013-1381-4
   Lee YL, 2014, IEEE T CIRC SYST VID, V24, P695, DOI 10.1109/TCSVT.2013.2283431
   Liu JM, 2019, IEEE COMPUT SOC CONF, P2070, DOI 10.1109/CVPRW.2019.00259
   Mishra M., 2012, INT J CRYPTOGRAPHY I, V2, P131
   Ogawa T, 2011, IEEE T MULTIMEDIA, V13, P974, DOI 10.1109/TMM.2011.2161760
   PLANTZ AR, 1971, IEEE T COMPUT, VC 20, P593, DOI 10.1109/T-C.1971.223307
   QI DX, 1994, FRACTAL ITS COMPUTER
   Sergeev IS, 2018, PROBL INFORM TRANSM+, V54, P343, DOI 10.1134/S0032946018040038
   Shen FM, 2017, IEEE T MULTIMEDIA, V19, P2022, DOI 10.1109/TMM.2017.2699863
   Song R. X., 2004, P CHINAGRAPH 2004, P324
   Sui LS, 2013, OPT LASER TECHNOL, V48, P530, DOI 10.1016/j.optlastec.2012.11.020
   Sun W., 2003, J. Image Graph., V8, P626
   Sun YP, 2016, IEEE T MULTIMEDIA, V18, P171, DOI 10.1109/TMM.2015.2496246
   Wang C, 2019, AAAI CONF ARTIF INTE, P5232
   Wang CA, 2017, IEEE T IMAGE PROCESS, V26, P1833, DOI 10.1109/TIP.2017.2666742
   Wang J, 2019, INT J THEOR PHYS, V58, P308, DOI 10.1007/s10773-018-3932-y
   Wu XT, 2019, J VIS COMMUN IMAGE R, V61, P74, DOI 10.1016/j.jvcir.2019.03.020
   Xue XH, 2005, IEEE T MULTIMEDIA, V7, P805, DOI 10.1109/TMM.2005.854471
   Yan CG, 2019, IEEE T MULTIMEDIA, V21, P2675, DOI 10.1109/TMM.2019.2903448
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yan CG, 2018, IEEE T MULTIMEDIA, V20, P3389, DOI 10.1109/TMM.2018.2838320
   Yan CG, 2014, IEEE T CIRC SYST VID, V24, P2077, DOI 10.1109/TCSVT.2014.2335852
   Yang CN, 2016, J SYST SOFTWARE, V116, P22, DOI 10.1016/j.jss.2015.01.031
   Yang WM, 2016, IEEE T MULTIMEDIA, V18, P313, DOI 10.1109/TMM.2016.2515997
   Yin JL, 2018, IEEE T MULTIMEDIA, V20, P3045, DOI 10.1109/TMM.2018.2820910
   Yu XY, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11101227
   Zhang YX, 2008, IEEE T MULTIMEDIA, V10, P1648, DOI 10.1109/TMM.2008.2007324
   Zhang YB, 2016, IEEE T MULTIMEDIA, V18, P405, DOI 10.1109/TMM.2015.2512046
   Zhao LP, 2018, IEEE T MULTIMEDIA, V20, P796, DOI 10.1109/TMM.2017.2758519
   Zhou X, 2018, SYMMETRY-BASEL, V10, DOI 10.3390/sym10070249
   Zhu SY, 2018, IEEE T MULTIMEDIA, V20, P525, DOI 10.1109/TMM.2017.2749162
   [朱喜玲 Zhu Xiling], 2015, [中国图象图形学报, Journal of Image and Graphics], V20, P618
NR 55
TC 2
Z9 2
U1 2
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1301
EP 1315
DI 10.1109/TMM.2020.2995258
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RU3SE
UT WOS:000645068200010
DA 2024-07-18
ER

PT J
AU Tian, SJ
   Liu, XP
   Liu, M
   Li, SH
   Yin, BC
AF Tian, Shengjing
   Liu, Xiuping
   Liu, Meng
   Li, Shuhua
   Yin, Baocai
TI Siamese Tracking Network With Informative Enhanced Loss
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Logistics; Target tracking; Training; Real-time systems; Machine
   learning; Benchmark testing; Correlation; Visual tracking; siamese
   network; enhanced loss; deep learning
ID OBJECT TRACKING
AB Designing an effective and uniform framework to meliorate tracking performance is very meaningful and essential. However, existing methods merely focus on single positive and negative instances corresponding to the exemplar, thoroughly ignoring the effective information hidden in other instances. To tackle this issue, in this paper, we present an informative enhanced loss based Siamese tracking network. Specifically, we introduce an informative enhanced loss to enable the network to capture information from an overall perspective. In other words, we construct dense connections among instances and exemplar. More importantly, we prove that our proposed loss can be transformed into the logistic loss and the triplet loss under particular parameter settings. Experiments on prevalent benchmarks demonstrate that the Siamese frameworks trained with our proposed loss indeed obtain better tracking results than original ones, and achieve promising performance against several state-of-the-art trackers on the real-time challenge.
C1 [Tian, Shengjing; Li, Shuhua] Dalian Univ Technol, Sch Math Sci, Dalian 116000, Peoples R China.
   [Liu, Xiuping; Yin, Baocai] Dalian Univ Technol, Dalian 116000, Peoples R China.
   [Liu, Meng] ShanDong Jianzhu Univ, Jinan 250000, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology;
   Shandong Jianzhu University
RP Liu, XP (corresponding author), Dalian Univ Technol, Dalian 116000, Peoples R China.
EM tye@mail.dlut.edu.cn; xpliu@dlut.edu.cn; mengliu.sdu@gmail.com;
   sue142857@gmail.com; ybc@dlut.edu.cn
RI Liu, Xiu/IYJ-9134-2023; Liu, Xiufang/I-8003-2015
OI Li, Shuhua/0000-0001-8941-7467; Liu, Meng/0000-0002-1582-5764
FU National Natural Science Foundation of China [61976040, 61632006,
   61702079, 61432003, U1811463]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61976040, 61632006, 61702079, 61432003, and U1811463.
CR [Anonymous], 2016, CVPR
   [Anonymous], 2006, PROC IEEE COMPUT SOC
   Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   Benfold B., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3457, DOI 10.1109/CVPR.2011.5995667
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Cheng Y, 2018, INT CONF INFO SCI, P472, DOI 10.1109/ICIST.2018.8426080
   Choi JW, 2015, ETRI J, V37, P551, DOI 10.4218/etrij.15.0114.0629
   Danelljan M., 2016, P EUR C COMP VIS
   Danelljan M., 2015, P IEEE INT C COMP VI, P5825
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2016.159
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Emami A., 2012, P IEEE 9 INT C ADV V
   Fan H, 2019, PROC CVPR IEEE, P7944, DOI 10.1109/CVPR.2019.00814
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fan H, 2017, IEEE I CONF COMP VIS, P5487, DOI 10.1109/ICCV.2017.585
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478
   Giancola S, 2019, PROC CVPR IEEE, P1359, DOI 10.1109/CVPR.2019.00145
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   He A., 2018, P EUR C COMP VIS ECC
   He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508
   Held D, 2016, LECT NOTES COMPUT SC, V9905, P749, DOI 10.1007/978-3-319-46448-0_45
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hong ZB, 2015, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2015.7298675
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Jung I, 2018, LECT NOTES COMPUT SC, V11208, P89, DOI 10.1007/978-3-030-01225-0_6
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee KH, 2015, IEEE T MULTIMEDIA, V17, P1429, DOI 10.1109/TMM.2015.2455418
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li HJ, 2014, APPL MECH MATER, V598, P194, DOI 10.4028/www.scientific.net/AMM.598.194
   Li X, 2019, PROC CVPR IEEE, P1369, DOI 10.1109/CVPR.2019.00146
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Long Y, 2018, IEEE INT CONF COMMUN, P158, DOI 10.1109/ICCChinaW.2018.8674473
   Lu XK, 2018, LECT NOTES COMPUT SC, V11218, P369, DOI 10.1007/978-3-030-01264-9_22
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Mengmeng Wang, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P5081, DOI 10.1109/ICRA.2017.7989593
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Rong EG, 2018, APPL POWER ELECT CO, P484, DOI 10.1109/APEC.2018.8341055
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sabirin H, 2012, IEEE T MULTIMEDIA, V14, P657, DOI 10.1109/TMM.2012.2187777
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Wang MM, 2017, PROC CVPR IEEE, P4800, DOI 10.1109/CVPR.2017.510
   Wang N., 2013, Advances in Neural Information Processing Systems, P809
   Wang N, 2019, PROC CVPR IEEE, P1308, DOI 10.1109/CVPR.2019.00140
   Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510
   Wang QR, 2019, IEEE T MULTIMEDIA, V21, P930, DOI 10.1109/TMM.2018.2869277
   Wang X, 2018, PROC CVPR IEEE, P4864, DOI 10.1109/CVPR.2018.00511
   Weinberger K.Q., 2006, Advances in neural information processing systems, P1473
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yao R, 2017, IEEE T MULTIMEDIA, V19, P772, DOI 10.1109/TMM.2016.2631727
   Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321
   Zhang TZ, 2017, PROC CVPR IEEE, P4819, DOI [10.1109/CVPR.2017.512, 10.1109/ICCV.2017.469]
   Zhang Y., 2018, P EUR C COMP VIS ECC, P351
   Zhu Z, 2018, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2018.00064
   Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7
NR 68
TC 20
Z9 20
U1 2
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 120
EP 132
DI 10.1109/TMM.2020.2978636
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600010
DA 2024-07-18
ER

PT J
AU Wang, Z
   Li, JG
   Jiang, YG
AF Wang, Zheng
   Li, Jianguo
   Jiang, Yu-Gang
TI Story-driven Video Editing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Visualization; Semantics; Image segmentation; Context
   modeling; Proposals; Streaming media; Multimedia content creation;
   multimodal retrieval and compose; vision and language
ID RETRIEVAL; LANGUAGE
AB This paper proposes a novel multimedia task: story-driven video editing. Given a story paragraph, this task aims to retrieve related video segments from a gallery of collected video segments and compose them into a video sequence by the storyline order. Our proposed baseline solution consists of three modules: a retrieval module, which returns lists of candidate segments for all query sentences in the story paragraph using an object-aware sentence-segment matching method; a sequence candidate proposal module, which aggregates the retrieved segment sets into a sequence proposal by the submodular optimization method; a sorting module, which arranges the candidates according to the storyline of the paragraph using the Sinkhorn network. We build a benchmark for this task including a reorganized version of the ActivityNet Captions dataset, a well-defined quantitative metric called Evaluation of Segment-to-Sequence Matching (ESSM) for measuring the difference between the generated video segment sequence and the ground truth. Quantitative results of the proposed baseline solution are reported. We hope this new task and benchmark will bring broad research attention and push forward a lot of novel online short video editing applications.
C1 [Wang, Zheng; Jiang, Yu-Gang] Fudan Univ, Sch Comp Sci, Shanghai 200438, Peoples R China.
   [Li, Jianguo] Ant Grp, Beijing 100020, Peoples R China.
C3 Fudan University
RP Jiang, YG (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai 200438, Peoples R China.
EM zhengwang17@fudan.edu.cn; lijg.zero@antgroup.com; ygj@fudan.edu.cn
RI Zheng, Wang/AHD-7429-2022
OI Zheng, Wang/0000-0002-6753-6569
FU National Key Research and Development Program of China [2018YFB1004300]
FX This work was supported by the National Key Research and Development
   Program of China under Grant 2018YFB1004300.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2014, ARXIV14124729
   [Anonymous], 2015, Advances in neural information processing systems
   [Anonymous], 2018, ARXIV181206587
   [Anonymous], 2018, ARXIV181110652
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Awad G., 2018, TRECVID WORKSH
   Bai S., 2018, EMPIRICAL EVALUATION
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Choi J, 2016, PROC CVPR IEEE, P3122, DOI 10.1109/CVPR.2016.340
   Cruz RS, 2017, PROC CVPR IEEE, P6044, DOI 10.1109/CVPR.2017.640
   Deng C, 2016, IEEE T MULTIMEDIA, V18, P208, DOI 10.1109/TMM.2015.2508146
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gkioxari G, 2018, PROC CVPR IEEE, P8359, DOI 10.1109/CVPR.2018.00872
   Gygli M, 2015, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2015.7298928
   Hoi SCH, 2008, IEEE T MULTIMEDIA, V10, P607, DOI 10.1109/TMM.2008.921735
   Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Kim G, 2015, PROC CVPR IEEE, P1993, DOI 10.1109/CVPR.2015.7298810
   Kim JH, 2018, ADV NEUR IN, V31
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Leibe B., 2017, ARXIV170307737CS
   Leskovec J, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P420
   Liang C, 2013, IEEE T MULTIMEDIA, V15, P401, DOI 10.1109/TMM.2012.2229972
   Logeswaran Lajanugen, 2016, ARXIV161102654
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Marcelino G, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P324, DOI 10.1145/3323873.3325047
   Mena G., 2018, ICLR, P1
   Miech Antoine, 2018, ARXIV180402516
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Ngo CW, 2002, IEEE T MULTIMEDIA, V4, P446, DOI 10.1109/TMM.2002.802022
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shen ZQ, 2017, PROC CVPR IEEE, P5159, DOI 10.1109/CVPR.2017.548
   Shin A., 2016, IEEE IMAGE PROC, P3364
   Sigurdsson GA, 2018, PROC CVPR IEEE, P7396, DOI 10.1109/CVPR.2018.00772
   Smeaton A. F., 2006, P 8 ACM INT WORKSHOP, P231
   Smith JR, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1799, DOI 10.1145/3123266.3127906
   Snoek CGM, 2007, IEEE T MULTIMEDIA, V9, P280, DOI 10.1109/TMM.2006.886275
   Song Y C, 2016, IJCAI, P2025
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tapaswi M, 2015, INT J MULTIMED INF R, V4, P3, DOI 10.1007/s13735-014-0065-9
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O., 2015, ICLR
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356520
   Wang SH, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1398, DOI 10.1145/3240508.3240535
   Wang ZW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P672, DOI 10.1145/3240508.3240583
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu Q.-L., 2018, ARXIV180201173
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Zhang BW, 2018, LECT NOTES COMPUT SC, V11217, P385, DOI 10.1007/978-3-030-01261-8_23
   Zhang DS, 2004, IEEE T MULTIMEDIA, V6, P450, DOI 10.1109/TMM.2004.827505
   Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11
NR 57
TC 4
Z9 4
U1 3
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4027
EP 4036
DI 10.1109/TMM.2020.3037461
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900009
DA 2024-07-18
ER

PT J
AU Zhai, GT
   Zhu, YC
   Min, XK
AF Zhai, Guangtao
   Zhu, Yucheng
   Min, Xiongkuo
TI Comparative Perceptual Assessment of Visual Signals Using Free Energy
   Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distortion; Visualization; Prediction algorithms; Quality assessment;
   Brain modeling; Distortion measurement; Computational modeling;
   Perceptual quality assessment; full-reference quality assessment;
   no-reference quality assessment; comparative quality assessment; free
   energy; autoregressive model
ID IMAGE QUALITY ASSESSMENT; PRINCIPLE; STATISTICS; PREDICTION; BRAIN; BLUR
AB In this paper, we put forward the concept of comparative perceptual quality assessment (C-PQA), which refers to the judgment of relative qualities of two visual signals of the same content, but subject to different types and levels of distortions. While it is straightforward for human observers to fulfill the CPQA task in daily lives, it remains a difficult challenge for the current research of perceptual quality assessment (PQA). Among the existing PQA algorithms, the full-reference (FR) and reducedreference (RR) methods both need prior knowledge of the original images while the no-reference (NR) algorithms usually work with a single input image. C-PQA is inherently different from those existing methods in that it takes an image pair as input and predicts their relative quality without using any knowledge about the original image. In this paper, we propose a brain theory inspired approach to C-PQA that emulates the process of comparing the relative quality of two visual stimuli as performed by the human visual system (HVS) within the framework of free energy minimization. The brain's internal generative models initialized on the inputs are then used to explain both images. During the internal generative modeling, a group of features are extracted and then integrated to determine the relative quality of two images. We designed a dedicated image database to test the proposed C-PQA algorithm. Experimental results show that the proposed method achieves up to 98% prediction accuracy in line with the subjective ratings, outperforming many state of the art PQA algorithms.
C1 [Zhai, Guangtao; Zhu, Yucheng; Min, Xiongkuo] Shanghai Jiao Tong Univ, Shanghai Key Lab Digital Media Proc & Transmiss, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Zhai, GT (corresponding author), Shanghai Jiao Tong Univ, Shanghai Key Lab Digital Media Proc & Transmiss, Shanghai 200240, Peoples R China.
EM zhaiguangtao@sjtu.edu.cn; zyc420@sjtu.edu.cn; minxiongkuo@sjtu.edu.cn
RI Zhai, Guangtao/X-5949-2019; Min, Xiongkuo/A-7097-2019
OI Zhai, Guangtao/0000-0001-8165-9322; Zhu, Yucheng/0000-0002-3069-060X;
   Min, Xiongkuo/0000-0001-5693-0416
FU Nation Natural Science Foundation of China [61831015, 61901260,
   61927809]
FX Manuscript received July 26, 2019; revised June 30, 2020 and August 13,
   2020; accepted September 29, 2020. Date of publication October 12, 2020;
   date of current version October 19, 2021. This work was supported by the
   Nation Natural Science Foundation of China under Grants 61831015,
   61901260, and 61927809. This article was presented in part at the IEEE
   ICASSP 2013. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Manoranjan Paul.
   (Corresponding author: Guangtao Zhai.)
CR Attias H, 2000, ADV NEUR IN, V12, P209
   Barbato G, 2011, J APPL STAT, V38, P2133, DOI 10.1080/02664763.2010.545119
   Barlow H., 1961, COGNITIVE PSYCHOL
   Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760
   Friston KJ, 2006, J PHYSIOL-PARIS, V100, P70, DOI 10.1016/j.jphysparis.2006.10.001
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   Greenacre M.J., 1993, J. Appl. Statist.., V20, P251, DOI [10.1080/02664769300000021, DOI 10.1080/02664769300000021]
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Gu K, 2015, IEEE SIGNAL PROC LET, V22, P1552, DOI 10.1109/LSP.2015.2413944
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Gu K, 2013, IEEE INT CON MULTI
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G. E., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P5, DOI 10.1145/168304.168306
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Katsigiannis S, 2018, QUAL USER EXPERIENCE, V3
   Knill DC, 2004, TRENDS NEUROSCI, V27, P712, DOI 10.1016/j.tins.2004.10.007
   Li X, 2002, IEEE IMAGE PROC, P449
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu HT, 2010, IEEE T CIRC SYST VID, V20, P529, DOI 10.1109/TCSVT.2009.2035848
   Liu YT, 2018, IEEE T MULTIMEDIA, V20, P379, DOI 10.1109/TMM.2017.2729020
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503
   MacKay D.J., 1995, P NIPS CIT, V10, P4083
   Marziliano P, 2004, SIGNAL PROCESS-IMAGE, V19, P163, DOI 10.1016/j.image.2003.08.003
   Marziliano P, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P57, DOI 10.1109/ICIP.2002.1038902
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3805, DOI 10.1109/TIP.2020.2966082
   Min XK, 2019, IEEE T MULTIMEDIA, V21, P2319, DOI 10.1109/TMM.2019.2902097
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Penny WD, 2000, NEURAL NETWORKS FOR SIGNAL PROCESSING X, VOLS 1 AND 2, PROCEEDINGS, P125, DOI 10.1109/NNSP.2000.889369
   Portilla J, 2003, IEEE T IMAGE PROCESS, V12, P1338, DOI 10.1109/TIP.2003.818640
   RISSANEN J, 1981, IEEE T INFORM THEORY, V27, P12, DOI 10.1109/TIT.1981.1056282
   Roberts SJ, 2002, IEEE T SIGNAL PROCES, V50, P2245, DOI 10.1109/TSP.2002.801921
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136
   Sheikh H.R., Live Image Quality Assessment Database
   Simoncelli EP, 2009, ESSENTIAL GUIDE TO IMAGE PROCESSING, 2ND EDITION, P205, DOI 10.1016/B978-0-12-374457-9.00009-3
   Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193
   Srivastava A, 2003, J MATH IMAGING VIS, V18, P17, DOI 10.1023/A:1021889010444
   THURSTONE LL, 1954, PSYCHOL REV, V61, P47, DOI 10.1037/h0060035
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P981, DOI 10.1109/ICIP.2000.899622
   Wang ZJ, 2011, IEEE SIGNAL PROC MAG, V28, P2, DOI 10.1109/MSP.2011.940297
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wu HR, 1997, IEEE SIGNAL PROC LET, V4, P317, DOI 10.1109/97.641398
   Wu QB, 2015, IEEE IMAGE PROC, P339, DOI 10.1109/ICIP.2015.7350816
   Wu XL, 2011, IEEE T IMAGE PROCESS, V20, P36, DOI 10.1109/TIP.2010.2061860
   Wu XL, 2009, IEEE T IMAGE PROCESS, V18, P552, DOI 10.1109/TIP.2008.2010638
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhai GT, 2013, INT CONF ACOUST SPEE, P1884, DOI 10.1109/ICASSP.2013.6637980
   Zhai GT, 2012, IEEE T IMAGE PROCESS, V21, P41, DOI 10.1109/TIP.2011.2161092
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhang XJ, 2008, IEEE T IMAGE PROCESS, V17, P887, DOI 10.1109/TIP.2008.924279
   Zhou W, 2020, INFORM SCIENCES, V528, P205, DOI 10.1016/j.ins.2020.04.030
   Zhou W, 2020, IEEE T IMAGE PROCESS, V29, P4070, DOI 10.1109/TIP.2020.2969777
   Zhou W, 2019, IEEE T IMAGE PROCESS, V28, P3946, DOI 10.1109/TIP.2019.2902831
   Zhu WH, 2019, IEEE T MULTIMEDIA, V21, P2334, DOI 10.1109/TMM.2019.2902484
   Zhu X, 2010, IEEE T IMAGE PROCESS, V19, P3116, DOI 10.1109/TIP.2010.2052820
   Zhu YC, 2020, IEEE T MULTIMEDIA, V22, P2331, DOI 10.1109/TMM.2019.2957986
   Zhu YC, 2018, SIGNAL PROCESS-IMAGE, V69, P15, DOI 10.1016/j.image.2018.05.010
NR 67
TC 26
Z9 26
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3700
EP 3713
DI 10.1109/TMM.2020.3029891
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100021
DA 2024-07-18
ER

PT J
AU Zhou, QY
   Zhao, LP
   Zhou, KL
   Lin, T
   Wang, HH
   Wang, SH
   Jiao, MC
AF Zhou, Qingyang
   Zhao, Liping
   Zhou, Kailun
   Lin, Tao
   Wang, Huihui
   Wang, Shuhui
   Jiao, Mengcao
TI String Prediction for 4:2:0 Format Screen Content Coding and Its
   Implementation in AVS3
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio video coding standard; screen content coding; string prediction;
   4:2:0 format
ID PERFORMANCE
AB In the past, string prediction (also known as string matching) was applied only to RGB and YUV 4:4:4 format screen content coding. This paper proposes a string prediction approach to 4:2:0 format screen content coding implemented in the third generation of Audio Video Standard (AVS3) in China. String prediction is applied to both YUV CU and Y CU. To further improve the coding performance, several improved technicals of string prediction are presented, including a mixed string searching strategy for finding the optimal reference string, a joint picture-level, CU-level, and pixel-level early termination strategy to reduce coding complexity, and two effective coding methods for string prediction parameters. For low-complexity hardware implementation of string prediction decoder, the memory access bandwidth is reduced by introducing string constraints. Meanwhile, string prediction reuses the reference pixel buffer of intra block copy (IBC). Compared with the newest AVS3 reference software HPM7.0 with string prediction disabled, the proposed string prediction approach achieves up to 18.48% Y BD-rate reduction. Using AVS3 Screen Content Coding (SCC) Common Test Condition and YUV test sequences in Text and Graphics with Motion category, the proposed technique achieves an average Y BD-rate reduction of 10.33%, 8.47%, 6.91% for All Intra (AI), Random Access (RA) and Low Delay (LD) configurations, respectively, with low additional encoding and decoding complexity. The proposed string prediction approach has been adopted in the newest AVS3 reference software HPM7.0.
C1 [Zhou, Qingyang; Zhou, Kailun; Lin, Tao; Wang, Huihui; Wang, Shuhui; Jiao, Mengcao] Tongji Univ, Coll Elect & Informat Engn, VLSI Lab, Shanghai 200092, Peoples R China.
   [Zhao, Liping] Shaoxing Univ, Dept Comp Sci & Engn, Shaoxing 312000, Peoples R China.
C3 Tongji University; Shaoxing University
RP Zhou, KL (corresponding author), Tongji Univ, Coll Elect & Informat Engn, VLSI Lab, Shanghai 200092, Peoples R China.; Zhao, LP (corresponding author), Shaoxing Univ, Dept Comp Sci & Engn, Shaoxing 312000, Peoples R China.
EM qingyangzhou95@gmail.com; zhaoliping_jian@126.com;
   kailun_zh@tongji.edu.cn; lintao@tongji.edu.cn; whh971683@163.com;
   shw@tongji.edu.cn; james6666@126.com
RI zhao, liping/N-4269-2017; Wang, Hui/HMU-9512-2023; wang,
   hao/HSE-7975-2023; wang, jian/HRB-9588-2023; wang, hui/HSG-6135-2023;
   wang, hui/GRS-4730-2022; WANG, HUIYUAN/IXX-2427-2023; wang,
   huimin/HDM-8421-2022
FU Natural Science Foundation of Zhejiang Province [LY19F020015]; National
   Science Foundation of China [61871289]; Public Service Technology
   Application Research Project of Shaoxing city [2018C10015]; Natural
   Science Foundation of Shanghai [18ZR1440600, 19ZR1461100]
FX This work was supported in part by the Natural Science Foundation of
   Zhejiang Province under Grant LY19F020015, in part by the National
   Science Foundation of China under Grant 61871289, in part by the Public
   Service Technology Application Research Project of Shaoxing city under
   Grant 2018C10015, and in part by the Natural Science Foundation of
   Shanghai under Grants 18ZR1440600 and 19ZR1461100. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Marco Grangetto. (Qingyang Zhou, Liping Zhao and
   Kailun Zhou contributed equally to this work.)
CR AVS Video Group, 2020, P AVS N2803 TEL APR
   Bjontegaard G., 2001, ITUTVCEGM33, P1
   Bross, 2020, P JVET R2001 TEL APR
   Chen CC, 2017, IEEE T CIRC SYST VID, V27, P1568, DOI 10.1109/TCSVT.2016.2543098
   [陈先义 Chen Xianyi], 2015, [电子与信息学报, Journal of Electronics & Information Technology], V37, P2685
   CHUA LO, 1988, INT J CIRC THEOR APP, V16, P317, DOI 10.1002/cta.4490160304
   Gao Wen., 2015, Advanced Video Coding Systems
   Guo LW, 2014, IEEE IMAGE PROC, P5556, DOI 10.1109/ICIP.2014.7026124
   Li J., 2019, PROC AVS M4972, P1
   Li Y., 2019, PROC AVS M4859, P1
   Lin T, 2017, J ELECTRON INF TECHN, V39, P351, DOI 10.11999/JEIT160560
   Lin T, 2013, PICT COD SYMP, P369, DOI 10.1109/PCS.2013.6737760
   Lin T, 2013, IEEE T CIRC SYST VID, V23, P173, DOI 10.1109/TCSVT.2012.2223871
   Ma S., 2017, TELECOMMUN SCI, V33, P2
   Ma SW, 2020, IEEE T CIRC SYST VID, V30, P1683, DOI 10.1109/TCSVT.2019.2910119
   Ma Z, 2017, IEEE T MULTIMEDIA, V19, P2322, DOI 10.1109/TMM.2017.2737944
   Ma Z, 2014, IEEE T IMAGE PROCESS, V23, P4399, DOI 10.1109/TIP.2014.2346995
   Nah JW, 2009, ICOIN: 2009 INTERNATIONAL CONFERENCE ON INFORMATION NETWORKING, P1
   Peng WH, 2016, IEEE J EM SEL TOP C, V6, P393, DOI 10.1109/JETCAS.2016.2608971
   Shen Q, 2018, LECT NOTES COMPUT SC, V11164, P3, DOI 10.1007/978-3-030-00776-8_1
   Shuhui Wang, 2010, Proceedings of the 2010 3rd International Congress on Image and Signal Processing (CISP 2010), P566, DOI 10.1109/CISP.2010.5647270
   Sun C, 2019, IEEE IMTC P, P1522, DOI 10.1109/i2mtc.2019.8827137
   Tsang SH, 2019, IEEE T MULTIMEDIA, V21, P2433, DOI 10.1109/TMM.2019.2907472
   Wang QH, 2020, CELL, V181, P894, DOI 10.1016/j.cell.2020.03.045
   Wang SQ, 2017, IEEE T MULTIMEDIA, V19, P660, DOI 10.1109/TMM.2016.2625276
   Wang SH, 2015, MULTIMED TOOLS APPL, V74, P7753, DOI 10.1007/s11042-014-2021-3
   Wang SH, 2014, MULTIMED TOOLS APPL, V71, P1263, DOI 10.1007/s11042-012-1274-y
   Wang SH, 2013, IET IMAGE PROCESS, V7, P484, DOI 10.1049/iet-ipr.2012.0439
   WANG YN, 2020, PSYCHOL HEALTH 0331, DOI DOI 10.1080/13548506.2020.1746817
   Xu XZ, 2016, IEEE J EM SEL TOP C, V6, P409, DOI 10.1109/JETCAS.2016.2597645
   Xu XY, 2020, IEEE GLOB COMM CONF, DOI 10.1109/GLOBECOM42002.2020.9348236
   YAN C, 2020, IEEE PATTERN ANAL, V99, P1, DOI DOI 10.1109/TPAMI.2020.2975798
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P3014, DOI 10.1109/TMM.2020.2967645
   Yu L, 2009, SIGNAL PROCESS-IMAGE, V24, P247, DOI 10.1016/j.image.2009.02.003
   Zhang JP, 2020, PARTICUL SCI TECHNOL, V38, P596, DOI 10.1080/02726351.2019.1571542
   Zhao L., 2017, CHIN J COMPUT, V40, P1
   Zhao LP, 2020, IEEE T MULTIMEDIA, V22, P786, DOI 10.1109/TMM.2019.2931414
   [赵利平 Zhao Liping], 2019, [计算机学报, Chinese Journal of Computers], V42, P2100
   [赵利平 Zhao Liping], 2018, [计算机学报, Chinese Journal of Computers], V41, P2482
   Zhao LP, 2018, IEEE T MULTIMEDIA, V20, P796, DOI 10.1109/TMM.2017.2758519
   Zhao LP, 2016, IEEE T MULTIMEDIA, V18, P339, DOI 10.1109/TMM.2015.2512539
   ZHOU C, 2020, P IEEE 11 SENS ARR M, P1
   Zhou Keren, 2020, ICS '20: Proceedings of the 34th ACM International Conference on Supercomputing, DOI 10.1145/3392717.3392752
   Zhou KL, 2016, IEEE J EM SEL TOP C, V6, P560, DOI 10.1109/JETCAS.2016.2599876
   Zhou Q, 2020, 2020 OPTICAL FIBER COMMUNICATIONS CONFERENCE AND EXPOSITION (OFC)
   Zhu WJ, 2013, PICT COD SYMP, P373, DOI 10.1109/PCS.2013.6737761
NR 46
TC 12
Z9 12
U1 2
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3867
EP 3876
DI 10.1109/TMM.2020.3033092
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100034
DA 2024-07-18
ER

PT J
AU Zou, YX
   Shi, YM
   Shi, DC
   Wang, YW
   Liang, YS
   Tian, YH
AF Zou, Yixiong
   Shi, Yemin
   Shi, Daochen
   Wang, Yaowei
   Liang, Yongsheng
   Tian, Yonghong
TI Adaptation-Oriented Feature Projection for One-Shot Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Testing; Training data; Computational modeling; Adaptation
   models; Task analysis; Image recognition; One-shot action recognition;
   adaptation-oriented feature projection; AOF; fast adaptation
ID GESTURE RECOGNITION; NETWORK; VIDEOS; FUSION
AB One-shot action recognition aims at recognizing actions in unseen classes in cases where only one training video is provided. Compared with one-shot image recognition, one-shot learning on videos is more difficult due to the fact that the temporal dimension of video may lead to greater variation. To handle this variation, it is important to conduct further adaptation in the one-shot training process, despite the scarcity of the training data. While meta-learning is an option for facilitating this adaptation, it cannot be directly applied for two reasons: first, deep networks for action recognition can make current meta-learning methods infeasible to run because of their high computational complexity; second, due to the greater variation in actions, the adapted performance may not be higher than the un-adapted one, making it difficult to train the model by means of meta-learning. To address these problems and facilitate the adaptation, we propose the Adaptation-Oriented Feature (AOF) projection for one-shot action recognition. We first pre-train the base network on seen classes. The output of the network is projected to the adaptation-oriented feature space by fusing the important feature dimensions that are sensitive to adaptation. Subsequently, a small dataset (a.k.a. task) is sampled from seen classes to simulate the unseen-class training and testing settings. The feature adaptation is performed on the training data of this task to integrate the distribution information of the adapted feature. In order to reduce over-fitting, the triplet loss is applied to handle temporal variation with fewer parameters during the adaptation. On the testing data of this task, the losses on both adapted and un-adapted features are calculated to train the projection matrix. This sampling-adaptation-training procedure is then repeated on seen classes until convergence. Extensive experimental results on two challenging one-shot action recognition datasets demonstrate that our proposed method outperforms state-of-the-art methods.
C1 [Zou, Yixiong; Shi, Yemin; Tian, Yonghong] Peking Univ, Sch EE&CS, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
   [Zou, Yixiong; Shi, Yemin; Wang, Yaowei; Tian, Yonghong] Pengcheng Lab, Shenzhen 518066, Peoples R China.
   [Shi, Daochen] NVIDIA Technol Ctr, Beijing 100020, Peoples R China.
   [Liang, Yongsheng] Harbin Inst Technol, Sch Elect & Informat Engn, Harbin 150001, Peoples R China.
C3 Peking University; Harbin Institute of Technology
RP Tian, YH (corresponding author), Peking Univ, Sch EE&CS, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
EM zoilsen@pku.edu.cn; shiyemin@pku.edu.cn; daochens@nvidia.com;
   wangyw@pcl.ac.cn; liangys@hit.edu.cn; yhtian@pku.edu.cn
FU National Key R&D Program of China [2017YFB1002400]; National Natural
   Science Foundation of China [U1611461, 61825101]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2017YFB1002400, in part by the National Natural Science
   Foundation of China under Grants U1611461 and 61825101, and in part by
   the NVIDIANVAIL Program and the VIDIASaturnVDGX-1AI supercomputer.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   [Anonymous], 2016, PERSON RE IDENTIFICA
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2014, CORR
   [Anonymous], 2017, META SGD LEARNING LE
   [Anonymous], P 14 INT C ART INT S
   Cabrera ME, 2017, IEEE INT CONF AUTOMA, P784, DOI 10.1109/FG.2017.98
   Cho K., 2014, P 8 WORKSH SYNT SEM, P103
   Deleu Tristan, 2018, EFFECTS NEGATIVE ADA
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan ZX, 2019, IEEE T MULTIMEDIA, V21, P363, DOI 10.1109/TMM.2018.2859620
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Finn C, 2017, PR MACH LEARN RES, V70
   Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459
   Goodfellow I. J., 2015, 3 INT C LEARNING REP
   Goyal P., 2017, ACCURATE LARGE MINIB
   Hawkins J., BIOL MACHINE INTELLI
   Hou JY, 2018, IEEE T MULTIMEDIA, V20, P1537, DOI 10.1109/TMM.2017.2771462
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jiang F, 2015, J MACH LEARN RES, V16, P227
   Kim D, 2017, IEEE IJCNN, P432, DOI 10.1109/IJCNN.2017.7965886
   Koch G., 2015, PROC ICML DEEP LEARN, V2, P1
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Liu JG, 2009, PROC CVPR IEEE, P1996
   Munkhdalai Tsendsuren, 2017, Proc Mach Learn Res, V70, P2554
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Nichol A., 2018, REPTILE SCALABLE MET
   Qi H, 2018, PROC CVPR IEEE, P5822, DOI 10.1109/CVPR.2018.00610
   Qiao SY, 2018, PROC CVPR IEEE, P7229, DOI 10.1109/CVPR.2018.00755
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Ravi S., 2016, INT C LEARNING REPRE
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shao L, 2015, IEEE T NEUR NET LEAR, V26, P1019, DOI 10.1109/TNNLS.2014.2330900
   Shi Y., 2018, IEEE Transactions on Automatic Control, P1
   Shi YM, 2017, IEEE I CONF COMP VIS, P716, DOI 10.1109/ICCV.2017.84
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Simonyan K, 2014, ADV NEUR IN, V27
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Snell J, 2017, ADV NEUR IN, V30
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Vinyals O., 2016, P ADV NEUR INF PROC, V29, P3637
   Wang X, 2015, PROCEEDINGS OF 2015 INTERNATIONAL CONFERENCE ON ESTIMATION, DETECTION AND INFORMATION FUSION ICEDIF 2015, P8, DOI 10.1109/ICEDIF.2015.7280148
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wang Y., 2018, PROC ADV NEURAL INF, P1
   Zhang L, 2018, IEEE T CIRC SYST VID, V28, P2562, DOI 10.1109/TCSVT.2017.2721108
   Zhang SW, 2018, IEEE T MULTIMEDIA, V20, P769, DOI 10.1109/TMM.2017.2758524
   Zhou F., 2018, Deep Meta-Learning: Learning to Learn in the Concept Space
   Zou Y, 2018, DIS 2018: COMPANION PUBLICATION OF THE 2018 DESIGNING INTERACTIVE SYSTEMS CONFERENCE, P1, DOI 10.1145/3197391.3205403
NR 51
TC 11
Z9 11
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2020
VL 22
IS 12
BP 3166
EP 3179
DI 10.1109/TMM.2020.2972128
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU9BG
UT WOS:000591817700012
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhang, YS
   Wu, J
   Cai, ZH
   Yu, PLS
AF Zhang, Yongshan
   Wu, Jia
   Cai, Zhihua
   Yu, Philip S.
TI Multi-View Multi-Label Learning With Sparse Feature Selection for Image
   Annotation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Correlation; Noise measurement; Kernel; Learning
   systems; Computer science; Task analysis; Feature selection; sparse
   learning; multi-view learning; multi-label learning; image annotation
ID UNSUPERVISED FEATURE-SELECTION; INFORMATION
AB In image analysis, image samples are always represented by multiple view features and associated with multiple class labels for better interpretation. However, multiple view data may include noisy, irrelevant and redundant features, while multiple class labels can be noisy and incomplete. Due to the special data characteristic, it is hard to perform feature selection on multi-view multi-label data. To address these challenges, in this paper, we propose a novel multi-view multi-label sparse feature selection (MSFS) method, which exploits both view relations and label correlations to select discriminative features for further learning. Specifically, the multi-labeled information is decomposed into a reduced latent label representation to capture higher level concepts and correlations among multiple labels. Multiple local geometric structures are constructed to exploit visual similarities and relations for different views. By taking full advantage of the latent label representation and multiple local geometric structures, the sparse regression model with an l2,1-norm and an Frobenius norm (F-norm) penalty terms is utilized to perform hierarchical feature selection, where the F-norm penalty performs high-level (i.e., view-wise) feature selection to preserve the informative views and the l2,1-norm penalty conducts low-level (i.e., rowwise) feature selection to remove noisy features. To solve the proposed formulation, we also devise a simple yet efficient iterative algorithm. Experiments and comparisons on real-world image datasets demonstrate the effectiveness and potential of MSFS.
C1 [Zhang, Yongshan; Cai, Zhihua] China Univ Geosci, Sch Comp Sci, Wuhan 430074, Peoples R China.
   [Wu, Jia] Macquarie Univ, Fac Sci & Engn, Dept Comp, Sydney, NSW 2109, Australia.
   [Yu, Philip S.] Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.
   [Yu, Philip S.] Tsinghua Univ, Inst Data Sci, Beijing 100084, Peoples R China.
C3 China University of Geosciences; Macquarie University; University of
   Illinois System; University of Illinois Chicago; University of Illinois
   Chicago Hospital; Tsinghua University
RP Cai, ZH (corresponding author), China Univ Geosci, Sch Comp Sci, Wuhan 430074, Peoples R China.; Wu, J (corresponding author), Macquarie Univ, Fac Sci & Engn, Dept Comp, Sydney, NSW 2109, Australia.
EM yszhang.cug@gmail.com; jia.wu@mq.edu.au; zhcai@cug.edu.cn;
   psyu@cs.uic.edu
RI WU, Jia/V-1766-2019
OI WU, Jia/0000-0001-9013-0818; Yu, Philip/0000-0002-3491-5968; Wu,
   Jia/0000-0002-1371-5801
FU National Natural Science Foundation of China [61773355, 96804590]; ARC
   DECRA [DE200100964]; US NSF [III-1526499, III-1763325, III-1909323,
   CNS-1930941]; Australian Research Council [DE200100964] Funding Source:
   Australian Research Council
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61773355, in part by the MQEPS under
   Grant 96804590 and ARC DECRA under Grant DE200100964, in part by the US
   NSF under Grants III-1526499, III-1763325, III-1909323, and CNS-1930941.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Sen-Ching Samson Cheung.
CR Bertsekas D. P., 1999, NONLINEAR PROGRAMMIN, P22
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Cai D., 2010, KDD, P333
   Nguyen CT, 2014, AAAI CONF ARTIF INTE, P2013
   Chang XJ, 2017, IEEE T NEUR NET LEAR, V28, P2294, DOI 10.1109/TNNLS.2016.2582746
   Chang XJ, 2014, AAAI CONF ARTIF INTE, P1171
   Chen WZ, 2007, IEEE DATA MINING, P451, DOI 10.1109/ICDM.2007.18
   Du L, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P209, DOI 10.1145/2783258.2783345
   Dumais ST, 2004, ANNU REV INFORM SCI, V38, P189
   Elisseeff A, 2002, ADV NEUR IN, V14, P681
   Feng ZX, 2012, 2011 3RD INTERNATIONAL CONFERENCE ON COMPUTER TECHNOLOGY AND DEVELOPMENT (ICCTD 2011), VOL 3, P343
   Fürnkranz J, 2008, MACH LEARN, V73, P133, DOI 10.1007/s10994-008-5064-8
   Gui J, 2017, IEEE T NEUR NET LEAR, V28, P1490, DOI 10.1109/TNNLS.2016.2551724
   He X., 2005, P ADV NEURAL INFORM, V18, P507
   Hong RC, 2016, IEEE T MULTIMEDIA, V18, P1555, DOI 10.1109/TMM.2016.2567071
   Huang Y, 2015, IEEE T MULTIMEDIA, V17, P1923, DOI 10.1109/TMM.2015.2476658
   Jian L., 2016, P 25 INT JOINT C ART, P1627
   Kong DG, 2012, PROC CVPR IEEE, P2352, DOI 10.1109/CVPR.2012.6247947
   Lee J, 2015, PATTERN RECOGN, V48, P2761, DOI 10.1016/j.patcog.2015.04.009
   Li J, 2016, PROCEEDINGS OF 2016 INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY AND ITS APPLICATIONS (ISITA 2016), P384
   Li ZC, 2015, IEEE T IMAGE PROCESS, V24, P5343, DOI 10.1109/TIP.2015.2479560
   Lin YJ, 2016, INFORM SCIENCES, V372, P256, DOI 10.1016/j.ins.2016.08.039
   Liu LL, 2012, ASIA-PAC INT SYM ELE, P269, DOI 10.1109/APEMC.2012.6237809
   Liu M, 2015, AAAI CONF ARTIF INTE, P2778
   Luo MN, 2018, IEEE T CYBERNETICS, V48, P648, DOI 10.1109/TCYB.2017.2647904
   Luo X, 2011, PROCEEDINGS OF 2011 INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY AND APPLICATION, ICCTA2011, P55, DOI 10.1049/cp.2011.0629
   Luo Y, 2015, IEEE T IMAGE PROCESS, V24, P2355, DOI 10.1109/TIP.2015.2421309
   Luo Y, 2013, IEEE T NEUR NET LEAR, V24, P709, DOI 10.1109/TNNLS.2013.2238682
   Ma ZG, 2012, IEEE T MULTIMEDIA, V14, P1021, DOI 10.1109/TMM.2012.2187179
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Peng J, 2010, ANN APPL STAT, V4, P53, DOI 10.1214/09-AOAS271
   Rakotomamonjy A, 2008, J MACH LEARN RES, V9, P2491
   Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5
   Shi L, 2014, IEEE DATA MINING, P977, DOI 10.1109/ICDM.2014.58
   Sun FM, 2014, IEEE T IMAGE PROCESS, V23, P1028, DOI 10.1109/TIP.2014.2298978
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P2837, DOI 10.1109/TMM.2019.2909860
   Tang J, 2014, IEEE T KNOWL DATA EN, V26, P2914, DOI 10.1109/TKDE.2014.2320728
   Tang J, 2013, HANDBOOK OF PROTEOLYTIC ENZYMES, VOLS 1 AND 2, 3RD EDITION, P27, DOI 10.1016/B978-0-12-382219-2.00003-X
   Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929
   White M., 2012, ADV NEURAL INFORM PR, P1682
   Xu JJ, 2014, IEEE T MULTIMEDIA, V16, P403, DOI 10.1109/TMM.2013.2291218
   Zhan K, 2019, IEEE T IMAGE PROCESS, V28, P1261, DOI 10.1109/TIP.2018.2877335
   Zhan K, 2018, IEEE T CYBERNETICS, V48, P2887, DOI 10.1109/TCYB.2017.2751646
   Zhang CQ, 2018, AAAI CONF ARTIF INTE, P4414
   Zhang CQ, 2020, IEEE T PATTERN ANAL, V42, P86, DOI 10.1109/TPAMI.2018.2877660
   Zhang J, 2019, PATTERN RECOGN, V95, P136, DOI 10.1016/j.patcog.2019.06.003
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
   Zhang ML, 2015, IEEE T PATTERN ANAL, V37, P107, DOI 10.1109/TPAMI.2014.2339815
   Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39
   Zhang Q., 2016, IJCAI, P2322
   Zhang Y., 2019, ACM T INTEL SYST TEC, V10, P1
   Zhu XF, 2018, IEEE T KNOWL DATA EN, V30, P517, DOI 10.1109/TKDE.2017.2763618
   Zhu XF, 2017, IEEE T NEUR NET LEAR, V28, P1263, DOI 10.1109/TNNLS.2016.2521602
   Zhu XF, 2016, IEEE T CYBERNETICS, V46, P450, DOI 10.1109/TCYB.2015.2403356
   Zhu XF, 2013, IEEE T MULTIMEDIA, V15, P633, DOI 10.1109/TMM.2012.2233723
   Zhu XF, 2013, PATTERN RECOGN, V46, P215, DOI 10.1016/j.patcog.2012.07.018
NR 56
TC 442
Z9 449
U1 12
U2 113
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2020
VL 22
IS 11
BP 2844
EP 2857
DI 10.1109/TMM.2020.2966887
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OJ1PR
UT WOS:000583740200001
OA Bronze
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Cui, RP
   Cao, Z
   Pan, WS
   Zhang, CS
   Wang, JQ
AF Cui, Runpeng
   Cao, Zhong
   Pan, Weishen
   Zhang, Changshui
   Wang, Jianqiang
TI Deep Gesture Video Generation With Learning on Regions of Interest
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video generation; action progress; regions of interest
ID RECOGNITION
AB Generating videos with semantic meaning, such as gestures in sign language, is a challenging problem. The model should not only learn to generate videos with realistic appearance, but also take notice of crucial details in frames to convey precise information. In this paper, we focus on the problem of generating long-term gesture videos containing precise and complete semantic meanings. We develop a novel architecture to learn the temporal and spatial transforms in regions of interest, i.e., gesticulating hands or face in our case. We adopt a hierarchical approach for generating gesture videos, by first making predictions on future pose configurations, and then using the encoder-decoder architecture to synthesize future frames based on the predicted pose structures. We develop the scheme of action progress in our architecture to represent how far the action has been performed during its expected execution, and to instruct our model to synthesize actionswith various paces. Our approach is evaluated on two challenging datasets for the task of gesture video generation. Experimental results show that our method can produce gesture videos with more realistic appearance and precise meaning than the state-of-the-art video generation approaches.
C1 [Cui, Runpeng; Wang, Jianqiang] Tsinghua Univ, State Key Lab Automot Safety & Energy, Beijing 100084, Peoples R China.
   [Cui, Runpeng] Tsinghua Univ, Dept Automat, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.
   [Cao, Zhong; Pan, Weishen; Zhang, Changshui] Tsinghua Univ, Inst Artificial Intelligence,Dept Automat, THUAI,State Key Lab Intelligent Technol & Syst, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing 100084, Peoples R China.
C3 Tsinghua University; Tsinghua University; Tsinghua University
RP Cui, RP (corresponding author), Tsinghua Univ, State Key Lab Automot Safety & Energy, Beijing 100084, Peoples R China.
EM cuirunpeng@mail.tsinghua.edu.cn; caozhong14@mails.tsinghua.edu.cn;
   pws15@mails.tsinghua.edu.cn; zcs@mails.tsinghua.edu.cn;
   wjqlws@mail.tsinghua.edu.cn
RI Wang, Jianqiang/F-2806-2017; Zhang, Chang/HTO-2939-2023
OI Cui, Runpeng/0000-0002-4737-788X; Wang, Jianqiang/0000-0003-4363-6108
FU National Natural Science Foundation of China [61876095, 61751308];
   Beijing Academy of Artificial Intelligence (BAAI); National Science Fund
   forDistinguishedYoung Scholars [51625503]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61876095 and 61751308, in part by the
   Beijing Academy of Artificial Intelligence (BAAI), and in part by the
   National Science Fund forDistinguishedYoung Scholars underGrant
   51625503.
CR [Anonymous], 2016, PROC 4 INT C LEARN R
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2017, ARXIV 1705 01781 CS
   Balakrishnan G, 2018, PROC CVPR IEEE, P8340, DOI 10.1109/CVPR.2018.00870
   Cai HY, 2018, LECT NOTES COMPUT SC, V11206, P374, DOI [10.1007/978-3-030-01216-8_, 10.1007/978-3-030-01216-8_23]
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chan C, 2019, IEEE I CONF COMP VIS, P5932, DOI 10.1109/ICCV.2019.00603
   Chao YW, 2017, PROC CVPR IEEE, P3643, DOI 10.1109/CVPR.2017.388
   Chen YQ, 2003, IEEE INTERNATIONAL WORKSHOP ON ANALYSIS AND MODELING OF FACE AND GESTURES, P236
   Cui RP, 2019, IEEE T MULTIMEDIA, V21, P1880, DOI 10.1109/TMM.2018.2889563
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Forster J, 2014, LREC 2014 - NINTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P1911
   Ghosh P, 2017, INT CONF 3D VISION, P458, DOI 10.1109/3DV.2017.00059
   Ginosar S, 2019, PROC CVPR IEEE, P3492, DOI 10.1109/CVPR.2019.00361
   Grieve-Smith A. B., 2002, Gesture and Sign Language in Human-Computer Interaction. International Gesture Workshop, GW 2001. Revised Papers (Lecture Notes in Artificial Intelligence Vol.2298), P134
   Havasi L, 2005, EUROCON 2005: THE INTERNATIONAL CONFERENCE ON COMPUTER AS A TOOL, VOL 1 AND 2 , PROCEEDINGS, P445
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Hou JY, 2018, IEEE T MULTIMEDIA, V20, P1537, DOI 10.1109/TMM.2017.2771462
   Irving A., 2005, P 7 INT ACM SIGACCES, P212
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jang Y., 2018, PROC INT C MACH LEAR, P2230
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kennaway R., 2001, INT GEST WORKSH, P146
   King DB, 2015, ACS SYM SER, V1214, P1
   Kipp Michael, 2011, Intelligent Virtual Agents. Proceedings 11th International Conference, IVA 2011, P113, DOI 10.1007/978-3-642-23974-8_13
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Liang XD, 2017, IEEE I CONF COMP VIS, P1762, DOI 10.1109/ICCV.2017.194
   Liu DL, 2017, 2017 INTERNATIONAL CONFERENCE ON SMART GRID AND ELECTRICAL AUTOMATION (ICSGEA), P406, DOI 10.1109/ICSGEA.2017.74
   Oh J., 2015, P NEURIPS, P2863
   Ong SCW, 2005, IEEE T PATTERN ANAL, V27, P873, DOI 10.1109/TPAMI.2005.112
   Pintea SL, 2014, LECT NOTES COMPUT SC, V8691, P172, DOI 10.1007/978-3-319-10578-9_12
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Shlizerman E, 2018, PROC CVPR IEEE, P7574, DOI 10.1109/CVPR.2018.00790
   Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359
   Simonyan K, 2014, ADV NEUR IN, V27
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song Y, 2011, IEEE SOUTHEASTCON, P59, DOI 10.1109/SECON.2011.5752906
   Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640
   Unterthiner T., 2018, Towards accurate generative models of video: A new metric & challenges
   Villegas R., 2017, P ICLR
   Villegas R., 2017, International Conference on Machine Learning, P3560
   Vondrick C., 2016, ADV NEURAL INFORM PR, P613, DOI DOI 10.13016/M26GIH-TNYZ
   Walker J, 2017, IEEE I CONF COMP VIS, P3352, DOI 10.1109/ICCV.2017.361
   Walker J, 2016, LECT NOTES COMPUT SC, V9911, P835, DOI 10.1007/978-3-319-46478-7_51
   Walker J, 2015, IEEE I CONF COMP VIS, P2443, DOI 10.1109/ICCV.2015.281
   Walker J, 2014, PROC CVPR IEEE, P3302, DOI 10.1109/CVPR.2014.416
   Wang T., 2018, ARXIV
   Wang TH, 2019, IEEE I CONF COMP VIS, P10490, DOI 10.1109/ICCV.2019.01059
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Zhang N, 2018, INT CONF INSTR MEAS, P403, DOI 10.1109/IMCCC.2018.00091
NR 53
TC 16
Z9 18
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2020
VL 22
IS 10
BP 2551
EP 2563
DI 10.1109/TMM.2019.2960700
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NT0FL
UT WOS:000572628000006
DA 2024-07-18
ER

PT J
AU Yuan, L
   Tay, FEH
   Li, P
   Feng, JS
AF Yuan, Li
   Tay, Francis Eng Hock
   Li, Ping
   Feng, Jiashi
TI Unsupervised Video Summarization With Cycle-Consistent Adversarial LSTM
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video summarization; cycle-consistent learning; LSTM; variation
   autoencoder
AB Video summarization is an important technique to browse, manage and retrieve a large amount of videos efficiently. The main objective of video summarization is to minimize the information loss when selecting a subset of video frames from the original video, hence the summary video can faithfully represent the overall story of the original video. Recently developed unsupervised video summarization approaches are free of requiring tedious annotation on important frames to train a video summarization model and thus are practically attractive. However, their performance is still limited due to the difficulty of minimizing information loss between the summary and original videos. In this paper, we address unsupervised video summarization by developing a novel Cycle-consistent Adversarial LSTM architecture to effectively reduce the information loss in the summary video. The proposed model, named Cycle-SUM, consists of a frame selector and a cycle-consistent learning based evaluator. The selector is a bi-directional LSTM network to capture the long-range relationship between video frames. To overcome the difficulty of specifying a suitable information preserving metric between original video and summary video, the evaluator is introduced to "supervise" selector to improve the video summarization quality. Specifically, the evaluator is composed of two generative adversarial networks (GANs), in which the forward GAN component is learned to reconstruct the original video from summary video, while the backward GAN learns to invert the process. We establish the relation between mutual information maximization and such cycle learning procedure and further introduce cycle-consistent loss to regularize the summarization. Extensive experiments on three video summarization benchmark datasets demonstrate a state-of-the-art performance, and show the superiority of the Cycle-SUM model compared with other unsupervised approaches.
C1 [Yuan, Li; Tay, Francis Eng Hock; Feng, Jiashi] Natl Univ Singapore, Singapore 119077, Singapore.
   [Li, Ping] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China.
C3 National University of Singapore; Hangzhou Dianzi University
RP Yuan, L (corresponding author), Natl Univ Singapore, Singapore 119077, Singapore.
EM ylustcnus@gmail.com; mpetayeh@nus.edu.sg; clouis.lee@gmail.com;
   elefjia@nus.edu.sg
RI Feng, Jiashi/AGX-6209-2022
OI Li, Ping/0000-0002-8515-7773; Yuan, Li/0000-0002-2120-5588; Feng,
   Jiashi/0000-0001-6843-0064
FU National Natural Science Foundation of China [61872122, 61502131];
   Zhejiang Provincial Natural Science Foundation of China [LY18F020015]
FX The work of P. Li was supported in part by the National Natural Science
   Foundation of China under Grants 61872122 and 61502131, and in part by
   the Zhejiang Provincial Natural Science Foundation of China under Grant
   LY18F020015.
CR [Anonymous], ARXIV160505396
   Arjovsky M., 2017, ARXIV170107875
   Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951
   Dang C, 2015, IEEE T IMAGE PROCESS, V24, P3742, DOI 10.1109/TIP.2015.2445572
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ejaz N, 2013, SIGNAL PROCESS-IMAGE, V28, P34, DOI 10.1016/j.image.2012.10.002
   de Avila SEF, 2011, PATTERN RECOGN LETT, V32, P56, DOI 10.1016/j.patrec.2010.08.004
   Glorot X., 2010, P INT C ART INT STAT, P249
   Gong BQ, 2014, ADV NEUR IN, V27
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gygli M, 2015, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2015.7298928
   Gygli M, 2014, LECT NOTES COMPUT SC, V8695, P505, DOI 10.1007/978-3-319-10584-0_33
   He D, 2016, ADV NEUR IN, V29
   Ji Z, 2020, IEEE T CIRC SYST VID, V30, P1709, DOI 10.1109/TCSVT.2019.2904996
   Jung Y, 2019, AAAI CONF ARTIF INTE, P8537
   Khosla A, 2013, PROC CVPR IEEE, P2698, DOI 10.1109/CVPR.2013.348
   Kim G, 2014, PROC CVPR IEEE, P4225, DOI 10.1109/CVPR.2014.538
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma D. P., 2013, ARXIV13126114
   Kingma DP., 2014, C TRACK P
   Kuanar SK, 2013, J VIS COMMUN IMAGE R, V24, P1212, DOI 10.1016/j.jvcir.2013.08.003
   Larsen A. B. L., 2015, ARXIV PREPRINT ARXIV
   Lee A.X., 2018, CoRR abs/1804.01523
   Lee YJ, 2012, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2012.6247820
   Li XL, 2017, IEEE T IMAGE PROCESS, V26, P3652, DOI 10.1109/TIP.2017.2695887
   Li YJ, 2010, 2010 INTERNATIONAL COLLOQUIUM ON COMPUTING, COMMUNICATION, CONTROL, AND MANAGEMENT (CCCM2010), VOL III, P1
   Lu CW, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2487868
   Lu Z, 2013, PROC CVPR IEEE, P2714, DOI 10.1109/CVPR.2013.350
   Ma Y.-F., 2002, ACM MULTIMEDIA, P533
   Mahasseni B, 2017, PROC CVPR IEEE, P2982, DOI 10.1109/CVPR.2017.318
   Mathieu M. F., 2016, ADV NEUR IN
   Mei SH, 2015, PATTERN RECOGN, V48, P522, DOI 10.1016/j.patcog.2014.08.002
   Meng JJ, 2016, PROC CVPR IEEE, P1039, DOI 10.1109/CVPR.2016.118
   Potapov D, 2014, LECT NOTES COMPUT SC, V8694, P540, DOI 10.1007/978-3-319-10599-4_35
   Pritch Y, 2007, IEEE I CONF COMP VIS, P833
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Song YL, 2015, PROC CVPR IEEE, P5179, DOI 10.1109/CVPR.2015.7299154
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Vondrick C., 2016, ADV NEURAL INFORM PR, P613, DOI DOI 10.13016/M26GIH-TNYZ
   Wang M, 2012, IEEE T MULTIMEDIA, V14, P975, DOI 10.1109/TMM.2012.2185041
   Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yuan L, 2019, AAAI CONF ARTIF INTE, P9143
   Zhang K, 2016, PROC CVPR IEEE, P1059, DOI 10.1109/CVPR.2016.120
   Zhang K, 2016, LECT NOTES COMPUT SC, V9911, P766, DOI 10.1007/978-3-319-46478-7_47
   Zhou K., 2018, P 32 AAAI C ART INT
   Zhu J.-Y., 2017, IEEE I CONF COMP VIS, P2223, DOI DOI 10.1109/ICCV.2017.244
NR 48
TC 32
Z9 33
U1 3
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2020
VL 22
IS 10
BP 2711
EP 2722
DI 10.1109/TMM.2019.2959451
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NT0FL
UT WOS:000572628000015
DA 2024-07-18
ER

PT J
AU Jiang, B
   Tang, J
   Luo, B
AF Jiang, Bo
   Tang, Jin
   Luo, Bin
TI Feature Matching With Intra-Group Sparse Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature matching; integer quadratic programming; matches selection;
   sparse models
ID GRAPH; RECOGNITION
AB Feature matching is a fundamental problem in computer vision area. In many real applications, one can usually obtain some potential (candidate) matches C by using some discriminative feature descriptors, such as SIFT descriptor. Then, the feature matching problem can be formulated as the problem of trying to select the correct matches S from the potential match set C. In this paper, we propose to solve matches selection by developing a novel intra-group sparse matching (IGSM) model. Our IGSM is motivated by a simple observation that the potential match set C can be divided into several non-overlapping groups C-i, among which the correct matches S are uniformly distributed. We thus develop an intra-group selection model to conduct matches selection at the intra-group level to incorporate the one-to-one matching constraint more in matches selection process. Our IGSM model has three main advantages: (1) The selection mechanism is parameter-free; (2) it generates an intra-group sparse solution which better maintains the one-to-one matching constraint in nature; (3) a simple yet effective update algorithm has been derived to solve IGSM model. The optimality and convergence of the algorithm are theoretically guaranteed. Experimental results on several image feature matching datasets show the effectiveness and efficiency of the proposed IGSM matching method.
C1 [Jiang, Bo; Tang, Jin; Luo, Bin] Anhui Univ, Sch Comp Sci & Technol, Minist Educ, Key Lab Intelligent Comp & Signal Proc, Hefei 230601, Peoples R China.
   [Jiang, Bo] Anhui Univ, Inst Phys Sci & Informat Technol, Hefei 230601, Peoples R China.
C3 Anhui University; Anhui University
RP Tang, J (corresponding author), Anhui Univ, Sch Comp Sci & Technol, Minist Educ, Key Lab Intelligent Comp & Signal Proc, Hefei 230601, Peoples R China.
EM zeyiabc@163.com; ahu_tj@163.com; ahu_lb@163.com
RI lu, bin/HPE-4790-2023
FU NSFC Key Projects of International (Regional) Cooperation and Exchanges
   [61860206004]; National Natural Science Foundation of China [61602001,
   61872005, 61671018]; Open fund for Discipline Construction, Institute of
   Physical Science and Information Technology, Anhui University
FX This work was supported in part by NSFC Key Projects of International
   (Regional) Cooperation and Exchanges under Grant 61860206004, in part by
   the National Natural Science Foundation of China under Grants 61602001,
   61872005, and 61671018, and in part by the Open fund for Discipline
   Construction, Institute of Physical Science and Information Technology,
   Anhui University.
CR Adamczewski K, 2015, IEEE I CONF COMP VIS, P109, DOI 10.1109/ICCV.2015.21
   Albarelli A, 2012, INT J COMPUT VISION, V97, P36, DOI 10.1007/s11263-011-0432-4
   Albarelli A, 2009, IEEE I CONF COMP VIS, P1319, DOI 10.1109/ICCV.2009.5459312
   [Anonymous], 2014, INT J COMPUT VISION, DOI DOI 10.1007/s11263-014-0707-7
   [Anonymous], 2012, INT J COMPUT VISION, DOI DOI 10.1007/s11263-011-0442-2
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Cho M, 2009, IEEE I CONF COMP VIS, P1280, DOI 10.1109/ICCV.2009.5459322
   Cho M, 2010, LECT NOTES COMPUT SC, V6315, P492
   Cour T., 2006, Advances in Neural Information Processing Systems, V19, P313, DOI [DOI 10.7551/MITPRESS/7503.003.0044, 10.7551/mitpress/7503.003.0044]
   Ding C, 2010, IEEE T PATTERN ANAL, V32, P45, DOI 10.1109/TPAMI.2008.277
   Egozi A, 2013, IEEE T PATTERN ANAL, V35, P18, DOI 10.1109/TPAMI.2012.51
   Enqvist O, 2009, IEEE I CONF COMP VIS, P1295, DOI 10.1109/ICCV.2009.5459319
   Gold S, 1996, IEEE T PATTERN ANAL, V18, P377, DOI 10.1109/34.491619
   Huang D, 2015, IEEE T CYBERNETICS, V45, P1823, DOI 10.1109/TCYB.2014.2360894
   Jiang B, 2019, INT J COMPUT VISION, V127, P1345, DOI 10.1007/s11263-019-01185-1
   Jiang B, 2015, AAAI CONF ARTIF INTE, P3790
   Jiang B, 2014, IEEE T IMAGE PROCESS, V23, P5175, DOI 10.1109/TIP.2014.2362614
   Kong D., 2014, Advances in Neural Information Processing Systems, P1655
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482
   Leordeanu M., 2009, NIPS, P1114
   Li XC, 2018, IEEE T MULTIMEDIA, V20, P1179, DOI 10.1109/TMM.2017.2763323
   Liu CL, 2011, PROC INT CONF DOC, P37, DOI 10.1109/ICDAR.2011.17
   Liu ZY, 2014, IEEE T PATTERN ANAL, V36, P1258, DOI 10.1109/TPAMI.2013.223
   Lou ZY, 2014, IEEE T MULTIMEDIA, V16, P2052, DOI 10.1109/TMM.2014.2346476
   Ma JY, 2015, IEEE T SIGNAL PROCES, V63, P1115, DOI 10.1109/TSP.2014.2388434
   Ng ES, 2010, IEEE IMAGE PROC, P2693, DOI 10.1109/ICIP.2010.5651903
   Rana A, 2019, IEEE T MULTIMEDIA, V21, P256, DOI 10.1109/TMM.2018.2839885
   Rodolà E, 2013, IEEE I CONF COMP VIS, P1169, DOI 10.1109/ICCV.2013.149
   Rodolà E, 2012, PROC CVPR IEEE, P182, DOI 10.1109/CVPR.2012.6247674
   Sang JT, 2012, IEEE T MULTIMEDIA, V14, P586, DOI 10.1109/TMM.2012.2188784
   van Wyk BJ, 2004, IEEE T PATTERN ANAL, V26, P1526, DOI 10.1109/TPAMI.2004.95
   Wang T, 2018, IEEE T PATTERN ANAL, V40, P1494, DOI 10.1109/TPAMI.2017.2716350
   Wang T, 2018, IEEE T PATTERN ANAL, V40, P2853, DOI 10.1109/TPAMI.2017.2767591
   Xinmai Yang, 2017, 2017 IEEE International Ultrasonics Symposium (IUS), DOI 10.1109/ULTSYM.2017.8092964
   Yang X, 2018, IEEE T CYBERNETICS, V48, P1432, DOI 10.1109/TCYB.2017.2697968
   Zaslavskiy M, 2009, IEEE T PATTERN ANAL, V31, P2227, DOI 10.1109/TPAMI.2008.245
   Zhang Z, 2016, PROC CVPR IEEE, P1202, DOI 10.1109/CVPR.2016.135
   Zhou F, 2012, PROC CVPR IEEE, P127, DOI 10.1109/CVPR.2012.6247667
NR 39
TC 3
Z9 3
U1 2
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2020
VL 22
IS 8
BP 2074
EP 2085
DI 10.1109/TMM.2019.2951466
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MR2MS
UT WOS:000553424500013
DA 2024-07-18
ER

PT J
AU Song, H
   Sun, C
   Wu, XX
   Chen, M
   Jia, YD
AF Song, Hao
   Sun, Che
   Wu, Xinxiao
   Chen, Mei
   Jia, Yunde
TI Learning Normal Patterns via Adversarial Attention-Based Autoencoder for
   Abnormal Event Detection in Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Decoding; Gallium nitride; Event detection; Generative
   adversarial networks; Image reconstruction; Anomaly detection; Abnormal
   event detection; Ada-Net; attention mechanism; generative adversarial
   network
ID ANOMALY DETECTION; CROWDED SCENES; LOCALIZATION; NETWORK; ONLINE; MOTION
AB Automatically detecting anomalies in videos is a challenging problem due to non-deterministic definitions of abnormal events and lack of sufficient training data. To address these issues, we propose an autoencoder coupled with attention model to discover normal patterns in videos via adversarial learning. Abnormal events are detected by diverging them from the normal patterns with the reconstruction error produced by the autoencoder. To this end, we build an end-to-end trainable adversarial attention-based autoencoder network, called Ada-Net, to make the reconstructed frames indistinguishable from original frames. The Ada-Net combines an autoencoder network and a GAN model that is used to benefit enhancing the reconstruction ability of the autoencoder. To further improve the reconstruction performance, we integrate an attention model into the decoder to dynamically select informative parts of encoding features for decoding. The attenion mechanism is helpful to preserving important information for learning intrinsic normal patterns. Evaluations on four challenging datasets, including the Subway, the UCSD Pedestrian, the CUHK Avenue, and the ShanghaiTech datasets, demonstrate the effectiveness of the proposed method.
C1 [Song, Hao; Sun, Che; Wu, Xinxiao; Jia, Yunde] Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
   [Chen, Mei] SUNY Albany, Dept Elect & Comp Engn, Albany, NY 12222 USA.
C3 Beijing Institute of Technology; State University of New York (SUNY)
   System; State University of New York (SUNY) Albany
RP Wu, XX (corresponding author), Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
EM songhao@bit.edu.cn; sunche@bit.edu.cn; wuxinxiao@bit.edu.cn;
   meichen@albany.edu; jiayunde@bit.edu.cn
OI Wu, Xinxiao/0000-0002-2056-6947; Sun, Che/0000-0002-7555-9146
FU Natural Science Foundation of China [61673062]
FX This work was supported by the Natural Science Foundation of China under
   Grant 61673062.
CR Abadi Martin, 2016, TENSORFLOW LARGE SCA, V16, P265
   Adam A, 2008, IEEE T PATTERN ANAL, V30, P555, DOI 10.1109/TPAMI.2007.70825
   Anjum N, 2008, IEEE T CIRC SYST VID, V18, P1555, DOI 10.1109/TCSVT.2008.2005603
   [Anonymous], 2015, BRIT MACHINE VISION
   Bengio Y., 2007, Advances in neural information processing systems, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024
   Biswas S., 2017, ANOMALY DETECTION VI
   Chen HK, 2016, PROCEEDINGS OF THE 2016 12TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA), P640, DOI 10.1109/WCICA.2016.7578533
   Chen Wang, 2017, Multimedia Tools and Applications, V76, P6263, DOI 10.1007/s11042-015-3199-8
   Chong YS, 2017, LECT NOTES COMPUT SC, V10262, P189, DOI 10.1007/978-3-319-59081-3_23
   Chu WQ, 2019, IEEE T MULTIMEDIA, V21, P246, DOI 10.1109/TMM.2018.2846411
   Cong Y, 2013, PATTERN RECOGN, V46, P1851, DOI 10.1016/j.patcog.2012.11.021
   De Geest R, 2018, IEEE WINT CONF APPL, P1549, DOI 10.1109/WACV.2018.00173
   Del Giorno A, 2016, LECT NOTES COMPUT SC, V9909, P334, DOI 10.1007/978-3-319-46454-1_21
   Dutta JK, 2015, AAAI CONF ARTIF INTE, P3755
   Feng YC, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P591, DOI 10.1145/2964284.2967290
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo DS, 2018, IEEE T MULTIMEDIA, V20, P3428, DOI 10.1109/TMM.2018.2839534
   Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hou JY, 2018, IEEE T MULTIMEDIA, V20, P1537, DOI 10.1109/TMM.2017.2771462
   Ionescu RT, 2017, IEEE I CONF COMP VIS, P2914, DOI 10.1109/ICCV.2017.315
   Jaechul Kim, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2921, DOI 10.1109/CVPRW.2009.5206569
   Jamadandi A., 2018, PREDGAN DEEP MULTISC
   Jiang F, 2009, IEEE IMAGE PROC, P1117, DOI 10.1109/ICIP.2009.5414535
   Jiang Y, 2017, IGGRAPH ASIA 2017 TECHNICAL BRIEFS (SA'17), DOI 10.1145/3145749.3149440
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li C, 2017, IEEE I CONF COMP VIS, P3667, DOI 10.1109/ICCV.2017.394
   Li Jiwei, 2017, P 2017 C EMP METH NA, P2157, DOI DOI 10.18653/V1/D17-1230
   Li T, 2015, IEEE T CIRC SYST VID, V25, P367, DOI 10.1109/TCSVT.2014.2358029
   Li WX, 2014, IEEE T PATTERN ANAL, V36, P18, DOI 10.1109/TPAMI.2013.111
   Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306
   Liu Juncheng., 2017, P IEEE C COMPUTER VI, P792, DOI DOI 10.1109/CVPR.2017.391
   Liu W, 2018, PROC CVPR IEEE, P6536, DOI 10.1109/CVPR.2018.00684
   Liu YR, 2011, SYNTHESIS-STUTTGART, P3133, DOI 10.1055/s-0030-1260184
   Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338
   Luo WX, 2017, IEEE I CONF COMP VIS, P341, DOI 10.1109/ICCV.2017.45
   Luo WX, 2017, IEEE INT CON MULTI, P439, DOI 10.1109/ICME.2017.8019325
   MAHADEVAN V, 2010, PROC CVPR IEEE, P1975, DOI DOI 10.1109/CVPR.2010.5539872
   Medel J. R., 2016, Anomaly detection in video using predictive convolutional long short-term memory networks
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Patraucean V., 2015, arXiv
   Piciarelli C, 2008, IEEE T CIRC SYST VID, V18, P1544, DOI 10.1109/TCSVT.2008.2005599
   Popoola OP, 2012, IEEE T SYST MAN CY C, V42, P865, DOI 10.1109/TSMCC.2011.2178594
   Roshtkhari MJ, 2013, COMPUT VIS IMAGE UND, V117, P1436, DOI 10.1016/j.cviu.2013.06.007
   Sabokrou M, 2018, PROC CVPR IEEE, P3379, DOI 10.1109/CVPR.2018.00356
   Saligrama V, 2012, PROC CVPR IEEE, P2112, DOI 10.1109/CVPR.2012.6247917
   Shi X., 2015, ADV NEURAL INFORM PR, DOI DOI 10.48550/ARXIV.1506.04214
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Sultani W, 2018, PROC CVPR IEEE, P6479, DOI 10.1109/CVPR.2018.00678
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tang X, 2013, IEEE IMAGE PROC, P3602, DOI 10.1109/ICIP.2013.6738743
   Wang SQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P636, DOI 10.1145/3240508.3240615
   Wang T, 2013, IEEE INT W PERFORM, P45, DOI 10.1109/PETS.2013.6523794
   Wu SD, 2010, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2010.5539882
   Xinyi Cui, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3161, DOI 10.1109/CVPR.2011.5995558
   Xu D, 2017, COMPUT VIS IMAGE UND, V156, P117, DOI 10.1016/j.cviu.2016.10.010
   Xu K, 2018, IEEE T MULTIMEDIA, V20, P1062, DOI 10.1109/TMM.2018.2818942
   Yang Z., 2017, ARXIV170304887
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zhang D, 2005, PROC CVPR IEEE, P611
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zhao Y, 2016, IEEE IMAGE PROC, P3354, DOI 10.1109/ICIP.2016.7532981
   Zhou SF, 2016, SIGNAL PROCESS-IMAGE, V47, P358, DOI 10.1016/j.image.2016.06.007
   Zhou SF, 2015, INT CONF ACOUST SPEE, P1300, DOI 10.1109/ICASSP.2015.7178180
NR 65
TC 54
Z9 61
U1 3
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2020
VL 22
IS 8
BP 2138
EP 2148
DI 10.1109/TMM.2019.2950530
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MR2MS
UT WOS:000553424500018
DA 2024-07-18
ER

PT J
AU He, ZW
   Cao, YP
   Du, L
   Xu, BB
   Yang, JX
   Cao, YL
   Tang, SL
   Zhuang, YT
AF He, Zewei
   Cao, Yanpeng
   Du, Lei
   Xu, Baobei
   Yang, Jiangxin
   Cao, Yanlong
   Tang, Siliang
   Zhuang, Yueting
TI MRFN: Multi-Receptive-Field Network for Fast and Accurate Single Image
   Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Single image super-resolution; deep learning; multi-receptive-field;
   loss function
ID QUALITY ASSESSMENT
AB Recently, convolutional neural network (CNN) based models have shown great potential in the task of single image super-resolution (SISR). However, many state-of-the-art SISR solutions are reproducing some tricks proven effective in other vision tasks, such as pursuing a deeper model. In this paper, we propose a new solution (named as Multi-Receptive-Field Network - MRFN), which outperforms existing SISR solutions in three different aspects. First, from receptive field: a novel multi-receptive-field (MRF) module is proposed to extract and fuse features in different receptive fields from local to global. Integrating these hierarchical features can generate better mappings on recovering high-fidelity details at different scales. Second, from network architectures: both dense skip connections and deep supervision are utilized to combine features from the current MRF module and preceding ones for training more representative features. Moreover, a deconvolution layer is embedded at the end of the network to avoid artificial priors induced by numerical data pre-processing (e.g., bicubic stretching), and speed up the restoration process. Finally, from error modeling: different from L1 and L2 loss functions, we proposed a novel two-parameter training loss called Weighted Huber loss function which can adaptively adjust the value of back-propagated derivative according to the residual value, thus fit the reconstruction error more effectively. Extensive qualitative and quantitative evaluation results on benchmark datasets demonstrate that our proposed MRFN can achieve more accurate recovering results than most state-of-the-art methods with significantly less complexity.
C1 [He, Zewei; Cao, Yanpeng; Xu, Baobei; Yang, Jiangxin; Cao, Yanlong] Zhejiang Univ, State Key Lab Fluid Power & Mechatron Syst, Sch Mechan Engn, Hangzhou 310027, Peoples R China.
   [He, Zewei; Cao, Yanpeng; Xu, Baobei; Yang, Jiangxin; Cao, Yanlong] Zhejiang Univ, Key Lab Adv Mfg Technol Zhejiang Prov, Sch Mechan Engn, Hangzhou 310027, Peoples R China.
   [Du, Lei] Shaoxing Univ, Yuanpei Coll, Shaoxing 312000, Peoples R China.
   [Tang, Siliang; Zhuang, Yueting] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
C3 Zhejiang University; Zhejiang University; Shaoxing University; Zhejiang
   University
RP Cao, YP (corresponding author), Zhejiang Univ, State Key Lab Fluid Power & Mechatron Syst, Sch Mechan Engn, Hangzhou 310027, Peoples R China.; Cao, YP (corresponding author), Zhejiang Univ, Key Lab Adv Mfg Technol Zhejiang Prov, Sch Mechan Engn, Hangzhou 310027, Peoples R China.; Tang, SL (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
EM zeweihe@zju.edu.cn; caoyp@zju.edu.cn; dulei@mail.ustc.edu.cn;
   21625177@zju.edu.cn; yangjx@zju.edu.cn; sdcaoyl@zju.edu.cn;
   siliang@zju.edu.cn; yzhuang@cs.zju.edu.cn
RI zhu, yujie/KBC-4009-2024; Cao, Yanpeng/HSE-0205-2023; zewei,
   he/AGK-4914-2022; Yang, Jiang/HZI-6764-2023; ZHANG,
   XIAOLONG/IZQ-4553-2023; li, yifei/IWU-7824-2023
OI zewei, he/0000-0003-4280-9708; Tang, Siliang/0000-0002-7356-9711
FU National Natural Science Foundation of China [51605428, 51575486,
   61751209]
FX This work was supported by the National Natural Science Foundation of
   China (No. 51605428, 51575486, and 61751209.)
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   DUCHON CE, 1979, J APPL METEOROL, V18, P1016, DOI 10.1175/1520-0450(1979)018<1016:LFIOAT>2.0.CO;2
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Ioffe S., 2015, P INT C MACH LEARN, VVolume 1, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Mao XJ, 2016, ADV NEUR IN, V29
   Marnerides D, 2018, COMPUT GRAPH FORUM, V37, P37, DOI 10.1111/cgf.13340
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sajjadi MSM, 2017, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2017.481
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Sun J, 2008, PROC CVPR IEEE, P2471, DOI 10.1109/CVPR.2008.4587659
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Timofte R, 2016, PROC CVPR IEEE, P1865, DOI 10.1109/CVPR.2016.206
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Zhang, 2016, 2016 Power Systems Computation Conference (PSCC), P1, DOI 10.1109/PSCC.2016.7540823
   Wei Z, 2017, PROC CVPR IEEE, P3947, DOI 10.1109/CVPR.2017.420
   Yang JQ, 2008, ITESS: 2008 PROCEEDINGS OF INFORMATION TECHNOLOGY AND ENVIRONMENTAL SYSTEM SCIENCES, PT 1, P1, DOI 10.1109/CVPR.2008.4587647
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
   Zhang L, 2012, IEEE IMAGE PROC, P1477, DOI 10.1109/ICIP.2012.6467150
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
NR 46
TC 64
Z9 68
U1 1
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 1042
EP 1054
DI 10.1109/TMM.2019.2937688
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400018
DA 2024-07-18
ER

PT J
AU Xu, K
   Sun, TF
   Jiang, XH
AF Xu, Ke
   Sun, Tanfeng
   Jiang, Xinghao
TI Video Anomaly Detection and Localization Based on an Adaptive
   Intra-Frame Classification Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Anomaly detection; Feature extraction; Deep learning; Task analysis;
   Adaptive systems; Adaptation models; Training; Anomaly detection; AICN;
   deep learning; motion convolutional layers; adaptive region pooling
ID ABNORMAL EVENT DETECTION; NEURAL-NETWORKS
AB Video anomaly detection and localization is still a challenging task in the computer vision field. Previous methods took this task as an outlier detection problem, which computed the deviation between the test samples and the normal patterns. In this paper, an adaptive intra-frame classification network (AICN) is proposed to transform this task to a multi-class classification problem. The contributions of our method are as follows. AICN is an end-to-end network for anomaly detection and localization. By using the motion convolutional layers and the shape convolutional layers, spatial-temporal features are extracted without resizing or splitting the frames before forward propagation. AICN enhances the adaptiveness of model. By using the adaptive region pooling layer and the intra-frame classifier, AICN is adaptive to frames with different resolutions and is easier to be applied on other scenes. AICN evaluates the abnormality of frames based on the intra-frame classification results. The intra-frame classification strategy reserves more connection information of sub-regions and makes the model outperform previous methods. The proposed method is examined on four public datasets with different background complexities and resolutions: UCSD Ped1 dataset, UCSD Ped2 dataset, Avenue dataset and Subway dataset. The results are further compared with previous approaches to confirm the effectiveness and the advantage of our method.
C1 [Xu, Ke; Jiang, Xinghao] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
   [Sun, Tanfeng] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Natl Engn Lab Informat Content Anal Technol, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University
RP Jiang, XH (corresponding author), Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
EM 113025816@sjtu.edu.cn; tfsun@sjtu.cdu.cn; xhjiang@sjtu.edu.cn
FU Nature Science Foundation of China [61572321, 61572320]; National Key
   Research and Development Projects of China [2018YFC0830703,
   2018YFC0831405]; Foundation of Shanghai Fusion and Innovative Research
   Laboratory for Procuratorial Big Data
FX This work was supported in part by the Nature Science Foundation of
   China (61572321 and 61572320), in part by the National Key Research and
   Development Projects of China (2018YFC0830703 and 2018YFC0831405), and
   in part by the Foundation of Shanghai Fusion and Innovative Research
   Laboratory for Procuratorial Big Data. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Elisa Ricci.
CR Adam A, 2008, IEEE T PATTERN ANAL, V30, P555, DOI 10.1109/TPAMI.2007.70825
   [Anonymous], P AS C COMP VIS
   [Anonymous], 2019, 2019 INT JOINT C NEU
   [Anonymous], 2015, P BRIT MACH VIS C BM
   Bao TL, 2017, MULTIMED TOOLS APPL, V76, P23213, DOI 10.1007/s11042-016-4100-0
   Caetano C, 2017, SIBGRAPI, P47, DOI 10.1109/SIBGRAPI.2017.13
   Chong YS, 2017, LECT NOTES COMPUT SC, V10262, P189, DOI 10.1007/978-3-319-59081-3_23
   Chu WQ, 2019, IEEE T MULTIMEDIA, V21, P246, DOI 10.1109/TMM.2018.2846411
   Colque RM, 2018, PROCEEDINGS OF THE 13TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2018), VOL 5: VISAPP, P293, DOI 10.5220/0006615202930300
   Cong Y, 2011, PROC CVPR IEEE, P1807, DOI 10.1109/CVPR.2011.5995434
   Cosar S, 2017, IEEE T CIRC SYST VID, V27, P683, DOI 10.1109/TCSVT.2016.2589859
   Cui XG, 2014, PROC SPIE, V9069, DOI 10.1117/12.2050229
   Tran D, 2014, IEEE T PATTERN ANAL, V36, P404, DOI 10.1109/TPAMI.2013.137
   Feng YC, 2017, NEUROCOMPUTING, V219, P548, DOI 10.1016/j.neucom.2016.09.063
   Goodfellow, 2017, ARXIV170100160
   Colque RVHM, 2017, IEEE T CIRC SYST VID, V27, P673, DOI 10.1109/TCSVT.2016.2637778
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Kim J, 2009, PROC CVPR IEEE, P2913
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li WX, 2014, IEEE T PATTERN ANAL, V36, P18, DOI 10.1109/TPAMI.2013.111
   Liu W, 2018, PROC CVPR IEEE, P6536, DOI 10.1109/CVPR.2018.00684
   Liu YR, 2011, SYNTHESIS-STUTTGART, P3133, DOI 10.1055/s-0030-1260184
   Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338
   Ma Z, 2013, IEEE T MULTIMEDIA, V15, P1628, DOI 10.1109/TMM.2013.2264928
   Mahadevan V, 2010, PROC CVPR IEEE, P1975, DOI 10.1109/CVPR.2010.5539872
   Mo X, 2014, IEEE T CIRC SYST VID, V24, P631, DOI 10.1109/TCSVT.2013.2280061
   Morris BT, 2008, IEEE T CIRC SYST VID, V18, P1114, DOI 10.1109/TCSVT.2008.927109
   Piciarelli C, 2008, IEEE T CIRC SYST VID, V18, P1544, DOI 10.1109/TCSVT.2008.2005599
   Popoola OP, 2012, IEEE T SYST MAN CY C, V42, P865, DOI 10.1109/TSMCC.2011.2178594
   Reddy V., 2011, Computer Vision and Pattern Recognition Workshops (CVPRW), 2011 IEEE Computer Society Conference on, P55
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Revathi AR, 2017, SIGNAL IMAGE VIDEO P, V11, P291, DOI 10.1007/s11760-016-0935-0
   Roy P, 2003, TRANSPORT RES REC, P96
   Sabokrou Mohammad, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P56, DOI 10.1109/CVPRW.2015.7301284
   Sabokrou M., 2018, P AS COMP VIS C, P488
   Sabokrou M, 2018, PROC CVPR IEEE, P3379, DOI 10.1109/CVPR.2018.00356
   Sabokrou M, 2018, COMPUT VIS IMAGE UND, V172, P88, DOI 10.1016/j.cviu.2018.02.006
   Sabokrou M, 2017, IEEE T IMAGE PROCESS, V26, P1992, DOI 10.1109/TIP.2017.2670780
   Saligrama V, 2012, PROC CVPR IEEE, P2112, DOI 10.1109/CVPR.2012.6247917
   Sánchez J, 2013, IMAGE PROCESS ON LIN, V3, P137, DOI 10.5201/ipol.2013.26
   Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12
   Song X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2438653.2438670
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Xiao T, 2015, IEEE SIGNAL PROC LET, V22, P1477, DOI 10.1109/LSP.2015.2410031
   Xu D, 2014, NEUROCOMPUTING, V143, P144, DOI 10.1016/j.neucom.2014.06.011
   Xu K, 2018, IEEE T MULTIMEDIA, V20, P1062, DOI 10.1109/TMM.2018.2818942
   Yuan Y, 2017, IEEE T CYBERNETICS, V47, P3597, DOI 10.1109/TCYB.2016.2572609
   Zhou SF, 2016, SIGNAL PROCESS-IMAGE, V47, P358, DOI 10.1016/j.image.2016.06.007
NR 49
TC 45
Z9 48
U1 2
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2020
VL 22
IS 2
BP 394
EP 406
DI 10.1109/TMM.2019.2929931
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KH5FZ
UT WOS:000510676300009
DA 2024-07-18
ER

PT J
AU Zhang, J
   Peng, YX
AF Zhang, Jian
   Peng, Yuxin
TI Multi-Pathway Generative Adversarial Hashing for Unsupervised
   Cross-Modal Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlation; Manifolds; Dogs; Data models; Semantics; Generative
   adversarial networks; Multimedia databases; Cross-modal hashing;
   generative adversarial networks; manifold structure
ID NETWORK
AB Cross-modal hashing aims to map heterogeneous cross-modal data into a common Hamming space, which can realize fast and flexible retrieval across different modalities. Unsupervised cross-modal hashing is more flexible and applicable than supervised methods, since no intensive labeling work is involved. However, existing unsupervised methods learn the hashing functions by preserving inter- and intra-correlations while ignoring the underlying manifold structure across different modalities, which is extremely helpful in capturing the meaningful nearest neighbors of different modalities for cross-modal retrieval. Furthermore, existing works mainly focus on pairwise relation modeling while ignoring the correlations within multiple modalities. To address the above-mentioned problems, in this paper, we propose a multi-pathway generative adversarial hashing approach for unsupervised cross-modal retrieval, which makes full use of a generative adversarial network's ability for unsupervised representation learning to exploit the underlying manifold structure of cross-modal data. The main contributions can be summarized as follows: First, we propose a multi-pathway generative adversarial network to model cross-modal hashing in an unsupervised fashion. In the proposed network, given the data of one modality, the generative model tries to fit the distribution over the manifold structure and selects informative data of other modalities to challenge the discriminative model. The discriminative model learns to distinguish the generated data and the true positive data sampled from the correlation graph to achieve better retrieval accuracy. These two models are trained in an adversarial way to improve each other and promote hashing function learning. Second, we propose a correlation graph-based approach to capture the underlying manifold structure across different modalities so that data of different modalities but within the same manifold can have a smaller Hamming distance to promote retrieval accuracy. Extensive experiments compared with state-of-the-art methods on three widely used datasets verify the effectiveness of our proposed approach.
C1 [Zhang, Jian; Peng, Yuxin] Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.
C3 Peking University
RP Peng, YX (corresponding author), Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.
EM pengyuxin@pku.edu.cn
FU National Natural Science Foundation of China [61771025]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61771025. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Martha
   Larson.
CR [Anonymous], 2017, ARXIV2017170104722
   [Anonymous], 1999, MODERN INFORM RETRIE
   [Anonymous], 2015, NATURE, DOI [DOI 10.1038/NATURE14539, 10.1038/nature14539]
   [Anonymous], IEEE T CYBERN
   [Anonymous], 2009, NUSWIDE: A real-world web image database from National University of Singapore, DOI DOI 10.1145/1646396.1646452
   Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928
   Cao Y, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1445, DOI 10.1145/2939672.2939812
   Cao Y, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P197, DOI 10.1145/2911996.2912000
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Deng C, 2016, IEEE T MULTIMEDIA, V18, P208, DOI 10.1109/TMM.2015.2508146
   Ding GG, 2016, IEEE T IMAGE PROCESS, V25, P5427, DOI 10.1109/TIP.2016.2607421
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Finn C, 2016, ADV NEUR IN, V29
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hu MQ, 2019, IEEE T IMAGE PROCESS, V28, P2770, DOI 10.1109/TIP.2018.2890144
   Hu MQ, 2018, IEEE T IMAGE PROCESS, V27, P545, DOI 10.1109/TIP.2017.2749147
   Hu Y, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P527, DOI 10.1145/2647868.2654906
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Irie G, 2014, PROC CVPR IEEE, P2123, DOI 10.1109/CVPR.2014.272
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Li JA, 2017, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2017.211
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Long MS, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P579, DOI 10.1145/2911451.2911493
   McEnnis D, 2005, P INT C MUS INF RETR, P600
   Odena A, 2017, PR MACH LEARN RES, V70
   Peng YX, 2017, FRONT INFORM TECH EL, V18, P44, DOI 10.1631/FITEE.1601787
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Radford A., 2015, ARXIV151106434
   Rastegari Mohammad, 2013, Book Predictable Dual-View Hashing
   Reed S, 2016, PR MACH LEARN RES, V48
   Schütt M, 2004, PEPTIDE REVOLUTION: GENOMICS, PROTEOMICS & THERAPEUTICS, P41
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang D, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3890
   Wang J, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P515, DOI 10.1145/3077136.3080786
   Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Wei Y, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P791, DOI 10.1145/2623330.2623688
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Wu BT, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3946
   Yang EK, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1064
   Yang E, 2018, IEEE T NEUR NET LEAR, V29, P5292, DOI 10.1109/TNNLS.2018.2793863
   Yang EK, 2017, AAAI CONF ARTIF INTE, P1618
   Yang Y, 2019, IEEE T KNOWL DATA EN, V31, P757, DOI 10.1109/TKDE.2018.2842190
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang J, 2019, IEEE T CIRC SYST VID, V29, P212, DOI 10.1109/TCSVT.2017.2771332
   Zhao X, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3511
   Zhen Y., 2012, P INT C NEUR INF PRO, P1376
   Zhou JL, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P415
   Zhuang YT, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P901, DOI 10.1145/2647868.2655059
NR 61
TC 59
Z9 65
U1 3
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2020
VL 22
IS 1
BP 174
EP 187
DI 10.1109/TMM.2019.2922128
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KB6AZ
UT WOS:000506577000016
DA 2024-07-18
ER

PT J
AU Dong, YZ
   Yang, XY
   Zhao, X
   Li, J
AF Dong, Yizhuo
   Yang, Xinyu
   Zhao, Xi
   Li, Juan
TI Bidirectional Convolutional Recurrent Sparse Network (BCRSN): An
   Efficient Model for Music Emotion Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Music emotion recognition; bidirectional convolutional recurrent sparse
   network; sequential-information-included affect-salient features
   selection; long short-term memory; Lasso regression
ID FEATURES
AB Music emotion recognition, which enables effective and efficient music organization and retrieval, is a challenging subject in the field of music information retrieval. In this paper, we propose a new bidirectional convolutional recurrent sparse network (BCRSN) for music emotion recognition based on convolutional neural networks and recurrent neural networks. Our model adaptively learns the sequential-information-included affect-salient features (SII-ASF) from the 2-D time-frequency representation (i.e., spectrogram) of music audio signals. By combining feature extraction, ASF selection, and emotion prediction, the BCRSN can achieve continuous emotion prediction of audio files. To reduce the high computational complexity caused by the numerical-type ground truth, we propose a weighted hybrid binary representation (WHBR) method that converts the regression prediction process into a weighted combination of multiple binary classification problems. We test our method on two benchmark databases, that is, the Database for Emotional Analysis in Music and MoodSwings Turk. The results show that the WHBR method can greatly reduce the training time and improve the prediction accuracy. The extracted SII-ASF is robust to genre, timbre, and noise variation and is sensitive to emotion. It achieves significant improvement compared to the best performing feature sets in MediaEval 2015. Meanwhile, extensive experiments demonstrate that the proposed method outperforms the state-of-the-art methods.
C1 [Dong, Yizhuo; Yang, Xinyu; Zhao, Xi] Xi An Jiao Tong Univ, Dept Comp Sci & Technol, Xian 710049, Peoples R China.
   [Li, Juan] Xi An Jiao Tong Univ, Mus Educ Ctr, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University
RP Li, J (corresponding author), Xi An Jiao Tong Univ, Mus Educ Ctr, Xian 710049, Peoples R China.
EM dyzhuo@stu.xjtu.edu.cn; yxyphd@mail.xjtu.edu.cn;
   xi.zhao@mail.xjtu.edu.cn; lijuan@xjtu.edu.cn
RI Wang, Rong/JQI-7854-2023
OI Wang, Rong/0009-0009-5350-5743
FU National Natural Science Foundation of China [61602366]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61602366. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. M. Shamim
   Hossain. (Yizhuo Dong and Xinyu Yang contributed equally to this work.)
CR Aljanaki A, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0173392
   [Anonymous], 2016, IJCAI
   [Anonymous], 2016, Proceedings of the 24th ACM international conference on Multimedia, DOI DOI 10.1145/2964284.2967196
   [Anonymous], 2016, IEEE Energy Conversion Congress and Exposition ECCE
   [Anonymous], P 5 INT WORKSH AUD V
   [Anonymous], 2012, ACM T INTEL SYST TEC, DOI DOI 10.1145/2168752.2168754
   [Anonymous], 2001, P COST G6 C DIG AUD
   [Anonymous], 2018, P ISMIR 2018 19 INT
   Basu K, 2013, INT WORKSHOP MICROPR, P62, DOI 10.1109/MTV.2013.13
   Cai M, 2016, SPEECH COMMUN, V77, P53, DOI 10.1016/j.specom.2015.12.003
   Chen S., 2017, P 7 ANN WORKSH AUD V, P19
   Choi K, 2017, INT CONF ACOUST SPEE, P2392, DOI 10.1109/ICASSP.2017.7952585
   Choi W, 2016, UBICOMP'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P805, DOI 10.1145/2971648.2971756
   Deng JJ, 2015, ACM T INTERACT INTEL, V5, DOI 10.1145/2723575
   Dieleman Sander, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6964, DOI 10.1109/ICASSP.2014.6854950
   Dieleman S., 2011, 12th International Society for Music Information Retrieval Conference (ISMIR-2011), P669
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Fan K., 2017, INT SOC MUSIC INF RE, P368
   Fan Y, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P445, DOI 10.1145/2993148.2997632
   Fu ZY, 2011, IEEE T MULTIMEDIA, V13, P303, DOI 10.1109/TMM.2010.2098858
   Grill T, 2015, EUR SIGNAL PR CONF, P1296, DOI 10.1109/EUSIPCO.2015.7362593
   Han Y, 2017, IEEE-ACM T AUDIO SPE, V25, P208, DOI 10.1109/TASLP.2016.2632307
   He Lang, 2015, P 5 INT WORKSH AUD V
   Humphrey EJ, 2012, 2012 11TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2012), VOL 2, P357, DOI 10.1109/ICMLA.2012.220
   Juslin PN, 2001, MUSIC EMOTION THEORY, DOI DOI 10.1016/j.infbeh.2009.11.003
   Latha C.P., 2016, Aptikom, V1, P88
   Li TLH, 2010, LECT NOTES ENG COMP, P546
   Li XX, 2016, INT CONF ACOUST SPEE, P544, DOI 10.1109/ICASSP.2016.7471734
   Lim W., 2016, 2016 IEEE Symposium on VLSI Circuits (VLSI-Circuits), P1
   Ma XC, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON AUDIO/VISUAL EMOTION CHALLENGE (AVEC'16), P35, DOI 10.1145/2988257.2988267
   Malik M., 2017, Proc. Sound Music Comput. Conf, P208
   Mao QR, 2014, IEEE T MULTIMEDIA, V16, P2203, DOI 10.1109/TMM.2014.2360798
   Montavon Gregoire., 2009, Proc. NIPS Workshop on deep learning for Speech Recognition and Related Applications, P1
   Panda R, 2020, IEEE T AFFECT COMPUT, V11, P614, DOI 10.1109/TAFFC.2018.2820691
   Speck JacquelinA., 2011, 12th International Society for Music Information Retrieval Conference (ISMIR 2011), number Ismir, P549
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Van den Oord A., 2013, P NIPS
   Wang J.-C., 2012, P 20 ACM INT C MULTI, P89, DOI DOI 10.1145/2393347.2396494
   Wu B, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P117, DOI 10.1145/2647868.2654904
   Wu XX, 2016, IEEE T MULTIMEDIA, V18, P1305, DOI 10.1109/TMM.2016.2557722
   Xianyu HS, 2016, INT CONF ACOUST SPEE, P549, DOI 10.1109/ICASSP.2016.7471735
   Yang XY, 2018, MULTIMEDIA SYST, V24, P365, DOI 10.1007/s00530-017-0559-4
   Yang YH, 2013, IEEE T MULTIMEDIA, V15, P1304, DOI 10.1109/TMM.2013.2265078
   Zhang JL, 2016, NEUROCOMPUTING, V208, P333, DOI 10.1016/j.neucom.2016.01.099
   Zhang SQ, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P281, DOI 10.1145/2911996.2912051
   Zheng WQ, 2015, INT CONF AFFECT, P827, DOI 10.1109/ACII.2015.7344669
NR 46
TC 40
Z9 43
U1 4
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2019
VL 21
IS 12
BP 3150
EP 3163
DI 10.1109/TMM.2019.2918739
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KJ8ZS
UT WOS:000512345200014
DA 2024-07-18
ER

PT J
AU Beyan, C
   Katsageorgiou, VM
   Murino, V
AF Beyan, Cigdem
   Katsageorgiou, Vasiliki-Maria
   Murino, Vittorio
TI A Sequential Data Analysis Approach to Detect Emergent Leaders in Small
   Groups
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emergent leader; social signal processing; nonverbal features; small
   group interactions; sequential data analysis; temporal data; restricted
   Boltzmann machines
ID REGION COVARIANCE; MEETINGS; RECOGNITION; ROLES
AB This paper addresses the problem of predicting emergent leaders (ELs) in small groups, that is, meetings. This is a long-lasting research problem for social and organizational psychology and a relevant problem that recently gained momentum in social computing. Toward this goal, we propose a novel method, which analyzes the temporal dependencies of the audio-visual data by applying unsupervised deep learning generative models (feature learning). To the best of our knowledge, this is the first attempt that sequential data processing is performed for EL detection. Feature learning results in a single feature vector per a given time interval and all feature vectors representing a participant are aggregated using novel fusion techniques. Finally, the EL detection is performed using the state-of-the-art single and multiple kernel learning algorithms. The proposed method shows (significantly) improved results compared to the state-of-the-art methods and it can be adapted to analyze various small group interactions given that it is a general approach.
C1 [Beyan, Cigdem; Murino, Vittorio] Ist Italiano Tecnol, Dept Pattern Anal & Comp Vis, I-16152 Genoa, Italy.
   [Katsageorgiou, Vasiliki-Maria] Ist Italiano Tecnol, Dept Adv Robot, I-16163 Genoa, Italy.
   [Murino, Vittorio] Univ Verona, Dept Comp Sci, I-37129 Verona, Italy.
C3 Istituto Italiano di Tecnologia - IIT; Istituto Italiano di Tecnologia -
   IIT; University of Verona
RP Beyan, C (corresponding author), Ist Italiano Tecnol, Dept Pattern Anal & Comp Vis, I-16152 Genoa, Italy.
EM cigdem.beyan@iit.it; vasiliki.katsageorgiou@iit.it;
   vittorio.murino@iit.it
RI Beyan, Cigdem/AAA-4235-2019; Murino, Vittorio/A-5570-2011
OI Beyan, Cigdem/0000-0002-9583-0087; Katsageorgiou,
   Vasiliki-Maria/0000-0003-4974-4767
CR Al-Hames M., 2006, Machine Learning for Multimodal Interaction. Second International Workshop, MLMI 2005. Revised Selected Papers (Lecture Notes in Computer Science Vol. 3869), P52
   Amer M. R., 2015, CORR
   Amer MR, 2018, INT J COMPUT VISION, V126, P440, DOI 10.1007/s11263-017-0997-7
   [Anonymous], 1991, Joining together: Group theory and group skills
   [Anonymous], 2016, P ACM ICMI ASSP4MI
   Avci U, 2016, IEEE T MULTIMEDIA, V18, P643, DOI 10.1109/TMM.2016.2521348
   Baltrusaitis T, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P354, DOI 10.1109/ICCVW.2013.54
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Beyan C, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1425, DOI 10.1145/3123266.3123404
   Beyan C, 2018, IEEE T MULTIMEDIA, V20, P441, DOI 10.1109/TMM.2017.2740062
   Beyan C, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P317, DOI 10.1145/2993148.2993175
   Boulanger-Lewandowski N., 2012, P 29 INT C MACH LEAR
   Chetty K, 2010, IEEE RAD CONF, P188, DOI 10.1109/RADAR.2010.5494627
   Coates A., 2011, P 14 INT C ART INT S, V15, P215
   Dielmann A, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P629
   Dielmann A, 2007, IEEE T MULTIMEDIA, V9, P25, DOI 10.1109/TMM.2006.886337
   Dong W, 2013, IEEE T MULTIMEDIA, V15, P83, DOI 10.1109/TMM.2012.2225039
   Dong W, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P271
   Favre S., 2009, P 17 ACM INT C MULT, P585
   Favre S., 2008, Proc. ACM International Conference on Multimodal Interfaces, P29
   Gatica-Perez D, 2005, INT CONF ACOUST SPEE, P489
   Gonen M., 2008, ICML, P352, DOI DOI 10.1145/1390156.1390201
   Gönen M, 2011, J MACH LEARN RES, V12, P2211
   Han XW, 2009, THIRD INTERNATIONAL CONFERENCE ON GENETIC AND EVOLUTIONARY COMPUTING, P343, DOI 10.1109/WGEC.2009.133
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hung H, 2011, IEEE T AUDIO SPEECH, V19, P847, DOI 10.1109/TASL.2010.2066267
   Hussein, 2013, INT JOINT C ART INT
   Kalimeri Kyriaki, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P124, DOI 10.1007/978-3-642-25446-8_14
   Kalimeri K, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P23
   Längkvist M, 2014, PATTERN RECOGN LETT, V42, P11, DOI 10.1016/j.patrec.2014.01.008
   McCowan I, 2005, IEEE T PATTERN ANAL, V27, P305, DOI 10.1109/TPAMI.2005.49
   McCowan I, 2003, INT CONF ACOUST SPEE, P748
   Otsuka K., 2006, Proc. Conf. Human Factors in Computing Systems, P1175
   Otsuka K., 2005, P 7 INT C MULT INT, P191, DOI DOI 10.1145/1088463.1088497
   Otsuka K, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P255
   Pang YW, 2008, IEEE T CIRC SYST VID, V18, P989, DOI 10.1109/TCSVT.2008.924108
   Porikli F., 2006, 2006 IEEE COMPUTER S, V1, P728, DOI [10.1109/CVPR.2006.94, DOI 10.1109/CVPR.2006.94]
   Ranzato M, 2010, PROC CVPR IEEE, P2551, DOI 10.1109/CVPR.2010.5539962
   Rienks R., 2006, P 8 INT C MULTIMODAL, P257
   Sanchez-Cortes D., 2013, THESIS
   Sanchez-Cortes D, 2013, J MULTIMODAL USER IN, V7, P39, DOI 10.1007/s12193-012-0101-0
   Sanchez-Cortes D, 2012, IEEE T MULTIMEDIA, V14, P816, DOI 10.1109/TMM.2011.2181941
   Sanchez-Cortes Dairazalia, 2010, P ACM ICMI MLMI, P8
   Sapru A, 2013, INTERSPEECH, P1529
   Sapru A, 2015, IEEE T MULTIMEDIA, V17, P746, DOI 10.1109/TMM.2015.2408437
   Stiefelhagen R, 2002, IEEE T NEURAL NETWOR, V13, P928, DOI 10.1109/TNN.2002.1021893
   Sutskever I., 2007, PROCEEDING 11 INT C, P544
   Sutskever I., 2008, P 21 INT C NEUR INF, P1601
   Taylor GW, 2011, J MACH LEARN RES, V12, P1025
   Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589
   Tuzel O, 2007, PROC CVPR IEEE, P1736
   Vinciarelli A., 2010, P MEAS BEH EINDH NET, P118
   Vinciarelli A, 2011, IEEE SYS MAN CYBERN, P374, DOI 10.1109/ICSMC.2011.6083694
   Zhang D, 2006, IEEE T MULTIMEDIA, V8, P509, DOI 10.1109/TMM.2006.870735
NR 54
TC 13
Z9 13
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2019
VL 21
IS 8
BP 2107
EP 2116
DI 10.1109/TMM.2019.2895505
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IK7YM
UT WOS:000476809700017
DA 2024-07-18
ER

PT J
AU Xie, WL
   Yao, HX
   Sun, XS
   Han, TT
   Zhao, SC
   Chua, TS
AF Xie, Wenlong
   Yao, Hongxun
   Sun, Xiaoshuai
   Han, Tingting
   Zhao, Sicheng
   Chua, Tat-Seng
TI Discovering Latent Discriminative Patterns for Multi-Mode Event
   Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Segment-level; visual topics; event representation; latent patterns;
   event epitomes
ID RECOGNITION
AB Representation of videos is essential since it conveys an understanding of video content and enables many higher level tasks to be tackled efficiently. However, it is challenging to propose a rational representation for complex event videos, as most video information is either noisy or redundant. In this paper, we propose a compact event representation method that can concisely describe the inner modes of events. We deem that an optimal event representation scheme should reflect the long-term and high-level visual semantics (visual topics) of events, so different from previous frame-level video semantics representation methods and concept-based video representation methods, we investigate the problem from the perspective of segment-level video representations. We then present three appealing properties of segment-level visual semantics. Based on the observation, we propose different algorithms that rely on a novel deep-visual-word-based video encoding method to discover latent discriminative patterns of events. Finally, our multi-mode event representation is obtained by concatenating the discovered patterns as inner modes. We adopt our event representation for representative event parts mining, which can highlight the visual topics of events and remarkably prune the raw videos. We validate our event representation method based on complex event detection task. Experimental results on two standard benchmarking datasets, MED11 and CCV Dataset, show that the proposed method can significantly outperform the state-of-the-art approaches.
C1 [Xie, Wenlong; Yao, Hongxun; Sun, Xiaoshuai; Han, Tingting] Harbin Inst Technol, Dept Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
   [Zhao, Sicheng] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
   [Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
C3 Harbin Institute of Technology; University of California System;
   University of California Berkeley; National University of Singapore
RP Yao, HX (corresponding author), Harbin Inst Technol, Dept Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
EM wlxie@hit.edu.cn; h.yao@hit.edu.cn; xiaoshuaisun@hit.edu.cn;
   ttinghan@hit.edu.cn; schzhao@gmail.com; chuats@comp.nus.edu.sg
RI HAN, TINGTING/GQZ-8692-2022
FU National Natural Science Foundation of China [61472103, 61772158,
   61702136, U1711265]; National Research Foundation, Prime Ministers
   Office, Singapore under its IRC@Singapore Funding Initiative as part of
   NExT++ project
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61472103, 61772158, 61702136, and
   U1711265, and in part by the National Research Foundation, Prime
   Ministers Office, Singapore under its IRC@Singapore Funding Initiative
   as part of NExT++ project. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Marco
   Bertini.
CR [Anonymous], 2011, P 1 INT C MULT RETR
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2015, NATURE, DOI [DOI 10.1038/NATURE14539, 10.1038/nature14539]
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], PROC CVPR IEEE
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], IEEE T AFFECT COMPUT
   [Anonymous], 2015, PROC 3 INT C LEARNIN
   [Anonymous], IEEE T MULTIMEDIA
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Cai D, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 2, P608, DOI 10.18653/v1/P17-2096
   Chenxia Wu, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P4362, DOI 10.1109/CVPR.2015.7299065
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Girdhar R, 2017, PROC CVPR IEEE, P3165, DOI 10.1109/CVPR.2017.337
   Goel S, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1434, DOI 10.1145/3123266.3123407
   Izadinia H, 2012, LECT NOTES COMPUT SC, V7575, P430, DOI 10.1007/978-3-642-33765-9_31
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang YG, 2015, IEEE T MULTIMEDIA, V17, P1174, DOI 10.1109/TMM.2015.2436813
   Katharopoulos A, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P332, DOI 10.1145/2964284.2967237
   Lai KT, 2014, PROC CVPR IEEE, P2251, DOI 10.1109/CVPR.2014.288
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Li C, 2017, IEEE T IMAGE PROCESS, V26, P2149, DOI 10.1109/TIP.2017.2670782
   Li MK, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON SERVICE OPERATIONS AND LOGISTICS, AND INFORMATICS (SOLI), P200, DOI 10.1109/SOLI.2016.7551687
   Li Y, 2015, PROC CVPR IEEE, P971, DOI 10.1109/CVPR.2015.7298699
   Ma ZG, 2014, IEEE T PATTERN ANAL, V36, P1789, DOI 10.1109/TPAMI.2014.2306419
   Mazloom M, 2016, IEEE T MULTIMEDIA, V18, P1378, DOI 10.1109/TMM.2016.2559947
   Mazloom M, 2014, IEEE T MULTIMEDIA, V16, P2214, DOI 10.1109/TMM.2014.2359771
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Pan PB, 2016, PROC CVPR IEEE, P1029, DOI 10.1109/CVPR.2016.117
   Phan S, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1255, DOI 10.1145/2733373.2806330
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Ramanathan V, 2015, IEEE I CONF COMP VIS, P4471, DOI 10.1109/ICCV.2015.508
   Ramanathan V, 2013, IEEE I CONF COMP VIS, P905, DOI 10.1109/ICCV.2013.117
   Song J, 2016, IEEE T IMAGE PROCESS, V25, P4999, DOI 10.1109/TIP.2016.2601260
   Sun C, 2013, IEEE I CONF COMP VIS, P913, DOI 10.1109/ICCV.2013.453
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Vondrick C., 2016, ADV NEURAL INFORM PR, P613, DOI DOI 10.13016/M26GIH-TNYZ
   Wang F, 2008, ADVANCES IN MATRIX THEORY AND ITS APPLICATIONS, VOL 1, P238
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Xie WL, 2018, SIGNAL PROCESS, V149, P82, DOI 10.1016/j.sigpro.2018.03.005
   Xie WL, 2016, INT CONF ACOUST SPEE, P1253, DOI 10.1109/ICASSP.2016.7471877
   Xu Z, 2015, IEEE I CONF COMP VIS, P3191, DOI 10.1109/ICCV.2015.365
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P346, DOI 10.1109/TMM.2015.2393635
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Ye GG, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P471, DOI 10.1145/2733373.2806221
   Yu LT, 2016, IEEE T MULTIMEDIA, V18, P1590, DOI 10.1109/TMM.2016.2557059
   Zhang XS, 2015, IEEE T MULTIMEDIA, V17, P1562, DOI 10.1109/TMM.2015.2449660
   Zhao SC, 2018, IEEE T CYBERNETICS, V48, P3218, DOI 10.1109/TCYB.2017.2762344
   Zhu LC, 2017, PROC CVPR IEEE, P1339, DOI 10.1109/CVPR.2017.147
NR 54
TC 6
Z9 6
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2019
VL 21
IS 6
BP 1425
EP 1436
DI 10.1109/TMM.2018.2879749
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IA1QQ
UT WOS:000469337400007
DA 2024-07-18
ER

PT J
AU Hsu, HW
   Wu, TY
   Wan, S
   Wong, WH
   Lee, CY
AF Hsu, Heng-Wei
   Wu, Tung-Yu
   Wan, Sheng
   Wong, Wing Hung
   Lee, Chen-Yi
TI QuatNet: Quaternion-Based Head Pose Estimation With Multiregression Loss
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural network (CNN); head pose estimation; ordinal
   regression; quaternion
AB Head pose estimation has attracted immense research interest recently, as its inherent information significantly improves the performance of face-related applications such as face alignment and face recognition. In this paper, we conduct an in-depth study of head pose estimation and present a multiregression loss function, an L2 regression loss combined with an ordinal regression loss, to train a convolutional neural network (CNN) that is dedicated to estimating head poses from RGB images without depth information. The ordinal regression loss is utilized to address the nonstationary property observed as the facial features change with respect to different head pose angles and learn robust features. The L2 regression loss leverages these features to provide precise angle predictions for input images. To avoid the ambiguity problem in the commonly used Euler angle representation, we further formulate the head pose estimation problem in quaternions. Our quaternion-based multiregression loss method achieves state-of-the-art performance on the AFLW2000, AFLW test set, and AFW datasets and is closing the gap with methods that utilize depth information on the BIWI dataset.
C1 [Hsu, Heng-Wei; Wan, Sheng; Lee, Chen-Yi] Natl Chiao Tung Univ, Inst Elect, Hsinchu 300, Taiwan.
   [Wu, Tung-Yu; Wong, Wing Hung] Stanford Univ, Inst Computat & Math Engn, Stanford, CA 94305 USA.
C3 National Yang Ming Chiao Tung University; Stanford University
RP Hsu, HW (corresponding author), Natl Chiao Tung Univ, Inst Elect, Hsinchu 300, Taiwan.
EM hengwzx@si2lab.org; tungyuwu@stanford.edu; vjod@si2lab.org;
   whwong@stanford.edu; cylee@si2lab.org
RI Wu, Tung-Yu/CAH-2223-2022; Hsu, Heng-Wei/GWN-0104-2022
FU MOST/MOE of Taiwan [105-2218-E-009-001/107W210]; National Science
   Foundation of USA [DMS-1407557]
FX This work was supported by MOST/MOE of Taiwan under Grant
   105-2218-E-009-001/107W210 and National Science Foundation of USA under
   Grant DMS-1407557. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Faith Porikli.
   (Corresponding author: Heng-Wei Hsu.)
CR [Anonymous], P BRIT MACH VIS C BM
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2016, P IEEE WINT C APPL C
   [Anonymous], 2017, P IEEE INT C COMP VI, DOI DOI 10.1109/ICCVW.2017.188
   [Anonymous], 2017, P IEEE C COMPUTER VI
   [Anonymous], ACCV 2014
   [Anonymous], IEEE Transactions on Pattern Analysis and Machine Intelligence
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2017, COMPUTER VISION PATT
   [Anonymous], 2018, P IEEE CVF C COMP VI
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.532
   [Anonymous], 2017, IEEE T PATTERN ANAL
   Gutiérrez PA, 2016, IEEE T KNOWL DATA EN, V28, P127, DOI 10.1109/TKDE.2015.2457911
   Belhumeur PN, 2013, IEEE T PATTERN ANAL, V35, P2930, DOI 10.1109/TPAMI.2013.23
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Chang KY, 2015, IEEE T IMAGE PROCESS, V24, P785, DOI 10.1109/TIP.2014.2387379
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Doyle OM, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0105542
   Fanelli G, 2013, INT J COMPUT VISION, V101, P437, DOI 10.1007/s11263-012-0549-0
   Fathian K, 2018, IEEE ROBOT AUTOM LET, V3, P857, DOI 10.1109/LRA.2018.2792142
   Feng SH, 2017, IEEE T MULTIMEDIA, V19, P136, DOI 10.1109/TMM.2016.2608786
   Fernández-Navarro F, 2013, IEEE T CYBERNETICS, V43, P2228, DOI 10.1109/TSMCC.2013.2247595
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jourabloo A, 2016, PROC CVPR IEEE, P4188, DOI 10.1109/CVPR.2016.454
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Köstinger M, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Kong SG, 2015, IEEE T IMAGE PROCESS, V24, P1801, DOI 10.1109/TIP.2015.2405483
   Kumar A, 2017, IEEE INT CONF AUTOMA, P258, DOI 10.1109/FG.2017.149
   Lepetit Vincent, 2005, Foundations and Trends in Computer Graphics and Vision, V1, P1, DOI 10.1561/0600000001
   Li L., 2006, Adv. Neural Inf. Process. Syst., V19, P865
   Masi I, 2016, PROC CVPR IEEE, P4838, DOI 10.1109/CVPR.2016.523
   Masi I, 2016, LECT NOTES COMPUT SC, V9909, P579, DOI 10.1007/978-3-319-46454-1_35
   Meyer GP, 2015, IEEE I CONF COMP VIS, P3649, DOI 10.1109/ICCV.2015.416
   Mukherjee SS, 2015, IEEE T MULTIMEDIA, V17, P2094, DOI 10.1109/TMM.2015.2482819
   Papazov C, 2015, PROC CVPR IEEE, P4722, DOI 10.1109/CVPR.2015.7299104
   Patacchiola M, 2017, PATTERN RECOGN, V71, P132, DOI 10.1016/j.patcog.2017.06.009
   Pérez-Ortiz M, 2014, APPL SOFT COMPUT, V14, P88, DOI 10.1016/j.asoc.2013.07.017
   Ranjan R, 2017, IEEE INT CONF AUTOMA, P17, DOI 10.1109/FG.2017.137
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Valenti R, 2012, IEEE T IMAGE PROCESS, V21, P802, DOI 10.1109/TIP.2011.2162740
   Wu Y, 2017, PROC CVPR IEEE, P5719, DOI 10.1109/CVPR.2017.606
   Yu Y, 2017, IEEE INT CONF AUTOMA, P711, DOI 10.1109/FG.2017.90
   Zhang XC, 2015, PROC CVPR IEEE, P4511, DOI 10.1109/CVPR.2015.7299081
   Zhou EJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P386, DOI 10.1109/ICCVW.2013.58
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
NR 51
TC 95
Z9 97
U1 1
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2019
VL 21
IS 4
BP 1035
EP 1046
DI 10.1109/TMM.2018.2866770
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HQ4XD
UT WOS:000462413700018
DA 2024-07-18
ER

PT J
AU Hu, D
   Nie, FP
   Li, XL
AF Hu, Di
   Nie, Feiping
   Li, Xuelong
TI Deep Binary Reconstruction for Cross-Modal Hashing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal hashing; binary reconstruction
ID IMAGE; CODES
AB To satisfy the huge storage space and organization capacity requirements in addressing big multimodal data, hashing techniques have been widely employed to learn binary representations in cross-modal retrieval tasks. However, optimizing the hashing objective under the necessary binary constraint is truly a difficult problem. A common strategy is to relax the constraint and perform individual binarizations over the learned real-valued representations. In this paper, in contrast to conventional two-stage methods, we propose to directly learn the binary codes, where the model can be easily optimized by a standard gradient descent optimizer. However, before that, we present a theoretical guarantee of the effectiveness of the multimodal network in preserving the inter-and intra-modal consistencies. Based on this guarantee, a novel multimodal deep binary reconstruction model is proposed, which can be trained to simultaneously model the correlation across modalities and learn the binary hashing codes. To generate binary codes and to avoid the tiny gradient problem, a novel activation function first scales the input activations to suitable scopes and, then, feeds them to the tanh function to build the hashing layer. Such a composite function is named adaptive tanh. Both linear and nonlinear scaling methods are proposed and shown to generate efficient codes after training the network. Extensive ablation studies and comparison experiments are conducted for the image2text and text2image retrieval tasks; the method is found to outperform several state-of-the-art deep-learning methods with respect to different evaluation metrics.
C1 [Hu, Di; Nie, Feiping] Northwestern Polytech Univ, Sch Comp Sci & Engn, Xian 710072, Shaanxi, Peoples R China.
   [Li, Xuelong] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Shaanxi, Peoples R China.
   [Li, Xuelong] Northwestern Polytech Univ, Ctr OPT IMagery Anal & Learning OPTIMAL, Xian 710072, Shaanxi, Peoples R China.
   [Li, Xuelong] Chinese Acad Sci, Xian Inst Opt & Precis Mech, Xian 710119, Shaanxi, Peoples R China.
C3 Northwestern Polytechnical University; Northwestern Polytechnical
   University; Northwestern Polytechnical University; Chinese Academy of
   Sciences; Xi'an Institute of Optics & Precision Mechanics, CAS
RP Hu, D (corresponding author), Northwestern Polytech Univ, Sch Comp Sci & Engn, Xian 710072, Shaanxi, Peoples R China.
EM hdui831@mail.nwpu.edu.cn; feipingnie@gmail.com; xuelong_li@nwpu.edu.cn
RI Nie, Feiping/B-3039-2012; Li, Xuelong/Z-3785-2019; Li,
   Xuelong/ABF-3381-2020; ARSLAN, Okan/AAA-3232-2020; li,
   xiang/GWM-6319-2022
OI Li, Xuelong/0000-0002-0019-4197; Nie, Feiping/0000-0002-0871-6519
FU National Key Research and Development Program of China [2018YFB1107400];
   National Natural Science Foundation of China [61761130079, 61772427,
   61751202]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018YFB1107400 and in part by
   the National Natural Science Foundation of China under Grants
   61761130079, 61772427, and 61751202. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Xavier Giro-i-Nieto. (Corresponding author: Di Hu.)
CR Andrew G., 2013, P ICML, P1247
   [Anonymous], 2012, P INT C NEUR INF PRO
   [Anonymous], 2009, NEURIPS
   [Anonymous], 2017, PROC IEEE INT C COMP
   [Anonymous], 2011, P ICML
   [Anonymous], ARXIV150807148
   [Anonymous], ARXIV170901305
   [Anonymous], 2014, Advances in neural information processing systems
   [Anonymous], 2017, ARXIV170200758
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], CVPR
   [Anonymous], 2016, P 2016 ACM MULT C MM, DOI DOI 10.1145/2964284.2967239
   [Anonymous], 2014, PROC VLDB ENDOW
   Baraldi L, 2017, IEEE T MULTIMEDIA, V19, P955, DOI 10.1109/TMM.2016.2644872
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Courbariaux M., 2016, BinaryNet: Training deep neural networks with weights and activa
   Ding GG, 2016, IEEE T IMAGE PROCESS, V25, P5427, DOI 10.1109/TIP.2016.2607421
   Ding K, 2017, IEEE T MULTIMEDIA, V19, P571, DOI 10.1109/TMM.2016.2625747
   Dong QL, 2018, SCI CHINA INFORM SCI, V61, DOI 10.1007/s11432-017-9234-7
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Gong YC, 2013, PROC CVPR IEEE, P484, DOI 10.1109/CVPR.2013.69
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Hao YB, 2017, IEEE T MULTIMEDIA, V19, P1, DOI 10.1109/TMM.2016.2610324
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hu D, 2016, PROC CVPR IEEE, P3574, DOI 10.1109/CVPR.2016.389
   Huang Y, 2015, IEEE T MULTIMEDIA, V17, P1923, DOI 10.1109/TMM.2015.2476658
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Li XL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1398, DOI 10.1145/3123266.3123355
   Li XL, 2017, AAAI CONF ARTIF INTE, P2203
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   Lu XQ, 2018, IEEE T IMAGE PROCESS, V27, P106, DOI 10.1109/TIP.2017.2755766
   Lu XQ, 2017, IEEE T IMAGE PROCESS, V26, P355, DOI 10.1109/TIP.2016.2627801
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Rastegari Mohammad, 2013, Book Predictable Dual-View Hashing
   Salakhutdinov R., 2009, AISTATS
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344
   Song JK, 2018, PATTERN RECOGN, V75, P175, DOI 10.1016/j.patcog.2017.03.021
   Sutskever I, 2014, ADV NEUR IN, V27
   Tieleman T., 2008, P 25 INT C MACHINE L, V307, P1064, DOI 10.1145/1390156
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang K., 2016, ABS160706215 CORR
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Xu X, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P305, DOI 10.1145/2911996.2912056
   Zhang L, 2018, IEEE T MULTIMEDIA, V20, P128, DOI 10.1109/TMM.2017.2723841
   Zhang T, 2016, PROC CVPR IEEE, P2036, DOI 10.1109/CVPR.2016.224
   Zhao Z, 2018, IEEE T MULTIMEDIA, V20, P430, DOI 10.1109/TMM.2017.2740022
   Zhou JL, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P415
NR 55
TC 85
Z9 95
U1 1
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2019
VL 21
IS 4
BP 973
EP 985
DI 10.1109/TMM.2018.2866771
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HQ4XD
UT WOS:000462413700013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, Y
   Jing, XY
   Nie, JH
   Gao, H
   Liu, J
   Jiang, GP
AF Liu, Ye
   Jing, Xiao-Yuan
   Nie, Jianhui
   Gao, Hao
   Liu, Jun
   Jiang, Guo-Ping
TI Context-Aware Three-Dimensional Mean-Shift With Occlusion Handling for
   Robust Object Tracking in RGB-D Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visual tracking; RGB-D camera; mean-shift; point cloud
ID REAL-TIME; PARTICLE FILTER; COMPLEX
AB Depth cameras have recently become popular and many vision problems can be better solved with depth information. But, how to integrate depth information into a visual tracker to overcome the challenges such as occlusion and background distraction is still underinvestigated in current literature on visual tracking. In this paper, we investigate a 3-D extension of a classical mean-shift tracker whose greedy gradient ascend strategy is generally considered as unreliable in conventional 2-D tracking. However, through careful study of the physical property of 3-D point clouds, we reveal that objects which may appear to be adjacent on a 2-D image will form distinctive modes in the 3-D probability distribution approximated by kernel density estimation, and finding the nearest mode using 3-D mean-shift can always work in tracking. Based on the understanding of 3-Dmean-shift, we propose two important mechanisms to further boost the tracker's robustness: one is to enable the tracker to be aware of potential distractions and make corresponding adjustments to the appearance model; and the other is to enable the tracker to detect and recover from tracking failures caused by total occlusion. The proposed method is both effective and computationally efficient. On a conventional personal computer, it runs at more than 60 FPS without graphical processing unit acceleration.
C1 [Liu, Ye; Jing, Xiao-Yuan; Nie, Jianhui; Jiang, Guo-Ping] Nanjing Univ Posts & Telecommun, Sch Automat, Nanjing 210023, Jiangsu, Peoples R China.
   [Liu, Ye; Nie, Jianhui; Jiang, Guo-Ping] Jiangsu Engn Lab Internet Things & Intelligent Ro, Nanjing 210023, Jiangsu, Peoples R China.
   [Gao, Hao] Nanjing Univ Posts & Telecommun, Inst Adv Technol, Nanjing 210023, Jiangsu, Peoples R China.
   [Liu, Jun] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanjing University of Posts & Telecommunications; Nanjing University of
   Posts & Telecommunications; Nanyang Technological University
RP Liu, Y (corresponding author), Nanjing Univ Posts & Telecommun, Sch Automat, Nanjing 210023, Jiangsu, Peoples R China.
EM yeliu@njupt.edu.cn; cnjingxy@njupt.edu.cn; njh19@163.com;
   tsgaohao@gmail.com; jliu029@ntu.edu.sg; jianggp@njupt.edu.cn
RI Gao, Hao/AAJ-7124-2020; Jiang, Guo/ISR-9858-2023; Zhang,
   Yunxuan/IXD-9283-2023
OI Liu, Jun/0000-0002-4365-4165; Liu, Ye/0000-0002-2686-3002
FU National Natural Science Foundation of China [61602255, 61571236];
   Natural Science Foundation for Colleges and Universities in Jiangsu
   Province [16KJB520032]; Youth Foundation of Jiangsu Province
   [BK20140892]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61602255 and 61571236, in part by the
   Natural Science Foundation for Colleges and Universities in Jiangsu
   Province under Grant 16KJB520032, and in part by the Youth Foundation of
   Jiangsu Province under Grant BK20140892. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Jianfei Cai. (Corresponding author: Ye Liu.)
CR [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2014, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2013.230
   Awwad S, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1115, DOI 10.1145/2733373.2806295
   Bibi A, 2016, PROC CVPR IEEE, P1439, DOI 10.1109/CVPR.2016.160
   Birchfield ST, 2005, PROC CVPR IEEE, P1158
   Collins RT, 2005, IEEE T PATTERN ANAL, V27, P1631, DOI 10.1109/TPAMI.2005.205
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761
   Dinh TB, 2011, PROC CVPR IEEE, P1177, DOI 10.1109/CVPR.2011.5995733
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19
   Hahn M, 2010, LECT NOTES COMPUT SC, V6219, P101
   Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378
   Hannuna  S., 2015, P BRIT MACH VIS C
   Hannuna S, 2019, J REAL-TIME IMAGE PR, V16, P1439, DOI 10.1007/s11554-016-0654-3
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Leichter I, 2012, IEEE T PATTERN ANAL, V34, P695, DOI 10.1109/TPAMI.2011.167
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   Liu J, 2018, IEEE T IMAGE PROCESS, V27, P1586, DOI 10.1109/TIP.2017.2785279
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu J, 2013, IEEE IMAGE PROC, P3088, DOI 10.1109/ICIP.2013.6738636
   Liu J, 2015, PATTERN RECOGN LETT, V53, P16, DOI 10.1016/j.patrec.2014.09.013
   Luber M, 2011, IEEE INT C INT ROBOT, P3844, DOI 10.1109/IROS.2011.6048836
   Ma B, 2016, IEEE T IMAGE PROCESS, V25, P5867, DOI 10.1109/TIP.2016.2615812
   Ma B, 2016, IEEE T IMAGE PROCESS, V25, P4199, DOI 10.1109/TIP.2016.2588329
   Ma B, 2015, IEEE I CONF COMP VIS, P4400, DOI 10.1109/ICCV.2015.500
   Ma C, 2017, IEEE T MULTIMEDIA, V19, P2415, DOI 10.1109/TMM.2017.2694219
   Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292
   Meshgi K, 2016, COMPUT VIS IMAGE UND, V150, P81, DOI 10.1016/j.cviu.2016.05.011
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Shen JB, 2019, IEEE T CYBERNETICS, V49, P1990, DOI 10.1109/TCYB.2018.2803217
   Shen JB, 2018, IEEE T INTELL TRANSP, V19, P162, DOI 10.1109/TITS.2017.2750082
   Song SR, 2013, IEEE I CONF COMP VIS, P233, DOI 10.1109/ICCV.2013.36
   Tyagi A., 2007, PROC WKSHP MOTION VI, P1
   Wang S, 2011, IEEE I CONF COMP VIS, P1323, DOI 10.1109/ICCV.2011.6126385
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P3137, DOI 10.1109/TIP.2015.2438550
   Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wen LY, 2012, LECT NOTES COMPUT SC, V7575, P716, DOI 10.1007/978-3-642-33765-9_51
   Wen LY, 2015, PROC CVPR IEEE, P2226, DOI 10.1109/CVPR.2015.7298835
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xie Q, 2018, IEEE T MULTIMEDIA, V20, P580, DOI 10.1109/TMM.2017.2751965
   Ye GZ, 2012, LECT NOTES COMPUT SC, V7573, P828, DOI 10.1007/978-3-642-33709-3_59
   Youngmin Park, 2011, 2011 IEEE International Symposium on Mixed and Augmented Reality, P121, DOI 10.1109/ISMAR.2011.6092377
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang TZ, 2019, IEEE T PATTERN ANAL, V41, P365, DOI 10.1109/TPAMI.2018.2797062
   Zhang TZ, 2019, IEEE T PATTERN ANAL, V41, P473, DOI 10.1109/TPAMI.2018.2797082
   Zhang TZ, 2018, IEEE T IMAGE PROCESS, V27, P2676, DOI 10.1109/TIP.2017.2781304
   Zhou Y, 2017, IEEE T MULTIMEDIA, V19, P1798, DOI 10.1109/TMM.2017.2689918
NR 52
TC 51
Z9 58
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2019
VL 21
IS 3
BP 664
EP 677
DI 10.1109/TMM.2018.2863604
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HN6YJ
UT WOS:000460333800012
DA 2024-07-18
ER

PT J
AU Amini, M
   Sadreazami, H
   Ahmad, MO
   Swamy, MNS
AF Amini, Marzieh
   Sadreazami, Hamidreza
   Ahmad, M. Omair
   Swamy, M. N. S.
TI A Channel-Dependent Statistical Watermark Detector for Color Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimedia security; copyright protection; statistical modeling;
   detection; receiver operating characteristics
AB Data security is a main concern in everyday data transmissions over the Internet. A possible solution to guarantee secure and legitimate transaction is via hiding a piece of tractable information into the multimedia signal, that is, watermarking. In this paper, we propose a new color image watermarking scheme and its corresponding detector in the sparse domain. The watermark detector aims at verifying the ownership and circumventing any unauthorized duplication of the digital data. Most of the existing color image watermarking schemes disregard the inter-channel dependencies. In view of this, we take into account the interchannel dependencies between RGB channels and interscale dependencies of the sparse coefficients of color images by employing the hidden Markov model. An efficient detector is designed by establishing a binary hypothesis test through which the existence of the hidden watermark is examined. Experiments are conducted to evaluate the performance of the proposed watermark detector for color images. The results show that the proposed detector provides detection rates higher than those provided by the other detectors, even in the presence of attacks. It is also shown that the proposed detector exhibits better performance in terms of the robustness of the embedded watermark.
C1 [Amini, Marzieh; Sadreazami, Hamidreza; Ahmad, M. Omair; Swamy, M. N. S.] Concordia Univ, Ctr Signal Proc & Commun, Montreal, PQ H3G 1M8, Canada.
C3 Concordia University - Canada
RP Ahmad, MO (corresponding author), Concordia Univ, Ctr Signal Proc & Commun, Montreal, PQ H3G 1M8, Canada.
EM ma_amini@ece.concordia.ca; h_sadrea@ece.concordia.ca;
   omair@ece.concordia.ca; swamy@ece.concordia.ca
RI Amini, Marzieh/JRZ-1889-2023; Amini, Marzieh/U-1420-2019
OI Amini, Marzieh/0000-0002-5217-2969; Swamy,
   M.N.Srikanta/0000-0002-3989-5476
FU Natural Sciences and Engineering Research Council of Canada;
   Regroupement Strategique en Microelectronique du Quebec
FX This work was supported in part by the Natural Sciences and Engineering
   Research Council of Canada and in part by the Regroupement Strategique
   en Microelectronique du Quebec. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Yongdong Zhang.
CR Aminanto M.E., 2017, PROC S CRYPTOGR INF, P1
   Amini M, 2017, SIGNAL PROCESS, V137, P213, DOI 10.1016/j.sigpro.2017.01.019
   [Anonymous], 2012, Signal Detection in Non-Gaussian Noise
   Barni M, 2002, IEEE T CIRC SYST VID, V12, P142, DOI 10.1109/76.993436
   Barni M, 2002, J ELECTRON IMAGING, V11, P87, DOI 10.1117/1.1426383
   Crouse MS, 1998, IEEE T SIGNAL PROCES, V46, P886, DOI 10.1109/78.668544
   Hernández JR, 2000, IEEE T IMAGE PROCESS, V9, P55, DOI 10.1109/83.817598
   Korus P, 2015, IEEE T MULTIMEDIA, V17, P157, DOI 10.1109/TMM.2014.2368696
   Kwitt R, 2009, IEEE IMAGE PROC, P4245, DOI 10.1109/ICIP.2009.5413715
   Liu KC, 2010, AEU-INT J ELECTRON C, V64, P112, DOI 10.1016/j.aeue.2008.11.006
   Liu XL, 2018, IEEE T CIRC SYST VID, V28, P1047, DOI 10.1109/TCSVT.2016.2633878
   McLachlan G., 2007, EM ALGORITHM EXTENSI
   Naskar R, 2013, IET IMAGE PROCESS, V7, P99, DOI 10.1049/iet-ipr.2012.0232
   Po DDY, 2006, IEEE T IMAGE PROCESS, V15, P1610, DOI 10.1109/TIP.2006.873450
   Rabizadeh M, 2016, J VIS COMMUN IMAGE R, V40, P324, DOI 10.1016/j.jvcir.2016.07.001
   Sadreazami H, 2015, IEEE T CIRCUITS-II, V62, P1159, DOI 10.1109/TCSII.2015.2468995
   Su QT, 2016, IET IMAGE PROCESS, V10, P817, DOI 10.1049/iet-ipr.2016.0048
   Tsui TK, 2008, IEEE T INF FOREN SEC, V3, P16, DOI 10.1109/TIFS.2007.916275
   Wang CT, 2012, IEEE T INF FOREN SEC, V7, P853, DOI 10.1109/TIFS.2012.2188797
   Wang XY, 2013, J SYST SOFTWARE, V86, P255, DOI 10.1016/j.jss.2012.08.015
   Wu HZ, 2017, IEEE T CIRC SYST VID, V27, P1620, DOI 10.1109/TCSVT.2016.2556585
NR 21
TC 38
Z9 39
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2019
VL 21
IS 1
BP 65
EP 73
DI 10.1109/TMM.2018.2851447
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HF5DV
UT WOS:000454253700006
DA 2024-07-18
ER

PT J
AU Avola, D
   Bernardi, M
   Cinque, L
   Foresti, GL
   Massaroni, C
AF Avola, Danilo
   Bernardi, Marco
   Cinque, Luigi
   Foresti, Gian Luca
   Massaroni, Cristiano
TI Exploiting Recurrent Neural Networks and Leap Motion Controller for the
   Recognition of Sign Language and Semaphoric Hand Gestures
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hand gesture recognition; sign language; semaphoric gestures; Leap
   Motion Controller (LMC); Recurrent Neural Network (RNN); Long Short Term
   Memory (LSTM)
ID DESCRIPTORS; INTERFACES; DESIGN; SENSOR; TIME
AB Hand gesture recognition is still a topic of great interest for the computer vision community. In particular, sign language and semaphoric hand gestures are two foremost areas of interest due to their importance in human-human communication and human-computer interaction, respectively. Any hand gesture can be represented by sets of feature vectors that change over time. Recurrent neural networks (RNNs) are suited to analyze this type of set thanks to their ability to model the long-term contextual information of temporal sequences. In this paper, an RNN is trained by using as features the angles formed by the finger bones of the human hands. The selected features, acquired by a leap motion controller sensor, are chosen because the majority of human hand gestures produce joint movements that generate truly characteristic corners. The proposed method, including the effectiveness of the selected angles, was initially tested by creating a very challenging dataset composed by a large number of gestures defined by the American sign language. On the latter, an accuracy of over 96% was achieved. Afterwards, by using the Shape Retrieval Contest (SHREC) dataset, a wide collection of semaphoric hand gestures, the method was also proven to outperform in accuracy competing approaches of the current literature.
C1 [Avola, Danilo; Foresti, Gian Luca] Univ Udine Polo Sci Matemat Informat & Multimedia, Dept Math & Compr Sci, I-33100 Udine, Italy.
   [Bernardi, Marco; Cinque, Luigi; Massaroni, Cristiano] Univ Roma La Sapienza, Fac Ingn Informaz Informat & Stat, Dept Comp Sci, I-00185 Rome, Italy.
C3 Sapienza University Rome
RP Massaroni, C (corresponding author), Univ Roma La Sapienza, Fac Ingn Informaz Informat & Stat, Dept Comp Sci, I-00185 Rome, Italy.
EM avola@di.uniroma1.it; bernardi@di.uniroma1.it; cinque@di.uniroma1.it;
   gianluca.foresti@uniud.it; massaroni@di.uniroma1.it
OI Bernardi, Marco/0000-0001-5477-1423; Avola, Danilo/0000-0001-9437-6217;
   Massaroni, Cristiano/0000-0002-6942-4851
FU MIUR under grant "Departments of Excellence 2018-2022" of the Department
   of Computer Science of Sapienza University
FX This work was supported in part by the MIUR under grant "Departments of
   Excellence 2018-2022" of the Department of Computer Science of Sapienza
   University.
CR [Anonymous], 2013, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2012.59
   [Anonymous], 2012, P 27 C IMAGE VISION
   [Anonymous], 2012, SUPERVISED SEQUENCE
   Athitsos V., 2008, PROC IEEE WORKSHOP C, P1
   Avola D, 2013, COMPUT METH PROG BIO, V110, P490, DOI 10.1016/j.cmpb.2013.01.009
   Barrientos F.A., 2002, international conference on Collaborative virtual environments, P113
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Bridle J.S, 1990, NEUROCOMPUTING, P227, DOI DOI 10.1007/978-3-642-76153-9_28
   Calinon S., 2007, 2007 2nd Annual Conference on Human-Robot Interaction (HRI), P255
   Chaudhry R, 2013, IEEE COMPUT SOC CONF, P471, DOI 10.1109/CVPRW.2013.153
   Cheng H, 2016, IEEE T CIRC SYST VID, V26, P1659, DOI 10.1109/TCSVT.2015.2469551
   De Smedt Q, 2016, IEEE COMPUT SOC CONF, P1206, DOI 10.1109/CVPRW.2016.153
   Devanne M, 2015, IEEE T CYBERNETICS, V45, P1340, DOI 10.1109/TCYB.2014.2350774
   Doe-Hyung Lee, 2010, Proceedings of the 5th International Conference on Computer Sciences and Convergence Information Technology (ICCIT 2010), P1092, DOI 10.1109/ICCIT.2010.5711226
   Dominio F, 2014, PATTERN RECOGN LETT, V50, P101, DOI 10.1016/j.patrec.2013.10.010
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Eyben F, 2009, 2009 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION & UNDERSTANDING (ASRU 2009), P376, DOI 10.1109/ASRU.2009.5373257
   Gonzalez-Dominguez J., 2014, P INT 2014 SING 1418, P2155, DOI DOI 10.21437/INTERSPEECH.2014-483
   Goza S. M., 2004, Proceedings of the 2004 conference on Human factors in computing systems-CHI'04, P623, DOI 10.1145/985692.985771
   Graves A, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P273, DOI 10.1109/ASRU.2013.6707742
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Guerry J., 2017, 10 EUR WORKSH 3D OBJ, P1
   Guna J, 2014, SENSORS-BASEL, V14, P3702, DOI 10.3390/s140203702
   Hermans M., 2013, Advances in neural information processing systems, V26
   Hong-Min Zhu, 2012, 2012 Ninth International Conference on Computer Graphics, Imaging and Visualization (CGIV), P49, DOI 10.1109/CGIV.2012.13
   Kendon Adam., 2004, Visible Action as utterance
   Kim T, 2013, IEEE I CONF COMP VIS, P1521, DOI 10.1109/ICCV.2013.192
   Li PY, 2015, IEEE I CONF COMP VIS, P819, DOI 10.1109/ICCV.2015.100
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   Lu W, 2016, IEEE SIGNAL PROC LET, V23, P1188, DOI 10.1109/LSP.2016.2590470
   Marin G, 2016, MULTIMED TOOLS APPL, V75, P14991, DOI 10.1007/s11042-015-2451-6
   Naguri CR, 2017, 2017 16TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P1130, DOI 10.1109/ICMLA.2017.00013
   Neverova N, 2016, IEEE T PATTERN ANAL, V38, P1692, DOI 10.1109/TPAMI.2015.2461544
   Ofli F, 2014, J VIS COMMUN IMAGE R, V25, P24, DOI 10.1016/j.jvcir.2013.04.007
   Ohn-Bar E, 2013, IEEE COMPUT SOC CONF, P912, DOI 10.1109/CVPRW.2013.134
   Ohn-Bar E, 2014, IEEE T INTELL TRANSP, V15, P2368, DOI 10.1109/TITS.2014.2337331
   Ohn-Bar E, 2013, IEEE COMPUT SOC CONF, P465, DOI 10.1109/CVPRW.2013.76
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Pierce J. S., 2002, Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2002, P105, DOI 10.1145/503376.503396
   Placidi G, 2013, COMPUT BIOL MED, V43, P1927, DOI 10.1016/j.compbiomed.2013.08.026
   Quek F., 2002, ACM Transactions on Computer-Human Interaction, V9, P171, DOI 10.1145/568513.568514
   Rautaray S. S., 2011, 2011 International Conference on Multimedia, Signal Processing and Communication Technologies (IMPACT 2011), P244, DOI 10.1109/MSPCT.2011.6150485
   Reale MJ, 2011, IEEE T MULTIMEDIA, V13, P474, DOI 10.1109/TMM.2011.2120600
   Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148
   SAVITZKY A, 1964, ANAL CHEM, V36, P1627, DOI 10.1021/ac60214a047
   Schaefer AM, 2008, NEUROCOMPUTING, V71, P2481, DOI 10.1016/j.neucom.2007.12.036
   Schramm R, 2015, IEEE T MULTIMEDIA, V17, P243, DOI 10.1109/TMM.2014.2377553
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Sokolova M, 2009, INFORM PROCESS MANAG, V45, P427, DOI 10.1016/j.ipm.2009.03.002
   Sundermeyer M, 2015, IEEE-ACM T AUDIO SPE, V23, P517, DOI 10.1109/TASLP.2015.2400218
   Suryanarayan Poonam, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3105, DOI 10.1109/ICPR.2010.760
   Truong A, 2016, VISUAL COMPUT, V32, P83, DOI 10.1007/s00371-014-1057-8
   Vikram S., 2013, CHI'13 Extended Abstracts on Human Factors in Computing Systems, P1179
   Wang C, 2015, IEEE T MULTIMEDIA, V17, P29, DOI 10.1109/TMM.2014.2374357
   Wang J, 2012, LECT NOTES COMPUT SC, V7573, P872, DOI 10.1007/978-3-642-33709-3_62
   Wang S.B., 2006, P IEEE COMP SOC C CO
   Weichert F, 2013, SENSORS-BASEL, V13, P6380, DOI 10.3390/s130506380
   Zhang CY, 2015, COMPUT VIS IMAGE UND, V139, P29, DOI 10.1016/j.cviu.2015.05.010
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
NR 59
TC 104
Z9 108
U1 5
U2 62
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2019
VL 21
IS 1
BP 234
EP 245
DI 10.1109/TMM.2018.2856094
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HF5DV
UT WOS:000454253700020
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Soltanian, M
   Ghaemmaghami, S
AF Soltanian, Mohammad
   Ghaemmaghami, Shahrokh
TI Hierarchical Concept Score Postprocessing and Concept-Wise Normalization
   in CNN-Based Video Event Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE WordNet tree; convolutional neural network; Columbia consumer video
   dataset; unstructured social activity attribute dataset; UCF101 dataset;
   ActivityNet dataset; average pooling; max pooling; support vector
   machine; mean average precision
ID LATE FUSION; ANNOTATION; CONTEXT
AB This paper is focused on video event recognition based on frame level convolutional neural network (CNN) descriptors. Using transfer learning, the image trained descriptors are applied to the video domain to make event recognition feasible in scenarios with limited computational resources. After fine-tuning of the existing CNN concept score extractors, pretrained on ImageNet, the output descriptors of the different fully connected layers are employed as frame descriptors. The resulting descriptors are hierarchically postprocessed and combined with novel and efficient pooling and normalization methods. As major contributions of this paper to the video event recognition, we present a postprocessing scheme in which the hierarchy and the relative shortest distance of concepts in WordNet concept tree is taken into account to alleviate uncertainty of the resulting concept scores at the output of the CNN. Besides, we propose a concept-wise power law normalization method that outperforms the widely used power law normalization. The integration of these approaches results in a high performance average (max) pooling-based video event recognition. Compared to the average (max) pooling combined with the state-of-the-art normalization methods and fine-tuned support vector machine classification, the proposed processing scheme improves the event recognition accuracy in terms of mean average precision over the Columbia consumer video and unstructured social activity attribute datasets, where achieves a pretty comparable result on UCF101 and ActivityNet datasets.
C1 [Soltanian, Mohammad] Sharif Univ Technol, Dept Elect Engn, Tehran 1136511155, Iran.
   [Ghaemmaghami, Shahrokh] Sharif Univ Technol, Elect Res Inst, Dept Elect Engn, Tehran 1136511155, Iran.
C3 Sharif University of Technology; Sharif University of Technology
RP Ghaemmaghami, S (corresponding author), Sharif Univ Technol, Elect Res Inst, Dept Elect Engn, Tehran 1136511155, Iran.
EM soltanian_m@ee.sharif.edu; ghaemmagm@sharif.edu
CR Aly Robin., 2013, The axes submissions at trecvid 2013
   [Anonymous], 2010, ABOUT WORDNET
   [Anonymous], 2016, P BRIT MACH VIS C
   [Anonymous], 2010, WordNet | A Lexical Database for English
   [Anonymous], 2015, MATLAB VIDEOUTILS
   [Anonymous], 2013, P 21 ACM INT C MULT, DOI 10.1145/2502081.2502171
   [Anonymous], 2013, ADV NEURAL INFORM PR
   [Anonymous], IMAGENET API DOCUMEN
   [Anonymous], 2017, PRETRAINED CNNS MATC
   [Anonymous], ARXIV170403503
   [Anonymous], P CVPR THUMOS CHALL
   [Anonymous], ARXIV151203740
   Ben Aoun Najib, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1547, DOI 10.1109/ICASSP.2014.6853857
   Boureau Y. L., 2010, P 27 INT C MACH LEAR, P111
   Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963
   BOUREAU YL, 2010, PROC CVPR IEEE, P2559, DOI DOI 10.1109/CVPR.2010.5539963
   Cao LL, 2009, IEEE T MULTIMEDIA, V11, P208, DOI 10.1109/TMM.2008.2009693
   Chandrasekhar V, 2016, SIGNAL PROCESS, V128, P426, DOI 10.1016/j.sigpro.2016.05.021
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chang XJ, 2017, IEEE T PATTERN ANAL, V39, P1617, DOI 10.1109/TPAMI.2016.2608901
   Chang XJ, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2234
   Chen BW, 2009, IEEE T MULTIMEDIA, V11, P295, DOI 10.1109/TMM.2008.2009703
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Chen M.-Y., 2009, Tech. Rep. CMU-CS- 09-161
   Chrisjmccormick, 2017, CONTRIBUTE WORD2VEC
   Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Eggert C, 2014, IEEE IMAGE PROC, P3018, DOI 10.1109/ICIP.2014.7025610
   Fu YW, 2012, LECT NOTES COMPUT SC, V7575, P530, DOI 10.1007/978-3-642-33765-9_38
   Fu YW, 2014, IEEE T PATTERN ANAL, V36, P303, DOI 10.1109/TPAMI.2013.128
   Fu YW, 2010, IEEE T MULTIMEDIA, V12, P717, DOI 10.1109/TMM.2010.2052025
   Gan C, 2016, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2016.106
   Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Geng J, 2015, IEEE T MULTIMEDIA, V17, P498, DOI 10.1109/TMM.2015.2398195
   Goldberg Y, 2014, ARXIV PREPRINT ARXIV
   Han YH, 2014, IEEE T MULTIMEDIA, V16, P1665, DOI 10.1109/TMM.2014.2321530
   Handler Abram., 2014, An empirical study of semantic similarity in WordNet and Word2Vec
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   HEILBRON FC, 2015, PROC CVPR IEEE, P961, DOI DOI 10.1109/CVPR.2015.7298698
   Inoue N, 2012, IEEE T MULTIMEDIA, V14, P1196, DOI 10.1109/TMM.2012.2191395
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jhuo IH, 2014, MACH VISION APPL, V25, P33, DOI 10.1007/s00138-013-0567-0
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiang Y. -G., 2011, P 1 ACM INT C MULT R, P29
   Jiang Y.-G., 2014, THUMOS challenge: Action recognition with a large number of classes
   Jiang YG, 2015, IEEE T MULTIMEDIA, V17, P1174, DOI 10.1109/TMM.2015.2436813
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Klaser A., 2008, P BMVC, P275, DOI DOI 10.5244/C.22.99
   Koniusz P, 2017, IEEE T PATTERN ANAL, V39, P313, DOI 10.1109/TPAMI.2016.2545667
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwak S, 2014, IEEE T PATTERN ANAL, V36, P1174, DOI 10.1109/TPAMI.2013.245
   Lan Z.-Z, 2013, P TRECVID 2013 WORKS, V1
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li XR, 2012, IEEE T MULTIMEDIA, V14, P1091, DOI 10.1109/TMM.2012.2191943
   Liu D, 2013, PROC CVPR IEEE, P803, DOI 10.1109/CVPR.2013.109
   Liu KH, 2008, IEEE T MULTIMEDIA, V10, P240, DOI 10.1109/TMM.2007.911826
   Long X, 2018, PROC CVPR IEEE, P7834, DOI 10.1109/CVPR.2018.00817
   Ma AJ, 2014, INT J COMPUT VISION, V109, P233, DOI 10.1007/s11263-014-0723-7
   Ma Z, 2013, IEEE T MULTIMEDIA, V15, P1628, DOI 10.1109/TMM.2013.2264928
   Mazloom M, 2016, IEEE T MULTIMEDIA, V18, P1378, DOI 10.1109/TMM.2016.2559947
   Mazloom M, 2014, IEEE T MULTIMEDIA, V16, P2214, DOI 10.1109/TMM.2014.2359771
   Mettes P, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P175, DOI 10.1145/2911996.2912036
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Oneata D., 2014, The lear submission at thumos
   Oneata D, 2013, IEEE I CONF COMP VIS, P1817, DOI 10.1109/ICCV.2013.228
   Peng XJ, 2016, LECT NOTES COMPUT SC, V9908, P744, DOI 10.1007/978-3-319-46493-0_45
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Phan S, 2014, IEEE IMAGE PROC, P1026, DOI 10.1109/ICIP.2014.7025204
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Ramanathan V, 2016, PROC CVPR IEEE, P3043, DOI 10.1109/CVPR.2016.332
   Ramanathan V, 2015, IEEE I CONF COMP VIS, P4471, DOI 10.1109/ICCV.2015.508
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Scovanner P., 2007, P 15 ACM INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Shroff N, 2010, IEEE T MULTIMEDIA, V12, P853, DOI 10.1109/TMM.2010.2058795
   Shyu ML, 2008, IEEE T MULTIMEDIA, V10, P252, DOI 10.1109/TMM.2007.911830
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soltanian M, 2017, IRAN CONF ELECTR ENG, P1549, DOI 10.1109/IranianCEE.2017.7985290
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Suha Kwak, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3345, DOI 10.1109/CVPR.2011.5995435
   Sun C, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P371, DOI 10.1145/2733373.2806226
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Trichet R, 2014, INT C PATT RECOG, P1940, DOI 10.1109/ICPR.2014.339
   Tseng VS, 2008, IEEE T MULTIMEDIA, V10, P260, DOI 10.1109/TMM.2007.911832
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   Vapnik V., 1998, STAT LEARNING THEORY, V3
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang L, 2017, SIGNAL PROCESS, V140, P45, DOI 10.1016/j.sigpro.2017.05.005
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XY, 2017, IEEE T PATTERN ANAL, V39, P1770, DOI 10.1109/TPAMI.2016.2616308
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wei P, 2017, IEEE T PATTERN ANAL, V39, P1165, DOI 10.1109/TPAMI.2016.2574712
   Wei X, 2017, IEEE T IMAGE PROCESS, V26, P2929, DOI 10.1109/TIP.2017.2691549
   Weinzaepfel P, 2015, IEEE I CONF COMP VIS, P3164, DOI 10.1109/ICCV.2015.362
   Wu ZX, 2016, PROC CVPR IEEE, P3112, DOI 10.1109/CVPR.2016.339
   Wu ZX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P167, DOI 10.1145/2647868.2654931
   Xu CS, 2008, IEEE T MULTIMEDIA, V10, P421, DOI 10.1109/TMM.2008.917346
   Xu CS, 2008, IEEE T MULTIMEDIA, V10, P1342, DOI 10.1109/TMM.2008.2004912
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Xu ZW, 2013, IEEE I CONF COMP VIS, P3440, DOI 10.1109/ICCV.2013.427
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Ye GN, 2012, PROC CVPR IEEE, P3021, DOI 10.1109/CVPR.2012.6248032
   Yu G, 2015, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2015.7298735
   Yu Shoou-I, 2016, ITE TRANS MEDIA TECH, V4, P227
   Zha Shengxin., 2015, BMVC
   Zhang XS, 2015, IEEE T MULTIMEDIA, V17, P1562, DOI 10.1109/TMM.2015.2449660
   Zhao WL, 2016, IEEE T MULTIMEDIA, V18, P1843, DOI 10.1109/TMM.2016.2585023
   Zhao ZC, 2016, NEUROCOMPUTING, V208, P378, DOI 10.1016/j.neucom.2016.06.002
NR 116
TC 15
Z9 15
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2019
VL 21
IS 1
BP 157
EP 172
DI 10.1109/TMM.2018.2844101
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HF5DV
UT WOS:000454253700014
DA 2024-07-18
ER

PT J
AU Lin, JC
   Wei, WL
   Liu, TL
   Yang, YH
   Wang, HM
   Tyan, HR
   Liao, HYM
AF Lin, Jen-Chun
   Wei, Wen-Li
   Liu, Tyng-Luh
   Yang, Yi-Hsuan
   Wang, Hsin-Min
   Tyan, Hsiao-Rong
   Liao, Hong-Yuan Mark
TI Coherent Deep-Net Fusion To Classify Shots In Concert Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Types of shots; convolutional neural networks; live concert; language of
   film
ID AGREEMENT; CLASSIFICATION
AB Varying types of shots is a fundamental element in the language of film, commonly used by a visual storytelling director. The technique is often used in creating professional recordings of a live concert, but meanwhile may not be appropriately applied in audience recordings of the same event. Such variations could cause the task of classifying shots in concert videos, professional or amateur, very challenging. To achieve more reliable shot classification, we propose a novel probabilistic-based approach, named as coherent classification net (CC-Net), by addressing three crucial issues. First, we focus on learning more effective features by fusing the layer-wise outputs extracted from a deep convolutional neural network (CNN), pretrained on a large-scale data set for object recognition. Second, we introduce a frame-wise classification scheme, the error weighted deep cross-correlation model (EW-Deep-CCM), to boost the classification accuracy. Specifically, the deep neural network-based cross-correlation model (deep-CCM) is constructed to not only model the extracted feature hierarchies of CNN independently, but also relate the statistical dependencies of paired features from different layers. Then, a Bayesian error weighting scheme for a classifier combination is adopted to explore the contributions from individual Deep-CCM classifiers to enhance the accuracy of shot classification in each image frame. Third, we feed the frame-wise classification results to a linear-chain conditional random field module to refine the shot predictions by taking into account the global and temporal regularities. We provide extensive experimental results on a data set of live concert videos to demonstrate the advantage of the proposed CC-Net over existing popular fusion approaches for shot classification.
C1 [Lin, Jen-Chun] Yuan Ze Univ, Dept Elect Engn, Chungli 32003, Taiwan.
   [Wei, Wen-Li; Liu, Tyng-Luh; Wang, Hsin-Min; Liao, Hong-Yuan Mark] Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.
   [Yang, Yi-Hsuan] Acad Sinica, Res Ctr IT Innovat, Taipei 11564, Taiwan.
   [Tyan, Hsiao-Rong] Chung Yuan Christian Univ, Informat & Comp Engn, Taoyuan 32023, Taiwan.
C3 Yuan Ze University; Academia Sinica - Taiwan; Academia Sinica - Taiwan;
   Chung Yuan Christian University
RP Liu, TL (corresponding author), Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.
EM jenchunlin@gmail.com; lilijinjin@gmail.com; liutyng@iis.sinica.edu.tw;
   yang@citi.sinica.edu.tw; whm@iis.sinica.edu.tw; tyan@ice.cycu.edu.tw;
   liao@iis.sinica.edu.tw
RI Wang, Hsin-Min/ABA-8747-2020; Lin, Jen-Chun/AAQ-3701-2021; Liao,
   Hong-Yuan Mark/AAQ-5514-2021; Wei, Wen-Li/AAQ-3848-2021
OI Wang, Hsin-Min/0000-0003-3599-5071; Lin, Jen-Chun/0000-0002-9237-4119;
   Wei, Wen-Li/0000-0002-6753-2824
FU MOST [107-2634-F-001-003, 107-2634-F-001-002]
FX This work was supported in part by MOST Grants 107-2634-F-001-003 and
   107-2634-F-001-002.
CR [Anonymous], SONGWRITERS TOOLKIT
   [Anonymous], 2001, P IEEE INT C MULT EX, DOI DOI 10.1109/ICME.2001.1237822
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2010, P NIPS
   [Anonymous], 2011, DIGITAL OVERDRIVE CO
   [Anonymous], CRFCHAIN MATLAB CODE
   [Anonymous], INTRO STAT RELATIONA
   Artstein R, 2008, COMPUT LINGUIST, V34, P555, DOI 10.1162/coli.07-034-R2
   Bagheri-Khaligh Ali., 2012, 2012 IEEE Southwest Symposium on Image Analysis and Interpretation, P109
   Bayat F, 2013, IRAN CONF MACH, P184, DOI 10.1109/IranianMVIP.2013.6779975
   Benini S, 2010, IEEE INT CON MULTI, P855, DOI 10.1109/ICME.2010.5582611
   Carletta J, 1996, COMPUT LINGUIST, V22, P249
   Cooper HM., 2009, HDB RES SYNTHESIS ME, V2nd edn
   Duan LY, 2005, IEEE T MULTIMEDIA, V7, P1066, DOI 10.1109/TMM.2005.858395
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hui Jiang, 2011, 2011 Proceedings of IEEE International Conference on Computer Science and Automation Engineering (CSAE), P757, DOI 10.1109/CSAE.2011.5952612
   Lafferty John, 2001, INT C MACH LEARN ICM
   LANDIS JR, 1977, BIOMETRICS, V33, P159, DOI 10.2307/2529310
   Lee YH, 2005, LECT NOTES ARTIF INT, V3651, P155
   Lin JC, 2012, IEEE T MULTIMEDIA, V14, P142, DOI 10.1109/TMM.2011.2171334
   Lukashevich H., 2008, Proc. of the 10th International Society of Music Information Retrieval, P375
   Mercado G., 2010, The Filmmaker's Eye: Learning (and Breaking) the Rules of Cinematic Composition
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saini M.K., 2012, Proceedings of the 20th International Conference on Multimedia, P139, DOI DOI 10.1145/2393347.2393373
   Shrestha P., 2010, Proceedings of the international conference on Multimedia (MM '10), P541, DOI DOI 10.1145/1873951.1874023
   Simonyan K., 2014, 14091556 ARXIV
   Thoma A., 2015, The International Journal of New Media, Technology and the Arts, V10, P17
   Wu CH, 2014, APSIPA TRANS SIGNAL, V3, DOI 10.1017/ATSIP.2014.11
   Wu Y, 2015, IEEE T CIRC SYST VID, V25, P1941, DOI 10.1109/TCSVT.2015.2416554
   Xu M, 2011, IEEE IMAGE PROC, DOI 10.1109/ICIP.2011.6116510
   Zhou BL, 2014, ADV NEUR IN, V27
NR 32
TC 9
Z9 9
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 3123
EP 3136
DI 10.1109/TMM.2018.2820904
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800020
DA 2024-07-18
ER

PT J
AU Horiguchi, S
   Amano, S
   Ogawa, M
   Aizawa, K
AF Horiguchi, Shota
   Amano, Sosuke
   Ogawa, Makoto
   Aizawa, Kiyoharu
TI Personalized Classifier for Food Image Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Incremental learning; domain adaptation; one-shot learning;
   personalization; food image classification; deep feature
ID ADAPTATION; CONTEXT
AB Currently, food image recognition tasks are evaluated against fixed datasets. However, in real-world conditions, there are cases in which the number of samples in each class continues to increase and samples from novel classes appear. In particular, dynamic datasets in which each individual user creates samples and continues the updating process often has content that varies considerably between different users, and the number of samples per person is very limited. A single classifier common to all users cannot handle such dynamic data. Bridging the gap between the laboratory environment and the real world has not yet been accomplished on a large scale. Personalizing a classifier incrementally for each user is a promising way to do this. In this paper, we address the personalization problem, which involves adapting to the user's domain incrementally using a very limited number of samples. We propose a simple yet effective personalization framework, which is a combination of the nearest class mean classifier and the 1-nearest neighbor classifier based on deep features. To conduct realistic experiments, we made use of a new dataset of daily food images collected by a food-logging application. Experimental results show that our proposed method significantly outperforms existing methods.
C1 [Horiguchi, Shota; Amano, Sosuke; Aizawa, Kiyoharu] Univ Tokyo, Dept Informat & Commun Engn, Tokyo 1138656, Japan.
   [Ogawa, Makoto] Foo Log Inc, Tokyo 1130033, Japan.
C3 University of Tokyo
RP Horiguchi, S (corresponding author), Univ Tokyo, Dept Informat & Commun Engn, Tokyo 1138656, Japan.
EM horiguchi@hal.t.u-tokyo.ac.jp; s_amano@hal.t.u-tokyo.ac.jp;
   ogawa@foo-log.co.jp; aizawa@hal.t.u-tokyo.ac.jp
RI Horiguchi, Shota/HDN-2832-2022
OI Horiguchi, Shota/0000-0002-3166-4956
FU JST CREST [JPMJCR1686]
FX This work was supported by JST CREST under Grant JPMJCR1686. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Tao Mei.
CR Aizawa K, 2015, IEEE MULTIMEDIA, V22, P4, DOI 10.1109/MMUL.2015.39
   Aizawa K, 2013, IEEE T MULTIMEDIA, V15, P2176, DOI 10.1109/TMM.2013.2271474
   [Anonymous], 2013, P INT C LEARN REPR
   [Anonymous], 2015, ASIAN NUTR C
   [Anonymous], 2015, INT C MACHINE LEARNI
   [Anonymous], 2015, P INT C MACH LEARN D
   [Anonymous], 2017, ARXIV171210151
   [Anonymous], 2014, P 31 INT C INT C MAC
   [Anonymous], ACM INT C MULT
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P BRIT MACH VIS C
   Bart E, 2005, PROC CVPR IEEE, P672
   Bartunov S., 2016, ARXIV160506065
   Beijbom O, 2015, IEEE WINT CONF APPL, P844, DOI 10.1109/WACV.2015.117
   Bendale A, 2015, PROC CVPR IEEE, P1893, DOI 10.1109/CVPR.2015.7298799
   Bettadapura V, 2015, IEEE WINT CONF APPL, P580, DOI 10.1109/WACV.2015.83
   Bitarafan A, 2016, IEEE T KNOWL DATA EN, V28, P2128, DOI 10.1109/TKDE.2016.2551241
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Cao LL, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P363, DOI 10.1145/2911996.2912069
   Cauwenberghs G, 2001, ADV NEUR IN, V13, P409
   Chen JJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1771, DOI 10.1145/3123266.3123428
   Chen JJ, 2017, LECT NOTES COMPUT SC, V10132, P588, DOI 10.1007/978-3-319-51811-4_48
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Chen M, 2009, IEEE IMAGE PROC, P289, DOI 10.1109/ICIP.2009.5413511
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   De Rosa R, 2015, IEEE DATA MINING, P733, DOI 10.1109/ICDM.2015.43
   Denton E, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1731, DOI 10.1145/2783258.2788576
   Herranz L, 2017, IEEE T MULTIMEDIA, V19, P430, DOI 10.1109/TMM.2016.2614861
   Hoffman J, 2014, PROC CVPR IEEE, P867, DOI 10.1109/CVPR.2014.116
   Jain V, 2011, PROC CVPR IEEE, P577, DOI 10.1109/CVPR.2011.5995317
   Kan MN, 2014, INT J COMPUT VISION, V109, P94, DOI 10.1007/s11263-013-0693-1
   Kitamura K, 2010, IEEE INT CON MULTI, P625, DOI 10.1109/ICME.2010.5583021
   Kwitt R, 2016, PROC CVPR IEEE, P78, DOI 10.1109/CVPR.2016.16
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li Xirong., 2011, Proceedings of the 19th ACM International Conference on Multimedia, MM '11, P233
   Matsuda Y., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P25, DOI 10.1109/ICME.2012.157
   Mensink T, 2012, LECT NOTES COMPUT SC, V7573, P488, DOI 10.1007/978-3-642-33709-3_35
   Min WQ, 2017, IEEE T MULTIMEDIA, V19, P1100, DOI 10.1109/TMM.2016.2639382
   Myers A, 2015, IEEE I CONF COMP VIS, P1233, DOI 10.1109/ICCV.2015.146
   Pronobis A, 2010, IMAGE VISION COMPUT, V28, P1080, DOI 10.1016/j.imavis.2010.01.015
   Qian ZM, 2015, SIGNAL PROCESS-IMAGE, V34, P61, DOI 10.1016/j.image.2015.03.008
   Ristin M, 2016, IEEE T PATTERN ANAL, V38, P490, DOI 10.1109/TPAMI.2015.2459678
   Royer A, 2015, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2015.7298746
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salvador A, 2017, PROC CVPR IEEE, P3068, DOI 10.1109/CVPR.2017.327
   Scheirer WJ, 2013, IEEE T PATTERN ANAL, V35, P1757, DOI 10.1109/TPAMI.2012.256
   Sharma P, 2012, PROC CVPR IEEE, P3298, DOI 10.1109/CVPR.2012.6248067
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Vázquez D, 2014, IEEE T PATTERN ANAL, V36, P797, DOI 10.1109/TPAMI.2013.163
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wilber MJ, 2015, IEEE I CONF COMP VIS, P981, DOI 10.1109/ICCV.2015.118
   Wong A, 2015, IEEE I CONF COMP VIS, P1197, DOI 10.1109/ICCV.2015.142
   Xu RH, 2015, IEEE T MULTIMEDIA, V17, P1187, DOI 10.1109/TMM.2015.2438717
   Yao T, 2015, PROC CVPR IEEE, P2142, DOI 10.1109/CVPR.2015.7298826
   Yeh T., 2008, PROC IEEE C COMPUT V, P1
   Zhu FQ, 2010, IEEE J-STSP, V4, P756, DOI 10.1109/JSTSP.2010.2051471
NR 57
TC 45
Z9 50
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2018
VL 20
IS 10
SI SI
BP 2836
EP 2848
DI 10.1109/TMM.2018.2814339
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GT9YH
UT WOS:000444903000024
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Huang, XH
   Dhall, A
   Goecke, R
   Pietikäinen, M
   Zhao, GY
AF Huang, Xiaohua
   Dhall, Abhinav
   Goecke, Roland
   Pietikainen, Matti
   Zhao, Guoying
TI Multimodal Framework for Analyzing the Affect of a Group of People
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Facial expression recognition; Group-level emotion recognition; Feature
   descriptor; Information aggregation; Multi-modality
ID TEXTURE CLASSIFICATION; REPRESENTATION; EMOTIONS; MODEL
AB With the advances in multimedia and the world wide web, users upload millions of images and videos everyone on social networking platforms on the Internet. From the perspective of automatic human behavior understanding, it is of interest to analyze and model the affects that are exhibited by groups of people who are participating in social events in these images. However, the analysis of the affect that is expressed by multiple people is challenging due to the varied indoor and outdoor settings. Recently, a few interesting works have investigated face-based group-level emotion recognition (GER). In this paper, we propose a multimodal framework for enhancing the affective analysis ability of GER in challenging environments. Specifically, for encoding a person's information in a group-level image, we first propose an information aggregation method for generating feature descriptions of face, upper body, and scene. Later, we revisit localized multiple kernel learning for fusing face, upper body, and scene information for GER against challenging environments. Intensive experiments are performed on two challenging group-level emotion databases (HAPPEI and GAFF) to investigate the roles of the face, upper body, scene information, and the multimodal framework. Experimental results demonstrate that the multimodal framework achieves promising performance for GER.
C1 [Huang, Xiaohua; Pietikainen, Matti; Zhao, Guoying] Univ Oulu, Ctr Machine Vis & Signal Anal, Oulu 90014, Finland.
   [Dhall, Abhinav] Indian Inst Technol Ropar, Dept Comp Sci & Engn, Rupnagar 140001, India.
   [Goecke, Roland] Univ Canberra, Human Ctr Technol Res Ctr, Bruce, ACT 2617, Australia.
   [Zhao, Guoying] Northwest Univ, Sch Informat & Technol, Xian 710069, Shaanxi, Peoples R China.
C3 University of Oulu; Indian Institute of Technology System (IIT System);
   Indian Institute of Technology (IIT) - Ropar; University of Canberra;
   Northwest University Xi'an
RP Huang, XH (corresponding author), Univ Oulu, Ctr Machine Vis & Signal Anal, Oulu 90014, Finland.
EM xiaohua.huang@oulu.fi; abhinav@iitrpr.ac.in; roland.goecke@ieee.org;
   matti.pietikainen@oulu.fi; guoying.zhao@oulu.fi
RI Goecke, Roland/F-7499-2013; ARSLAN, Okan/AAA-3232-2020; Dhall,
   Abhinav/AAF-4347-2019; Huang, Xiaohua/A-4878-2011; Zhao,
   Guoying/ABE-7716-2020
OI Goecke, Roland/0000-0003-2279-7041; Dhall, Abhinav/0000-0002-2230-1440;
   Zhao, Guoying/0000-0003-3694-206X; Pietikainen,
   Matti/0000-0003-2263-6731; Huang, Xiaohua/0000-0001-8897-3517
FU Jorma Ollila Grant of Nokia Foundation; Central Fund of Finnish Cultural
   Foundation; AI Grant of Kaute Foundation; Academy of Finland; Tekes
   Fidipro Program [1849/31/2015]; Tekes project [3116/31/2017]; Infotech;
   National Natural Science Foundation of China [61772419]
FX This work was supported in part by the Jorma Ollila Grant of Nokia
   Foundation; in part by the Central Fund of Finnish Cultural Foundation;
   in part by the AI Grant of Kaute Foundation, in part by the Academy of
   Finland; in part by the Tekes Fidipro Program under Grant 1849/31/2015
   and Tekes project (Grant No. 3116/31/2017); in part by the Infotech; and
   in part by the National Natural Science Foundation of China under Grant
   61772419. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Hatice Gunes.
CR [Anonymous], 2015, 2015 IEEECIC INT C C
   [Anonymous], P BMVC
   [Anonymous], P AS C COMP VIS
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], ARXIV13045634
   [Anonymous], P INT C COMP VIS
   [Anonymous], P 11 IEEE INT C WORK
   [Anonymous], P INT C MACH LEARN
   Barsade SG, 1998, RES MANAG GRP TEAM, V1, P81
   Bosch A., 2007, P 6 ACM INT C IMAGE, P401, DOI DOI 10.1145/1282280.1282340
   Cerekovic A, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P437, DOI 10.1145/2993148.2997628
   Chiu LC, 2013, IEEE T IMAGE PROCESS, V22, P3158, DOI 10.1109/TIP.2013.2259841
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dhall Abhinav, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163151
   Dhall A., 2012, ASIAN C COMPUTER VIS, P613
   Dhall A, 2015, INT CONF AFFECT, P255, DOI 10.1109/ACII.2015.7344580
   Dhall A, 2015, IEEE T AFFECT COMPUT, V6, P13, DOI 10.1109/TAFFC.2015.2397456
   Dhall A, 2010, LECT NOTES COMPUT SC, V6444, P485, DOI 10.1007/978-3-642-17534-3_60
   Eichner Marcin., 2009, BMVC
   Felsberg M, 2001, IEEE T SIGNAL PROCES, V49, P3136, DOI 10.1109/78.969520
   Fischer S, 2007, INT J COMPUT VISION, V75, P231, DOI 10.1007/s11263-006-0026-8
   Gonen M., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1425, DOI 10.1109/ICPR.2010.352
   Han YN, 2014, IEEE T CYBERNETICS, V44, P137, DOI 10.1109/TCYB.2013.2248710
   Hoai M, 2014, PROC CVPR IEEE, P875, DOI 10.1109/CVPR.2014.117
   Joshi J, 2013, INT CONF AFFECT, P492, DOI 10.1109/ACII.2013.87
   Kelly JR, 2001, ORGAN BEHAV HUM DEC, V86, P99, DOI 10.1006/obhd.2001.2974
   Kret ME, 2013, FRONT HUM NEUROSCI, V7, DOI 10.3389/fnhum.2013.00810
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Li JS, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P487, DOI 10.1145/2993148.2997636
   Li ZQ, 2015, PROC CVPR IEEE, P1356, DOI 10.1109/CVPR.2015.7298741
   Lin JC, 2012, IEEE T MULTIMEDIA, V14, P142, DOI 10.1109/TMM.2011.2171334
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Luo Y, 2016, IEEE T IMAGE PROCESS, V25, P414, DOI 10.1109/TIP.2015.2495116
   Mou W., 2015, PROC IEEE INT C WORK, P1
   Mou WX, 2016, IEEE COMPUT SOC CONF, P1478, DOI 10.1109/CVPRW.2016.185
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Ojansivu V, 2008, LECT NOTES COMPUT SC, V5099, P236, DOI 10.1007/978-3-540-69905-7_27
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Righart R, 2008, SOC COGN AFFECT NEUR, V3, P270, DOI 10.1093/scan/nsn021
   Righart R, 2006, CEREB CORTEX, V16, P1249, DOI 10.1093/cercor/bhj066
   Schindler K, 2008, NEURAL NETWORKS, V21, P1238, DOI 10.1016/j.neunet.2008.05.003
   Sikka K, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P517, DOI 10.1145/2522848.2531741
   Simonyan K., 2014, 14091556 ARXIV
   Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8
   Sun B, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P451, DOI 10.1145/2993148.2997640
   Surace L., 2017, P 19 ACM INT C MULTI, P593, DOI 10.1145/3136755.3143015
   Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/iccv.2009.5459207
   Weinrich C, 2012, IEEE INT C INT ROBOT, P2147, DOI 10.1109/IROS.2012.6386122
   Wu CH, 2013, IEEE T MULTIMEDIA, V15, P1880, DOI 10.1109/TMM.2013.2269314
   Wu F, 2016, PATTERN RECOGN, V60, P630, DOI 10.1016/j.patcog.2016.06.010
   Wu F, 2016, PATTERN RECOGN, V50, P143, DOI 10.1016/j.patcog.2015.08.012
   Wu JX, 2011, IEEE T PATTERN ANAL, V33, P1489, DOI 10.1109/TPAMI.2010.224
   Zeng ZH, 2007, IEEE T MULTIMEDIA, V9, P424, DOI 10.1109/TMM.2006.886310
   ZETZSCHE C, 1990, VISION RES, V30, P1111, DOI 10.1016/0042-6989(90)90120-A
   Zhang L, 2012, IMAGE VISION COMPUT, V30, P1043, DOI 10.1016/j.imavis.2012.09.003
   Zhang L, 2010, IEEE IMAGE PROC, P2677, DOI 10.1109/ICIP.2010.5651885
   Zhang WC, 2005, IEEE I CONF COMP VIS, P786
   Zhao GY, 2009, IEEE T MULTIMEDIA, V11, P1254, DOI 10.1109/TMM.2009.2030637
NR 59
TC 16
Z9 16
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2018
VL 20
IS 10
SI SI
BP 2706
EP 2721
DI 10.1109/TMM.2018.2818015
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GT9YH
UT WOS:000444903000014
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Poblete, B
   Guzmán, J
   Maldonado, J
   Tobar, F
AF Poblete, Barbara
   Guzman, Jheser
   Maldonado, Jazmine
   Tobar, Felipe
TI Robust Detection of Extreme Events Using Twitter: Worldwide Earthquake
   Monitoring
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
ID BURSTY
AB Timely detection and accurate description of extreme events, such as natural disasters and other crisis situations, are crucial for emergency management and mitigation. Extreme-event detection is challenging, since one has to rely upon reports from human observers appointed to specific geographical areas, or on an expensive and sophisticated infrastructure. In the case of earthquakes, geographically dense sensor networks are expensive to deploy and maintain. Therefore, only some regions-or even countries-are able to acquire useful information about the effects of earthquakes in their own territory. An inexpensive and viable alternative to this problem is to detect extreme real-world events through people's reactions in online social networks. In particular, Twitter has gained popularity within the scientific community for providing access to real-time "citizen sensor" activity. Nevertheless, the massive amount of messages in the Twitter stream, along with the noise it contains, underpin a number of difficulties when it comes to Twitter-based event detection. We contribute to address these challenges by proposing an online method for detecting unusual bursts in discrete-time signals extracted from Twitter. This method only requires a one-off semisupervised initialization and can he scaled to track multiple signals in a robust manner. We also show empirically how our proposed approach, which was envisioned for generic event detection, can he adapted for worldwide earthquake detection, where we compare the proposed model to the state of the art for earthquake tracking using social media. Experimental results validate our approach as a competitive alternative in terms of precision and recall to leading solutions, with the advantage of implementation simplicity and worldwide scalability.
C1 [Poblete, Barbara] Millennium Inst Foundat Res Data, Santiago, Chile.
   [Poblete, Barbara; Guzman, Jheser; Maldonado, Jazmine] Univ Chile, Dept Comp Sci, Santiago 8370456, Chile.
   [Tobar, Felipe] Univ Chile, Ctr Math Modeling, Santiago 8370456, Chile.
C3 Universidad de Chile; Universidad de Chile
RP Poblete, B (corresponding author), Millennium Inst Foundat Res Data, Santiago, Chile.
EM bpoblete@dcc.uchile.cl; dicotips@gmail.com; jazminemf@gmail.com;
   ftobar@dim.uchile.cl
RI Poblete, Barbara/H-8450-2013
OI Poblete, Barbara/0000-0002-7669-645X; Guzman,
   Jheser/0000-0001-9572-2556; Tobar, Felipe/0000-0003-2486-3583
FU Millennium Institute for Foundational Research on Data; Conicyt-PIA
   [AFB170001]; Fondecyt [11171165]
FX The work of B. Poblete was supported in part by the Millennium Institute
   for Foundational Research on Data. The work of F. Tobar was supported in
   part by Conicyt-PIA #AFB170001 (CMM) and in part by Fondecyt #11171165.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Xuan Song. (Corresponding author:
   Barbara Poblete.)
CR [Anonymous], 2017, Journal of Information Science
   [Anonymous], 2017, P C HUM COMP CROWDS
   Atefeh F, 2015, COMPUT INTELL-US, V31, P132, DOI 10.1111/coin.12017
   Atkinson GM, 2007, SEISMOL RES LETT, V78, P362, DOI 10.1785/gssrl.78.3.362
   Avvenuti M, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1749, DOI 10.1145/2623330.2623358
   Avvenuti M, 2014, INT CONF PERVAS COMP, P587, DOI 10.1109/PerComW.2014.6815272
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Dou W., 2012, P IEEE VISWEEK WORKS, P971
   Earle P, 2010, SEISMOL RES LETT, V81, P246, DOI 10.1785/gssrl.81.2.246
   Earle PS, 2011, ANN GEOPHYS-ITALY, V54, P708, DOI 10.4401/ag-5364
   Fechner G. T., 1948, ELEMENTS PSYCHOPHYSI
   Hutwagner L, 2003, J URBAN HEALTH, V80, pI89
   Kagan YY, 2003, PHYS EARTH PLANET IN, V135, P173, DOI 10.1016/S0031-9201(02)00214-5
   Kleinberg J, 2003, DATA MIN KNOWL DISC, V7, P373, DOI 10.1023/A:1024940629314
   Kwak Haewoon, 2010, WWW 10 P 19 INT C WO, P591, DOI DOI 10.1145/1772690.1772751
   Mathioudakis M., 2010, P 2010 ACM SIGMOD IN, P1155
   Phan X., 2007, GIBBSLDA PLUS PLUS C
   Phuvipadawat S., 2010, Proceedings of the 2010 IEEE/ACM International Conference on Web Intelligence-Intelligent Agent Technology - Workshops (WI-IAT 2010), P120, DOI 10.1109/WI-IAT.2010.205
   Ramage D., 2010, P INT AAAI C WEB SOC, V4, P130
   Rios G., P IEEE INT JOINT C N
   Robinson B, 2013, PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'13 COMPANION), P999
   Sakaki T., 2010, P 19 INT C WORLD WID, P851
   Sakaki T, 2013, IEEE T KNOWL DATA EN, V25, P919, DOI 10.1109/TKDE.2012.29
   Sobkowicz P, 2013, EPJ DATA SCI, V2, DOI 10.1140/epjds14
   Stein S., 2009, An introduction to seismology, earthquakes, and earth structure
   USGS, 2017, DAT PROD
   USGS, 2017, MOD MERC INT SCAL
   WELFORD BP, 1962, TECHNOMETRICS, V4, P419, DOI 10.2307/1266577
   Weng J., 2021, Proc. Int. AAAI Conf. Web Soc. Media, V5, P401, DOI [10.1609/icwsm.v5i1.14102, DOI 10.1609/ICWSM.V5I1.14102]
   Xie W, 2016, IEEE T KNOWL DATA EN, V28, P2216, DOI 10.1109/TKDE.2016.2556661
   Yin J, 2012, IEEE INTELL SYST, V27, P52, DOI 10.1109/MIS.2012.6
   Young J. C., 2013, TRANSFORMING EARTHQU, P64
NR 32
TC 43
Z9 48
U1 4
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2018
VL 20
IS 10
SI SI
BP 2551
EP 2561
DI 10.1109/TMM.2018.2855107
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GT9YH
UT WOS:000444903000002
DA 2024-07-18
ER

PT J
AU Shao, ZF
   Wu, WJ
   Wang, ZY
   Du, W
   Li, CY
AF Shao, Zhenfeng
   Wu, Wenjing
   Wang, Zhongyuan
   Du, Wan
   Li, Chengyuan
TI SeaShips: A Large-Scale Precisely Annotated Dataset for Ship Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object detection; ship dataset; neural networks; ship detection
ID CONVOLUTIONAL NETWORKS; SHAPE
AB In this paper, we introduce a new large-scale dataset of ships, called SeaShips, which is designed for training and evaluating ship object detection algorithms. The dataset currently consists of 31 455 images and covers six common ship types (ore carrier, bulk cargo carrier, general cargo ship, container ship, fishing boat, and passenger ship). All of the images are from about 10 080 real-world video segments, which are acquired by the monitoring cameras in a deployed coastline video surveillance system. They are carefully selected to mostly cover all possible imaging variations, for example, different scales, hull parts, illumination, viewpoints, backgrounds, and occlusions. All images are annotated with ship-type labels and high-precision bounding boxes. Based on the SeaShips dataset, we present the performance of three detectors as a baseline to do the following: 1) elementarily summarize the difficulties of the dataset for ship detection; 2) show detection results for researchers using the dataset; and 3) make a comparison to identify the strengths and weaknesses of the baseline algorithms. In practice, the SeaShips dataset would hopefully advance research and applications on ship detection.
C1 [Shao, Zhenfeng; Wu, Wenjing; Li, Chengyuan] Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & R, Wuhan 430079, Peoples R China.
   [Wang, Zhongyuan] Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Wuhan 430072, Hubei, Peoples R China.
   [Du, Wan] Univ Calif Merced, Comp Sci & Engn, Merced, CA 95340 USA.
C3 Wuhan University; Wuhan University; University of California System;
   University of California Merced
RP Wu, WJ (corresponding author), Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & R, Wuhan 430079, Peoples R China.
EM shaozhenfeng@whu.edu.cn; wuwenjing94@163.com; wzy_hope@163.com;
   wdu3@ucm.edu; lichengyuan@whu.edu.cn
RI Li, Chengyuan/AAO-3511-2020; wang, David/KFR-2555-2024; Du,
   Wan/M-4575-2016; Wang, Zhongyuan/ABD-2189-2020
OI Li, Chengyuan/0000-0002-4422-6584; Wang, Zhongyuan/0000-0002-9796-488X
FU National High-Resolution Earth Observation System Major Projects of
   China [02-Y30B19-9001-15/17]; National Natural Science Foundation of
   China [61671332, 41771452, 41771454]; Guangzhou Science and Technology
   Project [201604020070]; Key Research and Development Program of Hubei
   Province of China [2016AAA018]
FX This work was supported in part by the National High-Resolution Earth
   Observation System Major Projects of China under Grant
   02-Y30B19-9001-15/17, in part by the National Natural Science Foundation
   of China under Grants 61671332, 41771452, and 41771454, in part by the
   Guangzhou Science and Technology Project under Grant 201604020070, and
   in part by the Key Research and Development Program of Hubei Province of
   China under Grant 2016AAA018. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Xuan Song.
CR [Anonymous], 2017, arXiv
   Azadi S, 2017, PROC CVPR IEEE, P7369, DOI 10.1109/CVPR.2017.779
   Cao JL, 2016, IEEE T IMAGE PROCESS, V25, P5538, DOI 10.1109/TIP.2016.2609807
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Cutter G, 2015, 2015 IEEE WINTER APPLICATIONS AND COMPUTER VISION WORKSHOPS (WACVW), P57, DOI 10.1109/WACVW.2015.11
   Dai Jifeng, 2016, Advances in Neural Information Processing Systems, DOI DOI 10.1016/J.JPOWSOUR.2007.02.075
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gao W, 2008, IEEE T SYST MAN CY A, V38, P149, DOI 10.1109/TSMCA.2007.909557
   Geiger A., 2012, CVPR
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   Greenberg S, 2005, OPT ENG, V44, DOI 10.1117/1.1951547
   Griffin G., 2007, CALTECH 256 OBJECT C
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Huang GQ, 2009, INT J COMPUT INTEG M, V22, P579, DOI 10.1080/09511920701724934
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jain V., 2010, Fddb: A benchmark for face detection in unconstrained settings
   Kang K, 2016, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2016.95
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Längkvist M, 2014, PATTERN RECOGN LETT, V42, P11, DOI 10.1016/j.patrec.2014.01.008
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu G, 2014, IEEE GEOSCI REMOTE S, V11, P617, DOI 10.1109/LGRS.2013.2272492
   Liu W, 2015, ADV SOC SCI EDUC HUM, V12, P21
   Ouyang WL, 2016, PROC CVPR IEEE, P864, DOI 10.1109/CVPR.2016.100
   Qi SX, 2015, IEEE GEOSCI REMOTE S, V12, P1451, DOI 10.1109/LGRS.2015.2408355
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren MW, 1999, P SOC PHOTO-OPT INS, V3720, P467, DOI 10.1117/12.357188
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smith Raymond, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P411, DOI 10.1007/978-3-319-46604-0_30
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Tao WB, 2007, OPT ENG, V46, DOI 10.1117/1.2823159
   Tello M, 2005, IEEE GEOSCI REMOTE S, V2, P201, DOI 10.1109/LGRS.2005.845033
   Wang XL, 2017, PROC CVPR IEEE, P3039, DOI 10.1109/CVPR.2017.324
   Welinder P., 2010, Technical Report CNS-TR-2010-001
   Wu JL, 2013, ADV INTELL SYST, V181, P1
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   [邢相薇 Xing Xiangwei], 2015, [雷达学报, Journal of Radars], V4, P107
   Xu J, 2014, IEEE GEOSCI REMOTE S, V11, P2070, DOI 10.1109/LGRS.2014.2319082
   Yang G, 2014, IEEE GEOSCI REMOTE S, V11, P641, DOI 10.1109/LGRS.2013.2273552
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang RQ, 2016, INT ARCH PHOTOGRAMM, V41, P423, DOI 10.5194/isprsarchives-XLI-B7-423-2016
   Zhu CR, 2010, IEEE T GEOSCI REMOTE, V48, P3446, DOI 10.1109/TGRS.2010.2046330
NR 49
TC 168
Z9 196
U1 15
U2 123
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2018
VL 20
IS 10
SI SI
BP 2593
EP 2604
DI 10.1109/TMM.2018.2865686
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GT9YH
UT WOS:000444903000005
OA Bronze
DA 2024-07-18
ER

PT J
AU Dong, JF
   Li, XR
   Xu, DQ
AF Dong, Jianfeng
   Li, Xirong
   Xu, Duanqing
TI Cross-Media Similarity Evaluation for Web Image Retrieval in the Wild
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Web image retrieval; real-user query; cross-media similarity computation
ID SEARCH; MODELS
AB In order to retrieve unlabeled images by textual queries, cross-media similarity computation is a key ingredient. Although novel methods are continuously introduced, little has been done to evaluate these methods together with large-scale query log analysis. Consequently, how far have these methods brought us in answering real-user queries is unclear. Liven baseline methods that use relatively simple text/image matching, how much progress have advanced models made is also unclear. This paper takes a pragmatic approach to answering the two questions. Queries are automatically categorized according to the proposed query visualness measure and later connected to the evaluation of multiple cross-media similarity models on three test sets. Such a connection reveals that the success of the state of the art is mainly attributed to their good performance on visual-oriented queries, which account for only a small part of real-user queries. h quantify the current progress, we propose a simple text2image method, representing a novel query by a set of images selected from large-scale query log. Consequently, computing cross-media similarity between the query and a given image boils down to comparing the visual similarity between the given image and the selected images. Image retrieval experiments on the challenging Clickture dataset show that the proposed text2image is a strong baseline, comparing favorably to recent deep learning alternatives.
C1 [Dong, Jianfeng; Xu, Duanqing] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
   [Li, Xirong] Renmin Univ China, Key Lab Data Engn & Knowledge Engn, Sch Informat, Beijing 100872, Peoples R China.
C3 Zhejiang University; Renmin University of China
RP Li, XR (corresponding author), Renmin Univ China, Key Lab Data Engn & Knowledge Engn, Sch Informat, Beijing 100872, Peoples R China.
EM danieljf24@zju.edu.cn; xirong.li@gmail.com; xdq@zju.edu.cn
RI Li, Xirong/AAD-3347-2019
OI Li, Xirong/0000-0002-0220-8310
FU National Science Foundation of China [61672523]; Key Scientific Research
   Base for Digital Conservation of Cave Temples (Zhejiang University),
   State Administration for Cultural Heritage
FX This work was supported by the National Science Foundation of China
   under Grant 61672523 and by the Key Scientific Research Base for Digital
   Conservation of Cave Temples (Zhejiang University), State Administration
   for Cultural Heritage.
CR [Anonymous], 2016, ACM COMPUT SURVEYS, V49
   [Anonymous], 2010, PROC ACM SIGMM INT C
   [Anonymous], 2010, ACM INT C MULTIMEDIA
   [Anonymous], 2013, 1 INT C LEARN REPR I
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2013, P 21 ACM INT C MULTI
   [Anonymous], LECT NOTES COMPUTER
   [Anonymous], 2013, P ACM INT C MULT
   [Anonymous], 2007, P CIKM
   Bai B., 2009, NIPS, P64
   Bai YL, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P229, DOI 10.1145/2647868.2656402
   Bird Steven, 2009, NATURAL LANGUAGE PRO, DOI DOI 10.1007/S10579-010-9124-X
   Cappallo S, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1311, DOI 10.1145/2733373.2806335
   Cheng ZY, 2016, MULTIMEDIA SYST, V22, P509, DOI 10.1007/s00530-014-0432-7
   Chua T. -S., 2009, P INT C IM VID RETR, P527
   Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142
   Cui CR, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P895, DOI 10.1145/2733373.2806358
   Cui CR, 2015, J ASSOC INF SCI TECH, V66, P82, DOI 10.1002/asi.23163
   Deng C, 2016, IEEE T MULTIMEDIA, V18, P208, DOI 10.1109/TMM.2015.2508146
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deselaers T, 2008, INFORM RETRIEVAL, V11, P77, DOI 10.1007/s10791-007-9039-3
   Ding XM, 2016, IEEE T MULTIMEDIA, V18, P1616, DOI 10.1109/TMM.2016.2572000
   Dong JF, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P173, DOI 10.1145/2733373.2807419
   Enser P, 2008, J INF SCI, V34, P531, DOI 10.1177/0165551508091013
   Fang Q., 2013, P MSR BING IM RETR C
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Geoffrey EHinton., 2012, Improving neural networks by preventing co-adaptation of feature detectors
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Goodrum A, 2001, INFORM PROCESS MANAG, V37, P295, DOI 10.1016/S0306-4573(00)00033-9
   Grangier D, 2008, IEEE T PATTERN ANAL, V30, P1371, DOI 10.1109/TPAMI.2007.70791
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Hollink L, 2004, INT J HUM-COMPUT ST, V61, P601, DOI 10.1016/j.ijhcs.2004.03.002
   Hua X.-S., 2013, Proceedings of the 21st ACM International Conference on Multimedia, Oct. 21-25, ACM, P243
   Hua Y, 2016, IEEE T MULTIMEDIA, V18, P1201, DOI 10.1109/TMM.2016.2535864
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Kofler C, 2014, IEEE T MULTIMEDIA, V16, P1421, DOI 10.1109/TMM.2014.2315777
   Li XR, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P879, DOI 10.1145/2766462.2767773
   Li XR, 2009, IEEE T MULTIMEDIA, V11, P1310, DOI 10.1109/TMM.2009.2030598
   Liu YM, 2011, IEEE T PATTERN ANAL, V33, P1022, DOI 10.1109/TPAMI.2010.142
   Liu YQ, 2006, LECT NOTES COMPUT SC, V4182, P593
   Liu Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P955, DOI 10.1145/2733373.2806373
   Lu D, 2016, IEEE T MULTIMEDIA, V18, P1628, DOI 10.1109/TMM.2016.2568099
   Lu YJ, 2010, IEEE T MULTIMEDIA, V12, P288, DOI 10.1109/TMM.2010.2046292
   Metzler D, 2007, INFORM RETRIEVAL, V10, P257, DOI 10.1007/s10791-006-9019-z
   Pan YW, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P233, DOI 10.1145/2647868.2656404
   Pan YW, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P717, DOI 10.1145/2600428.2609568
   Pu HT, 2008, J INF SCI, V34, P275, DOI 10.1177/0165551507084140
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen J., 2006, P ACM INT C MULT, P569
   Shirahatti NV, 2005, PROC CVPR IEEE, P955
   Smith G, 2012, J AM SOC INF SCI TEC, V63, P2451, DOI 10.1002/asi.22742
   Sun AX, 2011, J AM SOC INF SCI TEC, V62, P2364, DOI 10.1002/asi.21659
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wang L., 2013, P MSR BING IM RETR C
   Wang M, 2010, IEEE T MULTIMEDIA, V12, P829, DOI 10.1109/TMM.2010.2055045
   Wu F, 2016, IEEE T IMAGE PROCESS, V25, P630, DOI 10.1109/TIP.2015.2507401
   Wu Y, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1986
   Xu Zhinan., 2014, 2014 IEEE 6th International Symposium on Wireless Vehicular Communications (WiVeC 2014), P1
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Yao T., 2015, P IEEE INT C COMP VI, P955
   Yu W, 2015, IEEE T MULTIMEDIA, V17, P2000, DOI 10.1109/TMM.2015.2480340
   Zhuang YT, 2008, IEEE T MULTIMEDIA, V10, P221, DOI 10.1109/TMM.2007.911822
NR 68
TC 16
Z9 16
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2018
VL 20
IS 9
BP 2371
EP 2384
DI 10.1109/TMM.2018.2796248
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GR1ZX
UT WOS:000442358200011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, Y
   Liu, J
   Cao, B
   Wang, CG
AF Li, Yun
   Liu, Jie
   Cao, Bin
   Wang, Chonggang
TI Joint Optimization of Radio and Virtual Machine Resources With Uncertain
   User Demands in Mobile Cloud Computing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mobile cloud computing; resource reservation; resource allocation;
   robust optimization
ID MANAGEMENT
AB The resource reservation is one of the key techniques to ensure the quality of service (QoS) of a multimedia application. In mobile cloud computing (MCC), the resource reservation and allocation (RRA) in advance can significantly reduce the total provisioning cost of cloud service providers. However, the uncertain features of mobile users' demands for resources make RRA challengeable. In MCC, the QoS of a mobile application, such as voice IP or video, is determined by both of the radio resource (RR) and the cloud virtual machine resource (VMR) allocated to the mobile application, so we should jointly allocate these two types of resources. In this paper, RRA with uncertain demands of mobile users is formulated as a robust optimization model. Logarithmic utility functions are defined to capture the mobile users' satisfaction, which show how to match the allocations between RRs and VMRs according to the resource demands of the mobile applications. Then, a robust joint resource reservation and allocation algorithm in MCC (JRRA-MCC) is proposed to realize the optimal provisioning of RRs and VMRs. Simulation results show that the proposed JRRA-MCC can minimize the total resource provisioning cost of cloud service providers and enhance the resource utilization efficiently.
C1 [Li, Yun; Liu, Jie; Cao, Bin] Chongqing Univ Post & Telecommun, Chongqing Key Lab Mobile Commun Technol, Chongqing 400065, Peoples R China.
   [Li, Yun] Southeast Univ, Natl Mobile Commun Res Lab, Nanjing 210018, Peoples R China.
   [Wang, Chonggang] InterDigital Commun, King Of Prussia, PA 19406 USA.
C3 Chongqing University of Posts & Telecommunications; Southeast University
   - China; InterDigital
RP Cao, B (corresponding author), Chongqing Univ Post & Telecommun, Chongqing Key Lab Mobile Commun Technol, Chongqing 400065, Peoples R China.
EM liyun@cqupt.edu.cn; liujiedemingtian@qq.com; caobin65@163.com;
   cgwang@ieee.org
RI Li, Yun/GRF-2207-2022
FU Program for the National Science Foundation of China [61671096,
   61701059]; Chongqing Research Program of Basic Science and Frontier
   Technology [cstc2017jcyjBX0005, cstc2015jcyjA40048]; Chongqing Science
   and Technology Innovation Leading Talent Support Program
   [CSTCCXLJRC201710]; Venture and Innovation Support Program for Chongqing
   Overseas Returnees; National Mobile Communications Research Laboratory,
   Southeast University [2015D07]
FX This work was supported by the Program for the National Science
   Foundation of China (61671096 and 61701059), by the Chongqing Research
   Program of Basic Science and Frontier Technology (cstc2017jcyjBX0005 and
   cstc2015jcyjA40048), by Chongqing Science and Technology Innovation
   Leading Talent Support Program (CSTCCXLJRC201710), by Venture and
   Innovation Support Program for Chongqing Overseas Returnees, and by the
   open research fund of the National Mobile Communications Research
   Laboratory, Southeast University (2015D07).
CR Abbasi A, 2015, 2015 IEEE 23RD INTERNATIONAL SYMPOSIUM ON QUALITY OF SERVICE (IWQOS), P369, DOI 10.1109/IWQoS.2015.7404756
   Aggarwal V, 2013, IEEE T MULTIMEDIA, V15, P789, DOI 10.1109/TMM.2013.2240287
   [Anonymous], 2014, AMAZON EC2 RESERVED
   Barbarossa S, 2013, IEEE INT WORK SIGN P, P26, DOI 10.1109/SPAWC.2013.6612005
   Ben-Tal A, 1998, MATH OPER RES, V23, P769, DOI 10.1287/moor.23.4.769
   Ben-Tal A, 2004, MATH PROGRAM, V99, P351, DOI 10.1007/s10107-003-0454-y
   Bertsimas D, 2004, OPER RES, V52, P35, DOI 10.1287/opre.1030.0065
   Chaisiri B.-S., 2009, 2009 IEEE AS PAC SER, P103, DOI DOI 10.1109/APSCC.2009.5394134
   Chaisiri S., 2011, Proceedings of the 2011 IEEE 19th International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS 2011), P85, DOI 10.1109/MASCOTS.2011.30
   Chaisiri S., 2010, 2010 IEEE International Conference on Service-Oriented Computing and Applications (SOCA), P1, DOI DOI 10.1109/SOCA.2010.5707147
   Chaisiri S, 2012, IEEE T SERV COMPUT, V5, P164, DOI 10.1109/TSC.2011.7
   Chase J, 2014, IEEE ICC, P2969, DOI 10.1109/ICC.2014.6883776
   Dinh HT, 2013, WIREL COMMUN MOB COM, V13, P1587, DOI 10.1002/wcm.1203
   Du J, 2016, IEEE T MULTIMEDIA, V18, P820, DOI 10.1109/TMM.2016.2537781
   Hu ML, 2012, IEEE INT CONF NETWOR, P204, DOI 10.1109/ICON.2012.6506559
   Hwang RH, 2014, IEEE T SERV COMPUT, V7, P561, DOI 10.1109/TSC.2013.35
   Kaewpuang R, 2013, IEEE J SEL AREA COMM, V31, P2685, DOI 10.1109/JSAC.2013.131209
   Liu FM, 2013, IEEE WIREL COMMUN, V20, P14
   Luo J., 2015, P 3 INT C CYB TECHN, P1
   Mark CCT, 2011, INT CON ADV INFO NET, P348, DOI 10.1109/AINA.2011.50
   Saki H, 2015, IEEE T MULTIMEDIA, V17, P333, DOI 10.1109/TMM.2015.2389032
   Shu P, 2013, IEEE INFOCOM SER, P195
   Si PB, 2014, IEEE ICC, P2270, DOI 10.1109/ICC.2014.6883661
   Wang XF, 2013, IEEE T MULTIMEDIA, V15, P811, DOI 10.1109/TMM.2013.2239630
   Wei CM, 2012, PR IEEE SEN ARRAY, P17, DOI [10.1109/CLOUD.2012.60, 10.1109/SAM.2012.6250460]
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Xiaoying Wang, 2011, Proceedings of the 2011 IEEE Asia-Pacific Services Computing Conference (APSCC), P147, DOI 10.1109/APSCC.2011.15
   Yousefyan S, 2013, 2013 5TH CONFERENCE ON INFORMATION AND KNOWLEDGE TECHNOLOGY (IKT), P55, DOI 10.1109/IKT.2013.6620038
   Yu L, 2014, INT CON DISTR COMP S, P258, DOI 10.1109/ICDCS.2014.34
NR 29
TC 95
Z9 106
U1 1
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2018
VL 20
IS 9
BP 2427
EP 2438
DI 10.1109/TMM.2018.2796246
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GR1ZX
UT WOS:000442358200015
DA 2024-07-18
ER

PT J
AU Yadati, K
   Larson, M
   Liem, CCS
   Hanjalic, A
AF Yadati, Karthik
   Larson, Martha
   Liem, Cynthia C. S.
   Hanjalic, Alan
TI Detecting Socially Significant Music Events Using Temporally Noisy
   Labels
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE EDM; event; break; build; drop; SoundCloud; timed comments
ID CLASSIFICATION; ANNOTATION
AB In this paper, we focus on event detection over the timeline of a music track. Such technology is motivated by the need for innovative applications such as searching, nonlinear access, and recommendation. Event detection over the timeline requires time-code level labels in order to train machine learning models. We use tinted comments from SoundCloud, a modern social music sharing platform, to obtain these labels. While in this way the need for tedious and time-consuming manual labeling can be reduced, the challenge is that timed comments are subject to additional temporal noise, as they occur in the temporal neighborhood of the actual events. We investigate the utility of such noisy timed comments as training labels through a case study, in which we investigate three types of events in electronic dance music (EDM): drop,build, and break. These socially significant events play a key role in an EDM track's unfolding and are popular in social media circles. These events are interesting for detection, and here we leverage the timed comments generated in the course of the online social activity around them. We propose a two-stage learning method that relies on noisy timed comments and, given a music track, marks the events on the timeline. In the experiments, we focus, in particular, on investigating to which extent noisy timed comments can replace manually acquired expert labels. The conclusions we draw during this study provide useful insights that motivate further research in the field of event detection.
C1 [Yadati, Karthik; Larson, Martha; Liem, Cynthia C. S.; Hanjalic, Alan] Delft Univ Technol, Dept Intelligent Syst, NL-2628 CD Delft, Netherlands.
   [Larson, Martha] Radboud Univ Nijmegen, NL-6525 HP Nijmegen, Netherlands.
C3 Delft University of Technology; Radboud University Nijmegen
RP Yadati, K (corresponding author), Delft Univ Technol, Dept Intelligent Syst, NL-2628 CD Delft, Netherlands.
EM n.k.yadati@tudelft.nl; m.larson@cs.ru.nl; c.c.s.liem@tudelft.nl;
   a.hanjalic@tudelft.nl
RI Larson, Martha/E-9983-2014
OI Hanjalic, Alan/0000-0002-5771-2549; Liem, Cynthia/0000-0002-5385-7695
FU European Commission's 7th Framework Program [610594, 601166]
FX This work was supported by the European Commission's 7th Framework
   Program under grant agreement no. 610594 (CrowdRec) and 601166
   (PHENICX).
CR [Anonymous], P IEEE AS PAC SIGN I
   [Anonymous], 2007, MIR MATLAB
   [Anonymous], COMPUTER SPEECH LANG
   [Anonymous], P INT C MUS INF RETR
   [Anonymous], 2014, PhD thesis
   Barrington L, 2010, IEEE T AUDIO SPEECH, V18, P602, DOI 10.1109/TASL.2009.2036306
   Bertin-Mahieux T., 2011, ISMIR, P591
   Brutti A, 2014, 2014 4TH JOINT WORKSHOP ON HANDS-FREE SPEECH COMMUNICATION AND MICROPHONE ARRAYS (HSCMA), P157, DOI 10.1109/HSCMA.2014.6843271
   Butler M., 2006, PROFILES POPULAR MUS
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Dennis J, 2011, IEEE SIGNAL PROC LET, V18, P130, DOI 10.1109/LSP.2010.2100380
   Frénay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894
   Fu ZY, 2011, IEEE T MULTIMEDIA, V13, P303, DOI 10.1109/TMM.2010.2098858
   Jiang YG, 2013, INT J MULTIMED INF R, V2, P73, DOI 10.1007/s13735-012-0024-2
   Li J., 2005, P INTERSPEECH, P566
   Lidy T, 2005, P 6 INT C MUS INF RE, P34
   Lo HY, 2011, IEEE T MULTIMEDIA, V13, P518, DOI 10.1109/TMM.2011.2129498
   Muller M., 2015, FUNDAMENTALS MUSIC P, P167, DOI 10.1007/978-3-319-21945-54
   Schluter Jan, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6979, DOI 10.1109/ICASSP.2014.6854953
   Serrà J, 2014, IEEE T MULTIMEDIA, V16, P1229, DOI 10.1109/TMM.2014.2310701
   Toledano DT, 2003, IEEE T SPEECH AUDI P, V11, P617, DOI 10.1109/TSA.2003.813579
   Vliegendhart R, 2015, IEEE T MULTIMEDIA, V17, P1372, DOI 10.1109/TMM.2015.2449086
   Wu B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P721, DOI 10.1145/2623330.2623625
   Xu P, 2014, P 2014 INT ACM WORKS, P57, DOI [10.1145/2660114.2660124, DOI 10.1145/2660114.2660124]
   Yadati K., 2014, Proceedings of the International Conference on Music Information Retrieval, P143
   Ziegler S, 2012, IEEE W SP LANG TECH, P342, DOI 10.1109/SLT.2012.6424247
NR 26
TC 6
Z9 6
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2018
VL 20
IS 9
BP 2526
EP 2540
DI 10.1109/TMM.2018.2801719
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GR1ZX
UT WOS:000442358200023
OA Green Published
DA 2024-07-18
ER

PT J
AU Chen, Y
   Zhang, H
   Zhang, XP
   Liu, R
AF Chen, Yong
   Zhang, Hui
   Zhang, Xiaopeng
   Liu, Rui
TI Regularized Semi-non-negative Matrix Factorization for Hashing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Learning to hash; non-negative matrix factorization; pairwise semantics;
   binary-like relaxation; stochastic learning
AB Learning with non-negative matrix factorization (NMF) has significantly benefited large numbers of fields such as information retrieval, computer vision, natural language processing, biomedicine, and neuroscience, etc. However, little research (with NMF) has scratched hashing, which is a sharp sword in approximately nearest neighbors search for economical storage and efficient hardware-level XOR operations. To explore more, we propose a novel hashing model, called Regularized Semi-NMF for Hashing (SeH), which is a minimal optimization between Semi-NMF, semantics preserving, and efficient coding. Tricks such as balance codes, binary-like relaxation, and stochastic learning are employed to yield efficient algorithms which raise the capabilities to deal with a large-scale dataset. SeH is shown to evidently improve retrieval effectiveness over some state-of-theart baselines on several public datasets (MSRA-CFW, Caltech256, Cifar10, and ImageNet) with different sample scales and feature representations. Furthermore, a case study on Caltech256, that is, three image queries are randomly selected and the corresponding search results are presented, would intuitively exhibit which method is better.
C1 [Chen, Yong; Zhang, Hui; Zhang, Xiaopeng; Liu, Rui] Beihang Univ BUAA, Dept Comp Sci & Engn, Beijing 100191, Peoples R China.
C3 Beihang University
RP Zhang, XP (corresponding author), Beihang Univ BUAA, Dept Comp Sci & Engn, Beijing 100191, Peoples R China.
EM chenyong@nlsde.buaa.edu.cn; hzhang@nlsde.buaa.edu.cn;
   zxpjustin@buaa.edu.cn; lr@buaa.edu.cn
CR Phan AH, 2012, NEURAL COMPUT APPL, V21, P623, DOI 10.1007/s00521-011-0652-0
   [Anonymous], 2006, Book Annosearch: Image auto-annotation by search, DOI DOI 10.1109/CVPR.2006.58
   Baeza-Yates Ricardo, 1999, MODERN INFORM RETRIE, V463
   Baker JM, 2009, IEEE SIGNAL PROC MAG, V26, P75, DOI 10.1109/MSP.2009.932166
   Boyd S., 2003, SUBGRADIENT METHODS
   Boyd S.P., 2004, Convex optimization, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441]
   Chen ZX, 2017, IEEE T MULTIMEDIA, V19, P123, DOI 10.1109/TMM.2016.2620604
   Chowdhury GG, 2003, ANNU REV INFORM SCI, V37, P51, DOI 10.1002/aris.1440370103
   Cichocki A., 2009, NONNEGATIVE MATRIX T, DOI 10.1002/9780470747278
   Ding C, 2006, P 12 ACM SIGKDD INT, P126, DOI 10.1145/1150402.1150420
   Ding C, 2010, IEEE T PATTERN ANAL, V32, P45, DOI 10.1109/TPAMI.2008.277
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Griffin G., 2007, CALTECH 256 OBJECT C
   Hao YB, 2017, IEEE T MULTIMEDIA, V19, P1, DOI 10.1109/TMM.2016.2610324
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024
   Jiang QY, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2248
   Jin ZM, 2013, IEEE I CONF COMP VIS, P257, DOI 10.1109/ICCV.2013.39
   Jin ZM, 2014, IEEE T CYBERNETICS, V44, P1362, DOI 10.1109/TCYB.2013.2283497
   Kang WC, 2016, AAAI CONF ARTIF INTE, P1230
   Kimura K., 2014, P 6 AS C MACH LEARN, P129
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Liu Hong, 2016, IJCAI, P1767, DOI DOI 10.1109/TIP.2016.2564638
   Liu W., 2014, P NEURAL INF PROCESS, P3419
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Mao XJ, 2017, IEEE T MULTIMEDIA, V19, P382, DOI 10.1109/TMM.2016.2614858
   Martyn P., 2011, SOCIOLOGICAL REFLECT
   Mukherjee L, 2015, IEEE I CONF COMP VIS, P4184, DOI 10.1109/ICCV.2015.476
   Nesterov Yurii, 2013, Introductory Lectures on Convex Optimization: A Basic Course
   Peng, 2006, AAAI, P342
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Trigeorgis G, 2014, PR MACH LEARN RES, V32, P1692
   Umbaugh S.E., 1997, Computer Vision and Image Processing: A Practical Approach Using Cviptools with Cdrom
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang J., IEEE transactions on pattern analysis and machine intelligence
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Wang YX, 2013, IEEE T KNOWL DATA EN, V25, P1336, DOI 10.1109/TKDE.2012.51
   Weiss Y., 2008, NIPS, V21, P1753
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Yu FX, 2014, PR MACH LEARN RES, V32, P946
   Zhang D, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P225
   Zhang PC, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P173, DOI 10.1145/2600428.2609600
   Zhang X, 2012, IEEE T MULTIMEDIA, V14, P995, DOI 10.1109/TMM.2012.2186121
   Zhu XF, 2014, IEEE T IMAGE PROCESS, V23, P3737, DOI 10.1109/TIP.2014.2332764
   Zuselevich S. N., 2012, MINIMIZATION METHODS
NR 47
TC 8
Z9 8
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1823
EP 1836
DI 10.1109/TMM.2017.2775220
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100018
DA 2024-07-18
ER

PT J
AU Liu, QJ
   Wang, WW
   de Campos, T
   Jackson, PJB
   Hilton, A
AF Liu, Qingju
   Wang, Wenwu
   de Campos, Teofilo
   Jackson, Philip J. B.
   Hilton, Adrian
TI Multiple Speaker Tracking in Spatial Audio via PHD Filtering and
   Depth-Audio Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-person tracking; spatial audio; binaural microphones; depth
   sensor; depth and audio; PHD filtering
ID TIME-VARYING NUMBER; INTENSITY
AB In the object-based spatial audio system, positions of the audio objects (e.g., speakers/talkers or voices) presented in the sound scene are required as important metadata attributes for object acquisition and reproduction. Binaural microphones are often used as a physical device to mimic human hearing and to monitor and analyze the scene, including localization and tracking of multiple speakers. The binaural audio tracker, however, is usually prone to the errors caused by room reverberation and background noise. To address this limitation, we present a multimodal tracking method by fusing the binaural audio with depth information (from a depth sensor, e.g., Kinect). More specifically, the probability hypothesis density (PHD) filtering framework is first applied to the depth stream, and a novel clutter intensity model is proposed to improve the robustness of the PHD filter when an object is occluded either by other objects or due to the limited field of view of the depth sensor. To compensate misdetections in the depth stream, a novel gap filling technique is presented to map audio azimuths obtained from the binaural audio tracker to 3D positions, using speaker-dependent spatial constraints learned from the depth stream. With our proposed method, both the errors in the binaural tracker and the misdetections in the depth tracker can be significantly reduced. Real-room recordings are used to show the improved performance of the proposed method in removing outliers and reducing misdetections.
C1 [Liu, Qingju; Wang, Wenwu; de Campos, Teofilo; Jackson, Philip J. B.; Hilton, Adrian] Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
C3 University of Surrey
RP Liu, QJ (corresponding author), Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
EM q.liu@surrey.ac.uk; w.wang@surrey.ac.uk; t.decampos@surrey.ac.uk;
   p.jackson@surrey.ac.uk; a.hilton@surrey.ac.uk
RI wang, wenwu/HOF-4371-2023; ARSLAN, Okan/AAA-3232-2020; Jackson, Philip J
   B/E-8422-2013; de Campos, Teofilo/ABF-8003-2020; Hilton,
   Adrian/N-3736-2014
OI Jackson, Philip J B/0000-0001-7933-5935; de Campos,
   Teofilo/0000-0001-6172-0229; Wang, Wenwu/0000-0002-8393-5703; LIU,
   Qingju/0000-0003-0778-2992; Hilton, Adrian/0000-0003-4223-238X
FU EPSRC Programme Grant S3A: Future Spatial Audio for an Immersive
   Listener Experience at Home [EP/L000539/1]; BBC as part of the BBC Audio
   Research Partnership; EPSRC [EP/M028321/1, EP/L000539/1, EP/P022529/1]
   Funding Source: UKRI
FX This work was supported in part by the EPSRC Programme Grant S3A: Future
   Spatial Audio for an Immersive Listener Experience at Home
   (EP/L000539/1) and in part by the BBC as part of the BBC Audio Research
   Partnership. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Abdulmotaleb El
   Saddik.
CR [Anonymous], 2011, BINAURAL ROOM IMPULS
   [Anonymous], 2007, MOBILE NETW APPL
   [Anonymous], 2015, PHASESPACE IMPULSE M
   Arras KO, 2008, IEEE INT CONF ROBOT, P1710, DOI 10.1109/ROBOT.2008.4543447
   Barnard M, 2014, IEEE T MULTIMEDIA, V16, P864, DOI 10.1109/TMM.2014.2301977
   Choi W., 2001, P IEEE INT C COMP VI, P1076
   Coleman P., 2016, P 60 AUD ENG SOC INT
   Dai CX, 2007, COMPUT VIS IMAGE UND, V106, P288, DOI 10.1016/j.cviu.2006.08.009
   Fallon MF, 2012, IEEE T AUDIO SPEECH, V20, P1409, DOI 10.1109/TASL.2011.2178402
   Francombe J., 2015, P 138 AUD ENG SOC CO
   Garofolo J. S., 1993, 4930 NASA STI REC
   Gatica-Perez D, 2007, IEEE T AUDIO SPEECH, V15, P601, DOI 10.1109/TASL.2006.881678
   Gebru I., 2017, IEEE T PATTERN ANAL
   Hoel P.G., 1976, ELEMENTARY STAT, V4th
   Jiang H, 2008, IEEE T MULTIMEDIA, V10, P997, DOI 10.1109/TMM.2008.2001379
   Kalman R E., 1960, J BASIC ENG, V82, P35, DOI DOI 10.1115/1.3662552
   Kilic V, 2016, IEEE T MULTIMEDIA, V18, P2417, DOI 10.1109/TMM.2016.2599150
   Kiliç V, 2015, IEEE T MULTIMEDIA, V17, P186, DOI 10.1109/TMM.2014.2377515
   KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830
   Krotosky SJ, 2007, COMPUT VIS IMAGE UND, V106, P270, DOI 10.1016/j.cviu.2006.10.008
   Lehmann E. A., 2006, EURASIP J ADV SIG PR, P1
   Levorato R, 2014, LECT NOTES COMPUT SC, V8810, P474, DOI 10.1007/978-3-319-11900-7_40
   Lian F, 2010, IEEE T AERO ELEC SYS, V46, P2066, DOI 10.1109/TAES.2010.5595616
   Liu QJ, 2016, INT CONF ACOUST SPEE, P1506, DOI 10.1109/ICASSP.2016.7471928
   Liu QJ, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P709, DOI 10.1109/ICCVW.2015.97
   Livingston MA, 2012, IEEE VIRTUAL REALITY CONFERENCE 2012 PROCEEDINGS, P119, DOI 10.1109/VR.2012.6180911
   Luber M, 2011, IEEE INT C INT ROBOT, P3844, DOI 10.1109/IROS.2011.6048836
   Ma WK, 2006, IEEE T SIGNAL PROCES, V54, P3291, DOI 10.1109/TSP.2006.877658
   Mahler RPS, 2003, IEEE T AERO ELEC SYS, V39, P1152, DOI 10.1109/TAES.2003.1261119
   McKenna SJ, 2000, COMPUT VIS IMAGE UND, V80, P42, DOI 10.1006/cviu.2000.0870
   Michalowski M. P., 2006, 1st Annual Conference on Human-Robot Interaction, P347
   Microsoft, 2015, KIN FOR WIND
   Mogelmose A, 2013, IEEE COMPUT SOC CONF, P301, DOI 10.1109/CVPRW.2013.52
   Morato C, 2014, J COMPUT INF SCI ENG, V14, DOI 10.1115/1.4025810
   O'Donovan A., 2007, IEEE C COMP VIS PATT, P1
   Qu W, 2007, IEEE T MULTIMEDIA, V9, P511, DOI 10.1109/TMM.2006.886266
   Raja Y, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P228, DOI 10.1109/AFGR.1998.670953
   Ristic B, 2012, IEEE T AERO ELEC SYS, V48, P1656, DOI 10.1109/TAES.2012.6178085
   Satongar D., 2014, P INT C SOUND VIB JU, P1775
   Sevrin L, 2015, IRBM, V36, P361, DOI 10.1016/j.irbm.2015.10.003
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Spinello L, 2011, IEEE INT C INT ROBOT, P3838, DOI 10.1109/IROS.2011.6048835
   Spors S, 2013, P IEEE, V101, P1920, DOI 10.1109/JPROC.2013.2264784
   Vermaak J, 2001, INT CONF ACOUST SPEE, P3021, DOI 10.1109/ICASSP.2001.940294
   Vermaak J, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P741, DOI 10.1109/ICCV.2001.937600
   Vo BN, 2006, IEEE T SIGNAL PROCES, V54, P4091, DOI 10.1109/TSP.2006.881190
   Vo BN, 2005, IEEE T AERO ELEC SYS, V41, P1224, DOI 10.1109/TAES.2005.1561884
   Yan F, 2008, IEEE T PATTERN ANAL, V30, P1814, DOI 10.1109/TPAMI.2007.70834
   Ye M, 2011, IEEE I CONF COMP VIS, P731, DOI 10.1109/ICCV.2011.6126310
   Zhang SL, 2015, IEEE T MULTIMEDIA, V17, P265, DOI 10.1109/TMM.2015.2390044
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
NR 51
TC 11
Z9 11
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1767
EP 1780
DI 10.1109/TMM.2017.2777671
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100014
DA 2024-07-18
ER

PT J
AU Bourtsoulatze, E
   Thomos, N
   Saltarin, J
   Braun, T
AF Bourtsoulatze, Eirina
   Thomos, Nikolaos
   Saltarin, Jonnahtan
   Braun, Torsten
TI Content-Aware Delivery of Scalable Video in Network Coding Enabled Named
   Data Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Network coding; named data networking; scalable video; rate allocation;
   forwarding strategy
ID FLOW
AB We propose a novel network coding (NC) enabled named data networking (NDN) architecture for scalable video delivery. Our architecture utilizes NC in order to address the problem that arises in the original NDN architecture, where optimal use of the bandwidth and caching resources necessitates the coordination of the Interest forwarding decisions. To optimize the performance of the proposed NC-based NDN architecture and render it appropriate for transmission of scalable video, we devise a novel rate allocation algorithm that decides on the optimal rates of Interests sent by clients and intermediate nodes. The flow of Data packets achieved by this algorithm maximizes the average quality of the video delivered to the client population. To support the handling of Interest and Data packets when intermediate nodes perform NC, we introduce the use of Bloom filters, which store efficiently additional information about the Interest and Data packets, and modify accordingly the standard NDN architecture. We also devise an optimized Interest forwarding strategy that implements the target rate allocation. The proposed architecture is evaluated for transmission of scalable video over PlanetLab topologies. The evaluation shows that the proposed scheme exploits optimally the available network resources.
C1 [Bourtsoulatze, Eirina; Saltarin, Jonnahtan; Braun, Torsten] Univ Bern, CH-3012 Bern, Switzerland.
   [Bourtsoulatze, Eirina] Imperial Coll London, London SW7 2AZ, England.
   [Thomos, Nikolaos] Univ Essex, Colchester CO4 3SQ, Essex, England.
C3 University of Bern; Imperial College London; University of Essex
RP Bourtsoulatze, E (corresponding author), Univ Bern, CH-3012 Bern, Switzerland.; Bourtsoulatze, E (corresponding author), Imperial Coll London, London SW7 2AZ, England.
EM e.bourtsoulatze@imperial.ac.uk; nthomos@essex.ac.uk;
   saltarin@inf.unibe.ch; braun@inf.unibe.ch
RI Bourtsoulatze, Eirina/ABG-5003-2021; Thomos, Nikolaos/AAU-2328-2020;
   Braun, Torsten/AAA-2592-2019
OI Braun, Torsten/0000-0001-5968-7108; Thomos, Nikolaos/0000-0001-7266-2642
FU Swiss National Science Foundation [149225]
FX This work was supported by the Swiss National Science Foundation under
   Grant 149225.
CR Afanasyev A., 2016, NDN0021
   Ahlswede R, 2000, IEEE T INFORM THEORY, V46, P1204, DOI 10.1109/18.850663
   Anastasiades C, 2015, IEEE ICC, P3026, DOI 10.1109/ICC.2015.7248788
   [Anonymous], 2016, P IEEE INFOCOM SAN F
   [Anonymous], MULT EXP ICME 2011 I
   [Anonymous], 2015, White Paper
   [Anonymous], 2017, P 23 AS PAC C COMM A
   [Anonymous], 2013, Technical Report
   BLOOM BH, 1970, COMMUN ACM, V13, P422, DOI 10.1145/362686.362692
   Byun D, 2013, IEEE ICC, P3738, DOI 10.1109/ICC.2013.6655136
   Chatzipanagiotis N, 2013, IEEE GLOB CONF SIG, P883, DOI 10.1109/GlobalSIP.2013.6737033
   Chou P. A., 2003, P 41 AL C COMM CONTR
   Cleju N, 2011, IEEE T MULTIMEDIA, V13, P1103, DOI 10.1109/TMM.2011.2161448
   DANTZIG GB, 1957, OPER RES, V5, P266, DOI 10.1287/opre.5.2.266
   Ho T., 2003, P 41 AL C COMM CONTR
   ITU-T, 2005, 1449610 ITUT ISOIEC
   Le A., 2015, IEEE ACM T NETWORK, V24, P2983
   Li ZP, 2005, IEEE INFOCOM SER, P2184
   Liu Y., 2013, P IEEE INT C COMM SY
   Llorca J, 2013, IEEE ICC, P3557, DOI 10.1109/ICC.2013.6655103
   Montpetit M.-J., 2012, Proceedings of the 1st ACM workshop on Emerging Name-Oriented Mobile Networking Design - Architecture, Algorithms, and Applications, NoM '12, P31
   Plass M.F., 2009, P 5 INT C EMERGING N, P1, DOI [10.1145/1658939.1658941, DOI 10.1145/1658939.1658941]
   Sherali HD, 1996, OPER RES LETT, V19, P105, DOI 10.1016/0167-6377(96)00019-3
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Thomos N, 2011, IEEE T MULTIMEDIA, V13, P776, DOI 10.1109/TMM.2011.2111364
   Tsilopoulos C., 2013, P PACK VID WORKSH 20, P1
   Tsilopoulos Christos., 2011, Proceedings of the ACM SIGCOMM workshop on Information-centric networking, P13
   Wu Q., 2013, Proceedings of the 3rd ACM SIGCOMM workshop on Information-centric networking, ICN '13, P41
   Xylomenos G, 2014, IEEE COMMUN SURV TUT, V16, P1024, DOI 10.1109/SURV.2013.070813.00063
NR 30
TC 13
Z9 14
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1561
EP 1575
DI 10.1109/TMM.2017.2767778
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400022
OA Green Accepted, Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Hsu, CC
   Lin, CW
AF Hsu, Chih-Chung
   Lin, Chia-Wen
TI CNN-Based Joint Clustering and Representation Learning with Feature
   Drift Compensation for Large-Scale Image Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural network (CNN); deep learning; image clustering;
   unsupervised learning
AB Given a large unlabeled set of images, how to efficiently and effectively group them into clusters based on extracted visual representations remains a challenging problem. To address this problem, we propose a convolutional neural network (CNN) to jointly solve clustering and representation learning in an iterative manner. In the proposed method, given an input image set, we first randomly pick k samples and extract their features as initial cluster centroids using the proposed CNN with an initial model pretrained from the ImageNet dataset. Mini-batch k-means is then performed to assign cluster labels to individual input samples for a mini-batch of images randomly sampled from the input image set until all images are processed. Subsequently, the proposed CNN simultaneously updates the parameters of the proposed CNN and the centroids of image clusters iteratively based on stochastic gradient descent. We also propose a feature drift compensation scheme to mitigate the drift error caused by feature mismatch in representation learning. Experimental results demonstrate the proposed method outperforms start-of-the-art clustering schemes in terms of accuracy and storage complexity on large-scale image sets containing millions of images.
C1 [Hsu, Chih-Chung; Lin, Chia-Wen] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 30013, Taiwan.
C3 National Tsing Hua University
RP Lin, CW (corresponding author), Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 30013, Taiwan.
EM m121754@gmail.com; cwlin@ee.nthu.edu.tw
RI Hsu, Chih-Chung/Y-4835-2019; Lin, Chia-Wen/ABH-6075-2020; Lin,
   Chia-Wen/M-4571-2013
OI Hsu, Chih-Chung/0000-0002-2083-4438; Lin, Chia-Wen/0000-0002-9097-2318
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   [Anonymous], 2015, ICLR
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], P 33 INT C LEARN SAN
   [Anonymous], PATTERN REC IN PRESS
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], INT J COMPUTER VISIO
   [Anonymous], IEEE T PATT IN PRESS
   [Anonymous], P INT C LEARN SAN JU
   [Anonymous], 2013, NIPS
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Avrithis Y, 2015, IEEE I CONF COMP VIS, P1502, DOI 10.1109/ICCV.2015.176
   Chadha A, 2017, IEEE T MULTIMEDIA, V19, P1596, DOI 10.1109/TMM.2017.2673415
   Gao LL, 2016, AAAI CONF ARTIF INTE, P1188
   Gdalyahu Y, 2001, IEEE T PATTERN ANAL, V23, P1053, DOI 10.1109/34.954598
   Gong YC, 2015, PROC CVPR IEEE, P19, DOI 10.1109/CVPR.2015.7298596
   GOWDA KC, 1978, PATTERN RECOGN, V10, P105
   Hariharan B, 2012, LECT NOTES COMPUT SC, V7575, P459, DOI 10.1007/978-3-642-33765-9_33
   Kim J., 2016, PROC CVPR IEEE, P1637
   Mclachlan G., 2004, Finite Mixture Models
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Nie FP, 2016, AAAI CONF ARTIF INTE, P1962
   Nie FP, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P977, DOI 10.1145/2623330.2623726
   Nie FP, 2010, LECT NOTES ARTIF INT, V6322, P451
   Nie FP, 2012, IEEE T SYST MAN CY B, V42, P17, DOI 10.1109/TSMCB.2011.2161607
   Nie FP, 2011, IEEE T NEURAL NETWOR, V22, P1796, DOI 10.1109/TNN.2011.2162000
   Oquab M, 2015, PROC CVPR IEEE, P685, DOI 10.1109/CVPR.2015.7298668
   Sculley D., 2010, P INT C WORLD WID WE, V19, P1177, DOI [DOI 10.1145/1772690.1772862, 10.1145/1772690.1772862]
   Song J, 2016, IEEE T IMAGE PROCESS, V25, P4999, DOI 10.1109/TIP.2016.2601260
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Xu W., 2003, P 26 ANN INT ACM SIG, P267
   Yang JW, 2016, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR.2016.556
   Zhang YB, 2016, IEEE T MULTIMEDIA, V18, P405, DOI 10.1109/TMM.2015.2512046
NR 36
TC 87
Z9 93
U1 2
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2018
VL 20
IS 2
BP 421
EP 429
DI 10.1109/TMM.2017.2745702
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FT2ER
UT WOS:000422953200013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kou, F
   Wei, Z
   Chen, WH
   Wu, XM
   Wen, CY
   Li, ZG
AF Kou, Fei
   Wei, Zhe
   Chen, Weihai
   Wu, Xingming
   Wen, Changyun
   Li, Zhengguo
TI Intelligent Detail Enhancement for Exposure Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Detail enhancement; edge-preserving smoothing pyramid; exposure fusion;
   gradient domain weighted least square; high dynamic range (HDR)
ID IMAGE; COMPRESSION; GRADIENT
AB Multiscale exposure fusion is a fast approach to fuse several differently exposed images captured at the same high dynamic range (HDR) scene into a high-quality low-dynamic range (LDR) image. The fused image is expected to include all details of the input images. However, the details in the brightest and darkest regions are usually not well preserved. Adding details that are extracted from the input images to the fused image is an efficient approach to overcome the problem. In this paper, a new gradient domain weighted least square based image smoothing algorithm is proposed to extract the details in the brightest and darkest regions of the HDR scene. The extracted details are then added to an image that is produced using an edge-preserving smoothing pyramid based multiscale exposure fusion algorithm. Experimental results show that the proposed detail enhanced exposure fusion algorithm can preserve details in saturated regions, especially the brightest regions, better than the state-of-the-art multiscale exposure fusion algorithms.
C1 [Kou, Fei; Chen, Weihai; Wu, Xingming] Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China.
   [Kou, Fei] Vivo Mobile Commun Co Ltd, New Tech Inst, Hangzhou, Zhejiang, Peoples R China.
   [Wei, Zhe; Wen, Changyun] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Li, Zhengguo] Inst Infocomm Res, Robot Dept, Singapore 138632, Singapore.
C3 Beihang University; Nanyang Technological University; Agency for Science
   Technology & Research (A*STAR); A*STAR - Institute for Infocomm Research
   (I2R)
RP Chen, WH (corresponding author), Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China.
EM koufei@buaa.edu.cn; weiz0008@ntu.edu.sg; whchen@buaa.edu.cn;
   xmwubuaa@163.com; ecywen@ntu.edu.sg; ezgli@i2r.a-star.edu.sg
RI Wen, Changyun/A-5018-2011; KOU, Fei/T-6048-2019; Chen, Wei/GZK-7348-2022
OI Wen, Changyun/0000-0001-9530-360X; /0009-0006-1337-808X
FU National Nature Science Foundation of China [61620106012, 61573048,
   61603020]; International Scientific and Technological Cooperation
   Projects of China [2015DFG12650]
FX This work was supported by the National Nature Science Foundation of
   China under the research Project 61620106012, Project 61573048, and
   Project 61603020, and in part by the International Scientific and
   Technological Cooperation Projects of China under Grant 2015DFG12650.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Judith Redi. (Corresponding author:
   Weihai Chen.)
CR Debevec P. E., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P369, DOI 10.1145/258734.258884
   DIZENZO S, 1986, COMPUT VISION GRAPH, V33, P116, DOI 10.1016/0734-189X(86)90223-9
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Eilertsen G, 2017, COMPUT GRAPH FORUM, V36, P565, DOI 10.1111/cgf.13148
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Kordelas GA, 2016, IEEE T MULTIMEDIA, V18, P155, DOI 10.1109/TMM.2015.2505905
   Kou F, 2017, IEEE INT CON MULTI, P1105, DOI 10.1109/ICME.2017.8019529
   Kou F, 2015, IEEE T IMAGE PROCESS, V24, P4528, DOI 10.1109/TIP.2015.2468183
   Kou F, 2013, C IND ELECT APPL, P1398
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Li ZG, 2012, IEEE T IMAGE PROCESS, V21, P4672, DOI 10.1109/TIP.2012.2207396
   Li ZG, 2017, IEEE T IMAGE PROCESS, V26, P1243, DOI 10.1109/TIP.2017.2651366
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P120, DOI 10.1109/TIP.2014.2371234
   Li ZG, 2014, IEEE T IND ELECTRON, V61, P7076, DOI 10.1109/TIE.2014.2314066
   Li ZG, 2014, IEEE T IMAGE PROCESS, V23, P4372, DOI 10.1109/TIP.2014.2349432
   Ma KD, 2015, IEEE IMAGE PROC, P1717, DOI 10.1109/ICIP.2015.7351094
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Mai ZC, 2013, IEEE T MULTIMEDIA, V15, P1503, DOI 10.1109/TMM.2013.2266633
   Mertens T, 2009, COMPUT GRAPH FORUM, V28, P161, DOI 10.1111/j.1467-8659.2008.01171.x
   Min DB, 2014, IEEE T IMAGE PROCESS, V23, P5638, DOI 10.1109/TIP.2014.2366600
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Shen R, 2011, IEEE T IMAGE PROCESS, V20, P3634, DOI 10.1109/TIP.2011.2150235
   Su Z, 2013, IEEE T MULTIMEDIA, V15, P535, DOI 10.1109/TMM.2012.2237025
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Tsai CY, 2012, IEEE T MULTIMEDIA, V14, P1140, DOI 10.1109/TMM.2012.2190390
   Wang SQ, 2016, IEEE T MULTIMEDIA, V18, P219, DOI 10.1109/TMM.2015.2510326
   Wang TH, 2015, IEEE T MULTIMEDIA, V17, P470, DOI 10.1109/TMM.2015.2403612
   Wei Z., IEEE T MULT IN PRESS
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yang QX, 2015, IEEE T IMAGE PROCESS, V24, P1919, DOI 10.1109/TIP.2015.2403238
NR 32
TC 67
Z9 68
U1 1
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2018
VL 20
IS 2
BP 484
EP 495
DI 10.1109/TMM.2017.2743988
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FT2ER
UT WOS:000422953200018
DA 2024-07-18
ER

PT J
AU Lu, YW
   Yuan, C
   Lai, ZH
   Li, XL
   Wong, WK
   Zhang, D
AF Lu, Yuwu
   Yuan, Chun
   Lai, Zhihui
   Li, Xuelong
   Wong, Wai Keung
   Zhang, David
TI Nuclear Norm-Based 2DLPP for Image Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image classification; preserving projections; robust; two-dimensional
ID NEIGHBORHOOD PRESERVING PROJECTION; DISCRIMINANT-ANALYSIS; FACE
   RECOGNITION; DIMENSIONALITY REDUCTION; REGULARIZATION; PCA
AB Two-dimensional locality preserving projections (2DLPP) that use 2D image representation in preserving projection learning can preserve the intrinsic manifold structure and local information of data. However, 2DLPP is based on the Euclidean distance, which is sensitive to noise and outliers in data. In this paper, we propose a novel locality preserving projection method called nuclear norm-based two-dimensional locality preserving projections (NN-2DLPP). First, NN-2DLPP recovers the noisy data matrix through low-rank learning. Second, noise in data is removed and the learned clean data points are projected on a new subspace. Without the disturbance of noise, data points belonging to the same class are kept as close to each other as possible in the new projective subspace. Experimental results on six public image databases with face recognition, object classification, and handwritten digit recognition tasks demonstrated the effectiveness of the proposed method.
C1 [Lu, Yuwu; Yuan, Chun] Tsinghua Univ, Grad Sch Shenzhen, Tsinghua CUHK Joint Res Ctr Media Sci Technol & S, Shenzhen 518055, Peoples R China.
   [Lai, Zhihui] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518055, Peoples R China.
   [Lai, Zhihui; Wong, Wai Keung] Hong Kong Polytech Univ, Inst Text & Clothing, Hong Kong, Hong Kong, Peoples R China.
   [Li, Xuelong] Chinese Acad Sci, State Key Lab Transient Opt & Photon, Ctr Opt IMagery Anal & Learning, Xian 710119, Shaanxi, Peoples R China.
   [Zhang, David] Hong Kong Polytech Univ, Biometr Res Ctr, Hong Kong, Hong Kong, Peoples R China.
C3 Tsinghua University; Tsinghua Shenzhen International Graduate School;
   Shenzhen University; Hong Kong Polytechnic University; Chinese Academy
   of Sciences; State Key Laboratory of Transient Optics & Photonics; Hong
   Kong Polytechnic University
RP Yuan, C (corresponding author), Tsinghua Univ, Grad Sch Shenzhen, Tsinghua CUHK Joint Res Ctr Media Sci Technol & S, Shenzhen 518055, Peoples R China.
EM luyuwu2008@163.com; yuanc@sz.tsinghua.edu.cn; lai_zhi_hui@163.com;
   xuelong_li@opt.ac.cn; calvin.wong@polyu.edu.hk;
   csdzhang@comp.polyu.edu.hk
RI Li, Xuelong/Z-3785-2019; li, xiang/GWM-6319-2022; Zhang, David
   D/O-9396-2016; Li, Xuelong/ABF-3381-2020; Lai, Zhihui/R-1000-2019
OI Zhang, David D/0000-0002-5027-5286; Lai, Zhihui/0000-0002-4388-3080; Li,
   Xuelong/0000-0002-0019-4197
FU Natural Science Foundation of China [61602270, 61375012, 61573248,
   61761130079, U1433112]; China Postdoctoral Science Foundation
   [2016M590100, 2016M590812]; National High Technology Research and
   Development Plan (863 Plan) [2011AA01A205]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61602270, Grant 61375012, Grant 61573248, Grant
   61761130079, and Grant U1433112, in part by the China Postdoctoral
   Science Foundation under Grant 2016M590100 and Grant 2016M590812, and in
   part by the National High Technology Research and Development Plan (863
   Plan) under Grant 2011AA01A205. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Wolfgang Hurst. (Corresponding author: Chun Yuan.)
CR [Anonymous], 1996, COLUMBIA OBJECT IMAG
   [Anonymous], 1998, TECH REP
   [Anonymous], 2007, 0749 U MASS
   Bao BK, 2012, IEEE T IMAGE PROCESS, V21, P3794, DOI 10.1109/TIP.2012.2192742
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Boyd S., 2004, CONVEX OPTIMIZATION
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chao JS, 2016, IEEE T MULTIMEDIA, V18, P25, DOI 10.1109/TMM.2015.2502552
   Chen BC, 2015, IEEE T MULTIMEDIA, V17, P804, DOI 10.1109/TMM.2015.2420374
   Ding C., 2006, P 23 INT C MACH LEAR, P281, DOI DOI 10.1145/1143844.1143880
   Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132
   Duda R., 1973, Pattern Classification and Scene Analysis
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Hou CP, 2009, OPT ENG, V48, DOI 10.1117/1.3149850
   Hu DW, 2007, PATTERN RECOGN, V40, P339, DOI 10.1016/j.patcog.2006.06.022
   Jian M, 2016, IEEE T MULTIMEDIA, V18, P458, DOI 10.1109/TMM.2016.2515367
   Khachatryan A, 2015, IEEE T KNOWL DATA EN, V27, P2377, DOI 10.1109/TKDE.2015.2416725
   Kim E, 2015, PROC CVPR IEEE, P915, DOI 10.1109/CVPR.2015.7298693
   Kussul E, 2004, IMAGE VISION COMPUT, V22, P971, DOI 10.1016/j.imavis.2004.03.008
   Kwak N, 2008, IEEE T PATTERN ANAL, V30, P1672, DOI 10.1109/TPAMI.2008.114
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lee SH, 2007, IEEE SIGNAL PROC LET, V14, P735, DOI 10.1109/LSP.2007.896438
   Li X, 2010, NEUROCOMPUTING, V73, P2571, DOI 10.1016/j.neucom.2010.05.016
   Li XL, 2010, IEEE T SYST MAN CY B, V40, P1170, DOI 10.1109/TSMCB.2009.2035629
   Li XL, 2010, IEEE T KNOWL DATA EN, V22, P145, DOI 10.1109/TKDE.2009.64
   Lin Z., 2013, CORR
   Liu G., 2010, P INT C MACH LEARN, P663
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu JM, 2014, IEEE T IMAGE PROCESS, V23, P4022, DOI 10.1109/TIP.2014.2343458
   Lu YJ, 2009, IEEE T MULTIMEDIA, V11, P1289, DOI 10.1109/TMM.2009.2030632
   Luo CZ, 2016, IEEE T MULTIMEDIA, V18, P40, DOI 10.1109/TMM.2015.2495248
   Martìnez AM, 2001, IEEE T PATTERN ANAL, V23, P228, DOI 10.1109/34.908974
   Mazumder R, 2010, J MACH LEARN RES, V11, P2287
   Nie F., 2012, AAAI, P655
   Nie FP, 2017, AAAI CONF ARTIF INTE, P2422
   Nie FP, 2016, AAAI CONF ARTIF INTE, P1969
   Nie FP, 2014, PR MACH LEARN RES, V32, P1062
   Nie FP, 2015, KNOWL INF SYST, V42, P525, DOI 10.1007/s10115-013-0713-z
   Nie FP, 2010, IEEE T IMAGE PROCESS, V19, P1921, DOI 10.1109/TIP.2010.2044958
   Pang YW, 2008, IEEE T SYST MAN CY B, V38, P1176, DOI 10.1109/TSMCB.2008.923151
   Qian JJ, 2015, PATTERN RECOGN, V48, P3145, DOI 10.1016/j.patcog.2015.04.017
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Samaria F. S., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P138, DOI 10.1109/ACV.1994.341300
   Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154
   Su YC, 2014, IEEE T MULTIMEDIA, V16, P1645, DOI 10.1109/TMM.2014.2322337
   Sun LA, 2011, IEEE T PATTERN ANAL, V33, P194, DOI 10.1109/TPAMI.2010.160
   Tang Y, 2016, IEEE IJCNN, P4199, DOI 10.1109/IJCNN.2016.7727747
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Wang HX, 2014, IEEE T CYBERNETICS, V44, P828, DOI 10.1109/TCYB.2013.2273355
   Wang QQ, 2016, NEUROCOMPUTING, V216, P192, DOI 10.1016/j.neucom.2016.07.038
   Wu M., 2007, P 24 INT C MACHINE L, P1039
   Yang J, 2005, PATTERN RECOGN, V38, P1125, DOI 10.1016/j.patcog.2004.11.019
   Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097
   Yang J, 2017, IEEE T PATTERN ANAL, V39, P156, DOI 10.1109/TPAMI.2016.2535218
   Zhang FL, 2015, IEEE T NEUR NET LEAR, V26, P2247, DOI 10.1109/TNNLS.2014.2376530
   Zhang HJ, 2012, PATTERN RECOGN, V45, P1866, DOI 10.1016/j.patcog.2011.11.002
   Zhang Z, 2017, IEEE T IMAGE PROCESS, V26, P1607, DOI 10.1109/TIP.2017.2654163
   Zhang Z, 2016, IEEE T IMAGE PROCESS, V25, P2429, DOI 10.1109/TIP.2016.2547180
   Zhang Z, 2015, KNOWL-BASED SYST, V86, P143, DOI 10.1016/j.knosys.2015.06.001
   Zhao HX, 2011, CHIN CONT DECIS CONF, P1259, DOI 10.1109/CCDC.2011.5968382
   Zhong FJ, 2013, IEEE T IMAGE PROCESS, V22, P3018, DOI 10.1109/TIP.2013.2253476
NR 62
TC 25
Z9 25
U1 4
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2017
VL 19
IS 11
BP 2391
EP 2403
DI 10.1109/TMM.2017.2703130
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FJ9CT
UT WOS:000413068200003
DA 2024-07-18
ER

PT J
AU Zhang, X
   Zhu, C
   Wu, HG
   Liu, Z
   Xu, YY
AF Zhang, Xiang
   Zhu, Ce
   Wu, Honggang
   Liu, Zhi
   Xu, Yuanyuan
TI An Imbalance Compensation Framework for Background Subtraction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Background subtraction; class imbalance; imbalance compensation;
   imbalanced learning
ID DENSITY-ESTIMATION; SEGMENTATION; MODEL; PIXEL; SURVEILLANCE; ALGORITHM;
   TRACKING
AB Class imbalance refers to the instance where the number of training samples for the majority classes is far more than that of the minority classes (relative imbalance), and the quality of training samples for the minority classes is inferior to that of the majority classes (absolute imbalance), which are further complicated by other imbalance factors, e.g., data overlapping. Video background subtraction aims to classify each pixel into two classes: foreground and background. This paper first reveals that background subtraction is a class imbalance problem, where the foreground and background are the minority and majority classes, respectively. By exploring spatial and temporal correlation inherent in video data, we present an imbalance compensation framework for background subtraction, which consists of two sequential modules, imbalance-compensated bilayer modeling, and imbalance-compensated Bayesian classification. In the first module, spatio-temporal oversampling (SOS) and selective downsampling (SDS) are proposed to compensate the imbalance at data level. SOS attempts to synthesize representative samples appended to the minority sample set, while SDS selectively deletes a number of majority samples in data overlapping areas. The rebalanced samples are then used to learn a bilayer model. In the second module, novel cost functions are proposed to compensate the effect of class imbalance at algorithm level. The cost functions are based on imbalance measurement, and used to construct the prior term in the Bayesian classification scheme. Experiments are conducted on public databases to demonstrate the effectiveness of the proposed method.
C1 [Zhang, Xiang; Zhu, Ce] Univ Elect Sci & Technol, Sch Elect Engn, Chengdu 611731, Sichuan, Peoples R China.
   [Zhang, Xiang; Zhu, Ce] Univ Elect Sci & Technol, Ctr Robot, Chengdu 611731, Sichuan, Peoples R China.
   [Wu, Honggang] Civil Aviat Adm China, Res Inst 2, Beijing 611731, Peoples R China.
   [Liu, Zhi] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
   [Xu, Yuanyuan] Hohai Univ, Coll Comp & Informat, Nanjing 210098, Jiangsu, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China; Shanghai University; Hohai
   University
RP Zhu, C (corresponding author), Univ Elect Sci & Technol, Sch Elect Engn, Chengdu 611731, Sichuan, Peoples R China.; Zhu, C (corresponding author), Univ Elect Sci & Technol, Ctr Robot, Chengdu 611731, Sichuan, Peoples R China.
EM uestchero@uestc.edu.cn; eczhu@uestc.edu.cn; wu-honggang@hotmail.com;
   liuzhisjtu@163.com; xuyu0004@e.ntu.edu.sg
RI LIU, Zhi/D-4518-2012; Zhu, Ce/AEN-1875-2022
OI LIU, Zhi/0000-0002-8428-1131; 
FU National Natural Science Foundation of China [61571102, 61471230,
   U163310166]; Natural Science Foundation of Jiangsu Province of China
   [20165024411]; Fundamental Research Funds for the Central Universities
   [ZYGX2014Z003, ZYGX2015J025]; National High Technology Research and
   Development Program of China [2015AA015903]; China Postdoctoral Science
   Foundation [2016M602672]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61571102, Grant 61471230, and Grant
   U163310166, in part by the Natural Science Foundation of Jiangsu
   Province of China under Grant 20165024411, in part by the Fundamental
   Research Funds for the Central Universities under Grant ZYGX2014Z003 and
   Grant ZYGX2015J025, in part by the National High Technology Research and
   Development Program of China under Grant 2015AA015903, and in part by
   the China Postdoctoral Science Foundation under Grant 2016M602672. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Shu-Ching Chen. (Corresponding
   author: Ce Zhu.)
CR [Anonymous], PATTERN RECOG LETT
   [Anonymous], 2013, AUGMENTED VISION REA
   [Anonymous], 2010, HDB PATTERN RECOGNIT
   [Anonymous], P IEEE INT C DAT MIN
   [Anonymous], 2010, P 2010 INT JOINT C N, DOI DOI 10.1109/IJCNN.2010.5596486
   [Anonymous], MACH VIS APPL
   [Anonymous], P IEEE INT C IM PROC
   [Anonymous], P IEEE INT C ADV VID
   [Anonymous], 2006, CVPR, DOI DOI 10.1109/CVPR.2006.69
   Bai X, 2010, LECT NOTES COMPUT SC, V6315, P617
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Bellinger C, 2012, 2012 11TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2012), VOL 2, P102, DOI 10.1109/ICMLA.2012.212
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Braham M., 2016, PROC IEEE INT C SYST, P1
   Brutzer S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1937, DOI 10.1109/CVPR.2011.5995508
   Chang HJ, 2012, PROC CVPR IEEE, P2088, DOI 10.1109/CVPR.2012.6247914
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Chen BH, 2014, IEEE T MULTIMEDIA, V16, P837, DOI 10.1109/TMM.2014.2298377
   Chen LW, 2015, CYTOJOURNAL, V12, DOI 10.4103/1742-6413.161608
   Cheng FC, 2012, IEEE T INTELL TRANSP, V13, P671, DOI 10.1109/TITS.2011.2174635
   Cucchiara R, 2003, IEEE T PATTERN ANAL, V25, P1337, DOI 10.1109/TPAMI.2003.1233909
   Cuevas C, 2013, IEEE T CIRC SYST VID, V23, P1, DOI 10.1109/TCSVT.2012.2202191
   De Gregorio M, 2014, IEEE COMPUT SOC CONF, P409, DOI 10.1109/CVPRW.2014.66
   Elgammal A, 2002, P IEEE, V90, P1151, DOI 10.1109/JPROC.2002.801448
   Friedman N., 1997, PROC UNCERTAINTY ART, P175
   Gallego J, 2009, IEEE IMAGE PROC, P3205, DOI 10.1109/ICIP.2009.5414380
   Haibo He, 2009, IEEE Transactions on Knowledge and Data Engineering, V21, P1263, DOI 10.1109/TKDE.2008.239
   Haines TSF, 2014, IEEE T PATTERN ANAL, V36, P670, DOI 10.1109/TPAMI.2013.239
   Han B, 2008, IEEE T PATTERN ANAL, V30, P1186, DOI 10.1109/TPAMI.2007.70771
   Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683
   Harville M, 2002, LECT NOTES COMPUT SC, V2352, P543
   He H, 2013, IMBALANCED LEARNING: FOUNDATIONS, ALGORITHMS, AND APPLICATIONS, P1, DOI 10.1002/9781118646106
   Jung CR, 2009, IEEE T MULTIMEDIA, V11, P571, DOI 10.1109/TMM.2009.2012924
   Lee DY, 2009, IEEE IMAGE PROC, P3177, DOI 10.1109/ICIP.2009.5414397
   Lee H, 2016, IEEE T MULTIMEDIA, V18, P2093, DOI 10.1109/TMM.2016.2595262
   Liu XY, 2009, IEEE T SYST MAN CY B, V39, P539, DOI 10.1109/TSMCB.2008.2007853
   Liu YP, 2013, IEEE T BIO-MED ENG, V60, P2794, DOI 10.1109/TBME.2013.2264772
   Liu Z, 2012, IEEE T IMAGE PROCESS, V21, P4204, DOI 10.1109/TIP.2012.2200492
   Maddalena L., 2012, 2012 IEEE COMP SOC C, P21, DOI [10.1109/CVPRW.2012.6238922, DOI 10.1109/CVPRW.2012.6238922]
   Maddalena L, 2008, IEEE T IMAGE PROCESS, V17, P1168, DOI 10.1109/TIP.2008.924285
   Morde A., 2012, Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Conference on, P15
   Mould N, 2012, IEEE IMAGE PROC, P1233, DOI 10.1109/ICIP.2012.6467089
   NONAKA Y., 2012, Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Conference on, P9, DOI DOI 10.1109/CVPRW.2012.6238920
   Qin M, 2016, IEEE T MULTIMEDIA, V18, P1283, DOI 10.1109/TMM.2016.2557729
   Sabirin H, 2012, IEEE T MULTIMEDIA, V14, P657, DOI 10.1109/TMM.2012.2187777
   Sheikh Y, 2005, IEEE T PATTERN ANAL, V27, P1778, DOI 10.1109/TPAMI.2005.213
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Sobral A, 2014, COMPUT VIS IMAGE UND, V122, P4, DOI 10.1016/j.cviu.2013.12.005
   St-Charles PL, 2015, IEEE T IMAGE PROCESS, V24, P359, DOI 10.1109/TIP.2014.2378053
   St-Charles PL, 2014, IEEE COMPUT SOC CONF, P414, DOI 10.1109/CVPRW.2014.67
   Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677
   Sundaram N, 2010, LECT NOTES COMPUT SC, V6311, P438, DOI 10.1007/978-3-642-15549-9_32
   Toyama K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P255, DOI 10.1109/ICCV.1999.791228
   Van Droogenbroeck M., 2012, 2012 IEEE COMP SOC C, P32
   Wallace B. C., 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P754, DOI 10.1109/ICDM.2011.33
   Wang B, 2014, IEEE COMPUT SOC CONF, P401, DOI 10.1109/CVPRW.2014.64
   Wang R, 2014, IEEE COMPUT SOC CONF, P420, DOI 10.1109/CVPRW.2014.68
   Wang TH, 2012, IEEE T MULTIMEDIA, V14, P389, DOI 10.1109/TMM.2011.2177078
   Wang Y, 2014, IEEE COMPUT SOC CONF, P393, DOI 10.1109/CVPRW.2014.126
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Yang Q, 2006, INT J INF TECH DECIS, V5, P597, DOI 10.1142/S0219622006002258
   Yin P, 2011, IEEE T PATTERN ANAL, V33, P30, DOI 10.1109/TPAMI.2010.65
   Yin Z., 2007, IEEE Computer Vision and Pattern Recognition, P1, DOI DOI 10.1109/CVPR.2007.383237
   Yu T., 2007, P 11 IEEE INT C COMP, P1
   Zhang X., 2014, Int. J. Antennas Propag, V2014, P1, DOI DOI 10.4018/IJIIT.2014070101
   Zhang XG, 2013, IEEE T MULTIMEDIA, V15, P1769, DOI 10.1109/TMM.2013.2280117
   Zhang YQ, 2015, NEUROCOMPUTING, V168, P454, DOI 10.1016/j.neucom.2015.05.082
   Zhu QS, 2012, INT C PATT RECOG, P198
   Zivkovic Z, 2006, PATTERN RECOGN LETT, V27, P773, DOI 10.1016/j.patrec.2005.11.005
NR 69
TC 29
Z9 32
U1 2
U2 50
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2017
VL 19
IS 11
BP 2425
EP 2438
DI 10.1109/TMM.2017.2701645
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FJ9CT
UT WOS:000413068200006
DA 2024-07-18
ER

PT J
AU Lu, GY
   Nie, LQ
   Sorensen, S
   Kambhamettu, C
AF Lu, Guoyu
   Nie, Liqiang
   Sorensen, Scott
   Kambhamettu, Chandra
TI Large-Scale Tracking for Images With Few Textures
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Camera pose estimation; epipolar geometry; feature extraction; feature
   tracking; outlier rejection; 3D reconstruction
ID STRUCTURE-FROM-MOTION; LOCALIZATION; FEATURES; ORB
AB Image tracking provides crucial insight for the image motion, which generates essential information for incremental structure-from-motion reconstruction and camera pose estimation. Typical usages, such as 3D reconstruction and visual odometry, all rely on robust and accurate local feature tracking through consecutive images. Current algorithms realize feature tracking through matching features extracted from discriminant textures in the images, for which distinctive image content is required to obtain accurate feature matching. For images with few textures, usually, an insufficient number of features are extracted to perform reliable tracking in a series of sequential images. We propose a method that makes use of a limited number of discriminate features to explore other features without strong discriminant power. We develop a feature integrating surrounding salient points distribution knowledge, raw pixel value, and coordinate information to discover a significant amount of features in weakly textured areas in an image. We also incorporate epipolar geometry in the feature correspondence calculation by taking the distance from the matching candidate to its corresponding point's epipolar line into account. To reduce the number of unreliable features, we project the estimated 3D points back to the images. The reprojection error is standardized according to the 3D point's depth, which reduces the bias introduced by the object distance to the camera. We conduct experiments on a large dataset of Arctic sea ice images, mainly composed by planes of ices and sea water. The experimental results demonstrate that our method can perform fast and accurate tracking in weakly textured images.
C1 [Lu, Guoyu] Rochester Inst Technol, Chester F Carlson Ctr Imaging Sci, Rochester, NY 14623 USA.
   [Nie, Liqiang] Shandong Univ, Sch Comp Sci & Technol, Jinan 250101, Shandong, Peoples R China.
   [Sorensen, Scott; Kambhamettu, Chandra] Univ Delaware, Dept Comp & Informat Sci, Newark, DE 19716 USA.
C3 Rochester Institute of Technology; Shandong University; University of
   Delaware
RP Lu, GY (corresponding author), Rochester Inst Technol, Chester F Carlson Ctr Imaging Sci, Rochester, NY 14623 USA.
EM luguoyu62@gmail.com; nieliqiang@sdu.edu.cn; sorensen@udel.edu;
   chandrak@udel.edu
OI Lu, Guoyu/0000-0002-2685-5563
FU NSF CDI Type I [1124664]
FX This work was supported by NSF CDI Type I under Grant 1124664. The guest
   editor coordinating the review of this manuscript and approving it for
   publication was Mr. Jingkuan Song. (Corresponding author: Guoyu Lu.)
CR Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293
   [Anonymous], 2016, P BRIT MACH VIS C
   [Anonymous], 2004, P 2004WORKSHOP STAT
   [Anonymous], 2015, TRENDS ROBOTICS, DOI 10.1561/9781680830255
   Badino H, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P222, DOI 10.1109/ICCVW.2013.37
   Bai YL, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P229, DOI 10.1145/2647868.2656402
   Bay H., 2008, COMPUT VIS IMAGE UND, V10, P346, DOI DOI 10.1016/j.cviu.2007.09.014
   Biswas R, 2012, PROC TECH, V4, P820, DOI 10.1016/j.protcy.2012.05.134
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Buczko M, 2016, IEEE INT VEH SYM, P478, DOI 10.1109/IVS.2016.7535429
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Castle R, 2008, TWELFTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P15, DOI 10.1109/ISWC.2008.4911577
   Cheng ZQ, 2013, IEEE T VIS COMPUT GR, V19, P1885, DOI 10.1109/TVCG.2013.15
   Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961
   Dai YC, 2014, INT J COMPUT VISION, V107, P101, DOI 10.1007/s11263-013-0684-2
   Deng H., 2007, PROC IEEE C COMPUT V, P1
   Donoser M., 2006, COMPUTER VISION PATT, V1, P553, DOI DOI 10.1109/CVPR.2006.107
   Dosovitskiy A, 2016, IEEE T PATTERN ANAL, V38, P1734, DOI 10.1109/TPAMI.2015.2496141
   Duchenne O, 2011, IEEE T PATTERN ANAL, V33, P2383, DOI 10.1109/TPAMI.2011.110
   Farid H, 2004, IEEE T IMAGE PROCESS, V13, P496, DOI 10.1109/TIP.2004.823819
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fraundorfer F, 2012, IEEE ROBOT AUTOM MAG, V19, P78, DOI 10.1109/MRA.2012.2182810
   Gao Y, 2011, IEEE T MULTIMEDIA, V13, P1007, DOI 10.1109/TMM.2011.2160619
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Harris C., 1988, ALVEY VISION C, P147151
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Ke Y., 2004, ACM MULTIMEDIA 04, P869
   Kitt B, 2010, IEEE INT VEH SYM, P486, DOI 10.1109/IVS.2010.5548123
   Klein G, 2009, INT SYM MIX AUGMENT, P83, DOI 10.1109/ISMAR.2009.5336495
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Li HQ, 2013, IEEE T MULTIMEDIA, V15, P594, DOI 10.1109/TMM.2012.2234730
   Li J, 2013, IEEE T MULTIMEDIA, V15, P2058, DOI 10.1109/TMM.2013.2280127
   Lim H, 2012, PROC CVPR IEEE, P1043, DOI 10.1109/CVPR.2012.6247782
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu GY, 2015, IEEE I CONF COMP VIS, P2434, DOI 10.1109/ICCV.2015.280
   Lu GY, 2016, NEUROCOMPUTING, V173, P83, DOI 10.1016/j.neucom.2015.07.106
   Lu GY, 2015, MULTIMED TOOLS APPL, V74, P479, DOI 10.1007/s11042-014-1977-3
   Lucas B., 1981, Proc. DARPA Image Understanding Workshop, P121
   Middelberg S, 2014, LECT NOTES COMPUT SC, V8690, P268, DOI 10.1007/978-3-319-10605-2_18
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Murillo AC, 2007, IEEE INT CONF ROBOT, P3901, DOI 10.1109/ROBOT.2007.364077
   Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513
   Nistér D, 2005, MACH VISION APPL, V16, P321, DOI 10.1007/s00138-005-0006-y
   Radenovic F, 2016, LECT NOTES COMPUT SC, V9905, P3, DOI 10.1007/978-3-319-46448-0_1
   Rodriguez A. L., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3097, DOI 10.1109/CVPR.2011.5995569
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sang JT, 2013, IEEE T MULTIMEDIA, V15, P1665, DOI 10.1109/TMM.2013.2268052
   Saponaro P, 2014, IEEE IMAGE PROC, P1847, DOI 10.1109/ICIP.2014.7025370
   Scaramuzza D, 2011, IEEE ROBOT AUTOM MAG, V18, P80, DOI 10.1109/MRA.2011.943233
   Scaramuzza D, 2011, INT J COMPUT VISION, V95, P74, DOI 10.1007/s11263-011-0441-3
   Seitz S. M., 2006, 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), V1, P519
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Song J., 2011, P 19 ACM INT C MULT, P423, DOI DOI 10.1145/2072298.2072354
   Song JK, 2016, IEEE T MULTIMEDIA, V18, P484, DOI 10.1109/TMM.2016.2515990
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   Triggs B, 1996, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.1996.517170
   Triggs B., 2000, VISION ALGORITHMS TH, P298, DOI DOI 10.1007/3-540-44480-7_21THISWORKWASSUPPORTEDINPARTBYTHEEUROPEAN
   Veeraraghavan H, 2006, IEEE INT CONF ROBOT, P3393, DOI 10.1109/ROBOT.2006.1642220
   Wan J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P157, DOI 10.1145/2647868.2654948
   Wang XY, 2011, IEEE I CONF COMP VIS, P209, DOI 10.1109/ICCV.2011.6126244
   Wendel A., 2011, 2011 IEEE International Conference on Robotics and Automation (ICRA 2011), P5792, DOI 10.1109/ICRA.2011.5980317
   Wu CC, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P127, DOI 10.1109/3DV.2013.25
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Zhang SG, 2010, PROCEEDINGS OF THE ASME JOINT RAIL CONFERENCE, VOL 2, P501, DOI 10.1145/1873951.1874018
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zheng L, 2014, IEEE T IMAGE PROCESS, V23, P3368, DOI 10.1109/TIP.2014.2330763
NR 70
TC 12
Z9 13
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2017
VL 19
IS 9
BP 2117
EP 2128
DI 10.1109/TMM.2017.2731044
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5XC
UT WOS:000411244200014
OA Bronze
DA 2024-07-18
ER

PT J
AU Xu, ZX
   Chen, L
   Dai, YM
   Chen, GC
AF Xu, Zhenxing
   Chen, Ling
   Dai, Yimeng
   Chen, Gencai
TI A Dynamic Topic Model and Matrix Factorization-Based Travel
   Recommendation Method Exploiting Ubiquitous Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic topic model (DTM); geotagged photo; travel recommendation
ID SYSTEM
AB The vast volumes of community-contributed geotagged photos (CCGPs) available on the Web can be utilized to make travel location recommendations. The sparsity of user location interactions makes it difficult to learn travel preferences, because a user usually visits only a limited number of travel locations. Static topic models can be used to solve the sparsity problem by considering user travel topics. However, all travel histories of a user are regarded as one document drawn from a set of static topics, ignoring the evolving of topics and travel preferences. In this paper, we propose a dynamic topic model (DTM) and matrix factorization (MF)-based travel recommendation method. A DTM is used to obtain the temporally fine-grained topic distributions (i.e., implicit topic information) of users and locations. In addition, a large amount of explicit information is extracted from the metadata and visual contents of CCGPs, check-ins, and point of interest categories datasets. The information is used to obtain user-user and location-location similarity information, which is imposed as two regularization terms to constraint MF. The proposed method is evaluated on a publicly available Flickr dataset. Experimental results demonstrate that the proposed method can generate significantly superior recommendations compared to other state-of-the-art travel location recommendation studies.
C1 [Xu, Zhenxing; Chen, Ling; Chen, Gencai] Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Zhejiang, Peoples R China.
   [Dai, Yimeng] Univ Melbourne, Sch Comp & Informat Syst, Melbourne, Vic 3010, Australia.
C3 Zhejiang University; University of Melbourne
RP Chen, L (corresponding author), Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Zhejiang, Peoples R China.
EM xuzhenx-ing2010@163.com; lingchen@zju.edu.cn;
   yimengd@student.unimelb.edu.au; chengc@zju.edu.cn
RI Chen, Ling/AAY-3744-2020; Xu, zhenxing/KFR-5915-2024
FU Ministry of Industry and Information Technology of China
   [2010ZX01042-002-003-001]; China Knowledge Centre for Engineering
   Sciences and Technology [CKCEST-2014-1-5]; National Natural Science
   Foundation of China [60703040, 61332017]; Science and Technology
   Department of Zhejiang Province [2011C13042, 2015C33002]
FX This work was supported in part by the Ministry of Industry and
   Information Technology of China under Grant 2010ZX01042-002-003-001, in
   part by the China Knowledge Centre for Engineering Sciences and
   Technology under Grant CKCEST-2014-1-5, in part by the National Natural
   Science Foundation of China under Grant 60703040 and Grant 61332017, and
   in part by the Science and Technology Department of Zhejiang Province
   under Grant 2011C13042 and Grant 2015C33002. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Marco Bertini. (Corresponding author: Ling Chen.)
CR [Anonymous], 2009, P 18 INT C WORLD WID
   [Anonymous], THESIS
   [Anonymous], 2011, Proceedings of the 20th International Conference on World Wide Web-WWW'11, DOI [DOI 10.1145/1963405.1963443, 10.1145/1963405.1963443]
   Arase Y., 2010, Proceedings of the international conference on Multimedia, MM '10, (New York, NY, USA), P133
   Bao M, 2013, GEOINFORMATICS RESOU, P480, DOI DOI 10.1007/978-3-642-41908-9_49
   Blei D.M., 2006, INT C MACHINE LEARNI, DOI [DOI 10.1145/1143844.1143859, 10.1145/1143844.1143859]
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Chen YY, 2013, IEEE T MULTIMEDIA, V15, P1283, DOI 10.1109/TMM.2013.2265077
   Cheng A.-J., 2011, P 19 ACM INT C MULTI, P83
   Clements M, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P851
   Cong Zheng, 2016, Database Systems for Advanced Applications. 21st International Conference, DASFAA 2016. Proceedings: LNCS 9642, P348, DOI 10.1007/978-3-319-32025-0_22
   Ester M., 1996, KDD-96 Proceedings. Second International Conference on Knowledge Discovery and Data Mining, P226
   Gao HJ, 2015, AAAI CONF ARTIF INTE, P1721
   Hofmann T, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P289
   Hofmann T., 2003, P 26 ANN INT ACM SIG, P259, DOI DOI 10.1145/860435.860483
   Iwata T, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1427
   Jiang K, 2013, NEUROCOMPUTING, V119, P17, DOI 10.1016/j.neucom.2012.02.049
   Kefalas P, 2016, IEEE T KNOWL DATA EN, V28, P604, DOI 10.1109/TKDE.2015.2496344
   Kennedy Lyndon., 2007, Proceedings of the 15th International Conference on Multimedia, P631, DOI DOI 10.1145/1291233.1291384
   Kisilevich S, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P855, DOI 10.1007/978-0-387-09823-4_44
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Kurashima T., 2010, CIKM, P579, DOI DOI 10.1145/1871437.1871513
   Kurashima T, 2013, KNOWL INF SYST, V37, P37, DOI 10.1007/s10115-012-0580-z
   Levi Gil, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P34, DOI 10.1109/CVPRW.2015.7301352
   Li YZ, 2015, ADV MECH ENG, V7, DOI 10.1177/1687814015606301
   Lian DF, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P831, DOI 10.1145/2623330.2623638
   Liu B, 2015, IEEE T KNOWL DATA EN, V27, P1167, DOI 10.1109/TKDE.2014.2362525
   Lu X., 2010, Proceedings of the 18th ACM International Conference on Multimedia, Firenze Italy, 25 October 2010, DOI [19.1145/1873951.1873972, DOI 10.1145/1873951.1873972]
   Luo JB, 2011, MULTIMED TOOLS APPL, V51, P187, DOI 10.1007/s11042-010-0623-y
   Majid A, 2015, DATA KNOWL ENG, V95, P66, DOI 10.1016/j.datak.2014.11.001
   Majid A, 2013, INT J GEOGR INF SCI, V27, P662, DOI 10.1080/13658816.2012.696649
   Peng P, 2016, IEEE T KNOWL DATA EN, V28, P994, DOI 10.1109/TKDE.2015.2489647
   Qian XM, 2014, IEEE T KNOWL DATA EN, V26, P1763, DOI 10.1109/TKDE.2013.168
   Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127
   Rendle S, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2168752.2168771
   Shi Y, 2011, Proceedings of the 5th AAAI Conference on Weblogs and Social Media, V5, P622
   Shi Y, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2483669.2483680
   Wang C, 2008, P 24 C UNCERTAINTY A, P579
   Wang XY, 2015, IEEE T MULTIMEDIA, V17, P409, DOI 10.1109/TMM.2014.2385473
   Wei X, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2909
   Xu ZX, 2015, NEUROCOMPUTING, V155, P99, DOI 10.1016/j.neucom.2014.12.043
   Yin Z., 2011, SIAM, P980
   Yuan Q, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P363
   Zhang J., 2015, ACM SIGSPATIAL Special, V7, P26, DOI DOI 10.1145/2876480.2876486
   Zheng Y., 2009, WWW, P791, DOI [10.1145/1526709.1526816, DOI 10.1145/1526709.1526816]
   Zheng YT, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2168752.2168770
   Zheng YT, 2011, LECT NOTES COMPUT SC, V6523, P262
   Zheng YT, 2011, MULTIMED TOOLS APPL, V51, P77, DOI 10.1007/s11042-010-0630-z
   Zheng Y, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1889681.1889683
   Zheng Y, 2011, ACM T WEB, V5, DOI 10.1145/1921591.1921596
NR 50
TC 32
Z9 32
U1 2
U2 43
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2017
VL 19
IS 8
BP 1933
EP 1945
DI 10.1109/TMM.2017.2688928
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5VV
UT WOS:000411240400020
DA 2024-07-18
ER

PT J
AU Dehais, J
   Anthimopoulos, M
   Shevchik, S
   Mougiakakou, S
AF Dehais, Joachim
   Anthimopoulos, Marios
   Shevchik, Sergey
   Mougiakakou, Stavroula
TI Two-View 3D Reconstruction for Food Volume Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computer vision; diabetes; stereo vision; volume measurement
ID DIETARY ASSESSMENT; ALGORITHM; ACCURACY; CAMERA
AB The increasing prevalence of diet-related chronic diseases coupled with the ineffectiveness of traditional diet management methods have resulted in a need for novel tools to accurately and automatically assess meals. Recently, computer vision-based systems that use meal images to assess their content have been proposed. Food portion estimation is the most difficult task for individuals assessing their meals and it is also the least studied area. This paper proposes a three-stage system to calculate portion sizes using two images of a dish acquired by mobile devices. The first stage consists in understanding the configuration of the different views, after which a dense three-dimensional (3D) model is built from the two images; finally, this 3D model serves to extract the volume of the different items. The system was extensively tested on 77 real dishes of known volume, and achieved an average error of less than 10% in 5.5 seconds per dish. The proposed pipeline is computationally tractable and requires no user input, making it a viable option for fully automated dietary assessment.
C1 [Dehais, Joachim; Anthimopoulos, Marios; Shevchik, Sergey; Mougiakakou, Stavroula] Univ Bern, ARTORG Ctr Biomed Engn Res, CH-3008 Bern, Switzerland.
   [Anthimopoulos, Marios] Bern Univ Hosp, Inselspital, Dept Emergency Med, CH-3010 Bern, Switzerland.
   [Mougiakakou, Stavroula] Bern Univ Hosp, Inselspital, Dept Endocrinol Diabet & Clin Nutr, CH-3010 Bern, Switzerland.
C3 University of Bern; University of Bern; University Hospital of Bern;
   University of Bern; University Hospital of Bern
RP Dehais, J (corresponding author), Univ Bern, ARTORG Ctr Biomed Engn Res, CH-3008 Bern, Switzerland.
EM joachim.dehais@artorg.unibe.ch; marios.anthimopoulos@artorg.unibe.ch;
   sshevchik@gmail.com; stavroula.mougiakakou@artorg.unibe.ch
OI Dehais, Joachim/0000-0003-0814-8475; Mougiakakou,
   Stavroula/0000-0002-6355-9982
FU Bern University Hospital "Inselspital"; European Commission [286408]
FX This work was supported by the Bern University Hospital "Inselspital"
   and the European Commission Seventh Framework Programme
   (FP7-PEOPLE-2011-IAPP) under Grant 286408. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Liang Zhou.
CR Almaghrabi R, 2012, IEEE IMTC P, P366
   [Anonymous], OB SIT TRENDS
   [Anonymous], 2009, 2009 WORKSH APPL COM, DOI [DOI 10.1109/WACV.2009.5403087, 10.1109/wacv.2009.5403087]
   [Anonymous], 2000, Diabetes Res. Clin. Pract.
   [Anonymous], 2009, P BRIT MACH VIS C
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   Anthimopoulos Marios, 2015, J Diabetes Sci Technol, V9, P507, DOI 10.1177/1932296815580159
   Bally L., 2016, P DIAB CAR
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Beasley J, 2005, NUTRITION, V21, P672, DOI 10.1016/j.nut.2004.11.006
   Carter MC, 2013, BRIT J NUTR, V109, P539, DOI 10.1017/S0007114512001353
   Chen HC, 2013, MEAS SCI TECHNOL, V24, DOI 10.1088/0957-0233/24/10/105701
   De Berg M., 2008, Computational Geometry: Algorithms and Applications, V17
   Dehais J, 2015, LECT NOTES COMPUT SC, V9281, P433, DOI 10.1007/978-3-319-23222-5_53
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fitzgibbon A.W., 1998, SMILE Workshop on 3D Structure from Multiple Images of Large-Scale Environments, Freiburg, Germany, V1506, P154
   Godwin SL, 2004, J AM DIET ASSOC, V104, P585, DOI 10.1016/j.jada.2004.01.006
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Hernandez T., 2006, J FOOD COMPOS ANAL, V19, P14
   Hirschmüller H, 2009, IEEE T PATTERN ANAL, V31, P1582, DOI 10.1109/TPAMI.2008.221
   Kikunaga S, 2007, J NUTR SCI VITAMINOL, V53, P109, DOI 10.3177/jnsv.53.109
   Kong FY, 2012, PERVASIVE MOB COMPUT, V8, P147, DOI 10.1016/j.pmcj.2011.07.003
   Livingstone MBE, 2004, BRIT J NUTR, V92, pS213, DOI 10.1079/BJN20041169
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030
   Martin CK, 2009, IEEE ENG MED BIO, P6869, DOI 10.1109/IEMBS.2009.5333123
   Moisan L, 2012, IMAGE PROCESS ON LIN, V2, P56, DOI 10.5201/ipol.2012.mmm-oh
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Nistér D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17
   Pollefeys M., 1999, P IEEE INT C COMP VI, V1, P456
   Rhyner D, 2016, J MED INTERNET RES, V18, DOI 10.2196/jmir.5567
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   SCHOELLER DA, 1990, CAN J PHYSIOL PHARM, V68, P941, DOI 10.1139/y90-143
   Van Meerbergen G, 2002, INT J COMPUT VISION, V47, P275, DOI 10.1023/A:1014562312225
   Xu C, 2013, IEEE IMAGE PROC, P2534, DOI 10.1109/ICIP.2013.6738522
   Zabih R., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P151, DOI 10.1007/BFb0028345
NR 37
TC 86
Z9 90
U1 1
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2017
VL 19
IS 5
BP 1090
EP 1099
DI 10.1109/TMM.2016.2642792
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5XS
UT WOS:000404056000016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liang, JQ
   Hu, QH
   Wang, WW
   Han, YH
AF Liang, Jianqing
   Hu, Qinghua
   Wang, Wenwu
   Han, Yahong
TI Semisupervised Online Multikernel Similarity Learning for Image
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retrieval; metric learning; multikernel learning; online
   multikernel similarity (OMKS); similarity learning; semisupervised;
   semisupervised online multikernel similarity (SSOMKS)
ID CLASSIFICATION
AB Metric learning plays a fundamental role in the fields of multimedia retrieval and pattern recognition. Recently, an online multikernel similarity (OMKS) learning method has been presented for content-based image retrieval (CBIR), which was shown to be promising for capturing the intrinsic nonlinear relations within multimodal features from large-scale data. However, the similarity function in this method is learned only from labeled images. In this paper, we present a new framework to exploit unlabeled images and develop a semisupervised OMKS algorithm. The proposed method is a multistage algorithm consisting of feature selection, selective ensemble learning, active sample selection, and triplet generation. The novel aspects of our work are the introduction of classification confidence to evaluate the labeling process and select the reliably labeled images to train the metric function, and a method for reliable triplet generation, where a new criterion for sample selection is used to improve the accuracy of label prediction for unlabeled images. Our proposed method offers advantages in challenging scenarios, in particular, for a small set of labeled images with high-dimensional features. Experimental results demonstrate the effectiveness of the proposed method as compared with several baseline methods.
C1 [Liang, Jianqing; Hu, Qinghua; Han, Yahong] Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
   [Wang, Wenwu] Univ Surrey, Fac Engn & Phys Sci, Dept Elect Engn, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
C3 Tianjin University; University of Surrey
RP Liang, JQ (corresponding author), Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
EM liangjianqing@tju.edu.cn; huqinghua@tju.edu.cn; w.wang@surrey.ac.uk;
   yahong@tju.edu.cn
RI Hu, Qinghua/B-8857-2008; wang, wenwu/HOF-4371-2023
OI Liang, Jianqing/0000-0002-1461-2329; Wang, Wenwu/0000-0002-8393-5703
FU National Program on Key Basic Research Project [2013CB329304]; National
   Natural Science Foundation of China [61222210, 61432011, U1435212]; New
   Century Excellent Talents in University [NCET-12-0399]
FX This work was supported in part by the National Program on Key Basic
   Research Project under Grant 2013CB329304, and in part by the National
   Natural Science Foundation of China under Grant 61222210, Grant
   61432011, and Grant U1435212. The work of Q. Hu was supported by the New
   Century Excellent Talents in University under Grant NCET-12-0399. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Marco Bertini.
CR Ahn H, 2007, COMPUT STAT DATA AN, V51, P6166, DOI 10.1016/j.csda.2006.12.043
   [Anonymous], 2005, ADV NEURAL INF PROCE
   [Anonymous], 2002, ADV NEURAL INF PROCE
   [Anonymous], IEEE T PATT IN PRESS
   [Anonymous], 2006, P 2006 IEEE COMPUTER, DOI DOI 10.1109/CVPR.2006.167
   [Anonymous], 2004, Advances in neural information processing systems (NIPS)
   Bar-Hillel A., 2003, ICML, P11
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Bellet A., 2013, CoRR
   Blewitt ME, 2008, NAT GENET, V40, P663, DOI 10.1038/ng.142
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Cai D., 2010, KDD, P333
   Chechik G, 2010, J MACH LEARN RES, V11, P1109
   Davis J. V., 2007, ICML, P209
   Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067
   Freund Y., 1996, INT C MACHINE LEARNI, P148
   Hoi SCH, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1823746.1823752
   Jain P, 2012, J MACH LEARN RES, V13, P519
   Jiang N, 2012, PROC CVPR IEEE, P1956, DOI 10.1109/CVPR.2012.6247897
   Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121
   Kunapuli Gautam, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P859, DOI 10.1007/978-3-642-33460-3_60
   Lam L, 1997, IEEE T SYST MAN CY A, V27, P553, DOI 10.1109/3468.618255
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Li LJ, 2014, PATTERN RECOGN, V47, P3120, DOI 10.1016/j.patcog.2014.03.021
   Lin Kevin, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P27, DOI 10.1109/CVPRW.2015.7301269
   Liu J., 2013, SLEP: Sparse Learning with Efficient Projections
   Lu X, 2016, NEURAL PROCESS LETT, V43, P905, DOI 10.1007/s11063-015-9444-3
   Lu ZY, 2013, INT CONF MACH LEARN, P583, DOI 10.1109/ICMLC.2013.6890359
   McFee B, 2011, J MACH LEARN RES, V12, P491
   Niu G, 2014, NEURAL COMPUT, V26, P1717, DOI 10.1162/NECO_a_00614
   Qian Q, 2015, MACH LEARN, V99, P353, DOI 10.1007/s10994-014-5456-x
   RAO CR, 1948, J ROY STAT SOC B, V10, P159
   Tao DC, 2006, IEEE T PATTERN ANAL, V28, P1088, DOI 10.1109/TPAMI.2006.134
   van Gestel T, 2004, MACH LEARN, V54, P5, DOI 10.1023/B:MACH.0000008082.80494.e0
   Vapnik V., 1982, Springer Series in Statistics
   Wang F, 2015, DATA MIN KNOWL DISC, V29, P534, DOI 10.1007/s10618-014-0356-z
   Wang J., 2012, NEURIPS, V25, P1601
   Wang J, 2014, PR MACH LEARN RES, V32, P370
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Wu P., 2013, P 21 ACM INT C MULT, P153, DOI DOI 10.1145/2502081.2502112
   Wu PC, 2016, IEEE T KNOWL DATA EN, V28, P454, DOI 10.1109/TKDE.2015.2477296
   Xia H, 2014, IEEE T PATTERN ANAL, V36, P536, DOI 10.1109/TPAMI.2013.149
   Xiang SM, 2012, IEEE T NEUR NET LEAR, V23, P1738, DOI 10.1109/TNNLS.2012.2212721
   Xie Pengtao., 2013, Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, P1806
   Xu B, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P525
   Xu C, 2015, IEEE T PATTERN ANAL, V37, P2531, DOI 10.1109/TPAMI.2015.2417578
   Xu C, 2014, IEEE T PATTERN ANAL, V36, P1559, DOI 10.1109/TPAMI.2013.2296528
   Ying YM, 2012, J MACH LEARN RES, V13, P1
   Zadeh PH, 2016, PR MACH LEARN RES, V48
   Zha ZJ, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1327
   Zhou ZH, 2002, ARTIF INTELL, V137, P239, DOI 10.1016/S0004-3702(02)00190-X
NR 51
TC 17
Z9 18
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2017
VL 19
IS 5
BP 1077
EP 1089
DI 10.1109/TMM.2016.2644862
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5XS
UT WOS:000404056000015
DA 2024-07-18
ER

PT J
AU Nie, XS
   Yin, YL
   Sun, JD
   Liu, J
   Cui, CR
AF Nie, Xiushan
   Yin, Yilong
   Sun, Jiande
   Liu, Ju
   Cui, Chaoran
TI Comprehensive Feature-Based Robust Video Fingerprinting Using Tensor
   Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Comprehensive feature; robust video fingerprinting; near-duplicate video
   detection (NDVD); multiple features
ID COPY DETECTION
AB Content-based near-duplicate video detection (NDVD) is essential for effective search and retrieval, and robust video fingerprinting is a good solution for NDVD. Most existing video fingerprinting methods use a single feature or concatenate different features to generate video fingerprints, and show good performance under single-mode modifications such as noise addition and blurring. However, when they suffer combined modifications, the performance is degraded to a certain extent because such features cannot characterize the video content completely. By contrast, the assistance and consensus among different features can improve the performance of video fingerprinting. Therefore, in the present study, we mine the assistance and consensus among different features based on a tensor model, and we present a new comprehensive feature to fully use them in the proposed video fingerprinting framework. We also analyze what the comprehensive feature really is for representing the original video. In this framework, the video is initially set as a high-order tensor that consists of different features, and the video tensor is decomposed via the Tucker model with a solution that determines the number of components. Subsequently, the comprehensive feature is generated by the low-order tensor obtained from tensor decomposition. Finally, the video fingerprint is computed using this feature. A matching strategy used for narrowing the search is also proposed based on the core tensor. The robust video fingerprinting framework is resistant not only to single-mode modifications but also to their combination.
C1 [Nie, Xiushan; Cui, Chaoran] Shandong Univ Finance & Econ, Sch Comp Sci & Technol, Jinan 250014, Shandong, Peoples R China.
   [Yin, Yilong; Liu, Ju] Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Shandong, Peoples R China.
   [Yin, Yilong; Liu, Ju] Shandong Univ, Sch Informat Sci & Engn, Jinan 250100, Shandong, Peoples R China.
   [Sun, Jiande] Shandong Normal Univ, Inst Data Sci & Technol, Jinan 250014, Shandong, Peoples R China.
C3 Shandong University of Finance & Economics; Shandong University;
   Shandong University; Shandong Normal University
RP Yin, YL (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Shandong, Peoples R China.; Yin, YL (corresponding author), Shandong Univ, Sch Informat Sci & Engn, Jinan 250100, Shandong, Peoples R China.
EM niexsh@sdufe.edu.cn; ylyin@sdu.edu.cn; jiandesun@hotmail.com;
   juliu@sdu.edu.cn; bruincui@foxmail.com
RI Nie, Xiushan/AAZ-6410-2020; Sun, Jiande/B-4681-2018
FU National Natural Science Foundation of China [61671274, 61573219]; NSFC
   Joint Fund with Guangdong under Key Project [U1201258]; Shandong Natural
   Science Funds for Distinguished Young Scholar [JQ201316]; Natural
   Science Foundation of Shandong Province [ZR2014FM012]; Fostering Project
   of Dominant Discipline and Talent Team of Shandong Province Higher
   Education Institutions
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61671274 and Grant 61573219, in part by
   the NSFC Joint Fund with Guangdong under Key Project U1201258, in part
   by Shandong Natural Science Funds for Distinguished Young Scholar under
   Grant JQ201316, in part by the Natural Science Foundation of Shandong
   Province under Grant ZR2014FM012, and in part by the Fostering Project
   of Dominant Discipline and Talent Team of Shandong Province Higher
   Education Institutions. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Winston Hsu.
   (Corresponding author: Yilong Yin.)
CR [Anonymous], 2010, CBCD EVALUATION PLAN
   Awad G, 2014, ACM T INFORM SYST, V32, DOI 10.1145/2629531
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Cai JJ, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P307, DOI 10.1145/2671188.2749320
   Ceulemans E, 2006, BRIT J MATH STAT PSY, V59, P133, DOI 10.1348/000711005X64817
   Chen L, 2008, PATTERN RECOGN LETT, V29, P1824, DOI 10.1016/j.patrec.2008.05.015
   Cirakman O, 2014, MULTIMED TOOLS APPL, V71, P1381, DOI 10.1007/s11042-012-1269-8
   Comon P, 2009, J CHEMOMETR, V23, P393, DOI 10.1002/cem.1236
   Coskun B, 2006, IEEE T MULTIMEDIA, V8, P1190, DOI 10.1109/TMM.2006.884614
   Dasgupta S, 2002, ADV NEUR IN, V14, P375
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1324, DOI 10.1137/S0895479898346995
   Esmaeili MM, 2011, IEEE T INF FOREN SEC, V6, P213, DOI 10.1109/TIFS.2010.2097593
   Hampapur A, 2002, P SOC PHOTO-OPT INS, V4676, P194
   Hansen L. B., 2006, Proceedings of the 8th World Congress on Genetics Applied to Livestock Production, Belo Horizonte, Minas Gerais, Brazil, 13-18 August, 2006, P01
   Jinchi Cai, 2015, 2015 IEEE International Vacuum Electronics Conference (IVEC), P1, DOI 10.1109/IVEC.2015.7223891
   Joly A, 2007, IEEE T MULTIMEDIA, V9, P293, DOI 10.1109/TMM.2006.886278
   Kim C, 2005, IEEE T CIRC SYST VID, V15, P127
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   KROONENBERG PM, 1980, PSYCHOMETRIKA, V45, P69, DOI 10.1007/BF02293599
   Lee S, 2008, IEEE T CIRC SYST VID, V18, P983, DOI 10.1109/TCSVT.2008.920739
   Li M., 2011, P IEEE RAD FREQ INT, P1, DOI DOI 10.1109/RFIC.2011.5940608
   Li M, 2013, IEEE T INF FOREN SEC, V8, P1709, DOI 10.1109/TIFS.2013.2278100
   Li M, 2012, IEEE T IMAGE PROCESS, V21, P4397, DOI 10.1109/TIP.2012.2206036
   Li Y. N., 2010, COPY DETECTION VISUA
   Liu XC, 2013, IEEE SIGNAL PROC LET, V20, P1253, DOI 10.1109/LSP.2013.2287006
   Menglin Jiang, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P374, DOI 10.1109/ICME.2012.189
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Morup M., 2008, THESIS
   Morup M, 2011, WIRES DATA MIN KNOWL, V1, P24, DOI 10.1002/widm.1
   Morup M, 2009, J CHEMOMETR, V23, P352, DOI 10.1002/cem.1223
   Mou L. T., 2011, ACM T MULTIM COMPUT, V10
   Nie XS, 2014, MULTIMED TOOLS APPL, V69, P429, DOI 10.1007/s11042-012-1341-4
   Nie XS, 2013, SCI CHINA INFORM SCI, V56, DOI 10.1007/s11432-012-4760-y
   Shen H.T., 2007, VLDB, P1374
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Wei SK, 2011, IEEE T CIRC SYST VID, V21, P15, DOI 10.1109/TCSVT.2011.2105554
   Wiselin D. G., 2012, INT J EMERGING TECHN, V2, P293
   Wu A. G., 2007, P ACM MM, P218
   Wu F, 2009, IEEE T MULTIMEDIA, V11, P868, DOI 10.1109/TMM.2009.2021724
   Xu C, 2013, ARXIV PREPRINT ARXIV
   Yang GB, 2012, COMPUT SECUR, V31, P33, DOI 10.1016/j.cose.2011.11.004
NR 41
TC 45
Z9 47
U1 1
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2017
VL 19
IS 4
BP 785
EP 796
DI 10.1109/TMM.2016.2629758
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EO0NT
UT WOS:000396395500010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, ZX
   Lu, JW
   Feng, JJ
   Zhou, J
AF Chen, Zhixiang
   Lu, Jiwen
   Feng, Jianjiang
   Zhou, Jie
TI Nonlinear Discrete Hashing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Binary code; discrete optimization; hashing; multilayer neural network;
   nonlinear transformation
ID LEARNING BINARY-CODES; QUANTIZATION; SCENE
AB In this paper, we propose a nonlinear discrete hashing approach to learn compact binary codes for scalable image search. Instead of seeking a single linear projection in most existing hashing methods, we pursue a multilayer network with nonlinear transformations to capture the local structure of data samples. Unlike most existing hashing methods that adopt an error-prone relaxation to learn the transformations, we directly solve the discrete optimization problem to eliminate the quantization error accumulation. Specifically, to leverage the similarity relationships between data samples and exploit the semantic affinities of manual labels, the binary codes are learned with the objective to: 1) minimize the quantization error between the original data samples and the learned binary codes; 2) preserve the similarity relationships in the learned binary codes; 3) maximize the information content with independent bits; and 4) maximize the accuracy of the predicted labels based on the binary codes. With an alternating optimization, the nonlinear transformation and the discrete quantization are jointly optimized in the hashing learning framework. Experimental results on four datasets including CIFAR10, MNIST, SUN397, and ILSVRC2012 demonstrate that the proposed approach is superior to several state-of-the-art hashing methods.
C1 [Chen, Zhixiang; Lu, Jiwen; Feng, Jianjiang; Zhou, Jie] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Lu, JW (corresponding author), Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
EM chen-zx10@mails.tsinghua.edu.cn; lujiwen@tsinghua.edu.cn;
   jfeng@tsinghua.edu.cn; jzhou@tsinghua.edu.cn
RI Feng, Jianjiang/I-3386-2012; Lu, Jiwen/C-5291-2009
OI Chen, Zhixiang/0000-0002-5636-6082; Lu, Jiwen/0000-0002-6121-5529
FU National Key Research and Development Program of China [2016YFB1001001];
   National Natural Science Foundation of China [61225008, 61672306,
   61572271, 61527808, 61373074, 61373090]; National 1000 Young Talents
   Plan Program; National Basic Research Program of China [2014CB349304];
   Ministry of Education of China [20120002110033]; Tsinghua University
   Initiative Scientific Research Program
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2016YFB1001001, in part by the
   National Natural Science Foundation of China under Grant 61225008, Grant
   61672306, Grant 61572271, Grant 61527808, Grant 61373074, and Grant
   61373090, in part by the National 1000 Young Talents Plan Program, in
   part by the National Basic Research Program of China under Grant
   2014CB349304, in part by the Ministry of Education of China under Grant
   20120002110033, and in part by the Tsinghua University Initiative
   Scientific Research Program. The guest editor coordinating the review of
   this manuscript and approving it for publication was Dr. Nan Cao.
   (Corresponding author: Jiwen Lu.)
CR Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   [Anonymous], P INT C MACH LEARN
   [Anonymous], 2009, NIPS
   [Anonymous], 2009, NEURIPS
   [Anonymous], 2013, NeurIPS
   Carreira-Perpiñán MA, 2015, PROC CVPR IEEE, P557, DOI 10.1109/CVPR.2015.7298654
   Charikar Moses S., 2002, P 34 ANN ACM S THEOR, P380, DOI DOI 10.1145/509907.509965
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donahue J, 2014, PR MACH LEARN RES, V32
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Gong YC, 2013, PROC CVPR IEEE, P484, DOI 10.1109/CVPR.2013.69
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378
   Ji JQ, 2014, IEEE T PATTERN ANAL, V36, P1963, DOI 10.1109/TPAMI.2014.2315806
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kim S, 2015, PROC CVPR IEEE, P1338, DOI 10.1109/CVPR.2015.7298739
   Kong W., 2012, P 26 AAAI C ART INT, P634, DOI DOI 10.5555/2900728.2900819
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   Li Deng, 2012, IEEE Signal Processing Magazine, V29, P141, DOI [10.1109/MSP.2012.2211477, DOI 10.1109/MSP.2012.2211477]
   Lin GS, 2014, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2014.253
   Lin GS, 2013, IEEE I CONF COMP VIS, P2552, DOI 10.1109/ICCV.2013.317
   Lin GS, 2014, LECT NOTES COMPUT SC, V8691, P613, DOI 10.1007/978-3-319-10578-9_40
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu W., 2014, P NEURAL INF PROCESS, P3419
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu Wei, 2011, Reports in Parasitology, V1, P1
   Mu YD, 2010, PROC CVPR IEEE, P3344, DOI 10.1109/CVPR.2010.5540024
   Norouzi M.E., 2011, ICML
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Rastegari M, 2015, PROC CVPR IEEE, P1501, DOI 10.1109/CVPR.2015.7298757
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen FM, 2013, PROC CVPR IEEE, P1562, DOI 10.1109/CVPR.2013.205
   Shi QF, 2009, J MACH LEARN RES, V10, P2615
   Strecha C, 2012, IEEE T PATTERN ANAL, V34, P66, DOI 10.1109/TPAMI.2011.103
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Weiss Y., 2008, ADV NEURAL INFORM PR, V21, P1753
   Weng L, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P259, DOI 10.1145/2671188.2749291
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
NR 44
TC 42
Z9 44
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2017
VL 19
IS 1
BP 123
EP 135
DI 10.1109/TMM.2016.2620604
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EH0SX
UT WOS:000391475200010
DA 2024-07-18
ER

PT J
AU Lalos, AS
   Nikolas, I
   Vlachos, E
   Moustakas, K
AF Lalos, Aris S.
   Nikolas, Iason
   Vlachos, Evangelos
   Moustakas, Konstantinos
TI Compressed Sensing for Efficient Encoding of Dense 3D Meshes Using
   Model-Based Bayesian Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
AB With the growing demand for easy and reliable generation of 3D models representing real-world or synthetic objects, new schemes for acquisition, storage, and transmission of 3D meshes are required. In principle, 3D meshes consist of vertex positions and vertex connectivity. Vertex position encoders are much more resource demanding than connectivity encoders, stressing the need for novel geometry compression schemes. The design of an accurate and efficient geometry compression system can be achieved by increasing the compression ratio without affecting the visual quality of the object and minimizing the computational complexity. In this paper, we present novel compression/reconstruction schemes that enable aggressive compression ratios, without significantly reducing the visual quality. The encoding is performed by simply executing additions/subtractions. The benefits of the proposed method become more apparent as the density of the meshes increases, while it provides a flexible framework to trade efficiency for reconstruction quality. We derive a novel Bayesian learning algorithm that models the most significant graph Fourier transform coefficients of each submesh, as a multivariate Gaussian distribution. Then we evaluate iteratively the distribution parameters using the expectation-maximization approach. To improve the performance of the proposed approach in highly under determined problems, we exploit the local smoothness of the partitioned surfaces. Extensive evaluation studies, carried out using a large collection of different 3D models, show that the proposed schemes, as compared to the state-of-the-art approaches, achieve competitive compression ratios, offering at the same time significantly lower encoding complexity.
C1 [Lalos, Aris S.; Nikolas, Iason; Moustakas, Konstantinos] Univ Patras, Elect & Comp Engn Dept, Patras 26504, Greece.
   [Vlachos, Evangelos] Univ Patras, Comp Engn & Informat Dept, Patras 26504, Greece.
C3 University of Patras; University of Patras
RP Lalos, AS (corresponding author), Univ Patras, Elect & Comp Engn Dept, Patras 26504, Greece.
EM aris.lalos@ece.upatras.gr; iason.nikolas@ece.upatras.gr;
   vlaxose@ceid.upatras.gr; moustakas@ece.upatras.gr
RI Lalos, Aris/W-6443-2019; Vlachos, Evangelos/Y-9808-2019
OI Lalos, Aris/0000-0003-0511-9302; Vlachos, Evangelos/0000-0003-1501-0722;
   Moustakas, Konstantinos/0000-0001-7617-227X
FU Greek Secretariat for Research and Technology Bilateral Collaboration
   Project MOMI-RAS [ISR_3215]; Project MyAirCoach [H2020-PHC-2014-2015,
   643607]
FX This work was supported by the Greek Secretariat for Research and
   Technology Bilateral Collaboration Project MOMI-RAS (ISR_3215) and by
   the H2020-PHC-2014-2015 Project MyAirCoach under Grant 643607. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Cha Zhang.
CR Abdulghani AM, 2010, IEEE ENG MED BIO, P1127, DOI 10.1109/IEMBS.2010.5627119
   Alexiadis DS, 2013, IEEE T MULTIMEDIA, V15, P339, DOI 10.1109/TMM.2012.2229264
   Alliez P, 2001, COMPUT GRAPH FORUM, V20, pC480, DOI 10.1111/1467-8659.00541
   [Anonymous], 2005, State of the Art Reports
   Cai W, 2013, 2013 IEEE SEVENTH INTERNATIONAL SYMPOSIUM ON SERVICE-ORIENTED SYSTEM ENGINEERING (SOSE 2013), P551, DOI 10.1109/SOSE.2013.30
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   Cayre F, 2003, SIGNAL PROCESS-IMAGE, V18, P309, DOI 10.1016/S0923-5965(02)00147-9
   Chen D, 2005, ACM T GRAPHIC, V24, P1259, DOI 10.1145/1095878.1095880
   Deering M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P13, DOI 10.1145/218380.218391
   Deng CW, 2012, IEEE T MULTIMEDIA, V14, P278, DOI 10.1109/TMM.2011.2181491
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Gumhold S., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P133, DOI 10.1145/280814.280836
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   Kumar K, 2010, COMPUTER, V43, P51, DOI 10.1109/MC.2010.98
   Levy Bruno., 2010, ACM SIGGRAPH 2010 Courses, P8
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Maglo A, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2693443
   Mamou K, 2009, IEEE IMAGE PROC, P3513, DOI 10.1109/ICIP.2009.5414075
   Mekuria R, 2014, IEEE T MULTIMEDIA, V16, P1809, DOI 10.1109/TMM.2014.2331919
   Pudlewski S, 2013, IEEE T MULTIMEDIA, V15, P2072, DOI 10.1109/TMM.2013.2280245
   Rachlin Y, 2008, ANN ALLERTON CONF, P813, DOI 10.1109/ALLERTON.2008.4797641
   Rossignac J, 1999, IEEE T VIS COMPUT GR, V5, P47, DOI 10.1109/2945.764870
   Rubinstein R, 2010, P IEEE, V98, P1045, DOI 10.1109/JPROC.2010.2040551
   Taubin G, 1998, ACM T GRAPHIC, V17, P84, DOI 10.1145/274363.274365
   Touma C, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P26
   Wang SX, 2013, IEEE T MULTIMEDIA, V15, P870, DOI 10.1109/TMM.2013.2240674
   Weidong Shi, 2010, 2010 IEEE 3rd International Conference on Cloud Computing (CLOUD 2010), P346, DOI 10.1109/CLOUD.2010.76
   Zhang H, 2010, COMPUT GRAPH FORUM, V29, P1865, DOI 10.1111/j.1467-8659.2010.01655.x
   Zhu WW, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940269
   Zhuo-ming D., 2011, P INT C EL COMM CONT, P601
NR 32
TC 16
Z9 20
U1 1
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2017
VL 19
IS 1
BP 41
EP 53
DI 10.1109/TMM.2016.2605927
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EH0SX
UT WOS:000391475200004
DA 2024-07-18
ER

PT J
AU Liao, HS
   Chen, L
   Song, YB
   Ming, H
AF Liao, Hongsen
   Chen, Li
   Song, Yibo
   Ming, Hao
TI Visualization-Based Active Learning for Video Annotation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Active learning; projection; video annotation; visualization
AB Video annotation is an effective way to facilitate content-based analysis for videos. Automatic machine learning methods are commonly used to accomplish this task. Among these, active learning is one of the most effective methods, especially when the training data cost a great deal to obtain. One of the most challenging problems in active learning is the sample selection. Various sampling strategies can be used, such as uncertainty, density, and diversity, but it is difficult to strike a balance among them. In this paper, we provide a visualization-based batch mode sampling method to handle such a problem. An iso-contour-based scatterplot is used to provide intuitive clues for the representativeness and informativeness of samples and assist users in sample selection. A semisupervised metric learning method is incorporated to help generate an effective scatterplot reflecting the high-level semantic similarity for visual sample selection. Moreover, both quantitative and qualitative evaluations are provided to show that the visualization-based method can effectively enhance sample selection in active learning.
C1 [Liao, Hongsen; Chen, Li; Song, Yibo; Ming, Hao] Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Chen, L (corresponding author), Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
EM liaohs082@gmail.com; chenlee@tsinghua.edu.cn;
   songyb15@mails.tsinghua.edu.cn; heroming7788@gmail.com
OI Liao, Hongsen/0000-0003-2491-968X
FU National Natural Science Foundation of China [61272225, 61572274,
   91515103, 51261120376]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61272225, Grant 61572274, Grant
   91515103, and Grant 51261120376. The guest editor coordinating the
   review of this manuscript and approving it for publication was Dr. Nan
   Cao. (Corresponding author: Li Chen.)
CR [Anonymous], 1978, MULTIDIMENSIONAL SCA
   [Anonymous], 2008, IEEE C COMP VIS PATT
   Babaee M, 2015, IEEE J-STARS, V8, P4687, DOI 10.1109/JSTARS.2015.2388496
   Brooks M, 2015, IEEE CONF VIS ANAL, P105, DOI 10.1109/VAST.2015.7347637
   Choo J, 2013, IEEE T VIS COMPUT GR, V19, P1992, DOI 10.1109/TVCG.2013.212
   Cohn DA, 1996, J ARTIF INTELL RES, V4, P129, DOI 10.1613/jair.295
   Cui QG, 2006, IEEE T VIS COMPUT GR, V12, P709, DOI 10.1109/TVCG.2006.161
   Dagli CK, 2006, LECT NOTES COMPUT SC, V4071, P123
   Gosselin P., 2004, Proceedings of the 1st international workshop on Computer vision meets databases, P51
   Heimerl F, 2012, IEEE T VIS COMPUT GR, V18, P2839, DOI 10.1109/TVCG.2012.277
   Hoi SCH, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1823746.1823752
   Hossain MS, 2012, IEEE T VIS COMPUT GR, V18, P2829, DOI 10.1109/TVCG.2012.258
   Huang SJ, 2014, IEEE T PATTERN ANAL, V36, P1936, DOI 10.1109/TPAMI.2014.2307881
   Jaegul Choo, 2010, 2010 Proceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2010), P27, DOI 10.1109/VAST.2010.5652443
   Joshi AJ, 2009, PROC CVPR IEEE, P2364
   Legg PA, 2013, IEEE T VIS COMPUT GR, V19, P2109, DOI 10.1109/TVCG.2013.207
   Liu JG, 2009, PROC CVPR IEEE, P1996
   Ming-yu Chen, 2005, 13th Annual ACM International Conference on Multimedia, P902, DOI 10.1145/1101149.1101342
   Paiva JGS, 2015, IEEE T VIS COMPUT GR, V21, P4, DOI 10.1109/TVCG.2014.2331979
   Paulovich FV, 2008, IEEE T VIS COMPUT GR, V14, P564, DOI 10.1109/TVCG.2007.70443
   Roy Nicholas, 2001, Toward optimal active learning through monte carlo estimation of error reduction, P441
   Settles B., 2009, University of Wisconsin-Madison Computer Sciences Technical Report 1648
   Settles B, 2008, P C EMP METH NAT LAN, P1070, DOI DOI 10.3115/1613715.1613855
   Tong S, 2002, J MACH LEARN RES, V2, P45, DOI 10.1162/153244302760185243
   Tong S, 2001, P 9 ACM INT C MULT, P107, DOI DOI 10.1145/500141.500159
   van den Elzen S., 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P151, DOI 10.1109/VAST.2011.6102453
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Van Kerm P, 2003, STATA J, V3, P148, DOI 10.1177/1536867X0300300204
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang M, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1899412.1899414
   Wu Y, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P529, DOI 10.1109/ICME.2006.262442
   Xu ZB, 2007, LECT NOTES COMPUT SC, V4425, P246
   Yang Y, 2015, INT J COMPUT VISION, V113, P113, DOI 10.1007/s11263-014-0781-x
   Zhang C, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P595
NR 34
TC 20
Z9 21
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2016
VL 18
IS 11
BP 2196
EP 2205
DI 10.1109/TMM.2016.2614227
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EA9BX
UT WOS:000386936900007
DA 2024-07-18
ER

PT J
AU Zhang, Z
   He, ZH
   Cao, GT
   Cao, WM
AF Zhang, Zhi
   He, Zhihai
   Cao, Guitao
   Cao, Wenming
TI Animal Detection From Highly Cluttered Natural Scenes Using
   Spatiotemporal Object Region Proposals and Patch Verification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Background modeling; camera-trap images; graph cut; object proposal;
   object verification
AB In this paper, we consider the animal object detection and segmentation from wildlife monitoring videos captured by motion-triggered cameras, called camera-traps. For these types of videos, existing approaches often suffer from low detection rates due to low contrast between the foreground animals and the cluttered background, as well as high false positive rates due to the dynamic background. To address this issue, we first develop a new approach to generate animal object region proposals using multilevel graph cut in the spatiotemporal domain. We then develop a cross-frame temporal patch verification method to determine if these region proposals are true animals or background patches. We construct an efficient feature description for animal detection using joint deep learning and histogram of oriented gradient features encoded with Fisher vectors. Our extensive experimental results and performance comparisons over a diverse set of challenging camera-trap data demonstrate that the proposed spatiotemporal object proposal and patch verification framework outperforms the state-of-the-art methods, including the recent Faster-RCNN method, on animal object detection accuracy by up to 4.5%.
C1 [Zhang, Zhi; He, Zhihai] Univ Missouri, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
   [Cao, Guitao] East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai 200062, Peoples R China.
   [Cao, Wenming] Shenzhen Univ, Coll Informat Engn, Shenzhen 518060, Peoples R China.
C3 University of Missouri System; University of Missouri Columbia; East
   China Normal University; Shenzhen University
RP Zhang, Z (corresponding author), Univ Missouri, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
EM zzbhf@mail.missouri.edu; hezhi@missouri.edu; gtcao@sei.ecnu.edu.cn;
   caom@shenzhenu.edu.cn
RI He, Zhihai/A-5885-2019
FU National Science Foundation [CyberSEES-1539389, CPS-1544794]; National
   Science Foundation of China [61375015]; Direct For Computer & Info Scie
   & Enginr; Division of Computing and Communication Foundations [1539389]
   Funding Source: National Science Foundation
FX This work was supported in part by the National Science Foundation under
   Grant CyberSEES-1539389 and Grant CPS-1544794. The work of W. Cao was
   supported in part by the National Science Foundation of China under
   Grant 61375015. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Alessandro Piva.
CR Allebosch G, 2015, LECT NOTES COMPUT SC, V9386, P130, DOI 10.1007/978-3-319-25903-1_12
   Allebosch Gianni., 2015, Computer Vision, Imaging and Computer Graphics Theory and Applications, P433
   [Anonymous], P EUR C COMPUT VIS
   [Anonymous], FAR CAN YOU GET COMB
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   De Gregorio M, 2015, LECT NOTES COMPUT SC, V9281, P493, DOI 10.1007/978-3-319-23222-5_60
   Doretto G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1236
   Elgammal A, 2000, EUR C COMP VIS, P751, DOI DOI 10.1007/3-540-45053-X_48
   Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276
   Fragkiadaki K, 2015, PROC CVPR IEEE, P4083, DOI 10.1109/CVPR.2015.7299035
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Goyette N., 2012, 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), DOI 10.1109/CVPRW.2012.6238919
   Gregorio M. D., 2004, P IEEE C COMP VIS PA, P409
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kahl F, 2004, LECT NOTES COMPUT SC, V3247, P117
   Kays R., 2014, Proceedings of the North American Conservation Biology, P80
   Kim K, 2005, REAL-TIME IMAGING, V11, P172, DOI 10.1016/j.rti.2004.12.004
   Ko T, 2008, LECT NOTES COMPUT SC, V5304, P276, DOI 10.1007/978-3-540-88690-7_21
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   MA B, 2012, P EUR C COMPUT VIS, P413, DOI DOI 10.1007/978-3-642-33863-241
   Mahadevan Vijay., 2008, Computer Vision and Pattern Recognition, IEEE Computer Society Conference on, P1, DOI 10.1109/CVPR.2008.4587576
   Monnet A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1305
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   Oneata D, 2014, LECT NOTES COMPUT SC, V8691, P737, DOI 10.1007/978-3-319-10578-9_48
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Oreifej O, 2010, PROC CVPR IEEE, P709, DOI 10.1109/CVPR.2010.5540147
   Perazzi F, 2015, IEEE I CONF COMP VIS, P3227, DOI 10.1109/ICCV.2015.369
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Perronnin F, 2007, PROC CVPR IEEE, P2272
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren XB, 2013, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2013.254
   Ren Y, 2003, MACH VISION APPL, V13, P332, DOI 10.1007/s00138-002-0091-0
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sermanet P., 2013, P INT C LEARN REPR D
   Sheikh Y, 2005, IEEE T PATTERN ANAL, V27, P1778, DOI 10.1109/TPAMI.2005.213
   St-Charles PL, 2015, IEEE WINT CONF APPL, P990, DOI 10.1109/WACV.2015.137
   St-Charles PL, 2014, IEEE COMPUT SOC CONF, P414, DOI 10.1109/CVPRW.2014.67
   Sun J, 2006, LECT NOTES COMPUT SC, V3952, P628
   Szegedy C., 2013, Advances in Neural Information Processing Systems, V26, P2553
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tilak S., 2011, INT J RES REV WIRELE, V1, P19
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wang B, 2014, IEEE COMPUT SOC CONF, P401, DOI 10.1109/CVPRW.2014.64
   Wang R, 2014, IEEE COMPUT SOC CONF, P420, DOI 10.1109/CVPRW.2014.68
   Williams C.K.I., PASCAL VISUAL OBJECT
   Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992
NR 52
TC 59
Z9 65
U1 2
U2 45
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2016
VL 18
IS 10
BP 2079
EP 2092
DI 10.1109/TMM.2016.2594138
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DX8NC
UT WOS:000384644800014
OA Bronze
DA 2024-07-18
ER

PT J
AU Lei, JJ
   Wang, BR
   Fang, YM
   Lin, WS
   Le Callet, P
   Ling, N
   Hou, CP
AF Lei, Jianjun
   Wang, Bingren
   Fang, Yuming
   Lin, Weisi
   Le Callet, Patrick
   Ling, Nam
   Hou, Chunping
TI A Universal Framework for Salient Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Iterative optimization; salient object detection; universal framework;
   visual attention
ID DETECTION MODEL; VISUAL SALIENCY; IMAGE; ATTENTION
AB In this paper, we propose a novel universal framework for salient object detection, which aims to enhance the performance of any existing saliency detection method. First, rough salient regions are extracted from any existing saliency detection model with distance weighting, adaptive binarization, and morphological closing. With the superpixel segmentation, a Bayesian decision model is adopted to refine the rough saliency map to obtain a more accurate saliency map. An iterative optimization method is designed to obtain better saliency results by exploiting the characteristics of the output saliency map each time. Through the iterative optimization process, the rough saliency map is updated step by step with better and better performance until an optimal saliency map is obtained. Experimental results on the public salient object detection datasets with ground truth demonstrate the promising performance of the proposed universal framework subjectively and objectively.
C1 [Lei, Jianjun; Wang, Bingren; Hou, Chunping] Tianjin Univ, Sch Elect Informat Engn, Tianjin 300072, Peoples R China.
   [Fang, Yuming] Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang 330032, Peoples R China.
   [Lin, Weisi] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Le Callet, Patrick] LUNAM Univ Nantes, IRCCyN UMR CNRS 6579, Polytech Nantes, F-44306 Nantes 3, France.
   [Ling, Nam] Santa Clara Univ, Dept Comp Engn, Santa Clara, CA 95053 USA.
C3 Tianjin University; Jiangxi University of Finance & Economics; Nanyang
   Technological University; Nantes Universite; Santa Clara University
RP Fang, YM (corresponding author), Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang 330032, Peoples R China.
EM jjlei@tju.edu.cn; brwang@tju.edu.cn; fa0001ng@e.ntu.edu.sg;
   wslin@ntu.edu.sg; patrick.lecallet@univ-nantes.fr; nling@scu.edu;
   hcp@tju.edu.cn
RI Lin, Weisi/A-3696-2011; Lei, Jianjun/P-2539-2018; Lin,
   Weisi/A-8011-2012; Le Callet, Patrick/F-5772-2010
OI Lin, Weisi/0000-0001-9866-1947; 
FU Natural Science Foundation of China [61271324, 61520106002, 91320201,
   61571212, 61471262]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61271324, Grant 61520106002, Grant 91320201, Grant
   61571212, and Grant 61471262. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof. K.
   Selcuk Candan.
CR Achanta R., 2010, SLIC Superpixels
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2007, PROC IEEE C COMPUT V, DOI 10.1109/CVPR.2007.383267
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Einhäuser W, 2008, J VISION, V8, DOI 10.1167/8.2.2
   Fang YM, 2012, IEEE T IMAGE PROCESS, V21, P3888, DOI 10.1109/TIP.2012.2199126
   Fang YM, 2012, IEEE T MULTIMEDIA, V14, P187, DOI 10.1109/TMM.2011.2169775
   Gallea R, 2014, IEEE T MULTIMEDIA, V16, P971, DOI 10.1109/TMM.2014.2305917
   Ganesan P., 2010, 2010 International Conference on Emerging Trends in Robotics and Communication Technologies (INTERACT 2010), P393, DOI 10.1109/INTERACT.2010.5706186
   Gao Y, 2015, IEEE T MULTIMEDIA, V17, P359, DOI 10.1109/TMM.2015.2389616
   Goferman S, 2010, PROC CVPR IEEE, P2376, DOI 10.1109/CVPR.2010.5539929
   Gopalakrishnan V, 2010, IEEE T IMAGE PROCESS, V19, P3232, DOI 10.1109/TIP.2010.2053940
   Gopalakrishnan V, 2009, IEEE T MULTIMEDIA, V11, P892, DOI 10.1109/TMM.2009.2021726
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Han SY, 2006, IEEE IMAGE PROC, P3097, DOI 10.1109/ICIP.2006.313095
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Imamoglu N, 2013, IEEE T MULTIMEDIA, V15, P96, DOI 10.1109/TMM.2012.2225034
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Le Callet P, 2013, P IEEE, V101, P2058, DOI 10.1109/JPROC.2013.2265801
   Le Meur O, 2006, IEEE T PATTERN ANAL, V28, P802, DOI 10.1109/TPAMI.2006.86
   Lei JJ, 2015, IEEE T MULTIMEDIA, V17, P457, DOI 10.1109/TMM.2015.2400823
   Li G, 2015, IEEE GLOB COMM CONF, DOI [10.1109/PESGM.2015.7286275, 10.1109/GLOCOM.2015.7417462]
   Li J, 2014, INT J COMPUT VISION, V107, P239, DOI 10.1007/s11263-013-0678-0
   Li J, 2012, IEEE T IMAGE PROCESS, V21, P1513, DOI 10.1109/TIP.2011.2179665
   Li J, 2013, IEEE T PATTERN ANAL, V35, P996, DOI 10.1109/TPAMI.2012.147
   Liu Tie, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383047
   Liu Z, 2014, IEEE T CIRC SYST VID, V24, P1522, DOI 10.1109/TCSVT.2014.2308642
   Liu Z, 2012, IEEE T MULTIMEDIA, V14, P1275, DOI 10.1109/TMM.2012.2190385
   Liu Z, 2010, IEEE IMAGE PROC, P253, DOI 10.1109/ICIP.2010.5652613
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   Navalpakkam V, 2010, P NATL ACAD SCI USA, V107, P5232, DOI 10.1073/pnas.0911972107
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Rutishauser U, 2004, PROC CVPR IEEE, P37
   Schauerte B, 2012, LECT NOTES COMPUT SC, V7573, P116, DOI 10.1007/978-3-642-33709-3_9
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Tatler BW, 2007, J VISION, V7, DOI 10.1167/7.14.4
   Wang M, 2012, IEEE T MULTIMEDIA, V14, P858, DOI 10.1109/TMM.2012.2187181
   Wang W, 2010, PROC CVPR IEEE, P2368, DOI 10.1109/CVPR.2010.5539927
   Wei XX, 2015, IEEE SIGNAL PROC LET, V22, P1345, DOI 10.1109/LSP.2015.2399621
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
NR 46
TC 68
Z9 68
U1 2
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2016
VL 18
IS 9
BP 1783
EP 1795
DI 10.1109/TMM.2016.2592325
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DT4GI
UT WOS:000381437800009
DA 2024-07-18
ER

PT J
AU Mazloom, M
   Li, XR
   Snoek, CGM
AF Mazloom, Masoud
   Li, Xirong
   Snoek, Cees G. M.
TI TagBook: A Semantic Video Representation Without Supervision for Event
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Event detection; video search; video tagging
ID FUSION; RECOGNITION
AB We consider the problem of event detection in video for scenarios where only a few, or even zero, examples are available for training. For this challenging setting, the prevailing solutions in the literature rely on a semantic video representation obtained from thousands of pretrained concept detectors. Different from existing work, we propose a new semantic video representation that is based on freely available social tagged videos only, without the need for training any intermediate concept detectors. We introduce a simple algorithm that propagates tags from a video's nearest neighbors, similar in spirit to the ones used for image retrieval, but redesign it for video event detection by including video source set refinement and varying the video tag assignment. We call our approach TagBook and study its construction, descriptiveness, and detection performance on the TRECVID 2013 and 2014 multimedia event detection datasets and the Columbia Consumer Video dataset. Despite its simple nature, the proposed TagBook video representation is remarkably effective for few-example and zero-example event detection, even outperforming very recent state-of-the-art alternatives building on supervised representations.
C1 [Mazloom, Masoud; Snoek, Cees G. M.] Univ Amsterdam, Inst Informat, NL-1098 XH Amsterdam, Netherlands.
   [Li, Xirong] Renmin Univ China, Sch Informat, Key Lab Data Engn & Knowledge Engn, Beijing 100872, Peoples R China.
   [Snoek, Cees G. M.] Qualcomm Res, NL-1098 XH Amsterdam, Netherlands.
C3 University of Amsterdam; Renmin University of China
RP Li, XR (corresponding author), Renmin Univ China, Sch Informat, Key Lab Data Engn & Knowledge Engn, Beijing 100872, Peoples R China.
EM m.mazloom@uva.nl; xirong.li@gmail.com; cgmsnoek@uva.nl
RI Li, Xirong/AAD-3347-2019
OI Li, Xirong/0000-0002-0220-8310
FU STW STORY project; Dutch national program COMMIT; National Science
   Foundation of China [61303184]; Fundamental Research Funds for the
   Central Universities; Research Funds of Renmin University of China
   [14XNLQ01]; Specialized Research Fund for the Doctoral Program of Higher
   Education [20130004120006]; Intelligence Advanced Research Projects
   Activity (IARPA) via Department of Interior National Business Center
   [D11PC20067]
FX This work was supported by the STW STORY project, by the Dutch national
   program COMMIT, by the National Science Foundation of China under Grant
   61303184, by the Fundamental Research Funds for the Central
   Universities, by the Research Funds of Renmin University of China under
   Grant 14XNLQ01, by the Specialized Research Fund for the Doctoral
   Program of Higher Education under Grant 20130004120006, and by the
   Intelligence Advanced Research Projects Activity (IARPA) via the
   Department of Interior National Business Center Contract Number
   D11PC20067. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Qi Tian.
   (Corresponding author: Xirong Li.)
CR [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], P NIST TRECVID WORKS
   [Anonymous], P 21 ACM INT C MULT
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], P NIST TRECVID WORKS
   [Anonymous], P ACM MULT SCOTTSD A
   [Anonymous], P NIST TRECVID WORKS
   [Anonymous], INT C MULT RETR GLAS
   [Anonymous], P NIST TRECVID WORKS
   [Anonymous], 2011, ACM INT C MULT RETR
   [Anonymous], INT C MULT RETR GLAS
   [Anonymous], P NIST TRECVID WORKS
   [Anonymous], 2013, THESIS
   [Anonymous], ACM INT C MULT RETR
   [Anonymous], ACM COMPUT IN PRESS
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], INT C MULT RETR GLAS
   [Anonymous], TRECVID MULT EV DET
   [Anonymous], PROC 2ND ACM INT CON
   [Anonymous], INT C MULT RETR SHAN
   Babaguchi N, 2002, IEEE T MULTIMEDIA, V4, P68, DOI 10.1109/6046.985555
   Ballan L, 2011, MULTIMED TOOLS APPL, V51, P279, DOI 10.1007/s11042-010-0643-7
   Bhattacharya S, 2014, PROC CVPR IEEE, P2243, DOI 10.1109/CVPR.2014.287
   Chang X, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P581, DOI 10.1145/2733373.2806218
   Chang XJ, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2234
   Ebadollahi S, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P881, DOI 10.1109/ICME.2006.262691
   Gkalelis N., 2011, 2011 9th International Workshop on Content-Based Multimedia Indexing (CBMI), P85, DOI 10.1109/CBMI.2011.5972525
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P309, DOI 10.1109/ICCV.2009.5459266
   Habibian A, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P17, DOI 10.1145/2647868.2654913
   Habibian A, 2014, COMPUT VIS IMAGE UND, V124, P110, DOI 10.1016/j.cviu.2014.02.003
   Haering N, 2000, IEEE T CIRC SYST VID, V10, P857, DOI 10.1109/76.867923
   Inoue N., 2011, P NIST TRECVID WORKS
   Izadinia H, 2012, LECT NOTES COMPUT SC, V7575, P430, DOI 10.1007/978-3-642-33765-9_31
   Jiang L, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P49, DOI 10.1145/2733373.2806237
   Jiang YG, 2015, IEEE T MULTIMEDIA, V17, P1174, DOI 10.1109/TMM.2015.2436813
   Jiang YG, 2013, INT J MULTIMED INF R, V2, P73, DOI 10.1007/s13735-012-0024-2
   Kordumova S, 2015, MULTIMED TOOLS APPL, V74, P1291, DOI 10.1007/s11042-014-2056-5
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai KT, 2014, PROC CVPR IEEE, P2251, DOI 10.1109/CVPR.2014.288
   Lai KT, 2014, LECT NOTES COMPUT SC, V8691, P675, DOI 10.1007/978-3-319-10578-9_44
   Lavee G, 2009, IEEE T SYST MAN CY C, V39, P489, DOI 10.1109/TSMCC.2009.2023380
   Li BX, 2001, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES, PROCEEDINGS, P132, DOI 10.1109/IVL.2001.990867
   Li WX, 2013, IEEE I CONF COMP VIS, P2728, DOI 10.1109/ICCV.2013.339
   Li XR, 2009, IEEE T MULTIMEDIA, V11, P1310, DOI 10.1109/TMM.2009.2030598
   Ma ZG, 2013, PROC CVPR IEEE, P2627, DOI 10.1109/CVPR.2013.339
   Mazloom M., 2013, Proceedings of the ACM Multimedia Conference, MM'13, P609
   Mazloom M, 2014, IEEE T MULTIMEDIA, V16, P2214, DOI 10.1109/TMM.2014.2359771
   Merler M, 2012, IEEE T MULTIMEDIA, V14, P88, DOI 10.1109/TMM.2011.2168948
   Myers GK, 2014, MACH VISION APPL, V25, P17, DOI 10.1007/s00138-013-0527-8
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Natarajan P, 2012, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2012.6247814
   Oh S, 2014, MACH VISION APPL, V25, P49, DOI 10.1007/s00138-013-0525-x
   Oneata D, 2013, IEEE I CONF COMP VIS, P1817, DOI 10.1109/ICCV.2013.228
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4
   Siersdorfer S, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P395, DOI 10.1145/1571941.1572010
   Sun C, 2014, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2014.329
   Tamrakar A, 2012, PROC CVPR IEEE, P3681, DOI 10.1109/CVPR.2012.6248114
   Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153
   Wu S, 2014, PROC CVPR IEEE, P2665, DOI 10.1109/CVPR.2014.341
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Yang Y, 2013, IEEE I CONF COMP VIS, P2104, DOI 10.1109/ICCV.2013.456
   Ye GG, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P471, DOI 10.1145/2733373.2806221
NR 63
TC 27
Z9 28
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2016
VL 18
IS 7
BP 1378
EP 1388
DI 10.1109/TMM.2016.2559947
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR2RW
UT WOS:000379752600013
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Dong, YY
   Pourazad, MT
   Nasiopoulos, P
AF Dong, Yuanyuan
   Pourazad, Mahsa T.
   Nasiopoulos, Panos
TI Human Visual System-Based Saliency Detection for High Dynamic Range
   Content
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Eye tracking; high dynamic range (HDR); saliency map; visual attention
ID DETECTION MODEL
AB The human visual system (HVS) attempts to select salient areas to reduce cognitive processing efforts. Computational models of visual attention try to predict the most relevant and important areas of videos or images viewed by the human eye. Such models, in turn, can be applied to areas such as computer graphics, video coding, and quality assessment. Although several models have been proposed, only one of them is applicable to high dynamic range (HDR) image content, and no work has been done for HDR videos. Moreover, the main shortcoming of the existing models is that they cannot simulate the characteristics of HVS under the wide luminous range found in HDR content. This paper addresses these issues by presenting a computational approach to model the bottom-up visual saliency for HDR input by combining spatial and temporal visual features. An analysis of eye movement data affirms the effectiveness of the proposed model. Comparisons employing three well-known quantitative metrics show that the proposed model substantially improves predictions of visual attention for HDR content.
C1 [Dong, Yuanyuan; Nasiopoulos, Panos] Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
   [Pourazad, Mahsa T.] TELUS Commun Inc, Quebec City, PQ, Canada.
   [Pourazad, Mahsa T.; Nasiopoulos, Panos] Univ British Columbia, ICICS, Vancouver, BC V6T 1Z4, Canada.
C3 University of British Columbia; University of British Columbia
RP Dong, YY; Nasiopoulos, P (corresponding author), Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.; Pourazad, MT (corresponding author), TELUS Commun Inc, Quebec City, PQ, Canada.
EM yuand@ece.ubc.ca; pourazad@icics.ubc.ca; panos@icics.ubc.ca
FU NSERC [STPGP 447339-13]; ICICS/TELUS People & Planet Friendly Home
   Initiative at UBC
FX This work was supported in part by the NSERC under Grant STPGP
   447339-13, and in part by the ICICS/TELUS People & Planet Friendly Home
   Initiative at UBC. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Qi Tian.
CR [Anonymous], P IS T SPIE ELECT IM
   [Anonymous], 2014, JTC1SC29WG11MPEG2014
   [Anonymous], 1987, Shifts in selective visual attention: Towards the underlying neural circuitry. matters of intelligence
   [Anonymous], 2004, OPT SCI TECHNOL
   [Anonymous], 2010, EXPT CTR 2 MAN VERS
   [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Bremond Roland, 2012, TRENDS TOPICS COMPUT, P118, DOI [10.1145/1620993, DOI 10.1145/1620993]
   CIE Publication 131, 1998, COLOR RES APPL, V23, P431
   DALY S, 1992, P SOC PHOTO-OPT INS, V1666, P2, DOI 10.1117/12.135952
   Dong Y., 2014, P 2 INT C EL SIGN PR, P56
   Fang YM, 2014, IEEE T CIRC SYST VID, V24, P27, DOI 10.1109/TCSVT.2013.2273613
   Fang YM, 2012, IEEE T MULTIMEDIA, V14, P187, DOI 10.1109/TMM.2011.2169775
   Goldberg J. H., 2002, Proceedings ETRA 2002. Eye Tracking Research and Applications Symposium, P51, DOI 10.1145/507072.507082
   Harel J., 2012, A saliency implementation in MATLAB
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hunt R.W.G., 2004, REPROD COLOR, V6th
   Itti L, 2005, VIS COGN, V12, P1093, DOI 10.1080/13506280444000661
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Kim MH, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531333
   Le Meur O, 2013, BEHAV RES METHODS, V45, P251, DOI 10.3758/s13428-012-0226-9
   Lee SH, 2014, IEEE IMAGE PROC, P1120, DOI 10.1109/ICIP.2014.7025223
   LUO MR, 1991, COLOR RES APPL, V16, P166, DOI 10.1002/col.5080160307
   Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410
   Mantiuk R, 2005, PROC SPIE, V5666, P204, DOI 10.1117/12.586757
   MANTIUK R, 2006, P SOC PHOTO-OPT INS, V6057, P311
   Moroney N., 2002, P COL IM C, P23
   Muller G. E., 1930, PSYCHOPHYSISCHE UNTE
   Nothdurft HC, 2000, VISION RES, V40, P1183, DOI 10.1016/S0042-6989(00)00031-6
   Seetzen H, 2004, ACM T GRAPHIC, V23, P760, DOI 10.1145/1015706.1015797
   Stankiewicz B. J., 2010, P SOC PHOTO-OPT INS, V7867
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Uscanga O. E., 1979, THESIS U AMSTERDAM A
   van Gog T, 2010, LEARN INSTR, V20, P95, DOI 10.1016/j.learninstruc.2009.02.009
   Vaudrey T., 2010, P ARTS TECHN 1 INT C, P215
   VOS JJ, 1971, VISION RES, V11, P799, DOI 10.1016/0042-6989(71)90003-4
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhao Q, 2011, J VISION, V11, DOI 10.1167/11.3.9
   Zhixiang Ren, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P158, DOI 10.1109/ICME.2012.173
NR 40
TC 35
Z9 37
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2016
VL 18
IS 4
BP 549
EP 562
DI 10.1109/TMM.2016.2522639
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DH4XZ
UT WOS:000372790300001
DA 2024-07-18
ER

PT J
AU Hu, Q
   Zhang, XY
   Shi, ZR
   Gao, ZY
AF Hu, Qiang
   Zhang, Xiaoyun
   Shi, Zhiru
   Gao, Zhiyong
TI Neyman-Pearson-Based Early Mode Decision for HEVC Encoding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE High efficiency video coding (HEVC); mode decision; Neyman-Peason;
   nonparametric density estimation
ID INTER CU DECISION; SIZE DECISION; EFFICIENCY
AB The high efficiency video coding (HEVC) standard has highly improved the coding efficiency by adopting hierarchical structures of coding unit (CU), prediction unit (PU), and transform unit (TU). However, enormous computational complexity is introduced due to the recursive rate-distortion optimization (RDO) process on all CUs, PUs and TUs. In this paper, we propose a fast and efficient mode decision algorithm based on the Neyman-Pearson rule, which consists of early SKIP mode decision and fast CU size decision. First, the early mode decision is modeled as a binary classification problem of SKIP/non-SKIP or split/unsplit. The Neyman-Pearson-based rule is employed to balance the rate-distortion (RD) performance loss and the complexity reduction by minimizing the missed detection with a constrained incorrect decision rate. A nonparametric density estimation scheme is also developed to calculate the likelihood function of the statistical parameters. Furthermore, an online training scheme is employed to periodically update the probability density distributions for different quantization parameters (QPs) and CU depth levels. The experimental results show that the proposed overall algorithm can save 65% and 58% computational complexity on average with a 1.29% and 1.08% Bjontegaard Delta bitrate (BDBR) increase for various test sequences under random access and low delay P conditions, respectively. The proposed overall scheme also has the advantage that it canmake the trade-off between the RD performance and time saving by setting different values for the incorrect decision rate.
C1 [Hu, Qiang; Zhang, Xiaoyun; Shi, Zhiru; Gao, Zhiyong] Shanghai Jiao Tong Univ, Dept Elect Engn, Inst Image Commun & Network Engn, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Zhang, XY (corresponding author), Shanghai Jiao Tong Univ, Dept Elect Engn, Inst Image Commun & Network Engn, Shanghai 200240, Peoples R China.
EM hq2902108007@sjtu.edu.cn; xiaoyun.zhang@sjtu.edu.cn;
   zhiru.shi@sjtu.edu.cn
OI Hu, Qiang/0000-0003-4645-9776
FU National Natural Science Foundation of China [61221001, 61133009,
   61301116]; 111 Project [B07022]; Shanghai Key Laboratory of Digital
   Media Processing and Transmissions under STCSM [12DZ2272600]; National
   Key Technology R&D Program of China [2013BAH53F04]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61221001, Grant 61133009, and Grant
   61301116, in part by the 111 Project under Grant B07022, in part by the
   Shanghai Key Laboratory of Digital Media Processing and Transmissions
   under STCSM Grant 12DZ2272600, and in part by the National Key
   Technology R&D Program of China under Grant 2013BAH53F04. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Yap-Peng Tan.
CR Ahn S, 2015, IEEE T CIRC SYST VID, V25, P422, DOI 10.1109/TCSVT.2014.2360031
   [Anonymous], JCTVCL1003
   [Anonymous], VCEGM33 ITUT SC16Q6
   [Anonymous], 2014, MITOCHONDRIAL DNA A
   [Anonymous], JCTVCI1002
   [Anonymous], JCTVCF045
   [Anonymous], JCTVCK1100
   [Anonymous], 2011, JCTVCF092
   Cassa MB, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P493, DOI 10.1109/PCS.2012.6213262
   Chai D, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL II, P464
   Corder G.W., 2009, NONPARAMETRIC STAT N
   Correa G, 2015, IEEE T CIRC SYST VID, V25, P660, DOI 10.1109/TCSVT.2014.2363753
   Gray RM, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P172, DOI 10.1109/SEQUEN.1997.666914
   Hu Q, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P502, DOI 10.1109/VCIP.2014.7051616
   Kim J, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P449, DOI 10.1109/PCS.2012.6213251
   Lee J, 2015, IEEE T CIRC SYST VID, V25, P411, DOI 10.1109/TCSVT.2014.2339612
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   Shen LQ, 2014, IEEE T CIRC SYST VID, V24, P1709, DOI 10.1109/TCSVT.2014.2313892
   Shen LQ, 2014, IEEE T IMAGE PROCESS, V23, P4232, DOI 10.1109/TIP.2014.2341927
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Shen XL, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-4
   Shen XL, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P453, DOI 10.1109/PCS.2012.6213252
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Vanne J, 2014, IEEE T CIRC SYST VID, V24, P1579, DOI 10.1109/TCSVT.2014.2308453
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiaotao Hou, 2014, 2014 IEEE International Conference on Consumer Electronics (ICCE), P7, DOI 10.1109/ICCE.2014.6775886
   XIE QB, 1993, IEEE T PATTERN ANAL, V15, P1326, DOI 10.1109/34.250849
   Xiong J, 2015, IEEE T MULTIMEDIA, V17, P2147, DOI 10.1109/TMM.2015.2491018
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P2141, DOI 10.1109/TMM.2014.2356795
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P559, DOI 10.1109/TMM.2013.2291958
   Zhang YF, 2013, IEEE DATA COMPR CONF, P53, DOI 10.1109/DCC.2013.13
   Zhang Y, 2015, IEEE T IMAGE PROCESS, V24, P2225, DOI 10.1109/TIP.2015.2417498
NR 32
TC 44
Z9 45
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2016
VL 18
IS 3
BP 379
EP 391
DI 10.1109/TMM.2015.2512799
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DG2WP
UT WOS:000371931600006
DA 2024-07-18
ER

PT J
AU Jian, M
   Jung, C
AF Jian, Meng
   Jung, Cheolkon
TI Semi-Supervised Bi-Dictionary Learning for Image Classification With
   Smooth Representation-Based Label Propagation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image classification; label propagation; semantic gap; semi-supervised
   dictionary learning; smooth representation
ID FACE RECOGNITION; COLLABORATIVE REPRESENTATION; SPARSE
AB In this paper, we propose semi-supervised bi-dictionary learning for image classification with smooth representation-based label propagation (SRLP). Natural images contain complex contents of multiple objects with complicated background, clutter, and occlusions, which prevents image features from belonging to a specific category. Therefore, we employ reconstruction-based classification to implement discriminative dictionary learning in a probabilistic manner. We jointly learn a discriminative dictionary called anchor in the feature space and its corresponding soft label called anchor label in the label space, where the combination of anchor and anchor label is referred to as bi-dictionary. The learnt bi-dictionary is utilized to bridge the semantic gap in image classification. First, SRLP constructs smoothed reconstruction problems for bi-dictionary learning. Then, SRLP produces the reconstruction coefficients in the feature space over the anchor to infer soft labels of samples in the label space. Experimental results demonstrate that the proposed method is capable of learning a pair of discriminative dictionaries for image classification in the feature and label spaces and outperforms the-state-of-the-art reconstruction-based classification ones.
C1 [Jian, Meng; Jung, Cheolkon] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
C3 Xidian University
RP Jian, M; Jung, C (corresponding author), Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
EM jianmeng648@163.com; zhengzk@xidian.edu.cn
FU National Natural Science Foundation of China [61271298]; International
   S&T Cooperation Program of China [2014DFG12780]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61271298 and by the International S&T Cooperation
   Program of China under Grant 2014DFG12780. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Marco Bertini.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2005, 1530 U WISC DEP COMP
   [Anonymous], 1998, STAT LEARNING THEORY
   [Anonymous], 2001, PATTERN CLASSIFICAT
   [Anonymous], 2004, Workshop on Generative Model Based Vision
   Babagholami-Mohamadabadi Behnam, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P192, DOI 10.1007/978-3-642-40994-3_13
   Chapelle O., 2002, P 15 INT C NEURAL IN, P601
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Graham Daniel B, 1998, CHARACTERISING VIRTU, P446
   Griffin Gregory, 2007, CALTECH 256 OBJECT C
   Hoi SCH, 2005, PROC CVPR IEEE, P302
   Hu H, 2014, PROC CVPR IEEE, P3834, DOI 10.1109/CVPR.2014.484
   Huang YZ, 2011, PROC CVPR IEEE, P1753, DOI 10.1109/CVPR.2011.5995682
   Jian M, 2014, IEEE T MULTIMEDIA, V16, P413, DOI 10.1109/TMM.2013.2291657
   Jian M, 2013, IEEE T SIGNAL PROCES, V61, P4416, DOI 10.1109/TSP.2013.2271479
   Jiang ZL, 2011, PROC CVPR IEEE, P1697, DOI 10.1109/CVPR.2011.5995354
   Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92
   Li L, 2012, IEEE T MULTIMEDIA, V14, P1401, DOI 10.1109/TMM.2012.2194993
   Lian XC, 2010, LECT NOTES COMPUT SC, V6314, P157, DOI 10.1007/978-3-642-15561-1_12
   Liu W., 2010, PROC ICML, P679
   Ma ZG, 2012, IEEE T MULTIMEDIA, V14, P1662, DOI 10.1109/TMM.2012.2199293
   Martinez A. M., 1998, THE AR FACE DATABASE
   Shi QF, 2011, PROC CVPR IEEE, P553, DOI 10.1109/CVPR.2011.5995556
   Shrivastava A, 2012, IEEE IMAGE PROC, P3113, DOI 10.1109/ICIP.2012.6467559
   Sim T, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P53, DOI 10.1109/AFGR.2002.1004130
   Tang JH, 2012, IEEE T IMAGE PROCESS, V21, P2354, DOI 10.1109/TIP.2011.2180916
   Wang M, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1899412.1899414
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yang JC, 2010, PROC CVPR IEEE, P3517, DOI 10.1109/CVPR.2010.5539958
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Yang M, 2012, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2012.6247931
   Yang M, 2011, IEEE I CONF COMP VIS, P543, DOI 10.1109/ICCV.2011.6126286
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhang L, 2011, NEUROCOMPUTING, V74, P568, DOI 10.1016/j.neucom.2010.09.022
   Zhang QA, 2010, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2010.5539989
   Zheng YG, 2013, NEUROCOMPUTING, V122, P398, DOI 10.1016/j.neucom.2013.06.013
   Zhou DY, 2004, ADV NEUR IN, V16, P321
NR 37
TC 23
Z9 27
U1 1
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2016
VL 18
IS 3
BP 458
EP 473
DI 10.1109/TMM.2016.2515367
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DG2WP
UT WOS:000371931600012
DA 2024-07-18
ER

PT J
AU Li, X
   Shen, B
   Liu, BD
   Zhang, YJ
AF Li, Xue
   Shen, Bin
   Liu, Bao-Di
   Zhang, Yu-Jin
TI A Locality Sensitive Low-Rank Model for Image Tag Completion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Automatic image annotation; image tag completion; locality sensitive
   model; low-rank matrix factorization
AB Many visual applications have benefited from the outburst of web images, yet the imprecise and incomplete tags arbitrarily provided by users, as the thorn of the rose, may hamper the performance of retrieval or indexing systems relying on such data. In this paper, we propose a novel locality sensitive low-rank model for image tag completion, which approximates the global nonlinear model with a collection of local linear models. To effectively infuse the idea of locality sensitivity, a simple and effective pre-processing module is designed to learn suitable representation for data partition, and a global consensus regularizer is introduced to mitigate the risk of overfitting. Meanwhile, low-rank matrix factorization is employed as local models, where the local geometry structures are preserved for the low-dimensional representation of both tags and samples. Extensive empirical evaluations conducted on three datasets demonstrate the effectiveness and efficiency of the proposed method, where our method outperforms pervious ones by a large margin.
C1 [Li, Xue; Zhang, Yu-Jin] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
   [Shen, Bin] Purdue Univ, W Lafayette, IN 47907 USA.
   [Liu, Bao-Di] China Univ Petr, Dept Informat & Control Engn, Qingdao 266580, Peoples R China.
C3 Tsinghua University; Purdue University System; Purdue University; China
   University of Petroleum
RP Li, X; Zhang, YJ (corresponding author), Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.; Shen, B (corresponding author), Purdue Univ, W Lafayette, IN 47907 USA.; Shen, B (corresponding author), Google Res, New York, NY 10011 USA.; Liu, BD (corresponding author), China Univ Petr, Dept Informat & Control Engn, Qingdao 266580, Peoples R China.
EM xue-li11@mails.tsinghua.edu.cn; stanshenbin@gmail.com;
   thu.liubaodi@gmail.com; zhang-yj@tsinghua.edu.cn
FU National Nature Science Foundation [NNSF: 61171118]; Specialized
   Research Fund for the Doctoral Program of Higher Education
   [SRFDP-20110002110057]; National Nature Science Foundation [NNSF:
   61171118]; Specialized Research Fund for the Doctoral Program of Higher
   Education [SRFDP-20110002110057]
FX This work was supported by the National Nature Science Foundation under
   Grant NNSF: 61171118, and by the Specialized Research Fund for the
   Doctoral Program of Higher Education under Grant SRFDP-20110002110057.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Lexing Xie.
CR [Anonymous], 2008, P 16 INT C MULTIMEDI, DOI [DOI 10.1145/1459359.1459577, 10.1145/1459359.1459577]
   [Anonymous], 2008, P 17 INT C WORLD WID
   [Anonymous], 2006, ADV NEURAL INF PROCE
   [Anonymous], TECH REP
   [Anonymous], 2014, INT C MACH LEARN
   [Anonymous], CORR
   [Anonymous], 2010, P ACM MULTIMEDIA
   [Anonymous], 2012, ADV NEURAL INF PROCE
   [Anonymous], 2014, ACMMM
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Bucak S. S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2801, DOI 10.1109/CVPR.2011.5995734
   Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P394, DOI 10.1109/TPAMI.2007.61
   Chen A., 2013, ICML, P1274
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Feng SL, 2004, PROC CVPR IEEE, P1002
   Feng SH, 2015, IEEE T IMAGE PROCESS, V24, P1223, DOI 10.1109/TIP.2015.2395816
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P309, DOI 10.1109/ICCV.2009.5459266
   Kalayeh MM, 2014, PROC CVPR IEEE, P184, DOI 10.1109/CVPR.2014.31
   Ladicky L., 2011, Proceedings of the 28th International Conference on Machine Learning (ICML-11), P985
   Lee J, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P85, DOI 10.1145/2566486.2567970
   Lee S, 2010, PATTERN RECOGN LETT, V31, P976, DOI 10.1016/j.patrec.2009.12.024
   Li X, 2016, NEUROCOMPUTING, V173, P425, DOI 10.1016/j.neucom.2014.12.121
   Lin ZJ, 2014, COMPUT VIS IMAGE UND, V124, P42, DOI 10.1016/j.cviu.2014.03.012
   Lin ZJ, 2013, PROC CVPR IEEE, P1618, DOI 10.1109/CVPR.2013.212
   Liu BD, 2014, INT CONF ACOUST SPEE
   Liu BD, 2013, PATTERN RECOGN, V46, P1879, DOI 10.1016/j.patcog.2012.11.018
   Liu D, 2011, IEEE T MULTIMEDIA, V13, P702, DOI 10.1109/TMM.2011.2134078
   Liu Dong., 2010, Proceedings of the International Conference on Multimedia, P491
   Liu X, 2012, APPL PHYS LETT, V101, DOI 10.1063/1.4767991
   Makadia A, 2008, LECT NOTES COMPUT SC, V5304, P316, DOI 10.1007/978-3-540-88690-7_24
   Putthividhya D, 2010, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2010.5540000
   Sang JT, 2012, IEEE T MULTIMEDIA, V14, P883, DOI 10.1109/TMM.2012.2188782
   Simonyan K., 2014, CORR
   Verma Y, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.25
   Verma Y, 2012, LECT NOTES COMPUT SC, V7574, P836, DOI 10.1007/978-3-642-33712-3_60
   Wang QF, 2014, LECT NOTES COMPUT SC, V8690, P425, DOI 10.1007/978-3-319-10605-2_28
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Wu L, 2013, IEEE T PATTERN ANAL, V35, P716, DOI 10.1109/TPAMI.2012.124
   Yang C., 2006, P IEEE INT C COMPUTE, P2057
   Yang KY, 2011, IEEE T MULTIMEDIA, V13, P662, DOI 10.1109/TMM.2011.2147777
   Yang Liu, 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P417, DOI 10.1109/ICDM.2011.141
   Zheng M, 2011, IEEE T IMAGE PROCESS, V20, P1327, DOI 10.1109/TIP.2010.2090535
NR 43
TC 21
Z9 23
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2016
VL 18
IS 3
BP 474
EP 483
DI 10.1109/TMM.2016.2518478
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DG2WP
UT WOS:000371931600013
DA 2024-07-18
ER

PT J
AU Chao, JS
   Steinbach, E
AF Chao, Jianshu
   Steinbach, Eckehard
TI Keypoint Encoding for Improved Feature Extraction From Compressed Video
   at Low Bitrates
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Coding; H.265/HEVC; keypoints; matching; prediction; retrieval; SIFT
ID DESCRIPTORS; PATCHES
AB In many mobile visual analysis applications, compressed video is transmitted over a communication network and analyzed by a server. Typical processing steps performed at the server include keypoint detection, descriptor calculation, and feature matching. Video compression has been shown to have an adverse effect on feature-matching performance. The negative impact of compression can be reduced by using the keypoints extracted from the uncompressed video to calculate descriptors from the compressed video. Based on this observation, we propose to provide these keypoints to the server as side information and to extract only the descriptors from the compressed video. First, we introduce four different frame types for keypoint encoding to address different types of changes in video content. These frame types represent a new scene, the same scene, a slowly changing scene, or a rapidly moving scene, and are determined by comparing features between successive video frames. Then, we propose Intra, Skip, and Inter modes of encoding the keypoints for different frame types. For example, keypoints for new scenes are encoded using the Intra mode, and keypoints for unchanged scenes are skipped. As a result, the bitrate of the side information related to keypoint encoding is significantly reduced. Finally, we present pairwise matching and image retrieval experiments conducted to evaluate the performance of the proposed approach using the Stanford mobile augmented reality dataset and 720p format videos. The results show that the proposed approach offers significantly improved feature matching and image retrieval performance at a given bitrate.
C1 [Chao, Jianshu; Steinbach, Eckehard] Tech Univ Munich, Chair Media Technol, D-80333 Munich, Germany.
C3 Technical University of Munich
RP Chao, JS (corresponding author), Tech Univ Munich, Chair Media Technol, D-80333 Munich, Germany.
EM jianshu.chao@tum.de; eckehard.steinbach@tum.de
OI Steinbach, Eckehard/0000-0001-8853-2703
CR [Anonymous], 2009, JM REF SOFTW
   [Anonymous], 2010, PROC ACM SIGMM INT C
   [Anonymous], 2011, MPEG 7
   [Anonymous], INT C MULT EXP
   Baroffio L., 2015, CORR
   Baroffio L, 2014, IEEE IMAGE PROC, P2794, DOI 10.1109/ICIP.2014.7025565
   Baroffio L, 2014, IEEE T IMAGE PROCESS, V23, P2262, DOI 10.1109/TIP.2014.2312617
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Chandrasekhar V., 2010, P INT WORKSH MOB MUL
   Chao JS, 2013, IEEE IMAGE PROC, P1675, DOI 10.1109/ICIP.2013.6738345
   Chao JS, 2012, IEEE IMAGE PROC, P709, DOI 10.1109/ICIP.2012.6466958
   Chao JS, 2011, IEEE IMAGE PROC, P301, DOI 10.1109/ICIP.2011.6116299
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Duan L., 2012, PROC 29 INT C INT C, P1
   Duan LY, 2015, IEEE DATA COMPR CONF, P323, DOI 10.1109/DCC.2015.72
   Duan LY, 2014, IEEE MULTIMEDIA, V21, P30, DOI 10.1109/MMUL.2013.66
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Makar M, 2014, IEEE T IMAGE PROCESS, V23, P3352, DOI 10.1109/TIP.2014.2331136
   Makar M, 2013, INT J SEMANT COMPUT, V7, P5, DOI 10.1142/S1793351X13400011
   Makar M, 2012, IEEE IMAGE PROC, P2505, DOI 10.1109/ICIP.2012.6467407
   Makar M, 2009, INT CONF ACOUST SPEE, P821, DOI 10.1109/ICASSP.2009.4959710
   Matas J., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P384
   Redondi A, 2013, IEEE IMAGE PROC, P2910, DOI 10.1109/ICIP.2013.6738599
   Redondi A, 2013, IEEE INT WORKSH MULT, P278, DOI 10.1109/MMSP.2013.6659301
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34
   Tsai S. S., 2012, P SPIE, V8499
   Wiegand T, 2005, IEEE T CIRC SYST VID, V15, P197, DOI 10.1109/TCSVT.2004.841690
   Yue HJ, 2013, IEEE T MULTIMEDIA, V15, P845, DOI 10.1109/TMM.2013.2239629
NR 30
TC 20
Z9 20
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2016
VL 18
IS 1
BP 25
EP 39
DI 10.1109/TMM.2015.2502552
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CZ5JW
UT WOS:000367139700004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Schmeing, M
   Jiang, XY
AF Schmeing, Michael
   Jiang, Xiaoyi
TI Faithful Disocclusion Filling in Depth Image Based Rendering Using
   Superpixel-Based Inpainting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Depth image based rendering (DIBR); disocclusion filling; view synthesis
ID VIDEO; COMPRESSION; CONSISTENT
AB Disocclusion filling is a critical problem in depth-based view synthesis. Exposed regions in the target view that correspond to occluded areas in the reference view have to be filled in a meaningful way. Current approaches aim to do this in a plausible way, mostly inspired by image inpainting techniques. However, disocclusion filling is a video-based problem which exhibits more information than just the current frame. By utilizing texture found in temporally adjacent frames, we propose to fill disocclusions in a faithful way, i.e., using texture that a real camera would observe in place of the virtual camera. Only if faithful information is not available we fall back to plausible filling. Our approach is designed for single view video-plus-depth where neighboring camera views are not available for disocclusion filling. In contrast to previous approaches, our method uses superpixels instead of square patches as filling entities to reduce the amount of artifacts introduced into the filling region. Despite its importance, faithfulness has not obtained the due attention yet. Our experiments show that situations are common where a simple plausible filling does not lead to satisfying filling results. Thus, it is important to stress faithful disocclusion filling. Our current work is an attempt in this direction.
C1 [Schmeing, Michael; Jiang, Xiaoyi] Univ Munster, Dept Math & Comp Sci, D-48149 Munster, Germany.
C3 University of Munster
RP Schmeing, M (corresponding author), Univ Munster, Dept Math & Comp Sci, D-48149 Munster, Germany.
EM m.schmeing@uni-muenster.de; xjiang@uni-muenster.de
RI Jiang, Xiaoyi/AAA-3532-2022
OI Jiang, Xiaoyi/0000-0001-7678-9528
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], P SOC PHOT OPT INSTR
   [Anonymous], 2011, 3DTV C TRUE VIS CAPT
   [Anonymous], P SOC PHOT OPT INSTR
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], 2011, INT SYST APPL ISA 20, DOI [DOI 10.1145/2063384.2063479, DOI 10.1145/2072298.2071964]
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Buyssens P, 2015, IEEE T IMAGE PROCESS, V24, P1809, DOI 10.1109/TIP.2015.2411437
   Cheng C.-M., 2008, PROC IEEE INT C PATT, P1
   Cheng CM, 2011, IEEE T BROADCAST, V57, P523, DOI 10.1109/TBC.2011.2139090
   Choi S, 2013, IEEE T IMAGE PROCESS, V22, P2429, DOI 10.1109/TIP.2013.2251646
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Daribo I, 2007, 2007 IEEE NINTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P312, DOI 10.1109/MMSP.2007.4412880
   Daribo I, 2011, IEEE T BROADCAST, V57, P533, DOI 10.1109/TBC.2011.2125110
   Fehn C, 2006, P IEEE, V94, P524, DOI 10.1109/JPROC.2006.870688
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Ilkoo Ahn, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P109, DOI 10.1109/ICME.2012.95
   Köppel M, 2012, IEEE INT WORKSH MULT, P25, DOI 10.1109/MMSP.2012.6343410
   Lee PJ, 2011, IEEE T MULTIMEDIA, V13, P246, DOI 10.1109/TMM.2010.2100372
   Lee Soo Chan, 2009, Commun Integr Biol, V2, P414
   Liu CX, 2007, VISUAL COMPUT, V23, P833, DOI 10.1007/s00371-007-0137-4
   Luo K, 2009, J ZHEJIANG UNIV-SC A, V10, P1738, DOI 10.1631/jzus.A0820806
   Mori Y, 2009, SIGNAL PROCESS-IMAGE, V24, P65, DOI 10.1016/j.image.2008.10.013
   Müller K, 2011, P IEEE, V99, P643, DOI 10.1109/JPROC.2010.2091090
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   Oh KJ, 2009, PCS: 2009 PICTURE CODING SYMPOSIUM, P233
   Peris M, 2012, INT C PATT RECOG, P1038
   Schmeing M., 2011, Pattern Recognition, Machine Intelligence and Biometrics, P279, DOI [10.1007/978-3-642-22407-212, DOI 10.1007/978-3-642-22407-212]
   Schmeing M, 2010, 2010 3DTV C TRUE VIS, P1, DOI DOI 10.1109/3DTV.2010.5506596
   Schmeing M, 2014, INT C PATT RECOG, P1073, DOI 10.1109/ICPR.2014.194
   Schmeing M, 2013, INT J PATTERN RECOGN, V27, DOI 10.1142/S0218001413540050
   Smolic A, 2011, P IEEE, V99, P607, DOI 10.1109/JPROC.2010.2098350
   Tang NC, 2011, IEEE T MULTIMEDIA, V13, P602, DOI 10.1109/TMM.2011.2112642
   Telea A., 2004, Journal of Graphics Tools, V9, P23, DOI 10.1080/10867651.2004.10487596
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zhang L, 2005, IEEE T BROADCAST, V51, P191, DOI 10.1109/TBC.2005.846190
   Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766
NR 38
TC 33
Z9 39
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2015
VL 17
IS 12
BP 2160
EP 2173
DI 10.1109/TMM.2015.2476372
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CW9JR
UT WOS:000365315500005
DA 2024-07-18
ER

PT J
AU Yuan, H
   Kwong, S
   Wang, X
   Gao, W
   Zhang, Y
AF Yuan, Hui
   Kwong, Sam
   Wang, Xu
   Gao, Wei
   Zhang, Yun
TI Rate Distortion Optimized Inter-View Frame Level Bit Allocation Method
   for MV-HEVC
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bit allocation; inter-view dependency; MV-HEVC; rate distortion
   optimization
ID MULTIVIEW VIDEO; 3-D VIDEO; MOTION
AB In multi-view video coding, since inter-view prediction has been adopted as an important coding tool which could improve coding efficiency greatly, inter-view dependency is inevitable, i.e., the distortion of the reference view (RV) picture could be propagated to the non-reference view (NRV) pictures. Therefore, in order to achieve higher coding efficiency, the inter-view dependency must be taken into account for inter-view bit allocation. In this paper, the inter-view dependency is analyzed in detail, and a rate-distortion (RD) model for NRVs is derived by taking the distortion of RV into account. Based on the derived RD model, the inter-view bit allocation is represented as a mathematical problem with an analytic form, and is solved by a convex optimization (Lagrangian Multiplier) method. Experimental results demonstrate that the RD performance and the inter-view quality consistency of the proposed method is better than existing methods, while the complexity of the proposed method is comparable with the existing methods.
C1 [Yuan, Hui] Shandong Univ, Sch Informat Sci & Engn, Jinan 250100, Peoples R China.
   [Yuan, Hui] Chinese Acad Sci, Shanghai Inst Microsyst & Informat Technol, Key Lab Wireless Sensor Network & Commun, Shanghai 200050, Peoples R China.
   [Yuan, Hui; Kwong, Sam; Gao, Wei] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Shenzhen Res Inst, Shenzhen 5180057, Peoples R China.
   [Wang, Xu] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
   [Zhang, Yun] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
C3 Shandong University; Chinese Academy of Sciences; Shanghai Institute of
   Microsystem & Information Technology, CAS; City University of Hong Kong;
   City University of Hong Kong; Shenzhen Research Institute, City
   University of Hong Kong; Shenzhen University; Chinese Academy of
   Sciences; Shenzhen Institute of Advanced Technology, CAS
RP Yuan, H (corresponding author), Shandong Univ, Sch Informat Sci & Engn, Jinan 250100, Peoples R China.
EM yuanhui0325@gmail.com; cssamk@cityu.edu.hk; wangxu@szu.edu.cn;
   gaowei262@126.com; yun.zhang@siat.ac.cn
RI Kwong, Sam/C-9319-2012; Yuan, Hui/B-6738-2013; Zhang, Yun/V-7261-2019;
   Yuan, Hui/HDO-3699-2022
OI Kwong, Sam/0000-0001-7484-7261; Yuan, Hui/0000-0001-5212-3393; Zhang,
   Yun/0000-0001-9457-7801; Yuan, Hui/0000-0001-5212-3393
FU National Natural Science Foundation of China [61201211, 61571274,
   61272289, 61501299]; Young Scholars Program of Shandong University
   [2015WLJH39]; City University of Hong Kong [9667094]; Ph.D. Programs
   Foundation, Ministry of Education of China [20120131120032]; Excellent
   Youth Scientist Award Foundation of Shandong Province [BS2012DX021]; Key
   Laboratory of Wireless Sensor Network and Communication, Chinese Academy
   of Sciences [2013002]; City University of Hong Kong Shenzhen Research
   Institute
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61201211, Grant 61571274, Grant
   61272289, and Grant 61501299, in part by the Young Scholars Program of
   Shandong University under Grant 2015WLJH39, in part by the City
   University of Hong Kong Applied Research Grant 9667094, in part by the
   Ph.D. Programs Foundation, Ministry of Education of China under Grant
   20120131120032, in part by the Excellent Youth Scientist Award
   Foundation of Shandong Province under Grant BS2012DX021, in part by the
   Key Laboratory of Wireless Sensor Network and Communication, Chinese
   Academy of Sciences under Grant 2013002, and in part by the City
   University of Hong Kong Shenzhen Research Institute. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Adrian Munteanu.
CR [Anonymous], 2001, SG16Q6 ITUT VCEG
   Bjontegaard G., 2008, P 35 M ITU T VID COD
   Chang Y, 2013, IEEE T BROADCAST, V59, P265, DOI 10.1109/TBC.2013.2240731
   Chen Y, 2014, IEEE T CIRC SYST VID, V24, P2090, DOI 10.1109/TCSVT.2014.2352571
   Chen Y, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/786015
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P3179, DOI 10.1109/TIP.2011.2158230
   Flierl M, 2007, IEEE T CIRC SYST VID, V17, P1474, DOI 10.1109/TCSVT.2007.903780
   Gray RM, 1998, IEEE T INFORM THEORY, V44, P2325, DOI 10.1109/18.720541
   Joint Collaborative Team for 3DV, 2014, 3D HTM SOFTW PLATF
   Joint Collaborative Team for 3DV, 2014, 3D VID TEST SEQ
   Kamaci N, 2005, IEEE T CIRC SYST VID, V15, P994, DOI 10.1109/TCSVT.2005.852400
   Lam EY, 2000, IEEE T IMAGE PROCESS, V9, P1661, DOI 10.1109/83.869177
   Lee HJ, 2000, IEEE T CIRC SYST VID, V10, P878, DOI 10.1109/76.867926
   Lim W., 2013, P 5 M ITU T ISO IEC
   Liu YW, 2011, IEEE T BROADCAST, V57, P562, DOI 10.1109/TBC.2011.2105652
   Merkle P, 2007, IEEE T CIRC SYST VID, V17, P1461, DOI 10.1109/TCSVT.2007.903665
   Müller K, 2013, IEEE T IMAGE PROCESS, V22, P3366, DOI 10.1109/TIP.2013.2264820
   Puri A, 2004, SIGNAL PROCESS-IMAGE, V19, P793, DOI 10.1016/j.image.2004.06.003
   REININGER RC, 1983, IEEE T COMMUN, V31, P835, DOI 10.1109/TCOM.1983.1095893
   Rusanovskyy D., 2013, P 3 M ITU T ISO IEC, P1
   Sansli D.B., 2014, P 3DTV C TRUE VISION, P1
   Schwarz H, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P101, DOI 10.1109/PCS.2012.6213296
   Shao F, 2014, IEEE J EM SEL TOP C, V4, P106, DOI 10.1109/JETCAS.2014.2298314
   Shao F, 2013, IEEE T MULTIMEDIA, V15, P1843, DOI 10.1109/TMM.2013.2269897
   Shao F, 2012, IEEE T MULTIMEDIA, V14, P157, DOI 10.1109/TMM.2011.2169045
   Sullivan GJ, 2013, IEEE J-STSP, V7, P1001, DOI 10.1109/JSTSP.2013.2283657
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tech G., 2013, P 5 M ITU T ISO IEC
   Vandenberghe L., 2008, CONVEX OPTIMIZATION, P215
   Wang QF, 2012, IEEE T CIRC SYST VID, V22, P875, DOI 10.1109/TCSVT.2011.2181229
   Wiegand T, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P542, DOI 10.1109/ICIP.2001.958171
   Xu Q, 2009, IEEE T CIRC SYST VID, V19, P1424, DOI 10.1109/TCSVT.2009.2026808
   Yao W, 2013, IEEE T BROADCAST, V59, P445, DOI 10.1109/TBC.2013.2248211
   Yuan H, 2011, IEEE T CIRC SYST VID, V21, P485, DOI 10.1109/TCSVT.2011.2125610
NR 34
TC 39
Z9 43
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2015
VL 17
IS 12
BP 2134
EP 2146
DI 10.1109/TMM.2015.2477682
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CW9JR
UT WOS:000365315500003
DA 2024-07-18
ER

PT J
AU Huang, Y
   Wang, W
   Wang, L
AF Huang, Yan
   Wang, Wei
   Wang, Liang
TI Unconstrained Multimodal Multi-Label Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-label learning; multi-task learning; multimodal learning;
   restricted Boltzmann machine
ID NEURAL-NETWORKS; REPRESENTATION; COLOR
AB Multimodal learning has been mostly studied by assuming that multiple label assignments are independent of each other and all the modalities are available. In this paper, we consider a more general problem where the labels contain dependency relationships and some modalities are likely to be missing. To this end, we propose a multi-label conditional restricted Boltzmann machine (ML-CRBM), which handles modality completion, fusion, and multi-label prediction in a unified framework. The proposed model is able to generate missing modalities based on observed ones, by explicitly modelling and sampling their conditional distributions. After that, it can discriminatively fuse multiple modalities to obtain shared representations under the supervision of class labels. To consider the co-occurrence of the labels, the proposed model formulates the multi-label prediction as a max-margin-based multi-task learning problem. Model parameters can be jointly learned by seeking a balance between being generative for modality generation and being discriminative for label prediction. We perform a series of experiments in terms of classification, visualization, and retrieval, and the experimental results clearly demonstrate the effectiveness of our method.
C1 [Huang, Yan; Wang, Wei; Wang, Liang] Chinese Acad Sci CASIA, Inst Automat, Natl Lab Pattern Recognit, Ctr Res Intelligent Percept & Comp, Beijing 100190, Peoples R China.
   [Wang, Liang] CASIA, Inst Automat, CAS Ctr Excellence Brain Sci & Intelligence Techn, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; Institute of Automation, CAS
RP Huang, Y (corresponding author), Chinese Acad Sci CASIA, Inst Automat, Natl Lab Pattern Recognit, Ctr Res Intelligent Percept & Comp, Beijing 100190, Peoples R China.
EM yhuang@nlpr.ia.ac.cn; wangwei@nlpr.ia.ac.cn; wangliang@nlpr.ia.ac.cn
RI Huang, Yan/HCH-6526-2022
OI Huang, Yan/0000-0002-8239-7229
FU National Natural Science Foundation of China [61175003, 61202328,
   61572504, 61420106015]; National Basic Research Program of China
   [2012CB316300]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61175003, Grant 61202328, Grant 61572504, and Grant
   61420106015, and by the National Basic Research Program of China under
   Grant 2012CB316300. The guest editor coordinating the review of this
   manuscript and approving it for publication was Dr. Guo-Jun Qi.
CR [Anonymous], 2013, P INT C MACH LEARN I
   [Anonymous], 2012, ADV NEURAL INFORM PR
   [Anonymous], 2010, PROC ACM SIGMM INT C
   [Anonymous], 2011, P ICML
   [Anonymous], 2014, Advances in neural information processing systems
   [Anonymous], 2000, Em: Proceedings of the 2000 ACM workshops on Multimedia, DOI DOI 10.1145/357744.357758
   [Anonymous], 2009, P ACM INT C IM VID R
   [Anonymous], 2004, Advances in Neural Information Processing Systems
   [Anonymous], 1995, STORAGE RETRIEVAL IM, DOI DOI 10.1117/12.205308
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863
   Chen N, 2012, IEEE T PATTERN ANAL, V34, P2365, DOI 10.1109/TPAMI.2012.64
   Guillaumin M, 2010, PROC CVPR IEEE, P902, DOI 10.1109/CVPR.2010.5540120
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2009, INT C NEURAL INF PRO, P1607
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hua Y, 2014, IEEE DATA MINING, P190, DOI 10.1109/ICDM.2014.65
   Huang J, 1997, PROC CVPR IEEE, P762, DOI 10.1109/CVPR.1997.609412
   Huang Y, 2013, IEEE IMAGE PROC, P2897, DOI 10.1109/ICIP.2013.6738596
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larochelle H., 2008, P 25 INT C MACH LEAR, P536
   Larochelle H, 2012, J MACH LEARN RES, V13, P643
   Lee H., 2009, P 26 ANN INT C MACH, P609
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   Masci J, 2014, IEEE T PATTERN ANAL, V36, P824, DOI 10.1109/TPAMI.2013.225
   Nguyen C, 2013, P 23 INT JOINT C ART, P1558
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Qi G., 2011, Proc. ACM International Conference on World Wide Web, P297
   Qi G.J., 2007, P 15 ACM INT C MULTI, P17, DOI DOI 10.1145/1291233.1291245
   Qi GJ, 2009, IEEE T PATTERN ANAL, V31, P1880, DOI 10.1109/TPAMI.2008.218
   Salakhutdinov R., 2007, P 24 INT C MACHINE L, P791
   Shapiro L.G., 2003, Computer Vision, Vsecond
   Smolensky P., 1986, Information processing in dynamical systems: Foundations of harmony theory
   Srivastava N., 2013, NIPS, P2094
   Srivastava Neelam., 2012, The Postcolonial Gramsci, P1
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sutskever I, 2007, J MACH LEARN RES, V2, P548
   Taylor G, 2009, P 26 ANN INT C MACH
   Taylor GW, 2007, ADV NEURAL INFORM PR, P1345, DOI DOI 10.7551/MITPRESS/7503.003.0173
   Verbeek J., 2010, Proc. ACM Multimedia Information Retrieval, P537
   Wang W, 2014, PROC VLDB ENDOW, V7, P649, DOI 10.14778/2732296.2732301
   Xie Pengtao., 2013, Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, P1806
   Xing E.P., 2005, P C UNCERTAINTY ARTI, P633
   Xu JJ, 2014, IEEE T MULTIMEDIA, V16, P403, DOI 10.1109/TMM.2013.2291218
   Yang JM, 2014, PROC CVPR IEEE, P320, DOI 10.1109/CVPR.2014.48
   Zhang ML, 2006, IEEE T KNOWL DATA EN, V18, P1338, DOI 10.1109/TKDE.2006.162
   Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39
NR 51
TC 47
Z9 49
U1 1
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2015
VL 17
IS 11
SI SI
BP 1923
EP 1935
DI 10.1109/TMM.2015.2476658
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CV2QV
UT WOS:000364102400006
DA 2024-07-18
ER

PT J
AU Mukherjee, SS
   Robertson, NM
AF Mukherjee, Sankha S.
   Robertson, Neil Martin
TI Deep Head Pose: Gaze-Direction Estimation in Multimodal Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural networks (CNNs); deep learning; gaze direction;
   head-pose; RGB-D
ID TRACKING; PATTERNS; MODEL
AB In this paper we present a convolutional neural network (CNN)-based model for human head pose estimation in low-resolution multi-modal RGB-D data. We pose the problem as one of classification of human gazing direction. We further fine-tune a regressor based on the learned deep classifier. Next we combine the two models (classification and regression) to estimate approximate regression confidence. We present state-of-the-art results in datasets that span the range of high-resolution human robot interaction (close up faces plus depth information) data to challenging low resolution outdoor surveillance data. We build upon our robust head-pose estimation and further introduce a new visual attention model to recover interaction with the environment. Using this probabilistic model, we show that many higher level scene understanding like human-human/scene interaction detection can be achieved. Our solution runs in real-time on commercial hardware.
C1 [Mukherjee, Sankha S.; Robertson, Neil Martin] Heriot Watt Univ, Visionlab, Edinburgh Res Partnership Engn & Math, Edinburgh EH14 4AS, Midlothian, Scotland.
   [Mukherjee, Sankha S.; Robertson, Neil Martin] Univ Edinburgh, Edinburgh EH8 9YL, Midlothian, Scotland.
C3 Heriot Watt University; University of Edinburgh
RP Mukherjee, SS (corresponding author), Heriot Watt Univ, Visionlab, Edinburgh Res Partnership Engn & Math, Edinburgh EH14 4AS, Midlothian, Scotland.
EM sm794@hw.ac.uk; n.m.robertson@hw.ac.uk
FU Engineering and Physical Sciences Research Council (EPSRC)
   [EP/K014277/1]; MOD University Defence Research Collaboration in Signal
   Processing; EPSRC [EP/K014277/1] Funding Source: UKRI
FX This work was supported by the Engineering and Physical Sciences
   Research Council (EPSRC) under Grant EP/K014277/1 and by the MOD
   University Defence Research Collaboration in Signal Processing. The
   guest editor coordinating the review of this manuscript and approving it
   for publication was Prof. Benoit Huet.
CR Alexandre L. A., 2014, P 13 INT C INT AUT S, P889
   [Anonymous], 2011, P INT S INT SIGN PRO
   [Anonymous], 2014, CORR
   [Anonymous], 2015, Deep Residual Learning for Image Recognition
   [Anonymous], 2015, P IEEE COMP SOC C CO
   Balas VE., 2007, 2006 World Automation Congress, WAC'06, P1
   Baxter RH, 2015, IEEE SIGNAL PROC LET, V22, P578, DOI 10.1109/LSP.2014.2364458
   Belyaev A, 2013, SIAM J IMAGING SCI, V6, P660, DOI 10.1137/12087092X
   BenAbdelkader C, 2010, LECT NOTES COMPUT SC, V6316, P518, DOI 10.1007/978-3-642-15567-3_38
   Benfold B., 2008, BMVC, P1
   Benfold B, 2011, IEEE I CONF COMP VIS, P2344, DOI 10.1109/ICCV.2011.6126516
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Boker SM, 2002, PSYCHOL METHODS, V7, P338, DOI 10.1037//1082-989X.7.3.338
   Cazzato D, 2014, SENSORS-BASEL, V14, P8363, DOI 10.3390/s140508363
   Chen C, 2012, PROC CVPR IEEE, P1544, DOI 10.1109/CVPR.2012.6247845
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Deng J., 2009, VIS SCI SOC, V186
   Fanelli G, 2013, INT J COMPUT VISION, V101, P437, DOI 10.1007/s11263-012-0549-0
   Fanelli G, 2011, PROC CVPR IEEE, P617, DOI 10.1109/CVPR.2011.5995458
   Funes Mora KennethAlberto., 2012, Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Conference on, P25, DOI DOI 10.1109/CVPRW.2012.6239182
   Garcia-Perez A., 2019, Designing and tracking knowledge management metrics, P163
   Gourier N, 2007, LECT NOTES COMPUT SC, V4122, P270
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Han B, 2014, PATTERN RECOGN LETT, V45, P145, DOI 10.1016/j.patrec.2014.03.017
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Ioffe S., 2015, INT C MACHINE LEARN, P448, DOI DOI 10.5555/3045118.3045167
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kendon A., 2004, GESTURE VISIBLE ACTI
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Langton SRH, 2004, PERCEPT PSYCHOPHYS, V66, P752, DOI 10.3758/BF03194970
   Ma BP, 2014, NEUROCOMPUTING, V143, P97, DOI 10.1016/j.neucom.2014.06.014
   Mardia K. V., 2009, DIRECTIONAL STAT, V494
   Marín-Jiménez MJ, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.22
   Padeleris P., 2012, COMPUTER VISION PATT, P42
   Peng X, 2015, COMPUT VIS IMAGE UND, V136, P92, DOI 10.1016/j.cviu.2015.03.008
   Robertson N, 2006, LECT NOTES COMPUT SC, V3952, P402
   Shuai Tang, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P525, DOI 10.1007/978-3-642-37444-9_41
   Simonyan K., 2014, CORR
   Siriteerakul T., 2010, 2010 Fourth Pacific-Rim Symposium on Image and Video Technology (PSIVT), P362, DOI 10.1109/PSIVT.2010.67
   Stiefelhagen R., 2004, P POINTING04 ICPR WO
   Tang YC, 2012, PROC CVPR IEEE, P2264, DOI 10.1109/CVPR.2012.6247936
   Tosato D, 2013, IEEE T PATTERN ANAL, V35, P1972, DOI 10.1109/TPAMI.2012.263
   Wang QC, 2015, SIGNAL PROCESS, V112, P34, DOI 10.1016/j.sigpro.2014.07.011
   Yang B, 2012, PROC CVPR IEEE, P1918, DOI 10.1109/CVPR.2012.6247892
NR 46
TC 121
Z9 130
U1 0
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2015
VL 17
IS 11
SI SI
BP 2094
EP 2107
DI 10.1109/TMM.2015.2482819
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CV2QV
UT WOS:000364102400021
DA 2024-07-18
ER

PT J
AU Ren, DN
   Chan, SHG
   Cheung, GN
   Zhao, V
   Frossard, P
AF Ren, Dongni
   Chan, S. -H. Gary
   Cheung, Gene
   Zhao, Vicky
   Frossard, Pascal
TI Anchor View Allocation for Collaborative Free Viewpoint Video Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Digital video broadcasting; multimedia computing
ID MULTIVIEW VIDEO; DELAY; MESH; COMPRESSION; NUCLEOLUS
AB In free viewpoint video, a viewer can choose at will any camera angle or the so-called "virtual view" to observe a dynamic 3-D scene, enhancing his/her depth perception. The virtual view is synthesized using texture and depth videos of two anchor camera views via depth-image-based rendering (DIBR). We consider, for the first time, collaborative live streaming of a free viewpoint video, where a group of users may interactively pull and cooperatively share streams of different anchor views. There is a cost to access the anchor views from the live source, a cost to "reconfigure" the peer network due to a change in selected anchors during view switching, and a distortion cost due to the distance of the virtual views to the received anchor views at users. We optimize the anchor views allocated to users so as to minimize the overall streaming cost given by the access cost, reconfiguration cost, and view distortion cost. We first show that, if the reconfiguration cost due to view switching is negligible, the view allocation problem can be optimally and efficiently solved in polynomial time using dynamic programming. For the case of non-negligible reconfiguration cost, the problem becomes NP-hard. We thus present a locally optimal and centralized algorithm inspired by Lloyd's algorithm used in non-uniform scalar quantization. We further propose a distributed algorithm with convergence guarantee, where each peer group independently makes merge-and-split decisions with a well-defined fairness criteria. Simulation results show that our algorithms achieve low streaming cost due to its excellent anchor view allocation.
C1 [Ren, Dongni; Chan, S. -H. Gary] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong 999077, Hong Kong, Peoples R China.
   [Cheung, Gene] Natl Inst Informat, Tokyo 1018430, Japan.
   [Zhao, Vicky] Univ Alberta, Elect & Comp Engn Dept, Edmonton, AB T6G 2R3, Canada.
   [Frossard, Pascal] Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland.
C3 Hong Kong University of Science & Technology; Research Organization of
   Information & Systems (ROIS); National Institute of Informatics (NII) -
   Japan; University of Alberta; Swiss Federal Institutes of Technology
   Domain; Ecole Polytechnique Federale de Lausanne
RP Ren, DN (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong 999077, Hong Kong, Peoples R China.
EM tonyren@cse.ust.hk; gchan@cse.ust.hk; cheung@nii.ac.jp;
   vzhao@ece.ualberta.ca; pascal.frossard@epfl.ch
RI Cheung, Gene/AAB-9284-2020; Frossard, Pascal/AAF-2268-2019
OI Cheung, Gene/0000-0002-5571-4137; Chan, Gary Shueng
   Han/0000-0003-4207-764X
FU Hong Kong Research Grant Council (RGC) General Research Fund [610713];
   HKUST [FSGRF13EG15]; Hong Kong Innovation and Technology Fund [UIM/246];
   Grants-in-Aid for Scientific Research [23700136] Funding Source: KAKEN
FX This work was supported in part by the Hong Kong Research Grant Council
   (RGC) General Research Fund under Grant 610713, HKUST under Grant
   FSGRF13EG15, and the Hong Kong Innovation and Technology Fund under
   Grant UIM/246. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Klara Nahrstedt.
CR [Anonymous], P INT WORKSH QUAL SE
   [Anonymous], N HOLLAND SYST SCI E
   [Anonymous], P 27 PICT COD S CHIC
   [Anonymous], SIGN INF PROC ASS AN
   [Anonymous], P IEEE WORKSH STREAM
   [Anonymous], 2010, P SPIE
   [Anonymous], IEEE INT C IM PROC G
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P3179, DOI 10.1109/TIP.2011.2158230
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P744, DOI 10.1109/TIP.2010.2070074
   Daribo Ismael, 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P167, DOI 10.1109/MMSP.2010.5662013
   Deb K., 2001, MULTIOBJECTIVE OPTIM
   Faigle U, 2001, INT J GAME THEORY, V30, P79, DOI 10.1007/s001820100065
   Flierl M, 2007, IEEE T CIRC SYST VID, V17, P1474, DOI 10.1109/TCSVT.2007.903780
   Florêncio D, 2009, INT CONF ACOUST SPEE, P657, DOI 10.1109/ICASSP.2009.4959669
   Fromen B, 1997, EUR J OPER RES, V98, P626, DOI 10.1016/0377-2217(95)00341-X
   Fujii T, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P437, DOI 10.1109/ICME.2006.262566
   Gersho A., 2003, Vector Quantization and Signal Compression
   Gokturk S. B., 2004, 2004 C COMPUTER VISI, V2004, P35
   Haowen Liu, 2011, Proceedings of the 2011 International Conference on Network Computing and Information Security (NCIS), P195, DOI 10.1109/NCIS.2011.137
   Huang H, 2012, IEEE INFOCOM SER, P2791, DOI 10.1109/INFCOM.2012.6195701
   Ilkoo Ahn, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P109, DOI 10.1109/ICME.2012.95
   Jin X, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P913, DOI 10.1109/ICME.2006.262668
   Kurutepe E, 2007, IEEE T CIRC SYST VID, V17, P1558, DOI 10.1109/TCSVT.2007.903664
   Liu Z, 2012, IEEE ICC, P2048, DOI 10.1109/ICC.2012.6363933
   Lu X., 2010, P 19 INT C COMP COMM, ppp
   Macchiavello B, 2012, IEEE IMAGE PROC, P1653, DOI 10.1109/ICIP.2012.6467194
   Magharei N, 2007, IEEE INFOCOM SER, P1424
   Mark W. R., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P7, DOI 10.1145/253284.253292
   Meng XF, 2013, COMPUT ELECTR ENG, V39, P326, DOI 10.1016/j.compeleceng.2012.11.001
   Merkle P, 2009, SIGNAL PROCESS-IMAGE, V24, P73, DOI 10.1016/j.image.2008.10.010
   Oh KJ, 2009, PCS: 2009 PICTURE CODING SYMPOSIUM, P233
   Owen G., 1995, GAME THEORY
   Ren D, 2008, IEEE INFOCOM SER, P1732
   Ren DN, 2014, IEEE T MULTIMEDIA, V16, P1874, DOI 10.1109/TMM.2014.2332139
   Ren DN, 2009, IEEE T MULTIMEDIA, V11, P1446, DOI 10.1109/TMM.2009.2032677
   Saad Walid, 2008, ICC Workshops 2008 - IEEE International Conference on Communications Workshops, P311, DOI 10.1109/ICCW.2008.65
   Scharstein D., 2007, PROC IEEE C COMPUT V, P1
   Sharma P, 2005, IEEE ICC, P1549
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiu XY, 2012, IEEE T MULTIMEDIA, V14, P1109, DOI 10.1109/TMM.2012.2191267
   Yiu WPK, 2007, IEEE MULTIMEDIA, V14, P50, DOI 10.1109/MMUL.2007.30
   Zhang M, 2007, IEEE J SEL AREA COMM, V25, P1678, DOI 10.1109/JSAC.2007.071207
NR 43
TC 10
Z9 12
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2015
VL 17
IS 3
BP 307
EP 322
DI 10.1109/TMM.2015.2389714
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CE1QB
UT WOS:000351585700004
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Mao, QR
   Dong, M
   Huang, ZW
   Zhan, YZ
AF Mao, Qirong
   Dong, Ming
   Huang, Zhengwei
   Zhan, Yongzhao
TI Learning Salient Features for Speech Emotion Recognition Using
   Convolutional Neural Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Affective-salient discriminative feature analysis; convolutional neural
   networks; feature learning; speech emotion recognition
AB As an essential way of human emotional behavior understanding, speech emotion recognition (SER) has attracted a great deal of attention in human-centered signal processing. Accuracy in SER heavily depends on finding good affect-related, discriminative features. In this paper, we propose to learn affect-salient features for SER using convolutional neural networks (CNN). The training of CNN involves two stages. In the first stage, unlabeled samples are used to learn local invariant features (LIF) using a variant of sparse auto-encoder (SAE) with reconstruction penalization. In the second step, LIF is used as the input to a feature extractor, salient discriminative feature analysis (SDFA), to learn affect-salient, discriminative features using a novel objective function that encourages feature saliency, orthogonality, and discrimination for SER. Our experimental results on benchmark datasets show that our approach leads to stable and robust recognition performance in complex scenes (e. g., with speaker and language variation, and environment distortion) and outperforms several well-established SER features.
C1 [Mao, Qirong; Dong, Ming] Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA.
   [Mao, Qirong; Huang, Zhengwei; Zhan, Yongzhao] Jiangsu Univ, Dept Comp Sci & Commun Engn, Zhenjiang 212013, Jiangsu, Peoples R China.
C3 Wayne State University; Jiangsu University
RP Mao, QR (corresponding author), Jiangsu Univ, Dept Comp Sci & Commun Engn, Zhenjiang 212013, Jiangsu, Peoples R China.
EM qr@mail.ujs.edu.cn; mdong@cs.wayne.edu; zhengwei.hg@gmail.com;
   yzzhan@mail.ujs.edu.cn
FU National Nature Science Foundation of China [61272211, 61170126]; Six
   Talent Peaks Foundation of Jiangsu Province [DZXX-027]
FX This work was supported in part by the National Nature Science
   Foundation of China under Grants 61272211 and 61170126, and by the Six
   Talent Peaks Foundation of Jiangsu Province under Grant DZXX-027. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Gokhan Tur.
CR [Anonymous], 1997, 5 EUR C SPEECH COMM
   [Anonymous], 2010, 2010 IEEE 39 APPL IM, DOI DOI 10.1109/AIPR.2010.5759701
   [Anonymous], 2009, THESIS U ERLANGEN NU
   Athanaselis T, 2005, NEURAL NETWORKS, V18, P437, DOI 10.1016/j.neunet.2005.03.008
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Burkhardt F, 2005, INTERSPEECH, V5, P1517, DOI DOI 10.21437/INTERSPEECH.2005-446
   Deng J, 2013, INT CONF AFFECT, P511, DOI 10.1109/ACII.2013.90
   El Ayadi M, 2011, PATTERN RECOGN, V44, P572, DOI 10.1016/j.patcog.2010.09.020
   Eyben P., 2009, PROC IEEE 4 INT HUMA, P576
   Gharavian D, 2012, NEURAL COMPUT APPL, V21, P2115, DOI 10.1007/s00521-011-0643-1
   Guo ZY, 2013, IEEE T MULTIMEDIA, V15, P621, DOI 10.1109/TMM.2012.2234729
   Haq S., 2009, INT C AUDITORY VISUA, P53
   Hu H, 2007, INTERSPEECH 2007: 8TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION, VOLS 1-4, P1013
   Huang Z., 2014, P 22 ACM INT C MULT
   Jun Deng, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4818, DOI 10.1109/ICASSP.2014.6854517
   Koolagudi SG, 2012, INT J SPEECH TECHNOL, V15, P495, DOI 10.1007/s10772-012-9150-8
   Koolagudi SG, 2012, INT J SPEECH TECHNOL, V15, P265, DOI 10.1007/s10772-012-9139-3
   Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
   Le D, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P216, DOI 10.1109/ASRU.2013.6707732
   Lee CM, 2005, IEEE T SPEECH AUDI P, V13, P293, DOI 10.1109/TSA.2004.838534
   Luengo I, 2010, IEEE T MULTIMEDIA, V12, P490, DOI 10.1109/TMM.2010.2051872
   Lugger Marko, 2009, 2009 17th European Signal Processing Conference (EUSIPCO 2009), P1225
   Mao QR, 2010, INT J HUM ROBOT, V7, P245, DOI 10.1142/S0219843610002088
   Mao X, 2010, IEICE T INF SYST, VE93D, P2324, DOI 10.1587/transinf.E93.D.2324
   Morrison D, 2007, SPEECH COMMUN, V49, P98, DOI 10.1016/j.specom.2006.11.004
   Nwe TL, 2003, SPEECH COMMUN, V41, P603, DOI 10.1016/S0167-6393(03)00099-2
   Pantic Maja, 2008, International Journal of Automomous and Adaptive Communications Systems, V1, P168, DOI 10.1504/IJAACS.2008.019799
   Pao TL, 2007, LECT NOTES COMPUT SC, V4681, P997
   Ramakrishnan S, 2013, TELECOMMUN SYST, V52, P1467, DOI 10.1007/s11235-011-9624-z
   Rao KS, 2013, INT J SPEECH TECHNOL, V16, P143, DOI 10.1007/s10772-012-9172-2
   Rifai S, 2012, LECT NOTES COMPUT SC, V7577, P808, DOI 10.1007/978-3-642-33783-3_58
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Schmidt EM, 2011, 2011 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS (WASPAA), P65, DOI 10.1109/ASPAA.2011.6082328
   Schuller B, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P577
   Sheikhan M, 2013, NEURAL COMPUT APPL, V23, P215, DOI 10.1007/s00521-012-0814-8
   Stuhlsatz A, 2011, INT CONF ACOUST SPEE, P5688
   Sun R, 2011, LECT NOTES COMPUT SC, V6975, P425, DOI 10.1007/978-3-642-24571-8_54
   Sungrack Y., 2011, IEEE T AUDIO SPEECH, V20, P585
   Tawari A, 2010, IEEE T MULTIMEDIA, V12, P502, DOI 10.1109/TMM.2010.2058095
   Ververidis D, 2006, SPEECH COMMUN, V48, P1162, DOI 10.1016/j.specom.2006.04.003
   Wei Zhang, 2012, 2012 International Conference on Computing, Measurement, Control and Sensor Network (CMCS), P91, DOI 10.1109/CMCSN.2012.24
   Wöllmer M, 2012, INT CONF ACOUST SPEE, P4157, DOI 10.1109/ICASSP.2012.6288834
   Wu CH, 2011, IEEE T AFFECT COMPUT, V2, P10, DOI 10.1109/T-AFFC.2010.16
   Yu D., 2013, ICLR, P1
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zeng ZH, 2007, IEEE T MULTIMEDIA, V9, P424, DOI 10.1109/TMM.2006.886310
   Zhang SQ, 2008, LECT NOTES COMPUT SC, V5264, P457
NR 47
TC 383
Z9 419
U1 15
U2 133
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2014
VL 16
IS 8
BP 2203
EP 2213
DI 10.1109/TMM.2014.2360798
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AU4HL
UT WOS:000345571300011
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Tarvainen, J
   Sjöberg, M
   Westman, S
   Laaksonen, J
   Oittinen, P
AF Tarvainen, Jussi
   Sjoberg, Mats
   Westman, Stina
   Laaksonen, Jorma
   Oittinen, Pirkko
TI Content-Based Prediction of Movie Style, Aesthetics, and Affect: Data
   Set and Baseline Experiments
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Aesthetics; content-based analysis; felt affect; film; machine learning;
   modeling; perceived affect; style
ID CONTENT REPRESENTATION; CIRCUMPLEX MODEL; WORD USAGE; EMOTION;
   EXPRESSION; ELEMENTS; VALENCE; AROUSAL; SPEECH; ART
AB The affective content of a movie is often considered to be largely determined by its style and aesthetics. Recently, studies have attempted to estimate affective movie content with computational features, but results have been mixed, one of the main reasons being a lack of data on perceptual stylistic and aesthetic attributes of film, which would provide a ground truth for the features. The distinctions between energetic and tense arousal as well as perceived and felt affect are also often neglected. In this study, we present a data set of ratings by 73 viewers of 83 stylistic, aesthetic, and affective attributes for a selection of movie clips containing complete scenes taken from mainstream movies. The affective attributes include the temporal progression of perceived and felt valence and arousal within the clips. The data set is aimed to be used to train algorithms that predict viewer assessments based on low-level computational features. With this data set, we performed a baseline study modeling the relation between a large selection of low-level computational features (i.e., visual, auditory, and temporal) and perceptual stylistic, aesthetic, and affective attributes of movie clips. Two algorithms were compared in a realistic prediction scenario: linear regression and the neural-network-based Extreme Learning Machine (ELM). Felt and perceived affect as well as stylistic attributes were shown to be equally easy to predict, whereas the prediction of aesthetic attributes failed. The performance of the ELM predictor was overall found to be slightly better than the linear regression. A feature selection experiment illustrated that features from all low-level computational modalities, visual, auditory and temporal, contribute to the prediction of the affect assessments. We have made our assessment data and extracted computational features publicly available.
C1 [Tarvainen, Jussi; Westman, Stina; Oittinen, Pirkko] Aalto Univ, Sch Sci, Dept Media Technol, Espoo 02150, Finland.
   [Sjoberg, Mats; Laaksonen, Jorma] Aalto Univ, Sch Sci, Dept Informat & Comp Sci, Espoo 02150, Finland.
C3 Aalto University; Aalto University
RP Tarvainen, J (corresponding author), Aalto Univ, Sch Sci, Dept Media Technol, Espoo 02150, Finland.
EM jussi.tarvainen@aalto.fi; mats.sjoberg@aalto.fi; stina.westman@aalto.fi;
   jorma.laaksonen@aalto.fi; pirkko.oittinen@aalto.fi
RI Laaksonen, Jorma/Q-1307-2016; Sjöberg, Mats/E-8332-2012; Westman,
   Stina/H-3358-2012
OI Laaksonen, Jorma/0000-0001-7218-3131; Westman,
   Stina/0000-0002-0497-9864; Sjoberg, Mats/0000-0002-3157-7668
FU Academy of Finland [255745, 251170]; Doctoral Program in User-Centered
   Information Technology; Academy of Finland (AKA) [255745] Funding
   Source: Academy of Finland (AKA)
FX This work was supported in part by the Next Media Tivit SHOK project
   under Grants 255745 and 251170 of the Academy of Finland and the
   Doctoral Program in User-Centered Information Technology. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Shin'ichi Satoh.
CR Adams B, 2002, IEEE T MULTIMEDIA, V4, P472, DOI 10.1109/TMM.2002.802016
   Adams B., 2001, ICME 2001 P IEEE INT, P1056
   Anderson E., 1999, LAPACK USERSGUIDE, Vthird
   [Anonymous], 2002, Introduction to MPEG- 7: Multimedia content description interface
   [Anonymous], AESTHETICS FILM
   [Anonymous], 2008, Empirical Studies of the Arts, DOI DOI 10.2190/EM.26.2.D
   [Anonymous], 2000, HDB EMOTIONS
   [Anonymous], P IEEE INT C MULT EX
   Arifin S., 2006, PROGRAMMABLE LOGIC A, P2
   Arifin S., 2007, Proceedings of the 15th International Conference on Multimedia, P68
   Augustin MD, 2012, I-PERCEPTION, V3, P319, DOI 10.1068/i0511aap
   Augustin MD, 2012, ACTA PSYCHOL, V139, P187, DOI 10.1016/j.actpsy.2011.10.004
   Baveye Y, 2013, INT CONF AFFECT, P13, DOI 10.1109/ACII.2013.9
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Benini S, 2011, IEEE T MULTIMEDIA, V13, P1356, DOI 10.1109/TMM.2011.2163058
   Bordwell David., 1997, HIST FILM STYLE
   Bordwell David., 1990, Film Art: An Introduction
   Canini L, 2009, IEEE IMAGE PROC, P1821, DOI 10.1109/ICIP.2009.5413556
   Carroll Noel., 1999, PASSIONATE VIEWS, P21
   Cutting JE, 2012, J EXP PSYCHOL HUMAN, V38, P1476, DOI 10.1037/a0027737
   Drucker H, 1997, ADV NEUR IN, V9, P155
   EKMAN P, 1969, SCIENCE, V164, P86, DOI 10.1126/science.164.3875.86
   Gabrielsson A, 2003, SER AFFECTIVE SCI, P503
   Ghyka Matila., 1977, GEOMETRY ART LIFE
   Gouyon F., 2000, P COST G 6 C DIG AUD, P3
   Greenwald M. K., 1989, Journal of Psychophysiology, V3, P51
   GREY JM, 1978, J ACOUST SOC AM, V63, P1493, DOI 10.1121/1.381843
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Ilie G, 2006, MUSIC PERCEPT, V23, P319, DOI 10.1525/mp.2006.23.4.319
   Irie G, 2009, IEEE INT CON MULTI, P522, DOI 10.1109/ICME.2009.5202548
   Jacobsen T, 2004, PSYCHOL REP, V94, P1253, DOI 10.2466/PR0.94.3.1253-1260
   Jain Sanjay K., 2009, 2009 24th International Symposium on Computer and Information Sciences (ISCIS), P575, DOI 10.1109/ISCIS.2009.5291884
   Juslin PN, 2004, J NEW MUSIC RES, V33, P217, DOI 10.1080/0929821042000317813
   Kallinen K., 2006, THESIS
   Kallinen K, 2006, MUSIC SCI, V10, P191, DOI 10.1177/102986490601000203
   Kang H.-B., 2003, Proceedings of the 11th ACM International Conference on Multimedia, P259
   Knautz K, 2011, J DOC, V67, P975, DOI 10.1108/00220411111183555
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Lartillot O., 2007, P DAFX, P81
   Leder H, 2004, BRIT J PSYCHOL, V95, P489, DOI 10.1348/0007126042369811
   Li DG, 2001, PATTERN RECOGN LETT, V22, P533, DOI 10.1016/S0167-8655(00)00119-7
   Lin XQ, 2008, 2008 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND INFORMATION TECHNOLOGY, PROCEEDINGS, P264, DOI 10.1109/MMIT.2008.87
   Lipscomb S.D., 1994, Psychomusicology, V13, P60, DOI DOI 10.1037/H0094101
   Mackendrick A., 2006, FILM MAKING
   MATTHEWS G, 1990, BRIT J PSYCHOL, V81, P17, DOI 10.1111/j.2044-8295.1990.tb02343.x
   Mitry Jean., 1997, AESTHETICS PSYCHOL C
   MURRAY IR, 1993, J ACOUST SOC AM, V93, P1097, DOI 10.1121/1.405558
   Parke R., 2007, Computers in Entertainment (CIE), V5, P1, DOI DOI 10.1145/1316511
   Picard R. W., 1997, AFFECTIVE COMPUTING
   Plantinga C, 2012, NEW LITERARY HIST, V43, P455
   Posner J, 2005, DEV PSYCHOPATHOL, V17, P715, DOI 10.1017/S0954579405050340
   Rasheed Z., 2003, IEEE T CIRCUITS SYST, V1, P1
   RUSSELL JA, 1977, J RES PERS, V11, P273, DOI 10.1016/0092-6566(77)90037-X
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Scherer K. R., 2000, APPRAISAL PROCESSES, P92
   Schimmack U, 2001, COGNITION EMOTION, V15, P81, DOI 10.1080/0269993004200123
   Schimmack U, 2002, EMOTION, V2, P412, DOI 10.1037//1528-3542.2.4.412
   Sicheng Zhao, 2011, Proceedings of the Sixth International Conference on Image and Graphics (ICIG 2011), P795, DOI 10.1109/ICIG.2011.181
   Silvia PJ, 2011, EMPIR STUD ARTS, V29, P73, DOI 10.2190/EM.29.1.e
   Simons RF, 1999, PSYCHOPHYSIOLOGY, V36, P619, DOI 10.1111/1469-8986.3650619
   Sinnerbrink R, 2012, SCREEN, V53, P148, DOI 10.1093/screen/hjs007
   Smeaton AF, 2010, COMPUT VIS IMAGE UND, V114, P411, DOI 10.1016/j.cviu.2009.03.011
   Smith GregM., 2007, Film Structure and the Emotion System
   Soleymani M., 2009, Affective Computing and Intelligent Interaction and Workshops, P1
   Soleymani M., 2008, P 2 ACM WORKSHOP MUL, P32, DOI DOI 10.1145/1460676.1460684
   Soleymani M, 2014, IEEE T MULTIMEDIA, V16, P1075, DOI 10.1109/TMM.2014.2305573
   Sonnenschein EdwardA., 1925, What Is Rhythm?
   SPOTTISWOODE R., 1950, A Grammar of Film
   Sun K, 2009, IEEE INT CON MULTI, P566, DOI 10.1109/ICME.2009.5202559
   Tan E.S., 1996, EMOTION STRUCTURE NA
   Tarvainen J, 2013, LECT NOTES COMPUT SC, V8212, P52, DOI 10.1007/978-3-319-02714-2_5
   Teixeira R. M. A., 2010, 5 IEEE INT C FUT INF, P1
   Thayer Robert E., 1990, The Biopsychology of Mood and Arousal
   VALDEZ P, 1994, J EXP PSYCHOL GEN, V123, P394, DOI 10.1037/0096-3445.123.4.394
   Wang HL, 2006, IEEE T CIRC SYST VID, V16, P689, DOI 10.1109/TCSVT.2006.873781
   Wu JX, 2011, IEEE T PATTERN ANAL, V33, P1489, DOI 10.1109/TPAMI.2010.224
   Xu M., 2008, Proceedings of the Ninth International Workshop on Multimedia Data Mining, P26
   Xu M, 2011, IEEE IMAGE PROC, DOI 10.1109/ICIP.2011.6116510
   Zhang SL, 2009, IEEE IMAGE PROC, P1853, DOI 10.1109/ICIP.2009.5413590
NR 80
TC 25
Z9 26
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2014
VL 16
IS 8
BP 2085
EP 2098
DI 10.1109/TMM.2014.2357688
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science; Telecommunications
GA AU4HL
UT WOS:000345571300001
DA 2024-07-18
ER

PT J
AU Zhu, ZL
   Guo, FD
   Yu, H
   Chen, C
AF Zhu, Zhiliang
   Guo, Fangda
   Yu, Hai
   Chen, Chen
TI Fast Single Image Super-Resolution via Self-Example Learning and Sparse
   Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Approximate K-singular value decomposition; sample mean square error;
   self-example; single image super-resolution; sparse representation
ID MEAN SQUARED ERROR; RECONSTRUCTION
AB In this paper, we propose a novel algorithm for fast single image super-resolution based on self-example learning and sparse representation. We propose an efficient implementation based on the K-singular value decomposition (SVD) algorithm, where we replace the exact SVD computation with a much faster approximation, and we employ the straightforward orthogonal matching pursuit algorithm, which is more suitable for our proposed self-example-learning-based sparse reconstruction with far fewer signals. The patches used for dictionary learning are efficiently sampled from the low-resolution input image itself using our proposed sample mean square error strategy, without an external training set containing a large collection of high-resolution images. Moreover, the - optimization-based criterion, which is much faster than - optimization-based relaxation, is applied to both the dictionary learning and reconstruction phases. Compared with other super-resolution reconstruction methods, our low-dimensional dictionary is a more compact representation of patch pairs and it is capable of learning global and local information jointly, thereby reducing the computational cost substantially. Our algorithm can generate high-resolution images that have similar quality to other methods but with an increase in the computational efficiency greater than hundredfold.
C1 [Zhu, Zhiliang; Guo, Fangda; Yu, Hai] Northeastern Univ, Software Coll, Shenyang 110819, Peoples R China.
   [Guo, Fangda] Univ Pavia, Dept Elect Comp & Biomed Engn, I-27100 Pavia, Italy.
   [Chen, Chen] Univ Texas Dallas, Dept Elect Engn, Richardson, TX 75080 USA.
C3 Northeastern University - China; University of Pavia; University of
   Texas System; University of Texas Dallas
RP Guo, FD (corresponding author), Northeastern Univ, Software Coll, Shenyang 110819, Peoples R China.
EM zzl@mail.neu.edu.cn; gfdyes@gmail.com; yuhai@mail.neu.edu.cn;
   chenchen870713@gmail.com
RI , Chen_Chen/A-8825-2015; Guo, Fangda/AAR-3989-2020; YU, Hai/E-6831-2018
OI , Chen_Chen/0000-0003-3957-7061; YU, Hai/0000-0002-8024-1781; Guo,
   Fangda/0000-0003-2401-6499
FU National Natural Science Foundation of China [61374178, 61202085];
   Liaoning Provincial Natural Science Foundation of China [201202076];
   Specialized Research Fund for the Doctoral Program of Higher Education
   [20120042120010]; Ph.D. Start-Up Foundation of Liaoning Province, China
   [20111001, 20121001, 20121002]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61374178 and Grant 61202085, by the Liaoning
   Provincial Natural Science Foundation of China under Grant 201202076, by
   the Specialized Research Fund for the Doctoral Program of Higher
   Education under Grant 20120042120010, and by the the Ph.D. Start-Up
   Foundation of Liaoning Province, China under Grant 20111001, Grant
   20121001, and Grant 20121002. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Tommaso Melodia.(Corresponding author: Fangda Guo.)
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2011, P ICME
   [Anonymous], TRCS200808 TECHN ISR
   Baker S., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P83, DOI 10.1109/AFGR.2000.840616
   Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210
   Capel D, 2001, PROC CVPR IEEE, P627
   Chen C, 2014, REMOTE SENS-BASEL, V6, P5795, DOI 10.3390/rs6065795
   Chen C, 2014, IEEE J-STARS, V7, P1047, DOI 10.1109/JSTARS.2013.2295610
   Chen C, 2014, IEEE T GEOSCI REMOTE, V52, P365, DOI 10.1109/TGRS.2013.2240307
   Chen C, 2012, CONF REC ASILOMAR C, P608, DOI 10.1109/ACSSC.2012.6489079
   Chen C, 2011, CONF REC ASILOMAR C, P1193, DOI 10.1109/ACSSC.2011.6190204
   Dai S., 2007, Computer Vision and Pattern Recognition, P1
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Fan N, 2009, I C COMP GRAPH IM VI, P349, DOI 10.1109/CGIV.2009.90
   Farsiu S, 2004, IEEE T IMAGE PROCESS, V13, P1327, DOI 10.1109/TIP.2004.834669
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Gao XB, 2012, IEEE T IMAGE PROCESS, V21, P3194, DOI 10.1109/TIP.2012.2190080
   Gao XB, 2012, IEEE T IMAGE PROCESS, V21, P469, DOI 10.1109/TIP.2011.2161482
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Murray JF, 2006, J VLSI SIG PROC SYST, V45, P97, DOI 10.1007/s11265-006-9774-5
   Ogle WC, 2005, AEROSP CONF PROC, P2222
   Pan ZX, 2013, IEEE T GEOSCI REMOTE, V51, P4864, DOI 10.1109/TGRS.2012.2230270
   Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207
   Suetake N, 2008, OPT REV, V15, P26, DOI 10.1007/s10043-008-0005-0
   Sun J., 2008, PROC IEEE C COMPUT V, P1
   Merino MT, 2007, IEEE T GEOSCI REMOTE, V45, P1446, DOI 10.1109/TGRS.2007.893271
   Tipping M. E., 2003, ADV NEURAL INFORM PR, P1303
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang S., 2011, Multi-Platform/Multi-Sensor Remote Sensing and Mapping (M2RSM), 2011 International Workshop on, P1, DOI [10.1109/M2RSM.2011.5697375, DOI 10.1109/M2RSM.2011.5697375]
   Yuan QQ, 2012, IEEE T CIRC SYST VID, V22, P379, DOI 10.1109/TCSVT.2011.2163447
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang KB, 2012, PROC CVPR IEEE, P1114, DOI 10.1109/CVPR.2012.6247791
NR 36
TC 107
Z9 118
U1 1
U2 53
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2014
VL 16
IS 8
BP 2178
EP 2190
DI 10.1109/TMM.2014.2364976
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AU4HL
UT WOS:000345571300009
DA 2024-07-18
ER

PT J
AU Dahmane, M
   Meunier, J
AF Dahmane, Mohamed
   Meunier, Jean
TI Prototype-Based Modeling for Facial Expression Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Facial expression recognition; HOG; prototype facial expression models;
   registration; SIFT-flow
ID RECOGNITION
AB Automatic facial expression analysis systems are aiming towards the application of computer vision techniques in human computer interaction, emotion analysis, and even medical care via a space mapping between the continuous emotion and a set of discrete expression categories. The main difficulty with these systems is the inherent problem of facial alignment due to person-specific appearance. Beside the facial representation problem, the same displayed facial expression may vary differently across humans; this can be true even for the same person in different contexts. To cope with these variable factors, we introduce the concept of prototype-based model as anchor modeling through a SIFT-flow registration. A set of prototype facial expression models is generated as a reference space of emotions on which face images are projected to generate a set of registered faces. To characterize the facial expression appearance, oriented gradients are processed on each registered image. We obtained the best results 87% with the person-independent evaluation strategy on JAFFE dataset (7-class expression recognition problem), and 83% on the complex setting of the GEMEP-FERA database (5-class problem).
C1 [Dahmane, Mohamed; Meunier, Jean] Univ Montreal, Dept Comp & Operat Res, Montreal, PQ H3C 3J7, Canada.
C3 Universite de Montreal
RP Dahmane, M (corresponding author), Univ Montreal, Dept Comp & Operat Res, Montreal, PQ H3C 3J7, Canada.
EM dahmanem@iro.umontreal.ca
FU Natural Sciences and Engineering Research Council of Canada (NSERC)
FX This work was supported by the Natural Sciences and Engineering Research
   Council of Canada (NSERC). The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Nicu Sebe.
CR Aleksic PS, 2006, IEEE T INF FOREN SEC, V1, P3, DOI 10.1109/TIFS.2005.863510
   [Anonymous], 1977, FACIAL ACTION CODING
   [Anonymous], INT J COMPUT THEORY
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], P CVPR WORKSH COMP V
   [Anonymous], ACOUST SPEECH SIG PR
   [Anonymous], PSYCHOPHYSIOL
   [Anonymous], 1998, STAT LEARNING THEORY
   [Anonymous], P INT C COMP VIS
   [Anonymous], BLUEPRINT AFFECTIVE
   [Anonymous], P EUR C COMP VIS
   [Anonymous], IMAG SCI BIOMED ENG
   [Anonymous], 2005, P IEEE INT C MULT EX
   [Anonymous], 2008, 8 IEEE INT C AUT FAC, DOI DOI 10.1109/AFGR.2008.4813406
   [Anonymous], SEMANTIC RATINGS DAT
   [Anonymous], P IEEE INT C IM PROC
   [Anonymous], P INT C COMP VIS
   [Anonymous], P INT C COMP VIS
   [Anonymous], 2006, Proc. Joint Conf. Inform. Sci. Issue Adv. Intell. Syst. Res
   Argyle M., 1990, BODILY COMMUNICATION, V2nd
   Bartlett M. S., 2006, Journal of Multimedia, V1, DOI 10.4304/jmm.1.6.22-35
   Bihan Jiang, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P314, DOI 10.1109/FG.2011.5771416
   Black MJ, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P660, DOI 10.1109/ICCV.1998.710788
   Buciu I, 2004, INT C PATT RECOG, P288, DOI 10.1109/ICPR.2004.1334109
   Çeliktutan O, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-13
   Cheng F, 2010, IEEE T NEURAL NETWOR, V21, P1685, DOI 10.1109/TNN.2010.2064176
   Chew SW, 2012, IEEE T SYST MAN CY B, V42, P1006, DOI 10.1109/TSMCB.2012.2194485
   Chu WS, 2013, PROC CVPR IEEE, P3515, DOI 10.1109/CVPR.2013.451
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cottrell G.W., 2003, Computational, Geometric, and Process Perspectives on Facial Cognition: Contexts and Challenges
   Cristianini N., 2012, An introduction to support vector machines and other kernel-based learning methods, V13th
   Cristinacce D., 2006, BRIT MACH VIS C, V1, P3
   Cruz A, 2012, INT C PATT RECOG, P1880
   Dahmane M., 2012, 2012 11th International Conference on Information Sciences, Signal Processing and their Applications (ISSPA), P175, DOI 10.1109/ISSPA.2012.6310540
   Dahmane Mohamed, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P884, DOI 10.1109/FG.2011.5771368
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dhall Abhinav, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P878, DOI 10.1109/FG.2011.5771366
   Donato G, 1999, IEEE T PATTERN ANAL, V21, P974, DOI 10.1109/34.799905
   Feng X., 2005, Pattern Recognition and Image Analysis, V15, P546
   Fu XF, 2008, ICNC 2008: FOURTH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 4, PROCEEDINGS, P115, DOI 10.1109/ICNC.2008.94
   Grites T., 2008, Academic advising: a comprehensive handbook, P1, DOI [10.1109/AFGR.2008.4813379, DOI 10.1109/AFGR.2008.4813379]
   Guo GD, 2003, PROC CVPR IEEE, P346
   Hsu C. W., 2010, Technical Report
   Hu Y., 2008, PROC IEEE INT C AUTO, P1
   Hu YX, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P651
   Kanade T., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P46, DOI 10.1109/AFGR.2000.840611
   Kotsia I, 2008, IMAGE VISION COMPUT, V26, P1052, DOI 10.1016/j.imavis.2007.11.004
   Levi K, 2004, PROC CVPR IEEE, P53
   Lucey S., 2007, Investigating Spontaneous Facial Action Recognition through AAM Representations of the Face
   Lucey S, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P155
   Lue HS, 2006, WCICA 2006: SIXTH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION, VOLS 1-12, CONFERENCE PROCEEDINGS, P2517
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   Lyons MJ, 1999, IEEE T PATTERN ANAL, V21, P1357, DOI 10.1109/34.817413
   MEHRABIAN A, 1968, PSYCHOL TODAY, V2, P53
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   MPLab, MPLAB GENKI DAT
   O'Toole AJ, 2005, IEEE T PATTERN ANAL, V27, P812, DOI 10.1109/TPAMI.2005.90
   Pantic M, 2005, IEEE SYS MAN CYBERN, P3358
   Pantic M., 2007, FACE RECOGNITION, P377
   Rentzeperis E, 2006, INT FED INFO PROC, V204, P187
   Sebe N, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P517, DOI 10.1109/AFGR.2004.1301585
   Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Shih F., 2010, Image Processing and Pattern Recognition: Fundamentals and Techniques
   Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154
   Soyel H., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P585, DOI 10.1109/FG.2011.5771463
   Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962
   Tian YL, 2005, HANDBOOK OF FACE RECOGNITION, P247, DOI 10.1007/0-387-27257-7_12
   Uddin MZ, 2009, IEEE T CONSUM ELECTR, V55, P2216, DOI 10.1109/TCE.2009.5373791
   Valstar Michel F., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P921, DOI 10.1109/FG.2011.5771374
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wallhoff F., Facial expressions and emotion database
   Wang J, 2007, COMPUT VIS IMAGE UND, V108, P19, DOI 10.1016/j.cviu.2006.10.011
   Wang YL, 2008, PPAR RES, V2008, DOI 10.1155/2008/209629
   Whitehill J, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P97
   Whitehill J, 2009, IEEE T PATTERN ANAL, V31, P2106, DOI 10.1109/TPAMI.2009.42
   Wu Tingfan., 2010, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P42
   YACOOB Y, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P70, DOI 10.1109/CVPR.1994.323812
   Yang S H., 2011, 2011 International Workshop on Multi-Platform/Multi-Sensor Remote Sensing and Mapping, P1
   Yang SF, 2012, IEEE T SYST MAN CY B, V42, P980, DOI 10.1109/TSMCB.2012.2192269
   Zhang ZY, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P454, DOI 10.1109/AFGR.1998.670990
NR 82
TC 34
Z9 35
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1574
EP 1584
DI 10.1109/TMM.2014.2321113
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200007
DA 2024-07-18
ER

PT J
AU Guo, YL
   Sohel, F
   Bennamoun, M
   Wan, JW
   Lu, M
AF Guo, Yulan
   Sohel, Ferdous
   Bennamoun, Mohammed
   Wan, Jianwei
   Lu, Min
TI An Accurate and Robust Range Image Registration Algorithm for 3D Object
   Modeling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D modeling; feature description; feature detection; object
   reconstruction; range image registration
ID MATCHING ALGORITHM; RECOGNITION; REPRESENTATION; INTEGRATION; SIGNATURES
AB Range image registration is a fundamental research topic for 3D object modeling and recognition. In this paper, we propose an accurate and robust algorithm for pairwise and multi-view range image registration. We first extract a set of Rotational Projection Statistics (RoPS) features from a pair of range images, and perform feature matching between them. The two range images are then registered using a transformation estimation method and a variant of the Iterative Closest Point (ICP) algorithm. Based on the pairwise registration algorithm, we propose a shape growing based multi-view registration algorithm. The seed shape is initialized with a selected range image and then sequentially updated by performing pairwise registration between itself and the input range images. All input range images are iteratively registered during the shape growing process. Extensive experiments were conducted to test the performance of our algorithm. The proposed pairwise registration algorithm is accurate, and robust to small overlaps, noise and varying mesh resolutions. The proposed multi-view registration algorithm is also very accurate. Rigorous comparisons with the state-of-the-art show the superiority of our algorithm.
C1 [Guo, Yulan; Wan, Jianwei; Lu, Min] Natl Univ Def Technol, Coll Elect Sci & Engn, Changsha, Hunan, Peoples R China.
   [Guo, Yulan; Sohel, Ferdous; Bennamoun, Mohammed] Univ Western Australia, Sch Comp Sci & Software Engn, Crawley, WA, Australia.
C3 National University of Defense Technology - China; University of Western
   Australia
RP Guo, YL (corresponding author), Natl Univ Def Technol, Coll Elect Sci & Engn, Changsha, Hunan, Peoples R China.
EM yulan.guo@nudt.edu.cn; ferdous.sohel@uwa.edu.au;
   mohammed.bennamoun@uwa.edu.au; kermitwjw@139.com; lumin@nudt.edu.cn
RI Lu, min/JPL-4028-2023; Bennamoun, Mohammed/C-2789-2013; guo,
   yu/GQZ-1392-2022; Sohel, Ferdous/C-2428-2013
OI Bennamoun, Mohammed/0000-0002-6603-3257; Sohel,
   Ferdous/0000-0003-1557-4907; Zhou, Pei/0000-0002-3885-6512
FU China Scholarship Council (CSC); Australian Research Council
   [DE120102960, DP110102166]
FX This work was supported by a China Scholarship Council (CSC) scholarship
   and Australian Research Council grants (DE120102960, DP110102166). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Sheng-Wei (Kuan-Ta) Chen.
CR Alexiadis DS, 2013, IEEE T MULTIMEDIA, V15, P339, DOI 10.1109/TMM.2012.2229264
   [Anonymous], 2009, IEEE INT C ROB AUT
   [Anonymous], 1991, IEEE ICRA
   ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965
   Bariya P, 2012, INT J COMPUT VISION, V99, P232, DOI 10.1007/s11263-012-0526-7
   Benjemaa R., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P34, DOI 10.1007/BFb0054732
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Chen H, 2007, PATTERN RECOGN LETT, V28, P1252, DOI 10.1016/j.patrec.2007.02.009
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Dalley G, 2002, COMPUT VIS IMAGE UND, V87, P104, DOI 10.1006/cviu.2002.0986
   Dorai C, 1998, IEEE T PATTERN ANAL, V20, P83, DOI 10.1109/34.655652
   Du H, 2011, UBICOMP'11: PROCEEDINGS OF THE 2011 ACM INTERNATIONAL CONFERENCE ON UBIQUITOUS COMPUTING, P75
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224
   Gao Y, 2011, IEEE T MULTIMEDIA, V13, P1007, DOI 10.1109/TMM.2011.2160619
   Guennebaud G, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239474
   Guo Y., IEEE T PATT IN PRESS
   Guo YL, 2013, OPTIK, V124, P2727, DOI 10.1016/j.ijleo.2012.08.035
   Guo YL, 2013, INT J COMPUT VISION, V105, P63, DOI 10.1007/s11263-013-0627-y
   Guo YL, 2013, IEEE WORK APP COMP, P1, DOI 10.1109/WACV.2013.6474992
   He B, 2012, OPTICS LASER TECHNOL
   Huber DF, 2003, IMAGE VISION COMPUT, V21, P637, DOI 10.1016/S0262-8856(03)00060-X
   Izadi S, 2011, P UIST, P559, DOI DOI 10.1145/2047196.2047270
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Liu YH, 2010, IEEE T PATTERN ANAL, V32, P12, DOI 10.1109/TPAMI.2008.280
   Malassiotis S, 2007, IEEE T PATTERN ANAL, V29, P1285, DOI 10.1109/TPAMI.2007.1060
   Masuda T, 2002, COMPUT VIS IMAGE UND, V87, P51, DOI 10.1006/cviu.2002.0982
   Masuda T, 2009, COMPUT VIS IMAGE UND, V113, P1158, DOI 10.1016/j.cviu.2009.05.003
   Mian AS, 2006, IEEE T PATTERN ANAL, V28, P1584, DOI 10.1109/TPAMI.2006.213
   Mian AS, 2006, INT J COMPUT VISION, V66, P19, DOI 10.1007/s11263-005-3221-0
   Neugebauer P. J., 1997, International Journal of Shape Modeling, V3, P71, DOI 10.1142/S0218654397000070
   Nishino K., 2002, Proceedings of the Fifth Asian Conference on Computer Vision, P454
   Ponce J, 2002, COMPUTER VISION MODE, DOI 10.5555/580035
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Rusu RB, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3384, DOI 10.1109/IROS.2008.4650967
   Salvi J, 2007, IMAGE VISION COMPUT, V25, P578, DOI 10.1016/j.imavis.2006.05.012
   Seitz S. M., 2006, 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), V1, P519
   Stavropoulos G, 2010, IEEE T MULTIMEDIA, V12, P692, DOI 10.1109/TMM.2010.2053023
   STEIN F, 1992, IEEE T PATTERN ANAL, V14, P125, DOI 10.1109/34.121785
   ter Haar FB, 2007, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2007, PROCEEDINGS, P137, DOI 10.1109/SMI.2007.10
   Thomas D, 2011, COMPUT VIS IMAGE UND, V115, P649, DOI 10.1016/j.cviu.2010.11.016
   Tombari F., 2010, P ACM WORKSH 3D OBJ, P57, DOI DOI 10.1145/1877808.1877821
   Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26
   Weise Thibaut, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1630, DOI 10.1109/ICCVW.2009.5457479
   Weise Thibaut., 2008, 26th IEEE Conference on Computer Vision and Pattern Recognition, CVPR, P1
   Williams J, 2001, COMPUT VIS IMAGE UND, V81, P117, DOI 10.1006/cviu.2000.0884
   Yamany SM, 2002, IEEE T PATTERN ANAL, V24, P1105, DOI 10.1109/TPAMI.2002.1023806
   Yulan Guo, 2013, GRAPP IVAPP, P86, DOI DOI 10.5220/0004277600860093
NR 48
TC 114
Z9 126
U1 7
U2 60
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1377
EP 1390
DI 10.1109/TMM.2014.2316145
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600019
DA 2024-07-18
ER

PT J
AU Wang, F
   Sun, ZH
   Jiang, YG
   Ngo, CW
AF Wang, Feng
   Sun, Zhanhu
   Jiang, Yu-Gang
   Ngo, Chong-Wah
TI Video Event Detection Using Motion Relativity and Feature Selection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature selection; motion relativity; video event detection
AB Event detection plays an essential role in video content analysis. In this paper, we present our approach based on motion relativity and feature selection for video event detection. First, we propose a new motion feature, namely Expanded Relative Motion Histogram of Bag-of-Visual-Words (ERMH-BoW) to employ motion relativity for event detection. In ERMH-BoW, by representing what aspect of an event with Bag-of-Visual-Words (BoW), we construct relative motion histograms between different visual words to depict the objects' activities or how aspect of the event. ERMH-BoW thus integrates both what and how aspects for a complete event description. Meanwhile, we show that by employing motion relativity, ERMH-BoW is invariant to the varying camera movement and able to honestly describe the object activities in an event. Furthermore, compared with other motion features, ERMH-BoW encodes not only the motion of objects, but also the interactions between different objects/scenes. Second, to address the high-dimensionality problem of the ERMH-BoW feature, we further propose an approach based on information gain and informativeness weighting to select a cleaner and more discriminative set of features. Our experiments carried out on several challenging datasets provided by TRECVID for the MED (Multimedia Event Detection) task demonstrate that our proposed approach outperforms the state-of-the-art approaches for video event detection.
C1 [Wang, Feng; Sun, Zhanhu] E China Normal Univ, Dept Comp Sci & Technol, Shanghai 200241, Peoples R China.
   [Jiang, Yu-Gang] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
   [Ngo, Chong-Wah] City Univ Hong Kong, Dept Comp Sci, Kowloon Tong, Hong Kong, Peoples R China.
C3 East China Normal University; Fudan University; City University of Hong
   Kong
RP Wang, F (corresponding author), E China Normal Univ, Dept Comp Sci & Technol, Shanghai 200241, Peoples R China.
EM fwang@cs.ecnu.edu.cn; ygj@fudan.edu.cn; cwngo@cs.cityu.edu.hk
OI Ngo, Chong Wah/0000-0003-4182-8261
FU Research Grants Council of the Hong Kong Special Administrative Region,
   China [CityU 120213]; National Natural Science Foundation of China
   [61103127, 61375016]; Shanghai Pujiang Program [12PJ1402700];
   Fundamental Research Funds for the Central Universities
FX This work was supported in part by the grant from the Research Grants
   Council of the Hong Kong Special Administrative Region, China (CityU
   120213), the National Natural Science Foundation of China (No. 61103127
   and 61375016), Shanghai Pujiang Program (No. 12PJ1402700), and the
   Fundamental Research Funds for the Central Universities. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Shin'ichi Satoh.
CR Aly R., 2012, P NIST TRECVID WORKS
   [Anonymous], 2008, P IEEE C COMP VIS PA
   [Anonymous], P NIST TRECVID WORKS
   [Anonymous], P INT C IM VID RETR
   [Anonymous], P 14 INT C MACH LEAR
   [Anonymous], P INT WORKSH VER LAR
   [Anonymous], 2008, P IEEE C COMP VIS PA
   [Anonymous], P EUR C COMP VIS
   Ardizzone E., 1999, P IEEE INT C MULT CO, V2
   Boiman O., 2005, P INT C COMP VIS
   Burghouts GJ, 2009, COMPUT VIS IMAGE UND, V113, P48, DOI 10.1016/j.cviu.2008.07.003
   Cao L., 2012, P NIST TRECVID WORKS
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen M.-Y., 2009, Tech. Rep. CMU-CS- 09-161
   Cheng H., 2012, P NIST TRECVID WORKS
   Cristianini N, 2002, ADV NEUR IN, V14, P367
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   Davis JW, 2001, IEEE WORKSHOP ON DETECTION AND RECOGNITION OF EVENTS IN VIDEO, PROCEEDINGS, P39, DOI 10.1109/EVENT.2001.938864
   Day Y. F., 1995, P INT C DAT ENG
   EBADOLLAHI S, 2006, P IEEE INT C MULT EX
   Everts I., 2013, P IEEE C COMP VIS PA
   Fiscus J., 2012 TRECVID MULTIME
   Gemert J., IEEE T PATTERN ANAL, V32, P1271
   Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711
   Haubold A., 2007, P INT C IM VID RETR
   Herbert B., 2006, P EUROPEAN C COMPUTE, P404
   Jiang Y. G., 2008, P ACM SIGIR
   Jingen Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3337, DOI 10.1109/CVPR.2011.5995353
   Ke Y., 2005, P INT C COMP VIS
   Kesorn K, 2012, IEEE T MULTIMEDIA, V14, P211, DOI 10.1109/TMM.2011.2170665
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Li K., 2012, P EUR C COMP VIS
   Li Z., 2008, Proceedings of the 16th ACM international conference on Multimedia, P671
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lucas B., 1981, DARPA IU WORKSHOP, P121
   Ma YF, 2003, EURASIP J APPL SIG P, V2003, P199, DOI 10.1155/S1110865703211021
   Martin A., 1997, Eurospeech
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Murata M., 2012, P NIST TRECVID WORKS
   Natarajan P., 2012, P NIST TRECVID WORKS
   Natarajan P., 2012, P IEEE C COMP VIS PA
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Shechtman E., 2005, P IEEE C COMP VIS PA
   Sun J., 2009, P INT C COMP VIS PAT
   Tamrakar A, 2012, PROC CVPR IEEE, P3681, DOI 10.1109/CVPR.2012.6248114
   Wang F., 2012, P NIST TRECVID WORKS
   Wang F., 2008, P ACM MULT C
   Wang H, 2011, PROC CVPR IEEE
   Wu X, 2009, P ACM MULT C
   Xu D, 2008, IEEE T PATTERN ANAL, V30, P1985, DOI 10.1109/TPAMI.2008.129
   Yu S, 2012, P NIST TRECVID WORKS
   Zelnik-Manor L, 2006, IEEE T PATTERN ANAL, V28, P1530, DOI 10.1109/TPAMI.2006.194
   Zhang T., 2010, P ACM MULT C
   Zhu G., 2009, P ACM MULT C
NR 55
TC 23
Z9 23
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1303
EP 1315
DI 10.1109/TMM.2014.2315780
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600012
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Fang, Q
   Sang, JT
   Xu, CS
   Rui, Y
AF Fang, Quan
   Sang, Jitao
   Xu, Changsheng
   Rui, Yong
TI Topic-Sensitive Influencer Mining in Interest-Based Social Media
   Networks via Hypergraph Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hypergraph learning; influencer mining; topic modeling
ID IMAGE RETRIEVAL; CLASSIFICATION
AB Social media is emerging as a new mainstream means of interacting around online media. Social influence mining in social networks is therefore of critical importance in real-world applications such as friend suggestion and photo recommendation. Social media is inherently multimodal, including rich types of user contributed content and social link information. Most of the existing research suffers from two limitations: 1) only utilizing the textual information, and/or 2) only analyzing the generic influence but ignoring the more important topic-level influence. To address these limitations, in this paper we develop a novel Topic-Sensitive Influencer Mining (TSIM) framework in interest-based social media networks. Specifically, we take Flickr as the study platform. People in Flickr interact with each other through images. TSIM aims to find topical influential users and images. The influence estimation is determined with a hypergraph learning approach. In the hypergraph, the vertices represent users and images, and the hyperedges are utilized to capture multi-type relations including visual-textual content relations among images, and social links between users and images. Algorithmwise, TSIM first learns the topic distribution by leveraging user-contributed images, and then infers the influence strength under different topics for each node in the hypergraph. Extensive experiments on a real-world dataset of more than 50 K images and 70 K comment/favorite links from Flickr have demonstrated the effectiveness of our proposed framework. In addition, we also report promising results of friend suggestion and photo recommendation via TSIM on the same dataset.
C1 [Fang, Quan; Sang, Jitao; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Rui, Yong] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Microsoft
   Research Asia; Microsoft
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM qfang@nlpr.ia.ac.cn; jtsang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn;
   yon-grui@microsoft.com
RI xu, cj/HJZ-3488-2023
FU National Basic Research Program of China [2012CB316304]; National
   Natural Science Foundation of China [61225009]; Beijing Natural Science
   Foundation [4131004]; Singapore National Research Foundation under its
   International Research Centre @ Singapore Funding Initiative
FX This work was supported in part by National Basic Research Program of
   China (No. 2012CB316304), National Natural Science Foundation of China
   (No. 61225009), and Beijing Natural Science Foundation (No. 4131004).
   This work also was supported by the Singapore National Research
   Foundation under its International Research Centre @ Singapore Funding
   Initiative and administered by the IDM Programme Office. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Vasileios Mezaris.
CR Agarwal S., 2006, P 23 INT C MACH LEAR, P17, DOI DOI 10.1145/1143844.1143847
   Anagnostopoulos A., 2008, P 14 ACM SIGKDD INT, P7, DOI 10.1145/1401890.1401897
   [Anonymous], 2011, P 19 ACM INT C MULT
   [Anonymous], 2004, P 12 ANN ACM INT C M, DOI [10.1145/1027527.1027608, DOI 10.1145/1027527.1027608]
   [Anonymous], 2011, P 17 ACM SIGKDD INT, DOI [DOI 10.1145/2020408, 10.1145/2020408.2020430]
   [Anonymous], 2002, P 11 INT C WORLD WID, DOI DOI 10.1145/511446.511513
   [Anonymous], 2008, Cambridge Series in Statistical and Probabilistic Mathematics
   [Anonymous], 2011, Proceedings of the 19th ACM international conference on Multimedia
   [Anonymous], 2008, Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM '08
   Bao BK, 2012, IEEE T MULTIMEDIA, V14, P199, DOI 10.1109/TMM.2011.2170557
   Bao BK, 2011, PATTERN RECOGN, V44, P598, DOI 10.1016/j.patcog.2010.10.001
   Blei David M., 2007, P NIPS
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Cheng A.-J., 2011, P 19 ACM INT C MULTI, P83
   Wang C, 2009, PROC CVPR IEEE, P1903, DOI [10.1109/CVPR.2009.5206800, 10.1109/CVPRW.2009.5206800]
   Crandall DavidJ., 2008, KDD, P160, DOI DOI 10.1145/1401890.1401914
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649
   Hotho A., 2006, P FGIR, V2006
   Huang YC, 2010, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2010.5540012
   Lin YR, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P527
   Liu Lu., 2010, CIKM, DOI DOI 10.1145/1871437.1871467
   Liu YB, 2009, LECT NOTES COMPUT SC, V5642, P84
   Neal R. M., 1999, Learning in Graphical Models, P355
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Page L, 1998, P 7 INT WORLD WID WE, DOI DOI 10.1007/978-3-319-08789-4_10
   Papadopoulos S, 2012, DATA MIN KNOWL DISC, V24, P515, DOI 10.1007/s10618-011-0224-z
   Press W., 1997, NUMERICAL RECIPES C
   Rogati M., 2010, Proceedings of the 19th international conference on World wide web, P981, DOI DOI 10.1145/1772690.1772790
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Singla P., 2008, Proceedings of the 17th international conference on World Wide Web, WWW '08, P655, DOI DOI 10.1145/1367497.1367586
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Tan S., 2011, TOMCCAP S, V7, P22
   Tang J, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P807
   Tao DC, 2006, IEEE T PATTERN ANAL, V28, P1088, DOI 10.1109/TPAMI.2006.134
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Tian XM, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2240136.2240139
   Torralba A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P273
   van Zwol Roelof., 2010, Multimedia, P1015
   Wang C., 2010, P 18 ACM INT C MULT, P391, DOI [10.1145/ 1873951.1874005, DOI 10.1145/1873951.1874005]
   Weng J., 2010, P 3 ACM INT C WEB SE, P261, DOI [10.1145/1718487.1718520, DOI 10.1145/1718487.1718520]
   Xie L, 2011, P 19 ACM INT C MULT, P53, DOI DOI 10.1145/2072298.2072307
   Yu J, 2013, PATTERN RECOGN, V46, P483, DOI 10.1016/j.patcog.2012.08.006
   Yu J, 2012, IEEE T SYST MAN CY B, V42, P1413, DOI 10.1109/TSMCB.2012.2192108
   Yu J, 2012, IEEE T IMAGE PROCESS, V21, P3262, DOI 10.1109/TIP.2012.2190083
   Yu J, 2011, IEEE T IMAGE PROCESS, V20, P3257, DOI 10.1109/TIP.2011.2158225
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhou Dengyong, 2006, 19 INT C NEURAL INFO, V19, P1601
   Zhu Jianke., 2008, P 16 ACM INT C MULTI, P41
   Zhuang Yueting., 2011, ACM MM, P1457
NR 56
TC 72
Z9 78
U1 1
U2 44
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 796
EP 812
DI 10.1109/TMM.2014.2298216
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500019
DA 2024-07-18
ER

PT J
AU Scott-Hayward, S
   Garcia-Palacios, E
AF Scott-Hayward, Sandra
   Garcia-Palacios, Emiliano
TI Channel Time Allocation PSO for Gigabit Multimedia Wireless Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimedia; particle swarm optimization; resource allocation; wireless
   personal area networks
ID UTILITY MAXIMIZATION; MANAGEMENT
AB This article introduces a resource allocation solution capable of handling mixed media applications within the constraints of a 60 GHz wireless network. The challenges of multimedia wireless transmission include high bandwidth requirements, delay intolerance and wireless channel availability. A new Channel Time Allocation Particle Swarm Optimization (CTA-PSO) is proposed to solve the network utility maximization (NUM) resource allocation problem. CTA-PSO optimizes the time allocated to each device in the network in order to maximize the Quality of Service (QoS) experienced by each user. CTA-PSO introduces network-linked swarm size, an increased diversity function and a learning method based on the personal best, Pbest, results of the swarm. These additional developments to the PSO produce improved convergence speed with respect to Adaptive PSO while maintaining the QoS improvement of the NUM. Specifically, CTA-PSO supports applications described by both convex and non-convex utility functions. The multimedia resource allocation solution presented in this article provides a practical solution for real-time wireless networks.
C1 [Scott-Hayward, Sandra; Garcia-Palacios, Emiliano] Queens Univ Belfast, Inst Elect Commun & Informat Technol, Belfast BT3 9DT, Antrim, North Ireland.
C3 Queens University Belfast
RP Scott-Hayward, S (corresponding author), Queens Univ Belfast, Inst Elect Commun & Informat Technol, Belfast BT3 9DT, Antrim, North Ireland.
EM s.scott-hayward@qub.ac.uk; e.garcia@ee.qub.ac.uk
FU EPSRC [EP/J006238/1, EP/G034303/1, EP/H049606/1, EP/K004379/1] Funding
   Source: UKRI
CR Abbas G, 2011, IET COMMUN, V5, P2371, DOI 10.1049/iet-com.2010.0163
   [Anonymous], 2011, WIR HOM VID MARK, P1
   [Anonymous], CISC VIS NETW IND FO
   Baguda Y. S., 2012, COMMUN NETW, P188
   Bo Wang, 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, P501
   Chiang M, 2005, IEEE INFOCOM SER, P2679
   Eberhart RC, 2001, IEEE C EVOL COMPUTAT, P81, DOI 10.1109/CEC.2001.934374
   Fazel M, 2005, IEEE DECIS CONTR P, P1867
   Goudarzi P., 2012, SIGNAL PROCESS IMAGE
   Hassan R., 2005, P 1 AIAA MULT DES OP
   Katsenou A, 2011, IEEE IMAGE PROC, P149, DOI 10.1109/ICIP.2011.6115728
   Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968
   Krone S, 2011, INT J MICROW WIREL T, V3, P189, DOI 10.1017/S1759078711000080
   Liu Damon Shing-Min, 2011, Proceedings of the 2011 Workshop on Digital Media and Digital Content Management (DMDCM 2011), P1, DOI 10.1109/DMDCM.2011.24
   Maltsev A., 2010, 80211090334R7 IEEE
   Niknam T, 2010, APPL ENERG, V87, P327, DOI 10.1016/j.apenergy.2009.05.016
   Park H, 2007, INT CONF ACOUST SPEE, P717
   Park H, 2007, IEEE T SIGNAL PROCES, V55, P3496, DOI 10.1109/TSP.2007.893755
   Scott S., 2012, P IEEE INT S ANT PRO, P1
   Seeling P, 2012, IEEE COMMUN SURV TUT, V14, P1142, DOI 10.1109/SURV.2011.082911.00067
   SHENKER S, 1995, IEEE J SEL AREA COMM, V13, P1176, DOI 10.1109/49.414637
   Shi L, 2008, COMPUT COMMUN, V31, P2257, DOI 10.1016/j.comcom.2008.02.016
   Shi Y., 1999, Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406), P1945, DOI 10.1109/CEC.1999.785511
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Touati C, 2006, COMPUT NETW, V50, P3242, DOI 10.1016/j.comnet.2005.12.006
   Vesterstrom J, 2004, IEEE C EVOL COMPUTAT, P1980
   Zhan ZH, 2009, IEEE T SYST MAN CY B, V39, P1362, DOI 10.1109/TSMCB.2009.2015956
NR 27
TC 15
Z9 16
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 828
EP 836
DI 10.1109/TMM.2014.2298211
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500021
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Duan, LY
   Ji, RR
   Chen, Z
   Huang, TJ
   Gao, W
AF Duan, Ling-Yu
   Ji, Rongrong
   Chen, Zhang
   Huang, Tiejun
   Gao, Wen
TI Towards Mobile Document Image Retrieval for Digital Library
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Digital library; Hamming space; inner-distance; JBIG2 compression; K-D
   tree; line drawing retrieval; mobile visual search; shape context
ID SHAPE; RECOGNITION; SIMILARITY
AB With the proliferation of mobile devices, recent years have witnessed an emerging potential to integrate mobile visual search techniques into digital library. Such a mobile application scenario in digital library has posed significant and unique challenges in document image search. The mobile photograph makes it tough to extract discriminative features from the landmark regions of documents, like line drawings, as well as text layouts. In addition, both search scalability and query delivery latency remain challenging issues in mobile document search. The former relies on an effective yet memory-light indexing structure to accomplish fast online search, while the latter puts a bit budget constraint of query images over the wireless link. In this paper, we propose a novel mobile document image retrieval framework, consisting of a robust Local Inner-distance Shape Context (LISC) descriptor of line drawings, a Hamming distance KD-Tree for scalable and memory-light document indexing, as well as a JBIG2 based query compression scheme, together with a Retinex based enhancement and an OTSU based binarization, to reduce the latency of delivering query while maintaining query quality in terms of search performance. We have extensively validated the key techniques in this framework by quantitative comparison to alternative approaches.
C1 [Duan, Ling-Yu; Ji, Rongrong; Chen, Zhang; Huang, Tiejun; Gao, Wen] Peking Univ, Sch Elect Engn & Comp Sci, Inst Digital Media, Beijing 100871, Peoples R China.
   [Ji, Rongrong] Xiamen Univ, Sch Informat Sci & Technol, Xiamen, Peoples R China.
C3 Peking University; Xiamen University
RP Ji, RR (corresponding author), Peking Univ, Sch Elect Engn & Comp Sci, Inst Digital Media, Beijing 100871, Peoples R China.
EM lingyu@pku.edu.cn; rrji@pku.edu.cn; cjie@pku.edu.cn; tjhuang@pku.edu.cn;
   wgao@pku.edu.cn
RI Huang, Tiejun/D-6161-2011
FU Chinese Natural Science Foundation [61271311, 61121002]; ZTE
   Corporation; CADAL Project Program
FX This work was supported by the Chinese Natural Science Foundation under
   Contract No. 61271311 and No. 61121002, and in part by the Research Fund
   of ZTE Corporation, and the CADAL Project Program. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Chia-Wen Lin.
CR Agarwal S, 2004, IEEE T PATTERN ANAL, V26, P1475, DOI 10.1109/TPAMI.2004.108
   [Anonymous], P NIPS
   [Anonymous], 2009, P 18 INT C WORLD WID
   [Anonymous], 2012, INT J COMPUT VISION, DOI DOI 10.1007/s11263-011-0472-9
   [Anonymous], 2012, IARCS ANN C FDN SOFT, DOI DOI 10.4230/LIPICS.FSTTCS.2012.48
   [Anonymous], OPT ENG
   [Anonymous], JTC1SC29WG1N1359 ISO
   [Anonymous], 1SC29WG11MPEG99 ISOI
   [Anonymous], 2008, BMVC
   [Anonymous], P SPIE EL IM
   Basri R, 1998, VISION RES, V38, P2365, DOI 10.1016/S0042-6989(98)00043-1
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bhownik TK, 2004, LECT NOTES COMPUT SC, V3316, P814
   Cao Y, 2011, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2011.5995460
   Chandrasekhar Vijay, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2504, DOI 10.1109/CVPRW.2009.5206733
   Chen DM, 2008, IEEE DATA COMPR CONF, P143, DOI 10.1109/DCC.2009.33
   Duan L.-Y., 2012, PROC VCIR, P1
   Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49
   Freund Yoav, 2007, Advances in Neural Information Processing Systems, P473
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   KIMIA BB, 1995, INT J COMPUT VISION, V15, P189, DOI 10.1007/BF01451741
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mao S, 2003, PROC SPIE, V5010, P197, DOI 10.1117/12.476326
   Nakai T, 2006, LECT NOTES COMPUT SC, V3872, P541
   Nister David, 2006, CVPR
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pu JT, 2006, COMPUT AIDED DESIGN, V38, P249, DOI 10.1016/j.cad.2005.10.009
   Rahman ZU, 2004, J ELECTRON IMAGING, V13, P100, DOI 10.1117/1.1636183
   Rattarangsi A., 1990, Proceedings. 10th International Conference on Pattern Recognition (Cat. No.90CH2898-5), P923, DOI 10.1109/ICPR.1990.118242
   Schindler G., 2007, 2007 IEEE Conference on Computer Vision and Pattern Recognition, P1
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Tu ZW, 2004, LECT NOTES COMPUT SC, V3023, P195
   Won CS, 2002, ETRI J, V24, P23, DOI 10.4218/etrij.02.0102.0103
   Yeh T, 2004, PROC CVPR IEEE, P76
   Zhang W, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P33, DOI 10.1109/3dpvt.2006.80
NR 38
TC 17
Z9 17
U1 0
U2 53
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 346
EP 359
DI 10.1109/TMM.2013.2293063
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800006
DA 2024-07-18
ER

PT J
AU Papadopoulos, GT
   Apostolakis, KC
   Daras, P
AF Papadopoulos, Georgios Th.
   Apostolakis, Konstantinos C.
   Daras, Petros
TI Gaze-Based Relevance Feedback for Realizing Region-Based Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gaze analysis; gaze-tracking; image retrieval; relevance feedback
ID BIASED DISCRIMINANT-ANALYSIS; SYSTEM; POSE
AB In this paper, a gaze-based Relevance Feedback (RF) approach to region-based image retrieval is presented. Fundamental idea of the proposed method comprises the iterative estimation of the real-world objects (or their constituent parts) that are of interest to the user and the subsequent exploitation of this information for refining the image retrieval results. Primary novelties of this work are: a) the introduction of a new set of gaze features for realizing user's relevance assessment prediction at region-level, and b) the design of a time-efficient and effective object-based RF framework for image retrieval. Regarding the interpretation of the gaze signal, a novel set of features is introduced by formalizing the problem under a mathematical perspective, contrary to the exclusive use of explicitly defined features that are in principle derived from the psychology domain. Apart from the temporal attributes, the proposed features also represent the spatial characteristics of the gaze signal, which have not been extensively studied in the literature so far. On the other hand, the developed object-based RF mechanism aims at overcoming the main limitation of region-based RF approaches, i.e., the frequently inaccurate estimation of the regions of interest in the retrieved images. Moreover, the incorporation of a single-camera image processing-based gaze tracker makes the overall system cost efficient and portable. As it is shown by the experimental evaluation, the proposed method outperforms representative globaland region-based explicit RF approaches, using a challenging general-purpose image dataset.
C1 [Papadopoulos, Georgios Th.; Apostolakis, Konstantinos C.; Daras, Petros] Ctr Res & Technol Hellas CERTH, Inst Informat Technol, GR-57001 Thessaloniki, Greece.
C3 Centre for Research & Technology Hellas
RP Papadopoulos, GT (corresponding author), Ctr Res & Technol Hellas CERTH, Inst Informat Technol, GR-57001 Thessaloniki, Greece.
EM papad@iti.gr; kapostol@iti.gr; daras@iti.gr
RI Daras, Petros/F-5284-2012
OI Daras, Petros/0000-0003-3814-6710; Apostolakis,
   Konstantinos/0000-0002-4609-4079
FU European Commission [FP7-258749 CEEDs]
FX This work was supported by the European Commission under contract
   FP7-258749 CEEDs. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Changsheng Xu.
CR Aggarwal G, 2002, IEEE T MULTIMEDIA, V4, P201, DOI 10.1109/TMM.2002.1017734
   Ahlberg J., 2001, CANDIDE-3 -- an updated parameterized face
   [Anonymous], 1999, The Nature Statist. Learn. Theory
   [Anonymous], ADV MULTIMEDIA MODON
   [Anonymous], 1970, Distributions in Statistics: Continuous Univariate Distributions
   [Anonymous], 2009, ICMI MLMI
   [Anonymous], 2010, P 2010 S EYE TRACK R
   [Anonymous], 2009, P 17 ACM INT C MULT, DOI DOI 10.1145/1631272.1631463
   [Anonymous], 2004, Passive driver gaze tracking with active appearance models
   Chen JX, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P189, DOI 10.1145/1344471.1344518
   Cheng E., 2006, P 14 ANN ACM INT C M, P173
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   DEMENTHON DF, 1995, INT J COMPUT VISION, V15, P123, DOI 10.1007/BF01450852
   Djordjevic D, 2007, IEEE T CIRC SYST VID, V17, P313, DOI 10.1109/TCSVT.2007.890634
   Faro Alberto., 2010, Proceedings of the Symposium on Eye-Tracking Research Appli- cations (ETRA), P73, DOI [10.1145/1743666.1743684, DOI 10.1145/1743666.1743684]
   Grigorova A, 2007, IEEE T MULTIMEDIA, V9, P1183, DOI 10.1109/TMM.2007.902828
   Hajimirza SN, 2012, IEEE T MULTIMEDIA, V14, P805, DOI 10.1109/TMM.2012.2186792
   Hall M.A., 1999, Correlation-based Feature Selection for Machine Learning
   Hanjalic A, 2008, P IEEE, V96, P541, DOI 10.1109/JPROC.2008.916338
   Harris R.J., 2001, A primer of multivariate statistics, V3rd
   Ishikawa Y., 1998, MINDREADER QUERYING, P551
   Jiang W, 2006, IEEE T IMAGE PROCESS, V15, P702, DOI 10.1109/TIP.2005.863105
   Jing F, 2004, IEEE T CIRC SYST VID, V14, P672, DOI 10.1109/TCSVT.2004.826775
   Kelly D., 2003, SIGIR Forum, V37, P18, DOI 10.1145/959258.959260
   Kherfi ML, 2006, IEEE T IMAGE PROCESS, V15, P1017, DOI 10.1109/TIP.2005.863969
   Klami A., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P134, DOI DOI 10.1145/1460096.1460120
   Klami A., 2010, P IEEE WORKSH MACH L
   Kohavi R., 1995, IJCAI-95. Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, P1137
   Kowler E, 2011, VISION RES, V51, P1457, DOI 10.1016/j.visres.2010.12.014
   Lee EC, 2009, MACH VISION APPL, V20, P319, DOI 10.1007/s00138-008-0129-z
   Li CY, 2008, IEEE T MULTIMEDIA, V10, P447, DOI 10.1109/TMM.2008.917421
   Liu DZ, 2009, IEEE T KNOWL DATA EN, V21, P729, DOI 10.1109/TKDE.2008.188
   Lucas B.D., DARPA IMAGE UNDERSTA
   Mezaris V, 2004, INT J PATTERN RECOGN, V18, P701, DOI 10.1142/S0218001404003393
   Mitchell T.M., 1997, Machine learning, V45
   O'Connor J. Robertson., STUDENTS T TEST MACT
   Ohno T., 2002, Proceedings ETRA 2002. Eye Tracking Research and Applications Symposium, P125, DOI 10.1145/507072.507098
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Papadopoulos GT, 2011, COMPUT VIS IMAGE UND, V115, P1288, DOI 10.1016/j.cviu.2011.05.005
   Pogalin E, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P57
   Rayner K, 1998, PSYCHOL BULL, V124, P372, DOI 10.1037/0033-2909.124.3.372
   Salvucci DD, 2000, 2000 S EYE TRACKING, P71, DOI [10.1145/355017.355028, DOI 10.1145/355017.355028]
   Sang JT, 2012, IEEE T MULTIMEDIA, V14, P963, DOI 10.1109/TMM.2011.2181344
   Su JH, 2011, IEEE T KNOWL DATA EN, V23, P360, DOI 10.1109/TKDE.2010.124
   Tao DC, 2006, IEEE T MULTIMEDIA, V8, P716, DOI 10.1109/TMM.2005.861375
   Tao DC, 2006, IEEE T PATTERN ANAL, V28, P1088, DOI 10.1109/TPAMI.2006.134
   Tax DMJ, 2002, INT C PATT RECOG, P124, DOI 10.1109/ICPR.2002.1048253
   Terissi LD, 2010, J UNIVERS COMPUT SCI, V16, P903
   Tian XM, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2240136.2240139
   Tian XM, 2010, IEEE T IMAGE PROCESS, V19, P805, DOI 10.1109/TIP.2009.2035866
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   Vasconcelos N, 2000, ADV NEUR IN, V12, P977
   Widdel H., 1984, THEORETICAL APPL ASP, P21
   Yang Y, 2012, IEEE T PATTERN ANAL, V34, P723, DOI 10.1109/TPAMI.2011.170
   Zhang LN, 2012, IEEE T SYST MAN CY B, V42, P282, DOI 10.1109/TSMCB.2011.2165335
   Zhang Y, 2010, PROCEEDINGS OF CHINA-CANADA WORKSHOP ON FINANCIAL ENGINEERING AND ENTERPRISE RISK MANAGEMENT 2010, P37, DOI 10.1145/1743666.1743674
   Zhou XS, 2003, MULTIMEDIA SYST, V8, P536, DOI 10.1007/s00530-002-0070-3
   Zhu J, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P131, DOI 10.1109/AFGR.2002.1004144
NR 59
TC 19
Z9 20
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 440
EP 454
DI 10.1109/TMM.2013.2291535
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800013
DA 2024-07-18
ER

PT J
AU Phan, R
   Androutsos, D
AF Phan, Raymond
   Androutsos, Dimitrios
TI Robust Semi-Automatic Depth Map Generation in Unconstrained Images and
   Video Sequences for 2D to Stereoscopic 3D Conversion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computer vision; depth maps; graph cuts; image segmentation; motion
   estimation; object tracking; random walks; semi-automatic; 2D to 3D
   image conversion; 2D to 3D video conversion
AB We describe a system for robustly estimating synthetic depth maps in unconstrained images and videos, for semi-automatic conversion into stereoscopic 3D. Currently, this process is automatic or done manually by rotoscopers. Automatic is the least labor intensive, but makes user intervention or error correction difficult. Manual is the most accurate, but time consuming and costly. Noting the merits of both, a semi-automatic method blends them together, allowing for faster and accurate conversion. This requires user-defined strokes on the image, or over several keyframes for video, corresponding to a rough estimate of the depths. After, the rest of the depths are determined, creating depth maps to generate stereoscopic 3D content, with Depth Image Based Rendering to generate the artificial views. Depth map estimation can be considered as a multi-label segmentation problem: each class is a depth. For video, we allow the user to label only the first frame, and we propagate the strokes using computer vision techniques. We combine the merits of two well-respected segmentation algorithms: Graph Cuts and Random Walks. The diffusion from Random Walks, with the edge preserving of Graph Cuts should give good results. We generate good quality content, more suitable for perception, compared to a similar framework.
C1 [Phan, Raymond; Androutsos, Dimitrios] Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
C3 Toronto Metropolitan University
RP Phan, R (corresponding author), Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
EM rphan@ee.ryerson.ca; dimitri@ee.ryerson.ca
CR [Anonymous], DIGITAL SIGNAL PROCE
   [Anonymous], 2012, 2012 IEEE COMP SOC C
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5
   Chen Y., 2011, P SPIE ELECT IMAGING, V7863
   Fehn C, 2006, P IEEE, V94, P524, DOI 10.1109/JPROC.2006.870688
   Friedland G, 2005, IEEE INT SYM MULTIM, P253
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   Goferman S, 2010, PROC CVPR IEEE, P2376, DOI 10.1109/CVPR.2010.5539929
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Gururajan Arunkumar, 2010, Proceedings 2010 IEEE Southwest Symposium on Image Analysis & Interpretation (SSIAI), P129, DOI 10.1109/SSIAI.2010.5483900
   Guttmann M., 2009, P IEEE INT C COMP VI
   Kalal Z., 2010, IEEE TPAMI, V6, P1
   Kuo TY, 2012, INT CONF ACOUST SPEE, P1433, DOI 10.1109/ICASSP.2012.6288160
   Lang M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778812
   Liu L., 2012, P IEEE INT C CONS EL
   Lucas Bruce D., ITERATIVE IMAGE REGI, P674, DOI DOI 10.1109/HPDC.2004.1323531
   Mortensen E. N., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P191, DOI 10.1145/218380.218442
   Ozuysal M., 2007, P IEEE CVPR
   Park M., 2011, Proc. ACM Multimedia, P1557
   Phan R., 2013, P INT C DSP
   Phan R., 2013, P SPIE, V8648
   Phan R., 2011, P IEEE ICIP
   Phan R, 2010, COMPUT VIS IMAGE UND, V114, P66, DOI 10.1016/j.cviu.2009.07.004
   Schnyder L., 2011, P ICIP
   Tao M. W., 2012, P COMP GRAPH FOR EUR, V31
   Vijayanarasimhan S., 2012, P ECCV
   Wang H., 2011, P IEEE INT C MECH SC
   Wang OL, 2011, PITUITARY, 3RD EDITION, P47, DOI 10.1016/B978-0-12-380926-1.10003-3
   Yan T, 2013, INT J COMPUT VISION, V102, P293, DOI 10.1007/s11263-012-0593-9
   Zhang ZB, 2011, IEEE IMAGE PROC, P909, DOI 10.1109/ICIP.2011.6116707
NR 31
TC 44
Z9 47
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 122
EP 136
DI 10.1109/TMM.2013.2283451
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100011
DA 2024-07-18
ER

PT J
AU Yadati, K
   Katti, H
   Kankanhalli, M
AF Yadati, Karthik
   Katti, Harish
   Kankanhalli, Mohan
TI CAVVA: Computational Affective Video-in-Video Advertising
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Ad-insertion; affect; arousal; contextual advertising; marketing and
   consumer psychology; valence
ID CONTEXT; PUPIL
AB Advertising is ubiquitous in the online community and more so in the ever-growing and popular online video delivery websites (e. g., YouTube). Video advertising is becoming increasingly popular on these websites. In addition to the existing pre-roll/post-roll advertising and contextual advertising, this paper proposes an in-stream video advertising strategy-Computational Affective Video-in-Video Advertising (CAVVA). Humans being emotional creatures are driven by emotions as well as rational thought. We believe that emotions play a major role in influencing the buying behavior of users and hence propose a video advertising strategy which takes into account the emotional impact of the videos as well as advertisements. Given a video and a set of advertisements, we identify candidate advertisement insertion points (step 1) and also identify the suitable advertisements (step 2) according to theories from marketing and consumer psychology. We formulate this two part problem as a single optimization function in a non-linear 0-1 integer programming framework and provide a genetic algorithm based solution. We evaluate CAVVA using a subjective user-study and eye-tracking experiment. Through these experiments, we demonstrate that CAVVA achieves a good balance between the following seemingly conflicting goals of (a) minimizing the user disturbance because of advertisement insertion while (b) enhancing the user engagement with the advertising content. We compare our method with existing advertising strategies and show that CAVVA can enhance the user's experience and also help increase the monetization potential of the advertising content.
C1 [Yadati, Karthik; Kankanhalli, Mohan] Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
   [Katti, Harish] Indian Inst Sci, Ctr Neurosci, Bangalore 560012, Karnataka, India.
C3 National University of Singapore; Indian Institute of Science (IISC) -
   Bangalore
RP Yadati, K (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
EM nyadati@comp.nus.edu.sg; harish2006@gmail.com; mohan@comp.nus.edu.sg
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015
FU Singapore NRF under its IRC@SG Funding Initiative
FX This research was partially carried out at the SeSaMe Centre. It was
   supported by the Singapore NRF under its IRC@SG Funding Initiative and
   administered by the IDMPO. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Nicu Sebe.
CR Adany R, 2013, MULTIMEDIA SYST, V19, P79, DOI 10.1007/s00530-012-0284-y
   Bradley MM, 2008, PSYCHOPHYSIOLOGY, V45, P602, DOI 10.1111/j.1469-8986.2008.00654.x
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   Broach VC, 1995, J ADVERTISING, V24, P45, DOI 10.1080/00913367.1995.10673488
   Coulter KS, 1998, J ADVERTISING, V27, P41, DOI 10.1080/00913367.1998.10673568
   Farris P.W., 2010, Marketing Metrics: The Definitive Guide to Measuring Marketing Performance
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   HESS EH, 1960, SCIENCE, V132, P349, DOI 10.1126/science.132.3423.349
   KAMINS MA, 1991, J ADVERTISING, V20, P1
   Kankanhalli M., 2013, LNCS, P106
   Katti H., 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P319, DOI 10.1109/ISM.2011.57
   Kennedy Lyndon., 2009, WWW 09, P311, DOI [DOI 10.1145/1526709.1526752, 10.1145/1526709.1526752]
   Liao Wei-Shing., 2008, P 31 ANN INT ACM SIG, P767, DOI [10.1145/1390334.1390494, DOI 10.1145/1390334.1390494]
   Mei T, 2009, IEEE T CIRC SYST VID, V19, P1866, DOI 10.1109/TCSVT.2009.2026949
   Mei Tao., 2007, Proceedings of the 15th International Conference on Multimedia, P1075
   Olafadottir I., 2008, P 6 INT C METH TECHN
   Plessis E.D., 2005, The advertised mind: groundbreaking insights into how our brains respond to advertising
   Rasheed Z, 2003, PROC CVPR IEEE, P343
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   WHITLEY D, 1994, STAT COMPUT, V4, P65, DOI 10.1007/BF00175354
NR 20
TC 48
Z9 54
U1 5
U2 67
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 15
EP 23
DI 10.1109/TMM.2013.2282128
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100002
DA 2024-07-18
ER

PT J
AU Colonnese, S
   Cuomo, F
   Melodia, T
AF Colonnese, Stefania
   Cuomo, Francesca
   Melodia, Tommaso
TI An Empirical Model of Multiview Video Coding Efficiency for Wireless
   Multimedia Sensor Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multiview video coding; MVC efficiency model; video sensor networks.
ID OBJECT
AB We develop an empirical model of the Multiview Video Coding (MVC) performance that can be used to identify and separate situations when MVC is beneficial from cases when its use is detrimental in wireless multimedia sensor networks (WMSN). The model predicts the compression performance of MVC as a function of the correlation between cameras with overlapping fields of view. We define the common sensed area (CSA) between different views, and emphasize that it depends not only on geometrical relationships among the relative positions of different cameras, but also on various object-related phenomena, e. g., occlusions and motion, and on low-level phenomena such as variations in illumination. With these premises, we first experimentally characterize the relationship between MVC compression gain (with respect to single view video coding) and the CSA between views. Our experiments are based on the H. 264 MVC standard, and on a low-complexity estimator of the CSA that can be computed with low inter-node signaling overhead. Then, we propose a compact empirical model of the efficiency of MVC as a function of the CSA between views, and we validate the model with different multiview video sequences. Finally, we show how the model can be applied to typical scenarios in WMSN, i.e., to clustered or multi-hop topologies, and we show a few promising results of its application in the definition of cross-layer clustering and data aggregation procedures.
C1 [Colonnese, Stefania; Cuomo, Francesca] Univ Roma La Sapienza, DIET, I-00184 Rome, Italy.
   [Melodia, Tommaso] SUNY Buffalo, Dept Elect Engn, Buffalo, NY 14260 USA.
C3 Sapienza University Rome; State University of New York (SUNY) System;
   State University of New York (SUNY) Buffalo
RP Colonnese, S (corresponding author), Univ Roma La Sapienza, DIET, I-00184 Rome, Italy.
EM colonnese@infocom.uniroma1.it; francesca.cuomo@uniroma1.it;
   tmelodia@buffalo.edu
RI Cuomo, Francesca/H-2528-2012
OI Cuomo, Francesca/0000-0002-9122-7993; Melodia,
   Tommaso/0000-0002-2719-1789; Colonnese, Stefania/0000-0002-1807-2155
CR Akyildiz IF, 2007, COMPUT NETW, V51, P921, DOI 10.1016/j.comnet.2006.10.002
   [Anonymous], 2012, P IEEE GLOBECOM
   [Anonymous], JVTAD005
   Arora R, 2011, DISTRIBUTED VIDEO SENSOR NETWORKS, P41, DOI 10.1007/978-0-85729-127-1_3
   Chen Y, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/786015
   Dai R, 2009, IEEE T MULTIMEDIA, V11, P1148, DOI 10.1109/TMM.2009.2026100
   Fu YW, 2010, IEEE T MULTIMEDIA, V12, P717, DOI 10.1109/TMM.2010.2052025
   Huadong Ma, 2005, 2005 International Conference on Wireless Networks, Communications and Mobile Computing (IEEE Cat. No.05EX1133), P987
   Hwang JN, 2011, DISTRIBUTED VIDEO SENSOR NETWORKS, P103, DOI 10.1007/978-0-85729-127-1_7
   Kulathumani V, 2011, DISTRIBUTED VIDEO SENSOR NETWORKS, P373, DOI 10.1007/978-0-85729-127-1_25
   Li HL, 2011, VIDEO SEGMENTATION AND ITS APPLICATIONS, P1, DOI 10.1007/978-1-4419-9482-0_1
   Medeiros H, 2008, IEEE J-STSP, V2, P448, DOI 10.1109/JSTSP.2008.2001310
   Merkle P, 2007, IEEE T CIRC SYST VID, V17, P1461, DOI 10.1109/TCSVT.2007.903665
   Montserrat T, 2009, IEEE IMAGE PROC, P2353, DOI 10.1109/ICIP.2009.5413610
   Neri A, 1998, SIGNAL PROCESS, V66, P219, DOI 10.1016/S0165-1684(98)00007-3
   Pudlewski S, 2012, IEEE T MOBILE COMPUT, V11, P1060, DOI 10.1109/TMC.2011.175
   Sankaranarayanan AC, 2011, DISTRIBUTED VIDEO SENSOR NETWORKS, P85, DOI 10.1007/978-0-85729-127-1_6
   Sheng J J., 2010, Modern chemical enhanced oil recovery: Theory and practice, V1st, P1, DOI [10.1016/C2009-0-20241-8, DOI 10.1016/C2009-0-20241-8]
   Thirumalai V, 2013, J VIS COMMUN IMAGE R, V24, P649, DOI 10.1016/j.jvcir.2011.12.004
   Vetro A, 2011, P IEEE, V99, P626, DOI 10.1109/JPROC.2010.2098830
   Wang P, 2011, IEEE T MULTIMEDIA, V13, P388, DOI 10.1109/TMM.2010.2100374
NR 21
TC 22
Z9 23
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 1800
EP 1814
DI 10.1109/TMM.2013.2271475
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900007
OA Green Published
DA 2024-07-18
ER

PT J
AU Pudlewski, S
   Melodia, T
AF Pudlewski, Scott
   Melodia, Tommaso
TI Compressive Video Streaming: Design and Rate-Energy-Distortion Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compressed sensing; video surveillance; video encoding; multimedia
   sensor networks
ID RECONSTRUCTION; COMMUNICATION; IMAGE
AB Real-time encoding and error-resilient wireless transmission of multimedia content using traditional encoding techniques requires relatively high processing and transmission power, while pervasive surveillance and monitoring systems often referred to as wireless multimedia sensor networks (WMSNs) [1] are generally composed of low-power, low-complexity devices. To bridge this gap, this article introduces and analyzes a compressive video sensing (CVS) encoder designed to reduce the required energy and computational complexity at the source node. The proposed encoder leverages the properties of compressed sensing (CS) to overcome many of the limitations of traditional encoding techniques, specifically lack of resilience to channel errors, and high computational complexity. Recognizing the inadequacy of traditional rate-distortion analysis to account for the constraints introduced by resource-limited devices, we introduce the notion of rate-energy-distortion, based on which we develop an analytical/empirical model that predicts the received video quality when the overall energy available for both encoding and transmission of each frame of a video is fixed and limited and the transmissions are affected by channel errors. The model allows comparing the received video quality, computation time, and energy consumption per frame of different wireless streaming systems, and can be used to determine the optimal allocation of encoded video rate and channel encoding rate for a given available energy budget. Based on the proposed model, we show that the CVS video encoder outperforms (in an energy constrained system) two common encoders suitable for a wireless multimedia sensor network environment; H.264/AVC intra and motion JPEG (MJPEG). Extensive results show that CVS is able to deliver video at good quality (an SSIM value of 0.8) through lossy wireless networks with lower energy consumption per frame than competing encoders.
C1 [Pudlewski, Scott; Melodia, Tommaso] SUNY Buffalo, Dept Elect Engn, Buffalo, NY 14260 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Buffalo
RP Pudlewski, S (corresponding author), SUNY Buffalo, Dept Elect Engn, Buffalo, NY 14260 USA.
EM smp25@buffalo.edu; tmelodia@buffalo.edu
OI Melodia, Tommaso/0000-0002-2719-1789; Pudlewski,
   Scott/0000-0001-6028-5974
FU Direct For Computer & Info Scie & Enginr; Division Of Computer and
   Network Systems [1117121] Funding Source: National Science Foundation
CR AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784
   Akyildiz IF, 2007, COMPUT NETW, V51, P921, DOI 10.1016/j.comnet.2006.10.002
   [Anonymous], 2003, ADV VID COD GEN AUD
   [Anonymous], 1992, DIG COMPR COD CONT T
   Asif MS, 2010, IEEE J-STSP, V4, P421, DOI 10.1109/JSTSP.2009.2039174
   Boyd S., 2004, CONVEX OPTIMIZATION
   Bruckstein AM, 2009, SIAM REV, V51, P34, DOI 10.1137/060657704
   Campbell AT, 2008, IEEE INTERNET COMPUT, V12, P12, DOI 10.1109/MIC.2008.90
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candès EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124
   Chan T, 2008, CAM REPORTS
   Chikkerur S, 2011, IEEE T BROADCAST, V57, P165, DOI 10.1109/TBC.2011.2104671
   Do TT, 2009, IEEE IMAGE PROC, P1393, DOI 10.1109/ICIP.2009.5414631
   Donoho D., 2006, SPARSE SOLUTION UNDE
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Duarte MF, 2008, IEEE SIGNAL PROC MAG, V25, P83, DOI 10.1109/MSP.2007.914730
   Figueiredo MAT, 2007, IEEE J-STSP, V1, P586, DOI 10.1109/JSTSP.2007.910281
   Gan L., 2008, P EUR SIGN PROC C EU
   Girod B, 2005, P IEEE, V93, P71, DOI 10.1109/JPROC.2004.839619
   GRAPS A, 1995, IEEE COMPUT SCI ENG, V2, P50, DOI 10.1109/99.388960
   Grosky WI, 2007, IEEE MULTIMEDIA, V14, P8, DOI 10.1109/MMUL.2007.82
   Gu YY, 2007, SIGNAL PROCESS-IMAGE, V22, P237, DOI 10.1016/j.image.2006.12.013
   HAGENAUER J, 1988, IEEE T COMMUN, V36, P389, DOI 10.1109/26.2763
   He ZH, 2005, IEEE T CIRC SYST VID, V15, P645, DOI 10.1109/TCSVT.2005.846433
   He ZH, 2008, IEEE T CIRC SYST VID, V18, P596, DOI 10.1109/TCSVT.2008.918802
   Hemami SS, 2010, SIGNAL PROCESS-IMAGE, V25, P469, DOI 10.1016/j.image.2010.05.009
   JPEG2000 Requirements and Profiles, 1999, N1271 ISOIEC JTC1SC2
   Kang LW, 2009, INT CONF ACOUST SPEE, P1169, DOI 10.1109/ICASSP.2009.4959797
   Li M, 2012, IEEE INT WORKSH MULT, P1, DOI 10.1109/MMSP.2012.6343406
   Mun S, 2011, IEEE DATA COMPR CONF, P183, DOI 10.1109/DCC.2011.25
   Nesterov I.E., 1994, Interior-Point Polynomial Algorithms in Convex Programming
   Ostermann J., 2004, IEEE Circuits and Systems Magazine, V4, P7, DOI 10.1109/MCAS.2004.1286980
   Pudlewski S., 2010, P IEEE INT C COMM IC
   Pudlewski S., 2011, P IEEE GLOB COMM C G
   Pudlewski S, 2012, IEEE T MOBILE COMPUT, V11, P1060, DOI 10.1109/TMC.2011.175
   Romberg J, 2008, IEEE SIGNAL PROC MAG, V25, P14, DOI 10.1109/MSP.2007.914729
   Soro S, 2009, ADV MULTIMED, V2009, DOI 10.1155/2009/640386
   Stankovic Vladimir., 2008, Proc. of the European Signal Processing Conf.(EUSIPCO), P2
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   SWELDENS W, 1995, P SOC PHOTO-OPT INS, V2569, P68, DOI 10.1117/12.217619
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793
   Wakin M., 2006, P PICT COD S PCS B
   Wakin MB, 2006, IEEE IMAGE PROC, P1273, DOI 10.1109/ICIP.2006.312577
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   WIEGAND T, 2007, JVTX201
NR 47
TC 25
Z9 25
U1 0
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 2072
EP 2086
DI 10.1109/TMM.2013.2280245
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900028
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tawari, A
   Trivedi, MM
AF Tawari, Ashish
   Trivedi, Mohan Manubhai
TI Face Expression Recognition by Cross Modal Data Association
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Facial expression recognition; audio-visual expression recognition; key
   frames selection; multi-modal expression recognition; emotion
   recognition; affective computing; affect analysis
ID MULTIMODAL EMOTION RECOGNITION; LOCAL BINARY PATTERNS; TEXTURE
   CLASSIFICATION; FACIAL EXPRESSION; SPEECH-PERCEPTION; INFORMATION
AB We present a novel facial expression recognition framework using audio-visual information analysis. We propose to model the cross-modality data correlation while allowing them to be treated as asynchronous streams. We also show that our framework can improve the recognition performance while significantly reducing the computational cost by avoiding redundant or insignificant frame processing by incorporating auditory information. In particular, we design a single good image representation of image sequence by weighted sums of registered face images where the weights are derived using auditory features. We use a still image based technique for the expression recognition task. Our framework, however, can be generalized to work with dynamic features as well. We performed experiments using eNTERFACE' 05 audio-visual emotional database containing six archetypal emotion classes: Happy, Sad, Surprise, Fear, Anger and Disgust. We present one-to-one binary classification as well as multi-class classification performances evaluated using both subject dependent and independent strategies. Furthermore, we compare multi-class classification accuracies with those of previously published literature which use the same database. Our analyses show promising results.
C1 [Tawari, Ashish; Trivedi, Mohan Manubhai] Univ Calif San Diego, Comp Vis & Robot Res Lab, La Jolla, CA 92093 USA.
C3 University of California System; University of California San Diego
RP Tawari, A (corresponding author), Univ Calif San Diego, Comp Vis & Robot Res Lab, La Jolla, CA 92093 USA.
EM atawari@ucsd.edu; mtrivedi@soe.ucsd.edu
FU National Science Foundation; U.C. Discovery Program
FX We thank the sponsorships of National Science Foundation, U.C. Discovery
   Program and Industry Partners for supporting the research. We would also
   like to thank anonymous reviewers and editors for their constructive
   feedback and helpful suggestions.
CR [Anonymous], 2005, MORGAN KAUFMANN SERI
   [Anonymous], P IEEE INT C AUT FAC
   [Anonymous], 2011, International Journal of Wavelets Multiresolution and Information Processing, DOI DOI 10.1142/S021969130400041X
   Bartlett MS, 2005, PROC CVPR IEEE, P568
   Bihan Jiang, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P314, DOI 10.1109/FG.2011.5771416
   Boersma P., 1993, P I PHONETIC SCI, V17, P97, DOI DOI 10.1371/JOURNAL.PONE.0069107
   Cohen I, 2003, COMPUT VIS IMAGE UND, V91, P160, DOI 10.1016/S1077-3142(03)00081-X
   Datcu D, 2008, EUROMEDIA '2008, P58
   Doshi A, 2009, IEEE T INTELL TRANSP, V10, P453, DOI 10.1109/TITS.2009.2026675
   Gajsek R, 2010, LECT NOTES ARTIF INT, V6231, P275, DOI 10.1007/978-3-642-15760-8_35
   Hu C., 2004, P COMPUTER VISION PA, V5, P81
   Kanade T., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P46, DOI 10.1109/AFGR.2000.840611
   Lucey Simon., 2007, FACE RECOGNITION DEL, P275
   MACDONALD J, 1978, PERCEPT PSYCHOPHYS, V24, P253, DOI 10.3758/BF03206096
   Mansoorizadeh M, 2010, MULTIMED TOOLS APPL, V49, P277, DOI 10.1007/s11042-009-0344-2
   Martin O., 2006, 22 INT C DATA ENG WO, P8, DOI [DOI 10.1109/ICDEW.2006.145, 10.1109/ICDEW.2006.145]
   McCall JC, 2004, INT C PATT RECOG, P958, DOI 10.1109/ICPR.2004.1334688
   Munhall KG, 2004, PSYCHOL SCI, V15, P133, DOI 10.1111/j.0963-7214.2004.01502010.x
   Murphy-Chutorian E, 2010, IEEE T INTELL TRANSP, V11, P300, DOI 10.1109/TITS.2010.2044241
   NETI C, 2000, FIN WORKSH 2000 REP
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Ojansivu V, 2008, LECT NOTES COMPUT SC, V5099, P236, DOI 10.1007/978-3-540-69905-7_27
   Paleari M, 2009, LECT NOTES COMPUT SC, V5371, P435, DOI 10.1007/978-3-540-92892-8_44
   Picard R. W., 1997, AFFECTIVE COMPUTING
   Saragih JM, 2009, IEEE I CONF COMP VIS, P1034, DOI 10.1109/ICCV.2009.5459377
   Sebe N, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P517, DOI 10.1109/AFGR.2004.1301585
   Shivappa ST, 2010, P IEEE, V98, P1692, DOI 10.1109/JPROC.2010.2057231
   Song ML, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P877
   Songfan Yang, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P866, DOI 10.1109/FG.2011.5771364
   Tawari A., 2012, P INT C PATT REC
   TAWARI A, 2010, P IEEE INT VEH S, P174
   Tawari A., 2010, P INT C PATT REC
   Tawari A, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P2997, DOI 10.1109/IJCNN.2011.6033615
   Tawari A, 2010, IEEE T MULTIMEDIA, V12, P502, DOI 10.1109/TMM.2010.2058095
   Tawari Ashish., 2012, P 2012 ACM ANN C EXT, P2261
   Tian YL, 2005, HANDBOOK OF FACE RECOGNITION, P247, DOI 10.1007/0-387-27257-7_12
   VALSTAR M., 2005, COMPUTER VISION PATT, P76, DOI DOI 10.1109/CVPR.2005.457
   Wang YJ, 2008, IEEE T MULTIMEDIA, V10, P936, DOI 10.1109/TMM.2008.927665
   Wu Tingfan., 2010, IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P42
   Zeng ZH, 2005, PROC CVPR IEEE, P967
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
NR 42
TC 39
Z9 46
U1 0
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2013
VL 15
IS 7
BP 1543
EP 1552
DI 10.1109/TMM.2013.2266635
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 236PQ
UT WOS:000325811800007
DA 2024-07-18
ER

PT J
AU Tao, DP
   Jin, LW
   Liu, WF
   Li, XL
AF Tao, Dapeng
   Jin, Lianwen
   Liu, Weifeng
   Li, Xuelong
TI Hessian Regularized Support Vector Machines for Mobile Image Annotation
   on the Cloud
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cloud computing; Hessian Eigenmaps and support vector machines; manifold
   regularization; mobile service
ID DISCRIMINANT-ANALYSIS; RELEVANCE-FEEDBACK; SUBSPACE; SERVICE;
   RECOGNITION; EIGENMAPS
AB With the rapid development of the cloud computing and mobile service, users expect a better experience through multimedia computing, such as automatic or semi-automatic personal image and video organization and intelligent user interface. These functions heavily depend on the success of image understanding, and thus large-scale image annotation has received intensive attention in recent years. The collaboration between mobile and cloud opens a new avenue for image annotation, because the heavy computation can be transferred to the cloud for immediately responding user actions. In this paper, we present a scheme for image annotation on the cloud, which transmits mobile images compressed by Hamming compressed sensing to the cloud and conducts semantic annotation through a novel Hessian regularized support vector machine on the cloud. We carefully explained the rationality of Hessian regularization for encoding the local geometry of the compact support of the marginal distribution and proved that Hessian regularized support vector machine in the reproducing kernel Hilbert space is equivalent to conduct Hessian regularized support vector machine in the space spanned by the principal components of the kernel principal component analysis. We conducted experiments on the PASCAL VOC'07 dataset and demonstrated the effectiveness of Hessian regularized support vector machine for large-scale image annotation.
C1 [Tao, Dapeng; Jin, Lianwen] S China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510640, Guangdong, Peoples R China.
   [Liu, Weifeng] China Univ Petr, Coll Informat & Control Engn, Dongying, Peoples R China.
   [Li, Xuelong] Chinese Acad Sci, Ctr Opt IMagery Anal & Learning OPTIMAL, State Key Lab Transient Opt & Photon, Xian Inst Opt & Precis Mech, Xian 710119, Shaanxi, Peoples R China.
C3 South China University of Technology; China University of Petroleum;
   State Key Laboratory of Transient Optics & Photonics; Chinese Academy of
   Sciences; Xi'an Institute of Optics & Precision Mechanics, CAS
RP Tao, DP (corresponding author), S China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510640, Guangdong, Peoples R China.
EM dapeng.tao@gmail.com; lianwen.jin@gmail.com; liuwf@upc.edu.cn;
   xuelong_li@opt.ac.cn
RI Li, Xuelong/Z-3785-2019; Li, Xuelong/ABF-3381-2020; li,
   xiang/GWM-6319-2022; liu, weifeng/B-7909-2008; Tao, Dapeng/E-8649-2013
OI Li, Xuelong/0000-0002-0019-4197; Tao, Dapeng/0000-0003-0783-5273
FU National Basic Research Program of China (973 Program) [2012CB316400];
   National Natural Science Foundation of China [61075021, 61271407,
   61125106, 91120302]; Guangdong Natural Science Funds [S2011020000541];
   Guangdong Scientific and Technology Research Plan [2012A010701001,
   2011B090400146]; Fundamental Research Funds for the Central Universities
   of China; South China University of Technology [2012ZP0002]; Natural
   Science Foundation for Youths of Shandong Province, China [ZR2011FQ016]
FX This work was supported by the National Basic Research Program of China
   (973 Program) under Grant 2012CB316400, the National Natural Science
   Foundation of China under Grants 61075021, 61271407, 61125106, and
   91120302, the Guangdong Natural Science Funds under Grant
   S2011020000541, the Guangdong Scientific and Technology Research Plan
   under Grants 2012A010701001 and 2011B090400146, the Fundamental Research
   Funds for the Central Universities of China, South China University of
   Technology, under Grant 2012ZP0002, and the Natural Science Foundation
   for Youths of Shandong Province, China, under Grant ZR2011FQ016. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Joel Rodrigues.
CR [Anonymous], P 17 ACM INT C MULT
   [Anonymous], 2003, P 11 ACM INT C MULT
   Armbrust M, 2010, COMMUN ACM, V53, P50, DOI 10.1145/1721654.1721672
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Bilenko M., 2004, P INT C MACH LEARN, P11
   Blei D, 2007, ADV NEURAL INF PROCE, V3, P993
   Boufounos PT, 2008, 2008 42ND ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS, VOLS 1-3, P16, DOI 10.1109/CISS.2008.4558487
   Bresnaban J., 2011, SCIENCECLOUDL L, P25
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   Chandrasekhar V, 2012, INT J COMPUT VISION, V96, P384, DOI 10.1007/s11263-011-0453-z
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chapelle O., 2006, SEMISUPERVISED LEARN, V2
   Chen Yixin, 2003, P 5 ACM SIGMM INT WO, P193
   Wang C, 2009, PROC CVPR IEEE, P1903, DOI [10.1109/CVPR.2009.5206800, 10.1109/CVPRW.2009.5206800]
   Cui JY, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P367
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Donoho DL, 2003, P NATL ACAD SCI USA, V100, P5591, DOI 10.1073/pnas.1031596100
   Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97
   Eells J., 1983, Selected Topics in Harmonic Maps, V50, DOI [10.1090/cbms/050, DOI 10.1090/CBMS/050]
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Foster I, 2005, SCIENCE, V308, P814, DOI 10.1126/science.1110411
   Gao Y, 2011, PROC INT CONF DOC, P885, DOI 10.1109/ICDAR.2011.181
   Geng B, 2012, IEEE T PATTERN ANAL, V34, P1227, DOI 10.1109/TPAMI.2012.57
   Geng B, 2012, IEEE T MULTIMEDIA, V14, P55, DOI 10.1109/TMM.2011.2174781
   Geng B, 2010, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2010.5540003
   Girod B, 2011, IEEE MULTIMEDIA, V18, P86, DOI 10.1109/MMUL.2011.48
   Gosselin PH, 2008, IEEE T IMAGE PROCESS, V17, P1200, DOI 10.1109/TIP.2008.924286
   Guan NY, 2012, IEEE T SIGNAL PROCES, V60, P2882, DOI 10.1109/TSP.2012.2190406
   GUILLAUMIN M, 2010, PROC CVPR IEEE, P902, DOI DOI 10.1109/CVPR.2010.5540120
   He X, 2008, IEEE T KNOWL DATA EN, V20, P189, DOI 10.1109/TKDE.2007.190692
   Iosup A, 2011, IEEE T PARALL DISTR, V22, P931, DOI 10.1109/TPDS.2011.66
   Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]
   Kim KwangI., 2009, Advances in Neural Information Processing Systems, P979
   Mell P, 2010, COMMUN ACM, V53, P50
   Morik K, 1999, MACHINE LEARNING, PROCEEDINGS, P268
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Papadopoulos S, 2011, IEEE MULTIMEDIA, V18, P52, DOI 10.1109/MMUL.2010.68
   Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1007/BF00116251
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Shalev-Shwartz S., 2007, P 24 INT C MACH LEAR, P807
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Shi R., 2007, Proceedings of the 15th International Conference on Multimedia, P341
   Song ML, 2012, IEEE T IMAGE PROCESS, V21, P341, DOI 10.1109/TIP.2011.2157514
   Song ML, 2010, IEEE T PATTERN ANAL, V32, P1537, DOI 10.1109/TPAMI.2009.74
   Steinke F., 2008, ADV NEURAL INF PROCE, V21, P1
   Tao DC, 2007, IEEE T PATTERN ANAL, V29, P1700, DOI 10.1109/TPAMI.2007.1096
   Tao DC, 2007, IEEE T KNOWL DATA EN, V19, P568, DOI 10.1109/TKDE.2007.1003
   Tao DC, 2006, IEEE T MULTIMEDIA, V8, P716, DOI 10.1109/TMM.2005.861375
   Tao DC, 2009, IEEE T PATTERN ANAL, V31, P260, DOI 10.1109/TPAMI.2008.70
   Tao DP, 2012, NEUROCOMPUTING, V91, P11, DOI 10.1016/j.neucom.2012.02.024
   Tao DC, 2006, IEEE T PATTERN ANAL, V28, P1088, DOI 10.1109/TPAMI.2006.134
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Tian XM, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2240136.2240139
   Tianyi Zhou, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P679, DOI 10.1109/ICDM.2010.135
   Tong S, 2001, P 9 ACM INT C MULT, P107, DOI DOI 10.1145/500141.500159
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Vaquero LM, 2011, INT J CLOUD APPL COM, V1, P34, DOI 10.4018/ijcac.2011010103
   Yen-Yu Lin, 2005, 13th Annual ACM International Conference on Multimedia, P249, DOI 10.1145/1101149.1101193
   Zhang Q, 2010, J INTERNET SERV APPL, V1, P7, DOI 10.1007/s13174-010-0007-6
   Zhang TH, 2009, IEEE T KNOWL DATA EN, V21, P1299, DOI 10.1109/TKDE.2008.212
   Zhou T., 2011, ARXIV11100073
   Zhu X., 2006, Computer Science, University of Wisconsin-Madison
NR 64
TC 98
Z9 103
U1 0
U2 52
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2013
VL 15
IS 4
BP 833
EP 844
DI 10.1109/TMM.2013.2238909
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148FK
UT WOS:000319228500012
DA 2024-07-18
ER

PT J
AU Moon, YH
   Yoon, KS
   Park, ST
   Shin, IH
AF Moon, Yong Ho
   Yoon, Kun Su
   Park, Sang-Taick
   Shin, Il Hong
TI A New Fast Encoding Algorithm Based on an Efficient Motion Estimation
   Process for the Scalable Video Coding Standard
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fast encoding; inter-layer prediction; motion estimation; scalable video
   coding; skip criterion
ID FAST MODE DECISION
AB In this paper, a new fast encoding algorithm based on an efficient motion estimation (ME) process is proposed to accelerate the encoding speed of the scalable video coding standard. Through analysis of the ME process performed in the enhancement layer, we discovered that there are redundant MEs and some MEs can simply be unified at the fully overlapped search range (FOSR). In order to make the unified ME more efficient, we theoretically derive a skip criterion to determine whether the computation of rate-distortion cost can be omitted. In the proposed algorithm, the unnecessary MEs are removed and a unified ME with the skip criterion is applied in the FOSR. Simulation results show that the proposed algorithm achieves computational savings of approximately 46% without coding performance degradation when compared with the original SVC encoder.
C1 [Moon, Yong Ho] Gyeongsang Natl Univ, Res Inst, Dept Informat Engn, Jinju 660701, Gyeongsangnam D, South Korea.
   [Yoon, Kun Su] Korea Aerosp Ind LTD, R&D Div, Avion Syst Dept, Sacheon 664710, Gyeongsangnam D, South Korea.
   [Park, Sang-Taick] Elect & Telecommun Res Inst, Smart TV Syst Res Team, Taejon 305700, South Korea.
   [Shin, Il Hong] Elect & Telecommun Res Inst, IPTV Res Dept, Taejon 305700, South Korea.
   [Moon, Yong Ho] Gyeongsang Natl Univ, Dept Informat, Jinju 660701, Gyeongsangnam D, South Korea.
   [Yoon, Kun Su] Korea Aerosp Ind LTD, Div Res & Dev, Avion Syst Dept, Aviat SW Sect, Sacheon 664710, Gyeongsangnam D, South Korea.
C3 Gyeongsang National University; Electronics & Telecommunications
   Research Institute - Korea (ETRI); Electronics & Telecommunications
   Research Institute - Korea (ETRI); Gyeongsang National University
RP Moon, YH (corresponding author), Gyeongsang Natl Univ, Res Inst, Dept Informat Engn, 900 Gajwa Dong, Jinju 660701, Gyeongsangnam D, South Korea.
EM yhmoon5@gnu.ac.kr; ksyoon@koreaaero.com; stpark@etri.re.kr;
   ssi@etri.re.kr
FU IT & RD Program of MKE/KCC/KEIT [2008-S-006-01]
FX This work was supported by the IT & RD Program of MKE/KCC/KEIT
   (2008-S-006-01, Development of Open-IPTV Technologies for Wired and
   Wireless Networks). The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Feng Wu.
CR Chen ZY, 2010, IEEE INT CON MULTI, P442, DOI 10.1109/ICME.2010.5583348
   Li H, 2006, IEEE T CIRC SYST VID, V16, P889, DOI 10.1109/TCSVT.2006.877404
   Lin HC, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P765, DOI 10.1109/ICME.2008.4607547
   Luo E., 2009, P PICT COD S FEB, V46, P1
   Park CS, 2010, ELECTRON LETT, V46, P280, DOI 10.1049/el.2010.2731
   Park CS, 2009, IEEE T CIRC SYST VID, V19, P1915, DOI 10.1109/TCSVT.2009.2031520
   Park CS, 2009, IEEE T CONSUM ELECTR, V55, P235, DOI 10.1109/TCE.2009.4814440
   Reichel J., 2007, JOINT SCALABLE VIDEO
   Ren JF, 2008, IEEE IMAGE PROC, P2464, DOI 10.1109/ICIP.2008.4712292
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Shen LQ, 2010, IEEE SIGNAL PROC LET, V17, P887, DOI 10.1109/LSP.2010.2066966
   Wiegand T., 2007, JOINT DRAFT 9 SVC AM
   Yeh CH, 2010, IEEE T CIRC SYST VID, V20, P563, DOI 10.1109/TCSVT.2010.2041825
NR 13
TC 13
Z9 14
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2013
VL 15
IS 3
BP 477
EP 484
DI 10.1109/TMM.2012.2232648
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109ZM
UT WOS:000316410900001
DA 2024-07-18
ER

PT J
AU Liang, C
   Xu, CS
   Cheng, J
   Min, WQ
   Lu, HQ
AF Liang, Chao
   Xu, Changsheng
   Cheng, Jian
   Min, Weiqing
   Lu, Hanqing
TI Script-to-Movie: A Computational Framework for Story Movie Composition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Movie composition; script and video analysis; computational framework
ID VIDEO
AB Traditional movie production has always been a highly professional work that needs team collaboration, advanced devices and techniques, and vast time and money investment. These high threshold requirements not only prevent mass amateur enthusiasts entering this field, but also hinder professionals quickly previewing their conceived story plots. In this paper, we raise a novel application, named script-to-movie (S2M) composition, to automatically produce new movies from existing videos in accordance with user created script. Our motivation is to liberate producers from complex filming and editing operations, thereby people's story idea can be instantly converted into the vivid movie video. To support the novel "What You Dream Is What You See" (WYDIWYS) production mode, we first propose a hierarchical alignment method to automatically construct a video material database with detailed semantic description. Considering diverse story plots in user designed script, the database contains abundant video materials about different characters appearing in various time and places conditions. On this basis, the S2M composition is formulated as a constrained optimization problem, where semantic story plot and syntactic visual content are synthetically considered to identify a group of optimal video segments to narrate the user designed script story. Both quantitative and qualitative experimental results are reported to illustrate the effectiveness of the proposed S2M application.
C1 [Liang, Chao; Xu, Changsheng; Cheng, Jian; Min, Weiqing; Lu, Hanqing] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Xu, Changsheng; Cheng, Jian; Min, Weiqing; Lu, Hanqing] China Singapore Inst Digital Media, Singapore 119615, Singapore.
C3 Chinese Academy of Sciences; Institute of Automation, CAS
RP Liang, C (corresponding author), Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Wuhan 430079, Peoples R China.
EM cliang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn; jcheng@nlpr.ia.ac.cn;
   wqmin@nlpr.ia.ac.cn; luhq@nlpr.ia.ac.cn
RI , chengjian/KGL-5551-2024; Liang, Chao/A-5929-2009; xu, cj/HJZ-3488-2023
OI , chengjian/0000-0003-1289-2758; 
FU National Program on Key Basic Research Project (973 Program)
   [2010CB327905, 2012CB316304]; National Natural Science Foundation of
   China [61225009, 61170127, 60975010, 60833006, 61070104, 90920303,
   61003161]
FX Manuscript received January 23, 2012; revised May 07, 2012 and July 03,
   2012; accepted July 11, 2012. Date of publication November 27, 2012;
   date of current version January 15, 2013. This work was supported in
   part by the National Program on Key Basic Research Project (973 Program,
   Project No. 2010CB327905, 2012CB316304) and the National Natural Science
   Foundation of China (Grant No. 61225009, 61170127, 60975010, 60833006,
   61070104, 90920303 and 61003161). The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Shrikanth Narayanan.
CR Alessandro V, P ACM MM, P261
   Arandjelovic O, P CVPR, P1513
   Chasanis V, 2009, CIVR, P1
   Cour T, P CVPR, P919
   Ding L, P ECCV 4, V6314, P410
   Fitzgibbon A. W, P ECCV 3, V3, P304
   Hua XS, 2004, IEEE T CIRC SYST VID, V14, P572, DOI 10.1109/TCSVT.2004.826750
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Liang C, P CVPR, P3377
   Motion Picture Association of America, 2011, THEATR MARK STAT 201
   Myers B. A, P JCDL, P106
   Ngo CW, 2005, IEEE T CIRC SYST VID, V15, P296, DOI 10.1109/TCSVT.2004.841694
   Nitta N, 2011, MULTIMED TOOLS APPL, V51, P649, DOI 10.1007/s11042-010-0633-9
   Porteous J, 1715, P ACM MM, P1715
   Pramod S. K, 2009, P BMVC
   Rasheed Z, 2005, IEEE T MULTIMEDIA, V7, P1097, DOI 10.1109/TMM.2005.858392
   Shen E.Y.T, P CHI, P809
   Shrestha P., 2010, Proceedings of the international conference on Multimedia (MM '10), P541, DOI DOI 10.1145/1873951.1874023
   Sivic J, 2005, LECT NOTES COMPUT SC, V3568, P226
   Ursu MF, 2008, MULTIMEDIA SYST, V14, P115, DOI 10.1007/s00530-008-0119-z
   Ursu MF, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1412196.1412198
   Vaucelle C, 2003, P SIGGRAPH, P1
   Wang F, P SDM, P1
   Wang JJ, 2008, MULTIMEDIA SYST, V14, P179, DOI 10.1007/s00530-008-0112-6
   Weng CY, 2009, IEEE T MULTIMEDIA, V11, P256, DOI 10.1109/TMM.2008.2009684
   Wu J., IEEE T PATT IN PRESS
   Yang P, 2011, COMPUT VIS IMAGE UND, V115, P456, DOI 10.1016/j.cviu.2010.11.015
   Yeung M, 1998, COMPUT VIS IMAGE UND, V71, P94, DOI 10.1006/cviu.1997.0628
   Zhang YF, 2009, IEEE T MULTIMEDIA, V11, P1276, DOI 10.1109/TMM.2009.2030629
NR 29
TC 11
Z9 12
U1 0
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2013
VL 15
IS 2
BP 401
EP 414
DI 10.1109/TMM.2012.2229972
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 075HN
UT WOS:000313875500015
DA 2024-07-18
ER

PT J
AU Lin, SS
   Yeh, IC
   Lin, CH
   Lee, TY
AF Lin, Shih-Syun
   Yeh, I-Cheng
   Lin, Chao-Hung
   Lee, Tong-Yee
TI Patch-Based Image Warping for Content-Aware Retargeting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retargeting; image warping; optimization
AB Image retargeting is the process of adapting images to fit displays with various aspect ratios and sizes. Most studies on image retargeting focus on shape preservation, but they do not fully consider the preservation of structure lines, which are sensitive to human visual system. In this paper, a patch-based retargeting scheme with an extended significance measurement is introduced to preserve shapes of both visually salient objects and structure lines while minimizing visual distortions. In the proposed scheme, a similarity transformation constraint is used to force visually salient content to undergo as-rigid-as-possible deformation, while an optimization process is performed to smoothly propagate distortions. These processes enable our approach to yield pleasing content-aware warping and retargeting. Experimental results and a user study show that our results are better than those generated by state-of-the-art approaches.
C1 [Lin, Shih-Syun; Yeh, I-Cheng; Lee, Tong-Yee] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 701, Taiwan.
   [Lin, Chao-Hung] Natl Cheng Kung Univ, Dept Geomat, Tainan 701, Taiwan.
C3 National Cheng Kung University; National Cheng Kung University
RP Lin, SS (corresponding author), Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 701, Taiwan.
EM catchylss@hotmail.com; ichenyeh@gmail.com; linhung@mail.ncku.edu.tw;
   tonylee@mail.ncku.edu.tw
RI Lin, Shih-Syun/ABD-8570-2020
OI Lin, Shih-Syun/0000-0002-8360-5819; Yeh, I-Cheng/0000-0003-1045-843X
FU National Science Council, Taiwan [NSC-99-2221-E-006-066-MY3,
   NSC-100-2628-E-006-031-MY3, NSC-100-2221-E-006-188-MY3,
   NSC-100-2119-M-006-025]
FX Manuscript received November 29, 2011; revised March 04, 2012 and June
   05, 2012; accepted June 20, 2012. Date of publication November 20, 2012;
   date of current version January 15, 2013. This work was supported in
   part by the National Science Council (contracts
   NSC-99-2221-E-006-066-MY3, NSC-100-2628-E-006-031-MY3,
   NSC-100-2221-E-006-188-MY3, and NSC-100-2119-M-006-025), Taiwan. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Chong-Wah Ngo.
CR [Anonymous], 2005, P 18 ANN ACM S US IN
   [Anonymous], ACM SIGGRAPH ASIA 20
   [Anonymous], 2005, Fundamentals of Computer Graphics
   [Anonymous], 2006, P 14 ACM INT C MULT
   [Anonymous], 2007, 2007 IEEE 11 INT C C, DOI DOI 10.1109/ICCV.2007.4409010
   [Anonymous], P WORKSH DYN VIS
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Chen LQ, 2003, MULTIMEDIA SYST, V9, P353, DOI 10.1007/s00530-003-0105-4
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Frankovich M, 2011, IEEE SIGNAL PROC LET, V18, P375, DOI 10.1109/LSP.2011.2140396
   Gal R., 2006, P 17 EUROGRAPHICS C, P297, DOI DOI 10.2312/EGWR/EGSR06/297-303
   Goferman S, 2010, PROC CVPR IEEE, P2376, DOI 10.1109/CVPR.2010.5539929
   Guo YW, 2009, IEEE T MULTIMEDIA, V11, P856, DOI 10.1109/TMM.2009.2021781
   Han DF, 2010, VISUAL COMPUT, V26, P749, DOI 10.1007/s00371-010-0480-8
   Huang QX, 2009, COMPUT GRAPH FORUM, V28, P1887, DOI 10.1111/j.1467-8659.2009.01567.x
   Jin Y, 2010, VISUAL COMPUT, V26, P769, DOI 10.1007/s00371-010-0472-8
   Liu YJ, 2011, COMPUT GRAPH FORUM, V30, P583, DOI 10.1111/j.1467-8659.2011.01881.x
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Santella A., 2006, Conference on Human Factors in Computing Systems. CHI2006, P771
   Shamir A, 2009, COMMUN ACM, V52, P77, DOI 10.1145/1435417.1435437
   Suh Bongwon, 2003, P ACM S US INT SOFTW, P95, DOI DOI 10.1145/964696.964707
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Wu HS, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866185
   Zhang GX, 2009, COMPUT GRAPH FORUM, V28, P1897, DOI 10.1111/j.1467-8659.2009.01568.x
NR 27
TC 86
Z9 98
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2013
VL 15
IS 2
BP 359
EP 368
DI 10.1109/TMM.2012.2228475
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 075HN
UT WOS:000313875500011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Inoue, N
   Shinoda, K
AF Inoue, Nakamasa
   Shinoda, Koichi
TI A Fast and Accurate Video Semantic-Indexing System Using Fast MAP
   Adaptation and GMM Supervectors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gaussian mixture model (GMM) supervectors; maximum a posteriori (MAP)
   adaptation; video semantic indexing
ID SCALE
AB We propose a fast maximum a posteriori (MAP) adaptation method for video semantic indexing that uses Gaussian mixture model (GMM) supervectors. In this method, a tree-structured GMM is utilzed to decrease the computational cost, where only the output probabilities of mixture components close to an input sample are precisely calculated. Experimental evaluation on the TRECVID 2010 dataset demonstrates the effectiveness of the proposed method. The calculation time of the MAP adaptation step is reduced by 76.2% compared with that of a conventional method. The total calculation time is reduced by 56.6% while keeping the same level of the accuracy.
C1 [Inoue, Nakamasa; Shinoda, Koichi] Tokyo Inst Technol, Dept Comp Sci, Tokyo 1528552, Japan.
C3 Tokyo Institute of Technology
RP Inoue, N (corresponding author), Tokyo Inst Technol, Dept Comp Sci, Tokyo 1528552, Japan.
EM inoue@ks.cs.titech.ac.jp; shinoda@cs.titech.ac.jp
RI Shinoda, Koichi/D-3198-2014
OI Shinoda, Koichi/0000-0003-1095-3203
CR [Anonymous], 2009, Proc. ACM International Confence on Multimedia
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], 2007, MIR
   [Anonymous], 2011, ACM T INTEL SYST TEC
   [Anonymous], 2004, P 2004WORKSHOP STAT
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], P TRECVID WORKSH GAI
   [Anonymous], HKT BOOK
   [Anonymous], 2004, P 21 INT C MACH LEAR
   Ayache S, 2008, LECT NOTES COMPUT SC, V4956, P187
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863
   Campbell WM, 2006, IEEE SIGNAL PROC LET, V13, P308, DOI 10.1109/LSP.2006.870086
   Chang S.-F., 2007, P INT WORKSHOP WORKS, P255, DOI DOI 10.1145/1290082.1290118
   Inoue N., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3220, DOI 10.1109/ICPR.2010.787
   Inoue N., 2011, Proceedings of the 19th ACM International Conference on Multimedia, P1357
   Jaakkola TS, 1999, ADV NEUR IN, V11, P487
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Perronnin F., 2007, P IEEE CVPR, P1
   Perronnin F, 2006, LECT NOTES COMPUT SC, V3954, P464
   Sinha SC, 2006, J COMPUT NONLIN DYN, V1, P1, DOI 10.1115/1.2004119
   Smeaton AF, 2009, SIGNALS COMMUN TECHN, P151, DOI 10.1007/978-0-387-76569-3_6
   van de Sande KEA, 2011, IEEE T MULTIMEDIA, V13, P60, DOI 10.1109/TMM.2010.2091400
   van de Weijer J, 2006, LECT NOTES COMPUT SC, V3952, P334
   van Gemert Jan C., 2008, Computer Vision. Proceedings 10th European Conference on Computer Vision, ECCV 2008, P696, DOI 10.1007/978-3-540-88690-7_52
   Wu C., 2007, Siftgpu: A gpu implementation of david lowe's scale invariant feature transform (sift)
   Yan F, 2009, IEEE DATA MINING, P1064, DOI 10.1109/ICDM.2009.84
   Yilmaz E., 2008, P 31 ANN INT ACM SIG
   Zhou X., 2008, MM 08 P 2008 ACM INT, P229, DOI DOI 10.1145/1459359.1459391.ISBN
   Zhou X, 2010, LECT NOTES COMPUT SC, V6315, P141, DOI 10.1007/978-3-642-15555-0_11
NR 31
TC 33
Z9 37
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
BP 1196
EP 1205
DI 10.1109/TMM.2012.2191395
PN 2
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QM
UT WOS:000306599400007
OA Green Published
DA 2024-07-18
ER

PT J
AU Wang, DQ
   Yeo, CK
AF Wang, Danqi
   Yeo, Chai Kiat
TI Exploring Locality of Reference in P2P VoD Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Association rule learning; locality of reference; peer-to-peer; VCR
   support
ID OVERLAY
AB A critical problem to peer-to-peer video-on-demand (P2P VoD) systems is to provide efficient user interactivity support. In this paper, we study intra-and inter-video operations separately, aiming to exploit the locality of reference in user access patterns and reduce the latency of these VoD operations. We first introduce the concepts of available, request and delivered locality in intra-video user access patterns and prove that high available locality exists in different videos by both simulation and theoretical analysis. Moreover, with a relaxed definition of data chunk holder, intra-video locality can facilitate a high likelihood of a peer seeking within a video, finding a holder of the requested data among its neighbors. Exploiting this property, an aggressive cached publish scheme is designed to build shortcuts over the DHT network so as to reduce the lookup delay. This scheme may be simple but it is practical and easy to implement. Inter-video locality is exploited via learning association rules from the collective viewing history. A fast association rule learning algorithm is proposed to infer the relations between videos in a distributed manner based on partial knowledge. Both search and content prefetch are incorporated to achieve low inter-video jump delay with minimal overhead. Our simulations demonstrate that the proposed schemes can reduce the buffer and lookup delay for seeking within a video and provide an efficient prediction-based prefetch scheme for inter-video access.
C1 [Wang, Danqi; Yeo, Chai Kiat] Nanyang Technol Univ, Sch Comp Engn, Ctr Multimedia & Network Technol, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Wang, DQ (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Ctr Multimedia & Network Technol, Singapore 639798, Singapore.
EM s080004@ntu.edu.sg; asckyeo@ntu.edu.sg
RI Yeo, Chai Kiat/A-3683-2011
OI Yeo, Chai Kiat/0000-0002-7618-1472
CR Agrawal R., 1993, SIGMOD Record, V22, P207, DOI 10.1145/170036.170072
   [Anonymous], P 3 ACM SIGOPS EUROS
   [Anonymous], 2007, WORKSHOP PEER TO PEE
   [Anonymous], P 20 IEEE INT PAR DI
   [Anonymous], 2009, CISCO VISUAL NETWORK
   Cheng B, 2007, IEEE ICC, P1698, DOI 10.1109/ICC.2007.284
   Cheng X, 2009, IEEE INFOCOM SER, P1152, DOI 10.1109/INFCOM.2009.5062028
   Chi HC, 2007, IEEE J SEL AREA COMM, V25, P119, DOI 10.1109/JSAC.2007.070112
   Crosby S A., 2007, An Analysis of BitTorrent's Two Kademlia-Based DHTs
   Cui Y, 2004, IEEE J SEL AREA COMM, V22, P91, DOI 10.1109/JSAC.2003.818799
   Do TT, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P1467, DOI 10.1109/ICC.2004.1312755
   Guo Y., 2003, Proceedings of the 12th International Conference on World Wide Web, P301, DOI DOI 10.1145/775152.775195
   Guo Y, 2008, CONSUM COMM NETWORK, P452, DOI 10.1109/ccnc08.2007.107
   Gupta Anjali., 2004, NSDI 04, P9
   He YF, 2009, IEEE T MULTIMEDIA, V11, P138, DOI 10.1109/TMM.2008.2008929
   He Y, 2009, IEEE T PARALL DISTR, V20, P528, DOI 10.1109/TPDS.2008.102
   Huang Y, 2008, ACM SIGCOMM COMP COM, V38, P375, DOI 10.1145/1402946.1403001
   Jin Shudong., 2001, SIGM PERF EVAL REV, V29, P2, DOI DOI 10.1145/507553.507554
   Leong B., 2004, P 12 INT C NETW ICON
   Liu JC, 2006, MULTIMED TOOLS APPL, V29, P211, DOI 10.1007/s11042-006-0013-7
   Maymounkov P, 2002, LECT NOTES COMPUT SC, V2429, P53
   Mulder W., 2010, THESIS KUTZTOWN U PE
   Tati K, 2004, LECT NOTES COMPUT SC, V3293, P44
   Wang D, 2008, IEEE T PARALL DISTR, V19, P503, DOI 10.1109/TPDS.2007.70748
   Wang Z., 2010, 2010 International Conference on Multimedia Technology, Ningbo, P1, DOI DOI 10.1109/ICCW.2010.5503924
   Yang X., 2009, P 8 INT WORKSH PEER
   Yang Y., 2010, P 29 IEEE INT C COMP, P1
   Yiu WPK, 2007, IEEE J SEL AREA COMM, V25, P1717, DOI 10.1109/JSAC.2007.071210
   Zhang M, 2007, IEEE J SEL AREA COMM, V25, P1678, DOI 10.1109/JSAC.2007.071207
   Zheng C., 2005, PROC ACM WORKSHOP AD, P29
NR 30
TC 4
Z9 4
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
BP 1309
EP 1323
DI 10.1109/TMM.2012.2191942
PN 2
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QM
UT WOS:000306599400016
DA 2024-07-18
ER

PT J
AU Yin, H
   Hui, W
   Li, HZ
   Lin, C
   Zhu, WW
AF Yin, Hao
   Hui, Wen
   Li, Hongzhi
   Lin, Chuang
   Zhu, Wenwu
TI A Novel Large-Scale Digital Forensics Service Platform for Internet
   Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content delivery network; digital forensics; load balancing; resource
   scheduling; video detection
AB The increasing transmission of illegal videos over the Internet imposes the needs to develop large-scale digital video forensics systems for prosecuting and deterring digital crimes in the Internet. In this paper, we propose, design, and implement a novel large-scale Digital Forensics Service Platform (DFSP) that can effectively detect illegal content from Internet videos. More specifically, we propose a distributed architecture by taking advantage of Content Delivery Network (CDN) to improve scalability, which can process enormous number of Internet videos in real time. We propose CDN-based Resource-Aware Scheduling (CRAS) algorithm, which schedules the tasks efficiently in the DFSP according to resource parameters, such as delay and computation load. We deploy the DFSP system in the Internet, which integrates the CDN-based distributed architecture and CRAS algorithm with a large-scale video detection algorithm, and evaluate the deployed system. Our evaluation results demonstrate the effectiveness of the platform.
C1 [Yin, Hao; Hui, Wen; Lin, Chuang] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Hui, Wen] Univ Sci & Technol Beijing, Sch Comp & Commun Engn, Beijing 100083, Peoples R China.
   [Li, Hongzhi] Columbia Univ, Dept Elect Engn, New York, NY 10027 USA.
   [Zhu, Wenwu] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Tsinghua University; University of Science & Technology Beijing;
   Columbia University; Microsoft Research Asia; Microsoft
RP Yin, H (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM huiwen@csnet1.cs.tsinghua.edu.cn
RI Lin, Lin/JTU-1595-2023
FU National Natural Science Foundation of China (NSFC); National Basic
   Research Program of China (973 Program) [2010CB328105, 2011CB302600,
   2012CB315800]; New Century Excellent Talents in University;  [60873254];
    [60736012];  [61170290];  [60932003]
FX This work was funded by the Project 60873254, 60736012, 61170290,
   60932003 supported by the National Natural Science Foundation of China
   (NSFC); the Project 2010CB328105, 2011CB302600, 2012CB315800 supported
   by the National Basic Research Program of China (973 Program); New
   Century Excellent Talents in University. This work was performed when H.
   Li was visiting Microsoft Research Asia as a research intern. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Jia Li.
CR [Anonymous], 2009, Proc. ACM International Confence on Multimedia
   [Anonymous], 2001, P IEEE INT C MULT EX, DOI DOI 10.1109/ICME.2001.1237822
   [Anonymous], P MIR NOV
   [Anonymous], 2007, W9216 ISOIEC MPEG
   [Anonymous], 2006, P 8 WORKSHOP MULTIME, DOI DOI 10.1145/1161366.1161375
   Asmundsson F.H., 2009, P 17 INT C MULT 2009, P999
   Bouaziz B, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1737, DOI 10.1109/ICME.2006.262886
   Carlsson N, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1671954.1671955
   Castillo C., 2005, ACM SIGRM FORUM, V39, P55
   Chan SHG, 2001, IEEE ACM T NETWORK, V9, P125, DOI 10.1109/90.917070
   DeCandia Giuseppe, 2007, Operating Systems Review, V41, P205, DOI 10.1145/1323293.1294281
   Delgadillo K., 1997, CISCO DISTRIBUTEDDIR
   Dong W., 2008, PROCEEDING ACM MULTI, P179, DOI DOI 10.1145/1459359.1459384
   Douze M., 2008, P TRECVID WORKSH
   El'arbi M., 2007, Proceedings of IEEE International Conference on Image Processing, V5, P481
   Fortino G, 2007, IEEE MULTIMEDIA, V14, P60, DOI 10.1109/MMUL.2007.29
   Fortino G, 2008, LECT NOTES ELECTR EN, V9, P297
   Gauch J., 2006, P IEEE INT C IM PROC, V3
   Guo CX, 2009, ACM SIGCOMM COMP COM, V39, P63, DOI 10.1145/1594977.1592577
   Guo Y, 2007, MULTIMED TOOLS APPL, V33, P109, DOI 10.1007/s11042-006-0067-6
   Jingwei Wang, 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, P493
   Kozat UC, 2009, IEEE T MULTIMEDIA, V11, P494, DOI 10.1109/TMM.2009.2012918
   Lee S, 2008, INT CONF ACOUST SPEE, P1237
   Lejsek H, 2010, P ACM WORKSH MULT FO, P1
   Lejsek H., 2009, P ACM WORKSH MULT FO, P19
   Maeno K, 2006, IEEE T MULTIMEDIA, V8, P32, DOI 10.1109/TMM.2005.861293
   Masa M, 2003, IEEE IPCCC, P5, DOI 10.1109/PCCC.2003.1203678
   Mol JJD, 2009, 2009 11TH IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM 2009), P342, DOI 10.1109/ISM.2009.16
   Pallis G, 2006, COMMUN ACM, V49, P101, DOI 10.1145/1107458.1107462
   Pierre G, 2006, IEEE COMMUN MAG, V44, P127, DOI 10.1109/MCOM.2006.1678120
   Popescu AC, 2004, LECT NOTES COMPUT SC, V3200, P128
   POPESCU AC, TR2004515 DARTM COLL
   Shang L., P INT C MULT
   Shen H. T., 2007, P ACM MULT C, P164
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Smith P, 2005, IEEE I CONF COMP VIS, P733
   Szymaniak M., 2003, P INT C WWW INT, P435
   Thouin F, 2007, IEEE NETWORK, V21, P42, DOI 10.1109/MNET.2007.334311
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   Wang WH, 2007, MM&SEC'07: PROCEEDINGS OF THE MULTIMEDIA & SECURITY WORKSHOP 2007, P35
   Wu A. G., 2007, P ACM MM, P218
   Yan Y, 2008, PROC INT CONF DATA, P853, DOI 10.1109/ICDE.2008.4497494
   Yin H., 2009, Proceedings of ACM International Conference on Multimedia, P25
   Yiu WPK, 2007, IEEE J SEL AREA COMM, V25, P1717, DOI 10.1109/JSAC.2007.071210
   Zhang X., 2007, Comprehensive Organometallic Chemistry III, P1
   Zhao WL, 2009, IEEE INT CON MULTI, P1624, DOI 10.1109/ICME.2009.5202830
   Zhong H, 2004, PROC CVPR IEEE, P819
NR 47
TC 11
Z9 11
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2012
VL 14
IS 1
BP 178
EP 186
DI 10.1109/TMM.2011.2170556
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OL
UT WOS:000302701100017
DA 2024-07-18
ER

PT J
AU Li, HL
   Liu, GH
   Ngan, KN
AF Li, Hongliang
   Liu, Guanghui
   Ngan, King Ngi
TI Guided Face Cartoon Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face cartoon; guided synthesis; linear model
ID MODEL; SEGMENTATION; RECOGNITION
AB In this paper, we propose a new method, called guided synthesis, to synthesize a face cartoon from a face photo. The guided synthesis is defined as a local linear model, which generates a cartoon image by incorporating the content of guidance images taken from the training set. Our synthesis operation is achieved based on four weight functions. The first is a photo-photo weight that aims to measure the similarity between an input photo patch and a training photo patch. The second is defined as a photo-cartoon weight, which is used to compute the likelihood by computing the similarity between a cartoon patch and an input photo patch. The third weight is defined in the synthesized photos, which is to set a smoothness constraint between neighboring synthesized patches. The final weight is designed to evaluate the similarity of a synthesized patch to an input patch based on the spatial distance. Experimental evaluation on a number of face photos demonstrates the good performance of the proposed method on the face cartoon synthesis.
C1 [Li, Hongliang; Liu, Guanghui] Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 610054, Peoples R China.
   [Ngan, King Ngi] Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong, Peoples R China.
C3 University of Electronic Science & Technology of China; Chinese
   University of Hong Kong
RP Li, HL (corresponding author), Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 610054, Peoples R China.
EM hlli@ee.uestc.edu.cn; ghliu@ee.uestc.edu.cn; knngan@ee.cuhk.edu.hk
RI Ngan, N/E-8240-2014; Liu, Guanghui/C-3658-2012
OI Ngan, N/0000-0003-1946-3235; Liu, Guanghui/0000-0002-4170-4552; Li,
   Hongliang/0000-0002-7481-095X
FU NSFC [60972109, 61101091]; Program for New Century Excellent Talents in
   University [NCET-08-0090]; Sichuan Province Science Foundation for
   Youths [2010JQ0003]
FX Manuscript received March 11, 2011; revised June 14, 2011 and August 26,
   2011; accepted September 02, 2011. Date of publication September 19,
   2011; date of current version November 18, 2011. This work was supported
   in part by NSFC (No. 60972109 and 61101091), in part by the Program for
   New Century Excellent Talents in University (NCET-08-0090), and in part
   by Sichuan Province Science Foundation for Youths (No. 2010JQ0003). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Zhu Liu.
CR [Anonymous], 2002, PROC 10 ACM INT C MU
   [Anonymous], 2007, P ACM INT C IM VID R
   [Anonymous], P ACM SIGGRAPH
   Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983
   BRUNELLI R, 1993, IEEE T PATTERN ANAL, V15, P1042, DOI 10.1109/34.254061
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Chen H, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P433, DOI 10.1109/ICCV.2001.937657
   Chen H., 2002, P 5 AS C COMP VIS
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Hsu RL, 2003, IEEE T PATTERN ANAL, V25, P1388, DOI 10.1109/TPAMI.2003.1240113
   Hwang WI, 2006, GRAPP 2006: PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS, P299
   Kadir T., 2004, EUROPEAN C COMPUTER, P404
   Li HL, 2008, J VIS COMMUN IMAGE R, V19, P320, DOI 10.1016/j.jvcir.2008.04.001
   Li HL, 2007, IEEE T CIRC SYST VID, V17, P1742, DOI 10.1109/TCSVT.2007.903326
   Li Stan., 2006, EUROPEAN C COMPUTER, V2353, P117
   Li SZ, 2004, IEEE T PATTERN ANAL, V26, P1112, DOI 10.1109/TPAMI.2004.68
   Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5
   Liu QS, 2005, PROC CVPR IEEE, P1005
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matas J., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P384
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Pedersen KI, 2000, IEEE T VEH TECHNOL, V49, P437, DOI 10.1109/25.832975
   Sehgal P., 2004, P INT C COMP GRAPH I
   Tang XO, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P687, DOI 10.1109/ICCV.2003.1238414
   Tomasi C., 1998, 6 INT C COMP VIS IEE, P839
   Turk M. A., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P586, DOI 10.1109/CVPR.1991.139758
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang J, 2004, ACM T GRAPHIC, V23, P574, DOI 10.1145/1015706.1015763
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   Wen F., 2008, P 16 ACM INT C MULT, P1021
   Xu ZJ, 2008, IEEE T PATTERN ANAL, V30, P955, DOI 10.1109/TPAMI.2008.50
   Zhang W, 2010, LECT NOTES COMPUT SC, V6316, P420, DOI 10.1007/978-3-642-15567-3_31
NR 36
TC 19
Z9 22
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2011
VL 13
IS 6
BP 1230
EP 1239
DI 10.1109/TMM.2011.2168814
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 852GD
UT WOS:000297343400005
DA 2024-07-18
ER

PT J
AU Fields, B
   Jacobson, K
   Rhodes, C
   d'Inverno, M
   Sandler, M
   Casey, M
AF Fields, Ben
   Jacobson, Kurt
   Rhodes, Christophe
   d'Inverno, Mark
   Sandler, Mark
   Casey, Michael
TI Analysis and Exploitation of Musician Social Networks for Recommendation
   and Discovery
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content-based retrieval; graph theory; music information retrieval;
   shortest path problem; social network services: MySpace
ID INFORMATION-RETRIEVAL; MAXIMUM-FLOW
AB This paper presents an extensive analysis of a sample of a social network of musicians. The network sample is first analyzed using standard complex network techniques to verify that it has similar properties to other web-derived complex networks. Content-based pairwise dissimilarity values between the musical data associated with the network sample are computed, and the relationship between those content-based distances and distances from network theory explored. Following this exploration, hybrid graphs and distance measures are constructed, and used to examine the community structure of the artist network. Finally, results of these investigations are shown to be mostly orthogonal between these distance spaces. These results are considered with a focus recommendation and discovery applications employing these hybrid measures as their basis.
C1 [Fields, Ben; Rhodes, Christophe; d'Inverno, Mark; Casey, Michael] Univ London, Intelligent Sound & Mus Syst Grp, Dept Comp, London SE14 6NW, England.
   [Jacobson, Kurt; Sandler, Mark] Queen Mary Univ London, Ctr Digital Mus, Sch Elect Engn & Comp Sci, London E1 4NS, England.
   [Casey, Michael] Dartmouth Coll, Dept Mus, Hanover, NH 03755 USA.
C3 University of London; University of London; Queen Mary University
   London; Dartmouth College
RP Fields, B (corresponding author), Univ London, Intelligent Sound & Mus Syst Grp, Dept Comp, London SE14 6NW, England.
EM b.fields@gold.ac.uk
OI Sandler, Mark/0000-0002-5691-8107
FU Engineering and Physical Sciences Research Council [EP/E017614/1,
   EP/E02274X/1]; Andrew W. Mellon Foundation; Networked Environments for
   Music Analysis (NEMA); EPSRC [EP/E02274X/1, EP/E017614/1] Funding
   Source: UKRI
FX This work was supported in part by the Engineering and Physical Sciences
   Research Council via the Online Music Recognition And Searching II
   (OMRAS2) project, reference numbers EP/E017614/1 and EP/E02274X/1, and
   as part of the Networked Environments for Music Analysis (NEMA) project,
   funded by The Andrew W. Mellon Foundation.
CR Adomavicius G, 2005, IEEE T KNOWL DATA EN, V17, P734, DOI 10.1109/TKDE.2005.99
   Ahuja R. K., 1993, Network flows: theory, algorithms, and applications
   Alghoniemy M., 2001, P IEEE INT C MULT EX
   AMARAL LAN, 2000, P NAT ACAD SCI
   ANGLADE A, 2007, P INT S MUS INF RETR
   [Anonymous], 2001, Proc. IEEE International Conference on Multimedia and Expo
   [Anonymous], 2000, ISMIR
   [Anonymous], 2007, INT C WORLD WID WEB
   AUCOUTURIER JJ, 2004, J NEGATIVE RESULTS S
   Berenzweig A, 2004, COMPUT MUSIC J, V28, P63, DOI 10.1162/014892604323112257
   CANO P, 2006, CHAOS INTERDISCIPL J
   Casey MA, 2008, P IEEE, V96, P668, DOI 10.1109/JPROC.2008.916370
   Clauset A, 2004, PHYS REV E, V70, DOI 10.1103/PhysRevE.70.066111
   Costa LD, 2007, ADV PHYS, V56, P167, DOI 10.1080/00018730601170527
   CUNNINGHAM SJ, 2006, P INT C MUS INF RETR
   Downie JS, 2008, ACOUST SCI TECHNOL, V29, P247, DOI 10.1250/ast.29.247
   DOWNIE JS, 2006, D LIB MAG        DEC
   ELIAS P, 1956, IRE T INFORM THEOR, V2, P117, DOI 10.1109/TIT.1956.1056816
   FIELDS B, 2008, P INT COMP MUS C AUG
   FIELDS B, 2008, P INT S MUS INF RETR
   FLEXER A, 2008, P INT S MUS INF RETR
   Gleiser PM, 2003, ADV COMPLEX SYST, V6, P565, DOI 10.1142/S0219525903001067
   GOLDBERG AV, 1988, J ACM, V35, P921, DOI 10.1145/48014.61051
   INCE RA, 2009, FRONT NEUROINFORMAT, V3
   JACOBSON K, 2008, P INT S MUS INF RETR
   JACOBSON K, 2008, P CMMR, P306
   Knees P., 2006, P MIR, P147
   KNEES P, 2009, P INT C MUS INF RETR
   KWAK H, 2006, CST2006262 KAIST
   Lambiotte R, 2006, EUR PHYS J B, V50, P183, DOI 10.1140/epjb/e2006-00115-0
   Lamere P, 2007, P INT C MUS INF RETR
   Lee SH, 2006, PHYS REV E, V73, DOI 10.1103/PhysRevE.73.016102
   MAHAJAN DK, 2010, P ACM MULT FLOR IT
   NAGAMOCHI H, 1992, SIAM J DISCRETE MATH, V5, P54, DOI 10.1137/0405004
   Newman MEJ, 2003, SIAM REV, V45, P167, DOI 10.1137/S003614450342480
   PAMPALK E, 2006, THESIS U WIEN WIEN
   Park J, 2007, INT J BIFURCAT CHAOS, V17, P2281, DOI 10.1142/S0218127407018385
   PONS P, 2005, ARXIVPHYSICS0512106V
   Raimond Y., 2007, P INT SOC MUS INF RE, P1
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Steuer R, 2002, BIOINFORMATICS, V18, pS231, DOI 10.1093/bioinformatics/18.suppl_2.S231
   TZANETAKIS G, 2007, INF SCI REF
NR 42
TC 15
Z9 16
U1 2
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2011
VL 13
IS 4
BP 674
EP 686
DI 10.1109/TMM.2011.2111365
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 805EX
UT WOS:000293710300008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Su, JH
   Chou, CL
   Lin, CY
   Tseng, VS
AF Su, Ja-Hwung
   Chou, Chien-Li
   Lin, Ching-Yung
   Tseng, Vincent S.
TI Effective Semantic Annotation by Image-to-Concept Distribution Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Entropy; image annotation; image-to-concept distribution; tf-idf
AB Image annotation based on visual features has been a difficult problem due to the diverse associations that exist between visual features and human concepts. In this paper, we propose a novel approach called Annotation by Image-to-Concept Distribution Model (AICDM) for image annotation by discovering the associations between visual features and human concepts from image-to-concept distribution. Through the proposed image-to-concept distribution model, visual features and concepts can be bridged to achieve high-quality image annotation. In this paper, we propose to use "visual features", "models", and "visual genes" which represent analogous functions to the biological chromosome, DNA, and gene. Based on the proposed models using entropy, tf-idf, rules, and SVM, the goal of high-quality image annotation can be achieved effectively. Our empirical evaluation results reveal that the AICDM method can effectively alleviate the problem of visual-to-concept diversity and achieve better annotation results than many existing state-of-the-art approaches in terms of precision and recall.
C1 [Su, Ja-Hwung; Tseng, Vincent S.] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 701, Taiwan.
   [Chou, Chien-Li] Natl Chiao Tung Univ, Dept Comp Sci & Informat Engn, Hsinchu 30050, Taiwan.
   [Lin, Ching-Yung] IBM TJ Watson Res Ctr, Hawthorne, NY 10532 USA.
C3 National Cheng Kung University; National Yang Ming Chiao Tung
   University; International Business Machines (IBM)
RP Su, JH (corresponding author), Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 701, Taiwan.
EM bb0820@ms22.hinet.net; fall-winds@gmail.com; chingyung@us.ibm.com;
   tsengsm@mail.ncku.edu.tw
FU National Science Council, Taiwan, R.O.C. [NSC99-2631-H-006-002,
   NSC99-2218-E-006-001]
FX This work was supported by National Science Council, Taiwan, R.O.C.
   under grants NSC99-2631-H-006-002 and NSC99-2218-E-006-001. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Zhu Liu.
CR [Anonymous], P ACM WORKSH LSMRM
   [Anonymous], P CVPR
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2004, MULTIMEDIA'04, DOI [10.1145/1027527.1027748, DOI 10.1145/1027527.1027748]
   [Anonymous], 2007, P INT WORKSHOP WORKS
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   EVERINGHAM M, 2009, PASCAL VISUAL CLASSE
   Fan JP, 2011, IEEE T IMAGE PROCESS, V20, P837, DOI 10.1109/TIP.2010.2073476
   LAVRENKO V, 2004, P IEEE INT C AC SPEE, P17
   Li J, 2003, IEEE T PATTERN ANAL, V25, P1075, DOI 10.1109/TPAMI.2003.1227984
   Makadia A, 2010, INT J COMPUT VISION, V90, P88, DOI 10.1007/s11263-010-0338-6
   NASIERDING G, 2009, P IEEE INT C SYST MA
   Nguyen LD, 2009, IEEE INT CON MULTI, P546, DOI 10.1109/ICME.2009.5202554
   Pan J.-Y., 2004, Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P653, DOI [DOI 10.1145/1014052.1014135, 10.1145/1014052, DOI 10.1145/1014052]
   Su JH, 2010, IEEE INT CON MULTI, P42, DOI 10.1109/ICME.2010.5582564
   Su JH, 2010, EXPERT SYST APPL, V37, P5068, DOI 10.1016/j.eswa.2009.12.003
   Tang JH, 2010, IEEE T MULTIMEDIA, V12, P131, DOI 10.1109/TMM.2009.2037373
   Tseng VS, 2008, IEEE T MULTIMEDIA, V10, P260, DOI 10.1109/TMM.2007.911832
   Tseng VS, 2007, APPLIED COMPUTING 2007, VOL 1 AND 2, P1056, DOI 10.1145/1244002.1244233
   Von Ahn L, 2004, P SIGCHI C HUM FACT, DOI DOI 10.1145/985692.985733
   Wang XJ, 2008, IEEE T PATTERN ANAL, V30, P1919, DOI 10.1109/TPAMI.2008.127
   Wong RCF, 2008, IEEE T PATTERN ANAL, V30, P1933, DOI 10.1109/TPAMI.2008.125
   Wu L., 2008, P ACM INT C MULTIMED, P31, DOI DOI 10.1145/1459359.1459364
   Wu Lei., 2009, Proceedings of the 17th ACM international conference on Multimedia. ACM, P135
   Zeng H.J., 2004, P 27 ANN INT ACM SIG, P210, DOI DOI 10.1145/1008992.1009030
NR 25
TC 31
Z9 34
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2011
VL 13
IS 3
BP 530
EP 538
DI 10.1109/TMM.2011.2129502
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765UI
UT WOS:000290733700012
DA 2024-07-18
ER

PT J
AU Triantafyllopoulou, D
   Passas, N
   Kaloxylos, A
   Merakos, L
AF Triantafyllopoulou, Dionysia
   Passas, Nikos
   Kaloxylos, Alexandros
   Merakos, Lazaros
TI Coordinated Handover Initiation and Cross-Layer Adaptation for Mobile
   Multimedia Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Burst profile; cross-layer design; encoding rate; handover initiation;
   power control
AB A cross-layer mechanism to improve the performance of real-time applications over IEEE 802.16e metropolitan area networks is presented. The proposed mechanism uses channel quality and service quality information from the physical and medium access control layers, respectively, to determine the most suitable burst profile, transmission power level and media encoding rate for a connection, or even initialize a handover execution. The main contribution of this mechanism is the integration of the handover initiation into the cross layer logic, aiming to improve the overall system performance. Extensive simulation results show that the proposed mechanism offers significant performance improvement in terms of packet loss rate, power consumption, throughput, and system capacity.
C1 [Triantafyllopoulou, Dionysia; Passas, Nikos; Merakos, Lazaros] Univ Athens, Dept Informat & Telecommun, Athens 5784, Greece.
   [Kaloxylos, Alexandros] Univ Peloponnese, Dept Telecommun Sci & Technol, Tripolis 22100, Greece.
C3 National & Kapodistrian University of Athens; University of Peloponnese
RP Triantafyllopoulou, D (corresponding author), Univ Athens, Dept Informat & Telecommun, Athens 5784, Greece.
EM siarina@di.uoa.gr; passas@di.uoa.gr; kaloxyl@uop.gr; merakos@di.uoa.gr
RI Merakos, Lazaros/AAM-6183-2021; Kaloxylos, Alexandros/AAL-8967-2021;
   Triantafyllopoulou, Dionysia/HJI-3025-2023
OI Triantafyllopoulou, Dionysia/0000-0002-8150-4803; Kaloxylos,
   Alexandros/0000-0003-2182-5290; , Lazaros/0000-0003-4822-2393
FU PENED [03ED909]; E.U.-European Social Fund; Greek Ministry of
   Development-General Secretariat of Research and Technology
FX This paper is part of the 03ED909 research project, implemented within
   the framework of the "Reinforcement Programme of Human Research
   Manpower" (PENED) and co-financed by National and Community Funds (75%
   from E.U.-European Social Fund and 25% from the Greek Ministry of
   Development-General Secretariat of Research and Technology).
CR ANDREWS JG, 2007, COMMUNICATION ENG EM
   [Anonymous], 80216E IEEE
   [Anonymous], 3550 RFC
   Argyriou A, 2008, IEEE T MULTIMEDIA, V10, P691, DOI 10.1109/TMM.2008.922776
   Bernaschi M, 2007, IEEE T MOBILE COMPUT, V6, P1035, DOI 10.1109/TMC.2007.1003
   Hasswa A, 2006, IEEE ICC, P240
   Liu M, 2008, IEEE T MOBILE COMPUT, V7, P846, DOI 10.1109/TMC.2007.70768
   Mussabbir QB, 2007, IEEE T VEH TECHNOL, V56, P3397, DOI 10.1109/TVT.2007.906987
   Özcelebi T, 2007, IEEE J SEL AREA COMM, V25, P760, DOI 10.1109/JSAC.2007.070512
   POULIN D, 2008, C80216MAINT05112R8 I
   RAPPAPORT TS, 1996, COMMUNICATION ENG EM
   *RFC, 2006, 4585 RFC
   TRIANTAFYLLOPOU.D, 2008, P IEEE INT S BROADB, P1
   Villalón J, 2007, IEEE J SEL AREA COMM, V25, P699, DOI 10.1109/JSAC.2007.070507
   Xenakis D, 2008, 2008 3RD INTERNATIONAL SYMPOSIUM ON WIRELESS PERVASIVE COMPUTING, VOLS 1-2, P165, DOI 10.1109/ISWPC.2008.4556189
   XERGIAS S, 2005, P 14 IEEE WORKSH LOC
NR 16
TC 7
Z9 7
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2009
VL 11
IS 6
BP 1131
EP 1139
DI 10.1109/TMM.2009.2026098
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 493YW
UT WOS:000269776700009
DA 2024-07-18
ER

PT J
AU Gopalakrishnan, V
   Hu, YQ
   Rajan, D
AF Gopalakrishnan, Viswanath
   Hu, Yiqun
   Rajan, Deepu
TI Salient Region Detection by Modeling Distributions of Color and
   Orientation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image modeling; salient regions; visual attention
ID VISUAL-ATTENTION; IMAGES; SCALE
AB We present a robust salient region detection framework based on the color and orientation distribution in images. The proposed framework consists of a color saliency framework and an orientation saliency framework. The color saliency framework detects salient regions based on the spatial distribution of the component colors in the image space and their remoteness in the color space. The dominant hues in the image are used to initialize an expectation-maximization (EM) algorithm to fit a Gaussian mixture model in the hue-saturation (H-S) space. The mixture of Gaussians framework in H-S space is used to compute the inter-cluster distance in the H-S domain as well as the relative spread among the corresponding colors in the spatial domain. Orientation saliency framework detects salient regions in images based on the global and local behavior of different orientations in the image. The oriented spectral information from the Fourier transform of the local patches in the image is used to obtain the local orientation histogram of the image. Salient regions are further detected by identifying spatially confined orientations and with the local patches that possess high orientation entropy contrast. The final saliency map is selected as either color saliency map or orientation saliency map by automatically identifying which of the maps leads to the correct identification of the salient region. The experiments are carried out on a large image database annotated with "ground-truth" salient regions, provided by Microsoft Research Asia, which enables us to conduct robust objective level comparisons with other salient region detection algorithms.
C1 [Gopalakrishnan, Viswanath; Hu, Yiqun; Rajan, Deepu] Nanyang Technol Univ, Sch Comp Engn, Ctr Multimedia & Network Technol, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Gopalakrishnan, V (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Ctr Multimedia & Network Technol, Singapore 639798, Singapore.
EM visw0005@ntu.edu.sg; yqhu@ntu.edu.sg; asdrajan@ntu.edu.sg
RI Rajan, Deepu/A-3666-2011; Gopalakrishnan, Viswanath/ABE-4111-2021
OI Gopalakrishnan, Viswanath/0000-0001-7813-877X
CR Achanta R, 2008, LECT NOTES COMPUT SC, V5008, P66
   [Anonymous], NEURAL BASIS VISUAL
   [Anonymous], 2004, Advances in Neural Information Processing Systems
   [Anonymous], 2003, P 11 ACM INT C MULTI, DOI DOI 10.1145/957013.957094
   [Anonymous], 2006, P IEEE CS C COMP VIS
   [Anonymous], P CVPR
   Bruce N., 2005, ADV NEURAL INFORM PR, V18, P155
   Chen LQ, 2003, MULTIMEDIA SYST, V9, P353, DOI 10.1007/s00530-003-0105-4
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   DUGUE AG, 2000, PATTERN RECOGNIT LET, V21, P1135
   Fan X, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P53
   Frintrop S, 2005, LECT NOTES COMPUT SC, V3663, P117
   Gao Dashan, 2007, P ICCV
   Gonzalez RC., Digital image processing third edition Pearson international edition prepared by Pearson Education
   Han JW, 2006, IEEE T CIRC SYST VID, V16, P141, DOI 10.1109/TCSVT.2005.859028
   Heidemann G, 2004, IEEE T PATTERN ANAL, V26, P817, DOI 10.1109/TPAMI.2004.29
   HOU X, 2007, P CVPR JUN
   Hu Y., 2004, P 12 ANN ACM INT C M, P340
   Hu Y., 2005, Proceedings of the ACM International Conference on Multimedia, P716
   Hu YQ, 2004, LECT NOTES COMPUT SC, V3332, P993
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Joseph JS, 1996, PERCEPT PSYCHOPHYS, V58, P651, DOI 10.3758/BF03213098
   Jost T, 2005, COMPUT VIS IMAGE UND, V100, P107, DOI 10.1016/j.cviu.2004.10.009
   Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855
   Kuan YH, 2008, IEEE T MULTIMEDIA, V10, P832, DOI 10.1109/TMM.2008.922853
   LINDEBERG T, 1993, INT J COMPUT VISION, V11, P283, DOI 10.1007/BF01469346
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Meur O L, 2006, IEEE T PATTERN ANAL, V28, P802
   Morse Bryan S., 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, P497
   Oliva A, 2003, IEEE IMAGE PROC, P253, DOI 10.1109/icip.2003.1246946
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Park SJ, 2002, LECT NOTES COMPUT SC, V2525, P418
   Pauwels EJ, 1999, COMPUT VIS IMAGE UND, V75, P73, DOI 10.1006/cviu.1999.0763
   REISFELD D, 1995, INT J COMPUT VISION, V14, P119, DOI 10.1007/BF01418978
   Snowden RJ, 2002, PSYCHOL SCI, V13, P180, DOI 10.1111/1467-9280.00433
   Stentiford F., 2007, P 5 INT C COMP VIS S
   van de Weijer J, 2006, IEEE T PATTERN ANAL, V28, P150, DOI 10.1109/TPAMI.2006.3
   Walther D, 2006, NEURAL NETWORKS, V19, P1395, DOI 10.1016/j.neunet.2006.10.001
   Yu ZW, 2007, IEEE T MULTIMEDIA, V9, P766, DOI 10.1109/TMM.2007.893351
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
NR 40
TC 107
Z9 127
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2009
VL 11
IS 5
BP 892
EP 905
DI 10.1109/TMM.2009.2021726
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 474KN
UT WOS:000268282300009
DA 2024-07-18
ER

PT J
AU Wu, X
   Ngo, CW
   Hauptmann, AG
AF Wu, Xiao
   Ngo, Chong-Wah
   Hauptmann, Alexander G.
TI Multimodal news story clustering with pairwise visual near-duplicate
   constraint
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE multimedia topic detection and tracking; near-duplicate visual
   constraint; news story clustering; video data mining
ID IMAGE
AB Story clustering is a critical step for news retrieval, topic mining, and summarization. Nonetheless, the task remains highly challenging owing to the fact that news topics exhibit clusters of varying densities, shapes, and sizes. Traditional algorithms are found to be ineffective in mining these types of clusters. This paper offers a new perspective by exploring the pairwise visual cues deriving from near-duplicate keyframes (NDK) for constraint-based clustering. We propose a constraint-driven co-clustering algorithm (CCC), which utilizes the near-duplicate constraints built on top of text, to mine topic-related stories and the outliers. With CCC, the duality between stories and their underlying multimodal features is exploited to transform features in low-dimensional space with normalized cut. The visual constraints are added directly to this new space, while the traditional DBSCAN is revisited to capitalize on the availability of constraints and the reduced dimensional space. We modify DBSCAN with two new characteristics for story clustering: 1) constraint-based centroid selection and 2) adaptive radius. Experiments on TRECVID-2004 corpus demonstrate that CCC with visual constraints is more capable of mining news topics of varying densities, shapes and sizes, compared with traditional k-means, DBSCAN, and spectral co-clustering algorithms.
C1 [Wu, Xiao; Hauptmann, Alexander G.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
   [Wu, Xiao; Ngo, Chong-Wah] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
C3 Carnegie Mellon University; City University of Hong Kong
RP Wu, X (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
EM cwngo@cs.eityu.edu.hk; cwngo@cs.eityu.edu.hk; alex@cs.cmu.edu
OI Ngo, Chong Wah/0000-0003-4182-8261; Wu, Xiao/0000-0002-8322-8558
CR AGGARWAL CC, 2003, P C VER LARG DAT BAS
   Allan J., 2002, INTRO TOPIC DETECTIO
   Ankerst M, 1999, SIGMOD RECORD, VOL 28, NO 2 - JUNE 1999, P49
   [Anonymous], P 14 ACM INT C MULT
   [Anonymous], 1996, P 2 INT C KNOWL DISC
   [Anonymous], 2001, P 18 INT C MACH LEAR
   [Anonymous], 2003, P 26 ANN INT ACM SIG, DOI DOI 10.1145/860435.860495
   [Anonymous], P ACM INT C MULT, DOI DOI 10.1145/1101149.1101167
   [Anonymous], 2004, P 12 ANN ACM INT C M
   [Anonymous], P 23 ANN INT ACM SIG
   Breunig M. M., 2000, SIGMOD Record, V29, P93, DOI 10.1145/335191.335388
   Cao F, 2006, SIAM PROC S, P328, DOI 10.1137/1.9781611972764.29
   Chang H., 2004, ACM PROC ICML, P153, DOI DOI 10.1145/1015330.1015391
   CHANG SF, 2005, P TRECVID GAITH MD
   CUTTING DR, 1992, SIGIR 92 : PROCEEDINGS OF THE FIFTEENTH ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P318
   Dhillon IS, 2001, P 7 ACM SIGKDD INT C, P269, DOI DOI 10.1145/502512.502550
   Dhillon IS, 2003, P 9 ACM SIGKDD INT C, P89, DOI DOI 10.1145/956750.956764
   Gao B., 2005, P 11 INT C KNOWLEDGE, P41, DOI DOI 10.1145/1081870.1081879
   Gauvain JL, 2002, SPEECH COMMUN, V37, P89, DOI 10.1016/S0167-6393(01)00061-9
   Hertz T, 2003, PROC CVPR IEEE, P668
   Hsu WH, 2006, IEEE IMAGE PROC, P141, DOI 10.1109/ICIP.2006.312379
   Ke Y., 2004, ACM MULTIMEDIA 04, P869
   Klein D., 2002, Tech. rep., P307
   Lange T, 2005, PROC CVPR IEEE, P731
   Law MHC, 2005, SIAM PROC S, P641
   Ngo CW, 2005, IEEE T CIRC SYST VID, V15, P296, DOI 10.1109/TCSVT.2004.841694
   Pantel P., 2002, Proceedings of SIGIR 2002. Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P199
   Qiu GP, 2004, INT C PATT RECOG, P991, DOI 10.1109/ICPR.2004.1333940
   SHENTAL N, 2003, P NEUR INF PROC SYST
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Steinbach M., 2000, P KDD WORKSH TEXT MI
   Wu X, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P117
   Wu X, 2006, IEEE SIGNAL PROC MAG, V23, P59
   Yu SX, 2004, IEEE T PATTERN ANAL, V26, P173, DOI 10.1109/TPAMI.2004.1262179
   Zha H., 2001, Proceedings of the tenth international conference on Information and knowledge management, P25
   ZHAI Y, 2005, P 13 ACM INT C MULT, P2
   ZHANG D.-Q., 2004, PROC ACM INT C MULTI, P877, DOI DOI 10.1145/1027527.1027730
   Zhang DQ, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P117
   Zhu XQ, 2003, MULTIMEDIA SYST, V9, P31, DOI 10.1007/s00530-003-0076-5
NR 39
TC 49
Z9 55
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2008
VL 10
IS 2
BP 188
EP 199
DI 10.1109/TMM.2007.911778
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 254HC
UT WOS:000252576700003
OA Green Published
DA 2024-07-18
ER

PT J
AU Shahbahrami, A
   Juurlink, B
   Vassiliadis, S
AF Shahbahrami, Asadollah
   Juurlink, Ben
   Vassiliadis, Stamatis
TI Implementing the 2-D wavelet transform on SIMD-enhanced general-purpose
   processors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE cache; Discrete Wavelet Transform; memory hierarchy; multimedia
   extensions; SIMD
AB The 2-D Discrete Wavelet Transform (DWT) consumes up to 68% of the JPEG2000 encoding time. In this paper, we develop efficient implementations of this important kernel on general-purpose processors (GPPs), in particular the Pentium 4 (P4). Efficient implementations of the 2-D DWT on the P4 must address three issues. First, the P4 suffers from a problem known as 64K aliasing, which can degrade performance by an order of magnitude. We propose two techniques to avoid 64K aliasing which improve performance by a factor of up to 4.20. Second, a straightforward implementation of vertical filtering incurs many cache misses. Cache performance can be improved by applying loop interchange, but there will still be many conflict misses if the filter length exceeds the cache associativity. Two methods are proposed to reduce the number of conflict misses which provide an additional performance improvement of up to 1.24. To show that these methods are general, results for the P3 and Opteron are also provided. Third, efficient implementations of the 2-D DWT must exploit the SIMD instructions supported by most GPPs, including the P4, and we present MMX and SSE implementations of horizontal and vertical filtering which provide a maximum speedup of 3.39 and 6.72, respectively.
C1 [Shahbahrami, Asadollah; Juurlink, Ben; Vassiliadis, Stamatis] Delft Univ Technol, Comp Engn Lab, EEMCS, NL-2628 CD Delft, Netherlands.
   [Shahbahrami, Asadollah] Univ Guilan, Fac Engn, Dept Elect Engn, Rasht, Iran.
C3 Delft University of Technology; University of Guilan
RP Shahbahrami, A (corresponding author), Delft Univ Technol, Comp Engn Lab, EEMCS, NL-2628 CD Delft, Netherlands.
EM shahbahrami@ce.et.tudelft.nl; benj@ce.et.tudelft.nl
RI Shahbahrami, Asadollah/ABD-2432-2020
OI Shahbahrami, Asadollah/0000-0002-5195-1688
CR Adams MD, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P241
   Adams MD, 2000, IEEE T IMAGE PROCESS, V9, P1010, DOI 10.1109/83.846244
   Andreopoulos Y, 2002, DSP 2002: 14TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING PROCEEDINGS, VOLS 1 AND 2, P201, DOI 10.1109/ICDSP.2002.1027873
   [Anonymous], 2004, IA-32 Intel architecture optimization: reference manual
   Chatterjee S, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P797, DOI 10.1109/ICME.2002.1035902
   CHAVER D, 2003, P 17 IEEE INT S PAR
   CHAVER D, 2002, P INT C HIGH PERF CO
   Chrysafis C, 2000, IEEE T IMAGE PROCESS, V9, P378, DOI 10.1109/83.826776
   COHEN A, 1992, COMMUN PUR APPL MATH, V45, P485, DOI 10.1002/cpa.3160450502
   Dang PP, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P321
   Ferretti M, 2001, J VLSI SIG PROC SYST, V28, P165, DOI 10.1023/A:1011161423836
   Komi H, 2001, P IEEE INT C MULT EX, P465
   KUTIL R, 2007, P 14 EUR INT C PAR D, P413
   LEE YD, 2004, ELECT LETT, V40
   LOPEZESTRADA AA, 2005, Patent No. 20050188172
   MEERWALD P, 2002, P VIS COMM IM PROC
   SHAHBAHRAMI A, 2005, P 16 IEEE INT C APPL
   SHAHBAHRAMI A, 2007, P 3 ACM INT C COMP F, P253
   Shahbahrami A, 2006, INT J PARALLEL PROG, V34, P237, DOI 10.1007/s10766-006-0015-0
   Sweldens W, 1996, APPL COMPUT HARMON A, V3, P186, DOI 10.1006/acha.1996.0015
   Trenas MA, 1998, INT CONF ACOUST SPEE, P1521, DOI 10.1109/ICASSP.1998.681739
   2004, IA 32 INTEL ARCHITEC, V3
NR 22
TC 16
Z9 18
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2008
VL 10
IS 1
BP 43
EP 51
DI 10.1109/TMM.2007.911195
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 245QZ
UT WOS:000251952200005
OA Green Published
DA 2024-07-18
ER

PT J
AU Eeckhaut, H
   Devos, H
   Lambert, P
   De Schrijver, D
   Van Lancker, W
   Nollet, V
   Avasare, P
   Clerckx, T
   Verdicchio, F
   Christiaens, M
   Schelkens, P
   Van De Walle, R
   Stroobandt, D
AF Eeckhaut, Hendrik
   Devos, Harald
   Lambert, Peter
   De Schrijver, Davy
   Van Lancker, Wim
   Nollet, Vincent
   Avasare, Prabhat
   Clerckx, Tom
   Verdicchio, Fabio
   Christiaens, Mark
   Schelkens, Peter
   Van de Walle, Rik
   Stroobandt, Dirk
TI Scalable, wavelet-based video: From server to hardware-accelerated
   client
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE MPEG-21 BSDL; negotiation; reconfigurable; hardware; scalable video;
   wavelets
ID RECONFIGURABLE HARDWARE; PERFORMANCE
AB Video source, carrier and client diversification have led the video coding community to develop scalable video codecs supporting efficient decoding at varying resolution, frame rate and quality. Scalable video has several advantages over a nonscalable approach, but a large scale deployment is far from trivial and a lot of open questions remain. To resolve these, we developed a complete video delivery chain for scalable wavelet-based video. This includes a video server, a negotiation framework, a video scaling infrastructure and two scalable video clients, one pure software client and one real-time, hardware accelerated client. This paper describes the complete chain and identifies and quantifies the impact of using scalable video in every link of this chain.
C1 Univ Ghent, Parallel Informat Syst PARIS Grp, B-9000 Ghent, Belgium.
   IMEC, MPSoc, B-3001 Louvain, Belgium.
   Vrije Univ Brussel VIB, Dept Elect Engn, B-1050 Brussels, Belgium.
C3 Ghent University; IMEC; Vrije Universiteit Brussel; Flanders Institute
   for Biotechnology (VIB)
RP Eeckhaut, H (corresponding author), Univ Ghent, Parallel Informat Syst PARIS Grp, B-9000 Ghent, Belgium.
EM Hendrik.Eeckhaut@elis.Ugent.be; Harald.Devos@elis.Ugent.be;
   peter.lambert@ugent.be; davy.deschrijver@ugent.be;
   wim.vanlancker@ugent.be; nollet@imec.be; avasare@imec.be;
   tclerckx@etro.vub.ac.be; fverdicc@etro.vub.ac.be;
   Mark.Christiaens@UGent.be; Peter.Schelekens@vub.ac.be;
   rik.vandewalle@ugent.be; Dirk.Stroobandt@Ugent.be
RI Schelkens, Peter/B-7831-2008; Lambert, Peter/D-7776-2016
OI Schelkens, Peter/0000-0003-0908-1655; Lambert, Peter/0000-0001-5313-4158
CR ANDREOPOULOS I, 2003, M9911 MPEG
   Andreopoulos Y, 2005, IEEE T SIGNAL PROCES, V53, P1398, DOI 10.1109/TSP.2005.843707
   Andreopoulos Y, 2004, SIGNAL PROCESS-IMAGE, V19, P653, DOI 10.1016/j.image.2004.05.007
   ANDREOPOULOS Y, 2005, THESIS VRIJE U BRUSS
   [Anonymous], 2006, STRATIX DEVICE HDB
   Avasare P., 2005, Proceedings of the 5th ACM International Conference on Embedded Software, P17, DOI DOI 10.1145/1086228.1086232
   Burnett I.S., 2006, MPEG 21 BOOK, V1st
   Choi SJ, 1999, IEEE T IMAGE PROCESS, V8, P155, DOI 10.1109/83.743851
   De Schrijver D, 2005, IEEE INT SYM MULTIM, P79
   De Schrijver D, 2006, MULTIMEDIA SYST, V11, P403, DOI 10.1007/s00530-006-0021-5
   DEVOS H, 2003, P PRORISC IEEE BEN W
   Devos H, 2007, LECT NOTES COMPUT SC, V4050, P159, DOI 10.1007/978-3-540-71528-3_11
   Eeckhaut H, 2005, DES AUT TEST EUROPE, P14, DOI 10.1109/DATE.2005.16
   EECKHAUT H, 2005, P SPIE WAV APPL IND, V6001, P169
   Flierl M, 2004, SIGNAL PROCESS-IMAGE, V19, P561, DOI 10.1016/j.image.2004.05.002
   HSIANG ST, 2006, JVTU133
   *ISO, 2005, 1449610 AVC
   *ISO, 2006, 2100082006 ISO IEC
   *ISO IEC, 2004, 2100072004 ISO IEC
   KUON I, 2006, FPGA 06, P21
   Lambert P, 2006, IEEE T CIRC SYST VID, V16, P134, DOI 10.1109/TCSVT.2005.857783
   Mignolet JY, 2003, DESIGN, AUTOMATION AND TEST IN EUROPE CONFERENCE AND EXHIBITION, PROCEEDINGS, P986
   Nollet V, 2005, DES AUT TEST EUROPE, P234, DOI 10.1109/DATE.2005.91
   OHM JR, 1994, IEEE T IMAGE PROCESS, V3, P559, DOI 10.1109/83.334985
   Park HW, 2000, IEEE T IMAGE PROCESS, V9, P577, DOI 10.1109/83.841935
   Pesquet-Popescu B, 2001, INT CONF ACOUST SPEE, P1793, DOI 10.1109/ICASSP.2001.941289
   Radha H, 1999, SIGNAL PROCESS-IMAGE, V15, P95, DOI 10.1016/S0923-5965(99)00026-0
   Radha HM, 2001, IEEE T MULTIMEDIA, V3, P53, DOI 10.1109/6046.966110
   Schelkens P., 2004, Proceedings of the SPIE - The International Society for Optical Engineering, V5266, P147, DOI 10.1117/12.516177
   SCHULZRINNE H, 1998, 2326 IETF RFC
   Schulzrinne H., 2003, 3550 IETF RFC
   SCHWARTZ H, 2004, MPEG04M11244
   Stroobandt D, 2004, LECT NOTES COMPUT SC, V3133, P203
   TAUBMAN D, 1994, IEEE T IMAGE PROCESS, V3, P572, DOI 10.1109/83.334984
   Turaga DS, 2005, SIGNAL PROCESS-IMAGE, V20, P1, DOI 10.1016/j.image.2004.08.006
   Vetro A, 2003, IEEE SIGNAL PROC MAG, V20, P16, DOI 10.1109/MSP.2003.1184335
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   XIONG R, IN PRESS IEEE T CIRC
NR 38
TC 10
Z9 10
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2007
VL 9
IS 7
BP 1508
EP 1519
DI 10.1109/TMM.2007.906606
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 224LB
UT WOS:000250447400015
DA 2024-07-18
ER

PT J
AU Grigorova, A
   De Natale, FGB
   Dagli, C
   Huang, TS
AF Grigorova, Anelia
   De Natale, Francesco G. B.
   Dagli, Charlie
   Huang, Thomas S.
TI Content-based image retrieval by feature adaptation and relevance
   feedback
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptive retrieval; content-based image retrieval; relevance feedback
AB The paper proposes an adaptive retrieval approach based on the concept of relevance-feedback, which establishes a link between high-level concepts and low-level features, using the user's feedback not only to assign proper weights to the features, but also to dynamically select them within a large collection of parameters. The target is to identify a set of relevant features according to a user query while at the same time maintaining a small sized feature vector to attain better matching and lower complexity. To this end, the image description is modified during each retrieval by removing the least significant features and better specifying the most significant ones. The feature adaptation is based on a hierarchical approach. The weights are then adjusted based on previously retrieved relevant and irrelevant images without further user-feedback. The algorithm is not fixed to a given feature set. It can be used with different hierarchical feature sets, provided that the hierarchical structure is defined a priori. Results achieved on different image databases and two completely different feature sets show that the proposed algorithm outperforms previously proposed methods. Further, it is experimentally demonstrated that it approaches the results obtained by state-of-the-art feature-selection techniques having complete knowledge of the data set.
C1 Univ Trent, Dept Informat & Commun Technol, I-38050 Trento, Italy.
   Univ Illinois, Beckman Inst Adv Sci & Technol, Urbana, IL 61801 USA.
C3 University of Trento; Fondazione Bruno Kessler; FBK-ICT - Center for
   Information & Communication Technology; University of Illinois System;
   University of Illinois Urbana-Champaign
RP Grigorova, A (corresponding author), Univ Trent, Dept Informat & Commun Technol, I-38050 Trento, Italy.
EM anelia@science.unitn.it; denatale@ing.unitn.it; dagli@uiuc.edu;
   t-huangl@uiuc.edu
RI yan, shuicheng/A-8531-2014; yan, shuicheng/HCH-9860-2022
OI yan, shuicheng/0000-0001-8906-3777; yan, shuicheng/0000-0003-4527-1018
CR AHA DW, 1999, ARTIFICIAL INTELLIGE
   [Anonymous], 1995, STORAGE RETRIEVAL IM, DOI DOI 10.1117/12.205308
   Baeza-Yates Ricardo, 1999, MODERN INFORM RETRIE, V463
   Chen LP, 2004, 10TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P273
   Eidenberger H., 2002, 2002 7th International Conference on Control, Automation, Robotics and Vision (IEEE Cat. No.02EX649), P174
   GONG SR, 2002, P INT C MACH LEARN C, V4, P2055
   Han SH, 1997, J COGNITIVE NEUROSCI, V9, P687, DOI 10.1162/jocn.1997.9.5.687
   Ishikawa Y., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P218
   LERAY P, 1998, 1998012 LIP6
   *MPEG, 2002, MPEG7
   Ng R, 1996, P SOC PHOTO-OPT INS, V2670, P50, DOI 10.1117/12.234809
   PARK DK, 2000, P ICME 2000, V1, P355
   QIAN F, 2001, P 2001 ACM WORKSH MU, P14
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Rui Y, 2000, PROC CVPR IEEE, P236, DOI 10.1109/CVPR.2000.855825
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   WANG T, 2001, P INT JOINT C NEUR N, V1, P103
   WITTEN IH, P ICONIP ANZISANNES, P192
   Wu P, 1999, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES (CBAIVL'99) - PROCEEDINGS, P3, DOI 10.1109/IVL.1999.781114
   Wu YM, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P581
   Young T.Y., 1986, HDB PATTERN RECOGNIT
   Zhou XS, 2001, PROC CVPR IEEE, P11
   Zhou XS, 2003, MULTIMEDIA SYST, V8, P536, DOI 10.1007/s00530-002-0070-3
NR 24
TC 44
Z9 54
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2007
VL 9
IS 6
BP 1183
EP 1192
DI 10.1109/TMM.2007.902828
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 215XJ
UT WOS:000249842000009
DA 2024-07-18
ER

PT J
AU Jurca, D
   Frossard, P
AF Jurca, Dan
   Frossard, Pascal
TI Video packet selection and scheduling for multipath streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE branch and bound; load balancing; multipath streaming; packet scheduling
ID ALGORITHMS
AB This paper addresses the problem of choosing the best streaming policy for distortion optimal multipath video delivery, under network bandwidth and playback delay constraints. The streaming policy consists in a joint selection of the network path and of the video packets to be transmitted, along with their sending time. A simple streaming model is introduced, which takes into account the video packet importance, and the dependencies between packets. A careful timing analysis allows to compute the quality perceived by the receiver for a constrained playback delay, as a function of the streaming policy. We derive an optimization problem based on a video abstraction model, under the assumption that the server knows, or can predict accurately the state of the network. A detailed analysis of constrained multipath streaming systems provides helpful insights to design an efficient branch and bound algorithm that finds the optimal streaming strategy. This solution allows to bound the performance of any scheduling strategy, but the complexity of the algorithm becomes rapidly intractable. We therefore propose a fast heuristic-based algorithm, built on load-balancing principles. It allows to reach close to optimal performance with a polynomial time complexity. The algorithm is then adapted to live streaming scenarios, where the server has only a partial knowledge of the packet stream, and the channel bandwidth. Extensive simulations show that the proposed algorithm only induces a negligible distortion penalty compared to the optimal strategy, even when the optimization horizon is limited, or the rate estimation is not perfect. Simulation results also demonstrate that the proposed scheduling solution performs better than common scheduling algorithms, and therefore represents a very efficient low-complexity multipath streaming algorithm, for both stored and live video services.
C1 Ecole Polytech Fed Lausanne, Signal Proc Inst, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Jurca, D (corresponding author), Ecole Polytech Fed Lausanne, Signal Proc Inst, CH-1015 Lausanne, Switzerland.
EM dan.jurca@epfl.ch; pascal.frossard@epfl.ch
RI Frossard, Pascal/AAF-2268-2019
CR [Anonymous], 2002, COMPUTERS INTRACTABI
   Apostolopoulos J, 2002, IEEE INFOCOM SER, P1736, DOI 10.1109/INFCOM.2002.1019427
   Apostolopoulos JG, 2004, IEEE COMMUN MAG, V42, P80, DOI 10.1109/MCOM.2004.1321395
   Chebrolu K, 2006, IEEE T MOBILE COMPUT, V5, P388, DOI 10.1109/TMC.2006.1599407
   Chebrolu K, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P4097, DOI 10.1109/ICC.2004.1313320
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   DECUETOS P, 2003, RR03078
   Golubchik L, 2002, PERFORM EVALUATION, V49, P429, DOI 10.1016/S0166-5316(02)00125-6
   HUANG J, 2003, P PACK VID WORKSH NA
   Jonsson J, 1997, PROC INT CONF PARAL, P158, DOI 10.1109/ICPP.1997.622580
   JURCA D, 2004, P PACK VID WORKSH IR
   JURCA D, 2006, P IEEE ICME TORO JUL
   JURCA D, 2004, TRITS2004025
   Kelly F.P., 1996, Stochastic Networks: Theory and Applications, P141
   MAN H, 2004, P SPIE IS T EL IM, P1148
   Michalewicz Z., 2000, How to Solve It: Modern Heuristics
   NGUYEN T, 2002, P PACK VID WORKSH PI
   NGUYEN T, 2003, P IEEE INT C MULT EX, V1, P1
   Radha HM, 2001, IEEE T MULTIMEDIA, V3, P53, DOI 10.1109/6046.966110
   Ribeiro V.J., 2003, PASSIVE ACTIVE MEASU
   Röder M, 2006, IEEE T MULTIMEDIA, V8, P170, DOI 10.1109/TMM.2005.861281
   SAMADZADEH FA, 1992, P ACM COMP SCI C APR, P477
   TIAN D, 2004, P IEEE INT C COMM IC
   Tian DH, 2004, IEEE WCNC, P1287, DOI 10.1109/WCNC.2004.1311374
   Zhang ZL, 2001, REAL-TIME IMAGING, V7, P255, DOI 10.1006/rtim.2001.0226
NR 25
TC 85
Z9 92
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2007
VL 9
IS 3
BP 629
EP 641
DI 10.1109/TMM.2006.888017
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148PW
UT WOS:000245088100017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kim, JH
AF Kim, Jae-Hoon
TI On delay-bounded multimedia internet-call access control in a WCDMA
   common channel
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE access control; multimedia internet; WCDMA
ID ADMISSION CONTROL; PERFORMANCE; POLICY
AB We address call access control in the Wideband Code Divisional Multiple Access (WCDMA) downlink common channels. Proposed is a policy consisting of two phases, the first screening calls for admission, and the second scheduling call services according to the most popular preemptive FIFO rule. The scheduling mechanism is first depicted on the infinite vertical time-strip with width representing the aggregate capacity of common channels, by denoting a call service as positioning the corresponding call-rectangle thereon. Enabled by this depiction, we apply the existing results on the two-dimensional (2-D) bin packing problem in predicting the sojourn time of an arriving call. The first phase of call admission control is based on the predictions, from the characteristics of which calls, once admitted, are guaranteed to be serviced within allowable delay. The performance of the proposed access control policy is extensively tested with realistic input data. The results are found convincingly affirmative for its practical applicability.
C1 Access Network & Mobile Device R&D Ctr, SK Telecom, Seoul 100999, South Korea.
C3 SK Group; SK Telecom
RP Kim, JH (corresponding author), Access Network & Mobile Device R&D Ctr, SK Telecom, Seoul 100999, South Korea.
EM jayhoon.kim@gmail.com
CR BAKER BS, 1982, J ALGORITHM, V3, P303, DOI 10.1016/0196-6774(82)90028-1
   BRUCKER P, 1997, SCHEDULINT ALGORITHM
   BRUCKER P, 2000, 17 ITC OCT, P793
   COFFMAN EG, 1980, SIAM J COMPUT, V9, P808, DOI 10.1137/0209062
   COMANCIU C, 2000, MULTIMEDIA EXPO 2000, P1265
   Comaniciu C, 2000, IEEE VTS VEH TECHNOL, P249, DOI 10.1109/VETECF.2000.886661
   CUNHA H, 1995, BUCS85010
   Das SK, 2003, IEEE J SEL AREA COMM, V21, P1790, DOI 10.1109/JSAC.2003.815928
   Holma Harry., 2000, WCDMA UMTS, V1
   Hu F, 2003, COMPUT NETW, V43, P263, DOI 10.1016/S1389-1286(03)00271-8
   Ishikawa Y, 1997, IEEE J SEL AREA COMM, V15, P1627, DOI 10.1109/49.634800
   *ITU R M, ITM2000 ITURM TASK G
   Johnson DavidS., 1974, SIAM Journal on computing, V3, P299, DOI DOI 10.1137/0203025
   Kazmi M, 2000, IEEE VTS VEH TECHNOL, P674, DOI 10.1109/VETECF.2000.887094
   Kim JB, 2000, IEEE T VEH TECHNOL, V49, P506, DOI 10.1109/25.832982
   Koo I, 2001, GLOB TELECOMM CONF, P2611, DOI 10.1109/GLOCOM.2001.966248
   KWON YK, 2002, WIREL NETW, V8, P495
   Liu TK, 1998, IEEE J SEL AREA COMM, V16, P845, DOI 10.1109/49.709448
   Shao HR, 2003, 23RD INTERNATIONAL CONFERENCE ON DISTRIBUTED COMPUTING SYSTEMS WORKSHOPS, P838
   Wu S, 2002, IEEE ACM T NETWORK, V10, P257, DOI 10.1109/90.993306
   1998, ANAL IMT 2000 TRAFFI
NR 21
TC 1
Z9 1
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2006
VL 8
IS 5
BP 1075
EP 1081
DI 10.1109/TMM.2006.879831
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 091TY
UT WOS:000241052400018
DA 2024-07-18
ER

PT J
AU Yu, YK
   Wong, KH
   Chang, MMY
AF Yu, Ying Kin
   Wong, Kin Hong
   Chang, Michael Ming Yuen
TI Merging artificial effects with marker-less video sequences based on the
   interacting multiple model method
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE augmented reality; Kalman filtering; interacting multiple model; pose
   tracking; structure from motion
ID TARGET TRACKING; MOTION; SHAPE; RECONSTRUCTION
AB Inserting synthetic objects into video sequences has gained much interest in recent years. Fast and robust vision-based algorithms are necessary to make such an application possible. Traditional pose tracking schemes using recursive structure from motion techniques adopt one Kalman filter and thus only favor a certain type of camera motion. We propose a robust simultaneous pose tracking and structure recovery algorithm using the interacting multiple model (IMM) to improve performance. In particular, a set of three extended Kalman filters (EKFs), each describing a frequently occurring camera motion in real situations (general, pure translation, pure rotation), is applied within the IMM framework to track the pose of a scene. Another set of EKFs, one filter for each model point, is used to refine the positions of the model features in the 3-D space. The filters for pose tracking and structure refinement are executed in an interleaved manner. The results are used for inserting virtual objects into the original video footage. The performance of the algorithm is demonstrated with both synthetic and real data. Comparisons with different approaches have been performed and show that our method is more efficient and accurate.
C1 Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
   Chinese Univ Hong Kong, Dept Informat Engn, Shatin, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong; Chinese University of Hong Kong
RP Yu, YK (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
EM ykyu@cse.cuhk.edu.hk; khwong@cse.cuhk.edu.hk; mchang@ie.cuhk.edu.hk
CR AZARBAYEJANI A, 1995, IEEE T PATTERN ANAL, V17, P562, DOI 10.1109/34.387503
   Beardsley PA, 1997, INT J COMPUT VISION, V23, P235, DOI 10.1023/A:1007923216416
   Blom H. A. P., 1984, Proceedings of the 23rd IEEE Conference on Decision and Control (Cat. No. 84CH2093-3), P656
   Bradshaw KJ, 1997, IEEE T PATTERN ANAL, V19, P219, DOI 10.1109/34.584099
   BROIDA TJ, 1990, IEEE T AERO ELEC SYS, V26, P639, DOI 10.1109/7.55557
   Chang MMY, 2005, IEEE T MULTIMEDIA, V7, P253, DOI 10.1109/TMM.2005.843344
   Chiuso A, 2002, IEEE T PATTERN ANAL, V24, P523, DOI 10.1109/34.993559
   DAEIPOUR E, 1995, IEEE T AERO ELEC SYS, V31, P706, DOI 10.1109/7.381918
   Davison AJ, 2002, IEEE T PATTERN ANAL, V24, P865, DOI 10.1109/TPAMI.2002.1017615
   GIBSON S, 2002, P INT S MIX AUGM REA
   Grewal Mohinder S., 1993, Kalman filtering: Theory and Practice with MATLAB
   Hartley R, 2000, MULTIPLE VIEW GEOMET
   Li XR, 2003, IEEE T AERO ELEC SYS, V39, P1333, DOI 10.1109/TAES.2003.1261132
   Lippiello V, 2001, PROCEEDINGS OF THE 2001 IEEE INTERNATIONAL CONFERENCE ON CONTROL APPLICATIONS (CCA'01), P702, DOI 10.1109/CCA.2001.973950
   Mazor E, 1998, IEEE T AERO ELEC SYS, V34, P103, DOI 10.1109/7.640267
   Poelman CJ, 1997, IEEE T PATTERN ANAL, V19, P206, DOI 10.1109/34.584098
   Szeliski R, 1997, IEEE T PATTERN ANAL, V19, P506, DOI 10.1109/34.589211
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   Tomasi C, 1991, DETECTION TRACKING P
   Triggs B., 2000, VISION ALGORITHMS TH, P298, DOI DOI 10.1007/3-540-44480-7_21THISWORKWASSUPPORTEDINPARTBYTHEEUROPEAN
   Trucco E., 1998, INTRO TECHNIQUES 3D
   Yu YK, 2005, IEEE T SYST MAN CY B, V35, P587, DOI 10.1109/TSMCB.2005.846665
   Yu YK, 2004, INT C PATT RECOG, P241, DOI 10.1109/ICPR.2004.1334143
   YU YK, IN PRESS IEEE T SY B
   YU YK, 2006, P IEEE C COMP VIS PA
NR 25
TC 9
Z9 10
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2006
VL 8
IS 3
BP 521
EP 528
DI 10.1109/TMM.2006.870734
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046OX
UT WOS:000237822000009
DA 2024-07-18
ER

PT J
AU Yin, H
   Lin, CA
   Qiu, F
   Liu, JC
   Min, GY
   Li, B
AF Yin, H
   Lin, CA
   Qiu, F
   Liu, JC
   Min, GY
   Li, B
TI CASM: A content-aware protocol for secure video multicast
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content-awareness; key management; secure multicast; selective
   encryption
ID KEY MANAGEMENT; WATERMARKING
AB Information security has been a critical issue in the design and development of reliable distributed communication systems and has attracted significant research efforts. A challenging task is how to maintain information security at a high level for multiple-destination video applications with the huge volume of data and dynamic property of clients. This paper proposes a novel Content-Aware Secure Multicast (CASM) protocol for video distribution that seamlessly integrates three important modules: 1) a scalable light-weight algorithm for group key management; 2) a content-aware key embedding algorithm that can make video quality distortion imperceptible and is reliable for clients to detect embedded keys; and 3) a smart two-level video encryption algorithm that can selectively encrypt a small set of video data only, and yet ensure the video as well as the embedded keys unrecognizable without a genuine key. The implementation of the CASM protocol is independent of the underlying multicast mechanism and is fully compatible with existing coding standards. Performance evaluation studies built upon a CASM prototype have demonstrated that CASM is highly robust and scalable in dynamic multicast environments. Moreover, it ensures secure distribution of key and video data with minimized communication and computation overheads. The proposed content-aware key embedding and encryption algorithms are fast enough to support real-time video multicasting.
C1 Tsinghua Univ, Comp Sci & Technol Dept, Beijing 100084, Peoples R China.
   Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
   Univ Bradford, Dept Comp, Bradford BD7 1DP, W Yorkshire, England.
   Hong Kong Univ Sci & Technol, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
C3 Tsinghua University; Simon Fraser University; University of Bradford;
   Hong Kong University of Science & Technology
RP Tsinghua Univ, Comp Sci & Technol Dept, Beijing 100084, Peoples R China.
EM hyin@csnet1.cs.ts-inghua.edu.cn; chlin@tsin-hua.edu.cn;
   fqiu@csnet1cs.tsinghua.edu.cn; jcliu@cs.sfu.ca; g.min@brad.ac.uk;
   bli@cs.ust.hk
OI Min, Geyong/0000-0003-1395-7314
CR Alattar AM, 2003, IEEE T CIRC SYST VID, V13, P787, DOI 10.1109/TCSVT.2003.815958
   Chan KC, 2003, IEEE NETWORK, V17, P30, DOI 10.1109/MNET.2003.1233915
   CHU H, 2002, ACM SIGCOMM COMPUT C, V32
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Lee SJ, 2001, ISIE 2001: IEEE INTERNATIONAL SYMPOSIUM ON INDUSTRIAL ELECTRONICS PROCEEDINGS, VOLS I-III, P272, DOI 10.1109/ISIE.2001.931796
   LI Y, 1996, P IEEE INT WORKSH MU
   LIU J, 2003, IEEE MULTIMEDIA, V10
   Maples T., 1995, P 4 INT C COMP COMM
   Meyer J., Security Mechanisms for Multimedia Data with the Example MPEG-1 Video
   Perrig A., 2001, P IEEE S SEC PRIV
   Qiao L., 1997, Las Vegas : Proceedings of the 1s International Conference on Imaging Science, Systems and Technology, P21
   Qiao L, 1998, INT J COMPUT GRAPH, V22
   RAFAELI S, 2000, THESIS LANCASTER U L
   SHI C, 1998, P 17 IEEE S REL DIST
   SHI CG, 1998, P 6 ACM INT MULT C B
   TANG L, 1996, P ACM MULT 96 BOST M
   Trappe W, 2003, IEEE T MULTIMEDIA, V5, P544, DOI 10.1109/TMM.2003.813279
   VETRO A, IEEE SIGNAL PROC MAR, P18
   Voyatzis G, 1999, P IEEE, V87, P1197, DOI 10.1109/5.771072
   Wang Y., 2002, VIDEO PROCESSING COM
   Wicker S.B., 1999, Reed-Solomon codes and their applications
   Yin H, 2005, LECT NOTES COMPUT SC, V3768, P246, DOI 10.1007/11582267_22
   YIN P, 2000, P IEEE INT C IM PROC
NR 23
TC 12
Z9 16
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2006
VL 8
IS 2
BP 270
EP 277
DI 10.1109/TMM.2005.864316
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 028GY
UT WOS:000236476300008
DA 2024-07-18
ER

PT J
AU Wu, YD
   Deng, RH
AF Wu, YD
   Deng, RH
TI Scalable authentication of MPEG-4 streams
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE authentication; digital signature; erasure correction coding; fine
   granular scalability; Merkle hash tree; MPEG-4
AB This paper presents three scalable and efficient schemes for authenticating MPEG-4 streams: the Flat Authentication Scheme, the Progressive Authentication Scheme, and the Hierarchical Authentication Scheme. All the schemes allow authentication of MPEG-4 streams over lossy networks by integrating seamlessly digital signatures and erasure correction coding with MPEG-4's fine granular scalability. A prominent feature of our schemes is their "sign once, verify many ways" property, i.e., they generate only one digital signature per compressed MPEG-4 object group, but allow clients to verify the authenticity of any down-scaled version of the original signed object group.
C1 Inst Infocomm Res, Informat Secur Dept, Singapore, Singapore.
   Singapore Management Univ, Sch Informat Syst, Singapore, Singapore.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); Singapore Management University
RP Inst Infocomm Res, Informat Secur Dept, Singapore, Singapore.
EM wydong@i2r.a-star.edu.sg; robertdeng@smu.edu.sg
RI DENG, Robert H./E-8547-2012
OI Deng, Robert/0000-0003-3491-8146
CR ACHIR N, 2003, P 10 INT C TEL, pI1452
   Andoh I, 2002, 2002 INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS, DIGEST OF TECHNICAL PAPERS, P280, DOI 10.1109/ICCE.2002.1014030
   [Anonymous], 2001, 144961 ISOIEC
   [Anonymous], 1996, LNCS, DOI DOI 10.1007/3-540-68697-5_1
   [Anonymous], 2000, P IEEE S SEC PRIV
   BELLARE M, 1997, LNCS, V1233, P00163, DOI DOI 10.1007/3-540-69053-0
   Chia CH, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON PERSONAL WIRELESS COMMUNICATIONS, P280
   DEVANBU P, 2001, P 14 IFIP WG 11 3 WO, V201, P101
   DEVANBU P, 2001, P 8 ACM C COMP COMM, P136
   Gennaro R, 1997, LECT NOTES COMPUT SC, V1294, P180
   GOLLE P, 2001, P NDSS SAN DIEG CA F
   GOODRICH MT, LNCS, V2433, P372
   Holliman M, 2000, IEEE T IMAGE PROCESS, V9, P432, DOI 10.1109/83.826780
   HSIAO HF, 2002, P IEEE INT S CIRC SY, P1441
   Ikkurthy P, 2002, C LOCAL COMPUT NETW, P421, DOI 10.1109/LCN.2002.1181814
   *ISO IEC, 2004, 14496132004E ISOIEC
   *ISO IEC, 2003, 144962 ISOIEC
   Jung Min Park, 2003, ACM Transactions on Information and Systems Security, V6, P258, DOI 10.1145/762476.762480
   KENT S, 2003, IP ENCAPSULATING SEC
   KRAWCZYK H, LNCS, V2139, P310
   KUHNE G, 1999, ACM MULTIMEDIA
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   LIN CY, 1999, SPIE SECURITY WATERM
   Lin S., 2004, Error Control Coding, Vsecond
   MERKLE RC, 1990, LECT NOTES COMPUT SC, V435, P218, DOI 10.1007/0-387-34805-0_21
   MINER S, 2001, P IEEE S SEC PRIV
   Mohr AE, 2000, IEEE J SEL AREA COMM, V18, P819, DOI 10.1109/49.848236
   National Institute of Standards and Technology, 1991, FED REGISTER, V56, P42980
   OCONNOR L, 2000, P 4 WORK C SMART CAR, P327
   Pang HH, 2004, PROC INT CONF DATA, P560, DOI 10.1109/ICDE.2004.1320027
   PANNETRAT A, EFFICIENT MULTICAST
   Peng Cheng, 2003, ACM MULTIMEDIA, P433
   Pereira F., 2002, IMSC Press multimedia series
   PERRIG A, 2001, P NDSS SAN DIEG CA F
   Pourmohammadi-Fallah Y, 2003, IEEE MULTIMEDIA, V10, P68, DOI 10.1109/MMUL.2003.1218258
   Pyun JY, 2002, 2002 INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS, DIGEST OF TECHNICAL PAPERS, P164, DOI 10.1109/ICCE.2002.1013973
   RIVEST RL, 1978, COMMUN ACM, V21, P120, DOI [10.1145/359340.359342, 10.1145/357980.358017]
   RIZZO L, 1997, ACM COMPUT COMMUN RE, V27
   Rohatgi P, 1999, 6TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P93, DOI 10.1145/319709.319722
   *SHS, 1995, FIPS PUBL, V1801
   VENKATRAMANI C, 2003, ACM MULTIMEDIA   NOV, P307
   WONG CK, P IEEE ICNP 98
   Wu YD, 2003, IEEE T CONSUM ELECTR, V49, P792, DOI 10.1109/TCE.2003.1261157
   Wu YD, 2002, LECT NOTES COMPUT SC, V2551, P354
   YAJNIK M, P IEEE INF 1999 NEW
   YANG XK, 2003, P INT S CIRC SYST, pII832
   YUAN C, 2003, P INT S CIRC SYST IS, pI1620
NR 47
TC 20
Z9 23
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2006
VL 8
IS 1
BP 152
EP 161
DI 10.1109/TMM.2005.861283
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 005UR
UT WOS:000234850000014
OA Green Published
DA 2024-07-18
ER

PT J
AU Daschiel, H
   Datcu, M
AF Daschiel, H
   Datcu, M
TI Design and evaluation of human-machine communication for image
   information mining
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE CBIR; human-machine communication; information representation and
   visualization; visual interface
ID RETRIEVAL; SYSTEM
AB Very large volumes of heterogenous data, like multimedia, earth observation images, scientific and engineering measurements, for instance, are continuously generated and stored. A typical case is the field of earth observation. The widespread availability of high resolution images does not only explore the volumes of data, but also brings order at magnitude in the image detail, thus enormously increasing the information content. However, today's concepts and technologies are still limited in communicating the information content to people for use in real life applications. In this paper, we overview a new concept for knowledge-driven image information mining (KIM) and both analyze and evaluate it from the perspective of human-machine communication. The KIM concept enables the information communication from a very large image repository to users via the Internet. The communication is at a semantic level of representation and is adapted to the user's conjecture.
C1 German Aerosp Ctr DLR, Remote Sensing Technol Inst IMF, D-82234 Oberpfaffenhofen, Germany.
C3 Helmholtz Association; German Aerospace Centre (DLR)
RP German Aerosp Ctr DLR, Remote Sensing Technol Inst IMF, D-82234 Oberpfaffenhofen, Germany.
RI DATCU, Mihai/G-1655-2016
CR [Anonymous], IEEE COMPUT
   [Anonymous], 1996, ADV KNOWLEDGE DISCOV
   [Anonymous], P IEEE INT GEOSC REM
   Brumby SP, 2000, PROC SPIE, V4049, P480, DOI 10.1117/12.410371
   Cox IJ, 2000, IEEE T IMAGE PROCESS, V9, P20, DOI 10.1109/83.817596
   Datcu M, 2003, IEEE T GEOSCI REMOTE, V41, P2923, DOI 10.1109/TGRS.2003.817197
   Giacinto G, 2004, PATTERN RECOGN, V37, P1499, DOI 10.1016/j.patcog.2004.01.005
   MACKAY DJC, 2004, EFFICIENT COMMUNICAT
   Pentland A, 1996, INT J COMPUT VISION, V18, P233, DOI 10.1007/BF00123143
   RASMUSSEN J, 1983, IEEE T SYST MAN CYB, V13, P257, DOI 10.1109/TSMC.1983.6313160
   RUI Y, 1997, P IEEE INT C IM PROC
   Santini S, 2001, IEEE T KNOWL DATA EN, V13, P337, DOI 10.1109/69.929893
   Schroder M, 1998, IEEE T GEOSCI REMOTE, V36, P1446, DOI 10.1109/36.718848
   Schröder M, 2000, IEEE T GEOSCI REMOTE, V38, P2288, DOI 10.1109/36.868886
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Smith J., 1997, THESIS COLUMBIA U NE
   WACHSMUTH S, 2000, VIDERE J COMPUT VIS, V1
NR 17
TC 11
Z9 12
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2005
VL 7
IS 6
BP 1036
EP 1046
DI 10.1109/TMM.2005.858383
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 986SY
UT WOS:000233471200005
DA 2024-07-18
ER

PT J
AU Li, WX
   You, J
   Zhang, D
AF Li, WX
   You, J
   Zhang, D
TI Texture-based palmprint retrieval using a layered search scheme for
   personal identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content-based image retrieval; image matching; palmprint feature
   extraction; personal identification; similarity measurement; texture
   features
AB This paper presents a new approach to palmprint retrieval for personal identification. Three key issues in image retrieval are considered-feature extraction, similarity measurement and fast search for the best match of the queried image in an image database. We propose a texture-based approach for palmprint feature representation. The concept of texture energy is introduced to define both global and local features of a palmprint, which are characterized with high convergence of inner-palm similarities and good dispersion of inter-palm discrimination. The searching is carried out in a layered fashion: the global features are first used to guide the fast selection of a small set of similar candidates from the database and then the local features are applied to determine the final output from the selected set of similar candidates. The experimental results illustrate the effectiveness of the proposed approach.
C1 Hong Kong Polytech Univ, Biometr Res Ctr, Dept Comp, Kowloon, Hong Kong, Peoples R China.
C3 Hong Kong Polytechnic University
RP Hong Kong Polytech Univ, Biometr Res Ctr, Dept Comp, Kowloon, Hong Kong, Peoples R China.
EM csdzhang@comp.polyu.edu.hk
RI Zhang, David D/O-9396-2016; Zhang, Hao/HHM-1940-2022
OI Zhang, David D/0000-0002-5027-5286; You, Jane/0000-0002-8181-4836
CR Ankerst M, 1998, IEEE T KNOWL DATA EN, V10, P996, DOI 10.1109/69.738362
   [Anonymous], 1999, Biometrics: Personal Identification in Networked Society
   BACH JR, 1993, IEEE T KNOWL DATA EN, V5, P619, DOI 10.1109/69.234774
   Duta N, 2002, PATTERN RECOGN LETT, V23, P477, DOI 10.1016/S0167-8655(01)00179-9
   Han CC, 2003, PATTERN RECOGN, V36, P371, DOI 10.1016/S0031-3203(02)00037-7
   Li WX, 2002, INT J PATTERN RECOGN, V16, P417, DOI 10.1142/S0218001402001757
   Peleg S, 1984, IEEE Trans Pattern Anal Mach Intell, V6, P518, DOI 10.1109/TPAMI.1984.4767557
   Shu W, 1998, OPT ENG, V37, P2359, DOI 10.1117/1.601756
   Shu W., 2001, INT J IMAGE GRAPH, V1, P135
   You J, 2002, PATTERN RECOGN, V35, P847, DOI 10.1016/S0031-3203(01)00100-5
   ZHANG D, IN PRESS PATT RECOGN
   Zhang D., 2000, AUTOMATED BIOMETRICS
   Zhang DP, 1999, PATTERN RECOGN, V32, P691, DOI 10.1016/S0031-3203(98)00117-4
   Zhu B, 2000, IEEE T IMAGE PROCESS, V9, P163, DOI 10.1109/83.817609
NR 14
TC 50
Z9 53
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2005
VL 7
IS 5
BP 891
EP 898
DI 10.1109/TMM.2005.854380
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 967IM
UT WOS:000232084900010
DA 2024-07-18
ER

PT J
AU Chien, SY
   Huang, YW
   Hsieh, BY
   Ma, SY
   Chen, LG
AF Chien, SY
   Huang, YW
   Hsieh, BY
   Ma, SY
   Chen, LG
TI Fast video segmentation algorithm with shadow cancellation, global
   motion compensation, and adaptive threshold techniques
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptive threshold; background registration; global motion compensation;
   MPEG-4 camera systems; object extraction; shadow cancellation; video
   segmentation
ID AUTOMATIC SEGMENTATION; MOVING-OBJECTS
AB Automatic video segmentation plays an important role in real-time MPEG-4 encoding systems. Several video segmentation algorithms have been proposed; however, most of them are not suitable for real-time applications because of high computation load and many parameters needed to be set in advance. This paper presents a fast video segmentation algorithm for MPEG-4 camera systems. With change detection and background registration techniques, this algorithm can give satisfying segmentation results with low computation load. The processing speed of 40 QCIF frames per second can be achieved on a personal computer with an 800 MHz Pentium-III processor. Besides, it has shadow cancellation mode, which can deal with light changing effect and shadow effect. A fast global motion compensation algorithm is also included in this algorithm to make it applicable in slight moving camera situations. Furthermore, the required parameters can be decided automatically, which can enhance the proposed algorithm to have adaptive threshold ability. It can be integrated into MPEG-4 videophone systems and digital cameras.
C1 Natl Taiwan Univ, DSPIC, Design Lab, Grad Inst Elect Engn, Taipei 106, Taiwan.
   Natl Taiwan Univ, Dept Elect Engn, Taipei 106, Taiwan.
   Vivotek Inc, Taipei 235, Taiwan.
C3 National Taiwan University; National Taiwan University
RP Natl Taiwan Univ, DSPIC, Design Lab, Grad Inst Elect Engn, Taipei 106, Taiwan.
EM shoayi@video.ee.ntu.edu.tw; yuwen@video.ee.ntu.edu.tw;
   bingyu@video.ee.ntu.edu.tw; syma@ieee.org; lgchen@video.ee.ntu.edu.tw
OI CHEN, LIANG-GEE/0000-0001-9746-9355; Chien, Shao-Yi/0000-0002-0634-6294;
   Huang, Yu-Wen/0000-0003-3045-1104
CR ADOLPH D, 1991, SIGNAL PROCESS IMAGE, V3
   BOOMGAARD RVD, 1992, CVGIP GRAPH MODELS I, V54
   Chien SY, 2000, P SOC PHOTO-OPT INS, V4067, P1087, DOI 10.1117/12.386607
   Foley J. D., 1994, Introduction to Computer Graphics", V55
   GUO J, 1998, P IEEE WORKSH MULT S
   GURCAN MN, 1999, P INT C IM PROC 1999, V3, P407
   Haralick R. M., 1992, COMPUTER ROBOT VISIO
   HBILI N, 2000, P VIS COMM IM PROC 2, P133
   HOETTER M, 1989, SIGNAL PROCESS, V16, P249, DOI 10.1016/0165-1684(89)90133-3
   HUANG YW, 2000, P 2000 AS PAC C MULT
   *ISO IEC JTC1 SC29, N4350 ISOIEC JTC1SC2
   *ISO IEC JTC1 SC29, 2001, N3908 ISOIEC JTC1SC2
   KIM DJ, 2000, P VIS COMM IM PROC 2, P1087
   Kim M, 1999, IEEE T CIRC SYST VID, V9, P1216, DOI 10.1109/76.809157
   Kompatsiaris I, 2000, IEEE T CIRC SYST VID, V10, P1388, DOI 10.1109/76.889030
   MA SY, 2000, P INT S INT SIGN PRO
   MECH R, 1998, SIGNAL POCESS, V66
   Meier T, 1998, IEEE T CIRC SYST VID, V8, P525, DOI 10.1109/76.718500
   Meier T, 1999, IEEE T CIRC SYST VID, V9, P1190, DOI 10.1109/76.809155
   MOSCHENI F, 1995, INT CONF ACOUST SPEE, P2261, DOI 10.1109/ICASSP.1995.479941
   Nicolas H, 2001, IEEE T IMAGE PROCESS, V10, P1239, DOI 10.1109/83.935039
   Peleg A, 1996, IEEE MICRO, V16, P42, DOI 10.1109/40.526924
   Serra J., 1983, IMAGE ANAL MATH MORP
   Sikora T, 1997, IEEE T CIRC SYST VID, V7, P19, DOI 10.1109/76.554415
   Smolic A, 1999, IEEE T CIRC SYST VID, V9, P1227, DOI 10.1109/76.809158
   Stauder J, 1999, IEEE T MULTIMEDIA, V1, P65, DOI 10.1109/6046.748172
   Wang DM, 1998, IEEE T CIRC SYST VID, V8, P539, DOI 10.1109/76.718501
   Wu S. F., 1990, Signal Processing: Image Communication, V2, P69, DOI 10.1016/0923-5965(90)90047-L
NR 28
TC 88
Z9 110
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2004
VL 6
IS 5
BP 732
EP 748
DI 10.1109/TMM.2004.834868
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 854XI
UT WOS:000223936800006
DA 2024-07-18
ER

PT J
AU Kashino, K
   Kurozumi, T
   Murase, H
AF Kashino, K
   Kurozumi, T
   Murase, H
TI A quick search method for audio and video signals based on histogram
   pruning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio fingerprinting; audio search; multimedia databases; multimedia
   information retrieval; video search
ID CONTENT-BASED RETRIEVAL; CLASSIFICATION
AB This paper proposes a quick method of similarity-based signal searching to detect and locate a specific audio or video signal given as a query in a stored long audio or video signal. With existing techniques, similarity-based searching may become impractical in terms of computing time in the case of searching through long-running (several-days' worth of) signals. The proposed algorithm, which is referred to as time-series active search, offers significantly faster search with sufficient accuracy. The key to the acceleration is an effective pruning algorithm introduced in the histogram matching stage. Through the pruning, the actual number of matching calculations can be reduced by 200 to 500 times compared with exhaustive search while guaranteeing exactly the same search result. Experiments show that the proposed method can correctly detect and locate a 15-s signal in a 48-h recording of TV broadcasts within 1 s, once the feature vectors are calculated and quantized. As extentions of the basic algorithm, efficient AND/OR search methods for searching for multiple query signals and a feature dithering method for coping with signal distortion are also discussed.
C1 NTT Corp, Commun Sci Labs, Atsugi, Kanagawa 2430198, Japan.
   Nagoya Univ, Grad Sch Informat Sci, Nagoya, Aichi 4648603, Japan.
C3 Nippon Telegraph & Telephone Corporation; Nagoya University
RP NTT Corp, Commun Sci Labs, Atsugi, Kanagawa 2430198, Japan.
EM kunio@eye.brl.ntt.co.jp; kurozumi@eye.brl.ntt.co.jp;
   murase@is.nagoya-u.ac.jp
CR [Anonymous], 2000, P 1 INT S MUS INF RE
   [Anonymous], 1967, P SPEECH COMMUN PROC
   BOYER RS, 1977, COMMUN ACM, V20, P762, DOI 10.1145/359842.359859
   Brunelli R, 2000, IEEE T MULTIMEDIA, V2, P164, DOI 10.1109/6046.865481
   Flickner M., 1995, IEEE COMPUT, V28, P23, DOI DOI 10.1109/2.410146
   Foote J, 1999, MULTIMEDIA SYST, V7, P2, DOI 10.1007/s005300050106
   Foote JT, 1997, P SOC PHOTO-OPT INS, V3229, P138, DOI 10.1117/12.290336
   Gong YH, 1996, MULTIMED TOOLS APPL, V2, P133, DOI 10.1007/BF00672252
   Hancock J., 1966, Signal Detection Theory
   Kashino K, 1999, INT CONF ACOUST SPEE, P2993, DOI 10.1109/ICASSP.1999.757470
   KEDEM B, 1986, P IEEE, V74, P1477, DOI 10.1109/PROC.1986.13663
   Knuth D. E., 1977, SIAM Journal on Computing, V6, P323, DOI 10.1137/0206024
   Ma WY, 1996, MULTIMED TOOLS APPL, V2, P35
   Pfeiffer S., 1996, Proceedings ACM Multimedia 96, P21, DOI 10.1145/244130.244139
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   Saunders J, 1996, INT CONF ACOUST SPEE, P993, DOI 10.1109/ICASSP.1996.543290
   Shneier M, 1996, IEEE T PATTERN ANAL, V18, P849, DOI 10.1109/34.531805
   Smith G, 1998, INT CONF ACOUST SPEE, P3777, DOI 10.1109/ICASSP.1998.679706
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   THONG JV, 2001, 200106 COMP CAMBR RE
   Vinod VV, 1997, PATTERN RECOGN, V30, P1787, DOI 10.1016/S0031-3203(96)00192-6
   WACTLAR HD, 2000, IM 2000
   Wold E, 1996, IEEE MULTIMEDIA, V3, P27, DOI 10.1109/93.556537
   WU JK, 1995, MULTIMEDIA SYST, V3, P25, DOI 10.1007/BF01236577
   Young SJ, 1997, INT CONF ACOUST SPEE, P199, DOI 10.1109/ICASSP.1997.599600
   Zhang T, 1998, P SOC PHOTO-OPT INS, V3527, P398, DOI 10.1117/12.325832
   Zhou Z, 2001, SCHOOL PSYCHOL INT, V22, P5, DOI 10.1177/01430343010221001
NR 27
TC 89
Z9 114
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2003
VL 5
IS 3
BP 348
EP 357
DI 10.1109/TMM.2003.813281
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 714BM
UT WOS:000184892500007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yensen, T
   Lariviere, JP
   Lambadaris, I
   Goubran, RA
AF Yensen, T
   Lariviere, JP
   Lambadaris, I
   Goubran, RA
TI HMM delay prediction technique for VoIP
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE communication systems; delay estimation; hidden Markov models; Internet;
   jitter; packet switching; playout buffering
AB This paper proposes a new algorithm for predicting audio packet playout delay for voice conferencing applications that use silence suppression. The proposed algorithm uses a hidden Markov model (HMM) to predict the playout delay. Several existing algorithms are reviewed to show that the HMM technique is based on a combination of various desirable features of other algorithms. Voice over Internet protocol (VolP) applications produce packets at a deterministic rate but various queuing delays are added to the packets by the network causing packet interarrival jitter. Playout delay prediction techniques schedule audio packets for playout and attempt to make a reasonable compromise between the number of lost packets, the one-way delay and the delay variation since these criteria cannot be optimized simultaneously. In particular, this paper will show that the proposed HMM technique makes a good compromise between the mean end-to-end delay, end-to-end delay standard deviation and average packet loss rate.
C1 Carleton Univ, Dept Syst & Comp Engn, Ottawa, ON K1S 5B6, Canada.
C3 Carleton University
RP Carleton Univ, Dept Syst & Comp Engn, Ottawa, ON K1S 5B6, Canada.
EM tyensen@sce.carleton; jpl@sce.carleton.ca; ioannis@sce.carleton.ca;
   goubran@sce.carleton
CR BRADY PT, 1969, AT&T TECH J, V48, P2445, DOI 10.1002/j.1538-7305.1969.tb01181.x
   BRADY PT, 1965, AT&T TECH J, V44, P1
   Chang CCK, 1999, ACM T INFORM SYST, V17, P1, DOI 10.1145/297117.297120
   *ITU T, 1998, H323 ITU T
   MATIC V, 2000, P MED EL C, V1, P348
   MINOLI D, 1998, DELIVERING VOICE OVE
   Moon SB, 1998, MULTIMEDIA SYST, V6, P17, DOI 10.1007/s005300050073
   Pinto J., 1999, Proceedings 24th Conference on Local Computer Networks. LCN'99, P224, DOI 10.1109/LCN.1999.802033
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   RAMJEE R, 1994, IEEE INFOCOM SER, P680, DOI 10.1109/INFCOM.1994.337672
   Rosenberg J., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P1705, DOI 10.1109/INFCOM.2000.832570
   Sreenan CJ, 2000, IEEE T MULTIMEDIA, V2, P88, DOI 10.1109/6046.845013
   YENSEN T, 1999, P IEEE INT S CIRC SY, V4, P348
   Yensen TN, 2001, IEEE T SPEECH AUDI P, V9, P168, DOI 10.1109/89.902283
NR 15
TC 13
Z9 17
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2003
VL 5
IS 3
BP 444
EP 457
DI 10.1109/TMM.2003.814793
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 714BM
UT WOS:000184892500015
DA 2024-07-18
ER

PT J
AU Shanableh, T
   Ghanbari, M
AF Shanableh, T
   Ghanbari, M
TI Loss concealment using B-pictures motion information
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE loss concealment; motion estimation; MPEG video; packet video
ID ERROR CONCEALMENT; VIDEO; RECOVERY; IMAGES; ALGORITHM; NETWORKS; DECODER
AB In this work, the motion parameters of the bidirectionally predicted pictures (B-pictures) of MPEG-1,2 are exploited for concealment of large-portions of corrupted anchor pictures (and vice versa) that might arise due to channel errors or packet losses. To further enhance the quality of the concealed pictures, we propose two methods of constraining the motion vectors of the B-pictures that strengthen the tie between them and those of the anchor pictures in the same picture subgroup. In one method, the macroblock decisions on the last B-picture in each subgroup is constrained to be bidirectional if those of the other B-pictures are not, such that the derived motion vectors for the concealment of the anchor picture are always composed from the forward and backward motion vectors of the bidirectional motions. Second, the bidirectional motion vectors of the B-pictures in each subgroup is constrained such that the vectorial sum of their forward and backward motion vectors results in an accurate motion prediction of the anchor picture. The experimental results show that while the composed motion vectors improve the quality of concealment over the conventional methods by more than 3-4 dB, another 2 dB improvement can be achieved by constraining the generation of the bidirectional motion vectors.
C1 Univ Essex, Dept Elect Syst Engn, Colchester CO4 3SQ, Essex, England.
C3 University of Essex
RP Amer Univ Sharjah, Dept Comp Sci, Sharjah, U Arab Emirates.
EM ghan@essex.ac.uk
RI Ghanbari, Mohammad/L-4053-2019; Shanableh, Tamer/AAC-7893-2021
OI Ghanbari, Mohammad/0000-0002-5482-8378; Shanableh,
   Tamer/0000-0002-7651-3094
CR [Anonymous], COMMUNICATION
   BYSTROM M, 1999, IEEE T CIRCUITS SYST, V9
   Carle G, 1997, IEEE NETWORK, V11, P24, DOI 10.1109/65.642357
   Chu WJ, 1998, IEEE T CIRC SYST VID, V8, P74, DOI 10.1109/76.660830
   GHANBARI M, 1989, IEEE J SEL AREA COMM, V7, P771, DOI 10.1109/49.32340
   Ghanbari M, 1993, IEEE T CIRC SYST VID, V3, P238, DOI 10.1109/76.224234
   HEMAMI SS, 1995, IEEE T IMAGE PROCESS, V4, P1023, DOI 10.1109/83.392344
   *ITU T REC H 263, 1998, VID COD LOW BIT RAT
   KWOK W, 1993, IEEE T CONSUM ELECTR, V39, P455, DOI 10.1109/30.234620
   LAM WM, 1995, IEEE T IMAGE PROCESS, V4, P533, DOI 10.1109/83.382489
   LAM WM, 1993, P INT C AC SPEECH SI, pV417
   LEE SH, 1993, P SOC PHOTO-OPT INS, V2094, P195, DOI 10.1117/12.157937
   LEE VHL, 1995, DRUG TARG D, V4, P3
   Lim CP, 1999, INT J IMAG SYST TECH, V10, P54, DOI 10.1002/(SICI)1098-1098(1999)10:1<54::AID-IMA6>3.0.CO;2-H
   MCCANNE S, 1996, P ACM SIGCOMM 96 STA, P117
   SHANABLEH T, 2000, P INT PACK VID WORKS
   SHIM C, 1994, IEEE J SEL AREA COMM, V12, P332, DOI 10.1109/49.272884
   SUN H, 1992, IEEE T CONSUM ELECTR, V38, P108, DOI 10.1109/30.156671
   SUN H, 1992, SPIE, V1818, P814
   SUN HF, 1995, IEEE T IMAGE PROCESS, V4, P470, DOI 10.1109/83.370675
   WADA M, 1989, IEEE J SEL AREA COMM, V7, P807, DOI 10.1109/49.32344
   WANG Y, 1993, IEEE T COMMUN, V41, P1544, DOI 10.1109/26.237889
   Zhu QF, 1993, IEEE T CIRC SYST VID, V3, P248, DOI 10.1109/76.224235
   1992, INFORMATION TECH NOV
   1996, INFORMATION TECH JUL
NR 25
TC 10
Z9 10
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2003
VL 5
IS 2
BP 257
EP 266
DI 10.1109/TMM.2003.811624
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 695HB
UT WOS:000183824100010
DA 2024-07-18
ER

PT J
AU Yeh, CH
   Kuo, CJ
AF Yeh, CH
   Kuo, CJ
TI Iteration-free clustering algorithm for nonstationary image database
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE CBIR; database updating; indexing structure; MPEG-7; nonstationary image
   database
ID RETRIEVAL; MPEG-7
AB Image database systems must effectively and efficiently handle and retrieve images from a large collection of images. A serious problem faced by these systems is the requirement to deal with the nonstationary database. In an image database system, image features ate typically organized into an indexing structure, and updating the indexing structure involves many computations. Here, this difficult problem is converted into a constrained optimization problem, and the iteration-free clustering (IFC) algorithm based on the Lagrangian function, is presented for adapting the existing indexing structure for a nonstationary database. Experimental results concerning recall and precision indicate that the proposed method provides a binary tree that is almost optimal. Simulation results further demonstrate that the proposed algorithm can maintain 94% precision in seven-dimensional feature space, even when the number of new-coming images is one-half the number of images in the original database. Finally, our IFC algorithm outperforms other methods usually applied to image databases.
C1 Natl Chung Cheng Univ, Dept Elect Engn, SAM Lab, Chiayi 62107, Taiwan.
   Delta Elect Inc, Taoyuan 333, Taiwan.
C3 National Chung Cheng University; Delta Electronics
RP Yeh, CH (corresponding author), Natl Chung Cheng Univ, Dept Elect Engn, SAM Lab, Chiayi 62107, Taiwan.
EM chyeh@sipi.usc.edu; chung.kuo@delta.com.tw
RI Kuo, Chung-Jen/HTO-0059-2023
CR Anderberg M.R., 1973, Probability and Mathematical Statistics
   [Anonymous], 1981, Practical Optimization
   [Anonymous], 1990, SIGMOD, DOI DOI 10.1145/93597.98741
   BENNETT K, 1999, INT C KDD 1999
   Berchtold S, 1998, PROC INT CONF DATA, P209, DOI 10.1109/ICDE.1998.655779
   Berman AP, 1999, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES (CBAIVL'99) - PROCEEDINGS, P55, DOI 10.1109/IVL.1999.781124
   Berman AP, 1999, COMPUT VIS IMAGE UND, V75, P175, DOI 10.1006/cviu.1999.0772
   BHANU B, 1998, IEEE INT C IM PROC, V2, P789
   BIMBO AD, 1999, VISUAL INFORMATION R
   BREITENEDER C, 2001, IEEE INT C DIG LIB R, P288
   CAN F, 1993, ACM T INFORM SYST, V11, P143, DOI 10.1145/130226.134466
   Can F., 1987, Proceedings of the Tenth Annual International ACMSIGIR Conference on Research and Development in Information Retrieval, P123, DOI 10.1145/42005.42019
   Chakrabarti K, 1999, PROC INT CONF DATA, P440, DOI 10.1109/ICDE.1999.754960
   CHARIKAR M, 1997, P 29 S THEOR COMP 19
   CHEN JY, 1997, IEEE INT C IM PROC, V2, P827
   Doermann D, 1998, COMPUT VIS IMAGE UND, V70, P287, DOI 10.1006/cviu.1998.0692
   Duda R., 1973, Pattern Classification and Scene Analysis
   Ferro A, 2001, IEEE T PATTERN ANAL, V23, P707, DOI 10.1109/34.935845
   FICHNER M, 1995, IEEE COMPUT, V289, P23
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Fu AW, 2000, VLDB J, V9, P154, DOI 10.1007/PL00010672
   GUDIVADA VN, 1995, COMPUTER, V28, P18, DOI 10.1109/2.410145
   Guttman A., 1984, SIGMOD Record, V14, P47, DOI 10.1145/971697.602266
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692
   HUANG Z, 1997, WORKSH RES ISSUES DA
   KING I, 1997, INT S MULT INF PROC, P215
   KING I, 1997, INT C NEUR INF PROC, P906
   KUAN J, 1997, IEEE T MED IMAGING, V20, P868
   LAU TK, 1998, IEEE INT JOINT C COM, V2, P932
   Looney CarlGrant., 1997, Pattern recognition using neural networks: theory and algorithms for engineers and scientists
   Lu G., 1998, International Conference on Computational Intelligence and Multimedia Applications 1998. ICCIMA 1998, P781
   MACQUEEN J, 1967, 5 BERK S PROB STAT 1
   Nack F, 1999, IEEE MULTIMEDIA, V6, P64, DOI 10.1109/93.809235
   Nack F, 1999, IEEE MULTIMEDIA, V6, P65, DOI 10.1109/93.790612
   Nepal S, 1999, PROC INT CONF DATA, P22, DOI 10.1109/ICDE.1999.754894
   PUNPITI P, 1999, IEEE INT C IM PROC, V1, P129
   SALTON G, 1983, INTRO MODERN INFORMA
   SAMET H, 1984, COMPUT SURV, V16, P187, DOI 10.1145/356924.356930
   SHEIKHOLESLAMI G, 1998, ACM MULT C, P3
   SITZMANN II, 2000, P 11 AUSTR DAT C 200, P127
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Smith JR, 1997, IEEE MULTIMEDIA, V4, P12, DOI 10.1109/93.621578
   Stehling RO, 2001, 2001 INTERNATIONAL DATABASE ENGINEERING & APPLICATIONS SYMPOSIUM, PROCEEDINGS, P356, DOI 10.1109/IDEAS.2001.938104
   Strang G., 1986, Introduction to Applied Mathematics
   Syeda-Mahmood TF, 2000, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES, PROCEEDINGS, P78, DOI 10.1109/IVL.2000.853844
   WOOD MEJ, 1997, P 1997 BRIT MACH VIS, P620
   YONG R, 1998, IEEE INT C AC SPEECH, V6, P3785
   YU D, 2000, IEEE INT C MULT EXP, V3, P1713
NR 48
TC 4
Z9 4
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2003
VL 5
IS 2
BP 223
EP 236
DI 10.1109/TMM.2003.811619
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 695HB
UT WOS:000183824100007
DA 2024-07-18
ER

PT J
AU Chen, HW
   Chai, XL
   Shao, F
   Wang, XJ
   Jiang, QP
   Meng, XC
   Ho, YS
AF Chen, Hangwei
   Chai, Xiongli
   Shao, Feng
   Wang, Xuejin
   Jiang, Qiuping
   Meng, Xiangchao
   Ho, Yo-Sung
TI Perceptual Quality Assessment of Cartoon Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image color analysis; Distortion; Measurement; Image coding; Quality
   assessment; Image quality; Feature extraction; Cartoon images; color
   change; no-reference image quality assessment; structural measure; color
   measure
AB In the animation industry, automatically predicting the quality of cartoon images based on the inputs of general distortions and color change is an urgent task, while the existing no-reference (NR) methods usually measure the perceptual quality of the natural images. In this paper, based on the observation that structure and color are the main factors affecting cartoon images quality, we proposed a new NR quality prediction metric for cartoon images, which fully takes gradient and color information into account. The experimental results on our newly constructed NBU-CIQAD dataset with color change and other existing cartoon image dataset demonstrate that the proposed method significantly outperforms existing no-references methods for the task of cartoon image quality assessment.
C1 [Chen, Hangwei; Chai, Xiongli; Shao, Feng; Jiang, Qiuping; Meng, Xiangchao] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Wang, Xuejin] Fujian Univ Technol, Sch Comp Sci & Math, Fuzhou 350118, Peoples R China.
   [Ho, Yo-Sung] Gwangju Inst Sci & Technol GIST, Sch Informat & Commun, Gwangju 500712, South Korea.
C3 Ningbo University; Fujian University of Technology; Gwangju Institute of
   Science & Technology (GIST)
RP Shao, F (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
EM 1010075746@qq.com; 747866472@qq.com; shaofeng@nbu.edu.cn;
   1020468620@qq.com; jiangqiuping@nbu.edu.cn; mengxiangchao@nbu.edu.cn;
   hoyo@gist.ac.kr
RI Qiuping, Jiang/AAO-2830-2021
OI Qiuping, Jiang/0000-0002-6025-9343; HO, YO-SUNG/0000-0002-7220-1034;
   Chai, Xiongli/0000-0002-4245-5391; Chen, Hangwei/0000-0002-3756-2029
FU Natural Science Foundation of China [62071261, 61901236, 41801252,
   61622109, R18F010008]; K. C. Wong Magna Fund in Ningbo University
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 62071261, 61901236, 41801252, and 61622109, and in
   part by the Natural Science Foundation of China under Grant R18F010008.
   It was also sponsored by K. C. Wong Magna Fund in Ningbo University.
CR Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen Y, 2020, IEEE T CIRC SYST VID, V30, P3282, DOI 10.1109/TCSVT.2019.2931589
   Fang YM, 2020, IEEE T CIRC SYST VID, V30, P4050, DOI 10.1109/TCSVT.2019.2951747
   Fang YM, 2020, IEEE T IMAGE PROCESS, V29, P1127, DOI 10.1109/TIP.2019.2940678
   Fang YM, 2018, IEEE T IMAGE PROCESS, V27, P1600, DOI 10.1109/TIP.2017.2781307
   Gu K, 2018, IEEE T VIS COMPUT GR, V24, P2689, DOI 10.1109/TVCG.2017.2771284
   Gu K, 2017, IEEE T IMAGE PROCESS, V26, P4005, DOI 10.1109/TIP.2017.2711279
   Gu K, 2018, IEEE T NEUR NET LEAR, V29, P1301, DOI 10.1109/TNNLS.2017.2649101
   Gu K, 2017, IEEE T CYBERNETICS, V47, P4559, DOI 10.1109/TCYB.2016.2575544
   Gu K, 2016, NEUROCOMPUTING, V196, P140, DOI 10.1016/j.neucom.2015.11.101
   Gu K, 2016, IEEE T CYBERNETICS, V46, P284, DOI 10.1109/TCYB.2015.2401732
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Gu K, 2014, IEEE T BROADCAST, V60, P555, DOI 10.1109/TBC.2014.2344471
   Hancheng Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14131, DOI 10.1109/CVPR42600.2020.01415
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Jiang QP, 2021, IEEE T IND INFORM, V17, P6062, DOI 10.1109/TII.2020.3035448
   Li LD, 2014, IEEE SIGNAL PROC LET, V21, P122, DOI 10.1109/LSP.2013.2294333
   Li QH, 2016, IEEE SIGNAL PROC LET, V23, P541, DOI 10.1109/LSP.2016.2537321
   Li T, 2021, DISPLAYS, V69, DOI 10.1016/j.displa.2021.102030
   Li T, 2021, IEEE T BROADCAST, V67, P438, DOI 10.1109/TBC.2020.3028335
   Liang LH, 2010, SIGNAL PROCESS-IMAGE, V25, P502, DOI 10.1016/j.image.2010.01.007
   Liu LX, 2016, SIGNAL PROCESS-IMAGE, V40, P1, DOI 10.1016/j.image.2015.10.005
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Lyu WJ, 2020, J VIS COMMUN IMAGE R, V69, DOI 10.1016/j.jvcir.2020.102797
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Ma L, 2011, IEEE T MULTIMEDIA, V13, P824, DOI 10.1109/TMM.2011.2109701
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P6054, DOI 10.1109/TIP.2020.2988148
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Min XK, 2018, SIGNAL PROCESS, V145, P127, DOI 10.1016/j.sigpro.2017.10.025
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Ni ZK, 2017, IEEE T IMAGE PROCESS, V26, P4818, DOI 10.1109/TIP.2017.2718185
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Panetta K, 2014, INT J BIOMED IMAGING, V2014, DOI 10.1155/2014/937849
   Panetta KA, 2008, IEEE T SYST MAN CY B, V38, P174, DOI 10.1109/TSMCB.2007.909440
   Shao F, 2018, IEEE T SYST MAN CY-S, V48, P1521, DOI 10.1109/TSMC.2017.2676180
   Sheikh H. R., IMAGE VIDEO QUALITY
   Sheikh H. R., 2006, Image and video quality assessment research at LIVE
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shi LK, 2020, IEEE T CIRC SYST VID, V30, P4114, DOI 10.1109/TCSVT.2019.2955011
   Sobel I.E., 1970, Camera models and machine perception
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Tian SS, 2018, IEEE T IMAGE PROCESS, V27, P1652, DOI 10.1109/TIP.2017.2781420
   VANDIJK AM, 1995, P SOC PHOTO-OPT INS, V2451, P90, DOI 10.1117/12.201231
   Wang XJ, 2021, IEEE T MULTIMEDIA, V23, P1173, DOI 10.1109/TMM.2020.2993942
   Wang XJ, 2021, IEEE T MULTIMEDIA, V23, P692, DOI 10.1109/TMM.2020.2986583
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu QB, 2016, IEEE T CIRC SYST VID, V26, P425, DOI 10.1109/TCSVT.2015.2412773
   Xiaoyin Duanmu, 2010, Proceedings of the Seventh International Conference on Information Technology: New Generations (ITNG 2010), P200, DOI 10.1109/ITNG.2010.231
   [徐琳 Xu Lin], 2015, [中国图象图形学报, Journal of Image and Graphics], V20, P1583
   Xu M, 2022, IEEE T PATTERN ANAL, V44, P2198, DOI 10.1109/TPAMI.2020.3028509
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhan YB, 2017, IEEE SIGNAL PROC LET, V24, P760, DOI 10.1109/LSP.2017.2688371
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang WX, 2021, IEEE T IMAGE PROCESS, V30, P3474, DOI 10.1109/TIP.2021.3061932
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
NR 61
TC 16
Z9 16
U1 1
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 140
EP 153
DI 10.1109/TMM.2021.3121875
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9E8KM
UT WOS:000937028400010
DA 2024-07-18
ER

PT J
AU Chen, J
   Duan, H
   Song, YX
   Cai, ZM
   Yang, GG
AF Chen, Jun
   Duan, Hui
   Song, Yuanxin
   Cai, Zemin
   Yang, Guangguang
TI Optical Flow Computation for Video Under the Dynamic Illumination
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Optical flow; illumination-invariant framework; L-0 norm regularization;
   edge-preserving regularizers
ID MOTION ESTIMATION; CNN
AB Optical flow computation for video under the dynamic illumination is a challenging issue in video multimedia applications. In this paper, we solve this issue by introducing an illumination-invariant framework for variational optical flow estimation. It consists of an illumination-invariance model that handles complex illumination changes and a data enhancement model that guarantees highly accurate optical flow estimation. In this framework, we design a log-correlation descriptor for the data term, which handles complex illumination changes by eliminating the common parameters shared by the neighboring pixels in the corresponding illumination change model while improving the accuracy of optical flow estimation by enhancing the discriminability of the data term matching. We also introduce a novel optical flow model with L0 norm regularization, which reconstructs optical flow field by a sparse flow gradient counting scheme. Different from other edge-preserving regularizers, it does not depend on local motion features, but locates important flow edges globally. Therefore, it will not cause edge blurriness due to avoiding local filtering or average operation. It is particularly effective for enhancing major flow edges while eliminating a manageable degree of low-amplitude motion structures to control smoothing and reduce oversegmentation artifacts. Even small-scale motion structures with high contrast can be preserved remarkably well. The experimental results show our method significantly outperforms previous illumination-robust optical flow methods in handling complex illumination changes, and achieves competitive evaluation results on the challengingMPI-Sintel and Kitti datasets.
C1 [Chen, Jun] Foshan Univ, Sch Ind Design & Ceram Art, Foshan 528000, Peoples R China.
   [Duan, Hui; Song, Yuanxin] Foshan Univ, Sch Mechatron Engn & Automat, Foshan 528000, Peoples R China.
   [Cai, Zemin] Shantou Univ, Dept Elect Engn, Shantou 515063, Peoples R China.
   [Cai, Zemin] Guangdong Prov Key Lab Digital Signal & Image Pro, Shantou 515063, Peoples R China.
   [Yang, Guangguang] Univ Portsmouth, Sch Comp, Portsmouth PO1 3HE, Hants, England.
C3 Foshan University; Foshan University; Shantou University; University of
   Portsmouth
RP Cai, ZM (corresponding author), Shantou Univ, Dept Elect Engn, Shantou 515063, Peoples R China.
EM chenj269@mail2.sysu.edu.cn; 3396249728@qq.com; 1531697411@qq.com;
   zmcai@stu.edu.cn; 240991183@qq.com
RI Chen, Jun/HHR-9485-2022
OI Chen, Jun/0000-0001-9693-6628; Cai, Zemin/0000-0002-7557-9526
FU National Natural Science Foundation of China [62002061, 61876104];
   Guangdong Natural Science Foundation of China [2019A1515111208,
   2021A1515011504]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62002061 and 61876104, and in part by
   the Guangdong Natural Science Foundation of China under Grants
   2019A1515111208 and 2021A1515011504.
CR Ali S, 2016, COMPUT VIS IMAGE UND, V145, P95, DOI 10.1016/j.cviu.2015.12.003
   Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2
   Bar-Haim Aviram, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7995, DOI 10.1109/CVPR42600.2020.00802
   Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143
   Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44
   Cai SZ, 2020, IEEE T INSTRUM MEAS, V69, P3538, DOI 10.1109/TIM.2019.2932649
   Chen J, 2021, 2021 IEEE 7TH INTERNATIONAL CONFERENCE ON VIRTUAL REALITY (ICVR 2021), P127, DOI 10.1109/ICVR51878.2021.9483834
   Chen J, 2018, IEEE T CIRC SYST VID, V28, P664, DOI 10.1109/TCSVT.2016.2615324
   Demetz O, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.50
   Demetz O, 2014, LECT NOTES COMPUT SC, V8689, P455, DOI 10.1007/978-3-319-10590-1_30
   Trinh DH, 2019, COMPUT VIS IMAGE UND, V179, P1, DOI 10.1016/j.cviu.2018.11.004
   Trinh DH, 2017, IEEE IMAGE PROC, P2533, DOI 10.1109/ICIP.2017.8296739
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Drulea M, 2013, IEEE T IMAGE PROCESS, V22, P3260, DOI 10.1109/TIP.2013.2263149
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hu P, 2018, IEEE T MULTIMEDIA, V20, P2814, DOI 10.1109/TMM.2018.2815784
   Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jabid T, 2010, IEEE ICCE
   Jiang SA, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9752, DOI 10.1109/ICCV48922.2021.00963
   Kim YH, 2005, IMAGE VISION COMPUT, V23, P365, DOI 10.1016/j.imavis.2004.05.010
   Kumar A, 2013, IEEE T IMAGE PROCESS, V22, P4136, DOI 10.1109/TIP.2013.2270374
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu L, 2020, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR42600.2020.00652
   Liu PP, 2019, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2019.00470
   Mei L, 2020, IEEE T CIRC SYST VID, V30, P495, DOI 10.1109/TCSVT.2019.2890861
   Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925
   Mileva Y, 2007, LECT NOTES COMPUT SC, V4713, P152
   Mohamed MA, 2014, IEEE T CIRC SYST VID, V24, P1499, DOI 10.1109/TCSVT.2014.2308628
   Molnár J, 2010, COMPUT VIS IMAGE UND, V114, P1104, DOI 10.1016/j.cviu.2010.07.006
   Muller Thomas, 2011, Pattern Recognition. Proceedings 33rd DAGM Symposium, P236, DOI 10.1007/978-3-642-23123-0_24
   Ranftl R, 2014, LECT NOTES COMPUT SC, V8689, P439, DOI 10.1007/978-3-319-10590-1_29
   Rashwan HA, 2013, LECT NOTES COMPUT SC, V8142, P354, DOI 10.1007/978-3-642-40602-7_38
   Rivera AR, 2013, IEEE T IMAGE PROCESS, V22, P1740, DOI 10.1109/TIP.2012.2235848
   Stein F, 2004, LECT NOTES COMPUT SC, V3175, P79
   Stone A, 2021, PROC CVPR IEEE, P3886, DOI 10.1109/CVPR46437.2021.00388
   Sun DQ, 2021, PROC CVPR IEEE, P10088, DOI 10.1109/CVPR46437.2021.00996
   Sun DQ, 2020, IEEE T PATTERN ANAL, V42, P1408, DOI 10.1109/TPAMI.2019.2894353
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Tak-Wai Hui, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P169, DOI 10.1007/978-3-030-58565-5_11
   Teed Zachary, 2020, EUR C COMP VIS, P402, DOI [DOI 10.1007/978-3-030-58536-524, DOI 10.1007/978-3-030-58536-5_24]
   Toth D., 2000, 4th IEEE Southwest Symposium on Image Analysis and Interpretation, P3, DOI 10.1109/IAI.2000.839561
   Tu ZG, 2019, SIGNAL PROCESS-IMAGE, V72, P9, DOI 10.1016/j.image.2018.12.002
   Wedel A, 2009, LECT NOTES COMPUT SC, V5604, P23, DOI 10.1007/978-3-642-03061-1_2
   Werlberger M, 2010, PROC CVPR IEEE, P2464, DOI 10.1109/CVPR.2010.5539945
   Xu H., 2021, IEEECVF, P10498
   Xu L, 2012, IEEE T PATTERN ANAL, V34, P1744, DOI 10.1109/TPAMI.2011.236
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zhang CX, 2021, IEEE T MULTIMEDIA, V24, P3340, DOI 10.1109/TMM.2021.3096083
   Zhang CX, 2020, IEEE T MULTIMEDIA, V22, P349, DOI 10.1109/TMM.2019.2929934
   Zhang CX, 2017, IEEE T IMAGE PROCESS, V26, P4055, DOI 10.1109/TIP.2017.2712279
   Zhang Q, 2014, PROC CVPR IEEE, P2830, DOI 10.1109/CVPR.2014.362
   Zhao SY, 2020, PROC CVPR IEEE, P6277, DOI 10.1109/CVPR42600.2020.00631
   Zheng YQ, 2020, PROC CVPR IEEE, P6748, DOI 10.1109/CVPR42600.2020.00678
   Zimmer H, 2011, INT J COMPUT VISION, V93, P368, DOI 10.1007/s11263-011-0422-6
NR 57
TC 1
Z9 1
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6285
EP 6300
DI 10.1109/TMM.2022.3207583
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500046
DA 2024-07-18
ER

PT J
AU Feng, YB
   Gao, JY
   Xu, CS
AF Feng, Yangbo
   Gao, Junyu
   Xu, Changsheng
TI Learning Dual-Routing Capsule Graph Neural Network for Few-Shot Video
   Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Routing; Graph neural networks; Computational complexity; Heuristic
   algorithms; Feature extraction; Task analysis; Semantics; Capsule
   network; few-shot learning; graph neural network; video classification
AB Few-shot video classification (video FSL), which learns classifiers for novel concepts, has gained increasing attention in the last few years from only a few samples. The existing methods rarely consider the local-global relation for video feature learning, which would ultimately result in low discriminative ability. Recently, the capsule network (CapsNet) has shown considerable potential in local-global relation learning in the image analysis field. However, CapsNet cannot be directly applied in video FSL since it ignores the interaction between videos and has high computational complexity. In this paper, a dual-routing capsule graph neural network (DR-CapsGNN) is proposed to solve the above issues. The DR-CapsGNN leverages CapsNet and a graph neural network (GNN) to explore local-global relations and to preserve the detailed properties. Specifically, the CapsGNN is used to learn video relations and structural information to generate high-quality hierarchical capsules. Furthermore, a novel dual-routing mechanism is designed to filter low-discriminative capsules from a holistic perspective and achieves high efficiency, which consists of inter-video and intra-video routing. Extensive experimental results demonstrate that our proposed approach performs favorably compared to state-of-the-art methods on two popular benchmarks.
C1 [Feng, Yangbo] Tianjin Univ Technol, Tianjin 300382, Peoples R China.
   [Gao, Junyu; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Gao, Junyu; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 101408, Peoples R China.
   [Xu, Changsheng] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
C3 Tianjin University of Technology; Chinese Academy of Sciences; Institute
   of Automation, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; Peng Cheng Laboratory
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Xu, CS (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 101408, Peoples R China.
EM ybfeng6@gmail.com; gaojunyu2015@ia.ac.cn; csxu@nlpr.ia.ac.cn
RI Gao, Junyu/HDO-5516-2022; xu, cj/HJZ-3488-2023
OI xu, chang sheng/0000-0001-8343-9665; Gao, Junyu/0000-0002-8105-5497
FU National Key Research amp; Development Plan of China [2020AAA0106200];
   National Natural Science Foundation of China [62036012, 61721004,
   62102415, 62072286, 61720106006, 61832002, 62072455, 62002355, U1836220,
   U1705262]; Key Research Program of Frontier Sciences of CAS
   [QYZD-JSSWJSC039]; Beijing Natural Science Foundation [L201001]; Open
   Research Projects of Zhejiang Lab [2022RC0AB02]; CCF-Hik vision Open
   Fund
FX This work was supported in part by the National Key Research &
   Development Plan of China under Grant 2020AAA0106200, in part by the
   National Natural Science Foundation of China under Grants 62036012,
   61721004, 62102415, 62072286, 61720106006, 61832002, 62072455, 62002355,
   U1836220, and U1705262, in part by the Key Research Program of Frontier
   Sciences of CAS under Grant QYZD-JSSWJSC039, in part by Beijing Natural
   Science Foundation under Grant L201001, in part by the Open Research
   Projects of Zhejiang Lab under Grant 2022RC0AB02, and in part by CCF-Hik
   vision Open Fund. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Dr. Xavier
   Alameda-Pineda.(Corresponding author: Changsheng Xu.)
CR Allen KR, 2019, PR MACH LEARN RES, V97
   Atwood J, 2016, NIPS, P2001
   Bishay M., 2019, ARXIV190709021
   Cappallo S, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P28, DOI 10.1145/3123266.3123437
   Defferrard M, 2016, ADV NEUR IN, V29
   Dong JF, 2022, IEEE T PATTERN ANAL, V44, P4065, DOI 10.1109/TPAMI.2021.3059295
   Duvenaudt D, 2015, ADV NEUR IN, V28
   Edraki M, 2020, AAAI CONF ARTIF INTE, V34, P10745
   Fei N., 2020, P INT C LEARN REPR, P1
   Finn C, 2017, PR MACH LEARN RES, V70
   Fu YQ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1142, DOI 10.1145/3394171.3413502
   Fu YQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P411, DOI 10.1145/3343031.3351015
   Gao J., 2021, P IEEECVF INT C COMP, P1523
   Gao JY, 2021, IEEE T MULTIMEDIA, V23, P3203, DOI 10.1109/TMM.2020.3021980
   Gao JY, 2021, IEEE T PATTERN ANAL, V43, P3476, DOI 10.1109/TPAMI.2020.2985708
   Gao JY, 2020, IEEE T MULTIMEDIA, V22, P3088, DOI 10.1109/TMM.2020.2969787
   Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478
   Gao JY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P690, DOI 10.1145/3240508.3240566
   Gao JY, 2019, AAAI CONF ARTIF INTE, P8303
   Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459
   Gori M, 2005, IEEE IJCNN, P729
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   Gu J., 2020, P AAAI C ART INT, P1
   Guo YR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P103, DOI 10.1109/ICCV48922.2021.00017
   Hahn T., 2019, Advances in neural information processing systems, V32, P7658
   Hamilton WL, 2017, ADV NEUR IN, V30
   Han-Jia Ye, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8805, DOI 10.1109/CVPR42600.2020.00883
   Henaff M, 2015, Arxiv, DOI arXiv:1506.05163
   Hinton G.E., 2018, P INT C LEARN REPR I
   Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6
   Hou YT, 2021, AAAI CONF ARTIF INTE, V35, P13036
   Hu YF, 2021, IEEE T MULTIMEDIA, V23, P4285, DOI 10.1109/TMM.2020.3039329
   Huang HX, 2021, IEEE T MULTIMEDIA, V23, P1666, DOI 10.1109/TMM.2020.3001510
   Kaidi Cao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10615, DOI 10.1109/CVPR42600.2020.01063
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   LaLonde R, 2018, Arxiv, DOI [arXiv:1804.04241, DOI 10.48550/ARXIV.1804.04241]
   Li ZG, 2017, Arxiv, DOI arXiv:1707.09835
   Liu Y, 2019, IEEE I CONF COMP VIS, P1232, DOI 10.1109/ICCV.2019.00132
   Ma D, 2021, PROC CVPR IEEE, P10943, DOI 10.1109/CVPR46437.2021.01080
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Morris C, 2019, AAAI CONF ARTIF INTE, P4602
   Niepert M, 2016, PR MACH LEARN RES, V48
   Perrett T, 2021, PROC CVPR IEEE, P475, DOI 10.1109/CVPR46437.2021.00054
   Qi MS, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3007, DOI 10.1145/3394171.3416269
   Rajasegaran J, 2019, PROC CVPR IEEE, P10717, DOI 10.1109/CVPR.2019.01098
   Ribeiro FD, 2020, AAAI CONF ARTIF INTE, V34, P3749
   Sabour S, 2017, ADV NEUR IN, V30
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Snell J, 2017, ADV NEUR IN, V30
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tsai Y.-H. H., 2020, P INT C LEARN REPR, P1
   Urooj A., 2021, P IEEE CVF C COMP VI, P8465
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang W, 2021, IEEE T MULTIMEDIA, V23, P2386, DOI 10.1109/TMM.2020.3011288
   Wu ZY, 2019, IEEE I CONF COMP VIS, P6658, DOI 10.1109/ICCV.2019.00676
   Xinyi Z., 2018, INT C LEARNING REPRE
   Xu ZW, 2017, PROC CVPR IEEE, P5358, DOI 10.1109/CVPR.2017.569
   Yang X, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1, DOI 10.1145/3404835.3462823
   Yang X, 2022, IEEE T IMAGE PROCESS, V31, P1204, DOI 10.1109/TIP.2022.3140611
   Yang X, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1339, DOI 10.1145/3397271.3401151
   Zaremba W., 2013, PROC INT C LEARN REP
   Zhang CR, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1641, DOI 10.1145/3343031.3351000
   Zhang DW, 2022, IEEE T PATTERN ANAL, V44, P3349, DOI 10.1109/TPAMI.2020.3046647
   Zhang H., 2020, P EUR C COMP VIS, P525
   Zhang LH, 2018, ADV NEUR IN, V31
   Zhang ZY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P257, DOI 10.1145/3240508.3240534
   Zhao A, 2021, IEEE WINT CONF APPL, P1389, DOI 10.1109/WACV48630.2021.00143
   Zheng YH, 2019, IEEE T MULTIMEDIA, V21, P2292, DOI 10.1109/TMM.2019.2900166
   Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49
   Zhou YY, 2019, AAAI CONF ARTIF INTE, P9324
   Zhu K, 2021, IEEE T MULTIMEDIA, V23, P3726, DOI 10.1109/TMM.2020.3031062
   Zhu LC, 2018, LECT NOTES COMPUT SC, V11211, P782, DOI 10.1007/978-3-030-01234-2_46
   Zhu YH, 2021, IEEE T MULTIMEDIA, V23, P1200, DOI 10.1109/TMM.2020.2993952
   Zhu Y, 2018, PROC CVPR IEEE, P9436, DOI 10.1109/CVPR.2018.00983
NR 75
TC 9
Z9 9
U1 1
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3204
EP 3216
DI 10.1109/TMM.2022.3156938
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200020
DA 2024-07-18
ER

PT J
AU Hou, JW
   Lin, WS
   Yue, GH
   Liu, WD
   Zhao, BQ
AF Hou, Jingwen
   Lin, Weisi
   Yue, Guanghui
   Liu, Weide
   Zhao, Baoquan
TI Interaction-Matrix Based Personalized Image Aesthetics Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; image aesthetics assessment
AB Personalized image aesthetics assessment (IAA) aims to estimate aesthetic experiences subject to the preferences of individual users, contrary to generic IAA that estimates aesthetic experiences subject to average preferences. Most existing personalized IAA methods treat personalized aesthetic experiences as deviations from a generic aesthetic experience, and therefore, personalized IAA models are designed to build upon the prior knowledge on generic IAA. However, we propose that acquiring knowledge on generic IAA is not necessary for building a personalized IAA model. Instead of modeling personalized IAA on the basis of generic IAA, this work proposes to directly estimate personalized aesthetic experiences from the interactions between image contents and user preferences (i.e., preference-content interaction), where interaction-matrices representing preference-content interactions are constructed without needs for prior generic IAA knowledge. To this end, we construct interaction-matrices from content features constructed from pre-trained image classification features and latent preference features. To realize a robust interaction-matrix based personalized IAA model, we discuss in detail on different strategies for constructing interaction-matrices and estimating personalized aesthetic scores from the interaction-matrices. Besides the personalized IAA scenario, we further propose strategies to adapt the proposed personalized IAA model to different scenarios of generic IAA. Extensive experiments show that: 1) our method significantly outperforms 5 previous relevant personalized IAA methods on FLICKR-AES dataset, especially the methods that require generic IAA knowledge as the basis; 2) in terms of generic IAA, the proposed approach also outperforms 13 generic IAA methods on AVA dataset.
C1 [Hou, Jingwen; Lin, Weisi; Liu, Weide; Zhao, Baoquan] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Yue, Guanghui] Shenzhen Univ, Hlth Sci Ctr, Sch Biomed Engn, Shenzhen 518060, Peoples R China.
C3 Nanyang Technological University; Shenzhen University
RP Lin, WS (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
EM jingwen003@e.ntu.edu.sg; weide001@e.ntu.edu.sg; yueguanghui@szu.edu.cn;
   bqzhao@ntu.edu.sg; wslin@ntu.edu.sg
RI Hou, Jingwen/KHV-1954-2024; Lin, Wei/D-3353-2012; Lin, Weisi/A-3696-2011
OI Hou, Jingwen/0000-0002-6397-0114; LIU, WEIDE/0000-0002-9855-4479; Lin,
   Weisi/0000-0001-9866-1947
FU Tier-1 Fund - Ministry of Education, Singapore [MOE2021 RG14/21]
FX This work was supported by Tier-1 Fund MOE2021 RG14/21 granted by the
   Ministry of Education, Singapore.
CR Chen C, 2020, ACM T INFORM SYST, V38, DOI 10.1145/3373807
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Cui CR, 2020, INFORM SCIENCES, V512, P780, DOI 10.1016/j.ins.2019.10.011
   Cui CR, 2019, IEEE T MULTIMEDIA, V21, P1209, DOI 10.1109/TMM.2018.2875357
   Deng ZH, 2019, AAAI CONF ARTIF INTE, P61
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   He XN, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2227
   He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569
   Hosu V, 2019, PROC CVPR IEEE, P9367, DOI 10.1109/CVPR.2019.00960
   Hou JW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4718, DOI 10.1145/3394171.3416271
   Hou JW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P816, DOI 10.1145/3394171.3413695
   Jin X, 2018, AAAI CONF ARTIF INTE, P77
   Jin X, 2019, IET COMPUT VIS, V13, P206, DOI 10.1049/iet-cvi.2018.5249
   Kao YY, 2015, IEEE IMAGE PROC, P1583, DOI 10.1109/ICIP.2015.7351067
   Kolesnikov Alexander, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P491, DOI 10.1007/978-3-030-58558-7_29
   Li C, 2010, P ACM INT C MULT, P827, DOI DOI 10.1145/1873951.1874089
   Li LD, 2020, IEEE T IMAGE PROCESS, V29, P3898, DOI 10.1109/TIP.2020.2968285
   Liu D, 2020, IEEE WINT CONF APPL, P3558, DOI [10.1109/WACV45572.2020.9093412, 10.1109/wacv45572.2020.9093412]
   Liu Hong, 2021, Advances in Neural Information Processing Systems, V34
   Locher P, 2008, SPATIAL VISION, V21, P55, DOI 10.1163/156856808782713762
   Locher PJ, 2003, ACTA PSYCHOL, V114, P147, DOI 10.1016/j.actpsy.2003.07.001
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Luo W, 2011, IEEE I CONF COMP VIS, P2206, DOI 10.1109/ICCV.2011.6126498
   Lv P, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1328, DOI 10.1145/3240508.3240635
   Ma S, 2017, PROC CVPR IEEE, P722, DOI 10.1109/CVPR.2017.84
   Ma S, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1053, DOI 10.1145/2647868.2655053
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   Murray N, 2017, Arxiv, DOI arXiv:1708.04890
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Obrador P., 2009, PROC 1 ACM SIGMM WOR, P65
   Park K, 2017, IEEE WINT CONF APPL, P1206, DOI 10.1109/WACV.2017.139
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Qiuyu Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14102, DOI 10.1109/CVPR42600.2020.01412
   Ren J, 2017, IEEE I CONF COMP VIS, P638, DOI 10.1109/ICCV.2017.76
   Rendle S, 2020, RECSYS 2020: 14TH ACM CONFERENCE ON RECOMMENDER SYSTEMS, P240, DOI 10.1145/3383313.3412488
   She DY, 2021, PROC CVPR IEEE, P8471, DOI 10.1109/CVPR46437.2021.00837
   Talebi H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P487, DOI 10.1109/ICCV48922.2021.00055
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Tang CX, 2022, AAAI CONF ARTIF INTE, P2344
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Touvron H, 2021, Arxiv, DOI [arXiv:2105.03404, DOI 10.48550/ARXIV.2105.03404]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang GL, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P957
   Wang WN, 2019, IEEE IMAGE PROC, P1875, DOI [10.1109/icip.2019.8803119, 10.1109/ICIP.2019.8803119]
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Yu J, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2553, DOI 10.1145/3343031.3356065
   Yu WH, 2022, PROC CVPR IEEE, P10809, DOI 10.1109/CVPR52688.2022.01055
   Yalniz IZ, 2019, Arxiv, DOI [arXiv:1905.00546, DOI 10.48550/ARXIV.1905.00546]
   Zeng H, 2020, IEEE T IMAGE PROCESS, V29, P1548, DOI 10.1109/TIP.2019.2941778
   Zhang LM, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2303650
   Zhang XD, 2019, IEEE T MULTIMEDIA, V21, P2815, DOI 10.1109/TMM.2019.2911428
   Zhu HC, 2022, IEEE T CYBERNETICS, V52, P1798, DOI 10.1109/TCYB.2020.2984670
   Zhu HC, 2023, IEEE T MULTIMEDIA, V25, P179, DOI 10.1109/TMM.2021.3123468
NR 54
TC 6
Z9 6
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5263
EP 5278
DI 10.1109/TMM.2022.3189276
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300046
DA 2024-07-18
ER

PT J
AU Hu, D
   Wang, Z
   Nie, FP
   Wang, R
   Li, XL
AF Hu, Di
   Wang, Zheng
   Nie, Feiping
   Wang, Rong
   Li, Xuelong
TI Self-Supervised Learning for Heterogeneous Audiovisual Scene Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Task analysis; Location awareness; Complexity theory;
   Training; Concurrent computing; Image analysis; Audiovisual sound
   localization and separation; heterogenous audiovisual scene analysis;
   multimodal audiovisual learning
ID BLIND SOURCE SEPARATION; SOUND SOURCE; SPEECH; TIME
AB Due to the difficulty of annotating large amounts of training data, directly learning the association of sound and its makers in natural videos is a challenging task for machines. In this paper, we present a novel audiovisual model that introduces a soft-clustering module as the audio and visual content detector, and regards the pervasive property of audiovisual concurrency as the latent supervision for inferring the correlation among detected contents. Furthermore, we discover for the first time that the complexity of data has an impact on the training efficiency and subsequent performance of audiovisual model, i.e., more complex data brings more obstacles to the model training, and degrades the performance of downstream audiovisual tasks. To address the issue of audiovisual learning, we propose a novel heterogeneous audiovisual scene analysis module that trains the model from simple to complex scene. We show that such ordered learning procedure rewards the model the merits of easy training and fast convergence. Meanwhile, our audiovisual model can also provide effective unimodal representation and cross-modal alignment performance. We further deploy the well-trained model into practical audiovisual sound localization and separation tasks. We show that our localization model significantly outperforms existing methods, based on which we show comparable performance in sound separation task by comparison to several related SOTA audiovisual learning methods without referring external visualsupervision.
C1 [Hu, Di] Renmin Univ China, Beijing Key Lab Big Data Management & Anal Method, Gaoling Sch Artificial Intelligence, Beijing 100872, Peoples R China.
   [Wang, Zheng] Xi An Jiao Tong Univ, Sch Software Engn, Xian 10698, Peoples R China.
   [Nie, Feiping; Wang, Rong; Li, Xuelong] Northwestern Polytech Univ, Sch Artificial Intelligence Opt & Elect iOPEN, Xian 710072, Shaanxi, Peoples R China.
   [Nie, Feiping; Wang, Rong; Li, Xuelong] Northwestern Polytech Univ, Key Lab Intelligent Interact & Applicat, Minist Ind & Informat Technol, Xian 710072, Shaanxi, Peoples R China.
C3 Renmin University of China; Xi'an Jiaotong University; Northwestern
   Polytechnical University; Northwestern Polytechnical University
RP Wang, Z (corresponding author), Xi An Jiao Tong Univ, Sch Software Engn, Xian 10698, Peoples R China.
EM dihu@ruc.edu.cn; zhengwangml@gmail.com; feipingnie@gmail.com;
   wangrong07@tsinghua.org.cn; li@nwpu.edu.cn
RI Li, Xue-long/AFU-6301-2022; Nie, Feiping/B-3039-2012; liu,
   mengjie/KDN-1890-2024; bai, yu/KHU-2608-2024; zhen, li/KGK-6604-2024
OI Li, Xue-long/0000-0003-2037-2525; Wang, Rong/0000-0001-9240-6726; Nie,
   Feiping/0000-0002-0871-6519
FU Intelligent Social Governance Platform, Major Innovation AMP; Planning
   Interdisciplinary Platform for the "Double-First Class" Initiative,
   Renmin University of China; Beijing Outstanding Young Scientist Program
   [BJJWZYJH012019100020098]; National Natural Science Foundation of China
   [62106272, 61871470]; 2021 Tencent AI Lab Rhino-Bird Focused Research
   Program [JR202141]; Young Elite Scientists Sponsorship Program by CAST;
   China Postdoctoral Science Foundation [2021M702599]
FX This work was supported in part by Intelligent Social Governance
   Platform, Major Innovation & Planning Interdisciplinary Platform for the
   "Double-First Class" Initiative, Renmin University of China, in part by
   the Beijing Outstanding Young Scientist Program under Grant
   BJJWZYJH012019100020098, in part by the National Natural Science
   Foundation of China under Grant 62106272, in part by 2021 Tencent AI Lab
   Rhino-Bird Focused Research Program under Grant JR202141, in part by the
   Young Elite Scientists Sponsorship Program by CAST, in part by the
   National Natural Science Foundation of China under Grant 61871470, and
   in part by China Postdoctoral Science Foundation under Grant
   2021M702599. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Dr. Concetto Spampinato.
CR [Anonymous], 2006, Acoustics, Speech and Signal Processing, DOI DOI 10.1109/ICASSP.2006.1661352
   Arandjelovic Relja, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P609, DOI 10.1109/ICCV.2017.73
   Arandjelovic Relja, 2018, P EUR C COMP VIS ECC, P435
   Arons B., 1992, J. Amer. Voice I/O Soc., V12, P35
   Aytar Y., 2016, Advances in neural information processing systems (NeurIPS), V29, P892
   BAUCKHAGE C, 2015, LECTURE NOTES ON DAT
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bobin J, 2007, IEEE T IMAGE PROCESS, V16, P2662, DOI 10.1109/TIP.2007.906256
   Braun S, 2017, EUR SIGNAL PR CONF, P548, DOI 10.23919/EUSIPCO.2017.8081267
   Casanovas AL, 2010, INT CONF ACOUST SPEE, P2486, DOI 10.1109/ICASSP.2010.5494896
   Casanovas AL, 2010, IEEE T MULTIMEDIA, V12, P358, DOI 10.1109/TMM.2010.2050650
   Chien JT, 2012, IEEE T AUDIO SPEECH, V20, P302, DOI 10.1109/TASL.2011.2161080
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   ELMAN JL, 1993, COGNITION, V48, P71, DOI 10.1016/0010-0277(93)90058-4
   Ephrat A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201357
   Fisher JW, 2004, IEEE T MULTIMEDIA, V6, P406, DOI 10.1109/TMM.2004.827503
   Gan Chuang, 2020, P IEEECVF C COMPUTER, P10478, DOI DOI 10.1109/CVPR42600.2020.01049
   GAO R, 2018, LEARNING TO SEPARATE, P35
   Gao RH, 2019, IEEE I CONF COMP VIS, P3878, DOI 10.1109/ICCV.2019.00398
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Gong C, 2016, IEEE T IMAGE PROCESS, V25, P3249, DOI 10.1109/TIP.2016.2563981
   GULLI A, 2017, DEEP LEARNING WITH K
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Holmes NP, 2005, CURR BIOL, V15, pR762, DOI 10.1016/j.cub.2005.08.058
   Hu D., 2020, Advances in Neural Information Processing Systems, P10077
   Hu D, 2019, PROC CVPR IEEE, P9240, DOI 10.1109/CVPR.2019.00947
   Innami S, 2012, COMPUT MATH APPL, V64, P1333, DOI 10.1016/j.camwa.2012.03.077
   Izadinia H, 2013, IEEE T MULTIMEDIA, V15, P378, DOI 10.1109/TMM.2012.2228476
   Jordan J, 2005, MAT SCI ENG A-STRUCT, V393, P1, DOI 10.1016/j.msea.2004.09.044
   Kidron E, 2005, PROC CVPR IEEE, P88
   Koch G., 2015, ICML DEEP LEARNING W, V2
   KORBAR B, 2018, P 32 INT C NEUR INF, P7773
   Li XL, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2201
   Li XL, 2017, AAAI CONF ARTIF INTE, P4147
   Li YQ, 2006, IEEE T SIGNAL PROCES, V54, P423, DOI 10.1109/TSP.2005.861743
   Lotfian R, 2019, IEEE-ACM T AUDIO SPE, V27, P815, DOI 10.1109/TASLP.2019.2898816
   Moon TK, 1996, IEEE SIGNAL PROC MAG, V13, P47, DOI 10.1109/79.543975
   Murali A, 2018, IEEE INT CONF ROBOT, P6453
   Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39
   Owens A, 2016, LECT NOTES COMPUT SC, V9905, P801, DOI 10.1007/978-3-319-46448-0_48
   Oya T., 2020, P ASIAN C COMPUTER V, P119
   PENHA G, 2020, CURRICULUM LEARNING, P699
   Piczak Karol J, 2015, MM 15 P 2015 ACM MUL, P1015, DOI DOI 10.1145/2733373.2806390
   Plinge Axel, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P614, DOI 10.1109/ICASSP.2014.6853669
   Pu J, 2017, INT CONF ACOUST SPEE, P2901, DOI 10.1109/ICASSP.2017.7952687
   Rohlfing C, 2016, INT CONF ACOUST SPEE, P474, DOI 10.1109/ICASSP.2016.7471720
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rui Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P292, DOI 10.1007/978-3-030-58565-5_18
   Senocak A, 2018, PROC CVPR IEEE, P4358, DOI 10.1109/CVPR.2018.00458
   Shimojo S, 2001, CURR OPIN NEUROBIOL, V11, P505, DOI 10.1016/S0959-4388(00)00241-5
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smith LB, 2018, TRENDS COGN SCI, V22, P325, DOI 10.1016/j.tics.2018.02.004
   SPIERTZ M, 2009, P INT C DIG AUD EFF, P1
   Tian YP, 2018, LECT NOTES COMPUT SC, V11206, P252, DOI 10.1007/978-3-030-01216-8_16
   Vajaria H, 2008, IEEE T CIRC SYST VID, V18, P1608, DOI 10.1109/TCSVT.2008.2005602
   Wang Z, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3009
   Xu XD, 2019, IEEE I CONF COMP VIS, P882, DOI 10.1109/ICCV.2019.00097
   Zhang ZX, 2019, IEEE T MULTIMEDIA, V21, P1289, DOI 10.1109/TMM.2018.2871949
   Zhao H, 2019, IEEE I CONF COMP VIS, P1735, DOI 10.1109/ICCV.2019.00182
   Zhao Hang, 2018, P EUR C COMP VIS ECC, P570, DOI DOI 10.1109/CVPR.2018.00374
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zibulevsky M, 2001, NEURAL COMPUT, V13, P863, DOI 10.1162/089976601300014385
   Zunino Andrea, 2015, P IEEE INT C COMP VI, P6
NR 63
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3534
EP 3545
DI 10.1109/TMM.2022.3162477
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500004
DA 2024-07-18
ER

PT J
AU Li, P
   Roch, MA
   Klinck, H
   Fleishman, E
   Gillespie, D
   Nosal, EM
   Shiu, Y
   Liu, XB
AF Li, Pu
   Roch, Marie A.
   Klinck, Holger
   Fleishman, Erica
   Gillespie, Douglas
   Nosal, Eva-Marie
   Shiu, Yu
   Liu, Xiaobai
TI Learning Stage-Wise GANs for Whistle Extraction in Time-Frequency
   Spectrograms
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Spectrogram; Generative adversarial networks; Data mining; Background
   noise; Data models; Whales; Training; Data augmentation; generative
   adversarial networks
ID AUTOMATED EXTRACTION; NEURAL-NETWORK; CLASSIFICATION; RECOGNITION;
   GENERATION; TRACKING; WHALE
AB Whistle contour extraction aims to derive animal whistles from time-frequency spectrograms as polylines. For toothed whales, whistle extraction results can serve as the basis for analyzing animal abundance, species identity, and social activities. During the last few decades, as long-term recording systems have become affordable, automated whistle extraction algorithms were proposed to process large volumes of recording data. Recently, a deep learning-based method demonstrated superior performance in extracting whistles under varying noise conditions. However, training such networks requires a large amount of labor-intensive annotation, which is not available for many species. To overcome this limitation, we present a framework of stage-wise generative adversarial networks (GANs), which compile new whistle data suitable for deep model training via three stages: generation of background noise in the spectrogram, generation of whistle contours, and generation of whistle signals. By separating the generation of different components in the samples, our framework composes visually promising whistle data and labels even when few expert annotated data are available. Regardless of the amount of human-annotated data, the proposed data augmentation framework leads to a consistent improvement in performance of the whistle extraction model, with a maximum increase of 1.69 in the whistle extraction mean F1-score. Our stage-wise GAN also surpasses one single GAN in improving whistle extraction models with augmented data.
C1 [Li, Pu] San Diego State Univ, Computat Sci Res Ctr, San Diego, CA 92182 USA.
   [Li, Pu] Univ Calif Irvine, Irvine, CA 92697 USA.
   [Roch, Marie A.; Liu, Xiaobai] San Diego State Univ, Dept Comp Sci, San Diego, CA 92182 USA.
   [Klinck, Holger] Cornell Univ, K Lisa Yang Ctr Conservat Bioacoust, Ithaca, NY 14853 USA.
   [Fleishman, Erica] Colorado State Univ, Dept Fish Wildlife & Conservat Biol, Corvallis, OR 97331 USA.
   [Gillespie, Douglas] Univ St Andrews, Scottish Oceans Inst, Sea Mammal Res Unit, St Andrews KY16 9AJ, Fife, Scotland.
   [Nosal, Eva-Marie] Univ Hawaii, Dept Ocean & Resources Engn, Honolulu, HI 96822 USA.
   [Shiu, Yu] Spotify Inc, New York, NY 10007 USA.
C3 California State University System; San Diego State University;
   University of California System; University of California Irvine;
   California State University System; San Diego State University; Cornell
   University; Colorado State University; University of St Andrews;
   University of Hawaii System; Spotify
RP Li, P (corresponding author), San Diego State Univ, Computat Sci Res Ctr, San Diego, CA 92182 USA.
EM pli5270@sdsu.edu; mroch@sdsu.edu; holger.klinck@cornell.edu;
   erica.fleishman@oregonstate.edu; dg50@st-andrews.ac.uk;
   nosal@hawaii.edu; atoultaro@gmail.com; xiaobai.liu@sdsu.edu
RI Roch, Marie/KGL-8020-2024
OI Klinck, Holger/0000-0003-1078-7268; Gillespie,
   Douglas/0000-0001-9628-157X
FU Michael Weise
FX No Statement Available
CR Agarwal S, 2021, IEEE COMPUT SOC CONF, P981, DOI 10.1109/CVPRW53098.2021.00109
   Althnian A, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11020796
   Antoniou A, 2018, Arxiv, DOI arXiv:1711.04340
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   AU WWL, 1985, J ACOUST SOC AM, V77, P726, DOI 10.1121/1.392341
   Bittner R. M., 2017, P INT SOC MUS INF RE, P63, DOI DOI 10.5281/ZENODO.1417937
   Bowles C, 2018, Arxiv, DOI [arXiv:1810.10863, DOI 10.48550/ARXIV.1810.10863]
   Brumm H, 2011, BEHAVIOUR, V148, P1173, DOI 10.1163/000579511X605759
   Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Cheng HS, 2018, SECUR COMMUN NETW, DOI 10.1155/2018/8392080
   Dadouchi F, 2013, J ACOUST SOC AM, V134, P2546, DOI 10.1121/1.4816579
   Dao Tri, 2019, Proc Mach Learn Res, V97, P1528
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Frid-Adar M, 2018, I S BIOMED IMAGING, P289, DOI 10.1109/ISBI.2018.8363576
   Galteri L, 2019, IEEE T MULTIMEDIA, V21, P2131, DOI 10.1109/TMM.2019.2895280
   Gillespie D, 2013, J ACOUST SOC AM, V134, P2427, DOI 10.1121/1.4816555
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gruden P, 2020, J ACOUST SOC AM, V148, P3014, DOI 10.1121/10.0002257
   Gruden P, 2016, J ACOUST SOC AM, V140, P1981, DOI 10.1121/1.4962980
   Halkias XC, 2006, APPL ACOUST, V67, P1164, DOI 10.1016/j.apacoust.2006.05.006
   Han K, 2014, IEEE-ACM T AUDIO SPE, V22, P2158, DOI 10.1109/TASLP.2014.2363410
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang SW, 2018, LECT NOTES COMPUT SC, V11213, P731, DOI 10.1007/978-3-030-01240-3_44
   Gulrajani I, 2017, ADV NEUR IN, V30
   Janik VM, 2013, MAR MAMMAL SCI, V29, P109, DOI 10.1111/j.1748-7692.2011.00549.x
   Jaramillo-Legorreta A, 2017, CONSERV BIOL, V31, P183, DOI 10.1111/cobi.12789
   Jeong J, 2019, ADV NEUR IN, V32
   Jiang JJ, 2019, APPL ACOUST, V150, P169, DOI 10.1016/j.apacoust.2019.02.007
   Kahl S, 2021, ECOL INFORM, V61, DOI 10.1016/j.ecoinf.2021.101236
   Karras T., 2018, P INF C LEARN REPR
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kershenbaum A, 2013, J ACOUST SOC AM, V134, P4435, DOI 10.1121/1.4828821
   Larkin K.G., 2016, arXiv
   Ngo LM, 2022, IEEE T MULTIMEDIA, V24, P377, DOI 10.1109/TMM.2021.3050672
   Lee CH, 2009, IEEE T MULTIMEDIA, V11, P670, DOI 10.1109/TMM.2009.2017635
   Lehtinen J., 2020, Advances in Neural Information Processing Systems (NeurIPS), V33, P12104
   Li P, 2021, INT C PATT RECOG, P10584, DOI 10.1109/ICPR48806.2021.9413272
   Li P, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9206992
   Liu S., 2018, PROC IEEE 10 INT C W, P1
   Lu JJ, 2022, IEEE T MULTIMEDIA, V24, P558, DOI 10.1109/TMM.2021.3054973
   Lu W. T., 2018, ISMIR, P521
   Mallawaarachchi A, 2008, J ACOUST SOC AM, V124, P1159, DOI 10.1121/1.2945711
   Mariani G, 2018, Arxiv, DOI arXiv:1803.09655
   Mellinger DK, 2006, APPL ACOUST, V67, P1226, DOI 10.1016/j.apacoust.2006.06.002
   Mellinger DK, 2011, J ACOUST SOC AM, V129, P4055, DOI 10.1121/1.3531926
   Met-Montot B, 2021, EUR SIGNAL PR CONF, P1185, DOI 10.23919/Eusipco47968.2020.9287312
   Mu J., 2020, P IEEE CVF C COMP VI, P12386
   Mun Seongkyu, 2017, DCASE, P93
   Osakabe T, 2021, PROC SPIE, V11766, DOI 10.1117/12.2590977
   Pandey S, 2020, BIOMED SIGNAL PROCES, V57, DOI 10.1016/j.bspc.2019.101782
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   Radford A, 2016, Arxiv, DOI [arXiv:1511.06434, DOI 10.48550/ARXIV.1511.06434]
   Ren JF, 2017, IEEE T MULTIMEDIA, V19, P447, DOI 10.1109/TMM.2016.2618218
   Rizzi A, 2017, IEEE T MULTIMEDIA, V19, P1405, DOI 10.1109/TMM.2017.2674603
   Roch MA, 2011, J ACOUST SOC AM, V130, P2212, DOI 10.1121/1.3624821
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Serra OM, 2020, ECOL INFORM, V55, DOI 10.1016/j.ecoinf.2019.101036
   SJARE BL, 1986, CAN J ZOOL, V64, P2824, DOI 10.1139/z86-406
   Taruski A.G., 1979, Behavior of Marine Animals, V3, P345
   Thomsen F, 2002, NATURWISSENSCHAFTEN, V89, P404, DOI 10.1007/s00114-002-0351-x
   Vijayan K, 2018, ASIAPAC SIGN INFO PR, P1893, DOI 10.23919/APSIPA.2018.8659615
   Waheed A, 2020, IEEE ACCESS, V8, P91916, DOI 10.1109/ACCESS.2020.2994762
   Wali A, 2022, COMPUT SPEECH LANG, V72, DOI 10.1016/j.csl.2021.101308
   Wang XQ, 2021, APPL ACOUST, V176, DOI 10.1016/j.apacoust.2020.107698
   White P., 2008, CAN ACOUST, V36, P146
   Xie C, 2020, PROC CVPR IEEE, P816, DOI 10.1109/CVPR42600.2020.00090
   Yost W. A., 2001, Fundamentals of hearing: an introduction, DOI DOI 10.1163/9789004501935
   Zeng JS, 2021, AAAI CONF ARTIF INTE, V35, P3270
   Zhang SQ, 2018, IEEE T MULTIMEDIA, V20, P1576, DOI 10.1109/TMM.2017.2766843
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 70
TC 3
Z9 3
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9302
EP 9314
DI 10.1109/TMM.2023.3251109
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200039
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, YJ
   Zhang, Z
   Chen, BZ
   Lu, GM
   Zhang, DV
AF Li, Yingjian
   Zhang, Zheng
   Chen, Bingzhi
   Lu, Guangming
   Zhang, David
TI Deep Margin-Sensitive Representation Learning for Cross-Domain Facial
   Expression Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Databases; Face recognition; Semantics; Data mining;
   Representation learning; Measurement; Domain adaptation; facial
   expression recognition; metric learning; mutual information; semantic
   representations
AB Cross-domain Facial Expression Recognition (FER) aims to safely transfer the learned knowledge from labeled source data to unlabeled target data, which is challenging due to the subtle difference between various expressions and the large discrepancy between domains. Existing methods mainly focus on reducing the domain shift for transferable features but fail to learn discriminative representations for recognizing facial expression, which may result in negative transfer under cross-domain settings. To this end, we propose a novel Deep Margin-Sensitive Representation Learning (DMSRL) framework, which can extract multi-level discriminative features during sematic-aware domain adaptation. Specifically, we design a semantic metric learning module based on the category prior of source data and generated pseudo labels of target data, which can facilitate discriminative intra-domain representation learning and transferable inter-domain knowledge discovery by enlarging the category margin. Moreover, we develop a mutual information minimization module by simultaneously distilling the domain-invariant components and eliminating the domain-sensitive ones, which benefits discriminative transferable feature learning by generating accurate pseudo target labels. Furthermore, instead of only utilizing the global features, we formulate a multi-level feature extracting module to concurrently get the local ones, which contain detailed information to distinguish the small changes among different expressions. These modules are jointly utilized in our DMSRL in an end-to-end manner to ensure the positive transfer of source knowledge. Extensive experimental results on seven databases demonstrate that our DMSRL can achieve superior performance against state-of-the-art baselines.
C1 [Li, Yingjian; Zhang, Zheng; Chen, Bingzhi; Lu, Guangming] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
   [Zhang, David] Chinese Univ Hong Kong, Sch Data Sci, Shenzhen 518172, Peoples R China.
   [Zhang, David] Shenzhen Inst Artificial Intelligence & Robot Soc, Shenzhen Res Inst Big Data, Shenzhen 518172, Peoples R China.
C3 Harbin Institute of Technology; The Chinese University of Hong Kong,
   Shenzhen; Shenzhen Institute of Artificial Intelligence & Robotics for
   Society; Shenzhen Research Institute of Big Data
RP Zhang, Z; Lu, GM (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
EM hit_lyj@126.com; darrenzz219@gmail.com; chenbingzhi.smile@gmail.com;
   luguangm@hit.edu.cn; davidzhang@cuhk.edu.cn
RI Zhang, David D/O-9396-2016; Zhang, Zhang/JAX-2097-2023; Zhang,
   Zheng/M-6325-2014
OI Zhang, David D/0000-0002-5027-5286; Zhang, Zheng/0000-0003-1470-6998;
   Lu, Guangming/0000-0003-1578-2634; Li, Yingjian/0000-0002-0653-4535;
   Chen, Bingzhi/0000-0002-2497-6214
FU National Key Research and Development Program of China [2018AAA0100100];
   NSFC [62176077, 62002085]; Guangdong Basic and Applied Basic Research
   Foundation [2019B1515120055]; Shenzhen Key Technical Project [2020N046];
   Shenzhen Fundamental Research Fund [JCYJ20210324132210025,
   GXWD20201230155427003-20200824103320001]; Medical Biometrics Perception
   and Analysis Engineering Laboratory, Shenzhen, China; Shenzhen Research
   Institute of Big Data and Shenzhen Institute of Artificial Intelligence
   and Robotics for Society; China Scholarship Council
FX This work was supported in part by the National Key Research and
   Development Program of China under Project 2018AAA0100100, in part by
   the NSFC under Grants 62176077 and 62002085, in part by the Guangdong
   Basic and Applied Basic Research Foundation under Grant 2019B1515120055,
   in part by the Shenzhen Key Technical Project under Grant 2020N046, in
   part by the Shenzhen Fundamental Research Fund under Grants
   JCYJ20210324132210025 and GXWD20201230155427003-20200824103320001, in
   part by the Medical Biometrics Perception and Analysis Engineering
   Laboratory, Shenzhen, China, in part by the Shenzhen Research Institute
   of Big Data and Shenzhen Institute of Artificial Intelligence and
   Robotics for Society, and in part by the Scholarship from the China
   Scholarship Council.
CR Agarwal S, 2019, IEEE T MULTIMEDIA, V21, P902, DOI 10.1109/TMM.2018.2871417
   Arora S, 2017, PR MACH LEARN RES, V70
   Belghazi MI, 2018, PR MACH LEARN RES, V80
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Calvo RA, 2010, IEEE T AFFECT COMPUT, V1, P18, DOI 10.1109/T-AFFC.2010.1
   Chen DL, 2023, IEEE T AFFECT COMPUT, V14, P1322, DOI 10.1109/TAFFC.2021.3077489
   Chen X., 2019, PR MACH LEARN RES, P1052
   Dahmane M, 2014, IEEE T MULTIMEDIA, V16, P1574, DOI 10.1109/TMM.2014.2321113
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Ge SM, 2020, IEEE T IMAGE PROCESS, V29, P6898, DOI 10.1109/TIP.2020.2995049
   Ge SM, 2019, IEEE T IMAGE PROCESS, V28, P2051, DOI 10.1109/TIP.2018.2883743
   Goodfellow Ian J., 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P117, DOI 10.1007/978-3-642-42051-1_16
   Happy SL, 2015, IEEE T AFFECT COMPUT, V6, P1, DOI 10.1109/TAFFC.2014.2386334
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hjelm R. D., 2019, PROC INT C LEARN REP, P1, DOI [DOI 10.48550/ARXIV.1808.06670, 10.48550/arXiv.1808.06670]
   Ji YL, 2019, NEUROCOMPUTING, V333, P231, DOI 10.1016/j.neucom.2018.12.037
   Lee CY, 2019, PROC CVPR IEEE, P10277, DOI 10.1109/CVPR.2019.01053
   Li S, 2022, IEEE T AFFECT COMPUT, V13, P1195, DOI 10.1109/TAFFC.2020.2981446
   Li S, 2022, IEEE T AFFECT COMPUT, V13, P881, DOI 10.1109/TAFFC.2020.2973158
   Li S, 2018, INT C PATT RECOG, P3092, DOI 10.1109/ICPR.2018.8545284
   Li S, 2019, IEEE T IMAGE PROCESS, V28, P356, DOI 10.1109/TIP.2018.2868382
   Li YJ, 2023, IEEE T AFFECT COMPUT, V14, P451, DOI 10.1109/TAFFC.2020.3031602
   Li YJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3312, DOI 10.1145/3474085.3475484
   Li YJ, 2022, IEEE T CIRC SYST VID, V32, P3178, DOI 10.1109/TCSVT.2021.3103760
   Li YJ, 2022, IEEE T CIRC SYST VID, V32, P3190, DOI 10.1109/TCSVT.2021.3103782
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P2439, DOI 10.1109/TIP.2018.2886767
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   da Silva FAM, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.2.023015
   Mollahosseini A, 2016, IEEE WINT CONF APPL
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Ni TG, 2021, IEEE T COMPUT SOC SY, V8, P1213, DOI 10.1109/TCSS.2020.3013938
   Pantic M, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P317, DOI 10.1109/ICME.2005.1521424
   Pons G, 2018, IEEE T AFFECT COMPUT, V9, P343, DOI 10.1109/TAFFC.2017.2753235
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song JM, 2019, PR MACH LEARN RES, V89
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Wang SF, 2010, IEEE T MULTIMEDIA, V12, P682, DOI 10.1109/TMM.2010.2060716
   Xie S., 2018, P 35 INT C MACHINE L, P5423
   Xie SY, 2019, IEEE T MULTIMEDIA, V21, P211, DOI 10.1109/TMM.2018.2844085
   Xie Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1255, DOI 10.1145/3394171.3413822
   Xu RJ, 2019, IEEE I CONF COMP VIS, P1426, DOI 10.1109/ICCV.2019.00151
   Yan Y, 2020, IEEE T MULTIMEDIA, V22, P2792, DOI 10.1109/TMM.2019.2962317
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Zavarez MV, 2017, SIBGRAPI, P405, DOI 10.1109/SIBGRAPI.2017.60
   Zhang FF, 2022, IEEE T MULTIMEDIA, V24, P1800, DOI 10.1109/TMM.2021.3072786
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao GY, 2011, IMAGE VISION COMPUT, V29, P607, DOI 10.1016/j.imavis.2011.07.002
   Zhong L, 2015, IEEE T CYBERNETICS, V45, P1499, DOI 10.1109/TCYB.2014.2354351
   Zhu YC, 2019, NEURAL NETWORKS, V119, P214, DOI 10.1016/j.neunet.2019.07.010
NR 55
TC 21
Z9 21
U1 5
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1359
EP 1373
DI 10.1109/TMM.2022.3141604
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100026
DA 2024-07-18
ER

PT J
AU Li, YX
   Yang, Q
   Chen, QC
   Hu, BT
   Wang, XL
   Ding, YX
   Ma, L
AF Li, Yunxin
   Yang, Qian
   Chen, Qingcai
   Hu, Baotian
   Wang, Xiaolong
   Ding, Yuxin
   Ma, Lin
TI Fast and Robust Online Handwritten Chinese Character Recognition With
   Deep Spatial and Contextual Information Fusion Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-modal fusion; contextual information; Online handwritten Chinese
   character recognition (OLHCCR)
AB Deep convolutional neuralnetworks have achieved fairly high accuracy for single online handwritten Chinese character recognition (SOLHCCR). However, in real application scenarios, users always write multiple characters to form a complete sentence, and previous contextual information holds significant potential for improving the accuracy, robustness and efficiency of recognition. In this work, we first propose a simple and straightforward model named the vanilla compositional network (VCN) by coupling convolutional neural network with a sequence modeling architecture (i.e., a recurrent neural network or Transformer), which exploits the handwritten character's previous contextual information. Although VCN performs much better than the previous state-of-the-art SOLHCCR models, it is a two-stage architecture in nature. It suffers from high fragility when confronting with poorly written characters such as sloppy writing, and missing or broken strokes, due to relying heavily on contextual information. To improve the robustness of the OLHCCR model, we further propose a novel deep spatial & contextual information fusion network (DSCIFN). It utilizes an autoregresssive framework pre-trained on a large-scale sentence corpora as the backbone component, and highly integrates the spatial features of handwritten characters and their previous contextual information in a multi-layer fusion module. To verify the effectiveness of models, we reorganize a new form of online Chinese handwritten character with its previous context dataset, named OHCCC. Extensive experimental results demonstrate that DSCIFN achieves state-of-the-art performance and has increased strong robustness compared to VCN and previous SOLHCCR models. The in-depth empirical analysis and case study indicate that DSCIFN can significantly improve the efficiency of handwriting input because it does not need complete strokes to recognize a handwritten Chinese character precisely.
C1 [Li, Yunxin; Yang, Qian; Chen, Qingcai; Hu, Baotian; Wang, Xiaolong; Ding, Yuxin; Ma, Lin] Harbin Inst Technol, Dept Comp Sci & Technol, Shenzhen 518055, Peoples R China.
C3 Harbin Institute of Technology
RP Hu, BT (corresponding author), Harbin Inst Technol, Dept Comp Sci & Technol, Shenzhen 518055, Peoples R China.
EM 19S051054@stu.hit.edu.cn; 20s051056@stu.hit.edu.cn;
   qingcai.chen@hit.edu.cn; hubaotian@hit.edu.cn; xlwangsz@hit.edu.cn;
   yxding@hit.edu.cn; forest.linma@gmail.com
RI Hu, Baotian/AAA-4102-2022; Chen, Qingcai/JVN-1580-2024
OI Hu, Baotian/0000-0001-7490-684X; 
FU Natural Science Foundation of China [62006061]; Strategic Emerging
   Industry Development Special Funds of Shenzhen [JCYJ20200109113441941];
   Stable Support Program for Higher Education Institutions of Shenzhen
   [GXWD20201230155427003-20200824155011001]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 62006061, in part by the Strategic Emerging Industry
   Development Special Funds of Shenzhen under Grant JCYJ20200109113441941,
   and in part by the Stable Support Program for Higher Education
   Institutions of Shenzhen under Grant
   GXWD20201230155427003-20200824155011001. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Ichiro Ide.(Correspondingauthor: Baotian Hu.).
CR Agarap AF, 2018, arXiv, DOI 10.48550/arXiv.1803.08375
   Bai ZL, 2005, PROC INT CONF DOC, P262
   Byerly A, 2021, Arxiv, DOI arXiv:2001.09136
   Carbune V, 2020, INT J DOC ANAL RECOG, V23, P89, DOI 10.1007/s10032-020-00350-4
   Chen DQ, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P2358
   Chung J., 2014, NIPS 2014 WORKSH DEE
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Das Abhishek, 2020, 2020 Proceedings of the International Conference on Communication and Signal Processing (ICCSP), P0538, DOI 10.1109/ICCSP48568.2020.9182218
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Gatt A, 2018, J ARTIF INTELL RES, V61, P65, DOI 10.1613/jair.5477
   Gehrmann S, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4098
   Graves A, 2014, Arxiv, DOI arXiv:1308.0850
   Harris ZS, 1954, WORD, V10, P146, DOI 10.1080/00437956.1954.11659520
   Haykin S., 1994, NEURAL NETWORKS COMP
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hou Y, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION, P2238, DOI 10.1109/ICInfA.2015.7279659
   Ung HQ, 2021, LECT NOTES COMPUT SC, V12917, P403, DOI 10.1007/978-3-030-86159-9_29
   Iqbal T., 2020, J KING SAUD UNIV-COM, P1319
   Jin LW, 2011, INT J DOC ANAL RECOG, V14, P53, DOI 10.1007/s10032-010-0116-6
   Jing K, 2019, Arxiv, DOI arXiv:1906.03591
   KAWAMURA A, 1992, 11TH IAPR INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION, PROCEEDINGS, VOL II, P183, DOI 10.1109/ICPR.1992.201750
   Lai SX, 2017, PATTERN RECOGN LETT, V89, P60, DOI 10.1016/j.patrec.2017.02.011
   LeCun Y., 1995, The handbook of brain theory and neural networks, V3361, DOI [10.5555/303568.303704, DOI 10.5555/303568.303704]
   Lewis M., 2020, P 58 ANN M ASS COMP, P7871, DOI 10.18653/v1/2020.acl-main.703
   Liu C.-L., 2006, P 10 INT WORKSH FRON, P1
   Liu CL, 2011, PROC INT CONF DOC, P37, DOI 10.1109/ICDAR.2011.17
   Liu CL, 2004, IEEE T PATTERN ANAL, V26, P198, DOI 10.1109/TPAMI.2004.1262182
   Liu X, 2020, IEEE T NEUR NET LEAR, V31, P4637, DOI 10.1109/TNNLS.2019.2956965
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Meng Y., 2019, Adv. Neural Inf. Process. Syst., V32
   Naik VA, 2017, INT CONF COMPUT
   Nwankpa C, 2018, Arxiv, DOI arXiv:1811.03378
   Patro Badri Narayana, 2018, P 27 INT C COMPUTATI, P2715
   Phan TV, 2011, PROC INT CONF DOC, P834, DOI 10.1109/ICDAR.2011.171
   Plamondon R, 2000, IEEE T PATTERN ANAL, V22, P63, DOI 10.1109/34.824821
   Qu XW, 2015, 2015 14TH IAPR INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS (MVA), P130, DOI 10.1109/MVA.2015.7153150
   Quiniou S, 2009, INT J PATTERN RECOGN, V23, P945, DOI 10.1142/S0218001409007442
   Radford A., 2019, LANGUAGE MODELS ARE
   Raffel C, 2020, J MACH LEARN RES, V21
   Rajpurkar P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P784
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Vaswani A, 2017, P ADV NEURAL INFORM, P30
   Wang DH, 2012, PATTERN RECOGN, V45, P3661, DOI 10.1016/j.patcog.2012.04.020
   Williams J, 2018, FIRST GRAND CHALLENGE AND WORKSHOP ON HUMAN MULTIMODAL LANGUAGE (CHALLENGE-HML), P11
   Xu Z., 2000, HIGH TECHNOL LETT, V1
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yang WX, 2016, PATTERN RECOGN, V58, P190, DOI 10.1016/j.patcog.2016.04.007
   Yang ZL, 2019, ADV NEUR IN, V32
   Yin F, 2013, PROC INT CONF DOC, P1464, DOI 10.1109/ICDAR.2013.218
   Zadeh A., 2017, P 2017 C EMP METH NA, P1103, DOI 10.18653/v1/D17-1115
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P221, DOI 10.1109/TMM.2018.2844689
   Zhang XY, 2018, IEEE T PATTERN ANAL, V40, P849, DOI 10.1109/TPAMI.2017.2695539
   Zhang XY, 2017, PATTERN RECOGN, V61, P348, DOI 10.1016/j.patcog.2016.08.005
   Zhang XY, 2013, IEEE T PATTERN ANAL, V35, P1773, DOI 10.1109/TPAMI.2012.239
   Zhou Shusen, 2010, P INT WORKSH DOC AN, P223, DOI 10.1145/1815330.1815359
NR 60
TC 3
Z9 3
U1 1
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2140
EP 2152
DI 10.1109/TMM.2022.3143324
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100042
DA 2024-07-18
ER

PT J
AU Li, Z
   Li, HL
   Luo, X
   Hu, YJ
   Lam, KY
   Kot, AC
AF Li, Zhi
   Li, Haoliang
   Luo, Xin
   Hu, Yongjian
   Lam, Kwok-Yan
   Kot, Alex C.
TI Asymmetric Modality Translation for Face Presentation Attack Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face presentation attack detection; asymmetric modality translation;
   modality fusion; unseen attack; illumination normalization
ID DOMAIN ADAPTATION
AB Face presentation attack detection (PAD) is an essentialmeasure to protect face recognition systems from being spoofed by malicious users and has attracted great attention from both academia and industry. Although most of the existing methods can achieve desired performance to some extent, the generalization issue of face presentation attack detection under cross-domain settings (e.g., the setting of unseen attacks and varying illumination) remains to be solved. In this paper, we propose a novel framework based on asymmetric modality translation for face presentation attack detection in bi-modality scenarios. Under the framework, we establish connections between two modality images of genuine faces. Specifically, a novel modality fusion scheme is presented that the image of one modality is translated to the other one through an asymmetric modality translator, then fused with its corresponding paired image. The fusion result is fed as the input to a discriminator for inference. The training of the translator is supervised by an asymmetric modality translation loss. Besides, an illumination normalization module based on Pattern of Local Gravitational Force (PLGF) representation is used to reduce the impact of illumination variation. We conduct extensive experiments on three public datasets, which validate that our method is effective in detecting various types of attacks and achieves state-of-the-art performance under different evaluation protocols.
C1 [Li, Zhi; Lam, Kwok-Yan] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Li, Haoliang] City Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China.
   [Luo, Xin; Hu, Yongjian] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Peoples R China.
   [Luo, Xin; Hu, Yongjian; Kot, Alex C.] China Singapore Int Joint Res Inst, Singapore, Singapore.
   [Kot, Alex C.] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University; City University of Hong Kong; South
   China University of Technology; Nanyang Technological University
RP Li, HL (corresponding author), City Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China.
EM zhi003@e.ntu.edu.sg; haoliang.li@cityu.edu.hk;
   201820132970@mail.scut.edu.cn; eeyjhu@scut.edu.cn;
   kwokyan.lam@ntu.edu.sg; eackot@ntu.edu.sg
RI Lam, Kwok-Yan/AAG-5325-2020
OI Li, Haoliang/0000-0002-8723-8112; Kot, Alex/0000-0001-6262-8125; Lam,
   Kwok-Yan/0000-0001-7479-7970; Hu, Yongjian/0000-0002-7775-3786
FU NTU-PKU\penalty-\@M Joint Research Institute; Science and Technology
   Foundation of Guangzhou Huangpu Development District [2019GH16];
   China-Singapore International Joint Research Institute [206-A018001];
   National Research Foundation, Prime Minister.s Office, Singapore under
   its Strategic Capability Research Centres Funding Initiative
FX This work was supported in part by the NTU-PKU\penalty-\@M Joint
   Research Institute (a collaboration between the Nanyang Technological
   University and Peking University that is sponsored by a donation from
   the Ng Teng Fong Charitable Foundation), in part by the Science and
   Technology Foundation of Guangzhou Huangpu Development District under
   Grant 2019GH16, in part by the China-Singapore International Joint
   Research Institute under Grant 206-A018001, and in part by the National
   Research Foundation, Prime Minister.s Office, Singapore under its
   Strategic Capability Research Centres Funding Initiative.
CR Agarwal A., 2016, INT CONF BIOMETR THE, P1
   Bhattacharjee D, 2021, IEEE T PATTERN ANAL, V43, P595, DOI 10.1109/TPAMI.2019.2930192
   Boulkenafet Z, 2015, IEEE IMAGE PROC, P2636, DOI 10.1109/ICIP.2015.7351280
   Boulkenafet Z, 2017, IEEE INT CONF AUTOMA, P612, DOI 10.1109/FG.2017.77
   Cai RZ, 2021, IEEE T INF FOREN SEC, V16, P937, DOI 10.1109/TIFS.2020.3026553
   Chingovska I., 2015, FACE RECOGNITION IMA
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Galbally J, 2014, IEEE T IMAGE PROCESS, V23, P710, DOI 10.1109/TIP.2013.2292332
   George A, 2021, IEEE T INF FOREN SEC, V16, P361, DOI 10.1109/TIFS.2020.3013214
   George A, 2020, IEEE T INF FOREN SEC, V15, P42, DOI 10.1109/TIFS.2019.2916652
   Georgescu AL, 2019, 2019 10TH INTERNATIONAL CONFERENCE ON SPEECH TECHNOLOGY AND HUMAN-COMPUTER DIALOGUE (SPED), DOI 10.1109/sped.2019.8906555
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heusch Guillaume, 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science, V2, P399, DOI 10.1109/TBIOM.2020.3010312
   Horn B., 1986, MIT ELECT ENG COMPUT
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang JL, 2022, IEEE T MULTIMEDIA, V24, P1435, DOI 10.1109/TMM.2021.3065230
   Huang JL, 2021, IEEE T MULTIMEDIA, V23, P1654, DOI 10.1109/TMM.2020.3001536
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jourabloo A, 2018, LECT NOTES COMPUT SC, V11217, P297, DOI 10.1007/978-3-030-01261-8_18
   Khosla Prannay, 2020, ADV NEURAL INFORM PR, V33, P18661
   Li HL, 2018, IEEE T INF FOREN SEC, V13, P2639, DOI 10.1109/TIFS.2018.2825949
   Li HL, 2018, IEEE T INF FOREN SEC, V13, P1794, DOI 10.1109/TIFS.2018.2801312
   Li L, 2019, IEEE T INF FOREN SEC, V14, P2246, DOI 10.1109/TIFS.2019.2895212
   Li Z, 2020, INT CONF ACOUST SPEE, P2852, DOI [10.1109/icassp40776.2020.9054420, 10.1109/ICASSP40776.2020.9054420]
   Liu AJ, 2021, IEEE WINT CONF APPL, P1178, DOI 10.1109/WACV48630.2021.00122
   Liu AJ, 2021, IEEE T INF FOREN SEC, V16, P2759, DOI 10.1109/TIFS.2021.3065495
   Liu SQ, 2021, IEEE T INF FOREN SEC, V16, P2683, DOI 10.1109/TIFS.2021.3050060
   Liu SQ, 2016, LECT NOTES COMPUT SC, V9911, P85, DOI 10.1007/978-3-319-46478-7_6
   Liu Y., 2020, EUR C COMP VIS, P406, DOI DOI 10.1007/978-3-030-58523
   Liu YJ, 2019, PROC CVPR IEEE, P4675, DOI 10.1109/CVPR.2019.00481
   Liu YJ, 2018, PROC CVPR IEEE, P389, DOI 10.1109/CVPR.2018.00048
   Pinto A, 2020, IEEE T INF FOREN SEC, V15, P3347, DOI 10.1109/TIFS.2020.2988168
   Quan RJ, 2021, IEEE T IMAGE PROCESS, V30, P3946, DOI 10.1109/TIP.2021.3066912
   Raghavendra R, 2015, IEEE T IMAGE PROCESS, V24, P1060, DOI 10.1109/TIP.2015.2395951
   Sepas-Moghaddam A, 2018, IEEE T INF FOREN SEC, V13, P1696, DOI 10.1109/TIFS.2018.2799427
   Shifeng Zhang, 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science, V2, P182, DOI 10.1109/TBIOM.2020.2973001
   Siegmund D, 2020, P INT C BIOM SPEC IN, P1
   Song JK, 2022, IEEE T MULTIMEDIA, V24, P791, DOI 10.1109/TMM.2021.3059336
   Sun WY, 2020, IEEE T INF FOREN SEC, V15, P3181, DOI 10.1109/TIFS.2020.2985530
   Wang GQ, 2020, PROC CVPR IEEE, P6677, DOI 10.1109/CVPR42600.2020.00671
   Wang GQ, 2021, IEEE T INF FOREN SEC, V16, P56, DOI 10.1109/TIFS.2020.3002390
   Wang ZZ, 2020, PROC CVPR IEEE, P5041, DOI 10.1109/CVPR42600.2020.00509
   Wei Zheng, 2021, IEEE Transactions on Biometrics, Behavior, and Identity Science, V3, P296, DOI 10.1109/TBIOM.2021.3066983
   Wen D, 2015, IEEE T INF FOREN SEC, V10, P746, DOI 10.1109/TIFS.2015.2400395
   Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032
   Wu XJ, 2021, IEEE T INF FOREN SEC, V16, P1440, DOI 10.1109/TIFS.2020.3035879
   Yu ZT, 2021, IEEE T PATTERN ANAL, V43, P3005, DOI 10.1109/TPAMI.2020.3036338
   Yu ZT, 2020, PROC CVPR IEEE, P5294, DOI 10.1109/CVPR42600.2020.00534
   Yu ZT, 2020, IEEE COMPUT SOC CONF, P2766, DOI 10.1109/CVPRW50498.2020.00333
   Yuanhan Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P70, DOI 10.1007/978-3-030-58610-2_5
   Yunpei Jia, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8481, DOI 10.1109/CVPR42600.2020.00851
   Zhan HJ, 2021, IEEE T MULTIMEDIA, V23, P133, DOI 10.1109/TMM.2020.2978669
   Zhang K., 2020, ARXIV PREPRINT ARXIV
   Zhang SF, 2019, PROC CVPR IEEE, P919, DOI 10.1109/CVPR.2019.00101
   Zhang XK, 2022, IEEE T MULTIMEDIA, V24, P1990, DOI 10.1109/TMM.2021.3074807
   Zitong Yu, 2021, IEEE Transactions on Biometrics, Behavior, and Identity Science, V3, P285, DOI 10.1109/TBIOM.2021.3065526
   Zitong Yu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P557, DOI 10.1007/978-3-030-58571-6_33
NR 58
TC 7
Z9 7
U1 7
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 62
EP 76
DI 10.1109/TMM.2021.3121140
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9E8KM
UT WOS:000937028400005
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Liu, KJ
   Lyu, S
   Lu, Y
AF Liu, Kaijun
   Lyu, Shujing
   Lu, Yue
TI Few-Shot Segmentation for Prohibited Items Inspection With Patch-Based
   Self-Supervised Learning and Prototype Reverse Validation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image segmentation; Inspection; X-ray imaging; Prototypes; Task
   analysis; Training; Supervised learning; Few-shot learning; X-ray
   prohibited items; object segmentation; self-supervised learning
AB Prohibited items inspection using X-ray screening is essential for reducing the risk of crime and terrorist attacks. The difficulty in prohibited items inspection lies in accurately detecting prohibited items in complex X-ray images and limited access to X-ray images containing prohibited items. Few-shot segmentation aims at learning with limited examples and assigning a category label to each image pixel. However, current few-shot methods are mostly full-supervised and less robust to the prohibited items categories that did not appear during training process. In this paper, we propose a method for few-shot prohibited items segmentation tasks which utilize unlabeled data and better leverage the representation of input samples during model training process. Specifically, a patch-based self-supervised embedding network is firstly devised as the base learner to learn an abstract representation of the observation from unlabeled samples. Then we apply few-shot learning and generate abstract representation related to prohibited items from support sample within the embedding space, which is followed by obtaining the corresponding class-specific prototype representations via masked average pooling. The distance between each pixel of query sample and prototypes are calculated to predict the label of each pixel. Moreover, prototype reverse validation strategy (PRV) is proposed to further exploit the support representation to assist training. Extensive experiments show that our proposed method outperforms the state-of-the-art by delivering a higher accuracy on automated prohibited items inspection and requiring less labeled samples.
C1 [Liu, Kaijun; Lyu, Shujing; Lu, Yue] East China Normal Univ, Shanghai Key Lab Multidimens Informat Proc, Shanghai 200062, Peoples R China.
C3 East China Normal University
RP Lu, Y (corresponding author), East China Normal Univ, Shanghai Key Lab Multidimens Informat Proc, Shanghai 200062, Peoples R China.
EM 13162333580@163.com; sjlv@cs.ecnu.edu.cn; ylu@cs.ecnu.edu.cn
RI Kaijun, Liu/AAB-1413-2019; lkj, kaijun/HLW-0595-2023
CR Akcay S, 2018, IEEE T INF FOREN SEC, V13, P2203, DOI 10.1109/TIFS.2018.2812196
   Caron M, 2019, IEEE I CONF COMP VIS, P2959, DOI 10.1109/ICCV.2019.00305
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen X, 2021, AUTOPHAGY, V17, P2054, DOI 10.1080/15548627.2020.1810918
   Cheng ZQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1897, DOI 10.1145/3343031.3350898
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Dosovitskiy A, 2014, ADV NEUR IN, V27
   Gidaris S., 2018, P 6 INT C LEARNING R
   Gidaris S, 2019, IEEE I CONF COMP VIS, P8058, DOI 10.1109/ICCV.2019.00815
   Goodfellow I.J., 2015, Nature, V521, P436, DOI DOI 10.1038/NATURE14539
   Gutmann Michael, 2010, P MACHINE LEARNING R, P297, DOI DOI 10.1145/3292500.3330651
   Heitz G, 2010, PROC CVPR IEEE, P2093, DOI 10.1109/CVPR.2010.5539887
   Huang YF, 2021, IEEE T MULTIMEDIA, V23, P176, DOI 10.1109/TMM.2020.2981994
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kechagias-Stamatis O., 2017, P SOC PHOT INSTR ENG, V10432
   Nguyen K, 2019, IEEE I CONF COMP VIS, P622, DOI 10.1109/ICCV.2019.00071
   Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35
   Li X, 2020, IEEE T IMAGE PROCESS, V29, P128, DOI 10.1109/TIP.2019.2930874
   Mery D, 2017, IEEE T SYST MAN CY-S, V47, P682, DOI 10.1109/TSMC.2016.2628381
   Mery D, 2015, J NONDESTRUCT EVAL, V34, DOI 10.1007/s10921-015-0315-7
   Miao CJ, 2019, PROC CVPR IEEE, P2114, DOI 10.1109/CVPR.2019.00222
   Pan XJ, 2020, IEEE T IMAGE PROCESS, V29, P6745, DOI 10.1109/TIP.2020.2993403
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Rakelly K., 2018, P ICLR WORKSH
   Schwaninger A., 2008, Amer. J. Trop. Med. Hyg., V90, P322
   Shaban Z. L. I. E. Amirreza, 2017, P BRIT MACH VIS C, P1, DOI 10
   She DY, 2020, IEEE T MULTIMEDIA, V22, P1358, DOI 10.1109/TMM.2019.2939744
   Simonyan K, 2015, IEEE INT C ICLR
   Skorupski J, 2016, EXPERT SYST APPL, V44, P114, DOI 10.1016/j.eswa.2015.08.032
   Snell J, 2017, ADV NEUR IN, V30
   Tang YX, 2017, IEEE T MULTIMEDIA, V19, P393, DOI 10.1109/TMM.2016.2614862
   Tian ZT, 2022, IEEE T PATTERN ANAL, V44, P1050, DOI 10.1109/TPAMI.2020.3013717
   Unay D, 2010, LECT NOTES COMPUT SC, V5853, P97, DOI 10.1007/978-3-642-11769-5_10
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang PC, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P97, DOI 10.1145/2964284.2967191
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Yona Falinie A. Gaus, 2019, 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA), P420, DOI 10.1109/ICMLA.2019.00079
   Zhang C, 2019, IEEE I CONF COMP VIS, P9586, DOI 10.1109/ICCV.2019.00968
   Zhang C, 2019, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR.2019.00536
   Zhang J, 2014, IEEE COMPUT SOC CONF, P266, DOI 10.1109/CVPRW.2014.48
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P470, DOI 10.1109/TMM.2013.2293424
   Zhang R, 2017, PROC CVPR IEEE, P645, DOI 10.1109/CVPR.2017.76
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhang XL, 2020, IEEE T CYBERNETICS, V50, P3855, DOI 10.1109/TCYB.2020.2992433
   Zhu K, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1019
   Zhu K, 2021, IEEE T MULTIMEDIA, V23, P3726, DOI 10.1109/TMM.2020.3031062
NR 47
TC 5
Z9 5
U1 6
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4455
EP 4463
DI 10.1109/TMM.2022.3176546
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200027
DA 2024-07-18
ER

PT J
AU Liu, S
   Bao, RD
   Zhu, DF
   Huang, SF
   Yan, Q
   Lin, L
   Dong, C
AF Liu, Si
   Bao, Renda
   Zhu, Defa
   Huang, Shaofei
   Yan, Qiong
   Lin, Liang
   Dong, Chao
TI Fine-Grained Face Editing via Personalized Spatial-Aware Affine
   Modulation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Faces; Modulation; Task analysis; Tensors; Training; Generators;
   Lighting; Fine-grained; face editing; generative adversarial networks
AB Fine-grained face editing, as a special case of image translation task, aims at modifying face attributes according to users' preference. Although generative adversarial networks (GANs) have achieved great success in general image translation tasks, these models cannot be directly applied in the face editing problem. Ideal face editing is challenging as it has two special requirements - personalization and spatial-awareness. To address these issues, we propose a novel Personalized Spatial-aware Affine Modulation (PSAM) method based on a general GAN structure. The key idea is to modulate the intermediate features in a personalized and spatial-aware manner, which corresponds to the face editing procedure. Specifically, for personalization, we adopt both the face image and the desired attribute as input to generate the modulation tensors. For spatial-aware, we set these tensors to be of the same size as the input image, allowing pixel-wise modulation. Extensive experiments in four fine-grained face editing tasks, i.e., makeup, expression, illumination and aging, demonstrate the effectiveness of the proposed PSAM method. The synthesis results of PSAM can be further boosted by a new transferable training strategy.
C1 [Liu, Si] Beihang Univ, Inst Artificial Intelligence, Beijing 100083, Peoples R China.
   [Bao, Renda; Zhu, Defa; Huang, Shaofei] Chinese Acad Sci, Inst Adv Technol, Beijing 100045, Peoples R China.
   [Dong, Chao] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shanghai AI Lab, Shenzhen 518055, Peoples R China.
   [Yan, Qiong] SenseTime Grp Ltd, Hong Kong, Peoples R China.
   [Lin, Liang] Sun Yat sen Univ, Guangzhou 510275, Peoples R China.
C3 Beihang University; Chinese Academy of Sciences; Chinese Academy of
   Sciences; Shenzhen Institute of Advanced Technology, CAS; Sun Yat Sen
   University
RP Dong, C (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shanghai AI Lab, Shenzhen 518055, Peoples R China.
EM liusi@buaa.edu.cn; baorenda@iie.ac.cn; zhudefa@iie.ac.cn;
   huangshaofei@iie.ac.cn; sophie.yanqiong@gmail.com; linliang@live.com;
   chao.dong@siat.ac.cn
RI Lin, Liang/IQR-8601-2023
OI Lin, Liang/0000-0003-2248-3755; liu, si/0000-0002-9180-2935; Huang,
   Shaofei/0000-0001-8996-9907
FU National Natural Science Foundation of China [62122010, 61876177];
   National Natural Science Foundation of China; Fundamental Research Funds
   for the Central Universities
FX This work was partly supported in part by National Natural Science
   Foundation of China under Grants 62122010, 61876177, and 61906184, and
   in part by the Fundamental Research Funds for the Central Universities.
   The Associate Editor coordinating the review of this manuscript and
   approving it for publication was Dr.Alexandros(Alexis)Michael Tourapis.
CR Chen CJ, 2013, INT CONF BIOMETR
   Chen CJ, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON IDENTITY, SECURITY AND BEHAVIOR ANALYSIS (ISBA)
   Chen CJ, 2016, INFORM FUSION, V32, P80, DOI 10.1016/j.inffus.2015.09.005
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Dantcheva A., 2012, 2012 IEEE Fifth International Conference On Biometrics: Theory, Applications And Systems (BTAS 2012), P391, DOI 10.1109/BTAS.2012.6374605
   de Vries H, 2017, ADV NEUR IN, V30
   Du SC, 2014, P NATL ACAD SCI USA, V111, pE1454, DOI 10.1073/pnas.1322355111
   Dumoulin V.., 2017, P INT C LEARN REPR I
   Fu Y, 2008, IEEE T MULTIMEDIA, V10, P578, DOI 10.1109/TMM.2008.921847
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Guo D, 2009, PROC CVPR IEEE, P73, DOI 10.1109/CVPRW.2009.5206833
   He D, 2016, ADV NEUR IN, V29
   He ZL, 2018, Arxiv, DOI arXiv:1711.10678
   Hensel M, 2017, ADV NEUR IN, V30
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma D. P., 2014, arXiv
   Li M, 2018, Arxiv, DOI arXiv:1610.05586
   Li TT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P645, DOI 10.1145/3240508.3240618
   Liu MY, 2017, ADV NEUR IN, V30
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Miyato T, 2018, INT C LEARN REPR
   Odena A, 2017, PR MACH LEARN RES, V70
   Perarnau G, 2016, Arxiv, DOI [arXiv:1611.06355, 10.48550/arXiv.1611.06355]
   Reed S, 2016, PR MACH LEARN RES, V48
   Shen W, 2017, PROC CVPR IEEE, P1225, DOI 10.1109/CVPR.2017.135
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang ZW, 2018, PROC CVPR IEEE, P7939, DOI 10.1109/CVPR.2018.00828
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zhang G, 2018, LECT NOTES COMPUT SC, V11210, P422, DOI 10.1007/978-3-030-01231-1_26
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang J., 2018, P 26 ACM INT C MULT
   Zhang ZF, 2017, PROC CVPR IEEE, P4352, DOI 10.1109/CVPR.2017.463
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 38
TC 2
Z9 2
U1 2
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4213
EP 4224
DI 10.1109/TMM.2022.3172548
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200011
DA 2024-07-18
ER

PT J
AU Roy, D
   Li, YY
   Jian, T
   Tian, P
   Chowdhury, K
   Ioannidis, S
AF Roy, Debashri
   Li, Yuanyuan
   Jian, Tong
   Tian, Peng
   Chowdhury, Kaushik
   Ioannidis, Stratis
TI Multi-Modality Sensing and Data Fusion for Multi-Vehicle Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Vehicle detection; tracking; multimodal data; fusion; latent embeddings;
   image; seismic; acoustic; radar
ID CHALLENGES; TRACKING
AB With the recent surge in autonomous driving vehicles, the need for accurate vehicle detection and tracking is critical now more than ever. Detecting vehicles from visual sensors fails in non-line-of-sight (NLOS) settings. This can be compensated by the inclusion of other modalities in a multi-domain sensing environment. We propose several deep learning based frameworks for fusing different modalities (image, radar, acoustic, seismic) through the exploitation of complementary latent embeddings, incorporating multiple state-of-the-art fusion strategies. Our proposed fusion frameworks considerably outperform unimodal detection. Moreover, fusion between image and non-image modalities improves vehicle tracking and detection under NLOS conditions. We validate our models on the real-world multimodal ESCAPE dataset, showing 33.16% improvement in vehicle detection by fusion (over visual inference alone) over test scenarios with 30-42% NLOS conditions. To demonstrate how well our framework generalizes, we also validate our models on the multimodal NuScene dataset, showing similar to 22% improvement over competing methods.
C1 [Roy, Debashri] Northeastern Univ, Dept Elect, Comp Engn, Boston, MA 02115 USA.
C3 Northeastern University
RP Roy, D (corresponding author), Northeastern Univ, Dept Elect, Comp Engn, Boston, MA 02115 USA.
EM d.roy@northeastern.edu; yuanyuanli@ece.neu.edu; jain@ece.neu.edu;
   pengtian@ece.neu.edu; krc@ece.neu.edu; ioannidis@ece.eu.edu
OI Ioannidis, Stratis/0000-0001-8355-4751; Roy,
   Debashri/0000-0002-9955-7137; Chowdhury, Kaushik/0000-0002-3570-2622
FU Air Force Research Laboratory; US National Science Foundation
   [CNS-2112471]; Roux Institute; Harold Alfond Foundation
FX This work was supported by the Air Force Research Laboratory, the US
   National Science Foundation Grant CNS-2112471, Roux Institute andthe
   Harold Alfond Foundation.
CR Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Beichen Zhang, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P1112, DOI 10.1145/3394171.3413885
   Blasch E., 2021, 2021 IEEE AEROSPACE, P1
   Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164
   Chadwick S, 2019, IEEE INT CONF ROBOT, P8311, DOI [10.1109/icra.2019.8794312, 10.1109/ICRA.2019.8794312]
   Chavez-Garcia RO, 2016, IEEE T INTELL TRANSP, V17, P525, DOI 10.1109/TITS.2015.2479925
   Chen HL, 2020, INT CONF ACOUST SPEE, P721, DOI [10.1109/ICASSP40776.2020.9053174, 10.1109/icassp40776.2020.9053174]
   Chen HH, 2019, IEEE INTERNET THINGS, V6, P7570, DOI 10.1109/JIOT.2019.2901093
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Feng D, 2021, IEEE T INTELL TRANSP, V22, P1341, DOI 10.1109/TITS.2020.2972974
   Fu YQ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1142, DOI 10.1145/3394171.3413502
   Garagic D., 2018, P IEEE AER C, P1, DOI 10.1109/AERO.
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jayakumar S. M, 2020, ICLR
   Jha H, 2019, 2019 6TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND INTEGRATED NETWORKS (SPIN), P590, DOI [10.1109/spin.2019.8711717, 10.1109/SPIN.2019.8711717]
   Kim KE, 2017, INT C CONTR AUTOMAT, P1075, DOI 10.23919/ICCAS.2017.8204375
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lahat D, 2015, P IEEE, V103, P1449, DOI 10.1109/JPROC.2015.2460697
   Lahat D, 2014, EUR SIGNAL PR CONF, P101
   Lekic V, 2019, COMPUT VIS IMAGE UND, V184, P1, DOI 10.1016/j.cviu.2019.04.002
   Li Y, 2020, IEEE SIGNAL PROC MAG, V37, P50, DOI 10.1109/MSP.2020.2973615
   Lin Q, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1094, DOI 10.1145/3394171.3413982
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu YH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1103, DOI 10.1145/3394171.3413984
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Mimouna A, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9040560
   Müller FD, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17020271
   Nabati R, 2021, IEEE WINT CONF APPL, P1526, DOI 10.1109/WACV48630.2021.00157
   Niu RX, 2018, AEROSP CONF PROC
   Nobis F., 2019, 2019 Sensor Data Fusion: Trends, Solutions, Applications (SDF), P1
   P. P. L, 2021, P NEUR DAT BECHM TRA
   Pan Q, 2007, PATTERN RECOGN LETT, V28, P2419, DOI 10.1016/j.patrec.2007.08.009
   Perez E, 2018, AAAI CONF ARTIF INTE, P3942
   Pérez-Rúa JM, 2019, PROC CVPR IEEE, P6959, DOI 10.1109/CVPR.2019.00713
   Petrovskaya A, 2009, AUTON ROBOT, V26, P123, DOI 10.1007/s10514-009-9115-1
   PwC, 2014, SENS FUT INT THINGS
   Qiu H, 2018, 2018 IEEE/ACM THIRD INTERNATIONAL CONFERENCE ON INTERNET-OF-THINGS DESIGN AND IMPLEMENTATION (IOTDI 2020), P48, DOI 10.1109/IoTDI.2018.00015
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Roy Debashri, 2019, 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA), P89, DOI 10.1109/ICMLA.2019.00023
   Roy D, 2020, IEEE T COGN COMMUN, V6, P783, DOI 10.1109/TCCN.2019.2948919
   Scalabel Project, 2021, SCAL OP SOURC WEB AN
   Semedo D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1152, DOI 10.1145/3394171.3413540
   Shen D, 2018, J ALGORITHMS COMPUT, V12, P311, DOI 10.1177/1748301818791507
   Shenoy D, 2018, P IEEE AER C, P1
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Stanislas L., 2015, P AUSTR C ROB AUT 20, V1, P10
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Vakil A, 2020, AEROSP CONF PROC, DOI 10.1109/aero47225.2020.9172254
   Vakil A, 2021, IEEE AERO EL SYS MAG, V36, P44, DOI 10.1109/MAES.2020.3006410
   Yi Y, 2020, IEEE T MULTIMEDIA, V22, P2454, DOI 10.1109/TMM.2019.2955300
   Yuan YT, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1085, DOI 10.1145/3394171.3413908
   Zhang XD, 2021, IEEE T MULTIMEDIA, V23, P611, DOI 10.1109/TMM.2020.2985526
   Zhao WD, 2018, IEEE T MULTIMEDIA, V20, P866, DOI 10.1109/TMM.2017.2760100
   Zulch P, 2019, AEROSP CONF PROC
NR 60
TC 14
Z9 14
U1 11
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2280
EP 2295
DI 10.1109/TMM.2022.3145663
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100053
DA 2024-07-18
ER

PT J
AU Tao, ZL
   Liu, XH
   Xia, YW
   Wang, X
   Yang, LF
   Huang, XL
   Chua, TS
AF Tao, Zhulin
   Liu, Xiaohao
   Xia, Yewei
   Wang, Xiang
   Yang, Lifang
   Huang, Xianglin
   Chua, Tat-Seng
TI Self-Supervised Learning for Multimedia Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimedia recommendation; self-supervised learning; graph neural
   network; micro-videos
AB Learning representations for multimedia content is critical for multimedia recommendation. Current representation learning methods roughly fall into two groups: (1) using the historical interactions to create ID embeddings of users and items, and (2) treating multi-modal data as the side information of items to enrich their ID embeddings. Each user-item interaction offers the supervisory signal to optimize the representation learning by the traditional supervised learning paradigm. Due to the overlook of the multi-modal patterns ($e.g.$, co-occurrence of visual, acoustic, textual features in micro-videos a user saw before, and her behavioral features) hidden in the data, these methods are insufficient to create powerful representations and obtain satisfactory recommendation accuracy. To capture multi-modal patterns in the data itself, we go beyond the supervised learning paradigm, and incorporate the idea of self-supervised learning (SSL) into multimedia recommendation. Specifically, SSL consists of two components: (1) data augmentation upon multi-modal contents, where we design three operators - feature dropout (FD), feature masking (FM), feature fine and coarse spaces (FAC) - to generate multiple views of individual items; and (2) contrastive learning, which differentiates the views of an item from the others' to distill additional supervisory signals. Clearly, SSL enables us to explore and exhibit the underlying relations among modalities, thereby resulting in powerful representations. We denote the generic framework by Self-supervised Learning-guided Multimedia Recommendation (SLMRec). Extensive experiments are performed on three real-world datasets, showing that SLMRec achieves significant improvements over several state-of-the-art baselines like LightGCN [1], MMGCN [2]. Further analysis shows how SSL affects recommendation performance.
C1 [Tao, Zhulin; Liu, Xiaohao; Xia, Yewei; Yang, Lifang; Huang, Xianglin] Commun Univ China, State Key Lab Media Convergence & Commun, Beijing 100024, Peoples R China.
   [Wang, Xiang] Univ Sci & Technol China, Minist Culture & Tourism, CCCD Key Lab, Hefei 230026, Anhui, Peoples R China.
   [Chua, Tat-Seng] Natl Univ Singapore, Kent Ridge 117543, Singapore.
C3 Communication University of China; Chinese Academy of Sciences;
   University of Science & Technology of China, CAS; National University of
   Singapore
RP Wang, X (corresponding author), Univ Sci & Technol China, Minist Culture & Tourism, CCCD Key Lab, Hefei 230026, Anhui, Peoples R China.
EM taozhulin@gmail.com; liuxiaohao@cuc.edu.cn; yeweixia@cuc.edu.cn;
   xiangwang@u.nus.edu; yanglifang@cuc.edu.cn; huangxl@cuc.edu.cn;
   chuats@comp.nus.edu.sg
RI Liu, Xiaohao/IAQ-0426-2023
OI tao, zhulin/0000-0001-9011-8464; Liu, Xiaohao/0000-0001-6037-8580
FU National Key Research and Development Program of China [2020YFB1406800];
   Fundamental Research Funds for the Central Universities [CUC22GZ003,
   CUC22GZ007, CUC22GZ035]; GH-fund B [20220202, ghfund202202023853]
FX This work was supported in part by the National KeyResearch and
   Development Program of China under Grant 2020YFB1406800,in part by the
   Fundamental Research Funds for the Central Universities underGrants
   CUC22GZ003, CUC22GZ007, and CUC22GZ035, and in part by GH-fund B under
   Grants 20220202 and ghfund202202023853. The Associate Editorcoordinating
   the review of this manuscript and approving it for publication wasDr.
   Wen-Huang Cheng
CR Alayrac J.-B., 2020, NeurIPS, V33, P25
   Barkan O, 2019, RECSYS 2019: 13TH ACM CONFERENCE ON RECOMMENDER SYSTEMS, P228, DOI 10.1145/3298689.3347038
   Chen JY, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P335, DOI 10.1145/3077136.3080797
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen XS, 2021, IEEE T MULTIMEDIA, V23, P484, DOI 10.1109/TMM.2020.2978618
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Guan JW, 2023, IEEE T KNOWL DATA EN, V35, P2379, DOI 10.1109/TKDE.2021.3117686
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He RN, 2016, AAAI CONF ARTIF INTE, P144
   He XN, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P639, DOI 10.1145/3397271.3401063
   He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569
   Hu W., 2020, ICLR
   Hu ZN, 2019, Arxiv, DOI arXiv:1905.13728
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Koren Y., 2008, P 14 ACM SIGKDD INT, P426
   Li Y., 2016, ICLR, P1, DOI DOI 10.48550/ARXIV.1511.05493
   Li Y. C., 2022, P IEEE CVF C COMP VI, P2928
   Liu AA, 2021, IEEE T MULTIMEDIA, V23, P4515, DOI 10.1109/TMM.2020.3043084
   Qiu JZ, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1150, DOI 10.1145/3394486.3403168
   Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127
   Rendle S., 2009, P 25 C UNCERTAINTY A, P452
   Tao ZL, 2020, INFORM PROCESS MANAG, V57, DOI 10.1016/j.ipm.2019.102076
   Tao ZL, 2020, INFORM PROCESS MANAG, V57, DOI 10.1016/j.ipm.2020.102277
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   Velickovic Petar, 2018, INT C LEARN REPR
   Wang X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P165, DOI 10.1145/3331184.3331267
   Wang X, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P185, DOI 10.1145/3077136.3080771
   Wei YW, 2022, IEEE T MULTIMEDIA, V24, P2701, DOI 10.1109/TMM.2021.3088307
   Wu JC, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P726, DOI 10.1145/3404835.3462862
   Wu SW, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3535101
   Wu Y., 2022, P INT C LEARN REPR, P2928
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xie X, 2021, Arxiv, DOI arXiv:2010.14395
   Xin X, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P931, DOI 10.1145/3397271.3401147
   Xu N, 2020, IEEE T MULTIMEDIA, V22, P1372, DOI 10.1109/TMM.2019.2941820
   Xue F, 2020, IEEE T MULTIMEDIA, V22, P2098, DOI 10.1109/TMM.2019.2951194
   Xue F, 2019, ACM T INFORM SYST, V37, DOI 10.1145/3314578
   Yao TS, 2021, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, CIKM 2021, P4321, DOI 10.1145/3459637.3481952
   You Yuning, 2020, Proc Mach Learn Res, V119, P10871
   Yuan D, 2021, IEEE T IMAGE PROCESS, V30, P976, DOI 10.1109/TIP.2020.3037518
   Zahálka J, 2015, IEEE T MULTIMEDIA, V17, P2235, DOI 10.1109/TMM.2015.2480007
   Zhou K, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P1893, DOI 10.1145/3340531.3411954
NR 43
TC 13
Z9 13
U1 5
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5107
EP 5116
DI 10.1109/TMM.2022.3187556
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300035
DA 2024-07-18
ER

PT J
AU Tu, RC
   Mao, XL
   Lin, QH
   Ji, WJ
   Qin, WZ
   Wei, W
   Huang, HY
AF Tu, Rong-Cheng
   Mao, Xian-Ling
   Lin, Qinghong
   Ji, Wenjin
   Qin, Weize
   Wei, Wei
   Huang, Heyan
TI Unsupervised Cross-Modal Hashing via Semantic Text Mining
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal retrieval; deep supervised hashing; semantic text mining;
   self-redefined-similarity loss
AB Cross-modal hashing has been widely used in multimedia retrieval tasks due to its fast retrieval speed and low storage cost. Recently, many deep unsupervised cross-modal hashing methods have been proposed to deal the unlabeled datasets. These methods usually construct an instance similarity matrix by fusing the image and text modality-specific similarity matrices as the guiding information to train the hashing networks. However, most of them directly use cosine similarities between the bag-of-words (BoW) vectors of text datapoints to define the text modality-specific similarity matrix, which fails to mine the semantic similarity information contained in the text modal datapoints and leads to the poor quality of the instance similarity matrix. To tackle the aforementioned problem, in this paper, we propose a novel Unsupervised Cross-modal Hashing via Semantic Text Mining, called UCHSTM. Specifically, UCHSTM first mines the correlations between the words of text datapoints. Then, UCHSTM constructs the text modality-specific similarity matrix for the training instances based on the mined correlations between their words. Next, UCHSTM fuses the image and text modality-specific similarity matrices as the final instance similarity matrix to guide the training of hashing model. Furthermore, during the process of training the hashing networks, a novel self-redefined-similarity loss is proposed to further correct some wrong defined similarities in the constructed instance similarity matrix, thereby further enhancing the retrieval performance. Extensive experiments on two widely used datasets show that the proposed UCHSTM outperforms state-of-the-art baselines on cross-modal retrieval tasks.
C1 [Tu, Rong-Cheng; Mao, Xian-Ling; Ji, Wenjin; Huang, Heyan] Beijing Inst Technol, Dept Comp Sci & Technol, Beijing 100081, Peoples R China.
   [Lin, Qinghong] Shenzhen Univ, Sch Comp Sci & Software Engn, Shenzhen 518052, Peoples R China.
   [Qin, Weize] Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Wei, Wei] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
C3 Beijing Institute of Technology; Shenzhen University; Chinese Academy of
   Sciences; Institute of Computing Technology, CAS; Huazhong University of
   Science & Technology
RP Mao, XL (corresponding author), Beijing Inst Technol, Dept Comp Sci & Technol, Beijing 100081, Peoples R China.
EM 3120205492@bit.edu.cn; maoxl@bit.edu.cn; linqinghong@email.szu.edu.cn;
   jwjwinky@163.com; qinweize@ict.ac.cn; weiw@hust.edu.cn; hhy63@bit.edu.cn
OI Wei, Wei/0000-0003-4488-0102
FU National Key Ramp;D Plan
FX No Statement Available
CR Cao Y, 2018, LECT NOTES COMPUT SC, V11205, P207, DOI 10.1007/978-3-030-01246-5_13
   Cao Y, 2017, AAAI CONF ARTIF INTE, P3974
   Cao Y, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P197, DOI 10.1145/2911996.2912000
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Da C, 2019, IEEE T PATTERN ANAL, V41, P2660, DOI 10.1109/TPAMI.2018.2867866
   Dejie Yang, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P44, DOI 10.1145/3372278.3390673
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Glorot X., 2011, JMLR Proceedings, V15, P315, DOI DOI 10.1002/ECS2.1832
   Hu HT, 2020, PROC CVPR IEEE, P3120, DOI 10.1109/CVPR42600.2020.00319
   Hua Y, 2016, IEEE T MULTIMEDIA, V18, P1201, DOI 10.1109/TMM.2016.2535864
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jiang QY, 2019, IEEE T IMAGE PROCESS, V28, P3490, DOI 10.1109/TIP.2019.2897944
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Jiang Z, 2020, IEEE T MULTIMEDIA, V22, P540, DOI 10.1109/TMM.2019.2929957
   Jin L, 2023, IEEE T NEUR NET LEAR, V34, P1838, DOI 10.1109/TNNLS.2020.2997020
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Lin QB, 2021, IEEE T MULTIMEDIA, V23, P550, DOI 10.1109/TMM.2020.2984081
   Lin QB, 2020, NEUROCOMPUTING, V396, P113, DOI 10.1016/j.neucom.2020.02.043
   Liong VE, 2017, IEEE I CONF COMP VIS, P4097, DOI 10.1109/ICCV.2017.439
   Liu AA, 2022, IEEE T CIRC SYST VID, V32, P3685, DOI 10.1109/TCSVT.2021.3107035
   Liu S, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1379, DOI 10.1145/3397271.3401086
   Liu X, 2022, IEEE T NEUR NET LEAR, V33, P6306, DOI 10.1109/TNNLS.2021.3076684
   Liu XB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1662, DOI 10.1145/3240508.3240683
   Luo X, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2518
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Radford Alec, 2018, IMPROVING LANGUAGE U, DOI DOI 10.18653/V1/N18-1202
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Shi YF, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4767
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Su SP, 2019, IEEE I CONF COMP VIS, P3027, DOI 10.1109/ICCV.2019.00312
   Tang HY, 2022, IEEE T MULTIMEDIA, V24, P1338, DOI 10.1109/TMM.2021.3063631
   Tu RC, 2023, IEEE T KNOWL DATA EN, V35, P6798, DOI 10.1109/TKDE.2022.3187023
   Tu RC, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P743
   Tu RC, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3466
   Wang D, 2018, IEEE T CIRC SYST VID, V28, P2703, DOI 10.1109/TCSVT.2017.2723302
   Wang D, 2019, IEEE T PATTERN ANAL, V41, P2466, DOI 10.1109/TPAMI.2018.2861000
   Wang D, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3890
   Wang W, 2021, IEEE T MULTIMEDIA, V23, P2386, DOI 10.1109/TMM.2020.3011288
   Wang XZ, 2020, NEUROCOMPUTING, V400, P255, DOI 10.1016/j.neucom.2020.03.019
   Wu YL, 2021, IEEE T MULTIMEDIA, V23, P559, DOI 10.1109/TMM.2020.2985540
   Xu N, 2020, IEEE T MULTIMEDIA, V22, P1372, DOI 10.1109/TMM.2019.2941820
   Xu RQ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P982
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Yu J, 2021, AAAI CONF ARTIF INTE, V35, P4626
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang J, 2018, AAAI CONF ARTIF INTE, P539
   Zhang PF, 2022, IEEE T MULTIMEDIA, V24, P466, DOI 10.1109/TMM.2021.3053766
   Zhang PF, 2021, WORLD WIDE WEB, V24, P563, DOI 10.1007/s11280-020-00859-y
   Zhou JL, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P415
   Zhu L, 2023, IEEE T KNOWL DATA EN, V35, P8838, DOI 10.1109/TKDE.2022.3218656
   Zhuang YT, 2008, IEEE T MULTIMEDIA, V10, P221, DOI 10.1109/TMM.2007.911822
NR 55
TC 8
Z9 8
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8946
EP 8957
DI 10.1109/TMM.2023.3243608
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP5R1
UT WOS:001133278300011
DA 2024-07-18
ER

PT J
AU Wan, C
   Huang, FJ
   Zhao, XF
AF Wan, Chen
   Huang, Fangjun
   Zhao, Xianfeng
TI Average Gradient-Based Adversarial Attack
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adversarial attack; black-box attack; dynamic set of adversarial
   examples; transferability
AB Deep neural networks (DNNs) are vulnerable to adversarial attacks which can fool the classifiers by adding small perturbations to the original example. The added perturbations in most existing attacks are mainly determined by the gradient of the loss function with respect to the current example. In this paper, a new average gradient-based adversarial attack is proposed. In our proposed method, via utilizing the gradient of each iteration in the past, a dynamic set of adversarial examples is constructed first in each iteration. Then, according to the gradient of the loss function with respect to all the examples in the constructed dynamic set and the current adversarial example, the average gradient can be calculated, which is used to determine the added perturbations. Different from the existing adversarial attacks, the proposed average gradient-based attack optimizes the added perturbations through a dynamic set of adversarial examples, where the size of the dynamic set increases with the number of iterations. Our proposed method possesses good extensibility and can be integrated into most existing gradient-based attacks. Extensive experiments demonstrate that, compared with the state-of-the-art gradient-based adversarial attacks, the proposed attack can achieve higher attack success rates and exhibit better transferability, which is helpful to evaluate the robustness of the network and the effectiveness of the defense method.
C1 [Wan, Chen] Sun Yat Sen Univ, Guangdong Prov Key Lab Informat Secur Technol, Guangzhou 510006, Peoples R China.
   [Wan, Chen] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Huang, Fangjun] Sun Yat Sen Univ, Guangdong Prov Key Lab Informat Secur Technol, Shenzhen 518107, Peoples R China.
   [Huang, Fangjun] Sun Yat Sen Univ, Sch Cyber Sci & Technol, Shenzhen 518107, Peoples R China.
   [Zhao, Xianfeng] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing 100195, Peoples R China.
   [Zhao, Xianfeng] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100195, Peoples R China.
C3 Sun Yat Sen University; Sun Yat Sen University; Sun Yat Sen University;
   Sun Yat Sen University; Chinese Academy of Sciences; Institute of
   Information Engineering, CAS; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS
RP Huang, FJ (corresponding author), Sun Yat Sen Univ, Guangdong Prov Key Lab Informat Secur Technol, Shenzhen 518107, Peoples R China.
EM wanchen18@outlook.com; huangfj@mail.sysu.edu.cn; zhaoxianfeng@iie.ac.cn
RI Zhao, Xianfeng/AAE-7278-2021
OI Zhao, Xianfeng/0000-0002-5617-8399; , Chen/0000-0002-3965-0030
FU National Natural Science Foundation of China
FX No Statement Available
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], 2014, P INT C LEARN REPR
   Bengio S, 2016, ARXIV
   Biggio B, 2018, PATTERN RECOGN, V84, P317, DOI 10.1016/j.patcog.2018.07.023
   Cai QZ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3740
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chen PY, 2017, P 10 ACM WORKSH ART, P15, DOI [10.1145/3128572.3140448, DOI 10.1145/3128572.3140448]
   Cohen Jeremy, 2019, INT C MACH LEARN PML, P1310
   Deng C, 2018, PATTERN RECOGN, V77, P306, DOI 10.1016/j.patcog.2017.10.007
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong X., 2020, P IEEE C COMPUTER VI, P12895
   Dong YP, 2020, PROC CVPR IEEE, P318, DOI 10.1109/CVPR42600.2020.00040
   Dong YP, 2019, PROC CVPR IEEE, P4307, DOI 10.1109/CVPR.2019.00444
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Goodfellow I.J., 2015, PROC 3 INT C LEARN R
   Guo C, 2019, PR MACH LEARN RES, V97
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He ZY, 2022, COMPUT SECUR, V118, DOI 10.1016/j.cose.2022.102720
   Jia XJ, 2022, PROC CVPR IEEE, P13388, DOI 10.1109/CVPR52688.2022.01304
   Jia XJ, 2019, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2019.00624
   Junhua Zou, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P563, DOI 10.1007/978-3-030-58542-6_34
   Li YW, 2020, AAAI CONF ARTIF INTE, V34, P11458
   Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191
   Lin J., 2020, P INT C LEARN REPR
   Liu JY, 2019, PROC CVPR IEEE, P4820, DOI 10.1109/CVPR.2019.00496
   Liu Y., 2017, PROC INT C LEARN REP
   Liu ZH, 2019, PROC CVPR IEEE, P860, DOI 10.1109/CVPR.2019.00095
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Madry A., 2018, ARXIV
   Maho T, 2021, PROC CVPR IEEE, P10425, DOI 10.1109/CVPR46437.2021.01029
   Mustafa A, 2020, IEEE T IMAGE PROCESS, V29, P1711, DOI 10.1109/TIP.2019.2940533
   Naseer M, 2020, PROC CVPR IEEE, P259, DOI 10.1109/CVPR42600.2020.00034
   Nesterov Yu. E., 1983, Doklady Akademii Nauk SSSR, V269, P543
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tramonti F, 2019, PSYCHOL HEALTH MED, V24, P27, DOI 10.1080/13548506.2018.1510131
   Wan C, 2021, AAAI CONF ARTIF INTE, V35, P10033
   Wang JW, 2022, IEEE T MULTIMEDIA, V24, P230, DOI 10.1109/TMM.2021.3050057
   Wang X., 2021, P 32 BRIT MACH VIS C, P1
   Wang XS, 2021, PROC CVPR IEEE, P1924, DOI 10.1109/CVPR46437.2021.00196
   Wang XS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16138, DOI 10.1109/ICCV48922.2021.01585
   Wu D., 2020, PROC INT C LEARN REP
   Wu L, 2018, Arxiv, DOI arXiv:1802.09707
   Wu WB, 2021, PROC CVPR IEEE, P9020, DOI 10.1109/CVPR46437.2021.00891
   Xiao CW, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3905
   Xiao ZH, 2021, PROC CVPR IEEE, P11840, DOI 10.1109/CVPR46437.2021.01167
   Xie CY, 2018, IEEE PHOTONICS J, V10, DOI 10.1109/JPHOT.2018.2809731
   Xie CH, 2019, PROC CVPR IEEE, P2725, DOI 10.1109/CVPR.2019.00284
   Xie CH, 2019, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2019.00059
   Yu XY, 2019, IEEE T NEUR NET LEAR, V30, P2805, DOI 10.1109/TNNLS.2018.2886017
   Yuan HJ, 2023, IEEE T MULTIMEDIA, V25, P203, DOI 10.1109/TMM.2021.3124083
   Zhang SH, 2023, IEEE T MULTIMEDIA, V25, P4296, DOI 10.1109/TMM.2022.3173533
   Zhang SD, 2021, IEEE T IMAGE PROCESS, V30, P6117, DOI 10.1109/TIP.2021.3092582
NR 54
TC 5
Z9 5
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9572
EP 9585
DI 10.1109/TMM.2023.3255742
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200016
DA 2024-07-18
ER

PT J
AU Wang, CW
   Xu, RT
   Xu, SB
   Meng, WL
   Zhang, XP
AF Wang, Changwei
   Xu, Rongtao
   Xu, Shibiao
   Meng, Weiliang
   Zhang, Xiaopeng
TI CNDesc: Cross Normalization for Local Descriptors Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Visualization; Task analysis; Three-dimensional
   displays; Standards; Optimization; Training; Local descriptors; cross
   normalization; efficient feature reuse backbone; image-level
   distribution consistent loss
AB For a long time, the local descriptors learning benefited from the use of L2 normalization, which projects the descriptor space onto the hypersphere. However, there is no free lunch in the world. Although hypersphere description space stabilizes the optimization and improves the repeatability of the descriptors, it causes the descriptors to have a denser distribution, which reduces the discrimination between descriptors and leads to some incorrect matches. To alleviate this problem, we propose the learnable cross normalization technology as an alternative to L2 normalization, which can achieve a consistent improvement in several of the current popular local descriptors. In addition, we propose an ER-Backbone that can efficiently reuse features in descriptors extraction and an IDC Loss that can provide an image-level description space distribution consistency constraint to further stimulate the performance of the local descriptors. Based on the above innovations, we provide a novel local descriptors extraction method named CNDesc. We perform experiments on image matching, homography estimation, 3D reconstruction, and visual localization tasks, and the results demonstrate that our CNDesc surpasses the current state-of-the-art local descriptors. Our code is available at https:// github.com/ vignywang/ CNDesc.
C1 [Wang, Changwei; Xu, Rongtao; Zhang, Xiaopeng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Wang, Changwei; Xu, Rongtao; Zhang, Xiaopeng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 101408, Peoples R China.
   [Xu, Shibiao] Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing 100876, Peoples R China.
   [Meng, Weiliang] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
   [Meng, Weiliang] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 101408, Peoples R China.
   [Meng, Weiliang] Zhejiang Lab, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Beijing University of Posts & Telecommunications; Chinese Academy of
   Sciences; Institute of Automation, CAS; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS; Zhejiang Laboratory
RP Xu, SB (corresponding author), Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing 100876, Peoples R China.; Meng, WL (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
EM wangchangwei2019@ia.ac.cn; xurongtao2019@ia.ac.cn;
   shibiao.xu@nlpr.ia.ac.cn; weiliang.meng@ia.ac.cn;
   xiaopeng.zhang@ia.ac.cn
OI Xu, Rongtao/0000-0003-4619-9679; wang, changwei/0000-0001-8259-7717;
   meng, wei liang/0000-0002-3221-4981
FU National Natural Science Foundation of China [U21A20515, 61972459,
   61971418, U2003109, 62171321, 62071157, 62162044]; Open Research Fund of
   Key Laboratory of Space Utilization, Chinese Academy of Sciences
   [LSU-KFJJ-2021-05]; OpenResearch Projects of Zhejiang Lab [2021KE0AB07];
   Open Projects Program of National Laboratory of Pattern Recognition
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants U21A20515, 61972459, 61971418,
   U2003109, 62171321, 62071157, and 62162044, in part by the Open Research
   Fund of Key Laboratory of Space Utilization, Chinese Academy of Sciences
   underGrant LSU-KFJJ-2021-05, in part by theOpenResearch Projects of
   Zhejiang Lab under Grant 2021KE0AB07, and in part by the Open Projects
   Program of National Laboratory of Pattern Recognition.
CR Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   ARORA S, 2018, THEORETICAL ANALYSIS
   Ba J. L., 2016, LAYER NORMALIZATION, DOI DOI 10.48550/ARXIV.1607.06450
   Bai C, 2021, IEEE T MULTIMEDIA, V23, P2199, DOI 10.1109/TMM.2021.3065578
   BALNTAS V, 2016, P BRIT MACH VIS C BM, V1
   Balntas V, 2020, IEEE T PATTERN ANAL, V42, P2825, DOI 10.1109/TPAMI.2019.2915233
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen W, 2022, IEEE T MULTIMEDIA, V24, P1844, DOI 10.1109/TMM.2021.3073279
   Choy C. B., 2016, ADV NEURAL INFORM PR, P2414, DOI DOI 10.5555/3157096.3157366
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Dong JM, 2015, PROC CVPR IEEE, P5097, DOI 10.1109/CVPR.2015.7299145
   Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828
   Fan B, 2021, IEEE T MULTIMEDIA, V23, P2770, DOI 10.1109/TMM.2020.3016122
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He K, 2018, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2018.00069
   Hu PP, 2022, IEEE T MULTIMEDIA, V24, P2139, DOI 10.1109/TMM.2021.3076340
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   LI B, 2019, ADV NEURAL INF PROCE, V32, P1590
   Li ZQ, 2018, PROC CVPR IEEE, P2041, DOI 10.1109/CVPR.2018.00218
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu R, 2022, MULTIMEDIA SYST, V28, P445, DOI 10.1007/s00530-020-00745-7
   Liu X, 2021, POLYM REV, V61, P1, DOI 10.1080/15583724.2020.1723022
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo ZX, 2020, PROC CVPR IEEE, P6588, DOI 10.1109/CVPR42600.2020.00662
   Luo ZX, 2019, PROC CVPR IEEE, P2522, DOI 10.1109/CVPR.2019.00263
   Melekhov I, 2021, INT CONF 3D VISION, P1144, DOI 10.1109/3DV53792.2021.00122
   Mishchuk A., 2017, P ADV NEURAL INFORM, P4826
   Mishkin D, 2018, LECT NOTES COMPUT SC, V11213, P287, DOI 10.1007/978-3-030-01240-3_18
   Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374
   Qianqian Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P757, DOI 10.1007/978-3-030-58452-8_44
   Revaud J., 2019, P 33 INT C NEUR INF, p12 414, DOI DOI 10.5555/3454287.3455400
   Sattler T, 2018, PROC CVPR IEEE, P8601, DOI 10.1109/CVPR.2018.00897
   Sattler T, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.76
   Schönberger JL, 2017, PROC CVPR IEEE, P6959, DOI 10.1109/CVPR.2017.736
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Simo-Serra E, 2015, IEEE I CONF COMP VIS, P118, DOI 10.1109/ICCV.2015.22
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song Xue, 2021, IEEE T MULTIMEDIA
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Taira H, 2018, PROC CVPR IEEE, P7199, DOI 10.1109/CVPR.2018.00752
   Tan WM, 2016, IEEE T MULTIMEDIA, V18, P128, DOI 10.1109/TMM.2015.2500727
   Tian Y., 2020, Advances in Neural Information Processing Systems, V33, P7401
   Tian YR, 2019, PROC CVPR IEEE, P11008, DOI 10.1109/CVPR.2019.01127
   Tian YR, 2017, PROC CVPR IEEE, P6128, DOI 10.1109/CVPR.2017.649
   Ulyanov Dmitry, 2016, arXiv
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1109/CSTIC.2018.8369274, 10.1007/s11263-019-01198-w]
   Yan C, 2022, IEEE T MULTIMEDIA, V24, P1665, DOI 10.1109/TMM.2021.3069562
   Yang X, 2021, IEEE T MULTIMEDIA, V23, P4208, DOI 10.1109/TMM.2020.3038323
   Zhang Haoyuan, 2021, IEEE Transactions on Multimedia
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
NR 58
TC 14
Z9 14
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3989
EP 4001
DI 10.1109/TMM.2022.3169331
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500033
DA 2024-07-18
ER

PT J
AU Wang, XQ
   Xiong, J
   Lin, WS
AF Wang, Xiaoqi
   Xiong, Jian
   Lin, Weisi
TI Visual Interaction Perceptual Network for Blind Image Quality Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distortion; Visualization; Semantics; Image quality; Nonlinear
   distortion; Training; Feature extraction; Blind image quality
   assessment; visual masking effects; convolutional neural networks;
   self-attention
ID MASKING; VISIBILITY; LUMINANCE
AB In observing images, the perception of the human visual system (HVS) is affected by both image contents and distortions. Obviously, the visual quality of the same image varies under different distortion types and intensities. Furthermore, the visual masking effects reveal that image content and distortion have a visual interaction, where the HVS presents different visibility of the identical distortion for different image contents. Based upon this, we propose a visual interaction perceptual network that can perceive both content and distortion of an image. The proposed model consists of three sub-modules: content perception module (CPM), distortion perception module (DPM), and visual interaction module (VIM). However, the subjective quality score cannot guide the model to explicitly learn the feature representations of image content and distortion. Thus, we perform a two-stage training procedure. In the first stage, we obtain CPM and DPM, where semantic features are extracted to recognize the image content in CPM, and distortion features are extracted to capture the image distortion type and intensity in DPM. In the second stage, the VIM is applied to model the interaction between semantic and distortion features, and the final predicted quality score is given by a fully connected layer. Experimental results demonstrate that the proposed method can achieve state-of-the-art performance on multiple benchmark databases, e.g., CSIQ, TID2013, KADID-10K, and KonIQ-10 K.
C1 [Wang, Xiaoqi; Xiong, Jian] Nanjing Univ Posts & Telecommun, Sch Commun & Informat Engn, Nanjing 210003, Peoples R China.
   [Lin, Weisi] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 Nanjing University of Posts & Telecommunications; Nanyang Technological
   University
RP Xiong, J (corresponding author), Nanjing Univ Posts & Telecommun, Sch Commun & Informat Engn, Nanjing 210003, Peoples R China.
EM xqwang.research@outlook.com; jxiong@njupt.edu.cn; wslin@ntu.edu.sg
RI Lin, Wei/D-3353-2012; Lin, Weisi/A-3696-2011
OI Lin, Weisi/0000-0001-9866-1947; Xiong, Jian/0000-0002-4720-4102
FU National Natural Science Foundation of China
FX No Statement Available
CR AHUMADA AJ, 1992, P SOC PHOTO-OPT INS, V1666, P365
   Bae SH, 2017, IEEE T CIRC SYST VID, V27, P1196, DOI 10.1109/TCSVT.2016.2539862
   Bae SH, 2016, IEEE T IMAGE PROCESS, V25, P2392, DOI 10.1109/TIP.2016.2545863
   Bianco S, 2018, SIGNAL IMAGE VIDEO P, V12, P355, DOI 10.1007/s11760-017-1166-8
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen DQ, 2020, IEEE T IMAGE PROCESS, V29, P6496, DOI 10.1109/TIP.2020.2990342
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen ZZ, 2010, IEEE T CIRC SYST VID, V20, P806, DOI 10.1109/TCSVT.2010.2045912
   Chou CH, 1995, IEEE T CIRC SYST VID, V5, P467, DOI 10.1109/76.475889
   Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Fang YM, 2015, IEEE SIGNAL PROC LET, V22, P838, DOI 10.1109/LSP.2014.2372333
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Golestaneh S Alireza, 2022, P IEEE CVF WINT C AP, P3209
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Jarsky T, 2011, J NEUROSCI, V31, P11003, DOI 10.1523/JNEUROSCI.2631-11.2011
   Jiang QP, 2018, IEEE T MULTIMEDIA, V20, P2035, DOI 10.1109/TMM.2017.2763321
   Kang L, 2015, IEEE IMAGE PROC, P2791, DOI 10.1109/ICIP.2015.7351311
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kim J, 2019, IEEE T NEUR NET LEAR, V30, P11, DOI 10.1109/TNNLS.2018.2829819
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   LEGGE GE, 1980, J OPT SOC AM, V70, P1458, DOI 10.1364/JOSA.70.001458
   Li DQ, 2019, IEEE T MULTIMEDIA, V21, P1221, DOI 10.1109/TMM.2018.2875354
   Lin HH, 2019, INT WORK QUAL MULTIM
   Lin WS, 2022, IEEE T MULTIMEDIA, V24, P3706, DOI 10.1109/TMM.2021.3106503
   Liu AM, 2010, IEEE T CIRC SYST VID, V20, P1648, DOI 10.1109/TCSVT.2010.2087432
   Liu HH, 2020, IEEE T IMAGE PROCESS, V29, P641, DOI 10.1109/TIP.2019.2933743
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Loshchilov Ilya, 2016, arXiv
   Ma JP, 2021, IEEE T IMAGE PROCESS, V30, P3650, DOI 10.1109/TIP.2021.3064195
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Macknik SL, 1998, NAT NEUROSCI, V1, P144, DOI 10.1038/393
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Ou FZ, 2022, IEEE T MULTIMEDIA, V24, P4197, DOI 10.1109/TMM.2021.3114551
   Pelli DG, 2013, VISION RES, V90, P10, DOI 10.1016/j.visres.2013.04.015
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Srinivas A, 2021, PROC CVPR IEEE, P16514, DOI 10.1109/CVPR46437.2021.01625
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   SWITKES E, 1988, J OPT SOC AM A, V5, P1149, DOI 10.1364/JOSAA.5.001149
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HK, 2021, IEEE T IMAGE PROCESS, V30, P487, DOI 10.1109/TIP.2020.3037525
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu JJ, 2020, IEEE T IMAGE PROCESS, V29, P7414, DOI 10.1109/TIP.2020.3002478
   Wu JJ, 2017, IEEE T IMAGE PROCESS, V26, P2682, DOI 10.1109/TIP.2017.2685682
   Wu QB, 2015, IEEE IMAGE PROC, P339, DOI 10.1109/ICIP.2015.7350816
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Ye P, 2012, IEEE T IMAGE PROCESS, V21, P3129, DOI 10.1109/TIP.2012.2190086
   You JY, 2021, IEEE IMAGE PROC, P1389, DOI 10.1109/ICIP42928.2021.9506075
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang P, 2015, PROC CVPR IEEE, P2394, DOI 10.1109/CVPR.2015.7298853
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhou BL, 2014, ADV NEUR IN, V27
NR 66
TC 3
Z9 3
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8958
EP 8971
DI 10.1109/TMM.2023.3243683
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000055
DA 2024-07-18
ER

PT J
AU Wang, XS
   Xie, QC
   Zhu, JH
   Xie, L
   Scharenborg, O
AF Wang, Xinsheng
   Xie, Qicong
   Zhu, Jihua
   Xie, Lei
   Scharenborg, Odette
TI AnyoneNet: Synchronized Speech and Talking Head Generation for Arbitrary
   Persons
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Avatar; facial landmark; speech synthesis; talking head generation
ID TEXT-TO-SPEECH; SPEAKER; VIDEO
AB Automatically generating videos in which synthesized speech is synchronized with lip movements in a talking head has great potential in many human-computer interaction scenarios. In this paper, we present an automatic method to generate synchronized speech and talking-head videos on the basis of text and a single face image of an arbitrary person as input. In contrast to previous text-driven talking head generation methods, which can only synthesize the voice of a specific person, the proposed method is capable of synthesizing speech for any person. Specifically, the proposed method decomposes the generation of synchronized speech and talking head videos into two stages, i.e., a text-to-speech (TTS) stage and a speech-driven talking head generation stage. The proposed TTS module is a face-conditioned multi-speaker TTS model that gets the speaker identity information from face images instead of speech, which allows us to synthesize a personalized voice on the basis of the input face image. To generate the talking head videos from the face images, a facial landmark-based method that can predict both lip movements and head rotations is proposed. Extensive experiments demonstrate that the proposed method is able to generate synchronized speech and talking head videos for arbitrary persons, in which the timbre of the synthesized voice is in harmony with the input face, and the proposed landmark-based talking head method outperforms the state-of-the-art landmark-based method on generating natural talking head videos.
C1 [Wang, Xinsheng; Zhu, Jihua] Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Peoples R China.
   [Wang, Xinsheng; Xie, Qicong; Xie, Lei] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.
   [Wang, Xinsheng; Scharenborg, Odette] Delft Univ Technol, Multimedia Comp Grp, NL-2628CD Delft, Netherlands.
C3 Xi'an Jiaotong University; Northwestern Polytechnical University; Delft
   University of Technology
RP Zhu, JH (corresponding author), Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Peoples R China.; Xie, L (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.
EM wangxinsheng@stu.xjtu.edu.cn; xieqicong@mail.nwpu.edu.cn;
   zhujh@xjtu.edu.cn; lxie@nwpu.edu.cn; O.E.Scharenborg@tudelft.nl
RI Wang, YUJIE/JXY-8442-2024; Xie, Lei/JWO-8567-2024
OI Wang, Xinsheng/0000-0003-1826-7419; Zhu, Jihua/0000-0002-3081-8781;
   Scharenborg, Odette/0000-0003-0693-8852
FU National Key R&D Program of China [2018AAA0102504, 2020AAA0108600]; Key
   Research and Development Program of Shaanxi [2021GY-025, 2021GXLH-Z-097]
FX This work was supported in part by the National Key R&D Program of China
   under Grants 2018AAA0102504 and 2020AAA0108600, and in part by the Key
   Research and Development Program of Shaanxi under Grants 2021GY-025 and
   2021GXLH-Z-097
CR [Anonymous], 2016, ARXIV PREPRINT ARXIV
   Battenberg E, 2020, INT CONF ACOUST SPEE, P6194, DOI [10.1109/icassp40776.2020.9054106, 10.1109/ICASSP40776.2020.9054106]
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Cai ZX, 2020, INTERSPEECH, P3974, DOI 10.21437/Interspeech.2020-1032
   Casanova E, 2021, Arxiv, DOI arXiv:2104.05557
   Chae W, 2020, KSII T INTERNET INF, V14, P3473, DOI 10.3837/tiis.2020.08.018
   Charles J, 2016, LECT NOTES COMPUT SC, V9915, P879, DOI 10.1007/978-3-319-49409-8_71
   Chen LL, 2020, Arxiv, DOI arXiv:2005.03201
   Chen LL, 2019, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2019.00802
   Chen LL, 2018, LECT NOTES COMPUT SC, V11211, P538, DOI 10.1007/978-3-030-01234-2_32
   Chen LL, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P349, DOI 10.1145/3126686.3126723
   Choi H. -S., 2019, P INT C LEARN REPR
   Chung JS, 2018, INTERSPEECH, P1086
   Chung JS, 2017, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2017.367
   Chung JS, 2017, LECT NOTES COMPUT SC, V10112, P87, DOI 10.1007/978-3-319-54184-6_6
   Cooper E, 2020, INT CONF ACOUST SPEE, P6184, DOI [10.1109/icassp40776.2020.9054535, 10.1109/ICASSP40776.2020.9054535]
   Donahue J., 2021, P INT C LEARN REPR
   Edwards P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925984
   Esser P, 2018, PROC CVPR IEEE, P8857, DOI 10.1109/CVPR.2018.00923
   Fan B, 2016, MULTIMED TOOLS APPL, V75, P5287, DOI 10.1007/s11042-015-2944-3
   Fan YC, 2015, INT CONF ACOUST SPEE, P4475, DOI 10.1109/ICASSP.2015.7178817
   Fang FM, 2019, INT CONF ACOUST SPEE, P6795, DOI [10.1109/icassp.2019.8683872, 10.1109/ICASSP.2019.8683872]
   Garrido P, 2015, COMPUT GRAPH FORUM, V34, P193, DOI 10.1111/cgf.12552
   GRIFFIN DW, 1984, IEEE T ACOUST SPEECH, V32, P236, DOI 10.1109/TASSP.1984.1164317
   Hati Y., 2019, P INT C AUD DISPL DE, P75
   Ilharco G., 2019, PROC 23 C COMPUTATIO, P55
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Ito K., 2017, The LJ Speech Dataset
   Ji XY, 2021, PROC CVPR IEEE, P14075, DOI 10.1109/CVPR46437.2021.01386
   Jia Y, 2018, ADV NEUR IN, V31
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Kong J., 2020, ADV NEURAL INFORM PR, V33, P17022
   Kumar K, 2019, ADV NEUR IN, V32
   Kumar R., 2017, arXiv
   Lei Ba J., 2016, arXiv
   Lele Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P35, DOI 10.1007/978-3-030-58545-7_3
   Li LT, 2022, SPEECH COMMUN, V137, P77, DOI 10.1016/j.specom.2022.01.002
   Liu Y, 2019, INTERSPEECH, P2873, DOI 10.21437/Interspeech.2019-2357
   Maas A.L., 2013, P 30 INT C MACH LEAR, V30, P3
   Nachmani Eliya, 2018, INT C MACHINE LEARNI, P3683
   Nagrani A, 2020, COMPUT SPEECH LANG, V60, DOI 10.1016/j.csl.2019.101027
   Nourabadi N. S., 2017, Ph.D. dissertation
   Oh TH, 2019, PROC CVPR IEEE, P7531, DOI 10.1109/CVPR.2019.00772
   Ping W., 2018, P INT C LEARN REPR, P1
   Prajwal KR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P484, DOI 10.1145/3394171.3413532
   Prenger R, 2019, INT CONF ACOUST SPEE, P3617, DOI [10.1109/ICASSP.2019.8683143, 10.1109/icassp.2019.8683143]
   Ren Y, 2019, ADV NEUR IN, V32
   Richard A, 2021, IEEE WINT CONF APPL, P41, DOI 10.1109/WACV48630.2021.00009
   Schreer O, 2008, IEEE T MULTIMEDIA, V10, P352, DOI 10.1109/TMM.2008.917336
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Segal AV, 2009, GEN ICP, DOI DOI 10.15607/RSS.2009.V.021
   Shen J, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4779, DOI 10.1109/ICASSP.2018.8461368
   Siarohin A, 2019, ADV NEUR IN, V32
   Chung JS, 2017, Arxiv, DOI arXiv:1705.02966
   Song Y, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P919
   Sotelo J., 2017, ICLR WORKSH TRACK
   Sun Y., 2021, P 30 INT JOINT C ART, P1018
   Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640
   Thies Justus, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P716, DOI 10.1007/978-3-030-58517-4_42
   van den Oord A., 2016, P 9 ISCA WORKSH SPEE, P125
   Vougioukas Konstantinos, 2019, CVPR WORKSHOPS, P37
   Wang YX, 2017, INTERSPEECH, P4006, DOI 10.21437/Interspeech.2017-1452
   Weiss RJ, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P5679, DOI 10.1109/ICASSP39728.2021.9413851
   Wen X, 2020, IEEE T VIS COMPUT GR, V26, P3457, DOI 10.1109/TVCG.2020.3023573
   Yamagishi J, 2009, IEEE T AUDIO SPEECH, V17, P1208, DOI 10.1109/TASL.2009.2016394
   Yang G, 2021, IEEE W SP LANG TECH, P492, DOI 10.1109/SLT48900.2021.9383551
   Yang S., 2016, P IEEE AS PAC SIGN I, P1
   Yi R, 2020, Arxiv, DOI arXiv:2002.10137
   Yu CZ, 2020, INTERSPEECH, P2027, DOI 10.21437/Interspeech.2020-2968
   Yu LY, 2021, IEEE T CIRC SYST VID, V31, P203, DOI 10.1109/TCSVT.2020.2973374
   Yu LY, 2019, IEEE T MULTIMEDIA, V21, P1621, DOI 10.1109/TMM.2018.2887027
   Zakharov E, 2019, IEEE I CONF COMP VIS, P9458, DOI 10.1109/ICCV.2019.00955
   Zhang SB, 2022, INT CONF ACOUST SPEE, P2659, DOI 10.1109/ICASSP43922.2022.9747380
   Zhou H, 2021, PROC CVPR IEEE, P4174, DOI 10.1109/CVPR46437.2021.00416
   Zhou H, 2019, AAAI CONF ARTIF INTE, P9299
   Zhou Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417774
NR 76
TC 1
Z9 1
U1 11
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6717
EP 6728
DI 10.1109/TMM.2022.3214100
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000002
DA 2024-07-18
ER

PT J
AU Yu, E
   Li, ZL
   Han, SD
   Wang, HW
AF Yu, En
   Li, Zhuoling
   Han, Shoudong
   Wang, Hongwei
TI RelationTrack: Relation-Aware Multiple Object Tracking With Decoupled
   Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Decoupling representation; deformable attention; multiple object
   tracking; optimization contradiction; transformer encoder
AB Existing online multiple object tracking (MOT) algorithms often consist of two subtasks, detection and re-identification (ReID). In order to enhance the inference speed and reduce the complexity, current methods commonly integrate these double subtasks into a unified framework. Nevertheless, detection and ReID demand diverse features. This issue results in an optimization contradiction during the training procedure. With the target of alleviating this contradiction, we devise a module named Global Context Disentangling (GCD) that decouples the learned representation into detection-specific and ReID-specific embeddings. As such, this module provides an implicit manner to balance the different requirements of these two subtasks. Moreover, we observe that preceding MOT methods typically leverage local information to associate the detected targets and neglect to consider the global semantic relation. To resolve this limitation, we develop a module, referred to as Guided Transformer Encoder (GTE), by combining the powerful reasoning ability of Transformer encoder and deformable attention. Unlike previous works, GTE avoids analyzing all the pixels and only attends to capture the relation between query nodes and a few self-adaptively selected key samples. Therefore, it is computationally efficient. Extensive experiments have been conducted on the MOT16, MOT17 and MOT20 benchmarks to demonstrate the superiority of the proposed MOT framework, namely RelationTrack. The experimental results indicate that RelationTrack has surpassed preceding methods significantly and established a new state-of-the-art performance, e.g., IDF1 of 70.5% and MOTA of 67.2% on MOT20.
C1 [Yu, En; Han, Shoudong; Wang, Hongwei] Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Natl Key Lab Sci & Technol Multispectral Informat, Wuhan, 430074, Peoples R China.
   [Li, Zhuoling] Tsinghua Univ, Shenzhen Int Grad Sch, Minist Educ, Shenzhen 518000, Peoples R China.
C3 Huazhong University of Science & Technology; Tsinghua University
RP Han, SD (corresponding author), Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Natl Key Lab Sci & Technol Multispectral Informat, Wuhan, 430074, Peoples R China.
EM yuen@hust.edu.cn; lzl20@mails.tsinghua.edu.cn; shoudonghan@hust.edu.cn;
   hongweiwang@hust.edu.cn
RI Wang, Hongwei/HFT-3345-2022; LI, ZHUOLING/KHE-1368-2024
OI Han, Shoudong/0000-0003-0572-4748; Yu, En/0000-0001-6292-6384
CR Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Chen L, 2018, IEEE INT CON MULTI, DOI 10.1097/PEC.0000000000001553
   Chen Q, 2021, PROC CVPR IEEE, P13034, DOI 10.1109/CVPR46437.2021.01284
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Ciaparrone G, 2020, NEUROCOMPUTING, V381, P61, DOI 10.1016/j.neucom.2019.11.023
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dendorfer P., 2020, arXiv
   Dollár P, 2009, PROC CVPR IEEE, P304, DOI 10.1109/CVPRW.2009.5206631
   Ess A., 2008, PROC IEEE C COMPUT V, P1
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Han SD, 2022, NEUROCOMPUTING, V476, P75, DOI 10.1016/j.neucom.2021.12.104
   Jinlong Peng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P145, DOI 10.1007/978-3-030-58548-8_9
   Kang K, 2018, IEEE T CIRC SYST VID, V28, P2896, DOI 10.1109/TCSVT.2017.2736553
   Kingma D. P., 2014, arXiv
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Leal-Taix‚ L, 2015, Arxiv, DOI arXiv:1504.01942
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li ZL, 2021, Arxiv, DOI arXiv:2102.12205
   Liang C, 2022, Arxiv, DOI arXiv:2010.12138
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Luiten J, 2021, INT J COMPUT VISION, V129, P548, DOI 10.1007/s11263-020-01375-2
   Luo WJ, 2018, PROC CVPR IEEE, P3569, DOI 10.1109/CVPR.2018.00376
   Manglik A, 2019, IEEE INT C INT ROBOT, P8081, DOI [10.1109/iros40897.2019.8967730, 10.1109/IROS40897.2019.8967730]
   Meinhardt T, 2022, Arxiv, DOI arXiv:2101.02702
   Milan A, 2016, Arxiv, DOI arXiv:1603.00831
   Minghao Yin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P191, DOI 10.1007/978-3-030-58555-6_12
   Muller J., 1992, Complex Var. Theory Appl., V18, P155
   Pang B, 2020, PROC CVPR IEEE, P6307, DOI 10.1109/CVPR42600.2020.00634
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Shan CB, 2020, Arxiv, DOI arXiv:2010.09015
   Shao S, 2018, Arxiv, DOI [arXiv:1805.00123, DOI 10.48550/ARXIV.1805.00123]
   Takahashi N, 2018, IEEE T MULTIMEDIA, V20, P513, DOI 10.1109/TMM.2017.2751969
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Welch G., 1995, An introduction to the kalman filter
   Weng XS, 2020, PROC CVPR IEEE, P6498, DOI 10.1109/CVPR42600.2020.00653
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Xingyi Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P474, DOI 10.1007/978-3-030-58548-8_28
   Xu YH, 2020, PROC CVPR IEEE, P6786, DOI 10.1109/CVPR42600.2020.00682
   Yang F, 2016, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2016.234
   Yin TW, 2021, PROC CVPR IEEE, P11779, DOI 10.1109/CVPR46437.2021.01161
   Yu HK, 2020, IEEE T MULTIMEDIA, V22, P3051, DOI 10.1109/TMM.2020.2972165
   Zhang SZ, 2021, IEEE T MULTIMEDIA, V23, P281, DOI 10.1109/TMM.2020.2977528
   Zhang Y, 2020, IEEE INTERNET THINGS, V7, P7892, DOI 10.1109/JIOT.2020.2996609
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
   Zhang Z, 2018, Arxiv, DOI arXiv:1811.11167
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
   Zhongdao Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P107, DOI 10.1007/978-3-030-58621-8_7
   Zhou XY, 2019, Arxiv, DOI [arXiv:1904.07850, 10.48550/arXiv.1904.07850]
   Zhu X., 2021, PROC INT C LEARN REP
   Zhu XZ, 2017, IEEE I CONF COMP VIS, P408, DOI 10.1109/ICCV.2017.52
NR 61
TC 45
Z9 47
U1 12
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2686
EP 2697
DI 10.1109/TMM.2022.3150169
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600018
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Zhang, HK
   Cai, Y
   Ren, HP
   Li, Q
AF Zhang, Huakui
   Cai, Yi
   Ren, Haopeng
   Li, Qing
TI Multimodal Topic Modeling by Exploring Characteristics of Short Text
   Social Media
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimodal; social media; topic models
ID CLASSIFICATION; WORDS
AB Millions of people post images and texts to express their feelings and point of views on social media everyday, especially on the short text social media such as Twitter or Weibo. As the images can provide important supplementary information for the text, many multimodal topic models have been developed to mine the topics from the multimodal social media content. We summarize three fundamental characteristics of the short text multimodal social media. The first is that the text of a short social media document generally belong to only one topic. The second is that the attached images can be relevant to multiple topics due to the rich information expressed in the images. The last is that although in most cases, text and images in social media posts are relevant, it should be noted that in a small number of cases, text and pictures are not relevant. However, most of the current multimodal topic models fail to model the these characteristics, and thus may produce low-quality topics. Based on these characteristics, we propose an unsupervised multimodal topic model SMMTM to model the short text multimodal social media documents. In the SMMTM model, only one topic is sampled for the the text while an image can belong to different topics. The correlation of the topics between the text and the images in a document are also formulated in an appropriate way. The experiments on three short text social media datasets with four evaluation metrics show the advantages of our model over the existing models.
C1 [Zhang, Huakui; Cai, Yi; Ren, Haopeng] South China Univ Technol, Key Lab Big Data & Intelligent Robot, Minist Educ, Guangzhou 510640, Peoples R China.
   [Zhang, Huakui; Cai, Yi; Ren, Haopeng] South China Univ Technol, Sch Software Engn, Guangzhou 510640, Peoples R China.
   [Li, Qing] Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China.
C3 South China University of Technology; South China University of
   Technology; Hong Kong Polytechnic University
RP Zhang, HK; Cai, Y (corresponding author), South China Univ Technol, Key Lab Big Data & Intelligent Robot, Minist Educ, Guangzhou 510640, Peoples R China.; Zhang, HK; Cai, Y (corresponding author), South China Univ Technol, Sch Software Engn, Guangzhou 510640, Peoples R China.
EM zhanghk1997@gmail.com; ycai@scut.edu.cn; se_renhp@mail.scut.edu.cn;
   csqli@comp.polyu.edu.hk
RI li, jing/JEF-8436-2023; yang, peng/JEZ-8452-2023; Li,
   Qing/JMH-1365-2023; li, jixiang/JXN-7599-2024; li, qing/JEF-9044-2023;
   Jia, Li/JVN-3095-2024; xiao, ming/KHT-1774-2024; Jiang,
   Yu/JEZ-9814-2023; peng, jin/JRW-4493-2023; WANG, Bin/JGM-2639-2023;
   Zhou, Hong/JKJ-1067-2023
OI Li, Qing/0000-0003-3370-471X; 
FU National Natural Science Foundation of China [62076100]; Fundamental
   Research Funds for the Central Universities SCUT [D2210010, D2200150,
   D2201300]; Science and Technology Planning Project of Guangdong Province
   [2020B0101100002]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62076100, in part by the Fundamental
   Research Funds for the Central Universities SCUT under Grant D2210010,
   D2200150, and D2201300, and in part by the Science and Technology
   Planning Project of Guangdong Province under Grant 2020B0101100002.
CR Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   Bian JW, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1807, DOI 10.1145/2505515.2505652
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Cai HY, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P89, DOI 10.1145/2733373.2806236
   Cao LL, 2007, IEEE I CONF COMP VIS, P1080
   Chen T, 2015, AAAI CONF ARTIF INTE, P30
   Chen YH, 2018, MULTIMED TOOLS APPL, V77, P23291, DOI 10.1007/s11042-017-5588-7
   Wang C, 2009, PROC CVPR IEEE, P1903, DOI [10.1109/CVPR.2009.5206800, 10.1109/CVPRW.2009.5206800]
   Das R, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P795
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Fang Q, 2015, IEEE T MULTIMEDIA, V17, P2281, DOI 10.1109/TMM.2015.2491019
   Garroppo RG, 2018, IEEE T MULTIMEDIA, V20, P2683, DOI 10.1109/TMM.2018.2811625
   Gong YY, 2018, NEUROCOMPUTING, V272, P170, DOI 10.1016/j.neucom.2017.06.056
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Heinrich G., 2005, PARAMETER ESTIMATION
   Hisano R, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2498
   Hoffman M., 2010, P ADV NEUR INF PROC, V23:856-864
   Hu YT, 2018, IEEE T MULTIMEDIA, V20, P927, DOI 10.1109/TMM.2017.2760101
   Huang QB, 2022, IEEE T MULTIMEDIA, V24, P2004, DOI 10.1109/TMM.2021.3074803
   Huang SR, 2017, IEEE T MULTIMEDIA, V19, P1314, DOI 10.1109/TMM.2017.2652074
   Jia YQ, 2011, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2011.6126524
   Krasnashchok K, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P247
   Lau J. H., 2014, P 14 C EUR CHAPT ASS, P530, DOI [DOI 10.3115/V1/E14-1056, 10.3115/v1/E14-1056]
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Li WX, 2017, IEEE T MULTIMEDIA, V19, P367, DOI 10.1109/TMM.2016.2616279
   Li XM, 2018, INFORM PROCESS MANAG, V54, P1345, DOI 10.1016/j.ipm.2018.05.009
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu HY, 2017, AAAI CONF ARTIF INTE, P1192
   Lu JS, 2019, ADV NEUR IN, V32
   Miao YS, 2017, 34 INT C MACHINE LEA, V70
   Mimno D, 2011, EMNLP, P262, DOI DOI 10.5555/2145432.2145462
   Nan F, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6345
   Newman D., 2010, AUTOMATIC EVALUATION
   Nie WZ, 2017, J VIS COMMUN IMAGE R, V48, P375, DOI 10.1016/j.jvcir.2016.11.015
   Peng M, 2019, IEEE T KNOWL DATA EN, V31, P1080, DOI 10.1109/TKDE.2018.2847707
   Putthividhya D, 2010, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2010.5540000
   Qian S., 2015, ACM T MULTIM COMPUT, V11, P1
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Rashid J, 2019, INFORM PROCESS MANAG, V56, DOI 10.1016/j.ipm.2019.102060
   Ren D, 2019, NEUROCOMPUTING, V358, P344, DOI 10.1016/j.neucom.2019.05.071
   Röder M, 2015, WSDM'15: PROCEEDINGS OF THE EIGHTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P399, DOI 10.1145/2684822.2685324
   Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756
   Sun JY, 2020, WIREL NETW, V26, P1549, DOI 10.1007/s11276-019-02009-3
   Teh Y, 2005, Adv. Neural Inf. Process. Syst., V17
   Thu Hoai Tran, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P5979, DOI 10.1109/ICASSP.2014.6854751
   Wang K, 2019, NEUROCOMPUTING, V325, P190, DOI 10.1016/j.neucom.2018.10.024
   Wang Z., 2014, ACM Trans. Multimedia Comput. Commun. Appl. (TOMM), V10, P1
   Xu ZX, 2017, IEEE T MULTIMEDIA, V19, P1933, DOI 10.1109/TMM.2017.2688928
   Xue F, 2020, IEEE T MULTIMEDIA, V22, P2098, DOI 10.1109/TMM.2019.2951194
   Xue F, 2019, J VIS COMMUN IMAGE R, V59, P1, DOI 10.1016/j.jvcir.2018.12.033
   Xue F, 2019, MULTIMED TOOLS APPL, V78, P141, DOI 10.1007/s11042-017-5605-x
   Yan X., 2013, P 22 INT C WORLD WID, P1445, DOI DOI 10.1145/2488388.2488514
   Zhang C, 2020, AAAI CONF ARTIF INTE, V34, P6737
   Zhang H, 2020, LECT NOTES COMPUT SC, V12113, P227, DOI 10.1007/978-3-030-59416-9_14
   Zhang X, 2018, IEEE T MULTIMEDIA, V20, P3223, DOI 10.1109/TMM.2018.2838334
   Zheng CM, 2021, IEEE T MULTIMEDIA, V23, P2520, DOI 10.1109/TMM.2020.3013398
   Zheng Y, 2016, IEEE T PATTERN ANAL, V38, P1056, DOI 10.1109/TPAMI.2015.2476802
   Zheng Y, 2014, PROC CVPR IEEE, P1370, DOI 10.1109/CVPR.2014.178
NR 59
TC 2
Z9 2
U1 22
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2430
EP 2445
DI 10.1109/TMM.2022.3147064
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100064
DA 2024-07-18
ER

PT J
AU Zhang, K
   Mao, ZD
   Liu, AA
   Zhang, YD
AF Zhang, Kun
   Mao, Zhendong
   Liu, An-An
   Zhang, Yongdong
TI Unified Adaptive Relevance Distinguishable Attention Network for
   Image-Text Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Optimization; Visualization; Training; Task analysis;
   Representation learning; Correlation; Image-text matching; attention
   network; unified adaptive relevance distinguishable learning
AB Image-text matching, as a fundamental cross-modal task, bridges the gap between vision and language. The core is to accurately learn semantic alignment to find relevant shared semantics in image and text. Existing methods typically attend to all fragments with word-region similarity greater than empirical threshold zero as relevant shared semantics, e.g., via a ReLU operation that forces the negative to zero and maintains the positive. However, this fixed threshold is totally isolated with feature learning, which cannot adaptively and accurately distinguish the varying distributions of relevant and irrelevant word-region similarity in training, inevitably limiting the semantic alignment learning. To solve this issue, we propose a novel Unified Adaptive Relevance Distinguishable Attention (UARDA) mechanism, incorporating the relevance threshold into a unified learning framework, to maximally distinguish the relevant and irrelevant distributions to obtain better semantic alignment. Specifically, our method adaptively learns the optimal relevance boundary between these two distributions to improve the model to learn more discriminative features. The explicit relevance threshold is well integrated into similarity matching, which kills two birds with one stone as: (1) excluding the disturbances of irrelevant fragment contents to aggregate precisely relevant shared semantics for boosting matching accuracy, and (2) avoiding the calculation of irrelevant fragment queries for reducing retrieval time. Experimental results on benchmarks show that UARDA can substantially and consistently outperform state-of-the-arts, with relative rSum improvements of 2%-4% (16.9%-35.3% for baseline SCAN), and reducing the retrieval time by 50%-73%.
C1 [Zhang, Kun; Mao, Zhendong; Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230022, Anhui, Peoples R China.
   [Zhang, Yongdong] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230022, Anhui, Peoples R China.
   [Liu, An-An] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Tianjin University
RP Mao, ZD (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230022, Anhui, Peoples R China.
EM kkzhang@mail.ustc.edu.cn; zdmao@ustc.edu.cn; anan0422@gmail.com;
   zhyd73@ustc.edu.cn
RI Zhang, Kun/KIH-2791-2024
OI Zhang, Kun/0000-0003-2140-2546
FU National Key Research and Development Program of China [2020YFB1406603];
   Science Fund for Creative Research Groups [62121002]; Fundamental
   Research Funds for the Central Universities [WK3480000010, WK3480000008]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020YFB1406603, in part by the
   Science Fund for Creative Research Groups under Grant 62121002, and in
   part by the Fundamental Research Funds for the Central Universities
   under Grants WK3480000010 and WK3480000008.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Chen TL, 2020, AAAI CONF ARTIF INTE, V34, P10583
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hu ZB, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P789
   Huang FR, 2019, IEEE T IMAGE PROCESS, V28, P2008, DOI 10.1109/TIP.2018.2882225
   Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Ji Z, 2019, IEEE I CONF COMP VIS, P5753, DOI 10.1109/ICCV.2019.00585
   Jiang M, 2020, PROC CVPR IEEE, P2977, DOI 10.1109/CVPR42600.2020.00305
   Karpathy A, 2014, ADV NEUR IN, V27
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Klein B, 2015, Arxiv, DOI arXiv:1411.7399
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li G, 2020, AAAI CONF ARTIF INTE, V34, P11336
   Li JN, 2020, IEEE T MULTIMEDIA, V22, P554, DOI 10.1109/TMM.2019.2930041
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Li XY, 2021, AAAI CONF ARTIF INTE, V35, P1966
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu C., 2020, P IEEE CVF C COMP VI, P10921, DOI DOI 10.1109/CVPR42600.2020.01093
   Liu CX, 2022, IEEE T MULTIMEDIA, V24, P103, DOI 10.1109/TMM.2020.3046855
   Liu CX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P3, DOI 10.1145/3343031.3350869
   Liu Y, 2017, IEEE I CONF COMP VIS, P4127, DOI 10.1109/ICCV.2017.442
   Liu Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1239, DOI 10.1145/3343031.3350986
   Ma L, 2015, IEEE I CONF COMP VIS, P2623, DOI 10.1109/ICCV.2015.301
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Niu ZX, 2017, IEEE I CONF COMP VIS, P1899, DOI 10.1109/ICCV.2017.208
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Qu LG, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1104, DOI 10.1145/3404835.3462829
   Sarafianos N, 2019, IEEE I CONF COMP VIS, P5813, DOI 10.1109/ICCV.2019.00591
   Song Y, 2019, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2019.00208
   Tan JH, 2019, IEEE T MULTIMEDIA, V21, P2686, DOI 10.1109/TMM.2019.2904878
   Tianlang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P549, DOI 10.1007/978-3-030-58601-0_33
   Wang LW, 2019, IEEE T PATTERN ANAL, V41, P394, DOI 10.1109/TPAMI.2018.2797921
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang SJ, 2020, IEEE WINT CONF APPL, P1497, DOI 10.1109/WACV45572.2020.9093614
   Wang T, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P12, DOI 10.1145/3343031.3350875
   Wang YX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3792
   Wang ZH, 2019, IEEE I CONF COMP VIS, P5763, DOI 10.1109/ICCV.2019.00586
   Wei JW, 2022, IEEE T PATTERN ANAL, V44, P6534, DOI 10.1109/TPAMI.2021.3088863
   Wei Jiwei, 2020, P IEEE C COMP VIS PA, P13005
   Wei K, 2019, IEEE I CONF COMP VIS, P3740, DOI 10.1109/ICCV.2019.00384
   Wu YL, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2088, DOI 10.1145/3343031.3350940
   Wu YL, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P825, DOI 10.1145/3240508.3240521
   Xu X, 2022, IEEE T PATTERN ANAL, V44, P3030, DOI 10.1109/TPAMI.2020.3045530
   Yan SY, 2021, PROC CVPR IEEE, P8092, DOI 10.1109/CVPR46437.2021.00800
   Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41
   Yang X, 2021, PROC CVPR IEEE, P16770, DOI 10.1109/CVPR46437.2021.01650
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Zhang LL, 2020, IEEE T MULTIMEDIA, V22, P775, DOI 10.1109/TMM.2019.2931352
   Zhang WQ, 2020, IEEE T MULTIMEDIA, V22, P1032, DOI 10.1109/TMM.2019.2935678
   Zhang Y, 2018, LECT NOTES COMPUT SC, V11205, P707, DOI 10.1007/978-3-030-01246-5_42
NR 56
TC 24
Z9 25
U1 10
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1320
EP 1332
DI 10.1109/TMM.2022.3141603
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100023
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Zhu, GY
   Zhou, Y
   Yao, R
   Zhu, HC
AF Zhu, Guanyu
   Zhou, Yong
   Yao, Rui
   Zhu, Hancheng
TI Cross-Class Bias Rectification for Point Cloud Few-Shot Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point cloud compression; Prototypes; Task analysis; Semantic
   segmentation; Three-dimensional displays; Shape; Semantics; Point cloud;
   few-shot; prototype; bias rectification
AB The point cloud is a densely distributed 3D (three-dimensional) data, and annotating the point cloud is a time-consuming and labor-intensive work. The existing semantics segmentation work adopts few-shot learning to reduce the dependence on labeling samples while improving the generalization of the model to new categories. Since point clouds are 3D structures with rich geometric features, even objects of the same category have feature differences that cannot be ignored. Therefore, a few samples (support set) used to train the model do not cover all the features of this category. There is a distribution difference between the support samples and the samples used to verify the model performance (query set). In this paper, we propose an efficient point cloud few-shot segmentation method based on prototypes for bias rectification. A prototype is a vector representation of a category in the metric space. To make the prototype representation of the support set closer to the query set features, we define a feature bias term and reduce the distribution distance between the two sets by fusing the support set features and the bias term. On this basis, we design a feature cross-reference module. By mining the co-occurring features of the support and query sets, it can generate a more representative prototype which captures the overall features of the point cloud. Extensive experiments on two challenging datasets demonstrate that our method outperforms the state-of-the-art method by an average of 3.31% in several N-way K-shot tasks, and achieves approximately 200 times faster reasoning speed.
C1 [Zhu, Guanyu; Zhou, Yong; Yao, Rui; Zhu, Hancheng] China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Peoples R China.
   [Zhu, Guanyu; Zhou, Yong; Yao, Rui; Zhu, Hancheng] Minist Educ Peoples Republ China, Engn Res Ctr Mine Digitizat, Xuzhou 221116, Peoples R China.
C3 China University of Mining & Technology
RP Zhou, Y (corresponding author), China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Peoples R China.; Zhou, Y (corresponding author), Minist Educ Peoples Republ China, Engn Res Ctr Mine Digitizat, Xuzhou 221116, Peoples R China.
EM tb19170009b2@cumt.edu.cn; yzhou@cumt.edu.cn; ruiyao@cumt.edu.cn;
   zhuhancheng@cumt.edu.cn
RI Wang, zhenhua/KFA-8731-2024; Yin, Jing/KDO-6274-2024; Li,
   Kunpeng/KFS-6306-2024; Wang, Zejun/KBB-8454-2024; chen,
   huan/KEC-2019-2024; TIAN, YI/KHU-9704-2024
OI Zhu, Guanyu/0000-0002-4867-7235; Yao, Rui/0000-0003-2734-915X
FU National Natural Science Foundation of China
FX No Statement Available
CR Ahmed SM, 2018, IEEE INT C INT ROBOT, P7350, DOI 10.1109/IROS.2018.8593910
   Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Chen CF, 2021, IEEE T MULTIMEDIA, V23, P2335, DOI 10.1109/TMM.2020.3009499
   Chen Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8362, DOI 10.1109/ICCV48922.2021.00827
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Finn C, 2017, PR MACH LEARN RES, V70
   Han GX, 2022, PROC CVPR IEEE, P5311, DOI 10.1109/CVPR52688.2022.00525
   Jamal MA, 2019, PROC CVPR IEEE, P11711, DOI 10.1109/CVPR.2019.01199
   Jiang L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6403, DOI 10.1109/ICCV48922.2021.00636
   Jinlu Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P741, DOI 10.1007/978-3-030-58452-8_43
   Krispel G, 2020, IEEE WINT CONF APPL, P1863, DOI 10.1109/WACV45572.2020.9093584
   Lewandowski B, 2019, IEEE INT CONF ROBOT, P4869, DOI 10.1109/ICRA.2019.8793712
   Liu H, 2021, IEEE T MULTIMEDIA, V23, P2045, DOI 10.1109/TMM.2020.3007331
   Mishra N., 2018, INT C LEARN REPR
   Munkhdalai T, 2017, PR MACH LEARN RES, V70
   Qi CR, 2017, ADV NEUR IN, V30
   Qiu S, 2022, IEEE T MULTIMEDIA, V24, P1943, DOI 10.1109/TMM.2021.3074240
   Rusu A. A., 2019, INT C LEARN REPR
   Snell J, 2017, ADV NEUR IN, V30
   Unal O, 2022, PROC CVPR IEEE, P2687, DOI 10.1109/CVPR52688.2022.00272
   Vinyals O., 2016, ADV NEURAL INFORM PR, P3630, DOI DOI 10.48550/ARXIV.1606.04080
   Wang Y, 2020, ECCV, DOI DOI 10.1007/978-3-030-58542-62
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wertheimer D, 2019, PROC CVPR IEEE, P6551, DOI 10.1109/CVPR.2019.00672
   Xu SQ, 2021, IEEE INT C INTELL TR, P3047, DOI 10.1109/ITSC48978.2021.9564951
   Xun Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13703, DOI 10.1109/CVPR42600.2020.01372
   Yang CK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7315, DOI 10.1109/ICCV48922.2021.00724
   Zhang Y., 2021, P IEEE CVF INT C COM, P15520
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao N, 2021, PROC CVPR IEEE, P8869, DOI 10.1109/CVPR46437.2021.00876
   Zhou DF, 2020, PROC CVPR IEEE, P1836, DOI 10.1109/CVPR42600.2020.00191
NR 32
TC 1
Z9 1
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9175
EP 9188
DI 10.1109/TMM.2023.3248150
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200008
DA 2024-07-18
ER

PT J
AU Li, TP
   Zhang, KH
   Shen, SW
   Liu, B
   Liu, QS
   Li, Z
AF Li, Tengpeng
   Zhang, Kaihua
   Shen, Shiwen
   Liu, Bo
   Liu, Qingshan
   Li, Zhu
TI Image Co-Saliency Detection and Instance Co-Segmentation Using Attention
   Graph Clustering Based Graph Convolutional Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Integrated circuits; Image segmentation; Task analysis; Feature
   extraction; Decoding; Clustering algorithms; Benchmark testing;
   Co-saliency detection; graph clustering; graph convolution; instance
   co-segmentation
ID OBJECT DISCOVERY; MODEL; CUTS
AB Co-Saliency Detection (CSD) is to explore the concurrent patterns and salient objects from a group of relevant images, while Instance Co-Segmentation (ICS) aims to identify and segment out all of these co-salient instances, generating corresponding mask for each instance. To simultaneously tackle these two tasks, we present a novel adaptive graph convolutional network with attention graph clustering (GCAGC) for CSD and ICS, termed as GCAGC-CSD and GCAGC-ICS, respectively. The GCAGC-CSD contains three key model designs: first, we develop a graph convolutional network architecture to extract multi-scale representations to characterize the intra- and inter-image consistency. Second, we propose an attention graph clustering algorithm to distinguish the salient foreground objects from common areas in an unsupervised manner. Third, we present a unified framework with encoder-decoder structure to jointly train and optimize the graph convolutional network, attention graph cluster, and CSD decoder in an end-to-end fashion. Afterwards, we design a salient instance segmentation network for GCAGC-ICS, and combine the outputs of GCAGC-CSD and the instance segmentation branch to obtain instance-aware co-segmentation masks. The proposed GCAGC-CSD and GCAGC-ICS are extensively evaluated on four CSD benchmark datasets (iCoseg, Cosal2015, COCO-SEG and CoSOD3k) and five ICS benchmark datasets (CoSOD3k, COCO-NONVOC, COCO-VOC, VOC12 and SOC), and achieve superior performance over state-of-the-arts on both tasks.
C1 [Li, Tengpeng; Zhang, Kaihua; Liu, Qingshan] Nanjing Univ Informat Sci, Jiangsu Key Lab Big Data Anal Technol B DAT, Nanjing 210000, Peoples R China.
   [Li, Tengpeng; Zhang, Kaihua; Liu, Qingshan] Nanjing Univ Informat Sci, Jiangsu Collaborat Innovat Ctr Atmospher Environm, Nanjing 210000, Peoples R China.
   [Shen, Shiwen; Liu, Bo] JD Digits, Mountain View, CA 94035 USA.
   [Li, Zhu] Univ Missouri, Dept Comp Sci & Elect Engn CSEE, Kansas City, MO 64110 USA.
C3 Nanjing University of Information Science & Technology; Nanjing
   University of Information Science & Technology; University of Missouri
   System; University of Missouri Kansas City
RP Zhang, KH (corresponding author), Nanjing Univ Informat Sci, Jiangsu Key Lab Big Data Anal Technol B DAT, Nanjing 210000, Peoples R China.; Zhang, KH (corresponding author), Nanjing Univ Informat Sci, Jiangsu Collaborat Innovat Ctr Atmospher Environm, Nanjing 210000, Peoples R China.
EM 1158257243@qq.com; zhkhua@gmail.com; shiwenshen@engineering.ucla.edu;
   kfliubo@gmail.com; qsliu@nuist.edu.cn; zhu.li@ieee.org
RI Liu, Qingqing/HMV-4816-2023; shen, shiwen/KBD-0433-2024; Li,
   Zhu/AAD-8182-2021; Chen, Rainie/ISS-6016-2023; liu,
   qingqing/HHD-0360-2022; Liu, Qing/GWC-9222-2022; Zhang,
   Kaihua/E-1026-2013
OI Li, Zhu/0000-0002-8246-177X; Zhang, Kaihua/0000-0002-1613-3401
FU National Major Project of China for New Generation of AI
   [2018AAA0100400]; Natural Science Foundation of China [61876088,
   61825601, U20B2065]; 333 High-level Talents Cultivation Project of
   Jiangsu Province [BRA2020291]; NSF [1747751]
FX Thisworkwas supported in part by National Major Project of China for New
   Generation of AI (No. 2018AAA0100400), in part by the Natural Science
   Foundation of China under Grants 61876088, 61825601, and U20B2065, in
   part by the 333 High-level Talents Cultivation Project of Jiangsu
   Province under Grant BRA2020291. The work of Zhu Li was supported by NSF
   Award 1747751. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Ramazan S Aygun.
CR Atwood J, 2016, NIPS, P2001
   Bader D., 2013, Graph Partitioning and Graph Clustering
   Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Bruna J., 2014, ABS13126203 CORR, P1, DOI [10.48550/arXiv.1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Cao XC, 2014, IEEE T IMAGE PROCESS, V23, P4175, DOI 10.1109/TIP.2014.2332399
   Chen XL, 2018, PROC CVPR IEEE, P7239, DOI 10.1109/CVPR.2018.00756
   Cho M, 2015, PROC CVPR IEEE, P1201, DOI 10.1109/CVPR.2015.7298724
   Collins E, 2018, LECT NOTES COMPUT SC, V11218, P352, DOI 10.1007/978-3-030-01264-9_21
   Cong RM, 2019, IEEE T MULTIMEDIA, V21, P1660, DOI 10.1109/TMM.2018.2884481
   Cong RM, 2019, IEEE T CIRC SYST VID, V29, P2941, DOI 10.1109/TCSVT.2018.2870832
   Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343
   Defferrard M, 2016, ADV NEUR IN, V29
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI 10.1109/TP'AMI.2007.1115
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2020, PROC CVPR IEEE, P2916, DOI 10.1109/CVPR42600.2020.00299
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Fan RC, 2019, PROC CVPR IEEE, P6096, DOI 10.1109/CVPR.2019.00626
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Fu HZ, 2015, PROC CVPR IEEE, P4428, DOI 10.1109/CVPR.2015.7299072
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Ge CJ, 2016, SIGNAL PROCESS-IMAGE, V44, P69, DOI 10.1016/j.image.2016.03.005
   Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799
   Gori M, 2005, IEEE IJCNN, P729
   Han JW, 2018, IEEE T CIRC SYST VID, V28, P2473, DOI 10.1109/TCSVT.2017.2706264
   Han JW, 2015, IEEE T GEOSCI REMOTE, V53, P3325, DOI 10.1109/TGRS.2014.2374218
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Henaff M., 2015, ARXIV150605163
   Hsu KJ, 2019, PROC CVPR IEEE, P8838, DOI 10.1109/CVPR.2019.00905
   Hsu KJ, 2018, LECT NOTES COMPUT SC, V11209, P502, DOI 10.1007/978-3-030-01228-1_30
   Jerripothula KR, 2018, IEEE T MULTIMEDIA, V20, P2466, DOI 10.1109/TMM.2018.2798294
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Jiang B, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1375, DOI 10.1145/3343031.3350860
   Kingma D. P., 2014, arXiv
   Kipf TN, 2016, ARXIV
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Lei JJ, 2016, IEEE T MULTIMEDIA, V18, P1783, DOI 10.1109/TMM.2016.2592325
   Li CH, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376209
   Li GY, 2020, IEEE T IMAGE PROCESS, V29, P4873, DOI 10.1109/TIP.2020.2976689
   Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34
   Li JX, 2021, IEEE T MULTIMEDIA, V23, P1397, DOI 10.1109/TMM.2020.2997192
   Li QM, 2018, AAAI CONF ARTIF INTE, P3538
   Li Qimai, 2019, PROC CVPR IEEE, P9582, DOI DOI 10.1109/CVPR.2019.00981
   Li RY, 2018, AAAI CONF ARTIF INTE, P3546
   Li XY, 2019, IEEE I CONF COMP VIS, P9734, DOI 10.1109/ICCV.2019.00983
   Li X, 2018, LECT NOTES COMPUT SC, V11219, P370, DOI 10.1007/978-3-030-01267-0_22
   Li YJ, 2015, IEEE SIGNAL PROC LET, V22, P588, DOI 10.1109/LSP.2014.2364896
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu Z, 2014, IEEE SIGNAL PROC LET, V21, P88, DOI 10.1109/LSP.2013.2292873
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Niepert M, 2016, PR MACH LEARN RES, V48
   Pan SR, 2020, IEEE T CYBERNETICS, V50, P2475, DOI 10.1109/TCYB.2019.2932096
   Qi XJ, 2017, IEEE I CONF COMP VIS, P5209, DOI 10.1109/ICCV.2017.556
   Ren J., 2020, P IEEE INT C MULT EX, P1
   Ren JR, 2020, NEUROCOMPUTING, V371, P137, DOI 10.1016/j.neucom.2019.09.010
   Ren Q., IEEE T MULTIMEDIA, DOI [10.1109/TMM.2020.2997192, DOI 10.1109/TMM.2020.2997192]
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song HK, 2016, IEEE SIGNAL PROC LET, V23, P1722, DOI 10.1109/LSP.2016.2615293
   Sun YZ, 2009, IEEE DATA MINING, P493, DOI 10.1109/ICDM.2009.43
   Tang K, 2014, PROC CVPR IEEE, P1464, DOI 10.1109/CVPR.2014.190
   Tasi CC, 2019, IEEE T IMAGE PROCESS, V28, P56, DOI 10.1109/TIP.2018.2861217
   Tsai CC, 2020, IEEE T MULTIMEDIA, V22, P1016, DOI 10.1109/TMM.2019.2936803
   Wang C, 2019, AAAI CONF ARTIF INTE, P8917
   Wang C, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3670
   Wang WG, 2019, IEEE I CONF COMP VIS, P9235, DOI 10.1109/ICCV.2019.00933
   Wang X, 2017, AAAI CONF ARTIF INTE, P203
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wei LN, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3041
   Wei XS, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3048
   Wei XS, 2019, PATTERN RECOGN, V88, P113, DOI 10.1016/j.patcog.2018.10.022
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41
   Yang LJ, 2011, IEEE T MULTIMEDIA, V13, P1295, DOI 10.1109/TMM.2011.2162399
   Ye LW, 2015, IEEE SIGNAL PROC LET, V22, P2073, DOI 10.1109/LSP.2015.2458434
   Zhang DW, 2019, IEEE T CIRC SYST VID, V29, P3622, DOI 10.1109/TCSVT.2018.2884173
   Zhang DW, 2019, INT J COMPUT VISION, V127, P363, DOI 10.1007/s11263-018-1112-4
   Zhang DW, 2020, IEEE T PATTERN ANAL, V42, P475, DOI 10.1109/TPAMI.2018.2881114
   Zhang DW, 2018, ACM T INTEL SYST TEC, V9, DOI 10.1145/3158674
   Zhang DW, 2017, IEEE T PATTERN ANAL, V39, P865, DOI 10.1109/TPAMI.2016.2567393
   Zhang DW, 2016, IEEE T NEUR NET LEAR, V27, P1163, DOI 10.1109/TNNLS.2015.2495161
   Zhang DW, 2015, PROC CVPR IEEE, P2994, DOI 10.1109/CVPR.2015.7298918
   Zhang K., 2020, P IEEE CVF C COMP VI, P9050
   Zhang KH, 2019, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2019.00321
   Zhao JX, 2019, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2019.00405
   Zhao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P455, DOI 10.1007/978-3-030-58610-2_27
   Zheng XJ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P959, DOI 10.1145/3240508.3240648
   Zhou J, 2018, ARTIF CELL NANOMED B, V46, pS1016, DOI 10.1080/21691401.2018.1442841
   Zhou YZ, 2018, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2018.00399
NR 99
TC 20
Z9 21
U1 6
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 492
EP 505
DI 10.1109/TMM.2021.3054526
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300038
OA hybrid
DA 2024-07-18
ER

PT J
AU Liu, J
   Yang, ZW
   Su, YT
   Yang, XK
AF Liu, Jing
   Yang, Ziwen
   Su, Yuting
   Yang, Xiaokang
TI TANet: Target Attention Network for Video Bit-Depth Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video bit-depth enhancement; self-attention mechanism; spatiotemporal
   feature fusion
ID EXPANSION
AB Video bit-depth enhancement (VBDE) reconstructs high-bit-depth (HBD) frames from a low-bit-depth (I,BD) video sequence. As neighboring frames contain a considerable amount of complementary information related to the center frame, it is vital for VBDE to exploit neighboring frames as much as possible. Conventional VBDE algorithms with explicit alignment across frames attempt to warp each neighboring frame to the center frame with estimated optical flow, taking into account only pairwise correlation. Most spatiotemporal fusion approaches involve direct concatenation or 3D convolution and treat all features equally, failing to focus on information related to the center frame. Therefore, in this paper, we introduce an improved nonlocal block as a global attentive alignment (GAA) module, which takes the whole input video sequence into consideration to capture features that are globally correlated, to perform implicit alignment. Furthermore, given the bulk of features extracted from the center and neighboring frames, we propose target-guided attention (TGA). TGA can exploit more center-frame-related details and facilitate feature fusion. The proposed network (dubbed TANet) is capable of effectively eliminating false contours and recovering the center frame in high quality, as demonstrated by the experimental results. TANet outperforms state-of-the-art models in terms of both PSNR and SSIM with low time consumption.
C1 [Liu, Jing; Yang, Ziwen; Su, Yuting] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Yang, Xiaokang] Shanghai Jiao Tong Univ, AI Inst, MoE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China.
C3 Tianjin University; Shanghai Jiao Tong University
RP Su, YT (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM jliu_tju@tju.edu.cn; yangs163email@163.com; ytsu@tju.edu.cn;
   xkyang@sjtu.edu.cn
RI Yang, Xiaokang/C-6137-2009
OI Yang, Xiaokang/0000-0003-4029-3322
FU Natural Science Foundation of Tianjin [20JCQNJC01150]; Key Laboratory of
   Artificial Intelligence, Ministry of Education [AI2020006]; National
   Science Foundation of China [61701341]
FX This work was supported in part by the Natural Science Foundation of
   Tianjin under Grant 20JCQNJC01150, in part by the Key Laboratory of
   Artificial Intelligence, Ministry of Education, under Grant AI2020006,
   and in part by National Science Foundation of China under Grant
   61701341.
CR Anderson J.R., 2009, COGNITIVE PSYCHOL IT, V7th
   Bahdanau D., 2015, PROC INT C LEARNING
   Bahdanau D., 2014, 3 INT C LEARN REPR
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Buades A, 2016, IEEE T IMAGE PROCESS, V25, P2573, DOI 10.1109/TIP.2016.2551639
   Byun J, 2019, LECT NOTES COMPUT SC, V11362, P67, DOI 10.1007/978-3-030-20890-5_5
   Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Cheng CH, 2009, IEEE INT SYMP CIRC S, P944, DOI 10.1109/ISCAS.2009.5117913
   Deng JN, 2020, AAAI CONF ARTIF INTE, V34, P10696
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Guan ZY, 2021, IEEE T PATTERN ANAL, V43, P949, DOI 10.1109/TPAMI.2019.2944806
   Hong RC, 2016, IEEE T IMAGE PROCESS, V25, P5814, DOI 10.1109/TIP.2016.2614132
   Hsiao YH, 2004, IEEE IMAGE PROC, P1449
   Hu HW, 2019, IEEE T MULTIMEDIA, V21, P510, DOI 10.1109/TMM.2018.2859831
   Hu P, 2018, IEEE T MULTIMEDIA, V20, P2814, DOI 10.1109/TMM.2018.2815784
   Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340
   Kappeler A, 2016, IEEE T COMPUT IMAG, V2, P109, DOI 10.1109/TCI.2016.2532323
   Katsaggelos A.K., 2007, Synth. Lectures Image, Video, Multimedia Process., V1, P1
   Kingma DP., 2014, ADAM METHOD STOCHAST
   Li JX, 2021, IEEE T MULTIMEDIA, V23, P1397, DOI 10.1109/TMM.2020.2997192
   Li S, 2019, PROC CVPR IEEE, P10514, DOI 10.1109/CVPR.2019.01077
   Liu J, 2020, IEEE SIGNAL PROC LET, V27, P2014, DOI 10.1109/LSP.2020.3035065
   Liu J, 2019, IEEE T MULTIMEDIA, V21, P2397, DOI 10.1109/TMM.2019.2897909
   Liu J, 2019, IEEE T IMAGE PROCESS, V28, P4926, DOI 10.1109/TIP.2019.2912294
   Liu J, 2018, IEEE T IMAGE PROCESS, V27, P4860, DOI 10.1109/TIP.2018.2803306
   Ma C, 2019, IEEE T PATTERN ANAL, V41, P2709, DOI [10.1109/TPAMI.2018.2865311, 10.1109/INTMAG.2018.8508195]
   Mittal G., 2012, VCIP, P1
   Pengfei Wan, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P170, DOI 10.1109/ICME.2012.118
   Su YT, 2019, NEUROCOMPUTING, V347, P200, DOI 10.1016/j.neucom.2019.04.011
   Sun W., 2017, International Forum on Digital TV and Wireless Multimedia Commun, V815, P255
   Tang XT, 2020, DISPLAYS, V62, DOI 10.1016/j.displa.2020.101944
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Ulichney R, 1998, P SOC PHOTO-OPT INS, V3300, P232, DOI 10.1117/12.298285
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan PF, 2016, IEEE T IMAGE PROCESS, V25, P2896, DOI 10.1109/TIP.2016.2553523
   Wang LG, 2020, IEEE T IMAGE PROCESS, V29, P4323, DOI 10.1109/TIP.2020.2967596
   Wang Q, 2020, IEEE T IMAGE PROCESS, V29, P7549, DOI 10.1109/TIP.2020.3004249
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yan PX, 2019, IEEE I CONF COMP VIS, P7283, DOI 10.1109/ICCV.2019.00738
   Yang R, 2018, PROC CVPR IEEE, P6664, DOI 10.1109/CVPR.2018.00697
   Yi P, 2019, IEEE I CONF COMP VIS, P3106, DOI 10.1109/ICCV.2019.00320
   Zhang J, 2021, DISPLAYS, V69, DOI 10.1016/j.displa.2021.102040
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao Y, 2021, IEEE T CIRC SYST VID, V31, P2063, DOI 10.1109/TCSVT.2020.2982505
   Zhao Y, 2019, IEEE T IMAGE PROCESS, V28, P2847, DOI 10.1109/TIP.2019.2891131
NR 47
TC 2
Z9 2
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4212
EP 4223
DI 10.1109/TMM.2021.3115039
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 4K8TP
UT WOS:000852215000005
DA 2024-07-18
ER

PT J
AU Ou, FZ
   Wang, YG
   Li, J
   Zhu, GP
   Kwong, S
AF Ou, Fu-Zhao
   Wang, Yuan-Gen
   Li, Jin
   Zhu, Guopu
   Kwong, Sam
TI A Novel Rank Learning Based No-Reference mage Quality Assessment Method
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE No reference image quality assessment; convolutional neural network;
   rank learning; synthetic distortion; authentic distortion
ID IMAGE; STATISTICS
AB Recently, applying deep learning to no-reference image quality assessment (NR-IQA) has received significant attention. Especially in the last five years, an increasing interest has been drawn to the studies of rank learning since it can help mitigate the problem of small IQA datasets. However, on one hand, existing rank learning is not suitable for the authentically distorted images due to the lack of generated rank samples. On the other hand, the output of existing rank loss functions is uncontrollable, resulting in reduced performance. Motivated by these two limitations, we propose a novel rank learning based NR-IQA method, termed controllable list-wise ranking IQA (CLRIQA) in this paper. To be specific, we first present an imaging-heuristic approach, in which the over- and under-exposure is formulated as an inverse of the Weber-Fechner law, and fusion strategy and compression are adopted, to simulate the authentic distortion and generate the rank image samples. These samples are label-free yet associated with quality ranking information. Then we design a controllable list-wise ranking (CLR) loss function by setting an upper and lower bound of rank range and introducing an adaptive margin to tune rank interval. Finally, both the generated rank samples and proposed CLR are used to pre-train a convolutional neural network. Moreover, to obtain a more accurate prediction model, we take advantage of the IQA datasets to fine-tune the pre-trained network further. Various experiments are conducted on the IQA benchmark datasets, and experimental results demonstrate the effectiveness of the proposed CLRIQA method. The source code and network model can be downloaded at the following web address: https://github.com / GZHU-DVL / CLRIQA.
C1 [Ou, Fu-Zhao; Wang, Yuan-Gen] Guangzhou Univ, Sch Comp Sci & Cyber Engn, Guangzhou 510006, Peoples R China.
   [Li, Jin] Guangzhou Univ, Inst Artificial Intelligence & Blockchain, Guangzhou 510006, Peoples R China.
   [Zhu, Guopu] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Kowloon Tong, Hong Kong, Peoples R China.
C3 Guangzhou University; Guangzhou University; Harbin Institute of
   Technology; City University of Hong Kong
RP Wang, YG (corresponding author), Guangzhou Univ, Sch Comp Sci & Cyber Engn, Guangzhou 510006, Peoples R China.
EM oufuzhao@e.gzhu.edu.cn; wangyg@gzhu.edu.cn; lijin@gzhu.edu.cn;
   guopu.zhu@hit.edu.cn; cssamk@cityu.edu.hk
RI Kwong, Sam/C-9319-2012; Ou, Fu-Zhao/HZI-2452-2023
OI Kwong, Sam/0000-0001-7484-7261; Ou, Fu-Zhao/0000-0003-1245-8345; Wang,
   Yuan-Gen/0000-0003-3010-4196; Zhu, Guopu/0000-0001-7956-5343
FU National Natural Science Foundation of China [61872099, 61872350]; Hong
   Kong GRF-RGC General Research Fund [CityU 11209819, CityU 11203820];
   Science and Technology Program of Guangzhou [201904010478];
   ScientificResearch Project of Guangzhou University [YJ2021004]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61872099 and 61872350, in part by Hong
   Kong GRF-RGC General Research Fund CityU 11209819 and CityU 11203820, in
   part by the Science and Technology Program of Guangzhou underGrant
   201904010478, and in part by the ScientificResearch Project of Guangzhou
   University under Grant YJ2021004.
CR Bianco S, 2018, SIGNAL IMAGE VIDEO P, V12, P355, DOI 10.1007/s11760-017-1166-8
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Branson S, 2014, Arxiv, DOI arXiv:1406.2952
   Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373
   Gao F, 2015, IEEE T NEUR NET LEAR, V26, P2275, DOI 10.1109/TNNLS.2014.2377181
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Gu J, 2019, AAAI CONF ARTIF INTE, P8336
   Gu J, 2018, IEEE T MULTIMEDIA, V20, P1140, DOI 10.1109/TMM.2017.2761993
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Hecht S, 1924, J GEN PHYSIOL, V7, P235, DOI 10.1085/jgp.7.2.235
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kim J, 2019, IEEE T NEUR NET LEAR, V30, P11, DOI 10.1109/TNNLS.2018.2829819
   Kim J, 2017, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2017.213
   Kim J, 2017, IEEE SIGNAL PROC MAG, V34, P130, DOI 10.1109/MSP.2017.2736018
   Kim J, 2017, IEEE J-STSP, V11, P206, DOI 10.1109/JSTSP.2016.2639328
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li DQ, 2019, IEEE T MULTIMEDIA, V21, P1221, DOI 10.1109/TMM.2018.2875354
   Li QH, 2019, NEUROCOMPUTING, V331, P189, DOI 10.1016/j.neucom.2018.11.015
   Lin HH, 2019, INT WORK QUAL MULTIM
   Lin KY, 2018, PROC CVPR IEEE, P732, DOI 10.1109/CVPR.2018.00083
   Liu XL, 2019, IEEE T PATTERN ANAL, V41, P1862, DOI 10.1109/TPAMI.2019.2899857
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   Ma L, 2016, IEEE T MULTIMEDIA, V18, P2228, DOI 10.1109/TMM.2016.2614187
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Ou FZ, 2019, IEEE IMAGE PROC, P1004, DOI [10.1109/icip.2019.8803047, 10.1109/ICIP.2019.8803047]
   Pareek H. H., 2014, ADV NEURAL INFORM PR, P361
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Ren HY, 2018, AAAI CONF ARTIF INTE, P7308
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Tang HX, 2014, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2014.368
   van Hateren JH, 2007, J VISION, V7, DOI 10.1167/7.4.1
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu QB, 2020, IEEE T CIRC SYST VID, V30, P3883, DOI 10.1109/TCSVT.2020.2972566
   Wu QB, 2018, IEEE T IMAGE PROCESS, V27, P2499, DOI 10.1109/TIP.2018.2799331
   Wu QB, 2017, IEEE T MULTIMEDIA, V19, P2490, DOI 10.1109/TMM.2017.2700206
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Xu L, 2016, DISPLAYS, V44, P21, DOI 10.1016/j.displa.2016.06.002
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Yan QS, 2019, IEEE T IMAGE PROCESS, V28, P2200, DOI 10.1109/TIP.2018.2883741
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Zeng H, 2018, IEEE IMAGE PROC, P609, DOI 10.1109/ICIP.2018.8451285
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang Q., 2017, U.S. Pat., Patent No. [9,734,567, 9734567]
   Zhang WX, 2021, IEEE T IMAGE PROCESS, V30, P3474, DOI 10.1109/TIP.2021.3061932
   Zhang WX, 2020, IEEE IMAGE PROC, P111, DOI [10.1109/icip40778.2020.9191278, 10.1109/ICIP40778.2020.9191278]
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhang Y, 2020, IEEE T NEUR NET LEAR, V31, P2716, DOI 10.1109/TNNLS.2018.2890310
NR 58
TC 14
Z9 14
U1 3
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4197
EP 4211
DI 10.1109/TMM.2021.3114551
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 4K8TP
UT WOS:000852215000004
DA 2024-07-18
ER

PT J
AU Shin, J
   Park, H
   Paik, J
AF Shin, Joongchol
   Park, Hasil
   Paik, Joonki
TI Region-Based Dehazing via Dual-Supervised Triple-Convolutional Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Atmospheric modeling; Degradation; Image edge detection; Brightness;
   Rail to rail outputs; Neural networks; Image restoration; Dehazing;
   image fusion; neural networks; optimization; self-supervised learning
ID IMAGE QUALITY; FEATURES; VISIBILITY; FUSION
AB Most physical model-based dehazing methods are subject to contrast degradation in a dark or shadow region because of the mismatch between the physical model and real haze. This degradation decreases the quality of dehazed images. Furthermore, the retinex-haze combined models can cause the brightness saturation problem in a haze region. For this reason, the retinex-haze combined approaches are not appropriate to enhance the real-world haze images. To solve these problems, we present a novel region-based dehazing method via dual-supervised triple-convolutional network (TCN). More specifically, the proposed network first simulates the mismatch problem based on the region-model. Next, we then train the proposed triple-convolutional network, which can estimate the degraded regions. We then present a novel dual-supervised learning method to efficiently train the networks using a non-ideal dataset. Experimental results show that the proposed method outperforms state-of-the-art approaches in solving complex haze problems. The output of the proposed network has a high-similarity index in most cases for various benchmark dataset. Our approach also produces high-quality images in real haze image datasets.
C1 [Shin, Joongchol; Park, Hasil; Paik, Joonki] Chung Ang Univ, Dept Images, Seoul 06974, South Korea.
C3 Chung Ang University
RP Paik, J (corresponding author), Chung Ang Univ, Dept Images, Seoul 06974, South Korea.
EM your2759@daum.net; hahaha2470@cau.ac.kr; paikj@cau.ac.kr
RI Paik, Joonki/AAN-7017-2020; Paik, Joonki/D-7635-2012
OI Shin, Joongchol/0000-0003-3818-6587; Paik, Joonki/0000-0002-8593-7155
FU Institute for Information and communications Technology Promotion (IITP)
   - Korea government(MSIT) [2017-0-00 250]; National R&D Program through
   the National Research Foundation of Korea (NRF) - Ministry of Science,
   and ICT [2020M3F6A1110350]
FX This work was supported in part by Institute for Information and
   communications Technology Promotion (IITP) grant funded by the Korea
   government(MSIT) (2017-0-00 250, Intelligent Defense Boundary
   Surveillance Technology Using Collaborative Reinforced Learning of
   Embedded Edge Camera, and Image Analysis), and in part by the National
   R&D Program through the National Research Foundation of Korea(NRF)
   funded by Ministry of Science, and ICT(2020M3F6A1110350). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Jingdong Wang.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   [Anonymous], 1952, VISION ATMOSPHERE
   [Anonymous], 2007, Advances in Neural Information Processing Systems
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Bui TM, 2018, IEEE T IMAGE PROCESS, V27, P999, DOI 10.1109/TIP.2017.2771158
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen C, 2016, LECT NOTES COMPUT SC, V9906, P576, DOI 10.1007/978-3-319-46475-6_36
   Chen QF, 2017, IEEE I CONF COMP VIS, P2516, DOI 10.1109/ICCV.2017.273
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Guo XP, 2019, IEEE T MULTIMEDIA, V21, P1982, DOI 10.1109/TMM.2019.2895292
   Guo XF, 2017, LECT NOTES COMPUT SC, V10635, P373, DOI 10.1007/978-3-319-70096-0_39
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Ishitani T, 2007, J ELECTRON MICROSC, V56, P145, DOI 10.1093/jmicro/dfm007
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jin ZW, 2017, IEEE T MULTIMEDIA, V19, P598, DOI 10.1109/TMM.2016.2617078
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D. P., 2015, PROC INT C LEARN REP, P1
   Kingma D. P., 2014, AUTOENCODING VARIATI, P3581
   Kostic Z, 2020, IEEE T MULTIMEDIA, V22, P1904, DOI 10.1109/TMM.2020.2966890
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Li Y, 2017, COMPUT VIS IMAGE UND, V165, P1, DOI 10.1016/j.cviu.2017.09.003
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Narasimhan S. G., 2003, Interactive (de) weathering of an image using physical models, V6, P1
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Oakley JP, 1998, IEEE T IMAGE PROCESS, V7, P167, DOI 10.1109/83.660994
   Rahman Z, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P1003, DOI 10.1109/ICIP.1996.560995
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Santra S, 2018, IEEE T IMAGE PROCESS, V27, P4598, DOI 10.1109/TIP.2018.2841198
   Santra S, 2016, INT C PATT RECOG, P1406, DOI 10.1109/ICPR.2016.7899834
   Sharma G, 2005, COLOR RES APPL, V30, P21, DOI 10.1002/col.20070
   Shen LH, 2019, IEEE T MULTIMEDIA, V21, P1093, DOI 10.1109/TMM.2018.2871955
   Shin J, 2020, IEEE T MULTIMEDIA, V22, P30, DOI 10.1109/TMM.2019.2922127
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang KT, 2014, PROC CVPR IEEE, P2995, DOI 10.1109/CVPR.2014.383
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang DZ, 2019, IEEE T MULTIMEDIA, V21, P2985, DOI 10.1109/TMM.2019.2920620
   Wang W, 2020, IEEE T MULTIMEDIA, V22, P2808, DOI 10.1109/TMM.2019.2963621
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie SY, 2019, IEEE T MULTIMEDIA, V21, P211, DOI 10.1109/TMM.2018.2844085
   Yang JC, 2019, IEEE T MULTIMEDIA, V21, P1750, DOI 10.1109/TMM.2018.2889562
   Yu F, 2015, P INT C LEARN REPR
   Yu T, 2019, IEEE ACCESS, V7, P114619, DOI 10.1109/ACCESS.2019.2936049
   Zeng ZL, 2020, IEEE T MULTIMEDIA, V22, P3064, DOI 10.1109/TMM.2020.2969782
   Zhang YF, 2017, IEEE IMAGE PROC, P3205, DOI 10.1109/ICIP.2017.8296874
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 57
TC 24
Z9 26
U1 3
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 245
EP 260
DI 10.1109/TMM.2021.3050053
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300019
DA 2024-07-18
ER

PT J
AU Shu, ZY
   Yang, SP
   Xin, SQ
   Pang, CY
   Jin, XG
   Kavan, LDL
   Liu, LG
AF Shu, Zhenyu
   Yang, Sipeng
   Xin, Shiqing
   Pang, Chaoyi
   Jin, Xiaogang
   Kavan, Ladislav
   Liu, Ligang
TI Detecting 3D Points of Interest Using Projective Neural Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Shape; Three-dimensional displays; Feature extraction; Two dimensional
   displays; Neural networks; Training; Cameras; 3D shapes; Point of
   interest; Convolutional neural networks
ID SALIENCY DETECTION; MESH SALIENCY
AB Detecting points of interest on 3D shapes is a fundamental research problem in geometry processing. Due to the complicated relationship between points of interest and their geometric features, detecting points of interest on any given 3D shape remains challenging. Due to the lack of training data, previous data-driven methods for detecting 3D points of interest mainly focus on utilizing hand-crafted geometric features to predict the probabilities of each point being a POI, which greatly limits detection performance. In this paper, we propose a novel algorithm for detecting 3D points of interest by using projective neural networks. Our method first projects the labeled training 3D shapes into multiple 2D views and then learns the required features from the 2D views in an end-to-end fashion. The points of interest on test 3D shapes are then automatically detected by applying the learned neural network and our improved density peak clustering. Our method relies neither on hand-crafted feature descriptors nor a large quantity of expensive 3D training data to obtain satisfactory results. Experimental results show significantly superior detection performance of our method over the state-of-the-art methods.
C1 [Shu, Zhenyu; Pang, Chaoyi] NingboTech Univ, Sch Comp & Data Engn, Ningbo 315100, Peoples R China.
   [Shu, Zhenyu; Pang, Chaoyi] Zhejiang Univ, Ningbo Inst, Ningbo 315100, Peoples R China.
   [Yang, Sipeng] Zhejiang Univ, Sch Mech Engn, Hangzhou 310058, Peoples R China.
   [Xin, Shiqing] Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Shandong, Peoples R China.
   [Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.
   [Kavan, Ladislav] Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.
   [Liu, Ligang] Univ Sci & Technol China, Sch Math Sci, Graph & Geometr Comp Lab, Hefei 230026, Anhui, Peoples R China.
C3 NingboTech University; Zhejiang University; Zhejiang University;
   Shandong University; Zhejiang University; Utah System of Higher
   Education; University of Utah; Chinese Academy of Sciences; University
   of Science & Technology of China, CAS
RP Yang, SP (corresponding author), Zhejiang Univ, Sch Mech Engn, Hangzhou 310058, Peoples R China.
EM shuzhenyu@nit.zju.edu.cn; 21825205@zju.edu.cn; xinshiqing@sdu.edu.cn;
   chaoyi.pang@qq.com; jin@cad.zju.edu.cn; ladislav.kavan@gmail.com;
   lgliu@ustc.edu.cn
RI Pang, Chaoyi/JQX-1513-2023; wang, qi/ITT-9652-2023
OI shu, zhenyu/0000-0001-5733-6638; Jin, Xiaogang/0000-0001-7339-2920; Xin,
   Shiqing/0000-0001-8452-8723
FU National Natural Science Foundation of China [61872321, 62025207,
   61772016, 61572022, 62036010]; National Science Foundation [IIS-1617172,
   IIS-1622360, IIS-1764071]; Open Research Projects of Zhejiang Lab
   [2019NB0AB03]; Ningbo Major Special Projects of the "Science and
   Technology Innovation 2025" [2020Z005, 2020Z007]; Ningbo Innovative Team
   [2016C11024]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61872321, 62025207, 61772016, 61572022,
   and 62036010, in part by National Science Foundation under Grants
   IIS-1617172, IIS-1622360, and IIS-1764071, in part by Open Research
   Projects of Zhejiang Lab underGrants 2019NB0AB03, in part by theNingbo
   Major Special Projects of the "Science andTechnology Innovation 2025"
   under Grants 2020Z005 and 2020Z007, and in part by Ningbo Innovative
   Team under Grant 2016C11024. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Sebastian Knorr.
CR Akilan T, 2019, IEEE T VEH TECHNOL, V68, P9478, DOI 10.1109/TVT.2019.2937076
   [Anonymous], 2011, SPIE EL IM INT SOC O
   [Anonymous], 2011, PROC EUROGRAPHICS 20, P79, DOI DOI 10.2312/3DOR/3DOR11/079-088
   Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005
   Bu SH, 2014, IEEE T MULTIMEDIA, V16, P2154, DOI 10.1109/TMM.2014.2351788
   Castellani U, 2008, COMPUT GRAPH FORUM, V27, P643, DOI 10.1111/j.1467-8659.2008.01162.x
   Chen JY, 2014, IEEE T MULTIMEDIA, V16, P337, DOI 10.1109/TMM.2013.2286580
   Chen XB, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185525
   Creusot C, 2013, INT J COMPUT VISION, V102, P146, DOI 10.1007/s11263-012-0605-9
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dutagaci H., 2011, Proceedings of the 4th Eurographics conference on 3D Object Retrieval, P65
   Dutagaci H, 2012, VISUAL COMPUT, V28, P901, DOI 10.1007/s00371-012-0746-4
   Feixas M, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1462055.1462056
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Gelfand N., 2005, P 3 EUR S GEOM PROC, V2, P5
   Harris C., 1988, ALVEY VISION C, P147151
   Jeong SW, 2017, IEEE T MULTIMEDIA, V19, P2692, DOI 10.1109/TMM.2017.2710802
   Jia SX, 2014, GRAPH MODELS, V76, P355, DOI 10.1016/j.gmod.2014.03.012
   Kalogerakis E, 2017, PROC CVPR IEEE, P6630, DOI 10.1109/CVPR.2017.702
   Katz S, 2005, VISUAL COMPUT, V21, P649, DOI 10.1007/s00371-005-0344-9
   Kim Y, 2006, IEEE T VIS COMPUT GR, V12, P925, DOI 10.1109/TVCG.2006.174
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Lavoue G., 2012, EUROGRAPHICS WORKSHO, P93
   Liu XY, 2016, COMPUT GRAPH-UK, V57, P12, DOI 10.1016/j.cag.2016.03.001
   Neubeck A, 2006, INT C PATT RECOG, P850, DOI 10.1109/icpr.2006.479
   Pratikakis I., 2010, EUR WORKSH 3D OBJ RE, P7, DOI DOI 10.2312/3DOR/3DOR10/007-014
   Qi CR, 2017, ADV NEUR IN, V30
   Rodriguez A, 2014, SCIENCE, V344, P1492, DOI 10.1126/science.1242072
   Shtrom E, 2013, IEEE I CONF COMP VIS, P3591, DOI 10.1109/ICCV.2013.446
   Shu ZY, 2019, IEEE T VIS COMPUT GR, V25, P2583, DOI 10.1109/TVCG.2018.2848628
   Sipiran I, 2011, VISUAL COMPUT, V27, P963, DOI 10.1007/s00371-011-0610-y
   Sipiran I, 2013, VISUAL COMPUT, V29, P1319, DOI 10.1007/s00371-013-0870-9
   Song R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2530691
   Song R, 2012, IEEE IMAGE PROC, P637, DOI 10.1109/ICIP.2012.6466940
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310
   Tasse F. P., 2016, SIGGRAPH ASIA 2016 T
   Tasse FP, 2015, IEEE I CONF COMP VIS, P163, DOI 10.1109/ICCV.2015.27
   Teran L, 2014, LECT NOTES COMPUT SC, V8689, P159, DOI 10.1007/978-3-319-10590-1_11
   Xie J, 2017, IEEE T MULTIMEDIA, V19, P2463, DOI 10.1109/TMM.2017.2698200
   Yang ZX, 2018, COGN COMPUT, V10, P908, DOI 10.1007/s12559-018-9598-1
   You Y, 2020, PROC IEEE CVF C COMP, p13 647
   Zhen QK, 2016, IEEE T MULTIMEDIA, V18, P1438, DOI 10.1109/TMM.2016.2557063
   Zou GY, 2008, COMPUT ANIMAT VIRT W, V19, P399, DOI 10.1002/cav.244
NR 46
TC 3
Z9 3
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1637
EP 1650
DI 10.1109/TMM.2021.3070977
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200030
OA Bronze
DA 2024-07-18
ER

PT J
AU Sun, LJ
   Feng, SH
   Liu, J
   Lyu, GY
   Lang, CY
AF Sun, Lijuan
   Feng, Songhe
   Liu, Jun
   Lyu, Gengyu
   Lang, Congyan
TI Global-Local Label Correlation for Partial Multi-Label Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlation; Noise measurement; Predictive models; Matrix decomposition;
   Task analysis; Manifolds; Sparse matrices; Partial multi-label learning;
   label correlations; label coefficient matrix; label manifold
ID LOW-RANK; MISSING LABELS; CLASSIFICATION; GRAPH
AB Partial Multi-label Learning (PML) addresses the scenario where each instance is assigned with multiple candidate labels, while only a subset of the labels are relevant. This task is very challenging because the training procedure can be misguided by the noisy (irrelevant) labels. Exploiting label correlations is useful for partial multi-label learning. However, the existing PML methods often ignore to explicitly and sufficiently leverage the label correlation information for handling the noisy labels. To this end, in this paper, we propose a novel Global-Local Label Correlation (GLC) approach for partial multi-label learning. On one hand, we introduce a label coefficient matrix to explicitly exploit the global structure information of labels from multiple subspaces. On the other hand, we present a new label manifold regularizer to capture the local label correlations to further improve the performance of our method. By jointly taking advantage of the global and local label correlations, our proposed approach achieves superior performance on both the synthetic and real-world data sets from diverse domains.
C1 [Sun, Lijuan; Feng, Songhe; Lyu, Gengyu; Lang, Congyan] Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
   [Sun, Lijuan; Feng, Songhe; Lyu, Gengyu; Lang, Congyan] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.
   [Liu, Jun] Singapore Univ Technol, Informat Syst Technol & Design Pillar, Singapore 487372, Singapore.
C3 Beijing Jiaotong University; Beijing Jiaotong University
RP Feng, SH (corresponding author), Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
EM 17112082@bjtu.edu.cn; shfeng@bjtu.edu.cn; jun_liu@sutd.edu.sg;
   18112030@bjtu.edu.cn; cylang@bjtu.edu.cn
OI Liu, Jun/0000-0002-4365-4165
FU National Natural Science Foundation of China [61872032, 61972030,
   62072027, 62076021]; Beijing Natural Science Foundation [4202058,
   4202057, 4202060]; National Key Research and Development Project
   [2018AAA0100300]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61872032, 61972030, 62072027, and
   62076021, in part by the Beijing Natural Science Foundation under Grants
   4202058, 4202057, and 4202060, and in part by National Key Research and
   Development Project 2018AAA0100300.
CR [Anonymous], 2010, P ACM SIGKDD
   [Anonymous], 2019, 33 AAAI C ART INT
   Bertsekas D. P., 1997, Journal of the Operational Research Society, V48, P334, DOI 10.1057/palgrave.jors.2600425
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Bucak S. S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2801, DOI 10.1109/CVPR.2011.5995734
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Chen YY, 2020, IEEE T IMAGE PROCESS, V29, P1426, DOI 10.1109/TIP.2019.2941319
   Chen ZS, 2020, AAAI CONF ARTIF INTE, V34, P3553
   Chen ZM, 2021, IEEE T MULTIMEDIA, V23, P1827, DOI 10.1109/TMM.2020.3003779
   Cheng WW, 2009, MACH LEARN, V76, P211, DOI 10.1007/s10994-009-5127-5
   Demsar J, 2006, J MACH LEARN RES, V7, P1
   Diplaris S, 2005, LECT NOTES COMPUT SC, V3746, P448
   Elisseeff A, 2002, ADV NEUR IN, V14, P681
   Fei Wu, 2015, IEEE Transactions on Big Data, V1, P109, DOI 10.1109/TBDATA.2015.2497270
   Feng L, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2294
   Feng SH, 2018, MULTIDIM SYST SIGN P, V29, P1351, DOI 10.1007/s11045-017-0505-9
   Fürnkranz J, 2008, MACH LEARN, V73, P133, DOI 10.1007/s10994-008-5064-8
   Huang D, 2016, IEEE T PATTERN ANAL, V38, P363, DOI 10.1109/TPAMI.2015.2448091
   Huang J, 2019, INFORM SCIENCES, V492, P124, DOI 10.1016/j.ins.2019.04.021
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Ji S., 2008, P 14 ACM SIGKDD INT, P381, DOI [DOI 10.1145/1401890.1401939, 10.1145/1401890.1401939]
   Ji SW, 2010, ACM T KNOWL DISCOV D, V4, DOI 10.1145/1754428.1754431
   Kapoor Ashish, 2012, Advances in Neural Information Processing Systems, P2645
   Li JD, 2018, ACM COMPUT SURV, V50, DOI 10.1145/3136625
   Li ZC, 2021, SCI CHINA INFORM SCI, V64, DOI 10.1007/s11432-020-3063-0
   Li ZC, 2020, INT J COMPUT VISION, V128, P2265, DOI 10.1007/s11263-020-01331-0
   Li ZW, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2612
   Lin Z., 2011, ADV NEURAL INFORM PR, V24, P612, DOI DOI 10.1007/S11263-013-0611-6
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu WW, 2017, J MACH LEARN RES, V18
   Liu WW, 2019, IEEE T PATTERN ANAL, V41, P408, DOI 10.1109/TPAMI.2018.2794976
   Liu WW, 2017, J MACH LEARN RES, V18
   Lyu GY, 2021, INFORM SCIENCES, V543, P454, DOI 10.1016/j.ins.2020.09.019
   Lyu GY, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P105, DOI 10.1145/3394486.3403053
   Mencía EL, 2008, LECT NOTES ARTIF INT, V5212, P50, DOI 10.1007/978-3-540-87481-2_4
   Montañes E, 2014, PATTERN RECOGN, V47, P1494, DOI 10.1016/j.patcog.2013.09.029
   Peng ZM, 2019, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2019.00053
   Pestian J.P., 2007, P WORKSH BIONLP 2007, P97, DOI 10.3115/1572392.1572411
   Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5
   Sun LJ, 2019, AAAI CONF ARTIF INTE, P5016
   Tsoumakas G., 2008, P ECMLPKDD 2008 WORK, P53, DOI DOI 10.1007/978-3-642-12837-0_11
   Wang HB, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3691
   Wu BY, 2018, INT J COMPUT VISION, V126, P875, DOI 10.1007/s11263-018-1085-3
   Wu JH, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P557, DOI 10.1145/3394486.3403098
   Wu JH, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P416, DOI 10.1145/3292500.3330901
   Xie MK, 2020, AAAI CONF ARTIF INTE, V34, P6454
   Xie MK, 2018, AAAI CONF ARTIF INTE, P4302
   Xu C, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1275, DOI 10.1145/2939672.2939798
   Xu LL, 2014, IEEE DATA MINING, P1067, DOI 10.1109/ICDM.2014.125
   Xu Miao, 2013, ADV NEURAL INFORM PR, P2301, DOI DOI 10.5555/2999792.2999869
   Xu N, 2020, AAAI CONF ARTIF INTE, V34, P6510
   Yu GX, 2018, IEEE DATA MINING, P1398, DOI 10.1109/ICDM.2018.00192
   Zhang M., 2020, IEEE T PATTERN ANAL, P1
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
   Zhang ML, 2015, IEEE T PATTERN ANAL, V37, P107, DOI 10.1109/TPAMI.2014.2339815
   Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39
   Zhang QW, 2018, AAAI CONF ARTIF INTE, P4446
   Zhang Y, 2017, IEEE C COMP VIS PATT, P5287, DOI DOI 10.1109/CVPR.2017.537
   Zhang YS, 2020, IEEE T MULTIMEDIA, V22, P2844, DOI 10.1109/TMM.2020.2966887
   Zhao FP, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4062
   Zhou Q, 2016, IEEE T PATTERN ANAL, V38, P266, DOI 10.1109/TPAMI.2015.2452911
   Zhu Y, 2018, IEEE T KNOWL DATA EN, V30, P1081, DOI 10.1109/TKDE.2017.2785795
   Zhuang LS, 2012, PROC CVPR IEEE, P2328, DOI 10.1109/CVPR.2012.6247944
NR 63
TC 24
Z9 24
U1 7
U2 39
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 581
EP 593
DI 10.1109/TMM.2021.3055959
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100006
DA 2024-07-18
ER

PT J
AU Yin, CX
   Tang, J
   Yuan, TT
   Xu, ZY
   Wang, YZ
AF Yin, Chengxiang
   Tang, Jian
   Yuan, Tongtong
   Xu, Zhiyuan
   Wang, Yanzhi
TI Bridging the Gap Between Semantic Segmentation and Instance Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Image segmentation; Real-time systems; Training; Task
   analysis; Neurons; Head; Semantic segmentation; instance segmentation;
   real-time; gap; sem2Ins
AB Fine-grained instance segmentation is considerably more complicated and challenging than semantic segmentation. Most existing instance segmentation methods only focus on accuracy without paying much attention to inference latency, which, is critical to real-time applications, such as autonomous driving. In this paper, we aim to bridge the gap between semantic segmentation and instance segmentation by presenting a novel real-time model for instance segmentation, Sem2Ins, which effectively generates instance boundaries according to a semantic segmentation by leveraging conditional generative adversarial networks (cGANs) coupled with deep supervision and a weighted fusion layer. Specifically, supervision is imposed on each output layer, and features from different levels are fused to produce a well-generated boundary map. Sem2Ins has the following desirable features: 1) Combined with some fast semantic segmentation methods, Sem2Ins runs at a real-time speed that is fairly well-balanced against accuracy; 2) Sem2Ins works flexibly with any semantic segmentation model for instance segmentation, and if the given semantic segmentation is sufficiently good, Sem2Ins even achieves state-of-the-art in terms of accuracy; 3) deep supervision and weighted fusion can be leveraged to generate high-quality boundaries; and 4) Sem2Ins can be easily extended to panoptic segmentation. Extensive experiments performed on the Cityscapes, WildDash, KITTI and COCO benchmarks have demonstrated that 1) Sem2Ins, when combined with PSPNet and DDRNet-23-Slim, consistently outperforms the state-of-the-art real-time solution (Box2Pix) in terms of both speed and accuracy; and 2) Sem2Ins combined with DPC performs comparably to some powerful detect-and-segment approaches.
C1 [Yin, Chengxiang] Syracuse Univ, Dept Elect Engn & Comp Sci, Syracuse, NY 13244 USA.
   [Tang, Jian; Xu, Zhiyuan] Midea Grp, Shanghai 201702, Peoples R China.
   [Yuan, Tongtong] Beijing Univ Technol, Beijing 100124, Peoples R China.
   [Wang, Yanzhi] Northeastern Univ, Dept Elect & Engn, Boston, MA 02115 USA.
C3 Syracuse University; Midea; Beijing University of Technology;
   Northeastern University
RP Tang, J (corresponding author), Midea Grp, Shanghai 201702, Peoples R China.
EM cyin02@syr.edu; tangjian22@midea.com; yuantt@bjut.edu.cn;
   xuzy70@midea.com; yanzhiwang@northeastern.edu
OI Yin, Chengxiang/0000-0002-3238-960X; Xu, Zhiyuan/0000-0003-2879-3244
CR Arnab A, 2017, PROC CVPR IEEE, P879, DOI 10.1109/CVPR.2017.100
   Bai M, 2017, PROC CVPR IEEE, P2858, DOI 10.1109/CVPR.2017.305
   Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LC, 2018, ADV NEUR IN, V31
   Chen LC, 2018, PROC CVPR IEEE, P4013, DOI 10.1109/CVPR.2018.00422
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   chenjun2hao, 2020, CHENJUN2HAO
   CoinCheung, 2020, US
   Cordts M., 2016, US
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343
   De Brabandere Bert, 2017, Semantic Instance Segmentation with a Discriminative Loss Function
   DeepSceneSeg, 2018, US
   Fang Yuxin, 2021, Proceedings of the IEEE/CVF International Conference on Computer Vision, P6910
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Girshick R., 2018, US
   Hayder Z, 2017, PROC CVPR IEEE, P587, DOI 10.1109/CVPR.2017.70
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hong Yuanduo, 2021, ARXIV210106085
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   King DB, 2015, ACS SYM SER, V1214, P1
   Kirillov A, 2019, PROC CVPR IEEE, P9396, DOI 10.1109/CVPR.2019.00963
   Kirillov A, 2017, PROC CVPR IEEE, P7322, DOI 10.1109/CVPR.2017.774
   KITTIdataset, KITTIDATASET
   Kong S, 2018, PROC CVPR IEEE, P9018, DOI 10.1109/CVPR.2018.00940
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   Lee Y, 2020, P IEEE CVF C COMP VI
   Li J, 2019, Arxiv, DOI arXiv:1812.01192
   Li YW, 2019, PROC CVPR IEEE, P7019, DOI 10.1109/CVPR.2019.00719
   Li Y, 2019, IEEE T MULTIMEDIA, V21, P875, DOI 10.1109/TMM.2018.2867720
   Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472
   Li Z., ARXIV210409805, V2021
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu CX, 2019, PROC CVPR IEEE, P82, DOI 10.1109/CVPR.2019.00017
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu S, 2017, IEEE I CONF COMP VIS, P3516, DOI 10.1109/ICCV.2017.378
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Long J., 2015, P IEEE C COMP VIS PA, P3431, DOI DOI 10.48550/ARXIV.1411.4038
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Nabavi S., 2018, US
   Neven D, 2019, PROC CVPR IEEE, P8829, DOI 10.1109/CVPR.2019.00904
   Paszke A, 2016, Arxiv, DOI [arXiv:1606.02147, 10.48550/arXiv.1606.02147, DOI 10.48550/ARXIV.1606.02147]
   Pinheiro PO, 2016, LECT NOTES COMPUT SC, V9905, P75, DOI 10.1007/978-3-319-46448-0_5
   Porzi L, 2019, PROC CVPR IEEE, P8269, DOI 10.1109/CVPR.2019.00847
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren MY, 2017, PROC CVPR IEEE, P293, DOI 10.1109/CVPR.2017.39
   Romera-Paredes B, 2016, LECT NOTES COMPUT SC, V9910, P312, DOI 10.1007/978-3-319-46466-4_19
   Ruan T, 2019, AAAI CONF ARTIF INTE, P4814
   Rufeng Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10223, DOI 10.1109/CVPR42600.2020.01024
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Teichmann MTT, 2018, Arxiv, DOI arXiv:1805.04777
   Tensorflow, 2018, US
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tianheng Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P660, DOI 10.1007/978-3-030-58568-6_39
   Treml M., 2016, SPEEDING SEMANTIC SE, V2, P1
   Uhrig J, 2018, IEEE INT VEH SYM, P292, DOI 10.1109/IVS.2018.8500621
   Valada A, 2020, INT J COMPUT VISION, V128, P1239, DOI 10.1007/s11263-019-01188-y
   van den Brand J, 2017, LECT NOTES COMPUT SC, V10116, P477, DOI 10.1007/978-3-319-54407-6_32
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xiong YW, 2019, PROC CVPR IEEE, P8810, DOI 10.1109/CVPR.2019.00902
   Yu C., 2021, INT J COMPUT VISION, V129, P3051, DOI [DOI 10.1007/s11263-021-01515-2, 10.1007/s11263-021-01515-2]
   Yu ZD, 2017, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2017.191
   Zand M, 2016, IEEE T IMAGE PROCESS, V25, P3233, DOI 10.1109/TIP.2016.2552401
   Zendel O, 2018, LECT NOTES COMPUT SC, V11210, P407, DOI 10.1007/978-3-030-01231-1_25
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhi Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P282, DOI 10.1007/978-3-030-58452-8_17
   Zhuang YQ, 2018, IEEE IMAGE PROC, P3698, DOI 10.1109/ICIP.2018.8451830
NR 77
TC 16
Z9 16
U1 4
U2 43
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4183
EP 4196
DI 10.1109/TMM.2021.3114541
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 4K8TP
UT WOS:000852215000003
DA 2024-07-18
ER

PT J
AU Zeng, HT
   Song, XH
   Chen, GW
   Jiang, SQ
AF Zeng, Haitao
   Song, Xinhang
   Chen, Gongwei
   Jiang, Shuqiang
TI Amorphous Region Context Modeling for Scene Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Feature extraction; Image segmentation; Convolution; Context
   modeling; Saliency detection; Layout; Graph neural network; scene
   recognition; semantic segmentation
ID CLASSIFICATION; STUFF
AB Scene images are usually composed of foreground and background regional contents. Some existing methods propose to extract regional contents with dense grids or objectness region proposals. However, dense grids may split the object into several discrete parts, learning semantic ambiguity for the patches. The objectness methods may focus on particular objects but only pay attention to the foreground contents and do not exploit the background that is key to scene recognition. In contrast, we propose a novel scene recognition framework with amorphous region detection and context modeling. In the proposed framework, discriminative regions are first detected with amorphous contours that can tightly surround the targets through semantic segmentation techniques. In addition, both foreground and background regions are jointly embedded to obtain the scene representations with the graph model. Based on the graph modeling module, we explore the contextual relations between the regions in geometric and morphology aspects, and generate the discriminative representations for scene recognition. Experimental results on MIT67 and SUN397 demonstrate the effectiveness and generality of the proposed method.
C1 [Zeng, Haitao; Song, Xinhang; Chen, Gongwei; Jiang, Shuqiang] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Zeng, Haitao; Song, Xinhang; Chen, Gongwei; Jiang, Shuqiang] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Jiang, SQ (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM haitao.zeng@vipl.ict.ac.cn; xinhang.song@vipl.ict.ac.cn;
   gongwei.chen@vipl.ict.ac.cn; sqjiang@ict.ac.cn
RI Zeng, Haitao/ISU-2331-2023
OI Zeng, Haitao/0000-0003-1538-461X; Chen, Gongwei/0000-0002-0634-6075;
   song, xinhang/0000-0002-0895-1076
FU National Key Research and Development Project of New Generation
   Artificial Intelligence of China [2018AAA0102500]; National Natural
   Science Foundation of China [61532018, 61902378, 62032022, U1936203];
   Beijing Natural Science Foundation [L182054, Z190020]; Lenovo
   Outstanding Young Scientists Program; National Postdoctoral Program for
   Innovative Talents [BX201700255]
FX This work was supported by the National Key Research and Development
   Project of New Generation Artificial Intelligence of China, under Grant
   2018AAA0102500, in part by the National Natural Science Foundation of
   China under Grants 61532018, 61902378, 62032022, and U1936203, in part
   by Beijing Natural Science Foundation under Grants L182054 and Z190020,
   in part by the Lenovo Outstanding Young Scientists Program, and in part
   by the National Postdoctoral Program for Innovative Talents under Grant
   BX201700255. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. L. Ballan.
CR Bassiouny A, 2014, IEEE IMAGE PROC, P981, DOI 10.1109/ICIP.2014.7025197
   Caesar H, 2018, PROC CVPR IEEE, P1209, DOI 10.1109/CVPR.2018.00132
   Chen GW, 2020, IEEE T IMAGE PROCESS, V29, P5877, DOI 10.1109/TIP.2020.2986599
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Cheng XJ, 2018, PATTERN RECOGN, V74, P474, DOI 10.1016/j.patcog.2017.09.025
   Dixit M, 2015, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2015.7298916
   Fredembach C, 2004, IEEE T PATTERN ANAL, V26, P1645, DOI 10.1109/TPAMI.2004.123
   Guo S, 2017, IEEE T IMAGE PROCESS, V26, P808, DOI 10.1109/TIP.2016.2629443
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heitz G, 2008, LECT NOTES COMPUT SC, V5302, P30, DOI 10.1007/978-3-540-88682-2_4
   Herranz L, 2016, PROC CVPR IEEE, P571, DOI 10.1109/CVPR.2016.68
   Huang GL, 2017, IEEE ICC
   Jiang SQ, 2019, IEEE T MULTIMEDIA, V21, P1609, DOI 10.1109/TMM.2018.2876830
   Jiang YN, 2012, LECT NOTES COMPUT SC, V7573, P730, DOI 10.1007/978-3-642-33709-3_52
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Kalayeh MM, 2017, PROC CVPR IEEE, P4227, DOI 10.1109/CVPR.2017.450
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Kipf TN, 2017, INT C LEARN REPR
   Koniusz P, 2018, PROC CVPR IEEE, P5774, DOI 10.1109/CVPR.2018.00605
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li LJ, 2014, INT J COMPUT VISION, V107, P20, DOI 10.1007/s11263-013-0660-x
   Li YS, 2017, IEEE I CONF COMP VIS, P5757, DOI 10.1109/ICCV.2017.613
   Lin D, 2014, PROC CVPR IEEE, P3726, DOI 10.1109/CVPR.2014.476
   Liu Y, 2018, AAAI CONF ARTIF INTE, P7178
   López-Cifuentes A, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107256
   Niepert M., MACH LEARN
   Pan YS, 2019, IEEE T IMAGE PROCESS, V28, P4716, DOI 10.1109/TIP.2019.2908795
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Seong H, 2020, IEEE ACCESS, V8, P82066, DOI 10.1109/ACCESS.2020.2989863
   Serrano N, 2004, PATTERN RECOGN, V37, P1773, DOI 10.1016/j.patcog.2004.03.003
   Shen JL, 2005, 11TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P340, DOI 10.1109/MMMC.2005.66
   Shi J, 2019, IEEE ACCESS, V7, P45230, DOI 10.1109/ACCESS.2019.2908448
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Song XH, 2017, IEEE T IMAGE PROCESS, V26, P2721, DOI 10.1109/TIP.2017.2686017
   Song XH, 2016, PATTERN RECOGN, V59, P98, DOI 10.1016/j.patcog.2016.01.019
   Szummer M, 1998, 1998 IEEE INTERNATIONAL WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO DATABASE, PROCEEDINGS, P42, DOI 10.1109/CAIVD.1998.646032
   Vailaya A, 2001, IEEE T IMAGE PROCESS, V10, P117, DOI 10.1109/83.892448
   Vailaya A, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P518, DOI 10.1109/MMCS.1999.779255
   Vaswani A, 2017, ADV NEUR IN, V30
   Vogel J, 2007, INT J COMPUT VISION, V72, P133, DOI 10.1007/s11263-006-8614-1
   Wang LM, 2017, IEEE T IMAGE PROCESS, V26, P2055, DOI 10.1109/TIP.2017.2675339
   Wang XL, 2018, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR.2018.00717
   Wang Z, 2017, IEEE T IMAGE PROCESS, V26, P2028, DOI 10.1109/TIP.2017.2666739
   Weng CQ, 2017, IEEE SIGNAL PROC LET, V24, P1143, DOI 10.1109/LSP.2016.2641020
   Wu RB, 2015, IEEE I CONF COMP VIS, P1287, DOI 10.1109/ICCV.2015.152
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Xie GS, 2017, IEEE T CIRC SYST VID, V27, P1263, DOI 10.1109/TCSVT.2015.2511543
   Xie L, 2020, IEEE T MULTIMEDIA, V22, P1182, DOI 10.1109/TMM.2019.2942478
   Xie L, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107205
   Yang M, 2015, IEEE IMAGE PROC, P402, DOI 10.1109/ICIP.2015.7350829
   Zeng HT, 2020, IEEE T MULTIMEDIA, V22, P1519, DOI 10.1109/TMM.2019.2944241
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao ZY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1760, DOI 10.1145/3240508.3240698
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zuo Z, 2016, IEEE T IMAGE PROCESS, V25, P2983, DOI 10.1109/TIP.2016.2548241
NR 58
TC 3
Z9 4
U1 3
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 141
EP 151
DI 10.1109/TMM.2020.3046877
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300011
DA 2024-07-18
ER

PT J
AU Zhang, JS
   Chen, KJ
   Qin, C
   Zhang, WM
   Yu, NH
AF Zhang, Jiansong
   Chen, Kejiang
   Qin, Chuan
   Zhang, Weiming
   Yu, Nenghai
TI Distribution-Preserving-Based Automatic Data Augmentation for Deep Image
   Steganalysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Feature extraction; Training; Convolution; Payloads;
   Steganography; Databases; Image; steganography; steganalysis; data
   augmentation
ID LEARNING FRAMEWORK; RESIDUAL NETWORK; STEGANOGRAPHY
AB In recent years, deep learning-based steganalyzers far outperformed handcrafted feature-based steganalyzers. However, a large amount of data is needed to train deep learning networks. For steganalysis tasks, the steganographic traces are subtle and the steganographic signals are difficult to be captured when the number of cover/stego pairs in the training set is insufficient. Data augmentation has been proved to be effective in improving accuracy and generalization for deep learning models. Yet not all data augmentation methods are universal for all tasks. When performing data augmentation, we argue that data distribution under the target tasks should be maintained. Since the steganalysis task is more concerned with the high-frequency signals of the images, if the high-frequency signals are unchanged, the data distribution from the perspective of steganalysis will remain largely unchanged. Based on this principle, we designed a neural network called cover augmentation network, which enriches the dataset by intelligently adding noise to the original cover to generate the augmented cover. Further, we designed a whole process of data augmentation based on the cover augmentation network. Experimental results show that the proposed data augmentation method can effectively improve the performance of steganalysis networks, and the advantage is significant at low payloads.
C1 [Zhang, Jiansong; Chen, Kejiang; Qin, Chuan; Zhang, Weiming; Yu, Nenghai] Univ Sci & Technol China, CAS Key Lab Electromagnet Space Informat, Hefei 230026, Peoples R China.
   [Zhang, Jiansong; Chen, Kejiang; Qin, Chuan; Zhang, Weiming; Yu, Nenghai] Anhui Prov Key Lab Cyberspace Secur Situat Awarene, Hefei, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Chen, KJ (corresponding author), Univ Sci & Technol China, CAS Key Lab Electromagnet Space Informat, Hefei 230026, Peoples R China.
EM zjs0014@mail.ustc.edu.cn; chenkj@ustc.edu.cn; qc94@mail.ustc.edu.cn;
   zhangwm@ustc.edu.cn; ynh@ustc.edu.cn
OI Zhang, Weiming/0000-0001-5576-6108; Chen, Kejiang/0000-0002-9868-3414;
   Qin, Chuan/0000-0002-5841-8210; Zhang, Jiansong/0000-0001-9288-8665
FU Natural Science Foundation of China [62102386, 62002334, 62072421];
   China Postdoctoral Science Foundation [2021M693091]; Open Fund of Anhui
   Province Key Laboratory of Cyberspace Security Situation Awareness, and
   Evaluation; Anhui Science Foundation of China [2008085QF296]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 62102386, 62002334, and 62072421, and in part by
   China Postdoctoral Science Foundation under Grant 2021M693091, and in
   part by Open Fund of Anhui Province Key Laboratory of Cyberspace
   Security Situation Awareness, and Evaluation , and in part by Anhui
   Science Foundation of China under Grant 2008085QF296.
CR [Anonymous], 2011, J. Inform. Hid. Multimed. Signal Process.
   Bas Patrick, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P59, DOI 10.1007/978-3-642-24178-9_5
   Bernard S, 2021, PROCEEDINGS OF THE 2021 ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, IH&MMSEC 2021, P105, DOI 10.1145/3437880.3460407
   Boroumand M, 2019, IEEE T INF FOREN SEC, V14, P1181, DOI 10.1109/TIFS.2018.2871749
   Bradley AP, 1997, PATTERN RECOGN, V30, P1145, DOI 10.1016/S0031-3203(96)00142-2
   Cheddad A, 2010, SIGNAL PROCESS, V90, P727, DOI 10.1016/j.sigpro.2009.08.010
   Cogranne R, 2015, IEEE T INF FOREN SEC, V10, P2627, DOI 10.1109/TIFS.2015.2470220
   Denemark T, 2014, IEEE INT WORKS INFOR, P48, DOI 10.1109/WIFS.2014.7084302
   Deng XQ, 2019, IH&MMSEC '19: PROCEEDINGS OF THE ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, P230, DOI 10.1145/3335203.3335739
   DeVries T, 2017, Arxiv, DOI [arXiv:1708.04552, DOI 10.48550/ARXIV.1708.04552]
   Feng GR, 2020, IEEE T CIRC SYST VID, V30, P376, DOI 10.1109/TCSVT.2019.2891778
   Filler T, 2011, IEEE T INF FOREN SEC, V6, P920, DOI 10.1109/TIFS.2011.2134094
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Furon T, 2008, EURASIP J INF SECUR, DOI 10.1155/2008/597040
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Glorot X., 2010, 13 INT C ARTIFICIAL, V9, P249
   Guo LJ, 2015, IEEE T INF FOREN SEC, V10, P2669, DOI 10.1109/TIFS.2015.2473815
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hestness J, 2017, Arxiv, DOI arXiv:1712.00409
   Holub V, 2014, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2014-1
   Holub V, 2013, PROC SPIE, V8665, DOI 10.1117/12.1000330
   Holub V, 2012, IEEE INT WORKS INFOR, P234, DOI 10.1109/WIFS.2012.6412655
   Hussain M, 2018, SIGNAL PROCESS-IMAGE, V65, P46, DOI 10.1016/j.image.2018.03.012
   Itzhaki T, 2021, IEEE INT WORKS INFOR, P98, DOI 10.1109/WIFS53200.2021.9648390
   Jang E., 2017, PROC 5 INT C LEARN R, V1050, P17
   Jin ZY, 2020, SIGNAL PROCESS, V170, DOI 10.1016/j.sigpro.2020.107455
   Kodovsky J, 2012, IEEE T INF FOREN SEC, V7, P432, DOI 10.1109/TIFS.2011.2175919
   Kodovsky J, 2011, PROC SPIE, V7880, DOI 10.1117/12.872279
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li B, 2018, IEEE T INF FOREN SEC, V13, P1242, DOI 10.1109/TIFS.2017.2780805
   Li B, 2014, IEEE IMAGE PROC, P4206, DOI 10.1109/ICIP.2014.7025854
   Li L, 2020, IEEE T MULTIMEDIA, V22, P2526, DOI 10.1109/TMM.2019.2959909
   Lie WN, 2005, IEEE T MULTIMEDIA, V7, P1007, DOI 10.1109/TMM.2005.858377
   Ma YY, 2019, IEEE T CIRC SYST VID, V29, P336, DOI 10.1109/TCSVT.2018.2799243
   Maddison C. J., 2017, ICLR, P1
   Mielikainen J, 2006, IEEE SIGNAL PROC LET, V13, P285, DOI 10.1109/LSP.2006.870357
   Mo XB, 2021, IEEE T INF FOREN SEC, V16, P4306, DOI 10.1109/TIFS.2021.3104140
   Park DS, 2019, INTERSPEECH, P2613, DOI 10.21437/Interspeech.2019-2680
   Ruiz Hugo, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12666), P439, DOI 10.1007/978-3-030-68780-9_36
   Sedighi V, 2016, IEEE T INF FOREN SEC, V11, P221, DOI 10.1109/TIFS.2015.2486744
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Tan SQ, 2021, IEEE T INF FOREN SEC, V16, P131, DOI 10.1109/TIFS.2020.3005304
   Tang WX, 2021, IEEE T INF FOREN SEC, V16, P952, DOI 10.1109/TIFS.2020.3025438
   Tang WX, 2017, IEEE SIGNAL PROC LET, V24, P1547, DOI 10.1109/LSP.2017.2745572
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu GS, 2016, IEEE SIGNAL PROC LET, V23, P708, DOI 10.1109/LSP.2016.2548421
   Yang JH, 2020, IEEE T INF FOREN SEC, V15, P839, DOI 10.1109/TIFS.2019.2922229
   Ye J, 2017, IEEE T INF FOREN SEC, V12, P2545, DOI 10.1109/TIFS.2017.2710946
   Yedroudj Mehdi, 2020, IH&MMSec '20: Proceedings of the 2020 ACM Workshop on Information Hiding and Multimedia Security, P39, DOI 10.1145/3369412.3395061
   Yedroudj M, 2018, ELECT IMAGING, DOI [10.2352/ISSN.2470-1173.2018.07.MWSF-317, DOI 10.2352/ISSN.2470-1173.2018.07.MWSF-317]
   Yedroudj M, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2092, DOI 10.1109/ICASSP.2018.8461438
   Yu IJ, 2020, ELECTRON LETT, V56, DOI 10.1049/el.2020.1951
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zeng JS, 2018, IEEE T INF FOREN SEC, V13, P1200, DOI 10.1109/TIFS.2017.2779446
   Zhang C., 2020, Adv. Neural Inf. Process. Syst., V33, P10223
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhu C, 2020, PROC 8 INT C LEARN R
NR 57
TC 7
Z9 8
U1 3
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4538
EP 4550
DI 10.1109/TMM.2021.3119994
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 7B6NT
UT WOS:000899248400008
DA 2024-07-18
ER

PT J
AU Zhang, N
   Zhao, Y
   Wang, C
   Wang, RG
AF Zhang, Ning
   Zhao, Yang
   Wang, Chao
   Wang, Ronggang
TI A Real-Time Semi-Supervised Deep Tone Mapping Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image color analysis; Generative adversarial networks; Training; Image
   coding; Dynamic range; Task analysis; Deep learning; High dynamic range;
   tone mapping; semi-supervised; light-weight
ID DYNAMIC-RANGE IMAGE; REPRODUCTION; COMPRESSION; ALGORITHM; MODEL
AB Tone mapping operators (TMOs) can compress the range of high dynamic range (HDR) images so that they can be displayed normally on the low dynamic range (LDR) devices. Recent TMOs based on deep neural networks can produce impressive results, but there are still some shortcomings. On the one hand, their supervised learning procedure requires a high-quality paired dataset which is hard to be accessed. On the other hand, they are too slow and heavy to meet the needs of practical applications. This paper proposes a real-time deep semi-supervised learning TMO to solve the above problems. The proposed method learns in a semi-supervised manner by combining the adversarial loss, cycle consistency loss, and the pixel-wise loss. The first two can simulate the image distributions in the real world from the unpaired LDR data and the latter can learn the guidance of paired LDR labels. In this way, the proposed method only requires HDR sources, unpaired high-quality LDR images, and a few well tone-mapped HDR-LDR pairs as training data. Furthermore, the proposed method divides tone mapping into luminance mapping and saturation adjustment and then processes them simultaneously. By this strategy, we can reconstruct each component more precisely. Based on the aforementioned improvements, we propose a lightweight tone mapping network that is efficient in tone mapping task (up to 5000x parameters-saving and 27x time-saving compared to the learning-based TMOs). Both quantitative and qualitative results demonstrate that the proposed method performs favorable against state-of-the-art TMOs.
C1 [Zhang, Ning; Wang, Ronggang] Peking Univ, Grad Sch, Sch Elect & Comp Engn, Shenzhen 518055, Guangdong, Peoples R China.
   [Zhao, Yang] Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Anhui, Peoples R China.
   [Wang, Chao] Max Planck Inst Info, Dept Comp Graph, D-66123 Munich, Germany.
C3 Peking University; Hefei University of Technology
RP Wang, RG (corresponding author), Peking Univ, Grad Sch, Sch Elect & Comp Engn, Shenzhen 518055, Guangdong, Peoples R China.
EM zhangn77@pku.edu.cn; yzhao@hfut.edu.cn; winchao@pku.edu.cn;
   rgwang@pkusz.edu.cn
RI Wang, Chao/JXM-2578-2024
OI Wang, Ronggang/0000-0003-0873-0465; Wang, Chao/0000-0002-9698-6737
FU National Natural Science Foundation of China [61672063, 62072013,
   61972129]; Shenzhen Research [JCYJ20180503182128089, 201806080921419290,
   RCJC20200714114435057]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61672063, 62072013, and 61972129 and in
   part by the Shenzhen Research under Projects JCYJ20180503182128089,
   201806080921419290, and RCJC20200714114435057.
CR [Anonymous], 1994, Graph. Gems, DOI DOI 10.1016/B978-0-12-336156-1.50054-9
   [Anonymous], 2017, ARXIV PREPRINT ARXIV
   B. Series, 2012, Recommendation ITU-R BT, V500, P500
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Barakat N, 2008, IEEE T IMAGE PROCESS, V17, P1864, DOI 10.1109/TIP.2008.2001414
   Boitard R, 2015, IEEE CONSUM ELECTR M, V4, P72, DOI 10.1109/MCE.2015.2463294
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660
   Chen Y, 2020, NEUROCOMPUTING, V411, P468, DOI 10.1016/j.neucom.2020.06.067
   Debevec P.E., 2008, P 24 ANN C COM GRAPH, P1
   Dong YY, 2016, IEEE T MULTIMEDIA, V18, P549, DOI 10.1109/TMM.2016.2522639
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Endo Y, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130834
   Fairchild MD, 2007, FIFTEENTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, AND APPLICATIONS, FINAL PROGRAM AND PROCEEDINGS, P233
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Guo XP, 2019, IEEE T MULTIMEDIA, V21, P1982, DOI 10.1109/TMM.2019.2895292
   Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352
   Huang YF, 2021, IEEE T MULTIMEDIA, V23, P176, DOI 10.1109/TMM.2020.2981994
   Khan IR, 2018, IEEE T IND ELECTRON, V65, P3469, DOI 10.1109/TIE.2017.2760247
   Kim T, 2017, PR MACH LEARN RES, V70
   Klambauer G, 2017, ADV NEUR IN, V30
   Kou F, 2018, IEEE T MULTIMEDIA, V20, P484, DOI 10.1109/TMM.2017.2743988
   Lee S, 2021, IEEE T MULTIMEDIA, V23, P2561, DOI 10.1109/TMM.2020.3013378
   Lee S, 2018, LECT NOTES COMPUT SC, V11206, P613, DOI 10.1007/978-3-030-01216-8_37
   Lee S, 2018, IEEE ACCESS, V6, P49913, DOI 10.1109/ACCESS.2018.2868246
   Li H, 2018, COMPUT VIS IMAGE UND, V168, P37, DOI 10.1016/j.cviu.2017.11.001
   Liang ZT, 2018, PROC CVPR IEEE, P4758, DOI 10.1109/CVPR.2018.00500
   Liang ZT, 2016, IEEE T IMAGE PROCESS, V25, P673, DOI 10.1109/TIP.2015.2507405
   Lin YT, 2017, IEEE T MULTIMEDIA, V19, P196, DOI 10.1109/TMM.2016.2605499
   Liu YL, 2020, PROC CVPR IEEE, P1648, DOI 10.1109/CVPR42600.2020.00172
   Mai ZC, 2013, IEEE T MULTIMEDIA, V15, P1503, DOI 10.1109/TMM.2013.2266633
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Marnerides D, 2018, COMPUT GRAPH FORUM, V37, P37, DOI 10.1111/cgf.13340
   Odena A., 2016, DISTILL, V1, P3, DOI [10.23915/distill.00003., DOI 10.23915/DISTILL, 10.23915/distill.00003, DOI 10.23915/DISTILL.00003]
   Patel V. A., 2017, P NAT C COMP VIS PAT, P220
   Radford A., 2015, ARXIV151106434
   Rana A, 2020, IEEE T IMAGE PROCESS, V29, P1285, DOI 10.1109/TIP.2019.2936649
   Rana A, 2019, IEEE T MULTIMEDIA, V21, P256, DOI 10.1109/TMM.2018.2839885
   Rawat W, 2017, NEURAL COMPUT, V29, P2352, DOI [10.1162/NECO_a_00990, 10.1162/neco_a_00990]
   Reinhard E, 2005, IEEE T VIS COMPUT GR, V11, P13, DOI 10.1109/TVCG.2005.9
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   ROMANIUK SG, 1993, NEURAL NETWORKS, V6, P1105, DOI 10.1016/S0893-6080(09)80022-1
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shan Q, 2010, IEEE T VIS COMPUT GR, V16, P663, DOI 10.1109/TVCG.2009.92
   Shibata T, 2016, PROC CVPR IEEE, P2745, DOI 10.1109/CVPR.2016.300
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   Toro W. M. M., 2019, ARXIV191209601
   Tsai CY, 2012, IEEE T MULTIMEDIA, V14, P1140, DOI 10.1109/TMM.2012.2190390
   TUMBLIN J, 1993, IEEE COMPUT GRAPH, V13, P42, DOI 10.1109/38.252554
   Xu J, 2020, IEEE T IMAGE PROCESS, V29, P5022, DOI 10.1109/TIP.2020.2974060
   Yeganeh H, 2013, IEEE T IMAGE PROCESS, V22, P657, DOI 10.1109/TIP.2012.2221725
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yuan Y, 2018, IEEE COMPUT SOC CONF, P814, DOI 10.1109/CVPRW.2018.00113
   Zhang N, 2019, 2019 IEEE VISUAL COM, P1
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 59
TC 9
Z9 10
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2815
EP 2827
DI 10.1109/TMM.2021.3089019
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000010
DA 2024-07-18
ER

PT J
AU Zhang, Q
   Nie, YW
   Zhu, L
   Xiao, CX
   Zheng, WS
AF Zhang, Qing
   Nie, Yongwei
   Zhu, Lei
   Xiao, Chunxia
   Zheng, Wei-Shi
TI A Blind Color Separation Model for Faithful Palette-Based Image
   Recoloring
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image color analysis; Computational modeling; Hemorrhaging; Distortion;
   Image coding; Visualization; Computer science; Color separation;
   palette-based recoloring
AB Palette-based image recoloring provides a simple yet effective way for color adjustment, which allows users to interactively manipulate the color of an image by editing a compact color palette. While remarkable progress has been made by previous methods, they have the common limitations that may produce unfaithful image recoloring results i.e., the obtained result does not respond faithfully to the palette adjustment, and tend to induce visual artifacts such as color bleeding and distortion. To address these limitations, we in this paper present a novel color separation model for palette-based recoloring. Akin to previous methods, our color separation model is built upon the assumption that color of each pixel in an image can be formulated as a linear combination of a small set of same basis colors. However, different from previous palette-based recoloring methods which typically rely on heuristic rules to build the color separation model, we experimentally reveal the underlying relationship between the color separation and the palette-based recoloring, and summarize three specialized color separation priors that allow more faithful palette-based recoloring. Based on these priors, we devise a blind color separation model that not only does not require known palette as input as done in previous methods, but also enables more effective palette-based recoloring with much less visual artifacts. Experiments on two datasets demonstrate that our method outperforms the state-of-the-art palette-based recoloring methods. In addition, we show some applications enabled by the proposed color separation model, including automatic pattern coloring generation, green screen keying and region-controllable color transfer.
C1 [Zhang, Qing; Zheng, Wei-Shi] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Zheng, Wei-Shi] Peng Cheng Aboratory, Shenzhen 518005, Peoples R China.
   [Zheng, Wei-Shi] Sun Yat Sen Univ, Key Lab Machine Intelligence & Adv Comp, Minist Educ, Guangzhou 510006, Peoples R China.
   [Nie, Yongwei] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Zhu, Lei] Univ Cambridge, Dept Appl Math & Theoret Phys, Cambridge CB2 1TN, England.
   [Xiao, Chunxia] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
C3 Sun Yat Sen University; Sun Yat Sen University; South China University
   of Technology; University of Cambridge; Wuhan University
RP Zheng, WS (corresponding author), Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.; Zheng, WS (corresponding author), Peng Cheng Aboratory, Shenzhen 518005, Peoples R China.
EM zhangqing.whu.cs@gmail.com; nieyongwei@scut.edu.cn; lz437@cam.ac.uk;
   cxxiao@whu.edu.cn; wszheng@ieee.org
RI zheng, wei/IQT-9639-2023
OI Zhu, Lei/0000-0003-3871-663X
FU National Key Research and Development Program of China [2016YFB1001001];
   NSFC [61802453, U1911401, U1811461, 62072191, U1803120]; Fundamental
   Research Funds for the Central Universities [D2190670, 19lgpy216];
   Guangdong NSF [2019A1515010860, 2020B1515120085, 2018B030312002];
   Guangzhou Research Project [201902010037]; Research Projects of Zhejiang
   Laboratory [2019KD0AB03]; Key-Area Research and Development Program of
   Guangzhou [202007030004]
FX This work was supported by the National Key Research and Development
   Program of China 2016YFB1001001, the NSFC 61802453, U1911401, and
   U1811461, in part by Fundamental Research Funds for the Central
   Universities 19lgpy216, Guangdong NSF under Project 2020B1515120085 and
   2018B030312002, in part by Guangzhou Research Project 201902010037, in
   part by Research Projects of Zhejiang Laboratory No. 2019KD0AB03, and in
   part by the Key-Area Research and Development Program of Guangzhou
   202007030004, NSFC 62072191, U1803120, Guangdong NSF under Project
   2019A1515010860, and Fundamental Research Funds for the Central
   Universities D2190670.. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. W. Li.
CR Afifi M, 2019, ELECTRON LETT, V55, P531, DOI 10.1049/el.2019.0064
   Afifi M., 2019, EUROGRAPHICS SHORT P, P33
   Aksoy Y, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3002176
   Aksoy Y, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2907940
   An XB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360639
   [Anonymous], 2005, Computational Aesthetics in Graphics, Visualization and Imaging, DOI [DOI 10.2312/COMPAESTH/COMPAESTH05/111-122, 10.2312/COMPAESTH/COMPAESTH05/111-122]
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Chang HW, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766978
   Chen XB, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185525
   Chen XW, 2014, PROC CVPR IEEE, pCP5, DOI 10.1109/CVPR.2014.365
   Cheng ZZ, 2015, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2015.55
   Deshpande A, 2017, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2017.307
   Farbman Z, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866171
   He MM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201365
   Huang CR, 2011, IEEE T MULTIMEDIA, V13, P950, DOI 10.1109/TMM.2011.2135844
   Hwang Y, 2014, PROC CVPR IEEE, P3342, DOI 10.1109/CVPR.2014.427
   Iizuka S, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925974
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Laffont PY, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601101
   Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35
   Lee JY, 2016, PROC CVPR IEEE, P2470, DOI 10.1109/CVPR.2016.271
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P1699, DOI 10.1109/TPAMI.2008.168
   Li Y, 2010, COMPUT GRAPH FORUM, V29, P2049, DOI 10.1111/j.1467-8659.2010.01791.x
   Lin Sharon, 2017, ARXIV170103754
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P1724, DOI 10.1109/TMM.2017.2780761
   Liu MY, 2017, ADV NEUR IN, V30
   Nguyen TV, 2013, IEEE T MULTIMEDIA, V15, P1910, DOI 10.1109/TMM.2013.2272919
   Pitié F, 2005, IEEE I CONF COMP VIS, P1434
   Pitié F, 2007, COMPUT VIS IMAGE UND, V107, P123, DOI 10.1016/j.cviu.2006.11.011
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Shugrina M., 2018, ARXIV180602918
   Song ZC, 2017, IEEE T MULTIMEDIA, V19, P702, DOI 10.1109/TMM.2016.2631123
   Su Z, 2014, IEEE T MULTIMEDIA, V16, P988, DOI 10.1109/TMM.2014.2305914
   Tai YW, 2005, PROC CVPR IEEE, P747
   Tan JC, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275054
   Tan JC, 2019, IEEE T VIS COMPUT GR, V25, P2791, DOI 10.1109/TVCG.2018.2858238
   Tan JC, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2988229
   Wang BY, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866172
   Wang M, 2010, IEEE T MULTIMEDIA, V12, P330, DOI 10.1109/TMM.2010.2046364
   Wang YL, 2019, COMPUT GRAPH FORUM, V38, P11, DOI 10.1111/cgf.13812
   Wong BY, 2012, IEEE T MULTIMEDIA, V14, P760, DOI 10.1109/TMM.2012.2188997
   Xiao Y, 2013, IEEE T MULTIMEDIA, V15, P549, DOI 10.1109/TMM.2012.2233725
   Xu K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618464
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Zhang Q, 2021, IEEE T MULTIMEDIA, V23, P189, DOI 10.1109/TMM.2020.2982045
   Zhang Q, 2017, IEEE T IMAGE PROCESS, V26, P1952, DOI 10.1109/TIP.2017.2671779
   Zhang R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073703
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhu J.-Y., 2017, Advances in Neural Information Processing Systems, V30, P466
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 52
TC 10
Z9 11
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1545
EP 1557
DI 10.1109/TMM.2021.3067463
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200023
DA 2024-07-18
ER

PT J
AU Zhang, YF
   Zheng, JB
   Jia, WJ
   Huang, WF
   Li, L
   Liu, NA
   Li, F
   He, XJ
AF Zhang, Yuan-fang
   Zheng, Jiangbin
   Jia, Wenjing
   Huang, Wenfeng
   Li, Long
   Liu, Nian
   Li, Fei
   He, Xiangjian
TI Deep RGB-D Saliency Detection Without Depth
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Saliency detection; Fuses; Decoding; Computational
   modeling; Predictive models; Visualization; Convolutional neural
   network; depth estimation; feature fusion; saliency detection
ID OBJECT DETECTION; MODEL; ATTENTION; FEATURES; NETWORK; DRIVEN; FUSION;
   IMAGE
AB The existing saliency detection models based on RGB colors only leverage appearance cues to detect salient objects. Depth information also plays a very important role in visual saliency detection and can supply complementary cues for saliency detection. Although many RGB-D saliency models have been proposed, they require to acquire depth data, which is expensive and not easy to get. In this paper, we propose to estimate depth information from monocular RGB images and leverage the intermediate depth features to enhance the saliency detection performance in a deep neural network framework. Specifically, we first use an encoder network to extract common features from each RGB image and then build two decoder networks for depth estimation and saliency detection, respectively. The depth decoder features can be fused with the RGB saliency features to enhance their capability. Furthermore, we also propose a novel dense multiscale fusion model to densely fuse multiscale depth and RGB features based on the dense ASPP model. A new global context branch is also added to boost the multiscale features. Experimental results demonstrate that the added depth cues and the proposed fusion model can both improve the saliency detection performance. Finally, our model not only outperforms state-of-the-art RGB saliency models, but also achieves comparable results compared with state-of-the-art RGB-D saliency models.
C1 [Zhang, Yuan-fang; Zheng, Jiangbin] Northwestern Polytech Univ, Sch Comp Sci, Xian 710129, Peoples R China.
   [Zhang, Yuan-fang; Jia, Wenjing; He, Xiangjian] Univ Technol Sydney, Fac Engn & IT, Ultimo, NSW 2007, Australia.
   [Huang, Wenfeng] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518000, Peoples R China.
   [Li, Long] Northwestern Polytech Univ, Sch Automat, Xian 710129, Peoples R China.
   [Liu, Nian] Mohamed Bin Zayed Univ Artificial Intelligence, Abu Dhabi 0000000, U Arab Emirates.
   [Li, Fei] Northwestern Polytech Univ, Sch Software, Xian 710129, Peoples R China.
C3 Northwestern Polytechnical University; University of Technology Sydney;
   Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Northwestern Polytechnical University; Northwestern Polytechnical
   University
RP Zheng, JB (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Xian 710129, Peoples R China.; He, XJ (corresponding author), Univ Technol Sydney, Fac Engn & IT, Ultimo, NSW 2007, Australia.
EM zyf.robinzhang@gmail.com; zhengjb@nwpu.edu.cn; Wenjing.Jia@uts.edu.au;
   wenfeng@outlook.com; longli.nwpu@gmail.com; liunian228@gmail.com;
   foreverfei875@gmail.com; xiangjian.he@uts.edu.au
RI He, Xiangjian/CAA-1461-2022; Jia, Weijia/W-6152-2019; li, long
   li/GPX-0267-2022
OI He, Xiangjian/0000-0001-8962-540X; Jia, Wenjing/0000-0002-0940-3338; Li,
   Long/0000-0002-1939-5941; Liu, Nian/0000-0002-0825-6081
FU Innovation Foundation for Doctor Dissertation of Northwestern
   Polytechnical University [CX201959]; Synergy Innovation Foundation of
   the University and Enterprise for Graduate Students in Northwestern
   Polytechnical University [XQ201910]; National Natural Science Foundation
   of China [61976037, 61972321]
FX This work was supported in part by Innovation Foundation for Doctor
   Dissertation of Northwestern Polytechnical University (CX201959), in
   part by Synergy Innovation Foundation of the University and Enterprise
   for Graduate Students in Northwestern Polytechnical University
   (XQ201910), and in part by the National Natural Science Foundation of
   China under Grant 61972321 and the National Natural Science Foundation
   of China under Grant 61976037.
CR [Anonymous], 2014, P INT C LEARN REPR
   Bak C, 2018, IEEE T MULTIMEDIA, V20, P1688, DOI 10.1109/TMM.2017.2777665
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Chang KY, 2011, IEEE I CONF COMP VIS, P914, DOI 10.1109/ICCV.2011.6126333
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Cheng Y., 2014, ICIMCS, P23
   Ciptadi A, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.112
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Elder, 2010, P IEEE C COMP VIS PA, P49, DOI [10.1109/CVPRW.2010.5543739, DOI 10.1109/CVPRW.2010.5543739]
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Fan XC, 2021, IEEE T MOBILE COMPUT, V20, P2154, DOI 10.1109/TMC.2020.2976936
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   Han JW, 2013, IEEE T CIRC SYST VID, V23, P2009, DOI 10.1109/TCSVT.2013.2242594
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Imamoglu N, 2013, IEEE T MULTIMEDIA, V15, P96, DOI 10.1109/TMM.2012.2225034
   Ioffe S., 2015, P INT C LEARN REPR S
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Li X, 2018, LECT NOTES COMPUT SC, V11219, P370, DOI 10.1007/978-3-030-01267-0_22
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lin X, 2019, IEEE T MULTIMEDIA, V21, P1646, DOI 10.1109/TMM.2018.2884474
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu ZY, 2019, NEUROCOMPUTING, V363, P46, DOI 10.1016/j.neucom.2019.07.012
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Nian Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13753, DOI 10.1109/CVPR42600.2020.01377
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Sun JG, 2015, IEEE T IMAGE PROCESS, V24, P1639, DOI 10.1109/TIP.2015.2403241
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang NN, 2019, IEEE ACCESS, V7, P55277, DOI 10.1109/ACCESS.2019.2913107
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wang WG, 2018, PROC CVPR IEEE, P1711, DOI 10.1109/CVPR.2018.00184
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xian K, 2018, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2018.00040
   Xiao XL, 2019, IEEE T IMAGE PROCESS, V28, P2126, DOI 10.1109/TIP.2018.2882156
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xu M, 2018, IEEE T MULTIMEDIA, V20, P1335, DOI 10.1109/TMM.2017.2767784
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yongri Piao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9057, DOI 10.1109/CVPR42600.2020.00908
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zhang J., 2020, P IEEE CVF C COMP VI, P8579, DOI DOI 10.1109/CVPR42600.2020.00861
   Zhang L, 2019, PROC CVPR IEEE, P6017, DOI 10.1109/CVPR.2019.00618
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhao JX, 2019, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2019.00405
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhou H., 2020, P IEEE C COMP VIS PA, P9141, DOI 10.1109/CVPR42600.2020.00916
   Zhu CB, 2017, IEEE INT CONF COMP V, P3008, DOI 10.1109/ICCVW.2017.355
NR 76
TC 7
Z9 7
U1 2
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 755
EP 767
DI 10.1109/TMM.2021.3058788
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100019
OA Green Published
DA 2024-07-18
ER

PT J
AU Li, D
   Du, CD
   Wang, HB
   Zhou, QY
   He, HG
AF Li, Dan
   Du, Changde
   Wang, Haibao
   Zhou, Qiongyi
   He, Huiguang
TI Deep Modality Assistance Co-Training Network for Semi-Supervised
   Multi-Label Semantic Decoding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Task analysis; Decoding; Correlation; Generators; Face
   recognition; Training; Multi-label; Modality; Assistance; Co-training;
   Semi-supervised
AB Multi-label semantic decoding is a challenging task with great scientific significance and application value. The existing methods mainly focus on label learning and ignore the amount of information contained in the sample itself, especially non-image sample, which may limit their performance. To address these issues, we propose a novel semi-supervised modality assistance co-training network, which utilizes image modality to assist non-image modality for multi-label learning. In real application, there are two thorny issues: (i) non-image modality tends to be missing owing to the difficulty in obtaining them; (ii) although the image modality is easy to obtain from the Internet, image label annotation is still time-consuming and expensive. Therefore, the proposed method utilizes a small number of paired & labeled images and non-image modalities, and a large number of unpaired & unlabeled images from web sources to improve results. It consists of the modality-specific feature generators, the feature translators and the label relationship network. Specifically, the modality-specific feature generators are used to generate different features (views) for each modality. Semantic translators are employed to capture the relationship between the paired modalities and impute the missing modality feature by using unpaired & unlabeled images. Label relation network is a graph convolution network (GCN) aiming to capture the correlation between labels. To mine the information in unlabeled features, the co-training mechanism is considered. With this mechanism, we introduce a multi-view orthogonality constraint and a multi-label co-regularization constraint. Extensive experiments on three computer vision and neuroscience datasets demonstrate the effectiveness of the proposed method.
C1 [Li, Dan; Du, Changde; Wang, Haibao; Zhou, Qiongyi; He, Huiguang] Chinese Acad Sci, Res Ctr Brain Inspired Intelligence, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Li, Dan; Du, Changde; Wang, Haibao; Zhou, Qiongyi; He, Huiguang] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [He, Huiguang] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Chinese Academy of Sciences
RP He, HG (corresponding author), Chinese Acad Sci, Res Ctr Brain Inspired Intelligence, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; He, HG (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.; He, HG (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing 100190, Peoples R China.
EM lidan2017@ia.ac.cn; duchangde@gmail.com; haibao.wang@hotmail.com;
   zhouqiongyi2018@ia.ac.cn; huiguang.he@ia.ac.cn
RI He, Huiguang/AAD-7643-2020
OI He, Huiguang/0000-0002-0684-1711
FU National Natural Science Foundation of China [62020106015, 61976209];
   CAS International Collaboration Key Project [173211KYSB20190024];
   Strategic Priority Research Program of CAS [XDA2700000]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62020106015 and 61976209, in part by
   the CAS International Collaboration Key Project under Grant
   173211KYSB20190024, and in part by the Strategic Priority Research
   Program of CAS under Grant XDA2700000.
CR Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Cevikalp H, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107164
   Chang N, 2019, SCI DATA, V6, DOI 10.1038/s41597-019-0052-3
   Chen TF, 2017, IEEE INT CON MULTI, P955, DOI 10.1109/ICME.2017.8019322
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Corneanu C, 2018, LECT NOTES COMPUT SC, V11216, P309, DOI 10.1007/978-3-030-01258-8_19
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Du CD, 2022, IEEE T NEUR NET LEAR, V33, P600, DOI 10.1109/TNNLS.2020.3028167
   Du CD, 2021, INFORM FUSION, V68, P118, DOI 10.1016/j.inffus.2020.11.003
   Du CD, 2019, IEEE INT CON MULTI, P13, DOI 10.1109/ICME.2019.00011
   Du CD, 2019, IEEE T NEUR NET LEAR, V30, P2310, DOI 10.1109/TNNLS.2018.2882456
   Du JC, 2019, J AM MED INFORM ASSN, V26, P1279, DOI 10.1093/jamia/ocz085
   Ge Zongyuan, 2018, ARXIV180707247
   He JF, 2018, NEUROCOMPUTING, V275, P1893, DOI 10.1016/j.neucom.2017.10.032
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang Y, 2015, IEEE T MULTIMEDIA, V17, P1923, DOI 10.1109/TMM.2015.2476658
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Huth AG, 2016, FRONT SYST NEUROSCI, V10, DOI 10.3389/fnsys.2016.00081
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Kipf TN, 2017, INT C LEARN REPR
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li D, 2018, INT C PATT RECOG, P3796, DOI 10.1109/ICPR.2018.8545855
   Li YN, 2016, LECT NOTES COMPUT SC, V9910, P684, DOI 10.1007/978-3-319-46466-4_41
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Ma CB, 2019, LECT NOTES COMPUT SC, V11769, P730, DOI 10.1007/978-3-030-32226-7_81
   Niu Xuesong., 2019, NeurIPS, V32
   Peterson L.E., 2009, SCHOLARPEDIA, V4, P1883, DOI 10.4249/scholarpedia.1883
   Qiao SY, 2018, LECT NOTES COMPUT SC, V11219, P142, DOI 10.1007/978-3-030-01267-0_9
   Shao ZW, 2018, LECT NOTES COMPUT SC, V11217, P725, DOI 10.1007/978-3-030-01261-8_43
   Srivastava N, 2014, J MACH LEARN RES, V15, P2949
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P2837, DOI 10.1109/TMM.2019.2909860
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang JY, 2017, IEEE I CONF COMP VIS, P531, DOI 10.1109/ICCV.2017.65
   Wu X, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3884
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Yeh CK, 2017, AAAI CONF ARTIF INTE, P2838
   Zhang CQ, 2018, AAAI CONF ARTIF INTE, P4414
   Zhang L, 2018, IEEE T MULTIMEDIA, V20, P128, DOI 10.1109/TMM.2017.2723841
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
   Zhao KL, 2016, IEEE T IMAGE PROCESS, V25, P3931, DOI 10.1109/TIP.2016.2570550
   Zhou Q., 2020, P INT JOINT C NEUR N, P1
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu Y, 2018, IEEE T KNOWL DATA EN, V30, P1081, DOI 10.1109/TKDE.2017.2785795
NR 45
TC 1
Z9 2
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG 18
PY 2021
VL 24
BP 3287
EP 3299
DI 10.1109/TMM.2021.3104980
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NE
UT WOS:000824706300001
DA 2024-07-18
ER

PT J
AU Xu, ZW
   Wang, GZ
   Wong, YK
   Kankanhalli, MS
AF Xu, Ziwei
   Wang, Guangzhi
   Wong, Yongkang
   Kankanhalli, Mohan S.
TI Relation-Aware Compositional Zero-Shot Learning for Attribute-Object
   Pair Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Task analysis; Semantics; Training; Image recognition;
   Feature extraction; Computational modeling; Compositional zero-shot
   learning; image recognition; message passing
AB This paper proposes a novel model for recognizing images with composite attribute-object concepts, notably for composite concepts that are unseen during model training. We aim to explore the three key properties required by the task - relation-aware, consistent, and decoupled-to learn rich and robust features for primitive concepts that compose attribute-object pairs. To this end, we propose the Blocked Message Passing Network (BMP-Net). The model consists of two modules. The concept module generates semantically meaningful features for primitive concepts, whereas the visual module extracts visual features for attributes and objects from input images. A message passing mechanism is used in the concept module to capture the relations between primitive concepts. Furthermore, to prevent the model from being biased towards seen composite concepts and reduce the entanglement between attributes and objects, we propose a blocking mechanism that equalizes the information available to the model for both seen and unseen concepts. Extensive experiments and ablation studies on two benchmarks show the efficacy of the proposed model.
C1 [Xu, Ziwei; Wong, Yongkang; Kankanhalli, Mohan S.] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
   [Wang, Guangzhi] Natl Univ Singapore, NUS Grad Sch Integrat Sci & Engn, Singapore 119077, Singapore.
C3 National University of Singapore; National University of Singapore
RP Xu, ZW (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
EM ziwei-xu@comp.nus.edu.sg; guangzhi.wang@u.nus.edu;
   yongkang.wong@nus.edu.sg; mohan@comp.nus.edu.sg
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015; Wong,
   Yongkang/0000-0002-1239-4428
FU National Research Foundation, Singapore under its Strategic Capability
   Research Centres Funding Initiative
FX This work was supported by National Research Foundation, Singapore under
   its Strategic Capability Research Centres Funding Initiative.
CR Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111
   Al-Halah Z, 2016, PROC CVPR IEEE, P5975, DOI 10.1109/CVPR.2016.643
   Alayrac JB, 2017, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2017.234
   Atzmon Y., 2020, PROC INT C NEURAL IN, P289
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bendale A, 2016, PROC CVPR IEEE, P1563, DOI 10.1109/CVPR.2016.173
   Bingjie Xu, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P2019, DOI 10.1109/CVPR.2019.00212
   Chang SY, 2014, IEEE DATA MINING, P60, DOI 10.1109/ICDM.2014.115
   Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575
   Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4
   Chen CY, 2014, PROC CVPR IEEE, P200, DOI 10.1109/CVPR.2014.33
   Chen L, 2018, PROC CVPR IEEE, P1043, DOI 10.1109/CVPR.2018.00115
   Chen XY, 2022, IEEE T MULTIMEDIA, V24, P177, DOI 10.1109/TMM.2020.3047546
   Cruz RS, 2018, IEEE WINT CONF APPL, P729, DOI 10.1109/WACV.2018.00085
   Huynh D, 2020, PROC CVPR IEEE, P4482, DOI 10.1109/CVPR42600.2020.00454
   Deng J, 2014, LECT NOTES COMPUT SC, V8689, P48, DOI 10.1007/978-3-319-10590-1_4
   Devlin J, 2018, N AM ASS COMP LING
   Dosovitskiy A, 2016, PROC CVPR IEEE, P4829, DOI 10.1109/CVPR.2016.522
   Duncan J, 2017, P NATL ACAD SCI USA, V114, P5295, DOI 10.1073/pnas.1621147114
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Gan C, 2015, AAAI CONF ARTIF INTE, P3769
   Gan C, 2016, PROC CVPR IEEE, P87, DOI 10.1109/CVPR.2016.17
   Gan C, 2016, INT J COMPUT VISION, V120, P61, DOI 10.1007/s11263-016-0893-6
   Gao JY, 2020, IEEE T MULTIMEDIA, V22, P3088, DOI 10.1109/TMM.2020.2969787
   Gao JY, 2019, AAAI CONF ARTIF INTE, P8303
   Guo JC, 2021, IEEE T MULTIMEDIA, V23, P524, DOI 10.1109/TMM.2020.2984091
   Guo-Sen Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P562, DOI 10.1007/978-3-030-58548-8_33
   Gupta A, 2007, PROC CVPR IEEE, P2564
   Han Y., 2012, P 20 ACM INT C MULT, P529
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G. E., 2012, 12070580 ARXIV
   Hwang SJ, 2011, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2011.5995543
   Isola P, 2015, PROC CVPR IEEE, P1383, DOI 10.1109/CVPR.2015.7298744
   Jiasen Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10434, DOI 10.1109/CVPR42600.2020.01045
   Jingen Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3337, DOI 10.1109/CVPR.2011.5995353
   Kingma DP, 2013, ARXIV
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Larsson G., 2017, ICLR, P1
   Li JN, 2020, IEEE WINT CONF APPL, P3008, DOI 10.1109/WACV45572.2020.9093343
   Li Y.-L., 2020, P IEEECVF C COMPUTER, P11316
   Liu F, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1175, DOI 10.1145/3343031.3350993
   Liu Y, 2019, IEEE I CONF COMP VIS, P6697, DOI 10.1109/ICCV.2019.00680
   Liu Y, 2017, IEEE I CONF COMP VIS, P2943, DOI 10.1109/ICCV.2017.318
   Lu JS, 2019, ADV NEUR IN, V32
   Miller A., 2016, P 2016 C EMP METH NA, P1400, DOI DOI 10.18653/V1/D16-1147
   Min SB, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2070, DOI 10.1145/3343031.3351092
   Misra I, 2017, PROC CVPR IEEE, P1160, DOI 10.1109/CVPR.2017.129
   Nagarajan T, 2018, LECT NOTES COMPUT SC, V11205, P172, DOI 10.1007/978-3-030-01246-5_11
   Nan ZX, 2019, AAAI CONF ARTIF INTE, P8811
   Parikh AP., 2016, EMNLP
   Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Purushwalkam S, 2019, IEEE I CONF COMP VIS, P3592, DOI 10.1109/ICCV.2019.00369
   Qi GJ, 2017, IEEE T PATTERN ANAL, V39, P1360, DOI 10.1109/TPAMI.2016.2587643
   Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25
   Radford A, Improving language understanding by generative pre-training
   Rahman S, 2018, IEEE T IMAGE PROCESS, V27, P5652, DOI 10.1109/TIP.2018.2861573
   Sadeghi MA, 2011, PROC CVPR IEEE, P1745, DOI 10.1109/CVPR.2011.5995711
   Shu XB, 2022, IEEE T PATTERN ANAL, V44, P3300, DOI 10.1109/TPAMI.2021.3050918
   Sukhbaatar S, 2015, ADV NEUR IN, V28
   Taylor WL, 1953, JOURNALISM QUART, V30, P415, DOI 10.1177/107769905303000401
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JD, 2012, INFORM RETRIEVAL, V15, P278, DOI 10.1007/s10791-012-9193-0
   Wang WG, 2019, IEEE I CONF COMP VIS, P9235, DOI 10.1109/ICCV.2019.00933
   Wang XL, 2018, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR.2018.00717
   Wei K, 2019, IEEE I CONF COMP VIS, P3740, DOI 10.1109/ICCV.2019.00384
   Xian YQ, 2019, PROC CVPR IEEE, P10267, DOI 10.1109/CVPR.2019.01052
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Xie Yaqi, 2019, NEURIPS, P4235
   Xu BJ, 2020, IEEE T MULTIMEDIA, V22, P1423, DOI 10.1109/TMM.2019.2943753
   Yao BP, 2010, PROC CVPR IEEE, P9, DOI 10.1109/CVPR.2010.5540234
   Yu A, 2014, PROC CVPR IEEE, P192, DOI 10.1109/CVPR.2014.32
   Yu YL, 2018, ADV NEUR IN, V31
   Yuming Shen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P614, DOI 10.1007/978-3-030-58517-4_36
   Zhang F., 2020, ACM MULTIMEDIA, V2020, P3367
   Zhang J, 2015, J ELECTR ENG TECHNOL, V10, P1264, DOI 10.5370/JEET.2015.10.3.1264
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zheng ZL, 2019, PROC CVPR IEEE, P3662, DOI 10.1109/CVPR.2019.00683
   Zhou TF, 2022, IEEE T PATTERN ANAL, V44, P2827, DOI 10.1109/TPAMI.2021.3049156
   Zhuo T, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P521, DOI 10.1145/3343031.3351040
NR 84
TC 7
Z9 7
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG 13
PY 2021
VL 24
BP 3652
EP 3664
DI 10.1109/TMM.2021.3104411
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NP
UT WOS:000824707400003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Pérez, P
   Janowski, L
   García, N
   Pinson, M
AF Perez, Pablo
   Janowski, Lucjan
   Garcia, Narciso
   Pinson, Margaret
TI Subjective Assessment Experiments That Recruit Few Observers With
   Repetitions (FOWR)
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Video recording; Reliability; Quality assessment;
   Observers; Adaptation models; Visualization; Subjective assessment;
   experiment design; video quality
ID QUALITY ASSESSMENT
AB Recent studies have shown that it is possible to characterize subject bias and variance in subjective assessment tests. Apparent differences among subjects can, for the most part, be explained by random factors. Building on that theory, we propose a subjective test design where four to six team members each rate the stimuli multiple times. The results are comparable to a high performing objective metric. This provides a quick and simple way to analyze new technologies and perform pre-tests for subjective assessment.
C1 [Perez, Pablo] Nokia Bell Labs, Madrid 28050, Spain.
   [Janowski, Lucjan] AGH Univ Sci & Technol, PL-30059 Krakow, Poland.
   [Garcia, Narciso] Univ Politecn Madrid, Madrid 28040, Spain.
   [Pinson, Margaret] Inst Telecommun Sci ITS, Boulder, CO 80305 USA.
C3 AGH University of Krakow; Universidad Politecnica de Madrid
RP Pérez, P (corresponding author), Nokia Bell Labs, Madrid 28050, Spain.
EM pablo.perez@nokia-bell-labs.com; janowski@ktagh.edu.pl;
   narciso@gti.ssr.upm.es; mpinson@its.bldrdoc.gov
RI Janowski, Lucjan/B-2264-2013; Pinson, Margaret H/A-8342-2013; García,
   Narciso/E-8603-2011
OI García, Narciso/0000-0002-0397-894X; Janowski,
   Lucjan/0000-0002-3151-2944; Pinson, Margaret/0000-0003-3369-4726; Perez,
   Pablo/0000-0002-3502-6791
FU European Union [957102]; Ministerio de Ciencia, Innovacion y
   Universidades (AEI/FEDER) of the Spanish Government [TEC2016-75981];
   Norwegian Financial Mechanism [2019/34/H/ST6/00599]; H2020 - Industrial
   Leadership [957102] Funding Source: H2020 - Industrial Leadership
FX The work of P. Perez and N. Garcia has received funding from the
   European Union's Horizon 2020 Research and Innovation Programme under
   Grant 957102 (5G-RECORDS). The work of N. Garcia has also been partially
   supported by theMinisterio de Ciencia, Innovacion y Universidades
   (AEI/FEDER) of the Spanish Government under Project TEC2016-75981
   (IVME). The work of L. Janowski has received funding from the Norwegian
   Financial Mechanism 2014-2021 under Project 2019/34/H/ST6/00599.
CR Akramullah Shahriar, 2014, Digital Video Concepts, Methods, and Metrics: Quality, Compression, Performance, and Power Trade-Off Analysis, DOI DOI 10.1007/978-1-4302-6713-3
   [Anonymous], 2015, 9 INT WORKSH VID PRO
   Brunnström K, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.5.053013
   Brysbaert M., 2018, NUMBER PARTICIPANTS
   Gardlo B, 2014, IEEE ICC, P1070, DOI 10.1109/ICC.2014.6883463
   Hossfeld T, 2017, INT WORK QUAL MULTIM
   Hossfeld T, 2014, IEEE T MULTIMEDIA, V16, P541, DOI 10.1109/TMM.2013.2291663
   Janowski L, 2014, TM14505 NTIA
   Janowski L., 2019, ARXIV190305940
   Janowski L, 2015, IEEE T MULTIMEDIA, V17, P2210, DOI 10.1109/TMM.2015.2484963
   Krasula Lukas, 2016, 2016 Eighth International Conference on Quality of Multimedia Experience (QoMEX), DOI 10.1109/QoMEX.2016.7498936
   KRIPPENDORFF K, 1987, QUAL QUANT, V21, P109
   Kumcu A, 2017, IEEE J-STSP, V11, P48, DOI 10.1109/JSTSP.2016.2638681
   Li Z., 2016, NETFLIX TECH BLOG, V6
   Li Z, 2017, IEEE DATA COMPR CONF, P52, DOI 10.1109/DCC.2017.26
   Narwaria M, 2018, IEEE T MULTIMEDIA, V20, P2063, DOI 10.1109/TMM.2018.2794266
   Pérez P, 2019, INT WORK QUAL MULTIM
   Pinson M, 2020, TR21550 NTIA
   Pinson M. H., 2018, ITS4S VIDEO QUALITY
   Pinson M. H., 2019, TM19537 NTIA
   Pinson MH, 2013, INT WORK QUAL MULTIM, P30, DOI 10.1109/QoMEX.2013.6603199
   Pinson MH, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-50
   Pinson MH, 2012, IEEE J-STSP, V6, P640, DOI 10.1109/JSTSP.2012.2215306
   Quan HT, 2011, IEEE T BROADCAST, V57, P1, DOI 10.1109/TBC.2010.2086750
   Seltman H.J., 2012, Experimental Design and Analysis
   Seufert M, 2019, INT WORK QUAL MULTIM
   Speranza Filippo, 2010, Proceedings of the 2010 Second International Workshop on Quality of Multimedia Experience (QoMEX 2010), P46, DOI 10.1109/QOMEX.2010.5518177
   Streijl RC, 2016, MULTIMEDIA SYST, V22, P213, DOI 10.1007/s00530-014-0446-1
   V. Q. E. Group, 2010, REP VAL VID QUAL MOD
   VanVoorhis CRW, 2007, TUTOR QUANT METHODS, V3, P43, DOI 10.20982/tqmp.03.2.p043
   Wang HQ, 2018, PROC SPIE, V10752, DOI 10.1117/12.2320813
   Winkler S, 2009, INT WORK QUAL MULTIM, P139, DOI 10.1109/QOMEX.2009.5246961
   Wu O, 2011, IEEE I CONF COMP VIS, P225, DOI 10.1109/ICCV.2011.6126246
NR 33
TC 6
Z9 6
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 21
PY 2021
VL 24
BP 3442
EP 3454
DI 10.1109/TMM.2021.3098450
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NG
UT WOS:000824706500001
OA hybrid, Green Submitted
DA 2024-07-18
ER

PT J
AU Bi, T
   Lyons, R
   Fox, G
   Muntean, GM
AF Bi, Ting
   Lyons, Roisin
   Fox, Grace
   Muntean, Gabriel-Miro
TI Improving Student Learning Satisfaction by Using an Innovative
   DASH-Based Multiple Sensorial Media Delivery Solution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Media; Education; Haptic interfaces; Olfactory; Virtual reality;
   Adaptive systems; Performance evaluation; Dash; learner satisfaction;
   multi-sensorial media-enhanced delivery; olfaction
ID OF-THE-ART; PERCEIVED SYNCHRONIZATION; PERFORMANCE; QUALITY; MEMORY
AB Recently, innovative technologies such as Virtual Reality (VR), Augmented Reality (AR), Mixed Reality (MR), and Multi-Sensorial Media (mulsemedia) have introduced new sensorial effects including vibration, smell, airflow, etc. to human life. These effects which have been largely deployed for entertainment, and gaming have positively impacted user satisfaction. This paper explores the potential of mulsemedia in the education context. It describes a novel Dynamic Adaptive Streaming over HTTP (DASH)-based Multi-sensory Media Delivery Solution (DASHMS) which supports adaptive mulsemedia content distribution based on the operational environment which includes network, device, and user settings.DASHMS was evaluated in a real-life educational experiment involving 44 students in an Irish university. The evaluation focused on both learner satisfaction, and the impact on learning. The results demonstrate the potential of adaptive multi-sensorial media delivery to result in a statistically significant increase in user experience. In terms of benefit to learning outcomes however, it was only memory recall which was statistically improved in the experiment.
C1 [Bi, Ting; Muntean, Gabriel-Miro] Dublin City Univ, Sch Elect Engn, Performance Engn Lab, Dublin, Ireland.
   [Lyons, Roisin; Fox, Grace] Dublin City Univ, Business Sch, Dublin, Ireland.
C3 Dublin City University; Dublin City University
RP Muntean, GM (corresponding author), Dublin City Univ, Sch Elect Engn, Performance Engn Lab, Dublin, Ireland.
EM biting1988@gmail.com; roisin.lyons@dcu.ie; grace.fox@dcu.ie;
   gabriel.muntean@dcu.ie
RI Lyons, Roisin/ADS-2582-2022; Muntean, Gabriel-Miro/U-6783-2019
OI Lyons, Roisin/0000-0002-3200-4079; Muntean,
   Gabriel-Miro/0000-0002-9332-4770; Bi, Ting/0000-0001-6196-5613
FU European Union [688503]; Science Foundation Ireland [12/RC/2289_P2,
   16/SP/3804]
FX This work was supported by European Union'sHorizon 2020 Research and
   Innovation programme underGrant 688503 for theNEWTONProject
   (http://newtonproject.eu) and Science Foundation Ireland under Grants
   12/RC/2289_P2 (Insight) and 16/SP/3804 (ENABLE). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Honggang Wang.
CR Anderson L. W., 2001, A Taxonomy for Learning, Teaching, and Assessing: A Revision of Bloom's Taxonomy of Educational Objectives
   [Anonymous], 2018, P EDULEARN C PALM MA
   [Anonymous], 2018, O-RAN: Towards an Open and Smart RAN
   Athanassiou N., 2003, Journal of Management Education, V27, P533, DOI DOI 10.1177/1052562903252515
   Ayres P, 2015, APPL COGNITIVE PSYCH, V29, P631, DOI 10.1002/acp.3142
   BARON RA, 1990, J APPL SOC PSYCHOL, V20, P368, DOI 10.1111/j.1559-1816.1990.tb00417.x
   Bi T, 2018, PROCEEDINGS OF THE 10TH ACM WORKSHOP ON IMMERSIVE MIXED AND VIRTUAL ENVIRONMENT SYSTEMS (MMVE'18), P1, DOI 10.1145/3210438.3210443
   Bi T, 2015, IEEE INT SYM BROADB
   Bloom B, 1956, TAXONOMY ED OBJECTIV
   Butcher K.R., 2014, The Cambridge Handbook of Multimedia Learning, P174, DOI [DOI 10.1017/CBO9781139547369.010, 10.1017/CBO9781139547369.010]
   Cannon H. M., 2014, DEV BUS SIMUL EXPERI
   Covaci A, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3233774
   Degel J, 1999, CHEM SENSES, V24, P317, DOI 10.1093/chemse/24.3.317
   Dinh HQ, 1999, P IEEE VIRT REAL ANN, P222, DOI 10.1109/VR.1999.756955
   Ghinea G, 2005, IEEE T MULTIMEDIA, V7, P786, DOI 10.1109/TMM.2005.850960
   Ghinea G, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2071396.2071398
   Ghinea G, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2617994
   Ghinea G, 2010, IEEE T SYST MAN CY A, V40, P657, DOI 10.1109/TSMCA.2010.2041224
   Ischer M, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.00736
   Mahmud K, 2004, 2004 IEEE 15TH INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR AND MOBILE RADIO COMMUNICATIONS, VOLS 1-4, PROCEEDINGS, P1090, DOI 10.1109/PIMRC.2004.1373867
   Murray N, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3108243
   Murray N, 2016, ACM COMPUT SURV, V48, DOI 10.1145/2816454
   Nurseitov Nurzhan, 2009, Proceedings on the ISCA 22nd International Conferenceon Computers and Their Applications in Industry and Engineering. CAINE-2009, P157
   PTC, CISC VIS NETW IND GL
   Ranasinghe N, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1139, DOI 10.1145/3123266.3123440
   Rasool S, 2016, VISUAL COMPUT, V32, P1311, DOI 10.1007/s00371-016-1224-1
   Richard E., 2006, Virtual Reality, V10, P207, DOI DOI 10.1007/S10055-006-0040-8
   Shannon S.V., 2008, Institute for Learning Styles Journal, V1, P14
   Tijou A, 2006, LECT NOTES COMPUT SC, V3942, P1223, DOI 10.1007/11736639_152
   Van Merrienboer J., 2014, CAMBRIDGE HDB MULTIM, V2nd, P104
   Verlinden JC, 2013, PROCEDIA ENGINEER, V60, P435, DOI 10.1016/j.proeng.2013.07.050
   VONBEKESY G, 1964, J APPL PHYSIOL, V19, P369, DOI 10.1152/jappl.1964.19.3.369
   WARM JS, 1991, J SOC COSMET CHEM, V42, P199
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Yuan ZH, 2015, IEEE T MULTIMEDIA, V17, P957, DOI 10.1109/TMM.2015.2431915
   Yuan ZH, 2015, IEEE T MULTIMEDIA, V17, P104, DOI 10.1109/TMM.2014.2371240
   Zou LH, 2019, IEEE ACCESS, V7, P89172, DOI 10.1109/ACCESS.2019.2926207
   Zou LH, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P315, DOI 10.1145/3083187.3084014
NR 38
TC 10
Z9 10
U1 5
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3494
EP 3505
DI 10.1109/TMM.2020.3025669
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100005
OA hybrid
DA 2024-07-18
ER

PT J
AU Huang, HX
   Zhang, JJ
   Zhang, J
   Xu, JS
   Wu, Q
AF Huang, Huaxi
   Zhang, Junjie
   Zhang, Jian
   Xu, Jingsong
   Wu, Qiang
TI Low-Rank Pairwise Alignment Bilinear Network For Few-Shot Fine-Grained
   Image Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Task analysis; Data models; Dogs; Covariance
   matrices; Neural networks; Training; Bilinear pooling; feature
   alignment; few-shot; fine-grained; low-rank; pairwise
AB Deep neural networks have demonstrated advanced abilities on various visual classification tasks, which heavily rely on the large-scale training samples with annotated ground-truth. However, it is unrealistic always to require such annotation in real-world applications. Recently, Few-Shot learning (FS), as an attempt to address the shortage of training samples, has made significant progress in generic classification tasks. Nonetheless, it is still challenging for current FS models to distinguish the subtle differences between fine-grained categories given limited training data. To filling the classification gap, in this paper, we address the Few-Shot Fine-Grained (FSFG) classification problem, which focuses on tackling the fine-grained classification under the challenging few-shot learning setting. A novel low-rank pairwise bilinear pooling operation is proposed to capture the nuanced differences between the support and query images for learning an effective distance metric. Moreover, a feature alignment layer is designed to match the support image features with query ones before the comparison. We name the proposed model Low-Rank Pairwise Alignment Bilinear Network (LRPABN), which is trained in an end-to-end fashion. Comprehensive experimental results on four widely used fine-grained classification data sets demonstrate that our LRPABN model achieves the superior performances compared to state-of-the-art methods.
C1 [Huang, Huaxi; Zhang, Jian; Xu, Jingsong; Wu, Qiang] Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
   [Zhang, Junjie] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia.
C3 University of Technology Sydney; University of Adelaide
RP Zhang, J (corresponding author), Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
EM Huaxi.Huang@student.uts.edu.au; junjie.zhang@adelaide.edu.au;
   Jian.Zhang@uts.edu.au; jingsong.xu@uts.edu.au; Qiang.Wu@uts.edu.au
RI Huang, Huaxi/AAR-8326-2020; Zhang, Junjie/IZD-9295-2023
OI Huang, Huaxi/0000-0002-6837-6747; Xu, Jingsong/0000-0002-9102-3616;
   Zhang, Jian/0000-0002-7240-3541; Wu, Qiang/0000-0001-5641-2483
FU Rail Manufacturing Cooperative Research Centre
FX The authors greatly appreciate the financial support from the Rail
   Manufacturing Cooperative Research Centre (funded jointly by
   participating rail organizations and the Australian Federal Governments
   Business-Cooperative Research Centres Program) through Project R3.7.3
   -Rail infrastructure defect detection through video analytics.
CR [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   Brown A L, 1975, Adv Child Dev Behav, V10, P103, DOI 10.1016/S0065-2407(08)60009-9
   Chen W.Y., 2019, ICLR, DOI DOI 10.1109/MSR.2015.54
   Cui Y, 2017, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2017.325
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Fei-Fei Fei-Fei L L, 1 WORKSH FIN GRAIN V, V2 2
   Finn C, 2017, PR MACH LEARN RES, V70
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gal Y, 2016, PR MACH LEARN RES, V48
   Gao SH, 2014, IEEE T IMAGE PROCESS, V23, P623, DOI 10.1109/TIP.2013.2290593
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Gao Z, 2020, AAAI CONF ARTIF INTE, V34, P3954
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang C, 2017, IEEE T MULTIMEDIA, V19, P673, DOI 10.1109/TMM.2016.2631122
   Huang HX, 2019, IEEE INT CON MULTI, P91, DOI 10.1109/ICME.2019.00024
   Iscen A, 2015, IEEE T IMAGE PROCESS, V24, P2369, DOI 10.1109/TIP.2015.2423557
   JOHN DR, 1986, J CONSUM RES, V13, P297, DOI 10.1086/209070
   Kim J., 2017, ICLR
   Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li PH, 2018, PROC CVPR IEEE, P947, DOI 10.1109/CVPR.2018.00105
   Li WB, 2019, PROC CVPR IEEE, P7253, DOI 10.1109/CVPR.2019.00743
   Li WB, 2019, AAAI CONF ARTIF INTE, P8642
   Lin TY, 2018, IEEE T PATTERN ANAL, V40, P1309, DOI 10.1109/TPAMI.2017.2723400
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu YH, 2019, INT J PSYCHIAT CLIN, V23, P164, DOI 10.1080/13651501.2019.1569238
   Lu Lu J J, P IEEE C COMP VIS PA, P2956
   Pham N, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P239, DOI 10.1145/2487575.2487591
   Pahde Frederik, 2018, ARXIV180605147
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P1487, DOI 10.1109/TIP.2017.2774041
   Qiao SY, 2018, PROC CVPR IEEE, P7229, DOI 10.1109/CVPR.2018.00755
   Rauber P., 2016, P EUR IEEE VGTC C VI, P73
   Ravi S., 2016, INT C LEARNING REPRE
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Schmidhuber Jurgen, 1987, THESIS TU MUNCHEN
   Snell J, 2017, ADV NEUR IN, V30
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Thrun S, 1998, LEARNING TO LEARN, P181
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wei XS, 2019, IEEE T IMAGE PROCESS, V28, P6116, DOI 10.1109/TIP.2019.2924811
   Xie LX, 2014, IEEE T IMAGE PROCESS, V23, P1994, DOI 10.1109/TIP.2014.2310117
   Xu JJ, 2014, IEEE T MULTIMEDIA, V16, P403, DOI 10.1109/TMM.2013.2291218
   Xu Z, 2017, IEEE T IMAGE PROCESS, V26, P135, DOI 10.1109/TIP.2016.2621661
   Yao HT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P342, DOI 10.1145/3123266.3123278
   Yao HT, 2016, IEEE T IMAGE PROCESS, V25, P4858, DOI 10.1109/TIP.2016.2599102
   Yao YZ, 2019, IEEE T MULTIMEDIA, V21, P184, DOI 10.1109/TMM.2018.2847248
   Yu CJ, 2018, LECT NOTES COMPUT SC, V11220, P595, DOI 10.1007/978-3-030-01270-0_35
   Zhang JJ, 2018, IEEE T MULTIMEDIA, V20, P2801, DOI 10.1109/TMM.2018.2812605
   Zhang LM, 2016, IEEE T IMAGE PROCESS, V25, P553, DOI 10.1109/TIP.2015.2502147
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang XP, 2016, IEEE T IMAGE PROCESS, V25, P878, DOI 10.1109/TIP.2015.2509425
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, P1713, DOI 10.1109/TIP.2016.2531289
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zhuang PQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1301, DOI 10.1145/3240508.3240616
NR 61
TC 74
Z9 81
U1 4
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1666
EP 1680
DI 10.1109/TMM.2020.3001510
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300015
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jiang, B
   Jiang, XY
   Tang, J
   Luo, B
AF Jiang, Bo
   Jiang, Xingyue
   Tang, Jin
   Luo, Bin
TI Co-Saliency Detection via a General Optimization Model and Adaptive
   Graph Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Optimization; Estimation; Adaptation models; Feature extraction;
   Predictive models; Computational modeling; Saliency detection;
   Co-saliency detection; graph learning; foreground and background prior;
   label propagation
ID OBJECT DETECTION; SEGMENTATION
AB Co-saliency detection is an important research problem, and has been widely used in computer vision area. One main challenge for co-saliency detection problem is how to explore both interactive information among different images and individual salient information within each image simultaneously in co-saliency estimation. In this paper, we propose a novel general optimization framework with adaptive graph learning for co-saliency estimation problem. The proposed model integrates multiple cues including background, and foreground priors, structural information of images, and image feature representation together to obtain a uniform, and accurate co-saliency estimation. One main benefit of the proposed co-saliency method is that it conducts co-saliency propagation, and prediction across different images while maintains the individual salient information of each image, which ensures the consistency, and communication across different images effectively in co-saliency estimation. To improve the accuracy of co-saliency estimation, we adaptively learn a neighborhood, and structured graph to conduct co-saliency propagation among superpixels. An effective optimization algorithm has been designed to seek the optimal solution for the proposed co-saliency optimization model. Experimental results on several widely used datasets show that our method outperforms some other related co-saliency detection methods.
C1 [Jiang, Bo; Jiang, Xingyue; Tang, Jin; Luo, Bin] Anhui Univ, Anhui Prov Key Lab Multimodal Cognit Computat, Sch Comp Sci & Technol, Hefei 230601, Peoples R China.
   [Jiang, Bo] Anhui Univ, Inst Phys Sci & Informat Technol, Hefei 230601, Peoples R China.
C3 Anhui University; Anhui University
RP Tang, J (corresponding author), Anhui Univ, Anhui Prov Key Lab Multimodal Cognit Computat, Sch Comp Sci & Technol, Hefei 230601, Peoples R China.
EM zeyiabc@163.com; 2430153753@qq.com; ahu_tj@163.com; ahu_lb@163.com
RI lu, bin/HPE-4790-2023
FU Major Project for New Generation of AI [2018AAA0100400]; NSFC Key
   Projects of International (Regional) Cooperation and Exchanges
   [61860206004]; Open fund for Discipline Construction, Institute of
   Physical Science and Information Technology, Anhui University;
   Cooperative Research Project Program of Nanjing Artificial Intelligence
   Chip Research, Institute of Automation, Chinese Academy of Sciences
FX This work was supported in part by Major Project for New Generation of
   AI under Grant 2018AAA0100400; in part by NSFC Key Projects of
   International (Regional) Cooperation and Exchanges underGrant
   61860206004; in part by Open fund for Discipline Construction, Institute
   of Physical Science and Information Technology, Anhui University; and in
   part by Cooperative Research Project Program of Nanjing Artificial
   Intelligence Chip Research, Institute of Automation, Chinese Academy of
   Sciences. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Fatih Porikli.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Cao XC, 2014, IEEE T IMAGE PROCESS, V23, P4175, DOI 10.1109/TIP.2014.2332399
   Chen YL, 2014, INT C PATT RECOG, P2305, DOI 10.1109/ICPR.2014.400
   Cheng MM, 2014, VISUAL COMPUT, V30, P443, DOI 10.1007/s00371-013-0867-4
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cong RM, 2019, IEEE T MULTIMEDIA, V21, P1660, DOI 10.1109/TMM.2018.2884481
   Dong XP, 2015, IEEE T IMAGE PROCESS, V24, P3966, DOI 10.1109/TIP.2015.2456636
   Dou H, 2017, IEEE T MULTIMEDIA, V19, P1718, DOI 10.1109/TMM.2017.2689327
   FAN K, 1949, P NATL ACAD SCI USA, V35, P652, DOI 10.1073/pnas.35.11.652
   Feichtinger HG., 2012, GABOR ANAL ALGORITHM
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Han JW, 2018, IEEE T IMAGE PROCESS, V27, P1639, DOI 10.1109/TIP.2017.2781424
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Huang J, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3569
   Jerripothula KR, 2018, IEEE T MULTIMEDIA, V20, P2466, DOI 10.1109/TMM.2018.2798294
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Jiang B, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1375, DOI 10.1145/3343031.3350860
   Jiang B, 2019, IEEE INT CON MULTI, P332, DOI 10.1109/ICME.2019.00065
   Jiang B, 2018, LECT NOTES COMPUT SC, V11164, P313, DOI 10.1007/978-3-030-00776-8_29
   Kai-Yueh Chang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2129, DOI 10.1109/CVPR.2011.5995415
   Li HL, 2013, IEEE T MULTIMEDIA, V15, P1896, DOI 10.1109/TMM.2013.2271476
   Li HL, 2011, IEEE T IMAGE PROCESS, V20, P3365, DOI 10.1109/TIP.2011.2156803
   Li YJ, 2015, IEEE SIGNAL PROC LET, V22, P588, DOI 10.1109/LSP.2014.2364896
   Liu Z, 2014, IEEE SIGNAL PROC LET, V21, P88, DOI 10.1109/LSP.2013.2292873
   Minaee S, 2019, IEEE T IMAGE PROCESS, V28, P3192, DOI 10.1109/TIP.2019.2894966
   Minaee S, 2016, IEEE J EM SEL TOP C, V6, P573, DOI 10.1109/JETCAS.2016.2597701
   Nie FP, 2016, AAAI CONF ARTIF INTE, P1302
   Nie FP, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P977, DOI 10.1145/2623330.2623726
   Quan R, 2016, PROC CVPR IEEE, P687, DOI 10.1109/CVPR.2016.81
   Rubinstein M, 2013, PROC CVPR IEEE, P1939, DOI 10.1109/CVPR.2013.253
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Simoncelli EP, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC444
   Wang WG, 2018, IEEE T CIRC SYST VID, V28, P1727, DOI 10.1109/TCSVT.2017.2701279
   Wang WG, 2016, IEEE T MULTIMEDIA, V18, P1011, DOI 10.1109/TMM.2016.2545409
   Wang Wenguan, 2015, IEEE Trans Image Process, V24, P3137, DOI 10.1109/TIP.2015.2438550
   Winn J, 2005, IEEE I CONF COMP VIS, P1800
   Xiong K, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3147
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yao XW, 2017, IEEE T IMAGE PROCESS, V26, P3196, DOI 10.1109/TIP.2017.2694222
   Ye LW, 2015, IEEE SIGNAL PROC LET, V22, P2073, DOI 10.1109/LSP.2015.2458434
   ZHANG D, 2018, ACM T INTEL SYST TEC, V9, P1
   Zhang DW, 2017, IEEE T PATTERN ANAL, V39, P865, DOI 10.1109/TPAMI.2016.2567393
   Zhang DW, 2016, IEEE T NEUR NET LEAR, V27, P1163, DOI 10.1109/TNNLS.2015.2495161
   Zhang DW, 2015, PROC CVPR IEEE, P2994, DOI 10.1109/CVPR.2015.7298918
   Zheng XJ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P959, DOI 10.1145/3240508.3240648
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 49
TC 6
Z9 6
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3193
EP 3202
DI 10.1109/TMM.2020.3021251
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000019
DA 2024-07-18
ER

PT J
AU Jiang, X
   Wei, SK
   Liu, T
   Zhao, RZ
   Zhao, Y
   Huang, H
AF Jiang, Xiang
   Wei, Shikui
   Liu, Ting
   Zhao, Ruizhen
   Zhao, Yao
   Huang, Heng
TI Blind Image Clustering for Camera Source Identification via Row-Sparsity
   Optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cameras; Clustering algorithms; Optimization; Correlation; Noise
   measurement; Reliability; Clustering methods; Image forensic; blind
   camera source clustering; photo response non-uniformity; row-sparsity
ID SENSOR PATTERN NOISE; NORMALIZED CUTS; REPRESENTATION
AB Given a set of images with the number of cameras providing those images unknown, how to blindly identify the sources of the images has been a critical problem in digital forensics. Although state-of-the-art methods have achieved impressive results, they have failed at suppressing outliers. When they deal with a noisy dataset, the performance is significantly degraded. To address this issue, we propose an optimization approach with sparsity constraints to simultaneously handle the how-many subproblem (i.e., the number of cameras) and the which-from-which subproblem (i.e., the image-camera relationship). In our approach, we first formulate the blind camera source clustering as a row-sparsity optimization problem, in which the representation errors are minimized and the outliers caused by noisy features are suppressed. Then, a new two-stage refinement method based on inter- and the intra-class differences is proposed to achieve a more accurate estimation of the number of cameras. Because strong sparsity constraints have been adopted and the interactive relationship among data points can be fully explored to distinguish the images originated from different cameras, the proposed method can effectively handle outliers. Extensive experiments on the popular Dresden dataset show that the proposed method outperforms existing methods in both identification accuracy and efficiency.
C1 [Jiang, Xiang; Wei, Shikui; Liu, Ting; Zhao, Ruizhen; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Jiang, Xiang; Wei, Shikui; Liu, Ting; Zhao, Ruizhen; Zhao, Yao] Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
   [Huang, Heng] Univ Pittsburgh, Dept Elect & Comp Engn, Pittsburgh, PA 15261 USA.
C3 Beijing Jiaotong University; Beijing Jiaotong University; Pennsylvania
   Commonwealth System of Higher Education (PCSHE); University of
   Pittsburgh
RP Wei, SK (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM 14112058@bjtu.edu.cn; shkwei@bjtu.edu.cn; 16112055@bjtu.edu.cn;
   rzhzhao@bjtu.edu.cn; yzhao@bjtu.edu.cn; heng.huang@pitt.edu
OI Zhao, Yao/0000-0002-8581-9554; Liu, Ting/0000-0003-3458-6567
FU National Key Research and Development of China [2017YFC1703503];
   National Natural Science Foundation of China [61972022, 61532005];
   Program of China Scholarships Council [201807095006]
FX This work was supported in part by the National Key Research and
   Development of China under Grant 2017YFC1703503, in part by the National
   Natural Science Foundation of China under Grants 61972022 and 61532005,
   and in part by the Program of China Scholarships Council under Grant
   201807095006. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Zixiang Xiong.
CR Amerini I, 2014, SIGNAL PROCESS-IMAGE, V29, P831, DOI 10.1016/j.image.2014.07.003
   [Anonymous], 2016, IEEE INT WORKS INFOR
   [Anonymous], 2010, 2010 IEEE INT WORKSH
   Bansal N, 2004, MACH LEARN, V56, P89, DOI 10.1023/B:MACH.0000033116.57574.95
   Bayram S., 2009, P INT C IMAG PROC, pIII
   Bloy GJ, 2008, IEEE T PATTERN ANAL, V30, P532, DOI 10.1109/TPAMI.2007.1183
   Caldelli Roberto., 2010, 2010 IEEE International Workshop on Information Forensics and Security, P1
   Chaux C, 2007, INVERSE PROBL, V23, P1495, DOI 10.1088/0266-5611/23/4/008
   Chen M, 2008, IEEE T INF FOREN SEC, V3, P74, DOI 10.1109/TIFS.2007.916285
   Chierchia G., 2011, 2011 17 INT C DIGITA, P1
   Chierchia G, 2014, IEEE T INF FOREN SEC, V9, P554, DOI 10.1109/TIFS.2014.2302078
   Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI 10.1109/TP'AMI.2007.1115
   Duchi J., 2008, P 25 INT C MACH LEAR, P272
   Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547
   Elhamifar E, 2016, IEEE T PATTERN ANAL, V38, P2182, DOI 10.1109/TPAMI.2015.2511748
   Fahmy OM, 2015, INT CONF SYST SIGNAL, P249, DOI 10.1109/IWSSIP.2015.7314223
   Feng XY, 2012, IEEE T MULTIMEDIA, V14, P536, DOI 10.1109/TMM.2012.2191946
   Filler Tomas, 2008, 2008 15th IEEE International Conference on Image Processing - ICIP 2008, P1296, DOI 10.1109/ICIP.2008.4712000
   Villalba LJG, 2015, EXPERT SYST APPL, V42, P1927, DOI 10.1016/j.eswa.2014.10.018
   Gloe T., 2010, P SAC 10 2010 ACM S, P1584
   Hu YJ, 2010, IEEE INT CON MULTI, P1481, DOI 10.1109/ICME.2010.5582952
   HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075
   Kang XG, 2012, IEEE T INF FOREN SEC, V7, P393, DOI 10.1109/TIFS.2011.2168214
   Kharrazi M, 2004, IEEE IMAGE PROC, P709
   Korus P, 2017, IEEE T INF FOREN SEC, V12, P809, DOI 10.1109/TIFS.2016.2636089
   Li CT, 2010, IEEE INT SYMP CIRC S, P3429, DOI 10.1109/ISCAS.2010.5537850
   Li CT, 2010, IEEE T INF FOREN SEC, V5, P280, DOI 10.1109/TIFS.2010.2046268
   Li RZ, 2015, INT CONF ACOUST SPEE, P1777, DOI 10.1109/ICASSP.2015.7178276
   Lin XF, 2017, IEEE T INF FOREN SEC, V12, P793, DOI 10.1109/TIFS.2016.2636086
   Lukás J, 2006, IEEE T INF FOREN SEC, V1, P205, DOI 10.1109/TIFS.2006.873602
   Luo Weiqi, 2007, Frontiers of Computer Science in China, V1, P166, DOI 10.1007/s11704-007-0017-0
   Marra F, 2017, IEEE T INF FOREN SEC, V12, P2197, DOI 10.1109/TIFS.2017.2701335
   Phan Q.-T., 2017, PROC 2 INT WORKSHOP, P1
   Qiao T, 2019, IEEE T MULTIMEDIA, V21, P1077, DOI 10.1109/TMM.2018.2872863
   Qiao T, 2017, SIGNAL PROCESS-IMAGE, V52, P74, DOI 10.1016/j.image.2016.12.011
   Qu ZH, 2013, INT CONF ACOUST SPEE, P3023, DOI 10.1109/ICASSP.2013.6638213
   Phan QT, 2019, IEEE T INF FOREN SEC, V14, P1902, DOI 10.1109/TIFS.2018.2886929
   RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239
   Shi JB, 1997, PROC CVPR IEEE, P731, DOI 10.1109/CVPR.1997.609407
   Su LC, 2018, IEEE T MULTIMEDIA, V20, P825, DOI 10.1109/TMM.2017.2760098
   Tabatabaei SAH, 2015, IEEE T MULTIMEDIA, V17, P945, DOI 10.1109/TMM.2015.2432672
   Thai TH, 2014, IEEE T IMAGE PROCESS, V23, P250, DOI 10.1109/TIP.2013.2290596
   Valsesia D, 2015, IEEE T MULTIMEDIA, V17, P1439, DOI 10.1109/TMM.2015.2455417
   Wilson R, 2003, IEEE T PATTERN ANAL, V25, P42, DOI 10.1109/TPAMI.2003.1159945
   Yu SX, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P313, DOI 10.1109/iccv.2003.1238361
   Zhao Y, 2005, DATA MIN KNOWL DISC, V10, P141, DOI 10.1007/s10618-005-0361-3
NR 46
TC 2
Z9 2
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2602
EP 2613
DI 10.1109/TMM.2020.3013449
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600005
DA 2024-07-18
ER

PT J
AU Li, LD
   Zhou, Y
   Wu, JJ
   Li, F
   Shi, GM
AF Li, Leida
   Zhou, Yu
   Wu, Jinjian
   Li, Fu
   Shi, Guangming
TI Quality Index for View Synthesis by Measuring Instance Degradation and
   Global Appearance
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distortion; Degradation; Image edge detection; Distortion measurement;
   Rendering (computer graphics); Image segmentation; View synthesis;
   quality assessment; depth-image-based rendering (DIBR); instance
   degradation; global appearance
ID DIBR; IMAGES; VIDEOS
AB Virtual view synthesis plays a vital role in the application of multi-view and free-viewpoint videos. Depth-image-based rendering (DIBR) is the most commonly used approach in view synthesis, and many DIBR algorithms have been proposed. However, how to evaluate the quality of DIBR-synthesized images and benchmark the DIBR algorithms are still very challenging, which may hinder the further development of the view synthesis technique. Hence, an effective quality metric for evaluating the distortions in view synthesis is urgently needed. With this motivation, this paper presents a quality index for view synthesis by simultaneously measuring local Instance DEgradation and global Appearance (IDEA). Due to the imperfection of rendering algorithms, local geometric distortions are easily introduced around instance contours, causing instance degradation, which is the dominant distortion in synthesized views. In this work, image instances are first detected and local instance degradation is measured based on discrete orthogonal moments. Meantime, we propose to measure the global appearance of synthesized images based on the superpixel representation. By integrating both local and global aspects of the distortions, a more accurate quality model is built for view synthesis. Extensive experiments and comparisons have demonstrated the superiority of the proposed method in evaluating the quality of DIBR-synthesized images and benchmarking the performance of view synthesis algorithms.
C1 [Li, Leida; Wu, Jinjian; Li, Fu; Shi, Guangming] Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
   [Zhou, Yu] China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Jiangsu, Peoples R China.
C3 Xidian University; China University of Mining & Technology
RP Li, LD (corresponding author), Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.; Zhou, Y (corresponding author), China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Jiangsu, Peoples R China.
EM ldli@xidian.edu.cn; zhouy7476@cumt.edu.cn;
   jinjian.wu@mail.xidian.edu.cn; fuli@mail.xidian.edu.cn;
   gmshi@xidian.edu.cn
RI Wang, Chen/JZE-6385-2024; Wu, Jinjian/GQH-0222-2022; Li,
   Li/AEM-3636-2022; li, li/HII-4157-2022
FU National Natural Science Foundation of China [61771473, 61991451,
   61672404, 61379143]; Natural Science Foundation of Jiangsu Province
   [BK20181354]; Six Talent Peaks High-level Talents in Jiangsu Province
   [XYDXX-063]; Fundamental Research Funds of the Central Universities of
   China [JC1904]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61771473, 61991451, 61672404, and
   61379143, in part by the Natural Science Foundation of Jiangsu Province
   under Grant BK20181354, in part by Six Talent Peaks High-level Talents
   in Jiangsu Province under Grant XYDXX-063, and in part by the
   Fundamental Research Funds of the Central Universities of China under
   Grant JC1904. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Hantao Liu.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Ahn I, 2013, IEEE T BROADCAST, V59, P614, DOI 10.1109/TBC.2013.2281658
   [Anonymous], 2015, P IEEE TRUE VIS CAPT
   [Anonymous], 2008, JTC1SC29WG11 ISOIEC
   Battisti F, 2015, SIGNAL PROCESS-IMAGE, V30, P78, DOI 10.1016/j.image.2014.10.005
   Bosc E, 2011, IEEE J-STSP, V5, P1332, DOI 10.1109/JSTSP.2011.2166245
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cermak G., 2009, VIDEO QUAL EXPERTS G
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Fang YM, 2019, IEEE ACCESS, V7, P132649, DOI 10.1109/ACCESS.2019.2941112
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Flusser J., 2009, Moments and Moment Invariants in Pattern Recognition
   Gangapure VN, 2018, IEEE T CIRC SYST VID, V28, P1263, DOI 10.1109/TCSVT.2017.2662743
   Gu K, 2020, IEEE T BROADCAST, V66, P127, DOI 10.1109/TBC.2019.2906768
   Gu K, 2018, IEEE T IMAGE PROCESS, V27, P394, DOI 10.1109/TIP.2017.2733164
   Gu K, 2014, IEEE IMAGE PROC, P506, DOI 10.1109/ICIP.2014.7025101
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Jakhetiya V, 2019, IEEE T IND INFORM, V15, P4120, DOI 10.1109/TII.2018.2888861
   Jin XD, 2017, IEEE T GEOSCI REMOTE, V55, P4285, DOI 10.1109/TGRS.2017.2690445
   Jung YJ, 2016, IEEE T CIRC SYST VID, V26, P1201, DOI 10.1109/TCSVT.2015.2430632
   Köppel M, 2010, IEEE IMAGE PROC, P1809, DOI 10.1109/ICIP.2010.5652138
   Li LD, 2016, IEEE T CYBERNETICS, V46, P39, DOI 10.1109/TCYB.2015.2392129
   Li LD, 2018, IEEE T MULTIMEDIA, V20, P914, DOI 10.1109/TMM.2017.2760062
   Li LD, 2016, IEEE T MULTIMEDIA, V18, P1085, DOI 10.1109/TMM.2016.2545398
   Li S, 2018, IEEE T MULTIMEDIA, V20, P1948, DOI 10.1109/TMM.2018.2791810
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Luo GB, 2016, PROC CVPR IEEE, P1781, DOI 10.1109/CVPR.2016.197
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Mori Y, 2009, SIGNAL PROCESS-IMAGE, V24, P65, DOI 10.1016/j.image.2008.10.013
   Müller K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/438148
   Mukundan R, 2001, IEEE T IMAGE PROCESS, V10, P1357, DOI 10.1109/83.941859
   Ndjiki-Nya P, 2010, IEEE INT CON MULTI, P424, DOI 10.1109/ICME.2010.5583559
   Onat E., 2017, 25 SIGN PROC COMM AP, P1
   Peng CL, 2017, IEEE T CIRC SYST VID, V27, P288, DOI 10.1109/TCSVT.2015.2502861
   Roberts L.G., 1965, OPTICAL ELECTRO OPTI, P159
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sandic-Stankovic D, 2016, J ELECTR ENG-SLOVAK, V67, P3, DOI 10.1515/jee-2016-0001
   Shao F, 2018, IEEE T MULTIMEDIA, V20, P659, DOI 10.1109/TMM.2017.2748460
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Stankovic N, 2015, INT C POWER ELECT DR, P12, DOI 10.1109/PEDS.2015.7203451
   Telea A., 2004, Journal of Graphics Tools, V9, P23, DOI 10.1080/10867651.2004.10487596
   Tian SS, 2018, IEEE T IMAGE PROCESS, V27, P1652, DOI 10.1109/TIP.2017.2781420
   Tian SS, 2017, INT CONF ACOUST SPEE, P1248, DOI 10.1109/ICASSP.2017.7952356
   Veksler Olga, 2010, Computer Vision-ECCV 2010, P211, DOI [10.1007/978-3-642-15555-0_16, DOI 10.1007/978-3-642-15555-0_16]
   Wang GC, 2020, IEEE T IMAGE PROCESS, V29, P1802, DOI 10.1109/TIP.2019.2945675
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yoon SS, 2014, IEEE IMAGE PROC, P2883, DOI 10.1109/ICIP.2014.7025583
   Zhan YB, 2017, IEEE T MULTIMEDIA, V19, P1837, DOI 10.1109/TMM.2017.2689923
   Zhan YB, 2017, IEEE SIGNAL PROC LET, V24, P760, DOI 10.1109/LSP.2017.2688371
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang YH, 2011, IEEE I CONF COMP VIS, P1387, DOI 10.1109/ICCV.2011.6126393
   Zhou Y, 2019, SIGNAL PROCESS-IMAGE, V74, P309, DOI 10.1016/j.image.2019.03.005
   Zhou Y, 2019, IEEE T IMAGE PROCESS, V28, P4566, DOI 10.1109/TIP.2019.2912463
   Zhou Y, 2018, IEEE T MULTIMEDIA, V20, P3019, DOI 10.1109/TMM.2018.2829607
   Zhou Y, 2018, J VIS COMMUN IMAGE R, V55, P30, DOI 10.1016/j.jvcir.2018.05.023
   Zhou Y, 2016, IEEE IMAGE PROC, P1012, DOI 10.1109/ICIP.2016.7532510
   Zhu C, 2016, IEEE T BROADCAST, V62, P82, DOI 10.1109/TBC.2015.2475697
NR 59
TC 17
Z9 18
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 320
EP 332
DI 10.1109/TMM.2020.2980185
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600025
DA 2024-07-18
ER

PT J
AU Liu, ZT
   Rehman, A
   Wu, M
   Cao, WH
   Hao, M
AF Liu, Zhen-Tao
   Rehman, Abdul
   Wu, Min
   Cao, Wei-Hua
   Hao, Man
TI Speech Personality Recognition Based on Annotation Classification Using
   Log-Likelihood Distance and Extraction of Essential Audio Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Speech recognition; Reliability; Training; Emotion
   recognition; Human computer interaction; Task analysis; Personality
   analysis; speech emotion recognition; acoustic features; annotation
   clustering
ID EMOTION REGULATION; TRAITS; PREDICTION; ROBUST; MODEL; IMPRESSIONS;
   RELIABILITY; LIKABILITY; CONTINUITY; SUPPORT
AB Speech personality recognition relies on training models that require an excessive number of features and are, in most cases, designed specifically for certain databases. As a result, when tested on different datasets, overfitted classifier models are not always reliable because their accuracy changes with changes in the domain of speakers. Moreover, personality annotations are often subjective, which creates variability in raters perception during labeling. These problems inhibit the effectiveness of speech personality recognition applications. To reduce the unexplained variance caused by unknown differences in raters perception, a structure that uses Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH) algorithm is proposed. Furthermore, a feature extraction method is proposed to filter out undesirable adulterations be it noise, silence, or uncertain pitch segments, while extracting essential audio features, i.e., signal power roll-off, pitch, and pause rate. Experiments on the standard SSPNet dataset records a relative 4% increase in overall accuracy when log-likelihood based annotations are used. Moreover, improved consistency in accuracy is observed when this method is tested on male and female subsets.
C1 [Liu, Zhen-Tao; Rehman, Abdul; Wu, Min; Cao, Wei-Hua; Hao, Man] China Univ Geosci, Sch Automat, Wuhan 430074, Peoples R China.
   [Liu, Zhen-Tao; Rehman, Abdul; Wu, Min; Cao, Wei-Hua; Hao, Man] Hubei Key Lab Adv Control & Intelligent Automat C, Wuhan 430074, Peoples R China.
   [Liu, Zhen-Tao; Rehman, Abdul; Wu, Min; Cao, Wei-Hua; Hao, Man] Minist Educ, Engn Res Ctr Intelligent Technol Geoexplorat, Wuhan 430074, Peoples R China.
C3 China University of Geosciences
RP Liu, ZT (corresponding author), China Univ Geosci, Sch Automat, Wuhan 430074, Peoples R China.; Liu, ZT (corresponding author), Hubei Key Lab Adv Control & Intelligent Automat C, Wuhan 430074, Peoples R China.
EM liuzhentao@cug.edu.cn; abdulrehman@cug.edu.cn; wumin@cug.edu.cn;
   weihuacao@cug.edu.cn; haoman@cug.edu.cn
RI Rehman, Abdul/ABA-2377-2021
OI Rehman, Abdul/0000-0003-2345-2256; Cao, Wei-Hua/0000-0002-9677-9586; Wu,
   Min/0000-0002-0668-8315
FU National Natural Science Foundation of China [61976197, 61403422,
   61273102]; Hubei Provincial Natural Science Foundation of China
   [2018CFB447, 2015CFA010]; Wuhan Science and Technology Project
   [2017010201010133, 2020010601012175]; 111 Project [B17040]; Fundamental
   Research Funds for National University, China University of Geosciences,
   Wuhan [1910491T01]
FX 2020; accepted September 11, 2020. Date of publication September 18,
   2020; date of current version September 24, 2021. This work was
   supported in part by the National Natural Science Foundation of China
   under Grants 61976197, 61403422, and 61273102, in part by the Hubei
   Provincial Natural Science Foundation of China under Grants 2018CFB447
   and 2015CFA010, in part by Wuhan Science and Technology Project under
   Grants 2017010201010133 and 2020010601012175, in part by 111 Project
   under Grant B17040, and in part by the Fundamental Research Funds for
   National University, China University of Geosciences, Wuhan, under Grant
   1910491T01. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Chi-Chun Lee.
   (Corresponding author: Zhen-Tao Liu.)
CR Abdelwahab M, 2017, INT CONF ACOUST SPEE, P5000, DOI 10.1109/ICASSP.2017.7953108
   An GZ, 2018, INTERSPEECH, P421
   Bäckström T, 2006, SIGNAL PROCESS, V86, P3286, DOI 10.1016/j.sigpro.2006.01.010
   Batrinca L, 2016, IEEE T MULTIMEDIA, V18, P659, DOI 10.1109/TMM.2016.2522763
   Bhat H. S., 2010, On the derivation of the Bayesian Information Criterion
   Biel JI, 2013, IEEE T MULTIMEDIA, V15, P41, DOI 10.1109/TMM.2012.2225032
   Bin Siddique F, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017): SYSTEM DEMONSTRATIONS, P121, DOI 10.18653/v1/P17-4021
   Bins J., 2011, PROC 8 IEEE INT C CO, V2, P159
   Bishop Christopher M., 2006, Pattern Recognition and Machine Learning, V4
   Bono JE, 2007, J OCCUP HEALTH PSYCH, V12, P177, DOI 10.1037/1076-8998.12.2.177
   Brester C, 2016, J ARTIF INTELL SOFT, V6, P243, DOI 10.1515/jaiscr-2016-0018
   Briley DA, 2014, PSYCHOL BULL, V140, P1303, DOI 10.1037/a0037091
   Carbonneau M. A., 2017, IEEE T AFFECT COMPUT
   Celiktutan O, 2015, 2015 24TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (RO-MAN), P815, DOI 10.1109/ROMAN.2015.7333602
   Chen SS, 1998, INT CONF ACOUST SPEE, P645, DOI 10.1109/ICASSP.1998.675347
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Cortes D. Sanchez, 2011, WORKSH MULT CORP MAC
   Costa PT, 2001, J PERS SOC PSYCHOL, V81, P322, DOI [10.1037/0022-3514.81.2.322, 10.1037//0022-3514.81.2.322]
   Cronbach LJ, 1951, PSYCHOMETRIKA, V16, P297
   DAVIS SB, 1980, IEEE T ACOUST SPEECH, V28, P357, DOI 10.1109/TASSP.1980.1163420
   Deng J, 2013, INT CONF AFFECT, P511, DOI 10.1109/ACII.2013.90
   Duxbury C., 2003, PROC DIGITAL AUDIO E, P6
   Essid S., 2005, SIGNAL IMAGE PROCESS
   Eyben F, 2016, IEEE T AFFECT COMPUT, V7, P190, DOI 10.1109/TAFFC.2015.2457417
   Eyben Florian, 2010, P 18 ACM INT C MULT, P1459
   Fleeson W, 2001, J PERS SOC PSYCHOL, V80, P1011, DOI 10.1037/0022-3514.80.6.1011
   Fraley C, 1998, COMPUT J, V41, P578, DOI 10.1093/comjnl/41.8.578
   Gallardo LF, 2017, INTERSPEECH, P904, DOI 10.21437/Interspeech.2017-328
   Gillet O, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL IV, PROCEEDINGS, P269
   GOLDBERG LR, 1990, J PERS SOC PSYCHOL, V59, P1216, DOI 10.1037/0022-3514.59.6.1216
   Hee O., 2014, International Journal of Innovation and Applied Studies, V5, P309
   Hossain MS, 2019, INFORM FUSION, V49, P69, DOI 10.1016/j.inffus.2018.09.008
   Howell RT, 2011, PERS INDIV DIFFER, V51, P797, DOI 10.1016/j.paid.2011.06.020
   KENRICK DT, 1980, PSYCHOL REV, V87, P88, DOI 10.1037/0033-295X.87.1.88
   Leak W B., 1968, Birch regeneration: a stochastic model. In
   Liu ZT, 2019, IEEE T COGN DEV SYST, V11, P517, DOI 10.1109/TCDS.2018.2868121
   Liu ZT, 2018, NEUROCOMPUTING, V273, P271, DOI 10.1016/j.neucom.2017.07.050
   Liu ZT, 2017, IEEE-CAA J AUTOMATIC, V4, P668, DOI 10.1109/JAS.2017.7510622
   MAKHOUL J, 1975, P IEEE, V63, P561, DOI 10.1109/PROC.1975.9792
   Mathieu B., 2010, P 11 INT C MUS INF R, P441
   McAdams DP, 2010, ANNU REV PSYCHOL, V61, P517, DOI 10.1146/annurev.psych.093008.100507
   McHugh ML, 2012, BIOCHEM MEDICA, V22, P276, DOI 10.11613/bm.2012.031
   Mohammadi G, 2015, INT CONF AFFECT, P484, DOI 10.1109/ACII.2015.7344614
   Mohammadi G, 2012, IEEE T AFFECT COMPUT, V3, P273, DOI 10.1109/T-AFFC.2012.5
   Moore BCJ, 1997, J AUDIO ENG SOC, V45, P224
   Muthusamy H, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0120344
   Ng W, 2009, J INDIVID DIF, V30, P100, DOI 10.1027/1614-0001.30.2.100
   Oudre L, 2011, IEEE T AUDIO SPEECH, V19, P2222, DOI 10.1109/TASL.2011.2139205
   Pohjalainen J, 2015, COMPUT SPEECH LANG, V29, P145, DOI 10.1016/j.csl.2013.11.004
   Ponce-López V, 2016, LECT NOTES COMPUT SC, V9915, P400, DOI 10.1007/978-3-319-49409-8_32
   Rammstedt B, 2007, J RES PERS, V41, P203, DOI 10.1016/j.jrp.2006.02.001
   Reddock CM, 2011, INT J SELECT ASSESS, V19, P119, DOI 10.1111/j.1468-2389.2011.00540.x
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Scheirer E, 1997, INT CONF ACOUST SPEE, P1331, DOI 10.1109/ICASSP.1997.596192
   Schuller B, 2015, COMPUT SPEECH LANG, V29, P100, DOI 10.1016/j.csl.2014.08.003
   Schuller B, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P254
   Schuller B, 2009, IMAGE VISION COMPUT, V27, P1760, DOI 10.1016/j.imavis.2009.02.013
   SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136
   SMITH BL, 1975, LANG SPEECH, V18, P145, DOI 10.1177/002383097501800203
   Smith JO, 1999, IEEE T SPEECH AUDI P, V7, P697, DOI 10.1109/89.799695
   Stanton K, 2016, J ABNORM PSYCHOL, V125, P960, DOI 10.1037/abn0000208
   Tavakol M, 2011, INT J MED EDUC, V2, P53, DOI 10.5116/ijme.4dfb.8dfd
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang YJ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356487
   Wei XS, 2018, IEEE T AFFECT COMPUT, V9, P303, DOI 10.1109/TAFFC.2017.2762299
   Wit E, 2012, STAT NEERL, V66, P217, DOI 10.1111/j.1467-9574.2012.00530.x
   Wöllmer M, 2009, INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2009, VOLS 1-5, P1555
   Yannakakis GN, 2017, INT CONF AFFECT, P248, DOI 10.1109/ACII.2017.8273608
   Zahorian SA, 2008, J ACOUST SOC AM, V123, P4559, DOI 10.1121/1.2916590
   Zhang T., 1996, BIRCH EFFICIENT DATA, V25, P103, DOI [DOI 10.1145/233269.233324, 10.1145/235968.233324]
NR 70
TC 9
Z9 9
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3414
EP 3426
DI 10.1109/TMM.2020.3025108
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000036
DA 2024-07-18
ER

PT J
AU Nawaz, M
   Yan, H
AF Nawaz, Mehmood
   Yan, Hong
TI Saliency Detection Using Deep Features and Affinity-Based Robust
   Background Subtraction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Saliency detection; Object detection; Image
   reconstruction; Image segmentation; Image color analysis; Neural
   networks; Attention map; background subtraction; salient region;
   affinity matrix; convolution neural network
ID OBJECT DETECTION; MODEL; ATTENTION
AB Most existing saliency methods measure fore- ground saliency by using the contrast of a foreground region to its local context, or boundary priors and spatial compactness. These methods are not powerful enough to extract a precise salient region from noisy and cluttered backgrounds. To evaluate the contrast of salient and background regions effectively, we consider high-level features from both supervised and unsupervised methods. We propose an affinity-based robust background subtraction technique and maximum attention map using a pre-trained convolution neural network. This affinity-based technique uses pixel similarities to propagate the values of salient pixels among foreground and background regions and their union. The salient pixel value controls the foreground and background information by using multiple pixel affinities. The maximum attention map is derived from the convolution neural network using features of the Pooling and Relu layers. This method can detect salient regions from images that have noisy and cluttered backgrounds. Our experimental results demonstrate the effectiveness of the proposed approach on six different saliency data sets and benchmarks and show that it improves the quality of detection beyond current saliency detection methods.
C1 [Nawaz, Mehmood; Yan, Hong] City Univ Hong Kong, Dept Elect Engn, Kowloon, Hong Kong 999077, Peoples R China.
C3 City University of Hong Kong
RP Nawaz, M (corresponding author), City Univ Hong Kong, Dept Elect Engn, Kowloon, Hong Kong 999077, Peoples R China.
EM mnawaz4-c@my.cityu.edu.hk; h.yan@cityu.edu.hk
OI YAN, Hong/0000-0001-9661-3095; Nawaz, Mehmood/0000-0002-0978-2163
FU Hong Kong Research Grants Council [C1007-15G]; Hong Kong Institute for
   Data Science [8730039]
FX This work was supported in part by Hong Kong Research Grants Council
   (Project C1007-15G) and in part by the Hong Kong Institute for Data
   Science under Grant 8730039.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Aksoy Y, 2017, PROC CVPR IEEE, P228, DOI 10.1109/CVPR.2017.32
   Alpert S, 2012, IEEE T PATTERN ANAL, V34, P315, DOI 10.1109/TPAMI.2011.130
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P742, DOI 10.1109/TIP.2014.2383320
   Borji A, 2012, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2012.6247711
   Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470
   Chen TS, 2016, IEEE T NEUR NET LEAR, V27, P1135, DOI 10.1109/TNNLS.2015.2506664
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Ciaparrone G, 2020, NEUROCOMPUTING, V381, P61, DOI 10.1016/j.neucom.2019.11.023
   Cong RM, 2019, IEEE T CIRC SYST VID, V29, P2941, DOI 10.1109/TCSVT.2018.2870832
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Fang YM, 2012, IEEE T MULTIMEDIA, V14, P187, DOI 10.1109/TMM.2011.2169775
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Gopalakrishnan V, 2009, IEEE T MULTIMEDIA, V11, P892, DOI 10.1109/TMM.2009.2021726
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   He XT, 2019, IEEE T CIRC SYST VID, V29, P1394, DOI 10.1109/TCSVT.2018.2834480
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Ji YZ, 2018, NEUROCOMPUTING, V316, P357, DOI 10.1016/j.neucom.2018.08.013
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Karklin Y, 2009, NATURE, V457, P83, DOI 10.1038/nature07481
   Kuen J, 2016, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2016.399
   Lei JJ, 2016, IEEE T MULTIMEDIA, V18, P1783, DOI 10.1109/TMM.2016.2592325
   Li CY, 2015, PROC CVPR IEEE, P2710, DOI 10.1109/CVPR.2015.7298887
   Li GB, 2018, AAAI CONF ARTIF INTE, P7024
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2016, IEEE T IMAGE PROCESS, V25, P5012, DOI 10.1109/TIP.2016.2602079
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Li ZC, 2017, IEEE INT CONF COMP V, P1199, DOI 10.1109/ICCVW.2017.145
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Nawaz M, 2020, EXPERT SYST APPL, V161, DOI 10.1016/j.eswa.2020.113654
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Riche N, 2013, SIGNAL PROCESS-IMAGE, V28, P642, DOI 10.1016/j.image.2013.03.009
   Rutishauser U, 2004, PROC CVPR IEEE, P37
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Tavakoli HR, 2011, LECT NOTES COMPUT SC, V6688, P666, DOI 10.1007/978-3-642-21227-7_62
   Tong N, 2015, PROC CVPR IEEE, P1884, DOI 10.1109/CVPR.2015.7298798
   Tu WC, 2016, PROC CVPR IEEE, P2334, DOI 10.1109/CVPR.2016.256
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang W., ARXIV190409146, V2019
   Wang ZL, 2017, IEEE T MULTIMEDIA, V19, P750, DOI 10.1109/TMM.2016.2636739
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Xie YL, 2013, IEEE T IMAGE PROCESS, V22, P1689, DOI 10.1109/TIP.2012.2216276
   Xu Y., 2018, IEEE T PATTERN ANAL
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Zeng Y, 2019, PROC CVPR IEEE, P6067, DOI 10.1109/CVPR.2019.00623
   Zhang DW, 2017, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2017.436
   Zhang F, 2015, IEEE T GEOSCI REMOTE, V53, P2175, DOI 10.1109/TGRS.2014.2357078
   Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang T., 2019, IEEE T MULTIMEDIA, V21
   Zhang T., 2019, IEEE T MULTIMEDIA, V21
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 69
TC 18
Z9 18
U1 2
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2902
EP 2916
DI 10.1109/TMM.2020.3019688
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600029
DA 2024-07-18
ER

PT J
AU Wang, H
   Du, YT
   Zhang, GX
   Cai, ZM
   Su, C
AF Wang, Hang
   Du, Youtian
   Zhang, Guangxun
   Cai, Zhongmin
   Su, Chang
TI Learning Fundamental Visual Concepts Based on Evolved Multi-Edge Concept
   Graph
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fundamental visual concepts; concept graph; model evolution; cross media
ID IMAGE ANNOTATION; TAG COMPLETION
AB In general, visual media comprises a set of elements of basic semantics, named fundamental visual concepts, that may not be semantically decomposed, such as objects, scenes and actions. This paper proposes a dynamic learning framework for fundamental visual concept learning from image-textual description paired data based on an evolved multi-edge concept graph (EMCG). First, we construct a multi-edge concept graph to represent the relationships between visual concept instances, in which we introduce two types of edges named visual edges and semantic edges to describe the connection strength in terms of visual appearance and semantic content. Second, we evolve the graph by updating connection strength based on the predicted results of concept learning. Finally, we present a growth algorithm for the multi-edge concept graph to handle cross-dataset concept learning. Driven by the predictions, the multi-edge concept graph can dynamically evolve over time by adjusting the connection strength to adapt better to the observations. In addition, our approach can be considered a weakly-supervised learning algorithm since no labeled concepts are employed for learning. Experimental results demonstrate that evolution can significantly improve the learning of fundamental visual concepts by 14.2%, 7.9% and 12.7% in terms of F1-score for the MSRC, VOC2012 and MSCOCO datasets, respectively, and that the proposed EMCG approach largely outperforms the compared approaches.
C1 [Wang, Hang; Du, Youtian; Zhang, Guangxun; Cai, Zhongmin] Xi An Jiao Tong Univ, Minist Educ, Key Lab Intelligent Networks & Network Secur, Xian 710049, Peoples R China.
   [Su, Chang] Cornell Univ, Dept Hlthcare Policy & Res Weill Cornell Med, New York, NY 10065 USA.
C3 Xi'an Jiaotong University; Cornell University; Weill Cornell Medicine
RP Du, YT (corresponding author), Xi An Jiao Tong Univ, Minist Educ, Key Lab Intelligent Networks & Network Secur, Xian 710049, Peoples R China.
EM wanghang1128@stu.xjtu.edu.cn; duyt@mail.xjtu.edu.cn;
   zhangguangxun@stu.xjtu.edu.cn; zmcai@mail.xjtu.edu.cn;
   chs4001@med.cornell.edu
RI Wang, Hang/GXG-2858-2022; Wang, Hang/HHR-9240-2022
OI Wang, Hang/0000-0001-9876-8285; Du, Youtian/0000-0002-1714-3433
FU National Key R&D Program of China [2018AAA0101501]; Science and
   Technology Project of SGCC (State Grid Corporation of China)
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018AAA0101501 and in part by the Science and Technology
   Project of SGCC (State Grid Corporation of China): fundamental theory of
   human-in-the-loop hybrid-augmented intelligence for power grid dispatch
   and control.
CR Amiri SH, 2015, PATTERN RECOGN, V48, P2241, DOI 10.1016/j.patcog.2015.01.015
   Amritkar C, 2018, P 2018 4 INT C COMP, P1
   [Anonymous], 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
   [Anonymous], 2018, [No title captured], P44
   Barnard K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P408, DOI 10.1109/ICCV.2001.937654
   Barrat A., 2008, DYNAMICAL PROCESSES
   Bouritsas G, 2018, PROC CVPR IEEE, P4914, DOI 10.1109/CVPR.2018.00516
   Boyn S, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms14736
   Chapelle O., 2006, SEMISUPERVISED LEARN
   Chen XL, 2013, IEEE I CONF COMP VIS, P1409, DOI 10.1109/ICCV.2013.178
   Dhouioui Z, 2014, 2014 PROCEEDINGS OF THE IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM 2014), P764, DOI 10.1109/ASONAM.2014.6921672
   Ding XM, 2016, IEEE T MULTIMEDIA, V18, P1616, DOI 10.1109/TMM.2016.2572000
   Divvala SK, 2014, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2014.412
   Du YT, 2019, IEEE T IMAGE PROCESS, V28, P3598, DOI 10.1109/TIP.2019.2899944
   Feng J, 2017, AAAI CONF ARTIF INTE, P1884
   Feng SL, 2004, PROC CVPR IEEE, P1002
   Gao LL, 2015, PROC CVPR IEEE, P4371, DOI 10.1109/CVPR.2015.7299066
   Gonçalves B, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0022656
   Gong YY, 2018, PROC CVPR IEEE, P8659, DOI 10.1109/CVPR.2018.00903
   Huang SJ, 2019, IEEE T PATTERN ANAL, V41, P2614, DOI 10.1109/TPAMI.2018.2861732
   Huang Zhongqiang, 2009, P 2009 C EMPIRICAL M, P832
   Jeon J., 2003, P 26 ANN INT ACM SIG
   Jia Yangqing., 2013, Advances in neural information processing systems 26, P1842
   Ke X, 2019, IEEE T MULTIMEDIA, V21, P2093, DOI 10.1109/TMM.2019.2895511
   Ke X, 2017, PATTERN RECOGN, V71, P60, DOI 10.1016/j.patcog.2017.05.020
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Lei CY, 2016, IEEE T MULTIMEDIA, V18, P687, DOI 10.1109/TMM.2015.2477277
   Li QN, 2013, PROC CVPR IEEE, P851, DOI 10.1109/CVPR.2013.115
   Liang JW, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P32, DOI 10.1145/3078971.3079003
   Lieberman E, 2005, NATURE, V433, P312, DOI 10.1038/nature03204
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin ZJ, 2013, PROC CVPR IEEE, P1618, DOI 10.1109/CVPR.2013.212
   Liu D., 2010, Proceedings of ACM International Conference on Multimedea, P25
   Liu J, 2009, PATTERN RECOGN, V42, P218, DOI 10.1016/j.patcog.2008.04.012
   Liu JJ, 2007, PROCEEDINGS OF THE 2007 CHINESE CONTROL AND DECISION CONFERENCE, P605, DOI 10.1145/1291233.1291380
   Murthy VN, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P603, DOI 10.1145/2671188.2749391
   Palla G, 2007, NATURE, V446, P664, DOI 10.1038/nature05670
   PAVAN KM, 2013, P IEEE 4 INT C COMP, P1
   Pavlopoulou MEG, 2017, 2017 12TH INTERNATIONAL WORKSHOP ON SEMANTIC AND SOCIAL MEDIA ADAPTATION AND PERSONALIZATION (SMAP 2017), P40, DOI 10.1109/SMAP.2017.8022665
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Petrov S., 2012, Overview of the 2012 shared task on parsing the web
   Pham AT, 2017, IEEE T PATTERN ANAL, V39, P2381, DOI 10.1109/TPAMI.2017.2647944
   Pham AT, 2015, PR MACH LEARN RES, V37, P2427
   Putthividhya D, 2010, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2010.5540000
   Ramon y Cajal S., 1894, ProcRoy Soc London, V55, P444, DOI 10.1098/rspl.1894.0063
   Reichart R., 2007, Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, P616
   Rosenfeld, 2005, SEMISUPERVISED LEARN
   Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sindhwani V., 2008, Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008), Helsinki, Finland, June 5-9, 2008, P976, DOI DOI 10.1145/1390156.1390279
   Song J, 2016, IEEE T IMAGE PROCESS, V25, P4999, DOI 10.1109/TIP.2016.2601260
   Suzuki Jun., 2008, Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, P665
   Takaffoli M, 2014, 2014 PROCEEDINGS OF THE IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM 2014), P9, DOI 10.1109/ASONAM.2014.6921553
   Wang H, 2011, PROC CVPR IEEE, P793, DOI 10.1109/CVPR.2011.5995379
   Wang H, 2009, IEEE I CONF COMP VIS, P2029, DOI 10.1109/ICCV.2009.5459447
   Wang Mei, 2008, Journal of Software, V19, P2449, DOI 10.3724/SP.J.1001.2008.02449
   Wang S.-J., 2019, P 2019 SIAM INT C DA, P289, DOI [10.1137/1.9781611975673.33, DOI 10.1137/1.9781611975673.33]
   Wasserman S., 1994, Social network analysis: Methods and applications'
   Wu BY, 2018, PROC CVPR IEEE, P7967, DOI 10.1109/CVPR.2018.00831
   Wu JJ, 2015, PROC CVPR IEEE, P3460, DOI 10.1109/CVPR.2015.7298968
   Wu L, 2013, IEEE T PATTERN ANAL, V35, P716, DOI 10.1109/TPAMI.2012.124
   Xu JJ, 2014, IEEE T MULTIMEDIA, V16, P403, DOI 10.1109/TMM.2013.2291218
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P346, DOI 10.1109/TMM.2015.2393635
   Yarowsky D., 1995, 33 ANN M ASS COMPUTA, P189, DOI DOI 10.3115/981658.981684
   Zhang ML, 2011, IEEE T SYST MAN CY B, V41, P1612, DOI 10.1109/TSMCB.2011.2157998
   Zhang MX, 2019, IEEE T IMAGE PROCESS, V28, P32, DOI 10.1109/TIP.2018.2855415
   Zhang T, 2016, PROC CVPR IEEE, P2036, DOI 10.1109/CVPR.2016.224
   Zighed D. A., 2002, Principles of Data Mining and Knowledge Discovery. 6th European Conference, PKDD 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2431), P475
NR 68
TC 1
Z9 1
U1 2
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4400
EP 4413
DI 10.1109/TMM.2020.3042072
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XM6HD
UT WOS:000728924800001
DA 2024-07-18
ER

PT J
AU Wu, HS
   Yan, W
   Li, P
   Wen, ZK
AF Wu, Huisi
   Yan, Wei
   Li, Ping
   Wen, Zhenkun
TI Deep Texture Exemplar Extraction Based on Trimmed T-CNN
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Standards; Histograms; Deep learning; Tools; Search
   problems; Task analysis; Deep learning; texture convolutional neural
   network; trimmed convolutional neural network; texture exemplar
   recognition; texture exemplar extraction
ID OBJECT DETECTION; NETWORK
AB Texture exemplar has been widely used in synthesizing 3D movie scenes and appearances of virtual objects. Unfortunately, conventional texture synthesis methods usually only emphasized on generating optimal target textures with arbitrary sizes or diverse effects, and put little attention to automatic texture exemplar extraction. Obtaining texture exemplars is still a labor intensive task, which usually requires carefully cropping and post-processing. In this paper, we present an automatic texture exemplar extraction based on Trimmed Texture Convolutional Neural Network (Trimmed T-CNN). Specifically, our Trimmed T-CNN is filter banks for texture exemplar classification and recognition. Our Trimmed T-CNN is learned with a standard ideal exemplar dataset containing thousands of desired texture exemplars, which were collected and cropped by our invited artists. To efficiently identify the exemplar candidates from an input image, we employ a selective search algorithm to extract the potential texture exemplar patches. We then put all candidates into our Trimmed T-CNN for learning ideal texture exemplars based on our filter banks. Finally, optimal texture exemplars are identified with a scoring and ranking scheme. Our method is evaluated with various kinds of textures and user studies. Comparisons with different feature-based methods and different deep CNN architectures (AlexNet, VGG-M, Deep-TEN and FV-CNN) are also conducted to demonstrate its effectiveness.
C1 [Wu, Huisi; Yan, Wei; Wen, Zhenkun] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
   [Li, Ping] Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong 999077, Peoples R China.
C3 Shenzhen University; Hong Kong Polytechnic University
RP Wu, HS (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
EM hswu@szu.edu.cn; 1744832780@qq.com; p.li@polyu.edu.hk; wenzk@szu.edu.cn
RI Li, Ping/AAO-2019-2020
OI Li, Ping/0000-0002-1503-0240; Wu, Huisi/0000-0002-0399-9089
FU National Natural Science Foundation of China [61973221]; Natural Science
   Foundation of Guangdong Province of China [2018A030313381,
   2019A1515011165]; Hong Kong Polytechnic University [P0030419, P0030929]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61973221, in part by the Natural Science
   Foundation of Guangdong Province of China under Grants 2018A030313381
   and 2019A1515011165, and in part by The Hong Kong Polytechnic University
   under Grants P0030419 and P0030929. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Joao M. Ascenso.
CR Andrearczyk V, 2016, PATTERN RECOGN LETT, V84, P63, DOI 10.1016/j.patrec.2016.08.016
   [Anonymous], 2018, ARXIV180101933
   [Anonymous], EXTERNAL REPORT
   Bell S, 2015, PROC CVPR IEEE, P3479, DOI 10.1109/CVPR.2015.7298970
   Bergmann U., 2017, Proceedings of the 34th International Conference on Machine Learning-Volume, P469
   Caputo B, 2005, IEEE I CONF COMP VIS, P1597, DOI 10.1109/iccv.2005.54
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen FH, 2018, IEEE T MULTIMEDIA, V20, P997, DOI 10.1109/TMM.2017.2757769
   Choi J, 2019, IEEE T MULTIMEDIA, V21, P2083, DOI 10.1109/TMM.2019.2892301
   Cimpoi M, 2016, INT J COMPUT VISION, V118, P65, DOI 10.1007/s11263-015-0872-3
   Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007
   Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461
   Dai DX, 2014, PROC CVPR IEEE, P3027, DOI 10.1109/CVPR.2014.387
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Elad M, 2017, IEEE T IMAGE PROCESS, V26, P2338, DOI 10.1109/TIP.2017.2678168
   Gatys L., 2015, NIPS
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gu K, 2020, IEEE T MULTIMEDIA, V22, P311, DOI 10.1109/TMM.2019.2929009
   Han S, 2015, ADV NEUR IN, V28
   Hayman E, 2004, LECT NOTES COMPUT SC, V2034, P253
   Huisi Wu, 2018, Computational Visual Media, V4, P173, DOI 10.1007/s41095-018-0106-z
   Jetchev N, 2017, ARXIV161108207CSSTAT
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lagae A, 2010, COMPUT GRAPH FORUM, V29, P2579, DOI 10.1111/j.1467-8659.2010.01827.x
   Lazebnik S, 2005, IEEE T PATTERN ANAL, V27, P1265, DOI 10.1109/TPAMI.2005.151
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Lin J, 2017, IEEE T MULTIMEDIA, V19, P1968, DOI 10.1109/TMM.2017.2713410
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Liu XP, 2012, IEEE T VIS COMPUT GR, V18, P1836, DOI 10.1109/TVCG.2012.75
   Lockerman Y., 2015, U.S. Patent, Patent No. [9,007,373, 9007373]
   Lockerman YD, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925964
   Lockerman YD, 2013, INT C COMP AID DES C, P397, DOI 10.1109/CADGraphics.2013.65
   Lu JY, 2009, COMPUT GRAPH FORUM, V28, P667, DOI 10.1111/j.1467-8659.2009.01407.x
   Moritz J, 2017, COMPUT GRAPH FORUM, V36, P177, DOI 10.1111/cgf.13117
   Raad L, 2018, ANN MATH SCI APPL, V3, P89
   Raad L, 2016, J MATH IMAGING VIS, V56, P260, DOI 10.1007/s10851-016-0656-6
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sendik O, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3015461
   Sharan L., 2009, J VISION, V9, P784, DOI 10.1167/9.8.784
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Ulyanov D, 2016, PR MACH LEARN RES, V48
   Wang JZ, 2018, IEEE T MULTIMEDIA, V20, P2578, DOI 10.1109/TMM.2018.2855081
   Wang WC, 2013, IEEE T IMAGE PROCESS, V22, P4237, DOI 10.1109/TIP.2013.2271426
   Wei LY, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360651
   Xiao HX, 2018, IEEE T MULTIMEDIA, V20, P3239, DOI 10.1109/TMM.2018.2830098
   Xie J, 2017, IEEE T MULTIMEDIA, V19, P2463, DOI 10.1109/TMM.2017.2698200
   Xu Y, 2009, INT J COMPUT VISION, V83, P85, DOI 10.1007/s11263-009-0220-6
   Xue J, 2017, PROC CVPR IEEE, P6940, DOI 10.1109/CVPR.2017.734
   Yang F, 2019, IEEE T IMAGE PROCESS, V28, P2502, DOI 10.1109/TIP.2018.2886807
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Yu TZ, 2019, IEEE T MULTIMEDIA, V21, P2504, DOI 10.1109/TMM.2019.2907060
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
   Zhang H, 2017, PROC CVPR IEEE, P2896, DOI 10.1109/CVPR.2017.309
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
NR 57
TC 3
Z9 3
U1 2
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4502
EP 4514
DI 10.1109/TMM.2020.3043130
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XM6HD
UT WOS:000728924800009
DA 2024-07-18
ER

PT J
AU Wu, YL
   Wang, SH
   Song, GL
   Huang, QM
AF Wu, Yiling
   Wang, Shuhui
   Song, Guoli
   Huang, Qingming
TI Augmented Adversarial Training for Cross-Modal Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal retrieval; data alignment; adversa-rial training
ID IMAGES
AB Cross-modal retrieval has received considerable attention in recent years. The core of cross-modal retrieval is to find a representation space to align data from different modalities according to their semantics. In this paper, we propose a cross-modal retrieval method that aligns data from different modalities by transferring one source modality to another target modality with augmented adversarial training. To preserve the semantic meaning in the modality transfer process, we employ the idea of conditional GANs and augment it. The key idea is to incorporate semantic information from the label space into the adversarial training process by sampling more semantic relevant and irrelevant source-target sample pairs. The augmented sample pairs improve the alignment from two aspects. First, relevant source-target sample pairs provide more training samples, leading to a better guidance of the alignment of fake targets and true paired targets. Second, relevant and irrelevant source-target sample pairs teach the discriminator to better distinguish true relevant pairs from fake relevant pairs, which guides the generator to better transfer from the source modality to the target modality. Extensive experiments compared with state-of-the-art methods show the promising power of our approach.
C1 [Wu, Yiling; Wang, Shuhui; Huang, Qingming] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Wu, Yiling] Huawei Cloud & AI, Shenzhen 518129, Peoples R China.
   [Song, Guoli] Peng Cheng Lab, Res Ctr Artificial Intelligence, Shenzhen 518066, Peoples R China.
   [Huang, Qingming] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101408, Peoples R China.
   [Huang, Qingming] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Huawei Technologies; Peng Cheng Laboratory; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS; Peng Cheng Laboratory
RP Wang, SH (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM yiling.wu@vipl.ict.ac.cn; waneshuhui@ict.ac.cn; songgl@pcl.ac.cn;
   qmhuang@ucas.ac.cn
OI Song, Guoli/0000-0002-5452-3697
FU National Key R&D Program of China [2018AAA0102003]; National Natural
   Science Foundation of China [61672497, 61620106009, 61836002, 61931008,
   U1636214]; Key Research Program of Frontier Sciences, CAS
   [QYZDJ-SSW-SYS013]; China Postdoctoral Science Foundation [119103S291]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018AAA0102003, in part by National Natural Science
   Foundation of China under Grants 61672497, 61620106009, 61836002,
   61931008, and U1636214, in part by the Key Research Program of Frontier
   Sciences, CAS: QYZDJ-SSW-SYS013, and in part by Project funded by China
   Postdoctoral Science Foundation under Grant 119103S291. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Mohammed Daoudi.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], 2010, BMVC
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bai B, 2010, INFORM RETRIEVAL, V13, P291, DOI 10.1007/s10791-009-9117-9
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Deng C, 2016, IEEE T MULTIMEDIA, V18, P208, DOI 10.1109/TMM.2015.2508146
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hua Y, 2016, IEEE T MULTIMEDIA, V18, P1201, DOI 10.1109/TMM.2016.2535864
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jia YQ, 2011, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2011.6126524
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Masci J, 2014, IEEE T PATTERN ANAL, V36, P824, DOI 10.1109/TPAMI.2013.225
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Reed S, 2016, PR MACH LEARN RES, V48
   Rosipal R, 2006, LECT NOTES COMPUT SC, V3940, P34, DOI 10.1007/11752790_2
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Song GL, 2021, IEEE T PATTERN ANAL, V43, P858, DOI 10.1109/TPAMI.2019.2942028
   Song GL, 2017, IEEE T IMAGE PROCESS, V26, P4168, DOI 10.1109/TIP.2017.2713045
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tran TQN, 2016, PROC CVPR IEEE, P2046, DOI 10.1109/CVPR.2016.225
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26, DOI DOI 10.1007/S12654-012-0173-1
   Verma Y, 2017, COMPUT VIS IMAGE UND, V154, P48, DOI 10.1016/j.cviu.2016.10.001
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang KY, 2016, IEEE T PATTERN ANAL, V38, P2010, DOI 10.1109/TPAMI.2015.2505311
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang SH, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1398, DOI 10.1145/3240508.3240535
   Yao T, 2015, IEEE I CONF COMP VIS, P28, DOI 10.1109/ICCV.2015.12
   Zhang H, 2018, AAAI CONF ARTIF INTE, P7542
   Zhang H, 2016, PROC CVPR IEEE, P1105, DOI 10.1109/CVPR.2016.125
   Zhang L, 2017, IEEE T MULTIMEDIA, V19, P1220, DOI 10.1109/TMM.2016.2646219
   Zhang L, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1355, DOI 10.1145/2964284.2964336
   Zhang X, 2018, LECT NOTES COMPUT SC, V11219, P614, DOI 10.1007/978-3-030-01267-0_36
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhuang YT, 2008, IEEE T MULTIMEDIA, V10, P221, DOI 10.1109/TMM.2007.911822
NR 51
TC 16
Z9 19
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 559
EP 571
DI 10.1109/TMM.2020.2985540
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QA6NL
UT WOS:000613560200004
DA 2024-07-18
ER

PT J
AU Xu, WR
   Yu, J
   Miao, ZJ
   Wan, LL
   Tian, Y
   Ji, Q
AF Xu, Wanru
   Yu, Jian
   Miao, Zhenjiang
   Wan, Lili
   Tian, Yi
   Ji, Qiang
TI Deep Reinforcement Polishing Network for Video Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Grammar; Visualization; Steel; Clamps; Task analysis; Decoding;
   Semantics; Video captioning; deep reinforcement learning; word
   polishing; grammar polishing
AB The video captioning task aims to describe video content using several natural-language sentences. Although one-step encoder-decoder models have achieved promising progress, the generations always involve many errors, which are mainly caused by the large semantic gap between the visual domain and the language domain and by the difficulty in long-sequence generation. The underlying challenge of video captioning, i.e., sequence-to-sequence mapping across different domains, is still not well handled. Inspired by the proofreading procedure of human beings, the generated caption can be gradually polished to improve its quality. In this paper, we propose a deep reinforcement polishing network (DRPN) to refine the caption candidates, which consists of a word-denoising network (WDN) to revise word errors and a grammar-checking network (GCN) to revise grammar errors. On the one hand, the long-term reward in deep reinforcement learning benefits the long-sequence generation, which takes the global quality of caption sentences into account. On the other hand, the caption candidate can be considered a bridge between visual and language domains, where the semantic gap is gradually reduced with better candidates generated by repeated revisions. In experiments, we present adequate evaluations to show that the proposed DRPN achieves comparable and even better performance than the state-of-the-art methods. Furthermore, the DRPN is model-irrelevant and can be integrated into any video captioning models to refine their generated caption sentences.
C1 [Xu, Wanru; Miao, Zhenjiang; Wan, Lili] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Xu, Wanru; Miao, Zhenjiang; Wan, Lili] Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
   [Xu, Wanru] Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
   [Yu, Jian; Tian, Yi] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.
   [Yu, Jian; Tian, Yi] Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
   [Ji, Qiang] Rensselaer Polytech Inst, Dept Elect & Comp Engn, Troy, NY 12180 USA.
C3 Beijing Jiaotong University; Beijing Jiaotong University; Beijing
   Jiaotong University; Beijing Jiaotong University; Rensselaer Polytechnic
   Institute
RP Xu, WR (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM xuwanru@bjtu.edu.cn; jianyu@bjtu.edu.cn; zjmiao@bjtu.edu.cn;
   llwan@bjtu.edu.cn; tianyi@bjtu.edu.cn; qji@ecse.rpi.edu
RI Yu, Jian/HJY-2670-2023
OI Wan, Lili/0000-0002-9520-5425
FU NSFC [61672089, 61703436, 61572064, 61906013, 61273274, 61876016,
   61632004]; National Key R&D Program of China [2018AAA0100302]; China
   Postdoctoral Science Foundation [2019M650469]; CELFA
FX This work was supported in part by NSFC under Grants 61672089, 61703436,
   61572064, 61906013, 61273274, 61876016, and 61632004, in part by CELFA,
   in part by the National Key R&D Program of China under Grant
   2018AAA0100302, and in part by China Postdoctoral Science Foundation
   under Grant 2019M650469.
CR Aafaq N, 2020, ACM COMPUT SURV, V52, DOI 10.1145/3355390
   Abadi M, 2016, ACM SIGPLAN NOTICES, V51, P1, DOI [10.1145/3022670.2976746, 10.1145/2951913.2976746]
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Banerjee S., 2005, P WORKSHOP INTRINSIC, P65
   Barbu A., 2012, ARXIV12042742
   Chen Changhao, 2018, ARXIV180907491
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen J, 2023, J BANK FINANC, V151, DOI 10.1016/j.jbankfin.2019.01.004
   Chen SX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P6283
   Chen YY, 2018, LECT NOTES COMPUT SC, V11217, P367, DOI 10.1007/978-3-030-01261-8_22
   Dai B, 2017, IEEE I CONF COMP VIS, P2989, DOI 10.1109/ICCV.2017.323
   Duan X, 2018, ADV NEUR IN, V31
   Fang KC, 2019, AAAI CONF ARTIF INTE, P8271
   Gan C, 2017, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2017.108
   Gan Z, 2017, PROC CVPR IEEE, P1141, DOI 10.1109/CVPR.2017.127
   Gao LL, 2020, IEEE T PATTERN ANAL, V42, P1112, DOI 10.1109/TPAMI.2019.2894139
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Hanckmann P, 2012, LECT NOTES COMPUT SC, V7583, P372, DOI 10.1007/978-3-642-33863-2_37
   Hao WL, 2018, AAAI CONF ARTIF INTE, P6894
   Hori C, 2017, IEEE I CONF COMP VIS, P4203, DOI 10.1109/ICCV.2017.450
   Khan M. U. G., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1480, DOI 10.1109/ICCVW.2011.6130425
   Kingma D. P., 2014, arXiv
   Kojima A, 2002, INT J COMPUT VISION, V50, P171, DOI 10.1023/A:1020346032608
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Li LJ, 2019, IEEE WINT CONF APPL, P339, DOI 10.1109/WACV.2019.00042
   Li W, 2018, PATTERN RECOGN LETT, V105, P23, DOI 10.1016/j.patrec.2017.10.012
   Li XP, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1166, DOI 10.1145/3343031.3350971
   Li Y, 2018, PROC CVPR IEEE, P7492, DOI 10.1109/CVPR.2018.00782
   Liang XD, 2017, IEEE I CONF COMP VIS, P3382, DOI 10.1109/ICCV.2017.364
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Long Xiang, 2018, T ASSOC COMPUT LING, V6, P173, DOI DOI 10.1162/TACL_A_00013
   Lu H., IEEE T MULTIMEDIA
   Mun J, 2019, PROC CVPR IEEE, P3581, DOI 10.1109/CVPR.2019.00675
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pasunuru R., 2017, ARXIV170802300
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Rohrbach M, 2013, IEEE I CONF COMP VIS, P433, DOI 10.1109/ICCV.2013.61
   Shen ZQ, 2017, PROC CVPR IEEE, P5159, DOI 10.1109/CVPR.2017.548
   Song JK, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2737
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Thomason J., 2014, COLING, P1218
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S., 2014, ARXIV14124729
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795
   Wang JB, 2018, PROC CVPR IEEE, P7512, DOI 10.1109/CVPR.2018.00784
   Wang X, 2018, PROC CVPR IEEE, P4213, DOI 10.1109/CVPR.2018.00443
   Wu X, 2018, PROC CVPR IEEE, P6829, DOI 10.1109/CVPR.2018.00714
   Xia YC, 2017, ADV NEUR IN, V30
   Xu J, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P537, DOI 10.1145/3123266.3123448
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yang Y, 2018, IEEE T IMAGE PROCESS, V27, P5600, DOI 10.1109/TIP.2018.2855422
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yin GJ, 2019, PROC CVPR IEEE, P6234, DOI 10.1109/CVPR.2019.00640
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Zhang W., 2019, P ASME DESIGN ENG TE, V59186, DOI [10.1115/detc2019-98525, DOI 10.1115/DETC2019-98525, 10.1115/DETC2019-98525]
   Zhao B, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1177
   Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911
   Zhu Z., 2018, P BRIT MACH VIS C 20, P1
NR 64
TC 26
Z9 26
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1772
EP 1784
DI 10.1109/TMM.2020.3002669
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300023
DA 2024-07-18
ER

PT J
AU Zhang, HF
   Gu, YF
   Yao, YZ
   Zhang, Z
   Liu, L
   Zhang, J
   Shao, L
AF Zhang, Haofeng
   Gu, Yifan
   Yao, Yazhou
   Zhang, Zheng
   Liu, Li
   Zhang, Jian
   Shao, Ling
TI Deep Unsupervised Self-Evolutionary Hashing for Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Binary codes; Training; Feature extraction; Quantization (signal);
   Semantics; Visualization; Optimization; Deep unsupervised hashing; image
   retrieval; self-evolutionary learning
ID REPRESENTATIONS; SPARSE; CODES
AB Hashing methods have proven to be effective in the field of large-scale image retrieval. In recent years, the performance of hashing algorithms based on deep learning has greatly exceeded that of non-deep methods. However, most of the outstanding hashing methods are supervised models that heavily rely on annotated labels. In order to circumvent the huge overhead of labeling large-scale datasets, some unsupervised hashing algorithms have been proposed, such as pseudo labels and pseudo pairs. Since the image labels are strictly unavailable, some hyper-parameters in these methods are difficult to be selected, e.g., the final result is very sensitive to the picked number of categories or the chosen threshold of similarity for pairs. In addition, the calculation of pseudo-labels in high-dimensional space is not only computationally complex, but also has low precision. Therefore, in order to alleviate these issues in this paper, we propose a simple but effective Deep Unsupervised Self-evolutionary Hashing (DUSH) algorithm, which utilizes a curriculum learning strategy to iteratively select pseudo pairs from easy to hard in low dimensional Hamming space. Extensive experiments are conducted on four popular datasets, including two single-label datasets and two multi-label datasets, and the results show that our method can significantly outperform the state-of-the-art methods.
C1 [Zhang, Haofeng; Gu, Yifan; Yao, Yazhou] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Zhang, Zheng] Harbin Inst Technol, Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.
   [Zhang, Zheng] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
   [Liu, Li; Shao, Ling] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.
   [Zhang, Jian] Univ Technol Sydney, Global Big Data Technol Ctr, Ultimo, NSW, Australia.
   [Zhang, Jian] Univ Technol Sydney, Fac Engn & Informat Technol, Ultimo, NSW, Australia.
C3 Nanjing University of Science & Technology; Harbin Institute of
   Technology; University of Macau; University of Technology Sydney;
   University of Technology Sydney
RP Zhang, HF (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
EM zhanghf@njust.edu.cn; guyfan@njust.edu.cn; yazhou.yao@njust.edu.cn;
   darrenzz219@gmail.com; liuli1213@gmail.com; Jian.Zhang@uts.edu.au;
   ling.shao@ieee.org
RI Shao, Ling/D-3535-2011; Zhang, Zhang/JAX-2097-2023; liu,
   li/ADL-2178-2022
OI liu, li/0000-0002-6669-9382; Yao, Yazhou/0000-0002-0337-9410; Zhang,
   Zheng/0000-0003-1470-6998; Zhang, Jian/0000-0002-7240-3541
FU National Natural Science Foundation of China [61872187, 61929104,
   62002085]; "111" Program [B13022]; Guangdong Basic and Applied Basic
   Research Foundation [2019A1515110475]
FX Manuscript received April 6, 2020; revised July 22, 2020; accepted
   September 13, 2020. Date of publication September 21, 2020; date of
   current version September 24, 2021. This work was supported in part by
   the National Natural Science Foundation of China under Grants 61872187,
   61929104 and 62002085, and in part by the "111" Program (No. B13022),
   and also in part by the Guangdong Basic and Applied Basic Research
   Foundation under Grant 2019A1515110475. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Wen-Huang Cheng. (Corresponding author: Haofeng
   Zhang.)
CR Bai JL, 2019, IEEE T MULTIMEDIA, V21, P3178, DOI 10.1109/TMM.2019.2920601
   Bengio Y., 2009, P 26 ANN INT C MACH, V60, P6, DOI [DOI 10.1145/1553374.1553380, 10.1145/1553374.1553380]
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Chang JL, 2020, IEEE T PATTERN ANAL, V42, P809, DOI 10.1109/TPAMI.2018.2889949
   Charikar M., 2002, P THIRY 4 ANN ACM S, P380
   Chen CQ, 2019, PROC CVPR IEEE, P627, DOI 10.1109/CVPR.2019.00072
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Cui P, 2018, IEEE T MULTIMEDIA, V20, P198, DOI 10.1109/TMM.2017.2724843
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dizaji KG, 2018, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR.2018.00386
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024
   Hu QH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1584, DOI 10.1145/3123266.3123403
   Jhuo IH, 2016, INT C PATT RECOG, P2288, DOI 10.1109/ICPR.2016.7899977
   Jiang QY, 2018, AAAI CONF ARTIF INTE, P3342
   Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lai HJ, 2016, IEEE T IMAGE PROCESS, V25, P2469, DOI 10.1109/TIP.2016.2545300
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li Q, 2017, ADV NEUR IN, V30
   Li WJ, 2016, IJCAI, P1711
   Lin KV, 2016, PROC CVPR IEEE, P1183, DOI 10.1109/CVPR.2016.133
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu HM, 2017, PROC CVPR IEEE, P6259, DOI 10.1109/CVPR.2017.663
   Liu L, 2017, INT J COMPUT VISION, V122, P439, DOI 10.1007/s11263-016-0931-4
   Liu W., 2014, P NEURAL INF PROCESS, P3419
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Lu XQ, 2017, IEEE T IMAGE PROCESS, V26, P355, DOI 10.1109/TIP.2016.2627801
   Norouzi M., 2012, ADV NEURAL INFORM PR
   Ou MD, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P895, DOI 10.1145/2783258.2783283
   Peng YX, 2020, IEEE T MULTIMEDIA, V22, P2061, DOI 10.1109/TMM.2019.2951462
   Shen FM, 2018, IEEE T PATTERN ANAL, V40, P3034, DOI 10.1109/TPAMI.2018.2789887
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen YM, 2019, INT J COMPUT VISION, V127, P1614, DOI 10.1007/s11263-019-01166-4
   Shi XS, 2017, AAAI CONF ARTIF INTE, P2541
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song J., 2017, LECT NOTES ARTIF INT, P223, DOI DOI 10.1007/978-3-319-71249-9_14
   Song JK, 2018, AAAI CONF ARTIF INTE, P394
   Song JK, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P827, DOI 10.1145/2733373.2806341
   Su S., 2018, ADV NEUR IN
   Sun LF, 2017, IEEE T MULTIMEDIA, V19, P609, DOI 10.1109/TMM.2016.2635589
   Do TT, 2016, LECT NOTES COMPUT SC, V9909, P219, DOI 10.1007/978-3-319-46454-1_14
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wang YB, 2020, IEEE T MULTIMEDIA, V22, P1458, DOI 10.1109/TMM.2019.2947197
   Weiss Yair, 2009, Advances in Neural Information Processing Systems, P1753, DOI DOI 10.5555/2981780.2981999
   Weng L., 2019, BIG DATA ANAL LARGE, P239
   Weng L, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P259, DOI 10.1145/2671188.2749291
   Wu DY, 2019, PROC CVPR IEEE, P9061, DOI 10.1109/CVPR.2019.00928
   Xie L, 2016, SIGNAL PROCESS, V124, P81, DOI 10.1016/j.sigpro.2015.10.010
   Yang EK, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1064
   Yang HF, 2018, IEEE T PATTERN ANAL, V40, P437, DOI 10.1109/TPAMI.2017.2666812
   Yao YZ, 2019, IEEE T MULTIMEDIA, V21, P184, DOI 10.1109/TMM.2018.2847248
   Ye RZ, 2016, IEEE T CYBERNETICS, V46, P718, DOI 10.1109/TCYB.2015.2414299
   Yuan Y, 2014, ADV SOC SCI EDUC HUM, V6, P14
   Zhang HF, 2018, IEEE T IMAGE PROCESS, V27, P1626, DOI 10.1109/TIP.2017.2781422
   Zhang J, 2018, IEEE T MULTIMEDIA, V20, P2400, DOI 10.1109/TMM.2018.2804763
   Zhang Z, 2019, IEEE T PATTERN ANAL, V41, P1774, DOI 10.1109/TPAMI.2018.2847335
   Zhu XF, 2014, IEEE T IMAGE PROCESS, V23, P3737, DOI 10.1109/TIP.2014.2332764
   Zieba M., 2018, ADV NEUR IN
NR 65
TC 10
Z9 10
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3400
EP 3413
DI 10.1109/TMM.2020.3025000
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000035
DA 2024-07-18
ER

PT J
AU Zhao, BX
   Xiong, HY
   Bian, J
   Guo, ZS
   Xu, CZ
   Dou, DJ
AF Zhao, Baoxin
   Xiong, Haoyi
   Bian, Jiang
   Guo, Zhishan
   Xu, Cheng-Zhong
   Dou, Dejing
TI COMO: Efficient Deep Neural Networks Expansion With COnvolutional MaxOut
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolution; Spatial resolution; Convolutional neural networks; Deep
   learning; Computer architecture; Transforms; Artificial neural networks;
   computational and artificial intelligence; feedforward neural networks;
   multilayer perceptrons; neural networks
AB In this paper, we extend the classic MaxOut strategy, originally designed for Multiple Layer Preceptors (MLPs), into COnvolutional MaxOut (COMO) - a new strategy making deep convolutional neural networks wider with parameter efficiency. Compared to the existing solutions, such as ResNeXt for ResNet or Inception for VGG-alikes, COMO works well on both linear architectures and the ones with skipped connections and residual blocks. More specifically, COMO adopts a novel split-transform-merge paradigm that extends the layers with spatial resolution reduction into multiple parallel splits. For the layer with COMO, each split passes the input feature maps through a 4D convolution operator with independent batch normalization operators for transformation, then merge into the aggregated output of the original sizes through max-pooling. Such a strategy is expected to tackle the potential classification accuracy degradation due to the spatial resolution reduction, by incorporating the multiple splits and max-pooling-based feature selection. Our experiment using a wide range of deep architectures shows that COMO can significantly improve the classification accuracy of ResNet/VGG-alike networks based on a large number of benchmark datasets. COMO further outperforms the existing solutions, e.g., Inceptions, ResNeXts, SE-ResNet, and Xception, that make networks wider, and it dominates in the comparison of accuracy versus parameter sizes.
C1 [Zhao, Baoxin; Xiong, Haoyi; Bian, Jiang; Dou, Dejing] Baidu Inc, Big Data Lab, Beijing 100085, Peoples R China.
   [Zhao, Baoxin] Univ Chinese Acad Sci, Shenzhen Coll Adv Technol, Shenzhen 518055, Peoples R China.
   [Zhao, Baoxin] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
   [Xiong, Haoyi; Dou, Dejing] Natl Engn Lab Deep Learning Technol & Applicat, Beijing, Peoples R China.
   [Bian, Jiang; Guo, Zhishan] Univ Cent Florida, Dept Elect & Comp Engn, Orlando, FL 32816 USA.
   [Xu, Cheng-Zhong] Univ Macau, State Key Lab IOTSC, Fac Sci & Technol, Macau 999078, Peoples R China.
C3 Baidu; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; Chinese Academy of Sciences; Shenzhen Institute of
   Advanced Technology, CAS; State University System of Florida; University
   of Central Florida; University of Macau
RP Xiong, HY (corresponding author), Baidu Inc, Big Data Lab, Beijing 100085, Peoples R China.
EM bx.zhao@siat.ac.cn; xhyccc@gmail.com; bjbj11111@knights.ucf.edu;
   zsguo@ucf.edu; czxu@um.edu.mo; doudejing@baidu.com
RI XIONG, HAOYI/E-5079-2015; XU, CHENGZHONG/AAX-1707-2020
OI XIONG, HAOYI/0000-0002-5451-3253; XU, CHENGZHONG/0000-0001-9480-0356;
   Zhao, Baoxin/0000-0002-3222-539X; Dou, Dejing/0000-0003-2949-6874; Guo,
   Zhishan/0000-0002-5967-1058
FU National Key R&D Program of China [2018YFB1402600, 2019YFB2102100];
   Shenzhen Discipline Construction Project for Urban Computing and Data
   Intelligence; Shenzhen Engineering Research Center for Beidou
   Positioning Service Improvement Technology [XMHT20190101035]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2019YFB2102100, in part by the National Key R&D Program of
   China under Grant 2018YFB1402600, in part by the Shenzhen Discipline
   Construction Project for Urban Computing and Data Intelligence, and in
   part by the Shenzhen Engineering Research Center for Beidou Positioning
   Service Improvement Technology under Grant XMHT20190101035.
CR Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fung I, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2511, DOI 10.1109/ICASSP.2018.8462280
   Goodfellow I., 2013, P 30 INT C INT C MAC, V28, P1319
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang YP, 2019, ADV NEUR IN, V32
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li W., 2017, ARXIV170505640
   Li XH, 2018, PR MACH LEARN RES, V80
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun WC, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P334, DOI 10.1109/VCIP.2014.7051574
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan MX, 2019, PR MACH LEARN RES, V97
   Thoma M, 2015, P INT C MACH LEARN, P2113
   Vielzeuf V., 2018, P EUROPEAN C COMPUTE
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Zagoruyko S., 2016, ARXIV160507146, DOI DOI 10.5244/C.30.87
NR 28
TC 6
Z9 7
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1722
EP 1730
DI 10.1109/TMM.2020.3002614
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300019
DA 2024-07-18
ER

PT J
AU Zhao, CR
   Lv, XB
   Zhang, Z
   Zuo, WM
   Wu, J
   Miao, DQ
AF Zhao, Cairong
   Lv, Xinbi
   Zhang, Zhang
   Zuo, Wangmeng
   Wu, Jun
   Miao, Duoqian
TI Deep Fusion Feature Representation Learning With Hard Mining
   Center-Triplet Loss for Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Measurement; Training; Optimization; Computational
   modeling; Lighting; Task analysis; Person re-identification;
   center-triplet model; fusion feature representation; hard mining
   center-triplet loss
ID NETWORK
AB Person re-identification (Re-ID) is a challenging task in the field of computer vision and focuses on matching people across images from different cameras. The extraction of robust feature representations from pedestrian images through CNNs with a single deterministic pooling operation is problematic as the features in real pedestrian images are complex and diverse. To address this problem, we propose a novel center-triplet (CT) model that combines the learning of robust feature representation and the optimization of metric loss function. Firstly, we design a fusion feature learning network (FFLN) with a novel fusion strategy consisting of max pooling and average pooling. Instead of adopting a single deterministic pooling operation, the FFLN combines two pooling operations that can learn high response values, bright features, and low response values, discriminative features simultaneously. Our model obtains more discriminative fusion features by adaptively learning the weights of the features learned by the corresponding pooling operations. In addition, we design a hard mining center-triplet loss (HCTL), a novel improved triplet loss, which effectively optimizes the intra/inter-class distance and reduces the cost of computing and mining hard training samples simultaneously, thereby enhancing the learning of robust feature representation. Finally, we proved our method can learn robust and discriminative feature representations for complex pedestrian images in real scenes. The experimental results also illustrate that our method achieves an 81.8% mAP and a 93.8% rank-1 accuracy on Market1501, a 68.2% mAP and an 83.3% rank-1 accuracy on DukeMTMC-ReID, and a 43.6% mAP and a 74.3% rank-1 accuracy on MSMT17, outperforming most state-of-the-art methods and achieving better performance for person re-identification.
C1 [Zhao, Cairong; Lv, Xinbi; Wu, Jun; Miao, Duoqian] Tongji Univ, Dept Comp Sci & Technol, Shanghai 1238, Peoples R China.
   [Zhang, Zhang] Chinese Acad Sci, CRIPAC, Beijing 100190, Peoples R China.
   [Zhang, Zhang] Chinese Acad Sci, NLPR, Beijing 100190, Peoples R China.
   [Zhang, Zhang] UCAS, Beijing 100049, Peoples R China.
   [Zuo, Wangmeng] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150090, Peoples R China.
C3 Tongji University; Chinese Academy of Sciences; Chinese Academy of
   Sciences; Institute of Automation, CAS; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS; Harbin Institute of
   Technology
RP Zhao, CR (corresponding author), Tongji Univ, Dept Comp Sci & Technol, Shanghai 1238, Peoples R China.
EM zhaocairong@tongji.edu.cn; 1833028@tongji.edu.cn; zzhang@nlpr.ia.ac.cn;
   cswmzuo@gmail.com; wujun@tongji.edu.cn; dqmiao@tongji.edu.cn
RI zhang, zheng/HCH-9684-2022; Miao, Duoqian/L-7378-2019; Yang,
   han/KFS-2671-2024; Zuo, Wangmeng/B-3701-2008
OI zhang, zhang/0000-0001-9425-3065; Zuo, Wangmeng/0000-0002-3330-783X
FU National Natural Science Foundation of China [61673299, 61203247,
   61573259, 61573255]; Fundamental Research Funds for the Central
   Universities; Open Project Program of the National Laboratory of Pattern
   Recognition
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61673299, 61203247, 61573259, and
   61573255 and in part by the Fundamental Research Funds for the Central
   Universities and the Open Project Program of the National Laboratory of
   Pattern Recognition. The authors contribute equally to this work.
CR Alex D., 2018, CLUSTER LOSS PERSON
   Ali M, 2018, P EUROPEAN C COMPUTE, P122
   [Anonymous], 2017, Margin Sample Mining Loss: A Deep Learning Based Method for Person Re-Identification
   [Anonymous], 2010, P 27 INT C MACHINE L
   [Anonymous], 2017, ARXIV170307737
   [Anonymous], 2016, PERSONNET PERSON RE
   Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Chen YB, 2017, IEEE INT CONF COMP V, P2590, DOI 10.1109/ICCVW.2017.304
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Ding GD, 2019, IEEE T MULTIMEDIA, V21, P2891, DOI 10.1109/TMM.2019.2916456
   Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005
   Guo YM, 2012, PATTERN RECOGN, V45, P3834, DOI 10.1016/j.patcog.2012.04.003
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XW, 2018, PROC CVPR IEEE, P1945, DOI 10.1109/CVPR.2018.00208
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Ming ZH, 2017, IEEE INT CONF COMP V, P1656, DOI 10.1109/ICCVW.2017.194
   Patruno C, 2019, PATTERN RECOGN, V89, P77, DOI 10.1016/j.patcog.2019.01.003
   Qi Lei, 2018, MASKREID MASK BASED
   Reddi S. J., 2018, INT C LEARN REPR
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu SH, 2019, INT J PAVEMENT ENG, V20, P33, DOI 10.1080/10298436.2016.1248204
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Yu DJ, 2014, LECT NOTES ARTIF INT, V8818, P364, DOI 10.1007/978-3-319-11740-9_34
   Yu Q., 2017, DEVIL IS MIDDLE EXPL
   Yu R, 2018, LECT NOTES COMPUT SC, V11220, P196, DOI 10.1007/978-3-030-01270-0_12
   ZHANG X, 2017, ALIGNEDREID SURPASSI
   Zhang X, 2017, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2017.578
   Zhao CR, 2020, PATTERN RECOGN, V97, DOI 10.1016/j.patcog.2019.107014
   Zhao CR, 2019, PATTERN RECOGN LETT, V117, P161, DOI 10.1016/j.patrec.2018.04.029
   Zhao CR, 2018, PATTERN RECOGN, V79, P79, DOI 10.1016/j.patcog.2018.01.033
   Zhao CR, 2017, PATTERN RECOGN, V71, P218, DOI 10.1016/j.patcog.2017.06.011
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng YT, 2018, PROC CVPR IEEE, P5089, DOI 10.1109/CVPR.2018.00534
   Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhou K., 2019, TORCHREID LIB DEEP L
NR 53
TC 72
Z9 76
U1 2
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2020
VL 22
IS 12
BP 3180
EP 3195
DI 10.1109/TMM.2020.2972125
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU9BG
UT WOS:000591817700013
DA 2024-07-18
ER

PT J
AU Thalmann, F
   Wiggins, GA
   Sandler, MB
AF Thalmann, Florian
   Wiggins, Geraint A.
   Sandler, Mark B.
TI Representing Modifiable and Reusable Musical Content on the Web With
   Constrained Multi-Hierarchical Structures
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Knowledge representation; semantic web; logic programming; music
   information retrieval; computer generated music
ID EXPLORATION; FRAMEWORK
AB The most commonly used formats for exchanging musical information today are limited in that they represent music as flat and rigid streams of events or as raw audio signals without any structural information about the content. Such files can only be listened to in a linear way and reused and manipulated in manners determined by a target application such as a Digital Audio Workstation. The publisher has no means to incorporate their intentions or understanding of the content. This article introduces an extension of the music formalism CHARM for the representation of modifiable and reusable musical content on the Web. It discusses how various kinds of multi-hierarchical graph structures together with logical constraints can be useful to model different musical situations. In particular, we focus on presenting solutions on how to interpret, navigate and schedule such structures in order for them to be played back. We evaluate the versatility of the representation in a number of practical examples created with a Web-based implementation based on Semantic Web technologies.
C1 [Thalmann, Florian; Sandler, Mark B.] Queen Mary Univ London, Ctr Digital Mus, London E1 4NS, England.
   [Wiggins, Geraint A.] Vrije Univ Brussel, Artificial Intelligence Lab, B-1050 Brussels, Belgium.
   [Wiggins, Geraint A.] Queen Mary Univ London, Cognit Sci Res Grp, London E1 4FZ, England.
C3 University of London; Queen Mary University London; Vrije Universiteit
   Brussel; University of London; Queen Mary University London
RP Thalmann, F (corresponding author), Queen Mary Univ London, Ctr Digital Mus, London E1 4NS, England.
EM thalm007@umn.edu; geraint.wiggins@vub.be; mark.sandler@qmul.ac.uk
RI ; Wiggins, Geraint/K-9443-2016
OI Sandler, Mark/0000-0002-5691-8107; Wiggins, Geraint/0000-0002-1587-112X
FU EPSRC [EP/L019981/1]; Fusing Audio and Semantic Technologies for
   Intelligent Music Production and Consumption; Royal Society; EPSRC
   [EP/L019981/1] Funding Source: UKRI
FX This work was supported in part by EPSRC under Grant EP/L019981/1 and in
   part by the Fusing Audio and Semantic Technologies for Intelligent Music
   Production and Consumption The work of M. B. Sandler was supported by
   the Royal Society as a recipient of aWolfson Research Merit Award.
CR Anders T, 2005, LECT NOTES COMPUT SC, V3389, P277
   Anders T., 2008, P INT COMP MUS C BEL
   Anders T, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1978802.1978809
   [Anonymous], 2014, P 1 INT WORKSH DIG L
   Aucouturier JJ, 2006, J NEW MUSIC RES, V35, P35, DOI 10.1080/09298210600696790
   Balaban M, 1996, COMPUT MUSIC J, V20, P96, DOI 10.2307/3681334
   Balaban M., 1988, P INT COMP MUS C COL, P56
   Barthet M, 2016, J AUDIO ENG SOC, V64, P673, DOI 10.17743/jaes.2016.0042
   Brinkman A. R., 1984, P INT COMP MUS C, P233
   Byrd W.J., 2010, THESIS
   Chen J, 2015, IEEE T MULTIMEDIA, V17, P1773, DOI 10.1109/TMM.2015.2460111
   Conklin N, 2002, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2002, P131, DOI 10.1109/INFVIS.2002.1173158
   Cuthbert M., 2010, P 11 INT SOC MUSIC I, P637, DOI DOI 10.5281/ZENODO.1416114
   Dannenberg R. B., 1986, P INT COMP MUS C, P130
   Dean M., 2004, SWRL: a semantic web Rule Language combining OWL and RuleML
   Fang Q, 2016, IEEE T MULTIMEDIA, V18, P702, DOI 10.1109/TMM.2016.2527602
   FURNAS GW, 1994, HUMAN FACTORS IN COMPUTING SYSTEMS, CHI '94 CONFERENCE PROCEEDINGS - CELEBRATING INTERDEPENDENCE, P330, DOI 10.1145/191666.191778
   Good M., 2001, XML C EXP, P03
   Graham M, 2010, INFORM VISUAL, V9, P235, DOI 10.1057/ivs.2009.29
   Harley N., 2015, P DEM 16 INT SOC MUS
   HARRIS PJF, 1991, INST PHYS CONF SER, V119, P55
   Hower W, 1996, KNOWL-BASED SYST, V9, P449, DOI 10.1016/S0950-7051(96)01055-6
   Hunt A., 2000, P TRENDS GEST CONTR
   Jiang YG, 2016, IEEE T MULTIMEDIA, V18, P2161, DOI 10.1109/TMM.2016.2614233
   Kifer M, 2008, LECT NOTES COMPUT SC, V5321, P1, DOI 10.1007/978-3-540-88808-6_1
   KRUMHANSL CL, 1982, MEM COGNITION, V10, P243, DOI 10.3758/BF03197636
   Kudumakis P, 2014, SIGNAL PROCESS-IMAGE, V29, P150, DOI 10.1016/j.image.2013.10.006
   Lerdahl F., 1985, A Generative Theory of Tonal Music
   Lewis D., 2011, P SUPP DIG HUM
   Li JX, 2012, IEEE T MULTIMEDIA, V14, P471, DOI 10.1109/TMM.2011.2181151
   Lokoc J, 2018, IEEE T MULTIMEDIA, V20, P3361, DOI 10.1109/TMM.2018.2830110
   Mazzola G., 2006, P DIG ART WEEKS P, P238
   McDermott D, 2002, LECT NOTES COMPUT SC, V2342, P250
   McGuffin M., 2004, P ACM C HYP, P153
   Meixner B, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3038925
   Olarte C., 2011, P CONSTR PROGR MUS, P133
   Pachet F., 1998, Proceedings ACM Multimedia 98, P351, DOI 10.1145/290747.290801
   Pachet F., 2000, P AUD ENG SOC CONV A, V109
   Pachet F, 2011, CONSTRAINTS, V16, P148, DOI 10.1007/s10601-010-9101-4
   Papadopoulos A, 2016, LECT NOTES COMPUT SC, V9892, P769, DOI 10.1007/978-3-319-44953-1_48
   Pohle T, 2007, IEEE T MULTIMEDIA, V9, P567, DOI 10.1109/TMM.2006.887991
   Raimond Y., 2008, Proceedings of the International Conference on Music Information Retrieval, P263
   Rolland P., 2002, P INT C MUS APPL US, P55
   SMAILL A, 1993, COMPUT HUMANITIES, V27, P7, DOI 10.1007/BF01830712
   Tan R, 2005, IEEE T MULTIMEDIA, V7, P869, DOI 10.1109/TMM.2005.854377
   Thalmann F., 2017, P WEB AUD C LOND UK
   Thalmann F., 2016, P WEB AUD C ATL GA U
   Thalmann F, 2018, ACM INT CONF PR SER, P1, DOI 10.1145/3243907.3243910
   Thalmann F, 2016, PROCEEDINGS OF AUDIO MOSTLY 2016 - A CONFERENCE ON INTERACTION WITH SOUND IN COOPERATION WITH ACM, P39, DOI 10.1145/2986416.2986445
   Thalmann F, 2016, IEEE INT C SEMANT CO, P47, DOI 10.1109/ICSC.2016.61
   Tojo S., 2012, P INT S COMP MUS MOD, P400
   Verborgh R, 2015, IEEE SOFTWARE, V32, P23, DOI 10.1109/MS.2015.63
   WIGGINS G, 1993, COMPUT MUSIC J, V17, P31, DOI 10.2307/3680941
   Wiggins G. A., 2009, P MOD METH MUS PROSP, P7
   Wiggins GeraintA., 1989, Proceedings of the IJCAI Workshop on Music and Artificial Intelligence, P63
   Wiggins GeraintA., 2010, Musica Scientia, V14, P231, DOI [DOI 10.1177/10298649100140S110, 10.1177/10298649100140-110]
   Wilmering T., 2016, P AUD ENG SOC CONV L, V141
   Wu Y, 2016, IEEE T MULTIMEDIA, V18, P2206, DOI 10.1109/TMM.2016.2614185
NR 58
TC 3
Z9 3
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2020
VL 22
IS 10
BP 2645
EP 2658
DI 10.1109/TMM.2019.2961207
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NT0FL
UT WOS:000572628000011
DA 2024-07-18
ER

PT J
AU Hu, GY
   Cui, B
   Yu, S
AF Hu, Guyue
   Cui, Bo
   Yu, Shan
TI Joint Learning in the Spatio-Temporal and Frequency Domains for
   Skeleton-Based Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Frequency-domain analysis; Transforms; Frequency synchronization;
   Semantics; Training; Skeleton; Data mining; Action recognition;
   frequency attention; synchronous local and non-local learning;
   soft-margin focal loss; multi-task learning
ID IMAGE; ENSEMBLE
AB Benefiting from its succinctness and robustness, skeleton-based action recognition has recently attracted much attention. Most existing methods utilize local networks (e.g. recurrent network, convolutional network, and graph convolutional network) to extract spatio-temporal dynamics hierarchically. As a consequence, the local and non-local dependencies, which contain more details and semantics respectively, are asynchronously captured in different level of layers. Moreover, existing methods are limited to the spatio-temporal domain and ignore information in the frequency domain. To better extract synchronous detailed and semantic information from multi-domains, we propose a residual frequency attention (rFA) block to focus on discriminative patterns in the frequency domain, and a synchronous local and non-local (SLnL) block to simultaneously capture the details and semantics in the spatio-temporal domain. In addition, to optimize the whole learning processes of the multi-branch network, we put it under a pseudo multi-task learning paradigm. During training, 1) a soft-margin focal loss (SMFL) is proposed to optimize the intra-branch separated learning process, which can automatically conduct data selection and encourage intrinsic margins in classifiers; 2) A mutual learning policy is also proposed to further facilitate the inter-branch collaborative learning process. Eventually, our approach achieves the state-of-the-art performance on several large-scale datasets for skeleton-based action recognition.
C1 [Hu, Guyue; Cui, Bo; Yu, Shan] Chinese Acad Sci, Brainnetome Ctr, Inst Automat, Beijing 100190, Peoples R China.
   [Hu, Guyue; Cui, Bo; Yu, Shan] Chinese Acad Sci, Natl Labo Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
   [Hu, Guyue; Cui, Bo; Yu, Shan] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Yu, Shan] CAS Ctr Excellence Brain Sci & Intelligence Techn, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; Institute of Automation, CAS; Chinese Academy of
   Sciences; University of Chinese Academy of Sciences, CAS
RP Hu, GY (corresponding author), Chinese Acad Sci, Brainnetome Ctr, Inst Automat, Beijing 100190, Peoples R China.; Hu, GY (corresponding author), Chinese Acad Sci, Natl Labo Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
EM guyue.hu@nlpr.ia.ac.cn; bo.cui@nlpr.ia.ac.cn; shan.yu@nlpr.ia.ac.cn
RI Yu, Shan/S-3123-2017; Hu, Guyue/AAG-9734-2020
OI Hu, Guyue/0000-0002-6198-8230; Yu, Shan/0000-0002-9008-6658; yu,
   shan/0000-0002-4385-6306
FU National Key Research and Development Program of China [2017YFA0105203];
   Strategic Priority Research Program of the Chinese Academy of Sciences
   (CAS) [XDB32040200]; Hundred-Talent Program of CAS
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2017YFA0105203, in part by the
   Strategic Priority Research Program of the Chinese Academy of Sciences
   (CAS) under Grant XDB32040200, and in part by the Hundred-Talent Program
   of CAS (for S.Y.).
CR [Anonymous], 2017, VAL ITS DER DISC
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], 2018, APPL POWER ELECT CO
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], 2010, P ADV NEUR INF PROC
   [Anonymous], 2017, 2017 IEEE 14 INT S
   [Anonymous], 2018, INT C ADV MECH SYST
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Beaudry C, 2014, IEEE IMAGE PROC, P1445, DOI 10.1109/ICIP.2014.7025289
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Elmadany NE, 2019, IEEE T MULTIMEDIA, V21, P1317, DOI 10.1109/TMM.2018.2875510
   Fan ZX, 2019, IEEE T MULTIMEDIA, V21, P363, DOI 10.1109/TMM.2018.2859620
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Hu G, 2019, ARXIV190802948
   Hu G, 2019, IEEE INT CON MULTI, P1216, DOI 10.1109/ICME.2019.00212
   Hu JF, 2015, PROC CVPR IEEE, P5344, DOI 10.1109/CVPR.2015.7299172
   Hussein M. E., 2013, P INT JOINT C ART IN, P2466
   Kay W., 2017, KINETICS HUMAN ACTIO
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kim S, 2019, IEEE WINT CONF APPL, P61, DOI 10.1109/WACV.2019.00014
   Kim TS, 2017, IEEE COMPUT SOC CONF, P1623, DOI 10.1109/CVPRW.2017.207
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115
   Lefkimmiatis S., 2017, P C COMP VIS PATT RE
   Li C, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P786
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Li M, 2016, IEEE T MULTIMEDIA, V18, P2293, DOI 10.1109/TMM.2016.2614228
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu J, 2018, IEEE T IMAGE PROCESS, V27, P1586, DOI 10.1109/TIP.2017.2785279
   Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Liu WY, 2016, PR MACH LEARN RES, V48
   Loshchilov I., 2015, P INT C MACH LEARN W
   Oyallon E, 2017, IEEE I CONF COMP VIS, P5619, DOI 10.1109/ICCV.2017.599
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Si C., 2018, P EUR C COMP VIS
   Sonka M., 2014, IMAGE PROCESSING ANA
   Tang YS, 2018, PROC CVPR IEEE, P5323, DOI 10.1109/CVPR.2018.00558
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339
   Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang X, 2017, P IEEE INT C COMP VI
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Xie C., 2018, P INT JOINT C ART IN, P1639
   Xu M, 2017, IEEE INT CON MULTI, P517, DOI 10.1109/ICME.2017.8019351
   Yan S., 2018, P ASS ADV ART INT
   Zhang P., 2019, C COMP VIS PATT REC
   Zhang P., 2018, P 15 EUR C COMP VIS, P136
   Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI [10.1109/ICCV.2017.233, 10.1109/ICCV.2017.231]
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhang YF, 2018, IEEE T MULTIMEDIA, V20, P1038, DOI 10.1109/TMM.2018.2808769
   Zhang YP, 2012, IEEE VTS VEH TECHNOL
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
NR 65
TC 27
Z9 29
U1 1
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2207
EP 2220
DI 10.1109/TMM.2019.2953325
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NE0TL
UT WOS:000562310200002
DA 2024-07-18
ER

PT J
AU Wu, YQ
   Xiang, YZ
   Guo, YT
   Tang, J
   Yin, ZX
AF Wu, Youqing
   Xiang, Youzhi
   Guo, Yutang
   Tang, Jin
   Yin, Zhaoxia
TI An Improved Reversible Data Hiding in Encrypted Images Using Parametric
   Binary Tree Labeling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Encryption; Binary trees; Labeling; Correlation; Data mining; Image
   restoration; Image encryption; reversible data hiding; parametric binary
   tree labeling; separately
ID EXPANSION
AB This work proposes an improved reversible data hiding scheme in encrypted images using parametric binary tree labeling(IPBTL-RDHEI), which takes advantage of the spatial correlation in the entire original image but not in small image blocks to reserve room for hiding data. Then the original image is encrypted with an encryption key and the parametric binary tree is used to label encrypted pixels into two different categories. Finally, one of the two categories of encrypted pixels can embed secret information by bit replacement. According to the experimental results, compared with several state-of-the-art methods, the proposed IPBTL-RDHEI method achieves higher embedding rate and outperforms the competitors. Due to the reversibility of IPBTL-RDHEI, the original plaintext image and the secret information can be restored and extracted losslessly and separately.
C1 [Wu, Youqing; Xiang, Youzhi; Tang, Jin; Yin, Zhaoxia] Anhui Univ, Sch Key Lab Intelligent Comp & Signal Proc, Minist Educ, Hefei 230601, Peoples R China.
   [Wu, Youqing; Guo, Yutang] Hefei Normal Univ, Sch Comp Sci & Technol, Hefei 230601, Peoples R China.
C3 Anhui University; Hefei Normal University
RP Yin, ZX (corresponding author), Anhui Univ, Sch Key Lab Intelligent Comp & Signal Proc, Minist Educ, Hefei 230601, Peoples R China.
EM wuyq.hfnu@qq.com; 1875738190@qq.com; guoyutang@hfnu.edu.cn;
   tj@ahu.edu.cn; yinzhaoxia@ahu.edu.cn
RI Yin, Zhaoxia/HRD-7425-2023
OI Yin, Zhaoxia/0000-0003-0387-4806
FU National Natural Science Foundation ofChina [61872003, U1636206,
   61872005, 61860206004]; Natural Science Foundation of Anhui Higher
   Education Institutions of China [KJ2013A217]
FX This work was supported in part by the National Natural Science
   Foundation ofChina underGrants 61872003, U1636206, 61872005, and
   61860206004 and in part by the Natural Science Foundation of Anhui
   Higher Education Institutions of China under Grant KJ2013A217. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Abdulmotaleb El Saddik.
CR Alattar AM, 2004, IEEE T IMAGE PROCESS, V13, P1147, DOI 10.1109/TIP.2004.828418
   [Anonymous], 2011, P 13 INF HID C PRAG
   Bas P., 2017, Image database of BOWS-2
   Celik MU, 2005, IEEE T IMAGE PROCESS, V14, P253, DOI 10.1109/TIP.2004.840686
   Chen KM, 2019, J VIS COMMUN IMAGE R, V58, P334, DOI 10.1016/j.jvcir.2018.12.023
   Chen XY, 2013, J SYST SOFTWARE, V86, P2620, DOI 10.1016/j.jss.2013.04.086
   Fridrich J, 2002, EURASIP J APPL SIG P, V2002, P185, DOI 10.1155/S1110865702000537
   Li XL, 2015, IEEE T INF FOREN SEC, V10, P2016, DOI 10.1109/TIFS.2015.2444354
   Li XL, 2013, IEEE T INF FOREN SEC, V8, P1091, DOI 10.1109/TIFS.2013.2261062
   Liao X, 2017, MULTIMED TOOLS APPL, V76, P20739, DOI 10.1007/s11042-016-3971-4
   Luo LX, 2010, IEEE T INF FOREN SEC, V5, P187, DOI 10.1109/TIFS.2009.2035975
   Ma KD, 2013, IEEE T INF FOREN SEC, V8, P553, DOI 10.1109/TIFS.2013.2248725
   Puech W, 2008, PROC SPIE, V6819, DOI 10.1117/12.766754
   Puteaux P, 2018, IEEE T INF FOREN SEC, V13, P1670, DOI 10.1109/TIFS.2018.2799381
   Puyang Y, 2018, IEEE INT WORKS INFOR
   Qian ZX, 2014, IEEE T MULTIMEDIA, V16, P1486, DOI 10.1109/TMM.2014.2316154
   Sachnev V, 2009, IEEE T CIRC SYST VID, V19, P989, DOI 10.1109/TCSVT.2009.2020257
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   Thodi DM, 2007, IEEE T IMAGE PROCESS, V16, P721, DOI 10.1109/TIP.2006.891046
   Tsai P, 2009, SIGNAL PROCESS, V89, P1129, DOI 10.1016/j.sigpro.2008.12.017
   Yi S, 2019, IEEE T MULTIMEDIA, V21, P51, DOI 10.1109/TMM.2018.2844679
   Zhang XP, 2013, IEEE T MULTIMEDIA, V15, P316, DOI 10.1109/TMM.2012.2229262
   Zhang XP, 2012, IEEE T INF FOREN SEC, V7, P826, DOI 10.1109/TIFS.2011.2176120
   Zhou JT, 2016, IEEE T CIRC SYST VID, V26, P441, DOI 10.1109/TCSVT.2015.2416591
NR 24
TC 71
Z9 85
U1 4
U2 68
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2020
VL 22
IS 8
BP 1929
EP 1938
DI 10.1109/TMM.2019.2952979
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MR2MS
UT WOS:000553424500002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yin, WB
   Shi, YH
   Zuo, WM
   Fan, XP
AF Yin, Wenbin
   Shi, Yunhui
   Zuo, Wangmeng
   Fan, Xiaopeng
TI A Co-Prediction-Based Compression Scheme for Correlated Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Decoding; Transforms; Convolutional codes; Optimization;
   Transform coding; Image reconstruction; Autoencoder; correlated images;
   image compression; rate distortion optimization; multi-stream networks
ID TRANSFORM
AB Deep learning has achieved a preliminary success in image compression due to the ability to learn the nonlinear spaces with compact features that training samples belong to. Unfortunately, it is not straightforward for the network based image compression methods to code multiple highly related images. In this paper, we propose a co-prediction based image compression (CPIC) which uses the multi-stream autoencoders to collaboratively code the multiple highly correlated images by enforcing the co-reference constraint on the multi-stream features. Patch samples fed into the multi-stream autoencoder, are generated through corresponding patch matching under permutation, which helps the autoencoder to learn the relationship among corresponding patches from the correlated images. Each stream network consists of encoder, decoder, importance map network and binarizer. In order to guide the allocation of local bit rate of the binary features, the important map network is employed to guarantee the compactness of learned features. A proxy function is used to make the binary operation for the code layer of the autoencoder differentiable. Finally, the network optimization is formulated as a rate distortion optimization. Experimental results prove that the proposed compression method outperforms JPEG2000 up to 1.5 dB in terms of PSNR.
C1 [Yin, Wenbin; Zuo, Wangmeng; Fan, Xiaopeng] Harbin Inst Technol, Dept Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Shi, Yunhui] Beijing Univ Technol, BJUT Fac Informat Technol, Beijing 100124, Peoples R China.
   [Fan, Xiaopeng] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 Harbin Institute of Technology; Beijing University of Technology; Peng
   Cheng Laboratory
RP Fan, XP (corresponding author), Harbin Inst Technol, Dept Comp Sci & Technol, Harbin 150001, Peoples R China.
EM ywb@hit.edu.cn; syhzm@bjut.edu.cn; cswmzuo@gmail.com; fxp@hit.edu.cn
RI Zuo, Wangmeng/B-3701-2008
FU National Natural Science Foundation of China [61972115, 61631017,
   61672066, 61976011, 61906008]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61972115, 61631017, 61672066, 61976011,
   and 61906008. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Zhu Li.
CR Balle J., 2017, P INT C LEARN REPR, P20
   Balle J, 2018, ICLR
   Boulgouris NV, 2001, IEEE T IMAGE PROCESS, V10, P1, DOI 10.1109/83.892438
   Dai W, 2009, IEEE INT SYMP INFO, P11, DOI 10.1109/ISIT.2009.5206032
   Diestel R., 2007, MATH GAZETTE, V4, P887
   Ding WP, 2007, IEEE T IMAGE PROCESS, V16, P416, DOI 10.1109/TIP.2006.888341
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1382, DOI 10.1109/TIP.2012.2231086
   FEIG E, 1995, INT CONF ACOUST SPEE, P2339, DOI 10.1109/ICASSP.1995.479961
   Ghanbari M., 2003, STANDARD CODECS IMAG, V2nd, P103
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gregor K, 2016, ADV NEUR IN, V29
   Haimi-Cohen R, 2016, SIGNAL PROCESS, V120, P71, DOI 10.1016/j.sigpro.2015.07.023
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Jegou H., 2008, INRIA HOLIDAY DATASE
   Jiang F, 2018, IEEE T CIRC SYST VID, V28, P3007, DOI 10.1109/TCSVT.2017.2734838
   Kondo H., 2000, P INT C COMM TECHN, P21
   Laska JN, 2012, IEEE T SIGNAL PROCES, V60, P3496, DOI 10.1109/TSP.2012.2194710
   Laska JN, 2011, APPL COMPUT HARMON A, V31, P429, DOI 10.1016/j.acha.2011.02.002
   Li MH, 2018, PROC CVPR IEEE, P6644, DOI 10.1109/CVPR.2018.00695
   Lin WS, 2006, IEEE T IMAGE PROCESS, V15, P2513, DOI 10.1109/TIP.2006.877415
   Liou CY, 2008, NEUROCOMPUTING, V71, P3150, DOI 10.1016/j.neucom.2008.04.030
   Liou CY, 2014, NEUROCOMPUTING, V139, P84, DOI 10.1016/j.neucom.2013.09.055
   Liu D, 2007, IEEE T CIRC SYST VID, V17, P1273, DOI 10.1109/TCSVT.2007.903663
   Liu XM, 2016, IEEE T IMAGE PROCESS, V25, P1649, DOI 10.1109/TIP.2016.2526910
   Marpe D, 2003, IEEE T CIRC SYST VID, V13, P620, DOI 10.1109/TCSVT.2003.815173
   Mentzer F, 2018, PROC CVPR IEEE, P4394, DOI 10.1109/CVPR.2018.00462
   Minnen D, 2018, ADV NEUR IN, V31
   Ollivier Y., 2014, J COMPUT SCI, V1, P1
   Saier MH, 2007, WATER AIR SOIL POLL, V181, P1, DOI 10.1007/s11270-007-9372-6
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Theis L., 2017, ICLR
   Theis L, 2015, ADV NEUR IN, V28
   Toderici G., 2016, ADV P INT C LEARN RE, P1
   Toderici G, 2017, PROC CVPR IEEE, P5435, DOI 10.1109/CVPR.2017.577
   van den Oord A, 2016, PR MACH LEARN RES, V48
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang LJ, 2012, IEEE T IMAGE PROCESS, V21, P2980, DOI 10.1109/TIP.2012.2188810
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Weng RL, 2016, IEEE T MULTIMEDIA, V18, P2066, DOI 10.1109/TMM.2016.2591508
   Wu XL, 2009, IEEE T IMAGE PROCESS, V18, P552, DOI 10.1109/TIP.2008.2010638
   Zeng B., 1993, ICASSP-93. 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing (Cat. No.92CH3252-4), P393, DOI 10.1109/ICASSP.1993.319830
   Zhai GT, 2008, IEEE T CIRC SYST VID, V18, P122, DOI 10.1109/TCSVT.2007.906942
   Zhang XF, 2013, IEEE T IMAGE PROCESS, V22, P4613, DOI 10.1109/TIP.2013.2274386
NR 44
TC 4
Z9 4
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2020
VL 22
IS 8
BP 1917
EP 1928
DI 10.1109/TMM.2019.2949393
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MR2MS
UT WOS:000553424500001
DA 2024-07-18
ER

PT J
AU Zhang, SL
   Zhang, L
   Hauptmann, AG
AF Zhang, Shunli
   Zhang, Li
   Hauptmann, Alexander G.
TI Fuzzy Least Squares Support Vector Machine With Adaptive Membership for
   Object Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target tracking; Adaptation models; Correlation; Support vector
   machines; Deep learning; Feature extraction; Object tracking; fuzzy
   learning; adaptive membership; fuzzy least squares support vector
   machine
ID VISUAL TRACKING; OCCLUSION
AB Fuzzy learning has been introduced into tracking and achieved great success. However, the membership in the existing fuzzy learning based tracking algorithm is fixed, which lacks the adaptivity to measure the importance of the samples. To improve the tracking adaptivity and flexibility, in this paper, we propose a novel tracking method based on fuzzy least squares support vector machine with adaptive membership (FLS-SVM-AM). First, we formulate tracking as an adaptive membership based fuzzy learning problem, which addresses the issue of fixed membership in existing methods and can better measure the importance of the training samples. Second, we present the FLS-SVM-AM method to build the appearance model, and develop an iterative optimization process to solve the FLS-SVM-AM problem. Third, we define a new membership based on the PASCAL VOC overlap rate and exponential function, which is used to measure the importance of different samples more accurately. Experimental results in the benchmark datasets demonstrate that the proposed method not only outperforms the existing fuzzy learning based tracking methods, but also is comparable to many state-of-the-art methods.
C1 [Zhang, Shunli] Beijing Jiaotong Univ, Sch Software Engn, Beijing 100044, Peoples R China.
   [Zhang, Li] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
   [Hauptmann, Alexander G.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
C3 Beijing Jiaotong University; Tsinghua University; Carnegie Mellon
   University
RP Zhang, SL (corresponding author), Beijing Jiaotong Univ, Sch Software Engn, Beijing 100044, Peoples R China.
EM slzhang@bjtu.edu.cn; chinazhangli@tsinghua.edu.cn; alex@cs.cmu.edu
FU National Natural Science Foundation of China [61976017, 61601021];
   Beijing Natural Science Foundation [L172022]; Fundamental Research Funds
   for the Central Universities [2016RC015]; U.S. Department of Commerce,
   National Institute of Standards and Technology [60NANB17D156];
   Intelligence Advanced Research Projects Activity via Department of
   Interior/Interior Business Center [D17PC00340]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61976017 and Grant 61601021, in part by
   the Beijing Natural Science Foundation under Grant L172022, in part by
   the Fundamental Research Funds for the Central Universities under Grant
   2016RC015, in part by the Financial Assistance Award 60NANB17D156 from
   U.S. Department of Commerce, National Institute of Standards and
   Technology, and in part by the Intelligence Advanced Research Projects
   Activity via Department of Interior/Interior Business Center contract
   D17PC00340. The associate editor coordinating the review of this
   manuscript and approving it for publicationwas Prof. Zhu Li.
CR [Anonymous], 2006, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2006.256
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.465
   [Anonymous], 2014, LECT NOTES COMPUT SC, DOI DOI 10.1007/978-3-319-10599-4_13
   [Anonymous], 2012, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2012.6247881
   [Anonymous], 2008, An Open and Portable Library of Computer Vision Algorithms
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M., 2017, ECO EFFICIENT CONVOL, P6638
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Dinh TB, 2011, PROC CVPR IEEE, P1177, DOI 10.1109/CVPR.2011.5995733
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Everingham M., 2005, The PASCAL Visual Object Classes Challenge Results (VOC 2005)
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Grabner H., 2006, BMVC, P47
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Havens TC, 2012, IEEE T FUZZY SYST, V20, P1130, DOI 10.1109/TFUZZ.2012.2201485
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hongyi Su, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9225, P1, DOI 10.1007/978-3-319-22180-9_1
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Klinkhammer BM, 2017, ARXIV170800251
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Lan L, 2018, IEEE T IMAGE PROCESS, V27, P4585, DOI 10.1109/TIP.2018.2843129
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li HX, 2011, PROC CVPR IEEE, P1305, DOI 10.1109/CVPR.2011.5995483
   Li Y, 2019, AAAI CONF ARTIF INTE, P8666
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Liu C, 2018, IEEE T MULTIMEDIA, V20, P889, DOI 10.1109/TMM.2017.2760633
   Maksai A, 2017, IEEE I CONF COMP VIS, P2563, DOI 10.1109/ICCV.2017.278
   Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292
   Melin P, 2014, IEEE T FUZZY SYST, V22, P1515, DOI 10.1109/TFUZZ.2013.2297159
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Sui Y, 2018, INT J COMPUT VISION, V126, P515, DOI 10.1007/s11263-017-1049-z
   Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357
   Wang XC, 2017, IEEE T IMAGE PROCESS, V26, P4765, DOI 10.1109/TIP.2017.2723239
   Wang XC, 2016, IEEE T PATTERN ANAL, V38, P2312, DOI 10.1109/TPAMI.2015.2513406
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xie Q, 2018, IEEE T MULTIMEDIA, V20, P580, DOI 10.1109/TMM.2017.2751965
   Yang YH, 2017, IEEE T CIRC SYST VID, V27, P1031, DOI 10.1109/TCSVT.2015.2513699
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang SL, 2020, IEEE T CYBERNETICS, V50, P270, DOI 10.1109/TCYB.2018.2868782
   Zhang SL, 2015, IEEE T IMAGE PROCESS, V24, P5723, DOI 10.1109/TIP.2015.2484068
   Zhang SL, 2015, IEEE T MULTIMEDIA, V17, P265, DOI 10.1109/TMM.2015.2390044
   Zhao B, 2017, INT J AUTOM COMPUT, V14, P119, DOI 10.1007/s11633-017-1053-3
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
   Zhou Y, 2017, IEEE T MULTIMEDIA, V19, P1798, DOI 10.1109/TMM.2017.2689918
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
NR 58
TC 9
Z9 9
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2020
VL 22
IS 8
BP 1998
EP 2011
DI 10.1109/TMM.2019.2952252
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MR2MS
UT WOS:000553424500007
OA hybrid
DA 2024-07-18
ER

PT J
AU Cui, XR
   Wang, D
   Wang, ZJ
AF Cui, Xinrui
   Wang, Dan
   Wang, Z. Jane
TI Feature-Flow Interpretation of Deep Convolutional Neural Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Computational modeling; Perturbation methods;
   Convolutional neural networks; Medical services; Birds; Model
   interpretability; feature-flow; sparse representation
AB Despite the great success of deep convolutional neural networks (DCNNs) in computer vision tasks, their black-box aspect remains a critical concern. The interpretability of DCNN models has been attracting increasing attention. In this work, we propose a novel model, Feature-fLOW INterpretation (FLOWIN) model, to interpret a DCNN by its feature-flow. The FLOWIN can express deep-layer features as a sparse representation of shallow-layer features. Based on that, it distills the optimal feature-flow for the prediction of a given instance, starting from deep layers to shallow layers. Therefore, the FLOWIN can provide an instance-specific interpretation, which presents its feature-flow units and their interpretable meanings for its network decision. The FLOWIN can also give the quantitative interpretation in which the contribution of each flow unit in different layers is used to interpret the net decision. From the class-level view, we can further understand networks by studying feature-flows within and between classes. The FLOWIN not only provides the visualization of the feature-flow but also studies feature-flow quantitatively by investigating its density and similarity metrics. In our experiments, the FLOWIN is evaluated on different datasets and networks by quantitative and qualitative ways to show its interpretability.
C1 [Cui, Xinrui; Wang, Dan; Wang, Z. Jane] Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
C3 University of British Columbia
RP Wang, D (corresponding author), Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
EM xinruic@ece.ubc.ca; danw@ece.ubc.ca; zjanew@ece.ubc.ca
RI Wang, Dan/HTR-9011-2023; Cui, Xinrui/KPB-0947-2024
OI Wang, Dan/0000-0001-6374-0418; Cui, Xinrui/0000-0001-9641-7801
FU Canadian Natural Sciences and Engineering Research Council (NSERC);
   International Doctoral Fellowship at the University of British Columbia
FX This work was supported in part by the Canadian Natural Sciences and
   Engineering Research Council (NSERC), in part by the Four Year Doctoral
   Fellowship, and in part by the International Doctoral Fellowship at the
   University of British Columbia.
CR Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254
   Cui XR, 2020, IEEE T NEUR NET LEAR, V31, P4143, DOI 10.1109/TNNLS.2019.2952322
   Cui XR, 2019, IEEE T MULTIMEDIA, V21, P2263, DOI 10.1109/TMM.2019.2902099
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371
   Freeman William, 2015, ARXIV150702379
   Gonzalez-Garcia A, 2018, INT J COMPUT VISION, V126, P476, DOI 10.1007/s11263-017-1048-0
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Kim B, 2018, PR MACH LEARN RES, V80
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Springenberg J. T., 2015, ARXIV PREPRINT ARXIV
   Wang D., 2020, ARXIV200201660
   Xu QY, 2019, IEEE IND ELEC, P6261, DOI 10.1109/IECON.2019.8927606
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang QS, 2018, PROC CVPR IEEE, P8827, DOI 10.1109/CVPR.2018.00920
   Zhou B., 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.319
   Zhou BL, 2018, LECT NOTES COMPUT SC, V11212, P122, DOI 10.1007/978-3-030-01237-3_8
NR 23
TC 7
Z9 7
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2020
VL 22
IS 7
BP 1847
EP 1861
DI 10.1109/TMM.2020.2976985
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MG4GJ
UT WOS:000545990500016
OA hybrid
DA 2024-07-18
ER

PT J
AU Zhang, XX
   Gui, SP
   Zhu, ZF
   Zhao, Y
   Liu, J
AF Zhang, Xingxing
   Gui, Shupeng
   Zhu, Zhenfeng
   Zhao, Yao
   Liu, Ji
TI Hierarchical Prototype Learning for Zero-Shot Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Prototypes; Visualization; Semantics; Training; Data models; Adaptation
   models; Object recognition; Zero-shot learning; prototype; transductive
   learning; unseen class
AB Zero-Shot Learning (ZSL) has received extensive attention and successes in recent years especially in areas of fine-grained object recognition, retrieval, and image captioning. Key to ZSL is to transfer knowledge from the seen to the unseen classes via auxiliary semantic prototypes (e.g., word or attribute vectors). However, the popularly learned projection functions in previous works cannot generalize well due to non-visual components included in semantic prototypes. Besides, the incompleteness of provided prototypes and captured images has less been considered by the state-of-the-art approaches in ZSL. In this paper, we propose a hierarchical prototype learning formulation to provide a systematical solution (named HPL) for zero-shot recognition. Specifically, HPL is able to obtain discriminability on both seen and unseen class domains by learning visual prototypes respectively under the transductive setting. To narrow the gap of two domains, we further learn the interpretable super-prototypes in both visual and semantic spaces. Meanwhile, the two spaces are further bridged by maximizing their structural consistency. This not only facilitates the representativeness of visual prototypes, but also alleviates the loss of information of semantic prototypes. An extensive group of experiments are then carefully designed and presented, demonstrating that HPL obtains remarkably more favorable efficiency and effectiveness, over currently available alternatives under various settings.
C1 [Zhang, Xingxing; Zhu, Zhenfeng; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Zhang, Xingxing; Zhu, Zhenfeng; Zhao, Yao] Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
   [Gui, Shupeng] Univ Rochester, Dept Comp Sci, Rochester, NY 14611 USA.
   [Liu, Ji] Kwai Inc, Ytech Seattle AI Lab, Seattle, WA 98101 USA.
C3 Beijing Jiaotong University; Beijing Jiaotong University; University of
   Rochester
RP Zhao, Y (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM zhangxing@bjtu.edu.cn; shupenggui@gmail.com; zhfzhu@bjtu.edu.cn;
   yzhao@bjtu.edu.cn; ji.liu.uwisc@gmail.com
RI Zhang, Xingxing/HGE-4445-2022; Gui, Shupeng/ABG-2776-2020
OI Zhang, Xingxing/0000-0002-9838-6962; Zhao, Yao/0000-0002-8581-9554
FU National Key Research and Development of China [2018AAA0102101];
   National Natural Science Foundation of China [61532005, 61976018,
   61572068, andU1936212]
FX This work was supported in part by the National Key Research and
   Development of China underGrant 2018AAA0102101, and in part by the
   National Natural Science Foundation of China under Grants 61532005,
   61976018, 61572068, andU1936212.
CR Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986
   Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Annadani Y, 2018, PROC CVPR IEEE, P7603, DOI 10.1109/CVPR.2018.00793
   [Anonymous], 2016, PROCEEDINGS OF THE I, DOI DOI 10.1109/CVPR.2016.649
   [Anonymous], ADV NEURAL INFORM PR
   BARTELS RH, 1972, COMMUN ACM, V15, P820, DOI 10.1145/361573.361582
   Chakraborty J, 2018, IEEE INT C SEMANT CO, P397, DOI 10.1109/ICSC.2018.00079
   Changpinyo S, 2017, IEEE I CONF COMP VIS, P3496, DOI 10.1109/ICCV.2017.376
   Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575
   Duda RO., 2012, Pattern classificatio
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354
   Fu ZY, 2018, IEEE T PATTERN ANAL, V40, P2009, DOI 10.1109/TPAMI.2017.2737007
   Guo YC, 2017, IEEE T IMAGE PROCESS, V26, P3277, DOI 10.1109/TIP.2017.2696747
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Kankuekul P, 2012, PROC CVPR IEEE, P3657, DOI 10.1109/CVPR.2012.6248112
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Kodirov E, 2015, IEEE I CONF COMP VIS, P2452, DOI 10.1109/ICCV.2015.282
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li Y, 2018, PROC CVPR IEEE, P7463, DOI 10.1109/CVPR.2018.00779
   Mensink T, 2014, PROC CVPR IEEE, P2441, DOI 10.1109/CVPR.2014.313
   Mikolov Tomas, 2013, Advances in Neural Information Processing Systems, P3111, DOI DOI 10.48550/ARXIV.1310.4546
   MORE JJ, 1994, ACM T MATH SOFTWARE, V20, P286, DOI 10.1145/192115.192132
   Morgado P, 2017, PROC CVPR IEEE, P2037, DOI 10.1109/CVPR.2017.220
   Niu L, 2018, PROC CVPR IEEE, P7171, DOI 10.1109/CVPR.2018.00749
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Radovanovic M, 2010, J MACH LEARN RES, V11, P2487
   Rahman S, 2018, IEEE T IMAGE PROCESS, V27, P5652, DOI 10.1109/TIP.2018.2861573
   Rohrbach M., 2013, Advances in neural information processing systems, P46
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saha, 2014, ADV NEURAL INFORM PR, P1853
   Schönfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844
   Socher R., 2013, P 2013 C EMP METH NA, V2013, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791
   Song J, 2018, PROC CVPR IEEE, P1024, DOI 10.1109/CVPR.2018.00113
   Sun YH, 2018, EURASIP J WIREL COMM, DOI 10.1186/s13638-018-1107-4
   Tong B, 2018, AAAI CONF ARTIF INTE, P2476
   Tsai YHH, 2017, IEEE I CONF COMP VIS, P3591, DOI 10.1109/ICCV.2017.386
   Verma VK, 2018, PROC CVPR IEEE, P4281, DOI 10.1109/CVPR.2018.00450
   Verma VK, 2017, LECT NOTES ARTIF INT, V10535, P792, DOI 10.1007/978-3-319-71246-8_48
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang Q, 2017, INT J COMPUT VISION, V124, P356, DOI 10.1007/s11263-017-1027-5
   Wang W, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3293318
   Wang XL, 2018, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR.2018.00717
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Ye M, 2017, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR.2017.542
   Yu Y., 2018, P ADV NEUR INF PROC, P5998
   Yu YL, 2018, IEEE T CYBERNETICS, V48, P2908, DOI 10.1109/TCYB.2017.2751741
   Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321
   Zhang XX, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3141
   Zhang XX, 2019, IEEE T NEUR NET LEAR, V30, P1954, DOI 10.1109/TNNLS.2018.2875347
   Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474
   ZHANG ZN, 2018, IOP CONF SER MAT SCI, V322, DOI DOI 10.1088/1757-899X/322/3/032006
   Zhao, 2018, INT C MACH LEARN, P5907
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zhu YZ, 2018, PROC CVPR IEEE, P1004, DOI 10.1109/CVPR.2018.00111
NR 60
TC 18
Z9 19
U1 1
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2020
VL 22
IS 7
BP 1692
EP 1703
DI 10.1109/TMM.2019.2959433
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MG4GJ
UT WOS:000545990500004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, YX
   Liu, ML
   Wang, WC
   Zhang, YH
   He, QH
AF Li, Yanxiong
   Liu, Mingle
   Wang, Wucheng
   Zhang, Yuhan
   He, Qianhua
TI Acoustic Scene Clustering Using Joint Optimization of Deep Embedding
   Learning and Clustering Iteration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Acoustics; Audio recording; Feature extraction; Clustering algorithms;
   Image analysis; Classification algorithms; Optimization; Acoustic scene
   clustering; deep embedding; agglomerative hierarchical clustering; audio
   content analysis
ID CONVOLUTIONAL NEURAL-NETWORKS; REPRESENTATION; CLASSIFICATION
AB Recent efforts have been made on acoustic scene classification in the audio signal processing community. In contrast, few studies have been conducted on acoustic scene clustering, which is a newly emerging problem. Acoustic scene clustering aims at merging the audio recordings of the same class of acoustic scene into a single cluster without using prior information and training classifiers. In this study, we propose a method for acoustic scene clustering that jointly optimizes the procedures of feature learning and clustering iteration. In the proposed method, the learned feature is a deep embedding that is extracted from a deep convolutional neural network (CNN), while the clustering algorithm is the agglomerative hierarchical clustering (AHC). We formulate a unified loss function for integrating and optimizing these two procedures. Various features and methods are compared. The experimental results demonstrate that the proposed method outperforms other unsupervised methods in terms of the normalized mutual information and the clustering accuracy. In addition, the deep embedding outperforms many state-of-the-art features.
C1 [Li, Yanxiong; Liu, Mingle; Wang, Wucheng; Zhang, Yuhan; He, Qianhua] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Guangdong, Peoples R China.
C3 South China University of Technology
RP Li, YX (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Guangdong, Peoples R China.
EM eeyxli@scut.edu.cn; 1324903969@qq.com; 347710323@qq.com;
   1285918314@qq.com; eeqhhe@scut.edu.cn
RI zhang, yuhan/HLH-1222-2023
OI zhang, yuhan/0009-0001-3739-7238
FU National Natural Science Foundation of China [61771200, 61571192,
   6191101570, 6191101514, 6191101306]; Project of International Science
   and Technology Cooperation of Guangdong province, China
   [2019A050509001]; Open Project Program of the National Laboratory of
   Pattern Recognition [201800004]; Fundamental Research Funds for the
   Central Universities, South China University of Technology (Research on
   Key Techniques for Analyzing Complex Audio Scene Contents, 2019)
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61771200, 61571192, 6191101570,
   6191101514, and 6191101306, in part by the Project of International
   Science and Technology Cooperation of Guangdong province, China under
   Grant 2019A050509001, in part by the Open Project Program of the
   National Laboratory of Pattern Recognition 201800004, and in part by the
   Fundamental Research Funds for the Central Universities, South China
   University of Technology (Research on Key Techniques for Analyzing
   Complex Audio Scene Contents, 2019). The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Chi-Chun Lee.
CR Abidin S, 2018, IEEE-ACM T AUDIO SPE, V26, P2112, DOI 10.1109/TASLP.2018.2854861
   Amiriparian S, 2018, EUR SIGNAL PR CONF, P977, DOI 10.23919/EUSIPCO.2018.8553225
   Miro XA, 2012, IEEE T AUDIO SPEECH, V20, P356, DOI 10.1109/TASL.2011.2125954
   [Anonymous], P DCASE CHALL
   [Anonymous], 1982, COMBINATORIAL OPTIMI
   [Anonymous], 2016, PROF DCASE
   [Anonymous], 2017, P DET CLASS AC SCEN
   Barchiesi D, 2015, IEEE SIGNAL PROC MAG, V32, P16, DOI 10.1109/MSP.2014.2326181
   Bisot V, 2017, IEEE-ACM T AUDIO SPE, V25, P1216, DOI 10.1109/TASLP.2017.2690570
   Chen WY, 2011, IEEE T PATTERN ANAL, V33, P568, DOI 10.1109/TPAMI.2010.88
   CHILDERS DG, 1977, P IEEE, V65, P1428, DOI 10.1109/PROC.1977.10747
   Crocco M, 2016, ACM COMPUT SURV, V48, DOI 10.1145/2871183
   Jakob A., 2017, P DCASE, P7
   Jiménez A, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P146, DOI 10.1109/ICASSP.2018.8461631
   Lee J, 2017, IEEE SIGNAL PROC LET, V24, P1208, DOI 10.1109/LSP.2017.2713830
   Li SY, 2018, EUR SIGNAL PR CONF, P2489, DOI 10.23919/EUSIPCO.2018.8553314
   Li YX, 2018, IEEE ACCESS, V6, P58043, DOI 10.1109/ACCESS.2018.2872931
   Li YX, 2018, 2018 INTERNATIONAL CONFERENCE ON AUDIO, LANGUAGE AND IMAGE PROCESSING (ICALIP), P371, DOI 10.1109/ICALIP.2018.8455765
   Li YX, 2018, MULTIMED TOOLS APPL, V77, P897, DOI 10.1007/s11042-016-4332-z
   Li YX, 2018, IEEE T INF FOREN SEC, V13, P965, DOI 10.1109/TIFS.2017.2774505
   Li YX, 2017, DIGIT SIGNAL PROCESS, V63, P123, DOI 10.1016/j.dsp.2016.12.012
   Lu L, 2008, IEEE T MULTIMEDIA, V10, P74, DOI 10.1109/TMM.2007.911304
   MacQueen J., 1967, P 5 BERK S MATH STAT, P281
   Mesaros A, 2018, IEEE-ACM T AUDIO SPE, V26, P379, DOI 10.1109/TASLP.2017.2778423
   Park S., 2017, P DET CLASS AC SCEN, P98
   Rakotomamonjy A, 2017, IEEE-ACM T AUDIO SPE, V25, P1253, DOI 10.1109/TASLP.2017.2690561
   Rakotomamonjy A, 2015, IEEE-ACM T AUDIO SPE, V23, P142, DOI 10.1109/TASLP.2014.2375575
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Salamon J, 2017, IEEE SIGNAL PROC LET, V24, P279, DOI 10.1109/LSP.2017.2657381
   Schröder J, 2017, IEEE-ACM T AUDIO SPE, V25, P1304, DOI 10.1109/TASLP.2017.2690569
   Singh A, 2018, EUR SIGNAL PR CONF, P837, DOI 10.23919/EUSIPCO.2018.8553052
   Stowell D, 2015, IEEE T MULTIMEDIA, V17, P1733, DOI 10.1109/TMM.2015.2428998
   Valenti M., 2016, P DET CLASS AC SCEN, P1
   Vargas A, 2018, PROCEEDINGS OF THE ASME INTERNATIONAL MECHANICAL ENGINEERING CONGRESS AND EXPOSITION, 2017, VOL 2
   Yang JW, 2016, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR.2016.556
   Yang WJ, 2017, IEEE-ACM T AUDIO SPE, V25, P1315, DOI 10.1109/TASLP.2017.2690558
   Yu D, 2015, SIGNALS COMMUN TECHN, P1, DOI 10.1007/978-1-4471-5779-3
   Zhang W, 2012, LECT NOTES COMPUT SC, V7572, P428, DOI 10.1007/978-3-642-33718-5_31
   Zhao X, 2018, PROCEEDINGS OF 2018 IEEE 4TH INFORMATION TECHNOLOGY AND MECHATRONICS ENGINEERING CONFERENCE (ITOEC 2018), P39, DOI 10.1109/ITOEC.2018.8740421
   Zhu YY, 2017, INTERNATIONAL SYMPOSIUM 2017 - RESEARCH AND PRACTICE OF HIGHER EDUCATION THEORY, P13
NR 40
TC 18
Z9 19
U1 1
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2020
VL 22
IS 6
BP 1385
EP 1394
DI 10.1109/TMM.2019.2947199
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU8YA
UT WOS:000538033100001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Strutz, T
   Möller, P
AF Strutz, Tilo
   Moeller, Phillip
TI Screen Content Compression Based on Enhanced Soft Context Formation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image color analysis; Image coding; Videos; Histograms; Probability
   distribution; Tools; Encoding; Probability distribution modelling;
   lossless coding; colour image compression; screen content coding; noise
   estimation
ID QUALITY ASSESSMENT; PREDICTION
AB The compression of screen content has attracted the interest of researchers in the last years as the market for transferring data from computer displays is growing. It has already been shown that especially those methods can effectively compress screen content which are able to predict the probability distribution of next pixel values. This prediction is typically based on a kind of learning process. The predictor learns the relationship between probable pixel colours and surrounding texture. Recently, an effective method called 'soft context formation' (SCF) had been proposed which achieves much lower bitrates for images with less than 8 000 colours than other state-of-the-art compression schemes. This paper presents an enhanced version of SCF. The average lossless compression performance has increased by about 5% in application to images with less than 8 000 colours and about 10% for images with up to 90 000 colours. In comparison to FLIF, FP8v3, and HEVC (HM - 16.20 + SCM - 8.8), it achieves savings of about 33%, 4%, and 11% on average. The improvements compared to the original version result from various modifications. The largest contribution is achieved by the local estimation of the probability distribution for unpredictable colours in stage II of the compression scheme.
C1 [Strutz, Tilo] Deutsch Telekom AG, Bonn, Germany.
   [Strutz, Tilo] Leipzig Univ Telecommun HfTL, Inst Commun Engn, D-04277 Leipzig, Germany.
   [Moeller, Phillip] Deutsch Telekom IT GmbH, D-13509 Berlin, Germany.
C3 Deutsche Telekom AG
RP Strutz, T (corresponding author), Deutsch Telekom AG, Bonn, Germany.
EM strutz@hftl.de; Phillip.Moeller@t-systems.com
RI Strutz, Tilo/ABE-4880-2020
OI Strutz, Tilo/0000-0001-5063-6515; Moeller, Phillip/0000-0002-4808-2610
CR [Anonymous], 2003, 15948 ISOIEC
   [Anonymous], 1992, 109181 ISOIEC
   [Anonymous], 2013, 144952 ISOIEC
   Bedi S, 2004, IMAGE VISION COMPUT, V22, P9, DOI 10.1016/S0262-8856(03)00139-2
   de Queiroz RL, 2000, IEEE T IMAGE PROCESS, V9, P1461, DOI 10.1109/83.862619
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   International Organization for Standardization, 2013, 2300822013 ISOIEC TR
   Kang JW, 2016, IEEE T MULTIMEDIA, V18, P2054, DOI 10.1109/TMM.2016.2595259
   Lin T, 2005, IEEE T IMAGE PROCESS, V14, P993, DOI 10.1109/TIP.2005.849776
   Ma Z, 2017, IEEE T MULTIMEDIA, V19, P2322, DOI 10.1109/TMM.2017.2737944
   Mahony M., DATA COMPRESSION EXP
   Mitrica I, 2019, IEEE T MULTIMEDIA, V21, P2157, DOI 10.1109/TMM.2019.2900168
   Moffat A., 1995, Proceedings. DCC '95 Data Compression Conference (Cat. No.95TH8037), P202, DOI 10.1109/DCC.1995.515510
   Moller P., 2019, 27 EUR SIGN PROC C E
   Ondrus J., FAST PAQ
   Pan ZT, 2013, IEEE T CIRC SYST VID, V23, P949, DOI 10.1109/TCSVT.2013.2243056
   Park SG, 2004, IEEE IMAGE PROC, P2251
   Peng WH, 2016, IEEE J EM SEL TOP C, V6, P393, DOI 10.1109/JETCAS.2016.2608971
   Peng XL, 2016, IEEE T IMAGE PROCESS, V25, P5601, DOI 10.1109/TIP.2016.2612884
   Pu W, 2016, IEEE J EM SEL TOP C, V6, P420, DOI 10.1109/JETCAS.2016.2605661
   Sanchez V, 2016, IEEE J EM SEL TOP C, V6, P497, DOI 10.1109/JETCAS.2016.2606513
   Sneyers J, 2016, IEEE IMAGE PROC, P66, DOI 10.1109/ICIP.2016.7532320
   Strutz T., ENHANCED SOFT CONTEX
   Strutz T, 2016, IEEE J EM SEL TOP C, V6, P508, DOI 10.1109/JETCAS.2016.2606550
   Strutz T, 2013, IEEE T CIRC SYST VID, V23, P1249, DOI 10.1109/TCSVT.2013.2242612
   Walls F., 2014, SID S, V45, P360, DOI [10.1002/j.2168-0159.2014.tb00097.x, DOI 10.1002/J.2168-0159.2014.TB00097.X]
   Wang SQ, 2017, IEEE T MULTIMEDIA, V19, P660, DOI 10.1109/TMM.2016.2625276
   Xu JZ, 2016, IEEE T CIRC SYST VID, V26, P50, DOI 10.1109/TCSVT.2015.2478706
   Yang H, 2015, IEEE T IMAGE PROCESS, V24, P4408, DOI 10.1109/TIP.2015.2465145
   Yang H, 2012, IEEE INT WORKSH MULT, P77, DOI 10.1109/MMSP.2012.6343419
   Yu H., 2015, JCTVCT1015R1
   Zhao LP, 2018, IEEE T MULTIMEDIA, V20, P796, DOI 10.1109/TMM.2017.2758519
   Zhu WJ, 2017, IEEE T BROADCAST, V63, P673, DOI 10.1109/TBC.2017.2711144
   Zhu WJ, 2015, IEEE T MULTIMEDIA, V17, P935, DOI 10.1109/TMM.2015.2428171
   Zhu WJ, 2014, IEEE T MULTIMEDIA, V16, P1316, DOI 10.1109/TMM.2014.2315782
NR 35
TC 6
Z9 7
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2020
VL 22
IS 5
BP 1126
EP 1138
DI 10.1109/TMM.2019.2941270
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LJ3VZ
UT WOS:000530097200002
DA 2024-07-18
ER

PT J
AU Ye, MX
   Yang, C
   Stankovic, V
   Stankovic, L
   Cheng, S
AF Ye, Minxiang
   Yang, Cheng
   Stankovic, Vladimir
   Stankovic, Lina
   Cheng, Samuel
TI Distinct Feature Extraction for Video-Based Gait Phase Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Legged locomotion; Kinematics; Tracking; Trajectory;
   Training; Foot; Feature extraction; gait phase classification
ID MICROSOFT KINECT; PATTERN-CLASSIFICATION; VALIDITY; TRACKING;
   RELIABILITY
AB Recent advances in image acquisition and analysis have resulted in disruptive innovation in physical rehabilitation systems facilitating cost-effective, portable, video-based gait assessment. While these inexpensive motion capture systems, suitable for home rehabilitation, do not generally provide accurate kinematics measurements on their own, image processing algorithms ensure gait analysis that is accurate enough for rehabilitation programs. This paper proposes high-accuracy classification of gait phases and muscle actions, using readings from low-cost motion capture systems. First, 12 gait parameters, drawn from the medical literature, are defined to characterize gait patterns. These proposed parameters are then used as input to our proposed multi-channel time-series classification and gait phase reconstruction methods. Proposed methods fully utilize temporal information of gait parameters, thus improving the final classification accuracy. The validation, conducted using 126 experiments, with 6 healthy volunteers and 9 stroke survivors with manually-labelled gait phases, achieves state-of-art classification accuracy of gait phase with lower computational complexity compared to previous solutions.(1)
C1 [Ye, Minxiang; Stankovic, Vladimir; Stankovic, Lina] Univ Strathclyde, Dept Elect & Elect Engn, Glasgow G1 1XW, Lanark, Scotland.
   [Yang, Cheng] York Univ, Dept Elect Engn & Comp Sci, Toronto, ON M3J 1P3, Canada.
   [Cheng, Samuel] Tongji Univ, Dept Elect & Informat Engn, Shanghai 201804, Peoples R China.
   [Cheng, Samuel] Univ Oklahoma, Sch Elect & Comp Engn, Norman, OK 73019 USA.
C3 University of Strathclyde; York University - Canada; Tongji University;
   University of Oklahoma System; University of Oklahoma - Norman
RP Ye, MX (corresponding author), Univ Strathclyde, Dept Elect & Elect Engn, Glasgow G1 1XW, Lanark, Scotland.
EM minxiang.ye@strath.ac.uk; cyang@cse.yorku.ca;
   vladimir.stankovic@strath.ac.uk; lina.stankovic@strath.ac.uk;
   samuel.cheng@ou.edu
RI Stankovic, Vladimir/L-6584-2016
OI Stankovic, Vladimir/0000-0002-1075-2420; Stankovic,
   Lina/0000-0002-8112-1976; Ye, Minxiang/0000-0003-0083-7145; Cheng,
   Samuel/0000-0002-5439-1137
FU European Unions Horizon 2020 research and innovation programme under the
   Marie Skodowska-Curie [734331]
FX This work was supported by the European Unions Horizon 2020 research and
   innovation programme under the Marie Skodowska-Curie under Grant
   agreement 734331. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Xiaokang Yang.
CR [Anonymous], [No title captured]
   [Anonymous], 2014, PROC INT C WEB OPEN
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   Bharathidason S., 2014, INT J COMPUTER APPL, P26, DOI DOI 10.5120/17749-8829
   Breiman L., 2017, CLASSIFICATION REGRE, DOI [10.1201/9781315139470, DOI 10.1201/9781315139470]
   Clark RA, 2012, GAIT POSTURE, V36, P372, DOI 10.1016/j.gaitpost.2012.03.033
   Farah JD, 2017, IEEE INT SYM MED MEA, P263, DOI 10.1109/MeMeA.2017.7985886
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Gholami F, 2017, IEEE J BIOMED HEALTH, V21, P1376, DOI 10.1109/JBHI.2016.2593692
   Gianaria E, 2016, IEEE IMAGE PROC, P1314, DOI 10.1109/ICIP.2016.7532571
   Glocker B, 2013, INT SYM MIX AUGMENT, P173, DOI 10.1109/ISMAR.2013.6671777
   Hsu C. W., 2010, Technical Report
   Jung JY, 2015, SENSORS-BASEL, V15, P27738, DOI 10.3390/s151127738
   Leu A., 2011, 2011 6th IEEE International Symposium on Applied Computational Intelligence and Informatics (SACI), P415, DOI 10.1109/SACI.2011.5873039
   Liu Z, 2018, IEEE T MULTIMEDIA, V20, P1321, DOI 10.1109/TMM.2017.2767781
   Liu ZY, 2006, IEEE T PATTERN ANAL, V28, P863, DOI 10.1109/TPAMI.2006.122
   Lo Presti L, 2016, PATTERN RECOGN, V53, P130, DOI 10.1016/j.patcog.2015.11.019
   Misgeld BJE, 2016, IEEE J BIOMED HEALTH, V20, P748, DOI 10.1109/JBHI.2015.2477245
   Mulroy S, 2003, GAIT POSTURE, V18, P114, DOI 10.1016/S0966-6362(02)00165-0
   Paolini G, 2014, IEEE T NEUR SYS REH, V22, P997, DOI 10.1109/TNSRE.2013.2282868
   Peters DM, 2013, J GERIATR PHYS THER, V36, P24, DOI 10.1519/JPT.0b013e318248e20d
   Petitjean F, 2011, PATTERN RECOGN, V44, P678, DOI 10.1016/j.patcog.2010.09.013
   PODSIADLO D, 1991, J AM GERIATR SOC, V39, P142, DOI 10.1111/j.1532-5415.1991.tb01616.x
   Ly QT, 2016, IEEE ENG MED BIO, P1599, DOI 10.1109/EMBC.2016.7591018
   Taborri J, 2014, SENSORS-BASEL, V14, P16212, DOI 10.3390/s140916212
   Vemulapalli R, 2016, PROC CVPR IEEE, P4471, DOI 10.1109/CVPR.2016.484
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang JS, 2012, IEEE T BIO-MED ENG, V59, P2884, DOI 10.1109/TBME.2012.2212245
   Wang QF, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P562, DOI 10.1109/3DV.2015.69
   Xuan Y. D., 2013, 2013 IEEE International Conference on Green Computing and Communications (GreenCom) and IEEE Internet of Things (iThings) and IEEE Cyber, Physical and Social Computing (CPSCom), P1729, DOI 10.1109/GreenCom-iThings-CPSCom.2013.318
   Yang C, 2019, IET SCI MEAS TECHNOL, V13, P563, DOI 10.1049/iet-smt.2018.5246
   Yang C, 2017, IEEE T MULTIMEDIA, V19, P1625, DOI 10.1109/TMM.2017.2672198
   Yang C, 2017, IEEE T MULTIMEDIA, V19, P822, DOI 10.1109/TMM.2016.2626969
   Yang C, 2016, IEEE ACCESS, V4, P650, DOI 10.1109/ACCESS.2016.2523803
   Yang C, 2016, J SENSORS, V2016, DOI 10.1155/2016/5036857
   Yang Y, 2014, IEEE SENS J, V14, P1633, DOI 10.1109/JSEN.2013.2296509
   Ye MX, 2017, IEEE INT CON MULTI, P1524, DOI 10.1109/ICME.2017.8019500
   Ye MX, 2016, IEEE J-STSP, V10, P877, DOI 10.1109/JSTSP.2016.2559446
   Yu-Ren Li, 2011, 2011 International Conference on Multimedia Technology, P2841
   Zhang C, 2016, INT CONF UBIQ ROBOT, P61, DOI 10.1109/URAI.2016.7734021
   Zhang JT, 2013, PHYSIOL MEAS, V34, pN63, DOI 10.1088/0967-3334/34/8/N63
   Zhang SG, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/3090343
   Zhao JB, 2015, IEEE ENG MED BIO, P6679, DOI 10.1109/EMBC.2015.7319925
   Zou Q, 2018, IEEE T CYBERNETICS, V48, P1136, DOI 10.1109/TCYB.2017.2682280
NR 51
TC 17
Z9 17
U1 4
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2020
VL 22
IS 5
BP 1113
EP 1125
DI 10.1109/TMM.2019.2942479
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LJ3VZ
UT WOS:000530097200001
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Chen, H
   Zhang, X
   Xu, YL
   Ma, Z
   Zhang, WJ
AF Chen, Hao
   Zhang, Xu
   Xu, Yiling
   Ma, Zhan
   Zhang, Wenjun
TI Efficient Mobile Video Streaming via Context-Aware RaptorQ-Based Unequal
   Error Protection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mobile video streaming; context-aware; raptorq; UEP; FEC
ID SCALABLE VIDEO; TRANSMISSION; CODES; SCHEME
AB Mobile video streaming systems typically apply the forward error correction (FEC) at the application layer to cope with packet-level transmission errors, which complements the bit-level correction mechanisms at the physical layer. However, most existing works fail to exploit the block-level dependencies in both intra and interframe coding modes of a single-layer compressed video, and thus are less efficient for the prevailing H.264/AVC and/or H.265/HEVC compatible single-layer video application. To this end, we propose a low-complexity FEC, i.e., context-aware RaptorQ (CA-RQ) with unequal error protection (UEP), to improve the error recovery performance of the single-layer mobile video streaming, through incorporating the block-level dependencies in the compressed video data. We use a packet-level video transmission distortion model that considers the dependencies in both spatial and temporal domains, to quantify the importance of video packets within a group of pictures (GoP). The compressed video packets are categorized and grouped into several classes according to their importance to construct the CA-RQ code with the UEP property. We provide a theoretical analysis on redundancy allocation bounds to demonstrate the superior performance of proposed CA-RQ over the standard RaptorQ code. In the meantime, extensive simulations have shown that our scheme not only offers much better subjective visual quality with less than 50 additional redundant symbols as compared to the Macroblock-Based UEP (MB-UEP) scheme, but also outperforms the MB-UEP and classical equal error protection (EEP)-based schemes, by a 0.45$\sim$5.71 and 0.94$\sim$6.78 margin, respectively, in reconstructed quality evaluated using the structural similarity (SSIM) index, across a reasonable range of redundancy proportions.
C1 [Chen, Hao; Xu, Yiling; Zhang, Wenjun] Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
   [Zhang, Xu; Ma, Zhan] Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210093, Peoples R China.
C3 Shanghai Jiao Tong University; Nanjing University
RP Xu, YL (corresponding author), Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.; Ma, Z (corresponding author), Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210093, Peoples R China.
EM chenhao1210@sjtu.edu.cn; xzhang17@nju.edu.cn; yl.xu@sjtu.edu.cn;
   mazhan@nju.edu.cn; zhangwenjun@sjtu.edu.cn
RI zhang, xu/GYE-3558-2022; zhang, xu/GRX-9733-2022; Sui,
   Yanwei/AAH-9928-2021; Zhang, Wenjun/GNH-2095-2022; pang,
   li/JVY-9355-2024; Ma, Zhan/HKW-2859-2023; Chen, Hao/AGI-0052-2022
OI Zhang, Wenjun/0000-0002-5282-3725; Ma, Zhan/0000-0003-3686-4057; Chen,
   Hao/0000-0002-1179-8199; Zhang, Xu/0000-0002-1882-736X
FU National Natural Science Foundation of China [61650101, 61571215,
   61420106008, 61902178]; Scientific Research Plan of the Science and
   Technology Commission of Shanghai Municipality [18511105400]; Shanghai
   Key Lab of Digital Media Processing and Transmission; Natural Science
   Foundation of Jiangsu [BK20190295]
FX This work was supported in part by the National Natural Science
   Foundation of China (61650101, 61571215, 61420106008, and 61902178), in
   part by the Scientific Research Plan of the Science and Technology
   Commission of Shanghai Municipality (18511105400), in part by the
   Shanghai Key Lab of Digital Media Processing and Transmission, and in
   part by the Natural Science Foundation of Jiangsu under Grant
   BK20190295. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Sanjeev Mehra. H.
   Chen and X. Zhang contributed to this paper equally.
CR Ahmad S, 2011, IEEE T MULTIMEDIA, V13, P92, DOI 10.1109/TMM.2010.2093511
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], 5053 IETF RFC
   [Anonymous], [No title captured]
   Bakhshali A, 2015, EURASIP J WIREL COMM, DOI 10.1186/s13638-015-0485-0
   Boyce JM, 2016, IEEE T CIRC SYST VID, V26, P20, DOI 10.1109/TCSVT.2015.2461951
   Chang YC, 2008, IEEE T CONSUM ELECTR, V54, P1066, DOI 10.1109/TCE.2008.4637589
   Chen X, 2017, IEEE T CIRC SYST VID, V27, P366, DOI 10.1109/TCSVT.2015.2511815
   Cisco, 2017, Cisco7 Feb.
   Deng KY, 2018, SIGNAL PROCESS-IMAGE, V63, P81, DOI 10.1016/j.image.2018.02.004
   Elliadka KM, 2014, 2014 INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY AND ITS APPLICATIONS (ISITA), P269
   Fadhel H, 2015, 2015 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P295, DOI 10.1109/GlobalSIP.2015.7418204
   Hellge C, 2011, IEEE T MULTIMEDIA, V13, P551, DOI 10.1109/TMM.2011.2129499
   Huo YK, 2017, IEEE T VEH TECHNOL, V66, P7136, DOI 10.1109/TVT.2017.2656798
   Huo YK, 2016, IEEE ACCESS, V4, P5659, DOI 10.1109/ACCESS.2016.2604238
   Joseph Jomole Susan, 2015, 2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions), P1, DOI 10.1109/ICRITO.2015.7359347
   Liu YS, 2017, 2017 IEEE 28TH ANNUAL INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR, AND MOBILE RADIO COMMUNICATIONS (PIMRC), DOI 10.1109/PIMRC.2017.8292263
   Liu Z, 2018, SIGNAL PROCESS, V147, P154, DOI 10.1016/j.sigpro.2018.01.009
   Luby M, 2002, ANN IEEE SYMP FOUND, P271, DOI 10.1109/SFCS.2002.1181950
   Luby M., 2011, 6330 RFC INT ENG TAS
   Luo ZY, 2013, IEEE T MULTIMEDIA, V15, P2208, DOI 10.1109/TMM.2013.2280561
   MASNICK B, 1967, IEEE T INFORM THEORY, V13, P600, DOI 10.1109/TIT.1967.1054054
   National Instruments, 2019, White Paper
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Sejdinovic D, 2009, IEEE T COMMUN, V57, P2510, DOI 10.1109/TCOMM.2009.09.070616
   Shi DX, 2015, 2015 8TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), P1343, DOI 10.1109/CISP.2015.7408091
   Shokrollahi A, 2006, IEEE T INFORM THEORY, V52, P2551, DOI 10.1109/TIT.2006.874390
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Bui TD, 2017, 2017 31ST INTERNATIONAL CONFERENCE ON INFORMATION NETWORKING (ICOIN), P206, DOI 10.1109/ICOIN.2017.7899505
   Wang Y, 2006, IEEE T CIRC SYST VID, V16, P716, DOI 10.1109/TCSVT.2006.875203
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu JY, 2018, IEEE T MULTIMEDIA, V20, P457, DOI 10.1109/TMM.2017.2741425
   Wu JY, 2016, IEEE J SEL AREA COMM, V34, P2231, DOI 10.1109/JSAC.2016.2577178
   Yang XK, 2005, IEEE T MULTIMEDIA, V7, P753, DOI 10.1109/TMM.2005.846782
   Yuan L, 2016, IEEE T MULTIMEDIA, V18, P1389, DOI 10.1109/TMM.2016.2557079
   Zakerinasab MR, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P411, DOI 10.1145/2733373.2806267
   Zhou C, 2015, IEEE T CIRC SYST VID, V25, P1002, DOI 10.1109/TCSVT.2014.2364418
   Zhu C, 2016, IEEE T VEH TECHNOL, V65, P1506, DOI 10.1109/TVT.2015.2413790
NR 41
TC 8
Z9 10
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2020
VL 22
IS 2
BP 459
EP 473
DI 10.1109/TMM.2019.2928497
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KH5FZ
UT WOS:000510676300014
DA 2024-07-18
ER

PT J
AU Cabrera-Quiros, L
   Tax, DMJ
   Hung, H
AF Cabrera-Quiros, Laura
   Tax, David M. J.
   Hung, Hayley
TI Gestures In-The-Wild: Detecting Conversational Hand Gestures in Crowded
   Scenes Using a Multimodal Fusion of Bags of Video Trajectories and Body
   Worn Acceleration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Trajectory; Acceleration; Gesture recognition; Noise measurement; Human
   computer interaction; Visualization; Feature extraction; Hand gestures;
   crowded mingles; dense trajectories; multiple instance learning; MILES;
   wearable acceleration
ID RECOGNITION
AB This paper addresses the detection of hand gestures during free-standing conversations in crowded mingle scenarios. Unlike the scenarios of the previous works in gesture detection and recognition, crowded mingle scenes have additional challenges such as cross-contamination between subjects, strong occlusions, and nonstationary backgrounds. This makes them more complex to analyze using computer vision techniques alone. We propose a multimodal approach using video and wearable acceleration data recorded via smart badges hung around the neck. In the video modality, we propose to treat noisy dense trajectories as bags-of-trajectories. For a given bag, we can have good trajectories corresponding to the subject, and bad trajectories due for instance to cross-contamination. However, we hypothesize that for a given class, it should be possible to learn trajectories that are discriminative while ignoring noisy trajectories. We do this by exploiting multiple instance learning via embedded instance selection as our multiple instance learning approach. This technique also allows us to identify which instances contribute more to the classification. By fusing the decisions of the classifiers from the video and wearable acceleration modalities, we show improvements over the unimodal approaches with an AUC of 0.69. We also present a static analysis and a dynamic analysis to assess the impact of noisy data on the fused detection results, showing that the moments of high occlusion in the video are compensated by the information from the wearables. Finally, we applied our method to detect speaking status, leveraging the close relationship found in the literature between hand gestures and speech.
C1 [Cabrera-Quiros, Laura; Tax, David M. J.; Hung, Hayley] Delft Univ Technol, Dept Intelligent Syst, Delft, Netherlands.
   [Cabrera-Quiros, Laura] Inst Tecnol Costa Rica, Escuela Ingn Elect, Cartago, Costa Rica.
C3 Delft University of Technology; Instituto Tecnologico de Costa Rica
RP Cabrera-Quiros, L (corresponding author), Delft Univ Technol, Dept Intelligent Syst, Delft, Netherlands.
EM l.c.cabreraquiros@tudelft.nl; d.m.j.tax@tudelft.nl; h.hung@tudelft.nl
RI Hung, Hayley/AAS-2215-2021
OI Hung, Hayley/0000-0003-0719-8948
CR Alameda-Pineda X, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P5, DOI 10.1145/2733373.2806238
   Alameda-Pineda X, 2016, IEEE T PATTERN ANAL, V38, P1707, DOI 10.1109/TPAMI.2015.2496269
   Ali S, 2010, IEEE T PATTERN ANAL, V32, P288, DOI 10.1109/TPAMI.2008.284
   Alon J, 2009, IEEE T PATTERN ANAL, V31, P1685, DOI 10.1109/TPAMI.2008.203
   Miro XA, 2012, IEEE T AUDIO SPEECH, V20, P356, DOI 10.1109/TASL.2011.2125954
   Asadi-Aghbolaghi M, 2017, IEEE INT CONF AUTOMA, P476, DOI 10.1109/FG.2017.150
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Hue BH, 2014, 2014 INTERNATIONAL CONFERENCE ON COMPUTING, MANAGEMENT AND TELECOMMUNICATIONS (COMMANTEL), P84, DOI 10.1109/ComManTel.2014.6825584
   Cabrera-Quiros L., IEEE T AFFECTIVE COM
   Camgoz NC, 2016, INT C PATT RECOG, P49, DOI 10.1109/ICPR.2016.7899606
   Cerekovic A, 2017, IEEE T AFFECT COMPUT, V8, P382, DOI 10.1109/TAFFC.2016.2545650
   Chai XJ, 2016, INT C PATT RECOG, P31, DOI 10.1109/ICPR.2016.7899603
   Chen YX, 2006, IEEE T PATTERN ANAL, V28, P1931, DOI 10.1109/TPAMI.2006.248
   CRISTANI M., 2011, International Joint Conference on Ambient Intelligence, P72
   Duin P., 2007, PRTOOLS4 1 MATLAB TO, V2600
   Escalera S, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P445, DOI 10.1145/2522848.2532595
   Gedik E, 2017, PERS UBIQUIT COMPUT, V21, P723, DOI 10.1007/s00779-017-1006-4
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Kendon A., 2015, GESTURE VISIBLE ACTI
   Kendon Adam, 1990, CUP Archive, P153
   Krauss R. M., 1996, ADV EXP SOCIAL PSYCH
   Liang H, 2014, IEEE T MULTIMEDIA, V16, P1241, DOI 10.1109/TMM.2014.2306177
   Marcos-Ramiro A., 2014, P 16 INT C MULT INT, P327
   Marcos-Ramiro A., 2013, P IEEE INT C FAC GES, P1
   McNeill D., 1992, Hand and Mind: What Gestures Reveal about Thought
   Platt JC, 2000, ADV NEUR IN, P61
   Quek F., 2002, ACM Transactions on Computer-Human Interaction, V9, P171, DOI 10.1145/568513.568514
   Rautaray SS, 2015, ARTIF INTELL REV, V43, P1, DOI 10.1007/s10462-012-9356-9
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148
   Stewart R, 2016, PROC CVPR IEEE, P2325, DOI 10.1109/CVPR.2016.255
   vanGemert C., 2015, P BRIT MACH VIS C
   Vinciarelli A, 2009, IMAGE VISION COMPUT, V27, P1743, DOI 10.1016/j.imavis.2008.11.007
   Wan J, 2016, IEEE COMPUT SOC CONF, P761, DOI 10.1109/CVPRW.2016.100
   Wang C, 2015, IEEE T MULTIMEDIA, V17, P29, DOI 10.1109/TMM.2014.2374357
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang PC, 2018, IEEE T MULTIMEDIA, V20, P1051, DOI 10.1109/TMM.2018.2818329
   Wang PC, 2016, INT C PATT RECOG, P13, DOI 10.1109/ICPR.2016.7899600
   Xiong YG, 2006, INT J COMPUT VISION, V69, P353, DOI 10.1007/s11263-006-8112-5
   Yang ZJ, 2014, IEEE T MULTIMEDIA, V16, P1766, DOI 10.1109/TMM.2014.2328311
   Yi Y, 2016, PATTERN RECOGN, V53, P148, DOI 10.1016/j.patcog.2015.11.022
NR 41
TC 9
Z9 9
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2020
VL 22
IS 1
BP 138
EP 147
DI 10.1109/TMM.2019.2922122
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KB6AZ
UT WOS:000506577000013
DA 2024-07-18
ER

PT J
AU Huang, XL
   Tang, XW
   Hu, F
AF Huang, Xin-Lin
   Tang, Xiao-Wei
   Hu, Fei
TI Dynamic Spectrum Access for Multimedia Transmission Over Multi-User,
   Multi-Channel Cognitive Radio Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cognitive radio networks; Dynamic spectrum access; Quality of service;
   Bayesian online learning
ID POLLACZEK-KHINCHIN FORMULA; THROUGHPUT ANALYSIS; SELECTION; PROTOCOL
AB The optimal spectrum access strategy is investigated for multi-user multi-channel scenario in cognitive radio networks. At first, an online learning method based on Dirichlet Process is adopted to predict the channel usage based on ACK/NACK feedbacks, which can avoid frequent information exchange among users. Based on the prediction result, the delay performance can be computed when a user transmits certain percentage of multimedia packets on a specific channel. Second, the packet delivery ratio (PDR) is derived from the prediction result of channel usage to reflect the accessing competition among multiple users. Finally, the quality of service (QoS) of multimedia applications is defined as the joint delay and throughput performances. Moreover, a dynamic spectrum access scheme is proposed to optimize the QoS metrics. The simulation results demonstrate that the QoS and the peak-signal-to-noise ratio (PSNR) of the proposed spectrum access algorithm outperform the three existing spectrum access algorithms, i.e., cognitive cross-layer algorithm, dynamic learning algorithm, and dynamic least interference algorithm. The proposed algorithm achieves more than 21.8 & x0025;, 5.4 & x0025;, and 3.9 & x0025; PDR enhancement and over 3.23 dB, 0.82 dB, and 0.50 dB PSNR gains, compared with those three algorithms, given the transmission power as 10, 20, and 30 units, respectively.
C1 [Huang, Xin-Lin; Tang, Xiao-Wei] Tongji Univ, Dept Informat & Commun Engn, Shanghai 201804, Peoples R China.
   [Hu, Fei] Univ Alabama, Dept Elect & Comp Engn, Tuscaloosa, AL 35487 USA.
C3 Tongji University; University of Alabama System; University of Alabama
   Tuscaloosa
RP Huang, XL (corresponding author), Tongji Univ, Dept Informat & Commun Engn, Shanghai 201804, Peoples R China.
EM xlhuang@tongji.edu.cn; xwtang@tongji.edu.cn; fei@eng.ua.edu
FU National Natural Science Foundation of China [U1733114, 61631017];
   Fundamental Research Funds for the Central Universities; Shanghai
   Rising-Star Program [19QA1409100]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants U1733114 and 61631017, in part by the
   Fundamental Research Funds for the Central Universities, and in part by
   the Shanghai Rising-Star Program under Grant 19QA1409100. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Honggang Wang.
CR Ali A, 2018, IEEE T VEH TECHNOL, V67, P7275, DOI 10.1109/TVT.2018.2832292
   Amjad M, 2018, IEEE COMMUN SURV TUT, V20, P1056, DOI 10.1109/COMST.2018.2794358
   [Anonymous], P IEEE LAT AM C COMM
   Baccelli F, 2006, IEEE T INFORM THEORY, V52, P421, DOI 10.1109/TIT.2005.862098
   Balapuwaduge IAM, 2017, IEEE T VEH TECHNOL, V66, P2771, DOI 10.1109/TVT.2016.2585200
   Bayhan S, 2013, IEEE T VEH TECHNOL, V62, P582, DOI 10.1109/TVT.2012.2225650
   Chang CS, 2005, IEEE INFOCOM SER, P960
   Chaoub A, 2019, TELECOMMUN SYST, V71, P321, DOI 10.1007/s11235-018-0498-1
   Chen C, 2011, IEEE T WIREL COMMUN, V10, P2135, DOI 10.1109/TWC.2011.041311.100626
   Cisco, 2017, Cisco7 Feb.
   Curtis FE, 2017, MATH PROGRAM, V161, P73, DOI 10.1007/s10107-016-1003-9
   Ding GR, 2016, IEEE J SEL AREA COMM, V34, P107, DOI 10.1109/JSAC.2015.2452532
   FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360
   Gahane L, 2018, IEEE T COMMUN, V66, P534, DOI 10.1109/TCOMM.2017.2754250
   He ZF, 2016, IEEE T MULTIMEDIA, V18, P1401, DOI 10.1109/TMM.2016.2564104
   Huang L, 2013, IEEE WIREL COMMUN, V20, P44, DOI 10.1109/MWC.2013.6507393
   Huang XL, 2015, IEEE J SEL AREA COMM, V33, P771, DOI 10.1109/JSAC.2014.2361075
   Huang XL, 2014, COMPUT COMMUN, V51, P48, DOI 10.1016/j.comcom.2014.06.004
   Huang XL, 2011, IEEE T VEH TECHNOL, V60, P2714, DOI 10.1109/TVT.2011.2153885
   Huang XP, 2016, AER ADV ENG RES, V43, P1
   Kaligineedi P, 2008, IEEE ICC, P3406, DOI 10.1109/ICC.2008.640
   Kim D, 2008, IEEE T WIREL COMMUN, V7, P5195, DOI 10.1109/T-WC.2008.070950
   Lee S, 2013, IEEE T WIREL COMMUN, V12, P4788, DOI 10.1109/TWC.2013.072613.130323
   Li JQ, 2019, INT J PROD RES, V57, P6922, DOI 10.1080/00207543.2019.1571687
   Li XF, 2012, IEEE T WIREL COMMUN, V11, P3900, DOI 10.1109/TWC.2012.092112.111425
   Mirtchev ST, 2017, ELECTRON LETT, V53, P27, DOI 10.1049/el.2016.2876
   Nocedal J, 2014, OPTIM METHOD SOFTW, V29, P837, DOI 10.1080/10556788.2013.858156
   Qingqi Pei, 2011, 2011 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery, P491, DOI 10.1109/CyberC.2011.104
   Renner IW, 2013, BIOMETRICS, V69, P274, DOI 10.1111/j.1541-0420.2012.01824.x
   Rodríguez I, 2013, CATEDR TOMADA, V1, P1, DOI 10.5195/ct/2013.57
   Saki H, 2015, IEEE T MULTIMEDIA, V17, P333, DOI 10.1109/TMM.2015.2389032
   SETHURAMAN J, 1994, STAT SINICA, V4, P639
   Shenoy SU, 2016, 2016 INTERNATIONAL CONFERENCE ON COMMUNICATION AND SIGNAL PROCESSING (ICCSP), VOL. 1, P907, DOI 10.1109/ICCSP.2016.7754279
   Shiang HP, 2008, IEEE T MULTIMEDIA, V10, P896, DOI 10.1109/TMM.2008.922851
   Sun XX, 2012, IEEE CONF COMPUT, P145, DOI 10.1109/INFCOMW.2012.6193477
   Tang H, 2012, IET COMMUN, V6, P974, DOI 10.1049/iet-com.2010.0553
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Tragos EZ, 2013, IEEE COMMUN SURV TUT, V15, P1108, DOI 10.1109/SURV.2012.121112.00047
   Tsiropoulos GI, 2016, IEEE COMMUN SURV TUT, V18, P824, DOI 10.1109/COMST.2014.2362796
   Wellens M., 2009, PROC IEEECOMMUN SOC, P1
   Xie N, 2017, IEEE T INFORM THEORY, V63, P3181, DOI 10.1109/TIT.2016.2640302
   Xing XS, 2013, IEEE WIREL COMMUN, V20, P90, DOI 10.1109/MWC.2013.6507399
   Yao RX, 2015, IEEE T MULTIMEDIA, V17, P434, DOI 10.1109/TMM.2015.2394385
   Zavala VM, 2014, SIAM J OPTIMIZ, V24, P528, DOI 10.1137/120888181
   Zeng FZ, 2011, IEEE J-STSP, V5, P37, DOI 10.1109/JSTSP.2010.2055037
   Zhang RQ, 2015, IEEE T INTELL TRANSP, V16, P411, DOI 10.1109/TITS.2014.2335746
   Zheng HT, 2005, 2005 1st IEEE International Symposium on New Frontiers in Dynamic Spectrum Access Networks, Conference Record, P56
NR 47
TC 29
Z9 29
U1 1
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2020
VL 22
IS 1
BP 201
EP 214
DI 10.1109/TMM.2019.2925960
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KB6AZ
UT WOS:000506577000018
DA 2024-07-18
ER

PT J
AU Bai, JL
   Ni, BB
   Wang, MS
   Li, ZF
   Cheng, S
   Yang, XK
   Hu, CP
   Gao, W
AF Bai, Jiale
   Ni, Bingbing
   Wang, Minsi
   Li, Zefan
   Cheng, Shuo
   Yang, Xiaokang
   Hu, Chuanping
   Gao, Wen
TI Deep Progressive Hashing for Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Saliency; image retrieval; deep hashing; graph-based recurrent neural
   networks
ID LEARNING BINARY-CODES; NETWORKS
AB Hashing is a widely adopted method based on an approximate nearest neighbor search and is used in large-scale image retrieval tasks. Conventional learning-based hashing algorithms employ end-to-end representation learning, which is a one-off technique. Because of the tradeoff between efficiency and performance, conventional learning-based hashing methods must sacrifice code length to improve performance, which increases their computational complexity. To improve the efficiency of binary codes, motivated by the "nonsalient-to-salient" attention scheme of humans, we propose a recursive hashing mechanism that maps progressively expanded salient regions to a series of binary codes. These salient regions are generated by a conventional saliency model based on bottom-up saliency-driven attention and a semantic-guided saliency model based on top-down task-driven attention. After obtaining a series of salient regions, we perform long-range temporal modeling of salient regions using a graph-based recurrent deep network to obtain more refined representative features. The later output nodes inherit aggregated information from all previous nodes and extract discriminative features from more salient regions. Therefore, this network possesses more significant information and satisfactory scalability. The proposed recursive hashing neural network, optimized by a triplet ranking loss, is end-to-end trainable. Extensive experimental results from several image retrieval benchmarks show the scalability of our method and demonstrate its strong performance compared with state-of-the-art methods.
C1 [Bai, Jiale; Ni, Bingbing; Wang, Minsi; Li, Zefan; Yang, Xiaokang; Hu, Chuanping] Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China.
   [Cheng, Shuo] Univ Calif San Diego, Dept Comp Sci, La Jolla, CA 92093 USA.
   [Yang, Xiaokang] Shanghai Jiao Tong Univ, MoE Key Lab Artificial Intelligence, AI Inst, Shanghai 200240, Peoples R China.
   [Gao, Wen] Peking Univ, Dept Comp Sci & Technol, Beijing 100091, Peoples R China.
C3 Shanghai Jiao Tong University; University of California System;
   University of California San Diego; Shanghai Jiao Tong University;
   Peking University
RP Ni, BB (corresponding author), Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China.
EM baijiale@sjtu.edu.cn; nibingbing@sjtu.edu.cn; mswang1994@gmail.com;
   leezf@sjtu.edu.cn; acccheng94@gmail.com; xkyang@sjtu.edu.cn;
   cphu@vip.sina.com; wgao@pku.edu.cn
RI Yang, Xiaokang/C-6137-2009
OI Yang, Xiaokang/0000-0003-4029-3322; Cheng, Shuo/0000-0002-4477-9875
FU National Science Foundation of China [U1611461]; China's Thousand Talent
   Program
FX This work was supported in part by the National Science Foundation of
   China (U1611461) and in part by joint research under Grant of SJTU-BIGO
   LIVE and China's Thousand Talent Program. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Mohammed Daoudi.
CR [Anonymous], P INT C LEARN REPR C
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2016, P ACM C MULT MM AMST
   [Anonymous], 2016, P 24 ACM INT C MULT
   [Anonymous], 2015, 2015 IEEE INT C MULT
   [Anonymous], 2014, Learning to execute
   [Anonymous], 2018, ARXIV180302987
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], CVPR
   [Anonymous], 2009, NIPS
   [Anonymous], 2016, ARXIV160706450
   [Anonymous], 2018, P EUR C COMP VIS ECC
   [Anonymous], P 10 INT WORKSH CONT
   [Anonymous], 2014, P 31 INT C INT C MAC
   [Anonymous], P ADV NEUR INF PROC
   [Anonymous], 2015, ARXIV151106343
   Bruce N., 2006, P ADV NEUR INF PROC, P155
   Cao Y, 2016, AAAI CONF ARTIF INTE, P3457
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Ding K, 2017, IEEE T MULTIMEDIA, V19, P571, DOI 10.1109/TMM.2016.2625747
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Duan LY, 2015, IEEE T MULTIMEDIA, V17, P828, DOI 10.1109/TMM.2015.2419973
   Durieux T, 2018, PROC INT SYMP SOFTW, P1, DOI 10.1109/ISSRE.2018.00012
   Duvenaudt D, 2015, ADV NEUR IN, V28
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Gong YC, 2013, PROC CVPR IEEE, P484, DOI 10.1109/CVPR.2013.69
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Guo C, 2014, IEEE CONF VIS ANAL, P353
   Hao YB, 2017, IEEE T MULTIMEDIA, V19, P1, DOI 10.1109/TMM.2016.2610324
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang CQ, 2018, IEEE T IMAGE PROCESS, V27, P4490, DOI 10.1109/TIP.2018.2839522
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kipf TN, 2016, ARXIV
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Li X, 2013, INT J UNCERTAIN FUZZ, V21, P1, DOI 10.1142/S0218488513400011
   Li Y., 2015, ARXIV
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu L, 2015, IEEE T IMAGE PROCESS, V24, P956, DOI 10.1109/TIP.2015.2390975
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu XL, 2013, PROC CVPR IEEE, P1570, DOI 10.1109/CVPR.2013.206
   Lv YM, 2015, IEEE T MULTIMEDIA, V17, P1225, DOI 10.1109/TMM.2015.2437712
   Niepert M, 2016, PR MACH LEARN RES, V48
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Norouzi M., 2012, ADV NEURAL INFORM PR
   Norouzi M.E., 2011, ICML
   Rahmani R., 2005, ACM WORKSHOP MULTIME, P227, DOI DOI 10.1145/1101826.1101863
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Salakhutdinov R., 2007, AISTATS, V2, P412
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Shen XB, 2018, IEEE T NEUR NET LEAR, V29, P4324, DOI 10.1109/TNNLS.2017.2763967
   Shen XB, 2017, IEEE T CYBERNETICS, V47, P4275, DOI 10.1109/TCYB.2016.2606441
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Song JK, 2018, AAAI CONF ARTIF INTE, P394
   Theeuwes J, 2000, CONTROL OF COGNITIVE PROCESSES: ATTENTION AND PERFORMANCE XVIII, P105
   Nguyen VA, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P989, DOI 10.1145/2647868.2655003
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang J, 2013, IEEE I CONF COMP VIS, P3032, DOI 10.1109/ICCV.2013.377
   Wang QF, 2014, LECT NOTES COMPUT SC, V8691, P378, DOI 10.1007/978-3-319-10578-9_25
   Wang TH, 2014, COMPUT VIS IMAGE UND, V120, P14, DOI 10.1016/j.cviu.2013.10.013
   Weiss Yair, 2009, Advances in Neural Information Processing Systems, P1753, DOI DOI 10.5555/2981780.2981999
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Wu Z., 2016, ARXIV160506885
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yu F., 2017, DILATED RESIDUAL NET, P472
   Zhai DM, 2018, IEEE T MULTIMEDIA, V20, P675, DOI 10.1109/TMM.2017.2749160
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zhao WQ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3504
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
   Zhu XF, 2017, IEEE T MULTIMEDIA, V19, P2033, DOI 10.1109/TMM.2017.2703636
NR 80
TC 20
Z9 21
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2019
VL 21
IS 12
BP 3178
EP 3193
DI 10.1109/TMM.2019.2920601
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KJ8ZS
UT WOS:000512345200016
DA 2024-07-18
ER

PT J
AU Huang, X
   Peng, YX
AF Huang, Xin
   Peng, Yuxin
TI TPCKT: Two-Level Progressive Cross-Media Knowledge Transfer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Media; Correlation; Knowledge transfer; Training; Semantics; Task
   analysis; Data models; Cross-media retrieval; knowledge transfer;
   two-level transfer; progressive training
ID REPRESENTATION
AB As multimedia data have been the main form of big data, cross-media retrieval becomes a research hotspot. It provides a flexible retrieval paradigm across different media types, such as using an image query to retrieve the relevant text, video, and audio. An effective model to establish cross-media correlation is indispensable for retrieval. Existing methods usually rely on labeled data for model training, but it is extremely labor consuming to collect and label cross-media data. Under this situation, it is a key issue toward the real application to transfer knowledge from existing data to new data, for reducing the human labor. However, little attention has been paid to knowledge transfer between two cross-media domains. Therefore, this paper proposes the approach of two-level progressive cross-media knowledge transfer (TPCKT), which transfers knowledge from large-scale cross-media data, to boost the retrieval accuracy on cross-media data of another domain. Its contributions are: First, two-level adversarial transfer architecture is proposed with domain discriminators in media-specific level and media-shared level, which have partially shared parameters to preserve cross-media consistency of transfer. The domain discrepancy between cross-media domains is fully reduced for boosting the retrieval accuracy. Second, progressive semantic transfer mechanism is proposed to iteratively select semantically related categories in two cross-media domains for transfer. This drives the transfer process with ascending difficulties, for addressing the difficulty from different label spaces, and ensuring the robustness of transfer. For the experiment, the large-scale cross-media dataset PKU XMediaNet serves as the source domain, and three widely used small-scale datasets are adopted as the target domains to perform retrieval. Experimental results show the promising improvement gained by the proposed TPCKT.
C1 [Huang, Xin; Peng, Yuxin] Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.
C3 Peking University
RP Peng, YX (corresponding author), Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.
EM pengyuxin@pku.edu.cn
FU National Natural Science Foundation of China [61771025, 61532005]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61771025 and Grant 61532005. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Engin Erzin.
CR Andrew Galen, 2010, INT C MACH LEARN, P3408
   [Anonymous], 2003, P ACM INT C MULT ACM
   [Anonymous], 2015, INT C MACH LEARN
   [Anonymous], 2011, P ICML
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], P INT C INT MULT COM
   [Anonymous], J MACHINE LEARNING R
   [Anonymous], 2012, PROC 5 ACM INT C WEB
   Bengio Y., 2009, P 26 ANN INT C MACH, V60, P6, DOI [DOI 10.1145/1553374.1553380, 10.1145/1553374.1553380]
   Cao ZJ, 2017, AAAI CONF ARTIF INTE, P81
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Gilakjani A.P., 2012, Journal of Studies in Education, V2, P104
   Gong C, 2016, IEEE T IMAGE PROCESS, V25, P3249, DOI 10.1109/TIP.2016.2563981
   Goodfellow I. J., 2015, 3 INT C LEARNING REP
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hinton G., 2015, COMPUT SCI, V2
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hu YT, 2018, IEEE T MULTIMEDIA, V20, P927, DOI 10.1109/TMM.2017.2760101
   Hua Y, 2016, IEEE T MULTIMEDIA, V18, P1201, DOI 10.1109/TMM.2016.2535864
   Huang R, 2017, IEEE INT VEH SYM, P1893, DOI 10.1109/IVS.2017.7995981
   Huang X, 2018, PROC CVPR IEEE, P8837, DOI 10.1109/CVPR.2018.00921
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Kim J., 2012, PROC COLING, P579
   Kim Y, 2014, IEEE ASME INT C ADV, P1747, DOI 10.1109/AIM.2014.6878336
   Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Mikolov T., 2013, INT C LEARNING REPRE, P1
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pawan Kumar M., 2010, NIPS
   Peng Y, 2016, P INT JOINT C ART IN, P3846
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Peng YX, 2016, IEEE T CIRC SYST VID, V26, P583, DOI 10.1109/TCSVT.2015.2400779
   Pentina A, 2015, PROC CVPR IEEE, P5492, DOI 10.1109/CVPR.2015.7299188
   Qi G., 2011, Proc. ACM International Conference on World Wide Web, P297
   Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466
   Rashtchian C., 2010, P NAACL HLT 2010 WOR, V2010, P139, DOI DOI 10.5555/1866696.1866717
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Simonyan K., 2014, 14091556 ARXIV
   Srivastava Neelam., 2012, The Postcolonial Gramsci, P1
   Supancic JS, 2013, PROC CVPR IEEE, P2379, DOI 10.1109/CVPR.2013.308
   Tang JH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2998574
   Tsai YHH, 2016, PROC CVPR IEEE, P5081, DOI 10.1109/CVPR.2016.549
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang SH, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1398, DOI 10.1145/3240508.3240535
   Wang SH, 2012, PROC CVPR IEEE, P2240, DOI 10.1109/CVPR.2012.6247933
   Wei YC, 2017, IEEE T CYBERNETICS, V47, P449, DOI 10.1109/TCYB.2016.2519449
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Zhai XH, 2014, IEEE T CIRC SYST VID, V24, P965, DOI 10.1109/TCSVT.2013.2276704
   Zhang JG, 2017, IEEE T CYBERNETICS, V47, P960, DOI 10.1109/TCYB.2016.2535122
NR 55
TC 15
Z9 15
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2019
VL 21
IS 11
BP 2850
EP 2862
DI 10.1109/TMM.2019.2911456
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JJ7VX
UT WOS:000494363000013
DA 2024-07-18
ER

PT J
AU Liu, J
   Liu, PP
   Su, YT
   Jing, PG
   Yang, XK
AF Liu, Jing
   Liu, Pingping
   Su, Yuting
   Jing, Peiguang
   Yang, Xiaokang
TI Spatiotemporal Symmetric Convolutional Neural Network for Video
   Bit-Depth Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video bit-depth enhancement; encoder-decoder network; spatiotemporal
   symmetry; convolutional neural networks; feature fusion
ID SUPERRESOLUTION
AB In contrast to the high sensitivity of human eyes and rapid development of modern display devices in terms of dynamic range, mainstream multimedia sources are generally at relatively lower hit depths (BDs). Therefore, BD enhancement (BDE), which attempts to transform low-BD multimedia sources into high-BD sources, is considered of significant research value. Current BDE algorithms are based on images rather than videos. However, for massive numbers of videos, temporal continuity among frames should be considered. Thus, in this paper, we propose a spatiotemporal symmetric BDE network for videos based on an encoder-decoder network. Consecutive frames are input into five subnets in the encoder, where the convolutional filters in the temporal symmetric subnets share the same weights to achieve lower model complexity. In addition, symmetric skip connections are introduced between the symmetric convolutionat/deconvolutional layers of the encoder/decoder to pass features and alleviate the gradient diffusion problem. The experimental results show that our model can efficiently eliminate false contours and chroma distortions. The model significantly outperforms state-of-the-art image BDE algorithms and single-frame baseline models in terms of PSNR and SSIM.
C1 [Liu, Jing; Liu, Pingping; Su, Yuting; Jing, Peiguang] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Yang, Xiaokang] Shanghai Jiao Tong Univ, AI Inst, MoE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China.
C3 Tianjin University; Shanghai Jiao Tong University
RP Jing, PG (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM jliu_tju@tju.edu.cn; ppliu@tju.edu.cn; ytsu@tju.edu.cn;
   pgjing@tju.edu.cn; xkyang@sjtu.edu.cn
RI Liu-Zeng, Jing/F-8582-2011; Yang, Xiaokang/C-6137-2009
OI Yang, Xiaokang/0000-0003-4029-3322; Liu, Pingping/0000-0002-2971-9487;
   Jing, Peiguang/0000-0003-2648-7358
FU China Postdoctoral Science Foundation [2018M641648]; National Science
   Foundation of China [61701341, 61572356, 61802277]
FX This work was supported in part by China Postdoctoral Science Foundation
   under Grant 2018M641648, and in part by the National Science Foundation
   of China under Grants 61701341, 61572356, and 61802277. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Raouf Hamzaoui.
CR [Anonymous], [No title captured]
   Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Cheng CH, 2009, IEEE INT SYMP CIRC S, P944, DOI 10.1109/ISCAS.2009.5117913
   Drulea M, 2011, IEEE INT C INTELL TR, P318, DOI 10.1109/ITSC.2011.6082986
   Fan R, 2017, IEEE IMAGE PROC, P1847, DOI 10.1109/ICIP.2017.8296601
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Hsiao YH, 2004, IEEE IMAGE PROC, P1449
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kappeler A, 2016, IEEE T COMPUT IMAG, V2, P109, DOI 10.1109/TCI.2016.2532323
   Kovaleski RP, 2014, 2014 27TH SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND IMAGES (SIBGRAPI), P49, DOI 10.1109/SIBGRAPI.2014.29
   Liao RJ, 2015, IEEE I CONF COMP VIS, P531, DOI 10.1109/ICCV.2015.68
   Liu D, 2018, IEEE T IMAGE PROCESS, V27, P3432, DOI 10.1109/TIP.2018.2820807
   Liu J, 2018, IEEE T IMAGE PROCESS, V27, P4860, DOI 10.1109/TIP.2018.2803306
   Liu RS, 2018, INT J DIGIT MULTIMED, V2018, DOI [10.1155/2018/7543875, 10.1109/TMM.2018.2812605]
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Mittal G., 2012, VCIP, P1
   Pengfei Wan, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P170, DOI 10.1109/ICME.2012.118
   PKingma D., 2013, P INT C LEARN REPR
   Qiu ZF, 2018, IEEE T MULTIMEDIA, V20, P939, DOI 10.1109/TMM.2017.2759504
   Rezende Danilo Jimenez, 2014, ARXIV14014082
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Sun W., 2017, International Forum on Digital TV and Wireless Multimedia Commun, V815, P255
   Tao X, 2017, IEEE I CONF COMP VIS, P4482, DOI 10.1109/ICCV.2017.479
   Ulichney R. A., 2000, P SOC PHOTO-OPT INS, V3300, P232
   Wan PF, 2016, IEEE T IMAGE PROCESS, V25, P2896, DOI 10.1109/TIP.2016.2553523
   Wang WJ, 2018, IEEE ACCESS, V6, P23767, DOI 10.1109/ACCESS.2018.2829908
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zhang XP, 2017, IEEE T MULTIMEDIA, V19, P2736, DOI 10.1109/TMM.2017.2710803
NR 31
TC 9
Z9 10
U1 2
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2019
VL 21
IS 9
BP 2397
EP 2406
DI 10.1109/TMM.2019.2897909
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IT6VX
UT WOS:000483015200019
DA 2024-07-18
ER

PT J
AU Choi, J
   Cho, H
   Song, J
   Yoon, SM
AF Choi, Jungwoo
   Cho, Heeryon
   Song, Jinjoo
   Yoon, Sang Min
TI SketchHelper: Real-Time Stroke Guidance for Freehand Sketch Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Stroke-based modeling; sketch based sketch retrieval; shadow-guided
   drawing; deep learning
AB Text-based retrieval systems have been popular, but content-based retrieval systems have gained widespread acceptance in recent years to directly retrieve diverse media based on their visual content, such as color, texture, and shape. Among many content-based retrieval systems, sketch-based media retrieval systems have attracted attention recently with the proliferation of tablet PCs and smart mobile devices. Sketch-based retrieval requires the user to draw a freehand sketch query, but freehand drawing can be challenging for those with limited drawing skills. This degrades retrieval performance, since successful retrieval depends on the quality of the sketch query image drawn by the user. To address this issue, we propose a real-time stroke guidance for freehand sketch retrieval that continuously displays next-stroke shadow sketches on the canvas based on the user's step-by-step partial strokes. We train a stroke guidance network that learns the mapping between the step-wise stroke relations to predict the user's next stroke. The proposed stroke guidance for freehand sketch retrieval system runs on a five step next-stroke prediction model that identifies candidate next-stroke sketches from a database of millions of sketches. The system retrieves variable number of sketch object classes at different drawing stages. During the initial sketching stage, diverse drawing possibilities are covered by retrieving multiple sketch classes; as the sketching progresses, the intended sketch class is narrowed down to one. Deep binary hashing is employed for efficient similarity matching of relevant next-stroke sketches. We extend the Google QuickDraw dataset to create a five step sketch stroke database. Qualitative and quantitative experiments are conducted to verify the effectiveness of the proposed system, which can be utilized for drawing guidance, tracing, and sketch retrieval. Tracing refers to the act of copying the shadowed line of a guiding image by drawing over its lines.
C1 [Choi, Jungwoo; Cho, Heeryon; Song, Jinjoo; Yoon, Sang Min] Kookmin Univ, Coll Comp Sci, HCI Lab, Seoul 02707, South Korea.
C3 Kookmin University
RP Yoon, SM (corresponding author), Kookmin Univ, Coll Comp Sci, HCI Lab, Seoul 02707, South Korea.
EM noirmist@kookmin.ac.kr; heeryon@kookmin.ac.kr; decpearl@kookmin.ac.kr;
   smyoon@kookmin.ac.kr
RI Cho, Heeryon/E-9478-2011; Cho, Heeryon/C-9255-2013
OI Cho, Heeryon/0000-0001-9912-1002; Cho, Heeryon/0000-0001-9912-1002;
   Song, Jinjoo/0000-0003-3335-5644
FU National Research Foundation of Korea - Korean Government
   [2015R1A5A7037615, 2016R1D1A1B04932889, 2017R1A2B4011015]
FX This work was supported by the National Research Foundation of Korea
   funded by Korean Government under Grant 2015R1A5A7037615, Grant
   2016R1D1A1B04932889, and Grant 2017R1A2B4011015. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Xiaochun Cao. (Corresponding author: Sang Min
   Yoon.)
CR [Anonymous], 2013, ICML
   [Anonymous], 2013, ACM Transactions on Graphics (TOG)
   Bu SH, 2014, IEEE T MULTIMEDIA, V16, P2154, DOI 10.1109/TMM.2014.2351788
   Creswell Antonia., 2016, EUR C COMP VIS, P798, DOI DOI 10.1007/978-3-319-46604-0_55
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   Ha David, 2018, INT C LEARN REPR
   Hu R, 2010, IEEE IMAGE PROC, P1025, DOI 10.1109/ICIP.2010.5649331
   Iarussi E., 2013, P 26 ANN ACM S USER, DOI [DOI 10.1145/2501988.2501997, 10.1145/2501988.2501997]
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kingma D. P., 2014, arXiv
   Leal-Taixé L, 2016, IEEE COMPUT SOC CONF, P418, DOI 10.1109/CVPRW.2016.59
   Lee J., 2008, P EUROGRAPHICS WORKS, P97, DOI DOI 10.2312/SBM/SBM08/097-104
   Lee YJ, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964922
   Li ZC, 2015, IEEE T MULTIMEDIA, V17, P1989, DOI 10.1109/TMM.2015.2477035
   Lin Kevin, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P27, DOI 10.1109/CVPRW.2015.7301269
   Liu L, 2017, PROC CVPR IEEE, P2298, DOI 10.1109/CVPR.2017.247
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma C, 2016, IMAGE VISION COMPUT, V46, P64, DOI 10.1016/j.imavis.2015.11.007
   Matsui Y, 2017, IEEE T VIS COMPUT GR, V23, P1852, DOI 10.1109/TVCG.2016.2554113
   Qian XM, 2016, IEEE T IMAGE PROCESS, V25, P195, DOI 10.1109/TIP.2015.2497145
   Saavedra J.M., 2015, P BRIT MACH VIS C 20
   Su QK, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601202
   Wang F, 2015, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2015.7298797
   Wang S, 2015, IEEE T MULTIMEDIA, V17, P1045, DOI 10.1109/TMM.2015.2431492
   Xie J., 2014, P 27 ANN ACM S USER, P407
   Xie J, 2017, IEEE T MULTIMEDIA, V19, P2463, DOI 10.1109/TMM.2017.2698200
   Xu P, 2018, PROC CVPR IEEE, P8090, DOI 10.1109/CVPR.2018.00844
   YOON S.M., 2010, Proceedings of the international conference on Multimedia, P193
   [于谦 Yu Qian], 2015, [高分子通报, Polymer Bulletin], P1
   Yuqi Zhang, 2016, Advances in Multimedia Information Processing - PCM 2016. 17th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 9917, P689, DOI 10.1007/978-3-319-48896-7_68
   Zaremba W., 2014, CORR
   Zhang YT, 2016, IEEE T MULTIMEDIA, V18, P1604, DOI 10.1109/TMM.2016.2568138
NR 34
TC 21
Z9 24
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2019
VL 21
IS 8
BP 2083
EP 2092
DI 10.1109/TMM.2019.2892301
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IK7YM
UT WOS:000476809700015
DA 2024-07-18
ER

PT J
AU Hu, HW
   Ma, B
   Shen, JB
   Sun, HQ
   Shao, L
   Porikli, F
AF Hu, Hongwei
   Ma, Bo
   Shen, Jianbing
   Sun, Hanqiu
   Shao, Ling
   Porikli, Fatih
TI Robust Object Tracking Using Manifold Regularized Convolutional Neural
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural networks; deep learning; deep tracker; manifold
   regularization; object tracking; online tracking
ID VISUAL TRACKING
AB In visual tracking, usually only a small number of samples are labeled, and most existing deep learning based trackers ignore abundant unlabeled samples that could provide additional information for deep trackers to boost their tracking performance. An intuitive way to explain unlabeled data is to incorporate manifold regularization into the common classification loss functions, but the high computational cost may prohibit those deep trackers from practical applications. To overcome this issue, we propose a two-stage approach to a deep tracker that takes into account both labeled and unlabeled samples. The annotation of unlabeled samples is propagated from its labeled neighbors first by exploring the manifold space that these samples are assumed to lie in. Then, we refine it by training a deep convolutional neural network using both labeled and unlabeled data in a supervised manner. Online visual tracking is further carried out under the framework of particle filters with the presented manifold regularized deep model being updated every few frames. Experimental results on different tracking datasets demonstrate that our tracker outperforms most existing tracking approaches. The source code and results are available at: https://github.com/shenjianbing/MRCNNTracking.
C1 [Hu, Hongwei; Ma, Bo] Beijing Inst Technol, Beijing Lab Intelligent Informa Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China.
   [Shen, Jianbing] Beijing Inst Technol, Beijing Lab Intelligent Informat Technol, Sch Comp Sci, Beijing 100081, Peoples R China.
   [Shen, Jianbing; Shao, Ling] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.
   [Sun, Hanqiu] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
   [Shao, Ling] Univ East Anglia, Sch Comp Sci, Norwich NR5 8HZ, Norfolk, England.
   [Porikli, Fatih] Australian Natl Univ, Res Sch Engn, Canberra, ACT 0200, Australia.
C3 Beijing Institute of Technology; Beijing Institute of Technology;
   Chinese University of Hong Kong; University of East Anglia; Australian
   National University
RP Ma, B (corresponding author), Beijing Inst Technol, Beijing Lab Intelligent Informa Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China.
EM huhongwei@bit.edu.cn; bma000@bit.edu.cn; shenjianbing@bit.edu.cn;
   hanqiu@cse.cuhk.edu.hk; ling.shao@uea.ac.uk; fatih.porikli@anu.edu.au
RI Shao, Ling/D-3535-2011; Shen, Jianbing/U-8796-2019
OI Shao, Ling/0000-0002-8264-6117; Shen, Jianbing/0000-0002-4109-8353
FU National Natural Science Foundation of China [61472036]; Beijing Natural
   Science Foundation [4182056]; Australian Research Council's Discovery
   Projects funding scheme [DP150104645]; Specialized Fund for Joint
   Building Program of Beijing Municipal Education Commission
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61472036, in part by the Beijing Natural
   Science Foundation under Grant 4182056, in part by the Australian
   Research Council's Discovery Projects funding scheme under Grant
   DP150104645 and in part by Specialized Fund for Joint Building Program
   of Beijing Municipal Education Commission. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Xavier Giro-i-Nieto.
CR [Anonymous], P INT C MACH LEARN
   [Anonymous], 2009, P ADV NEUR INF PROC
   [Anonymous], 2016, CVPR
   [Anonymous], 2014, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2013.230
   [Anonymous], IEEE T PATTERN ANAL
   Bai YC, 2012, PROC CVPR IEEE, P1854, DOI 10.1109/CVPR.2012.6247884
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Dong-Hyun Lee, 2013, ICML WORKSH CHALL RE, P1
   Grandvalet Y, 2004, Advances in neural information processing systems, V17
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hong S, 2015, PR MACH LEARN RES, V37, P597
   Kim HU, 2015, IEEE I CONF COMP VIS, P3011, DOI 10.1109/ICCV.2015.345
   Kingma D. P., 2014, Advances in neural information processing systems, P3581
   Kolesnikov A, 2016, LECT NOTES COMPUT SC, V9908, P695, DOI 10.1007/978-3-319-46493-0_42
   Kristan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P564, DOI 10.1109/ICCVW.2015.79
   Lafferty J. D., 2003, P INT C MACH LEARN, P912, DOI DOI 10.5555/3041838.3041953
   Lienhart R, 2002, IEEE IMAGE PROC, P900
   Ma B, 2015, IEEE T MULTIMEDIA, V17, P1818, DOI 10.1109/TMM.2015.2463221
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292
   Ning JF, 2016, PROC CVPR IEEE, P4266, DOI 10.1109/CVPR.2016.462
   Qi YK, 2018, IEEE T IMAGE PROCESS, V27, P3857, DOI 10.1109/TIP.2018.2797482
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen JB, 2019, IEEE T CYBERNETICS, V49, P1990, DOI 10.1109/TCYB.2018.2803217
   Shen JB, 2018, IEEE T INTELL TRANSP, V19, P162, DOI 10.1109/TITS.2017.2750082
   Shen JB, 2016, IEEE T IMAGE PROCESS, V25, P5933, DOI 10.1109/TIP.2016.2616302
   Shen JB, 2014, IEEE T IMAGE PROCESS, V23, P1451, DOI 10.1109/TIP.2014.2302892
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Wang LJ, 2018, IEEE T CYBERNETICS, V48, P1030, DOI 10.1109/TCYB.2017.2675910
   Wang LJ, 2016, PROC CVPR IEEE, P1373, DOI 10.1109/CVPR.2016.153
   Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xu K, 2016, IEEE COMPUT SOC CONF, P1324, DOI 10.1109/CVPRW.2016.167
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
NR 48
TC 39
Z9 40
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2019
VL 21
IS 2
BP 510
EP 521
DI 10.1109/TMM.2018.2859831
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HJ7GT
UT WOS:000457364400019
DA 2024-07-18
ER

PT J
AU Chu, WQ
   Xue, HY
   Yao, CW
   Cai, D
AF Chu, Wenqing
   Xue, Hongyang
   Yao, Chengwei
   Cai, Deng
TI Sparse Coding Guided Spatiotemporal Feature Learning for Abnormal Event
   Detection in Large Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video analysis; unsupervised feature learning; sparse coding; anomaly
   detection
ID ANOMALY DETECTION; CLASSIFICATION; RECOGNITION; TRACKING; ONLINE; SCALE
AB Abnormal event detection in large videos is an important task in research and industrial applications, which has attracted considerable attention in recent years. Existing methods usually solve this problem by extracting local features and then learning an outlier detection model on training videos. However, most previous approaches merely employ hand-crafted visual features, which is a clear disadvantage due to their limited representation capacity. In this paper, we present a novel unsupervised deep feature learning algorithm for the abnormal event detection problem. To exploit the spatiotemporal information of the inputs, we utilize the deep three-dimensional convolutional network (C3D) to perform feature extraction. Then, the key problem is how to train the C3D network without any category labels. Here, we employ the sparse coding results of the hand-crafted features generated from the inputs to guide the unsupervised feature learning. Specifically, we define a multilevel similarity relationship between these inputs according to the statistical information of the shared atoms. In the following, we introduce the quadruplet concept to model the multilevel similarity structure, which could be used to construct a generalized triplet loss for training the C3D network. Furthermore, the C3D network could be utilized to generate the features for sparse coding again, and this pipeline could be iterated for several times. By jointly optimizing between the sparse coding and the unsupervised feature learning, we can obtain robust and rich feature representations. Based on the learned representations, the sparse reconstruction error is applied to predicting the anomaly score of each testing input. Experiments on several publicly available video surveillance datasets in comparison with a number of existing works demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods.
C1 [Chu, Wenqing; Xue, Hongyang; Cai, Deng] Zhejiang Univ, State Key Lab CAD&CG, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
   [Yao, Chengwei] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Yao, CW (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
EM wqchu16@gmail.com; hyxue@outlook.com; yaochw@zju.edu.cn;
   dengcai@gmail.com
RI 薛, 弘扬/JCE-9569-2023
OI 薛, 弘扬/0000-0003-3161-3566
FU National Nature Science Foundation of China [61751307]; National Youth
   Top-notch Talent Support Program
FX This work was supported in part by the National Nature Science
   Foundation of China under Grant 61751307 and in part by the National
   Youth Top-notch Talent Support Program.
CR Adam A, 2008, IEEE T PATTERN ANAL, V30, P555, DOI 10.1109/TPAMI.2007.70825
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], P EUR C COMPUT VIS
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], 2017, P IEEE INT C COMP VI
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], P EUR C COMPUT VIS
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, BMVC
   [Anonymous], 2011, P ICCV
   Basharat A., 2008, PROC IEEE C COMPUT V, P1
   Benezeth Y., 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2458, DOI 10.1109/CVPRW.2009.5206686
   Bertini M, 2012, COMPUT VIS IMAGE UND, V116, P320, DOI 10.1016/j.cviu.2011.09.009
   Bin Zhao, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3313, DOI 10.1109/CVPR.2011.5995524
   Chong YS, 2017, LECT NOTES COMPUT SC, V10262, P189, DOI 10.1007/978-3-319-59081-3_23
   Chu WQ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1561
   Cong Y, 2011, PROC CVPR IEEE, P1807, DOI 10.1109/CVPR.2011.5995434
   Coppi D, 2016, IEEE T CIRC SYST VID, V26, P762, DOI 10.1109/TCSVT.2015.2416555
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Dutta JK, 2015, AAAI CONF ARTIF INTE, P3755
   Hamid R, 2005, PROC CVPR IEEE, P1031
   Han TT, 2017, IEEE T MULTIMEDIA, V19, P712, DOI 10.1109/TMM.2016.2631881
   Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86
   Vu H, 2017, LECT NOTES ARTIF INT, V10234, P641, DOI 10.1007/978-3-319-57454-7_50
   i Nieto Xavier Giro, 2016, ADV NEURAL INFORM PR
   Jaechul Kim, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2921, DOI 10.1109/CVPRW.2009.5206569
   Jerripothula KR, 2016, LECT NOTES COMPUT SC, V9911, P187, DOI 10.1007/978-3-319-46478-7_12
   Jiang F, 2011, COMPUT VIS IMAGE UND, V115, P323, DOI 10.1016/j.cviu.2010.10.008
   Jiang F, 2009, IEEE T IMAGE PROCESS, V18, P907, DOI 10.1109/TIP.2008.2012070
   Jiang YG, 2015, IEEE T MULTIMEDIA, V17, P1174, DOI 10.1109/TMM.2015.2436813
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Khatoonabadi SH, 2013, IEEE T IMAGE PROCESS, V22, P300, DOI 10.1109/TIP.2012.2214049
   Kratz L, 2009, PROC CVPR IEEE, P1446, DOI 10.1109/CVPRW.2009.5206771
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   Li D, 2016, LECT NOTES COMPUT SC, V9908, P678, DOI 10.1007/978-3-319-46493-0_41
   Li J, 2010, INT J COMPUT VISION, V90, P150, DOI 10.1007/s11263-010-0354-6
   Li Y, 2016, IEEE T IMAGE PROCESS, V25, P5905, DOI 10.1109/TIP.2016.2616297
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338
   Mahadevan V, 2010, PROC CVPR IEEE, P1975, DOI 10.1109/CVPR.2010.5539872
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Piciarelli C, 2008, IEEE T CIRC SYST VID, V18, P1544, DOI 10.1109/TCSVT.2008.2005599
   Roshtkhari MJ, 2013, PROC CVPR IEEE, P2611, DOI 10.1109/CVPR.2013.337
   Roshtkhari MJ, 2013, COMPUT VIS IMAGE UND, V117, P1436, DOI 10.1016/j.cviu.2013.06.007
   Sabokrou M, 2017, IEEE T IMAGE PROCESS, V26, P1992, DOI 10.1109/TIP.2017.2670780
   Saligrama V, 2012, PROC CVPR IEEE, P2112, DOI 10.1109/CVPR.2012.6247917
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Soomro K., 2012, ARXIV12120402CS
   Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677
   Sun LF, 2017, IEEE T MULTIMEDIA, V19, P609, DOI 10.1109/TMM.2016.2635589
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang L., 2016, P ECCV
   Wang XG, 2006, LECT NOTES COMPUT SC, V3953, P110, DOI 10.1007/11744078_9
   Wu SD, 2010, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2010.5539882
   Xiang T, 2008, IEEE T PATTERN ANAL, V30, P893, DOI 10.1109/TPAMI.2007.70731
   Xie YL, 2011, PROC CVPR IEEE, P25, DOI 10.1109/CVPR.2011.5995648
   Xinyi Cui, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3161, DOI 10.1109/CVPR.2011.5995558
   Yan Y, 2016, IEEE T MULTIMEDIA, V18, P2494, DOI 10.1109/TMM.2016.2602938
   Yang JW, 2016, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR.2016.556
   Yang Y, 2014, IEEE T MULTIMEDIA, V16, P1677, DOI 10.1109/TMM.2014.2323014
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023
   Yuan J, 2016, PROC CVPR IEEE, P3093, DOI 10.1109/CVPR.2016.337
   Zhang D, 2005, PROC CVPR IEEE, P611
   Zhang TZ, 2009, PROC CVPR IEEE, P1940, DOI 10.1109/CVPRW.2009.5206809
   Zhang XF, 2016, PROC CVPR IEEE, P1114, DOI 10.1109/CVPR.2016.126
   Zhu LC, 2017, PROC CVPR IEEE, P1339, DOI 10.1109/CVPR.2017.147
   Zhu LC, 2017, INT J COMPUT VISION, V124, P409, DOI 10.1007/s11263-017-1033-7
   Zhu XF, 2013, IEEE T MULTIMEDIA, V15, P633, DOI 10.1109/TMM.2012.2233723
NR 74
TC 79
Z9 81
U1 2
U2 40
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2019
VL 21
IS 1
BP 246
EP 255
DI 10.1109/TMM.2018.2846411
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HF5DV
UT WOS:000454253700021
DA 2024-07-18
ER

PT J
AU Ding, XY
   Chen, ZZ
AF Ding, Xiaoying
   Chen, Zhenzhong
TI Improving Saliency Detection Based on Modeling Photographer's Intention
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Saliency; attention; intention; intention rate
ID REGION DETECTION; VISUAL-ATTENTION; OBJECT DETECTION; BOTTOM-UP; SUN
AB A photographer's intention towards a photo evokes the viewer's attentional response. By analyzing the photographer's intention, we can have a better understanding of what the photographers want to convey to the viewers, thus helping to improve image analyzing and understanding. In this paper, a novel method is presented to improve saliency detection based on exploring the relationship between the photographer's intention and the viewer's attention. An intention rate is derived to quantify the intention and integrated with traditional saliency detection in a unified framework accordingly. We evaluate the proposed scheme with several classic saliency detection algorithms on different datasets. The experimental results demonstrate the superior improvements of our method.
C1 [Ding, Xiaoying; Chen, Zhenzhong] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Hubei, Peoples R China.
C3 Wuhan University
RP Chen, ZZ (corresponding author), Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Hubei, Peoples R China.
EM dingxiaoying@whu.edu.cn; zzchen@ieee.org
RI Chen, Zhenzhong/C-2529-2015
OI Ding, xiaoying/0000-0001-7244-2594
FU National Natural Science Foundation of China [61471273, 61771348]; Wuhan
   Morning Light Plan of Youth Science and Technology [2017050304010302]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61471273 and 61771348, and in part by
   the Wuhan Morning Light Plan of Youth Science and Technology under Grant
   2017050304010302.
CR Achanta R, 2008, LECT NOTES COMPUT SC, V5008, P66
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226
   [Anonymous], 2016, P 2016 ACM MULT C
   [Anonymous], 2015, P 2015 IEEE 17 INT W
   [Anonymous], 2015, ARXIV150501173
   [Anonymous], 2016, AS C COMP VIS
   [Anonymous], 2009, ADV NEURAL INFORM PR
   Armitage E, 2015, DEV PSYCHOL, V51, P1201, DOI 10.1037/a0039571
   Borji A., 2012, CVPR, DOI DOI 10.1109/CVPR.2012.6247706
   Borji A., 2015, P CVPR WORKSH FUT DA
   Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727
   Borji Ali., 2012, Proceedings of the AAAI Conference on Artificial Intelligence, V26, P1529
   Bruce N., 2010, Journal of Vision, V7, P950, DOI [10.1167/7.9.950, DOI 10.1167/7.9.950]
   Chen ZZ, 2013, IEEE SIGNAL PROC LET, V20, P95, DOI 10.1109/LSP.2012.2230442
   Crete F, 2007, PROC SPIE, V6492, DOI 10.1117/12.702790
   Do E.Y., 2000, DESIGN STUD, V21, P483, DOI DOI 10.1016/S0142-694X(00)00020-X
   Dong YY, 2016, IEEE T MULTIMEDIA, V18, P549, DOI 10.1109/TMM.2016.2522639
   Fang C, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1105, DOI 10.1145/2647868.2654979
   Fang YM, 2015, INFORM SCIENCES, V309, P1, DOI 10.1016/j.ins.2015.03.004
   Fang YM, 2012, IEEE T IMAGE PROCESS, V21, P3888, DOI 10.1109/TIP.2012.2199126
   Fang YM, 2012, IEEE T MULTIMEDIA, V14, P187, DOI 10.1109/TMM.2011.2169775
   Fu KR, 2017, IEEE T MULTIMEDIA, V19, P1531, DOI 10.1109/TMM.2017.2679898
   Gao DS, 2009, IEEE T PATTERN ANAL, V31, P989, DOI 10.1109/TPAMI.2009.27
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Gopalakrishnan V, 2009, IEEE T MULTIMEDIA, V11, P892, DOI 10.1109/TMM.2009.2021726
   Hae Jong Seo, 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P45, DOI 10.1109/CVPR.2009.5204207
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hou X., 2007, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2007.383267
   Huang F, 2017, IEEE T IMAGE PROCESS, V26, P1911, DOI 10.1109/TIP.2017.2669878
   Huang K., 2006, Advances in neural information processing systems, V19, P609, DOI DOI 10.7551/MITPRESS/7503.001.0001
   Imamoglu N, 2013, IEEE T MULTIMEDIA, V15, P96, DOI 10.1109/TMM.2012.2225034
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jetley S, 2016, PROC CVPR IEEE, P5753, DOI 10.1109/CVPR.2016.620
   Jiang HZ, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.110
   Jiang QP, 2018, IEEE T MULTIMEDIA, V20, P2035, DOI 10.1109/TMM.2017.2763321
   Jiang QP, 2018, IEEE T CYBERNETICS, V48, P1276, DOI 10.1109/TCYB.2017.2690452
   Jiang ZL, 2013, PROC CVPR IEEE, P2043, DOI 10.1109/CVPR.2013.266
   Judd T., 2012, A benchmark of computational models of saliency to predict human fixations
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kanan C, 2009, VIS COGN, V17, P979, DOI 10.1080/13506280902771138
   Kimchi R, 2008, PSYCHOL SCI, V19, P660, DOI 10.1111/j.1467-9280.2008.02140.x
   KOSE G, 1985, BRIT J DEV PSYCHOL, V3, P373, DOI 10.1111/j.2044-835X.1985.tb00989.x
   Kümmerer M, 2017, IEEE I CONF COMP VIS, P4799, DOI 10.1109/ICCV.2017.513
   Li GB, 2016, IEEE T IMAGE PROCESS, V25, P5012, DOI 10.1109/TIP.2016.2602079
   Li L, 2013, IEEE MULTIMEDIA, V20, P13, DOI 10.1109/MMUL.2013.15
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Li Z., 2009, P ACM C MULT, P517
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu Z, 2012, IEEE T IMAGE PROCESS, V21, P4204, DOI 10.1109/TIP.2012.2200492
   Lux M., 2010, P 2010 ACM WORKSHOP, P41, DOI DOI 10.1145/1878061.1878075
   Lux M., 2012, P ACM MULT 2012 WORK, P17
   Lux M., 2010, CHI 10 HUM FACT COMP, P3913, DOI DOI 10.1145/1753846.1754078
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Mahadevan V, 2009, PROC CVPR IEEE, P1007, DOI 10.1109/CVPRW.2009.5206573
   Martinez Benjamin., 1995, Visual forces: an introduction to design
   Mei T, 2005, PROC SPIE, V5960, P268, DOI 10.1117/12.631387
   Mei T, 2007, IEEE T MULTIMEDIA, V9, P66, DOI 10.1109/TMM.2006.886357
   Ni BB, 2014, IEEE T MULTIMEDIA, V16, P1779, DOI 10.1109/TMM.2014.2329275
   Ni BB, 2013, IEEE T MULTIMEDIA, V15, P1138, DOI 10.1109/TMM.2013.2241042
   Niu YZ, 2012, IEEE T MULTIMEDIA, V14, P783, DOI 10.1109/TMM.2012.2186122
   Ong EP, 2003, SEVENTH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, VOL 1, PROCEEDINGS, P469, DOI 10.1109/ISSPA.2003.1224741
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Ramanishka V, 2017, PROC CVPR IEEE, P3135, DOI 10.1109/CVPR.2017.334
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Seo HJ, 2009, J VISION, V9, DOI 10.1167/9.12.15
   Sharples M., 2003, Visual Communication, V2, P303, DOI DOI 10.1177/14703572030023004
   Shi JP, 2015, PROC CVPR IEEE, P657, DOI 10.1109/CVPR.2015.7298665
   Tian HW, 2014, IEEE T IMAGE PROCESS, V23, P4389, DOI 10.1109/TIP.2014.2350914
   Tu WC, 2016, PROC CVPR IEEE, P2334, DOI 10.1109/CVPR.2016.256
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Webster M., 2004, Merriam-Webster's Collegiate Dictionary
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang JM, 2017, IEEE T PATTERN ANAL, V39, P576, DOI 10.1109/TPAMI.2016.2547384
   Yin WY, 2014, IEEE T MULTIMEDIA, V16, P184, DOI 10.1109/TMM.2013.2283468
   Yu JG, 2016, IEEE T MULTIMEDIA, V18, P273, DOI 10.1109/TMM.2015.2505908
   Zhang FL, 2013, IEEE T MULTIMEDIA, V15, P1480, DOI 10.1109/TMM.2013.2268051
   Zhang JM, 2013, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2013.26
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
NR 82
TC 9
Z9 9
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2019
VL 21
IS 1
BP 124
EP 134
DI 10.1109/TMM.2018.2851389
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA HF5DV
UT WOS:000454253700011
DA 2024-07-18
ER

PT J
AU Su, ZP
   Zhang, GF
   Yue, F
   Chang, LJ
   Jiang, JG
   Yao, X
AF Su, Zhaopin
   Zhang, Guofu
   Yue, Feng
   Chang, Lejie
   Jiang, Jianguo
   Yao, Xin
TI SNR-Constrained Heuristics for Optimizing the Scaling Parameter of
   Robust Audio Watermarking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio watermarking; spread spectrum; scaling parameter; constrained
   optimization; heuristic search
ID SCHEME; IMPERCEPTIBILITY; MASKING; IMAGE
AB In spread spectrum (SS) based robust audio watermarking, the scaling parameter is an important factor for balancing between robustness and imperceptibility. There have been intense studies of the embedded parameter optimization in light of the signal-to-noise ratio (SNR), but little attention has been given to the constrained SNR. Moreover, traditional population-based stochastic search algorithms for optimizing the embedded parameter significantly increase the computation pressure of the corresponding audio watermarking schemes. This paper comprehensively investigates the effect of the constrained SNR on the optimization of the scaling parameter, from both model and algorithmic perspectives. Specifically, the empirical relationship between the scaling parameter, robustness, and imperceptibility is first analyzed in detail. Next, an SNR-constrained optimization model is presented. Then, to solve the proposed model and find the current optimal scaling parameter for watermark embedding, a binary search algorithm and a heuristic search (HS) algorithm are, respectively, developed. Finally, we embed the proposed model and heuristics in the SS-based audio watermarking scheme and compare the integrated technique (called SS-SNR-HS) with the existing similar schemes. The experimental results demonstrate that SS-SNR-HS not only is computationally simple, but also achieves better balance between imperceptibility and robustness and, thus, seems promising in copyright protection of online digital audio.
C1 [Su, Zhaopin; Zhang, Guofu; Yue, Feng; Chang, Lejie; Jiang, Jianguo] Hefei Univ Technol, Sch Comp & Informat, Hefei 230601, Anhui, Peoples R China.
   [Su, Zhaopin; Zhang, Guofu; Yue, Feng; Chang, Lejie; Jiang, Jianguo] Anhui Prov Key Lab Ind Safety & Emergency Technol, Hefei 230601, Anhui, Peoples R China.
   [Su, Zhaopin; Zhang, Guofu; Yue, Feng; Chang, Lejie; Jiang, Jianguo] Minist Educ, Engn Res Ctr Safety Crit Ind Measurement & Contro, Hefei 230601, Anhui, Peoples R China.
   [Yao, Xin] Southern Univ Sci & Technol, Dept Comp Sci & Engn, Shenzhen Key Lab Computat Intelligence, Shenzhen 518055, Peoples R China.
   [Yao, Xin] Univ Birmingham, Sch Comp Sci, CERCIA, Birmingham B15 2TT, W Midlands, England.
C3 Hefei University of Technology; Southern University of Science &
   Technology; University of Birmingham
RP Zhang, GF (corresponding author), Hefei Univ Technol, Sch Comp & Informat, Hefei 230601, Anhui, Peoples R China.; Zhang, GF (corresponding author), Anhui Prov Key Lab Ind Safety & Emergency Technol, Hefei 230601, Anhui, Peoples R China.
EM szp@hfut.edu.cn; zgf@hfut.edu.cn; yuefeng@hfut.edu.cn;
   ljchang@mail.hfut.edu.cn; jgjiang@hfut.edu.cn; xiny@sustc.edu.cn
RI YAO, XIN/W-2158-2018; Yue, Feng/S-6565-2017
OI YAO, XIN/0000-0001-8837-4442; Zhang, Guofu/0000-0002-6794-348X
FU National Natural Science Foundation of China [61573125, 61329302,
   61371155]; Engineering and Physical Sciences Research Council
   [EP/J017515/1]; Fundamental Research Funds for the Central Universities
   [JZ2017YYPY0232]; Anhui Provincial Natural Science Foundation
   [1608085MF131]; Science and Technology Innovation Committee Foundation
   of Shenzhen [ZDSYS201703031748284]; EPSRC [EP/J017515/1] Funding Source:
   UKRI
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61573125, Grant 61329302, and Grant
   61371155; in part by the Engineering and Physical Sciences Research
   Council under Grant EP/J017515/1; in part by the Fundamental Research
   Funds for the Central Universities under Grant JZ2017YYPY0232; in part
   by the Anhui Provincial Natural Science Foundation under Grant
   1608085MF131; and in part by the Science and Technology Innovation
   Committee Foundation of Shenzhen under Grant ZDSYS201703031748284. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Marco Grangetto.
CR Baras C, 2006, IEEE T AUDIO SPEECH, V14, P1772, DOI 10.1109/TASL.2006.879808
   Bassia P, 2001, IEEE T MULTIMEDIA, V3, P232, DOI 10.1109/6046.923822
   Bhat V, 2011, MULTIMED TOOLS APPL, V52, P369, DOI 10.1007/s11042-010-0515-1
   Chen ST, 2016, MULTIMED TOOLS APPL, V75, P4735, DOI 10.1007/s11042-015-2500-1
   Cormen T. H., 2009, Introduction to Algorithms, VSecond
   Deb K., 2001, MULTIOBJECTIVE OPTIM, DOI DOI 10.1109/TEVC.2002.804322
   Eiben AE, 2015, NATURE, V521, P476, DOI 10.1038/nature14544
   Erkücük S, 2006, IEEE T MULTIMEDIA, V8, P925, DOI 10.1109/TMM.2006.879879
   Fallahpour M, 2015, IEEE-ACM T AUDIO SPE, V23, P1273, DOI 10.1109/TASLP.2015.2430818
   Hong RC, 2017, IEEE T IMAGE PROCESS, V26, P4128, DOI 10.1109/TIP.2017.2710635
   Hong RC, 2016, IEEE T IMAGE PROCESS, V25, P1124, DOI 10.1109/TIP.2016.2514499
   Hua G, 2016, SIGNAL PROCESS, V128, P222, DOI 10.1016/j.sigpro.2016.04.005
   Hua G, 2015, IEEE-ACM T AUDIO SPE, V23, P227, DOI 10.1109/TASLP.2014.2387385
   Hwang MJ, 2018, IEEE T MULTIMEDIA, V20, P45, DOI 10.1109/TMM.2017.2721642
   Kalantari NK, 2009, IEEE T AUDIO SPEECH, V17, P1133, DOI 10.1109/TASL.2009.2019259
   Kang XG, 2011, IEEE T MULTIMEDIA, V13, P181, DOI 10.1109/TMM.2010.2098850
   Katzenbeisser SC, 2000, ART H COMP SCI LIBR, P17
   Khaldi K, 2013, IEEE T AUDIO SPEECH, V21, P675, DOI 10.1109/TASL.2012.2227733
   Ko BS, 2005, IEEE T MULTIMEDIA, V7, P212, DOI 10.1109/TMM.2005.843366
   Larbi SD, 2005, IEEE T SIGNAL PROCES, V53, P816, DOI 10.1109/TSP.2004.839899
   Lei BY, 2015, SIGNAL PROCESS, V113, P80, DOI 10.1016/j.sigpro.2014.11.007
   Lei BY, 2013, IEEE T AUDIO SPEECH, V21, P2368, DOI 10.1109/TASL.2013.2277929
   Lei BY, 2012, SIGNAL PROCESS, V92, P1985, DOI 10.1016/j.sigpro.2011.12.021
   Li W, 2006, IEEE T MULTIMEDIA, V8, P60, DOI 10.1109/TMM.2005.861291
   Lie WN, 2006, IEEE T MULTIMEDIA, V8, P46, DOI 10.1109/TMM.2005.861292
   Lin Y, 2011, IET SIGNAL PROCESS, V5, P623, DOI 10.1049/iet-spr.2010.0069
   Mohsenfar SM, 2015, MULTIMED TOOLS APPL, V74, P759, DOI 10.1007/s11042-013-1694-3
   Natgunanathan I, 2012, IEEE T AUDIO SPEECH, V20, P2232, DOI 10.1109/TASL.2012.2199111
   Nishimura R, 2012, IEEE T AUDIO SPEECH, V20, P2461, DOI 10.1109/TASL.2012.2203810
   Pérez-Freire L, 2009, IEEE T INF FOREN SEC, V4, P2, DOI 10.1109/TIFS.2008.2009603
   Robert A, 2005, IEEE T MULTIMEDIA, V7, P727, DOI 10.1109/TMM.2005.846781
   Sang T, 2001, IEEE T COMMUN, V49, P620, DOI 10.1109/26.917768
   Sprent P., 2000, APPL NONPARAMETRIC S, V3rd
   Wang XY, 2006, IEEE T SIGNAL PROCES, V54, P4835, DOI 10.1109/TSP.2006.881258
   Wang XY, 2009, IEEE MULTIMEDIA, V16, P60, DOI 10.1109/MMUL.2009.44
   Xiang SJ, 2007, IEEE T MULTIMEDIA, V9, P1357, DOI 10.1109/TMM.2007.906580
   Xiang Y, 2015, IEEE-ACM T AUDIO SPE, V23, P2228, DOI 10.1109/TASLP.2015.2476755
   Xiang Y, 2014, IEEE-ACM T AUDIO SPE, V22, P1413, DOI 10.1109/TASLP.2014.2328175
   Xiang Y, 2012, IEEE T INF FOREN SEC, V7, P383, DOI 10.1109/TIFS.2011.2173678
   Xiang Y, 2011, IEEE T MULTIMEDIA, V13, P2, DOI 10.1109/TMM.2010.2080668
NR 40
TC 19
Z9 24
U1 1
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2018
VL 20
IS 10
SI SI
BP 2631
EP 2644
DI 10.1109/TMM.2018.2812599
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GT9YH
UT WOS:000444903000008
DA 2024-07-18
ER

PT J
AU Do, TT
   Hoang, T
   Pomponiu, V
   Zhou, YR
   Chen, Z
   Cheung, NM
   Koh, D
   Tan, A
   Tan, SH
AF Thanh-Toan Do
   Tuan Hoang
   Pomponiu, Victor
   Zhou, Yiren
   Chen, Zhao
   Cheung, Ngai-Man
   Koh, Dawn
   Tan, Aaron
   Tan, Suat-Hoon
TI Accessible Melanoma Detection Using Smartphones and Mobile Image
   Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimedia-based healthcare; malignant melanoma (MM); mobile image
   analysis; feature selection; human-computer interface
ID MUTUAL INFORMATION; VISUAL-SEARCH; SKIN-LESIONS; DIAGNOSIS;
   CLASSIFICATION; FEATURES; MICROSCOPY; BORDER; RULE
AB We investigate the design of an entire mobile imaging system for early detection of melanoma. Different from previous work, we focus on smartphone-captured visible light images. Our design addresses two major challenges. First, images acquired using a smartphone under loosely-controlled environmental conditions may be subject to various distortions, and this makes melanoma detection more difficult. Second, processing performed on a smartphone is subject to stringent computation and memory constraints. In our work, we propose a detection system that is optimized to run entirely on the resource-constrained smartphone. Our system intends to localize the skin lesion by combining a lightweight method for skin detection with a hierarchical segmentation approach using two fast segmentation methods. Moreover, we study an extensive set of image features and propose new numerical features to characterize a skin lesion. Furthermore, we propose an improved feature selection algorithm to determine a small set of discriminative features used by the final lightweight system. In addition, we study the human-computer interface (HCI) design to understand the usability and acceptance issues of the proposed system. Our extensive evaluation on an image dataset provided by National Skin Center - Singapore (117 benign nevi and 67 malignant melanoma) confirms the effectiveness of the proposed system for melanoma detection: 89.09% sensitivity at specificity >= 90%.
C1 [Thanh-Toan Do; Tuan Hoang; Pomponiu, Victor; Zhou, Yiren; Chen, Zhao; Cheung, Ngai-Man; Koh, Dawn] Singapore Univ Technol & Design, Singapore 487372, Singapore.
   [Tan, Aaron; Tan, Suat-Hoon] Natl Skin Ctr, Singapore 308205, Singapore.
C3 Singapore University of Technology & Design; National Skin Centre
RP Cheung, NM (corresponding author), Singapore Univ Technol & Design, Singapore 487372, Singapore.
EM thanh-toan.do@adelaide.edu.au; hoangnguyenanhtuan123456789@gmail.com;
   victor.pomponiu@gmail.com; yiren_zhou@mymail.sutd.edu.sg;
   zhaoch21@gmail.com; ngaiman_cheung@sutd.edu.sg; dawn_koh@sutd.edu.sg;
   aarontan@nsc.gov.sg; shtan@nsc.gov.sg
OI NGUYEN ANH TUAN, HOANG/0000-0002-1076-8043; Zhao, Jason
   Chen/0000-0002-0840-8022; Cheung, Ngai-Man (Man)/0000-0003-0135-3791
CR Alaa AM, 2016, IEEE T MULTIMEDIA, V18, P1942, DOI 10.1109/TMM.2016.2589160
   [Anonymous], MEL SIGNS SYMPT
   [Anonymous], [No title captured]
   [Anonymous], P IEEE 13 INT S BIOM
   [Anonymous], MOB MED APPL GUID IN
   [Anonymous], CHI2010 P 28 ANN
   [Anonymous], 2015, DERMOSCOPY IMAGE ANA
   [Anonymous], DERMOSCOPY IMAGE ANA
   [Anonymous], INT J SIGNAL IMAGE P
   [Anonymous], MATLAB IMPLEMENTATIO
   [Anonymous], MATLAB IMPLEMENTATIO
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Argenziano G, 1998, ARCH DERMATOL, V134, P1563, DOI 10.1001/archderm.134.12.1563
   Bao P, 2006, IEEE T MULTIMEDIA, V8, P382, DOI 10.1109/TMM.2005.864337
   BATTITI R, 1994, IEEE T NEURAL NETWOR, V5, P537, DOI 10.1109/72.298224
   Celebi ME, 2007, COMPUT MED IMAG GRAP, V31, P362, DOI 10.1016/j.compmedimag.2007.01.003
   Celebi ME, 2013, SKIN RES TECHNOL, V19, pE252, DOI 10.1111/j.1600-0846.2012.00636.x
   Chen T, 2014, IEEE T MULTIMEDIA, V16, P612, DOI 10.1109/TMM.2014.2301978
   Cricri F, 2014, IEEE T MULTIMEDIA, V16, P917, DOI 10.1109/TMM.2014.2307552
   Do Thanh-Toan, 2014, Annu Int Conf IEEE Eng Med Biol Soc, V2014, P6752, DOI 10.1109/EMBC.2014.6945178
   Doukas C, 2012, IEEE ENG MED BIO, P2444, DOI 10.1109/EMBC.2012.6346458
   Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056
   Estévez PA, 2009, IEEE T NEURAL NETWOR, V20, P189, DOI 10.1109/TNN.2008.2005601
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Fukunaga K., 1990, Introduction to StatisticalPattern Recognition
   Ganster H, 2001, IEEE T MED IMAGING, V20, P233, DOI 10.1109/42.918473
   Garnavi R, 2012, IEEE T INF TECHNOL B, V16, P1239, DOI 10.1109/TITB.2012.2212282
   Giotis I, 2015, EXPERT SYST APPL, V42, P6578, DOI 10.1016/j.eswa.2015.04.034
   Giotis I, 2013, J MATH IMAGING VIS, V47, P79, DOI 10.1007/s10851-012-0356-9
   Giotis I, 2012, NEUROCOMPUTING, V81, P33, DOI 10.1016/j.neucom.2011.10.018
   Girod B, 2011, IEEE SIGNAL PROC MAG, V28, P61, DOI 10.1109/MSP.2011.940881
   Gu YL, 2014, PROC SPIE, V9120, DOI 10.1117/12.2046112
   Gulshan V, 2010, PROC CVPR IEEE, P3129, DOI 10.1109/CVPR.2010.5540073
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Haider S, 2014, IEEE ENG MED BIO, P6455, DOI 10.1109/EMBC.2014.6945106
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Jafari MH, 2016, IEEE IMAGE PROC, P2638, DOI 10.1109/ICIP.2016.7532837
   Jafari MH, 2017, INT J COMPUT ASS RAD, V12, P1021, DOI 10.1007/s11548-017-1567-8
   Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198
   Kawahara J, 2016, I S BIOMED IMAGING, P1397, DOI 10.1109/ISBI.2016.7493528
   La Torre E, 2006, STUD HEALTH TECHNOL, V124, P983
   Li CM, 2010, IEEE T IMAGE PROCESS, V19, P3243, DOI 10.1109/TIP.2010.2069690
   Li HQ, 2013, IEEE T MULTIMEDIA, V15, P594, DOI 10.1109/TMM.2012.2234730
   Liu F, 2013, IEEE T MULTIMEDIA, V15, P1025, DOI 10.1109/TMM.2013.2244204
   Lu YA, 2015, IEEE SIGNAL PROC LET, V22, P534, DOI 10.1109/LSP.2014.2357015
   Maglogiannis I, 2009, IEEE T INF TECHNOL B, V13, P721, DOI 10.1109/TITB.2009.2017529
   Maier T, 2015, J EUR ACAD DERMATOL, V29, P663, DOI 10.1111/jdv.12648
   Marghoob AA, 2009, J INVEST DERMATOL, V129, P11, DOI 10.1038/jid.2008.388
   Menzies SW, 1996, MELANOMA RES, V6, P55, DOI 10.1097/00008390-199602000-00008
   Min WQ, 2014, IEEE T MULTIMEDIA, V16, P623, DOI 10.1109/TMM.2014.2302744
   Munia TTK, 2017, IEEE ENG MED BIO, P4281, DOI 10.1109/EMBC.2017.8037802
   Nanni L, 2010, ARTIF INTELL MED, V49, P117, DOI 10.1016/j.artmed.2010.02.006
   Nejati H, 2016, IEEE SIGNAL PROC MAG, V33, P30, DOI 10.1109/MSP.2016.2549996
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pomponiu V, 2016, IEEE IMAGE PROC, P2623, DOI 10.1109/ICIP.2016.7532834
   Ramlakhan K, 2011, PROC INT C TOOLS ART, P138, DOI 10.1109/ICTAI.2011.29
   Ribeiro MX, 2008, IEEE T MULTIMEDIA, V10, P277, DOI 10.1109/TMM.2007.911837
   Sadri AR, 2013, IEEE T BIO-MED ENG, V60, P1134, DOI 10.1109/TBME.2012.2227478
   SIEGEL RL, 2016, CA-CANCER J CLIN, V66, P7, DOI DOI 10.3322/CAAC.21654
   Wadhawan T, 2011, IEEE ENG MED BIO, P3180, DOI 10.1109/IEMBS.2011.6090866
   Walther D, 2006, NEURAL NETWORKS, V19, P1395, DOI 10.1016/j.neunet.2006.10.001
   Wang F, 2009, PATTERN RECOGN, V42, P2863, DOI 10.1016/j.patcog.2009.04.015
   Wolf JA, 2013, JAMA DERMATOL, V149, P422, DOI 10.1001/jamadermatol.2013.2382
   Xie FY, 2016, IEEE T BIO-MED ENG, V63, P1248, DOI 10.1109/TBME.2015.2493580
   Zhou WG, 2014, IEEE T MULTIMEDIA, V16, P601, DOI 10.1109/TMM.2014.2301979
   Zhou YR, 2018, IEEE T CIRC SYST VID, V28, P46, DOI 10.1109/TCSVT.2016.2600261
NR 67
TC 34
Z9 35
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2018
VL 20
IS 10
SI SI
BP 2849
EP 2864
DI 10.1109/TMM.2018.2814346
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GT9YH
UT WOS:000444903000025
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, J
   Peng, YX
AF Zhang, Jian
   Peng, Yuxin
TI Query-Adaptive Image Retrieval by Deep-Weighted Hashing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep weighted hashing; query-adaptive; image retrieval
ID CODE RANKING; SIMILARITY; SEARCH; SCENE
AB Hashing methods have attracted much attention for large-scale image retrieval. Some deep hashing methods have achieved promising results by taking advantage of the strong representation power of deep networks recently. However, existing deep hashing methods treat all hash hits equally. On one hand, a large number of images share the same distance to a query image due to the discrete Hamming distance, which raises a critical issue of image retrieval where fine-grained rankings are very important. On the other hand, different hash bits actually contribute to the image retrieval differently, and treating them equally greatly affects the retrieval accuracy. To address the above two problems, we propose the query-adaptive deep weighted hashing approach, which can perform fine-grained ranking for different queries by weighted Hamming distance. First, a novel deep hashing network is proposed to learn the hash codes and corresponding classwise weights jointly, so that the learned weights can reflect the importance of different hash bits for different image classes. Second, a query-adaptive image retrieval method is proposed, which rapidly generates hash bit weights for different query images by fusing its semantic probability and the learned classwise weights. Fine-grained image ranking is then performed by the weighted Hamming distance, which can provide more accurate ranking than the traditional Hamming distance. Experiments on four widely used datasets show that the proposed approach outperforms eight state-of-the-art hashing methods.
C1 [Zhang, Jian; Peng, Yuxin] Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.
C3 Peking University
RP Peng, YX (corresponding author), Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.
EM pengyuxin@pku.edu.cn
RI Peng, Yuxin/U-7376-2019; peng, yu/GXW-2071-2022
FU National Natural Science Foundation of China [61771025, 61532005]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61771025 and Grant 61532005.
CR [Anonymous], 2016, IJCAI
   [Anonymous], 2009, NIPS
   [Anonymous], 2009, NEURIPS
   Chechik G, 2010, J MACH LEARN RES, V11, P1109
   Chen ZX, 2017, IEEE T MULTIMEDIA, V19, P123, DOI 10.1109/TMM.2016.2620604
   Chua T.-S., 2014, P ACM INT C IM VID R
   Ding K, 2017, IEEE T MULTIMEDIA, V19, P571, DOI 10.1109/TMM.2016.2625747
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Hao YB, 2017, IEEE T MULTIMEDIA, V19, P1, DOI 10.1109/TMM.2016.2610324
   Hinton G. E., 2012, 12070580 ARXIV
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Irie G, 2014, PROC CVPR IEEE, P2123, DOI 10.1109/CVPR.2014.272
   Ji TX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1005, DOI 10.1145/2647868.2655018
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang QY, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2248
   Jiang YG, 2013, IEEE T MULTIMEDIA, V15, P442, DOI 10.1109/TMM.2012.2231061
   Jiang Yu-Gang., 2011, Proceedings of the 1st ACM International Conference on Multimedia Retrieval, page, P16
   Kafai M, 2014, IEEE T MULTIMEDIA, V16, P1090, DOI 10.1109/TMM.2014.2305633
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Lee D.-H., 2013, WORKSHOP CHALLENGES, V3, P896
   Li P, 2013, IEEE T MULTIMEDIA, V15, P141, DOI 10.1109/TMM.2012.2199970
   Li WJ, 2016, IJCAI, P1711
   Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1109/PLASMA.2013.6634954, 10.1017/S1368980013002176]
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Liu XL, 2016, IEEE T IMAGE PROCESS, V25, P907, DOI 10.1109/TIP.2015.2505180
   Lv Q., 2007, P 33 INT C VER LARG, P950
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sablayrolles A, 2017, INT CONF ACOUST SPEE, P1732, DOI 10.1109/ICASSP.2017.7952453
   Schütt M, 2004, PEPTIDE REVOLUTION: GENOMICS, PROTEOMICS & THERAPEUTICS, P41
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang Jianfeng., 2013, ACM Multimedia, P133
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang Jun., 2010, ICML, P1127
   Wang QF, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3911
   Weiss Yair, 2009, Advances in Neural Information Processing Systems, P1753, DOI DOI 10.5555/2981780.2981999
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Zhang L, 2013, PROC CVPR IEEE, P1586, DOI 10.1109/CVPR.2013.208
   Zhang L, 2013, PROCEEDINGS OF THE EIGHTH INTERNATIONAL SYMPOSIUM ON VITICULTURE AND ENOLOGY (2013), P123
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
   Zhang X, 2012, PROC CVPR IEEE, P2058, DOI 10.1109/CVPR.2012.6247910
   Zhang YD, 2014, IEEE T MULTIMEDIA, V16, P1127, DOI 10.1109/TMM.2014.2306392
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
NR 51
TC 40
Z9 42
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2018
VL 20
IS 9
BP 2400
EP 2414
DI 10.1109/TMM.2018.2804763
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GR1ZX
UT WOS:000442358200013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, LM
   Liu, MF
   Chen, L
   Qiu, LX
   Zhang, C
   Hu, YX
   Zimmermann, R
AF Zhang, Luming
   Liu, Maofu
   Chen, Lei
   Qiu, Lanxin
   Zhang, Chao
   Hu, Yuxing
   Zimmermann, Roger
TI Online Modeling of Esthetic Communities Using Deep Perception Graph
   Analytics
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Aesthetic community; deep perception graph; visual perception; online
   learning; gaze shifting
AB Accurately detecting esthetic communities from a large number of Internet users (e.g., Flickr(1) or Picasa(2) users) is a useful technique that can facilitate several applications, such as image retargeting, visual esthetic assessment, and fashion recommendation. Conventional approaches cannot appropriately handle this task due to the following challenges: first, it is difficult to online update the detected esthetic communities since these photos may uploaded/removed frequently; second, human visual perception is essential to describe esthetic characteristics, but integrating it into an existing mining algorithm is challenging; and third, flat models cannot encode human visual perception precisely, especially for those sophisticated sceneries. To solve these problems, we propose deep perception graph analytics, an incremental pipeline where the esthetic relations among users are described by utilizing their gaze shifting paths (GSPs). Specifically, we first propose an aggregation-based deep network that formulates GSP representation into a unified framework. Afterward, the deep perception graph is constructed where the esthetic discrepancy between users is measured by their GSP distributions. Accordingly, we adopt a dense subgraph discovery algorithm that efficiently detects the communities belonging to each esthetic style. Finally, an online Gaussian mixture model (GMM) learning model is designed, which dynamically updates the GMM parameters in order to describe esthetic communities given the photos are uploaded/removed on the fly. Experiments on a million-scale image set crawled from Flickr demonstrate the efficiency and effectiveness of our method.
C1 [Zhang, Luming] Hefei Univ Technol, Dept CSIE, Hefei 230009, Anhui, Peoples R China.
   [Liu, Maofu] Wuhan Univ Sci & Technol, Coll Comp Sci & Technol, Wuhan, Hubei, Peoples R China.
   [Chen, Lei] State Grid Zhejiang Elect Power Co, Hangzhou 310007, Zhejiang, Peoples R China.
   [Qiu, Lanxin] State Grid Zhejiang Elect Power Co, Informat & Telecommun Branch, Hangzhou 310007, Peoples R China.
   [Zhang, Chao] Univ Illinois, Dept Comp Sci, Champaign, IL 61801 USA.
   [Hu, Yuxing] Tsinghua Univ, Sch Aerosp Engn, Beijing 100084, Peoples R China.
   [Zimmermann, Roger] Natl Univ Singapore, Sch Comp, Singapore 19077, Singapore.
C3 Hefei University of Technology; Wuhan University of Science &
   Technology; State Grid Corporation of China; State Grid Corporation of
   China; University of Illinois System; University of Illinois
   Urbana-Champaign; Tsinghua University; National University of Singapore
RP Hu, YX (corresponding author), Tsinghua Univ, Sch Aerosp Engn, Beijing 100084, Peoples R China.
EM zglumg@nus.edu.sg; liumaofu@wust.edu.cn; leichen@grid.com;
   chengl@126.com; chaozhangzju@gmail.com; yuxinghu@gmail.com;
   rogerz@comp.nus.edu.sg
RI Lei, Ming/JAD-1050-2023; Zimmermann, Roger/D-7944-2015; Zhang,
   Chao/AAR-7251-2020; zhang, lu/GRO-2969-2022
OI Zimmermann, Roger/0000-0002-7410-2590; Zhang, Chao/0000-0003-3009-598X; 
FU National Natural Science Foundation of China [61572169, 61472266];
   National University of Singapore (Suzhou) Research Institute, Suzhou,
   China; Fundamental Research Funds for the Central Universities
FX This research was supported in part by the National Natural Science
   Foundation of China under Grant 61572169 and Grant 61472266; in part by
   the National University of Singapore (Suzhou) Research Institute,
   Suzhou, China; and in part by the Fundamental Research Funds for the
   Central Universities.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Ahn YY, 2010, NATURE, V466, P761, DOI 10.1038/nature09182
   [Anonymous], PROC ACM INT CONF
   [Anonymous], 2004, PHYS REV E, DOI DOI 10.1103/PHYSREVE.69.066133
   [Anonymous], 2010, ACM MULTIMEDIA
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Cai D., 2005, PROC INT JOINT C ART
   Castelhano MS, 2009, J VISION, V9, DOI 10.1167/9.3.6
   Chen YW, 2006, STUD FUZZ SOFT COMP, V207, P315
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Frank M, 2012, J MACH LEARN RES, V13, P459
   Gregory S, 2008, LECT NOTES ARTIF INT, V5211, P408, DOI 10.1007/978-3-540-87479-9_45
   Guo YW, 2009, IEEE T MULTIMEDIA, V11, P856, DOI 10.1109/TMM.2009.2021781
   Hong RC, 2016, IEEE T MULTIMEDIA, V18, P1555, DOI 10.1109/TMM.2016.2567071
   JOHNSON SC, 1967, PSYCHOMETRIKA, V32, P241, DOI 10.1007/BF02289588
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Kendall MG, 1940, BIOMETRIKA, V31, P324, DOI DOI 10.1093/BIOMET/31.3-4.324
   Kimura Akisato., 2013, ACM Multimedia, P565, DOI DOI 10.1145/2502081.2502149
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lancichinetti A, 2008, PHYS REV E, V78, DOI 10.1103/PhysRevE.78.046110
   Li L.-j., 2010, NIPS
   Li YX, 2012, IEEE T MULTIMEDIA, V14, P1618, DOI 10.1109/TMM.2012.2199292
   Lin SS, 2013, IEEE T MULTIMEDIA, V15, P359, DOI 10.1109/TMM.2012.2228475
   Liu H., 2010, Proceedings of the 27th International Conference on Machine Learning (ICML-10), P671
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Luo W, 2011, IEEE I CONF COMP VIS, P2206, DOI 10.1109/ICCV.2011.6126498
   Ma ZG, 2012, IEEE T MULTIMEDIA, V14, P1662, DOI 10.1109/TMM.2012.2199293
   MacKay D J C, 2002, Information Theory, Inference andLearning Algorithms
   Mahoney Michael, 2010, P 19 INT C WORLD WID, P631, DOI DOI 10.1145/1772690.1772755
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   Marchesotti L, 2015, INT J COMPUT VISION, V113, P246, DOI 10.1007/s11263-014-0789-2
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Negoescu RA, 2010, IEEE T MULTIMEDIA, V12, P399, DOI 10.1109/TMM.2010.2050649
   Ni BB, 2013, IEEE T MULTIMEDIA, V15, P1138, DOI 10.1109/TMM.2013.2241042
   Nishiyama M., 2009, P 17 ACM INT C MULTI, P669
   Ordonez C, 2005, KNOWL INF SYST, V7, P135, DOI 10.1007/s10115-003-0141-6
   Papadimitriou S, 2008, LECT NOTES ARTIF INT, V5212, P170, DOI 10.1007/978-3-540-87481-2_12
   Press W. H., 2007, NUM REC ART SCI COMP
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   STRICKER M, 1995, P SOC PHOTO-OPT INS, V2410, P381, DOI 10.1117/12.205308
   Wang W, 2016, IEEE MULTIMEDIA, V23, P80, DOI 10.1109/MMUL.2016.69
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Xu J, 2014, J VISION, V14, DOI 10.1167/14.1.28
   Yang J, 2012, IEEE DATA MINING, P1170, DOI 10.1109/ICDM.2012.139
   Yang TB, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P927
   Yoshida T, 2010, IEEE IMAGE PROC, P349, DOI 10.1109/ICIP.2010.5651018
   Zhang LJ, 2011, IEEE T PATTERN ANAL, V33, P2026, DOI 10.1109/TPAMI.2011.20
   Zhang LM, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2303650
   Zhang LM, 2014, IEEE T IMAGE PROCESS, V23, P2235, DOI 10.1109/TIP.2014.2311658
   Zhang N, 2014, PROC CVPR IEEE, P1637, DOI 10.1109/CVPR.2014.212
   Zhang SL, 2013, IEEE T IMAGE PROCESS, V22, P2889, DOI 10.1109/TIP.2013.2251650
   Zhang YZ, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P997
NR 57
TC 8
Z9 8
U1 2
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1462
EP 1474
DI 10.1109/TMM.2017.2769799
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400014
DA 2024-07-18
ER

PT J
AU Liu, C
   Liu, P
   Zhao, W
   Tang, XL
AF Liu, Chang
   Liu, Peng
   Zhao, Wei
   Tang, Xianglong
TI Robust Tracking and Redetection: Collaboratively Modeling the Target and
   Its Context
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Collaborative modeling; fusion of tracking results; Re-detection; target
   tracking
ID VISUAL TRACKING; OBJECT TRACKING
AB Robust object tracking and redetection require stably predicting the trajectory of the target object and recovering from tracking failure by quickly redetecting it when it is lost during long-term tracking. The locations of the target and the background are calculated relative to the region occupied by the object. The effect of tracking can be enhanced by isolating the target and the background, modeling and tracking them, respectively, and integrating their tracking results. In this study, we propose an approach that builds motion models for the target and its context. Tracking results from a target tracker and a context tracker are integrated through linear fusion to predict the position of the target. A kernelized correlation filter tracker is used to track the target in the predicted position. When the target is lost, it can be quickly recovered by searching in the given field of view using a target model built and updated through observation models that are constructed prior to the loss of the target. Our approach is not sensitive to the segmentation of the target and the context. The motion models and observation models of the target and the context work together in the tracking process, whereas the target model alone is involved in redetection. Experiments to test our proposed approach, which simultaneously models the target and its context, showed that it can effectively enhance the robustness of long-term tracking.
C1 [Liu, Chang; Liu, Peng; Zhao, Wei; Tang, Xianglong] Harbin Inst Technol, Dept Comp Sci, Harbin 150001, Heilongjiang, Peoples R China.
C3 Harbin Institute of Technology
RP Zhao, W (corresponding author), Harbin Inst Technol, Dept Comp Sci, Harbin 150001, Heilongjiang, Peoples R China.
EM 15b903045@hit.edu.cn; pengliu@hit.edu.cn; zhaowei@hit.edu.cn;
   tangxl@hit.edu.cn
RI Liu, Pengju/U-3399-2019
OI Liu, Pengju/0000-0001-8413-9621; Liu, Chang/0000-0002-1228-873X
FU National Natural Science Foundation of China [61671175]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61671175. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Yap-Peng
   Tan.
CR [Anonymous], 2014, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2013.230
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd
   Bao CL, 2012, PROC CVPR IEEE, P1830, DOI 10.1109/CVPR.2012.6247881
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Briechle K, 2001, PROC SPIE, V4387, P95, DOI 10.1117/12.421129
   Chen Z., 2015, Comput. Sci., V53, P68
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kristan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P564, DOI 10.1109/ICCVW.2015.79
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Leistner C., 2008, IEEE COMPUTER VISION, P1, DOI DOI 10.1109/CVPR.2008.4587629
   Li HX, 2016, IEEE T IMAGE PROCESS, V25, P1834, DOI 10.1109/TIP.2015.2510583
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   Liu Q, 2014, IET COMPUT VIS, V8, P419, DOI 10.1049/iet-cvi.2013.0134
   Liu S, 2016, PROC CVPR IEEE, P4312, DOI 10.1109/CVPR.2016.467
   Liu T, 2015, PROC CVPR IEEE, P4902, DOI 10.1109/CVPR.2015.7299124
   Lu D., 2008, P INT S COMP INT DES, V2, P195
   Ma B, 2016, IEEE T IMAGE PROCESS, V25, P5867, DOI 10.1109/TIP.2016.2615812
   Ma B, 2016, IEEE T CYBERNETICS, V46, P2411, DOI 10.1109/TCYB.2015.2477879
   Ma B, 2016, IEEE T IMAGE PROCESS, V25, P4199, DOI 10.1109/TIP.2016.2588329
   Ma B, 2015, IEEE I CONF COMP VIS, P4400, DOI 10.1109/ICCV.2015.500
   Ma B, 2015, IEEE T MULTIMEDIA, V17, P1818, DOI 10.1109/TMM.2015.2463221
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Mei X, 2011, PROC CVPR IEEE, P1257
   Oron S, 2012, PROC CVPR IEEE, P1940, DOI 10.1109/CVPR.2012.6247895
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Wang N., 2013, Advances in Neural Information Processing Systems, P809
   Wang NY, 2015, IEEE I CONF COMP VIS, P3101, DOI 10.1109/ICCV.2015.355
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yang HX, 2011, NEUROCOMPUTING, V74, P3823, DOI 10.1016/j.neucom.2011.07.024
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Yuan Y, 2015, IEEE T MULTIMEDIA, V17, P1125, DOI 10.1109/TMM.2015.2440996
   Zhang KH, 2014, LECT NOTES COMPUT SC, V8693, P127, DOI 10.1007/978-3-319-10602-1_9
   Zhang KH, 2014, IEEE T PATTERN ANAL, V36, P2002, DOI 10.1109/TPAMI.2014.2315808
   Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908
   Zhou Y, 2016, INT J COMPUT VISION, V118, P337, DOI 10.1007/s11263-015-0879-9
NR 45
TC 18
Z9 18
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2018
VL 20
IS 4
BP 889
EP 902
DI 10.1109/TMM.2017.2760633
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FZ5HM
UT WOS:000427623000010
DA 2024-07-18
ER

PT J
AU Zhu, CJ
   Yu, L
   Xiong, ZX
AF Zhu, Changjian
   Yu, Li
   Xiong, Zixiang
TI A Noncoverage Field Model for Improving the Rendering Quality of Virtual
   Views
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image-based rendering; virtual view; camera; geometric configuration;
   rendering quality assessment
ID LIGHT FIELDS; COMPRESSION; IMAGES; VIDEO; TV
AB Rendering quality optimization theory is one of the most basic and fascinating components of image-based rendering (IBR). The rendering quality of virtual views is related to the information in the source images. Capturing the image information depends on the geometric configuration of the camera (GCC) and, in particular, the positions and shooting directions of the cameras. Therefore, the rendering quality of virtual views can be improved by optimizing the GCC. This paper investigates the relationship between the GCC and the geometric configurations of the virtual view (GCVV). The influence of the GU: on the rendering quality is also analyzed. Based on the relationships among the GCC, GCVV, and rendering quality, a mathematical model of the noncoverage field (NCF) is proposed to quantify the rendering quality. The performance of the NCF is also analyzed using a set of GCVVs and GCCs. Furthermore, an optimization algorithm based on the NCF that simultaneously optimizes the position and direction of the GCC is presented. The proposed technique can be applied to obtain the optimal rendering quality of IBR for the linear case of camera positions and virtual views. Finally, experimental results are presented and compared with the theoretical results.
C1 [Zhu, Changjian; Yu, Li] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Hubei, Peoples R China.
   [Xiong, Zixiang] Texas A&M Univ, Dept Elect & Comp Engn, College Stn, TX 77840 USA.
   [Xiong, Zixiang] Monash Univ, Dept Elect & Comp Syst Engn, Clayton, Vic 3168, Australia.
C3 Huazhong University of Science & Technology; Texas A&M University
   System; Texas A&M University College Station; Monash University
RP Yu, L (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Hubei, Peoples R China.
EM changjianzhu@hust.edu.cn; hustlyu@hust.edu.cn; zx@ece.tamu.edu
FU National Natural Science Foundation of China [61231010]; 863 High-Tech
   Research and Development Program [2015AA015903]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61231010, and in part by the 863
   High-Tech Research and Development Program under Grant 2015AA015903.
CR Berent J, 2007, IEEE SIGNAL PROC MAG, V24, P34, DOI 10.1109/MSP.2007.905883
   BEZDEK JC, 1987, IEEE T SYST MAN CYB, V17, P873, DOI 10.1109/TSMC.1987.6499296
   BEZDEK JC, 1980, IEEE T PATTERN ANAL, V2, P1, DOI 10.1109/TPAMI.1980.4766964
   BOLLES RC, 1987, INT J COMPUT VISION, V1, P7, DOI 10.1007/BF00128525
   Buehler C, 2001, COMP GRAPH, P425, DOI 10.1145/383259.383309
   Chai JX, 2000, COMP GRAPH, P307, DOI 10.1145/344779.344932
   Chen C, 2009, IEEE IMAGE PROC, P3769, DOI 10.1109/ICIP.2009.5414363
   Cho D, 2017, IEEE T PATTERN ANAL, V39, P1504, DOI 10.1109/TPAMI.2016.2606397
   CHOE HW, 1992, IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS, P349, DOI 10.1109/FUZZY.1992.258640
   Davis A, 2012, COMPUT GRAPH FORUM, V31, P305, DOI 10.1111/j.1467-8659.2012.03009.x
   Do MN, 2012, IEEE T IMAGE PROCESS, V21, P708, DOI 10.1109/TIP.2011.2163895
   Do MN, 2011, IEEE SIGNAL PROC MAG, V28, P58, DOI 10.1109/MSP.2010.939075
   Fleishman S, 2000, COMPUT GRAPH FORUM, V19, P101, DOI 10.1111/1467-8659.00447
   Gilliam C, 2014, IEEE T IMAGE PROCESS, V23, P502, DOI 10.1109/TIP.2013.2292363
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Hedman P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982420
   Isaksen A, 2000, COMP GRAPH, P297, DOI 10.1145/344779.344929
   Jain R, 2016, IEEE T THZ SCI TECHN, V6, P649, DOI 10.1109/TTHZ.2016.2584861
   Kubota A, 2007, IEEE SIGNAL PROC MAG, V24, P10, DOI 10.1109/MSP.2007.905873
   Lee Z, 2014, IEEE T MULTIMEDIA, V16, P2168, DOI 10.1109/TMM.2014.2355131
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Lim H, 2014, IEEE T MULTIMEDIA, V16, P726, DOI 10.1109/TMM.2014.2299771
   Liu SX, 2009, IMAGING SCI J, V57, P250, DOI 10.1179/136821909X12476507838352
   Maugey T, 2013, IEEE T MULTIMEDIA, V15, P1070, DOI 10.1109/TMM.2013.2246147
   Navarro J, 2017, IEEE T IMAGE PROCESS, V26, P1873, DOI 10.1109/TIP.2017.2666041
   Ng KT, 2012, IEEE T MULTIMEDIA, V14, P1631, DOI 10.1109/TMM.2012.2199291
   Nguyen HT, 2009, IEEE T IMAGE PROCESS, V18, P703, DOI 10.1109/TIP.2009.2012884
   PAL NR, 1995, IEEE T FUZZY SYST, V3, P370, DOI 10.1109/91.413225
   Safaei F., 2013, PROC IEEE INT C MULT, P1
   Schirmacher H, 1999, COMPUT GRAPH FORUM, V18, pC151, DOI 10.1111/1467-8659.00336
   Shidanshidi H., 2011, ICME, P1
   Shidanshidi H., 2011, MMSP, P1
   Shidanshidi H, 2015, IEEE T MULTIMEDIA, V17, P1677, DOI 10.1109/TMM.2015.2447274
   Shum H.-Y., 2007, IMAGE BASED RENDERIN, P71
   Shum HY, 2003, IEEE T CIRC SYST VID, V13, P1020, DOI 10.1109/TCSVT.2003.817360
   Snavely N, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360614
   Takahashi K., 2005, P INT C AUT ROB AG, P1
   Takahashi K, 2006, SIGNAL PROCESS-IMAGE, V21, P519, DOI 10.1016/j.image.2006.03.001
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Tao MW, 2017, IEEE T PATTERN ANAL, V39, P546, DOI 10.1109/TPAMI.2016.2554121
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109
   Vagharshakyan S, 2018, IEEE T PATTERN ANAL, V40, P133, DOI 10.1109/TPAMI.2017.2653101
   Vaish Vaibhav., 2004, CVPR
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Werner T., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P73, DOI 10.1109/ICPR.1996.545994
   Zangwill WI, 1967, Manag Sci, V13, P344, DOI [10.1287/mnsc.13.5.344, DOI 10.1287/MNSC.13.5.344]
   Zhang C, 2004, SIGNAL PROCESS-IMAGE, V19, P1, DOI 10.1016/j.image.2003.07.001
   Zhang C, 2003, IEEE T CIRC SYST VID, V13, P1038, DOI 10.1109/TCSVT.2003.817350
   Zhang C, 2007, IEEE T MULTIMEDIA, V9, P520, DOI 10.1109/TMM.2006.888010
NR 50
TC 5
Z9 5
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2018
VL 20
IS 3
BP 738
EP 753
DI 10.1109/TMM.2017.2750409
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FW5ZH
UT WOS:000425397500018
DA 2024-07-18
ER

PT J
AU Ferrari, C
   Lisanti, G
   Berretti, S
   Del Bimbo, A
AF Ferrari, Claudio
   Lisanti, Giuseppe
   Berretti, Stefano
   Del Bimbo, Alberto
TI A Dictionary Learning-Based 3D Morphable Shape Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action unit detection; dictionary learning; dense correspondence;
   emotion recognition; 3D morphable model
ID FACIAL EXPRESSION RECOGNITION; FACE RECOGNITION
AB Face analysis from 2D images and videos is a central task in many multimedia applications. Methods developed to this end perform either face recognition or facial expression recognition, and in both cases results are negatively influenced by variations in pose, illumination, and resolution of the face. Such variations have a lower impact on 3D face data, which has given the way to the idea of using a 3D morphable model as an intermediate tool to enhance face analysis on 2D data. In this paper, we propose a new approach for constructing a 3D morphable shape model (called DL-3DMM) and show our solution can reach the accuracy of deformation required in applications where fine details of the face are concerned. For constructing the model, we start from a set of 3D face scans with large variability in terms of ethnicity and expressions. Across these training scans, we compute a point-to-point dense alignment, which is accurate also in the presence of topological variations of the face. The DL-3DMM is constructed by learning a dictionary of basis components on the aligned scans. The model is then fitted to 2D target faces using an efficient regularized ridge-regression guided by 2D/3D facial landmark correspondences in order to generate pose-normalized face images. Comparison between the DL-3DMM and the standard PCA-based 3DMM demonstrates that in general a lower reconstruction error can be obtained with our solution. Application to action unit detection and emotion recognition from 2D images and videos shows competitive results with state of the art methods on two benchmark datasets.
C1 [Ferrari, Claudio; Lisanti, Giuseppe; Berretti, Stefano; Del Bimbo, Alberto] Univ Florence, Dept Informat Engn, I-50139 Florence, Italy.
C3 University of Florence
RP Ferrari, C (corresponding author), Univ Florence, Dept Informat Engn, I-50139 Florence, Italy.
EM claudio.ferrari@unifi.it; giuseppe.lisanti@unifi.it;
   stefano.berretti@unifi.it; alberto.delbimbo@unifi.it
RI Berretti, Stefano/U-9004-2019; Lisanti, Giuseppe/AAG-8699-2020; Ferrari,
   Claudio/X-3742-2019
OI Berretti, Stefano/0000-0003-1219-4386; Lisanti,
   Giuseppe/0000-0002-0785-9972; Ferrari, Claudio/0000-0001-9465-6753
FU Office of the Director of National Intelligence (ODNI), Intelligence
   Advanced Research Projects Activity (IARPA) via IARPA [2014-14071600011]
FX This work was supported in part by the Office of the Director of
   National Intelligence (ODNI), Intelligence Advanced Research Projects
   Activity (IARPA), via IARPA Contract 2014-14071600011. The views and
   conclusions contained herein are those of the authors and should not be
   interpreted as necessarily representing the official policies or
   endorsements, either expressed or implied, of ODNI, IARPA, or the U.S.
   Government. The U.S. Government is authorized to reproduce and
   distribute reprints for Governmental purpose notwithstanding any
   copyright annotation thereon. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Cha
   Zhang. (Corresponding author: Claudio Ferrari.)
CR Amberg Brian, 2007, CVPR '07. IEEE Conference on Computer Vision and Pattern Recognition, P1
   Amberg B., 2008, PROC 8 IEEE INT C AU, P1
   [Anonymous], IEEE T AFFE IN PRESS
   [Anonymous], P COMPUT SCI
   [Anonymous], COMP INT MULT UND IW
   [Anonymous], 2016, CORR
   [Anonymous], 2015, P IEEE INT C AUT FAC, DOI DOI 10.1109/FG.2015.7163082
   [Anonymous], 2015, P FAC AN AN FAA 2015
   [Anonymous], 2016, CORR
   [Anonymous], FACIAL EXPRESSION RE
   [Anonymous], 2016, CORR
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983
   Blanz V, 2003, COMPUT GRAPH FORUM, V22, P641, DOI 10.1111/1467-8659.t01-1-00712
   Booth J, 2016, PROC CVPR IEEE, P5543, DOI 10.1109/CVPR.2016.598
   Brunton A, 2014, LECT NOTES COMPUT SC, V8689, P297, DOI 10.1007/978-3-319-10590-1_20
   Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389
   Chu WS, 2013, PROC CVPR IEEE, P3515, DOI 10.1109/CVPR.2013.451
   Cosker D., 2010, P S APPL PERCEPTION, P101, DOI DOI 10.1145/1836248.1836268
   Cosker D, 2011, IEEE I CONF COMP VIS, P2296, DOI 10.1109/ICCV.2011.6126510
   De La Torre F, 2009, P 2009 3 INT C AFF C, P1, DOI 10.1109/ACII.2009.5349358
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Farkas LG, 1994, Anthropometry of Head and Face, Vsecond
   Ferrari C, 2016, INT C PATT RECOG, P1047, DOI 10.1109/ICPR.2016.7899774
   Ferrari C, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P509, DOI 10.1109/3DV.2015.63
   Georgakis C, 2016, IEEE T IMAGE PROCESS, V25, P2021, DOI 10.1109/TIP.2016.2539502
   Hjortsjo C.-H., 1969, MANS FACE MIMIC LANG
   Hu GS, 2016, LECT NOTES COMPUT SC, V9912, P73, DOI 10.1007/978-3-319-46484-8_5
   Jabon ME, 2011, IEEE PERVAS COMPUT, V10, P84, DOI 10.1109/MPRV.2010.46
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Kimmel R, 1998, P NATL ACAD SCI USA, V95, P8431, DOI 10.1073/pnas.95.15.8431
   Li HX, 2015, PROC CVPR IEEE, P4055, DOI 10.1109/CVPR.2015.7299032
   Li YQ, 2013, IEEE T AFFECT COMPUT, V4, P127, DOI 10.1109/T-AFFC.2013.5
   Lu XG, 2008, IEEE T PATTERN ANAL, V30, P1346, DOI 10.1109/TPAMI.2007.70784
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Mairal J., 2009, P 26 ANN INT C MACHI, P689, DOI 10.1145/1553374.1553463
   Masi I, 2016, PROC CVPR IEEE, P4838, DOI 10.1109/CVPR.2016.523
   Masi I, 2014, INT C PATT RECOG, P4477, DOI 10.1109/ICPR.2014.766
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Patel A, 2009, PROC CVPR IEEE, P1327, DOI 10.1109/CVPRW.2009.5206522
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Perakis P, 2013, IEEE T PATTERN ANAL, V35, P1552, DOI 10.1109/TPAMI.2012.247
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   Piatkowska E, 2012, LECT NOTES COMPUT SC, V7208, P147
   Kassim SRA, 2006, IEEE IMAGE PROC, P661
   Romdhani S, 2005, PROC CVPR IEEE, P986
   Sandbach G, 2012, IMAGE VISION COMPUT, V30, P683, DOI 10.1016/j.imavis.2012.06.005
   Shahlaei Davoud, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163128
   Staal FCR, 2015, J CRANIO MAXILL SURG, V43, P528, DOI 10.1016/j.jcms.2015.02.005
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tawari A, 2013, IEEE T MULTIMEDIA, V15, P1543, DOI 10.1109/TMM.2013.2266635
   Valstar MF, 2012, IEEE T SYST MAN CY B, V42, P966, DOI 10.1109/TSMCB.2012.2200675
   Wang YJ, 2012, IEEE T MULTIMEDIA, V14, P597, DOI 10.1109/TMM.2012.2189550
   Wang ZH, 2013, IEEE I CONF COMP VIS, P3304, DOI 10.1109/ICCV.2013.410
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu Y, 2016, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2016.370
   Xiangyu Zhu, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163096
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yi D, 2013, PROC CVPR IEEE, P3539, DOI 10.1109/CVPR.2013.454
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Zen G, 2016, IEEE T MULTIMEDIA, V18, P775, DOI 10.1109/TMM.2016.2523421
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhang L, 2005, COMPUTER GRAPHICS INTERNATIONAL 2005, PROCEEDINGS, P11
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhen QK, 2016, IEEE T MULTIMEDIA, V18, P1438, DOI 10.1109/TMM.2016.2557063
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
NR 66
TC 21
Z9 24
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2017
VL 19
IS 12
BP 2666
EP 2679
DI 10.1109/TMM.2017.2707341
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FN8HG
UT WOS:000416263200003
DA 2024-07-18
ER

PT J
AU Varini, P
   Serra, G
   Cucchiara, R
AF Varini, Patrizia
   Serra, Giuseppe
   Cucchiara, Rita
TI Personalized Egocentric Video Summarization of Cultural Tour on User
   Preferences Input
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computer vision; feedforward neural networks; knowledge discovery
AB In this paper, we propose a new method for customized summarization of egocentric videos according to specific user preferences, so that different users can extract different summaries from the same stream. Our approach, tailored on a cultural heritage scenario, relies on creating a short synopsis of the original video focused on key shots, in which concepts relevant to user preferences can be visually detected and the chronological flow of the original video is preserved. Moreover, we release a new dataset, composed of egocentric streams taken in uncontrolled scenarios, capturing tourists cultural visits in six art cities, with geolocalization information. Our experimental results show that the proposed approach is able to leverage user's preferences with an accent on storyline chronological flow and on visual smoothness.
C1 [Varini, Patrizia; Cucchiara, Rita] Univ Modena & Reggio Emilia, Dept Engn Enzo Ferrari, I-41100 Modena, Italy.
   [Serra, Giuseppe] Univ Udine, Dept Math Comp Sci & Phys, I-33100 Udine, Italy.
C3 Universita di Modena e Reggio Emilia; University of Udine
RP Serra, G (corresponding author), Univ Udine, Dept Math Comp Sci & Phys, I-33100 Udine, Italy.
EM patrizia.varini@unimore.it; giuseppe.serra@uniud.it;
   rita.cucchiara@unimore.it
RI Cucchiara, Rita/L-3006-2015; Serra, Giuseppe/M-3572-2015
OI Serra, Giuseppe/0000-0002-4269-4501
CR [Anonymous], 2016 IEEE Winter Conf. Appl. Comput. Vision, DOI [DOI 10.1109/WACV.2016.7477708, 10.1109/WACV.2016.7477708]
   Auer S, 2007, LECT NOTES COMPUT SC, V4825, P722, DOI 10.1007/978-3-540-76298-0_52
   Beineke LW, 2002, DISCRETE MATH, V252, P31, DOI 10.1016/S0012-365X(01)00180-7
   Bolaños M, 2017, IEEE T HUM-MACH SYST, V47, P77, DOI 10.1109/THMS.2016.2616296
   Brandes U, 2005, LECT NOTES COMPUT SC, V3404, P533
   Chen F, 2011, IEEE T CIRC SYST VID, V21, P193, DOI 10.1109/TCSVT.2011.2106271
   Collobert R, 2008, P 25 ICML, P160, DOI 10.1145/1390156.1390177
   Crete F, 2007, PROC SPIE, V6492, DOI 10.1117/12.702790
   Cristianini N., 2000, INTRO SUPPORT VECTOR
   del Molino AG, 2017, IEEE T HUM-MACH SYST, V47, P65, DOI 10.1109/THMS.2016.2623480
   Dijkstra E. W., 1959, NUMER MATH, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   GYGLI M, 2015, PROC CVPR IEEE, P3090, DOI DOI 10.1109/CVPR.2015.7298928
   Gygli M, 2014, LECT NOTES COMPUT SC, V8695, P505, DOI 10.1007/978-3-319-10584-0_33
   Hagberg A. A., 2008, EXPLORING NETWORK ST, P11, DOI DOI 10.1016/J.JELECTROCARD.2010.09.003
   Henderson JM, 2007, CURR DIR PSYCHOL SCI, V16, P219, DOI 10.1111/j.1467-8721.2007.00507.x
   Hulpus I., 2013, P 6 ACM INT C WEB SE, P465
   Ji Z., 2010, P INT C SIGN PROC SY, DOI DOI 10.1109/ICSPS.2010.5555504
   Joshi N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766954
   Kamvar Sepandar D., 2003, P 12 INT C WORLD WID, P261, DOI DOI 10.1145/775152.775190
   Langville A.N., 2008, MATH INTELL, V30, P68, DOI DOI 10.1007/BF02985759
   Lee YJ, 2012, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2012.6247820
   Li BX, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P169
   Li BX, 2001, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES, PROCEEDINGS, P132, DOI 10.1109/IVL.2001.990867
   Lidon A., 2015, CORR
   Lin YL, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P443, DOI 10.1109/ICCVW.2015.65
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Lu Z, 2013, PROC CVPR IEEE, P2714, DOI 10.1109/CVPR.2013.350
   Mikolov Tomas, 2013, EFFICIENT ESTIMATION
   Newman MEJ, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.026113
   Ng HW, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P325, DOI 10.1109/ICME.2002.1035784
   Okamoto M., 2013, Pacific-Rim Symposium on Image and Video Technology, P431
   Peng YX, 2006, IEEE T CIRC SYST VID, V16, P612, DOI 10.1109/TCSVT.2006.873157
   Poleg Y, 2015, PROC CVPR IEEE, P4768, DOI 10.1109/CVPR.2015.7299109
   Sharghi A, 2016, LECT NOTES COMPUT SC, V9912, P3, DOI 10.1007/978-3-319-46484-8_1
   Shu TM, 2015, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR.2015.7299088
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun M, 2014, LECT NOTES COMPUT SC, V8695, P472, DOI 10.1007/978-3-319-10584-0_31
   Swetha S., 2016, P IEEE WINT APPL COM, P1
   Varini P., 2015, P 5 ACM INT C MULT R, P539
   Wang M, 2012, IEEE T MULTIMEDIA, V14, P975, DOI 10.1109/TMM.2012.2185041
   Xiong Z., 2006, IMAGE VIDEO PROCESSI
   Xu J, 2015, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2015.7298836
   Yao Ting, 2016, PROC CVPR IEEE, P982, DOI DOI 10.1109/CVPR.2016.112
   Zhang K, 2016, PROC CVPR IEEE, P1059, DOI 10.1109/CVPR.2016.120
   Zhang K, 2016, LECT NOTES COMPUT SC, V9911, P766, DOI 10.1007/978-3-319-46478-7_47
   Zhao B, 2014, PROC CVPR IEEE, P2513, DOI 10.1109/CVPR.2014.322
NR 49
TC 25
Z9 26
U1 2
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2017
VL 19
IS 12
BP 2832
EP 2845
DI 10.1109/TMM.2017.2705915
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FN8HG
UT WOS:000416263200015
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kao, CC
   Miao, YT
   Hsu, WC
AF Kao, Chih-Chen
   Miao, Yu-Tsung
   Hsu, Wei-Chung
TI A Pipeline-Based Ray-Tracing Runtime System for HSA-Compliant Frameworks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Heterogeneous system architecture (HSA); irregular program; ray-tracing;
   runtime technology; shared virtual memory
AB Ray-tracing has received great attention over the years due to the high demand for global illumination appliances. Due to its embarrassingly parallel characteristics, the ray-tracing algorithm has been ported to the graphics processing unit (GPU) on heterogeneous systems that run thousands of threads in a single-instruction-multiple-thread fashion. However, the irregularity of ray-tracing causes a performance penalty on the GPU. The control flow divergence and early-termination problems severely degrade the hardware utilization, which makes the GPU computation inefficient while traversing through each iteration of the algorithm. Furthermore, additional overheads caused by data marshalling and load unbalancing negate the benefits of using heterogeneous systems. To tackle these issues, we designed a pipeline-based runtime methodology that leverages the features of heterogeneous system architecture (HSA)-compliant heterogeneous frameworks, such as shared virtual memory and fast kernel dispatching. This method merges the workloads from different iteration stages and dispatches them simultaneously. The merged workload is further assigned to a heterogeneous queue to enhance load balancing and scalability. With the proposed technologies, the performance of ray-tracing is enhanced significantly while effectively increasing the utilization of HSA-compliant heterogeneous systems. Based on the experiment results, the throughput becomes 4.37 times greater than the original setup on average in a single GPU mode and would always yield a greater throughput with a heterogeneous queue on multiple cores.
C1 [Kao, Chih-Chen; Miao, Yu-Tsung; Hsu, Wei-Chung] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10617, Taiwan.
C3 National Taiwan University
RP Kao, CC (corresponding author), Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10617, Taiwan.
EM elros.morlin@gmail.com; r03922122@ntu.edu.tw; hsuwc@csie.ntu.edu.tw
OI Kao, Chih-Chen/0000-0002-7631-2284
CR Aila T., 2009, P C HIGH PERFORMANCE, P145, DOI [DOI 10.1145/1572769.1572792, 10.1145/1572769.1572792]
   [Anonymous], 2012, 2012 INNOVATIVE PARA, DOI DOI 10.1109/INPAR.2012.6339596
   Antwerpen D., 2011, P ACM SIGGRAPH S HIG, P41, DOI [10.1145/2018323.2018330, DOI 10.1145/2018323.2018330]
   Ashbaugh B., 2015, P 3 INT WORKSH OPENC
   Baghsorkhi SS, 2010, PPOPP 2010: PROCEEDINGS OF THE 2010 ACM SIGPLAN SYMPOSIUM ON PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING, P105, DOI 10.1145/1693453.1693470
   Barringer R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601222
   Benthin C, 2006, RT 06: IEEE SYMPOSIUM ON INTERACTIVE RAY TRACING 2006, PROCEEDINGS, P15
   Benthin C, 2012, IEEE T VIS COMPUT GR, V18, P1438, DOI 10.1109/TVCG.2011.277
   Boulos S, 2008, RT08: IEEE/EG SYMPOSIUM ON INTERACTIVE RAY TRACING 2008, PROCEEDINGS, P131, DOI 10.1109/RT.2008.4634633
   Costa Vasco, 2015, International Journal of Creative Interfaces and Computer Graphics, V6, P1, DOI 10.4018/IJCICG.2015070101
   Garanzha K, 2010, COMPUT GRAPH FORUM, V29, P289, DOI 10.1111/j.1467-8659.2009.01598.x
   Gelado I, 2008, ICS'08: PROCEEDINGS OF THE 2008 ACM INTERNATIONAL CONFERENCE ON SUPERCOMPUTING, P299
   Gupta K, 2015, PSYCHOL SEX, V6, P209, DOI 10.1080/19419899.2013.858644
   Hummel M. D., 2010, U.S. Patent, Patent No. [7 653 803, 7653803]
   Kaeli D., 2015, HETEROGENEOUS COMPUT, V3rd
   Kao C. C., 2016, P IEEE INT C MULT EX, P1
   Kao CC, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING (DSP), P100, DOI 10.1109/ICDSP.2015.7251838
   Laine S., 2013, 5 HIGH PERFORM GRAPH, P137, DOI [10.1145/2492045.2492060, DOI 10.1145/2492045.2492060]
   Momcilovic S, 2014, IEEE T MULTIMEDIA, V16, P108, DOI 10.1109/TMM.2013.2284892
   Mukherjee S, 2016, INT SYM PERFORM ANAL, P183, DOI 10.1109/ISPASS.2016.7482093
   Nocak J., 2010, PROC 31 ANN C EUR AS, P61
   Pajot A, 2011, COMPUT GRAPH FORUM, V30, P315, DOI 10.1111/j.1467-8659.2011.01863.x
   Parker SG, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778803
   Pharr M., 2010, PHYS BASED RENDERING
   Rogers P., 2013, HOT CHIPS, V25
   Sartori J, 2013, IEEE T MULTIMEDIA, V15, P279, DOI 10.1109/TMM.2012.2232647
   Tzeng Stanley, 2010, P C HIGH PERF GRAPH, P29
   Wald I, 2001, COMPUT GRAPH FORUM, V20, pC153, DOI 10.1111/1467-8659.00508
   Wald I.:., 2011, BOOK ACTIVE THREAD C, P51, DOI DOI 10.1145/2018323.2018331
   Wald I, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601199
NR 30
TC 1
Z9 3
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2017
VL 19
IS 11
BP 2450
EP 2462
DI 10.1109/TMM.2017.2697825
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FJ9CT
UT WOS:000413068200008
DA 2024-07-18
ER

PT J
AU Liu, ZG
   Zhang, LM
   Liu, Q
   Yin, YF
   Cheng, L
   Zimmermann, R
AF Liu, Zhenguang
   Zhang, Luming
   Liu, Qi
   Yin, Yifang
   Cheng, Li
   Zimmermann, Roger
TI Fusion of Magnetic and Visual Sensors for Indoor Localization:
   Infrastructure-Free and More Effective
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural network; indoor localization; magnetic field;
   particle filter; visual image
ID PERFORMANCE; VISION
AB Accurate and infrastructure-free indoor positioning can be very useful in a variety of applications. However, most existing approaches (e.g., WiFi and infrared-based methods) for indoor localization heavily rely on infrastructure, which is neither scalable nor pervasively available. In this paper, we propose a novel indoor localization and tracking approach, termed VMag, that does not require any infrastructure assistance. The user can be localized while simply holding a smartphone. To the best of our knowledge, the proposed method is the first exploration of fusing geomagnetic and visual sensing for indoor localization. More specifically, we conduct an in-depth study on both the advantageous properties and the challenges in leveraging the geomagnetic field and visual images for indoor localization. Based on these studies, we design a context-aware particle filtering framework to track the user with the goal of maximizing the positioning accuracy. We also introduce a neural-network-based method to extract deep features for the purpose of indoor positioning. We have conducted extensive experiments on four different indoor settings including a laboratory, a garage, a canteen, and an office building. Experimental results demonstrate the superior performance of VMag over the state of the art with these four indoor settings.
C1 [Liu, Zhenguang; Liu, Qi; Yin, Yifang; Zimmermann, Roger] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
   [Zhang, Luming] Hefei Univ Technol, Dept Comp & Informat, Hefei 230000, Peoples R China.
   [Zhang, Luming] Natl Univ Singapore, Suzhou Res Inst, Suzhou 215123, Peoples R China.
   [Cheng, Li] Natl Univ Singapore, Bioinformat Inst, Agcy Sci Technol & Res, Singapore 117417, Singapore.
   [Cheng, Li] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
C3 National University of Singapore; Hefei University of Technology;
   National University of Singapore; Agency for Science Technology &
   Research (A*STAR); A*STAR - Bioinformatics Institute (BII); National
   University of Singapore; National University of Singapore
RP Liu, ZG (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
EM zhenguangliu@zju.edu.cn; zglumg@gmail.com; qiliu@u.nus.edu;
   idmyiny@nus.edu.sg; chengli@bii.a-star.edu.sg; rogerz@comp.nus.edu.sg
RI Cheng, Li/AAU-6734-2020; Wang, Yiru/JMB-2281-2023; zhang,
   lu/GRO-2969-2022; Liu, Qi/AAE-3162-2019; Lei, Ming/JAD-1050-2023;
   Zimmermann, Roger/D-7944-2015
OI Cheng, Li/0000-0003-3261-3533; Liu, Qi/0000-0001-5378-6404; Zimmermann,
   Roger/0000-0002-7410-2590; yin, yifang/0000-0002-6525-6133
FU Singapore's Ministry of Education Academic Research Fund Tier 1 [T1
   251RES1415]; National Natural Science Foundation of China [61572169,
   61472266]; National University of Singapore Suzhou Research Institute,
   Suzhou; Fundamental Research Funds for the Central Universities
FX This work was supported by Singapore's Ministry of Education Academic
   Research Fund Tier 1 under Grant T1 251RES1415, by the National Natural
   Science Foundation of China under Grant 61572169 and Grant 61472266, by
   the National University of Singapore Suzhou Research Institute, Suzhou,
   and by the Fundamental Research Funds for the Central Universities. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Liang Zhou.
CR Alsehly Firas., 2011, Indoor Positioning and Indoor Navigation (IPIN), 2011 International Conference on, P1
   [Anonymous], 2015, Int. J. Eng. Res. Technol
   [Anonymous], 2016, TOMCCAP, DOI DOI 10.1016/J.YMPEV.2016.12.037
   [Anonymous], 2015, ARXIV PREPRINT ARXIV
   [Anonymous], 2012, PARTICLE FILTERING
   [Anonymous], P EUR SIGN PROC C
   [Anonymous], 2012, International Conference on Indoor Positioning and Indoor Navigation (IPIN), DOI DOI 10.1109/IPIN.2012.6418880
   [Anonymous], P INT C COMP VIS
   Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   Baniukevic A, 2013, 2013 IEEE 14TH INTERNATIONAL CONFERENCE ON MOBILE DATA MANAGEMENT (MDM 2013), VOL 1, P207, DOI 10.1109/MDM.2013.30
   Bisio I, 2013, IEEE T MULTIMEDIA, V15, P858, DOI 10.1109/TMM.2013.2239631
   Carrillo D, 2015, SENSORS-BASEL, V15, P17168, DOI 10.3390/s150717168
   Doulamis AD, 2003, IEEE T NEURAL NETWOR, V14, P150, DOI 10.1109/TNN.2002.806645
   Doulamis AD, 2000, IEEE T NEURAL NETWOR, V11, P137, DOI 10.1109/72.822517
   Doulamis N, 2000, REAL-TIME IMAGING, V6, P327, DOI 10.1006/rtim.1999.0185
   Doulamis N, 2014, IEEE IMAGE PROC, P848, DOI 10.1109/ICIP.2014.7025170
   Glorot X., 2010, P INT C ART INT STAT, P249
   Guan T, 2013, IEEE T MULTIMEDIA, V15, P1688, DOI 10.1109/TMM.2013.2265674
   Haverinen J, 2009, ROBOT AUTON SYST, V57, P1028, DOI 10.1016/j.robot.2009.07.018
   He X, 2015, SENSORS-BASEL, V15, P31464, DOI 10.3390/s151229867
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Kiliç V, 2015, IEEE T MULTIMEDIA, V17, P186, DOI 10.1109/TMM.2014.2377515
   Kim J, 2008, IEEE T CONSUM ELECTR, V54, P954, DOI 10.1109/TCE.2008.4637573
   Li F, 2012, UBICOMP'12: PROCEEDINGS OF THE 2012 ACM INTERNATIONAL CONFERENCE ON UBIQUITOUS COMPUTING, P421
   Li HY, 2015, SENSORS-BASEL, V15, P31244, DOI 10.3390/s151229850
   Ming Liu, 2013, International Journal of Distributed Sensor Networks, DOI 10.1155/2013/272916
   Minotto Vicente P., 2014, IEEE Transactions on Multimedia, V16, P1032, DOI 10.1109/TMM.2014.2305632
   Nguyen VV, 2012, KSII T INTERNET INF, V6, P835, DOI 10.3837/tiis.2012.03.004
   Papaioannou S, 2014, IEEE INT CONF MOB, P109, DOI 10.1109/MASS.2014.52
   Ravi N, 2006, SEVENTH IEEE WORKSHOP ON MOBILE COMPUTING SYSTEMS & APPLICATIONS, PROCEEDINGS, P19, DOI 10.1109/WMCSA.2006.11
   Shu YC, 2015, IEEE J SEL AREA COMM, V33, P1443, DOI 10.1109/JSAC.2015.2430274
   Subbu KP, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508054
   Wang W, 2016, DECIS SUPPORT SYST, V87, P80, DOI 10.1016/j.dss.2016.05.002
   Zhang C, 2015, IEEE T MOBILE COMPUT, V14, P387, DOI 10.1109/TMC.2014.2319824
   Zhang LM, 2016, IEEE T IMAGE PROCESS, V25, P553, DOI 10.1109/TIP.2015.2502147
   Zhang LM, 2013, IEEE T IMAGE PROCESS, V22, P5071, DOI 10.1109/TIP.2013.2278465
   Zhou BL, 2014, ADV NEUR IN, V27
NR 38
TC 91
Z9 96
U1 3
U2 103
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2017
VL 19
IS 4
BP 874
EP 888
DI 10.1109/TMM.2016.2636750
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EO0NT
UT WOS:000396395500017
DA 2024-07-18
ER

PT J
AU Zou, Q
   Ni, LH
   Wang, Q
   Hu, ZW
   Li, QQ
   Wang, S
AF Zou, Qin
   Ni, Lihao
   Wang, Qian
   Hu, Zhongwen
   Li, Qingquan
   Wang, Song
TI Local Pattern Collocations Using Regional Co-occurrence Factorization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Co-occurrence matrix; feature descriptor; object recognition; painting
   classification; pattern collocation
ID IMAGE RETRIEVAL; SCALE; CLASSIFICATION; FEATURES
AB Human vision benefits a lot from pattern collocations in visual activities such as object detection and recognition. Usually, pattern collocations display as the co-occurrences of visual primitives, e.g., colors, gradients, or textons, in neighboring regions. In the past two decades, many sophisticated local feature descriptors have been developed to describe visual primitives, and some of them even take into account the co-occurrence information for improving their discriminative power. However, most of these descriptors only consider feature co-occurrence within a very small neighborhood, e.g., 8-connected or 16-connected area, whichwould fall short in describing pattern collocations built up by feature cooccurrences in a wider neighborhood. In this paper, we propose to describe local pattern collocations by using a new and general regional co-occurrence approach. In this approach, an input image is first partitioned into a set of homogeneous superpixels. Then, features in each superpixel are extracted by a variety of local feature descriptors, based on which a number of patterns are computed. Finally, pattern co-occurrences within the superpixel and between the neighboring superpixels are calculated and factorized into a final descriptor for local pattern collocation. The proposed regional co-occurrence framework is extensively tested on a wide range of popular shape, color, and texture descriptors in terms of image and object categorizations. The experimental results have shown significant performance improvements by using the proposed framework over the existing popular descriptors.
C1 [Zou, Qin; Ni, Lihao; Wang, Qian] Wuhan Univ, Sch Comp Sci, State Key Lab Software Engn, Wuhan 430072, Peoples R China.
   [Hu, Zhongwen; Li, Qingquan] Shenzhen Univ, Shenzhen Key Lab Spatial Smart Sensing & Serv, Shenzhen 518060, Peoples R China.
   [Wang, Song] Univ South Carolina, Dept Comp Sci & Engn, Columbia, SC 29200 USA.
C3 Wuhan University; Shenzhen University; University of South Carolina
   System; University of South Carolina Columbia
RP Wang, Q (corresponding author), Wuhan Univ, Sch Comp Sci, State Key Lab Software Engn, Wuhan 430072, Peoples R China.
EM qzou@whu.edu.cn; lhni@whu.edu.cn; qianwang@whu.edu.cn; zwhoo@szu.edu.cn;
   liqq@szu.edu.cn; songwang@cec.sc.edu
RI Zou, Qin/AFM-0040-2022; Li, Mengqi/AAG-6804-2021; Zou,
   Qin/GVU-2237-2022; Hu, Zhongwen/AAX-7567-2021
OI Wang, Qian/0000-0002-8967-8525; Hu, Zhongwen/0000-0003-2689-3196; Zou,
   Qin/0000-0001-7955-0782; Wang, Song/0000-0003-4152-5295
FU National Basic Research Program of China [2012CB725303]; National
   Natural Science Foundation of China [61301277, 61672376, 41371431]
FX This work was supported by the National Basic Research Program of China
   under Grant 2012CB725303, and the National Natural Science Foundation of
   China under Grant 61301277, Grant 61672376, and Grant 41371431.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2008, IEEE C COMP VIS PATT, DOI [10.1109/CVPR.2008.4587799, DOI 10.1109/CVPR.2008.4587799]
   [Anonymous], CNSTR2011001 CAL I
   [Anonymous], 2012, ABST APPL ANAL
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2012, PASCAL VISUAL OBJECT
   [Anonymous], 2008, An Open and Portable Library of Computer Vision Algorithms
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2014, ACMMM
   Assari SM, 2014, PROC CVPR IEEE, P2529, DOI 10.1109/CVPR.2014.324
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Benavente R, 2008, J OPT SOC AM A, V25, P2582, DOI 10.1364/JOSAA.25.002582
   Chen G, 2013, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2013.235
   Chen M, 2009, IEEE IMAGE PROC, P289, DOI 10.1109/ICIP.2009.5413511
   Clausi DA, 2005, IEEE T IMAGE PROCESS, V14, P925, DOI 10.1109/TIP.2005.849319
   Condorovici RG, 2013, LECT NOTES COMPUT SC, V7944, P687
   Crandall D, 2004, PROC CVPR IEEE, P379
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   DAVIS LS, 1979, IEEE T PATTERN ANAL, V1, P251, DOI 10.1109/TPAMI.1979.4766921
   Fernando Basura, 2014, International Journal of Computer Vision, V108, P186, DOI 10.1007/s11263-014-0700-1
   Gao JZ, 2009, IEEE I CONF COMP VIS, P2122, DOI 10.1109/ICCV.2009.5459465
   Gelzinis A, 2007, PATTERN RECOGN, V40, P2367, DOI 10.1016/j.patcog.2006.12.004
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Henderson HV., 1981, Linear Multilinear Algebra, V9, P271, DOI [10.1080/03081088108817379, DOI 10.1080/03081088108817379]
   Hu SS, 2016, IEEE T IMAGE PROCESS, V25, P3411, DOI 10.1109/TIP.2016.2568460
   Huang J, 1997, PROC CVPR IEEE, P762, DOI 10.1109/CVPR.1997.609412
   Ito S, 2010, LECT NOTES COMPUT SC, V6312, P209, DOI 10.1007/978-3-642-15552-9_16
   Jiang YG, 2015, IEEE T MULTIMEDIA, V17, P1174, DOI 10.1109/TMM.2015.2436813
   Jiang YN, 2012, PROC CVPR IEEE, P3100, DOI 10.1109/CVPR.2012.6248042
   Khan FS, 2014, MACH VISION APPL, V25, P1385, DOI 10.1007/s00138-014-0621-6
   Khan R, 2013, PROC CVPR IEEE, P2866, DOI 10.1109/CVPR.2013.369
   Ladicky L, 2010, LECT NOTES COMPUT SC, V6315, P239, DOI 10.1007/978-3-642-15555-0_18
   Liao S, 2015, IEEE T MULTIMEDIA, V17, P1058, DOI 10.1109/TMM.2015.2436057
   Liu GH, 2008, PATTERN RECOGN, V41, P3521, DOI 10.1016/j.patcog.2008.06.010
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo CZ, 2016, IEEE T MULTIMEDIA, V18, P40, DOI 10.1109/TMM.2015.2495248
   Micusik Branislav, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P625, DOI 10.1109/ICCVW.2009.5457645
   Mita T, 2008, IEEE T PATTERN ANAL, V30, P1257, DOI 10.1109/TPAMI.2007.70767
   Ni BB, 2012, PROC CVPR IEEE, P3514, DOI 10.1109/CVPR.2012.6248094
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Palm C, 2004, PATTERN RECOGN, V37, P965, DOI 10.1016/j.patcog.2003.09.010
   Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Qi GJ, 2010, IEEE T MULTIMEDIA, V12, P278, DOI 10.1109/TMM.2010.2046270
   Qi XB, 2012, LECT NOTES COMPUT SC, V7577, P158, DOI 10.1007/978-3-642-33783-3_12
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tang JH, 2015, IEEE T IMAGE PROCESS, V24, P2827, DOI 10.1109/TIP.2015.2421443
   van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809
   Wang HX, 2014, IEEE T IMAGE PROCESS, V23, P1805, DOI 10.1109/TIP.2014.2308416
   Wang Q, 2018, IEEE T DEPEND SECURE, V15, P496, DOI 10.1109/TDSC.2016.2593444
   Watanabe T, 2009, LECT NOTES COMPUT SC, V5414, P37, DOI 10.1007/978-3-540-92957-4_4
   Wu L, 2009, IEEE T MULTIMEDIA, V11, P286, DOI 10.1109/TMM.2008.2009692
   Yang S, 2010, PROC CVPR IEEE, P2249, DOI 10.1109/CVPR.2010.5539907
   Yang Y, 2011, IEEE I CONF COMP VIS, P1465, DOI 10.1109/ICCV.2011.6126403
   Yuan J., 2007, PROC IEEE C COMPUT V, P1
   Yuan JW, 2009, COGNITION EMOTION, V23, P1221, DOI 10.1080/02699930802416453
   Yuan JS, 2011, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2011.5995476
   Zhang DQ, 2013, MULTIMED TOOLS APPL, V67, P179, DOI 10.1007/s11042-011-0940-9
   Zhang SL, 2011, IEEE T IMAGE PROCESS, V20, P2664, DOI 10.1109/TIP.2011.2128333
   Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528
   Zheng Y.-T., 2008, PROC IEEE C COMPUT V, P1
   Zhou WG, 2014, IEEE T MULTIMEDIA, V16, P601, DOI 10.1109/TMM.2014.2301979
   Zou Q, 2015, IEEE IMAGE PROC, P696, DOI 10.1109/ICIP.2015.7350888
   Zou Q, 2014, PATTERN RECOGN LETT, V49, P146, DOI 10.1016/j.patrec.2014.07.002
NR 68
TC 11
Z9 11
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2017
VL 19
IS 3
BP 492
EP 505
DI 10.1109/TMM.2016.2619912
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN2VW
UT WOS:000395869400006
DA 2024-07-18
ER

PT J
AU Han, Y
   Xu, C
   Baciu, G
   Li, M
   Islam, MR
AF Han, Yu
   Xu, Chen
   Baciu, George
   Li, Min
   Islam, Md. Robiul
TI Cartoon and Texture Decomposition-Based Color Transfer for Fabric Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Color transfer; fabric image; image decomposition; total generalized
   variation (TGV); variational model
ID TOTAL VARIATION MINIMIZATION; COLORIZATION; RESTORATION; SPARSE;
   SEGMENTATION; MODEL
AB A color design process for fabric images can resort to a solution of a color transfer problem based on given color themes. Usually, the color transfer process contains an image segmentation phase and an image construction phase. In this paper, a novel color transfer method for fabric images is proposed. Compared with classical color transfer methods, the new method has the following three main innovations. First, the new method, in its image segmentation phase, follows an assumption that a fabric image can be decomposed into cartoon and texture components, which means the new color transfer method, in its image segmentation, phase incorporates an image decomposition process. The advantage of the innovation is that the cartoon component is more suitable than the original image to be used to partition the fabric image. Second, the new color transfer method can generate more vivid color transfer results since the above texture component is used to describe yarn texture details in the image construction phase. Third, the total generalized variation (TGV) regularizer is used to further improve the performance of image decomposition. Here, the TGV regularizer is good at estimating the weak lightness variation of the cartoon component with the CIELab color scheme. In addition, by using the augmented Lagrange multiplier method, we derive an efficient algorithm to search for the solutions to the proposed color transfer problem. Numerical results demonstrate that the proposed color transfer method can generate better results for fabric images.
C1 [Han, Yu; Xu, Chen; Li, Min] Shenzhen Univ, Coll Math & Stat, Shenzhen 518060, Peoples R China.
   [Baciu, George] Hong Kong Polytech Univ, Coll Math & Computat Sci, Hong Kong, Hong Kong, Peoples R China.
   [Islam, Md. Robiul] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
C3 Shenzhen University; Hong Kong Polytechnic University; Shenzhen
   University
RP Han, Y (corresponding author), Shenzhen Univ, Coll Math & Stat, Shenzhen 518060, Peoples R China.
EM hany@szu.edu.cn; xuchen_szu@szu.edu.cn; csgeorge@comp.polyu.edu.hk;
   limin800@szu.edu.cn; robiulkuet@gmail.com
RI Baciu, George/AAU-7143-2021
OI BACIU, George/0000-0002-1766-6357
FU NSFC [61402290, 61472257]; DPCHE [20134408110001]; JCYJ
   [20160520161847267]; Hong Kong RGC GRF [PolyU 5101/11E, 5100/12E,
   5100/13E]; HD Video R & D Platform for Intelligent Analysis and
   Processing in Guangdong Engineering Technology Research Centre of
   Colleges and Universities [GCZX-A1409];  [2014KQNCX134]
FX This work was supported in part by the NSFC under Grant 61402290 and
   Grant 61472257, in part by Grant 2014KQNCX134, in part by the DPCHE
   under Grant 20134408110001, in part by the JCYJ under Grant
   20160520161847267, in part by the Hong Kong RGC GRF under Grant PolyU
   5101/11E, Grant 5100/12E, and Grant 5100/13E, and also in part by The HD
   Video R & D Platform for Intelligent Analysis and Processing in
   Guangdong Engineering Technology Research Centre of Colleges and
   Universities under Grant GCZX-A1409. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Winston Hsu.
CR Abadpour A, 2007, J VIS COMMUN IMAGE R, V18, P15, DOI 10.1016/j.jvcir.2006.08.001
   [Anonymous], 2013, ACM T GRAPHIC, DOI DOI 10.1145/2461912.2461988
   [Anonymous], 2006, Mathematical problems in image processing: Partial differential equations and the calculus of variations
   Bell Sean, 2014, ACM Transactions on Graphics, V33, DOI 10.1145/2601097.2601206
   BI S, 2015, ACM T GRAPHICS SIGGR, V34
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Bredies K, 2010, SIAM J IMAGING SCI, V3, P492, DOI 10.1137/090769521
   Bugeau A, 2014, IEEE T IMAGE PROCESS, V23, P298, DOI 10.1109/TIP.2013.2288929
   Chambolle A, 2004, J MATH IMAGING VIS, V20, P89
   Chan TF, 2006, J MATH IMAGING VIS, V26, P85, DOI 10.1007/s10851-006-6865-7
   Chang HW, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766978
   Cohen-Or D, 2006, ACM T GRAPHIC, V25, P624, DOI 10.1145/1141911.1141933
   Dejun Zheng, 2012, Proceedings of the 2012 11th IEEE International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC), P346, DOI 10.1109/ICCI-CC.2012.6311173
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Garces E, 2012, COMPUT GRAPH FORUM, V31, P1415, DOI 10.1111/j.1467-8659.2012.03137.x
   Gilles J, 2009, ADV IMAG ELECT PHYS, V158, P89, DOI 10.1016/S1076-5670(09)00008-1
   Goldstein T, 2010, J SCI COMPUT, V45, P272, DOI 10.1007/s10915-009-9331-z
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   Han Y, 2015, NEUROCOMPUTING, V168, P575, DOI 10.1016/j.neucom.2015.05.069
   Han Y, 2013, TEXT RES J, V83, P638, DOI 10.1177/0040517512452953
   Jiang LL, 2008, J MATH IMAGING VIS, V30, P125, DOI 10.1007/s10851-007-0051-4
   Kang SH, 2007, IEEE T IMAGE PROCESS, V16, P2251, DOI 10.1109/TIP.2007.903257
   Knoll F, 2013, MAGN RESON MED, V70, P40, DOI 10.1002/mrm.24426
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li CM, 2011, IEEE T IMAGE PROCESS, V20, P2007, DOI 10.1109/TIP.2011.2146190
   Li YF, 2012, PATTERN RECOGN LETT, V33, P111, DOI 10.1016/j.patrec.2011.09.036
   Lin WC, 2007, IEEE T PATTERN ANAL, V29, P777, DOI 10.1109/TPAMI.2007.1053
   Liu SG, 2012, J VIS COMMUN IMAGE R, V23, P173, DOI 10.1016/j.jvcir.2011.09.006
   Liu XM, 2008, CATAL COMMUN, V9, P1, DOI 10.1016/j.catcom.2007.05.020
   Liu YX, 2005, INT J COMPUT VISION, V62, P145, DOI 10.1007/s11263-005-4639-0
   Meyer Y., 2001, OSCILLATING PATTERNS, V22
   Ng MK, 2014, IEEE T AUTOM SCI ENG, V11, P943, DOI 10.1109/TASE.2014.2314240
   Ng MK, 2013, IEEE T IMAGE PROCESS, V22, P2233, DOI 10.1109/TIP.2013.2246520
   O'Donovan P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964958
   Ono S, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2299067
   Osher S, 2003, MULTISCALE MODEL SIM, V1, P349, DOI 10.1137/S1540345902416247
   Pang JH, 2014, IEEE IMAGE PROC, P4687, DOI 10.1109/ICIP.2014.7025950
   Pang JH, 2013, INT CONF ACOUST SPEE, P1578, DOI 10.1109/ICASSP.2013.6637917
   Papadakis N, 2011, IEEE T IMAGE PROCESS, V20, P1682, DOI 10.1109/TIP.2010.2095869
   Park M, 2009, IEEE T PATTERN ANAL, V31, P1804, DOI 10.1109/TPAMI.2009.73
   Rabin J, 2011, IEEE IMAGE PROC, P1541, DOI 10.1109/ICIP.2011.6115740
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Schaeffer H, 2013, SIAM J IMAGING SCI, V6, P226, DOI 10.1137/110854989
   SHEN L., 2008, CVPR
   Shen L, 2013, IEEE T PATTERN ANAL, V35, P2904, DOI 10.1109/TPAMI.2013.136
   Su Z, 2014, IEEE T MULTIMEDIA, V16, P988, DOI 10.1109/TMM.2014.2305914
   Tai YW, 2005, PROC CVPR IEEE, P747
   Wong BY, 2012, IEEE T MULTIMEDIA, V14, P760, DOI 10.1109/TMM.2012.2188997
   Wu CL, 2012, J SCI COMPUT, V50, P145, DOI 10.1007/s10915-011-9477-3
   Xiang Y, 2009, PATTERN RECOGN LETT, V30, P682, DOI 10.1016/j.patrec.2009.01.004
   Xu JL, 2014, J SYST ENG ELECTRON, V25, P168, DOI 10.1109/JSEE.2014.00020
   Xu L., 2012, ACM Transactions on Graphics (TOG), V31, P1, DOI DOI 10.1145/2366145.2366158
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yatziv L, 2006, IEEE T IMAGE PROCESS, V15, P1120, DOI 10.1109/TIP.2005.864231
   Ye GZ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601135
   Zheng DJ, 2015, TEXT RES J, V85, P1520, DOI 10.1177/0040517514561920
   Zheng DJ, 2015, COLOR RES APPL, V40, P304, DOI 10.1002/col.21881
   Zou Z., 2016, COLOR RES APPL
NR 59
TC 42
Z9 46
U1 0
U2 45
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2017
VL 19
IS 1
BP 80
EP 92
DI 10.1109/TMM.2016.2608000
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EH0SX
UT WOS:000391475200007
DA 2024-07-18
ER

PT J
AU Yuan, XL
   Wang, XY
   Wang, C
   Weng, J
   Ren, K
AF Yuan, Xingliang
   Wang, Xinyu
   Wang, Cong
   Weng, Jian
   Ren, Kui
TI Enabling Secure and Fast Indexing for Privacy-Assured Healthcare
   Monitoring via Compressive Sensing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cloud computing; compressive sensing; fast encrypted indexing;
   multimedia-based healthcare; privacy-aware healthcare
ID SERVICE
AB As e-health technology continues to advance, health related multimedia data is being exponentially generated from healthcare monitoring devices and sensors. Coming with it are the challenges on how to efficiently acquire, index, and process such a huge amount of data for effective healthcare and related decision making, while respecting user's data privacy. In this paper, we propose a secure cloud-based framework for privacy-aware healthcare monitoring systems, which allows fast data acquisition and indexing with strong privacy assurance. For efficient data acquisition, we adopt compressive sensing for easy data sampling, compression, and recovery. We then focus on how to secure and fast index the resulting large amount of continuously generated compressed samples, with the goal to achieve secure selected retrieval over compressed storage. Among others, one particular challenge is the practical demand to cope with the incoming data samples in high acquisition rates. For that problem, we carefully exploit recent efforts on encrypted search, efficient content-based indexing techniques, and fine-grained locking algorithms, to design a novel encrypted index with high-performance customization. It achieves memory efficiency, provable security, as well as greatly improved building speed with nontrivial multithread support. Comprehensive evaluations on Amazon Cloud show that our encrypted design can securely index 1 billion compressed data samples within only 12 min, achieving a throughput of indexing almost 1.4 million encrypted samples per second. Accuracy and visual evaluation on a real healthcare dataset shows good quality of high-value retrieval and recovery over encrypted data samples.
C1 [Yuan, Xingliang; Wang, Xinyu; Wang, Cong] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   [Yuan, Xingliang; Wang, Xinyu; Wang, Cong] City Univ Hong Kong, Shenzhen Res Inst, Hong Kong, Hong Kong, Peoples R China.
   [Weng, Jian] Jinan Univ, Sch Informat Technol, Guangzhou 510632, Guangdong, Peoples R China.
   [Weng, Jian] Shenzhen Univ, Guangdong Prov Big Data Collaborat Innovat Ctr, Shenzhen 518060, Peoples R China.
   [Ren, Kui] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
C3 City University of Hong Kong; Shenzhen Research Institute, City
   University of Hong Kong; City University of Hong Kong; Jinan University;
   Shenzhen University; State University of New York (SUNY) System; State
   University of New York (SUNY) Buffalo
RP Yuan, XL (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.; Yuan, XL (corresponding author), City Univ Hong Kong, Shenzhen Res Inst, Hong Kong, Hong Kong, Peoples R China.
EM xyuancs@gmail.com; xy.w@my.cityu.edu.hk; congwang@cityu.edu.hk;
   cryptjweng@gmail.com; kuiren@buffalo.edu
RI Ren, Kui/AGE-3662-2022; Yuan, Xingliang/Z-4306-2019
OI Yuan, Xingliang/0000-0002-3701-4946; Wang, Xinyu/0000-0002-2602-5551;
   Wang, Cong/0000-0003-0547-315X; Weng, Jian/0000-0003-4067-8230; Ren,
   Kui/0000-0003-3441-6277
FU Research Grants Council of Hong Kong [CityU 138513, CityU 11276816];
   Natural Science Foundation of China [61572412]; Innovation and
   Technology Commission of Hong Kong under ITF [ITS/307/15]; AWS Education
   Research Grant; U.S. National Science Foundation [CNS-1262277]; National
   Science Foundation of China [61272413, 61133014, 61272415, 61472165];
   Research Fund for the Doctoral Program of Higher Education of China
   [20134401110011]; Special Fund for Applied Science and Technology
   Development and Transformation of Major Scientific and Technological
   Achievements
FX This work was supported in part by Research Grants Council of Hong Kong
   under Project CityU 138513 and Project CityU 11276816, in part by the
   Natural Science Foundation of China under Project 61572412, in part by
   the Innovation and Technology Commission of Hong Kong under ITF Project
   ITS/307/15, and in part by the AWS Education Research Grant. The work of
   K. Ren was supported by the U.S. National Science Foundation under Grant
   CNS-1262277. The work of J. Weng was supported by the National Science
   Foundation of China under Project 61272413, Project 61133014, Project
   61272415, and Project 61472165, by the Research Fund for the Doctoral
   Program of Higher Education of China under Grant 20134401110011, and by
   the 2016 Special Fund for Applied Science and Technology Development and
   Transformation of Major Scientific and Technological Achievements. The
   guest editor team coordinated the review of this manuscript and approved
   it for publication.
CR Amazon, 2015, ARCH HIP SEC COMPL A
   Amazon, 2016, AM EC2 INST TYP
   Amazon, 2016, AWS DIR CONN
   Andoni A., 2016, E2LSH PACKAGE
   Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   [Anonymous], 2013, P ACM SIGSAC C COMP, DOI 10.1145/2508859.2516730
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Cash D., 2014, P C NETW DISTR SYST
   Common Crawl, 2016, COMM CRAWL CORP
   Cossalter M, 2010, IEEE T MULTIMEDIA, V12, P168, DOI 10.1109/TMM.2010.2041105
   Curtmola Reza, 2006, P 13 ACM C COMP COMM, DOI DOI 10.1145/1180405.1180417
   Deng CW, 2012, IEEE T MULTIMEDIA, V14, P278, DOI 10.1109/TMM.2011.2181491
   Divekar Atul, 2009, 2009 43rd Asilomar Conference on Signals, Systems and Computers, P109, DOI 10.1109/ACSSC.2009.5470158
   Fan Bin, 2013, 10 USENIX S NETW SYS, P371
   Gebel R., 2012, KL1P SPARSE RECOVERY
   Goyal V., 2006, P 2006 INT C PRIVACY, P1
   Hahn F, 2014, CCS'14: PROCEEDINGS OF THE 21ST ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P310, DOI 10.1145/2660267.2660297
   Har-Peled S., 2012, Theory of computing, V8, P321, DOI [DOI 10.4086/TOC.2012.V008A014, 10.4086/toc.2012.v008a014]
   Hosseini M., 2015, What will the future look like under Industry 4 . 0 and digital transformation in the healthcare space ?
   Hua Y, 2013, IEEE INFOCOM SER, P1303
   Kamara S., 2012, P 19 ACM C COMP COMM, P965
   KUNG HT, 1981, ACM T DATABASE SYST, V6, P213, DOI 10.1145/319566.319567
   Kuzu M, 2012, PROC INT CONF DATA, P1156, DOI 10.1109/ICDE.2012.23
   Lu WJ, 2014, IEEE ACCESS, V2, P125, DOI 10.1109/ACCESS.2014.2307057
   Lu WJ, 2010, IEEE IMAGE PROC, P2093, DOI 10.1109/ICIP.2010.5653399
   Lv Q, 2007, P 33 INT C VER LARG, P950
   Mamaghanian H, 2011, IEEE T BIO-MED ENG, V58, P2456, DOI 10.1109/TBME.2011.2156795
   Needell D, 2010, COMMUN ACM, V53, P93, DOI 10.1145/1859204.1859229
   Qmed, 2015, PHIL HOOKS AM HOST M
   Ren K, 2012, IEEE INTERNET COMPUT, V16, P69, DOI 10.1109/MIC.2012.14
   Shoaib M., 2011, 2011 IEEE 13th International Conference on e-Health Networking, Applications and Services (Healthcom 2011), P326, DOI 10.1109/HEALTH.2011.6026773
   Slashdot, 2015, PRIV MED DAT 1 5 MIL
   Tao YF, 2009, ACM SIGMOD/PODS 2009 CONFERENCE, P563
   Veeraraghavan A, 2011, IEEE T PATTERN ANAL, V33, P671, DOI 10.1109/TPAMI.2010.87
   Wang C., 2013, P NETW DISTR SYST SE
   Wang C, 2014, IEEE INFOCOM SER, P2130, DOI 10.1109/INFOCOM.2014.6848155
   Wang C, 2013, IEEE T EMERG TOP COM, V1, P166, DOI 10.1109/TETC.2013.2273797
   Yuan XL, 2015, LECT NOTES COMPUT SC, V9327, P40, DOI 10.1007/978-3-319-24177-7_3
   Yuan XL, 2014, INT CON DISTR COMP S, P198, DOI 10.1109/ICDCS.2014.28
   Zhang J., 2014, P 6 USENIX C HOT TOP
NR 40
TC 31
Z9 35
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2016
VL 18
IS 10
BP 2002
EP 2014
DI 10.1109/TMM.2016.2602758
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DX8NC
UT WOS:000384644800008
OA hybrid
DA 2024-07-18
ER

PT J
AU Ahn, I
   Kim, C
AF Ahn, Ilkoo
   Kim, Changick
TI Face and Hair Region Labeling Using Semi-Supervised Spectral
   Clustering-Based Multiple Segmentations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face segmentation; hair segmentation; multiple segmentations (MSs);
   spectral clustering (SC); graph cut
ID IMAGE SEGMENTATION; COLOR
AB The multiple segmentation (MS) scheme is considered to be a way to get a better spatial support for various shaped objects in image segmentation. The MS scheme assumes that the segmented regions (i.e., segments) can be treated as hypotheses for object support rather than mere partitionings of the image. As for attaining each segmentation in the MS scheme, one of the most popular methods is to employ spectral clustering (SC). When applied to image segmentation tasks, SC groups a set of pixels or small regions into unique segments. While it has been popularly used in image segmentation, it often fails to deal with images containing objects with complex boundaries. To split the image as close to the object boundaries as possible, some prior knowledge can be used to guide the clustering algorithm toward appropriate partitioning of the data. In semisupervised clustering, prior knowledge is often formulated as pairwise constraints. In this paper, we propose an MS technique combined with constrained SC to build a face and hair region labeler. To put it concretely, pairwise constraints modified to fit the problem of labeling face regions are added to SC and multiple segments are generated by the constrained SC. Then, the labeling is conducted by estimating the likelihoods for each segment to belong to the target object classes. Experiments are conducted on three datasets and the results show that the proposed scheme offers useful tools for labeling the face images.
C1 [Ahn, Ilkoo] Korea Adv Inst Sci & Technol, Dept Elect Engn, Daejeon 34141, South Korea.
   [Ahn, Ilkoo] Korea Inst Oriental Med, Daejeon 34054, South Korea.
   [Kim, Changick] Korea Adv Inst Sci & Technol, Dept Elect Engn, Taejon 305701, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST); Korea
   Institute of Oriental Medicine (KIOM); Korea Advanced Institute of
   Science & Technology (KAIST)
RP Ahn, I (corresponding author), Korea Adv Inst Sci & Technol, Dept Elect Engn, Daejeon 34141, South Korea.; Ahn, I (corresponding author), Korea Inst Oriental Med, Daejeon 34054, South Korea.
EM ahn19@kaist.ac.kr; changick@kaist.ac.kr
RI Kim, Changick/C-1779-2011
FU Brain Korea 21 Plus Program, Korea
FX This work was supported by the Brain Korea 21 Plus Program, Korea. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Winston Hsu.
CR [Anonymous], 2007, TECH REP 07 49
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Collins M, 2002, MACH LEARN, V48, P253, DOI 10.1023/A:1013912006537
   Gould S, 2009, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2009.5459211
   Hoiem D, 2007, INT J COMPUT VISION, V75, P151, DOI 10.1007/s11263-006-0031-y
   Huang GQ, 2009, INT J COMPUT INTEG M, V22, P579, DOI 10.1080/09511920701724934
   Jiang HQ, 2015, IEEE T MULTIMEDIA, V17, P3, DOI 10.1109/TMM.2014.2368273
   Kae A, 2013, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2013.263
   KASSON JM, 1992, ACM T GRAPHIC, V11, P373, DOI 10.1145/146443.146479
   Kim TH, 2010, PROC CVPR IEEE, P3201, DOI 10.1109/CVPR.2010.5540078
   Klein D., 2002, Tech. rep., P307
   Kong H, 2010, IEEE T IMAGE PROCESS, V19, P2211, DOI 10.1109/TIP.2010.2045715
   Kosecká J, 2002, LECT NOTES COMPUT SC, V2353, P476
   Kumar MP, 2010, PROC CVPR IEEE, P3217, DOI 10.1109/CVPR.2010.5540072
   Le THN, 2013, IEEE T IMAGE PROCESS, V22, P3097, DOI 10.1109/TIP.2013.2259835
   Leung T, 2001, INT J COMPUT VISION, V43, P29, DOI 10.1023/A:1011126920638
   Li HL, 2009, IEEE T MULTIMEDIA, V11, P77, DOI 10.1109/TMM.2008.2008922
   Li ZG, 2012, PROC CVPR IEEE, P789, DOI [10.1109/CVPR.2012.6247750, 10.1109/ISRA.2012.6219309]
   Malisiewicz T., 2007, Proceedings of the British Machine Vision Conference 2007, DOI DOI 10.5244/C.21.55
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Pantofaru C, 2008, LECT NOTES COMPUT SC, V5304, P481, DOI 10.1007/978-3-540-88690-7_36
   Ratnasingam S, 2012, IEEE T IMAGE PROCESS, V21, P3612, DOI 10.1109/TIP.2012.2193135
   Raza SH, 2013, PROC CVPR IEEE, P3081, DOI 10.1109/CVPR.2013.396
   Ren ZL, 2013, PROC CVPR IEEE, P2011, DOI 10.1109/CVPR.2013.262
   Russell B.C., 2006, Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on, V2, P1605, DOI DOI 10.1109/CVPR.2006.326
   Scheffler C, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.53
   Ugarriza LG, 2009, IEEE T IMAGE PROCESS, V18, P2275, DOI 10.1109/TIP.2009.2025555
   Wagstaff K., 2000, AAAI/IAAI, V1097, P577
   Wang N, 2012, PROC CVPR IEEE, P662, DOI 10.1109/CVPR.2012.6247734
   Wang N, 2011, LECT NOTES COMPUT SC, V6494, P171, DOI 10.1007/978-3-642-19318-7_14
NR 30
TC 18
Z9 21
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2016
VL 18
IS 7
BP 1414
EP 1421
DI 10.1109/TMM.2016.2551698
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR2RW
UT WOS:000379752600016
DA 2024-07-18
ER

PT J
AU Zhou, W
   Zhang, JZ
   Zhou, X
   Liu, ZY
   Liu, XX
AF Zhou, Wei
   Zhang, Jingzhi
   Zhou, Xin
   Liu, Zhenyu
   Liu, Xiaoxiang
TI A High-Throughput and Multi-Parallel VLSI Architecture for HEVC
   Deblocking Filter
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deblocking filter; high efficiency video coding (HEVC); parallel
   processing; VLSI
ID DE-BLOCKING FILTER; VIDEO; EFFICIENCY; HARDWARE
AB This paper presents a high-throughput and multi-parallel VLSI hardware architecture for the deblocking filter in the HEVC video coding standard. First, an implementation-friendly and fast boundary judgment method is proposed to avoid using the original recursion loop approach. Then a dedicated parallel VLSI architecture composed of four parallel filtering cores is presented based on the proposed boundary judgment method. With the parallel luma/chroma filtering and parallel vertical/horizontal edges filtering order, the proposed VLSI architecture can process filtering operations for one largest coding unit (LCU) with less filtering cycles than other conventional approaches. Furthermore, filtering efficiency is improved due to a novel ping-pang buffer architecture and the on-chip single-port SRAM with dedicated data arrangement in the memory modules. Experimental results demonstrate that the proposed deblocking filter architecture improves the performance by 28-89% at the expense of the slightly increased gate count compared to the previously known architecture in HEVC. The proposed architecture can reach a high operating clock frequency of 278 MHz with TSMC 90 nm library and meet the real time requirement of the deblocking filter for 8K x 4K video format at 123 frame/s.
C1 [Zhou, Wei; Zhang, Jingzhi] Northwestern Polytech Univ, Sch Elect & Informat, Xian 710072, Peoples R China.
   [Zhou, Xin] Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China.
   [Liu, Zhenyu] Tsinghua Univ, Res Inst Informat Technol, Beijing 100084, Peoples R China.
   [Liu, Xiaoxiang] Complex Syst Inc, Calgary, AB T2L 2K7, Canada.
C3 Northwestern Polytechnical University; Northwestern Polytechnical
   University; Tsinghua University
RP Zhou, W; Zhang, JZ (corresponding author), Northwestern Polytech Univ, Sch Elect & Informat, Xian 710072, Peoples R China.; Zhou, X (corresponding author), Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China.; Liu, ZY (corresponding author), Tsinghua Univ, Res Inst Informat Technol, Beijing 100084, Peoples R China.; Liu, XX (corresponding author), Complex Syst Inc, Calgary, AB T2L 2K7, Canada.
EM zhouwei@nwpu.edu.cn; zhangjingzhi@mail.nwpu.edu.cn; xinzhou@nwpu.edu.cn;
   liuzhenyu73@mail.tsinghua.edu.cn; liu@complexsysteminc.com
RI Zhou, Wei/AAG-8797-2020; Liu, Zhenyu/AAD-4789-2020
FU National Natural Science Foundation of China [60902101]; Fundamental
   Research Funds for the Central Universities [3102014JCQ01057]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 60902101, and in part by the Fundamental
   Research Funds for the Central Universities under Grant 3102014JCQ01057.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Leonel Sousa.
CR BOJNORDI MN, 2006, P IEEE ICASSP, P925
   Chang SC, 2005, IEEE T CONSUM ELECTR, V51, P249, DOI 10.1109/TCE.2005.1405728
   Chao YC, 2006, 2006 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS, P1260
   Cho S, 2015, IEEE T MULTIMEDIA, V17, P778, DOI 10.1109/TMM.2015.2418995
   Heng-Yao Lin, 2006, 2006 IEEE International Symposium on Circuits and Systems (IEEE Cat. No. 06CH37717C)
   Hwangbo W, 2010, IEEE T MULTIMEDIA, V12, P157, DOI 10.1109/TMM.2010.2041099
   Khurana G, 2006, IEEE T CONSUM ELECTR, V52, P536, DOI 10.1109/TCE.2006.1649676
   Le HHN, 2014, J INF SCI ENG, V30, P281
   McCann K., 2013, JOINT COLL TEAM VID
   Norkin A, 2012, IEEE T CIRC SYST VID, V22, P1746, DOI 10.1109/TCSVT.2012.2223053
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   Ozcan E, 2013, IEEE T CONSUM ELECTR, V59, P714, DOI 10.1109/TCE.2013.6626260
   Rhee CE, 2014, IEEE T MULTIMEDIA, V16, P947, DOI 10.1109/TMM.2014.2306396
   Shen WW, 2013, IEEE INT SYMP CIRC S, P673, DOI 10.1109/ISCAS.2013.6571936
   Shih SY, 2006, ASIA S PACIF DES AUT, P170
   Shih SY, 2005, IEEE INT SYMP CIRC S, P4529
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Ugur K., 2010, JOINT COLL TEAM VID
   Vanne J, 2012, IEEE T CIRC SYST VID, V22, P1885, DOI 10.1109/TCSVT.2012.2223013
   Vijay S., 2010, Proceedings of the 2010 IEEE Workshop on Signal Processing Systems (SiPS 2010), P116, DOI 10.1109/SIPS.2010.5624773
   Zhai GT, 2008, IEEE T MULTIMEDIA, V10, P735, DOI 10.1109/TMM.2008.922849
   Zhu JY, 2013, IEEE IMAGE PROC, P1967, DOI 10.1109/ICIP.2013.6738405
NR 22
TC 16
Z9 16
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2016
VL 18
IS 6
BP 1034
EP 1047
DI 10.1109/TMM.2016.2537217
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DM1LP
UT WOS:000376107100008
DA 2024-07-18
ER

PT J
AU Zhang, Y
   Zimmermann, R
AF Zhang, Ying
   Zimmermann, Roger
TI Efficient Summarization From Multiple Georeferenced User-Generated
   Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Geo-tagging; location-based service; sensor data mining; video
   summarization
ID FRAMEWORK
AB The rapid developments in camera technology and mobile devices bring a flourish of user-generated videos with rich geographic metadata, which can be great information resources for prospective tourists to preview a place of interest. In a video retrieval system, a simple query will return many videos. To provide users with a convenient way to explore videos, we generate summarization from multiple georeferenced videos, which is composed of segments from different input videos but complementing each other to preserve the regions of interest (ROIs) among the original inputs. Different from conventional ROI detection techniques which extract objects from a single video with data-intensive computing, the proposed strategy solely leverages the geographic metadata among multiple videos (we term the ROI as Geographic-ROI/GROI). A Gaussian-based model is proposed to formulate the capture intention distribution in geo-space for each video-frame. By selecting such characteristics from keyframes in each video, we successfully detect the GROIs in a few milliseconds with average error distances within a few meters. Based on this, we select representative video segments capturing popular GROIs and compose them in a coherent manner as a final summarization. To speed up the processing, we represent videos according to their geographic characteristics and actively select their representative pieces (geo-keys). The highest quality videos among the local scope (key-neighborhood) for geo-keys are added to the summary with an appropriate travel route determined by their spatial consistency. Experiments indicate our summarization approach out-performs the state-of-the-arts by preserving the original GROIs with better accuracy and within less time.
C1 [Zhang, Ying; Zimmermann, Roger] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
C3 National University of Singapore
RP Zhang, Y; Zimmermann, R (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
EM zhang.ying@u.nus.edu; rogerz@comp.nus.edu.sg
RI Zimmermann, Roger/D-7944-2015
OI Zimmermann, Roger/0000-0002-7410-2590
FU Singapore National Research Foundation under its International Research
   Centre at Singapore Funding Initiative
FX This work was supported by the Singapore National Research Foundation
   under its International Research Centre at the Singapore Funding
   Initiative, administered by the IDM Programme Office through the Centre
   of Social Media Innovations for Communities (COSMIC). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Xiao-Ping Zhang.
CR Aizawa K, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P398, DOI 10.1109/ICIP.2001.958135
   Aizawa Kiyoharu., 2004, CARPE 04 P THE 1 ACM, P22, DOI DOI 10.1145/1026653.1026656
   [Anonymous], P INT C MULT MOD MMM
   [Anonymous], 2010, Image Analysis for Multimedia Interactive Services (WIAMIS), 2010 11th International Workshop on, DOI DOI 10.1109/WIC0M.2010.5601233
   [Anonymous], P ACM S US INT SOFTW
   [Anonymous], 2008, Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval
   [Anonymous], 1976, 388 CARN MELL U MAN
   [Anonymous], P ACM MULT SYST C
   [Anonymous], P ACM INT C MULT
   [Anonymous], 2006, PROC IEEE COMPUT SOC
   [Anonymous], PAGERANK CITATION TA
   Ay S., 2008, Proceedings of the 16th ACM international conference on Multimedia (MM '08), P309
   Bradley K., 2001, Business, P75
   Cai D, 2012, IEEE T KNOWL DATA EN, V24, P707, DOI 10.1109/TKDE.2011.104
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   Ferman AM, 2003, IEEE T MULTIMEDIA, V5, P244, DOI 10.1109/TMM.2003.811617
   Graham C., 1965, Vision and Visual Perception
   Hao J., 2011, Proceedings of the 19th ACM International Conference on Multimedia, MM'11, P1013
   Li Yanrui, 2010, J Biomed Biotechnol, V2010, P716515, DOI 10.1155/2010/716515
   Lu X., 2010, Proceedings of the 18th ACM International Conference on Multimedia, Firenze Italy, 25 October 2010, DOI [19.1145/1873951.1873972, DOI 10.1145/1873951.1873972]
   Ma Y.-F., 2002, ACM MULTIMEDIA, P533
   Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410
   Money AG, 2008, J VIS COMMUN IMAGE R, V19, P121, DOI 10.1016/j.jvcir.2007.04.002
   Osberger W, 1998, P SOC PHOTO-OPT INS, V3299, P148, DOI 10.1117/12.320106
   Otsuka I, 2005, IEEE T CONSUM ELECTR, V51, P112, DOI 10.1109/TCE.2005.1405707
   Shao J, 2010, COMPUT SCI INF SYST, V7, P85, DOI 10.2298/CSIS1001085S
   Shen Z., 2011, MM, P93
   Shipman F, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P753
   Simon I., 2007, PROC IEEE INT C COMP, P1
   Tai CH, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P1209
   Tjondronegoro D, 2004, IEEE MULTIMEDIA, V11, P22, DOI 10.1109/MMUL.2004.28
   Toyama Kentaro., 2003, P 11 ACM INT C MULTI, P156
   Wang F, 2009, IEEE INT CON MULTI, P1326, DOI 10.1109/ICME.2009.5202747
   Xu CS, 2008, IEEE T MULTIMEDIA, V10, P1342, DOI 10.1109/TMM.2008.2004912
   Yang Y, 2014, IEEE T MULTIMEDIA, V16, P1677, DOI 10.1109/TMM.2014.2323014
   Yingbo Li, 2011, 2011 9th International Workshop on Content-Based Multimedia Indexing (CBMI), P163, DOI 10.1109/CBMI.2011.5972539
   Yu K., 2006, P 23 INT C MACH LEAR, P1081
   Zhang Y, 2013, I S MOD ANAL SIM COM, P172, DOI 10.1109/MASCOTS.2013.25
NR 38
TC 17
Z9 18
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2016
VL 18
IS 3
BP 418
EP 431
DI 10.1109/TMM.2016.2520827
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DG2WP
UT WOS:000371931600009
DA 2024-07-18
ER

PT J
AU Sadreazami, H
   Ahmad, MO
   Swamy, MNS
AF Sadreazami, Hamidreza
   Ahmad, M. Omair
   Swamy, M. N. S.
TI Multiplicative Watermark Decoder in Contourlet Domain Using the Normal
   Inverse Gaussian Distribution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contourlet transform; digital image watermarking; normal inverse
   Gaussian (NIG) distribution; watermark extraction
ID IMAGE WATERMARKING; OPTIMUM DETECTION; ALGORITHM
AB In recent years, many works on digital image watermarking have been proposed all aiming at protection of the copyright of an image document or authentication of data. This paper proposes a novel watermark decoder in the contourlet domain. It is known that the contourlet coefficients of an image are highly non-Gaussian and a proper distribution to model the statistics of the contourlet coefficients is a heavy-tailed PDF. It has been shown in the literature that the normal inverse Gaussian (NIG) distribution can suitably fit the empirical distribution. In view of this, statistical methods for watermark extraction are proposed by exploiting the NIG as a prior for the contourlet coefficients of images. The proposed watermark extraction approach is developed using the maximum likelihood method based on the NIG distribution. Closed-form expressions are obtained for extracting the watermark bits in both clean and noisy environments. Experiments are performed to verify the robustness of the proposed decoder. The results show that the proposed decoder is superior to other decoders in terms of providing a lower bit error rate. It is also shown that the proposed decoder is highly robust against various kinds of attacks such as noise, rotation, cropping, filtering, and compression.
C1 [Sadreazami, Hamidreza; Ahmad, M. Omair; Swamy, M. N. S.] Concordia Univ, Ctr Commun & Signal Proc, Montreal, PQ H3G 1M8, Canada.
C3 Concordia University - Canada
RP Sadreazami, H; Ahmad, MO; Swamy, MNS (corresponding author), Concordia Univ, Ctr Commun & Signal Proc, Montreal, PQ H3G 1M8, Canada.
EM h_sadrea@encs.concordia.ca; omair@encs.concordia.ca;
   swamy@encs.concordia.ca
FU Natural Sciences and Engineering Research Council (NSERC) of Canada;
   Regroupement Strategique en Microelectronique du Quebec (ReSMiQ)
FX This work was supported in part by the Natural Sciences and Engineering
   Research Council (NSERC) of Canada, and in part by the Regroupement
   Strategique en Microelectronique du Quebec (ReSMiQ). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Alessandro Piva.
CR Akhaee A., 2010, IEEE T IMAGE PROCESS, V19, P700
   Akhaee MA, 2011, IEEE T INF FOREN SEC, V6, P883, DOI 10.1109/TIFS.2011.2146250
   Akhaee MA, 2009, IEEE T MULTIMEDIA, V11, P822, DOI 10.1109/TMM.2009.2012922
   Amirmazlaghani M, 2015, EXPERT SYST APPL, V42, P1960, DOI 10.1016/j.eswa.2014.10.015
   [Anonymous], 2010, Parallel Distributed Processing, Workshops and Phd Forum (IPDPSW), 2010 IEEE International Symposium on
   [Anonymous], 2000, SURPRISINGLY EFFECTI
   [Anonymous], Probability, Random Variables and Stochastic Processes
   [Anonymous], 1998, FUNDEMENTALS STAT SI
   Balado F, 2005, LECT NOTES COMPUT SC, V3710, P336
   Barni M, 2003, IEEE T SIGNAL PROCES, V51, P1118, DOI 10.1109/TSP.2003.809371
   Barni M, 2001, IEEE T IMAGE PROCESS, V10, P783, DOI 10.1109/83.918570
   Bi N, 2007, IEEE T IMAGE PROCESS, V16, P1956, DOI 10.1109/TIP.2007.901206
   Bian Y, 2013, IEEE T IMAGE PROCESS, V22, P2372, DOI 10.1109/TIP.2013.2246177
   Briassouli A, 2005, IEEE T MULTIMEDIA, V7, P700, DOI 10.1109/TMM.2005.850970
   Candès EJ, 1999, PHILOS T R SOC A, V357, P2495, DOI 10.1098/rsta.1999.0444
   Cheng CJ, 2014, J DISP TECHNOL, V10, P263, DOI 10.1109/JDT.2013.2295619
   Cheng Q, 2003, IEEE T SIGNAL PROCES, V51, P906, DOI 10.1109/TSP.2003.809374
   Cheng Q, 2001, IEEE T MULTIMEDIA, V3, P273, DOI 10.1109/6046.944472
   Coatrieux G, 2013, IEEE T INF FOREN SEC, V8, P111, DOI 10.1109/TIFS.2012.2224108
   Cox I., 2001, Digital Watermarking
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Deng CZ, 2009, FIFTH INTERNATIONAL CONFERENCE ON INFORMATION ASSURANCE AND SECURITY, VOL 1, PROCEEDINGS, P313, DOI 10.1109/IAS.2009.21
   Do MN, 2005, IEEE T IMAGE PROCESS, V14, P2091, DOI 10.1109/TIP.2005.859376
   Do MN, 2001, THESIS
   DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425
   Eriksson A, 2009, J DERIV, V16, P23, DOI 10.3905/JOD.2009.16.3.023
   Ghannam S, 2009, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2009.5414260
   Ghannam S, 2009, 2009 SECOND INTERNATIONAL CONFERENCE ON THE APPLICATIONS OF DIGITAL INFORMATION AND WEB TECHNOLOGIES (ICADIWT 2009), P545, DOI 10.1109/ICADIWT.2009.5273921
   Hamghalam M, 2014, IET IMAGE PROCESS, V8, P162, DOI 10.1049/iet-ipr.2013.0386
   Hamghalam M, 2013, IET IMAGE PROCESS, V7, P451, DOI 10.1049/iet-ipr.2012.0693
   Hernández JR, 2000, IEEE T IMAGE PROCESS, V9, P55, DOI 10.1109/83.817598
   IGRD T, 2005, SIGNAL PROCESS, V85, P1655
   Jayalakshmi M, 2006, IIH-MSP: 2006 INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, PROCEEDINGS, P449
   Jayalakshmi M, 2006, INT C PATT RECOG, P861
   Kalantari NK, 2010, IEEE T IMAGE PROCESS, V19, P1504, DOI 10.1109/TIP.2010.2042646
   Karlis D, 2002, STAT PROBABIL LETT, V57, P43, DOI 10.1016/S0167-7152(02)00040-8
   Kumar A., 2011, INT J MULTIMEDIA APP, V3, P122
   Langelaar GC, 2000, IEEE SIGNAL PROC MAG, V17, P20, DOI 10.1109/79.879337
   Nezhadarya E, 2011, IEEE T INF FOREN SEC, V6, P1200, DOI 10.1109/TIFS.2011.2163627
   Ng TM, 2005, IEEE SIGNAL PROC LET, V12, P285, DOI 10.1109/LSP.2005.843776
   Po DDY, 2006, IEEE T IMAGE PROCESS, V15, P1610, DOI 10.1109/TIP.2006.873450
   Poor H. Vincent, 1994, An introduction to signal detection and estimation
   Rahman SMM, 2009, IEEE T IMAGE PROCESS, V18, P1782, DOI 10.1109/TIP.2009.2021313
   Sadreazami H, 2014, IEEE INT SYMP CIRC S, P1288, DOI 10.1109/ISCAS.2014.6865378
   Sadreazami H., 2014, IEEE 27 CAN C EL COM, P1
   Sadreazami H, 2014, IEEE T IMAGE PROCESS, V23, P4348, DOI 10.1109/TIP.2014.2339633
   Sadreazami H, 2012, AEU-INT J ELECTRON C, V66, P364, DOI 10.1016/j.aeue.2011.09.001
   Seitz J., 2005, DIGITAL WATERMARKING
   Selesnick IW, 2005, IEEE SIGNAL PROC MAG, V22, P123, DOI 10.1109/MSP.2005.1550194
   Song HH, 2008, SIGNAL PROCESS-IMAGE, V23, P162, DOI 10.1016/j.image.2008.01.005
   Zareian M, 2013, IET IMAGE PROCESS, V7, P432, DOI 10.1049/iet-ipr.2013.0048
   Zhou Y, 2012, IET IMAGE PROCESS, V6, P1136, DOI 10.1049/iet-ipr.2012.0148
   [No title captured]
NR 53
TC 70
Z9 74
U1 1
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2016
VL 18
IS 2
BP 196
EP 207
DI 10.1109/TMM.2015.2508147
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DB3HX
UT WOS:000368402400005
DA 2024-07-18
ER

PT J
AU Shih, KT
   Chen, HH
AF Shih, Kuang-Tsu
   Chen, Homer H.
TI Exploiting Perceptual Anchoring for Color Image Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Anchoring theory; color appearance model; color image enhancement; human
   perception
ID APPEARANCE MODEL; ADAPTATION; QUALITY; POWER
AB The preservation of image quality under various display conditions becomes more and more important in the multimedia era. A considerable amount of effort has been devoted to compensating the quality degradation caused by dim LCD backlight for mobile devices and desktop monitors. However, most previous enhancement methods for backlight-scaled images only consider the luminance component and overlook the impact of color appearance on image quality. In this paper, we propose a fast and elegant method that exploits the anchoring property of human visual system to preserve the color appearance of backlight-scaled images as much as possible. Our approach is distinguished from previous ones in many aspects. First, it has a sound theoretical basis. Second, it takes the luminance and chrominance components into account in an integral manner. Third, it has low complexity and can process 720p high-definition videos at 35 frames per second without flicker. The superior performance of the proposed method is verified through psychophysical tests.
C1 [Shih, Kuang-Tsu] Natl Taiwan Univ, Grad Inst Commun Engn, Taipei 10617, Taiwan.
   [Chen, Homer H.] Natl Taiwan Univ, Dept Elect Engn, Grad Inst Commun Engn, Taipei 10617, Taiwan.
   [Chen, Homer H.] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10617, Taiwan.
C3 National Taiwan University; National Taiwan University; National Taiwan
   University
RP Shih, KT (corresponding author), Natl Taiwan Univ, Grad Inst Commun Engn, Taipei 10617, Taiwan.; Chen, HH (corresponding author), Natl Taiwan Univ, Dept Elect Engn, Grad Inst Commun Engn, Taipei 10617, Taiwan.
EM shihkt@gmail.com; homer@ntu.edu.tw
OI Chen, Homer/0000-0002-8795-1911
FU National Science Council of Taiwan [NSC 100-2221-E-002-197-MY3];
   National Taiwan University [NTU-CESRP-102R7609-2]; Himax Technologies,
   Inc. [101-S-C37]
FX This work was supported in part by the National Science Council of
   Taiwan under Contract NSC 100-2221-E-002-197-MY3, in part by the
   National Taiwan University under Contract NTU-CESRP-102R7609-2, and in
   part by Himax Technologies, Inc. under Contract 101-S-C37. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Wolfgang Hurst.
CR [Anonymous], 2010, P USENIX ANN TECH C
   [Anonymous], 2008, Color Gamut Mapping
   [Anonymous], 2000, 619664 IEC
   Banterle F., 2006, P 4 INT C COMP GRAPH, P349
   BERNS RS, 1993, COLOR RES APPL, V18, P299, DOI 10.1002/col.5080180504
   Braun GJ, 1999, J ELECTRON IMAGING, V8, P380, DOI 10.1117/1.482706
   Cho H, 2009, IEEE T CONSUM ELECTR, V55, P839, DOI 10.1109/TCE.2009.5174463
   Cho SI, 2013, J DISP TECHNOL, V9, P112, DOI 10.1109/JDT.2012.2236299
   Deng CW, 2012, IEEE T MULTIMEDIA, V14, P1127, DOI 10.1109/TMM.2012.2191270
   Fairchild M.D., 2005, Color Appearance Models, V2nd
   FAIRCHILD MD, 1993, COLOR RES APPL, V18, P178, DOI 10.1002/col.5080180308
   Ghinea G, 2005, IEEE T MULTIMEDIA, V7, P786, DOI 10.1109/TMM.2005.850960
   Grossberg MD, 2004, PROC CVPR IEEE, P452
   Guruprasad R, 2015, IEEE T MULTIMEDIA, V17, P1630, DOI 10.1109/TMM.2015.2436821
   Heidrich Wolfgang, ERIK REINHARD
   Huang TH, 2013, IEEE T IMAGE PROCESS, V22, P4587, DOI 10.1109/TIP.2013.2272517
   Huang TH, 2012, IEEE INT WORKSH MULT, P192, DOI 10.1109/MMSP.2012.6343439
   Hung-Shing C, 2000, IS&T'S NIP16: INTERNATIONAL CONFERENCE ON DIGITAL PRINTING TECHNOLOGIES, P783
   HUNT RWG, 1991, COLOR RES APPL, V16, P146, DOI 10.1002/col.5080160306
   Iranli A, 2006, IEEE T VLSI SYST, V14, P1103, DOI 10.1109/TVLSI.2006.884151
   Kwak Y, 2001, NINTH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P355
   Lan TH, 2003, IEEE T MULTIMEDIA, V5, P267, DOI 10.1109/TMM.2003.812714
   Li C., 2002, PERFORMANCE CIECAM02, P28
   Lin C.-W., 2014, Proceedings of the 51st Annual Design Automation Conference (DAC'14), P1
   Lombardo A, 2014, IEEE T MULTIMEDIA, V16, P2307, DOI 10.1109/TMM.2014.2350257
   MacDonald L, 2002, J IMAGING SCI TECHN, V46, P228
   Massouh N., 2014, P EUR WORKSH VIS INF, P1
   Moroney N., 2002, P COL IM C, P23
   Morovic J, 2001, J IMAGING SCI TECHN, V45, P283
   Morovic J., 2001, P SPIE PHOTONICS W, P114
   NAYATANI Y, 1987, COLOR RES APPL, V12, P231, DOI 10.1002/col.5080120504
   NAYATANI Y, 1990, COLOR RES APPL, V15, P210, DOI 10.1002/col.5080150407
   Nimmagadda Y, 2010, IEEE T MULTIMEDIA, V12, P650, DOI 10.1109/TMM.2010.2052024
   Pei SC, 2012, IEEE SIGNAL PROC LET, V19, P813, DOI 10.1109/LSP.2012.2220352
   Rahman ZU, 2004, J ELECTRON IMAGING, V13, P100, DOI 10.1117/1.1636183
   Robertson A.R., 1977, Color Res. Appl., V2, P7, DOI [10.1002/j.1520-6378.1977.tb00104.x, DOI 10.1002/J.1520-6378.1977.TB00104.X]
   Shih KT, 2013, IEEE INT WORKSH MULT, P153, DOI 10.1109/MMSP.2013.6659280
   Smith T., 1931, Trans. Opt. Soc, V33, P73, DOI DOI 10.1088/1475-4878/33/3/301
   Tamura N., 2003, Journal of the Society for Information Display, V11, P333, DOI 10.1889/1.1825664
   Tsai PS, 2009, IEEE T CIRC SYST VID, V19, P574, DOI 10.1109/TCSVT.2009.2014022
   Wang TH, 2015, IEEE T MULTIMEDIA, V17, P470, DOI 10.1109/TMM.2015.2403612
   Wei Y, 2006, IEEE T MULTIMEDIA, V8, P866, DOI 10.1109/TMM.2006.876232
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Yin WY, 2011, IEEE T MULTIMEDIA, V13, P432, DOI 10.1109/TMM.2011.2129501
   Zeng H., 2006, P IS T SID 9 COL IM, P240
NR 45
TC 13
Z9 13
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2016
VL 18
IS 2
BP 300
EP 310
DI 10.1109/TMM.2015.2503918
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DB3HX
UT WOS:000368402400013
DA 2024-07-18
ER

PT J
AU Sun, YP
   Tao, XM
   Li, Y
   Dong, LH
   Lu, JH
AF Sun, Yipeng
   Tao, Xiaoming
   Li, Yang
   Dong, Linhao
   Lu, Jianhua
TI HEMS: Hierarchical Exemplar-Based Matching-Synthesis for Object-Aware
   Image Reconstruction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Exemplar; image coding; image synthesis; low bit-rate; salient object;
   visual quality
ID VISUAL-ATTENTION; NEUROBIOLOGICAL MODEL; COMPRESSION; REGION; FOVEATION;
   REMOVAL; SEARCH
AB Motivated by the attention on salient objects, conventional region-of-interest (ROI)-based image coding approaches attempt to assign more bits to ROIs and fewer bits to other regions. Thus, the perceptual quality of salient object regions is improved by sacrificing the quality of non-ROI regions with unpleasant artifacts. To address this issue, we concentrate on the efficient compression of object-centered images by encoding salient objects and background features separately. To fully recover the object and background, we propose a hierarchical exemplar-based matching-synthesis (HEMS) approach to reconstruct the image from exemplars. In the proposed framework, once the salient object regions are encoded, only the quantized color features and local descriptors of the background are kept, achieving bit-rate reduction. To make it possible and practical to reconstruct background regions, the hierarchical framework is designed in three layers, including relevant image search, patch candidates matching, and distortion optimized image synthesis. In the hierarchical framework, firstly, image search from an external database returns relevant images, limiting the search space to a feasible number of patch candidates. Secondly, patches are matched by color features to select the appropriate candidates. Finally, the distortion optimized image synthesis further makes it possible to automatically choose the most suitable texture sample, and seamlessly reconstruct the image. Compared to the conventional ROI-based image coding schemes, the proposed approach can achieve better visual quality on both ROI and background regions.
C1 [Sun, Yipeng; Tao, Xiaoming; Lu, Jianhua] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
   [Sun, Yipeng] Alipay Inc, Beijing 100080, Peoples R China.
   [Li, Yang] Google Inc, Pittsburgh, PA 15206 USA.
   [Dong, Linhao] Tsinghua Univ, Sch Aerosp Engn, Beijing 100084, Peoples R China.
C3 Tsinghua University; Google Incorporated; Tsinghua University
RP Tao, XM (corresponding author), Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
EM albertsun.syp@alibaba-inc.com; taoxm@tsinghua.edu.cn;
   liy-11@mails.tsinghua.edu.cn; linhaodong@tsinghua.edu.cn;
   lhh-dee@tsinghua.edu.cn
RI Tao, XiaoMing/A-9992-2010; Li, Yang/GWM-7879-2022; Li, Yang/A-3496-2015
OI Li, Yang/0000-0002-2053-6393; Li, Yang/0000-0003-2202-8525
FU National Basic Research Program of China (973 Program) [2013CB329006];
   National Natural Science Foundation of China (NSFC) [61471220, 61321061,
   61101071]; Tsinghua University Initiative Scientific Research Program;
   Tsinghua-Qualcomm Joint Research program
FX This work was supported by the National Basic Research Program of China
   (973 Program) under Grant 2013CB329006, by the National Natural Science
   Foundation of China (NSFC) under Grant 61471220, Grant 61321061, and
   Grant 61101071, by the Tsinghua University Initiative Scientific
   Research Program, and by the Tsinghua-Qualcomm Joint Research program.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Jing-Ming Guo. (Corresponding
   author: Xiaoming Tao.)
CR [Anonymous], 2000, 154441 ISOIEC
   [Anonymous], 2013, H 264 AVC REF SOFTW
   [Anonymous], 2013, KAK JPEG2000 SOFTW D
   [Anonymous], 2012, JCTVCJ1003
   [Anonymous], 2014, PROC BRIT MACH VIS C
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Christopoulos C, 2000, IEEE SIGNAL PROC LET, V7, P247, DOI 10.1109/97.863146
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Criminisi A, 2003, PROC CVPR IEEE, P721
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Farbman Z, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531373
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Galíc I, 2008, J MATH IMAGING VIS, V31, P255, DOI 10.1007/s10851-008-0087-0
   Girod B, 2011, IEEE SIGNAL PROC MAG, V28, P61, DOI 10.1109/MSP.2011.940881
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hays J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239455
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Li HQ, 2013, IEEE T MULTIMEDIA, V15, P594, DOI 10.1109/TMM.2012.2234730
   Liu D, 2007, IEEE T CIRC SYST VID, V17, P1273, DOI 10.1109/TCSVT.2007.903663
   Liu LJ, 2003, IEEE SIGNAL PROC LET, V10, P35, DOI 10.1109/LSP.2002.807867
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Manning Christopher D., 2008, INTRO INFORM RETRIEV
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   Marpe D, 2006, IEEE COMMUN MAG, V44, P134, DOI 10.1109/MCOM.2006.1678121
   Nadenau MJ, 2003, IEEE T IMAGE PROCESS, V12, P58, DOI 10.1109/TIP.2002.807358
   Ndjiki-Nya P, 2012, SIGNAL PROCESS-IMAGE, V27, P579, DOI 10.1016/j.image.2012.01.003
   Nister David, 2006, CVPR
   OLSHAUSEN BA, 1993, J NEUROSCI, V13, P4700
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Qi H, 2014, IEEE T MULTIMEDIA, V16, P1963, DOI 10.1109/TMM.2014.2345026
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Rubinstein M, 2013, PROC CVPR IEEE, P1939, DOI 10.1109/CVPR.2013.253
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Sanchez V, 2004, IEEE T CIRC SYST VID, V14, P1149, DOI 10.1109/TCSVT.2004.833168
   Sang JT, 2013, IEEE T MULTIMEDIA, V15, P1665, DOI 10.1109/TMM.2013.2268052
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Sun YP, 2014, IEEE T CIRC SYST VID, V24, P2004, DOI 10.1109/TCSVT.2014.2319652
   Taubman D, 2000, IEEE T IMAGE PROCESS, V9, P1158, DOI 10.1109/83.847830
   TSOTSOS JK, 1995, ARTIF INTELL, V78, P507, DOI 10.1016/0004-3702(95)00025-9
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2001, IEEE T IMAGE PROCESS, V10, P1397, DOI 10.1109/83.951527
   Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566
   Xiong ZW, 2010, IEEE T IMAGE PROCESS, V19, P1651, DOI 10.1109/TIP.2010.2044960
   Yue HJ, 2013, IEEE T MULTIMEDIA, V15, P845, DOI 10.1109/TMM.2013.2239629
   Zheng L, 2014, IEEE T IMAGE PROCESS, V23, P3604, DOI 10.1109/TIP.2014.2329182
NR 51
TC 5
Z9 5
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2016
VL 18
IS 2
BP 171
EP 181
DI 10.1109/TMM.2015.2496246
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DB3HX
UT WOS:000368402400003
DA 2024-07-18
ER

PT J
AU Fang, Q
   Xu, CS
   Sang, JT
   Hossain, MS
   Muhammad, G
AF Fang, Quan
   Xu, Changsheng
   Sang, Jitao
   Hossain, M. Shamim
   Muhammad, Ghulam
TI Word-of-Mouth Understanding: Entity-Centric Multimodal Aspect-Opinion
   Mining in Social Media
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Application; knowledge mining; probabilistic topic model
AB Most existing approaches on aspect-opinion mining focus on the text domain and cannot be applied to social media where the aspects are essentially multimodal and the opinions depend on the specific aspects. To address the problem of multimodal aspect-opinion mining for entities by leveraging multiple cross-collection sources in social media, in this paper we propose a multimodal aspect-opinion model (mmAOM) considering both user-generated photos and textual documents to simultaneously capture correlations between textual and visual modalities, as well as associations between aspects and opinions. By identifying the aspects and the corresponding opinions related to entities, we apply the mmAOM to entity association visualization and multimodal aspect-opinion retrieval. We have conducted extensive experiments on real-world datasets of entities including Flickr photos, Tripadvisor reviews, and news articles. Qualitative and quantitative evaluation results have validated the effectiveness of the multimodal aspect-opinion mining model, and demonstrated the utility of the derived aspects and opinions from mmAOM in applications of entity association visualization and aspect-opinion retrieval.
C1 [Fang, Quan; Xu, Changsheng; Sang, Jitao] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Hossain, M. Shamim] King Saud Univ, Coll Comp & Informat Sci, Dept Software Engn, Riyadh 11543, Saudi Arabia.
   [Muhammad, Ghulam] King Saud Univ, Coll Comp & Informat Sci, Dept Comp Engn, Riyadh 11543, Saudi Arabia.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; King Saud
   University; King Saud University
RP Fang, Q (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM qfang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn; jtsang@nlpr.ia.ac.cn;
   mshossain@ksu.edu.sa; ghulam@ksu.edu.sa
RI xu, cj/HJZ-3488-2023; Hossain, M. Shamim/K-1362-2014; Guizani,
   Mohsen/AAX-4534-2021; Muhammad, Ghulam/H-5884-2011
OI Hossain, M. Shamim/0000-0001-5906-9422; Guizani,
   Mohsen/0000-0002-8972-8094; Muhammad, Ghulam/0000-0002-9781-3969
FU National Basic Research Program of China [2012CB316304]; National
   Natural Science Foundation of China [61225009, 61432019, 61332016,
   61303176]; Beijing Natural Science Foundation [4131004]; Deanship of
   Scientific Research, King Saud University, Riyadh, Saudi Arabia
   [RGP-1436-023]
FX This work was supported in part by the National Basic Research Program
   of China under Grant 2012CB316304, in part by the National Natural
   Science Foundation of China under Grant 61225009, Grant 61432019, Grant
   61332016, and Grant 61303176, in part by the Beijing Natural Science
   Foundation under Grant 4131004, and in part by the Deanship of
   Scientific Research, King Saud University, Riyadh, Saudi Arabia, under
   research group project RGP-1436-023. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Vasileios Mezaris.
CR Akiva N., 2008, P 2 INT C WEBL SOC M, P170
   [Anonymous], 2010, P 17 INT C WORLD WID, DOI DOI 10.1145/1772690.1772732
   [Anonymous], 2013, PROC THE 21 ACM INT
   [Anonymous], 2012, Mining Text Data, DOI [10.1007/978-1-4614-3223-4_13, DOI 10.1007/978-1-4614-3223-4_13, DOI 10.1007/978-1-4614-3223-413]
   [Anonymous], 2008, Cambridge Series in Statistical and Probabilistic Mathematics
   Araújo M, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P75, DOI 10.1145/2567948.2577013
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   Bian JW, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1807, DOI 10.1145/2505515.2505652
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Borth D., 2013, P 21 ACM INT C MULTI, P223, DOI 10.1145/2502081.2502282
   Carmel D., 2009, P CIKM, P1227
   Carter KM, 2009, 2009 IEEE/SP 15TH WORKSHOP ON STATISTICAL SIGNAL PROCESSING, VOLS 1 AND 2, P405, DOI 10.1109/SSP.2009.5278554
   Cheng A.-J., 2011, P 19 ACM INT C MULTI, P83
   Dodge Jesse., 2012, HLT-NAACL, P762
   Esuli Andrea., 2006, LREC 2006 Proceedings, 2006, S, P417
   Fang Yi., 2012, Proceedings of the Fifth ACM International Conference on Web Search and Data Mining (WSDM '12), P63, DOI DOI 10.1145/2124295.2124306
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Hu M., 2004, P 10 ACM SIGKDD INT, P168, DOI DOI 10.1145/1014052.1014073
   Hutto C., 2014, P INT AAAI C WEB SOC, V8, P216, DOI [DOI 10.1609/ICWSM.V8I1.14550, 10.1609/icwsm.v8i1.14550]
   Jia YQ, 2011, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2011.6126524
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   KELLER KL, 1993, J MARKETING, V57, P1, DOI 10.2307/1252054
   Kim G, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P623, DOI 10.1145/2556195.2556212
   Li QN, 2013, PROC CVPR IEEE, P851, DOI 10.1109/CVPR.2013.115
   Machajdik J., 2010, P 18 ACM INT C MULT, P83, DOI DOI 10.1145/1873951.1873965
   Maynard D, 2013, BCS SGAI WORKSH SOC
   Meng X, 2012, P 18 ACM SIGKDD INT, P379, DOI [DOI 10.1145/2339530.2339592, 10.1145/2339530.2339592]
   Moghaddam Samaneh., 2012, Proceedings of the 21st ACM international conference on Information and knowledge management, P803, DOI DOI 10.1145/2396761.2396863
   Mukherjee A., 2012, P 50 ANN M ASS COMP, V1, P339
   Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6
   Sun AX, 2011, SOCIAL MEDIA MODELING AND COMPUTING, P3, DOI 10.1007/978-0-85729-436-4_1
   Thelwall Mike, 2013, Computational Linguistics and Intelligent Text Processing. 14th International Conference, CICLing 2013. Proceedings, P1, DOI 10.1007/978-3-642-37256-8_1
   Virtanen S., 2012, Proceedings of the 28th Conference on Uncertainty in Artificial Intelligence, UAI '12, P843
   Wang WN, 2008, IEEE IMAGE PROC, P117, DOI 10.1109/ICIP.2008.4711705
   Xie L, 2011, P 19 ACM INT C MULT, P53, DOI DOI 10.1145/2072298.2072307
   Zafarani R., 2014, SOCIAL MEDIIA MINING
   Zhao X., 2010, Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid, P56
   Zhao Y, 2004, MACH LEARN, V55, P311, DOI 10.1023/B:MACH.0000027785.44527.d6
   Zhu Jianke., 2008, P 16 ACM INT C MULTI, P41
NR 41
TC 33
Z9 34
U1 1
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2015
VL 17
IS 12
BP 2281
EP 2296
DI 10.1109/TMM.2015.2491019
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CW9JR
UT WOS:000365315500015
DA 2024-07-18
ER

PT J
AU Lu, X
   Lin, Z
   Jin, HL
   Yang, JC
   Wang, JZ
AF Lu, Xin
   Lin, Zhe
   Jin, Hailin
   Yang, Jianchao
   Wang, James. Z.
TI Rating Image Aesthetics Using Deep Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Automatic feature learning; deep neural networks; image aesthetics
ID QUALITY
AB This paper investigates unified feature learning and classifier training approaches for image aesthetics assessment. Existing methods built upon handcrafted or generic image features and developed machine learning and statistical modeling techniques utilizing training examples. We adopt a novel deep neural network approach to allow unified feature learning and classifier training to estimate image aesthetics. In particular, we develop a double-column deep convolutional neural network to support heterogeneous inputs, i.e., global and local views, in order to capture both global and local characteristics of images. In addition, we employ the style and semantic attributes of images to further boost the aesthetics categorization performance. Experimental results show that our approach produces significantly better results than the earlier reported results on the AVA dataset for both the generic image aesthetics and content-based image aesthetics. Moreover, we introduce a 1.5-million image dataset (IAD) for image aesthetics assessment and we further boost the performance on the AVA test set by training the proposed deep neural networks on the IAD dataset.
C1 [Lu, Xin; Wang, James. Z.] Penn State Univ, Coll Informat Sci & Technol, University Pk, PA 16802 USA.
   [Lin, Zhe; Jin, Hailin; Yang, Jianchao] Adobe Syst Inc, Adobe Res, San Jose, CA 95110 USA.
C3 Pennsylvania Commonwealth System of Higher Education (PCSHE);
   Pennsylvania State University; Pennsylvania State University -
   University Park; Adobe Systems Inc.
RP Lu, X (corresponding author), Penn State Univ, Coll Informat Sci & Technol, University Pk, PA 16802 USA.
EM xinlu@psu.edu; zlin@adobe.com; hljin@adobe.com; jiayang@adobe.com;
   jwang@psu.edu
RI Wang, James/JAD-0675-2023; Lu, Xinrui/R-6072-2019
OI Lu, Xinrui/0000-0002-3789-0372; Wang, James/0000-0003-4379-4173
CR [Anonymous], 2008, PROC INT C MACHINE L
   [Anonymous], 2011, ACM International Conference on Multimedia MM
   [Anonymous], 2010, ACM MULTIMEDIA 2010
   [Anonymous], 2014, P BMVC
   [Anonymous], 2013, NIPS
   Arnheim R., 1974, Art and Visual Perception, a Psychology of the Creative Eye
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Donahue J., 2013, ARXIV13101531V1
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Khosla A, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P867, DOI 10.1145/2566486.2567996
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Litzel O., 1974, On Photographic Composition
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu Xin, 2012, Proc ACM Int Conf Multimed, V2012, P229, DOI 10.1145/2393347.2393384
   Luo W, 2011, IEEE I CONF COMP VIS, P2206, DOI 10.1109/ICCV.2011.6126498
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Marchesotti L, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.7
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   NIEKAMP W, 1981, ECTJ-EDUC COMMUN TEC, V29, P37
   Nishiyama M, 2011, PROC CVPR IEEE, P33, DOI 10.1109/CVPR.2011.5995539
   O'Donovan P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964958
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Panigrahi S., 2021, IEEE Transactions on Knowledge and Data Engineering, V194, P781, DOI [DOI 10.1109/TKDE.2009.191, 10.1007/978-981-15-5971-6_83]
   Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465
   Sun Y, 2013, IEEE I CONF COMP VIS, P1489, DOI 10.1109/ICCV.2013.188
NR 32
TC 145
Z9 155
U1 2
U2 42
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2015
VL 17
IS 11
SI SI
BP 2021
EP 2034
DI 10.1109/TMM.2015.2477040
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CV2QV
UT WOS:000364102400015
DA 2024-07-18
ER

PT J
AU Samuel, A
   Sarfraz, MI
   Haseeb, H
   Basalamah, S
   Ghafoor, A
AF Samuel, Arjmand
   Sarfraz, Muhammad I.
   Haseeb, Hammad
   Basalamah, Saleh
   Ghafoor, Arif
TI A Framework for Composition and Enforcement of Privacy-Aware and
   Context-Driven Authorization Mechanism for Multimedia Big Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Access control; context; data privacy; formal verification; multimedia
   databases
ID SPATIOTEMPORAL VEHICLE TRACKING; MODEL; MANAGEMENT; SECURITY
AB The proliferation of multimedia big data for dissemination and sharing of massive amounts of information raises important security and privacy concerns. One such concern is the composition and enforcement of privacy policies in order to securely manage access of multimedia big data. Several researchers have pointed out that for proper enforcement of privacy policies, the privacy requirements should be captured in access control systems. In this paper, we propose a hybrid approach where privacy requirements are captured in an access control system and present a framework for composition and enforcement of privacy policies. The focus is to allow a user, not a system or security administrator to compose conflict free policies for their online multimedia data. An additional requirement is that such a policy be context-aware. We also present a methodology for verifying the privacy policy in order to ensure correctness and logical consistency. The verification process is also used to ensure that sensitive security requirements are not violated when privacy rules are enforced. A prototype, named Intelligent Privacy Manager (iPM), has been implemented for sharing of multimedia big data in a secure and private manner.
C1 [Samuel, Arjmand] Microsoft Res, Redmond, WA 98052 USA.
   [Sarfraz, Muhammad I.; Haseeb, Hammad; Ghafoor, Arif] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47906 USA.
   [Basalamah, Saleh] Umm Al Qura Univ, KACST GIS Technol Innovat Ctr, Mecca 24381, Saudi Arabia.
C3 Microsoft; Purdue University System; Purdue University; Umm Al Qura
   University
RP Samuel, A (corresponding author), Microsoft Res, Redmond, WA 98052 USA.
EM arjmands@microsoft.com; msarfraz@purdue.edu; hammad_pakistan@yahoo.com;
   smbasalamah@uqu.edu.sa; ghafoor@purdue.edu
OI Basalamah, Saleh/0000-0002-2276-8307
FU U.S. National Science Foundation [IIS-0964639]; Div Of Information &
   Intelligent Systems; Direct For Computer & Info Scie & Enginr [0964639]
   Funding Source: National Science Foundation
FX This work was supported by the U.S. National Science Foundation under
   Grant IIS-0964639. The guest editor coordinating the review of this
   manuscript and approving it for publication was Dr. Shu-Ching Chen.
CR [Anonymous], 2005, 7 OASIS
   Ardagna CA, 2008, J COMPUT SECUR, V16, P369, DOI 10.3233/JCS-2008-0328
   Ashley Paul., 2002, PROCEEDING ACM WORKS, P103, DOI DOI 10.1145/644527.644538
   Atluri V, 2004, IEEE T DEPEND SECURE, V1, P238, DOI 10.1109/TDSC.2004.32
   Bertino Elisa., 2005, SACMAT 05, P29
   Bhatti R., 2005, ACM Transactions on Information and Systems Security, V8, P388, DOI 10.1145/1108906.1108909
   Bhatti R, 2005, COMPUTER, V38, P60, DOI 10.1109/MC.2005.296
   Bhatti R, 2006, IEEE T SOFTWARE ENG, V32, P330, DOI 10.1109/TSE.2006.49
   Bowen J. P., 2001, Z FORMAL SPECIFICATI
   Señor IC, 2012, COMPUTER, V45, P27, DOI 10.1109/MC.2012.285
   Chen SC, 2005, IEEE ROBOT AUTOM MAG, V12, P50, DOI 10.1109/MRA.2005.1411419
   Chen SC, 2003, IEEE T INTELL TRANSP, V4, P154, DOI 10.1109/TITS.2003.821290
   Holzmann GJ, 1997, IEEE T SOFTWARE ENG, V23, P279, DOI 10.1109/32.588521
   Jackson D, 2002, ACM T SOFTW ENG METH, V11, P256, DOI 10.1145/505145.505149
   Jacobson I, 1999, IEEE SOFTWARE, V16, P96
   Jajodia S, 2001, ACM T DATABASE SYST, V26, P214, DOI 10.1145/383891.383894
   Joshi JBD, 2005, IEEE T KNOWL DATA EN, V17, P4, DOI 10.1109/TKDE.2005.1
   Joshi JBD, 2005, IEEE T DEPEND SECURE, V2, P157, DOI 10.1109/TDSC.2005.18
   Joshi JBD, 2002, IEEE T MULTIMEDIA, V4, P215, DOI 10.1109/TMM.2002.1017735
   Jürjens J, 2001, INT FED INFO PROC, V74, P489
   Jurjens J., 2001, Fundamental Approaches to Software Engineering. 4th International Conference, FASE 2001. Held as Part of the Joint European Conferences on Theory and Practice of Software, ETAPS 2001. Proceedings (Lecture Notes in Computer Science Vol.2029), P187
   Karjoth G, 2003, LECT NOTES COMPUT SC, V2482, P69
   Karjoth G, 2002, P IEEE CSFW, P271, DOI 10.1109/CSFW.2002.1021821
   Pearson S, 2011, COMPUTER, V44, P60, DOI 10.1109/MC.2011.225
   Privacy Rights Clearinghouse San Diego CA USA, 2005, CHRON DAT BREACH
   Raghupathi W, 2014, HEALTH INF SCI SYST, V2, DOI 10.1186/2047-2501-2-3
   Samuel A., 2007, 200708 CERIAS TR PUR
   Schunter M., 2003, 3485 IBM RZ
   Soon Ae Chun, 2001, Data and Applications Security. Developments and Directions. IFIP TC11/WG11.3 Fourteenth Annual Working Conference on Database Security, P233
   Weill P., 2013, Place to space: Migrating to eBusiness Models
   Zao J., 2003, P SACMAT
   Zhang K, 2014, IEEE COMMUN MAG, V52, P58, DOI 10.1109/MCOM.2014.6766086
   Zhu WW, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940269
NR 33
TC 30
Z9 33
U1 0
U2 53
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2015
VL 17
IS 9
SI SI
BP 1484
EP 1494
DI 10.1109/TMM.2015.2458299
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CP0RN
UT WOS:000359583000009
DA 2024-07-18
ER

PT J
AU Wang, DX
   Cui, P
   Ou, MD
   Zhu, WW
AF Wang, Daixin
   Cui, Peng
   Ou, Mingdong
   Zhu, Wenwu
TI Learning Compact Hash Codes for Multimodal Representations Using
   Orthogonal Deep Structure
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; multimodal hashing; similarity search
AB As large-scale multimodal data are ubiquitous in many real-world applications, learning multimodal representations for efficient retrieval is a fundamental problem. Most existing methods adopt shallow structures to perform multimodal representation learning. Due to a limitation of learning ability of shallow structures, they fail to capture the correlation of multiple modalities. Recently, multimodal deep learning was proposed and had proven its superiority in representing multimodal data due to its high nonlinearity. However, in order to learn compact and accurate representations, how to reduce the redundant information lying in the multimodal representations and incorporate different complexities of different modalities in the deep models is still an open problem. In order to address the aforementioned problem, in this paper we propose a hashing-based orthogonal deep model to learn accurate and compact multimodal representations. The method can better capture the intra-modality and inter-modality correlations to learn accurate representations. Meanwhile, in order to make the representations compact, the hashing-based model can generate compact hash codes and the proposed orthogonal structure can reduce the redundant information lying in the codes by imposing orthogonal regularizer on the weighting matrices. We also theoretically prove that, in this case, the learned codes are guaranteed to be approximately orthogonal. Moreover, considering the different characteristics of different modalities, effective representations can be attained with different number of layers for different modalities. Comprehensive experiments on three real-world datasets demonstrate a substantial gain of our method on retrieval tasks compared with existing algorithms.
C1 [Wang, Daixin; Cui, Peng; Ou, Mingdong; Zhu, Wenwu] Tsinghua Univ, Dept Comp Sci & Technol, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Wang, DX (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
FU National Basic Research Program of China [2015CB352300]; National
   Natural Science Foundation of China [61370022, 61210008]; International
   Science and Technology Cooperation Program of China [2013DFG12870]
FX This work was supported by the National Basic Research Program of China
   under Grant 2015CB352300, the National Natural Science Foundation of
   China under Grant 61370022 and Grant 61210008, and the International
   Science and Technology Cooperation Program of China under Grant
   2013DFG12870. The guest editor coordinating the review of this
   manuscript and approving it for publication was Dr. Shu-Ching Chen.
CR [Anonymous], J PHARM ALLIED HLTH, DOI [10.3923/jpahs.2011.1.15, DOI 10.3923/JPAHS.2011.1.15]
   [Anonymous], 2004, P 20 ACM S COMP
   [Anonymous], 2010, PROC ACM SIGMM INT C
   [Anonymous], 2011, P ICML
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2012, Book A probabilistic model for multimodal hash function learning, DOI DOI 10.1145/2339530.2339678
   [Anonymous], 2012, IMPROVING NEURAL NET
   [Anonymous], 2013, P 2013 C EMPIRICAL M
   [Anonymous], 2004, Advances in Neural Information Processing Systems
   [Anonymous], 2012, Advances in neural information processing systems
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bordes A., 2012, Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics, PMLR, V22, P127
   Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Fan HQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P933, DOI 10.1145/2647868.2654960
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Guillaumin M, 2010, PROC CVPR IEEE, P902, DOI 10.1109/CVPR.2010.5540120
   Hinton G. E., 2012, Neural networks: tricks of the trade, P599
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2009, INT C NEURAL INF PRO, P1607
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hyvärinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5
   Joly Alexis., 2008, PROCEEDING 16 ACM IN, P209
   Kang Y, 2012, IEEE DATA MINING, P930, DOI 10.1109/ICDM.2012.24
   Le Q. V., 2011, P 28 INT C INT C MAC, P265
   Li X, 2013, PROC CVPR IEEE, P2419, DOI 10.1109/CVPR.2013.313
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu Wei, 2011, Reports in Parasitology, V1, P1
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Lv Q, 2007, P 33 INT C VER LARG, P950
   Norouzi M.E., 2011, ICML
   Ou MD, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P230
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Rastegari Mohammad, 2013, Book Predictable Dual-View Hashing
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Srivastava N., 2012, P INT C MACH LEARN W
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742
   Torralba A., 2008, PROC IEEE C COMPUT V, P1
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Wang W, 2014, PROC VLDB ENDOW, V7, P649, DOI 10.14778/2732296.2732301
   Weiss Y., 2008, NIPS, V9, P6
   WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Wu ZX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P167, DOI 10.1145/2647868.2654931
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Zhai Deming., 2013, Proceedings of the 23rd International Joint Conference on Artificial Intelligence, P2754
   Zhang D, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P225
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
NR 54
TC 77
Z9 81
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2015
VL 17
IS 9
SI SI
BP 1404
EP 1416
DI 10.1109/TMM.2015.2455415
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CP0RN
UT WOS:000359583000002
DA 2024-07-18
ER

PT J
AU Zhou, Z
   Shi, F
   Wu, W
AF Zhou, Zhong
   Shi, Feng
   Wu, Wei
TI Learning Spatial and Temporal Extents of Human Actions for Action
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action localization; action recognition; discriminative latent variable
   model; split-and-merge
ID FRAMEWORK; MODELS
AB For the problem of action detection, most existing methods require that relevant portions of the action of interest in training videos have been manually annotated with bounding boxes. Some recent works tried to avoid tedious manual annotation, and proposed to automatically identify the relevant portions in training videos. However, these methods only concerned the identification in either spatial or temporal domain, and may get irrelevant contents from another domain. These irrelevant contents are usually undesirable in the training phase, which will lead to a degradation of the detection performance. This paper advances prior work by proposing a joint learning framework to simultaneously identify the spatial and temporal extents of the action of interest in training videos. To get pixel-level localization results, our method uses dense trajectories extracted from videos as local features to represent actions. We first present a trajectory split-and-merge algorithm to segment a video into the background and several separated foreground moving objects. In this algorithm, the inherent temporal smoothness of human actions is exploited to facilitate segmentation. Then, with the latent SVM framework on segmentation results, spatial and temporal extents of the action of interest are treated as latent variables that are inferred simultaneously with action recognition. Experiments on two challenging datasets show that action detection with our learned spatial and temporal extents is superior than state-of-the-art methods.
C1 [Zhou, Zhong; Shi, Feng; Wu, Wei] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
C3 Beihang University
RP Zhou, Z (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM zz@buaa.edu.cn; supersf2008@hotmail.com; wuwei@buaa.edu.cn
RI Shi, Feng/G-3247-2012
FU National 863 Program of China [2012AA011803]; Natural Science Foundation
   of China [61472020, 61170188]; National Key Technology R&D Program of
   China [2012BAI06B01]
FX This work was supported by the National 863 Program of China under Grant
   2012AA011803, the Natural Science Foundation of China under Grants
   61472020 and 61170188, and the National Key Technology R&D Program of
   China under Grant 2012BAI06B01. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Cees
   Snoek.
CR [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P BRIT MACH VIS C SE
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Brendel W, 2011, IEEE I CONF COMP VIS, P778, DOI 10.1109/ICCV.2011.6126316
   Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143
   Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21
   Cheriyadat AM, 2009, IEEE I CONF COMP VIS, P865, DOI 10.1109/ICCV.2009.5459311
   Du Tran, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3321, DOI 10.1109/CVPR.2011.5995416
   Efros AA, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P726
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Hoi SCH, 2008, IEEE T MULTIMEDIA, V10, P607, DOI 10.1109/TMM.2008.921735
   Hu YX, 2009, IEEE I CONF COMP VIS, P128, DOI 10.1109/ICCV.2009.5459153
   Jain M, 2014, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2014.100
   Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330
   Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396
   Jiang YG, 2012, LECT NOTES COMPUT SC, V7576, P425, DOI 10.1007/978-3-642-33715-4_31
   Ke Y., 2007, PROC IEEE INT C COMP
   Lan T, 2011, IEEE I CONF COMP VIS, P2003, DOI 10.1109/ICCV.2011.6126472
   Ma SG, 2013, IEEE I CONF COMP VIS, P2744, DOI 10.1109/ICCV.2013.341
   Magnus J.R., 1999, MATRIX DIFFERENTIAL
   Raptis M, 2012, PROC CVPR IEEE, P1242, DOI 10.1109/CVPR.2012.6247807
   Rodriguez M. D., 2008, PROC IEEE C COMPUT V, P1
   Satkin S, 2010, LECT NOTES COMPUT SC, V6311, P536, DOI 10.1007/978-3-642-15549-9_39
   Shapovalova N, 2012, LECT NOTES COMPUT SC, V7578, P55, DOI 10.1007/978-3-642-33786-4_5
   Shi F, 2013, IEEE I CONF COMP VIS, P3088, DOI 10.1109/ICCV.2013.383
   Tian YC, 2013, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2013.341
   Tran D., 2012, ADV NEURAL INFORM PR, P350
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vig E, 2012, LECT NOTES COMPUT SC, V7578, P84, DOI 10.1007/978-3-642-33786-4_7
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wu SD, 2011, IEEE I CONF COMP VIS, P1419, DOI 10.1109/ICCV.2011.6126397
   Xie YL, 2011, PROC CVPR IEEE, P25, DOI 10.1109/CVPR.2011.5995648
   Yu G, 2011, IEEE T MULTIMEDIA, V13, P507, DOI 10.1109/TMM.2011.2128301
   Yu T., 2009, P 26 ANN INT C MACH, P1169, DOI DOI 10.1145/1553374.1553523
   Zhang TZ, 2012, IEEE T MULTIMEDIA, V14, P1206, DOI 10.1109/TMM.2012.2191944
NR 36
TC 37
Z9 39
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2015
VL 17
IS 4
BP 512
EP 525
DI 10.1109/TMM.2015.2404779
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CE1QH
UT WOS:000351586300005
OA Bronze
DA 2024-07-18
ER

PT J
AU Xue, YY
   Erkin, B
   Wang, Y
AF Xue, Yuanyi
   Erkin, Beril
   Wang, Yao
TI A Novel No-Reference Video Quality Metric for Evaluating Temporal
   Jerkiness due to Frame Freezing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Neural network; packet loss; temporal jerkiness; video quality metric
AB In this work, we propose a novel no-reference (NR) video quality metric that evaluates the impact of frame freezing due to either packet loss or late arrival. Our metric uses a trained neural network acting on features that are chosen to capture the impact of frame freezing on the perceived quality. The considered features include the number of freezes, freeze duration statistics, inter-freeze distance statistics, frame difference before and after the freeze, normal frame difference, and the ratio of them. We use the neural network to find the mapping between features and subjective test scores. We optimize the network structure and the feature selection through a cross-validation procedure, using training samples extracted from both VQEG and LIVE video databases. The resulting feature set and network structure yields accurate quality prediction for both the training data containing 54 test videos and a separate testing dataset including 14 videos, with Pearson correlation coefficients greater than 0.9 and 0.8 for the training set and the testing set, respectively. Our proposed metric has low complexity and could be utilized in a system with real-time processing constraint.
C1 [Xue, Yuanyi; Erkin, Beril; Wang, Yao] NYU, Polytech Sch Engn, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
C3 New York University; New York University Tandon School of Engineering
RP Xue, YY (corresponding author), NYU, Polytech Sch Engn, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
EM yxue@nyu.edu; be521@nyu.edu; yw523@nyu.edu
OI Wang, Yao/0000-0003-3199-3802
CR [Anonymous], 2012, ITU T REC P 1201
   Hastie T., 2009, ELEMENTS STAT LEARNI
   Huynh-Thu Q., 2009, P 16 IEEE INT C IM P, P2221
   Moorthy AK, 2012, IEEE J-STSP, V6, P652, DOI 10.1109/JSTSP.2012.2212417
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Rodríguez DZ, 2012, IEEE T CONSUM ELECTR, V58, P985, DOI 10.1109/TCE.2012.6311346
   Webster A., 2010, REPORT VALIDATION VI
   Wolf S., 2009, P INT WORKSH VID PRO
   Wolf S., 2011, TM11482 NAT TEL INF
   Yamagishi K., 2013, P INT WORKSH VID PRO
   Yammine G, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P341, DOI 10.1109/PCS.2012.6213315
NR 11
TC 29
Z9 30
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2015
VL 17
IS 1
BP 134
EP 139
DI 10.1109/TMM.2014.2368272
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AX6QW
UT WOS:000347047400013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lombardo, A
   Panarello, C
   Schembra, G
AF Lombardo, Alfio
   Panarello, Carla
   Schembra, Giovanni
TI A Model-Assisted Cross-Layer Design of an Energy-Efficient Mobile Video
   Cloud
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Energy saving; Markov models; mobile video clouds; QoS; system design
ID END BATCH TRANSMISSION; ERROR CONTROL; MPEG VIDEO; WIRELESS; CONSUMPTION
AB In the last decade, one of the main goals in wireless telecommunications has been to reduce energy consumption of mobile devices. However, making a network device green can cause performance deterioration. The target of this paper is to propose a cross-layer approach for the design of a mobile video cloud for the uplink transmission towards the Internet. The proposed approach is adaptive in both the video sources and the wireless transmitter. A source Rate Controller is applied to compensate transmission bandwidth reduction due to the energy saving policies. Energy saving in wireless transmission on the mobile cloud cellular channel is achieved by introducing an energy-efficient ARQ protocol. This protocol can apply different transmission laws, in order to exploit the correlation of the cellular channel behavior. An analytical model of the system is defined to compare the transmission laws, and provide some design guidelines to choose one of them and design its parameters.
C1 [Lombardo, Alfio; Schembra, Giovanni] Univ Catania, DIEEI, I-95125 Catania, Italy.
   [Panarello, Carla] Univ Catania, CNIT Res Unit, I-95125 Catania, Italy.
C3 University of Catania; University of Catania
RP Lombardo, A (corresponding author), Univ Catania, DIEEI, I-95125 Catania, Italy.
EM alfio.lombardo@dieei.unict.it; carla.panarello@dieei.unict.it;
   gio-vanni.schembra@dieei.unict.it
RI Schembra, Giovanni/AAA-3947-2021
OI Schembra, Giovanni/0000-0002-7432-8389; Lombardo,
   Alfio/0000-0003-3617-7732
FU Programma Operativo Nazionale Ricerca and Competitivit
FX This work was supported by the Programma Operativo Nazionale Ricerca and
   Competitivit 2007- 2013 within the project PON04a2_E SINERGREEN RES
   NOVAE Smart Energy Master per il governo energetico del territorio. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Klara Nahrstedt.
CR *3GPP, 2011, 36300 3GPP TS
   Agrawal P, 1996, IEEE PERS COMMUN, V3, P18, DOI 10.1109/98.490750
   [Anonymous], 2016, COOPERATION WIRELESS
   [Anonymous], 2008, P NSDI 08 P 5 USENIX
   [Anonymous], 2011, 2011 IEEE 73 VEH TEC
   [Anonymous], COMM SOFTW SERV MULT
   [Anonymous], GLOBECOM
   [Anonymous], CISC VIS NETW IND GL
   Ausavapattanakun K, 2007, IEEE T COMMUN, V55, P198, DOI 10.1109/TCOMM.2006.885092
   Austin T, 2004, COMPUTER, V37, P81, DOI 10.1109/MC.2004.1297253
   Badia L., 2005, P IEEE GLOBECOM 2005, V6, P5
   BALANI R, 2007, TRUCLANESL20071201
   Bolla R, 2011, IEEE COMMUN SURV TUT, V13, P223, DOI 10.1109/SURV.2011.071410.00073
   Bruschi R., 2013, P ICC 2013 BUD HUNG, P9
   Feeney LM, 2001, IEEE INFOCOM SER, P1548, DOI 10.1109/INFCOM.2001.916651
   Fitzek F.H. P., 2007, Cognitive Wireless Networks: Concepts, Methodologies and Visions Inspiring the Age of Enlightenment of Wireless Communication
   Frossard P, 2007, IEEE J SEL AREA COMM, V25, P641, DOI 10.1109/JSAC.2007.070501
   Galluccio L, 2005, IEEE T WIREL COMMUN, V4, P2777, DOI 10.1109/TWC.2005.858028
   Galluccio L, 2005, IEEE J SEL AREA COMM, V23, P369, DOI 10.1109/JSAC.2004.839386
   Hauske G., 2003, MOMUC 2003, P5
   HEFFES H, 1986, IEEE J SEL AREA COMM, V4, P856, DOI 10.1109/JSAC.1986.1146393
   Huang J., 2012, P ACM MOB LOW WOOD B
   Issariyakul T, 2005, IEEE ICC, P3505
   Issariyakul T, 2005, IEEE ICC, P3494
   Jones CE, 2001, WIREL NETW, V7, P343, DOI 10.1023/A:1016627727877
   Kalic G., 2012, P MIPRO 2012 OP CROA
   Kamal AE, 1996, IEEE INFOCOM SER, P248, DOI 10.1109/INFCOM.1996.497900
   Kumar KS, 2008, WIREL COMMUN MOB COM, V8, P871, DOI 10.1002/wcm.534
   La Corte A., 1997, COMPUT NETW ISDN SYS, V29
   Licandro F., 2007, P EURASIP J WIRE JAN, P34
   Licandro F, 2008, MULTIMEDIA SYST, V14, P155, DOI 10.1007/s00530-008-0121-5
   Licandro F, 2008, SIG COM TEC, P235
   Lombardo A, 1998, IEEE INFOCOM SER, P217, DOI 10.1109/INFCOM.1998.659657
   Lombardo A, 2004, IEEE T MULTIMEDIA, V6, P142, DOI 10.1109/TMM.2003.819750
   Lombardo Alfio, 2010, Performance Evaluation Review, V38, P76, DOI 10.1145/1925019.1925035
   Lombardo A, 2003, IEEE ACM T NETWORK, V11, P47, DOI 10.1109/TNET.2002.804830
   Lombardo A., 2012, P IEEE GLOBECOM 12 A, P3
   Lombardo A., 2012, P IEEE GREENCOM 2012, P25
   Lombardo A., 2010, P E EN 2010 PASS GER, P13
   Lombardo Alfio., 2013, Network Protocols and Algorithms, V5, P41
   Miao GW, 2009, WIREL COMMUN MOB COM, V9, P529, DOI 10.1002/wcm.698
   Panarello C., 2012, P E EN 2012 MADR SPA, P9
   Pedersen MV, 2012, P IEEE, V100, P1400, DOI 10.1109/JPROC.2012.2189806
   Sharma A., 2009, P ACM CONEXT ROM IT
   Soljanin E., 2004, ADV NETWORK INFORM T, P321
   Son K., 2012, P IEEE INFOCOM, P25
   TSE DNC, 1995, IEEE J SEL AREA COMM, V13, P1028, DOI 10.1109/49.400658
   VUCETIC B, 1991, IEEE T COMMUN, V39, P653, DOI 10.1109/26.87156
   Wang F, 2012, IEEE INFOCOM SER, P199, DOI 10.1109/INFCOM.2012.6195578
   Wu Huahui, 2005, ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), V1, P315
   Xiao Y., 2008, P NGMAST 2008 CARD U
   Xin Jin, 2010, Proceedings 2010 IEEE 16th International Conference on Parallel and Distributed Systems (ICPADS 2010), P800, DOI 10.1109/ICPADS.2010.78
   Xu J, 2007, IEEE T WIREL COMMUN, V6, P2305, DOI 10.1109/TWC.2007.05838
   Zhu WW, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940269
   Zorzi M, 1997, IEEE PERS COMMUN, V4, P27, DOI 10.1109/98.637380
   Zorzi M, 1997, IEEE T COMPUT, V46, P279, DOI 10.1109/12.580424
NR 56
TC 14
Z9 15
U1 1
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2014
VL 16
IS 8
BP 2307
EP 2322
DI 10.1109/TMM.2014.2350257
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AU4HL
UT WOS:000345571300019
DA 2024-07-18
ER

PT J
AU Liu, HX
   Song, B
   Tian, F
   Qin, H
AF Liu, Haixiao
   Song, Bin
   Tian, Fang
   Qin, Hao
TI Joint Sampling Rate and Bit-Depth Optimization in Compressive Video
   Sampling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compressed sensing; rate allocation; rate distortion optimization; video
   sampling
AB Compressed sensing is a novel technology that exploits sparsity of a signal to perform sampling below the Nyquist rate, and thus has great potential in low-complexity video sampling and compression applications, due to the significant reduction of the sampling rate (SR) and computational complexity. However, most current work about compressive video sampling (CVS) has focused on real-valued measurements without being quantized, and thus is not applicable to engineering practices. Moreover, in many circumstances, the total number of bits is often constrained. Therefore, how to achieve a compromise between the number of measurements and the number of bits per measurement to maximize the visual quality is a great challenge for CVS, which has still not been addressed in literature. In this paper, we first present a novel distortion model that reveals the relationship between distortion, SR, and quantization bit-depth (B). Then, using this model, we propose a joint SR - B optimization algorithm, by which we are able to easily derive the values of SR and B. Finally, we present an adaptive and unidirectional CVS framework with rate-distortion (RD) optimized rate allocation, wherein we use video characteristics extracted from partial sampling to allocate the required bits for each block, and then implement "optimized" video sampling and measurement quantization with the estimated SR and B, respectively. Simulation results show that our proposal offers comparable RD performance to the conventional method, with a 4.6 dB improvement in the average PSNR.
C1 [Liu, Haixiao; Song, Bin; Tian, Fang; Qin, Hao] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
C3 Xidian University
RP Liu, HX (corresponding author), Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
EM hxliu@stu.xidian.edu.cn; bsong@mail.xidian.edu.cn;
   ftian@stu.xidian.edu.cn; hqin@mail.xidian.edu.cn
FU National Natural Science Foundation of China [61271173, 61372068];
   Research Fund for the Doctoral Program of Higher Education of China
   [20130203110005]; Fundamental Research Funds for the Central
   Universities [K5051301033]; 111 Project [B08038]; ISN State Key
   Laboratory
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61271173 and Grant 61372068, the Research Fund for the
   Doctoral Program of Higher Education of China under Grant
   20130203110005, the Fundamental Research Funds for the Central
   Universities under Grant K5051301033, the 111 Project under Grant
   B08038, and the ISN State Key Laboratory. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Tommaso Melodia.
CR [Anonymous], 2012, JCTVCK1003 ITUTISOIE
   Boufounos PT, 2012, IEEE T INFORM THEORY, V58, P1861, DOI 10.1109/TIT.2011.2173899
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Chen H.-W., 2010, P SOC PHOTO-OPT INS, V7744
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Duarte MF, 2008, IEEE SIGNAL PROC MAG, V25, P83, DOI 10.1109/MSP.2007.914730
   Duarte MF, 2005, 2005 39th Asilomar Conference on Signals, Systems and Computers, Vols 1 and 2, P1537
   Gabay D., 1976, Computers & Mathematics with Applications, V2, P17, DOI 10.1016/0898-1221(76)90003-1
   Gunturk CSinan., 2010, Information Sciences and Systems (CISS), 2010 44th Annual Conference on, IEEE, P1
   Laska JN, 2012, IEEE T SIGNAL PROCES, V60, P3496, DOI 10.1109/TSP.2012.2194710
   Liu HX, 2013, IEEE SIGNAL PROC LET, V20, P315, DOI 10.1109/LSP.2013.2245893
   LIU Haixiao, 2011, P IEEE INT C SIGN PR, P1
   Liu Y, 2013, IEEE T CIRC SYST VID, V23, P438, DOI 10.1109/TCSVT.2012.2207269
   Liu ZR, 2011, IEEE T CIRC SYST VID, V21, P1704, DOI 10.1109/TCSVT.2011.2133890
   Liu ZR, 2010, IEEE IMAGE PROC, P1649, DOI 10.1109/ICIP.2010.5654000
   Ma SW, 2005, IEEE T CIRC SYST VID, V15, P1533, DOI 10.1109/TCSVT.2005.857300
   Ou YF, 2014, IEEE T IMAGE PROCESS, V23, P2473, DOI 10.1109/TIP.2014.2303636
   Ou YF, 2011, IEEE T CIRC SYST VID, V21, P286, DOI 10.1109/TCSVT.2010.2087833
   Pudlewski S, 2013, IEEE COMMUN SURV TUT, V15, P754, DOI 10.1109/SURV.2012.121912.00154
   Pudlewski S, 2012, IEEE T MOBILE COMPUT, V11, P1060, DOI 10.1109/TMC.2011.175
   WAKIN M. B., 2006, P PCS APR, P711
   Zheng J, 2009, OPT ENG, V48, DOI 10.1117/1.3206733
NR 24
TC 27
Z9 29
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1549
EP 1562
DI 10.1109/TMM.2014.2328324
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200005
DA 2024-07-18
ER

PT J
AU Khan, I
AF Khan, Imran
TI Non-Rigid Structure-From-Motion With Uniqueness Constraint and Low Rank
   Matrix Fitting Factorization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Non-rigid structure-from-motion; uniqueness constraint; low rank matrix
   fitting; least squares estimation
ID SHAPE
AB Non-rigid structure-from-motion is one of the difficult and challenging problems in computer vision, especially when the only input available is 2D correspondences in monocular video sequence. This paper proposed a new constraint based framework for underconstrained non-rigid structure-from-motion problem to constrain the space of solution. The proposed method is based on a point trajectory approach with an additional uniqueness constraint applied to shape coefficients to reduce the basis required to construct the non-rigid 3D shape. A framework for occluded and incomplete measured data is also proposed using low rank matrix fitting which is a robust factorization scheme for the matrix completion problem. This method offers not only new theoretical insight, but also a practical, everyday solution, to non-rigid structure-from-motion. The proposed method is positively compared to the state-of-the-art in non-rigid structure-from-motion, providing improved results on high-frequency deformations of both articulated and simpler deformable shapes.
C1 Ctr Adv Studies Engn, Dept Comp Engn, Islamabad, Pakistan.
RP Khan, I (corresponding author), Ctr Adv Studies Engn, Dept Comp Engn, Islamabad, Pakistan.
EM imi_case@yahoo.com
CR AanULs H., 2002, ESTIMATION DEFORMABL
   Aggarwal JK, 1997, IEEE NONRIGID AND ARTICULATED MOTION WORKSHOP, PROCEEDINGS, P90, DOI 10.1109/NAMW.1997.609859
   Akhter I., 2008, ADV NEURAL INFORM PR, P41
   [Anonymous], 2008, P IEEE C COMP VIS PA
   [Anonymous], 1996, J HOPKINS STUDIES MA
   Barth AdamT., 2008, BodyNets'08: Proceedings of the ICST 3rd international conference on Body area networks, P1
   BLACK MJ, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P374, DOI 10.1109/ICCV.1995.466915
   Brand M, 2005, PROC CVPR IEEE, P122
   Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941
   Buchanan AM, 2005, PROC CVPR IEEE, P316
   Chen SY, 2012, MATH PROBL ENG, V2012, DOI 10.1155/2012/856523
   Del Bue A., 2006, PROC IEEE C COMPUTER, V1, P1191
   Del Bue A, 2012, IEEE T PATTERN ANAL, V34, P1496, DOI 10.1109/TPAMI.2011.238
   Gavrila DM, 1999, COMPUT VIS IMAGE UND, V73, P82, DOI 10.1006/cviu.1998.0716
   Gotardo PFU, 2011, IEEE I CONF COMP VIS, P802, DOI 10.1109/ICCV.2011.6126319
   Moeslund TB, 2001, COMPUT VIS IMAGE UND, V81, P231, DOI 10.1006/cviu.2000.0897
   Nastar C, 1996, IEEE T PATTERN ANAL, V18, P1067, DOI 10.1109/34.544076
   Olsen S., 2007, P BRIT MACH VIS C, V2, P3
   Paladini Marco, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2898, DOI 10.1109/CVPRW.2009.5206602
   Rengier F, 2010, INT J COMPUT ASS RAD, V5, P335, DOI 10.1007/s11548-010-0476-x
   Studholme C, 1999, PATTERN RECOGN, V32, P71, DOI 10.1016/S0031-3203(98)00091-0
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   Torresani L, 2001, PROC CVPR IEEE, P493
   Torresani L., 2003, ADV NEURAL INF PROCE, V16
   Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752
   Wen ZW, 2012, MATH PROGRAM COMPUT, V4, P333, DOI 10.1007/s12532-012-0044-1
   Xiao J, 2004, LECT NOTES COMPUT SC, V2034, P573
   Zaheer A, 2011, IEEE I CONF COMP VIS, P2447, DOI 10.1109/ICCV.2011.6126529
   Zhu JK, 2006, LECT NOTES COMPUT SC, V3951, P186
NR 29
TC 9
Z9 11
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1350
EP 1357
DI 10.1109/TMM.2014.2308415
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600016
DA 2024-07-18
ER

PT J
AU Stütz, T
   Autrusseau, F
   Uhl, A
AF Stuetz, Thomas
   Autrusseau, Florent
   Uhl, Andreas
TI Non-Blind Structure-Preserving Substitution Watermarking of H.264/CAVLC
   Inter-Frames
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video watermarking; H.264; compressed video; information hiding; data
   embedding; video quality; subjective quality
ID QUALITY ASSESSMENT; IMAGE QUALITY; VIDEO
AB In this work we propose a novel non-blind H.264/CAVLC structure-preserving substitution watermarking algorithm. The proposed watermarking algorithm enables extremely efficient watermark embedding by simple bit substitutions (substitution watermarking). The bit-substitutions change the motion vector differences of non-reference frames. Furthermore our watermarking algorithm can be applied in applications scenarios which require that watermarking preserves the length of the bitstream units (structure-preserving watermarking). The watermark detection works in the image domain and thus is robust to video format changes. The quality and robustness of the approach are in depth evaluated and analyzed, the quality evaluation is backed up by subjective evaluations. Comparison to the state-of-the-art indicates a superior performance of our watermarking algorithm.
C1 [Stuetz, Thomas] Univ Appl Sci Salzburg, Dept Multimedia Technol, A-5412 Puch Salzburg, Austria.
   [Autrusseau, Florent] Univ Nantes, LUNAM Univ, CNRS, IRCCyN,UMR 6597,Polytech Nantes, F-44306 Nantes, France.
   [Uhl, Andreas] Salzburg Univ, Dept Comp Sci, A-5020 Salzburg, Austria.
C3 Nantes Universite; Ecole Centrale de Nantes; Centre National de la
   Recherche Scientifique (CNRS); Salzburg University
RP Stütz, T (corresponding author), Univ Appl Sci Salzburg, Dept Multimedia Technol, A-5412 Puch Salzburg, Austria.
EM thomas.stuetz@fh-salzburg.ac.at; uhl@cosy.sbg.ac.at
RI Autrusseau, Florent/S-1200-2018
OI Autrusseau, Florent/0000-0002-2690-0029
FU FFG bridge project [834165]
FX This work was supported in part by the FFG bridge project 834165. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Ton Kalker.
CR [Anonymous], 2008, P910 ITU
   [Anonymous], 2003, H.264 and MPEG-4 video compression: video coding for next generation multimedia
   [Anonymous], 2007, H264 ITU T
   [Anonymous], 2004, BT50011 ITUR
   BAKER SH, 1989, IEEE T CONSUM ELECTR, V35, P319, DOI 10.1109/30.44287
   BARTEN PGJ, 1990, J OPT SOC AM A, V7, P2024, DOI 10.1364/JOSAA.7.002024
   Carnec M, 2008, SIGNAL PROCESS-IMAGE, V23, P239, DOI 10.1016/j.image.2008.02.003
   Carosi M., 2010, P SPIE C EL IM, P7542
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Cheng H, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P735
   Chug TY, 1998, IEEE T CONSUM ELECTR, V44, P895, DOI 10.1109/30.713211
   Chupeau B., 2006, P 2006 INT C AC SPEE, VII, P4
   Cox IJ., 2007, DIGITAL WATERMARKING
   Eggers J. J., 2002, INFORMED WATERMARKIN
   Jesty L.C., 1958, P IEE, V105B, P425, DOI DOI 10.1049/PI-B-1.1958.0320
   Langelaar GC, 2001, IEEE T IMAGE PROCESS, V10, P148, DOI 10.1109/83.892451
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MM VQEG, 2008, FIN REP VID QUAL EXP
   Mobasseri B.G., 2005, Proceedings of the 7th workshop on Multimedia and security, P91, DOI DOI 10.1145/1073170.1073187
   Noorkami M, 2005, IEEE IMAGE PROC, P1229
   Noorkami M, 2007, IEEE T INF FOREN SEC, V2, P14, DOI 10.1109/TIFS.2006.890306
   Pankajakshan V, 2010, IEEE IMAGE PROC, P2589, DOI 10.1109/ICIP.2010.5651583
   Pereira S., 2001, INFORM HIDING, P340, DOI [10.1007/3-540-45496-925, DOI 10.1007/3-540-45496-925]
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zou D., 2010, P IEEE INT C MULT EX
   Zou D., 2009, P SPIE MED FOR SEC S, V7254
   Zou DK, 2008, INT CONF ACOUST SPEE, P1749, DOI 10.1109/ICASSP.2008.4517968
NR 28
TC 46
Z9 49
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1337
EP 1349
DI 10.1109/TMM.2014.2310595
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600015
OA Green Published
DA 2024-07-18
ER

PT J
AU Song, W
   Tjondronegoro, DW
AF Song, Wei
   Tjondronegoro, Dian W.
TI Acceptability-Based QoE Models for Mobile Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Acceptability; mobile video; modeling; pleasantness; quality of
   experience (QoE)
ID QUALITY ASSESSMENT; EXPERIENCE
AB Quality of experience (QoE) measures the overall perceived quality of mobile video delivery from subjective user experience and objective system performance. Current QoE prediction models have two main limitations: (1) insufficient consideration of the factors influencing QoE, and (2) limited studies on QoE models for acceptability prediction. In this paper, a set of novel acceptability-based QoE models, denoted as A-QoE, is proposed based on the results of comprehensive user studies on subjective quality acceptance assessments. Themodels are able to predict users' acceptability and pleasantness in various mobile video usage scenarios. Statistical nonlinear regression analysis has been used to build the models with a group of influencing factors as independent predictors, which include encoding parameters and bitrate, video content characteristics, and mobile device display resolution. The performance of the proposed A-QoE models has been compared with three well-known objective Video Quality Assessment metrics: PSNR, SSIM and VQM. The proposed A-QoE models have high prediction accuracy and usage flexibility. Future user-centred mobile video delivery systems can benefit from applying the proposed QoE-based management to optimize video coding and quality delivery strategies.
C1 [Song, Wei; Tjondronegoro, Dian W.] Queensland Univ Technol, Sch Informat Syst, Sci & Engn Fac, Brisbane, Qld 4001, Australia.
C3 Queensland University of Technology (QUT)
RP Song, W (corresponding author), Queensland Univ Technol, Sch Informat Syst, Sci & Engn Fac, Brisbane, Qld 4001, Australia.
EM w1.song@qut.edu.au; dian@qut.edu.au
RI Tjondronegoro, Dian/AAE-4685-2022
OI Tjondronegoro, Dian/0000-0001-7446-2839
CR Agboma F., 2008, PROC 6 INT C ADV MOB, P111
   Agboma F, 2012, TELECOMMUN SYST, V49, P85, DOI 10.1007/s11235-010-9355-6
   [Anonymous], E MOD COMP MOD US TR
   [Anonymous], 2007, OP MOD VID TEL APPL
   [Anonymous], 2004, P 2004 C HUM FACT CO
   [Anonymous], 2004, OBJ PERC VID QUAL ME
   [Anonymous], P 40 HICSS IEEE COMP
   [Anonymous], 1999, SUBJ VID QUA ASS MET
   Chikkerur S, 2011, IEEE T BROADCAST, V57, P165, DOI 10.1109/TBC.2011.2104671
   Christensen R., 1997, Log-linear models and logistic regression, V2nd
   Coverdale P, 2011, IEEE SIGNAL PROC MAG, V28, P91, DOI 10.1109/MSP.2011.942467
   Cranley N, 2007, J NETW COMPUT APPL, V30, P983, DOI 10.1016/j.jnca.2005.12.006
   de Koning TCM, 2007, PROC SPIE, V6507, DOI 10.1117/12.704159
   De Pessemier T, 2011, INT WORK QUAL MULTIM, P125, DOI 10.1109/QoMEX.2011.6065689
   Eichhorn A., 2009, IEEE INT C COMM, P1, DOI [10.1109/ICC.2009.5305948, DOI 10.1109/ICC.2009.5305948]
   Gottschalk PG, 2005, ANAL BIOCHEM, V343, P54, DOI 10.1016/j.ab.2005.04.035
   ITU-T SG 12, 2007, 12 ITUT SG
   Joskowicz J., 2010, TELECOMMUN SYST, V46, P14
   Jumisko-Pyykko S., 2008, INT J DIGITAL MULTIM, V2008, P20
   Ketyko I. n., 2010 IEEE INT S BROA
   Khan A, 2010, IET COMMUN, V4, P1337, DOI 10.1049/iet-com.2009.0422
   Menkovski V., 2009, PROCEED INGS 7 INT C, P52
   O'Hara K, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P857
   Ohm Jens-Rainer., 1999, Bildsignalverarbeitung fuer multimedia-systeme. skript
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Raake A, 2011, IEEE SIGNAL PROC MAG, V28, P68, DOI 10.1109/MSP.2011.942472
   Ries Michal, 2008, Journal of Communications, V3, P41, DOI 10.4304/jcm.3.1.41-50
   Schatz R., 2011 P IEEE INT C CO, P1
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P335, DOI 10.1109/TIP.2009.2034992
   Song W, 2010, INT J SOFTW ENG KNOW, V20, P1045, DOI 10.1142/S0218194010005067
   Song Wei, 2010, Proceedings of the 2010 Asia-Pacific Power and Energy Engineering Conference (APPEEC 2010), DOI 10.1109/APPEEC.2010.5449415
   Song Wei., 2011, Proceedings_of_the_19th_ACM international_conference_on_Multimedia, P403
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zhang F, 2013, IEEE T IMAGE PROCESS, V22, P1534, DOI 10.1109/TIP.2012.2233486
   Zinner Thomas, 2010, Proceedings of the 2010 Second International Workshop on Quality of Multimedia Experience (QoMEX 2010), P29, DOI 10.1109/QOMEX.2010.5518277
NR 37
TC 62
Z9 68
U1 1
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 738
EP 750
DI 10.1109/TMM.2014.2298217
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500014
DA 2024-07-18
ER

PT J
AU Fiandrotti, A
   Bioglio, V
   Grangetto, M
   Gaeta, R
   Magli, E
AF Fiandrotti, Attilio
   Bioglio, Valerio
   Grangetto, Marco
   Gaeta, Rossano
   Magli, Enrico
TI Band Codes for Energy-Efficient Network Coding With Application to P2P
   Mobile Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Network coding; Rateless codes; P2P; mobile streaming; energy-efficiency
AB A key problem in network coding (NC) lies in the complexity and energy consumption associated with the packet decoding processes, which hinder its application in mobile environments. Controlling and hence limiting such factors has always been an important but elusive research goal, since the packet degree distribution, which is the main factor driving the complexity, is altered in a non-deterministic way by the random recombinations at the network nodes. In this paper we tackle this problem with a new approach and propose Band Codes (BC), a novel class of network codes specifically designed to preserve the packet degree distribution during packet encoding, recombination and decoding. BC are random codes over GF(2) that exhibit low decoding complexity, feature limited and controlled degree distribution by construction, and hence allow to effectively apply NC even in energy-constrained scenarios. In particular, in this paper we motivate and describe our new design and provide a thorough analysis of its performance. We provide numerical simulations of the BC performance in order to validate the analysis and assess the overhead of BC with respect to a conventional random NC scheme. Moreover, experiment in a real-world application, namely peer-to-peer mobile media streaming using a random-push protocol, show that BC reduce the decoding complexity by a factor of two with negligible increase of the encoding overhead, paving the way for the application of NC to power-constrained devices.
C1 [Fiandrotti, Attilio; Bioglio, Valerio; Magli, Enrico] Politecn Torino, Dept Elect & Telecommun, I-10129 Turin, Italy.
   [Grangetto, Marco; Gaeta, Rossano] Univ Turin, Dept Comp Sci, I-10149 Turin, Italy.
C3 Polytechnic University of Turin; University of Turin
RP Fiandrotti, A (corresponding author), Politecn Torino, Dept Elect & Telecommun, I-10129 Turin, Italy.
EM attilio.fiandrotti@polito.it; valerio.bioglio@polito.it;
   marco.grangetto@di.unito.it; rossano.gaeta@di.unito.it;
   enrico.magli@polito.it
RI Grangetto, Marco/AFM-8024-2022; GAETA, Rossano/C-6256-2011
OI Grangetto, Marco/0000-0002-2709-7864; Bioglio,
   Valerio/0000-0001-8418-5986; GAETA, Rossano/0000-0002-6521-403X;
   Fiandrotti, Attilio/0000-0002-9991-6822
FU European Union [COAST-ICT-248036]; Universita di Torino; Compagnia di
   San Paolo; Italian Ministry of Education and Research
FX This publication is based partly on work performed within Project
   COAST-ICT-248036 which is funded by the European Union, partly on work
   performed within project AMALFI which is funded by Universita di Torino
   and Compagnia di San Paolo, partly on work performed within project
   ARACNE, a PRIN funded by the Italian Ministry of Education and Research.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Pal Halvorsen.
CR Ahlswede R, 2000, IEEE T INFORM THEORY, V46, P1204, DOI 10.1109/18.850663
   Angelopoulos G, 2011, LECT NOTES COMPUT SC, V6827, P137, DOI 10.1007/978-3-642-23041-7_14
   [Anonymous], 2009, EU FP7 P2P NEXT PROJ
   Chou P.A., 2003, Proc. Annual Allerton Conference on Communication control and Computing, V41, P40
   Chun B, 2003, ACM SIGCOMM COMP COM, V33, P3, DOI 10.1145/956993.956995
   Fiandrotti A., 2012, P IEEE INT C MULT EX, P194
   Fiandrotti A., 2011, P IEEE INT WORKSH MU, P1
   Fragouli C, 2006, ACM SIGCOMM COMP COM, V36, P63, DOI 10.1145/1111322.1111337
   HAKEN A, 2002, Patent No. 6486803
   Heide J, 2008, J COMMUN NETW-S KOR, V10, P403, DOI 10.1109/JCN.2008.6389856
   Katti S, 2008, IEEE ACM T NETWORK, V16, P497, DOI 10.1109/TNET.2008.923722
   Luby M, 2002, ANN IEEE SYMP FOUND, P271, DOI 10.1109/SFCS.2002.1181950
   Lucani DE, 2010, IEEE INT SYMP INFO, P2403, DOI 10.1109/ISIT.2010.5513768
   Lucani DE, 2009, GLOB TELECOMM CONF, P4601
   MacKay DJC, 2005, IEE P-COMMUN, V152, P1062, DOI 10.1049/ip-com:20050237
   Puducheri S, 2007, IEEE T INFORM THEORY, V53, P3740, DOI 10.1109/TIT.2007.904982
   Shojania H, 2009, NOSSDAV 09: 18TH INTERNATIONAL WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO, P37
   Shokrollahi A, 2006, IEEE T INFORM THEORY, V52, P2551, DOI 10.1109/TIT.2006.874390
   Soro A, 2009, GLOB TELECOMM CONF, P746
   Studholme C, 2006, 2006 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, VOLS 1-6, PROCEEDINGS, P509, DOI 10.1109/ISIT.2006.261768
   Thomos N, 2011, IEEE INT SYMP INFO, P2736, DOI 10.1109/ISIT.2011.6034070
   Toldo M., 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P400, DOI 10.1109/MMSP.2010.5662054
   Vingelmann P., 2010, Proc. ACM Multimedia Workshop Mobile Cloud Media Comput, P3
   Wang M, 2007, IEEE J SEL AREA COMM, V25, P1655, DOI 10.1109/JSAC.2007.071205
   Wang M, 2007, IEEE T MULTIMEDIA, V9, P1554, DOI 10.1109/TMM.2007.907460
NR 25
TC 30
Z9 34
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 521
EP 532
DI 10.1109/TMM.2013.2285518
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800019
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hossfeld, T
   Keimel, C
   Hirth, M
   Gardlo, B
   Habigt, J
   Diepold, K
   Phuoc, TG
AF Hossfeld, Tobias
   Keimel, Christian
   Hirth, Matthias
   Gardlo, Bruno
   Habigt, Julian
   Diepold, Klaus
   Phuoc Tran-Gia
TI Best Practices for QoE Crowdtesting: QoE Assessment With Crowdsourcing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Best practices; cognition; human computer interaction; instrumentation
   and measurementmultimedia systems; testing
ID QUALITY
AB Quality of Experience (QoE) in multimedia applications is closely linked to the end users' perception and therefore its assessment requires subjective user studies in order to evaluate the degree of delight or annoyance as experienced by the users. QoE crowdtesting refers to QoE assessment using crowdsourcing, where anonymous test subjects conduct subjective tests remotely in their preferred environment. The advantages of QoE crowdtesting lie not only in the reduced time and costs for the tests, but also in a large and diverse panel of international, geographically distributed users in realistic user settings. However, conceptual and technical challenges emerge due to the remote test settings. Key issues arising from QoE crowdtesting include the reliability of user ratings, the influence of incentives, payment schemes and the unknown environmental context of the tests on the results. In order to counter these issues, strategies and methods need to be developed, included in the test design, and also implemented in the actual test campaign, while statistical methods are required to identify reliable user ratings and to ensure high data quality. This contribution therefore provides a collection of best practices addressing these issues based on our experience gained in a large set of conducted QoE crowdtesting studies. The focus of this article is in particular on the issue of reliability and we use video quality assessment as an example for the proposed best practices, showing that our recommended two-stage QoE crowdtesting design leads to more reliable results.
C1 [Hossfeld, Tobias; Hirth, Matthias; Phuoc Tran-Gia] Univ Wurzburg, Inst Comp Sci, D-97074 Wurzburg, Germany.
   [Keimel, Christian; Habigt, Julian; Diepold, Klaus] Tech Univ Munich, Inst Data Proc, D-80333 Munich, Germany.
   [Gardlo, Bruno] Telecommun Res Ctr Vienna FTW, A-1220 Vienna, Austria.
C3 University of Wurzburg; Technical University of Munich
RP Hossfeld, T (corresponding author), Univ Wurzburg, Inst Comp Sci, D-97074 Wurzburg, Germany.
EM tobias.hossfeld@uni-wuerzburg.de; christian.keimel@tum.de;
   matthias.hirth@uni-wuerzburg.de; gardlo@ftw.at; jh@tum.de; kldi@tum.de;
   phuoc.trangia@uni-wuerzburg.de
RI Keimel, Christian/K-1008-2015; Hirth, Matthias/S-6290-2019
OI Hirth, Matthias/0000-0002-1359-363X; Hossfeld,
   Tobias/0000-0003-0173-595X; Keimel, Christian/0000-0002-4234-7745
FU COST Action [IC1003 Qualinet]; Deutsche Forschungsgemeinschaft (DFG) [HO
   4770/1-1, TR257/31-1]
FX This work was supported in part by the COST Action IC1003 Qualinet and
   in part by the Deutsche Forschungsgemeinschaft (DFG) under Grants HO
   4770/1-1 and TR257/31-1. The authors alone are responsible for the
   content. The associate editor coordinating the review of this manuscript
   and approving it for publication was Dr. Sheng-Wei (Kuan-Ta) Chen.
CR Alonso Omar, 2008, SIGIR Forum, V42, P9, DOI 10.1145/1480506.1480508
   [Anonymous], 2012, Handbook of inter-rater reliability: the definitive guide to measuring the extent of Agreement among Raters
   [Anonymous], 2011, CHI 11 HUM FACT COMP
   [Anonymous], METH SUBJ ASS QUAL T
   [Anonymous], 2008, SUBJ VID QUAL ASS ME
   [Anonymous], 2009, P ACM SIGKDD WORKSH
   [Anonymous], 2010, Simple augmented reality
   Assembly I. R., 1997, METHODS SUBJECTIVE A
   Assembly I.R., 2009, METHODOLOGY SUBJECTI
   Chen KT, 2010, IEEE NETWORK, V24, P28, DOI 10.1109/MNET.2010.5430141
   Chen KT, 2009, MATH COMPUT SCI ENG, P491, DOI 10.1145/1631272.1631339
   Dai P, 2010, AAAI CONF ARTIF INTE, P1168
   De Simone F, 2010, INT CONF ACOUST SPEE, P2430, DOI 10.1109/ICASSP.2010.5496296
   Downs JS, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2399
   Eickhoff C, 2012, P ACM SIGIR C RES DE
   Eickhoff C., 2012, INF RETRIEV, V15, P1
   Eickhoff C., 2011, CSDM 11 P WORKSHOP C, P11
   Estellés-Arolas E, 2012, J INF SCI, V38, P189, DOI 10.1177/0165551512437638
   Gardlo B., 2012, P QOMEX 2012 YARR VA
   Gardlo B, 2012, PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE - RADIOELEKTRONIKA 2012, P55
   Harris C., 2011, P WORKSH CROWDS SEAR, P15
   Hayes AF, 2007, COMMUN METHODS MEAS, V1, P77, DOI 10.1080/19312450709336664
   Hirth M., 2012, MATH COMPUT MODEL
   Hirth M., 2011, P WORKSH FUT INT NEX
   Hossfeld Tobias, 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P494, DOI 10.1109/ISM.2011.87
   Hossfeld T., 2012, P QOMEX 2012 YARR VA
   Hossfeld T., 2013, SPRINGERS COMPUTER C
   Hossfeld T., 2011, P 23 INT TEL C ITC 2
   Hossfeld T., 2011, P QOMEX 2011 MECH BE
   Hossfeld T., 2011, P INT TEL C ITC SAN
   Hossfeld T, 2012, IEEE COMMUN MAG, V50, P28, DOI 10.1109/MCOM.2012.6178831
   Hsueh P.-Y., 2009, HLT 09, P27
   Ickin S, 2012, IEEE COMMUN MAG, V50, P48, DOI 10.1109/MCOM.2012.6178833
   Ipeirotis P. G., 2010, P ACM SIGKDD WORKSHO, P64, DOI 10.1145/1837885.1837906
   Keimel C., 2012, 2012 Proceedings of the 19th International Packet Video Workshop (PV 2012), P155, DOI 10.1109/PV.2012.6229729
   Keimel C., 2012, P 4 INT WORKSH QUAL
   Keimel C, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P245, DOI 10.1109/PCS.2012.6213338
   Kim S.-H., 2012, P BELIV 2012 WORKSH
   Kittur A, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P453
   Kittur A, 2008, CSCW: 2008 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK, CONFERENCE PROCEEDINGS, P37
   Le Callet P., 2012, QUALINET WHITE PAPER
   Lee JS, 2012, IEEE COMMUN MAG, V50, P38, DOI 10.1109/MCOM.2012.6178832
   Oleson David, 2011, AAAI WORKSH
   Owl S., 2012, JAVA MARKET SHARE
   Ribeiro F, 2011, INT CONF ACOUST SPEE, P2416
   Sabou Marta, 2012, P 12 INT C KNOWL MAN, P17, DOI [DOI 10.1145/2362456.2362479, 10.1145/2362456.2362479]
   Schatz R., 2012, J AUDIO ENG SCO JAES
   Schatz R., 2013, SPRINGERS COMPUTER C
   Shaw AD, 2011, P ACM 2011 C COMP SU, DOI DOI 10.1145/1958824.1958865
   Simone F. D., 2009, P QOMEX 2009 SAN DIE
   Soler M. D., 2010, Albeitar, P4
   Suri S, 2011, HUM COMP 2011 AAAI W
   von Ahn L, 2006, COMPUTER, V39, P92, DOI 10.1109/MC.2006.196
   von Ahn Luis., 2004, CHI, DOI DOI 10.1145/985692.985733
   Wu C., IEEE T MULT IN PRESS
   Xu Q, 2012, IEEE T MULTIMEDIA, V14, P844, DOI 10.1109/TMM.2012.2190924
NR 56
TC 165
Z9 179
U1 0
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 541
EP 558
DI 10.1109/TMM.2013.2291663
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800021
DA 2024-07-18
ER

PT J
AU Kserawi, M
   Jung, S
   Lee, D
   Sung, J
   Rhee, JKK
AF Kserawi, Malaz
   Jung, Sangsu
   Lee, Dujeong
   Sung, Jihoon
   Rhee, June-Koo Kevin
TI Multipath Video Real-Time Streaming by Field-Based Anycast Routing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimedia streaming; wireless mesh network; wireless/mobile multimedia
AB Wireless mesh networking (WMN) for video surveillance provides a strong potential for rapid deployment in a large community, for which reliability and survivability for real-time streaming are the key performance measure. However, current routing protocols do not provide a robust solution to meet video transmission requirements such as providing load balancing for the high data rate and delay requirements. We propose an application of field-based anycast routing (FAR) protocol, which utilizes rapid routing dynamics inspired by an electrostatic potential field model governed by Poisson's equation. This routing metric takes into account geometric proximity and congestion degree in order to increase delivery ratio and decrease end-to-end delay, which determine the quality of the delivered video. In addition, FAR protects node failure with an on-the-fly rerouting process that guarantees the continuity of video streaming. Simulation results show 100% delivery ratio in congestion situations and it shows tolerance to different delay requirements compared with AODV and FDMR protocols in terms of delivering real-time and non real-time video surveillance which verifies FAR as a strong candidate for video transmission over WMN.
C1 [Kserawi, Malaz; Lee, Dujeong; Sung, Jihoon; Rhee, June-Koo Kevin] Korea Adv Inst Sci & Technol, Dept Elect Engn, Taejon 305701, South Korea.
   [Jung, Sangsu] MtoV Inc, Taejon, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Kserawi, M (corresponding author), Korea Adv Inst Sci & Technol, Dept Elect Engn, Taejon 305701, South Korea.
EM malaz@kaist.ac.kr; sjung@mtov.net; dj.lee@kaist.ac.kr;
   sung.jh@kaist.ac.kr; rhee.jk@kaist.ac.kr
RI Rhee, June-Koo/C-1781-2011
FU MSIP (Ministry of Science, ICT & Future Planning), Korea in the ICT RD
   Program
FX This work was supported by the MSIP (Ministry of Science, ICT & Future
   Planning), Korea in the ICT R&D Program 2013. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Wenwu Zhu.
CR [Anonymous], 80211E2005 IEEE
   Evans L., 1998, PARTIAL DIFFERENTIAL
   Hu PZ, 2009, LECT NOTES COMPUT SC, V5585, P193
   Huang CM, 2003, AINA 2003: 17TH INTERNATIONAL CONFERENCE ON ADVANCED INFORMATION NETWORKING AND APPLICATIONS, P373
   Jung S, 2009, IEEE COMMUN LETT, V13, P429, DOI 10.1109/LCOMM.2009.090110
   Kalantari M, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P4028, DOI 10.1109/ICC.2004.1313307
   Kei CH, 2008, J INF SCI ENG, V24, P425
   Khilar PM, 2008, ICIT 2008: PROCEEDINGS OF THE 11TH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY, P153, DOI 10.1109/ICIT.2008.17
   Klaue J, 2003, LECT NOTES COMPUT SC, V2794, P255, DOI 10.1007/978-3-540-45232-4_16
   Lenders V., 2008, P IEEE WIR COMM NETW
   Licandro F, 2007, EURASIP J WIREL COMM, DOI 10.1155/2007/31976
   MCCAHILL M, 2002, 6 EUR COMM URBANEYE, P20
   MEER JVV, 2003, 3640 RFC
   Metz C, 2002, IEEE INTERNET COMPUT, V6, P94, DOI 10.1109/4236.991450
   Mitchell AR., 1980, The Finite Difference Method in Partial Differential Equations, V1
   Perkins C., 2003, Internet RFCs
   TASSIULAS L, 1992, IEEE T AUTOMAT CONTR, V37, P1936, DOI 10.1109/9.182479
   Xie G, 2008, IEEE T MULTIMEDIA, V10, P1687, DOI 10.1109/TMM.2008.2007291
   Zhang YS, 2010, IEEE T MULTIMEDIA, V12, P886, DOI 10.1109/TMM.2010.2065217
NR 19
TC 14
Z9 14
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 533
EP 540
DI 10.1109/TMM.2013.2293315
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800020
DA 2024-07-18
ER

PT J
AU Lee, JS
AF Lee, Jong-Seok
TI On Designing Paired Comparison Experiments for Subjective Multimedia
   Quality Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quality of experience (QoE); subjective quality assessment; paired
   comparison; Bradley-Terry model; mean opinion score (MOS)
ID PERCEPTION
AB This paper investigates the issue of designing paired comparison-based subjective quality assessment experiments for reliable results. In particular, the convergence behavior of the quality scores estimated from paired comparison results is considered. Via an extensive computer simulation experiment, the estimation performance in terms of the root mean squared error, the rank order correlation coefficient, and the change of the estimated scores with respect to the number of subjects are mathematically modeled. Furthermore, it is confirmed that the models coincide with the theoretical convergence behavior. Issues such as the effect of human errors and the underlying distribution of the true quality scores are also examined.
C1 Yonsei Univ, Sch Integrated Technol, Inchon 406840, South Korea.
C3 Yonsei University
RP Lee, JS (corresponding author), Yonsei Univ, Sch Integrated Technol, Inchon 406840, South Korea.
EM jong-seok.lee@yonsei.ac.kr
RI Lee, Jong-Seok/AAF-5197-2020
OI Lee, Jong-Seok/0000-0001-5255-4425
FU Ministry of Science, ICT and Future Planning (MSIP), Korea, under the IT
   Consilience Creative Program [NIPA-2013-H0203-13-1002]; Basic Science
   Research Program [2013R1A1A1007822]; MSIP through the National Research
   Foundation of Korea
FX This work was supported in part by the Ministry of Science, ICT and
   Future Planning (MSIP), Korea, under the IT Consilience Creative Program
   (NIPA-2013-H0203-13-1002) supervised by the National IT Industry
   Promotion Agency, and in part by the Basic Science Research Program
   (2013R1A1A1007822) funded by the MSIP through the National Research
   Foundation of Korea. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Sheng-Wei
   (Kuan-Ta) Chen.
CR [Anonymous], 2009, RMRSGTR216WWW USDA F
   [Anonymous], 2002, METHODOLOGY SUBJECTI
   [Anonymous], 1969, The method of paired comparisons
   [Anonymous], J ELECT IMAG
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.1093/biomet/39.3-4.324
   Cattelan M, 2012, STAT SCI, V27, P412, DOI 10.1214/12-STS396
   Chen KT, 2010, IEEE NETWORK, V24, P28, DOI 10.1109/MNET.2010.5430141
   Chen KT, 2009, MATH COMPUT SCI ENG, P491, DOI 10.1145/1631272.1631339
   Elliott MA, 2010, RELIAB ENG SYST SAFE, V95, P750, DOI 10.1016/j.ress.2010.02.013
   Kim SJ, 2012, IEEE GLOBE WORK, P1357, DOI 10.1109/GLOCOMW.2012.6477780
   Lee JS, 2013, MULTIMED TOOLS APPL, V67, P31, DOI 10.1007/s11042-012-1011-6
   Lee JS, 2011, IEEE T MULTIMEDIA, V13, P882, DOI 10.1109/TMM.2011.2157333
   Li J, 2012, IEEE IMAGE PROC, P629, DOI 10.1109/ICIP.2012.6466938
   Marden J.I., 1995, ANAL MODELING RANK D
   Moorthy AK, 2010, IEEE T CIRC SYST VID, V20, P587, DOI 10.1109/TCSVT.2010.2041829
   MORAN PAP, 1947, BIOMETRIKA, V34, P363, DOI 10.1093/biomet/34.3-4.363
   Pinson MH, 2012, IEEE J-STSP, V6, P640, DOI 10.1109/JSTSP.2012.2215306
   Ponomarenko N., 2009, ADV MODERN RADIOELEC, V10, P30, DOI 10.1109/TIP.2015.2439035
   Rajae-Joordens R, 2005, DISPLAYS, V26, P1, DOI 10.1016/j.displa.2004.09.003
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Simons G, 1999, ANN STAT, V27, P1041
   Winkler S, 2008, IEEE T BROADCAST, V54, P660, DOI 10.1109/TBC.2008.2000733
   Winkler S, 2009, INT WORK QUAL MULTIM, P139, DOI 10.1109/QOMEX.2009.5246961
   Xu Q, 2012, IEEE T MULTIMEDIA, V14, P844, DOI 10.1109/TMM.2012.2190924
NR 25
TC 28
Z9 29
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 564
EP 571
DI 10.1109/TMM.2013.2292590
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800023
DA 2024-07-18
ER

PT J
AU Xiong, J
   Li, HL
   Wu, QB
   Meng, FM
AF Xiong, Jian
   Li, Hongliang
   Wu, Qingbo
   Meng, Fanman
TI A Fast HEVC Inter CU Selection Method Based on Pyramid Motion Divergence
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE HEVC; inter prediction; motion divergence; H.264; video coding; CU
   decision
AB The newly developed HEVC video coding standard can achieve higher compression performance than the previous video coding standards, such as MPEG-4, H.263 and H.264/AVC. However, HEVC's high computational complexity raises concerns about the computational burden on real-time application. In this paper, a fast pyramid motion divergence (PMD) based CU selection algorithm is presented for HEVC inter prediction. The PMD features are calculated with estimated optical flow of the downsampled frames. Theoretical analysis shows that PMD can be used to help selecting CU size. A k nearest neighboring like method is used to determine the CU splittings. Experimental results show that the fast inter prediction method speeds up the inter coding significantly with negligible loss of the peak signal-to-noise ratio.
C1 [Xiong, Jian; Li, Hongliang; Wu, Qingbo; Meng, Fanman] Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Xiong, J (corresponding author), Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 611731, Peoples R China.
EM hlli@uestc.edu.cn
RI Wu, Qingbo/AAF-6872-2019; Wu, Qingbo/M-5065-2015
OI Wu, Qingbo/0000-0003-2936-6340; Xiong, Jian/0000-0002-8346-178X; Li,
   Hongliang/0000-0002-7481-095X; Xiong, Jian/0000-0002-4720-4102
FU NSFC [61271289]; Ph.D. Programs Foundation of Ministry of Education of
   China [20110185110002]; National High Technology Research and
   Development Program of China (863 Program) [2012AA011503]
FX This work was supported in part by NSFC (No. 61271289), The Ph.D.
   Programs Foundation of Ministry of Education of China (No.
   20110185110002), and National High Technology Research and Development
   Program of China (863 Program, No. 2012AA011503). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Zhihai (Henry) He.
CR Bjontegaard G., 2001, VCEG M33 AUST TX US
   Bossen F., 2012, P 11 M JCTVC K1100 O
   Bross B., 2012, P 9 M JCTVC I1003 AP
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Cassa MB, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P493, DOI 10.1109/PCS.2012.6213262
   Chang LC, 2011, IEEE T CIRC SYST VID, V21, P660, DOI 10.1109/TCSVT.2011.2129770
   Dane G, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P309
   Dong J, 2009, IEEE T CIRC SYST VID, V19, P1462, DOI 10.1109/TCSVT.2009.2026792
   Han JN, 2010, INT CONF ACOUST SPEE, P726, DOI 10.1109/ICASSP.2010.5495043
   Han WJ, 2010, IEEE T CIRC SYST VID, V20, P1709, DOI 10.1109/TCSVT.2010.2092612
   Jie Leng, 2011, 2011 International Conference on Multimedia and Signal Processing (CMSP), P56, DOI 10.1109/CMSP.2011.167
   Kim I.-K., 2012, P 9 M JCTVC I1002 GE
   Kim J, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P449, DOI 10.1109/PCS.2012.6213251
   Lee H, 2012, OPT ENG, V51, DOI 10.1117/1.OE.51.11.117001
   Li HL, 2004, IEEE T MULTIMEDIA, V6, P624, DOI [10.1109/TMM.2004.830812, 10.1109/tmm.2004.830812]
   Li HL, 2009, IEEE T MULTIMEDIA, V11, P77, DOI 10.1109/TMM.2008.2008922
   Lin WY, 2011, IEEE T CIRC SYST VID, V21, P237, DOI 10.1109/TCSVT.2011.2106290
   Liu C.-H., 2008, Global Telecommunications Conference, P1
   MAUERSBERGER W, 1979, ELECTRON LETT, V15, P664, DOI 10.1049/el:19790472
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Shen XL, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P453, DOI 10.1109/PCS.2012.6213252
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiong J, 2013, OPT ENG, V52, DOI 10.1117/1.OE.52.7.071504
NR 23
TC 181
Z9 194
U1 0
U2 66
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 559
EP 564
DI 10.1109/TMM.2013.2291958
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800022
DA 2024-07-18
ER

PT J
AU Jin, CT
   Guillon, P
   Epain, N
   Zolfaghari, R
   van Schaik, A
   Tew, AI
   Hetherington, C
   Thorpe, J
AF Jin, Craig T.
   Guillon, Pierre
   Epain, Nicolas
   Zolfaghari, Reza
   van Schaik, Andre
   Tew, Anthony I.
   Hetherington, Carl
   Thorpe, Jonathan
TI Creating the Sydney York Morphological and Acoustic Recordings of Ears
   Database
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fast multipole boundary element method; head-related transfer function;
   morphological data; virtual auditory space; 3D audio; 3D mesh models
ID BOUNDARY-ELEMENT METHOD
AB This paper introduces the process for creating the Sydney York Morphological and Acoustic Recordings of Ears (SYMARE) database. The SYMARE database supports research exploring the relationship between the morphology of human outer ears and their acoustic filtering properties-a relationship that is viewed by many as holding the key to human spatial hearing and the future of 3D personal audio. The SYMARE database is comprised of acoustically measured head-related impulse responses for 61 listeners (48 male/13 female), multiple high-resolution surface mesh models (upper torso, head and ears) for these listeners obtained from magnetic resonance imaging (MRI) data, and the corresponding simulated HRIR data for these listeners generated using the Fast Multipole Boundary Element Method (FM-BEM). In this work, we compare acoustically measured HRIR data for 61 listeners with the listeners' corresponding simulated HRIR data generated using the FM-BEM.
C1 [Jin, Craig T.; Guillon, Pierre; Epain, Nicolas; Zolfaghari, Reza; van Schaik, Andre] Univ Sydney, CARLab, Sch Elect & Informat Engn, Sydney, NSW 2006, Australia.
   [Tew, Anthony I.; Hetherington, Carl; Thorpe, Jonathan] Univ York, Dept Elect, York YO10 5DD, N Yorkshire, England.
C3 University of Sydney; University of York - UK
RP Jin, CT (corresponding author), Univ Sydney, CARLab, Sch Elect & Informat Engn, Sydney, NSW 2006, Australia.
EM craig.jin@sydney.edu.au; ait1@ohm.york.ac.uk
OI van Schaik, Andre/0000-0001-6140-017X; Jin, Craig/0000-0003-4636-753X;
   Guillon, Pierre/0009-0005-1755-6464
FU UK Engineering and Physical Sciences Research Council [GR/T28140/01];
   Australian Research Council [DP110102920]
FX This work was supported in part by the UK Engineering and Physical
   Sciences Research Council (grant GR/T28140/01) and in part by the
   Australian Research Council's Discovery Projects funding scheme (project
   number DP110102920). This paper is an extended version of the original
   paper which appeared in the Proceedings of IEEE ICME 2012 Conference and
   was among the top-rated 4% of ICME' 12 submissions. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Xian-Sheng Hua.
CR Algazi VR, 2001, J ACOUST SOC AM, V109, P1110, DOI 10.1121/1.1349185
   Cheng CI, 1999, INT CONF ACOUST SPEE, P961, DOI 10.1109/ICASSP.1999.759854
   Chirikjian G. S., 2004, International Journal of Computational Intelligence and Applications, V4, P401, DOI 10.1142/S1469026804001410
   Guillon P., 2009, THESIS U MAINE MANS
   Gumerov NA, 2009, J ACOUST SOC AM, V125, P191, DOI 10.1121/1.3021297
   Huopaniemi J., 2007, P 30 AES INT C INT A
   Jot J.-M., 1998, P AES 105 CONV SAN F
   Kreuzer W, 2009, J ACOUST SOC AM, V126, P1280, DOI 10.1121/1.3177264
   Middlebrooks JC, 1999, J ACOUST SOC AM, V106, P1480, DOI 10.1121/1.427176
   MIDDLEBROOKS JC, 1989, J ACOUST SOC AM, V86, P89, DOI 10.1121/1.398224
   MOLLER H, 1995, J AUDIO ENG SOC, V43, P300
   Pralong D, 1996, J ACOUST SOC AM, V100, P3785, DOI 10.1121/1.417337
   Pralong D., 1996, VIRTUAL AUDITORY SPA, P109, DOI DOI 10.1007/978-3-662-22594-3_4
   SCHENCK HA, 1968, J ACOUST SOC AM, V44, P41, DOI 10.1121/1.1911085
NR 14
TC 56
Z9 64
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 37
EP 46
DI 10.1109/TMM.2013.2282134
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100004
DA 2024-07-18
ER

PT J
AU Zhao, Q
   Wan, L
   Feng, W
   Zhang, JW
   Wong, TT
AF Zhao, Qiang
   Wan, Liang
   Feng, Wei
   Zhang, Jiawan
   Wong, Tien-Tsin
TI Cube2Video: Navigate Between Cubic Panoramas in Real-Time
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cubic panoramas; Temporal smoothness; Triangle-to-triangle
   homography-based warping; Video-viewing mode; Virtual navigation.
AB Online virtual navigation systems enable users to hop from one panorama to another, which belong to a sparse point-to-point collection, resulting in a less pleasant viewing experience. In this paper, we present a novel method, namely Cube2Video, to support navigating between cubic panoramas in a video-viewing mode. Our method circumvents the intrinsic challenge of cubic panoramas, i.e., the discontinuities between cube faces, in an efficient way. The proposed method extends the matching-triangulation-interpolation procedure with special considerations of the spherical domain. A triangle-to-triangle homography-based warping is developed to achieve physically plausible and visually pleasant interpolation results. The temporal smoothness of the synthesized video sequence is improved by means of a compensation transformation. As experimental results demonstrate, our method can synthesize pleasant video sequences in real time, thus mimicking walking or driving navigation.
C1 [Zhao, Qiang; Feng, Wei] Tianjin Univ, Sch Comp Sci & Technol, Tianjin Key Lab Cognit Comp & Applicat, Tianjin 300072, Peoples R China.
   [Wan, Liang; Zhang, Jiawan] Tianjin Univ, Sch Comp Software, Tianjin 300072, Peoples R China.
   [Wong, Tien-Tsin] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Tianjin University; Tianjin University; Chinese University of Hong Kong
RP Wan, L (corresponding author), Tianjin Univ, Sch Comp Software, Tianjin 300072, Peoples R China.
EM qiangzhao_tju@163.com; lwan@tju.edu.cn; wfeng@tju.edu.cn;
   jwzhang@tju.edu.cn; ttwong@acm.org
RI Feng, Wei/E-3985-2016
OI Feng, Wei/0000-0003-3809-1086; Zhang, Jiawan/0000-0002-0667-6744
FU National Natural Science Foundation of China [61100122, 61100121]; New
   Century Excellent Talents in University [NCET-11-0365]; Tianjin Science
   Foundation for Youth [12JCQNJC00100]; The Tianjin Key Lab for Advanced
   Signal Processing, Civil Aviation University of China [TJKLASP-2012-2];
   General Research Fund of Hong Kong [CUHK 417411];  [CUHK SHIAE 8115034]
FX The work was supported in part by the National Natural Science
   Foundation of China (61100122 and 61100121), New Century Excellent
   Talents in University (NCET-11-0365), Tianjin Science Foundation for
   Youth (12JCQNJC00100), the research Fund from The Tianjin Key Lab for
   Advanced Signal Processing, Civil Aviation University of China
   (TJKLASP-2012-2), General Research Fund of Hong Kong (CUHK 417411), and
   a research grant CUHK SHIAE 8115034.
CR Anguelov D, 2010, COMPUTER, V43, P32, DOI 10.1109/MC.2010.170
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Chen B, 2009, UIST 2009: PROCEEDINGS OF THE 22ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P223
   Chen S. E., 1993, Computer Graphics Proceedings, P279, DOI 10.1145/166117.166153
   Chen S. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P29, DOI 10.1145/218380.218395
   Chi Peng, 2010, 2010 International Computer Symposium (ICS 2010), P319, DOI 10.1109/COMPSYM.2010.5685494
   Corporation M., BING MAPS DRIV DIR T
   Domin M, 2008, MATH VIS, P185, DOI 10.1007/978-3-540-72630-2_11
   Fan B, 2010, PROC CVPR IEEE, P390, DOI 10.1109/CVPR.2010.5540186
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   FU CW, 1999, P 10 EUR WORKSH REND, P169
   Goesele M., 2010, ACM SIGGRAPH 2010 PA
   Guan XY, 2009, 2009 INTERNATIONAL CONFERENCE ON CYBERWORLDS, P96, DOI 10.1109/CW.2009.63
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Kim D.-H., 2004, ACCV, V1, P1109
   Kolhatkar Shanat, 2010, Proceedings of the 2010 Seventh Canadian Conference on Computer and Robot Vision (CRV 2010), P55, DOI 10.1109/CRV.2010.14
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Lhuillier M., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P139, DOI 10.1109/CVPR.1999.784621
   Lim J, 2010, IEEE T PATTERN ANAL, V32, P1907, DOI 10.1109/TPAMI.2010.113
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Manning R. A., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P388, DOI 10.1109/CVPR.1999.786968
   McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398
   Seitz S. M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P21, DOI 10.1145/237170.237196
   Shi F, 2009, 2009 CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION, P200, DOI 10.1109/CRV.2009.34
   Sin AMK, 2005, IEEE T IMAGE PROCESS, V14, P241, DOI 10.1109/TIP.2004.840690
   Siu AMK, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P101
   Tsai MJ, 2010, INT CONF ACOUST SPEE, P1578, DOI 10.1109/ICASSP.2010.5495529
   Wang Chen, 2009, International Journal of Virtual Reality, V8, P87
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wexler Y, 2000, PROC CVPR IEEE, P576, DOI 10.1109/CVPR.2000.855871
   XIAO J, 2002, P EUROGRAPHICS 02, P153
   Yan-Fai Chan, 1999, Proceedings. Seventh Pacific Conference on Computer Graphics and Applications (Cat. No.PR00293), P231, DOI 10.1109/PCCGA.1999.803367
   Yeung K. H., 2000, THESIS U HONG KONG H
   Zhang CX, 2011, APPL OPTICS, V50, P4286, DOI 10.1364/AO.50.004286
   Zitnick CL, 2007, INT J COMPUT VISION, V75, P49, DOI 10.1007/s11263-006-0018-8
NR 35
TC 33
Z9 45
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 1745
EP 1754
DI 10.1109/TMM.2013.2280249
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900003
DA 2024-07-18
ER

PT J
AU Wu, JJ
   Lin, WS
   Shi, GM
   Liu, AM
AF Wu, Jinjian
   Lin, Weisi
   Shi, Guangming
   Liu, Anmin
TI Reduced-Reference Image Quality Assessment With Visual Information
   Fidelity
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image quality assessment; reduced-reference; internal generative
   mechanism; information fidelity
ID FREE-ENERGY PRINCIPLE; SIMILARITY; BRAIN
AB Reduced-reference (RR) image quality assessment (IQA) aims to use less data about the reference image and achieve higher evaluation accuracy. Recent research on brain theory suggests that the human visual system (HVS) actively predicts the primary visual information and tries to avoid the residual uncertainty for image perception and understanding. Therefore, the perceptual quality relies to the information fidelities of the primary visual information and the residual uncertainty. In this paper, we propose a novel RR IQA index based on visual information fidelity. We advocate that distortions on the primary visual information mainly disturb image understanding, and distortions on the residual uncertainty mainly change the comfort of perception. We separately compute the quantities of the primary visual information and the residual uncertainty of an image. Then the fidelities of the two types of information are separately evaluated for quality assessment. Experimental results demonstrate that the proposed index uses few data (30 bits) and achieves high consistency with human perception.
C1 [Wu, Jinjian; Shi, Guangming] Xidian Univ, Sch Elect Engn, Minist Educ China, Key Lab Intelligent Percept & Image Understanding, Xian, Peoples R China.
   [Lin, Weisi; Liu, Anmin] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
C3 Xidian University; Nanyang Technological University
RP Lin, WS (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM jinjian.wu@mail.xidian.edu.cn; wslin@ntu.edu.sg; gmshi@xidian.edu.cn;
   liua0002@ntu.edu.sg
RI Lin, Weisi/A-3696-2011; Lin, Weisi/A-8011-2012; Wu,
   Jinjian/GQH-0222-2022
OI Lin, Weisi/0000-0001-9866-1947; 
FU Major State Basic Research Development Program of China (973 Program)
   [2013CB329402]; NSF of China [61033004, 61070138, 61072104, 61227004];
   Fundamental Research Funds for the Central Universities [K50513100005,
   K5051202034]
FX This work was supported by the Major State Basic Research Development
   Program of China (973 Program) (No. 2013CB329402), NSF of China (No.
   61033004, 61070138, 61072104, and 61227004), and the Fundamental
   Research Funds for the Central Universities (No. K50513100005 and
   K5051202034).. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Sheng-Wei (Kuan-Ta)
   Chen.
CR [Anonymous], 2003, Final report from the video quality experts group on the validation of objective models of video quality assessment
   Chen G.H., 2006, P 2006 IEEE INT C AC, V2, pII
   Friston KJ, 2006, J PHYSIOL-PARIS, V100, P70, DOI 10.1016/j.jphysparis.2006.10.001
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   Gao XB, 2009, IEEE T IMAGE PROCESS, V18, P1409, DOI 10.1109/TIP.2009.2018014
   Knill DC, 2004, TRENDS NEUROSCI, V27, P712, DOI 10.1016/j.tins.2004.10.007
   Li Q, 2009, IEEE J-STSP, V3, P202, DOI 10.1109/JSTSP.2009.2014497
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Ma L, 2011, IEEE T MULTIMEDIA, V13, P824, DOI 10.1109/TMM.2011.2109701
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Ponomarenko N., 2008, Tampere image database
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   Sheikh H. R., 2004, IMAGE VIDEO QUALITY
   Soundararajan R, 2012, IEEE T IMAGE PROCESS, V21, P517, DOI 10.1109/TIP.2011.2166082
   Sternberg R. J., 2003, Cognitive Psychology
   Wang Z, 2005, PROC SPIE, V5666, P149, DOI 10.1117/12.597306
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wu JJ, 2012, J VIS COMMUN IMAGE R, V23, P845, DOI 10.1016/j.jvcir.2012.04.010
   Zhai G., IEEE T IMAG IN PRESS
   Zhang XJ, 2008, IEEE T IMAGE PROCESS, V17, P887, DOI 10.1109/TIP.2008.924279
NR 21
TC 134
Z9 142
U1 0
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2013
VL 15
IS 7
BP 1700
EP 1705
DI 10.1109/TMM.2013.2266093
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 236PQ
UT WOS:000325811800020
DA 2024-07-18
ER

PT J
AU Zhang, FL
   Wang, M
   Hu, SM
AF Zhang, Fang-Lue
   Wang, Miao
   Hu, Shi-Min
TI Aesthetic Image Enhancement by Dependence-Aware Object Recomposition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image enhancement; photograph composition; region dependence
ID PHOTO; SEGMENTATION; RECOGNITION; SIMILARITY; COLOR; MODEL
AB This paper proposes an image-enhancement method to optimize photograph composition by rearranging foreground objects in the photograph. To adjust objects' positions while keeping the original scene content, we first perform a novel structure dependence analysis on the image to obtain the dependencies between all background regions. To determine the optimal positions for foreground objects, we formulate an optimization problem based on widely used heuristics for aesthetically pleasing pictures. Semantic relations between foreground objects are also taken into account during optimization. The final output is produced by moving foreground objects, together with their dependent regions, to optimal positions. The results show that our approach can effectively optimize photographs with single or multiple foreground objects without compromising the original photograph content.
C1 [Zhang, Fang-Lue; Wang, Miao; Hu, Shi-Min] Tsinghua Univ, Dept Comp Sci & Technol, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Zhang, FL (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
EM z.fanglue@gmail.com; wangmiaoxdu@gmail.com; shimin@tsinghua.edu.cn
RI Hu, Shi-Min/AAW-1952-2020
FU National Basic Research Project of China [2011CB302205]; Natural Science
   Foundation of China [61120106007]; National High Technology Research and
   Development Program of China [2012AA011802]; National Significant
   Science and Technology Program [2012ZX01039001 003]
FX This work was supported in part by the National Basic Research Project
   of China under Project 2011CB302205, the Natural Science Foundation of
   China under Project 61120106007, the National High Technology Research
   and Development Program of China under Project 2012AA011802, and the
   National Significant Science and Technology Program under Project
   2012ZX01039001 003. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Zhu Liu.
CR Abdullah R, 2011, LECT NOTES COMPUT SC, V6815, P13, DOI 10.1007/978-3-642-22571-0_2
   Aksoy S, 2001, PATTERN RECOGN LETT, V22, P563, DOI 10.1016/S0167-8655(00)00112-4
   [Anonymous], 1976, PHOTOGRAPHIC COMPOSI
   [Anonymous], 2010, P 18 ACM INT C MULTI
   [Anonymous], 2011, DIGITAL SLR HDB
   Arnheim R., 1969, ART VISUAL PERCEPTIO
   Banerjee S, 2007, IEEE T IMAGE PROCESS, V16, P1807, DOI 10.1109/TIP.2007.898992
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bhattacharya S, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2037676.2037678
   Bie XH, 2011, J COMPUT SCI TECH-CH, V26, P1011, DOI 10.1007/s11390-011-1197-5
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Cheng MM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778820
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Dahan MJ, 2012, VISUAL COMPUT, V28, P1181, DOI 10.1007/s00371-011-0667-7
   Datta R., 2006, P ECCV, P7
   Delong A, 2010, PROC CVPR IEEE, P2173, DOI 10.1109/CVPR.2010.5539897
   Farbman Z, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531373
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Guo YW, 2012, COMPUT GRAPH FORUM, V31, P2193, DOI 10.1111/j.1467-8659.2012.03212.x
   Huang H, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024189
   Huang TJ, 2011, SCI CHINA INFORM SCI, V54, P2461, DOI 10.1007/s11432-011-4487-1
   Joshi N., 2010, ACM T GRAPH, V29, P30
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968
   Leder H, 2004, BRIT J PSYCHOL, V95, P489, DOI 10.1348/0007126042369811
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Ling Y, 2012, VISUAL COMPUT, V28, P733, DOI 10.1007/s00371-012-0691-2
   Liu L., 2010, Electroactive Polymer Actuators and Devices (EAPAD) 2010, 8 March 2010, P1
   Liu LG, 2010, COMPUT GRAPH FORUM, V29, P469, DOI 10.1111/j.1467-8659.2009.01616.x
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Merrell P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964982
   Nishiyama M., 2009, P 17 ACM INT C MULTI, P669
   Ou LC, 2006, COLOR RES APPL, V31, P191, DOI 10.1002/col.20208
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Porter T., 1984, Computers & Graphics, V18, P253
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Santella A., 2006, Conference on Human Factors in Computing Systems. CHI2006, P771
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Wang D, 2011, VISUAL COMPUT, V27, P853, DOI 10.1007/s00371-011-0559-x
   Wang J, 2007, FOUND TRENDS COMPUT, V3, P97, DOI 10.1561/0600000019
   Wang XH, 2012, J COMPUT SCI TECH-CH, V27, P1119, DOI 10.1007/s11390-012-1290-4
   Weber E. A., 1980, VISION COMPOSITION P
   Xiao CX, 2011, IEEE T VIS COMPUT GR, V17, P1122, DOI 10.1109/TVCG.2010.226
   Xue S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185580
   Zhang Y, 2011, VISUAL COMPUT, V27, P739, DOI 10.1007/s00371-011-0583-x
   Zheng YY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185595
   Zhong F, 2011, VISUAL COMPUT, V27, P707, DOI 10.1007/s00371-011-0588-5
NR 51
TC 47
Z9 54
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2013
VL 15
IS 7
BP 1480
EP 1490
DI 10.1109/TMM.2013.2268051
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 236PQ
UT WOS:000325811800002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhou, L
   Wang, HH
AF Zhou, Liang
   Wang, Haohong
TI Toward Blind Scheduling in Mobile Media Cloud: Fairness, Simplicity, and
   Asymptotic Optimality
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mobile media cloud; blind scheduling; asymptotic optimality; fairness;
   simplicity
AB Recent advances in mobile media cloud (MMC) make it possible for users to enjoy the multimedia applications at anytime and anywhere. Most existing scheduling algorithms for MMC assume that the system parameters, such as user demand rate and server service time, are known to the scheduler. However, this assumption is invalid in many practical scenarios. In this paper, we consider a blind scenario where the above system parameters are unavailable. We aim at developing a blind scheduling algorithm (BSA) that performs well across magnitudes of fairness, simplicity and asymptotic optimality for a relatively general MMC. Specifically, the blind scheduling is first formulated as a finite time horizon optimization problem and fairness is required to be maintained at each time point with a given probability from the scheduler. Next, BSA routes the new users to the media service provider (MSP) whose weighted idle time is the longest, then assigns the available MSPs according to the fairness on the idle time. Importantly, we demonstrate that BSA is asymptotically optimal in the Halfin-Whitt heavy traffic (HWHT) regime. The asymptotic optimality is in the sense that the scheduling asymptotically and stochastically minimizes the steady-state waiting time of all the users. Our analysis also shows that in the HWHT regime the heterogeneous MSP system outperforms its homogeneous MSP counterpart in terms of the user waiting time. Moreover, we apply BSA to the program recommendation system and investigate its property.
C1 [Zhou, Liang] Nanjing Univ Posts & Telecommun, Key Lab Broadband Wireless Commun & Sensor Networ, Minist Educ, Nanjing 210003, Peoples R China.
   [Wang, Haohong] TCL Corp, TCL Res Amer, Santa Clara, CA 95127 USA.
C3 Nanjing University of Posts & Telecommunications; TCL Inc.
RP Zhou, L (corresponding author), Nanjing Univ Posts & Telecommun, Key Lab Broadband Wireless Commun & Sensor Networ, Minist Educ, Nanjing 210003, Peoples R China.
EM liang.zhou@ieee.org; haohong@ieee.org
FU State Key Development Program of Basic Research of China [2013CB329005];
   National Natural Science Foundation of China [61201165, 61271240];
   Priority Academic Program Development of Jiangsu Higher Education
   Institutions; Nanjing University of Posts and Telecommunications
   Foundation [NY211032]
FX This work was supported in part by the State Key Development Program of
   Basic Research of China (2013CB329005), National Natural Science
   Foundation of China (Grant No. 61201165 and No. 61271240), Priority
   Academic Program Development of Jiangsu Higher Education Institutions,
   and Nanjing University of Posts and Telecommunications Foundation (Grant
   No. NY211032). The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Joel Rodrigues.
CR Armony M., BLIND FAIR ROUTING L
   Armony M, 2010, OPER RES, V58, P624, DOI 10.1287/opre.1090.0777
   Baliga J, 2011, P IEEE, V99, P149, DOI 10.1109/JPROC.2010.2060451
   Boloor K., 2010, P IEEE CLOUDCOM
   Boloor K, 2010, GLOB TELECOMM CONF
   Boyd S.P., 2004, Convex optimization, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441]
   Browne S., 1995, Advances in Queueing: Theory, Methods, and Open Problems, P463
   Endo PT, 2011, IEEE NETWORK, V25, P42, DOI 10.1109/MNET.2011.5958007
   Ginneken B. V., 2006, P INT C PATT REC
   Gurvich I, 2009, MATH OPER RES, V34, P363, DOI 10.1287/moor.1080.0366
   Huang YM, 2009, IEEE J SEL AREA COMM, V27, P400, DOI 10.1109/JSAC.2009.090505
   Iosup A, 2011, IEEE T PARALL DISTR, V22, P931, DOI 10.1109/TPDS.2011.66
   Javadi B., 2011, IEEE T PARALL DISTR, V22, P1045
   Juhnke E., 2011, P IEEE CLOUD
   Kleinrock L., 1976, Queuing Systems, V2
   Lai CF, 2011, FUTURE GENER COMP SY, V27, P823, DOI 10.1016/j.future.2010.10.002
   Lai CF, 2010, IEEE SYST J, V4, P262, DOI 10.1109/JSYST.2010.2047175
   Li ZJ, 2011, IEEE INTELL SYST, V26, P73, DOI 10.1109/MIS.2011.10
   LIGGETT TM, 1985, ANN PROBAB, V13, P1279, DOI 10.1214/aop/1176992811
   Liu X, 2010, IEEE NETWORK, V24, P25, DOI 10.1109/MNET.2010.5510915
   Meng SC, 2011, IEEE T KNOWL DATA EN, V23, P1328, DOI 10.1109/TKDE.2011.70
   Mousumi P., 2011, P WICT
   Oliveira LML, 2011, INT J COMMUN SYST, V24, P1445, DOI 10.1002/dac.1228
   Rodrigues JJPC, 2010, INT J COMMUN SYST, V23, P963, DOI 10.1002/dac.1099
   Van den Bossche R., 2010, P IEEE CLOUD
   Wang L, 2012, IEEE T PARALL DISTR, V23, P296, DOI 10.1109/TPDS.2011.144
   Warneke D, 2011, IEEE T PARALL DISTR, V22, P985, DOI 10.1109/TPDS.2011.65
   Wen Y. G., 2012, P IEEE INFOCOM
   Wickramarathne TL, 2011, IEEE T KNOWL DATA EN, V23, P175, DOI 10.1109/TKDE.2010.88
   Wu H., IEEE T IND IN PRESS
   Zhang F., 2011, P IEEE CLOUDCOM
   Zhang G., 2011, P WCSP
   Zhou L., 2012, P ICNC
   Zhou L, 2011, IEEE T MULTIMEDIA, V13, P1040, DOI 10.1109/TMM.2011.2160716
   Zhou L, 2010, IEEE J SEL AREA COMM, V28, P409, DOI 10.1109/JSAC.2010.100412
   Zhu WW, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940269
NR 36
TC 34
Z9 34
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2013
VL 15
IS 4
BP 735
EP 746
DI 10.1109/TMM.2013.2241044
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148FK
UT WOS:000319228500003
DA 2024-07-18
ER

PT J
AU Anand, S
   Subbalakshmi, KP
   Chandramouli, R
AF Anand, S.
   Subbalakshmi, K. P.
   Chandramouli, R.
TI A Quantitative Model and Analysis of Information Confusion in Social
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Social networks; information; confusion; aggression; passiveness
AB Information consumers in online social networks receive information from multiple information providers, which results in confusion. The amount of confusion depends on three main factors-(a) attributes of the source, (b) characteristics of the consumer and (c) trust relation between the information provider and the consumer. While information confusion has been qualitatively observed in social networks, no quantitative model or analysis was presented. We present the first quantitative model to analyze confusion in the presence of multiple information providers. We address the following fundamental issues-(i) What is a good model for confusion? (ii) How does the quality of information degrade due to confusion? (iii) What are good strategies for the information providers to control the power or the intensity with which the information is transmitted? The scenario is modeled as a non-cooperative game with pricing, whose Nash equilibrium provides the solution to the questions posed above. We use data from Twitter (e. g., on full body scan in airports) and diabetes outreach networks to illustrate the analysis. We use the solution of the non-cooperative game to study the confusion levels of consumers, in terms of the aggressiveness and passiveness of the information providers. Results indicate that confusion levels are high in in networks in which all information providers are equally trusted. In networks where information providers are unequally trusted, the confusion levels are moderate.
C1 [Anand, S.] Rutgers State Univ, Wireless Informat Networks Lab WINLAB, New Brunswick, NJ 08901 USA.
   [Subbalakshmi, K. P.; Chandramouli, R.] Stevens Inst Technol, Dept Elect & Comp Engn, Hoboken, NJ 07030 USA.
C3 Rutgers University System; Rutgers University New Brunswick; Stevens
   Institute of Technology
RP Anand, S (corresponding author), Rutgers State Univ, Wireless Informat Networks Lab WINLAB, New Brunswick, NJ 08901 USA.
EM anands72@winlab.rutgers.edu; ksubbala@stevens.edu; mouli@stevens.edu
RI Subbalakshmi, Koduvayur/JYO-3634-2024
OI Subbalakshmi, Koduvayur/0000-0002-1670-9378; Chandramouli,
   Rajarathnam/0000-0001-7889-740X
CR Adams P., 2010, NOISY CHANNEL    JUL
   [Anonymous], 2009, Sentiment140
   [Anonymous], 2000, Game theory evolving: A problem-centered introduction to modeling strategic behavior
   [Anonymous], 2010, P INT C COMP LING CO
   Ariely D., 2010, The Upside of Irrationality: The Unexpected Benefits of Defying Logic at Work and at Home
   Chen S, 2007, LINEAR ALGEBRA APPL, V426, P610, DOI 10.1016/j.laa.2007.05.040
   Chung K. S. K., 2005, P C APPL SOC NETW AN
   Corteville L., 2009, An interorganizational social network analysis of the Michigan Diabetes Outreach Networks: Measuring relationships in community networks
   Fiedler M., 1962, Mat Fyz Casopis, V12, P123
   Garg A., 2009, P SOC MOB WEB WORKSH
   Golbeck J., 2006, ACM Transactions on Internet Technology, V6, P497, DOI 10.1145/1183463.1183470
   Jin X, 2011, P INT C VER LARG DAT
   Johansen C, 2001, JNCI-J NATL CANCER I, V93, P203, DOI 10.1093/jnci/93.3.203
   Kovach S., 2007, LIFE EXT MAG     AUG
   Kumar R, 2002, COMPUTER, V35, P32, DOI 10.1109/MC.2002.1046971
   Luenberger DG, 1984, LINEAR NONLINEAR PRO
   Meyer C.D., 1972, Matrix Theory and Applied Linear Algebra
   Morris T., 2010, All a Twitter: A personal and professional guide to social networking with Twitter, V1st
   Mui L., 2002, THESIS MIT CAMBRIDGE
   Olson M., 2009, BRAND DEV        JUL
   Perera R. W. D., 2010, P IEEE MIL COMM C MI
   POOLE G, 1974, SIAM REV, V16, P419, DOI 10.1137/1016079
   Reed D, 2011, PERSONAL NETWORK NEW
   Singh V. K., 2009, P SIGMM WORKSH SOC M
   SOLOMON H, 1960, MATH THINKING MEASUR
   Tseng C-Y, 2009, P IEEE INT C COMP SC
   Varian H.R., 1996, Intermediate Microeconomics: A Modern Approach, V4th
   Viterbi A. J., 1995, CDMA PRINCIPLES SPRE
   Yardi S., 2008, P ACM SPEC INT GROUP
NR 29
TC 5
Z9 6
U1 2
U2 43
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2013
VL 15
IS 1
BP 207
EP 223
DI 10.1109/TMM.2012.2225031
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 058PS
UT WOS:000312646600017
DA 2024-07-18
ER

PT J
AU Salem, F
   Yagle, AE
AF Salem, Faisal
   Yagle, Andrew E.
TI Non-Parametric Super-Resolution Using a Bi-Sensor Camera
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Low-resolution basis; polyphase components; sampling rate diversity;
   super-resolution
ID IMAGE REGISTRATION; RECONSTRUCTION; MAP
AB Multiframe super-resolution is the problem of reconstructing a single high-resolution (HR) image from several low-resolution (LR) versions of it. We assume that the original HR image undergoes different linear transforms, where each transform can be approximated as a set of linear shift-invariant transforms over different subregions of the HR image. The linearly transformed versions of the HR image are then downsampled, resulting in different LR images. Under the assumption of linearity, these LR images can form a basis that spans the set of the polyphase components (PPCs) of the HR image. We propose sampling rate diversity, where a secondary LR image, acquired by a secondary sensor of different (lower) sampling rate, is used as a reference to make known portions (subpolyphase components) of the PPCs of the reconstructed HR image. This setup allows for non-parametric reconstruction of the PPCs, where no knowledge of the underlying transforms is required, by solving for the expansion coefficients of the PPCs, in terms of the LR basis.
C1 [Salem, Faisal] King Abdulaziz City Sci & Technol, Riyadh 11442, Saudi Arabia.
   [Yagle, Andrew E.] Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.
C3 King Abdulaziz City for Science & Technology; University of Michigan
   System; University of Michigan
RP Salem, F (corresponding author), King Abdulaziz City Sci & Technol, Riyadh 11442, Saudi Arabia.
EM falsalem@kacst.edu.sa; aey@eecs.umich.edu
OI Salem, Faisal/0000-0001-9099-5635
FU King Abdulaziz City for Science and Technology
FX This work was supported in part by King Abdulaziz City for Science and
   Technology. This work was completed when F. Salem was a Ph.D. student at
   the University of Michigan. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Zhihai
   (Henry) He.
CR Alam MS, 2000, IEEE T INSTRUM MEAS, V49, P915, DOI 10.1109/19.872908
   [Anonymous], 2004, DISTINGUISHED DISSER
   Babacan SD, 2011, IEEE T IMAGE PROCESS, V20, P984, DOI 10.1109/TIP.2010.2080278
   Baboulaz L, 2009, IEEE T IMAGE PROCESS, V18, P281, DOI 10.1109/TIP.2008.2009378
   Banerjee S, 2009, IEEE T MULTIMEDIA, V11, P455, DOI 10.1109/TMM.2009.2012925
   BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984
   Belekos SP, 2010, IEEE T IMAGE PROCESS, V19, P1451, DOI 10.1109/TIP.2010.2042115
   Ben-Ezra M, 2005, IEEE T PATTERN ANAL, V27, P977, DOI 10.1109/TPAMI.2005.129
   Ben-Ezra M, 2011, IEEE T PATTERN ANAL, V33, P1370, DOI 10.1109/TPAMI.2010.213
   BROWN LG, 1992, COMPUT SURV, V24, P325, DOI 10.1145/146370.146374
   Chambolle A, 2004, J MATH IMAGING VIS, V20, P89
   Chaudhuri S., 2005, MOTION FREE SUPER RE
   Chaudhuri S., 2001, SUPER RESOLUTION IMA
   Cheng MH, 2011, SIGNAL PROCESS, V91, P1284, DOI 10.1016/j.sigpro.2010.12.016
   Farsiu S, 2004, IEEE T IMAGE PROCESS, V13, P1327, DOI 10.1109/TIP.2004.834669
   Golub GH, 1999, SIAM J MATRIX ANAL A, V21, P185, DOI 10.1137/S0895479897326432
   He Y, 2009, IMAGE VISION COMPUT, V27, P364, DOI 10.1016/j.imavis.2008.05.010
   Huffel S.V., 1991, The Total Least Squares Problem: Computational Aspects and Analysis
   Ji H, 2009, IEEE T PATTERN ANAL, V31, P649, DOI 10.1109/TPAMI.2008.103
   Li XL, 2010, SIGNAL PROCESS, V90, P405, DOI 10.1016/j.sigpro.2009.05.028
   Lim J.S., 1990, Two-dimensional Signal and Image Processing
   Lin ZC, 2004, IEEE T PATTERN ANAL, V26, P83, DOI 10.1109/TPAMI.2004.1261081
   Markovsky I, 2007, SIGNAL PROCESS, V87, P2283, DOI 10.1016/j.sigpro.2007.04.004
   Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207
   PAXMAN RG, 1992, J OPT SOC AM A, V9, P1072, DOI 10.1364/JOSAA.9.001072
   Protter M, 2009, IEEE T IMAGE PROCESS, V18, P36, DOI 10.1109/TIP.2008.2008067
   Rajan D, 2003, IEEE T PATTERN ANAL, V25, P1102, DOI 10.1109/TPAMI.2003.1227986
   Rajan D, 2001, INT CONF ACOUST SPEE, P1837, DOI 10.1109/ICASSP.2001.941300
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Shankar PM, 2008, J OPT SOC AM A, V25, P1199, DOI 10.1364/JOSAA.25.001199
   Shi GM, 2011, IEEE T IMAGE PROCESS, V20, P276, DOI 10.1109/TIP.2010.2052271
   Sroubek F, 2007, IEEE T IMAGE PROCESS, V16, P2322, DOI 10.1109/TIP.2007.903256
   Stern A, 2001, APPL OPTICS, V40, P4706, DOI 10.1364/AO.40.004706
   Tai YW, 2010, IEEE T PATTERN ANAL, V32, P1012, DOI 10.1109/TPAMI.2009.97
   Takeda H, 2009, IEEE T IMAGE PROCESS, V18, P1958, DOI 10.1109/TIP.2009.2023703
   Vandewalle P, 2007, IEEE T SIGNAL PROCES, V55, P3687, DOI 10.1109/TSP.2007.894257
   Vogel CR, 1998, IEEE T IMAGE PROCESS, V7, P813, DOI 10.1109/83.679423
   Wirawan, 1999, 1999 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258), P3229, DOI 10.1109/ICASSP.1999.757529
   Yang H, 2008, IEEE SIGNAL PROC LET, V15, P289, DOI 10.1109/LSP.2007.911743
   Yuan QQ, 2010, IEEE T IMAGE PROCESS, V19, P3157, DOI 10.1109/TIP.2010.2055571
   Zhang LP, 2010, SIGNAL PROCESS, V90, P848, DOI 10.1016/j.sigpro.2009.09.002
NR 41
TC 3
Z9 12
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2013
VL 15
IS 1
BP 27
EP 40
DI 10.1109/TMM.2012.2225037
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 058PS
UT WOS:000312646600003
DA 2024-07-18
ER

PT J
AU Kuo, YH
   Cheng, WH
   Lin, HT
   Hsu, WH
AF Kuo, Yin-Hsi
   Cheng, Wen-Huang
   Lin, Hsuan-Tien
   Hsu, Winston H.
TI Unsupervised Semantic Feature Discovery for Image Object Retrieval and
   Tag Refinement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image graph; image object retrieval; semantic feature discovery; tag
   refinement
AB We have witnessed the exponential growth of images and videos with the prevalence of capture devices and the ease of social services such as Flickr and Facebook. Meanwhile, enormous media collections are along with rich contextual cues such as tags, geo-locations, descriptions, and time. To obtain desired images, users usually issue a query to a search engine using either an image or keywords. Therefore, the existing solutions for image retrieval rely on either the image contents (e.g., low-level features) or the surrounding texts (e.g., descriptions, tags) only. Those solutions usually suffer from low recall rates because small changes in lighting conditions, viewpoints, occlusions, or (missing) noisy tags can degrade the performance significantly. In this work, we tackle the problem by leveraging both the image contents and associated textual information in the social media to approximate the semantic representations for the two modalities. We propose a general framework to augment each image with relevant semantic (visual and textual) features by using graphs among images. The framework automatically discovers relevant semantic features by propagation and selection in textual and visual image graphs in an unsupervised manner. We investigate the effectiveness of the framework when using different optimization methods for maximizing efficiency. The proposed framework can be directly applied to various applications, such as keyword-based image search, image object retrieval, and tag refinement. Experimental results confirm that the proposed framework effectively improves the performance of these emerging image retrieval applications.
C1 [Kuo, Yin-Hsi] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 106, Taiwan.
   [Cheng, Wen-Huang] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 115, Taiwan.
   [Lin, Hsuan-Tien] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
   [Hsu, Winston H.] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10617, Taiwan.
   [Hsu, Winston H.] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10617, Taiwan.
C3 National Taiwan University; Academia Sinica - Taiwan; National Taiwan
   University; National Taiwan University; National Taiwan University
RP Kuo, YH (corresponding author), Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 106, Taiwan.
EM kuonini@cmlab.csie.ntu.edu.tw; whcheng@citi.sinica.edu.tw;
   htlin@csie.ntu.edu.tw; winston@csie.ntu.edu.tw
RI Cheng, Wen-Huang/AAK-2774-2020; Lin, Hsuan-Tien/AAE-4359-2020
OI Lin, Hsuan-Tien/0000-0003-2968-0671
CR Ames M, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P971
   [Anonymous], 2006, ADV NEURAL INF PROCE
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], 2006, Book Annosearch: Image auto-annotation by search, DOI DOI 10.1109/CVPR.2006.58
   [Anonymous], P ACM WORKSH LSMRM
   [Anonymous], 2008, 2008 IEEE C COMPUTER
   Chen DM, 2011, PROC CVPR IEEE, P737, DOI 10.1109/CVPR.2011.5995610
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2009, PROC CVPR IEEE, P17, DOI 10.1109/CVPRW.2009.5206531
   Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137
   Douze M, 2011, PROC CVPR IEEE, P745, DOI 10.1109/CVPR.2011.5995595
   Elsayed T., 2008, P 46 ANN M ASS COMPU, P265, DOI DOI 10.3115/1557690.1557767
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Gammeter S, 2009, IEEE I CONF COMP VIS, P614, DOI 10.1109/ICCV.2009.5459180
   Kennedy L., 2009, Proc. Workshop on Web-scale Multimedia Corpus, P17
   Kennedy Lyndon., 2007, Proceedings of the 15th International Conference on Multimedia, P631, DOI DOI 10.1145/1291233.1291384
   Kuo YH, 2011, PROC CVPR IEEE, P905, DOI 10.1109/CVPR.2011.5995639
   Li Xirong., 2010, Proceedings of the ACM International Conference on Image and Video Retrieval, CIVR '10, P10
   Li Xirong., 2008, Proceedings of the 1st ACM International Conference on Multimedia Information Retrieval, MIR '08, P180
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma H, 2010, IEEE T MULTIMEDIA, V12, P462, DOI 10.1109/TMM.2010.2051360
   Mairal J., 2009, P 26 ANN INT C MACHI, P689, DOI 10.1145/1553374.1553463
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   Mallapragada PK, 2010, PROC CVPR IEEE, P3073, DOI 10.1109/CVPR.2010.5540062
   Mezaris V., 2004, TRECVID WORKSH GAITH
   Philbin J., 2008, P CVPR, P1
   Philbin J, 2010, LECT NOTES COMPUT SC, V6313, P677
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Thomee B., 2010, P INT C MULTIMEDIA M, P1473
   Turcot Panu, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P2109, DOI 10.1109/ICCVW.2009.5457541
   Wang X., 2004, ACM INT C MULTIMEDIA, P944
   Wang XG, 2011, PROC CVPR IEEE, P857, DOI 10.1109/CVPR.2011.5995399
   Wang XJ, 2010, PROC CVPR IEEE, P2987, DOI 10.1109/CVPR.2010.5540046
   Yang Y.-H., 2008, ACM MULT VANC BC CAN
   Zhang X, 2009, IEEE I CONF COMP VIS, P1103, DOI 10.1109/ICCV.2009.5459354
NR 35
TC 34
Z9 38
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
SI SI
BP 1079
EP 1090
DI 10.1109/TMM.2012.2190386
PN 1
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QL
UT WOS:000306599300013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, M
   Hong, RC
   Li, GD
   Zha, ZJ
   Yan, SC
   Chua, TS
AF Wang, Meng
   Hong, Richang
   Li, Guangda
   Zha, Zheng-Jun
   Yan, Shuicheng
   Chua, Tat-Seng
TI Event Driven Web Video Summarization by Tag Localization and Key-Shot
   Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Event-driven summarization; key-shot identification; tag localization;
   web videos
AB With the explosive growth of web videos on the Internet, it becomes challenging to efficiently browse hundreds or even thousands of videos. When searching an event query, users are often bewildered by the vast quantity of web videos returned by search engines. Exploring such results will be time consuming and it will also degrade user experience. In this paper, we present an approach for event driven web video summarization by tag localization and key-shot mining. We first localize the tags that are associated with each video into its shots. Then, we estimate the relevance of the shots with respect to the event query by matching the shot-level tags with the query. After that, we identify a set of key-shots from the shots that have high relevance scores by exploring the repeated occurrence characteristic of key sub-events. Following the scheme in [6] and [22], we provide two types of summaries, i.e., threaded video skimming and visual-textual storyboard. Experiments are conducted on a corpus that contains 60 queries and more than 10 000 web videos. The evaluation demonstrates the effectiveness of the proposed approach.
C1 [Wang, Meng; Hong, Richang] Hefei Univ Technol, Hefei 230009, Peoples R China.
   [Li, Guangda; Zha, Zheng-Jun; Yan, Shuicheng; Chua, Tat-Seng] Natl Univ Singapore, Singapore 117417, Singapore.
C3 Hefei University of Technology; National University of Singapore
RP Wang, M (corresponding author), Hefei Univ Technol, Hefei 230009, Peoples R China.
EM hongrc.hfut@gmail.com
RI Wang, Meng/ITR-8699-2023; Yan, Shuicheng/HCI-1431-2022; Zha,
   Zheng-Jun/AAE-8408-2020; Zha, Zheng-Jun/AAF-8667-2020
OI Zha, Zheng-Jun/0000-0003-2510-8993; 
FU National Science Foundation of China [61172164]; NExT Research Center;
   MDA, Singapore [WBS:R-252-300-001-490]; Division Of Physics; Direct For
   Mathematical & Physical Scien [0835713] Funding Source: National Science
   Foundation
FX This work was supported in part by the National Science Foundation of
   China (61172164) and the "NExT Research Center" funded by MDA,
   Singapore, under Grant WBS:R-252-300-001-490. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Qi Tian.
CR Cao JW, 2004, ACM-IEEE J CONF DIG, P214, DOI 10.1145/996350.996398
   Cha M., 2007, SIGCOMM IMC SAN DIEG
   CHEN BW, 2003, IEEE T MULTIMEDIA, V9, P295
   Cheng X., 2007, 7 ACM SIGCOMM C INT
   Chua T.-S., 2009, P CIVR JUL 8 10
   Chua T.-S., 2009, ACM MULT WORKSH LARG
   Duan LX, 2010, PROC CVPR IEEE, P1959, DOI 10.1109/CVPR.2010.5539870
   Duygulu P., 2003, 11 ACM INT C MULT BE
   Fisher R.A., 1970, STAT METHODS RES WOR
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Heesch D., 2004, P EUR C INF RETR SUN
   Hong R., 2009, ACM MULT 1 WORKSH SO
   Hong R., 2010, ACM INT C IM VID RET
   Hong RC, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2037676.2037681
   Hong RC, 2010, LECT NOTES COMPUT SC, V5916, P556, DOI 10.1007/978-3-642-11301-7_55
   Huet B, 2006, SIG COM TEC, P27
   Jia Y., 2008, P ACM SIGMM INT C MU
   Li G., 2009, P 17 ACM MULT BEIJ C
   Li G., 2011, INT C MULT RETR TREN
   Li Y., 2010, CBMI
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Medioni G, 2001, IEEE T PATTERN ANAL, V23, P873, DOI 10.1109/34.946990
   Money AG, 2008, J VIS COMMUN IMAGE R, V19, P121, DOI 10.1016/j.jvcir.2007.04.002
   Natsev Apostol., 2007, MULTIMEDIA 07, P991
   Neo S. Y., 2007, ACM 14 INT C MULT AU
   NIST, TREC TEXT RETR C
   NIST, TRECVID VID EV FOR O
   Settles B, 2008, ADV NEURAL INFORM PR, V21, P1289
   Shen JL, 2008, IEEE T CIRC SYST VID, V18, P1587, DOI 10.1109/TCSVT.2008.2005607
   Shen JL, 2011, MULTIMEDIA SYST, V17, P421, DOI 10.1007/s00530-010-0223-8
   Shyu ML, 2008, IEEE T MULTIMEDIA, V10, P252, DOI 10.1109/TMM.2007.911830
   Siersdorfer S., 2009, 32 INT ACM SIGIR C R
   Tan S., 2010, ACM 17 INT C MULT FL
   Tang S., 2008, TRECVID WORKSH
   Ulges A., 2008, INT C CONT BAS IM VI
   Ulges A, 2010, COMPUT VIS IMAGE UND, V114, P429, DOI 10.1016/j.cviu.2009.08.002
   Wang F., 2009, INT C MULT EXP NEW Y
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang M, 2009, COMPUT VIS IMAGE UND, V113, P384, DOI 10.1016/j.cviu.2008.08.003
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Wu X, 2006, IEEE SIGNAL PROC MAG, V23, P59
   Wu X., 2007, 15 INT ACM C MULT AU
   Yahiaoui I, 2003, EURASIP J APPL SIG P, V2003, P48, DOI 10.1155/S1110865703210052
   Yang Hui., 2003, P 26 ANN INT ACM SIG, P33, DOI [DOI 10.1145/860435.860444, 10.1145/860435.860444]
   Yu-Chyeh Wu, 2004, Proceedings. IEEE Sixth International Symposium on Multimedia Software, P294
   Zhao WL, 2007, IEEE T MULTIMEDIA, V9, P1037, DOI 10.1109/TMM.2007.898928
   Zhu XQ, 2003, MULTIMEDIA SYST, V9, P31, DOI 10.1007/s00530-003-0076-5
NR 47
TC 209
Z9 222
U1 0
U2 56
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
SI SI
BP 975
EP 985
DI 10.1109/TMM.2012.2185041
PN 1
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QL
UT WOS:000306599300004
DA 2024-07-18
ER

PT J
AU Narwaria, M
   Lin, WS
   Liu, AM
AF Narwaria, Manish
   Lin, Weisi
   Liu, Anmin
TI Low-Complexity Video Quality Assessment Using Temporal Quality
   Variations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Multimedia and Expo (ICME)
CY JUL 11-15, 2011
CL Univ Ramon Llull, La Salle, Barcelona, SPAIN
SP IEEE, IEEE Signal Proc Soc, IEEE Circuits & Syst Soc, IEEE Comp Soc, IEEE Commun Soc
HO Univ Ramon Llull, La Salle
DE Machine learning; spatial quality; temporal quality variations; video
   quality assessment (VQA)
ID CONTRAST; MODEL
AB Objective video quality assessment (VQA) is the use of computational models to evaluate the video quality in line with the perception of the human visual system (HVS). It is challenging due to the underlying complexity, and the relatively limited understanding of the HVS and its intricate mechanisms. There are three important issues that arise in objective VQA in comparison with image quality assessment: 1) the temporal factors apart from the spatial ones also need to be considered, 2) the contribution of each factor (spatial and temporal) and their interaction to the overall video quality need to be determined, and 3) the computational complexity of the resultant method. In this paper, we seek to tackle the first issue by utilizing the worst case pooling strategy and the variations of spatial quality along the temporal axis with proper analysis and justification. The second issue is addressed by the use of machine learning; we believe this to be more convincing since the relationship between the factors and the overall quality is derived via training with substantial ground truth (i.e., subjective scores). Experiments conducted using publicly available video databases show the effectiveness of the proposed full-reference (FR) algorithm in comparison to the relevant existing VQA schemes. Focus has also been placed on demonstrating the robustness of the proposed method to new and untrained data. To that end, cross-database tests have been carried out to provide a proper perspective of the performance of proposed scheme as compared to other VQA methods.
   The third issue regarding the computational costs also plays a key role in determining the feasibility of a VQA scheme for practical deployment given the large amount of data that needs to be processed/analyzed in real time. A limitation of many existing VQA algorithms is their higher computational complexity. In contrast, the proposed scheme is more efficient due to its low complexity without jeopardizing the prediction accuracy.
C1 [Narwaria, Manish; Lin, Weisi; Liu, Anmin] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Narwaria, M (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM mani0018@ntu.edu.sg; wslin@ntu.edu.sg; liua0002@ntu.edu.sg
RI Lin, Weisi/A-3696-2011; Lin, Weisi/A-8011-2012
OI Lin, Weisi/0000-0001-9866-1947; Narwaria, Manish/0000-0001-7789-5322
CR [Anonymous], 2004, Kernel Methods for Pattern Analysis
   [Anonymous], 2003, Final report from the video quality experts group on the validation of objective models of video quality assessment
   [Anonymous], 2009, LIVE VID QUAL DAT
   [Anonymous], 2005, Digital Video Quality: Vision Models and Metrics
   [Anonymous], 2020, INT TELECOMMUNICATIO
   Barkowsky M, 2009, IEEE J-STSP, V3, P266, DOI 10.1109/JSTSP.2009.2015375
   Bovik A.C, 2005, A visual information fidelity approach to video quality assessment
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Chikkerur S, 2011, IEEE T BROADCAST, V57, P165, DOI 10.1109/TBC.2011.2104671
   De Simone F, 2009, INT WORK QUAL MULTIM, P204, DOI 10.1109/QOMEX.2009.5246952
   Girod Bernd, 1993, P207
   HAMBERG R, 1995, J OPT SOC AM A, V12, P2573, DOI 10.1364/JOSAA.12.002573
   Hekstra AP, 2002, SIGNAL PROCESS-IMAGE, V17, P781, DOI 10.1016/S0923-5965(02)00056-5
   Huynh-Thu Q, 2010, SIGNAL PROCESS-IMAGE, V25, P535, DOI 10.1016/j.image.2010.03.006
   Lambrecht CJV, 1996, P SOC PHOTO-OPT INS, V2668, P450, DOI 10.1117/12.235440
   Lee K, 2010, IEEE IMAGE PROC, P2493, DOI 10.1109/ICIP.2010.5652884
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu TJ, 2010, IEEE INT CON MULTI, P697, DOI 10.1109/ICME.2010.5583094
   Lubin Jeffrey, 1993, P163
   Ma L, 2010, IEEE IMAGE PROC, P2501, DOI 10.1109/ICIP.2010.5649188
   MANTEL C, 2009, P IEEE INT WORKSH QU, P94
   Moorthy AK, 2010, IEEE T CIRC SYST VID, V20, P1653, DOI 10.1109/TCSVT.2010.2087470
   Moorthy AK, 2009, IEEE J-STSP, V3, P193, DOI 10.1109/JSTSP.2009.2015374
   NACHMIAS J, 1974, VISION RES, V14, P1039, DOI 10.1016/0042-6989(74)90175-8
   Narwaria M, 2009, IEEE INT WORKSH MULT, P1, DOI 10.1109/MMSP.2009.5293244
   Ninassi A, 2009, IEEE J-STSP, V3, P253, DOI 10.1109/JSTSP.2009.2014806
   Nothdurft HC, 2000, VISION RES, V40, P1183, DOI 10.1016/S0042-6989(00)00031-6
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Scholkopf B., 2002, Encyclopedia of Biostatistics
   Seshadrinathan K, 2007, INT CONF ACOUST SPEE, P869
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P335, DOI 10.1109/TIP.2009.2034992
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Suresh N., 2006, P INT C AC SPEECH SI, P941
   Wan S, 2010, PROC SPIE, V7744, DOI 10.1117/12.863193
   Wandell B. A., FDN VISION
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, SIGNAL PROCESS-IMAGE, V19, P121, DOI 10.1016/S0923-5965(03)00076-6
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2007, J OPT SOC AM A, V24, pB61, DOI 10.1364/JOSAA.24.000B61
   WEBSTER AA, 1993, P SOC PHOTO-OPT INS, V1913, P15, DOI 10.1117/12.152700
   Winkler S, 1999, PROC SPIE, V3644, P175, DOI 10.1117/12.348438
   Winkler S, 2008, IEEE T BROADCAST, V54, P660, DOI 10.1109/TBC.2008.2000733
   You JY, 2010, IEEE INT CON MULTI, P914, DOI 10.1109/ICME.2010.5583037
   Zhai GT, 2008, IEEE T MULTIMEDIA, V10, P1316, DOI 10.1109/TMM.2008.2004910
NR 44
TC 58
Z9 64
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 525
EP 535
DI 10.1109/TMM.2012.2190589
PN 1
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 943ZG
UT WOS:000304166300004
DA 2024-07-18
ER

PT J
AU Tan, E
   Chou, CT
AF Tan, Evan
   Chou, Chun Tung
TI A Frame Rate Optimization Framework for Improving Continuity in Video
   Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive media playout; frame rate; Lyapunov; optimization; streaming;
   video
ID ADAPTIVE MEDIA PLAYOUT; DELAY; BUFFER; QUALITY; DESIGN; JITTER; AUDIO
AB This paper aims to reduce the prebuffering requirements, while maintaining continuity, for video streaming. Current approaches do this by making use of adaptive media playout (AMP) to reduce the playout rate. However, this introduces playout distortion to the viewers and increases the viewing latency. We approach this by proposing a frame rate optimization framework that adjusts both the encoder frame generation rate and the decoder playout frame rate. Firstly, we model this problem as the joint adjustment of the encoder frame generation interval and the decoder playout frame interval. This model is used with a discontinuity penalty virtual buffer to track the accumulated difference between the receiving frame interval and the playout frame interval. We then apply Lyapunov optimization to the model to systematically derive a pair of decoupled optimization policies. We show that the occupancy of the discontinuity penalty virtual buffer is correlated to the video discontinuity and that this framework produces a very low playout distortion in addition to a significant reduction in the prebuffering requirements compared to existing approaches. Secondly, we introduced a delay constraint into the framework by using a delay accumulator virtual buffer. Simulation results show that the delay constrained framework provides a superior tradeoff between the video quality and the delay introduced compared to the existing approach. Finally, we analyzed the impact of delayed feedback between the receiver and the sender on the optimization policies. We show that the delayed feedbacks have a minimal impact on the optimization policies.
C1 [Tan, Evan; Chou, Chun Tung] Univ New S Wales, Sch Comp Sci & Engn, Sydney, NSW 2052, Australia.
C3 University of New South Wales Sydney
RP Tan, E (corresponding author), Univ New S Wales, Sch Comp Sci & Engn, Sydney, NSW 2052, Australia.
EM evant@cse.unsw.edu.au; ctchou@cse.unsw.edu.au
RI Chou, Chun Tung/D-2844-2012; Tan, Evan/GYJ-6078-2022
OI Chou, Chun Tung/0000-0003-4512-7155; 
CR [Anonymous], GLOB TEL C 2005 GLOB
   [Anonymous], 2006, RESOURCE ALLOCATION
   Cheolhong An, 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, P89
   Chuang HC, 2007, IEEE T MULTIMEDIA, V9, P1273, DOI 10.1109/TMM.2007.902884
   Claypool M., 1999, ACM INT C MULTIMEDIA, P115
   Conklin GJ, 2001, IEEE T CIRC SYST VID, V11, P269, DOI 10.1109/76.911155
   Damnjanovic I, 2010, IEEE T MULTIMEDIA, V12, P247, DOI 10.1109/TMM.2010.2046296
   Deshpande S.G., 2008, Proceedings of ACM Multimedia, P777
   Deshpande S, 2007, PROC SPIE, V6507, DOI 10.1117/12.704531
   Dua A, 2007, GLOB TELECOMM CONF, P5226
   Fujimoto K, 2004, TELECOMMUN SYST, V25, P259, DOI 10.1023/B:TELS.0000014784.20034.74
   Gulliver SR, 2007, IEEE T BROADCAST, V53, P449, DOI 10.1109/TBC.2007.896955
   Handley M., 2003, TCP FRIENDLY RATE CO
   Kalman M, 2004, IEEE T CIRC SYST VID, V14, P841, DOI 10.1109/TCSVT.2004.828335
   Kalman M, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P189, DOI 10.1109/ICIP.2002.1038937
   Laoutaris N, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-10, CONFERENCE RECORD, P969, DOI 10.1109/ICC.2001.937381
   Lee H., 2002, IEEE T CIRCUITS SYST, V10, P878
   Li Y, 2008, IEEE T MULTIMEDIA, V10, P885, DOI 10.1109/TMM.2008.922860
   Li Y, 2006, IEEE T MULTIMEDIA, V8, P830, DOI 10.1109/TMM.2006.876236
   Li Z. G., 2003, P JVT G012 R1 7 M PA, V14
   Liang GF, 2008, IEEE T MULTIMEDIA, V10, P1128, DOI 10.1109/TMM.2008.2001364
   Liang YJ, 2003, IEEE T MULTIMEDIA, V5, P532, DOI 10.1109/TMM.2003.819095
   Markopoulou AP, 2003, IEEE ACM T NETWORK, V11, P747, DOI 10.1109/TNET.2003.818179
   Merritt L., 2007, IEEE, 1-4244-1437-7/07, P309
   Merritt Loren., x264: A high performance h. 264/avc encoder
   Moon SB, 1998, MULTIMEDIA SYST, V6, P17, DOI 10.1007/s005300050073
   Narbutt M, 2005, IEEE INTERNET COMPUT, V9, P28, DOI 10.1109/MIC.2005.72
   Neely M. J., 2003, Ph.D. thesis
   Neely MJ, 2006, IEEE T INFORM THEORY, V52, P2915, DOI 10.1109/TIT.2006.876219
   Ou YF, 2008, IEEE IMAGE PROC, P689, DOI 10.1109/ICIP.2008.4711848
   Roccetti M, 2001, MULTIMED TOOLS APPL, V14, P23, DOI 10.1023/A:1011303506685
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Seo J., 2008, P IEEE GLOB TEL C GL, P1
   Steinbach E, 2001, IEEE IMAGE PROC, P962, DOI 10.1109/ICIP.2001.959207
   Steinbach E., 2002, P IEEE INT C IM PROC, V1, P962
   Steyaert B, 2008, ANN OPER RES, V162, P159, DOI 10.1007/s10479-008-0322-5
   Stockhammer T, 2004, IEEE T MULTIMEDIA, V6, P268, DOI 10.1109/TMM.2003.822795
   Su YF, 2009, IEEE T MULTIMEDIA, V11, P1331, DOI 10.1109/TMM.2009.2030543
   Tan E., 2011, FRAME RATE OPTIMIZAT
   Tao DY, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1798
   Verhelst W., 1993, P ICASSP 93, V2
   Yuang M., 2002, IEEE J SEL AREA COMM, V15, P136
NR 42
TC 8
Z9 8
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 910
EP 922
DI 10.1109/TMM.2011.2180706
PN 2
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700021
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tsai, TH
   Lin, CY
AF Tsai, Tsung-Han
   Lin, Chung-Yuan
TI Exploring Contextual Redundancy in Improving Object-Based Video Coding
   for Video Sensor Networks Surveillance
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contextual redundancy coding; intelligent video surveillance;
   object-based video coding; operational rate-distortion theory; visual
   sensor network
ID ALGORITHMS
AB In recent years, intelligent video surveillance attempts to provide content analysis tools to understand and predict the actions via video sensor networks (VSN) for automated wide-area surveillance. In this emerging network, visual object data is transmitted through different devices to adapt to the needs of the specific content analysis task. Therefore, they raise a new challenge for video delivery: how to efficiently transmit visual object data to various devices such as storage device, content analysis server, and remote client server through the network. Object-based video encoder can be used to reduce transmission bandwidth with minor quality loss. However, the involved motion-compensated technique often leads to high computational complexity and consequently increases the cost of VSN. In this paper, contextual redundancy associated with background and foreground objects in a scene is explored. A scene analysis method is proposed to classify macroblocks (MBs) by type of contextual redundancy. The motion search is only performed on the specific type of context of MB which really involves salient motion. To facilitate the encoding by context of MB, an improved object-based coding architecture, namely dual-closed-loop encoder, is derived. It encodes the classified context of MB in an operational rate-distortion-optimized sense. The experimental results show that the proposed coding framework can achieve higher coding efficiency than MPEG-4 coding and related object-based coding approaches, while significantly reducing coding complexity.
C1 [Tsai, Tsung-Han; Lin, Chung-Yuan] Natl Cent Univ, Dept Elect Engn, Jhongli 3201, Taoyuan, Taiwan.
C3 National Central University
RP Tsai, TH (corresponding author), Natl Cent Univ, Dept Elect Engn, Jhongli 3201, Taoyuan, Taiwan.
EM han@ee.ncu.edu.tw; yashiro@dsp.ee.ncu.edu.tw
FU National Science Council, Taiwan [100-2220-E-008-001]
FX This work was supported by the National Science Council, Taiwan, under
   Grant 100-2220-E-008-001. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Thinh
   Nguyen.
CR AACH T, 1995, SIGNAL PROCESS-IMAGE, V7, P147, DOI 10.1016/0923-5965(95)00003-F
   [Anonymous], PATTERN RECOGNITION
   [Anonymous], IEEE T CIRCUITS SYST
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], P IEEE INT S CIRC SY
   [Anonymous], REC FIN DRA IN PRESS
   [Anonymous], P IEEE ICME
   [Anonymous], P IEEE INT C IM PROC
   [Anonymous], P IEEE INT C NETW SE
   [Anonymous], ELSEVIER COMPUT NETW
   [Anonymous], EURASIP J EMBED SYST
   [Anonymous], NEURAL COMPUTAT
   [Anonymous], ACM J COMPUT SURVEYS
   Babu R. V., 2006, PROC IEEE INT C CONT, P1
   Cavallaro A, 2005, IEEE T CIRC SYST VID, V15, P1200, DOI 10.1109/TCSVT.2005.854240
   Charfi Y, 2009, IEEE WIREL COMMUN, V16, P44, DOI 10.1109/MWC.2009.4907559
   Chen TC, 2006, IEEE T CIRC SYST VID, V16, P673, DOI 10.1109/TCSVT.2006.873163
   Fleck S, 2008, P IEEE, V96, P1698, DOI 10.1109/JPROC.2008.928765
   Ho WKH, 2005, IEEE T MULTIMEDIA, V7, P615, DOI 10.1109/TMM.2005.850959
   Hsieh JW, 2008, IEEE T MULTIMEDIA, V10, P372, DOI 10.1109/TMM.2008.917403
   Hu WM, 2004, IEEE T SYST MAN CY C, V34, P334, DOI 10.1109/TSMCC.2004.829274
   *ISO IEC, 2001, 144962 ISOIEC
   Kim KJ, 1999, IEEE T IMAGE PROCESS, V8, P1142, DOI 10.1109/83.777097
   Kondi LP, 2004, IEEE T CIRC SYST VID, V14, P528, DOI 10.1109/TCSVT.2004.825569
   Lee DS, 2005, IEEE T PATTERN ANAL, V27, P827, DOI 10.1109/TPAMI.2005.102
   Liao SC, 2010, PROC CVPR IEEE, P1301, DOI 10.1109/CVPR.2010.5539817
   Neal R.M., 1999, LEARNING GRAPHICAL M
   Radke RJ, 2005, IEEE T IMAGE PROCESS, V14, P294, DOI 10.1109/TIP.2004.838698
   SALEMBIER P, 1994, IEEE T IMAGE PROCESS, V3, P639, DOI 10.1109/83.334980
   SIKORA T, 1995, IEEE T CIRC SYST VID, V5, P59, DOI 10.1109/76.350781
   Trivedi MM, 2005, IEEE INTELL SYST, V20, P58, DOI 10.1109/MIS.2005.86
   Valera M, 2005, IEE P-VIS IMAGE SIGN, V152, P192, DOI 10.1049/ip-vis:20041147
   Yu Y, 2005, INT CONF ACOUST SPEE, P693
NR 33
TC 22
Z9 24
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 669
EP 682
DI 10.1109/TMM.2011.2180705
PN 2
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700002
DA 2024-07-18
ER

PT J
AU Ji, W
   Li, Z
   Chen, YQ
AF Ji, Wen
   Li, Zhu
   Chen, Yiqiang
TI Joint Source-Channel Coding and Optimization for Layered Video
   Broadcasting to Heterogeneous Devices
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Elastic video; fountain coding; heterogeneous QoS; optimization; SVC;
   video broadcasting
ID UNEQUAL ERROR PROTECTION; SCALABLE VIDEO; RESOURCE-ALLOCATION; RATELESS
   CODES; MULTICAST; TRANSMISSION; EXTENSION
AB Heterogeneous quality-of-service (QoS) video broadcast over wireless network is a challenging problem, where the demand for better video quality needs to be reconciled with different display size, variable channel condition requirements. In this paper, we present a framework for broadcasting scalable video to heterogeneous QoS mobile users with diverse display devices and different channel conditions. The framework includes joint video source-channel coding and optimization. First, we model the problem of broadcasting a layered video to heterogeneous devices as an aggregate utility achieving problem. Second, based on scalable video coding, we introduce the temporal-spatial content distortion metric to build adaptive layer structure, so as to serve mobile users with heterogeneous QoS requirements. Third, joint Fountain coding protection is introduced so as to provide flexible and reliable video stream. Finally, we use dynamic programming approach to obtain optimal layer broadcasting policy, so as to achieve maximum broadcasting utility. The objective is to achieve maximum overall receiving quality of the heterogeneous QoS receivers. Experimental results demonstrate the effectiveness of the solution.
C1 [Ji, Wen; Chen, Yiqiang] Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100190, Peoples R China.
   [Li, Zhu] Futurewei Technol, Bridgewater, NJ 08807 USA.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Huawei Technologies
RP Ji, W (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100190, Peoples R China.
EM jiwen@ict.ac.cn; zhu.li@ieee.org; yqchen@ict.ac.cn
FU National Natural Science Foundation of China [61001194]; Hong Kong
   Research Grant Council (RGC); Hong Kong Polytechnic University
FX Manuscript received February 24, 2011; revised July 06, 2011 and October
   10, 2011; accepted November 06, 2011. Date of publication November 29,
   2011; date of current version March 21, 2012. This work was supported in
   part by the National Natural Science Foundation of China (61001194), and
   Hong Kong Research Grant Council (RGC) and Hong Kong Polytechnic
   University new faculty grant. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Christophe De Vleeschouwer.
CR Abanoz TB, 2009, IEEE IMAGE PROC, P3741, DOI 10.1109/ICIP.2009.5414496
   Ahmad S, 2011, IEEE T MULTIMEDIA, V13, P92, DOI 10.1109/TMM.2010.2093511
   Ahmad S, 2010, IEEE T CIRC SYST VID, V20, P275, DOI 10.1109/TCSVT.2009.2031545
   [Anonymous], P IEEE ICC
   Bertsekas D. P, 2005, DYNAMIC PROGRAMMING, V1
   Byers JW, 2002, IEEE J SEL AREA COMM, V20, P1528, DOI 10.1109/JSAC.2002.803996
   Bystrom M, 2000, IEEE J SEL AREA COMM, V18, P880, DOI 10.1109/49.848242
   Cataldi P, 2010, IEEE T IMAGE PROCESS, V19, P1491, DOI 10.1109/TIP.2010.2042985
   Cernea DC, 2008, IEEE T MULTIMEDIA, V10, P503, DOI 10.1109/TMM.2008.917407
   Cheolhong An, 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, P89
   Chiang M, 2007, P IEEE, V95, P255, DOI 10.1109/JPROC.2006.887322
   Crowcroft J., 1998, P SIGCOMM AUG, V18, P247
   Eklund C., 2006, STANDARDS INFORM NET
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P511, DOI 10.1109/TCSVT.2002.800313
   Huang JW, 2008, IEEE T CIRC SYST VID, V18, P582, DOI 10.1109/TCSVT.2008.919109
   Jaspar X, 2007, P IEEE, V95, P1345, DOI 10.1109/JPROC.2007.896491
   Ji W., 2010, P ACM MULT OCT, P1223
   Ji W., 2011, P ICC JUN, P1
   Jurca D, 2009, IEEE T CIRC SYST VID, V19, P1315, DOI 10.1109/TCSVT.2009.2022800
   Kuo WH, 2011, IEEE T MULTIMEDIA, V13, P116, DOI 10.1109/TMM.2010.2082350
   Li Y, 2009, IEEE T MULTIMEDIA, V11, P1182, DOI 10.1109/TMM.2009.2026102
   Li Z, 2005, IEEE T IMAGE PROCESS, V14, P1550, DOI 10.1109/TIP.2005.854477
   Li Zhu, 2009, IEEE C CCNC JAN, V10, P1, DOI DOI 10.1088/0953-8984/21/9/095001
   Liu JC, 2004, IEEE T WIREL COMMUN, V3, P656, DOI 10.1109/TWC.2003.821216
   Liu ZY, 2010, IEEE J SEL AREA COMM, V28, P445, DOI 10.1109/JSAC.2010.100415
   Luby M, 2002, ANN IEEE SYMP FOUND, P271, DOI 10.1109/SFCS.2002.1181950
   Luby M, 2007, IEEE T BROADCAST, V53, P235, DOI 10.1109/TBC.2007.891703
   Maani E, 2008, IEEE T IMAGE PROCESS, V17, P1663, DOI 10.1109/TIP.2008.2001402
   Maani E, 2010, IEEE T CIRC SYST VID, V20, P407, DOI 10.1109/TCSVT.2009.2035846
   Maani E, 2009, IEEE T IMAGE PROCESS, V18, P2022, DOI 10.1109/TIP.2009.2023152
   MacKay DJC, 2005, IEE P-COMMUN, V152, P1062, DOI 10.1049/ip-com:20050237
   Majumdar A, 2002, IEEE T CIRC SYST VID, V12, P524, DOI 10.1109/TCSVT.2002.800315
   Multimedia Broadcast/Multicast Service (MBMS), 26346V610 3GPP TS MB
   Ou YF, 2011, IEEE T CIRC SYST VID, V21, P286, DOI 10.1109/TCSVT.2010.2087833
   OZAROW LH, 1994, IEEE T VEH TECHNOL, V43, P359, DOI 10.1109/25.293655
   Qian LM, 1999, IEEE DATA COMPR CONF, P414, DOI 10.1109/DCC.1999.755691
   Rahnavard N, 2007, IEEE T INFORM THEORY, V53, P1521, DOI 10.1109/TIT.2007.892814
   Raja N., 2007, EURASIP J ADV SIGNAL
   RAMCHANDRAN K, 1993, IEEE J SEL AREA COMM, V11, P6, DOI 10.1109/49.210540
   REICHEL J, 2006, JVTS202
   Schierl T., 2007, P ICIP SEP
   Schierl T, 2007, IEEE T CIRC SYST VID, V17, P1204, DOI 10.1109/TCSVT.2007.905528
   Schierl T, 2009, IEEE WIREL COMMUN, V16, P64, DOI 10.1109/MWC.2009.5300304
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Segall CA, 2007, IEEE T CIRC SYST VID, V17, P1121, DOI 10.1109/TCSVT.2007.906824
   Shokrollahi A, 2006, IEEE T INFORM THEORY, V52, P2551, DOI 10.1109/TIT.2006.874390
   Shutoy HY, 2007, IEEE J-STSP, V1, P295, DOI 10.1109/JSTSP.2007.901516
   Shutto N., 2007, P WCNC MAR, P2894
   Stoufs M, 2008, IEEE T CIRC SYST VID, V18, P1657, DOI 10.1109/TCSVT.2008.2004922
   Subramanian VG, 2010, IEEE T INFORM THEORY, V56, P2416, DOI 10.1109/TIT.2010.2040860
   Taubman D, 2005, IEEE T IMAGE PROCESS, V14, P1006, DOI 10.1109/TIP.2005.846028
   van der Schaar M, 2006, IEEE T MOBILE COMPUT, V5, P755, DOI 10.1109/TMC.2006.81
   Vukobratovic D, 2009, IEEE T MULTIMEDIA, V11, P1094, DOI 10.1109/TMM.2009.2026087
   Wagner JP, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1501, DOI 10.1109/ICME.2006.262827
   Ying-Hong Wang, 2009, 2009 Symposia and Workshops on Ubiquitous, Autonomic and Trusted Computing in conjunction with the UIC 2009 and ATC 2009 Conferences, P1, DOI [10.1109/ICBBE.2009.5163482, 10.1109/UIC-ATC.2009.19]
   Wang Y, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P577, DOI 10.1109/ICME.2008.4607500
   Wien M, 2007, IEEE T CIRC SYST VID, V17, P1194, DOI 10.1109/TCSVT.2007.905530
   Xiang W, 2009, IEEE T CIRC SYST VID, V19, P1730, DOI 10.1109/TCSVT.2009.2022787
   Xu Q, 2007, IEEE J SEL AREA COMM, V25, P851, DOI 10.1109/JSAC.2007.070520
   Yu Ying, 2009, Instrument Techniques and Sensor, P1
   Yuanyi Xue, 2010, 2010 18th International Packet Video Workshop (PV 2010), P201, DOI 10.1109/PV.2010.5706839
   Zhai F, 2006, IEEE T IMAGE PROCESS, V15, P40, DOI 10.1109/TIP.2005.860353
   Zhai F., 2007, Synthesis Lectures on Image, Video, and Multimedia Processing, V3, P1, DOI DOI 10.2200/S00061ED1V01Y200707IVM010
   Zhang Q, 2005, IEEE T CIRC SYST VID, V15, P482, DOI 10.1109/TCSVT.2005.844454
NR 64
TC 45
Z9 48
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2012
VL 14
IS 2
BP 443
EP 455
DI 10.1109/TMM.2011.2177645
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OZ
UT WOS:000302702500017
DA 2024-07-18
ER

PT J
AU Shuai, HH
   Yang, DN
   Cheng, WH
   Chen, MS
AF Shuai, Hong-Han
   Yang, De-Nian
   Cheng, Wen-Huang
   Chen, Ming-Syan
TI MobiUP: An Upsampling-Based System Architecture for High-Quality Video
   Streaming on Mobile Devices
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mobile video; video upsampling
ID IMAGE REPRESENTATION; ENHANCEMENT; RECOGNITION; RESOLUTION; SEQUENCES;
   SCHEME
AB Nowadays, mobile video streaming enables people to access digital content, such as online TV shows, music videos, sports reports, and news programs, anytime, anywhere. However, current streaming services in mobile networks are subject to the available wireless bandwidth shared among many users and can only provide videos with limited resolutions. Moreover, on recently developed high-resolution mobile devices, such as iPhone, Google Nexus One, Nokia N97, and SonyEricsson X10, the resolution of video streaming is much lower than the devices can actually support. As a result, existing video upsampling schemes usually introduce visual artifacts. In response to the above problem, we bridge the resolution gap between streaming videos and client screens, and propose a novel upsampling-based system architecture, called MobiUP, to enable high-quality video streaming onto mobile devices. To avoid modifying existing codecs for video streaming, MobiUP upsamples videos with decoded frames and appends a limited amount of metadata to the streaming videos for facilitating high-quality and real-time conversion from low resolution to high fullscreen resolution on the client side. In other words, the proposed upsampling architecture complements current systems. Therefore, MobiUP is generic and flexible, and it can be implemented easily on mobile devices for practical use. The implementation results demonstrate that, although the appended metadata is less than 8% of the total transmitted data, it improves the quality of the upsampled video significantly. Meanwhile, the computation time of MobiUP Client is close to that of bilinear upsampling algorithms implemented on mobile devices.
C1 [Shuai, Hong-Han; Yang, De-Nian; Cheng, Wen-Huang; Chen, Ming-Syan] Acad Sinica, Res Ctr Informat Technol Innovat CITI, Taipei 11529, Taiwan.
   [Shuai, Hong-Han; Chen, Ming-Syan] Natl Taiwan Univ, Grad Inst Commun Engn, Taipei 10617, Taiwan.
C3 Academia Sinica - Taiwan; National Taiwan University
RP Shuai, HH (corresponding author), Acad Sinica, Res Ctr Informat Technol Innovat CITI, Taipei 11529, Taiwan.
EM hhshuai@citi.sinica.edu.tw; dnyang@citi.sinica.edu.tw;
   whcheng@citi.sinica.edu.tw; mschen@citi.sinica.edu.tw
RI Cheng, Wen-Huang/AAK-2774-2020; Yang, De-Nian/AAQ-5465-2021
OI Yang, De-Nian/0000-0002-3765-9293; Chen, Ming-Syan/0000-0002-0711-8197
FU National Science Council (NSC), Taipei, Taiwan [100-2221-E-001-006-MY2,
   99-2219-E-002-029]
FX Manuscript received August 15, 2010; revised November 22, 2010 and March
   06, 2011; accepted April 28, 2011. Date of publication May 19, 2011;
   date of current version September 16, 2011. This work was supported in
   part by grants from the National Science Council (NSC
   100-2221-E-001-006-MY2 and NSC 99-2219-E-002-029), Taipei, Taiwan. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Francesco G. B. De Natale.
CR Adami N, 2007, IEEE T CIRC SYST VID, V17, P1238, DOI 10.1109/TCSVT.2007.906828
   Anagnostopoulos CNE, 2008, IEEE T INTELL TRANSP, V9, P377, DOI 10.1109/TITS.2008.922938
   [Anonymous], 2004, P 2004 C HUM FACT CO
   [Anonymous], 2009, P ICCV
   [Anonymous], P ICCV
   [Anonymous], ACM T GRAPH SIGGRAPH
   [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   Bao P, 2005, IEEE T PATTERN ANAL, V27, P1485, DOI 10.1109/TPAMI.2005.173
   Belfiore S, 2005, IEEE T MULTIMEDIA, V7, P316, DOI 10.1109/TMM.2005.843347
   Bellard F., FFMPEG MULTIMEDIA SY
   Black M. J., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P231, DOI 10.1109/ICCV.1993.378214
   Cheng WH, 2008, IEEE T CIRC SYST VID, V18, P1639, DOI 10.1109/TCSVT.2008.2005608
   Cho S, 2002, IEEE T CIRC SYST VID, V12, P157
   COX IJ, 1995, P ICIP
   Drajic D, 2007, IEEE T CONSUM ELECTR, V53, P1456, DOI 10.1109/TCE.2007.4429237
   Dugad R, 2001, IEEE T CIRC SYST VID, V11, P461, DOI 10.1109/76.915353
   DUGAD R, 1999, P ICIP OCT, V2, P903
   ESSEN V, 1992, SCIENCE, V255, P419
   EZRA MB, 2007, P ICCV
   Fattal R., 2007, P SIGGRAPH
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   GOLUB GH, 1970, NUMER MATH, V14, P403, DOI 10.1007/BF02163027
   Greenspan H, 2000, IEEE T IMAGE PROCESS, V9, P1035, DOI 10.1109/83.846246
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P511, DOI 10.1109/TCSVT.2002.800313
   Hirakawa K, 2006, IEEE T IMAGE PROCESS, V15, P2730, DOI 10.1109/TIP.2006.877352
   Ho CP, 2007, EURASIP J IMAGE VIDE, DOI 10.1155/2007/45201
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Irani M., 1993, Journal of Visual Communication and Image Representation, V4, P324, DOI 10.1006/jvci.1993.1030
   *ISO IEC MPEG TEST, 2004, N6383 ISOIEC MPEG TE
   Kanakia H, 1995, IEEE ACM T NETWORK, V3, P671, DOI 10.1109/90.477713
   Keogh E. J., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P285, DOI 10.1145/347090.347153
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   KNOCHE H, 2005, P 13 ANN ACM INT C M
   KNOCHE H, 2008, P 16 ACM INT C MULT
   LAM WM, 1992, P IEEE INT C AC SPEE
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Lin JL, 2005, IEEE T CIRC SYST VID, V15, P3, DOI 10.1109/TCSVT.2004.839997
   Ling YR, 2007, ISPRS J PHOTOGRAMM, V61, P381, DOI 10.1016/j.isprsjprs.2006.11.002
   Liu H., 1998, Mobile Networks and Applications, V3, P49, DOI 10.1023/A:1019108328296
   Malo J, 2006, IEEE T IMAGE PROCESS, V15, P68, DOI 10.1109/TIP.2005.860325
   Morse BS, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P227, DOI 10.1109/ICIP.1998.999013
   MULGREW B, 1998, DIGITAL SIGNAL PROCE
   PANG WM, 2008, ACM T GRAPH SIGGRAPH, V27
   PRATT A, 1978, DIGITAL IMAGE PROCES
   Protter M, 2009, IEEE T IMAGE PROCESS, V18, P1899, DOI 10.1109/TIP.2009.2022440
   Rath TM, 2003, PROC CVPR IEEE, P521
   Roth S, 2005, PROC CVPR IEEE, P860
   SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   SEPPA M, 2007, MED IMAGE ANAL
   Shen K, 1999, IEEE T CIRC SYST VID, V9, P109, DOI 10.1109/76.744279
   STETTINER Y, 1994, INT C PATT RECOG, P174, DOI 10.1109/ICPR.1994.577150
   Sun J, 2008, PROC CVPR IEEE, P2471, DOI 10.1109/CVPR.2008.4587659
   Thévenaz P, 2000, BIOMED EN S, P393
   TIAN D, 2008, P 2 INT S INT INF TE, V1, P899
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   UNSER M, 1991, IEEE T PATTERN ANAL, V13, P277, DOI 10.1109/34.75515
   VUKADINOVI V, 2005, P ACM INT S MOD AN S, P349
   WAN S, 2006, P MOB MULT COMM
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   WEN J, 1997, P IEEE INT C IM PROC, V2, P65
   Yang Jian., 2008, P CVPR
NR 63
TC 12
Z9 16
U1 1
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 1077
EP 1091
DI 10.1109/TMM.2011.2156910
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300020
DA 2024-07-18
ER

PT J
AU Petridis, S
   Pantic, M
AF Petridis, Stavros
   Pantic, Maja
TI Audiovisual Discrimination Between Speech and Laughter: Why and When
   Visual Information Might Help
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human behavior analysis; laughter-versus-speech discrimination; neural
   networks; principal components analysis (PCA)
ID RECOGNITION; FEATURES; MODELS; AUDIO
AB Past research on automatic laughter classification/detection has focused mainly on audio-based approaches. Here we present an audiovisual approach to distinguishing laughter from speech, and we show that integrating the information from audio and video channels may lead to improved performance over single-modal approaches. Both audio and visual channels consist of two streams (cues), facial expressions and head pose for video and cepstral and prosodic features for audio. Two types of experiments were performed: 1) subject-independent cross-validation on the AMI dataset and 2) cross-database experiments on the AMI and SAL datasets. We experimented with different combinations of cues with the most informative being the combination of facial expressions, cepstral, and prosodic features. Our results suggest that the performance of the audiovisual approach is better on average than single-modal approaches. The addition of visual information produces better results when it comes to female subjects. When the training conditions are less diverse in terms of head movements than the testing conditions (training on the SAL dataset, testing on the AMI dataset), then no improvement was observed with the addition of visual information. On the other hand, when the training conditions are similar (cross validation on the AMI dataset), or more diverse (training on the AMI dataset, testing on the SAL dataset), in terms of head movements than is the case in the testing conditions, an absolute increase of about 3% in the F1 rate for laughter is reported when visual information is added to audio information.
C1 [Petridis, Stavros; Pantic, Maja] Univ London Imperial Coll Sci Technol & Med, Dept Comp, London, England.
   [Pantic, Maja] Univ Twente, EEMCS, NL-7500 AE Enschede, Netherlands.
C3 Imperial College London; University of Twente
RP Petridis, S (corresponding author), Univ London Imperial Coll Sci Technol & Med, Dept Comp, London, England.
EM stavros.petridis04@imperial.ac.uk; m.pantic@imperial.ac.uk
FU European Research Council [ERC-2007-StG-203143]; European Community
   [211486]
FX This work was supported by the European Research Council under the ERC
   Starting Grant agreement no. ERC-2007-StG-203143 (MAHNOB). The work of
   S. Petridis was supported in part by the European Community's 7th
   Framework Programme [FP7/2007-2013] under grant agreement no. 211486
   (SEMAINE). The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Shrikanth
   Narayanan.
CR [Anonymous], P ICMI
   [Anonymous], P INT C CYB IEEE
   [Anonymous], 1953, KATHIMERINI
   [Anonymous], 2001, Emotions, qualia, and consciousness, DOI [DOI 10.1142/9789812810687_0033, 10.1142/9789812810687_0033]
   [Anonymous], P NIST M REC WORKSH
   [Anonymous], P 9 INT C MULT INT I
   [Anonymous], 2007, P INT WORKSH PHON LA
   [Anonymous], 2000, Laughter: A Scientific Investigation
   [Anonymous], 2007, Interspeech
   [Anonymous], 2005, P 6 INT 2005 9 EUR C
   [Anonymous], INT C NEUR NETW SAN
   Bachorowski JA, 2001, J ACOUST SOC AM, V110, P1581, DOI 10.1121/1.1391244
   Bachorowski JA, 2001, PSYCHOL SCI, V12, P252, DOI 10.1111/1467-9280.00346
   Boersma P., 1993, P I PHONETIC SCI, P97, DOI DOI 10.1371/JOURNAL.PONE.0069107
   BOUSMALIS K, 2009, P IEEE INT C AFF COM, V2
   Busso C, 2009, IEEE T AUDIO SPEECH, V17, P582, DOI 10.1109/TASL.2008.2009578
   Cai R, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P37
   CAMPBELL N, 2002, P LANG RES EV C
   COHN JF, 2005, INT J WAVELETS MULTI, V2, P121
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   DOUGLASCOWIE E, P WORKSH CORP RES EM, P1
   Dupont S, 2000, IEEE T MULTIMEDIA, V2, P141, DOI 10.1109/6046.865479
   Eibl-Eibesfeldt I., 1973, SOCIAL COMMUNICATION, P163
   Ellis D.P. W., 2005, PLP and RASTA (and MFCC, and inversion) in Matlab
   FURNAS GW, 1987, COMMUN ACM, V30, P964, DOI 10.1145/32206.32212
   González-Jiménez D, 2007, IEEE T INF FOREN SEC, V2, P413, DOI 10.1109/TIFS.2007.903543
   Graciarena M, 2006, INT CONF ACOUST SPEE, P1033
   Grammer K., 1990, NAT RLICHKEIT SPRACH, P192
   Guo GD, 2003, IEEE T NEURAL NETWOR, V14, P209, DOI 10.1109/TNN.2002.806626
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hudenko WJ, 2009, J AUTISM DEV DISORD, V39, P1392, DOI 10.1007/s10803-009-0752-1
   Janin A, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P364
   Kipper S, 2003, J NONVERBAL BEHAV, V27, P255, DOI 10.1023/A:1027384817134
   Knox MT, 2008, INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2008, VOLS 1-5, P797
   Laskowski K., 2007, INTERSPEECH, P1258
   Laskowski K, 2008, LECT NOTES COMPUT SC, V5237, P149, DOI 10.1007/978-3-540-85853-9_14
   Liu Y, 2006, COMPUT SPEECH LANG, V20, P468, DOI 10.1016/j.csl.2005.06.002
   LOCKERD A, 2002, P CHI HUM FACT COMP, P574
   Marks I.M., 2007, Hands on help: Computer-aided psychotherapy
   McCowan Iain, 2005, Proceedings of the 5th International Conference on Methods and Techniques in Behavioral Research, V88
   *NIST, 2004, RICH TRANSCR 2004 SP
   Oostdijk N., 2000, SPOKEN DUTCH CORPUS, P887
   Pantic M, 2007, LECT NOTES COMPUT SC, V4451, P47
   Pantic M, 2009, IEEE SIGNAL PROC MAG, V26, P173, DOI 10.1109/MSP.2009.934186
   Patras I, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P97, DOI 10.1109/AFGR.2004.1301515
   Petridis S, 2009, IEEE INT CON MULTI, P1444, DOI 10.1109/ICME.2009.5202774
   Petridis S, 2008, INT CONF ACOUST SPEE, P5117, DOI 10.1109/ICASSP.2008.4518810
   Petridis Stavros., 2008, Proceedings of the 2008 International Conference on Content-Based Image and Video Retrieval, P329
   Potamianos G, 2003, P IEEE, V91, P1306, DOI 10.1109/JPROC.2003.817150
   PROVINE RR, 1991, ETHOLOGY, V89, P115
   PROVINE RR, 1993, ETHOLOGY, V95, P291
   Rabiner Lawrence, 2010, Digital Processing of Speech Signals
   Reuderink B, 2008, LECT NOTES COMPUT SC, V5237, P137, DOI 10.1007/978-3-540-85853-9_13
   Rothgänger H, 1998, NATURWISSENSCHAFTEN, V85, P394, DOI 10.1007/s001140050522
   SCHERER KR, 1994, EMOTIONS: ESSAYS ON EMOTION THEORY, P161
   SCHROEDER M, 2006, P SPEECH PROS DRESD, P1
   Schuller B, 2008, LECT NOTES ARTIF INT, V5078, P99, DOI 10.1007/978-3-540-69369-7_12
   Trouvain J., 2003, P INT C PHON SCI, P2793
   TRUONG K, 2007, P WORKSH PHON LAUGHT
   Truong KP, 2007, SPEECH COMMUN, V49, P144, DOI 10.1016/j.specom.2007.01.001
   Valstar MF, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P38
   Vettin J, 2004, J NONVERBAL BEHAV, V28, P93, DOI 10.1023/B:JONB.0000023654.73558.72
   Vinciarelli A, 2009, IMAGE VISION COMPUT, V27, P1743, DOI 10.1016/j.imavis.2008.11.007
   Vlasenko B, 2007, LECT NOTES COMPUT SC, V4738, P139
   Yehia HC, 2002, J PHONETICS, V30, P555, DOI 10.1006/jpho.2002.0165
   Yin B, 2006, INT C PATT RECOG, P254
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
NR 67
TC 35
Z9 35
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2011
VL 13
IS 2
BP 216
EP 234
DI 10.1109/TMM.2010.2101586
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 738SG
UT WOS:000288661800005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xiang, SM
   Pan, CH
   Nie, FP
   Zhang, CS
AF Xiang, Shiming
   Pan, Chunhong
   Nie, Feiping
   Zhang, Changshui
TI Interactive Image Segmentation With Multiple Linear Reconstructions in
   Windows
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Comparative study; interactive image segmentation; multiple linear
   reconstructions in windows
AB This paper proposes an algorithm for interactive image segmentation. The task is formulated as a problem of graph-based transductive classification. Specifically, given an image window, the color of each pixel in it will be reconstructed linearly with those of the remaining pixels in this window. The optimal reconstruction weights will be kept unchanged to linearly reconstruct their class labels. The label reconstruction errors are estimated in each window. These errors are further collected together to develop a learning model. Then, the class information about the user specified foreground and background pixels are integrated into a regularization framework. Under this framework, a globally optimal labeling is finally obtained. The computational complexity is analyzed, and an approach for speeding up the algorithm is presented. Comparative experimental results illustrate the validity of our algorithm.
C1 [Xiang, Shiming; Pan, Chunhong] Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.
   [Nie, Feiping; Zhang, Changshui] Tsinghua Univ, Dept Automat, State Key Lab Intelligent Technol & Syst, TNList, Beijing 100084, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Tsinghua
   University
RP Xiang, SM (corresponding author), Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.
EM smxiang@gmail.com; chpan@nlpr.ia.ac.cn; feipingnie@gmail.com;
   zcs@mail.tsinghua.edu.cn
RI zhang, chao/IXD-9965-2023; Nie, Feiping/B-3039-2012; Zhang,
   Chang/HTO-2939-2023
FU Hi-Tech Research and Development (863) Program of China [2009AA012104];
   National Natural Science Foundation of China [60873161, 60975037];
   National Basic Research Program of China [2009CB320602]
FX This work was supported in part by the Hi-Tech Research and Development
   (863) Program of China (Grant No. 2009AA012104), in part by the National
   Natural Science Foundation of China (Grant No. 60873161, 60975037), and
   in part by the National Basic Research Program of China (Grant No.
   2009CB320602). The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Ton Kalker.
CR [Anonymous], P COMP GRAPH SIGGRAP, DOI DOI 10.1145/218380.218442
   [Anonymous], 1998, STAT LEARNING THEORY
   Barrett WA, 2002, ACM T GRAPHIC, V21, P777, DOI 10.1145/566570.566651
   Blake A, 2004, LECT NOTES COMPUT SC, V3021, P428
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Chuang YY, 2001, PROC CVPR IEEE, P264
   Gleicher M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P183, DOI 10.1145/218380.218441
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li S. Z., 2009, Markov random field modeling in image analysis
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mortensen E. N., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P452, DOI 10.1109/CVPR.1999.784720
   Pérez P, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P524, DOI 10.1109/ICCV.2001.937670
   Protiere A, 2007, IEEE T IMAGE PROCESS, V16, P1046, DOI 10.1109/TIP.2007.891796
   REESE LJ, 2002, P EUROGRAPHICS, V21, P714
   Rother C., 2004, ACM Transactions on Graphics (SIGGRAPH), V23, P309
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Ruzon MA, 2000, PROC CVPR IEEE, P18, DOI 10.1109/CVPR.2000.855793
   Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721
   Tan KH, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P337, DOI 10.1109/ICCV.2001.937538
   Wang J, 2005, IEEE I CONF COMP VIS, P936
   Xiang SM, 2008, PATTERN RECOGN, V41, P3600, DOI 10.1016/j.patcog.2008.05.018
   Xiang SM, 2010, IEEE T PATTERN ANAL, V32, P2039, DOI 10.1109/TPAMI.2010.35
   Xiang SM, 2009, IEEE T IMAGE PROCESS, V18, P1623, DOI 10.1109/TIP.2009.2018570
   Yang L, 2004, INT C PATT RECOG, P303, DOI 10.1109/ICPR.2004.1334180
   Zhao DL, 2006, PATTERN RECOGN, V39, P2233, DOI 10.1016/j.patcog.2006.05.007
   Zheng YJ, 2009, IEEE I CONF COMP VIS, P889, DOI 10.1109/ICCV.2009.5459326
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhu X., 2003, P 20 INT C MACH LEAR, V3, P58, DOI DOI 10.1109/18.850663
NR 29
TC 12
Z9 13
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2011
VL 13
IS 2
BP 342
EP 352
DI 10.1109/TMM.2010.2103930
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 738SG
UT WOS:000288661800015
OA Green Published
DA 2024-07-18
ER

PT J
AU Bentley, ES
   Kondi, LP
   Matyjas, JD
   Medley, MJ
   Suter, BW
AF Bentley, Elizabeth S.
   Kondi, Lisimachos P.
   Matyjas, John D.
   Medley, Michael J.
   Suter, Bruce W.
TI Spread Spectrum Visual Sensor Network Resource Management Using an
   End-to-End Cross-Layer Design
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Code division multiple access (CDMA); convolutional codes; cross-layer;
   H.264; joint source-channel coding; multimedia communications; power
   control; resource allocation; spread spectrum; visual sensor network
ID POWER-CONTROL
AB In this paper, we propose an approach to manage network resources for a direct sequence code division multiple access (DS-CDMA) visual sensor network where nodes monitor scenes with varying levels of motion. It uses cross-layer optimization across the physical layer, the link layer, and the application layer. Our technique simultaneously assigns a source coding rate, a channel coding rate, and a power level to all nodes in the network based on one of two criteria that maximize the quality of video of the entire network as a whole, subject to a constraint on the total chip rate. One criterion results in the minimal average end-to-end distortion amongst all nodes, while the other criterion minimizes the maximum distortion of the network. Our experimental results demonstrate the effectiveness of the cross-layer optimization.
C1 [Bentley, Elizabeth S.; Kondi, Lisimachos P.; Matyjas, John D.; Medley, Michael J.; Suter, Bruce W.] USAF, Res Lab, Rome, NY 13441 USA.
   [Kondi, Lisimachos P.] Univ Ioannina, Dept Comp Sci, GR-45110 Ioannina, Greece.
C3 United States Department of Defense; United States Air Force; University
   of Ioannina
RP Bentley, ES (corresponding author), USAF, Res Lab, Rome, NY 13441 USA.
FU Marie Curie International Reintegration Grant within the 7th European
   Community Framework Programme
FX Manuscript received July 10, 2009; revised January 31, 2010 and July 16,
   2010; accepted September 17, 2010. Date of publication October 11, 2010;
   date of current version January 19, 2011. This work was supported by a
   Marie Curie International Reintegration Grant within the 7th European
   Community Framework Programme. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Qian
   Zhang.
CR BENTLEY ES, 2009, P VIS COMM IM PROC C
   BYSTROM M, 1999, P IEEE INT C IM PROC, P359
   Chan YS, 2003, IEEE J SEL AREA COMM, V21, P1516, DOI 10.1109/JSAC.2003.815228
   CHOW KY, 2006, P 2006 1 INT S WIR P
   HAGENAUER J, 1988, IEEE T COMMUN, V36, P389, DOI 10.1109/26.2763
   Kondi LP, 2002, IEEE T IMAGE PROCESS, V11, P1043, DOI 10.1109/TIP.2002.802507
   Kondi LR, 2001, INT CONF ACOUST SPEE, P1377, DOI 10.1109/ICASSP.2001.941185
   Kwon H, 2006, IEEE T WIREL COMMUN, V5, P3689, DOI 10.1109/TWC.2006.05038
   Madan R, 2006, IEEE T WIREL COMMUN, V5, P3142, DOI 10.1109/TWC.2006.04770
   Messier GG, 2008, IEEE T WIREL COMMUN, V7, P2877, DOI 10.1109/TWC.2008.070208
   Pynadath ES, 2006, IEEE IMAGE PROC, P25, DOI 10.1109/ICIP.2006.313152
   Schulzrinne H, 2003, RTP TRANSPORT PROTOC
   Stockhammer T, 2003, IEEE T CIRC SYST VID, V13, P657, DOI 10.1109/TCSVT.2003.815167
   Zahariadis T., 2007, 2007 14th International Workshop in Systems, Signals and Image Processing and 6th EURASIP Conference focused on Speech and Image Processing, Multimedia Communications and Services - EC-SIPMCS 2007, P335, DOI 10.1109/IWSSIP.2007.4381110
NR 14
TC 18
Z9 18
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2011
VL 13
IS 1
BP 125
EP 131
DI 10.1109/TMM.2010.2086441
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 708TJ
UT WOS:000286386900013
DA 2024-07-18
ER

PT J
AU Seferoglu, H
   Markopoulou, A
   Kozat, UC
   Civanlar, MR
   Kempf, J
AF Seferoglu, Hulya
   Markopoulou, Athina
   Kozat, Ulas C.
   Civanlar, M. Reha
   Kempf, James
TI Dynamic FEC Algorithms for TFRC Flows
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Application layer FEC; congestion control; TCP friendly rate control
   (TFRC); video transport
ID CONGESTION; TCP
AB Media flows coexist with TCP-based data traffic on the Internet and are required to be TCP-friendly. The TCP protocol slowly increases its sending rate until episodes of congestion occur, and then it quickly reduces its rate to remove congestion. However, media flows can be sensitive to even brief episodes of congestion. In this paper, we are interested in protecting media flows from TCP-induced congestion while maintaining their TCP friendliness. In particular, we consider media flows carried over the TCP-Friendly Rate Control (TFRC) protocol and we design algorithms that dynamically adapt the level of forward error correction (FEC) based on the congestion state of the network. To this end, first, we investigate the loss and delay characteristics of TFRC flows in several TCP-induced congestion scenarios, and we develop novel predictors of loss events based on packet delay information. Second, we use these predictors to dynamically adapt the level of FEC protection based on the predicted level of congestion. We show that this technique can significantly improve the overhead versus reliability trade-off compared to fixed FEC. Third, we select the FEC and original media packets within each FEC block, in a rate-distortion optimized way, and we show that this technique significantly improves media quality.
C1 [Seferoglu, Hulya; Markopoulou, Athina] Univ Calif Irvine, Dept Elect Engn & Comp Sci, Irvine, CA 92697 USA.
   [Kozat, Ulas C.] DOCOMO USA Labs, Palo Alto, CA 94304 USA.
   [Civanlar, M. Reha] Ozyegin Univ, Istanbul, Turkey.
   [Kempf, James] Ericsson Res, San Jose, CA 95134 USA.
C3 University of California System; University of California Irvine; NTT
   Docomo; Ozyegin University; Ericsson
RP Seferoglu, H (corresponding author), Univ Calif Irvine, Dept Elect Engn & Comp Sci, Irvine, CA 92697 USA.
EM hseferog@uci.edu; athina@uci.edu; kozat@docomolabs-usa.com;
   reha.civanlar@ozyegin.edu.tr; james.kempf@ericsson.com
RI Civanlar, Mehmet/M-9929-2019
OI Civanlar, Mehmet/0000-0002-6171-5814; Markopoulou,
   Athina/0000-0003-1803-8675
FU DOCOMO USA Labs [DCL-45547]; National Science Foundation [0747110];
   Direct For Computer & Info Scie & Enginr [0747110] Funding Source:
   National Science Foundation; Division Of Computer and Network Systems
   [0747110] Funding Source: National Science Foundation
FX Manuscript received November 08, 2009; revised March 30, 2010; accepted
   June 03, 2010. Date of publication June 28, 2010; date of current
   version November 17, 2010. This work was supported in part by a grant
   from DOCOMO USA Labs under Contract DCL-45547 and by the National
   Science Foundation CAREER Award 0747110. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. S.-H. Gary Chan.
CR Andren J, 1998, GLOBECOM 98: IEEE GLOBECOM 1998 - CONFERENCE RECORD, VOLS 1-6, P1118, DOI 10.1109/GLOCOM.1998.776899
   ANDREW AM, 1979, INFORM PROCESS LETT, V9, P216, DOI 10.1016/0020-0190(79)90072-3
   [Anonymous], NETWORK SIMULATOR NS
   [Anonymous], 2003, 3448 RFC
   Bi JP, 2002, IEEE SYMP COMP COMMU, P3, DOI 10.1109/ISCC.2002.1021650
   Bolot J.-C., 1993, Journal of High Speed Networks, V2, P305
   Bolot JC, 1999, IEEE INFOCOM SER, P1453, DOI 10.1109/INFCOM.1999.752166
   Borella MS, 1998, PROCEEDINGS OF THE 1998 ICPP WORKSHOPS ON ARCHITECTURAL AND OS SUPPORT FOR MULTIMEDIA APPLICATIONS - FLEXIBLE COMMUNICATION SYSTEMS - WIRELESS NETWORKS AND MOBILE COMPUTING, P3, DOI 10.1109/ICPPW.1998.721868
   Brakmo L. S., 1994, Computer Communication Review, V24, P24, DOI 10.1145/190809.190317
   BROCKNERS F, 1999, P KOMM VERT SYST DAR, P250
   Cai JF, 2001, INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: CODING AND COMPUTING, PROCEEDINGS, P10, DOI 10.1109/ITCC.2001.918757
   Chou P.A., 2007, Multimedia over IP and Wireless Networks: Compression, Networking, and Systems
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   CLARKSON K, SHORT COMPLETE PLANA
   Duda R. O., 2000, PATTERN CLASSIFICATI
   FLETCHER R., 1987, PRACTICAL METHODS OP
   Futemma S, 2005, CONSUM COMM NETWORK, P110
   GANG S, 2008, P CISP HAIN CHIN MAY, P671
   *H264 AVC SOFTW CO, 2006, H 264 AVC REF SOFTW
   Huitema C, 1997, PROTOCOLS FOR HIGH-SPEED NETWORK V, P109
   *INT TRAFF REP, ANALOGX
   *ITU T, 2005, H264 ITUT
   KARANDE SS, 2004, P IEEE ICIP SING OCT
   Kunniyur S, 2003, IEEE ACM T NETWORK, V11, P689, DOI 10.1109/TNET.2003.818183
   Kunniyur SS, 2004, IEEE ACM T NETWORK, V12, P286, DOI 10.1109/TNET.2004.826291
   LEE TE, 2002, P 4 AS PAC C IND ENG, P1
   Lee TWA, 2001, GLOB TELECOMM CONF, P1994, DOI 10.1109/GLOCOM.2001.965922
   Liang YJ, 2002, CONF REC ASILOMAR C, P1315
   Marsan MA, 2005, IEEE ACM T NETWORK, V13, P1289, DOI 10.1109/TNET.2005.860102
   Martin J, 2003, IEEE ACM T NETWORK, V11, P356, DOI 10.1109/TNET.2003.813038
   Misra V, 2000, ACM SIGCOMM COMP COM, V30, P151, DOI 10.1145/347057.347421
   Moon S.B., 1998, CORRELATION PACKET D
   NGUYEN T, 2002, P PACK VID PITTSB PA, P212
   Park KH, 1998, IEEE IC COMP COM NET, P196, DOI 10.1109/ICCCN.1998.998777
   PRASAD RS, 2004, P PFLDNET WORKSH ARG
   Röder M, 2004, IEEE DATA COMPR CONF, P192
   Rubenstein D, 2002, IEEE ACM T NETWORK, V10, P381, DOI 10.1109/TNET.2002.1012369
   Seferoglu H., 2009, P PACK VID, P1
   Seferoglu H, 2007, SIGNAL PROCESS-IMAGE, V22, P529, DOI 10.1016/j.image.2007.04.002
   SHAN Y, 2008, P SPIE VCIP SAN JOS, P7
   SHOKROLLAHI A, 2004, P ISIT CHIC IL JUL, P2551
   TAAL JR, 2002, P IEEE ISCAS SCOTTSD, P53
   WAKAMIYA N, 2003, P QOS IP MIL IT FEB, P539
   Wei DX, 2006, IEEE ACM T NETWORK, V14, P1246, DOI 10.1109/TNET.2006.886335
   Wu Huahui., 2003, NOSSDAV 03, P122
NR 45
TC 3
Z9 3
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2010
VL 12
IS 8
BP 869
EP 885
DI 10.1109/TMM.2010.2053840
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 681ZW
UT WOS:000284365100008
DA 2024-07-18
ER

PT J
AU Masala, E
   Vesco, A
   Baldi, M
   De Martin, JC
AF Masala, Enrico
   Vesco, Andrea
   Baldi, Mario
   De Martin, Juan Carlos
TI Optimized H.264 Video Encoding and Packetization for Video Transmission
   Over Pipeline Forwarding Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Flexible macroblock ordering; pipeline forwarding
AB Previous works showed that the quality-of-service (QoS) requirements of multimedia applications can be optimally satisfied by pipeline forwarding (PF) by providing end-to-end delay guarantees as well as high network resource utilization. However, the unavoidable mismatch between reserved resources and the unpredictable traffic profile of a video stream has an impact on the resulting application layer quality. Therefore, a new low-complexity H. 264 video encoding and packetization scheme based on a distortion-optimized macroblock grouping technique is designed here to maximize the performance of video transmission on PF networks. The scheme considers the perceptual importance of the different parts of the video data to group the most important information in few packets that are the natural candidates to receive the deterministic service provided by PF. Results show peak signal-to-noise ratio (PSNR) gains up to 2.5 dB over traditional video encoding and packetization schemes, as well as more graceful degradation in case of high network load.
C1 [Masala, Enrico; Vesco, Andrea; Baldi, Mario; De Martin, Juan Carlos] Politecn Torino, Control & Comp Engn Dept, I-10129 Turin, Italy.
C3 Polytechnic University of Turin
RP Masala, E (corresponding author), Politecn Torino, Control & Comp Engn Dept, I-10129 Turin, Italy.
EM masala@polito.it; an-drea.vesco@polito.it; mario.baldi@polito.it;
   demartin@polito.it
RI Masala, Enrico/B-6973-2008
OI Masala, Enrico/0000-0001-8906-354X; Vesco, Andrea/0000-0001-7431-6655
CR Aravind R, 1996, IEEE T CIRC SYST VID, V6, P426, DOI 10.1109/76.538925
   Baldi M, 2000, IEEE ACM T NETWORK, V8, P31, DOI 10.1109/90.836476
   Baldi M, 2000, IEEE ACM T NETWORK, V8, P479, DOI 10.1109/90.865076
   BALDI M, 2000, P F SCS S PERF EV CO
   Baldi M, 2008, IEEE T BROADCAST, V54, P542, DOI 10.1109/TBC.2008.2000553
   Baldi M, 2007, COMPUT NETW, V51, P4092, DOI 10.1016/j.comnet.2007.04.019
   BUCCIOL P, 2007, J ADV MULT
   Chakareski J, 2005, IEEE T CIRC SYST VID, V15, P1257, DOI 10.1109/TCSVT.2005.854227
   Charny A., 2000, P QUAL FUT INT SERV
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   De Vito F, 2004, 2004 IEEE 6TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P79
   De Vito F, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P141, DOI 10.1109/ICME.2002.1035738
   GILBERT EN, 1960, BELL SYST TECH J, V39, P1253, DOI 10.1002/j.1538-7305.1960.tb03959.x
   Hannuksela MM, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P537, DOI 10.1109/ICIP.2002.1039026
   *ITU T, 2003, H2641449610 ITUT ISO
   Jerbi A, 2005, IEEE T CIRC SYST VID, V15, P1175, DOI 10.1109/TCSVT.2005.852619
   *JVT, 2007, JVT REF SOFTW V 11 0
   Lambert P, 2006, J VIS COMMUN IMAGE R, V17, P358, DOI 10.1016/j.jvcir.2005.05.008
   Li CS, 1998, COMPUT NETWORKS ISDN, V30, P2359, DOI 10.1016/S0169-7552(98)00258-X
   Liu Y, 2008, IEEE T CIRC SYST VID, V18, P134, DOI 10.1109/TCSVT.2007.913754
   Masala E, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P345
   Masala E, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P111, DOI 10.1109/MMSP.2001.962720
   Zhang R, 2000, IEEE J SEL AREA COMM, V18, P966, DOI 10.1109/49.848250
   Zhang R, 2001, CONF REC ASILOMAR C, P210, DOI 10.1109/ACSSC.2001.986907
   1997, UCB LBNL VINT NETWOR
NR 25
TC 8
Z9 9
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2009
VL 11
IS 5
BP 972
EP 985
DI 10.1109/TMM.2009.2021784
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 474KN
UT WOS:000268282300015
DA 2024-07-18
ER

PT J
AU Ren, JC
   Jiang, JM
AF Ren, Jinchang
   Jiang, Jianmin
TI Hierarchical Modeling and Adaptive Clustering for Real-Time
   Summarization of Rush Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Activity level; adaptive clustering; hierarchical modelling; TRECVID;
   video rushes summarization
ID CONTENT REPRESENTATION; SCENE DETECTION; SIMILARITY; ABSTRACTION;
   EXTRACTION; RETRIEVAL; FRAMEWORK; ADAPTATION; HIGHLIGHTS; SEQUENCES
AB In this paper, we provide detailed descriptions of a proposed new algorithm for video summarization, which are also included in our submission to TRECVID'08 on BBC rush summarization. Firstly, rush videos are hierarchically modeled using the formal language technique. Secondly, shot detections are applied to introduce a new concept of V-unit for structuring videos in line with the hierarchical model, and thus junk frames within the model are effectively removed. Thirdly, adaptive clustering is employed to group shots into clusters to determine retakes for redundancy removal. Finally, each most representative shot selected from every cluster is ranked according to its length and sum of activity level for summarization. Competitive results have been achieved to prove the effectiveness and efficiency of our techniques, which are fully implemented in the compressed domain. Our work does not require high-level semantics such as object detection and speech/audio analysis which provides a more flexible and general solution for this topic.
C1 [Ren, Jinchang; Jiang, Jianmin] Univ Bradford, Digital Media & Syst Inst, Bradford BD7 1DP, W Yorkshire, England.
C3 University of Bradford
RP Ren, JC (corresponding author), Univ Bradford, Digital Media & Syst Inst, Bradford BD7 1DP, W Yorkshire, England.
EM j.ren@bradford.ac.uk; j.jiang1@bradford.ac.uk
OI Ren, Jinchang/0000-0001-6116-3194
FU EU [IST-216709]; LIVE [IST-4-027312]; EPSRC [EP/E061419/1] Funding
   Source: UKRI
FX This work was supported by EU IST projects HERMES ( Contract No.
   IST-216709) and LIVE ( Contract No. IST-4-027312).
CR [Anonymous], 2007, P CIVR
   Babaguchi N, 2004, IEEE T MULTIMEDIA, V6, P575, DOI [10.1109/TMM.2004.830811, 10.1109/tmm.2004.830811]
   Bescós J, 2007, SIGNAL PROCESS-IMAGE, V22, P651, DOI 10.1016/j.image.2007.05.009
   Calic J, 2007, IEEE T CIRC SYST VID, V17, P931, DOI 10.1109/TCSVT.2007.897466
   Cerneková Z, 2006, IEEE T CIRC SYST VID, V16, P82, DOI 10.1109/TCSVT.2005.856896
   Chang SF, 2005, P IEEE, V93, P148, DOI 10.1109/JPROC.2004.839600
   Chen LH, 2003, J VIS COMMUN IMAGE R, V14, P358, DOI 10.1016/S1047-3203(03)00036-1
   Cheung SCS, 2005, IEEE T MULTIMEDIA, V7, P524, DOI 10.1109/TMM.2005.846906
   Cheung SCS, 2003, IEEE T CIRC SYST VID, V13, P59, DOI 10.1109/TCSVT.2002.808080
   Dimitrova N, 2004, IEEE MULTIMEDIA, V11, P7, DOI 10.1109/MMUL.2004.6
   Doulamis AD, 2000, SIGNAL PROCESS, V80, P1049, DOI 10.1016/S0165-1684(00)00019-0
   Doulamis N, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P297, DOI 10.1109/ICME.2002.1035777
   Doulamis ND, 2000, IEEE T CIRC SYST VID, V10, P501, DOI 10.1109/76.844996
   Drew MS, 2003, IMAGE VISION COMPUT, V21, P705, DOI 10.1016/S0262-8856(03)00065-9
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   Ferman AM, 2003, IEEE T MULTIMEDIA, V5, P244, DOI 10.1109/TMM.2003.811617
   Fonseca PM, 2004, SIGNAL PROCESS-IMAGE, V19, P685, DOI 10.1016/j.image.2004.04.005
   Gianluigi C, 2006, J REAL-TIME IMAGE PR, V1, P69, DOI 10.1007/s11554-006-0001-1
   Guironnet M, 2007, EURASIP J IMAGE VIDE, DOI 10.1155/2007/60245
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P1114, DOI 10.1109/TMM.2005.858397
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P1280, DOI 10.1109/76.809162
   Hanjalic A, 2007, IEEE T CIRC SYST VID, V17, P261, DOI 10.1109/TCSVT.2007.890833
   Ju SX, 1998, IEEE T CIRC SYST VID, V8, P686, DOI 10.1109/76.718513
   Kashino K, 2003, IEEE T MULTIMEDIA, V5, P348, DOI 10.1109/TMM.2003.813281
   Kim C, 2002, IEEE T CIRC SYST VID, V12, P1128, DOI 10.1109/TCSVT.2002.806813
   Kim JG, 2003, INT J IMAG SYST TECH, V13, P267, DOI 10.1002/ima.10067
   Lee JH, 2003, IEEE T CONSUM ELECTR, V49, P742, DOI 10.1109/TCE.2003.1233813
   Lehane B, 2007, EURASIP J IMAGE VIDE, DOI 10.1155/2007/14615
   Li Y, 2006, IEEE SIGNAL PROC MAG, V23, P79
   Li Z, 2005, IEEE T CIRC SYST VID, V15, P1245, DOI 10.1109/TCSVT.2005.854230
   Li Z, 2005, IEEE T IMAGE PROCESS, V14, P1550, DOI 10.1109/TIP.2005.854477
   Lienhart R, 1997, COMMUN ACM, V40, P54, DOI 10.1145/265563.265572
   Liu TM, 2003, IEEE T CIRC SYST VID, V13, P1006, DOI 10.1109/TCSVT.2003.816521
   Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410
   Money AG, 2008, J VIS COMMUN IMAGE R, V19, P121, DOI 10.1016/j.jvcir.2007.04.002
   Ngo CW, 2005, IEEE T CIRC SYST VID, V15, P296, DOI 10.1109/TCSVT.2004.841694
   Otsuka I, 2005, IEEE T CONSUM ELECTR, V51, P112, DOI 10.1109/TCE.2005.1405707
   OVER P, 2008, P INT WORKSH TRECVID
   Peng YX, 2006, IEEE T CIRC SYST VID, V16, P612, DOI 10.1109/TCSVT.2006.873157
   REN JC, IEEE T CIRC IN PRESS
   REN JC, 2008, P INT WORKSH TRECVID
   Tjondronegoro D, 2004, IEEE MULTIMEDIA, V11, P22, DOI 10.1109/MMUL.2004.28
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   WANG HL, 2000, CHIN ANIM QUANRANTIN, V17, P36
   WANG T, 2007, P INT WORKSH TRECVID, P79
   You JY, 2007, IEEE T CIRC SYST VID, V17, P273, DOI 10.1109/TCSVT.2007.890857
   Zhu LD, 2004, SENSOR ACTUAT B-CHEM, V98, P115, DOI 10.1016/S0925-4005(03)00640-3
   Zhu XQ, 2005, IEEE T MULTIMEDIA, V7, P648, DOI 10.1109/TMM.2005.850977
   Zhu XQ, 2003, MULTIMEDIA SYST, V9, P31, DOI 10.1007/s00530-003-0076-5
NR 50
TC 41
Z9 42
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2009
VL 11
IS 5
BP 906
EP 917
DI 10.1109/TMM.2009.2021782
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 474KN
UT WOS:000268282300010
OA Green Accepted
DA 2024-07-18
ER

EF